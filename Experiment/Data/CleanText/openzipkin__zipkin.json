{
    "johanoskarsson": "+1\n. +1\n. Done in #10\n. +1\n. @carrino - if you put together a start script feel free to contribute it back, would be helpful for others trying out Zipkin.\n. @carrino - if you put together a start script feel free to contribute it back, would be helpful for others trying out Zipkin.\n. Fixed in #32\n. Fixed in #32\n. +1\n. Odd, that worked when I ran it on EC2 the other day. I know they've been working on elephant-bird artifacts recently though so something might have changed. I'll look into it.\n. In the mean time you should be fine with fetching this branch https://github.com/kevinweil/elephant-bird/tree/branch-3.0 and running mvn install in that, then use the snapshot version in Zipkin. Apologies for the inconvenience.\n. Looks like they've change the url to the elephant-bird repo. I've updated it in this pull request #14. Can you give that one a go and see if it works for you?\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. Fixed by @glynd in #116. Thanks!\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. Yeah we squash all commits before we merge.\n. Yeah we squash all commits before we merge.\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1.\n. +1. \n. +1\n. +1\n. +1\n. Yeah as long as it all works on 2.9.2. Feel free to post a pull request.\nThanks!\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1 except the above nit\n. +1\n. Good point about the version. We actually use the old school Ruby 1.8.7. This week is kinda crazy but I'll try to make some time to try this pull request out on 1.8.7 too. \nAs a more long term solution @franklinhu is working on a branch that will replace the ruby parts with scala. It turns out that we didn't need a lot of logic in the web layer besides what the query daemon can do and the javascript.\n. +1\n. There's one unresolved nit, but other than that +1\n. +1\n. +1\n. Thanks for this, we'll merge it shortly. (turns out there's a bug in our merging script :)\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. Do we always want the spans to be merged and sorted? As long as there's no breakage and noticeable overhead I'm +1 on the change.\n. Do we always want the spans to be merged and sorted? As long as there's no breakage and noticeable overhead I'm +1 on the change.\n. Ok, sounds good to me. +1\n. Ok, sounds good to me. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. Couple of nits that you may or may not wish to fix. +1 otherwise. Note that as usual I have no idea what is going on in the javascript, so I will assume it works well.\n. +1\n. +1\n. Is null unsupported there? Is that a regression or has it always been?\n. +1\n. Rubberstamp +1 due to lack of JS knowledge :)\n. +1\n. This stuff is complex enough to warrant a README describing how it works, what configs to change and so on.\n. +1 except for the above\n. Looks good, but should be possible to squeeze in a test or two\n. +1\n. +1. I assume you'll adjust the original readme into a shorter form in another branch?\n. +1\n. Do we want both? Either way, I'm +1\n. +1\n. +1\n. +1\n. +1. Make sure you canary this before full rollout since it touches a lot of core code\n. +1. See nit above.\n. +1\n. +1\n. +1\n. Overall a good idea. Some feedback:\n- I'd change zipkin-cassie to zipkin-cassandra so as not to tie it to the client used.\n- We might get away without the -core modules for now, we will get tons of modules with this change.\n- Make sure you clearly write up which module does what in the readme (or maybe even a readme per module)\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. Wouldn't it be better to leave out timestamps completely out of the json if we don't have a value for it?\n. +1\n. +1\n. +1\n. +1. Perhaps add a readme describing the module but also mentioning that it is experimental as per your email.\n. +1\nYay!\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. Perhaps add a test?\n. +1\n. +1\n. +1\n. Thanks for contributing this back. Great to have support for more storage systems.\nI'll add a few comments inline, but one general request would be to add more comments. Class level describing what the class does and how + method level if it's not immediately clear what is going on (err on the side of adding too many comments).\n. For those of us that aren't familiar with redis a quick readme describing how to use this module with redis would be cool (including a quick starter on how to get redis running or a link to such a doc).\n. For the comments you don't have to add one if the trait you extend have comments already, unless your implementation does something worth bringing up.\nFor private methods that are complex or hard to understand I would add a comment, but not needed for the ones where the name and parameters or the code is self explanatory. For public methods I would do the same, but lower your sensitivity to complex methods, meaning: unless it's absolutely clear what is going on I'd add a comment. Methods like getTimeToLive: Duration you don't have to add a comment to for example.\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. Some more javadoc in general would be nice, otherwise +1\n. +1\n. Seems to have broken the build. Do the tests pass locally for you?\n. +1\n. +1\n. +1 from me, but Franklin is probably a better reviewer for this one\n. +1\n. I'll let Franklin review this one, but to my untrained frontend eyes it looks good.\n. This is quite the beast of a branch. Generally it would be easier to review if it was split into a few different pull requests. Seems there's a couple of things happening: removing hadoop component, upgrading sbt, upgrading dependencies and cleaning up the deprecation notices. Do they all have to be done in one go?\nI can't remember, what was the reason for removing the Hadoop module?\n. Is this change something that is required by sbt? Otherwise it would be preferable to keep the same directory structure as all other projects:\nzipkin-thrift/src/main/thrift/zipkinCore.thrift \u2192 ...hrift/src/main/resources/thrift/\n. Beyond the above comments it gets a +1 for me, but we should make sure at least Franklin reviews it too since it's i a massive change. \nI kind of skimmed most of the sbt stuff, don't think I can provide much in terms of useful feedback there.\n. Mostly because I prefer squashed commits. I generally work on a branch and commit all kinds of crap while I work on it. It's the end result that counts.\n/Johan\nOn 17 maj 2013, at 10:39, Brian Degenhardt wrote:\n\nWhy not just use the web-based tools? \nOn Fri, May 17, 2013 at 10:30 AM, Johan Oskarsson \nnotifications@github.comwrote: \n\nI attempted to merge a pull request, but it seems our little merge scripts \nis broken somehow. \n[marburg zipkin (master)]$ bin/git-pull-request.rb merge 245 \nExecuting: 'git config --get github.token' \nwarning: peer certificate won't be verified in this SSL session \nExecuting: 'git config --get github.token' \nwarning: peer certificate won't be verified in this SSL session \nbin/git-pull-request.rb:187:in `merge': Not merging into master \n(RuntimeError) \nfrom bin/git-pull-request.rb:306 \n\u2014 \nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/246 \n. \n\u2014\nReply to this email directly or view it on GitHub.\n. +1\n. +1 from me.\nI didn't have time to dig deep into the monoid stuff so I'll trust you on that, will try to read up later and we can always have a discussion about it if I have concerns.\n. +1 from me.\nI didn't have time to dig deep into the monoid stuff so I'll trust you on that, will try to read up later and we can always have a discussion about it if I have concerns.\n. Maybe open a ticket for the flaky test?\n+1 on this change\n. Maybe open a ticket for the flaky test?\n+1 on this change\n. Apologies for the slow reply on my part. Lowercasing the AnnotationsIndex would be fine by me.\n\n\nThanks!\n/Johan\nOn 5 nov 2013, at 00:43, Linlin Fu wrote:\n\nThe root cause is, in cassandra database, AnnotationsIndex stores RowKey with uppercase letters, but ServiceNames, ServiceNameIndex, SpanNames, ServiceSpanNameIndex use only lowercase letters.\nThere are two solution, one is AnnotationsIndex uses lowercase letters, the other solution is ServiceNames, ServiceNameIndex, SpanNames, ServiceSpanNameIndex NOT convert to lowercase. Which solution is preferred from your perspective? I can make the change and send a pull request.\n\u2014\nReply to this email directly or view it on GitHub.\n. Apologies for the slow reply on my part. Lowercasing the AnnotationsIndex would be fine by me.\n\nThanks!\n/Johan\nOn 5 nov 2013, at 00:43, Linlin Fu wrote:\n\nThe root cause is, in cassandra database, AnnotationsIndex stores RowKey with uppercase letters, but ServiceNames, ServiceNameIndex, SpanNames, ServiceSpanNameIndex use only lowercase letters.\nThere are two solution, one is AnnotationsIndex uses lowercase letters, the other solution is ServiceNames, ServiceNameIndex, SpanNames, ServiceSpanNameIndex NOT convert to lowercase. Which solution is preferred from your perspective? I can make the change and send a pull request.\n\u2014\nReply to this email directly or view it on GitHub.\n. +1\n. This seems useful to me and not a big change code/ui wise from what I can tell. I'll let someone that knows front end stuff chime in though.\n. The standard approach to this is to have all servers on UTC time and ntpd running to sync the times. We do try to \"rejig\" the times during display as best we can, but the assumption made is that the servers are reasonably in sync.\n. Nice one!\n. We should probably remove this. A start might be to see if we can serve the static content outside of Ruby.\n. Let's make this file a symlink\n. The service name is hardcoded to zipkinui here, port is also set to 0 always.\n. This should probably also be configurable and have a more sensible default\n. Could just use the CoreClient in constants instead?\n. Would be nice with a comment here describing what the adapter does, what the annotaitonTypeType means and so on.\n. Should this be empty?\n. No need for this file to be changed as discussed on IM\n. Would be nice with a class level comment explaining the purpose of this class, same for the other filters and new classes.\n. Unused import\n. Remove this block and put the Apache header in the file\n. A comment describing what the preprocessing does would be great\n. Can this use the preprocessing util?\n. Can this use the preprocessing util?\n. A comment here describing what the job does. Also add Apache header.\n. Can this use the preprocessing util?\n. A comment here describing what the job does. Also apache header.\n. Can this use the preprocessing util?\n. A comment describing the job and an apache header.\n. Can this use the preprocessing util?\n. A javadoc that describes what this does. A return type would also be nice.\n. Method Javadoc and a return type would be cool.\n. Please remove this comment and add an apache header.\n. Do we need the println here?\n. Let's remove this block and add an apache header.\n. This test doesn't really test anything\n. Remove this block and add apache header\n. Let's remove this println\n. Remove this println\n. Remove this\n. Remove this\n. Javadoc here that describes the job\n. Remove this block and add apache2 header\n. Let's remove this\n. Let's remove this\n. Try using a case class to label the entries in the map (if possible in Scalding). See for example: http://daily-scala.blogspot.com/2010/05/zipwithindex.html\n. This duplicates the one in util\n. For some of this stuff it might make sense to depend on zipkin-common and use the Span object in there. That already has code for extracting service name for example. We can leave that for a follow up branch though.\n. Probably more Scala-like to do a match here\n. Reuse preprocessing here?\n. Can we remove this code?\n. Let's add a comment here describing what it does\n. Let's add a javadoc for each of these sources describing what they do\n. Can these lines be removed?\n. Do we need these two commented out lines?\n. Can we remove this?\n. Indentation gets a bit weird here\n. See if you can use a case here instead of _1 as in the other class\n. Indent this, gets hard to read otherwise\n. Is this match needed? I don't know if Scalding supports it but normally you can do things like:\n\ncollection foreach { case (traceId: Long, spanId: Long) => println(traceId) }\nor something similar.\n. Is this used?\n. Is this used?\n. I'm getting this now:\n[error] /Users/johan/Dev/zipkin/zipkin-hadoop/src/main/scala/com/twitter/zipkin/hadoop/DependencyTree.scala:21: PrepSpanSource is not a member of sources\n[error] import sources.{PreprocessedSpanSource, PrepSpanSource, Util}\n. Our code style is to have this on the above line. I know that's slightly different from what IntelliJ reformats to but let's try to stick to our version.\n. Is it possible to use the Thrift isSetParent_id (or whatever it is called) here instead? As mentioned the id !=0 is fine as a hack, but if possible we should figure out the correct solution.\n. Let's keep this one out of master, I think we had some problems with it causing the build to fail.\n. Let's remove it completely if it's not used.\n. clientgit?\n. Is this needed? Let's come up with a better name in that case. Just adding a 1 to the name isn't very descriptive.\n. Let's remove this\n. And this\n. I assume this wasn't intended?\n. If we don't store any data in this format the change is fine. Later on if we keep data around what you'd want to do is leave 9 empty and keep service_name at 10. That way when you read old data you won't get unexpected results.\n. Let's add a comment here describing that the deps below are from elephant bird and why we made that intransitive.\n. Do we need all of these now?\n. This isn't needed since the job runner isn't using sbt-assembly.\n. Do you need elephantbird for the job runner too?\n. Let's add a bit more meat here. Make sure it's clear that it's a collector server we're talking about and that the popular keys power the typeahead part of the lookup page on the ui (the annotations/kv annotations input boxes).\n. Let's call it hadoop-job-runner instead\n. Let's add a comment here and on the TraceTimeline that explains what it is\n. Is this used anywhere? I'd prefer if we could put it somewhere other than util. Ideally wherever it is used, but if that's not doable let's create a new file called AnnotationConstants or something like that.\n. Typo\n. Our convention is to have this on the line above\n. We can use \"find\" here since it's only one entry we're looking for.\n. Is this needed?\n. Have you tried pulling in the same version of finagle that we use for the rest of the project? Too different?\n. Can these be removed?\n. Move out the logging to before the match?\n. I'd suggest adding some comments to this class. Maybe even describe the use and purpose of each endpoint?\n. Shouldn't this be in a config and not hardcoded?\n. Would be nice with a few comments in here. Maybe describing the type of requests or just what happens in the apply method\n. Should this be $ENDTIME $ENDTIME?\n. Maybe include the actual sbt/git commands here?\n. Perhaps make a note here that we don't actually fully rely on zipkin-finatra yet but that it is the future?\n. This hostname should not be in the open source version\n. Please describe what the input format is like.\n. What does ssnm stand for? Worth expanding into the full name or at least describe in comment?\n. Since you're doing the same for all of them you can just catch Throwable.\n. Worth looking into how much work it would be to use util-logging instead lf println:ing stuff. That way we can have the default to log to the console but can also redirect to local file or via scribe.\n. Is there no library we rely on that has this already? Util? Apache Commons? Worth moving to util otherwise?\n. Move this out to a constant or even a parameter?\n. Rename dur to something that also tells us the unit type\n. You should be able to use find here instead of foreach.\n. Indentation is out of whack here and below\n. Revert this change\n. Is there anything we can do to avoid this step? Seems annoying that we'd require a third party http server for just that.\n. Not important, but you can tighten up a lot of these match clauses by doing them on one line\n. Should be services I assume\n. This is getting kinda indentation heavy.  One thing would be to to val blah = request.params.... and then return the Some(blah) version. Perhaps break the time_annotation part into a private method? I dunno.\n. You need to pull in master\n. Nop diff\n. Nop diff\n. Isn't this calling itself?\n. Let's add a comment here describing each these two column families.\n. Do a git checkout master filename to get that version\n. You shouldn't have to block here, but instead chain the futures so you can return one that executes this first and then the batch\n. Is this for some future use? Vs just using the string straight in the QueryRequest\n. Move this out into a constant (or perhaps config?) somewhere with a description as to why we do it\n. ZIPKIN_TRACE_URL should be a setting and not a constant. Must have missed that in another branch.\n. Let's change all the println to proper logging statements\n. Wouldn't it make more sense to split out the client and the email into two classes? Create an instance of the client and send multiple emails to it.\n. Some botched merge here\n. Should be closed\n. We should keep the license.\n. Comment describing this in more detail would be nice. What's the unit?\n. Technically we can get parent ids that are 0, even though it's not all that likely. If it's possible to use an option here that would be preferable.\n. Since we're running out of time, let's just put a TODO here in that case describing the issue.\n. No big deal, but you can call apply by just adding (). So instead of .apply() just do ().\n. Remove this and add license.\n. Should be a var\n. Is this reported as a metric elsewhere? Seems like something we'd like to know.\n. Can you add a comment here describing what the class does?\n. This should change too I assume?\n. Something more descriptive?\n. Perhaps describe what the collector sampling stage does in a bit more detail here? Mention that if the collector gets too much traffic the next step would be to use the adaptive sampler.\n. Seems restrictive to hardcode 0.0.0.0 here. Add another val for host?\n. Comment here describing what the expiresAt is seconds would be nice\n. I'd generally prefer to not cram in too much in addition to the method declaration on one line\n. Some of these don't seem to be tested, worth adding a few specs\n. There's some old Cassandra references here that I assume can be safely removed\n. I'd actually prefer javadoc style here with a @param expiresAt  so that it shows up properly in generated docs.\n. No need for the variable\n. I assume Ostrich doesn't support POST?\n. Should probably add some tests for this\n. Wow, this feels very wrong.\n. Would be nice to keep this\n. How does it shut down now?\n. These don't do much.\n. Probably worth adding a short javadoc to some of these methods\n. It's easier to read the code if the return type is included in public methods. For private ones I don't have a strong opinion either way.\n. Forgot one thing in the last review: would be nice with some quick tests for these new methods too\n. I don't quite understand this change of return type. Was this incorrect before?\n. I'd prefer to keep this as close to the original scribe file as possible (since that's beyond our control). Ideally this file would just be brought in via an artifact from the scribe project, but I don't think they publish it.\n. There's already a floor method in Time\n. This is a bit unclear to me. What is meant by running behind a local Zipkin instance?\n. There's also a Redis storage system used by Tumblr. Since Cassandra is the one we've focused on maybe just mention that there's another one in passing.\n. Hmm, I'd rather we don't put a TODO in the readme. Verify the steps below?\n. This is a bit unclear. Statistical information about stuff? Why isn't m1 called mean if that's what it is?\n. Bring over some of the api comment from the rest api here since that would be converted into javadoc in some generated code (or whatever the language might be).\n. Thrift convention is start_time\n. endtime too?\n. There's no Time.fromMilliseconds?\n. I think there's a min on Time you can use directly\n. I'm not very familiar with implicits or the Monoid stuff, but if we're just merging two dependencies why not make that a method on the Dependencies instance instead? One could argue the method should be called + too, but I don't know if that's better or worse.\n. Nit: we're trying to adhere to ):  without the space before : \nI know we still have this style in some places, but we're trying to clean that up\n. Isn't it better if we just close the store here and the store takes care of closing the aggregates if needed?\n. Nit: duration_moments\n. Remind me again why we have to convert the ip to a string to compare? Would be nice to avoid if possible\n. ",
    "carrino": "I could have sworn I tried that.  And I see it in my history. But yes, that seems to work.  :)\n. ",
    "eirslett": "I'm closing this issue since it's very old. We now have multiple documented ways of deploying zipkin, including docker containers.\n. Closing due to lack of activity.\n. Closing due to lack of activity.\n. Closing due to lack of activity. We're moving away from Scribe and recommend using Kafka instead.\n. Closing due to lack of activity.\n. This should be fairly well documented by now.\n. Closing due to lack of activity.\n. This probably doesn't apply anymore.\n. The hadoop job is already replaced with a spark job.\n. I think this is already implemented.\n. The releases should work now. They have been changed a lot since 2012 :wink:\n. Closing due to lack of activity.\n. Fantastic, thanks a lot! :-)\nio.zipkin it is!\n. Closing due to lack of activity.\n. Closing due to lack of activity. Feel free to reopen if this is still a problem!\n. Closing due to lack of activity. It has also been superseded by other issues about configuration.\n. Closing due to lack of activity. The dependency view works out-of-the-box with MySQL, and for Cassandra, it works with the Spark job.\n. Closing due to lack of activity. We're already on Scala 2.11 anyways.\n. My understanding is that this was implemented in a PR. So this issue should be fixed. If not, please reopen!\n. Closing due to lack of activity. HBase support was dropped.\n. Closing due to lack of activity. zipkin-web/zipkin-ui is also refactored a lot. This issue probably doesn't apply anymore.\n. We could base64-encode the ipv6 address, in JSON it's a pain to work with binary data...\n. Closing due to lack of activity. hbase support is dropped.\n. I think they are documented now. Anyways, closing due to lack of activity.\n. Zipkin is not the right tool for this - use something like sensu.\n. +1\n. Closing due to lack of activity.\n. Is this in the context of Java, instrumenting a Java application with Brave, but it doesn't already have a Brave plugin for it? Or is it more general, like a haskell application, given we have no haskell zipkin library?\n. Is this in the context of Java, instrumenting a Java application with Brave, but it doesn't already have a Brave plugin for it? Or is it more general, like a haskell application, given we have no haskell zipkin library?\n. Closing due to lack of activity. I'm sure this should be fixed by now? Or maybe not?\n. By the way, if somebody's impatient to try out the frontend UI, you can run it (locally) from https://github.com/eirslett/zipkin/tree/feature/aggregate-web-ui. But the branch is experimantal and wil be rebased without notice!\n. By the way, if somebody's impatient to try out the frontend UI, you can run it (locally) from https://github.com/eirslett/zipkin/tree/feature/aggregate-web-ui. But the branch is experimantal and wil be rebased without notice!\n. I updated the PR according to @johnynek's comments. Also had to bump versions of finagle and twitter-util to 6.20.0 because of some classpath crashes in zipkin-web.\n. I updated the PR according to @johnynek's comments. Also had to bump versions of finagle and twitter-util to 6.20.0 because of some classpath crashes in zipkin-web.\n. @sprsquish @johnynek We've run this job in production for some time now. We had to override some memory settings though: https://github.com/finn-no/zipkin/commit/a75fe749ba6754f3128078d9e5b2117adc39d785\nIs it possible to set these hadoop flags (for this job only) without \"hardcoding\" it in Scalding?\nIs there anything else that needs to be fixed before you can merge the PR?\n. @sprsquish @johnynek We've run this job in production for some time now. We had to override some memory settings though: https://github.com/finn-no/zipkin/commit/a75fe749ba6754f3128078d9e5b2117adc39d785\nIs it possible to set these hadoop flags (for this job only) without \"hardcoding\" it in Scalding?\nIs there anything else that needs to be fixed before you can merge the PR?\n. @sprsquish @johnynek Have you had time to review this? Is there something that has to be fixed in this PR? Or is it something that you will end up not merging eventually?\n. @sprsquish @johnynek Have you had time to review this? Is there something that has to be fixed in this PR? Or is it something that you will end up not merging eventually?\n. Ok, I rebased it against master, but unfortunately, with the latest changes, the tests fail :-(\nCould you please help me out a bit, if you see what the problem is?\n. Ok, I rebased it against master, but unfortunately, with the latest changes, the tests fail :-(\nCould you please help me out a bit, if you see what the problem is?\n. +1\n. +1\n. Closing due to lack of activity.\n. Closing due to lack of activity.\n. If you want to populate test data:\n1) Configure a local Zipkin instance running on Cassandra\n2) Run zipkin-tracegen\n3) Run the hadoop job\n4) Look at the web UI - traces should now be generated.\nAlternatively: I'll see if I can make a modified JSON dump of our production data and host it somewhere as a static page.\n. If you want to populate test data:\n1) Configure a local Zipkin instance running on Cassandra\n2) Run zipkin-tracegen\n3) Run the hadoop job\n4) Look at the web UI - traces should now be generated.\nAlternatively: I'll see if I can make a modified JSON dump of our production data and host it somewhere as a static page.\n. OK, I (think) I fixed the indentation now.\n. OK, I (think) I fixed the indentation now.\n. Ok, I guess it's cumbersome to set up hadoop and everything just for some sample data, so here's an alternative.\n1) Start Zipkin with a local MySQL instance\n2) Dump this into the database: https://gist.github.com/eirslett/302b070cf6bff9598420\n3) You're ready to go!\n. Ok, I guess it's cumbersome to set up hadoop and everything just for some sample data, so here's an alternative.\n1) Start Zipkin with a local MySQL instance\n2) Dump this into the database: https://gist.github.com/eirslett/302b070cf6bff9598420\n3) You're ready to go!\n. Yes, it would definitely be nice to dump that test data into the example project.\nBut I'm not quite sure how to solve the dependency links between modules: zipkin-anormdb probably shouldn't know about test data, since that's only relevant for zipkin-example. But AnormSpanStore doesn't expose a public API with \"importSql\" functionality, and I don't think it should? So how should we dump the data into Anorm? (Or, eventually, should we break any of the dependency barriers I mentioned?)\n. Yes, it would definitely be nice to dump that test data into the example project.\nBut I'm not quite sure how to solve the dependency links between modules: zipkin-anormdb probably shouldn't know about test data, since that's only relevant for zipkin-example. But AnormSpanStore doesn't expose a public API with \"importSql\" functionality, and I don't think it should? So how should we dump the data into Anorm? (Or, eventually, should we break any of the dependency barriers I mentioned?)\n. com.twitter.zipkin.storage.Aggregates. has a storeDependencies method. I could hardcode the test data in a AggregateTestData Scala object and put that either in zipkin-example or zipkin-tracegen? Then the test data should also work independent of which database backend you're using.\n. com.twitter.zipkin.storage.Aggregates. has a storeDependencies method. I could hardcode the test data in a AggregateTestData Scala object and put that either in zipkin-example or zipkin-tracegen? Then the test data should also work independent of which database backend you're using.\n. I commited the SQL test dump into zipkin-tracegen for now, let me know if you want me to convert it to a Scala dump!\n. I commited the SQL test dump into zipkin-tracegen for now, let me know if you want me to convert it to a Scala dump!\n. I'm late here, but I think it should be possible to replace scribe with fluentd: https://github.com/fluent/fluentd\nFluentd comes with a Scribe plugin, so you wouldn't need to make any changes to the Zipkin source code.\nIt's also possible to set up fluentd with secure forwarding, so you could potentially ship your trace data to a public cloud, different from where you're running your application, and then run Zipkin there.\n. I'm late here, but I think it should be possible to replace scribe with fluentd: https://github.com/fluent/fluentd\nFluentd comes with a Scribe plugin, so you wouldn't need to make any changes to the Zipkin source code.\nIt's also possible to set up fluentd with secure forwarding, so you could potentially ship your trace data to a public cloud, different from where you're running your application, and then run Zipkin there.\n. It would be nice to have a logstash-compatible data collection pipeline as well. It should be safe to assume that people have something logstash-ish on their servers, if they are building a microservice platform in the first place?\n. I know fluentd does \"routing pr topic\", so you can send \"zipkin\" messages to zipkin-collector and \"log\" messages to elasticsearch/kibana. Can we do the same thing with logstash now?\n. @michaelsembwever was this fixed?\n. How about using HikariCP?\n. Interesting... So they're performing approximately the same? ( @brettwooldridge is this a normal case for HikariCP? I haven't used it myself but I've heard its performance is a quite dramatic improvement over other connection pools.)\n. Closing due to lack of activity. The Dependency tab should be there now.\n. Great :-) +1\n. Are these the kinds of APIs we're talking about?\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-common/src/main/scala/com/twitter/zipkin/storage/SpanStore.scala\nGiven this not-Java-friendly API:\nscala\ntrait MyStore\nclass MyStoreImpl\nOne easy way to provide Java friendliness for traits, is to make abstract classes for them:\nscala\ntrait MyStore\nabstract class AbstractMyStore extends MyStore\nAnd then use that from Java:\njava\nclass MyStoreImpl extends AbstractMyStore {}\nTwitter already does this in a couple of places, e.g. AbstractFunction.\nIn these cases it could be enough to just provide stub abstract classes for Java developers to implement.\nOne thing I think could be particularly valuable, is to rip out the Zipkin stuff from Finagle, rewrite it more or less exactly from Scala to Java, and then use that code from both Finagle, Brave, and other places. (Basically, a reference implementation without external dependencies)\n. https://github.com/twitter/util/blob/master/util-core/src/main/scala/com/twitter/util/Closable.scala#L11\nMaybe this works?\nabstract class AbstractMyStore extends AbstractClosable with MyStore\nand then in Java\nclass MyStoreImpl extends AbstractMyStore {}\n. None of the stuff inside the openzipkin/zipkin repository (or twitter/zipkin) is actually released as libraries (as far as I know), but only deployed to production directly from the master branch. And it's self-contained. So as long as we refactor across the whole project, backwards compatibility shouldn't be an issue.\n. Lookout has done some work on publishing zipkin builds to bintray; we could let people use them instead og building from source.\nOne of the issues then is how to plug in the various storage backends, runtime? Dynamic linking?\n. > which will help attract more developers to the community\nI'm afraid it could repel some Scala developers though... I can only speak for myself, but I'd much prefer most of (~99 %) the code base to still be Scala, with only a few Java hooks here and there. (Of course with the exception of the instrumentation code that's going into people's business applications, which should be Java-based like Brave.) The difference between rewriting \"key interfaces\" and \"the whole Zipkin domain model\" is pretty big, and deciding to rewrite the entire Zipkin core is a different decision altogether, I would say.\n. I suggest we make some small changes to the Zipkin server-side code so that it's easier to use both from Java and Scala, without having to rewrite all of it to Java.\n. +1 Great work!\n. Remember to assign adrian to the issues first!\n. I suggest we use the firefox directory as root, and call this repo zipkin-firefox-extension (or something like that). I don't think this plugin will browser-agnostic anytime soon...\n. Maybe even dig up the history since before sprsquish made that big-move-commit...\n. It has been green for a while now! :smile:\n. +1\n. There's some confusion about what should actually be rewritten... It would be a lot of effort to port the whole Zipkin code base to Java, just to get rid of the Scala dependency inside query/collector/web.\nWhat we discussed at the workshop was to extract the \"Tracer\" client from finagle, rewrite it in Java and then use that?\nOur use case at FINN was that we wanted to use Zipkin without having Scala in all our ~300 applications. Collector/query/web are 3 projects that are written in Scala anyways (using the Scala ecosystem and maintained by Scala developers) so that's not a problem at all.\nI really don't want to see Zipkin rewritten with Spring/Struts/Hibernate etc...\n. @b can you clarify a bit, is the Scala reluctancy about not wanting the Scala dependencies in their business applications (very understandable) or is it about not wanting to deploy Zipkin at all because Zipkin is written in Scala? (somewhat less understandable)\n. @NiteshKant could you please clarify a bit what Netflix' point of view is (or was, regarding the choice of reimplementing Dapper in Java)\nYou explained at the workshop, but clarification is always nice :-) Was avoiding Scala in the business applications the deciding factor, or is/was it very important that the Zipkin applications themselves were also Java?\n. One thing that could potentially cause some confusion is that we have different ways of configuring collector/query (with .Scala files) than web (flags). Would it be worth the effort to rewrite to one \"universal\" method?\n. One thing that separates Zipkin from in-house applications is the need to replace backend technologies like Cassandra/MySQL/HBase. So simple flags might not be enough - e.g. MySQL may have other configuration options than Redis. In that regard, configuration as code is actually quite nice...\nThe way we configure Zipkin today (with Scala files and eval) is actually quite close to Spring XML configuration, except it's Scala, so we actually ship the Scala compiler with the Zipkin distribution (which of course increases artifact size and dependency tree quite a bit). I think we even load the Scala compiler in-memory as part of Zipkin, but I'm not quite sure how the eval module is written.\n. If you take a deep look at the config stuff, there's actually a lot going on besides just setting properties, like wiring in resolvers for looking up Cassandra (ZooKeeper or static), injecting custom stats integrations pr. service (statsd maybe) and so on. I'm not sure if that can be expressed in a clean way just by using properties?\n. +1\n. +1 go for it\n. Multi-module sbt projects often have monolith build definitions inside project/, I think one of the reasons is that it's easier to share common configuration between modules (versions of Scala, Finagle, for example)\nI'm not sure if that's possible to do in any other way. I've had some multi-module projects with separate .sbt files in every module, which I eventually refactored to have everything in the same build file.\n. Oh, looks like you made it work! Disregard my comment ;-)\n. mostly about the effort ;-)\n. You can use @Rule in Scala if you use JUnitSuite:\nhttp://doc.scalatest.org/1.7/org/scalatest/junit/JUnitSuite.html\nThen you won't need the painful $.MODULE$ stuff that comes from bad Java-Scala-interop... (I'm working on making Span more Java-friendly!)\n. Great work! I love that you could delete all that code!\nkafkaRule could maybe be a lazy val, but whatever.\nCheers! :beers:\n. Looks very good! +1 But maybe squash them into 1 commit?\n. That looks good to me! :-) +1 for merge, let's just wait for another reviewer then\n. I'm going to merge this now, looks like it didn't break anything - but it definitely fixed the Windows build. (Nobody probably cares, except for me)\n. Sounds like a good feature! Could you please post some simple screenshots?\n. This looks great!\nOnly nitpick is maybe the zoom button could be placed in the lower right part of the screen, together with the back-to-top button? (That was in a recently merged commit)\n. Sweet! What does it mean that the text on a span is red instead of black?\n. +1 for merge! Once it's rebased/squashed\n. I think I've actually encountered this issue as well, and applied the same fix. +1 for merge!\n. Two hours is too long, yes. This might not be a good option after all...\n. I'll close this one.\n. Is it possible to render a graph of the dependencies between the projects?\n. Thanks a lot for the graph! It's quite connected...\n. There was a +1 on closing this ticket, so I'll do it!\n. I was thinking the same... why do we have both zipkin-kafka and zipkin-receiver-kafka... I think we should dump it.\n. +1 for mockito :-)\n. +1\n. Nobody even knows what zipkin-kafka is... Nuke it! :boom:\n. Where does async-http-client come from? Should be easy to replace with finagle-http if we have control over the source.\n. Aah, I think I can spot the problem - publish to bintray uses Dispatch, while Scrooge uses Finagle (with an older version of Netty, probably)\nI thought sbt used separate class loaders for each plugin, maybe not...\n. Oops, thanks! Looks like a simple bug, so I'll merge it right away.\n. +1 let's move it out :-)\n. Closing due to lack of activity.\n. Thanks, that looks great! +1\n. +1\nMaybe Scala 2.11 makes the build faster?\n. +1\n. Or maybe use 0.8.17?\n. We should be able to revert back to scala-extensions once we're on Scala 2.11.\n. Or so I think...\n. +1\n. +1 for moving redis out!\n. zipkin.io was recently fixed, thanks @abesto ! I'll close this issue, I don't think it's relevant any longer.\n. How should one override the configuration then? I think they should be kept outside of the jar until we have replaced them with something else. We need at least some kind of support for runtime configuration? E.g. the Cassandra host...\n. With some form of hierarchy?\n-backend=cassandra -backend-cassandra-host=123 -backend-cassandra-port=456\n-backend=X -backend-X-Y=...\nWe also discussed using Guice, and letting users wire up the dependencies as JSON.\n. spring-core maybe, and xml-based configuration? The Finagle stack doesn't use servlets, only pure Finagle HTTP.\n. But this issue is derailing into a duplicate of #466 \n. So we put them in src/main/resources or something?\n. Your point about bin scripbs is quite good; changing them isn't trivial (given some people may have put the bin scripts in their puppet setup etc)\n. Closing due to lack of activity.\n. Closing due to lack of activity.\n. +1\n. @adriancole was this fixed?\n. @abesto I think your work on the publishing/releasing resolved this.\n. Wow, this is some Great work! :grinning: :clap: :+1: \n. We can run zipkin-aggregate off a released version of zipkin-cassandra (with cassie). For now. But I'm pretty sure @michaelsembwever has some experience working with Cassandra taps?\n. Is zipkin the most continuously delivered product at Twitter now? :stuck_out_tongue_winking_eye: \n. Well, let's not worry about zipkin-mongodb and zipkin-hbase releases until somebody actually asks for it.\n. +1 LGTM! :beers:\n. I have no strong opinions. Scala traits (e.g. SpanStore) aren't instantiatable from Java anyways, you'll need to make an abstract class that mixes in the trait, and then in Java extend from that class. Will moving to j.u.Closeable solve that?\n. Sweet!\nA last, finishing touch; should we add a migration guide or changelog? Where the TLDR version is; wipe your old Cassandra keyspace and start from scratch?\n. Oh, sorry for the lack of response!\n+1 LGTM!\n. > We don't need the fat jar for -aggregate\nDo you mean we don't need the tar.gz file? Hadoop only accepts single jars, so one needs to ship it all-in-one.\n. It's showing this:\nhttps://raw.githubusercontent.com/finn-no/zipkin/finn-master/zipkin-finn.png\nA map over all the services and how they interact.\nUnfortunately, as a side effect of the Cassandra driver rewrite, we lost support for the aggregate job, and I haven't had time (yet) to rewrite it.\nI thought the first step would be to make a functioning prototype that runs on Hadoop in docker (with zipkin-docker), but that involves forking Twitter's Bijection library due to some Scala compilation issues.\n. Nice! +1\n. It's green! Let's do it! +1\n. But are we sure Twitter will be able to stay up to date? (Do we care?)\n. But Twitter runs on the published fat-jars now anyways, right? Developers working on Zipkin can fork the project from GitHub and build locally with JDK8 and Gradle. (Pants is gone anyways)\n. Ok, source compatibility with Java 7 is probably a good idea then. (We won't gain anything by not being source compatible)\n. Nice feature! But could you put the javascript code together with the rest of the javascript? The input box could be mounted as a flight.js component.\n. +1 LGTM\n. Twitter the company isn't using Cassandra anymore, but Zipkin the open source software is using Cassandra a lot (I think Cassandra is regarded as the main storage backend)\nZipkin was originally written at Twitter, but now, the project has been moved out into its own organisation.\n. +1 :beers:\n. Will this break backwards-compatibility for clients that send traces that are not snappy-encoded?\n. Timing checks could be moved to unit tests that we can run whenever we need to benchmark changes.\n. We could add a cache layer with an environment variable to set TTL (default to 0 seconds, which disables the cache?)\nIdeally, people shouldn't have to do custom builds, if we can avoid it?\n. Another option is to deploy varnish as middleware on top of zipkin-query, to handle the caching. Then we only need cache headers, but the deployment becomes much more complex.\n. I haven't tested the UI with 1000 service names yet. Might be worthwhile to stub the JSON API calls, and measure the impact on DOM rendering time in isolation?\n. I tried creating 1000 service names in the UI to measure the impact on rendering time:\n``` diff\ndiff --git a/zipkin-web/src/main/resources/app/js/component_ui/serviceName.js b/zipkin-web/src/main/resources/app/js/component_ui/serviceName.js\nindex f5b4cc6..55fb66e 100644\n--- a/zipkin-web/src/main/resources/app/js/component_ui/serviceName.js\n+++ b/zipkin-web/src/main/resources/app/js/component_ui/serviceName.js\n@@ -23,6 +23,11 @@ define(\n   this.updateServiceNameDropdown = function(ev, data) {\n     $('#serviceName').empty()\n\n\ndata.serviceNames = [];\nfor (let i = 0; i < 1000; i++) {\ndata.serviceNames.push('service'+i);\n}\n+\n         $.each(data.serviceNames, function(i, item) {\n             $('').val(item).text(item).appendTo('#serviceName');\n         });\n```\n\nThis basically replaces the API response with a list of service names, service0, service1, service2 ... service999.\nThe impact on rendering time was minimal, there was no visual cue showing that the latency was higher than when rendering 15-20 service names.\n. Aah, we need to encodeURIComponent() it in the JavaScript layer, I guess,\n. I've experienced the same thing.\n. I think it would have to happen on the server side (unless you want 10 TB of trace data sent to the browser so it can be filtered there)\nActually it would have to happen in the database query - I have no idea how that should work... The easiest thing would probably be to write an Elasticsearch backend, which is much more powerful when it comes to searching.\n. Closing due to lack of activity. Feel free to reopen if more people ask for it, or we have the resources to implement it.\n. Great work! :)\n. Great work! :)\n. Zipkin-web should definitely be a javascript-only app, either hosted from zipkin-query or on top of an http server like apache/nginx!\n. Zipkin-web should definitely be a javascript-only app, either hosted from zipkin-query or on top of an http server like apache/nginx!\n. Raise your hand if you want test coverage on the JavaScript!\n. Yup, it would be nice to use something else than dagre!\nOne option we could do in the meantime is to bundle dagre-d3 with its version of d3 in a self-contained, isolated module. Then, the rest of the UI could be developed independently, and draw the graph by using the graph's public API. jQuery and D3 would be hidden inside the private scope and not pollute the global namespace.\n. Yup, it would be nice to use something else than dagre!\nOne option we could do in the meantime is to bundle dagre-d3 with its version of d3 in a self-contained, isolated module. Then, the rest of the UI could be developed independently, and draw the graph by using the graph's public API. jQuery and D3 would be hidden inside the private scope and not pollute the global namespace.\n. The half-dirty version (because it could be dirtier) would be to commit d3 and dagre-d3 (the current versions) into version control, rename them to \"d3-legacy\" and \"dagre-d3-legacy\", and import them in require.js with these names.\n. The half-dirty version (because it could be dirtier) would be to commit d3 and dagre-d3 (the current versions) into version control, rename them to \"d3-legacy\" and \"dagre-d3-legacy\", and import them in require.js with these names.\n. Which hook?\n. Which hook?\n. Off with their heads!\n. Off with their heads!\n. Is this still an issue? I suspect we have an openzipkin user now, but I'm not so sure.\n. Closing due to lack of activity. Zipkin is no longer based on Finagle directly, but on Finatra.\nI think you're left with 2 options:\n1) put nginx/apache/something in front of zipkin, to handle authentication (cannot be fine-grained)\n2) fork zipkin, to make a version with authentication support\nWe're quite limited on resources, so I don't think autentication is something we would prioritise. (Feel free to reopenthis issue if any project members disagree!)\n. You could deploy the POST servers separately from the GET servers, behind a network firewall, if that's sufficient?. You could deploy the POST servers separately from the GET servers, behind a network firewall, if that's sufficient?. Is this still a problem?\n. Closing due to lack of activity. Is this still a problem? If so, feel free to reopen!\n. I think #921 fixed this.\n. Closing due to lack of activity. And it seems like the issue is resolved.\n. Closing due to lack of activity. Feel free to reopen if we should fix it! The logic that calculates number of spans is rewritten to JavaScript.\n. +1 for removing this restriction! That also lets us encode information in the span IDs (like where in the call tree the span originates from)\n. Is this fixed?\n. Is this fixed?\n. If anybody wants to take on this:\nhttp://mrhaki.blogspot.no/2010/11/gradle-goodness-set-java-version.html\n@zhy4606 @sreeram-boyapati do you have time to submit a PR for something like this?\n. \n. Maybe rename them to something more descriptive?\nOr should we close this issue? (due to lack of activity)\n. That's the way it is supposed to be. Then, you can generate thrift files yourself. I think zipkin-scrooge is a package that contains auto-generated code.\n. The UI has been rewritten, currently it shows a blank page until the js is loaded, which is somewhat ugly, but can be improved later on - in any case, since it's a pure javascript UI this won't be a problem in the future.\n. The logic for this is ported to JavaScript. There are karma tests, so it should be easy enough to add a test that verifies the wanted behavior, and then changes the order of precedence.\n. One of the problems with the dependencies view today is that it assumes a directed asyclic graph, while introducing haproxy into the mix would create circular dependencies, like in \"a -> haproxy -> b -> haproxy -> c\".\nIt assumes a DAG because... nobody would ever introduce cyclic dependencies between microservices... right...?\n. There are also a couple of other \"valid\" cases for not having a DAG, for example, at FINN.no we have something like this:\nvarnish -> finn.no -> varnish -> api -> database\nThe underlying Spark job doesn't assume a DAG, that's only our UI. So we could rewrite it with another JavaScript library than dagre/d3, but it would take some work. (PRs welcome, I suppose?)\n. It does happen sometimes, I'm not sure why. Dagre is DAG-specific: https://github.com/cpettitt/dagre\nMaybe the behavior is undetermined when you provide a cyclic graph...\nAnyways, the UI won't break entirely if you have cycles! It's just that the rendering library is originally meant for DAGs.\n. I'll close this issue - elasticsearch issues should go here: https://github.com/openzipkin/zipkin-scala-elasticsearch\n. I'll close this due to lack of activity and disagreement on whether this is the right direction for upstream Zipkin or not.\n. It's difficult to implement trace-at-a-time, since different parts/spans of the same trace can potentially be processed by different collector instances. There's no \"stickyness\".\n. Aah! I see now, what you mean.\nA simple way to solve it, is to queue up spans and send them to kafka in bulk once every 5-10 seconds.\nIn a more complex implementation it should be possible for spans to bypass that queue if they are tagged with the X-Trace-Debug HTTP flag. (I think that's what it's called) So that if you're a developer who's actively triggering traces for debugging with Zipkin, they will appear in the UI almost instantly.\n. Aah! I see now, what you mean.\nA simple way to solve it, is to queue up spans and send them to kafka in bulk once every 5-10 seconds.\nIn a more complex implementation it should be possible for spans to bypass that queue if they are tagged with the X-Trace-Debug HTTP flag. (I think that's what it's called) So that if you're a developer who's actively triggering traces for debugging with Zipkin, they will appear in the UI almost instantly.\n. > The datepicker.css was not needed? The datepicker UI still working?\nOops, I didn't check for that! Thanks for noticing! I guess I was too focused on the JavaScript ;)\n\nTests run alright, just it hangs on bin/web\n\nCan you check what's going on inside the zipkin-web/node_modules folder? npmInstall is supposed to run npm install and fetch a ton of modules.\n. > The datepicker.css was not needed? The datepicker UI still working?\nOops, I didn't check for that! Thanks for noticing! I guess I was too focused on the JavaScript ;)\n\nTests run alright, just it hangs on bin/web\n\nCan you check what's going on inside the zipkin-web/node_modules folder? npmInstall is supposed to run npm install and fetch a ton of modules.\n. All the stylesheets are included again now! They too are packaged with Webpack into a nice bundle.\n. All the stylesheets are included again now! They too are packaged with Webpack into a nice bundle.\n. Once this is merged, I'll try to work a bit more on moving server-side logic to the client. (But I didn't want to include everything in the same PR)\n. Once this is merged, I'll try to work a bit more on moving server-side logic to the client. (But I didn't want to include everything in the same PR)\n. The static resources should end up in zipkin-web/src/main/resources/dist. With gradle, it puts it in zipkin-web/build/resources/main/dist too. And from there it should be packaged into the shadow jar.\nAre you sure the node/npm commands were run successfully?\nIt should do \"npm install\" and \"npm run-script build\". (You can find that in build.gradle)\n. The static resources should end up in zipkin-web/src/main/resources/dist. With gradle, it puts it in zipkin-web/build/resources/main/dist too. And from there it should be packaged into the shadow jar.\nAre you sure the node/npm commands were run successfully?\nIt should do \"npm install\" and \"npm run-script build\". (You can find that in build.gradle)\n. Is it possible to add a task in Gradle that verifies that \"main.min.js\" and \"main.min.css\" exist? That could be a fail-safe option.\n. I think I could reproduce it locally now, I deliberately added a syntax error in the javascript code to make the frontend build fail - but it fails silently, and gradle reports it as a build success - although without the compiled assets.\n. I think this fix should help!\n. > after upgrading my local node to 5.5 and nuking zipkin-web/node_modules.. bin/web works (albeit slower to start than before due to node processing things)!\nActually, you shouldn't need node at all in order to build the project. Try renaming your \"node\" binary\" to \"nodedfgdsfg\", and see if the gradle still builds?\n\nWe should add zipkin-web/node_modules to the cache section in .travis.yml, right?\n\nSounds like a good idea!\n\nOn caching: https://docs.npmjs.com/cli/cache says npm has a download cache, ~/.npm by default. Adding that to the cache section sounds like an even better idea.\n\nThat is possible (if Travis doesn't cache it by default?), but then we will have an increase in build time, because npm modules will have to be copied into the zipkin-web/node_modules folder. But it might be safer - if we remove a module from package.json, it won't be removed from node_modules automatically, so I guess caching ~/.npm might be the best idea in that regard.\n\n@eirslett how do you feel about npm shrinkwrap?\n\nIt's a natural next step to think of. Build stability is key!\nBut I can make another pull request with shrinkwrap later on, so this one doesn't get too big?\n\nAdding npm test to the zipkin-web:check also sounds like a great idea to me; maybe it'll encourage front-end tests in the future. \n\nYes! We should have browser tests. I think using karma/phantomjs is a good fit, since it has good webpack support. Again, I'd want to put that in a separate PR.\n. > right now, running tests doesn't actually invoke node, so a failure would wait until post merge. maybe we can do something to make node do things eagerly on pull request?\nMaybe Travis should run the whole build process instead of just the tests? Might take a little longer, though. But in the long run, I imagine zipkin-web being in a separate git repository, since it slows down the build quite a bit.\n. Closing this due to lack of activity - seems like people agree on staying on the old driver version until more people have upgraded to a recent Cassandra version.\n. Thanks a lot for finding that bug!\nLike you say, we might want to wipe dist as well. That could be done with https://github.com/johnagan/clean-webpack-plugin \n. Thanks a lot for finding that bug!\nLike you say, we might want to wipe dist as well. That could be done with https://github.com/johnagan/clean-webpack-plugin \n. It's just the way I configured webpack. We could change it to target, if that's better.\n(You could try changing this line: https://github.com/openzipkin/zipkin/blob/master/zipkin-web/webpack.config.js#L30)\n. It's just the way I configured webpack. We could change it to target, if that's better.\n(You could try changing this line: https://github.com/openzipkin/zipkin/blob/master/zipkin-web/webpack.config.js#L30)\n. Moving the output to target/ is more gradle-idiomatic, while keeping it in src/main/resources/dist is more node.js-idiomatic, if you think of that part of the project as a \"separate\" build that only interacts with the parent project via the gradle-node-plugin.\nI don't have a strong opinion about it. If somebody wants to move it to target/ and their build still works, I'm fine with changing it.\n. We could set it to\nmustache\n<li {{#isTracePage}}class=\"active\"{{/isTracePage}}>...</li>\nAnd then, in zipkin-web/scala, add \"isTracePage\" to true to the data that is sent to MustacheRenderer.\n(Same with isDependencyPage)\n. We could set it to\nmustache\n<li {{#isTracePage}}class=\"active\"{{/isTracePage}}>...</li>\nAnd then, in zipkin-web/scala, add \"isTracePage\" to true to the data that is sent to MustacheRenderer.\n(Same with isDependencyPage)\n. javascript\n      this.getSpansByService = function(svc) {\n        var spans = this.spansByService[svc];\n        if (spans === undefined)\n          this.spansByService[svc] = spans = $();\n        else if (spans.jquery === undefined)\n          this.spansByService[svc] = spans = $('#' + spans.join(',#'));\n        return spans;\n      };\nhttps://github.com/openzipkin/zipkin/blob/3523407844e9d2e09ed305426145996911e27539/zipkin-web/src/main/resources/app/js/component_ui/trace.js#L63\nThis is the craziest getter function I've ever seen in my life...\n. Looks good to me!\n. Thanks for the quick review!\n. Thanks for the quick review!\n. Try running npm install manually inside the docker container? (If you can install npm there)\nAnd see if the file exists?\n. Try running npm install manually inside the docker container? (If you can install npm there)\nAnd see if the file exists?\n. It's in devDependencies in package.json. When you run npm install, it should install every dependency. (When you run npm install --production, it skips the devDependencies)\n. It's in devDependencies in package.json. When you run npm install, it should install every dependency. (When you run npm install --production, it skips the devDependencies)\n. Try checking if there's an environment variable in the build, NODE_ENV=production. (Which npm could in theory pick up, and then skip installing the devDependencies).\n. Try checking if there's an environment variable in the build, NODE_ENV=production. (Which npm could in theory pick up, and then skip installing the devDependencies).\n. It's not supposed to be set to any particular value, but it should not be set to \"production\" (since it's a build). That is for all node/npm projects in general.\n. It's not supposed to be set to any particular value, but it should not be set to \"production\" (since it's a build). That is for all node/npm projects in general.\n. That's a good idea, I'll make one! If you show me the fixes you did, I'll try to make them work together with my changes as well!\n. That's a good idea, I'll make one! If you show me the fixes you did, I'll try to make them work together with my changes as well!\n. Yes, I imagine that zipkin-web (the scala backend) would disappear, and zipkin-query could host the assets directly. The only thing that really remains in that scala backend now, is the logic for transforming trace JSON to mustache JSON.\nCaching of API calls might be even better to do in zipkin-query, using Guava cache or something like that. Caching static assets is maybe better to do with ETags. But the impact of loading javascript and css is much lower when they're bundled, since HTTP over TCP gives a per-connection overhead.\n. Here's a potential solution to nr. 1:\nCreate a new flight component, (component_ui/navbar.js) and in CommonUI (page/common.js), attach it to the menu element the same way EnvironmentUI is attached. Then, it should listen on an event which, when triggered, should mark the correct menu element as active.\nInside page/default, page/trace and page/dependencies, the event should be fired immediately as the page loads.\nPersonally I would prefer this solution, but it's a bit more work to implement.\nHere's another possible solution to nr. 1:\nUse your mustache diff as-is.\nIn page/common.js, we call tmpl() directly without any arguments, but we could have called it like this:\njavascript\ntmpl({\n  isIndexPage: true,\n  isDependenciesPage: false\n})\nthe challenge then is to figure out which menu item should be active. You could do that with something like const isDependenciesPage = window.location.pathname.indexOf('/dependencies') === 0\nand later on, we can integrate it properly with the frontend router.\n. Regarding nr 2 - that sounds like a bug that I'll have to investigate. queryResults should be recognized...\n. :+1: \n. :+1: \n. Can we rename them from .scala to .scalaconf? Maybe the IDE doesn't detect them as source code then.\n. I think I found it! I changed the signature of a method.\n. Looks like the build is fixed now.\n. It was added here: https://github.com/openzipkin/zipkin/commit/ac33a58\nI'm pretty sure that should be the correct cache headers...\n. Works in my browser:\n\n. This issue is about caching assets/css/js specifically, there's another issue regarding caching of API calls. I suggest you repost the comment there!\nRegarding assets caching, I'll close this issue, since we already serve cache headers.\n. When you hit the refresh button, it clears the cache... try opening the UI, go to the dependencies page, then back to the index page, is it loaded from cache then?\n. @abesto the node/npm/webpack/karma stuff slows down the build quite a lot. Could you please help out with some gradle magic to improve the build time? Incremental builds etc.?\n. :-)\nI'm also working on top of this patch to add unit tests while rewriting logic from Scala to JavaScript.\nEventually, I think the zipkin-web submodule will be packaged as a tar.gz or zip artifact, and then imported as a gradle dependency into zipkin-query and zipkin-java. (Which is a consideration when thinking about optimizing the build)\n. I'm good.\n. The current implementation uses a /config.json endpoint, which will suffice until it doesn't.\n. It would be nice for the frontend UI to poll on a health endpoint as well. I think it sould be sufficient to return OK, to verify that the server is running and reachable. We could then show a simple \"query server unreachable\" message, which is more helpful than a UI that doesn't work.\n. This was fixed in #997.\n. I must admit this feels very hacky and brittle to me... but it could work as a temporary migration path. In the future, I suggest we should only support collecting a list of spans. For Zipkin v2, we could provide a deprecation warning - add warning log messages whenever the collector receives single spans, and not a list of spans?\n. Don't get me wrong, I'm +1 for this change. I don't like the solution, but I still think it's the best solution - there's no smooth migration path. One possible alternative would be to consume span collections on a separate kafka topic, but then we get lots of additional complexity from handling two topics.\n. +1\n. :shipit:\n. The +/- buttons haven't worked for some time now. Maybe the best way to find the problem is to do a git bisect? We'd need to go 6 months back in time, I guess.\n. That should work well together with the javascript UI. As long as every API call is under /api/v1, then we can return assets for all other endpoints.\n. That's great! Thanks a lot! :beers: \n. Can gradle check file diffs as well? Given that package.json (and shrinkwrap.json) isn't changed, we shouldn't need to invoke npm at all. The same goes for the webpack build.\n. Thanks a lot! :-D I'll have a look at the diffs!\ndurationStr is somewhat dumber implemented in JavaScript, it only shows it as number of milliseconds with 3 digits precision. I'll have to implement the Scala logic that shows the best measuring unit based on the given value.\n. I've now pushed a similar JavaScript port of the trace rendering.\nIt's being compared in the browser the same way as with the index/default page.\nMore unit tests would be nice, but I'm also looking forward to receiving some feedback from people who have tested this with real-life traces? @adriancole @yurishkuro do you have time to run this branch locally and look for errors in the browser console debugger?\nWhen this PR is fixed and merged, all the heavy lifting is done. There's no more business logic on the server side of zipkin-web (it's basically a reverse proxy + static file server, written in Scala), so we will almost be ready to package zipkin-web as a .zip/tar.gz of assets, and possibly serve them directly from zipkin-query.\n. @adriancole the durationStr thing should be fixed now. Was that the only problem with the JSON you posted?\n. Thanks for reporting! The two errors should both be fixed now.\n. Thanks for finding all the bugs! :+1: :+1:\n\nExpected to have \"e3a98be7d2f22205\" but got \"a25ceb4232891c35\"\n\nAre you sure you posted the correct data? I cannot find any of these IDs in that JSON.\n. I fixed the \"sa -> Server Address\" bug. Also added source map support to karma, so it's easier to see where a bug is coming from.\n. Thanks a lot for finding all these bugs!\nI fixed the \"Local Address\" bug, I think.\nThe ordering problem is probably due to implementation differences in browsers; stable vs unstable sort. Both the traces have duration 4 ms/4000 microseconds. V8 does unstable sort, Webkit does stable sort (I think), and Scala appears to do a bit of both, depending on the method used.\nI stabilised the sort by \"tie-breaking\" on trace ID, so if the durations are equal, we sort by trace ID.\n. I'll ditch this PR and make a new one, without the comparisons, and with only a single commit.\n. Closing in favor of #1017.\n. It looks like span duration comes from the API. So there's probably a bug in zipkin-query?\n. It's a wrap! :beers:\n. Looks like this might have broken the README.md file on Github...  I guess we could simply hotlink the images.\n. Looks like this might have broken the README.md file on Github...  I guess we could simply hotlink the images.\n. @abesto Do you have any input on how we could publish the UI to some remote artifact repo, and then re-fetch it for zipkin-web/zipkin-query/zipkin-java?\nMaybe something like using npm to publish to bintray?\n. @abesto Do you have any input on how we could publish the UI to some remote artifact repo, and then re-fetch it for zipkin-web/zipkin-query/zipkin-java?\nMaybe something like using npm to publish to bintray?\n. Let's start with #1024 as a middle ground for now?\n. Let's start with #1024 as a middle ground for now?\n. Closing in favor of #1044.\n. I think it's easier if they're in the same directory (the html/css/js files), but nothing's set in stone, we can configure that in webpack very easily. They're just in the same folder for simplicity - if simplicity isn't achieved, then that is pointless.\nI think we should try to mimic apache behavior in zipkin-web, that it serves every file in the folder normally, and on / it serves index.html. (So maybe something like an \"alias\" for the route handler?)\nBTW, there are no other html files, index.html will be the only html file that is ever served. It's not even a bit HTML file, just enough boilerplate to be able to import the css/javascript code and bootstrap the application. Even if you go to a page like /dependencies, you will still get index.html from the server, but the javascript UI will look at window.location and figure out what to render based on that.\n. I think it's easier if they're in the same directory (the html/css/js files), but nothing's set in stone, we can configure that in webpack very easily. They're just in the same folder for simplicity - if simplicity isn't achieved, then that is pointless.\nI think we should try to mimic apache behavior in zipkin-web, that it serves every file in the folder normally, and on / it serves index.html. (So maybe something like an \"alias\" for the route handler?)\nBTW, there are no other html files, index.html will be the only html file that is ever served. It's not even a bit HTML file, just enough boilerplate to be able to import the css/javascript code and bootstrap the application. Even if you go to a page like /dependencies, you will still get index.html from the server, but the javascript UI will look at window.location and figure out what to render based on that.\n. LGTM!\nLet's just distribute it as a .jar for now, we could always change it later.\n. LGTM!\nLet's just distribute it as a .jar for now, we could always change it later.\n. Hmm, i get a weird error:\n```\nUnknown error\ncom.twitter.zipkin.web.Handlers$$anon$1.com$twitter$zipkin$web$Handlers$$anon$$getStream(Handlers.scala:100)\ncom.twitter.zipkin.web.Handlers$$anon$1$$anonfun$getRenderer$1$$anonfun$apply$6$$anonfun$apply$8.apply(Handlers.scala:112)\ncom.twitter.zipkin.web.Handlers$$anon$1$$anonfun$getRenderer$1$$anonfun$apply$6$$anonfun$apply$8.apply(Handlers.scala:111)\nscala.Option.flatMap(Option.scala:171)\ncom.twitter.zipkin.web.Handlers$$anon$1$$anonfun$getRenderer$1$$anonfun$apply$6.apply(Handlers.scala:111)\ncom.twitter.zipkin.web.Handlers$$anon$1$$anonfun$getRenderer$1$$anonfun$apply$6.apply(Handlers.scala:111)\nscala.Option.orElse(Option.scala:289)\ncom.twitter.zipkin.web.Handlers$$anon$1$$anonfun$getRenderer$1.apply(Handlers.scala:110)\ncom.twitter.zipkin.web.Handlers$$anon$1$$anonfun$getRenderer$1.apply(Handlers.scala:109)\nscala.Option.orElse(Option.scala:289)\ncom.twitter.zipkin.web.Handlers$$anon$1.getRenderer(Handlers.scala:108)\ncom.twitter.zipkin.web.Handlers$$anon$1.apply(Handlers.scala:126)\ncom.twitter.zipkin.web.Handlers$$anon$1.apply(Handlers.scala:95)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:13)\ncom.twitter.zipkin.web.Handlers$$anonfun$1.apply(Handlers.scala:67)\ncom.twitter.zipkin.web.Handlers$$anonfun$1.apply(Handlers.scala:66)\ncom.twitter.finagle.Filter$$anon$6.apply(Filter.scala:206)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:13)\ncom.twitter.zipkin.web.Handlers$$anonfun$4.apply(Handlers.scala:76)\ncom.twitter.zipkin.web.Handlers$$anonfun$4.apply(Handlers.scala:75)\ncom.twitter.finagle.Filter$$anon$6.apply(Filter.scala:206)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:13)\ncom.twitter.zipkin.web.Handlers$$anonfun$collectStats$1$$anonfun$apply$3.apply(Handlers.scala:60)\ncom.twitter.zipkin.web.Handlers$$anonfun$collectStats$1$$anonfun$apply$3.apply(Handlers.scala:60)\ncom.twitter.finagle.stats.Stat$.timeFuture(Stat.scala:39)\ncom.twitter.finagle.stats.Stat$.timeFuture(Stat.scala:48)\ncom.twitter.zipkin.web.Handlers$$anonfun$collectStats$1.apply(Handlers.scala:60)\ncom.twitter.zipkin.web.Handlers$$anonfun$collectStats$1.apply(Handlers.scala:59)\ncom.twitter.finagle.Filter$$anon$6.apply(Filter.scala:206)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.http.HttpMuxer.apply(HttpMuxer.scala:62)\ncom.twitter.finagle.http.HttpMuxer.apply(HttpMuxer.scala:25)\ncom.twitter.finagle.ServiceProxy.apply(Service.scala:120)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.tracing.ServerDestTracingProxy$$anon$1.apply(DestinationTracing.scala:32)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.service.DeadlineFilter.apply(DeadlineFilter.scala:168)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.filter.DtabStatsFilter.apply(DtabStatsFilter.scala:37)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.service.StatsFilter.apply(StatsFilter.scala:146)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.filter.ExceptionSourceFilter.apply(ExceptionSourceFilter.scala:33)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.filter.MkJvmFilter$$anon$1.apply(JvmFilter.scala:29)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.filter.ServerStatsFilter.apply(ServerStatsFilter.scala:50)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.tracing.AnnotatingTracingFilter.apply(TraceInitializerFilter.scala:159)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:13)\ncom.twitter.zipkin.web.FilteredHttpEntrypointTraceInitializer$$anonfun$com$twitter$zipkin$web$FilteredHttpEntrypointTraceInitializer$$newRootSpan$1.apply(FilteredHttpEntrypointTraceInitializer.scala:39)\ncom.twitter.zipkin.web.FilteredHttpEntrypointTraceInitializer$$anonfun$com$twitter$zipkin$web$FilteredHttpEntrypointTraceInitializer$$newRootSpan$1.apply(FilteredHttpEntrypointTraceInitializer.scala:32)\ncom.twitter.util.Local.let(Local.scala:141)\ncom.twitter.finagle.context.Context$class.let(Context.scala:176)\ncom.twitter.finagle.context.MarshalledContext.let(Context.scala:232)\ncom.twitter.finagle.tracing.Trace$$anonfun$letTracerAndId$1.apply(Trace.scala:216)\ncom.twitter.util.Local.let(Local.scala:141)\ncom.twitter.finagle.context.Context$class.let(Context.scala:176)\ncom.twitter.finagle.context.LocalContext.let(LocalContext.scala:8)\ncom.twitter.finagle.tracing.Trace$.letTracerAndId(Trace.scala:215)\ncom.twitter.zipkin.web.FilteredHttpEntrypointTraceInitializer$.com$twitter$zipkin$web$FilteredHttpEntrypointTraceInitializer$$newRootSpan(FilteredHttpEntrypointTraceInitializer.scala:32)\ncom.twitter.zipkin.web.FilteredHttpEntrypointTraceInitializer$$anonfun$1.apply(FilteredHttpEntrypointTraceInitializer.scala:22)\ncom.twitter.zipkin.web.FilteredHttpEntrypointTraceInitializer$$anonfun$1.apply(FilteredHttpEntrypointTraceInitializer.scala:20)\ncom.twitter.finagle.Filter$$anon$6.apply(Filter.scala:206)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.filter.MonitorFilter$$anonfun$apply$1.apply(MonitorFilter.scala:35)\ncom.twitter.finagle.filter.MonitorFilter$$anonfun$apply$1.apply(MonitorFilter.scala:35)\ncom.twitter.util.Future$$anonfun$monitored$1.apply$mcV$sp(Future.scala:121)\ncom.twitter.util.Monitor$$anonfun$apply$1.apply$mcV$sp(Monitor.scala:38)\ncom.twitter.util.Monitor$$anonfun$apply$1.apply(Monitor.scala:38)\ncom.twitter.util.Monitor$$anonfun$apply$1.apply(Monitor.scala:38)\ncom.twitter.util.Monitor$$anonfun$using$1.apply(Monitor.scala:110)\ncom.twitter.util.Monitor$.restoring(Monitor.scala:117)\ncom.twitter.util.Monitor$.using(Monitor.scala:108)\ncom.twitter.util.Monitor$class.apply(Monitor.scala:37)\ncom.twitter.util.Future$Monitored.apply(Future.scala:128)\ncom.twitter.util.Future$.monitored(Future.scala:120)\ncom.twitter.finagle.filter.MonitorFilter.apply(MonitorFilter.scala:34)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.http.filter.ServerContextFilter$$anonfun$apply$1.apply(ContextFilter.scala:17)\ncom.twitter.finagle.http.filter.ServerContextFilter$$anonfun$apply$1.apply(ContextFilter.scala:17)\ncom.twitter.finagle.http.codec.HttpContext$.read(HttpContext.scala:44)\ncom.twitter.finagle.http.filter.ServerContextFilter.apply(ContextFilter.scala:17)\ncom.twitter.finagle.http.filter.ServerContextFilter.apply(ContextFilter.scala:13)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.http.filter.DtabFilter.apply(DtabFilter.scala:22)\ncom.twitter.finagle.http.filter.DtabFilter.apply(DtabFilter.scala:11)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.http.codec.HttpServerDispatcher.dispatch(HttpServerDispatcher.scala:73)\ncom.twitter.finagle.dispatch.GenSerialServerDispatcher$$anonfun$com$twitter$finagle$dispatch$GenSerialServerDispatcher$$loop$1.apply(ServerDispatcher.scala:51)\ncom.twitter.finagle.dispatch.GenSerialServerDispatcher$$anonfun$com$twitter$finagle$dispatch$GenSerialServerDispatcher$$loop$1.apply(ServerDispatcher.scala:45)\ncom.twitter.util.Future$$anonfun$flatMap$1.apply(Future.scala:921)\ncom.twitter.util.Future$$anonfun$flatMap$1.apply(Future.scala:920)\ncom.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:112)\ncom.twitter.util.Promise$Transformer.k(Promise.scala:112)\ncom.twitter.util.Promise$Transformer.apply(Promise.scala:122)\ncom.twitter.util.Promise$Transformer.apply(Promise.scala:103)\ncom.twitter.util.Promise$$anon$1.run(Promise.scala:381)\ncom.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:178)\ncom.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:136)\ncom.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:207)\ncom.twitter.concurrent.Scheduler$.submit(Scheduler.scala:92)\ncom.twitter.util.Promise.runq(Promise.scala:350)\ncom.twitter.util.Promise.updateIfEmpty(Promise.scala:721)\ncom.twitter.util.Promise.update(Promise.scala:694)\ncom.twitter.util.Promise.setValue(Promise.scala:670)\ncom.twitter.concurrent.AsyncQueue.offer(AsyncQueue.scala:111)\ncom.twitter.finagle.netty3.transport.ChannelTransport.handleUpstream(ChannelTransport.scala:55)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142)\norg.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142)\ncom.twitter.finagle.netty3.channel.ChannelRequestStatsHandler.messageReceived(ChannelRequestStatsHandler.scala:32)\norg.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)\norg.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.channel.SimpleChannelUpstreamHandler.messageReceived(SimpleChannelUpstreamHandler.java:124)\ncom.twitter.finagle.http.codec.RespondToExpectContinue.messageReceived(RespondToExpectContinue.scala:30)\norg.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.channel.SimpleChannelUpstreamHandler.messageReceived(SimpleChannelUpstreamHandler.java:124)\ncom.twitter.finagle.http.codec.PayloadSizeHandler.messageReceived(PayloadSizeHandler.scala:23)\norg.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.handler.codec.http.HttpContentEncoder.messageReceived(HttpContentEncoder.java:82)\norg.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\norg.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)\norg.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)\norg.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)\norg.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\norg.jboss.netty.handler.codec.http.HttpServerCodec.handleUpstream(HttpServerCodec.java:56)\ncom.twitter.finagle.http.SafeHttpServerCodec.handleUpstream(Codec.scala:36)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142)\ncom.twitter.finagle.netty3.channel.ChannelStatsHandler.messageReceived(ChannelStatsHandler.scala:68)\norg.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\norg.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\norg.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\norg.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\norg.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\norg.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)\norg.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\norg.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\norg.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\norg.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\njava.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\njava.lang.Thread.run(Unknown Source)\n```\nThis is on the / page, when running gradlew zipkin-web:shadowJar and then java -jar on that artifact...\n. Hmm, i get a weird error:\n```\nUnknown error\ncom.twitter.zipkin.web.Handlers$$anon$1.com$twitter$zipkin$web$Handlers$$anon$$getStream(Handlers.scala:100)\ncom.twitter.zipkin.web.Handlers$$anon$1$$anonfun$getRenderer$1$$anonfun$apply$6$$anonfun$apply$8.apply(Handlers.scala:112)\ncom.twitter.zipkin.web.Handlers$$anon$1$$anonfun$getRenderer$1$$anonfun$apply$6$$anonfun$apply$8.apply(Handlers.scala:111)\nscala.Option.flatMap(Option.scala:171)\ncom.twitter.zipkin.web.Handlers$$anon$1$$anonfun$getRenderer$1$$anonfun$apply$6.apply(Handlers.scala:111)\ncom.twitter.zipkin.web.Handlers$$anon$1$$anonfun$getRenderer$1$$anonfun$apply$6.apply(Handlers.scala:111)\nscala.Option.orElse(Option.scala:289)\ncom.twitter.zipkin.web.Handlers$$anon$1$$anonfun$getRenderer$1.apply(Handlers.scala:110)\ncom.twitter.zipkin.web.Handlers$$anon$1$$anonfun$getRenderer$1.apply(Handlers.scala:109)\nscala.Option.orElse(Option.scala:289)\ncom.twitter.zipkin.web.Handlers$$anon$1.getRenderer(Handlers.scala:108)\ncom.twitter.zipkin.web.Handlers$$anon$1.apply(Handlers.scala:126)\ncom.twitter.zipkin.web.Handlers$$anon$1.apply(Handlers.scala:95)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:13)\ncom.twitter.zipkin.web.Handlers$$anonfun$1.apply(Handlers.scala:67)\ncom.twitter.zipkin.web.Handlers$$anonfun$1.apply(Handlers.scala:66)\ncom.twitter.finagle.Filter$$anon$6.apply(Filter.scala:206)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:13)\ncom.twitter.zipkin.web.Handlers$$anonfun$4.apply(Handlers.scala:76)\ncom.twitter.zipkin.web.Handlers$$anonfun$4.apply(Handlers.scala:75)\ncom.twitter.finagle.Filter$$anon$6.apply(Filter.scala:206)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:13)\ncom.twitter.zipkin.web.Handlers$$anonfun$collectStats$1$$anonfun$apply$3.apply(Handlers.scala:60)\ncom.twitter.zipkin.web.Handlers$$anonfun$collectStats$1$$anonfun$apply$3.apply(Handlers.scala:60)\ncom.twitter.finagle.stats.Stat$.timeFuture(Stat.scala:39)\ncom.twitter.finagle.stats.Stat$.timeFuture(Stat.scala:48)\ncom.twitter.zipkin.web.Handlers$$anonfun$collectStats$1.apply(Handlers.scala:60)\ncom.twitter.zipkin.web.Handlers$$anonfun$collectStats$1.apply(Handlers.scala:59)\ncom.twitter.finagle.Filter$$anon$6.apply(Filter.scala:206)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.http.HttpMuxer.apply(HttpMuxer.scala:62)\ncom.twitter.finagle.http.HttpMuxer.apply(HttpMuxer.scala:25)\ncom.twitter.finagle.ServiceProxy.apply(Service.scala:120)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.tracing.ServerDestTracingProxy$$anon$1.apply(DestinationTracing.scala:32)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.service.DeadlineFilter.apply(DeadlineFilter.scala:168)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.filter.DtabStatsFilter.apply(DtabStatsFilter.scala:37)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.service.StatsFilter.apply(StatsFilter.scala:146)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.filter.ExceptionSourceFilter.apply(ExceptionSourceFilter.scala:33)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.filter.MkJvmFilter$$anon$1.apply(JvmFilter.scala:29)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.filter.ServerStatsFilter.apply(ServerStatsFilter.scala:50)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.tracing.AnnotatingTracingFilter.apply(TraceInitializerFilter.scala:159)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:13)\ncom.twitter.zipkin.web.FilteredHttpEntrypointTraceInitializer$$anonfun$com$twitter$zipkin$web$FilteredHttpEntrypointTraceInitializer$$newRootSpan$1.apply(FilteredHttpEntrypointTraceInitializer.scala:39)\ncom.twitter.zipkin.web.FilteredHttpEntrypointTraceInitializer$$anonfun$com$twitter$zipkin$web$FilteredHttpEntrypointTraceInitializer$$newRootSpan$1.apply(FilteredHttpEntrypointTraceInitializer.scala:32)\ncom.twitter.util.Local.let(Local.scala:141)\ncom.twitter.finagle.context.Context$class.let(Context.scala:176)\ncom.twitter.finagle.context.MarshalledContext.let(Context.scala:232)\ncom.twitter.finagle.tracing.Trace$$anonfun$letTracerAndId$1.apply(Trace.scala:216)\ncom.twitter.util.Local.let(Local.scala:141)\ncom.twitter.finagle.context.Context$class.let(Context.scala:176)\ncom.twitter.finagle.context.LocalContext.let(LocalContext.scala:8)\ncom.twitter.finagle.tracing.Trace$.letTracerAndId(Trace.scala:215)\ncom.twitter.zipkin.web.FilteredHttpEntrypointTraceInitializer$.com$twitter$zipkin$web$FilteredHttpEntrypointTraceInitializer$$newRootSpan(FilteredHttpEntrypointTraceInitializer.scala:32)\ncom.twitter.zipkin.web.FilteredHttpEntrypointTraceInitializer$$anonfun$1.apply(FilteredHttpEntrypointTraceInitializer.scala:22)\ncom.twitter.zipkin.web.FilteredHttpEntrypointTraceInitializer$$anonfun$1.apply(FilteredHttpEntrypointTraceInitializer.scala:20)\ncom.twitter.finagle.Filter$$anon$6.apply(Filter.scala:206)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.filter.MonitorFilter$$anonfun$apply$1.apply(MonitorFilter.scala:35)\ncom.twitter.finagle.filter.MonitorFilter$$anonfun$apply$1.apply(MonitorFilter.scala:35)\ncom.twitter.util.Future$$anonfun$monitored$1.apply$mcV$sp(Future.scala:121)\ncom.twitter.util.Monitor$$anonfun$apply$1.apply$mcV$sp(Monitor.scala:38)\ncom.twitter.util.Monitor$$anonfun$apply$1.apply(Monitor.scala:38)\ncom.twitter.util.Monitor$$anonfun$apply$1.apply(Monitor.scala:38)\ncom.twitter.util.Monitor$$anonfun$using$1.apply(Monitor.scala:110)\ncom.twitter.util.Monitor$.restoring(Monitor.scala:117)\ncom.twitter.util.Monitor$.using(Monitor.scala:108)\ncom.twitter.util.Monitor$class.apply(Monitor.scala:37)\ncom.twitter.util.Future$Monitored.apply(Future.scala:128)\ncom.twitter.util.Future$.monitored(Future.scala:120)\ncom.twitter.finagle.filter.MonitorFilter.apply(MonitorFilter.scala:34)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.http.filter.ServerContextFilter$$anonfun$apply$1.apply(ContextFilter.scala:17)\ncom.twitter.finagle.http.filter.ServerContextFilter$$anonfun$apply$1.apply(ContextFilter.scala:17)\ncom.twitter.finagle.http.codec.HttpContext$.read(HttpContext.scala:44)\ncom.twitter.finagle.http.filter.ServerContextFilter.apply(ContextFilter.scala:17)\ncom.twitter.finagle.http.filter.ServerContextFilter.apply(ContextFilter.scala:13)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.Service$$anon$2.apply(Service.scala:16)\ncom.twitter.finagle.http.filter.DtabFilter.apply(DtabFilter.scala:22)\ncom.twitter.finagle.http.filter.DtabFilter.apply(DtabFilter.scala:11)\ncom.twitter.finagle.Filter$$anon$1.apply(Filter.scala:70)\ncom.twitter.finagle.http.codec.HttpServerDispatcher.dispatch(HttpServerDispatcher.scala:73)\ncom.twitter.finagle.dispatch.GenSerialServerDispatcher$$anonfun$com$twitter$finagle$dispatch$GenSerialServerDispatcher$$loop$1.apply(ServerDispatcher.scala:51)\ncom.twitter.finagle.dispatch.GenSerialServerDispatcher$$anonfun$com$twitter$finagle$dispatch$GenSerialServerDispatcher$$loop$1.apply(ServerDispatcher.scala:45)\ncom.twitter.util.Future$$anonfun$flatMap$1.apply(Future.scala:921)\ncom.twitter.util.Future$$anonfun$flatMap$1.apply(Future.scala:920)\ncom.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:112)\ncom.twitter.util.Promise$Transformer.k(Promise.scala:112)\ncom.twitter.util.Promise$Transformer.apply(Promise.scala:122)\ncom.twitter.util.Promise$Transformer.apply(Promise.scala:103)\ncom.twitter.util.Promise$$anon$1.run(Promise.scala:381)\ncom.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:178)\ncom.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:136)\ncom.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:207)\ncom.twitter.concurrent.Scheduler$.submit(Scheduler.scala:92)\ncom.twitter.util.Promise.runq(Promise.scala:350)\ncom.twitter.util.Promise.updateIfEmpty(Promise.scala:721)\ncom.twitter.util.Promise.update(Promise.scala:694)\ncom.twitter.util.Promise.setValue(Promise.scala:670)\ncom.twitter.concurrent.AsyncQueue.offer(AsyncQueue.scala:111)\ncom.twitter.finagle.netty3.transport.ChannelTransport.handleUpstream(ChannelTransport.scala:55)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142)\norg.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142)\ncom.twitter.finagle.netty3.channel.ChannelRequestStatsHandler.messageReceived(ChannelRequestStatsHandler.scala:32)\norg.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)\norg.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.channel.SimpleChannelUpstreamHandler.messageReceived(SimpleChannelUpstreamHandler.java:124)\ncom.twitter.finagle.http.codec.RespondToExpectContinue.messageReceived(RespondToExpectContinue.scala:30)\norg.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.channel.SimpleChannelUpstreamHandler.messageReceived(SimpleChannelUpstreamHandler.java:124)\ncom.twitter.finagle.http.codec.PayloadSizeHandler.messageReceived(PayloadSizeHandler.scala:23)\norg.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.handler.codec.http.HttpContentEncoder.messageReceived(HttpContentEncoder.java:82)\norg.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\norg.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)\norg.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)\norg.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)\norg.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\norg.jboss.netty.handler.codec.http.HttpServerCodec.handleUpstream(HttpServerCodec.java:56)\ncom.twitter.finagle.http.SafeHttpServerCodec.handleUpstream(Codec.scala:36)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\norg.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142)\ncom.twitter.finagle.netty3.channel.ChannelStatsHandler.messageReceived(ChannelStatsHandler.scala:68)\norg.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\norg.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\norg.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\norg.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\norg.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\norg.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)\norg.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\norg.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\norg.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\norg.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\njava.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\njava.lang.Thread.run(Unknown Source)\n```\nThis is on the / page, when running gradlew zipkin-web:shadowJar and then java -jar on that artifact...\n. Are you sure ?raw is the best format? How about ?clock_skew=false?\n. Are you sure ?raw is the best format? How about ?clock_skew=false?\n. I rebased on top of master.\nCould somebody please give me a +1 for merge?\n(If there aren't any bugs, of course)\n. I rebased on top of master.\nCould somebody please give me a +1 for merge?\n(If there aren't any bugs, of course)\n. \n. \n. The idea was that, like you said, people could host the ui from a web server like apache httpd, and simply configure it by creating their own config.json.\n. Maybe we could check whether the first character in a message is [ and if that's the case, interpret it as a json list of spans.\n. Maybe we could check whether the first character in a message is [ and if that's the case, interpret it as a json list of spans.\n. @gena01 Pull requests are always welcome :wink:\nI'd say if somebody makes a satisfying implementation of this, we should merge it.\n. Great, thanks a lot!! +1\n. Great, thanks a lot!! +1\n. We could serve them under \"dist\", so the folder structure would be /static/index.html, /static/dist/app.min.js and so on.\n. +1\n. Yes, that sounds like a better idea!\n. Given that we're (probably) retiring the Scala code anyways, maybe the UI module can live with zipkin-java? (It stays in this repository openzipkin/zipkin, while the java code is moved from openzipkin/zipkin-java to openzipkin/zipkin)\n. > Generates separate index.html for /dependency/\nThis would break in other cases, e.g. the /trace/:traceId endpoint. And it wouldn't be very forwards-compatible once we add more routes.\n. Maybe this could be a configuration option, just like default query limit?\nThe UI refactoring is pretty much done now, so people can send patches.\n. No, unfortunately we're lagging behind on that part :-/ You'll have to hard-code it into the config.json handler for now. (There is one handler in zipkin-query and one in zipkin-web, of course it's temporary that it's duplicated)\n. :grinning: Go for it! :beer:\n. I'm still not convinced we need an assets directory - finatra solves this in zipkin-query quite elegantly, so we could simply forward all HTTP traffic directly from zipkin-web to zipkin-query as-is. (See my comment in #1050)\n. The code looks good to me! (but I haven't tested it)\n. What shows up in the browser debugger? Do you get 404 on any resources? Any console error messages? What is the HTML that is served?\n. Great work! The code looks good to me! (But I haven't tested it)\nDid you try spinning it up manually, with kafka and all?\n. Maybe we should do this more generally, e.g. all zipkin flags should be able to read from environment variables with the same name; e.g. the \"zipkin.collector.storage.ttl\" should be able to read the environment variable ZIPKIN_COLLECTOR_STORAGE_TTL. That should make it work for all flags - a more generic solution. @adriancole wdyt? Would this require changes in twitter libraries upstream?\n. This is the recursive function that fails:\njavascript\nfunction totalServiceTime(stamps, acc = 0) {\n  if (stamps.length === 0) {\n    return acc;\n  } else {\n    const ts = _(stamps).minBy((s) => s.timestamp);\n    const [current, next] = _(stamps)\n        .partition((t) =>\n          t.timestamp >= ts.timestamp\n          && t.timestamp + t.duration <= ts.timestamp + ts.duration)\n        .value();\n    const endTs = Math.max(...current.map((t) => t.timestamp + t.duration));\n    return totalServiceTime(next, acc + (endTs - ts.timestamp));\n  }\n}\nUnfortunately, it fails on some traces, but so far it has been difficult to reproduce. @adriancole got the same error, but couldn't reproduce later...\nIf you set a breakpoint there somewhere, do you see whether current and/or next is changed from function cal to function call? If not, it results in an infinite loop. The interesting thing is to find the type of data that makes the function loop forever!\n. I suspect this may be caused by one ore more spans that either miss timestamp or duration. Could that be right? You can see the raw JSON data in the chrome debugger by inspecting the network call.\n. They're definitely supposed to be cached.\nIf you look at ZipkinQueryController, you could try doing something like this (pseudocode):\n``` scala\nimport com.twitter.conversions.time._ // To get the \"10.minutes\" syntax sugar\n// ...\nget(\"/:\") { request: Request =>\n    val resp = response.ok.fileOrIndex(\n    request.params(\"\"),\n    \"index.html\"\n    )\nresp.cacheControl = 10.minutes\nresp\n\n}\n```\nIf it works, and you're in the mood for a PR... well... :wink:\nAbout setting charset=utf-8 when the content type is image/png; that seems more like a bug in Finatra, so it should probably be filed as an issue there.\n. They're definitely supposed to be cached.\nIf you look at ZipkinQueryController, you could try doing something like this (pseudocode):\n``` scala\nimport com.twitter.conversions.time._ // To get the \"10.minutes\" syntax sugar\n// ...\nget(\"/:\") { request: Request =>\n    val resp = response.ok.fileOrIndex(\n    request.params(\"\"),\n    \"index.html\"\n    )\nresp.cacheControl = 10.minutes\nresp\n\n}\n```\nIf it works, and you're in the mood for a PR... well... :wink:\nAbout setting charset=utf-8 when the content type is image/png; that seems more like a bug in Finatra, so it should probably be filed as an issue there.\n. I would say they belong to the same issue.\nIn the long term, we should set something like 1 minute timeout on index.html, and 1 year timeout on the assets - given that we add hashes to the asset names. I think we can do that with webpack.\n. I would say they belong to the same issue.\nIn the long term, we should set something like 1 minute timeout on index.html, and 1 year timeout on the assets - given that we add hashes to the asset names. I think we can do that with webpack.\n. Let's go for the path name, but the opposite ;) no caching for api, 1 minute caching for index, 10 minutes for config, 1 year for everything else?\n. Let's go for the path name, but the opposite ;) no caching for api, 1 minute caching for index, 10 minutes for config, 1 year for everything else?\n. If you cd zipkin-ui and npm run dev, will it still work? Live-reload etc.?\n. If you cd zipkin-ui and npm run dev, will it still work? Live-reload etc.?\n. Try starting zipkin-query instead of zipkin-web?\nAnd it looks like the documentation is outdated, too...\nThe \"Developing\" section must be moved from zipkin-web/README.md to zipkin-ui/README.md.\nAnd we need to update webpack.config.js so that it proxies to port 9411 (the query server directly) instead of 8080 (web).\n. Try starting zipkin-query instead of zipkin-web?\nAnd it looks like the documentation is outdated, too...\nThe \"Developing\" section must be moved from zipkin-web/README.md to zipkin-ui/README.md.\nAnd we need to update webpack.config.js so that it proxies to port 9411 (the query server directly) instead of 8080 (web).\n. But I tested this change, it works with livereload!\n. But I tested this change, it works with livereload!\n. It should be possible to add this in trace.mustache, I think.\nSomething like <a href=\"/api/v1/trace/{{traceId}}\">JSON</a>\n. It should be possible to add this in trace.mustache, I think.\nSomething like <a href=\"/api/v1/trace/{{traceId}}\">JSON</a>\n. I agree with @yurishkuro here. Right-click and open in a new tab.\n. I agree with @yurishkuro here. Right-click and open in a new tab.\n. LGTM!\n. LGTM!\n. Wouldn't it be better if they were called startTs and endTs? Anyways, shouldn't be too difficult to implement UI-wise!\n. Wouldn't it be better if they were called startTs and endTs? Anyways, shouldn't be too difficult to implement UI-wise!\n. > Key length of 97122 is longer than maximum of 65535\nIs this due to a 95 kb span you're trying to store? (Spans are typically between 500 bytes and 2 kb big, usually never more than ~5 kb, though there's no formal limit.)\nSome spans will get very big if you add lots and lots of binary annotations to it, but more than 64 kb is extreme. I don't even think our backend/zipkin-collector checks for that.\nThis is just my random guess though; could be something completely different.\n. > Key length of 97122 is longer than maximum of 65535\nIs this due to a 95 kb span you're trying to store? (Spans are typically between 500 bytes and 2 kb big, usually never more than ~5 kb, though there's no formal limit.)\nSome spans will get very big if you add lots and lots of binary annotations to it, but more than 64 kb is extreme. I don't even think our backend/zipkin-collector checks for that.\nThis is just my random guess though; could be something completely different.\n. I think zipkin-collector should ignore failures, not try to re-consume broken messages from kafka. We could add a sanity check for 64 kb span size, but then there's probably some other violation we haven't thought of that could happen (or a new storage backend, like elasticsearch, with its own quirks)\n. I think zipkin-collector should ignore failures, not try to re-consume broken messages from kafka. We could add a sanity check for 64 kb span size, but then there's probably some other violation we haven't thought of that could happen (or a new storage backend, like elasticsearch, with its own quirks)\n. Is it implemented in the API backends yet?\n. Is it implemented in the API backends yet?\n. But I believe this issue is a duplicate of #1063 ?\n. But I believe this issue is a duplicate of #1063 ?\n. Good work! :+1:\n. Good work! :+1:\n. > Java implementation has lower complexity in terms of technology used (TBD: how exactly?)\nJava is a more widely known programming language than Scala, and the Spring framework/stack is more widely known than the Finagle/Twitter framework/stack. The technical complexity is about the same (I think), but more people know Java than Scala.\n\nNo built-in Scribe collector support, breaking feature parity\n\nI think we should add scribe support to zipkin-java. The java port should be a drop-in replacement for the scala implementation, so we don't need to make any changes to the instrumented applications.\n\nMulti-process Zipkin provides extra resiliency\n\n,\n\nThey [collector/query] operate independently, such that a degradation in one process does not affect the ability of the other to serve requests.\n\nThis could be solved in the infrastructure, by only sending query traffic to some instances, and only sending collector traffic to other instances. I'm not concerned about query workloads taking down a collector (it's very low volume), but a collector which is receiving too many spans could possibly crash, and then the query functionality wouldn't work either. So for extra resiliency, you could deploy one query-only instance that is guaranteed to answer, even when the collectors are flooded. (Unless the storage backend is also kneeling)\n\nHow do we know that an http-backed Zipkin tracer is as performant as Finagle's scribe-backed Zipkin tracer?\n\nWe should write a couple of load tests:\nfinagle -> scribe -> zipkin-scala\nfinagle -> http -> zipkin-scala (if this is supported?)\nfinagle -> scribe -> zipkin-java\nfinagle -> http -> zipkin-java\nand see how many spans per second we can process.\nAlso, measure how latency in the storage backend affects server performance, and how much traffic the collector can take before it crashes.\n\nmoving the fundamental infra away from [~~scribe and~~] scala isn't inherently problematic as much as it is labor intensive.\n\nGiven that Zipkin is distributed as a shaded jar, Scala is already an implementation detail from the infrastructure point of view; it's just a jvm process, or even just a Docker container. But given that Twitter builds Zipkin from source, for Manhattan support (is that still true?) then there would be a rewrite effort, since the APIs would be Java, not Scala.\n. > Java implementation has lower complexity in terms of technology used (TBD: how exactly?)\nJava is a more widely known programming language than Scala, and the Spring framework/stack is more widely known than the Finagle/Twitter framework/stack. The technical complexity is about the same (I think), but more people know Java than Scala.\n\nNo built-in Scribe collector support, breaking feature parity\n\nI think we should add scribe support to zipkin-java. The java port should be a drop-in replacement for the scala implementation, so we don't need to make any changes to the instrumented applications.\n\nMulti-process Zipkin provides extra resiliency\n\n,\n\nThey [collector/query] operate independently, such that a degradation in one process does not affect the ability of the other to serve requests.\n\nThis could be solved in the infrastructure, by only sending query traffic to some instances, and only sending collector traffic to other instances. I'm not concerned about query workloads taking down a collector (it's very low volume), but a collector which is receiving too many spans could possibly crash, and then the query functionality wouldn't work either. So for extra resiliency, you could deploy one query-only instance that is guaranteed to answer, even when the collectors are flooded. (Unless the storage backend is also kneeling)\n\nHow do we know that an http-backed Zipkin tracer is as performant as Finagle's scribe-backed Zipkin tracer?\n\nWe should write a couple of load tests:\nfinagle -> scribe -> zipkin-scala\nfinagle -> http -> zipkin-scala (if this is supported?)\nfinagle -> scribe -> zipkin-java\nfinagle -> http -> zipkin-java\nand see how many spans per second we can process.\nAlso, measure how latency in the storage backend affects server performance, and how much traffic the collector can take before it crashes.\n\nmoving the fundamental infra away from [~~scribe and~~] scala isn't inherently problematic as much as it is labor intensive.\n\nGiven that Zipkin is distributed as a shaded jar, Scala is already an implementation detail from the infrastructure point of view; it's just a jvm process, or even just a Docker container. But given that Twitter builds Zipkin from source, for Manhattan support (is that still true?) then there would be a rewrite effort, since the APIs would be Java, not Scala.\n. We could keep the zipkin-scala module, but deprecate/sunset it - only accept bug fixes that are contributed by people who still use the Scala implementation. At the very least, let project maintainers add features in zipkin-java and not expect them to reimplement it in zipkin-scala.\nThey would still get any new UI updates, since the UI is just a single-page webapp that can be hosted in nginx. The exception would be if the UI relies on new API features that aren't implemented yet - maybe the /health endpoint on the server chould expose a \"features\" array? \"features\": [\"dependency\", \"search\"] and then we could adjust the UI accordingly.\n. We could keep the zipkin-scala module, but deprecate/sunset it - only accept bug fixes that are contributed by people who still use the Scala implementation. At the very least, let project maintainers add features in zipkin-java and not expect them to reimplement it in zipkin-scala.\nThey would still get any new UI updates, since the UI is just a single-page webapp that can be hosted in nginx. The exception would be if the UI relies on new API features that aren't implemented yet - maybe the /health endpoint on the server chould expose a \"features\" array? \"features\": [\"dependency\", \"search\"] and then we could adjust the UI accordingly.\n. Another question, in terms of workload, is whether we would spend more resources on deprecating zipkin-scala (it's an effort in itself) than it would take to simply maintain the two side-by-side.\n. It's not really a big problem. But if somebody wants to fix it, we could first get the list of services, then pick the first service in the list and ask for spans for that service, instead of reading service name from the query string.\n. Yes, the event that triggers the API call should only be triggered when service name is set in the query string. We use query-string for that already, in other places.\n. Looks like nexus has a strict requirement that a \"main jar\" must be present. This is a hack, but we could do something like echo \"This is the wrong jar, use the -all jar instead\" > README.txt && tar -gzxfxxz README.txt > zipkin-query-service.jar, and then upload that.\nOr we could rename the -all jar to just the \"main\" jar name, or we could duplicate it (publish the same jar under two different names) for backwards compatibility.\nOr we could just stop syncing zipkin-query-service to maven central, and only publish it to jcenter, if that's a possibility?\n. I guess this should work:\nhttps://github.com/johnrengelman/shadow#publishing-the-shadow-jar-as-a-standalone-artifact\n(without the -all extension)\n. No, there are no concrete plans. When Zipkin was first designed at Twitter, 64 bits was a well thought-out id length, it's a balance between size and chance of collision.\n. I had some comments; otherwise, great work!\n. +1 for adriancole's suggestion!\n. Otherwise, LGTM! ship it\n. Great work!\nMaybe the only thing missing, that we should add, is to catch exceptions in traceToMustache.js, so that any bugs in my sloppy JavaScript code will be shown with a stack trace, instead of rendering a black page :stuck_out_tongue: \n. You must run the zipkin-dependencies-spark job. It's a single jar file that you can execute.\n. You must run the zipkin-dependencies-spark job. It's a single jar file that you can execute.\n. Remember, the dependencies page is an aggregate of the last hours (or up to last 7 days) of data, so it may take some time before it disappears.\n. Remember, the dependencies page is an aggregate of the last hours (or up to last 7 days) of data, so it may take some time before it disappears.\n. I'm not quite sure I get what the error is... what would the expected output look like?\n. I'm not quite sure I get what the error is... what would the expected output look like?\n. +1\nNo reason, except nobody has made a PR yet :wink:\n. +1\nNo reason, except nobody has made a PR yet :wink:\n. +1 LGTM\n. That's fine with me!\n. @negatioN interesting for FINN?\n. Thanks a lot, good work!\nNote that you will need npm (and thus node) installed in order to run \"npm run dev\".\n. I know - I wrote that code ;-) what I mean is that we need npm on $PATH somewhere so that you can \"npm run dev\" from the command line.\nOne option is to use a helper script, so you can run \"./npm run dev\" or something like that?\n. Nice!\n. They are supposed to be micros. Multiply by 1000 ;-)\n. They are supposed to be micros. Multiply by 1000 ;-)\n. But if you use Brave, that should happen under the hood...\n. But if you use Brave, that should happen under the hood...\n. Yes, you can java -jar 3 instances if you want to, on 3 servers, with one instance each. And then configure the instances to only behave as one of the \"roles\". (I think it's possible to disable roles)\n. +1 for rounding up\n. Sounds like a good idea to me!\n. The Zipkin data model doesn't support async spans like tracing kafka messages. It would need to be redesigned. (There's some work going on about Zipkin data model v2 I think)\n. The Zipkin data model doesn't support async spans like tracing kafka messages. It would need to be redesigned. (There's some work going on about Zipkin data model v2 I think)\n. One thing you could do, to represent it, is to start a new trace, with a new trace id, on the consumer side, and then add a binary annotation with an ID that correlates the two traces. But we have no UI support or special tooling support for that yet.\n. One thing you could do, to represent it, is to start a new trace, with a new trace id, on the consumer side, and then add a binary annotation with an ID that correlates the two traces. But we have no UI support or special tooling support for that yet.\n. We could simply hardcode the UI to always be served under /zipkin/. And set up redirects, to that requests to the old URLs are automatically redirected to /zipkin/xxx.\n. We could simply hardcode the UI to always be served under /zipkin/. And set up redirects, to that requests to the old URLs are automatically redirected to /zipkin/xxx.\n. @gurinderu It's impossible, the only place it runs is under /zipkin/ (without /services).. @gurinderu It's impossible, the only place it runs is under /zipkin/ (without /services).. Yes, we don't control the HTML file from the server, it's just a static HTML file, so we cannot inject any attributes.. Yes, we don't control the HTML file from the server, it's just a static HTML file, so we cannot inject any attributes.. Have a go at it!. Have a go at it!. > I think most companies don't really need Cassandra on the backend. I suspect a high-performance alternative for medium-size data will something that will appeal to this audience. What do you think?\nI believe Cassandra is the most widely used Zipkin backend for production workloads.\n\nno one is actively working on it quite yet, hoping someone can pick it up. Regardless, it is probably best started as a separate repo I think.\n\nThis is a good idea. Historically, we've already abandoned one DB backend (HBase, the support for that was eventually discontinued), and having implementations in separate repositories means it's up to module maintainers to make sure the backends are up to date. If support for ObscureDB/FadDB is implemented, and then the original authors lose interest and/or nobody else wants to maintain it, then the burden will not be in the core team, it will be on the developers who want to run Zipkin on XxxDB. (I'm not saying PostgreSQL is obscure, it's widely used in the tech world)\nThere is a balance to strike, between supporting modules in the core repo or separate repos. MySQL was chosen as the relational/simple-to-setup SQL alternative, and Cassandra for the high-scale deployment solution. Elasticsearch was recently added to leverage some of the query functions supported in Lucene (I think).\n. How about we make a Java 6-compliant fork of the instrumentation library? zipkin-java6? Separate repo? Supporting Java 5 or 6 is an additional burden to contributors; it could be trivial, it could be nontrivial (I'm not sure), but things like okio/moshi only working with Java 7+ is a problem.\nMaintaining the fork would be on a best-effort basis, possibly driven by Zipkin users who still use Java 5/6?\n. Would it be able to setup a transitive dependency on cassandra as well? Ease of deployment is always a good thing :)\n. Would it be able to setup a transitive dependency on cassandra as well? Ease of deployment is always a good thing :)\n. Sounds like a useful feature! Maybe it could be a separate application that uses zipkin's API?\n. Sounds like a useful feature! Maybe it could be a separate application that uses zipkin's API?\n. You don't need to install node or npm to open the project in Eclipse.\n. You don't need to install node or npm to open the project in Eclipse.\n. Could this be merged upstream to libthrift as well? Or is it too Zipkin-specific?\n. Could this be merged upstream to libthrift as well? Or is it too Zipkin-specific?\n. We should add ?sortOrder=something to the URL (replaceHistory) and then have the crossroads router read that param when it delegates to the views. Somebody would have to send a PR for it.\n. We use a <div> now for every span in the visualisation. I suppose we could replace it with <svg> and draw a trapezoid instead...\nOr even render the entire visualisation with SVG?\n. They are supposed to be hidden, because only non-core annotations are supposed to be shown. These annotations are actually a separate feature.\n. I agree to these points ^^\nAlso if the API is similar to other tracers (Finagle, zipkin-js, Brave etc.) then it's easier to reason about it. Similar naming conventions, maybe even similar names of public methods in the API/library?\nAlso, unit tests are important (PHPUnit). And a build script, to generate thrift files, run the tests etc.\nMaybe, similar to Brave and zipkin-js, have a monorepo with all composer packages for Zipkin PHP, with one core package, one package per transport and one package per instrumentation. It takes some tweaking to make that work with composer, which assumes one package per repository (I think)\n. Thanks for finding that one! It's an old link from back when Twitter hosted the Zipkin project.\nThis is the updated link: http://zipkin.io/\n. Thanks for finding that one! It's an old link from back when Twitter hosted the Zipkin project.\nThis is the updated link: http://zipkin.io/\n. It used to, but support was removed eventually, because nobody used it, or maintained the code base. You can find the code in the git history of this project, if you dig it up and you're willing to refresh it (the storage model has changed a bit since then), then it's supported :wink:\n. It used to, but support was removed eventually, because nobody used it, or maintained the code base. You can find the code in the git history of this project, if you dig it up and you're willing to refresh it (the storage model has changed a bit since then), then it's supported :wink:\n. You just java -jar zipkin.jar, if you have Java installed on the server. Or you can docker run -d -p 9411:9411 openzipkin/zipkin if you use Docker. That is all you have to do.\nFor running Cassandra (or MySQL, or Elasticsearch) there is a lot of documentation other places on the Internet.\nEvery company has its own way of deploying software to their servers, so it's not generally possible to make more detailed documentation than that.\nIf you have more detailed questions, you can ask in Zipkin's gitter chat channel, people are usually very helpful there!\n. You just java -jar zipkin.jar, if you have Java installed on the server. Or you can docker run -d -p 9411:9411 openzipkin/zipkin if you use Docker. That is all you have to do.\nFor running Cassandra (or MySQL, or Elasticsearch) there is a lot of documentation other places on the Internet.\nEvery company has its own way of deploying software to their servers, so it's not generally possible to make more detailed documentation than that.\nIf you have more detailed questions, you can ask in Zipkin's gitter chat channel, people are usually very helpful there!\n. Yes, we should send them as strings!\n. Yes, we should send them as strings!\n. I suspect this problem is on the zipkin side. The JSON that comes from the Java server could be bad. If it sends the IDs as numbers instead of strings? Or are they sent as strings from the Zipkin server?\n. You have 128 bits of storage to put your trace id in. If some other framework provides a trace id encoded as a string, then you can convert that string to a 128 bit integer.. 128 bit (originally 64 bit) was chosen for performance, I think. Faster to lookup, less overhead when transporting data/forwarding headers/less storage space used. Zipkin data is often the biggest database a company has.. Thanks!. Ok, here is the solution we can implement (it was discussed before) Always serve the zipkin ui under a context path, like /zipkin/. Any requests that don't start with that URL will automatically be forwarded. That means there will be no special handling when using a proxy.. /api/ can be duplicate-served under /zipkin/api/ for a while, I guess not all http clients people use handle 302? But all browsers will.. The problem is with crossroads, which doesn't know what to serve (which route) if it doesn't know about the context path.. > is it possible to read this prefix from config.json\nThere's a chicken-and-egg problem there, as the app doesn't know the location of the config.json file (today we assume it's located at root)\nWe could try to heuristically detect whether we're mounted under root or not:\n```javascript\n// inside the web UI code:\n// check if pathname starts with /zipkin/, and if it does,\n// assume that we're mounted under a /zipkin/ context path\nconst isMountedAtRoot = window.location.pathname.indexOf('/zipkin/') !== 0;\nconst basePath = isMountedAtRoot ? '/' : '/zipkin/';\n```\nThat is assuming we don't have any request paths in our application that start with /zipkin/, but I don't think we have.\nWe would then support two use cases: 1) mounting zipkin on root, as we do today, and 2) mounting zipkin under /zipkin/.\nThe case we would not support is 3), to mount zipkin under an arbitrary context path, but that would be an edge case. I don't think we should take on the burden of supporting it. (Why would somebody want to mount an app under /zipkin/ that is not zipkin?)\nThe context path mapping itself can either be done on the zipkin server side if we choose to implement it (MOUNT_CONTEXT_PATH=true) or in a proxy setup (apache 2 or nginx) running in front of the server. In the latter case, this issue can be solved purely on the UI side, with no server side changes necessary. People who mount apps under a context path, with multiple apps on the same domain, are already using a frontend proxy anyways.. > is it possible to read this prefix from config.json\nThere's a chicken-and-egg problem there, as the app doesn't know the location of the config.json file (today we assume it's located at root)\nWe could try to heuristically detect whether we're mounted under root or not:\n```javascript\n// inside the web UI code:\n// check if pathname starts with /zipkin/, and if it does,\n// assume that we're mounted under a /zipkin/ context path\nconst isMountedAtRoot = window.location.pathname.indexOf('/zipkin/') !== 0;\nconst basePath = isMountedAtRoot ? '/' : '/zipkin/';\n```\nThat is assuming we don't have any request paths in our application that start with /zipkin/, but I don't think we have.\nWe would then support two use cases: 1) mounting zipkin on root, as we do today, and 2) mounting zipkin under /zipkin/.\nThe case we would not support is 3), to mount zipkin under an arbitrary context path, but that would be an edge case. I don't think we should take on the burden of supporting it. (Why would somebody want to mount an app under /zipkin/ that is not zipkin?)\nThe context path mapping itself can either be done on the zipkin server side if we choose to implement it (MOUNT_CONTEXT_PATH=true) or in a proxy setup (apache 2 or nginx) running in front of the server. In the latter case, this issue can be solved purely on the UI side, with no server side changes necessary. People who mount apps under a context path, with multiple apps on the same domain, are already using a frontend proxy anyways.. Nice! :shipit:. Nice! :shipit:. Note that 64 bit integers are tricky in JavaScript, which doesn't have them.\nInterestingly, it's possible to represent the number of milliseconds since 1970 in only ~52 bits, while JavaScript can represent whole numbers up to 53 bits. (Just a coincidence) So it will silently work until we eventually encounter a \"year 2000 problem\". \nIt might be safer to store timestamps as strings (possibly compact/encoded), even if that could impact performance?. Note that 64 bit integers are tricky in JavaScript, which doesn't have them.\nInterestingly, it's possible to represent the number of milliseconds since 1970 in only ~52 bits, while JavaScript can represent whole numbers up to 53 bits. (Just a coincidence) So it will silently work until we eventually encounter a \"year 2000 problem\". \nIt might be safer to store timestamps as strings (possibly compact/encoded), even if that could impact performance?. I'll close this issue since it's a duplicate of #1229.. I did an experiment a while ago, I ported the current UI to react/redux. Which worked pretty well, except I was only 50 % done (for the sake of the experiment) so I never finished it.\nStarting from scratch comes with the risk of never getting finished, like zipkin-ui... also I'm not sure if the Angular 2 + TypeScript stack is as familiar to general developers as React+Redux (or both might be inaccessible, I'm not sure).. The last renovation effort stalled at 80 % progress, I think one of the mistakes was to do it from scratch, instead of aiming for a gradual rewrite. By doing it gradually, the migration can be prioritized or stalled, and there will still be benefits from the partial migration. (Though more tech debt from having two tech stacks in the same UI). LGTM, thanks. I'm surprised this is an issue. We use Mustache for templating, which escapes input by default.... I'm surprised this is an issue. We use Mustache for templating, which escapes input by default.... Looks like there's a lot of cowboy jQuery in our views, that don't escape input properly.\nI'm not sure if it's faster to just port the code to React/Redux, than to look through all the vulnerabilities and fix them.... Looks like there's a lot of cowboy jQuery in our views, that don't escape input properly.\nI'm not sure if it's faster to just port the code to React/Redux, than to look through all the vulnerabilities and fix them.... I found it, it's all in this file: https://github.com/openzipkin/zipkin/blob/master/zipkin-ui/js/component_ui/spanPanel.js#L35\nIn some places, we're doing\njavascript\nconst html = `<tag>${unescapedStuff}</tag>`;\nand the fix is to do\n``javascript\n// https://stackoverflow.com/a/12034334\nconst entityMap = {\n  '&': '&amp;',\n  '<': '&lt;',\n  '>': '&gt;',\n  '\"': '&quot;',\n  \"'\": '&#39;',\n  '/': '&#x2F;',\n  '': '`',\n  '=': '='\n};\nfunction escapeHtml (string) {\n  return String(string).replace(/[&<>\"'`=\\/]/g, s => entityMap[s]);\n}\nconst html = <tag>${escapeHtml(unescapedStuff)}</tag>;\n```\nI don't have time now to spin up Zipkin with test data and confirm that it works, but that should be the fix.. I made a quickie PR. @sirtyro do you still have a server available with the tampered data? Could you try running my patch and see if it helps?. @adriancole can you pick up this PR from here?. Great!. We should definitely keep it in mind if more people ask for a public test server (rule of 3?) but we would need infrastructure to support it, so it wouldn't be trivial.. The extension could be updated to support both... check one and fallback to the other. A bit tricky if /zipkin/config.json returns index.html though... Maybe it's possible to check the content-type header, or check if the payload is valid json.... Zipkin already has a feature to download a trace as a JSON file (and I think also one to upload a JSON file and render it in the UI). Wouldn't that cover the use case? You could just store the JSON file wherever you want, on a hard drive, dropbox etc...\nI do agree with @adriancole's comments in general.. Another solution is to run a cache proxy like Varnish in front of Zipkin, and make it cache results for 30 days. (Assuming that the amount of Zipkin data developers look at will be very small relative to the total amount of Zipkin data, so it could comfortably fit on a cache instance). > Without this feature, we'll first have to download the trace, attach it to the bug ticket, and upload if back to zipkin server for looking at the trace. This feature provides a convenient way to archive traces with a single click.\nWith the Varnish solution, I guess you could just copy/paste a link to the trace in the bug report?. If we're going to support ScyllaDB I suggest we should run integration tests against it. Somebody would need to set up Travis with ScyllaDB.. There's already an existing open source project you can use for this, fluentd. I used it with Scribe (before Zipkin went to kafka town), it works pretty well. The main project is written in Ruby, but parts of it have been ported to Go. We might need to write some documentation about how you can use it with Zipkin efficiently.. So line 41 and 42 should just be replaced with .sumByKey?\n. On alternative option is to accept  varargs, e.g.\nscala\nlibraryDependencies ++= finagle(\"core\", \"http\", \"thrift\")\nwhich could be implemented with something like\nscala\ndef finagle(name: String*) = name map { n => \"com.twitter\" %% (\"finagle-\" + n) % \"6.26.0\" }\n. Why do we have both Project.defaultSettings and defaultSettings?\n. You could use apply/splash:\nscala\nlibraryDependencies ++= Seq(\n  junit,\n  finagle(\"core\",\"thrift\"):_*,\n  hadoop(\"core\"):_*\n)\nbut many is fine as well.\nI guess :_* is also quite magic for people to read, so maybe it's better to keep it simple, like with many.\n. No, they were just for testing so one of you guys (or Travis) could verify it was correct without starting sbt in debug mode and step through it until you find the value. According to Travis, it looks correct, I'll remove the println statements and push a new commit to this PR.\n. Does this work if you say sbt project zipkin-scrooge, that it won't change the working directory?\n. The assembly stuff is still quite fragile, and unfortunately we don't have tests for it yet... you'll have to start up Hadoop and everything just to see that it works. Does this version bump break anything, or is it fine?\nWe should probably have a Hadoop docker image just to test that this job works...\n. Well, not really important at all. Mostly when you work with Scrooge, which isn't that often.\n. Is it possible to do something like this?\ntraceIds.map(traceId => spans.get(traceId).flatten)\n.flatten will (example given) turn Seq(None, None, Some(1), None, Some(2)) into Seq(1,2) so you don't need to filter by isDefined and map on .get.\n. .get is dangerous... (purists say it shouldn't be in the Scala library in the first place)\nMaybe .flatten works here too?\n. I think this is the same as .map(Long.unbox) (mentioning it since Long.box was used above)\n. I suggest we give ._1 and ._2 names, for readability:\nscala\n.groupBy { case (whatevernameofthekey, _) => whatevernameofthekey }\n. I think you can use\nscala\n.mapValues(_.values)\nor something with .flatten? (I'm not sure if that's possible)\n. Ah, it's a Future. You'll have to use\nscala\ngetSpansByTraceId(traceId) map { spans => spans foreach { span =>\nThen you won't need to return Future.Unit in the last line.\n. An intermediary variable could be used here, for readability?\nval traceIds = spanName match { ... }\ntraceIds.map { case ... }\n. How about repository.getSpanTtlInSeconds(traceId)? A longer method name to compansate for the lack of time unit?\n. I guess you don't need thread-safe constructs inside a ThreadLocal? Wouldn't a Long be enough here? Or maybe this field is shared across threads?\n. We could even use 0.9.2!\n. the if/if-else/else + isInstanceOf looks to me like it could be written as pattern matching?\ne match { case _:CancellationException => ...\n. That's a tricky one... we could try\nscala\n(e, e.getCause) match {\n  case (_:CancellationException, _:CancelledRequestException) => ok\n  case (_:QueueFullException, _) => pushbackCounter.incr(); tryLater\n  case _ => ...\nor\nscala\ne match {\n  case ex: CancellationException if ex.getCause.isInstanceOf[CancelledRequestException] => ok\n  case ex: QueueFullException =>  => pushbackCounter.incr(); tryLater\n  case _ => ...\nbut you're right, it's not so easy to express with pattern matching as one would think...\n. Oops, I'll remove this, it wasn't intented to be commited\n. Oops, this was also not meant to be commited.\n. jquery-cookie is replace with js-cookie: https://github.com/carhartl/jquery-cookie\n. Can we remove this line without a too big impact on build time? It's a relatively rare case that we want to remove node modules we've introduced, but still, we might want to have them removed from node_modules anyways. Some node.js tools do sanity checks on the whole node_modules folder for whatever reasons, even if some modules aren't being used.\n. Yes, getSpansByService(svc) basically returns spans (the argument from L289), except when spans is a list of strings - then it will convert the list of strings into a jquery object, before returning that jquery object. (like you can see in its implementation)\n. What the code is trying to do is, getAllSpans, which isn't implemented, so it loops through all the service names, and then gets all spans for every service name.\n. Finally, ES6 destructuring (similar to unapply/pattern matching in Scala) and harmony modules!\n. I changed the markup while I was here... navbar-text is more correct than navbar-brand, according to the Bootstrap docs.\n. That's right. And you can read them in window.config.\n. It's all rendered based on JSON API calls, so we should rather add cache-control headers to the API. (In zipkin-query, and zipkin-web should forward the cache headers in CopyRenderer)\n. What is the expensive part of it? The call between the query server and the storage engine/cassandra or the browser rendering? It sounds strange that it takes such a long time... is it due to very many service names?\nJavaScript converting a list of names into html is very quick (as quick as if it were done in Java) so it sounds like either IO or an expensive algorithm is to blame...\nDoes it take several seconds before this commit? Or only after this commit? Or regardless?\n. object spread syntax: for each key in modelView, put the key and its value into this object.\n. shorthand for { limit: limit, minDuration: minDuration, endTs: endTs } etc\n. Yes, the spans are loaded async from the frontend code. (I think they were loaded twice)\n. This merely replicates the existing behavior :wink:\nI agree that we should replace it with traces.length > 0.\n. > What happens if I GET the home page with ?serviceName=x?\nThe backend performs a query against the storage engine, for traces with serviceName x. So the query is performed.\n. It seems to be configurable as a flag in zipkin-query. To be honest, I'm not 100 % sure what it does.\n. Could somebody find a commit in the history (before I made the refactorings) with the Scala logic that sets it up? I'm might have overseen it.\n. lodash is great for doing functional stuff on collections, mimicking the Scala implementation which uses Scala's collection API a lot. Moment is for formatting datetime.\n. chai is an assertion framework used for javascript unit tests, we'd never run it in production code but I've included it here temporarily to perform the deep-equals comparison.\n. I guess this if-check is redundant, but it will be removed anyways, when we remove the comparison code.\n. Sorting service durations alphabetically by name was more deterministic than the existing sort order (which seems random)\n. It didn't look like annotations and binaryAnnotations were in use, in index.mustache.\n. That's the whole point, it's going to get lost. (Chai will be removed as a dependency, only kept in tests.)\nCommenting about Chai in the unit tests is somewhat redundant, since it's a core part of the node.js ecosystem. It's like explaining in src/test/scala what ScalaTest is.\n. :stuck_out_tongue_winking_eye: \n. :-/ not sure how this ended up here, will remove it before merge\n. This is basicalle #1014, which was needed here, since it's implemented that way in JS.\n. Should be fixed now. I updated the PR.\n. I was thinking (but it's a separate/later issue) that we should consider sending trace summaries over the wire instead of complete traces, in the default/index/search results page. And only send the complete trace data when you're looking at a single trace.\nAnyways, should we go that direction, the code is still there in git.\n. Oh sorry, I didn't know that :-/ We should probably fix that (allow zipkin to ignore properties it hasn't heart of) so we don't deployment scripts...\n. One of the arrow syntax perks: We don't have to worry about binding \"this\" all the time :smile:\n. Is it possible to use /static here and assets/app.min.js in \"filename\"?\n. Ok, I see. Is it possible to set HtmlWebpackPlugin's filename in terms of __dirname maybe? It doesn't feel quite right to put a generated resource in the parent directory of where it's supposed to be :-/\nBefore we merge this, maybe we could open a PR on zipkin-query and zipkin-java to see what the corresponding diff would be there? I'm not so convinced it will be so much easier, but then again I haven't looked to much at it, so I might be wrong. I'm a bit -1 for this change, unless it gives us real benefits on the other side...\n. I'd rather not maintain index-routes.json... I firmly believe that serving index.html as a fallback for every route that has no other route handler should be a basic feature of any web server; if it's not, then it should be that server's responsibility to bear the burden of finding a workaround.\n(I'm sorry that I'm so stubborn)\n. ~~404~~ \"route not found\" will be handled by crossroads on the client side. If crossroads cannot find a matching route in window.location, we show an error message. (Technically it's not implemented in zipkin-ui today, but should be on the short-term todo list)\n. What's a .* package?\n. Since we're sunsetting zipkin-web, I suggest that we hard-code routes for serving index.html in here?\nWe could do the same with static assets.\nFinatra, which is used in zipkin-query, solves this quite elegantly.\nAnother alternative is to rewrite zipkin-web to use the http proxy from here: https://twitter.github.io/finagle/guide/Quickstart.html\nSince assets are served from zipkin-query, then zipkin-web could forward all HTTP traffic to zipkin-query, as-is, without any route logic.\n. > if we know something's not there, I have a slight preference to let the server already tell me so.\nThe problem is that we often don't know that on beforehand on the server side. E.g. if you hit /trace/nonexistenttrace, the server would say that route exists. Except we want to show an error message, \"trace not found\". But in order to do that, we need to do the API call from the client, to find out that the trace indeed doesn't exist.\nThe problem with debugging here, is that our configuration of crossroads in zipkin-ui isn't configured to handle scenarios where it gets a location and it has no route handler for that location. We should present a generic \"not found\" message in the UI. In the network tab, we would see that we got a 404 from the api request.\nSome initial head-scratching is unfortunately the cost of migrating to pure client-side applications... but on the other hand, what we gain in the long term, is the complete separation of frontend and backend code, so there are no \"leaks\" between them.\n. \"Why is it packaged as a .jar and not a .zip?\"\nWhich could be answered by something like \"since many zipkin servers are java-based, it's convenient to distribute the ui as a jar, which can be imported by the Gradle build tool. A .jar file is really only a .zip file, and can be treated as such. It can be opened by any program that can extract zip files.\"\n. thrift span to span to span? :-/\n. It shouldn't - expect is already used in other places in the tests. E.g. in traceSummary.test.js:\njavascript\nexpect(traceSummary([])).to.equal(null);\nAre you sure it's not some other lint error?\nThey're also \"magic imports\", they exist in the global scope without us having to import expect and should from chai manually. A karma plugin does that for us.\n. Is there a better name for this function? It seems to like does more than just parse the query?\n. Maybe add // 24 hours as a comment here?\n. I guess it's not necessary (just a suggestion for readability), but a set of parentheses here would help clarify, in case you cannot remember operator precendence between || and - :wink:\njavascript\nquery.startTs || (endTs - this.attr.config('defaultLookback'));\n. We don't need multiline style here unless we import more than ~4 four items (I think).\njavascript\nimport {a} from 'x';\nimport {a, b} from 'y';\nimport {a, b, c} from 'z';\nimport {\n  a,\n  b,\n  c,\n  d\n} from 'zzz';\nWDYT?\n. This is only needed if we compare really, really deep data structures, like e.g. traces. (For prettier output)\n. should.not.exist(parsed.startTs) or expect(parsed.startTs).to.not.exist()\n. Isn't it a little bit inconsistent that endTs is a string, while lookback is a number?\n. Could this somehow be a template partial instead?\n. We could load babel-polyfill and actually use a JavaScript Set?\n. Maybe use newlines here?\n. Defined, but never used (try running npm run lint)\n. The raw data doesn't belong inside the modelview; this function is supposed to transform the incoming raw data into a format that is ready for mustache rendering.\nCan you try refactoring, so that raw is not passed as part of the modelview?\n. The parameter that is called \"modelview\" today could be renamed to \"data\", and then \"data\" would contain two properties: \"modelview\" and \"raw\"?\n. I guess this isn't needed anymore?\n. You don't need <button>here, you can just put the classes on the A directly, like this: <a class=\"btn btn-primary btn-xs badge\"> (I think)\n. This extra space isn't needed\n. Ah, I see it now! Sorry.\n. No, it doesn't have the value property. How about using an explicit click handler on the a element? I think that's cleaner overall.\n. You can do const isError = a.key === 'error' to avoid the let.\n. Oh, I missed that, sorry!\n. It's maybe easier to just add --optimize-minimize --optimize-dedupe here, to the CLI arguments, then we don't need to tweak the webpack config or set environment variables etc.\nPassing environment variables to the script like this will also not work on Windows, so we'd have to use crossenv or something like that.. I don't think karma understands these flags... anyways for unit tests we prefer fast runs over minimized code. (After all, unit test code is not shipped to the user). This would affect the API as well.. Should we sanity check that it ends with /? Looks like the rest of the code assumes that. We could either show an error to the user, or just append / here?. There's a lot of code duplication going on here... maybe we should make a apiClient.js which basically just accepts relative paths and wraps $.ajax?. ",
    "aruld": "When I clone that project, I don't get the maven pom.xml, instead I see build.xml. I tried running ant install-local target but it failed with protoc error. Any clues?\n. nevermind, I figured out the 3.0 branch. trying that now.\n. No, I believe EB 3 was never pushed to maven central (https://groups.google.com/forum/?fromgroups#!topic/elephantbird-dev/7lFPQUNbB6A) which explains the original problem. Anyways, I was able to checkout EB3 branch using:\ngit clone https://github.com/kevinweil/elephant-bird.git -b branch-3.0 eb3\nand ran mvn package, which fails because I don't have LZO libraries available on my system. \nI believe I'll wait until EB 3 is publicly available. Thanks for your help!\n. My bad, thought the PR was merged. Applying PR locally fixes this problem. Thanks again!\n. ",
    "caniszczyk": "There's a maven mirror now:\nhttps://twitter.com/travisci/status/213719183060516864\nCan we try the build again?\n. There's a maven mirror now:\nhttps://twitter.com/travisci/status/213719183060516864\nCan we try the build again?\n. Awesome, maybe it would be good to squash these commits into one before merging :)?\n. Awesome, maybe it would be good to squash these commits into one before merging :)?\n. I'd also update the README to link to the build status:\nZipkin \nThis is so awesome btw, thanks for doing it!\n. I'd also update the README to link to the build status:\nZipkin \nThis is so awesome btw, thanks for doing it!\n. On just a random side note... have you look at AngularJS vs. Backbone?\nToo late now but something for future reference: http://www.angularjs.org/\n. Looks like Kafka is being moved to the ASF incubator which is good:\nhttps://github.com/apache/kafka\nWhat sucks is they have no releases yet there.\nWe should probably start adding a NOTICE (e.g., https://github.com/twitter/scalding/blob/develop/NOTICE) file to track these third party dependencies...\n. Apparently there is a 0.7.1 release... not available on any maven repo that I can find...\nhttp://incubator.apache.org/kafka/downloads.html\n. Thanks guys, much appreciated!\n. Because it automatically syncs to Maven Central.\nI'm slowly weaning Twitter off of maven.twttr.com for most things and having artifacts published to Maven Central instead.\n. Looks fine, would be great to have this change coincide moving to Maven :)\nTo sync to maven central, we need spruce up our POM most likely with extra metadata:\nhttps://docs.sonatype.org/display/Repository/Sonatype+OSS+Maven+Repository+Usage+Guide#SonatypeOSSMavenRepositoryUsageGuide-6.CentralSyncRequirement\n. Also not sure if you're aware guys, Redis is available on Travis CI for testing purposes... you just have to ensure it's started: http://about.travis-ci.org/docs/user/database-setup/\n. What dependencies are on maven.twttr.com that we can't publish to Sonatype OSS and Maven Central?\nFinagle?\n. sadness, we'll try again\nAlso, we should ensure when we move to Maven we enable Zinc for faster compilation:\nhttp://blog.typesafe.com/zinc-and-incremental-compilation\nIn the future, we may also want to just review our whole dependency tree and see if we can refactor to minimize them:\ne.g., 'mvn dependency:tree' should tell us quite a bit\n. Ya, Travis CI has lots of amazing services that are available:\nhttp://about.travis-ci.org/docs/user/database-setup/\n. Ya, Travis CI has lots of amazing services that are available:\nhttp://about.travis-ci.org/docs/user/database-setup/\n. @adriancole should have the power now :)\n. Please add copyright headers to new source files.\n. Please add copyright headers to new source files.\n. Please add copyright headers to new source files.\n. Please add copyright headers to new source files.\n. Please add copyright headers to new source files.\n. Please add copyright headers to new source files.\n. ",
    "franklinhu": "We're building successfully on Travis CI now.\n. +1, maybe add to the log message to say exactly what was changed\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. +1\n. Fixed upstream in Ruby code in #53\n. +1\n. +1\n. Merged thanks!\n. +1\n. +1\n. The vast majority of the js/css is copied over from zipkin-web and doesn't need to be reviewed. It's only there because sbt doesn't seem to package symlinked resources.\n. Merged thanks!\n. +1\n. Woops this is unnecessary\n. +1, maybe modify the commit msg when you actually merge it though\n. +1, maybe modify the commit msg when you actually merge it though\n. +1\n. +1\n. I'm unsure about the first one and will take a closer look tomorrow.\nFor the second one, core annotations are the four annotations that we expect to see in every span:\n- client send (cs)\n- server receive (sr)\n- server send (ss)\n- client receive (cr)\nThese fully describe a method call. We don't index these since we expect all spans to contain them, and you already get that kind of functionality from searching by span name or service name. We only really want to index custom annotations that users provide in their application code.\n. I'm unsure about the first one and will take a closer look tomorrow.\nFor the second one, core annotations are the four annotations that we expect to see in every span:\n- client send (cs)\n- server receive (sr)\n- server send (ss)\n- client receive (cr)\nThese fully describe a method call. We don't index these since we expect all spans to contain them, and you already get that kind of functionality from searching by span name or service name. We only really want to index custom annotations that users provide in their application code.\n. I guess there's nothing theoretically wrong with a core annotation existing in the annotations index; it's mainly just a performance optimization to avoid unnecessary writes.\n. I guess there's nothing theoretically wrong with a core annotation existing in the annotations index; it's mainly just a performance optimization to avoid unnecessary writes.\n. First point is fixed in #93, going to go ahead and close this issue\n. First point is fixed in #93, going to go ahead and close this issue\n. I think any Trace that gets returned in a Query call will be merged and sorted anyway for easier consumption so there shouldn't be an extra performance hit.\n. I think any Trace that gets returned in a Query call will be merged and sorted anyway for easier consumption so there shouldn't be an extra performance hit.\n. Should make the title and description of the pull request a bit more descriptive\n. +1\n. +1\n. Just two small nits, be sure to update the pull request title and description. +1 other than that\n. +1\n. +1\n. This looks good, but I only see the changes to collector.sh in the diff. Did you forget to add the config for Finatra?\n. +1\n. Merged, thanks for your contribution @glynd!\n. +1\n. Do you have the line number for the \"sorted again\" in application.js?\n. I believe that sorts the query results on the index page. Weren't you talking about on the spans in the visualization on the individual trace page?\n. Do you have any test data that breaks it? I'm trying to reproduce locally. Thanks!\n. +1\n. I don't think I ever tested it in the Finatra app. Pretty sure Rails massaged the nil into a valid thrift argument.\n. Hadn't actually seen that. Hm will keep that in mind for the future\n. Fixed in #137\n. Fixed in #137\n. Yeah it might be better to split this into separate pull requests\n- backend api changes (collector and query)\n- hadoop changes\n- web/UI changes\n. One nit, +1 other than that.\n. One nit, +1 other than that.\n. Ugh wrong head ref\n. For complex queries (ones that require more than one index lookup), we do some fancy stuff:\n- Get one column (timestamp) from each slice and find the minimum (adjusted timestamp). Because we're computing a set intersection , it's impossible to have valid trace IDs that occur after the latest value for a particular slice. \n- Run the actual queries that fetch the appropriate number of columns with the adjusted timestamp (padded by a minute to take care of write delays, time skew).\n. For complex queries (ones that require more than one index lookup), we do some fancy stuff:\n- Get one column (timestamp) from each slice and find the minimum (adjusted timestamp). Because we're computing a set intersection , it's impossible to have valid trace IDs that occur after the latest value for a particular slice. \n- Run the actual queries that fetch the appropriate number of columns with the adjusted timestamp (padded by a minute to take care of write delays, time skew).\n. +1, just change the title to be more descriptive\n. +1, just change the title to be more descriptive\n. Yup going to do that after this gets merged\n. Yup going to do that after this gets merged\n. I just changed it back to /traces/ since that's what's currently in place in the ruby version\n. I just changed it back to /traces/ since that's what's currently in place in the ruby version\n. Could you be more specific about the issue?\n. Could you be more specific about the issue?\n. Is there a particular reason why you need zipkin-test-0.1.0.zip? I can regenerate the jar, but 0.2.0 is probably a better bet if you're looking to use zipkin. Also, zipkin-test really only has a couple of resources for testing that probably aren't too useful for running the service.\n. Is there a particular reason why you need zipkin-test-0.1.0.zip? I can regenerate the jar, but 0.2.0 is probably a better bet if you're looking to use zipkin. Also, zipkin-test really only has a couple of resources for testing that probably aren't too useful for running the service.\n. If you're looking for examples of how to instrument your own Java service, we have some tips at http://twitter.github.com/zipkin/instrument.html. For any broader questions, we suggest you join the Zipkin Users Group and we'd be happy to help you out! https://groups.google.com/group/zipkin-user\n. @mosesn I've already moved them to zipkin-common so they can be shared across all the modules\n. I believe we're in the process of upgrading Finagle to use SLF4J 1.6.x, so this should be fixed in the near future\n. This also relates to #152\n. That makes sense. Closing this PR and will figure out a cleaner way to do this.\n. I added a 'Full documentation' section in the readme, and also moved some of the sections from doc/overview.md since I feel like they belong on the main readme.\n. scald.rb was removed so this is no longer an issue.\n. scald.rb was removed so this is no longer an issue.\n. Yeah that was kind of silly. It's now updated to just shove the Option into the JsonSpan, so jerkson should take care of it.\n. They didn't seem to have an official Scala 2.9.* build, and the 2.8 jar is ABI incompatible with 2.9. I just used one that @jasonjckn published on clojars. I'll add the NOTICE file.\n. This might be a Cassandra specific use case that should be refactored into the query service. The motivation behind only fetching one column first for each subquery is to address sparse return sets. \nFor example, imagine you're joining two subqueries that fetch slices (S1 and S2) from their respective indices. These slices are ordered by timestamp, so let's let the last entry in each slice have timestamp T1 and T2. Since spans from the same trace probably have a small difference in timestamps, we should set the endTimestamp to be min(T1, T2). This will fix the problem where you have a very sparse index joined with a giant one so you only zero in on the time slice where the small index has entries.\nYou can see that do the actual query a bit father down in that method.\nAre you seeing bad/unexpected behavior in Redis?\n. This makes sense and looks good. Could you add a unit test so it won't regress in the future?\n. Going to go ahead and merge this since we have some users who are blocked by this.\n. +1 Thanks!\n. Filed the ticket: https://issues.sonatype.org/browse/OSSRH-4607\n@caniszczyk can you sanity check that?\n. Looks good to me. Thanks!\n. @caniszczyk We have Travis CI disabled right now due to maven.twttr.com's flakiness :(\n. @jaredlwong Just sent #263. Will get that merged if it passes.\n. @jaredlwong Just sent #263. Will get that merged if it passes.\n. @jaredlwong Could you pull in the latest changes from master? That should fix the TravisCI redis issues. Thanks!\n. @jaredlwong Could you pull in the latest changes from master? That should fix the TravisCI redis issues. Thanks!\n. lgtm\n. lgtm\n. lgtm, fire at will.\n. lgtm, fire at will.\n. Gave some cursory comments, but will sit down and do a more in-depth review in the near future. \nAlso, slight style nit: spaces after colons for type declarations\nmyThing: Array[Byte]\nrather than\nmyThing:Array[Byte]\n. You may want to rebase this branch against master since the anorm stuff was merged?\nEdit: or just send a PR for the changes against the branch this was built on?\n. You may want to rebase this branch against master since the anorm stuff was merged?\nEdit: or just send a PR for the changes against the branch this was built on?\n. lgtm\n. lgtm\n. +1\n. +1\n. +1, but :frowning: \n. +1, but :frowning: \n. Nice catch!\n. Nice catch!\n. Hmm looks like you're right. We're happy to merge pull requests :)\n. Hmm looks like you're right. We're happy to merge pull requests :)\n. So it looks like this was an oversight in the way that AnormDB joined spans with annotations and binary annotations. I've posted #308, which should fix this issue. @wadey if you try the branch fix-anorm-span-merging does it work?\n. So it looks like this was an oversight in the way that AnormDB joined spans with annotations and binary annotations. I've posted #308, which should fix this issue. @wadey if you try the branch fix-anorm-span-merging does it work?\n. @elliottneilclark Sorry didn't see this PR until now. Looks good to me!\n. +1\n. +1\n. +1 thanks for the fix!\n. lgtm +1\n. +1\n. +1\n. \"identifies an individual request\"\n. s/id/Id/\n. Can put these sentences in separate lines. Will make it a bit easier to read\n. query script should be in zipkin-server/config/query-prod.scala still\n. same as above\n. \"non-Scribe\"\nalso maybe change the comma to a semicolon\n. IP address\n. Apparently not. Handed by Rails black magic.\n. Fixed in 1dd303552a6664a5867eba23ef13369bd9a0a266\n. Fixed in    56a6f08\n. Fixed in    729df15\n. Fixed in 729df15\n. Fixed in    48e2efe\n. Fixed in    6535894\n. Nit: absolute imports are preferred to relative ones\n. add some stats here so we can see how much debug traffic we get\n. license header\n. you should be incrementing a different counter\n. this name is kinda unintuitive. something like traceExists or tracesExist would be better\n. Since all the values are going to be Some(..), you're better of just using .map rather than .flatmap and returning the boolean in the closure rather than the Option. It reduces some overhead\n. It may be a bit cleaner if we return a map<i64, bool> rather than just a list, although not necessary\n. You can avoid using the HashSet with something like\nids.flatMap {\n    // return None if we don't want to include the id\n    // return Some(id) if we do\n  }.toSet\n. Add a different counter for this, so we can see how many times it's called\n. Shouldn't need this import\n. Should put if/else on separate lines for better readability, with braces\n. It seems to make more sense to have processKey still take a List[String] and have the caller do the flattening\n. This is okay for now, but we may want to consider using a templating engine like mustache for this kind of stuff. Concatenating HTML strings is kinda janky\nhttps://github.com/spullara/mustache.java/\n. Any reason why you moved this into the singleton rather than in an the class instance? Keeping state in singletons can be scary\n. Ok that sounds alright. For clarity you should import scala.collection.mutable and construct the object with mutable.HashMap\n. you can just make them case classs since they don't have a body\n. Map[String, EmailContent]() should be sufficient\n. I think in general people like to put any functions that are passed in second in case people want to pass in lambda. You could do something like\ndef traverseFileTree(f: File)(func: File => Unit)\nwhich makes lambdas a bit prettier\n. Try to keep lines under 100 characters for readability\n. You should make these configurable\n. Multiline if/else statements are preferred\n. Remove this\n. These should be configurable too\n. Same thing with the nulls here. Return type of Some[List[String]] would be good\n. Remove internal ref\n. pretty sure you don't need the .rb at the end\n. try to use com.twitter.logging.Logger rather than println in prod code\n. avoid nulls\n. remove this\n. Talked to Johan IRL about this. The python server is just nice to have if you're doing JS/CSS development since the resources get packaged in the jar and won't be refreshed without a full compile; it's not necessary to run the Scala UI.\n. Remove commented out\n. There should probably be a license header here\n. Ddi you intend to attach a blur handler?\n. Probably doesn't need to be fixed, but I believe jQuery selectors are evaluated in ascending hierarchy, so for something like .js-zipkin-navbar > li > a will get all the a anchors on the page, select those that have li as parents, etc. This can get pretty expensive if you have lots of anchors on the page. In contrast, if you attach a class to the anchor selector a.some-class then there will be fewer objects initially selected.\n. e is usually used for events. Should avoid using it for elements. Can use el instead\n. spacing\n. You can remove this\n. This should probably happen in a template\n. empty?\n. You can just add this to localResources. It's not a big deal if we serve the CSS and JS on all the pages\n. If you add the JS and CSS to localResources you won't need this function\n. Uhh any reason this isn't the full license header?\n. Why not just add a CSS class to the container on the aggregates page?\n. Is this date value supposed to be hardcoded?\n. Ideally we want all style in the stylesheets\n. This probably doesn't need to be wrapped in an anonymous function\n. Instead of iterating through the pills and binding a click handler to each you can just bind it via the selector\njavascript\n$(\".js-zipkin-navbar > li\").click(function(elem) {...});\n. Can just do \njavascript\n} else {\n  parent.removeClass(\"active\");\n}\n. It looks like all of this is nested inside _init but it's at a lower indentation level.\n. It's probably simpler if you just use the selector...\njavascript\n$(\".js-zipkin-navbar > li.active\").removeClass(\"active\");\n. remove?\n. You don't actually use this passed in variable so you can omit it. It's global anyway\n. aria-labelled-by?\n. Fixed in b1a5057\n. whitespace\n. Variable names prefixed with _ make me feel kinda dirty when I use them.\nIt might be cleaner to just subclass Email with TestEmail and and construct the right one in MailConfig. TestEmail can then just override the send method use the right defaults and call the super method.\n. You can ignore this one, but if you want to avoid having java.util.List everywhere you can do import java.util.{List => JList} then just use JList everywhere\n. When you close it should also wrap it in a try{...} finally{<close here>} to make sure it closes even if you have exceptions\n. This one should be closed too\n. You can just make it a default param:\nscala\ndef apply(testMode: Boolean = false)\n. Is it defined somewhere else? If not, it should be var s so we don't pollute the global scope.\n. Good catch. Fixed in e515858\n. should sort the imports on all files\n. docs about what this does?\n. probably shouldn't be calling .get on the Option\n. for brevity we can omit the \"be able to\"\n. This breaks the linking of the futures. If you want to convert a Future[A] into Future[Unit] you can use myFuture.unit\n. Not a huge fan of the for(..) syntax. might be clearer as something like\nspan.serviceNames collect { case name if name != \"\" => serviceArray.add(name) }\n. .unit instead?\n. .unit?\n. would prefer to wrap each of these method blocks in braces\n. maybe a brace at the start of the function, and the Future.join inside?\n. would be clearer if styled like:\nif (...) {\n  ...\n} else {\n  ...\n}\n. This breaks the future chaining. If converting to Future[Void], prefer myFuture.voided\n. A little confused about what this is supposed to do. It's pushing a key/value to Redis, then setting the expiration?\nIs it okay for the expiration to be set before lPush does? It seems like this should be more like:\ndatabase.lPush(string, List(value)) flatMap { _ =>\n  defaultTTL match {\n    case Some(ttl) => database.expire(string, ttl.inSeconds).unit\n    case None => Future.Unit\n  }\n}\n. Yeah I think we should make the interface return Future[Unit] :\\ I'll file a ticket\n. we can fix it in a future review\n. Yeah that's a bit clearer\n. Should use com.twitter.zipkin.common.AnnotationType.String rather than converting the generated thrift struct.\n. Same here\n. These should be @param annotations\n. I don't really like passing in the ZipkinServerBuilder as an argument, but subclassing it causes all sorts of Scala horror and breaks the niceties from using case classes. Not sure if there's a cleaner way.\n. Done\n. Finatra does config through system properties right now. It'd be a stopgap until @capotej gets out a more robust configuration system.\n. The implementation is in zipkin-collector-scribe com.twitter.zipkin.builder.Scribe. Needed a way to abstract out the thing that stands up the server, since it varies a lot (scribe/thrift, json over http, etc). The filter allows the service to dump whatever they want into the queue, and potentially do the expensive stuff in the worker threads.\n. whitespace\n. don't need the override since pageTitle is abstract.\n. Would prefer to use trait rather than abstract class\n. :)\nMaybe it makes sense to move all of these into a flattened CONTRIBUTORS.md file\n. snapshots?\n. indentation\n. indentation is a bit funky\n. There are other contributors too. \n$ git log | grep Author: | sort | uniq\n    Author: @franklinhu\n    Author: @glynd\n    Author: @jerryli9876\n    Author: @johanoskarsson\n    Author: @mosesn\n    Author: @olix0r\n    Author: @shawnps\n    Author: @timjr\n    Author: @viktortnk\n    Author: franklinhu\n    Author: johanoskarsson\nAuthor: Brian Degenhardt <bdegenhardt@tw-mbp13-bdegenhardt.local>\nAuthor: Brian Degenhardt <bdegenhardt@twitter.com>\nAuthor: Brian Degenhardt <bmd@twitter.com>\nAuthor: Franklin Hu <franklin@thisisfranklin.com>\nAuthor: Franklin Hu <franklin@twitter.com>\nAuthor: Jerry Li <jli@twitter.com>\nAuthor: Johan Oskarsson <>\nAuthor: Johan Oskarsson <johan@oskarsson.nu>\nAuthor: Moses Nakamura <nnythm@gmail.com>\nAuthor: Oliver V. Gould <olix0r+github@gmail.com>\nAuthor: Tim Daly, Jr <timjr@yahoo-inc.com>\nAuthor: Twitter, Inc <>\nAuthor: Viktor Taranenko <viktortnk@gmail.com>\nAuthor: bmdhacks <bdegenhardt@twitter.com>\n. ZOOKEEPER_VERSION?\n. Oh jeez I forgot about this. This should get fixed in the future, not in this review.\n. whitespace\n. \"which visualizes traces\"?\n. Are the other core annotations converted on the server? Might make sense to do it all in the same place\n. semicolons? :trollface:\n. let's sort these imports\n. space after the colon\n. should invalid and zero be vals?\n. Scala convention is to use CapitalCamelCase for constants\n. Why use Longs rather than Time objects?\n. This seems like it belongs outside as a constant and refed here\n. start with capital for constants\n. may make sense to add an isValid method to Trace instead of having a separate instance\n. This is a bit confusing and conflates a few conditions. Maybe split out the invalid trace condition separately and have the for comprehension only check for ID matching?\nif (!l.isValid || !r.isValid) {\n  // throw an exception\n}\n. I'm starting to think it may make sense to split this into two classes since conceptually there's a \"raw\" trace with an unmerged list of spans and a merged/finalized trace and the monoid only operates on the former.\n. I guess this wasn't caught in the initial review, but we should we a templates rather than generating HTML by hand. Check out Hogan\n. Is DB a singleton? Would be better to pass it into the AnormIndex as an argument.\nEdit: After looking at the rest, it's probably best to pass in the connection as a parameter.\n. Instead of row you can do something like\nresult map { case (tId, ts) => ... }\n. You likely want Future.value(...)\n. The type params for the inner tuple are a bit opaque; should use a case class for better readability.\n. same here\n. and here\n. May want to take a look at Local in twitter/util since that does fancy exceution context stuff:\nhttps://github.com/twitter/util/blob/master/util-core/src/main/scala/com/twitter/util/Local.scala\n. We should probably namespace these under com.twitter.zipkin.storage.hbase\n. It's recommended to use JavaConverters instead of JavaConversions to avoid invisible implicit magic\n. Future.Unit\n. No need for the trailing braces\n. It'd be good to avoid potential namespace collisions with other projects (however unlikely)\n. You can use a .flatMap here instead of .map { .. }.flatten\n. Do we want to impose a sort order here? If not, we can probably simplify the end part to .toSet.take(limit)\n. same thing here\n. I don't think we want the entire underlying array from the ByteBuffer since it could be shared. We'll have to copy it out to a newly allocated Arrray[Byte]\n. Future.Unit\n. Future.value(..)\n. This may be a bit more readable\noption match {\n  case Some(result) => result.list().asScala\n  case None => Seq.empty[KeyValue]\n}\n. make this value configurable?\n. :\\\nMaybe Retry should operate over Future[T] so it can respond to errors accordingly? Can use\nfuture.transform {\n  case Return(r) => r\n  case Throw(t) => // log the error and try again\n}\n. Should try to avoid blocking\n. Need to copy out the correct segment of bytes we want\n. Same here\n. Bytes.toString(value.array, value.position, value.remaining)\n. Bytes.toString(value.array, value.position, value.remaining)\n. Maybe .toSeq.distinct.take(limit) then?\n. Why are we multiplying by 1000? t is in picoseconds?\n. Seq.empty[String] is slightly clearer\n. same here\n. Is the type annotation necessary here?\n. If Left is always an error string, then I think we actually want to return a Try[A]. Try has two subtypes: Return and Throw.\nhttps://github.com/twitter/util/blob/master/util-core/src/main/scala/com/twitter/util/Try.scala\n. Sorry I should be more clear, is the type annotation on the FuturePool call necessary?\n. We should use Constants.CoreAnnotations and also add the client/server address annotations to that list.\n. We generally try to put closures in braces and move chained calls onto their own lines:\nval spanAnnos = annos.filter { a =>\n  a.traceId == span.traceId && a.spanId == span.spanId\n}.map { ..\n. Same thing here\n. We'll want to remove the uppercase FINAGLE_VERSION a couple lines above this.\n. Also are there API changes in this Finagle upgrade? Would be nice to upgrade Ostrich and Cassie as well.\n. 90 seconds seems a bit long... maybe make this configurable as an arg to the constructor too?\n. Let's vertical align this section on the equals to make it a bit more readable\n. You probably want to also update the dependency for the kafka module up near line 270\n. Could make this a Try and then match on Return and Throw\n. Why do these need to be cast into java.util.List[java.lang.Object]?\n. Oh because of Storm. We may as well just create them as Java objects rather than doing the conversions?\n. I know this is parallel to the original code, but any downsides to just replacing this with an AsyncSemaphore and  FuturePool?\n. How are Flag[Duration] parsed on the command line? As millis? It may be good to have some documentation here so users don't have to go digging.\n. I believe the original one was wired up to update the config values in ZK for convenience.\n. sort these imports\n. Any reason not to use one of the many existing libs for zk clients?\n. case null | \"\" => None\n. This will blow up if ids is empty\n. What's the reasoning behind this change? The endTs argument is supposed to be an upper bound for \"get me all traces before this time\" not after.\n. Ideally we'd be able to support the debug flag here, but if not just make a note that we'd like to fix it in the future?\n. We may as well move all the variable setting to the beginning of the method for consistency. Also, let's make a note that get_or_create_trace_id depends on sample_rate being already set.\n. ",
    "travisbot": "This pull request fails (merged 13fdc321 into 72fd02b9).\n. This pull request fails (merged 196effe9 into 72fd02b9).\n. This pull request fails (merged 500bbac9 into 72fd02b9).\n. This pull request fails (merged 3bc3390f into 2a52e7dc).\n. This pull request fails (merged 8070184b into 2a52e7dc).\n. This pull request fails (merged dac24b7b into 2a52e7dc).\n. This pull request fails (merged 0f6720db into 2a52e7dc).\n. This pull request passes (merged 472c9a7c into 72fd02b9).\n. This pull request passes (merged 472c9a7c into 72fd02b9).\n. This pull request passes (merged 5ed30aaa into f010be41).\n. This pull request passes (merged aaa1e12f into 2a52e7dc).\n. This pull request fails (merged 05aa37fc into 2a52e7dc).\n. This pull request fails (merged 787227f5 into 73297c1d).\n. This pull request passes (merged 9604d695 into 73297c1d).\n. This pull request fails (merged 378fa716 into f417c9a1).\n. This pull request fails (merged 1fdaa70c into f417c9a1).\n. This pull request passes (merged f4498b03 into f417c9a1).\n. This pull request passes (merged f4498b03 into f417c9a1).\n. This pull request passes (merged 8b39e349 into f417c9a1).\n. This pull request passes (merged 8b39e349 into f417c9a1).\n. This pull request passes (merged bfbf54c1 into f417c9a1).\n. This pull request passes (merged 2ce7618f into f417c9a1).\n. This pull request passes (merged 2213ba2b into b053a9e3).\n. This pull request fails (merged 38359dba into b053a9e3).\n. This pull request passes (merged f491b8bc into b053a9e3).\n. This pull request fails (merged 86bf2ebb into 2d00d901).\n. This pull request fails (merged 86bf2ebb into 2d00d901).\n. This pull request passes (merged f921c37e into b655f8c4).\n. This pull request passes (merged f921c37e into b655f8c4).\n. This pull request passes (merged 6e190085 into a4447346).\n. This pull request passes (merged 6e190085 into a4447346).\n. This pull request passes (merged 628e5cbd into a4447346).\n. This pull request passes (merged 628e5cbd into a4447346).\n. This pull request fails (merged d160bc20 into a4447346).\n. This pull request fails (merged d160bc20 into a4447346).\n. This pull request passes (merged 1737a581 into 483c04f0).\n. This pull request passes (merged 1737a581 into 483c04f0).\n. This pull request passes (merged 0a6c4b40 into 18bdbbad).\n. This pull request passes (merged 0a6c4b40 into 18bdbbad).\n. This pull request passes (merged 65737cc4 into aa67fff2).\n. This pull request passes (merged 65737cc4 into aa67fff2).\n. This pull request passes (merged d6632fd7 into aa67fff2).\n. This pull request passes (merged d6632fd7 into aa67fff2).\n. This pull request fails (merged 567bf454 into aa67fff2).\n. This pull request fails (merged 567bf454 into aa67fff2).\n. This pull request passes (merged ed86cba6 into aa67fff2).\n. This pull request passes (merged ed86cba6 into aa67fff2).\n. This pull request fails (merged de6a07db into aa67fff2).\n. This pull request passes (merged a51440d8 into aa67fff2).\n. This pull request passes (merged 9f1089b7 into aa67fff2).\n. This pull request passes (merged 9f1089b7 into aa67fff2).\n. This pull request passes (merged d0373fae into 6b7af551).\n. This pull request passes (merged 1422e071 into 857d26ec).\n. This pull request fails (merged daf3862b into 857d26ec).\n. This pull request passes (merged c359ac67 into 857d26ec).\n. This pull request passes (merged 96a4df45 into bb74b8db).\n. This pull request passes (merged d44239ae into b0f50d4c).\n. This pull request passes (merged 7748126d into b0f50d4c).\n. This pull request passes (merged 5cef68b3 into b0f50d4c).\n. This pull request passes (merged e784a02e into b0f50d4c).\n. This pull request passes (merged 55def3b4 into b0f50d4c).\n. This pull request passes (merged ab5668b1 into c0720a18).\n. This pull request fails (merged 1d75aed5 into c0720a18).\n. This pull request passes (merged 361c6820 into c0720a18).\n. This pull request passes (merged 12129f6c into c0720a18).\n. This pull request passes (merged 78de8a33 into c0720a18).\n. This pull request fails (merged efa8a122 into c0720a18).\n. This pull request passes (merged 07931fb5 into c0720a18).\n. This pull request passes (merged 58b03000 into c0720a18).\n. This pull request passes (merged 16e7659c into c0720a18).\n. This pull request passes (merged 9d21c9b8 into c0720a18).\n. This pull request passes (merged 3ee6ecf3 into c0720a18).\n. This pull request passes (merged c6bb331e into 8fae7e63).\n. This pull request fails (merged c27ea4f6 into 6047e9dd).\n. This pull request passes (merged b4e5c88f into 727bd005).\n. This pull request passes (merged c7e41593 into 727bd005).\n. This pull request passes (merged 734f8dc5 into 080ddfbe).\n. This pull request passes (merged 5e09b29b into 080ddfbe).\n. This pull request passes (merged 24bd1625 into 080ddfbe).\n. This pull request passes (merged 632362c6 into 080ddfbe).\n. This pull request fails (merged 0d57769c into 080ddfbe).\n. This pull request passes (merged b9a8443a into 080ddfbe).\n. This pull request passes (merged e5ec4b8e into 080ddfbe).\n. This pull request passes (merged f44f0031 into 080ddfbe).\n. This pull request fails (merged e8d3ecf4 into 4fc612b3).\n. This pull request passes (merged 28f57ba6 into 080ddfbe).\n. This pull request fails (merged b0d20e0a into 7836a00c).\n. This pull request fails (merged 72ea1799 into 7836a00c).\n. This pull request passes (merged aaec2e0c into 7836a00c).\n. This pull request fails (merged 2fe3317e into 7836a00c).\n. This pull request passes (merged c447502f into b86eadcf).\n. This pull request fails (merged 52e6e0f8 into 4fc612b3).\n. This pull request fails (merged 83e28182 into 87a73d63).\n. This pull request fails (merged 040b8b4f into cf30f5ae).\n. This pull request fails (merged 040b8b4f into cf30f5ae).\n. This pull request fails (merged 65f3a956 into cf30f5ae).\n. This pull request fails (merged 65f3a956 into cf30f5ae).\n. This pull request fails (merged 57bbc61f into cf30f5ae).\n. This pull request fails (merged 57bbc61f into cf30f5ae).\n. ",
    "bmdhacks": "Zipkin now on 2.9.2\n. worksforme\n. We should remove more things to resolve issues ;-)\n. The reason dapper has divisible sample rates is because endpoints participating in the same trace can have independent sample rates, so dapper needs them to line up occasionally.  Zipkin preserves the sample rate down the line, thus allowing us to have a much more varied rate.\n. Finagle 6.4 now has scala 2.10 artifacts, so this should be working now.\n. +1\nPlease run the bin/git-pull-request.rb command to merge this change\n. I'm not sure what I can do without more information.  The fact that Control-F isn't working on your browser seems to point to a client-side issue, but I'll leave this issue open for a bit to give you some time to track it down.\n. Thanks for doing this!  Apart from my two nits, this looks great.  We'll get it merged asap.\n. Yes, see franklin's request right above this.  Rebase to master and the\ntests should pass.\nOn Thu, Jun 27, 2013 at 4:25 PM, Jared Wong notifications@github.comwrote:\n\nYou guys have any idea about the redis errors?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/pull/253#issuecomment-20161815\n.\n. Ah right, there's a flaky test, I'm rerunning it.\n\nOn Thu, Jun 27, 2013 at 4:34 PM, Jared Wong notifications@github.comwrote:\n\nI'm fairly sure that it's rebased to master. Here are the last 5 commits\nfrom my fork\n77db949 Added better support for Redis.\n1c0cab5 Zookeeper Fixes Our recent changes to bind to 0.0.0.0 was causing\nthe scribe collector to publish an incorrect address in zookeeper.\n4052873 Merge branch 'master' of github.com:twitter/zipkin\n40db92f Fixing Build Properties Generation For whatever reason sbt does\nnot\nwant to change the resourceGenerators inside of the BuildProperties\nplugin,\nso I just do it in each project I need it in.\n3eeb8fc Added an indicator of duration to the Firefox extension\nvisualizations\nSee\nhttps://github.com/jaredlwong/zipkin/commits/redis\nhttps://github.com/twitter/zipkin/commits/master\nOn Thu, Jun 27, 2013 at 4:26 PM, Brian Degenhardt\nnotifications@github.comwrote:\n\nYes, see franklin's request right above this. Rebase to master and the\ntests should pass.\nOn Thu, Jun 27, 2013 at 4:25 PM, Jared Wong notifications@github.comwrote:\n\nYou guys have any idea about the redis errors?\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/pull/253#issuecomment-20161815>\n.\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/pull/253#issuecomment-20161854>\n.\n\n\nJared Wong :: jaredlwong@gmail.com\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/pull/253#issuecomment-20162121\n.\n. nevermind, looks like the redis tests still fail.  Moses, are you around?\n Did these tests run in travis before?\n\nOn Thu, Jun 27, 2013 at 4:39 PM, Brian Degenhardt\nbdegenhardt@twitter.comwrote:\n\nAh right, there's a flaky test, I'm rerunning it.\nOn Thu, Jun 27, 2013 at 4:34 PM, Jared Wong notifications@github.comwrote:\n\nI'm fairly sure that it's rebased to master. Here are the last 5 commits\nfrom my fork\n77db949 Added better support for Redis.\n1c0cab5 Zookeeper Fixes Our recent changes to bind to 0.0.0.0 was causing\nthe scribe collector to publish an incorrect address in zookeeper.\n4052873 Merge branch 'master' of github.com:twitter/zipkin\n40db92f Fixing Build Properties Generation For whatever reason sbt does\nnot\nwant to change the resourceGenerators inside of the BuildProperties\nplugin,\nso I just do it in each project I need it in.\n3eeb8fc Added an indicator of duration to the Firefox extension\nvisualizations\nSee\nhttps://github.com/jaredlwong/zipkin/commits/redis\nhttps://github.com/twitter/zipkin/commits/master\nOn Thu, Jun 27, 2013 at 4:26 PM, Brian Degenhardt\nnotifications@github.comwrote:\n\nYes, see franklin's request right above this. Rebase to master and the\ntests should pass.\nOn Thu, Jun 27, 2013 at 4:25 PM, Jared Wong notifications@github.comwrote:\n\nYou guys have any idea about the redis errors?\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/pull/253#issuecomment-20161815>\n.\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/pull/253#issuecomment-20161854>\n.\n\n\nJared Wong :: jaredlwong@gmail.com\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/pull/253#issuecomment-20162121\n.\n. I'm hoping to give this issue some love this week.  Franklin and I are on\ncall last week and next so our time might be a bit restricted until after\nthe fourth.\n\n\nOn Mon, Jul 1, 2013 at 9:22 AM, Jared Wong notifications@github.com wrote:\n\n@franklinhu https://github.com/franklinhu Have you had the chance to\nlook at this?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/pull/253#issuecomment-20292760\n.\n. http://your.zipkin.hostname.example.com/traces/\n\nOn Fri, Jun 7, 2013 at 3:22 PM, Derek Chiang (Enchi Jiang) \nnotifications@github.com wrote:\n\nIs there a way to get the URL to a specific trace? So, having a trace id,\nhow do I programmatically get the URL to the web page of that trace?\nThanks,\nDerek\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/254\n.\n. One is signed hex and one is unsigned decimal?\n\nOn Sat, Jun 8, 2013 at 10:47 AM, Derek Chiang (Enchi Jiang) \nnotifications@github.com wrote:\n\nThat's what I thought but it didn't appear so. The trace id and the id in\nthe URL appeared different on my machine... What could be the reasons?\nBest Regards,\nDerek Chiang\n(Sent from Android phone)\nOn Jun 7, 2013 3:27 PM, \"Brian Degenhardt\" notifications@github.com\nwrote:\n\nhttp://your.zipkin.hostname.example.com/traces/\nOn Fri, Jun 7, 2013 at 3:22 PM, Derek Chiang (Enchi Jiang) \nnotifications@github.com wrote:\n\nIs there a way to get the URL to a specific trace? So, having a trace\nid,\nhow do I programmatically get the URL to the web page of that trace?\nThanks,\nDerek\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/issues/254>\n.\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/issues/254#issuecomment-19136305>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/254#issuecomment-19152576\n.\n. Sorry, I mean one is unsigned hex and one is signed decimal.\n\nOn Sun, Jun 9, 2013 at 8:21 PM, Brian Degenhardt bdegenhardt@twitter.comwrote:\n\nOne is signed hex and one is unsigned decimal?\nOn Sat, Jun 8, 2013 at 10:47 AM, Derek Chiang (Enchi Jiang) \nnotifications@github.com wrote:\n\nThat's what I thought but it didn't appear so. The trace id and the id in\nthe URL appeared different on my machine... What could be the reasons?\nBest Regards,\nDerek Chiang\n(Sent from Android phone)\nOn Jun 7, 2013 3:27 PM, \"Brian Degenhardt\" notifications@github.com\nwrote:\n\nhttp://your.zipkin.hostname.example.com/traces/\nOn Fri, Jun 7, 2013 at 3:22 PM, Derek Chiang (Enchi Jiang) \nnotifications@github.com wrote:\n\nIs there a way to get the URL to a specific trace? So, having a trace\nid,\nhow do I programmatically get the URL to the web page of that trace?\nThanks,\nDerek\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/issues/254>\n.\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/issues/254#issuecomment-19136305>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/254#issuecomment-19152576\n.\n. Sorry, I mean one is unsigned hex and one is signed decimal.\n\n\nOn Sun, Jun 9, 2013 at 8:21 PM, Brian Degenhardt bdegenhardt@twitter.comwrote:\n\nOne is signed hex and one is unsigned decimal?\nOn Sat, Jun 8, 2013 at 10:47 AM, Derek Chiang (Enchi Jiang) \nnotifications@github.com wrote:\n\nThat's what I thought but it didn't appear so. The trace id and the id in\nthe URL appeared different on my machine... What could be the reasons?\nBest Regards,\nDerek Chiang\n(Sent from Android phone)\nOn Jun 7, 2013 3:27 PM, \"Brian Degenhardt\" notifications@github.com\nwrote:\n\nhttp://your.zipkin.hostname.example.com/traces/\nOn Fri, Jun 7, 2013 at 3:22 PM, Derek Chiang (Enchi Jiang) \nnotifications@github.com wrote:\n\nIs there a way to get the URL to a specific trace? So, having a trace\nid,\nhow do I programmatically get the URL to the web page of that trace?\nThanks,\nDerek\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/issues/254>\n.\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/issues/254#issuecomment-19136305>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/254#issuecomment-19152576\n.\n. We haven't built/tested artifacts on scala 2.10\nThere's a lot of dependencies on artifacts that we rely on that are not\navailable for 2.10.  You're welcome to give it a shot, but the short answer\nfor the question of whether we have them is \"no\"\n\n\nI'm not sure why you'd need it though.  Surely brave can work with the\nstandard 2.9.2 version of zipkin.\nOn Tue, Jun 11, 2013 at 4:26 AM, Andreas Neumann\nnotifications@github.comwrote:\n\nI'd like to use some parts of zipkin to get them to run with brave (\nhttps://github.com/kristofa/brave-zipkin-spancollector. ).\nThus just running zipkin would not be enough.\nThe implementation uses:\n- zipkin-common\n- zipkin-scrooge for which I didn't find an artifact compatible with\n  Scala 2.10.\nI tried checking out the project and changing the Scala Version in the\nbuild.sbt to generate the aritifacts locally. But the Scala Version seems\nto be defined/overridden somewhere else. Also checked the build properties\nfound in the project directory but no trace of a version definition except\nfor the one in the build.sbt .\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/255#issuecomment-19255748\n.\n. We haven't built/tested artifacts on scala 2.10\nThere's a lot of dependencies on artifacts that we rely on that are not\navailable for 2.10.  You're welcome to give it a shot, but the short answer\nfor the question of whether we have them is \"no\"\n\nI'm not sure why you'd need it though.  Surely brave can work with the\nstandard 2.9.2 version of zipkin.\nOn Tue, Jun 11, 2013 at 4:26 AM, Andreas Neumann\nnotifications@github.comwrote:\n\nI'd like to use some parts of zipkin to get them to run with brave (\nhttps://github.com/kristofa/brave-zipkin-spancollector. ).\nThus just running zipkin would not be enough.\nThe implementation uses:\n- zipkin-common\n- zipkin-scrooge for which I didn't find an artifact compatible with\n  Scala 2.10.\nI tried checking out the project and changing the Scala Version in the\nbuild.sbt to generate the aritifacts locally. But the Scala Version seems\nto be defined/overridden somewhere else. Also checked the build properties\nfound in the project directory but no trace of a version definition except\nfor the one in the build.sbt .\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/255#issuecomment-19255748\n.\n. Zipkin depends on twitter.util.eval which is not supported in scala 2.10.  In order to move to 2.10 we need to change the configuration mechanism away from eval.\n. Gah, did the wrong operation.  Gonna squash-merge by hand. :-p\n. Gah, did the wrong operation.  Gonna squash-merge by hand. :-p\n. lgtm - I'll let you sleep on it and if you don't have anything you want to change tomorrow we'll merge.\n. lgtm - I'll let you sleep on it and if you don't have anything you want to change tomorrow we'll merge.\n. Yeah, it can logically be viewed that way, but the implementation is a bit different because there's no way of having span ids influence the scala type.\n\nI created a special zero span that invokes identity when added to any span, and the plus operation just throws an assertion if the spans do not have matching id's.\nMostly this is useful for some scalding jobs I'm working on, but I think a second phase of this will be to go back and simplify some of the span aggregation code in the collector.\n. Yeah, it can logically be viewed that way, but the implementation is a bit different because there's no way of having span ids influence the scala type.\nI created a special zero span that invokes identity when added to any span, and the plus operation just throws an assertion if the spans do not have matching id's.\nMostly this is useful for some scalding jobs I'm working on, but I think a second phase of this will be to go back and simplify some of the span aggregation code in the collector.\n. Turns out we don't really need to do this.  It was a good exercise, but not enough benefit.\n. Works for me, go ahead and merge this.\n. Works for me, go ahead and merge this.\n. Oh, I didn't know you can do this.  Groovy.\n. Oh, I didn't know you can do this.  Groovy.\n. I can't merge it until the tests pass.  Franklin added redis to the travis\nsetup so it should be just a rebase to fix it.  Can somebody do that and\nresubmit?\nOn Thu, Jun 27, 2013 at 9:08 AM, Moses Nakamura notifications@github.comwrote:\n\n@shijinkui https://github.com/shijinkui, @jaredlwonghttps://github.com/jaredlwonghas a PR (#293) out to fix this. It hasn't been merged into master yet, but\nyou can run git pull git@github.com:jaredlwong/zipkin.git redis in a\nbranch in your zipkin repository to update it manually.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/266#issuecomment-20133850\n.\n. I can't merge it until the tests pass.  Franklin added redis to the travis\nsetup so it should be just a rebase to fix it.  Can somebody do that and\nresubmit?\n\nOn Thu, Jun 27, 2013 at 9:08 AM, Moses Nakamura notifications@github.comwrote:\n\n@shijinkui https://github.com/shijinkui, @jaredlwonghttps://github.com/jaredlwonghas a PR (#293) out to fix this. It hasn't been merged into master yet, but\nyou can run git pull git@github.com:jaredlwong/zipkin.git redis in a\nbranch in your zipkin repository to update it manually.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/266#issuecomment-20133850\n.\n. This is fixed now\n. not exactly the new contributor I was looking for but I'll take it :-)\n. not exactly the new contributor I was looking for but I'll take it :-)\n. -1 now the build breaks so you should fix the problems :trollface:\n. -1 now the build breaks so you should fix the problems :trollface:\n. You can!  Send a pull request and I'll merge it!\n\nOn Thu, Jun 27, 2013 at 6:44 PM, \u65f6\u91d1\u9b41 notifications@github.com wrote:\n\n[image: qq20130628-2]https://f.cloud.github.com/assets/648508/719692/107b3514-df94-11e2-8417-a0c3192faccb.png\nin my local branch, i have modify it.\nwho can fix it in master branch ?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/268\n.\n. You can!  Send a pull request and I'll merge it!\n\nOn Thu, Jun 27, 2013 at 6:44 PM, \u65f6\u91d1\u9b41 notifications@github.com wrote:\n\n[image: qq20130628-2]https://f.cloud.github.com/assets/648508/719692/107b3514-df94-11e2-8417-a0c3192faccb.png\nin my local branch, i have modify it.\nwho can fix it in master branch ?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/268\n.\n. Also, how hard is it to add Aggregates?\n. Lastly, please write some unit tests.\n. It'd be nice to get an aggregates store in there too.  I've been going over the code, I'll give you some more review feedback next week.  This looks really good.  We might even try using this at twitter.\n. Yeah, I'm actually fixing up the aggregate store tests anyways, so feel free to wait a bit.\nAlso, I need to to sit down with Chris Trezzo to learn about hbase schema design to make sure it's good.  Really stoked on this contribution though and I'm chomping to get some more time to help out with it.\n. Oh hey, I merged my dependencies work and messed up the interface.  Do you have time to rebase?\n. Oh hey, I merged my dependencies work and messed up the interface.  Do you have time to rebase?\n. Ok, done reviewing.  Lemme know about my nits, but I'm cool with merging it as-is.\n. Ok, done reviewing.  Lemme know about my nits, but I'm cool with merging it as-is.\n. Is this good to merge then from your perspective?\n. Is this good to merge then from your perspective?\n. Some of the unit tests fail.\n. Some of the unit tests fail.\n. Thanks for the recommendation.  We don't have a ton of manpower to satisfy external requests like this, but I'd love to work with you through the process of submitting a pull request to fix the things you see that are broken.\n. Does this incorporate the image links patch recently sent by the guy on HN which moves the images to github CDN?\n. Do we care about ordering of inserts in zipkin?\n. Do we care about ordering of inserts in zipkin?\n. Please rebase and reopen also.\n. Please rebase and reopen also.\n. Strange that this even compiled before.  I wonder where the return call returned to, since there probably wasn't anything up-stack from that.  Perhaps it just evaporated the thread without fulfilling the promise.\n. Strange that this even compiled before.  I wonder where the return call returned to, since there probably wasn't anything up-stack from that.  Perhaps it just evaporated the thread without fulfilling the promise.\n. I'm OK with kicking the migrations can down the road.\n. I'm OK with kicking the migrations can down the road.\n. Do you have the finagle ruby gems included in your environment?\n. Do you have the finagle ruby gems included in your environment?\n. I'm working on a fix.  In the meantime, your zipkin build should not fail\nit should just complain a lot.\n\nOn Mon, Aug 19, 2013 at 11:19 AM, Sam Xiao notifications@github.com wrote:\n\nI guess the maven repo is down and move to somewhere else according to\nMK's comment here, anyone knows where the new location is?\nhttps://groups.google.com/forum/#!topic/travis-ci/zqneGcTEv4g\nIt has been shut down. Since travis-ci.org infrastructure is now\nU.S.-based, a separate\nmirror is no longer necessary.\nMK\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/292#issuecomment-22892747\n.\n. I'm working on a fix.  In the meantime, your zipkin build should not fail\nit should just complain a lot.\n\nOn Mon, Aug 19, 2013 at 11:19 AM, Sam Xiao notifications@github.com wrote:\n\nI guess the maven repo is down and move to somewhere else according to\nMK's comment here, anyone knows where the new location is?\nhttps://groups.google.com/forum/#!topic/travis-ci/zqneGcTEv4g\nIt has been shut down. Since travis-ci.org infrastructure is now\nU.S.-based, a separate\nmirror is no longer necessary.\nMK\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/292#issuecomment-22892747\n.\n. This should be fixed now\n\nOn Mon, Aug 19, 2013 at 11:28 AM, Sam Xiao notifications@github.com wrote:\n\n@bmdhacks https://github.com/bmdhacks Thanks! It does fail because it\ncouldn't access to download any pom files, thus can't get anything build.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/292#issuecomment-22893375\n.\n. Why sad face?\n. Looks like serialization was added to util.Time so I don't need this anymore\n. That's crazy.  Try deleting your ~/.m2 directory.  or at least running sbt\nclean.  Or just trying on a different machine.\n\nOn Sun, Aug 25, 2013 at 4:56 AM, robbinfan notifications@github.com wrote:\n\nWhen I build zipkin, got error like this:\n[error]\n/home/ubuntu/twitter/zipkin/zipkin-common/src/main/scala/com/twitter/zipkin/builder/ZipkinServerBuilder.scala:18:\nStatsReceiver is not a member of com.twitter.finagle.stats\n[error] import com.twitter.finagle.stats.{StatsReceiver,\nOstrichStatsReceiver}\n[error] ^\n[error]\n/home/ubuntu/twitter/zipkin/zipkin-common/src/main/scala/com/twitter/zipkin/builder/ZipkinServerBuilder.scala:19:\nobject tracing is not a member of package com.twitter.finagle\n[error] import com.twitter.finagle.tracing.{Tracer, NullTracer}\n[error] ^\n[error]\n/home/ubuntu/twitter/zipkin/zipkin-common/src/main/scala/com/twitter/zipkin/builder/ZipkinServerBuilder.scala:38:\nnot found: type StatsReceiver\n[error] statsReceiver : StatsReceiver = new OstrichStatsReceiver,\n[error] ^\n[error]\n/home/ubuntu/twitter/zipkin/zipkin-common/src/main/scala/com/twitter/zipkin/builder/ZipkinServerBuilder.scala:38:\nclass file needed by OstrichStatsReceiver is missing.\n[error] reference type StatsReceiverWithCumulativeGauges of package\ncom.twitter.finagle.stats refers to nonexisting symbol.\n[error] statsReceiver : StatsReceiver = new OstrichStatsReceiver,\n[error] ^\n[error]\n/home/ubuntu/twitter/zipkin/zipkin-common/src/main/scala/com/twitter/zipkin/builder/ZipkinServerBuilder.scala:48:\nnot found: type StatsReceiver\n[error] def statsReceiver(s: StatsReceiver) : ZipkinServerBuilder =\ncopy(statsReceiver = s)\n[error] ^\n[error]\n/home/ubuntu/twitter/zipkin/zipkin-common/src/main/scala/com/twitter/zipkin/config/ZipkinConfig.scala:19:\nStatsReceiver is not a member of com.twitter.finagle.stats\n[error] import com.twitter.finagle.stats.{OstrichStatsReceiver,\nStatsReceiver}\n[error] ^\n[error]\n/home/ubuntu/twitter/zipkin/zipkin-common/src/main/scala/com/twitter/zipkin/config/ZipkinConfig.scala:20:\nobject tracing is not a member of package com.twitter.finagle\n[error] import com.twitter.finagle.tracing.{NullTracer, Tracer}\n[error] ^\n[error]\n/home/ubuntu/twitter/zipkin/zipkin-common/src/main/scala/com/twitter/zipkin/config/ZipkinConfig.scala:38:\nnot found: type StatsReceiver\n[error] lazy val statsReceiver: StatsReceiver = new OstrichStatsReceiver\n[error] ^\n[error]\n/home/ubuntu/twitter/zipkin/zipkin-common/src/main/scala/com/twitter/zipkin/config/ZipkinConfig.scala:38:\nclass file needed by OstrichStatsReceiver is missing.\n[error] reference type StatsReceiverWithCumulativeGauges of package\ncom.twitter.finagle.stats refers to nonexisting symbol.\n[error] lazy val statsReceiver: StatsReceiver = new OstrichStatsReceiver\n[error] ^\n[error]\n/home/ubuntu/twitter/zipkin/zipkin-common/src/main/scala/com/twitter/zipkin/query/Trace.scala:18:\nobject tracing is not a member of package com.twitter.finagle\n[error] import com.twitter.finagle.tracing.{Trace => FTrace}\n[error] ^\n[error] 10 errors found\nerror http://zipkin-common/compile:compile Compilation failed\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/295\n.\n. Ship it!\n\nShip It!\n. Thanks for the contribution!\n. It's a pretty big change to work around it, not that I wouldn't welcome a\npatch.  Note that this is in no way related to whether Twitter offers ipv6\nendpoints to our customers.\nOn Tue, Oct 8, 2013 at 3:07 PM, Nikita Vetoshkin\nnotifications@github.comwrote:\n\nIf I'm right host identifier in trace is currently named ipv4 and encoded\nas int32 and used as integer throughout the system (column types, etc).\nThis automaticaly disables ipv6 only hosts from being compatible with\nzipkin. Are there any plans or thoughts how one can add such support or\nmaybe workaround can be found?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/306\n.\n. It's located here: http://maven.twttr.com/org/apache/thrift/libthrift/\nwhich should be in the repositories listed in the build.  I've seen people have trouble building in China because of the great firewall blocks twitter properties, perhaps that's what you're seeing?\n. Ship it!\nShip it!\n. I think that's a known issue.  I haven't had time to look into it.\n\nOn Mon, Oct 28, 2013 at 2:34 AM, Linlin Fu notifications@github.com wrote:\n\nZipkin runs on cassandra, but cannot find traces with any specified\nannotation. The cassandra is setup with\nzipkin-cassandra/src/schema/cassandra-schema.txt. Should any indexes be\ncreated?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/310\n.\n. shipit\n. me too.  shipit.\n\nOn Thu, Jan 30, 2014 at 1:28 PM, Franklin Hu notifications@github.comwrote:\n\nlgtm +1\n\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/pull/331#issuecomment-33735449\n.\n. LGTM\nshipit\n. shipit\n. shipit\n. shipit.\nDid this break travis before or do we not compile it normally?\n. LGTM, but I suppose we'll know for sure when we actually run it in production\n. neat\n\nOn Tue, Mar 4, 2014 at 9:07 AM, Jeff Smick notifications@github.com wrote:\n\nPlay around with the UI and example data: http://zipkin.io\n\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/pull/347#issuecomment-36648213\n.\n. shipit\n. Although once you write your own conversion for the ip addresses now you have to write unit tests for it :-)\n. shipit\n. LGTM but can you show me what this produces sometime at work?\n. Isn't that list going to get really large?  Shouldn't you reduce it into some statistical monoids as you read it?\n. https://github.com/twitter/algebird/blob/develop/algebird-core/src/main/scala/com/twitter/algebird/QTree.scala\n\nOn Mon, Mar 17, 2014 at 5:00 PM, Chang Su notifications@github.com wrote:\n\nYes, you are right. And I'm actually running the job using a monoid to\ncompute the avg. But so far I've not been able to come up with the proper\nmonoid for percentile :(\n\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/pull/353#issuecomment-37885934\n.\n. Does this cause any problems of dropped info from one of the spans, like endpoint or service name data?\n. Wow that's simple.  shipit!\n. Works for me.  Perhaps it's a problem with your network requiring a proxy of some sort?\n. LGTM\n. I think I found a bug:\nhttp://zipkin.io/traces/77e8019056bbae52?serviceName=semper\nClick Expand All twice.  Now try to expand collapse 'vitae'\n\nOn Wed, Apr 23, 2014 at 8:55 AM, Jeff Smick notifications@github.comwrote:\n\nYou can play around with example data in the new UI here:\nhttp://zipkin.io/\n\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/pull/381#issuecomment-41178823\n.\n. I haven't tried the Kafka receiver, does that not work?\n. I haven't tried the Kafka receiver, does that not work?\n. But no, there's no plans at twitter to replace scribe.\n. But no, there's no plans at twitter to replace scribe.\n. Good call, totally slipped my mind.  On it's way...\n\nOn Thu, Mar 14, 2013 at 1:31 PM, Johan Oskarsson\nnotifications@github.comwrote:\n\nIn zipkin-common/src/main/scala/com/twitter/zipkin/common/Endpoint.scala:\n\n@@ -32,6 +35,25 @@ object Endpoint {\n case class Endpoint(ipv4: Int, port: Short, serviceName: String)\n   extends Ordered[Endpoint] {\n-  /*\n-   * Return the java.net.InetSocketAddress which contains host/port\n-   /\n-  def getInetSocketAddress: InetSocketAddress = {\n\nForgot one thing in the last review: would be nice with some quick tests\nfor these new methods too\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/pull/235/files#r3381053\n.\n. Temporary until finagle 6.3.1 is released.  Finagle 6.3.0 pulls in scrooge 2 and causes problems, so I have to wait for 6.3.1\n. The finagle-redis api changed in recent versions which broke a lot of this.  Moses Nakamura emailed me and said he recommended most of the changes to Marius and that he'll be happy to go through zipkin-redis and fix it up after I commit a working build.\n. Scrooge won't compile it in current form... lemme find out why.  It may be that old scrooge just transparently capitalized service names by default.\n. Turns out new-scrooge transparently capitalizes things.  I'm not sure what to do about this.\n. Ok, I'll rebase -i and pull it out.\n. fixed\n. This might be nitpicky, but the scala way\u2122 would be to remove the parens from the function call\n. zipkin has worked on jdk 1.7 for a while now.  This change was done because Isaac was trying to get zipkin web working under ssh port forwarding.  It shouldn't break any backward compatibility because a recent change made all services bind to 0.0.0.0\nDoes this change cause you problems on your machine?\n. cql is a twitter internal tool.  Python is not needed for zipkin.\n. zookeeper is not needed on OSX\n. These are not needed as bin/collector (and all other bin/* scripts run the dev configs)\n. not necessary\n. even this is not necessary as the bin/XXX commands will compile zipkin first\n. no technically installed now, you can just run it out of it's git directory\n. Use /* */ for multiline comments, but my preference would be to remove this to keep the config simple for noobies.  Talking about JDK versions will scare them away and is not necessary for understanding the existing config.\n. I'll clarify that.  m1 is called m1 for parity with Algebird.\n. There is no Time.fromMicroseconds, no.  I'll put up a pull request for util though.\n. The Monoid stuff works really well inside of Scalding.  I'm going to be reusing this structure in the scalding jobs to calculate aggregate dependencies.\n\nFYI, look forward to a monoid definition for Trace and Span as well for the same reason.\n. I couldn't decide.  In practice all the services (Index, Collector, Storage) all get shutdown at the same time, so it doesn't matter.  I suppose moving them all into the collector shutdown at least puts it all in the same place.  I'll make that change.\n. It's just a config file for development.  It's not vital, and this is not the zipkin code.  It's there for noobies to use when they get started.  With that in mind, keeping things as simple as possible is preferable.  I still vote to take it out.\n. The problem is that the stores are not ostrich services, they are finagle clients, and thus cannot be stored in ServiceTracker.  The Index and Storage services are ostrich services, so they can close the respective index and storage handles to cassie, but the aggregates one does not have its own service.  My solution to this was to stuff it in the collector, but it's probably best to just close all the handles in the collector service (which is then tracked in the service tracker).\n. I'd just leave the ssh-port-forwarding detail out.  The vast majority of readers of a README document are first-time users, not people at twitter with massive existing zipkin clusters full of data.\n. Also, there's some implicit sugar you can import from Algebird to get the + operator on classes with implicit monoid conversions, so you can do: Dependency1 + Dependency2, but I chose to use the explicit form for readability.\n. Please remove the todo :-)\n. It might be nice to be able to build a redis-only query daemon, as well as a cassandra-only query without having to build/test the other unused component.\n. do what now?\nhttp://www.youtube.com/watch?v=h06cSTAqAYk\n. good point.\n. No they're not\n. What do you tink about making this a var, so it can be changed?\n. yes\n. com.twitter.util.Time is not serializable :-p\n. The convention for most of these algebird things is to have Class.zero be lowercase\n. How do other extensions do it?\n. I prefer to keep the implicit pollution constrained within the object\n. actually this method is never used\n. fromString is not used anywhere\n. From my reading, this can just be cut out without any horrible problem.\n. Can you remove this file and move it to the collector config\n. This looks structured enough to be a map of case classes, instead of a map of maps.\n. It is preferable to move structure to case classes instead of arbitrary maps of key/values.  If configuration is variable between drivers, it's OK, but otherwise let's try to make this as structured as possible.\n. I don't really mind this change, but I'm just curious about the impetus for it.\n. Isn't the Null bound here redundant to subclassing the Mapping trait?  I can't think of how you could have a non-nullable Mapping.  Or are you just including it here as pseudo-documentation that having Mapper[Null] is ok?\n. This method, while cool, is not used anywhere in the code.\n. Semicolon.\nAlso return is gauche, but I don't mind its use here.\n. You don't need the else here.\nAlso, I'm not super stoked on returning a closed connection.\nCan you research just returning the collection and getting it to auto-close on garbage-collection, thus obviating the need for keepAlive\n. Actually, this method is never called with keepAlive=false, so even if you can't figure out how to close the handle on GC it's OK.\n. I don't think this method is needed since the case class should provide a default constructor already.\n. What part of the code does this comment refer to?  Seems like above, but maybe below.\n. I'd just catch SQLException to be clearer about what's going on here.  Maybe even push the try up into the 'case blob' bit.\n. you don't need to make rowToByteArray implicit.  Cut out the implicit and call get like this:\nscala\n  def bytes(columnName: String): RowParser[Array[Byte]] = {\n    get[Array[Byte]](columnName)(rowToByteArray)\n  }\n. This applies to all usages of this, but it's more idiomatic in scala to supply closure arguments (especially multiline closures) via curly braces:  \nscala\n    Future {\n      result map { case (tId, ts) =>\n        IndexedTraceId(traceId = tId, timestamp = ts)\n      }\n    }\n. Curly braces etc. etc.\n. Why store span_name in the annotation?\nAlso, service_name is technically nullable isn't it?  The Endpoint structure on the Annotation is optional.\n. Same issue here as above. re: span_name and service_name\n. Please no copy/paste.  Bubble this up into zipkin-common if needed.\n. Since this is SQL, I'm more inclined to just do subselects instead of denormalizing the data.\n. Future {\netc... couple other places to get below this.\n. I could make a constant, but it's still hard-coded in the css\n. I couldn't come up with a function that didn't take a jillion arguments.  Each line does same thing, but with subtle differences.\n. not possible to assign to this in javascript.\n. The problem is that d3 sortof caters to a specific style where Node#draw() won't work since it bases things off of selections.\n. Delete the implicit keyword here and it will still work just fine.\n. The ( police is back!  cols_spans.map { col => ...\n. This statement and the one above it.\n.map {\n. and below it\n. and here.  Oh, and this is a superduper nitpick, but Anno means year in spanish and keeps confusing me.  Totally ok if you ignore me.  I'm just typing now for no reason.  Seriously I need top stop this isn't helpful.\n. 1 microsecond = 1000 nanoseconds\nn microseconds  = n_1000 nanoseconds\nTime.fromNanoseconds(n_1000)\n. Although I dislike this conversion.  I submitted fromMicroseconds to util, which should be released now.  Gonna up the artifact versions to pull that in.\n. I'd prefer a more descriptive expansion.  annotation_key, annotation_value isn't that bad.\n. Gratuitous type annotation.  Either annotate the method or the future pool, no need for both.\n. same here\n. etc\n. go find the rest of them\n. Do you nead a future pool for each storage object, or can you define one in an object that is reused.  Gimme the pros/cons of both approaches.  I think one future pool would be easier to tune than many.\n. I just committed to master new deps that include Time.fromMicroseconds\n. TODO: we added CoreAddress annotations in Isaac's PR so whatever gets merged first we probably want to fix the other one.\n. com.twitter.zipkin.util.Util.getArrayFromBuffer has recently been added,\nalthough I just noticed the double-util in the class path which I might fix.\n. UberNit:  don't really need that last idBytes val if you're just gonna turn around and return it.\nYes, I'm trying to look for things to critique.\n. Make this an object for less GC churn.\nscala\nobject Retry {\n  def apply[T](n: Int)(f: => T): T => {\n    ...\n  }\n}\n. Also this is generic enough to chuck into zipkin-common Util\n. [sp]retries exhausted\nAlso might be more readable to use a for comprehension instead of recursion:\n``` scala\n  def applyT(f: => T) : T = {\n    var result:Option[T] = None\n    var throwable:Option[Throwable] = None\nfor (i <- 0 until n if result.isEmpty) {\n  try {\n    result = Option(f)\n  } catch {\n    case e:Throwable => { throwable = Some(e) }\n  }\n}\n\nif (result.isEmpty && throwable.isDefined) {\n  throw new RetriesExhaustedException(\"%d retries exhausted\".format(n), throwable)\n}\n\nresult.get\n\n}\n```\nGot the nitpick meter up to 100% !!!\n. put maxThreads first and pass it in to newFixedThreadPool?\n. Does the query and collector service shutdown cleanly when you hit localhost:9901/shutdown.txt etc?\n. I vote to have a single global thread pool.\n. This needs to be fixed to supply the right args without finatra.\n. How did this even work before?  New mustache version?\n. You don't need to rename this anymore.  I think I just did it because Jerkson had a Generator class.\n. asJavaMap is deprecated\n. Scroll down to the comment that talks aboiut Finatra request.  Needs fixin.\n. scala.collection.mutable is now unused\n. I'm not a huge fan of the side-effecting render operation.  Any way to make it more functional?\n. Doesn't the anorm code already have it's own future pool?\n. Hbase might have one too... I haven't looked.\n. Perhaps a comment on why this is the only operation to modify on a span.\nIt's for pinning right?  /* Used for pinning /\n. Like we discussed, I'm uncomfortable with the similarity between CassandraSpanStore and CassieSpanStore.  The names don't dictate their differing roles.\n. Cassandra does snappy by default now, but there's a legacy issue here where we're double-compressing.  Might be worth looking at removing this.\n. please be more clear about what you mean to fix\n. I'd prefer to call this getAllServiceNames\n. unused import\n. You could probably define a cassie ClusterBase that worked on modern resolvers instead of this jankiness.  ClusterBase isn't that complicated.\n. Please explain in comments that this is necessary due to Cassie api being out of date.\n. make observer private?\n. Do we have a standard terminology for resolver names?  Is \"location\" it?  If not we should have one.\n. Do we want to zipkin trace zipkin?\n. Can you biject into the actual span class instead of the scrooge generated one like we do in the hadoop source?\n. You don't have to implement it in terms of the Bijection library (which is quite small) but I would prefer if the Scheme outputs common.Span instead of gen.Span.\n. Please bump the version.  I say 2.0 but I suck at reading the semver tea leaves.\n. Probably want to bind to 0.0.0.0, not just the public ip.  I agree with announcing the public ip, but bind to the wildcard.\n. This is missing from the Project.scala file\n. I'm ok with all caps.\n. This isn't used anywhere.\n. I have an issue with \"self\" as the name.  Self implies a handle to the class of the given object, and as an external api it's kinda misnamed.  I'd prefer something like apply.\n. The above is so close to Var.observe (which returns a closable).  I don't think the semantics are different enough to warrant a whole new class.\n. Continuing the idea from above, this is so close to Var.observe\n. Why the type change here?\n. Just a suggestion, but you could make this a package object.\n. This is unused\n. It'd be nice to bubble up methods like this into the Trace object, or at least in IndexedTraceId object.\n. The above Monoid does not look correct to me.  I would think you'd use Span.mergeSpan to do plus.  Also, this zero is not commutative.  I wrote a span monoid in a discarded PR, if you want to see:\nhttps://github.com/twitter/zipkin/pull/260\n. If you're going to refactor this, it's begging for a for comprehension\n. Untyped cases are deprecated\n. isn't this just flatMap?\n. What's your plan for this?\n. ",
    "mosesn": "hmm, neat.\n. neat, thanks.\n. I'm pretty sure this guy is the offender.\n. I'm pretty sure that's in this guy ,\nthe one I pointed you to  looks for children, which makes sense in a tree\ncontext, but not in a trace context.  Also, application-index has multiple\nsort orders, such as by duration and timestamp that I don't believe the\nindividual trace page support.\nOn Fri, Aug 10, 2012 at 4:17 PM, Franklin Hu notifications@github.comwrote:\n\nI believe that sorts the query results on the index page. Weren't you\ntalking about on the spans in the visualization on the individual trace\npage?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/108#issuecomment-7655447.\n. I can't repro it either, so I'm going to assume we had a mass hallucination in my office, or chrome now uses a stable sort.  Sweet.\n. In the future, you might want to make many, smaller pull requests instead of just one massive one.  Also, you probably don't want to put all of that commented out stuff in, if you just want to be able to use it later, obliterate it, because it's still in git.  Source control is magic :sparkles:\n. Where will the Aggregates, Index, and Storage interfaces live?  Core?\n. Where will the Aggregates, Index, and Storage interfaces live?  Core?\n. I think jerkson treats options that way automagically.\n\nEdit: yeah, you probably want to leave them as options rather than getOrElse-ing them.\n. I agree.\n. I am only getting one trace back.  I think what is happening is that, because I am only asking about one span, it goes to the \"case head :: nil\" case and then executes the query by itself.  Clearly, head.execute is executing with a limit of one trace id.  It seems like this might not be the correct solution to the problem, because I didn't understand what was going on, but I think this is definitely a problem.  This change fixed my case where I was only getting one trace back.  I invited you to chat on gchat, which might be a more efficient way of figuring this out.\n. This newer fix changes the \"one slice requested\" case to fit the \"many slices requested\" solution.\n. If I want to be able to look at traces which are between 5PM and 7PM today, because there was an incident and I want to investigate it, I can't do that.  If there are 10K traces, and it just gives me 100 uniformly distributed from the past five days, probably very few of them will be in that 2 hour timespan.\n. Why is it better to be on Sonatype OSS than maven.twttr?\n. Neat, good to know.\n. re comments:\ndo you want me to add comments to private methods too, or only non-private?  also, should methods which override methods in traits (which already have comments) also be commented?\n. Thanks for all of your prompt feedback.  I've made changes that address all of the comments you've laid out for me.  Looking forward to more input, thanks!\n. @johanoskarsson I added a redis doc, you can probably yum/apt-get install redis-server if you're on linux and try it out yourself.  Not sure how fully featured you want the redis doc to be.  I have basically no experience writing technical documents except for my own gratification, so I know that I am pretty nooby.  Also, is it in the right place?\nre: comments, I added comments where they seemed useful.  I tended toward scaladoc-style comments rather than // comments because I feel like it's easier for // comments to get separated from the code they're referring to, unless you have something like\nscala\ncode() //comment describing code.\n. They don't require redis-server to be running, they do need redis to be installed on the machine that hosts it, though.  RedisCluster spins up a new instance of redis.  RedisCluster is also the way that finagle-redis tests itself, so it seems like right now it is the standard.\n. Ok, I think it should be good now.  Weird, I thought I had fixed the last of the @param problems already.\n. Which version of java are you using?\n. why do you use a string instead of a long?  I don't think it matters, just curious.\n. This is the same format as what gets sent to the collector, right?  Seems like that will make it easier if you're in dev mode and want to be able to grab the exact trace that just happened on your instance of a service/app.\n. I don't know. I vaguely remember Franklin fixing them, but it could be a\nfalse memory.\nOn Jun 27, 2013 7:40 PM, \"Brian Degenhardt\" notifications@github.com\nwrote:\n\nnevermind, looks like the redis tests still fail. Moses, are you around?\nDid these tests run in travis before?\nOn Thu, Jun 27, 2013 at 4:39 PM, Brian Degenhardt\nbdegenhardt@twitter.comwrote:\n\nAh right, there's a flaky test, I'm rerunning it.\nOn Thu, Jun 27, 2013 at 4:34 PM, Jared Wong notifications@github.comwrote:\n\nI'm fairly sure that it's rebased to master. Here are the last 5\ncommits\nfrom my fork\n77db949 Added better support for Redis.\n1c0cab5 Zookeeper Fixes Our recent changes to bind to 0.0.0.0 was\ncausing\nthe scribe collector to publish an incorrect address in zookeeper.\n4052873 Merge branch 'master' of github.com:twitter/zipkin\n40db92f Fixing Build Properties Generation For whatever reason sbt does\nnot\nwant to change the resourceGenerators inside of the BuildProperties\nplugin,\nso I just do it in each project I need it in.\n3eeb8fc Added an indicator of duration to the Firefox extension\nvisualizations\nSee\nhttps://github.com/jaredlwong/zipkin/commits/redis\nhttps://github.com/twitter/zipkin/commits/master\nOn Thu, Jun 27, 2013 at 4:26 PM, Brian Degenhardt\nnotifications@github.comwrote:\n\nYes, see franklin's request right above this. Rebase to master and\nthe\ntests should pass.\nOn Thu, Jun 27, 2013 at 4:25 PM, Jared Wong notifications@github.comwrote:\n\nYou guys have any idea about the redis errors?\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/pull/253#issuecomment-20161815>\n.\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/pull/253#issuecomment-20161854>\n.\n\n\nJared Wong :: jaredlwong@gmail.com\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/pull/253#issuecomment-20162121>\n.\n\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/pull/253#issuecomment-20162394\n.\n. I don't know. I vaguely remember Franklin fixing them, but it could be a\nfalse memory.\nOn Jun 27, 2013 7:40 PM, \"Brian Degenhardt\" notifications@github.com\nwrote:\nnevermind, looks like the redis tests still fail. Moses, are you around?\nDid these tests run in travis before?\nOn Thu, Jun 27, 2013 at 4:39 PM, Brian Degenhardt\nbdegenhardt@twitter.comwrote:\n\nAh right, there's a flaky test, I'm rerunning it.\nOn Thu, Jun 27, 2013 at 4:34 PM, Jared Wong notifications@github.comwrote:\n\nI'm fairly sure that it's rebased to master. Here are the last 5\ncommits\nfrom my fork\n77db949 Added better support for Redis.\n1c0cab5 Zookeeper Fixes Our recent changes to bind to 0.0.0.0 was\ncausing\nthe scribe collector to publish an incorrect address in zookeeper.\n4052873 Merge branch 'master' of github.com:twitter/zipkin\n40db92f Fixing Build Properties Generation For whatever reason sbt does\nnot\nwant to change the resourceGenerators inside of the BuildProperties\nplugin,\nso I just do it in each project I need it in.\n3eeb8fc Added an indicator of duration to the Firefox extension\nvisualizations\nSee\nhttps://github.com/jaredlwong/zipkin/commits/redis\nhttps://github.com/twitter/zipkin/commits/master\nOn Thu, Jun 27, 2013 at 4:26 PM, Brian Degenhardt\nnotifications@github.comwrote:\n\nYes, see franklin's request right above this. Rebase to master and\nthe\ntests should pass.\nOn Thu, Jun 27, 2013 at 4:25 PM, Jared Wong notifications@github.comwrote:\n\nYou guys have any idea about the redis errors?\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/pull/253#issuecomment-20161815>\n.\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/pull/253#issuecomment-20161854>\n.\n\n\nJared Wong :: jaredlwong@gmail.com\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/pull/253#issuecomment-20162121>\n.\n\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/pull/253#issuecomment-20162394\n.\n. Typically people just clone the repo and run it.  What are the artifacts you're referring to?  You shouldn't really care which version of scala zipkin is running against, because it's a standalone service.\n\nIf you're referring to the client library for Tracing, try taking a look-see at finagle-zipkin, which is built against 2.10.\n. Typically people just clone the repo and run it.  What are the artifacts you're referring to?  You shouldn't really care which version of scala zipkin is running against, because it's a standalone service.\nIf you're referring to the client library for Tracing, try taking a look-see at finagle-zipkin, which is built against 2.10.\n. @daandl if you want to build it yourself, you will have to add scalaVersion as a setting in zipkinSettings in the Project.scala file.\n. @daandl if you want to build it yourself, you will have to add scalaVersion as a setting in zipkinSettings in the Project.scala file.\n. So the set of spans with the same id is one monoid, (identity is the span without annotations), the set of traces with the same id is another monoid (identity is the trace without any spans)?\n. So the set of spans with the same id is one monoid, (identity is the span without annotations), the set of traces with the same id is another monoid (identity is the trace without any spans)?\n. You have an old version of finagle-redis somewhere on your classpath.\n. You have an old version of finagle-redis somewhere on your classpath.\n. Ah this is in master.  I thought someone else had fixed it, I'll hack something together right now.\n. Ah this is in master.  I thought someone else had fixed it, I'll hack something together right now.\n. @shijinkui, @jaredlwong has a PR #253 out to fix this.  It hasn't been merged into master yet, but you can run git pull git@github.com:jaredlwong/zipkin.git redis in a branch in your zipkin repository to update it manually.\n. @shijinkui, @jaredlwong has a PR #253 out to fix this.  It hasn't been merged into master yet, but you can run git pull git@github.com:jaredlwong/zipkin.git redis in a branch in your zipkin repository to update it manually.\n. TracingFilter will register a Tracer with Trace.\n. TracingFilter will register a Tracer with Trace.\n. What is the fancy new autoloading?  I'm just curious.\n. What is the fancy new autoloading?  I'm just curious.\n. The path for storage.anormdb should instead be storage/anormdb.\n. this is pretty neat!  have you found that hbase is fast enough to keep a fairly snappy web ui?\n. this is pretty neat!  have you found that hbase is fast enough to keep a fairly snappy web ui?\n. Future {} runs the task synchronously, but this does it async, so if you're not Await.resulting on every Future, you could have an ordering issue.\n. Future {} runs the task synchronously, but this does it async, so if you're not Await.resulting on every Future, you could have an ordering issue.\n. I think this PR fixes it.\n. I think this PR fixes it.\n. The test that failed on jdk6 is just flaky.\n. The test that failed on jdk6 is just flaky.\n. Good call!  I'm probably wrong then, since effective scala is generally very good advice.  However, I find that having many different exit points for my functions drives me nuts.\n. Good call!  I'm probably wrong then, since effective scala is generally very good advice.  However, I find that having many different exit points for my functions drives me nuts.\n. @adriancole let me try to find someone from Twitter who has more background on this.\n. It looks like mustache.java is on maven central.  Which version of sbt are\nyou using?\nOn Sun, Dec 8, 2013 at 6:03 PM, \u5f20\u5fd7\u6850 notifications@github.com wrote:\n\nI can over the wall, I can access the http://maven.twttr.com/,but I can't\nfind the\nhttp://maven.twttr.com/com/github/spullara/mustache/java/mustache.java/0.8.12...\nand more\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/316#issuecomment-30101247\n.\n. It used to be possible to run zipkin without ZK.  Are we now saying that it's impossible?  Maybe we should provide a built in in-memory ZK for stand-alone clusters?\n. LGTM\n. Absolutely!  We accept pull requests :+1: \nfinagle-redis already supports AUTH, so it shouldn't be too tricky.\n. I suspect that the problem is that far-jarring screws up the META-INF files.  You could try to convince sbt-assembly to concatenate the META-INF files instead of obliterating the extras.\n. with strategy \"first\" is probably wrong.  You probably want to merge with strategy \"concat\" if it's possible.\n. LGTM, but I don't know ruby very well.  @jamescway have you reviewed this code?  Does it look good to you?\n. LGTM, but I don't know ruby very well.  @jamescway have you reviewed this code?  Does it look good to you?\n. Historically, we've found that 100 works fine for us.  What is the problem you've been finding with 100?  And why do you think 10 should be the global default, rather than changing your setting to use 10 instead?\n. My concern is that in big distributed systems, the long tail typically ends up very bad\u2013if you end up in a bad situation 20% of the time in the first system, 20% of the time in the second system, and 20% of the time in the third system, and it's really bad when you see all of them, then you'll see it less than 1% of the time.  Using a sample of 10, it's going to be pretty unlikely that you'll see the problem.  For very small systems, which don't have as deep hierarchies, it's not as bad, but in that kind of situation, it calls into question why we use zipkin at all.\n\nWith that said, as a basic debugging tool, if you're assuming all queries are alike, 10 clearly makes sense.  My concern is that one of the main use cases of zipkin is for surfacing interesting traces, and it's difficult to do that with such a small sample-size.\nI'd be interested in knowing:\nA.  Why doesn't latency scale linearly with the number of items we're fetching?\nB.  What are the main use-cases of zipkin, and which of them should we optimize for?\nC.  How much do zipkin users care about latency?  What is acceptable?\n. If you really want it to scream on mysql, I'd suggest checking out finagle-mysql.  It's pretty basic, but it's implemented from the ground-up to be asynchronous, so it plays more nicely with the existing zipkin abstractions.\n. Yeah, we wouldn't be able to just switch, either\u2013finagle-mysql implements the mysql protocol, so it only supports mysql, unlike anorm, which also supports things like sqlite, so it would have to be a totally separate mysql-only implementation.\n. +1, looks like it was useful for some zipkin maintainer's workflow, but if it's not useful for yours, reinventing it doesn't look that hard (and git will still have it in the future).\n. +1, looks like it was useful for some zipkin maintainer's workflow, but if it's not useful for yours, reinventing it doesn't look that hard (and git will still have it in the future).\n. I like the idea, but does it make sense semantically for zipkin to announce scribe?  Maybe we should let folks provide a name, and then depend on finagle-zk?\n. @olix0r Your new suggestion works for me!\n. +1 for rewriting it in java if you want it to be java-friendly.\n. that style works well for me.  I'm not sure that 10 makes sense though\u2013did you guys discuss what your main use cases are at the workshop?\n. What do you mean majority?  I didn't see anybody express a strong opinion other than me and noslowerdna?\n. Oh, you merged that in?  Why did you do that without discussing it?\n. If you've decided unilaterally to ignore discussion, it seems like it would be polite to say, \"This is a bikeshed, I don't want to discuss it any further\" rather than, \"OK, let's open it up for discussion, does anyone else have something to say\" and then merging without actually discussing it.\nI'm bothered that I thought we were having a discussion about what zipkin is for, and the face it should provide to folks who are new, and I felt like it was signaled to me that it was indeed important (and indeed, someone made a PR about it!) and then there was a total about-face without any real discussion.  The default is important, because when folks try out zipkin, and try to see if it's useful, they'll see the default.\n. @adriancole and I resolved our disagreement off-list, and we agreed that my questions on that PR were out of scope for the change.\n@rtyler I agree that we should be working to make sure contributors feel welcome, and that being able to merge a PR quickly is a great way to get a contributor excited about the project.  At the same time, I'd appreciate it if we're explicit about why we're not going to address a concern.  I would have deeply appreciated the explanation you just provided on PR 443.\n. lgtm, :shipit:\n. finagle's unit tests assume that you've on ipv4\u2013in particular, asking for localhost on ipv6 can give you one of four different results, but it's only one on ipv4.  finagle-memcached also spins up a real memcached, and we haven't gotten around to figure out how to get finagle to speak ipv4 to memcached when it's preferring ipv6.  I think zipkin once had a similar localhost issue, but my guess is that this is no longer the case, if tests pass reliably.\n. @nikhil2406 yoooo this is sweet :+1: \n. It sounds to me like the main concern people have is that the migration story isn't clear for everybody.  I think @eirslett's idea of maintaining the zipkin-scala module, at least for now, is a good idea.  The other way forward would be to get zipkin-java compatible with finagle, check that it meets zipkin-scala performance, and provide a migration guide before killing zipkin-scala.  Although @adriancole points out that the scala-zipkin backend isn't the most resource-intensive process, work has gone into it to make sure that unexpected increases in span write volume doesn't overload our storage backend, which doesn't seem to be there yet in zipkin-java.\nI don't think anyone is ideologically opposed to zipkin-java and it looks solid.  I don't think we should block this on zipkin-java achieving feature parity with zipkin-scala.  But I think it would be nice to see a clearer migration story before we drop zipkin-scala.\n. +1 for moving it to a separate repository as long as we can still use it with mainstream zipkin.  I agree that the dependency on ZK is troubling, and we should fix it.  With that said, as far as I can tell it's the key feature that lets zipkin control its write traffic, which is what scaled it at Twitter, so I think it's worth keeping around in a usable capacity.\n. @adriancole I'll talk to our zipkin team and get them to chime in.. @adriancole let's archive it, if we (Twitter) want to use it we can maintain it.  We're using something similar but not this implementation today.  As a side note, we've updated to 1.28.1.. LGTM\n. Good catch!  LGTM\n. I like this\u2013in practice we run into collisions frequently.  I'm not sure I like increasing the size of the thrift frame.  What would you estimate the overall increase in storage size would be for TBinary and for TCompact?\n. Yeah, I'm beginning to lean toward hi/lo.\n. Good point, done.\n. Done.\n. Moved it all over to the next line\n. pretty much fully tested now\n. can't wait until organize imports for scala-ide can handle effective scala conventions . . . but done.\n. added them.\n. good point, made it a map instead.\n. done.\n. neat, didn't know about that, thanks\n. In general I like the for syntax, but I think you're right that collect makes it cleaner in this case.\n. Yep, completely forgot to clean up this class.  Much nicer now.\n. Oops, I completely forgot how @param works, I'll go back and change all of those tonight.  Good catch.\n. Does Future[Void] even use future chaining?  Also, why is it Future[Void] instead of Future[Unit]?\nedit: ahh, I thought Future[Void] was special, didn't realize it was just a Future wrapped around a null.  Still seems wrong to use Future[Void] instead of Future[Unit].\n. The scala style guide seems to suggest that I should not use braces in this case.  I'm not certain that this will become more clear if I wrap it in curly braces.\n. The Scala style guide suggests omitting braces for if/else expressions, but I can see the argument for\nscala\nif (span.name != \"\")\n  Future.join(\n    for (serviceName <- span.serviceNames.toSeq\n      if serviceName != \"\")\n      yield spanMap.add(serviceName, span.name)\n  )\nelse\n  Future.Unit\nIs that more like what you would like to see?\n. Yeah, you're right.  I didn't really understand Futures when I wrote that.\n. fixed this to use .voided for now, I'll switch it when the trait changes.\n. changed this to look like this\n. thanks, fixed all of these.\n. The \"rule\" is that if it doesn't have side effects, omit the parentheses, if it has side effects, always use parentheses.\nIs this done to migrate to java 1.7?  Is this an intentional breaking of backwards compatibility?\n. getLoopbackAddress is 1.7 only, so it seems like it would break on jdk 1.6.  Is it generally agreed that zipkin is only supposed to work for jdk 1.7?\n. small nitpick but scala-tools doesn't exist anymore\n. If you really don't care, we could shove them into ServiceTracker and be done with it\n. what's the point of this dependency?\n. Maybe this should be a test-only dependency.\n. why is this a no-op?\n. this suffix notation is a little hard to read, I think\n. for short things like this you don't need braces\n. why do you use the raw hbase client instead of asynchbase?\n. do we need slf4j in the project?  or is this just an hbase dependency?\n. prefer NamedThreadPoolFactory, so we can have useful thread dumps.\n. FutureTask + ExecutorService is more for java compat, I think.  Since this is in scala, it should be fine to just wrap the ExecutorService in a FuturePool and use it raw.\n. it seems like we're going to use this so infrequently that it will be fine to just grab the lock.  however, if we start having to lock frequently, it might make sense to just use a concurrentset.\n. I think this is to get around the 0.0.0.0 vs. 127.0.0.1 issue, where sometimes 127.0.0.1 isn't supported, so we just want to rely on java to do it right.  However, I didn't add in the getHostString, so I might be mistaken.\n. You never check that this succeeded, is that ok?\n. I don't understand what this comment means.\n. you aren't checking whether cache.put has succeeded, is that ok?\n. I don't understand why you're doing this.  It seems like you're getting a thread just to block.  Wouldn't it be the same if you just didn't use the futurepool and didn't await?\n. never mind, I thought this was a network call.\n. I can't tell how this ended up being fixed, but I think it's generally a little dirty to use finalizers.  There's probably a better way to do it, maybe by using scala-arm or something like that.\n. wasn't exactly sure what you were getting at--I was hesitant to modify Constants.CoreAnnotations because I wasn't sure why it didn't already have \"ca\" and \"sa\".\n. Is this the new way to use Zipkin?  Do you think we'll be able to go in a direction where you don't need to write any code to deploy zipkin?\n. new AtomicInteger(0) is a little more explicit\n. This might be a little more clear as\nSeq.fill(maxConcurrency)(FuturePool.unboundedPool { loop() })\nWhy do you want to hold references to these though?\n. Ahh, I see, Future.join(workers).\n. I'm not sure if you care about this, there's a small race condition here.  If running is true, it will pass this, but if a close happens in between, and then if running is set to false and the queue is empty, then the workers will die, and the item won't drain properly.\n. If you want, there's a c.t.u.CountDownLatch which uses twitter util classes/idioms.\n. This might be a little simpler as\nscala\ntype Filter = SimpleFilter[Seq[Span], Unit]\n. Convenience\n. Is the premise that each store should retry by itself?\n. should this be a URI instead of a string?\n. line length?\n. Not sure if this would be clearer as Closable.sequence(zkNode.toSeq :+ service: _*)--I think it's a toss-up.\n. I think it's idiomatic for these constants to be lower camel case, no?\n. sort imports?\n. line length?\n. there are a ton of flags here.  I don't think it's worth doing immediately, but can we consolidate them in the future?\n. What is going on here?  I don't understand.\n. why is this named in?\n. where does this number come from?\n. Could you document how you decided on this outlier check?  What do you expect the distribution to look like?  What ratio do you expect that this will catch?\n. It looks like you use the invariant here that apply will be called regularly.  Could you document that?\nWhat makes this different from an exponential moving average?  Could you just use that instead?\n. How did you choose 0.9?\n. I had trouble figuring out what bufferLoc meant--could you call it bufferIdx, curIdx, headIdx, head or similar instead?\n. Could we call this a RingBuffer?\n. break this up\n. It's worth noting that name is restricted by the HttpMux DSL\n. rateVar.changes.register(Witness(rate))\n. (curRate == 1) || {\n...\n}\nmight be a little more straight forward, it's a toss-up.\n. lower camel case?\n. Having something scoped to the same name as a counter seems messy . . . like giving a file and a directory the same name.  Can you change one?\n. prefer Future.Done\n. Future.Done\n. closeAwaitably?\n. closeAwaitably?\n. then do we need flagOfURI anymore?  what is it used for?\n. Scala style guide says for classes, vals should be lower camel--I can't find any examples of this style (upper camel) except on objects.\nMost of your vals in classes follow lower camel.  Is there something I'm missing that makes these vals special?\n. I wonder if you can scrap force users to use your suffixes (/election, /requestRate etc) and just have them specify a base, and then replace windowSize, threshold etc with precision and accuracy metrics.  Not a fully baked thought, might not be possible with the distribution we're expecting.\n. Ahh, could you add a comment?\n. It actually looks like there's a RingBuffer in c.t.u, can you use that one?\n. +1 2.0\n. Iterators are not Seqs, you should be safe.\n. The OG Try is c.t.u.Try.\n. client.auth is asynchronous, so this isn't in sequence.  flatMap is the sequence operator for Futures.  Here's one possible way of implementing this:\nscala\nval authed = authPassword.map(p => client.auth(StringToChannelBuffer(p))) getOrElse Future.Done\nAwait.result(authed before Future.value(new RedisIndex {\n  val database = client\n  val ttl = Some(self.ttl)\n}), 10.seconds)\n. @sprsquish is zipkin following semver?\n. yes, this change is rad\n. maybe say queryLimit() instead of queryLimit.apply()?\n. ",
    "suryatech": "+1\n. +1\n. ",
    "glynd": "Hopefully that solves it - still not used to these new fangled source control systems ;)\n. ",
    "gavri": "Closed because this https://github.com/twitter/zipkin/pull/106 covers it\n. ",
    "jerryli9876": "Yeah I apologize for the massive pull request; I didn't even realize that I hadn't made so many commits.\n. The idea was that we'd populate this once at the beginning of postprocessing, then we'd never edit it again. That way we'd save some time (instead of populating it every time we postprocess some job) and we'd also make sure that the standardized names are consistent throughout all the jobs, once we start using standardized names.\n. I made that change because I noticed that every time you want to process a line, you need to break it down into tokens so you can figure out what is the key and what is the value anyways\n. I honestly can't figure out what the diff here is. The version in my repo is exactly the same as the one in master.\n. The problem is that I need to set width of the container to some larger number than usual (otherwise the dependency graph looks horrible) and if I include it in the localResources the width of the other pages is also set to something large.\n. That's good to know. For this page, both are equivalent though, at least, I believe.\n. What's the preferred format?\n. AFAIK, the only way to do that is to retain the original span throughout all the processing, and that's already super slow as it is.\n. Config expects an apply() method without any parameters.\n. ",
    "GauravNaik": "What i meant was in the Downloads section \"zipkin-test-0.1.0.zip\"  leads to a file not found error . \n. What i meant was in the Downloads section \"zipkin-test-0.1.0.zip\"  leads to a file not found error . \n. No particular reason for the zip . I was looking to try out Zipkin . But i am not able to figure out some things . So was looking out for some simple java examples where i could get the feel of things. \n. ",
    "rtyler": "@mosesn if this is still important to you, would you mind elaborating on what you mean? What would the ideal acceptance criteria be?\n. @adriancole sounds good to me, can you get twitter to just point zipkin.io at GitHub's nameservers for CNAME purposes?\n. I've moved this ticket along with the source code, see openzipkin/browser-extension#1\n. @robbinfan With the new Gradle-based build system that #503 introduced, I believe this issue has been fixed. Will you re-open it if that's not the case?\n. @AllienPhone I'm not seeing this behavior, can you please confirm it's still broken for you?\nAlso, what is the command that is being executed originally?\n. With the merge of #503, the only potential way that I think this would be a problem is if a user were on a network that was blocking the maven repo. I'm going to close this out.\nPlease re-open if you can reproduce this error on HEAD of master\n. Closing this as a duplicate\n. With a clean machine, located in the US, I had no problem executing bin/collector locally with today's HEAD of master.\n@yuchen99 which SHA1 are you seeing troubles with?\n. @pje FWIW we've been cutting some forked versions to rubygems for a while using lookout-zipkin-tracer\n@jamescway is working on getting some of our changes incorporated into the openzipkin repos\n. @pje FWIW we've been cutting some forked versions to rubygems for a while using lookout-zipkin-tracer\n@jamescway is working on getting some of our changes incorporated into the openzipkin repos\n. I believe this overlaps with some work that @jamescway did, which is getting superseded by @michaelsembwever's Datastax work\n. Hopefully some of @jamescway's work for #457 will help alleviate this, but I think this is a separate task unto itself\n. I think @abesto's work moving us over to Gradle in  #503 has helped tremendously. I'm going to close this now\n. :+1: \n. @adriancole LGTM, I'll create a new repo for the gem elsewhere\n. @adriancole LGTM, I'll create a new repo for the gem elsewhere\n. :+1: \n. :+1: \n. @adriancole tomorrow let's move this, you will need admin privs to the twitter/zipkin repo and owner privs to openzipkin org (which you already have)\nOnce that's ready, I can walk you through it so we don't lose anytihng\n. @3thinkthendoit is this not a bug? I labelled it as such because the original title says that the sampleRote doesn't work\n. Travis is not yet green on master, so I'm just going to merge this with reckless abandon\n. @eirslett you're killin' me! mind just filing an issue on that repo to do that? :P\n. Looks like @adriancole has fixed a good chunk of things thus far, woohoo!\n. @mosesn I threw a :+1: in on what looked like a pull request that had been stalled, but was completely merge-able..\nThe questions raised in the pull request:\n\nA. Why doesn't latency scale linearly with the number of items we're fetching?\nB. What are the main use-cases of zipkin, and which of them should we optimize for?\nC. How much do zipkin users care about latency? What is acceptable?\n\nI think these are great questions to ask and discuss in terms of overall direction for Zipkin, but I do not see them as show-stopper questions for merging a very minor pull request. From the outside-looking-in on a number of projects, it's always encouraging to see my pull requests merged, even if they do spawn subsequent work (like that captured in this ticket). IMO this is important to foster a good community of contributors.\nI think this issue is a good example of how to handle things like this. It was a clearly minor change but as has been raised, defaults are important (of course, until this issue is resolved it's not a default so much as a constant). That's not something a casual contributor should be deadlocked in, otherwise the project likely will lose them forever.\nPersonally, I think the best way to answer those questions (A, B, C) is to get releases built and start getting feedback from other users which have not traditionally participated in the project\n. @mosesn duly noted \n\n. I'm generally in favor of this. For my own understanding, what overlap does this have with brave?\n. shrug\n. @adriancole how did you discern between builder issues and #440? :)\n. Well, the build doesn't look worse so that's something? #440 is still alive and well, these builds failed with:\n[warn]     ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]     ::              FAILED DOWNLOADS            ::\n[warn]     :: ^ see resolution messages for details  ^ ::\n[warn]     ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]     :: io.netty#netty;3.10.1.Final!netty.jar(bundle)\n[warn]     ::::::::::::::::::::::::::::::::::::::::::::::\n:crying_cat_face: \n. @travisbrown I don't think I understand the pattern that links our build issues with the ones you link. We're not seeing JVMs crater, we're just seeing inconsistencies with the ability of a Travis builder to fetch the same dependency the N times the sbt build tries to grab it.\n. an timid :+1: \nI think @eirslett should sign off and merge this if he's good with it\n. \n. @eirslett Travis is running the build on a Linux box, is that not good enough? :)\n. @adriancole I'm going to merge this so we can get more success with other PRs, mind opening another PR for the verbose GC bits?\n. that's some split!\n. Ah yes, MVZ\n\nAnyways, I think this is a reasonable step. What I would suggest also thinking about is what the criteria should be for something to be \"core\" versus not core. Cassandra/scribe are core more because of momentum it seems, should that always be the case? Is this the Zipkin Top Ten Most Wanted modules?\n. Should we close this ticket?\n. Should we close this ticket?\n. Oh whoops, I forgot to :+1: this yesterday when I looked at it\n. See #514\n. This probably isn't going to work now that #503 has been merged <_<\n. This is relevant to my interests :)\n. Did a cursory review, I might be misunderstanding some things so don't take my comments as blocking or anything like that\n. I'm going to merge this since the build failures are @adriancole's fault and this change looks like it will dramatically improve everybody's lives :)\n. one suggestion but mostly looks fine to me\n. \n. @abesto it's worth mentioning that dependencies in the shadow jar are unzipped, so there's a whole lot of class files running around. I'm assuming that you've already run this code to make sure that it loads the application correctly?\nOther than that concern, :+1: \n. :+1: \n. :+1: \n. See also Working Effectively With Legacy Code <_<\n. :+1: \n. :+1: :trollface: \n. :+1: :trollface: \n. Is signing happening on bintray's side or in Travis?\n. @abesto is there a downside to disabling signing in general?\n. :+1: \n. Are these printlns going to stay in here forever? Any reason they can't go away?\n. why not put that comment into the code itself for posterity :)\n. Generally I try to define things like this, where possible, in gradle.properties that way they can be more easily overwritten on the command line with -P if need be.\nI understand that this doesn't quite work when these attributes are invoking some logic but something to take under consideration\n. The order of these declarations will be the order in which servers are queried for a dependency. Considering how freakin' unreliable maven.twttr.com has been, perhaps move it to the bottom so it's called as little as possible :P\n. Shouldn't this be declared as a dependency in the zipkin-cassandra/build.grade then?\n. Shouldn't this be declared as a dependency in the zipkin-receiver-kafka/build.grade then?\n. This task should have a description\n. I'm not sure why this is necessary, .apply pluginhas been idempotent in my experience. The way that this code is being called is from asubprojects {}` which means the apply call will work on each new (sub)project evaluated.\nAFAICT this should be safe to nuke\n. Not sure I understand why this list needs to be declared here instead of just as enumerated dependencies below\n. This buildscript {} block is the example same as the one in zipkin-scrooge but this one applies the 'idl' plugin, and the other applies the 'scrooge' plugin.\nAre both plugins defined in this griddle dep?\n. Not a technical downside as far as I'm aware, but at least as far as Gradle 2.x is concerned it's definitely not a conventional means of applying plugins which has the potential to confuse those less than familiar with Gradle\n. No, I was more confused because most Gralde plugins that I've seen define one plugin per dependency/jar\n. I suggest changing this to rootProject.buildDir that way the generated DB file will be placed in build/ and naturally ignored by git and cleaned up by ./gradlew clean\n. ",
    "mauhiz": "So, has anything been published yet?\n. So, has anything been published yet?\n. sorry - even like this it does not fully work. Will submit a PR again when I got everything working.\n. sorry - even like this it does not fully work. Will submit a PR again when I got everything working.\n. ",
    "jpinner": "@caniszczyk seems like the last release was end of Aug, 2013 -- any chance of cutting a new release and publishing it to sonatype?\n. @caniszczyk seems like the last release was end of Aug, 2013 -- any chance of cutting a new release and publishing it to sonatype?\n. any updates?\n. Some of the traits can't be instantiated in Java even if you create abstract classes for them in Scala because of how they mix in closable.\nSent from my iPhone\n\nOn Jul 10, 2015, at 3:50 AM, Eirik Sletteberg notifications@github.com wrote:\nAre these the kinds of APIs we're talking about?\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-common/src/main/scala/com/twitter/zipkin/storage/SpanStore.scala\nGiven this not-Java-friendly API:\ntrait MyStore\nclass MyStoreImpl\nOne easy way to provide Java friendliness for traits, is to make abstract classes for them:\ntrait MyStore\nabstract class AbstractMyStore extends MyStore\nAnd then use that from Java:\nclass MyStoreImpl extends AbstractMyStore {}\nTwitter already does this in a couple of places, e.g. AbstractFunction.\nIn these cases it could be enough to just provide stub abstract classes for Java developers to implement.\nOne thing I think could be particularly valuable, is to rip out the Zipkin stuff from Finagle, rewrite it more or less exactly from Scala to Java, and then use that code from both Finagle, Brave, and other places. (Basically, a reference implementation without external dependencies)\n\u2014\nReply to this email directly or view it on GitHub.\n. @eirslett earlier you commented that: 'Collector/query/web are 3 projects that are written in Scala anyways (using the Scala ecosystem and maintained by Scala developers) so that's not a problem at all.' and @kristofa concurred\n\nWe've found it impossible to write pure-java extensions to these projects -- for example we wanted to write a splunk-based span store using the splunk java sdk and could not do it without writing scala.\nWe (lyft) would prefer the ability to extend the backend components as necessary without having to introduce the scala language / compiler. Simplify providing these components as standalone jars does not allow for this (with the current use of traits as interfaces). \n. ",
    "smparkes": "@jpinner Yes. We're working on scheduling it now.\n. @jpinner Yes. We're working on scheduling it now.\n. ",
    "adriancole": "I think the idea is that openzipkin will drive artifact publication @rtyler can invite you to the org as well!\n. The https://github.com/openzipkin org includes a zipkin-core team which has jpinner in it now too. The plan is to publish artifacts to https://bintray.com/openzipkin/maven after merging some in-flight sbt-bintray  work Lookout built.\n. Just to touch this again.\nInside Twitter, maven central is the preferred repo, and our pants 3rdparty will only look at that and https://maven.twttr.com/\nI can manually push to the latter until central sync is supported. That would be ideally something that's already published to bintray. Once something is published somewhere, I can move twitter onto openzipkin, which would be very sweet.\n. @rtyler @eirslett @michaelsembwever @kristofa @jamescway (think this is openzipkin founding parents)...\nSo, are you thinking com.github.openzipkin:zipkin-whatev?\nOnce we decide on group id, I can start the sonatype ticket.\n. Twitter donated zipkin.io to us. @aasta has access to the godaddy stuff. I suspect we could use this for group id, etc? cc @eirslett @rtyler \n. @rtyler I just added you to have manage products ability on godaddy, so\nlemme know if that didn't work.\n. https://issues.sonatype.org/browse/OSSRH-16669\nI'd like a backup.. @eirslett @michaelsembwever @abesto @kristofa @aasta .. can one or more of you register on https://issues.sonatype.org and add a comment ^^ with your username?\n. soo.. we did this, and it didn't even take 3 years!\nhttps://repo1.maven.org/maven2/io/zipkin/zipkin-common/1.2.1/\n. Depends on what you mean..\nOrdering the spans that are returned is pretty commodity and doesnt need to\nbe done server side. Hidden form etc (or query which only applies to the\nUI) are low hanging fruit.\nChanging the actual query is overkill for the problem and would be\nexpensive to implement\n. Depends on what you mean..\nOrdering the spans that are returned is pretty commodity and doesnt need to\nbe done server side. Hidden form etc (or query which only applies to the\nUI) are low hanging fruit.\nChanging the actual query is overkill for the problem and would be\nexpensive to implement\n. feel free to open at some point. This looks helpful, but needs a rebase\n. feel free to open at some point. This looks helpful, but needs a rebase\n. This, issue came up again in armeria from @trustin. I wonder if we want to consider a tactical workaround or not?\nhttps://github.com/line/armeria/pull/195#discussion_r69403649\nRight now, there's no special indexing in zipkin for the ip address anyway, so it is more or less how we handle the union concern of ipv4, ipv6 or I don't know. This would permeate all sorts of tracers, who probably already came onto this problem.\nFor example, I know opentracing expresses the ipv6 of a peer as a \"tag\". However, whatever we come up with needs to be able to work for both the local endpoint (the one making annotations), and also the address binary annotations (which designate client vs server)\ncc @kristofa @eirslett @abesto @yurishkuro @jcarres-mdsol @basvanbeek for ideas.\n. This, issue came up again in armeria from @trustin. I wonder if we want to consider a tactical workaround or not?\nhttps://github.com/line/armeria/pull/195#discussion_r69403649\nRight now, there's no special indexing in zipkin for the ip address anyway, so it is more or less how we handle the union concern of ipv4, ipv6 or I don't know. This would permeate all sorts of tracers, who probably already came onto this problem.\nFor example, I know opentracing expresses the ipv6 of a peer as a \"tag\". However, whatever we come up with needs to be able to work for both the local endpoint (the one making annotations), and also the address binary annotations (which designate client vs server)\ncc @kristofa @eirslett @abesto @yurishkuro @jcarres-mdsol @basvanbeek for ideas.\n. Thx for the feedback!\nat least in thrift I don't think we'll be able to effectively deprecate\nipv4. This is used a lot, and will be hard to ever remove (in this model)\nWe could probably support ipv6 as a peer field with not much impact. For\nexample, someone may have both types of addresses to log, and if they don't\nhave ipv4, they could log 0 (which I've noticed some do anyway). 128 bits\nare annoying as you can't stuff them into a number, we'd have to use bytes\nin the thrift field (even if we do fully spelled out in json).\nwe could do the either/or (union type), just it is a little trickier for\nimplementors than a separate ipv6 field, and requires us to do some\nprioritization if someone set both ipv4 and ip of type ipv4, if that makes\nsense.\n. ps by \"ipv4 .. This is used a lot\" I don't mean zipkin uses it, rather\nthere's a lot of instrumentation that use it. Easier to add than\nremove.\n. We can't change the type of ipv4, as that would be incompatible with thrift\ndef. Adding a new string field to hold the ipv4 value would add confusion\nand also bloat the field.\nhttps://github.com/openzipkin/zipkin-api/blob/master/thrift/zipkinCore.thrift#L249\nWe can have flexibility of adding ipv6, to the thrift, but it seems\nconfusing to have a different encoding process than what's used for ipv4.\n(remember thrifts are binary anyway. people don't look at them)\nKeep in mind that choice affects the highest count of things in zipkin:\nannotation and binary annotations. We have to be conscious about\nefficiency, knowing that using string representation of ipv6 is up to 3x\nthe size as binary.\nThat said, remember that in json we use stringified fields anyway. Most of\nthis talk is about the thrift encoding, which is the most sensitive one.\nTL;DR; is since there are only two types of ip addresses, and we already\nsupport the former in native form, and can't change it without adding\nconfusion, the simplest and most consistent way to implement ipv6 would be\nto add a field in its binary form.\nThis keeps this sort of change low-hanging, least confusion and bears in\nmind that our next model may not even have an Endpoint type! (ex if it is\nsingle-host spans, we can simply make things like this tags)\n. ps here's a link about model problems, and behind my desire for this issue to result in a tactical change vs an incompatible one. It is choosing battles..\nEx I have time to help with a tactical change like this, and prefer to punt incompatible changes to something that packs more punch  https://github.com/openzipkin/zipkin/issues/939\nI'm thinking about thrift implications (cassandra), json implications (elasticsearch), and schema implications (another column in mysql), as well how to make this a \"add this here\" step for instrumentors. It is a lot of steps already, and adding something incompatible or inconsistent makes it less inviting for someone to jump in and do.\n. so here's a summary of compatible options that I think are possible and relatively equivalent work unless called out otherwise. I'm ok with either, actually, and happy to have folks vote for whatever they prefer and implement accordingly.\nAdd Endpoint.address[String]\nThis would hold an opaque value and deprecates ipv4. We could say that this overrides any value held in ipv4, and perhaps backfill it at query time (as we do with Span.timestamp).\nThrift\nWe'd mark ipv4 deprecated and add the following:\nthrift\nstruct Endpoint {\n--snip--\n /**\n  * Holds the text representation of a IPv4 or IPv6 address associated with this endpoint\n  * \n  * IPv4 address: Ex. \"1.2.3.4\" per RFC 2673, section 3.2.\n  * IPv6 address: Ex. \"2001:0db8:85a3:0000:0000:8a2e:0370:7334\" per RFC 2373, section 2.2.\n  */\n  4: optional string address\nJson\nI'm not aware of a json-schema type that describes both ipv4 and ipv6.\nAdd Endpoint.ipv6[bytes]\nThis would be optional and hold the 128 bits for the ipv6 address and doesn't affect ipv4.\nThrift\nWe'd the following:\nthrift\nstruct Endpoint {\n--snip--\n /**\n  * IPv6 host address packed into 16 bytes. Ex Inet6Address.getBytes()\n  */\n  4: optional binary ipv6\nJson\nUses text format, just like the \"ipv4\" currently does. Document \"ipv6\" as the the json schema type of the same name.\nPros and Cons\naddress[String] will require a larger thrift field and row, although in both cases it is bounded by largest text form of an IP (50bytes). Some may object to the rows * 50 bytes enlarging of annotation/binary annotation namespace, and can choose not to store this.\nipv6[bytes] is separate from ipv4, which means an endpoint could have both an ipv6 and ipv4 association. It is more consistent in approach, but the current approach of ipv4 in thrift has proven annoying to some. This choice still needs a column even if it only needs to be 16bytes long.\n. @mosesn if you don't mind.. could you give a \"twitter answer\" on this topic?\n. in json, I'm not suggesting binary, rather the normal string encoding (like\nwe do for ipv4)\n. Thinking out loud here...\na related change we could do is leniently parse ipv4 in thrift, defaulting to 0.0.0.0. For example, I've noticed people stuff '0' when they don't have an ipv4. We couldn't return null from Endpoint.ipv4, as that would break people, but we could default it to zero and document the practice. This would keep those with ipv6 endpoints a little smaller as they don't have to encode a fake ipv4 address.\ncc @liyichao @trustin on this one\n. Thanks for the feedback!\n. here's the impl. when merged I can update zipkin-api's repo (to update the thrift and json model definition) https://github.com/openzipkin/zipkin/pull/1178\n. here's the api definition change https://github.com/openzipkin/zipkin-api/pull/20\n. closed via #442\n. old issue, but fyi elasticsearch is being implemented here: https://github.com/openzipkin/zipkin-java/pull/107\n. old issue, but fyi elasticsearch is being implemented here: https://github.com/openzipkin/zipkin-java/pull/107\n. @wadey what's your current thinking on this? @eirslett etc?\nI recognize this is over 1.5years old. should we pull this in or close it?\n(closing in 3 days unless I hear back)\n. @wadey what's your current thinking on this? @eirslett etc?\nI recognize this is over 1.5years old. should we pull this in or close it?\n(closing in 3 days unless I hear back)\n. ack. thanks for the response!\n. ack. thanks for the response!\n. Closing this issue a we are removing H2 from the default list. Since we don't run tests or allow docker configuration, H2 may already be broken without us knowing: #853\nIn the near future, we will release an alternative to anorm, jOOQ, in the java project. If there's enough interest to contribute docs and tests for other databases, we can always pick this back up.\n. @eirslett wdyt about this one?\n. oops. this patch no longer applies. @synk you mind rebasing it?\n. closing as we want to keep the pull request queue fresh and relevant. Please ping if you get around to rebasing etc!\n. Closing this issue a we are removing PostgreSQL from the default list. Since we don't run tests or allow docker configuration, PostgreSQL may already be broken without us knowing: #853\nIn the near future, we will release an alternative to anorm, jOOQ, in the java project. If there's enough interest to contribute docs and tests for other databases, we can always pick this back up.\n. @ggsonigg you'll have better luck raising such a question on the Brave repo, as that's the repo which would have a change (about how to use Brave). When you do, please be specific about which README files you read and found they aren't instructive enough. It is hard to guess what type of docs people want. https://github.com/openzipkin/brave/issues\n. @ggsonigg you'll have better luck raising such a question on the Brave repo, as that's the repo which would have a change (about how to use Brave). When you do, please be specific about which README files you read and found they aren't instructive enough. It is hard to guess what type of docs people want. https://github.com/openzipkin/brave/issues\n. This seems to be uncontested, and accidentally overlooked. I'll see if this is mergable still or not.\n. This seems to be uncontested, and accidentally overlooked. I'll see if this is mergable still or not.\n. @maz well.. better late then never? :) closing via 3875f2a7eb6cc7e2baa6568196e1f34706127805\n. @maz well.. better late then never? :) closing via 3875f2a7eb6cc7e2baa6568196e1f34706127805\n. > Do you have a version with mongodb? Thanks\nonce there was but it was removed years ago due to no maintenance. the code\nfrom back then is irrelevant now because the model has changed also\nlanguage and frameworks.\n. This seems testable. mind adding one?\n. This seems testable. mind adding one?\n. Our existing test didn't catch this as span5's minimum timestamp and maximum duration happened to also be the trace's minimum timestamp and maximum duration. I'm correcting the test to not hide problems like this.\nhttps://github.com/twitter/zipkin/blob/master/zipkin-common/src/main/scala/com/twitter/zipkin/storage/util/SpanStoreValidator.scala#L237\n. Our existing test didn't catch this as span5's minimum timestamp and maximum duration happened to also be the trace's minimum timestamp and maximum duration. I'm correcting the test to not hide problems like this.\nhttps://github.com/twitter/zipkin/blob/master/zipkin-common/src/main/scala/com/twitter/zipkin/storage/util/SpanStoreValidator.scala#L237\n. ok I have a test coming and will also merge this.\n. ok I have a test coming and will also merge this.\n. added tests via 41a6b665c779fe67f72a9af47b8ade452c5c22c5 closed via e6e90b2930572d3c7b5363b935925ac36c0bbd82\n. added tests via 41a6b665c779fe67f72a9af47b8ade452c5c22c5 closed via e6e90b2930572d3c7b5363b935925ac36c0bbd82\n. please verify. I think this is fixed now\n. please verify. I think this is fixed now\n. apologies, but closing this for a couple reasons.\n- the originating repository no longer exists\n- there's a lot of drift that makes this conflict greatly\nFeel free to submit again, and thanks tons for giving this a try!\n. apologies, but closing this for a couple reasons.\n- the originating repository no longer exists\n- there's a lot of drift that makes this conflict greatly\nFeel free to submit again, and thanks tons for giving this a try!\n. sorry.. meant to credit you, but this commit was accidentally squashed into the one I made to fix the hbase build. Thanks all the same! 658f972d1f1389b52facc228c808a3844ef75ec1\n. sorry.. meant to credit you, but this commit was accidentally squashed into the one I made to fix the hbase build. Thanks all the same! 658f972d1f1389b52facc228c808a3844ef75ec1\n. @eirslett you cool with this?\n. @eirslett you cool with this?\n. okie dokie.. on the way!\n. okie dokie.. on the way!\n. Thanks for the help @synk! bcac2a322e0305c1fb43a75ebac8007337fdbd01\n. Thanks for the help @synk! bcac2a322e0305c1fb43a75ebac8007337fdbd01\n. If you can rebase this, happy to help merge it in!\n. If you can rebase this, happy to help merge it in!\n. I'm going to try and get this merged, rebasing etc..\n. I'm going to try and get this merged, rebasing etc..\n. closed via 13391db08f0e8ae581dcb86b80a629da3716959c\nThanks for the great work here.\n. closed via 13391db08f0e8ae581dcb86b80a629da3716959c\nThanks for the great work here.\n. raising internally, but LGTM\n. raising internally, but LGTM\n. ps you get 1000 \"great job\"s for adding a new spec file\n. ps you get 1000 \"great job\"s for adding a new spec file\n. looks good. I ran the following to test and worked fine. Will bubble upstream in a bit.\nbash\nrvm --default 1.9.3\nbundle install\nbundle exec rspec spec\n. looks good. I ran the following to test and worked fine. Will bubble upstream in a bit.\nbash\nrvm --default 1.9.3\nbundle install\nbundle exec rspec spec\n. all in!\n. all in!\n. via eaecda241744edfd3cef04c799ce8f050477ca2e\n. via eaecda241744edfd3cef04c799ce8f050477ca2e\n. I checked internally and we aren't using this script, so I'm just going to merge this!\n. I checked internally and we aren't using this script, so I'm just going to merge this!\n. sorry about attribution, but everybody knows this was yours :)\n09a35015cff6b10a577d1d610f173c2c74266da7\n. sorry about attribution, but everybody knows this was yours :)\n09a35015cff6b10a577d1d610f173c2c74266da7\n. openjdk 7 build unfortunately still gets killed, but that was also the case prior https://travis-ci.org/twitter/zipkin/jobs/68721928\nbash\n0.65s$ ./bin/sbt ++$TRAVIS_SCALA_VERSION clean test\ndownloading sbt-launch.jar\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 1188k  100 1188k    0     0  7026k      0 --:--:-- --:--:-- --:--:-- 20.3M\n*** buffer overflow detected ***: java terminated\n. openjdk 7 build unfortunately still gets killed, but that was also the case prior https://travis-ci.org/twitter/zipkin/jobs/68721928\nbash\n0.65s$ ./bin/sbt ++$TRAVIS_SCALA_VERSION clean test\ndownloading sbt-launch.jar\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 1188k  100 1188k    0     0  7026k      0 --:--:-- --:--:-- --:--:-- 20.3M\n*** buffer overflow detected ***: java terminated\n. mental note check to see if it is indeed pulled internally\n. mental note check to see if it is indeed pulled internally\n. good news and bad news... good news is that OSS is synced to our internal stuff! bad news is that the build is broke, most likely due to us not using sbt to build. carrot to the one who fixes first!\n. good news and bad news... good news is that OSS is synced to our internal stuff! bad news is that the build is broke, most likely due to us not using sbt to build. carrot to the one who fixes first!\n. don't mean to be bikesheddy, but do you strongly prefer /bin/bash vs /usr/bin/env bash?\nhttp://stackoverflow.com/questions/21612980/why-is-usr-bin-env-bash-superior-to-bin-bash\n. don't mean to be bikesheddy, but do you strongly prefer /bin/bash vs /usr/bin/env bash?\nhttp://stackoverflow.com/questions/21612980/why-is-usr-bin-env-bash-superior-to-bin-bash\n. Thanks! can you rebase this and squash into a single commit? I'll merge after\n. Thanks! can you rebase this and squash into a single commit? I'll merge after\n. thx.. will get this through now!\n. thanks so much!\nclosed via d09d50b8fb3cdb75f11bffc93825f472c8193791\n. looks like other Twitter things are on 0.13.8 want to bump it again?\nhttps://github.com/twitter/scrooge/blob/master/project/build.properties\n. looks like other Twitter things are on 0.13.8 want to bump it again?\nhttps://github.com/twitter/scrooge/blob/master/project/build.properties\n. closed via b8183ffcd94f6932b44d382fda2eb009df9d9d13\n. closed via b8183ffcd94f6932b44d382fda2eb009df9d9d13\n. I'd recommend waiting until this closes, as it will likely affect who would be doing the next release https://groups.google.com/forum/#!topic/zipkin-dev/hug_bWIqdLY\n. I'd recommend waiting until this closes, as it will likely affect who would be doing the next release https://groups.google.com/forum/#!topic/zipkin-dev/hug_bWIqdLY\n. great. \nmy 2p, not necessarily twitter's\nI think this repo is too big anyway. If there's a standalone repo for gems, it makes sense for sooo many reasons. I'll try an socialize an internal switch once the gems are in openzipkin. Then, if we're lucky, we can just rm -rf this.\n. so here's latest thinking of mine. re-orging gems out of the repo might end up holding some good work hostage. It is probably best to go ahead and get something out. @rtyler you ok with this version number (for now)? we can always bump it again for openzipkin or similar.\n. so here's latest thinking of mine. re-orging gems out of the repo might end up holding some good work hostage. It is probably best to go ahead and get something out. @rtyler you ok with this version number (for now)? we can always bump it again for openzipkin or similar.\n. closed via 1c22afb3bb502a49aa2463c39d714860a3a8efb5\n. closed via 1c22afb3bb502a49aa2463c39d714860a3a8efb5\n. Yeah I suspect so. Folks are generally moving towards Kafka, though some\nuse other systems like kenesis.\nWe have an issue about Kafka so maybe high time we call this a few real\ndecision ;)\n. closed via #682\n. I'd recommend waiting until this closes, as it affects where new features should land https://groups.google.com/forum/#!topic/zipkin-dev/hug_bWIqdLY\n. I'd recommend waiting until this closes, as it affects where new features should land https://groups.google.com/forum/#!topic/zipkin-dev/hug_bWIqdLY\n. @drpacman as we've already merged some other things, feel free to rebase and dust off for review\n. @drpacman as we've already merged some other things, feel free to rebase and dust off for review\n. I'm going to go ahead and dust this off and merge it. thanks tons for doing this. I've heard others interested in redis.\n. I'm going to go ahead and dust this off and merge it. thanks tons for doing this. I've heard others interested in redis.\n. closed via 3fdd6d63e4ce9dfdf02faac4b380826009bdaf51\n. closed via 3fdd6d63e4ce9dfdf02faac4b380826009bdaf51\n. thanks for the fix and manually testing it!\n. thanks for the fix and manually testing it!\n. one day... we'll have nice tests which will catch things like this! Thanks.\n. one day... we'll have nice tests which will catch things like this! Thanks.\n. thanks!\n. thanks!\n. I think our other stuff is at 2.10.5. This probably needs to be in a general tune-up PR.\nhttps://github.com/twitter/scrooge/blob/master/project/Build.scala#L67\n. I think our other stuff is at 2.10.5. This probably needs to be in a general tune-up PR.\nhttps://github.com/twitter/scrooge/blob/master/project/Build.scala#L67\n. closed via b8183ffcd94f6932b44d382fda2eb009df9d9d13\n. closed via b8183ffcd94f6932b44d382fda2eb009df9d9d13\n. thx\n. thx\n. yeap. nice catch\n. yeap. nice catch\n. closed via 86544f7ae39384864a9134fdf3eadaea63e59ce9\nNote: this isn't guaranteed to fix anything\n. gracias!\n. closed via ecf5d6e79b759a33a003e8e82028564caa46fe1e\n. LGTM\n. thanks, @noslowerdna, appreciate you taking time to resolve this w/ test cases\n. closed via 6f478dbf0aa91244dfb0195d5c0ea40ab09b7d43\n. I can make the change internal to twitter to set our default to 100 on deployment, if necessary. This is less about what works in Twitter, and more about what is generally a better default. Let's get a tie-break?\nwanna ping folks here and see what they feel about 100 vs 10? https://gitter.im/openzipkin/zipkin\n. Configuration is something that should be configurable. Regardless of\nwhether 100 is arbitrary or a well tuned best default, there is a bug here\nwe shouldn't lose sight of. Let's get that merged especially if this\nconversation is going to get laborious.\n. ok all, once this rebases on master, it will only include the change to the default constant. hoping we can get closure on this.\n. ok all, once this rebases on master, it will only include the change to the default constant. hoping we can get closure on this.\n. this should be configurable, as there are valid reasons for different defaults. I opened up this to ensure folks can get what they want https://github.com/openzipkin/zipkin/issues/462\n. @a-roberts we used to have 100 then set to 10. It has been 2 years, we can revisit the topic, but review ^^ first?. @a-roberts we used to have 100 then set to 10. It has been 2 years, we can revisit the topic, but review ^^ first?. @henrikno @IceCreamYou @Trundle @eirslett cursory glance looks ok (although in general, this stuff should have tests) If I don't hear back otherwise, I'll merge by Friday\n. @postwait you might also be interested in this.\n. fyi we've two options on this:\n- merge with DBCP and convert later if a compelling case exists for HikariCP - this allows the bulk of the change, which so far seems uncontested, to merge.\n- wait until others get on board with DBCP vs HikariCP - might result in higher performance, but delays the change\nHistorically we've struggled with delayed change, it sometimes forces people into forks, or delays their move out of forks. Anecdotally, delay seems at odds with engagement, which is something we are trying to turn around. For this reason alone, I suggest we merge.\n@eirslett wdyt?\n. @brettwooldridge awesome. good luck with dadops!\n. fyi I can't merge this for technical reasons right now. Hopefully, they'll resolve by Monday.\n. fyi I can't merge this for technical reasons right now. Hopefully, they'll resolve by Monday.\n. technical issues resolved.. merging\n. technical issues resolved.. merging\n. closed via 2f5f5f547631934bd0c5616c3bdeb41f114555c0\n. closed via 2f5f5f547631934bd0c5616c3bdeb41f114555c0\n. Thanks for all the help, folks. Particularly those interested in performance should scroll up to read the update from @brettwooldridge on db pools (github doesn't notify on edit). This topic might be best pulled into a separate issue/pull request, which we could quickly address before releasing 1.2.\n. Thanks for all the help, folks. Particularly those interested in performance should scroll up to read the update from @brettwooldridge on db pools (github doesn't notify on edit). This topic might be best pulled into a separate issue/pull request, which we could quickly address before releasing 1.2.\n. fwiw I think @brettwooldridge makes a strong case, but up to y'all to decide to raise a PR for switcheroo or not.\n. fwiw I think @brettwooldridge makes a strong case, but up to y'all to decide to raise a PR for switcheroo or not.\n. @rtyler thx the tracer gem and the browser plugin both seem like they're better off in their own repos\n. @rtyler thx the tracer gem and the browser plugin both seem like they're better off in their own repos\n. closed via 743e8470f3b4ed7c950a3610060807b6241e3b7c\n. closed via 743e8470f3b4ed7c950a3610060807b6241e3b7c\n. FYI this gem is archived here https://github.com/openzipkin/zipkin-query\n. FYI this gem is archived here https://github.com/openzipkin/zipkin-query\n. @yurishkuro I didn't have a design in mind, but your thoughts seem to be on the right path\n. @yurishkuro I didn't have a design in mind, but your thoughts seem to be on the right path\n. closed via e4f0fd64234c78d649695ef1209904318aac5a9b\n. I'll answer the easy question: This is the same UI as Twitter uses.\n. FWIW, your experience is common. The first time setup and UI experiences\nare lacking and contributors are actively brainstorming on how to address\nhigh priority topics like this.\n. :+1: \n. @brettwooldridge fyi\n. Thanks for the follow-up @noslowerdna!\nclosed via 8d99dff3d944277c38d1b2be138e8d7370cf5bbc\n. this relates to #451 (prefer JVM interop over scala types) which was a topic continually discussed in the last 3 days by several collaborators.\n. @olix0r ack!\n. interested parties include at least @jpinner @dsyer @autoletics @kristofa @eirslett @xylus @michaelsembwever and myself. Many of these folks can write scala, but are working in interop scenarios that prefer java. (correct me if I'm wrong folks)\n. @postwait based on your presentation last year, I suspect you are also interested in this.\n. @autoletics so I think this isn't the right issue to track a move towards a completely different model, as it scopes this far beyond type conversion. My advice would be to create a separate issue for moving to the probes api, if that's your goal. It would be less distracting and also easier to find in the issues list.\nmy 2p on where the idea of probes-api convergence:\nThe idea of a general api for JVM-based tracing implementations is also broader than zipkin itself, as it crossed HTrace, Pinpoint, and other Dapper clones. It may well be the case that all of the dapper clones no longer wish to be dapper clones and instead probes api implementors, but that's still not great to have as a sidebar here. Such a goal would only be implementable if each of the projects had an explicit desire to do that. I'm not sure all are there, yet. How about opening an issue in https://github.com/autoletics/probes-api issues list, to demonstrate the value of converging perhaps with an adapter? That way instead of telling people what's best, they would be able to see that it is best and ask for it. Regardless, the engagement and visibility of the probes-api project would increase as a result, which could only help your aim which is to make this a standard api.\nI'm sure you have feedback on my advice, but I'm concerned that this thread will be distracted further. If you don't mind, ping me on any other channel, or even in a probes-api issue, and I'd be happy to follow-up with you. In the mean time, let's try to keep this issue scoped as I don't want people to not want to comment on it at all because it got stuck in a probes-api discussion. Fair?\n. Good point. This is a hidden advantage for sure. At any rate we could\nrelease 1.2 and do whatever we need here (breaks and all) as 2.0.\n. > This is helpful, but doesn't address the related problem of wanting to\n\navoid having to build and deploy scala components at all. Is there already\nan issue tracking that? It is something we would like to help with,\nregardless\n\nGood point, Ben. Please add an issue for a pure java implementation. Via\ndiscussions, at least Netflix, Lyft and users of Brave would be interested.\nYou'll also at least get my hands helping.\nThis issue could be considered a milestone towards minimal dependency\nzipkin.\n. > Lookout has done some work on publishing zipkin builds to bintray; we\n\ncould let people use them instead og building from source.\nOne of the issues then is how to plug in the various storage backends,\nruntime? Dynamic linking?\nThere are a number of ways to attack configuration of backends. At first,\nwe can just do the port work. Care to bump the config concern out to its\nown issue?\n. Actually, maybe it is simpler just to rename this issue to rewrite in java?\nWe can always raise incremental pull requests.. Wdyt?\n. Well put!\n. Fwiw, we do have java services at Twitter (ex storage) and of course\nservices inherited via acquisition are rarely Scala.\n\nHere are some points I have encountered in a recent project to make a\ncommon model across java and Scala (I attempted to abstract some of our\ntime series processing code used for an alerting system).\nThe technical issues are daunting, but solvable. Regardless, they need to\nbe solved in an environment that desires the solution.\nOn converting finagle shared libraries towards java:\n- scrooge finagle java support didn't work. I think eirik may have fixed\n  this.\n- even if it did, Scala code uses the Scala types generated by scrooge.\n- finagle code tends to always return futures, and these are Twitter\n  specific futures.\n- Twitter libraries tend to have a lot of deps and also one on a specific\n  Scala runtime.\n- guava conversion seemed to be the only way out, though we know guava\n  versions are sensitive for everyone.\n- current scala+finagle mix isn't java 8 friendly, so yeah no lambdas\nOn finagle services vs ones written without finagle (aka share IDL not\nlibraries).\n- when t-mux isn't used, other thrift impls can interop  (Like Facebook\n  swift)\n- this means you don't have to use scrooge, as you aren't trying to share\n  libraries, rather types (or IDL)\n- this isn't that different when thinking about how to address Scala 2.12,\n  java or another language.\nOn pushing thrift to an extension\n- thrift is often conflated with finagle and by extension Scala in Twitter\n  codebases (even ones written in Java)\n- by using plain types and not using scrooge, we could open to codec that\n  isn't thrift (ex json) and doesn't pull in vast dependency trees that are\n  coupled to specific Scala versions.\n- since many prefer json or protobuf, this indirection is likely worth\n  whatever conversion cost incurred\n  - most build systems can run JMH, so we can at least measure that\nHope this helps.\n. SUMMARY:\n463 will address an alternative pure-java backend.\nThis issue focuses on tactical work to make the existing scala/finagle backend more friendly to java or otherwise non-scala implementors. For example, this could include progress such as using scrooge to generate java types (which still have dependencies on scala and finagle).\n. sorry no-one's looked at this yet. I don't have an opinion.\n@eirslett wdyt?\n. I believe twitter/zipkin has all the changes present in openzipkin/zipkin.\n. and done!\n. Thanks for the code. I will try to convert this into a test case.\n. Hi, @3thinkthendoit so I don't know how to move this to finagle.. if I could I would.\nIf I understand the issue correctly, you are suggesting that eventhough the finagle sampler is set to zero, it still sinks the trace.\nThere are tests that cover dropping traces when the sample rate is zero\nhttps://github.com/twitter/finagle/blob/develop/finagle-zipkin/src/test/scala/com/twitter/finagle/zipkin/thrift/SamplerTest.scala#L37\nFor this reason, I suspect it may be a config issue. I'd have a look at this\nhttps://github.com/twitter/finagle/tree/develop/finagle-zipkin\nIf you use the ServerBuilder or ClientBuilder, and assign a tracer there, it seems the case that you would be able to use it implicitly via Trace.record etc.\nIf you still have issues, I'd direct you to finaglers, as not only are there relevant topics there, but in the case you have found a bug, it would be addressable there.\nhttps://groups.google.com/forum/#!searchin/finaglers/ZipkinTracer\nHope this helps\n. :shipit:\n. @eirslett good point. sgtm\n. @petemounce have you seen this about metrics? https://github.com/openzipkin/zipkin/tree/master/zipkin-server#metrics\n. @mosesn pls suggest a preferred approach and I'll ensure that the internal twitter deployment of zipkin-web has a default query limit of 100.\nex.\nscala\n  val queryLimit = flag(\"zipkin.web.query.limit\", 10, \"Default query limit for trace results\")\n. 10 was already decided by majority on the other pull request and don't want\nto start a bikeshed on that. I just wanted to make sure you can have the\ndefault you want inside Twitter.\n. We don't have to further discussion about a default value, when it can be\noverridden. You are the only person who asserted further discussion was\nneeded.\nYou made the case about why you want default to be 100. The originator made\nthe case about 10 and the act of merging that was seconded by rtyler and\nimplicitly me.\nThis is OpenZipkin: we haven't made a rule that we need 100% consensus. Nor\ncan we force people to discuss something they are not interested in\ndiscussing further.\nResolving this issue will allow you to have the value 100 and if later the\ncommunity wants to change the default to 101, we can do that. In the mean\ntime, we really do have bigger fish to fry.\n. Moses, there was no discussion for 2 weeks, despite an attempt to encourage folks to do so. There was no unilateralism, as two others besides me indicated they wanted the change. I can tell you aren't satisfied; I'm disappointed that I couldn't please you even by raising this issue.\n. Here's the style of argument that supports this: -zipkin.web.query.limit=100\n. :+1:\nI suspect if we do this well, we can provide a future without the need to maintain both zipkin and brave separately.\n. I think this is getting to be mailing list worthy, but at any rate, good\ndiscussion.\nThe big concern is that there are a few people who want Scala+Finagle impl\nto persist.\nCan we think about brave? How can we permit can those who can't deploy,\nfeel limited by, or simply don't like Scala to still be a part of zipkin?\nBrave happens to have a subset of features.. Would it be that bad to allow\na java server impl (or golang for that matter)?\nI'm guessing the solution is keep the code in a different repo, similar to\nbrave, so that it doesn't threaten those who still enjoy the current\ncodebase. Would that work?\n. OK I will be more clear.\nWe can break down this task, but I desire to help those writing\ninstrumentation or backends not require a tree of deps and interop problems.\nBrave partially solves that problem, but this isn't just about what brave\nchose to solve. I didn't intend to get us down that path. Bear in mind that\nmany aren't going to desire the Twitter server entrypoint for reasons\nincluding dependencies, but also config management and bootstrap. Consider\nnetflixoss and others who use spring boot or other means to bootstrap\nservices. There's also the relative unpopularity of sbt within java\necosystem, too!\nIt seems best to allow a java repo to exist regardless of Brave and just\nmove the corresponding issues there, right? I can count at least 3 folks\nwho would work on that and so it is more when than if.\nPersonally, this is of most interest to me as I like my work to be of\nhighest reuse, and I wish to engage those who have similar interest.\nHowever this is more my morning/weekend work, like the work I did on Brave.\nI still have to write Scala at work anyway.\nAny reason we shouldn't just move this discussion to a new repo of willing\nhands and let them decide what to do?\n. I would prefer zipkin to just slowly introduce java where it makes sense.\nIf preference towards status quo (finagle/scala) interferes with that, I'd\nprefer to make a repository in OpenZipkin or DistributedTracing to allow\nprogress to happen.\nI don't expect all backends to be in a mono-repo, as especially people's\nlocal implementations won't be (ex. inside Twitter, we have a Manhattan\nstorage backend). I don't really want to conflate this issue with the\nmono-repo pros/cons though, but it is interesting that cassandra will be\nrewritten in java anyway.\nOn SBT, our build really limits us, and this is only SBT because of scala.\nI imagine there will be folks who would prefer to be more productive by\nescaping that. Agreed it is another issue, but scala is related.\n. @michaelsembwever the issue as posted doesn't split hairs about client or server side, though I agree it would end up being broken down into work like that. I'm worried that I am dominating the discussion, and in attempts to answer in terms of the options you propose. server-side-B should be possible as an alternate or a rewrite.\nWe shouldn't conflate this issue with a vote for brave as ideal interfaces for client-side-A. Since this is a new community, it should be a choice how to implement client-side-(A|B), not a force function of current inventory.\nWe may get more engagement if the community are permitted to build both the server and client side in tandem in a manner they decide to, without the burden of a legacy codebase.\nI'd like to hear what others think, and fear that further back/forth on my opinion may reduce the chance others will engage at all.\n. I think we should keep this open or move it to a new repo. There are still\nmany people, myself included, who thing that the Scala implementation is a\nlarge source of drag. This persists regardless of classification systems.\nJust reopening yet another issue to say the same thing is tantamount to a\nshells game.\n. SUMMARY:\nThis discussion brought to foreground a key issue, which is whether openzipkin/zipkin will allow a pure-java implementation to overtake the existing scala/finagle one. The current scala/finagle one is desired by active contributors including @eirslett and @kristofa \nTo allow mutual progress, a pure-server backend should start in its own repository under openzipkin. This repo will focus on allowing JVM backends to be written without a dependency on a specific scala runtime, directly or indirectly. For example, it wouldn't require configuration to be written in scala, nor would accident of thrift serialization end up requiring finagle. It wouldn't focus on ancillary sharing activities, such as attempts to get instrumentation to share the same libraries (like context propagation). It is very important that such a project has a narrow and solvable mission.\nI'd expect key stakeholders to be those who've raised their hands before.. at least @b @jpinner @NiteshKant @dsyer @spencergibb. If folks have a good idea for a repo name besides java-zipkin, or if you wish collaborate, raise your hand!\nI'll create the repo sometime in the next week and offer to help draft initial libraries.\n. ok here it is.. we'll have some issues to discuss before code can be formalized https://github.com/openzipkin/zipkin-java\n. Goes without saying, but if at any time openzipkin/zipkin wants to replace components with those in java-zipkin, of course that would be the most ideal outcome.\n. Note, I tried to follow existing convention.. this doesn't mean I love it :)\ncc @noslowerdna @mosesn @rtyler \n. thx for the review!\n. confirmed this works by deploying to a less used zone in prod at twitter\n. oh yeah.. the syntax I verified is this -zipkin.web.query.limit=100\n. > One thing that could potentially cause some confusion is that we have\n\ndifferent ways of configuring collector/query (with .Scala files) than web\n(flags). Would it be worth the effort to rewrite to one \"universal\" method?\nI noticed the inconsistency too. Refactoring how we do config in general is\nprobably due as things have improved in the last few years. Care to open a\ntop-level issue on how you think config ought be handled?\n. Sounds not great, but thanks for reporting.\n\nWhat build of zipkin are you running? (Ex are you building off a fork? If\nso, which commit?)\n. At Twitter, static config that varies by environment is passed via flags or\nin some cases json files. These are set in pants args when running a\nprocess on laptop or in Aurora config.\nThe most thought in this area has been in Finatra. A part of this was\nspinning out a project to address config, as particularly the lack of\ntestability has been troublesome. For example the most recent apps can be\nspawned safely managed by a test lifecycle (embedded).\nhttps://github.com/twitter/finatra/blob/master/inject/README.md\nHope this helps!\n. Thanks for the help, Travis!\n:shipit:\n. I tried this and it glitched out for reasons I don't recall. Glad you are\ngiving it another go!\n. umm.. basically there's flakey tests (ones that require services that flake\nout), and massive (and conflicting) dep trees. This is independent of\nbuilders, but makes builders easier?\n. yanked for no good reason except yeah I can do this now.\n. so.. I am trying to just remove the repository references for started. Running what I think is clean process locally.\n``` bash\nsupernuke to redownload all the things\n$ rm -rf ~/.sbt/staging/  ~/.ivy2/cache/ ~/.m2/repository\n$ sbt clean test\n```\n. that worked.. will raise a PR\n. actually hold on.. testing that a related example still builds\n. k https://github.com/openzipkin/zipkin/pull/471\n. looking at :8ball: to see if this build will pass\n. lgtm on green (and even if not green, I guess.. as long as it is not worse than before :))\n. @travisbrown https://github.com/openzipkin/zipkin/pull/472 on the getHostName thingy\n. Tested by building the example.\nbash\n$ cat ~/.sbt/0.13/global.sbt\npublishTo := Some(Resolver.file(\"file\",  new File(Path.userHome.absolutePath+\"/.m2/repository\")))\n$ sbt publish\n$ cd doc/examples/collector\n$ sbt clean test\n. build failures are unrelated\n. I'm going to rebase this and see if anything helped.\n. /me crosses fingers\n. I'm happy to see effort here! Thanks so much for helping. I don't know what\nis idiomatic is SBT, but clarifying and deduping sounds like a step\nforward. I will test this against my WIP code for Kafka tomorrow AM Pacific\ntime.\n. is this obviated by #478\n. :shipit: as currently everything is hosed and this makes things better.\n. rebased over latest decoupled build\n. @jamescway PTAL\n. Thx will change it!\n. @eirslett ok.. was a little tricky, but yeah better than interop on the Span type for sure. PTAL!\n. @eirslett forgive me for merging :) I think I addressed your concern and happy to revise further in a latter change as necessary!\n. cc @rtyler @abesto\n. ps JDK 8 happened to be in the same commit, but I've no strong feelings about that.\n. :shipit: (also prefer less commits, ideally 1)\n. thanks!\n. :shipit: \n. one passed! probably should verbose:gc in case the other was in a gc spiral.. will do that tomorrow.\n. Cool!\n. :shipit:\nI'm cool on merging this. Even if there are future UX enhancements later,\nmy guess is that this is largely what we want. For example, zoomable ui is\na key feature of HTrace.\nAfter this goes prod, I can facilitate further feedback. Iterations will be\nquite easy once releases are possible, and I suspect we will have a release\nprocess quite soon.\n. +1\n. get the idea, just wondering if some sort of already existing retry thing could work. will take a look in a bit\n. note there are many scala warnings.. this is just the deprecation side of things\n. Let's drop the last commit as it will certainly break people using \"dev\" or \"redis\" We do want to move to factories, but only when we can at least keep \"dev/anorm\", \"redis\" and \"cassandra\" working.\nI'm good on the change, just a couple suggestions about test and logging. Good work!\n. great job\n. merging based on a fly-by by @spencergibb happy to address more stuff post-merge\n. This should hopefully result in publication of the fat jars cc @dsyer\n. Dooo iittt ;)\n. Gracias!\n. The entrypoint for query is ThriftQueryService not QueryService\n. irrelevant now, mountains have moved since :)\n. Good progress! Here are some thoughts:\n- I wouldn't block this on embedded cassandra, as that's polish. We can just use travis service/local cassandra for now (like we do today).\n- Mixed feelings about the separate -java and -aggregate projects. The artifact graph is already complex, and something we've been working hard to make sense of. Can we delay this optimization? For example, add a java source tree and make the deps for -aggregate optional. \nI can help look at the test, but any chance you can move this branch to openzipkin? Easier for us to collaborate there.\n. ps.. given a choice, I'd just delete cassandra-aggregate vs move it to an incompatible peer project. I'm sure @eirslett if not others could make short work of that once the core cassandra is in.\n. Based on offline chat, here's my take:\n- the -java project is already in use for a project that writes traces directly into cassandra vs via scribe etc. Maybe the name could be changed, but understand where that's coming from. Personally, I prefer -core suffix for library code.\n- we should definitely delete the -aggregate codebase as opposed to orphaning it in a way that doesn't compile. This is part of collateral damage needed to get zipkin running again. Hopefully, @eirslett could help add a new -aggregate codebase after this is merged (based on git history or from scratch).\nNeed some folks to weigh in, as I'd like to depend on this work in order to complete other work (like removing Index/Store, and the Builder stuff).\n. Great point. Once our jars are published, we could switch the dep in the\naggregated module until the new code is ported over.\n. Here are some hints on how to retain support for Builder w/o having to implement Index or Storage:\nthere's a bridge you can use ex here: https://github.com/openzipkin/zipkin/blob/master/zipkin-collector-service/config/collector-redis.scala#L30\nalso here https://github.com/openzipkin/zipkin/pull/583/files#diff-1b0789c140f012e2404ae48339a50569\n. The following commit completes this change https://github.com/adriancole/zipkin/commit/84731aca6a4d162bce404580723be874beaef0d9\nI'll raise a PR, squashing that into mck's work.\n. raised #585 to merge this in (removing the temp commits and squashing)\n. ok PTAL\n. yeah, well redis didn't actually run the tests prior (only did clear() actually!), so there may be some flakes to squash over time. beats zero coverage, though!\n. ok build's green. I will help with flakes or disabling things that flake on certain backends.\n. :shipit:\n. :clap: \n. deploys fine internally at twitter\n. will merge on green. Thanks for having a look @abesto!\n. must be in the runnings :)\n. +1\n. makes sense\n. weird.. I thought ./gradlew install already did this?\n. Sounds confusing for sure.\n. @michaelsembwever added your note for release here. thx!\n. This was collateral damage from moving off cassie. You can build aggregates from commit 6c68d55e0d1b62893d49dba81aa4224fba68ddcf for now. \n. Here's the commit to resurrect files from: https://github.com/openzipkin/zipkin/commit/6e8fdad0bfa4c1de16215841f4c04f9710e3e686#diff-9e4ef735c2cc6a6622c3f51167c24b80\nNote: when re-implementing this. I think it is preferable to put the hadoop/scalding code for cassandra into the zipkin-aggregate tree rather than zipkin-cassandra. zipkin-aggregate is the only user of this, and it is relatively dependency heavy.\n. moved here https://github.com/openzipkin/zipkin-aggregate/issues/2\n. @eirslett PTAL\ncc @jpinner @NiteshKant\n. updated with the trivial change needed to make the other storage interface, Aggregates, java friendlier.\n. rebased, but no other progress\n. rebased, but no other progress\n. To whom it may concern. I looked into resurrecting this. The problem is that there is very little test coverage to say what \"should\" happen. For example, the tests are trivial and rely mostly on whatever algebird is supposed to do. Best first step would be to backfill tests for the working implementation (anormdb and/or zipkin-hbase), then try to make a cassandra impl that passes those tests.\n. PTAL!\n. PTAL!\n. merging so I can release, which could help progress https://github.com/openzipkin/zipkin-dependencies-hadoop\n. merging so I can release, which could help progress https://github.com/openzipkin/zipkin-dependencies-hadoop\n. these sound like miles forward. thanks!\n. :shipit: \n. I'm ok with this. :shipit: \n. thanks, @abesto.. let's give this a shot!\n. happy to address any feedback post-merge!\n. Note: this is now running in production at Twitter\n. lol!\n. I like this flow, it is well documented and covers the cases, we've got so far.\n. Thanks for chipping the way towards snapshots.\n. Can you separate the bug fix from the enhancement?\nThe enhancement will be confusing to those who don't set this variable, which will be everyone except you. I'd personally prefer no environment in the UI than a false one.\n. gracias!\n. I can't wait to use this!\n. Change looks good! I'd love to have a follow-up that drops the internal constants that just snake case things, but it isn't blocking.\n. weird.. thought I merged this!\n. weird.. thought I merged this!\n. Whoah.. Is this for real?! :)\n. Sad we need to check in a big text file to accomplish this :(\n:shipit: on green\n. Yep. You should have a login with github option at the top when you click\ninto Travis link. Once logged into travis, there should be a circular arrow\nyou can click to kick a job.\n. In jclouds we had stores that acted differently wrt consistency. We created\na method \"assertEventually\" that invoked an assertion with exponential\nbackoff (ex invoked both the retrieve and compare) where this was permitted\nto be different.\nWhere it wasn't permitted to be different, the stores themselves blocked\nbefore they returned success. This allowed all stores to have the same\nbehavior regardless of impl. Otherwise, as @abesto said, people will be\nvery surprised to find they can't read their writes.\nI'd suggest changing this code to subclass CassandraSpanStore upon\ninitialization. Override setttl with a method that calls super then has the\nblocking loop before returning. This allows the test to stay in tact (ex\nnot subject to copy/paste drift).\nAdd to the README that this op is eventually consistent.\nWdyt?\n. In jclouds we had stores that acted differently wrt consistency. We created\na method \"assertEventually\" that invoked an assertion with exponential\nbackoff (ex invoked both the retrieve and compare) where this was permitted\nto be different.\nWhere it wasn't permitted to be different, the stores themselves blocked\nbefore they returned success. This allowed all stores to have the same\nbehavior regardless of impl. Otherwise, as @abesto said, people will be\nvery surprised to find they can't read their writes.\nI'd suggest changing this code to subclass CassandraSpanStore upon\ninitialization. Override setttl with a method that calls super then has the\nblocking loop before returning. This allows the test to stay in tact (ex\nnot subject to copy/paste drift).\nAdd to the README that this op is eventually consistent.\nWdyt?\n. > How would people know?\n\nArn't the writes we're talking about here coming from an event system,\nand the system at large is already asynchronous?\nA developer of SpanStore should know what expectations are, especially as\nthis is the only time we've hit this.\n\nMaybe just document the method as 'Returns once the backend accepts the ttl\nrequest'. Or document the whole SpanStore type as 'all commands that return\nvoid are processed asynchronously'\n\nI think trying to make the store strictly consistent is a bit of a\npointless exercise here.\nNot about that, just saying it is now (else all stores would flake). We\nshould document that it isn't.\nMy scala remains too young to accomplish that.\nCould you provide an example?\nWill do, but sadly need an hour to be at a laptop :(\n. > How would people know?\nArn't the writes we're talking about here coming from an event system,\nand the system at large is already asynchronous?\nA developer of SpanStore should know what expectations are, especially as\nthis is the only time we've hit this.\n\nMaybe just document the method as 'Returns once the backend accepts the ttl\nrequest'. Or document the whole SpanStore type as 'all commands that return\nvoid are processed asynchronously'\n\nI think trying to make the store strictly consistent is a bit of a\npointless exercise here.\nNot about that, just saying it is now (else all stores would flake). We\nshould document that it isn't.\nMy scala remains too young to accomplish that.\nCould you provide an example?\nWill do, but sadly need an hour to be at a laptop :(\n. PS I volunteer to address all of this post merge!\n. PS I volunteer to address all of this post merge!\n. He clicked merge.. Did you see him click merge? :)\n\nI will fix the root issue later, which is that the SpanStore api needs to be documented and tested assuming writes are async.\n. He clicked merge.. Did you see him click merge? :)\nI will fix the root issue later, which is that the SpanStore api needs to be documented and tested assuming writes are async.\n. :shipit:\ncompletely on board with not running tests as a part of release process.\n. :shipit:\ncompletely on board with not running tests as a part of release process.\n. LGTM with nits\n. LGTM with nits\n. :shipit: \n. :shipit: \n. mind updating the spec? When done, I'll help merge this.\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-cassandra/src/test/scala/com/twitter/zipkin/cassandra/CassandraSpanStoreFactorySpec.scala\n. mind updating the spec? When done, I'll help merge this.\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-cassandra/src/test/scala/com/twitter/zipkin/cassandra/CassandraSpanStoreFactorySpec.scala\n. ps in the future, you can just open a PR when you are proposing both the problem and the fix.\n. ps in the future, you can just open a PR when you are proposing both the problem and the fix.\n. great job. Can you squash these into a single commit? I'll merge on green after.\n. great job. Can you squash these into a single commit? I'll merge on green after.\n. The aggregates api still exists in HBase and Anorm. It is partially\nre-implemented in cassandra\nhttps://github.com/openzipkin/zipkin/pull/599/files#diff-d45fcb9231702e1188c7c55289a48c30R19\nI understand that none of the apis matter as the hadoop job hasn't\nbeen re-implemented, but even if it was, only cassandra would work?\nhttps://github.com/openzipkin/zipkin-aggregate/issues/2\nRegardless, this issue is about anorm. It would be nice to know what\nan implementation of the hadoop job would need to do (ex. a unit test\nin the aggregate project might help explain this). Even better would\nbe a non-hadoop answer. For example, some explanation somewhere of\nwhat data must exist for the aggregate UI to operate. I suspect many\nwon't want to run hadoop, but could process the data in another\nfashion.\nThoughts?\n. It would be best to have dependencies on projects that aren't orphaned\nor at risk of being orphaned. Considering the pain of our past, I'd love to \nnot have version locking to a specific scala runtime or EOL JRE.\nI know that during the workshop, Jason from groupon mentioned their tracing\nanalysis story would be via Spark. Any insight into why that may or may not\nbe a good choice for our aggregate functionality? I've not tried this, but\nit seems to have an in-memory mode as well.\n. Parking the discussion on how to implement aggregates here #656, unless we find there's a low-hanging fruit means to implement something for anormdb with current infrastructure.\n. I'm closing this particular issue as the aggregates UI does work with recent versions. Instructions on loading are in the zipkin-anormdb README\n. Wow. Much simpler! :shipit:\nDoes this still build the zip or tarball that isnt published? If so, eventually would be nice to not spend time making them. If no, fantastic!\n. Wow. Much simpler! :shipit:\nDoes this still build the zip or tarball that isnt published? If so, eventually would be nice to not spend time making them. If no, fantastic!\n. emoji lover!\n. emoji lover!\n. merging as JDK8 passed. killing other jobs\n. merging as JDK8 passed. killing other jobs\n. +1\n. +1\n. fantastico\n. Weird I thought I tried that on ./gradlew install last night. (Shrug)\n. Weird I thought I tried that on ./gradlew install last night. (Shrug)\n. LGTM\n. LGTM\n. This ended up hurting more than helping. there's no output https://github.com/travis-ci/travis-ci/issues/4190 and also change detection seems strange\n18.79schange detected:\n/home/travis/.gradle/daemon/2.6/daemon-2295.out.log\n/home/travis/.gradle/daemon/2.6/daemon-2295.out.log\n/home/travis/.gradle/daemon/2.6/registry.bin\n/home/travis/.gradle/daemon/2.6/registry.bin\n/home/travis/.gradle/daemon/2.6/registry.bin.lock\n/home/travis/.gradle/daemon/2.6/registry.bin.lock\nrolled back via d77aef6\n. cc @rtyler @eirslett @kristofa @jamescway \n. cc @rtyler @eirslett @kristofa @jamescway \n. Thanks for the quick look, folks!\n. similar status\nFound 1 configuration(s) to publish.\nUploading to https://api.bintray.com/content/openzipkin/zipkin/zipkin/1.2.1-rc25/io/zipkin/zipkin-web/1.2.1-rc25/zipkin-web-1.2.1-rc25.jar...\nUploaded to 'https://api.bintray.com/content/openzipkin/zipkin/zipkin/1.2.1-rc25/io/zipkin/zipkin-web/1.2.1-rc25/zipkin-web-1.2.1-rc25.jar'.\nUploading to https://api.bintray.com/content/openzipkin/zipkin/zipkin/1.2.1-rc25/io/zipkin/zipkin-web/1.2.1-rc25/zipkin-web-1.2.1-rc25-javadoc.jar...\nUploaded to 'https://api.bintray.com/content/openzipkin/zipkin/zipkin/1.2.1-rc25/io/zipkin/zipkin-web/1.2.1-rc25/zipkin-web-1.2.1-rc25-javadoc.jar'.\nUploading to https://api.bintray.com/content/openzipkin/zipkin/zipkin/1.2.1-rc25/io/zipkin/zipkin-web/1.2.1-rc25/zipkin-web-1.2.1-rc25-sources.jar...\nUploaded to 'https://api.bintray.com/content/openzipkin/zipkin/zipkin/1.2.1-rc25/io/zipkin/zipkin-web/1.2.1-rc25/zipkin-web-1.2.1-rc25-sources.jar'.\nUploading to https://api.bintray.com/content/openzipkin/zipkin/zipkin/1.2.1-rc25/io/zipkin/zipkin-web/1.2.1-rc25/zipkin-web-1.2.1-rc25-all.jar...\nUploaded to 'https://api.bintray.com/content/openzipkin/zipkin/zipkin/1.2.1-rc25/io/zipkin/zipkin-web/1.2.1-rc25/zipkin-web-1.2.1-rc25-all.jar'.\nUploading to https://api.bintray.com/content/openzipkin/zipkin/zipkin/1.2.1-rc25/io/zipkin/zipkin-web/1.2.1-rc25/zipkin-web-1.2.1-rc25.pom...\nUploaded to 'https://api.bintray.com/content/openzipkin/zipkin/zipkin/1.2.1-rc25/io/zipkin/zipkin-web/1.2.1-rc25/zipkin-web-1.2.1-rc25.pom'.\n:zipkin-web:bintrayUpload (Thread[Daemon worker,5,main]) completed. Took 34.463 secs.\n:bintrayUpload (Thread[Daemon worker,5,main]) started.\n:bintrayUpload\nExecuting task ':bintrayUpload' (up-to-date check took 0.001 secs) due to:\n  Task has not declared any outputs.\nFound 0 configuration(s) to publish.\n. I'll tag again and just wait longer.. Not clicking anywhere. Let's see if\nit just finishes.\n. Actually, could someone else tag? 1.2.1-rc26\nTakes a while to finish and I'm afk\n. maybe we can add logging to the signing routine? (if they accept pull requests). perhaps our huge jars take a while to sign.\n. calling this done. 2 successful deploys in a row. Key is to wait. Also wait at least 5 minutes before clicking on maven sync button in bintray.\n. This is timing out again.. when it does, here's the command to sign manually. Then, go to bintray and publish the artifacts.\n``` bash\nsubstitute whatever version instead of 1.8.1\n$ curl https://$USER:$BINTRAY_API_KEY@api.bintray.com/gpg/openzipkin/maven/zipkin/versions/1.8.1 -XPOST\n```\n. ok this should be safe to merge, since docker only uses fat jars now.\n. yes.. the more output likely will keep travis happy.. let's watch! https://travis-ci.org/openzipkin/zipkin/jobs/76685357\n. cc @eirslett \n. @eirslett I'll move #597 to the new repo\n. one passed, so merging.\nThis is another bad weather day for jcenter\n. @dsyer @spencergibb any good choices of env variable name?\n. done\n. sorry it is a bit of a mess.. I'll look at this after catching up on change like https://github.com/openzipkin/zipkin/commit/c69db3bfffb855f766ba1754cfe35e7bcc12fce5\n. completely knackered, but agree this is really bad. please nag on https://gitter.im/openzipkin/zipkin if this becomes critical and still left undone.\n. Looks like this was partially implemented as environment variables in collector like so\nscala\nval serverPort = sys.env.get(\"COLLECTOR_PORT\").getOrElse(\"9410\").toInt\nval adminPort = sys.env.get(\"COLLECTOR_ADMIN_PORT\").getOrElse(\"9900\").toInt\nMind if I just implement that convention with COLLECTOR, QUERY, WEB?\n. Yep you're right! https://github.com/openzipkin/zipkin/pull/714/files\n. should be good, now\n. should be good, now\n. supernit which is to make the first line of your commit message similar to the title, as it currently reads \"problem: \" :)\nLGTM\n. supernit which is to make the first line of your commit message similar to the title, as it currently reads \"problem: \" :)\nLGTM\n. great progress!\n. great progress!\n. thanks!\n. just one spelling thing otherwise LGTM\n. just one spelling thing otherwise LGTM\n. actually there's more to remove.. the only user of spanStore.getTracesDuration was ordering!\n. actually there's more to remove.. the only user of spanStore.getTracesDuration was ordering!\n. ok this is much better... far more mileage out of removing support for Order!\n. ok this is much better... far more mileage out of removing support for Order!\n. @jamescway yeah.. just verified old UI still works fine (extra unnecessary order param is ignored)\n. @jamescway yeah.. just verified old UI still works fine (extra unnecessary order param is ignored)\n. not sure, but we may be able to update to scala 2.11 after this?\n. not sure, but we may be able to update to scala 2.11 after this?\n. happy\n. happy\n. @mosesn safe to assume twitter will be able to build on JDK8 this year, right?\n. @mosesn safe to assume twitter will be able to build on JDK8 this year, right?\n. I know zipkin was running on JRE 8 when I left, but yeah building is a different topic. At worst, we could switch the compiler (if not already set) to produce Java 7 bytecode.\n. I know zipkin was running on JRE 8 when I left, but yeah building is a different topic. At worst, we could switch the compiler (if not already set) to produce Java 7 bytecode.\n. ps if we want to decouple, add this to the main build.gradle after apply plugin java\nsourceCompatibility = 1.7\n    targetCompatibility = 1.7\n. ps if we want to decouple, add this to the main build.gradle after apply plugin java\nsourceCompatibility = 1.7\n    targetCompatibility = 1.7\n. No twitter doesn't use the fat-jars (or the -service projects at all).\nThere's an internal TwitterServer, configured by flags, which uses the\nFactory approach as opposed to the ostrich stuff.\nIf zipkin's libraries are source-level 8, then pants (the internal build\ntool) would need to run on 1.8 to compile them. @stuhood might know if that\nmigration is complete.\nRegardless, twitter can update to 1.4.0 (released today) as that was still\ncompiled to target Java 7. cc @CaitieM20\n. No twitter doesn't use the fat-jars (or the -service projects at all).\nThere's an internal TwitterServer, configured by flags, which uses the\nFactory approach as opposed to the ostrich stuff.\nIf zipkin's libraries are source-level 8, then pants (the internal build\ntool) would need to run on 1.8 to compile them. @stuhood might know if that\nmigration is complete.\nRegardless, twitter can update to 1.4.0 (released today) as that was still\ncompiled to target Java 7. cc @CaitieM20\n. @jamescway thanks for updating the settings.. I'll run this locally and see if the classes are built properly.. hang tight!\n. @jamescway thanks for updating the settings.. I'll run this locally and see if the classes are built properly.. hang tight!\n. LGTM 51 is Java 7\n``` bash\n$ ./gradlew -version\n\nGradle 2.6\nBuild time:   2015-08-10 13:15:06 UTC\nBuild number: none\nRevision:     233bbf8e47c82f72cb898b3e0a96b85d0aad166e\nGroovy:       2.3.10\nAnt:          Apache Ant(TM) version 1.9.3 compiled on December 23 2013\nJVM:          1.8.0_51 (Oracle Corporation 25.51-b03)\nOS:           Mac OS X 10.10.5 x86_64\n$ ./gradlew clean install\n--snip--\n$ file ./zipkin-common/build/classes/main/com/twitter/zipkin/util/Util.class\n./zipkin-common/build/classes/main/com/twitter/zipkin/util/Util.class: compiled Java class data, version 51.0\n```\n. LGTM 51 is Java 7\n``` bash\n$ ./gradlew -version\n\nGradle 2.6\nBuild time:   2015-08-10 13:15:06 UTC\nBuild number: none\nRevision:     233bbf8e47c82f72cb898b3e0a96b85d0aad166e\nGroovy:       2.3.10\nAnt:          Apache Ant(TM) version 1.9.3 compiled on December 23 2013\nJVM:          1.8.0_51 (Oracle Corporation 25.51-b03)\nOS:           Mac OS X 10.10.5 x86_64\n$ ./gradlew clean install\n--snip--\n$ file ./zipkin-common/build/classes/main/com/twitter/zipkin/util/Util.class\n./zipkin-common/build/classes/main/com/twitter/zipkin/util/Util.class: compiled Java class data, version 51.0\n```\n. For those watching at home.. the collector can now have kafka enabled. There's configuration options here: https://github.com/openzipkin/zipkin/tree/master/zipkin-receiver-kafka\ncan't wait to see @clehene's end-to-end\n. PS besides htrace, I think we should inventory other kafka instrumentation. For example, Brave, Ruby and Finagle. At some point, we should be able to trace zipkin itself without scribe, too.\n. htrace, not hbase :) Anyway, e2e tests aren't likely to happen in this repo.\n. I'll delete it now. should unblock you\n. I'll delete it now. should unblock you\n. The problem is that we are publishing examples that are neither valid nor exist.\n. The problem is that we are publishing examples that are neither valid nor exist.\n. Lgtm thanks!\n. Lgtm thanks!\n. sweeeet\n. sweeeet\n. This is a documentation bug. Twitter doesn't use cassandra anymore, and regardless, we should bring this up to speed.\nconfigKey=dev is a specific anormdb setup that uses file sharing. We could quite easily add a mysql config based on a jdbc connect string. I've wondered myself why we haven't (that or postgres). I can help fix the doc bug, but can you share a little more about how you'd be deploying this? docker?\n. This is a documentation bug. Twitter doesn't use cassandra anymore, and regardless, we should bring this up to speed.\nconfigKey=dev is a specific anormdb setup that uses file sharing. We could quite easily add a mysql config based on a jdbc connect string. I've wondered myself why we haven't (that or postgres). I can help fix the doc bug, but can you share a little more about how you'd be deploying this? docker?\n. cassandra remains the most supported, as it is used in docker, etc. as\nEirik mentioned zipkin is its own thing. Feel free to join us!\nhttps://gitter.im/openzipkin/zipkin\n. cassandra remains the most supported, as it is used in docker, etc. as\nEirik mentioned zipkin is its own thing. Feel free to join us!\nhttps://gitter.im/openzipkin/zipkin\n. ps #853 trims SQL databases to a more relevant list, namely drivers you can configure OOB\n. Here's a start. I managed to get it working locally today and updated this: https://github.com/openzipkin/zipkin/tree/master/zipkin-anormdb\nbash\n$ mysql -uroot -e \"create database if not exists zipkin\"\n$ MYSQL_USER=root ./bin/collector mysql\n$ MYSQL_USER=root ./bin/query mysql\n$ ./bin/tracegen\n...\n. PS here's a working docker-compose pointing to amazon RDS. cc @abesto\nyaml\ncollector:\n  image: quay.io/openzipkin/zipkin-collector:1.7.1\n  environment:\n    - MYSQL_HOST=zipkin.lalalalalalala.us-west-2.rds.amazonaws.com\n    - MYSQL_USER=zipkin\n    - MYSQL_PASS=NOT_TELLING\n  entrypoint: /bin/sh\n  command: -c \"java -cp ${ANORMPATCH_CLASSPATH}:/zipkin/zipkin-collector.jar com.twitter.zipkin.collector.Main -f /collector-mysql.scala\"\n  expose:\n    - 9410\n  ports:\n    - 9410:9410\n    - 9900:9900\nquery:\n  image: quay.io/openzipkin/zipkin-query:1.7.1\n  environment:\n    - MYSQL_HOST=zipkin.lalalalalalala.us-west-2.rds.amazonaws.com\n    - MYSQL_USER=zipkin\n    - MYSQL_PASS=NOT_TELLING\n  entrypoint: /bin/sh\n  command: -c \"java -cp ${ANORMPATCH_CLASSPATH}:/zipkin/zipkin-query.jar com.twitter.zipkin.query.Main -f /query-mysql.scala\"\n  expose:\n    - 9411\n  ports:\n    - 9411:9411\n    - 9901:9901\nweb:\n  image: quay.io/openzipkin/zipkin-web:1.7.1\n  links:\n    - query\n  ports:\n    - 8080:8080\n    - 9990:9990\n. Only suggestion I have is that it is probably in the wrong repo. Docs for\ndocker should be in docker-zipkin else we are surely to duplicate efforts.\nRegardless, we can link out to this.\nMake sense?\n. FYI twitter doesn't use any of the affected code.\n. FYI twitter doesn't use any of the affected code.\n. PTAL things are adjusted except @clehene valid feedback on env variable naming. Leaving this way to be consistent with how cassandra is being configured. I really want us to do better when we move from working around ostrich to designing our cli interface in #466\n. PTAL things are adjusted except @clehene valid feedback on env variable naming. Leaving this way to be consistent with how cassandra is being configured. I really want us to do better when we move from working around ostrich to designing our cli interface in #466\n. going to review-after-commit this one.. feel free to tell me things to follow-up on\n. This actually works now\n\n. ps eventhough the thrifts were re-organized, they are completely compatible. Here's an example of running master's web against the endpoints.\n\n. you're right.. it can get flakey. should we change the scripts to package (not via clean so it is faster) and launch the jar directly?\n. fwiw.. if ever I run into a problem, I repeat after a ./gradlew clean \n. I don't think we are going to change the dev instructions right now, as they work for most people\n. I don't think we are going to change the dev instructions right now, as they work for most people\n. Looks like an issue in the cassandra layer\n. please verify!\n. please verify!\n. this needs to be reverted once jcenter is back https://github.com/openzipkin/docker-zipkin/commit/60ce238de4f9c7bcc4dca5f81df08235b9366c8c\n. https://bintray.com/openzipkin/maven/zipkin/view has some of our things.. I'll try to do a release\n. Note, the package used to be https://bintray.com/openzipkin/zipkin/zipkin and is now https://bintray.com/openzipkin/maven/zipkin\nHopefully, the scripts still work and all that's needed is an update to the release doc, with the right url to click for maven central sync.\nhttps://github.com/openzipkin/zipkin/blob/master/RELEASE.md\n. changed scripts and docs to use the \"maven\" package instead of the \"zipkin\" one https://github.com/openzipkin/zipkin/commit/fdf1bbe9b8209bc6438fa9462f9c8279574bee84\n. back in business. latest version is 1.6.0\n. thanks for the help. only one nit\n. cc @eirslett @michaelsembwever @gneokleo\n. @gneokleo NP bumped to 1.7.0-SNAPSHOT\n. also updated the description\n. PS I'm offline until Monday. Anyone else can feel free to look at this,\nelse I will then.\n. @jfeltesse-mdsol so I think we still want this to install the schema, even if it is reading from files. (preserving the install=true contract). I'll add work on your commit to do that. \n. ps I think these should be the same file. the only reason the indexes weren't in the default formerly was that index syntax isn't portable.\n. actually I see what you mean. mysql needs a separate step to install the database anyway, and probably prod usage wouldn't want to give it access to change its schema.\n. #705 \n. Thanks for the start! I added polish on the other.\n. I think the point on host being down might be a red herring?\nI get this error working against travis' cassandra after it says it is added.\n[ScalaTest-1] INFO com.datastax.driver.core.Cluster - New Cassandra host /127.0.0.1:9042 added\nCassandraDependencyStoreSpec:\n- getDependencies_insideTheInterval *** FAILED ***\n  com.datastax.driver.core.exceptions.InvalidQueryException: Unable to find compaction strategy class 'org.apache.cassandra.db.compaction.DateTieredCompactionStrategy'\n. I think the point on host being down might be a red herring?\nI get this error working against travis' cassandra after it says it is added.\n[ScalaTest-1] INFO com.datastax.driver.core.Cluster - New Cassandra host /127.0.0.1:9042 added\nCassandraDependencyStoreSpec:\n- getDependencies_insideTheInterval *** FAILED ***\n  com.datastax.driver.core.exceptions.InvalidQueryException: Unable to find compaction strategy class 'org.apache.cassandra.db.compaction.DateTieredCompactionStrategy'\n. I had to use the version of cassandra from our docker files. That said, I'm not knowledgeable enough to know if you can just this compaction class in a different distro.\nbash\ncurl -SL http://downloads.datastax.com/community/dsc-cassandra-2.2.1-bin.tar.gz | tar xz && dsc-cassandra-2.2.1/bin/cassandra\n. I had to use the version of cassandra from our docker files. That said, I'm not knowledgeable enough to know if you can just this compaction class in a different distro.\nbash\ncurl -SL http://downloads.datastax.com/community/dsc-cassandra-2.2.1-bin.tar.gz | tar xz && dsc-cassandra-2.2.1/bin/cassandra\n. @danchia it always happens if you use the default travis cassandra service\n. @danchia it always happens if you use the default travis cassandra service\n. option 3.. ask travis to update?\n. option 3.. ask travis to update?\n. most relevant travis issue: https://github.com/travis-ci/travis-ci/issues/3952\n. Here's the latest workaround:\n``` yaml\nTrusty Beta requires sudo per https://docs.travis-ci.com/user/ci-environment/\nsudo: required\ndist: trusty\nManually install cassandra until https://github.com/travis-ci/travis-ci/issues/3952\nbefore_install:\n  - curl -SL http://downloads.datastax.com/community/dsc-cassandra-2.2.3-bin.tar.gz | tar xz\n  - dsc-cassandra-*/bin/cassandra > /dev/null\n```\n. love it except for the stats. if we want, we can start a microbenchmark around compression time, otherwise, I'd leave it out.\n. PS this merged after a minor version increment\n. @wadey this one's for you\n. I agree, except not as unit tests, but microbenchmarks (JMH).\n. cc @kristofa @yurishkuro\n. ok much cleaner impl now, based on advise from @kristofa and feedback from @yurishkuro \nzipkin traces now appear as services \"zipkin-web\" and \"zipkin-query\"\n. Tested with:\n``` bash\n$ bin/collector\n$ SCRIBE_PORT=9410 bin/query\n$ SCRIBE_PORT=9410 bin/web\n$ curl -s localhost:8080/api/services\nnow see services pop up!\n$ curl -s localhost:8080/api/services\n[\"zipkin-query\",\"zipkin-web\"]\n```\n. cc @dsyer @jamescway @spencergibb @jfeltesse-mdsol I'm not sure this is actually causing problems in real life, but will look into it later.\n. tested with local anormdb both dev and mysql mode. no more log clutter, but spans still log.\n. neat idea\n. looks like it is worth a shot. thanks!\n. Note this bumps version\n. cc @spencergibb @wadey @kristofa \n. Here's some feedback on this topic.\nFrom @kristofa \nAbout duration for an annotation. This can be used to track duration of certain parts of your app. For example I had the idea of submitting an annotation with duration for measuring Json serialisation time. The annotation is displayed in Zipkin-web on the Span at its time stamp. Unfortunately the actual duration is not displayed in Zipkin-web. That's why in Brave I added the duration, if specified, to the annotation name.\nFrom @yurishkuro \nI had a meeting today with our mobile team, they are interested in using Zipkin tracing for measuring timing of various functions within the app, ranging across time to open a controller, time to resolve DNS, time to call a service, etc. Not all of these use cases fit directly into Zipkin RPC model, some are more like stages of an operation rather than sub-spans. An annotation's duration might be an interesting one to use.\nI'd suggest that if folks want to use duration, then let's evaluate it against real use cases. You may find overlaps with sub-service spans, normal metrics, or even find that duration would work better on BinaryAnnotation (which has both key and value). Duration certainly does seem handy, but we do need to face the facts that this is a broken and confusing part of zipkin.\nLeaving something handy, but broke in zipkin is worse than removing it, IMHO. Decent use cases, tests and documentation would be the only way we could prevent repeating status quo, which is broken and has led to curious implementations and bugs in an unusable feature.\n. Thinking about alternatives here, too. One thing we've tossed around is having a common data model for annotations. Let's take the case of GC, for example.\nGC happens to be something that isn't caused by the span, it is metadata present, which could help explain timing, but it could be far longer than the span itself and hint at a visualization concern. Nevertheless, let's consider how we might do this.\nAs opposed to an Annotation, which has a timestamp, and a name, but no value like this:\nvalue=Gc(10,0.PSScavenge,2015-09-17 12:57:12 +0000,525.milliseconds+970.microseconds)\nduration=525970\nWhat if this were a BinaryAnnotation which doesn't have a timestamp, but does have a value:\nkey=gc.durationMillis\ntype=I32\nvalue=525970\nSpeaking with @dsyer here, this seems to make more sense, as you can query against it, and its namespace explains itself. Moreover, it is ancillary information to explain the span (ex. it isn't a sub-span, or an activity directly caused by it)\n. Merging as I think @kristofa and @yurishkuro are on board:\n@kristofa \nYes. I agree. You can also track such metrics with a regular monitoring system and graph them, calculate percentiles etc.\n@yurishkuro\n+1 for local span. As alternative to duration, I think it's more structured, and can be rendered nicer. ... kill the durations, kill zem all\n. See also https://issues.apache.org/jira/browse/HTRACE-260\n. @mzagar @eirslett either of you have some cycles to help with this by chance?\n. Great. I suspect this won't impact you if you are only working in\nJavaScript layer. But just in case:\nhttps://github.com/openzipkin/zipkin/pull/728\n. Great. I suspect this won't impact you if you are only working in\nJavaScript layer. But just in case:\nhttps://github.com/openzipkin/zipkin/pull/728\n. so realizing that the getServices isn't actually called by the javascript yet. Also, I noticed a special case. WDYT about this?\nif services.size < 3 don't cache\notherwise set TTL on the http response to 30 seconds?\n. Honestly I don't know what's the right value.. there's a special case of\nbootstrapping, which I think the minimum count would cover.. Perhaps 5m\nafter that is a decent value. Probably only way to find out is to spike it!\nThanks for the offer for the javascript change! Note that ideally we'd\nremove the call from Handlers.scala while at it.\n. @mzagar \"/api/services\" can be put back in.. sure. We just want to remove the call in the Handlers.scala side once that's done.\nPS we had a chat on https://gitter.im/openzipkin/zipkin about wanting to do caching at the http-level, addressed by the browser. Browsers know how to deal with cache headers w/o creating special-caches in scala :) Best way forward is to do as much as possible to get the UI javascript-only. Put another way, a tactical scala cache just for services would probably end up as tech debt.\n. @yurishkuro @eirslett let's continue the chat from https://github.com/openzipkin/zipkin/pull/983#discussion_r53009101 here, as the history is here\n. To summarize status quo, the query server sends back a cache header when there are more than 3 services present:\nscala\nassert(response.headerMap.get(\"Cache-Control\") == Some(\"max-age=300, must-revalidate\"))\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-query/src/test/scala/com/twitter/zipkin/query/ZipkinQueryServerFeatureTest.scala#L167\nThe actual age is assigned via the flag \"zipkin.queryService.servicesMaxAge\", which is incidentally not currently mapped as an environment variable.\nThe zipkin-web server should forward this header all the way to javascript (or any other caller). I suspect if it wasn't, @mzagar would have complained :P However, we have no test on zipkin-web to verify forwarding doesn't lose the header. The main reason is that zipkin-web is an older framework than finatra, and doesn't support feature tests.\n. PS something Eirik mentioned was interesting.\nIf 100s of service names strings are that slow, and for some reason HTTP cache headers aren't an option, a decorating SpanStore which caches in memory would be something those doing custom builds could add, if the middleware itself doesn't have a caching feature.\n. > We could add a cache layer with an environment variable to set TTL\n\n(default to 0 seconds, which disables the cache?)\nIdeally, people shouldn't have to do custom builds, if we can avoid it?\nI don't really want to do anything on this :) but the practical issue on\ncassandra remains, so happy to discuss.\n\nI really think we should get most mileage out of the other cache setting we\nhave now (cache-control). If we find this is a fools errand, I'd prefer we\nremove it and make a different parameter that works. one in-one out.\nIn general I don't like the idea of doing in-memory caching in the query\nserver. Guava and similar caches add more edge cases to the code, and in\nthis case, it is one api that sends very little data. What happens if\nsomehow spanNames degrades in performance.. do we need another env property?\n. > We already did that a few months ago, and I am still occasionally seeing\n\nthe dropdown staying blank for a second or more after page loading. I feel\nit's a combination of transferring large JSON (via two hops due to query\nservice running separately) and DOM creation altogether result in this\nvisible latency. It just seems like the wrong design that the UI needs to\nkeep re-loading this large list. Soon our list of services will be\napproaching a 1000.\nThanks for the updating us on things tried so far, @yurishkuro!\n\nSo, there are plenty of control and monitoring UIs with relatively stable things like buckets, application lists or tenants. I doubt that the supporting UIs require modification to the services they serve. Ex I doubt someone changed S3 bucket api to cache in support of AWS console, although S3 bucket list is quite a fast api anyway! I tend to see refresh buttons on UIs.. but I'm sure there are other ways, too (ex buddy servers etc)\nIs there a way we can solve this lag problem without affecting the query server directly? I'd prefer us to not have the maintenance of special-casing certain endpoints due to rendering delays, if at all possible.\n. @mansu are you saying that your browsers aren't honoring the cache-control headers? or that you feel the timeout is too short?\nBear in mind that the UI is pure javascript and hits a normal http api. In other words, cache headers can also be affected by nginx or similar normal web tier tech.\nhttps://github.com/openzipkin/docker-zipkin/blob/master/zipkin-ui/nginx.conf\nex. here's a caching option https://www.nginx.com/resources/wiki/start/topics/examples/reverseproxycachingexample/. @mansu lol it is already configurable, just not documented\nHere's the default, which is in seconds.\nzipkin.query.names-max-age:300\nThis value can be overridden by system properties or any other alternative supported by Spring Boot. For example, you can upper-case it as an ENV variable. ZIPKIN_QUERY_NAMES_MAX_AGE=10000000. cc @dsyer @jfeltesse-mdsol \n. I think this is resolved, now. https://github.com/openzipkin/zipkin/commit/47f48debaa4e083efdee057d6291a7744afd7de4\n. I think this is resolved, now. https://github.com/openzipkin/zipkin/commit/47f48debaa4e083efdee057d6291a7744afd7de4\n. To answer your question, there's no way to do this from the server side.\nThe sting matching is exact both on key and value.\nIt might be possible to filter in the JavaScript though.\n. I think you might misunderstand me, I'm tossing out a workaround that\ndoesn't imply teaching storage systems regex or similar.\nWe should still keep good query in mind, and i think ES is something we\nshould support, too. Let's discuss this at the working group? Surely others\nhave this in mind, too.\nAbout the workaround, which might still be invalid even how I see it.\nThe current api filters with a limit, right? If you search for traces and\nare already filtering by service, name, timestamp, key.. I don't know what\nnumber you'd have to place into limit to get 10GB of data back. ;)\nLet's say you had a good query, just fuzzy on one point. Ex you already\nhave http response code in there and you chose to return 100 results whose\nkey are X but value Y isn't something you are sure about.\nA workaround could be to allow a regex refine client-side which could\nfilter the results you asked for. This is a workaround, but easier than\ntrying to get all impls to do full text search ;)\n. ps here's what I was talking about wrt a real solution.. feel free to add desires here: https://docs.google.com/document/d/13KPpzl4WkawrLYddN6rtqDpuqSs0Yw1vszgWORyrH1o/edit\n. there was a mention about more powerful query with elasticsearch. this is an FYI that elasticsearch is nearing completion here: https://github.com/openzipkin/zipkin-java/pull/107\n. @teedilo are you suggesting doing such a query in ES is impossible? or that it is arduous? I've no doubt the latter is true, but I'd be surprised if the former was.\nFor example, nested queries look something like this.. https://gist.github.com/adriancole/3ed301de18b75ddd415e4a8a4b4c7cca. @teedilo are you suggesting doing such a query in ES is impossible? or that it is arduous? I've no doubt the latter is true, but I'd be surprised if the former was.\nFor example, nested queries look something like this.. https://gist.github.com/adriancole/3ed301de18b75ddd415e4a8a4b4c7cca. LGTM. thanks!\n. \nStill works! i.e. the darkness of the line corresponds to call count.\n. @liyichao this keeps the same query capabilities as exist right now. That said, we should be able to change that more easily in the future with a json api (which actually has tests).\nI saw from gitter you'd like below.. I'll use this syntax for what's currently supported!\n```\nparams:\nstart_time -- optional: default to include all traces\nend_time -- optional: default to now\nannotation -- optional: annotation query\nservice_name -- optional\nmethod_name -- optional\n```\n. @liyichao this keeps the same query capabilities as exist right now. That said, we should be able to change that more easily in the future with a json api (which actually has tests).\nI saw from gitter you'd like below.. I'll use this syntax for what's currently supported!\n```\nparams:\nstart_time -- optional: default to include all traces\nend_time -- optional: default to now\nannotation -- optional: annotation query\nservice_name -- optional\nmethod_name -- optional\n```\n. @liyichao FYI I've left the query syntax as-is.. just changed the plumbing from underneath it. Once this is in, let's have the conversation and/or feel free to raise a PR to revise the api.\n. @liyichao FYI I've left the query syntax as-is.. just changed the plumbing from underneath it. Once this is in, let's have the conversation and/or feel free to raise a PR to revise the api.\n. PTAL I think this is a good place to progress from\n. PTAL I think this is a good place to progress from\n. summary of the implementation is that we moved the json-converting queries from zipkin-web to zipkin-query and forwarded ones from zipkin-dependencies (as the javascript calls the api directly). The framework used is Finatra 2, which is far more supportable than Ostrich.\nI've ideas for follow-up work, including moving the web server also to Finatra and/or replacing it with a pure javascript app, which could be hosted on the query server (and drop the need for zipkin-web entirely). We can also look at removing ostrich completely from zipkin-query-service, porting the query config scala files to finatra/twitter-inject modules. I hope that this work will help others build on zipkin, and help us finish the tech debt clearing.\n. summary of the implementation is that we moved the json-converting queries from zipkin-web to zipkin-query and forwarded ones from zipkin-dependencies (as the javascript calls the api directly). The framework used is Finatra 2, which is far more supportable than Ostrich.\nI've ideas for follow-up work, including moving the web server also to Finatra and/or replacing it with a pure javascript app, which could be hosted on the query server (and drop the need for zipkin-web entirely). We can also look at removing ostrich completely from zipkin-query-service, porting the query config scala files to finatra/twitter-inject modules. I hope that this work will help others build on zipkin, and help us finish the tech debt clearing.\n. verified the UI operates with no impact\n. verified the UI operates with no impact\n. going to merge on green as dependency store is not wholly implemented, yet, anyway due to https://github.com/openzipkin/zipkin-dependencies-hadoop/issues/2\n. going to merge on green as dependency store is not wholly implemented, yet, anyway due to https://github.com/openzipkin/zipkin-dependencies-hadoop/issues/2\n. Yeah this makes loads of sense. cc @jamescway @clehene\n. Yeah this makes loads of sense. cc @jamescway @clehene\n. Thanks!\n. Thanks!\n. sweet! will merge on green\n. sweet! will merge on green\n. kicked the build\n. kicked the build\n. ps thank you so much.. I didn't connect the dots here and was wondering why the service name picker looked weird!\n. ps thank you so much.. I didn't connect the dots here and was wondering why the service name picker looked weird!\n. this didn't improve things, so rolling it back. Because #706, we'd have to install a custom cassandra. Even after that, the flakes persist. Maybe there's some way to get cassandra to have a higher consistency model when running in single-node mode for tests.\n. this didn't improve things, so rolling it back. Because #706, we'd have to install a custom cassandra. Even after that, the flakes persist. Maybe there's some way to get cassandra to have a higher consistency model when running in single-node mode for tests.\n. nice one!\n. nice one!\n. Makes sense.. I forgot we have this advantage as we have a separate call to get span names.\n. Makes sense.. I forgot we have this advantage as we have a separate call to get span names.\n. your fix is correct. I'll bump the version and cherry-pick it in. thanks!\n. your fix is correct. I'll bump the version and cherry-pick it in. thanks!\n. Do you have a proxy that prevents access to maven.twttr com?\n. Do you have a proxy that prevents access to maven.twttr com?\n. the reason I think it is best to merge this is because:\n- It is is of least surprise, as it matches all other impls\n- It draws into focus where dates are actually used: dependencies\n- timestamp type in CQL doesn't match SQL default microsecond precision\n. the reason I think it is best to merge this is because:\n- It is is of least surprise, as it matches all other impls\n- It draws into focus where dates are actually used: dependencies\n- timestamp type in CQL doesn't match SQL default microsecond precision\n. @rjbhewei this should fix your javac 7 thing.. I compiled locally on javac 7 w/o failure\n. @rjbhewei this should fix your javac 7 thing.. I compiled locally on javac 7 w/o failure\n. we've redundant checks on travis. the first failed due to maven.twttr.biz flaking out, I consider this a pass.\n. we've redundant checks on travis. the first failed due to maven.twttr.biz flaking out, I consider this a pass.\n. tested all backends: cassandra, dev, mysql, redis\n. tested all backends: cassandra, dev, mysql, redis\n. cool. thanks!\n. cool. thanks!\n. @michaelsembwever I'd really like to merge this or some revision of it today. These build failures in cassandra are killing productivity\n. @michaelsembwever I'd really like to merge this or some revision of it today. These build failures in cassandra are killing productivity\n. @michaelsembwever by manual testing, you mean bring up the stack and click around/make traces? I can do that\n. ok ran collector, query, tracegen, web... all with finagle scribe logging.\n. fyi logback 1.1.7 now supports compact3 profile \n. fyi logback 1.1.7 now supports compact3 profile \n. red is because cassandra flaked again. the other green shows that this is mergable as these checks are redundant.\n. red is because cassandra flaked again. the other green shows that this is mergable as these checks are redundant.\n. ps the java project is now in sync with the schema used by anorm\n. Is it broken or inefficient?\n. Regardless, will look into it now, just wondering if the UI is broken and I\nsomehow didn't notice!\n. looking now.. should have a fix shortly. thanks for keeping an eye out!\n. ps wow.. this did happen!\n. ok here's the fix #749\n. Pushing 1.13.3 immediately. Eta 1hr\n. :both_hands:\n. Pushing version 1.13.3 immediately. Will take an hour or so to reach docker\n. Sure will take a look in a bit. Thanks for the help!\n. ps did you verify this works? I can regardless.\n. I don't think we want a configuration parameter for such an edge case as\nmax services before caching.  I literally meant to hard-code it to 3. That\nshould be easy to test as all you need to do is sink spans with more than 3\nservice names (endpoints) in the test.\n. looks good..  mind squashing these commits?\n. closed via https://github.com/openzipkin/zipkin/commit/fd2cadf62fa1e01cc87b79a30f9d675e9e62eeee\nThanks!\n. Can you verify with 1.14.0 and also that there's nothing in the classpath\nexcept the uber jar?\n. I noticed this along the way: https://github.com/openzipkin/zipkin/pull/753\n. I can't reproduce this with 1.13.2 or 1.14.0. However we should merge #753 to squash the error.\nCould your classpath has something besides the collector jar in it?\ncurl -SL https://repo1.maven.org/maven2/io/zipkin/zipkin-collector-service/1.14.0/zipkin-collector-service-1.14.0-all.jar > collector.jar\nKAFKA_ZOOKEEPER=127.0.0.1 CASSANDRA_CONTACT_POINTS=127.0.0.1 java -jar collector.jar -f /collector-cassandra.scala\n. I feel your pain. Glad you found it.\n. This is not a blocker, but this will eliminate clutter in the console output when using kafka or the adaptive sampler\n. ok updated, including backfilled tests for span merges and endTs\n. I hate to say it, but I think we are nearly a point where recreating a UI would be less work than maintaining it.\nGood news: It is possible to update all libraries with the exception of dagre-d3.\nBad news: there's custom trimming involved, and this isn't represented in the bower file. This means I can't update it easily unless we want to check in a lot of junk.\nUgly news:  dagre-d3 is very tangled. If you try to update it, you'll get in a mess between versions of dagre and d3. After a few hours, I was unable to sort problems such as undefined references to functions with requirejs shims. See #772, which says all that work is wasted as we need another means to create sankey or similar diagram.\nNext step:\nIf we don't want to throw away the UI, we need to maintain it far better than we are now. We probably want to stop checking in assets into src/main/resources and using something like bower-installer to install them at build time like other deps.\ncc @abesto @eirslett \n. in case anyone wants to pick this up, here's the diff that works as long as you don't touch anything related to d3.\ndiff\ndiff --git a/zipkin-web/src/main/resources/bower.json b/zipkin-web/src/main/resources/bower.json\nindex acdf7cf..9595949 100644\n--- a/zipkin-web/src/main/resources/bower.json\n+++ b/zipkin-web/src/main/resources/bower.json\n@@ -2,14 +2,14 @@\n   \"name\": \"zipkin\",\n   \"version\": \"0.0.0\",\n   \"dependencies\": {\n-    \"flight\": \"~1.1.0\",\n-    \"requirejs\": \"~2.1.5\",\n-    \"bootstrap\": \"~3.1.1\",\n-    \"bootstrap-datepicker\": \"~1.3.0\",\n-    \"jquery-timeago\": \"~1.3.1\",\n-    \"chosen\": \"https://github.com/harvesthq/chosen/releases/download/v1.1.0/chosen_v1.1.0.zip\",\n-    \"jquery-cookie\": \"~1.4.0\",\n-    \"momentjs\": \"~2.5.1\",\n+    \"flight\": \"~1.5.1\",\n+    \"requirejs\": \"~2.1.20\",\n+    \"bootstrap\": \"~3.3.5\",\n+    \"bootstrap-datepicker\": \"~1.4.1\",\n+    \"jquery-timeago\": \"~1.4.3\",\n+    \"chosen\": \"~1.4.2\",\n+    \"jquery-cookie\": \"~1.4.1\",\n+    \"momentjs\": \"~2.10.6\",\n     \"dagre-d3\": \"~0.2.9\"\n   }\n }\n. in case anyone wants to pick this up, here's the diff that works as long as you don't touch anything related to d3.\ndiff\ndiff --git a/zipkin-web/src/main/resources/bower.json b/zipkin-web/src/main/resources/bower.json\nindex acdf7cf..9595949 100644\n--- a/zipkin-web/src/main/resources/bower.json\n+++ b/zipkin-web/src/main/resources/bower.json\n@@ -2,14 +2,14 @@\n   \"name\": \"zipkin\",\n   \"version\": \"0.0.0\",\n   \"dependencies\": {\n-    \"flight\": \"~1.1.0\",\n-    \"requirejs\": \"~2.1.5\",\n-    \"bootstrap\": \"~3.1.1\",\n-    \"bootstrap-datepicker\": \"~1.3.0\",\n-    \"jquery-timeago\": \"~1.3.1\",\n-    \"chosen\": \"https://github.com/harvesthq/chosen/releases/download/v1.1.0/chosen_v1.1.0.zip\",\n-    \"jquery-cookie\": \"~1.4.0\",\n-    \"momentjs\": \"~2.5.1\",\n+    \"flight\": \"~1.5.1\",\n+    \"requirejs\": \"~2.1.20\",\n+    \"bootstrap\": \"~3.3.5\",\n+    \"bootstrap-datepicker\": \"~1.4.1\",\n+    \"jquery-timeago\": \"~1.4.3\",\n+    \"chosen\": \"~1.4.2\",\n+    \"jquery-cookie\": \"~1.4.1\",\n+    \"momentjs\": \"~2.10.6\",\n     \"dagre-d3\": \"~0.2.9\"\n   }\n }\n. That sounds like a good way to progress. I don't know how that maps into\nsteps to do.. for copy/paste grunts like me.. can you say how to do that\nex. take this part out of bower and move this directory here, change\nmain.js, etc? (or even better PR) :)\n. That sounds like a good way to progress. I don't know how that maps into\nsteps to do.. for copy/paste grunts like me.. can you say how to do that\nex. take this part out of bower and move this directory here, change\nmain.js, etc? (or even better PR) :)\n. well one thing we'd have to consider is that currently, there's a hook in zipkin-web that allows you to monkey with src/main/resources. If we remove that, then we could indeed merge in the assets from 'wherever', defaulting to bower config except the pinned d3 stuff.\n. well one thing we'd have to consider is that currently, there's a hook in zipkin-web that allows you to monkey with src/main/resources. If we remove that, then we could indeed merge in the assets from 'wherever', defaulting to bower config except the pinned d3 stuff.\n. It isn't so much about bower, as an assumption you can write to the path\nassets are at... and libs are under there.\n. It isn't so much about bower, as an assumption you can write to the path\nassets are at... and libs are under there.\n. @eirslett well, I'm just thinking that these flags lift expectations of where assets are stored etc into public api. If we re-organize into pulling bower deps as opposed to storing them in the source tree, we'd break below's contract (maybe for the best).\nval webCacheResources = flag(\"zipkin.web.cacheResources\", false, \"cache static resources and mustache templates\")\n  val webResourcesRoot = flag(\"zipkin.web.resourcesRoot\", \"zipkin-web/src/main/resources\", \"on-disk location of resources\")\n. @eirslett well, I'm just thinking that these flags lift expectations of where assets are stored etc into public api. If we re-organize into pulling bower deps as opposed to storing them in the source tree, we'd break below's contract (maybe for the best).\nval webCacheResources = flag(\"zipkin.web.cacheResources\", false, \"cache static resources and mustache templates\")\n  val webResourcesRoot = flag(\"zipkin.web.resourcesRoot\", \"zipkin-web/src/main/resources\", \"on-disk location of resources\")\n. first step: https://github.com/openzipkin/zipkin/pull/890/files\nonce in, we can give a go at bower (except for the dagre-d3-legacy thing)\n. looks fine. update the README? (zipkin-anormdb/)\n. Np thanks for the help!\n. Looks useful to me.\n. some test failures you might want to look at\n. gracias\n. I think the bug/key insight is that we are inconsistent. Many parts of the\ncode treat span name case insensitively, but some don't. I'm not sure we\nshould be case sensitive, and I don't mind us enforcing lowercase on the\nwire (just as http2 does). However, we should be consistent with the answer\nof if we are case sensitive or not. If we are, that only works for certain\ncharacter sets, so might imply a constraint on the character set of span\nname.\n. My 2p is either we do lowercase on store (which implies searches also\nlowercase on the way in), or we leave it case sensitive (and be consistent\nabout it :P)\n. I love that you fixed this. Since this is quite subtle.. can you put an example query in ZipkinQueryServerFeatureTest, to ensure the encoding rules are known?\n. I'm gonna merge as we need to do a release. if you don't mind, add a test to ZipkinQueryServerFeatureTest if you don't forget :) The other coverage is great.\n. hmm \"MAX(a_timestamp) as end_ts\" is called by getTraceIdsByName. I'll\nadd a test to see what happens if you call that and there's no\nendpoint with the corresponding service name..\n. hmm \"MAX(a_timestamp) as end_ts\" is called by getTraceIdsByName. I'll\nadd a test to see what happens if you call that and there's no\nendpoint with the corresponding service name..\n. So empty data test passed. only guess is that there's an annotation\nmissing data in the a_timestamp field. Can you do a select null or\nempty on that? Should be impossible, but worth a try.\n. So empty data test passed. only guess is that there's an annotation\nmissing data in the a_timestamp field. Can you do a select null or\nempty on that? Should be impossible, but worth a try.\n. Is this reproducible without the test apps? Ex. By just clicking refresh on\nzipkin, are you also missing cs annotations? This should be testable I\nthink.. Maybe by curl and then curl against the api's trace endpoint to\nverify.\n. Is this reproducible without the test apps? Ex. By just clicking refresh on\nzipkin, are you also missing cs annotations? This should be testable I\nthink.. Maybe by curl and then curl against the api's trace endpoint to\nverify.\n. mind adjusting and adding a test like this to SpanStoreSpec that shows what expectations are? I'm not saying that the below test is what we want, just an example of how to write one.\n```\n  @Test def spanNamesGoLowercase() {\n    result(store(Seq(span1.copy(name = \"Yak\"))))\nresult(store.getSpanNames(\"service\")) should be(List(\"yak\"))\nresult(store.getTraces(QueryRequest(\"service\", Some(\"Yak\")))) should be(empty)\nresult(store.getTraces(QueryRequest(\"service\", Some(\"yak\")))) should be(\n  Seq(span1.copy(name = \"yak\"))\n)\n\n}\n```\n. mind adjusting and adding a test like this to SpanStoreSpec that shows what expectations are? I'm not saying that the below test is what we want, just an example of how to write one.\n```\n  @Test def spanNamesGoLowercase() {\n    result(store(Seq(span1.copy(name = \"Yak\"))))\nresult(store.getSpanNames(\"service\")) should be(List(\"yak\"))\nresult(store.getTraces(QueryRequest(\"service\", Some(\"Yak\")))) should be(empty)\nresult(store.getTraces(QueryRequest(\"service\", Some(\"yak\")))) should be(\n  Seq(span1.copy(name = \"yak\"))\n)\n\n}\n```\n. Thanks! Lg on green\nNit:\nLets avoid forcing navigation to see what test input thats only used once is. I know many tests in zipkin are like that, but it is bad practice as it adds unnecessary and distracting indirection. In some cases, for example, I've found misnamed variables hiding bugs. Inlining eliminates that problem space.\n. Thanks! Lg on green\nNit:\nLets avoid forcing navigation to see what test input thats only used once is. I know many tests in zipkin are like that, but it is bad practice as it adds unnecessary and distracting indirection. In some cases, for example, I've found misnamed variables hiding bugs. Inlining eliminates that problem space.\n. To add more to my Nit (digging a deeper hole):\nI should say inlining or locally declaring input where only used once. Ex. A misnamed variable is obvious when it is declared and used within the same few lines. Your variable is fine, just people do refactors and break things. Locals or inlining is great as say a future PR breaks this, you can usually see in the diff its context, whereas if the input is declared at the top of the file, you cant.\n. To add more to my Nit (digging a deeper hole):\nI should say inlining or locally declaring input where only used once. Ex. A misnamed variable is obvious when it is declared and used within the same few lines. Your variable is fine, just people do refactors and break things. Locals or inlining is great as say a future PR breaks this, you can usually see in the diff its context, whereas if the input is declared at the top of the file, you cant.\n. Lemme know if I lost you. Apologies if I over communicated. I don't mind\nhelping to get this merged.\n. sure.. kicking now\n. sure you are pushed (and doing ./gradlew clean check from root)? test fail looks legit:\n*** 1 TEST FAILED ***\nAnormSpanStoreSpec:\n- spanNamesGoLowercase *** FAILED ***\n  org.scalatest.exceptions.TestFailedException: List(\"SPAN_NAME\") was not equal to List(\"span_name\") (SpanStoreSpec.scala:226)\n. Looks simple enough. Can you add a note to zipkinCore.thrift under span name that these are lowercase? I know it is separate issues, but extremely similar is the service name. Is that easy enough to address in this PR?\n. Looks simple enough. Can you add a note to zipkinCore.thrift under span name that these are lowercase? I know it is separate issues, but extremely similar is the service name. Is that easy enough to address in this PR?\n. thanks tons\n. thanks tons\n. Ship libraries? Ex. if there's no custom code, then there's no reason to javac. Maybe upstream or find some internal build pipeline that permits use of javac 8. Often there's at least an experimental build for a JDK as old as 8 is.\n. Ship libraries? Ex. if there's no custom code, then there's no reason to javac. Maybe upstream or find some internal build pipeline that permits use of javac 8. Often there's at least an experimental build for a JDK as old as 8 is.\n. For example, I think you guys needed kafka, and the upstream collector now supports it. Is there anything else needed to flip over? once there's nothing left, then there's no code you'd need to run javac 8 against.\nunless.. you are saying your servers actually can't run jre 8, either, which makes the solution unfortunately simpler, which is use a version before the flip and lean elbows on whatever's blocking the ability to coexist with JRE 8.\n. For example, I think you guys needed kafka, and the upstream collector now supports it. Is there anything else needed to flip over? once there's nothing left, then there's no code you'd need to run javac 8 against.\nunless.. you are saying your servers actually can't run jre 8, either, which makes the solution unfortunately simpler, which is use a version before the flip and lean elbows on whatever's blocking the ability to coexist with JRE 8.\n. starting this now\n. starting this now\n. @cacoco this was super unintuitive. Does finatra have a way to filter paths that are traced?\n. @cacoco this was super unintuitive. Does finatra have a way to filter paths that are traced?\n. Naver sends a nextSpanId\nhttps://github.com/naver/pinpoint/blob/master/thrift/src/main/thrift/Trace.thrift#L51\nI don't think htrace address sequence explicitly (independent of time)\nhttps://github.com/apache/incubator-htrace/blob/master/htrace-hbase/src/main/protobuf/Span.proto#L29\nThere's more on the topic of sequence here:\nhttps://docs.google.com/document/d/15wUGaoCnd91EEpy5RhdWU4DwsHBYeP0wGtfFjMLYt6A/edit\n. well both of your points are valid. It was a guess that nextSpanId was\nescalated to a real type member (as opposed to any given annotation) for\nthe purpose of sequencing. I might be wrong. the async stuff, however, is\nmuch more clearly about sequence. @Xylus can you verify?\n. The use case that leads to this is frame-granularity tracing, like every\nmethod call or cherry-picked ones. Without explicit ordering, you may not\nbe able to reconstruct the call graph exactly how it existed using only\ntime, particularly as timestamps are really millis *1000 in a lot of\ninstrumentation. Feel free to challenge this assumption.\n. @yurishkuro good point. Moved to https://docs.google.com/document/d/1ixxEs9TvhiGjJObGbRSPhSna3zHdadoUTQIZ5JKgLzU/edit\n. @Xylus great detail. I think I understood this completely?!\n@michaelsembwever in brave, finagle, etc, how do you think we can get to micros precision? Do share your tricks!\n. Thanks for reporting!\nIf this is easy for you to repeat, can you please include the json of the\ntrace you are mentioning?\nex.\ncurl -s 192.168.99.100:9411/api/v1/traces/899ad63733ca9560 |jq .\n. we like perfect, we'll work on the less than :P\n. ACK this is a problem. we can't tell which the service is the parent without looking at the annotations. annotations within a span have mixed endpoints, so it is ambiguous.\nFor example, a binary annotation of server address, or a normal annotation of server receive or server send would indicate who the server is. similarly with client.\n. The zipkin-web terminates all traffic from the UI. One way to reduce surface area is to only expose zipkin-web. There's work to do in order to lock that down, and bear in mind I've not seen all things possible in finagle-land.\nzipkin-web is a TwitterServer, and very similar to Finatra. Most patterns of Finatra could be applied to zipkin-web. To do authentication, you'd likely end up writing a filter or explicit code. I've found two examples:  finatra-angular-example and finatra-tweet-example\nThe subject of authorization would be slightly more complex. There's a mix of responsibilities inside zipkin-web, presumably folks optimizing for how much data is going to the browser, but honestly, I've no idea. This mix presents itself as some javascript components calling json endpoints, and mustache template rendering doing other things. This scatters the authorization concern a bit, as if there was only a \"normal\" json query api, you could just secure that. For example, to authorize by service, code would likely need to wrap the underlying SpanStore object as that's the choke-point for data.\nThe work involved in unwinding this problem is essentially refactoring the javascript UI to only use the query server's api. Authorization code could be much simpler, even at the http abstraction. That said, there would be a caveat, which is that you need to inspect the json to ensure a call to getTraces (by id) doesn't accidentally return data the user shouldn't see. That's because it isn't likely all storage implementations will be able to support transparent authorization.\nRegardless, once the UI only makes calls to a narrowly defined set of endpoints, you'd have relatively straight-forward authorization. This would be whatever you can code in finatra, or any number of established patterns supported by Spring Boot, such as OAuth (as zipkin-java is a boot app)\n. Re-opening for @devinsba who has restarted an investigation on this (#1537). I'm adding some notes to help frame thing, and thankful for the help.\nThe current topic is custom auth (also how any instrumentation might produce it). This will be easier for something like sleuth (as it uses the same language and library), so particularly important to consider non-jvm instrumentation, too.\nTo frame things, I think the idea is to understand what's sustainable and sensible to support wrt auth either directly (some auth module that covers all http), or indirectly (swap out http ui or collector for something secured). This is important as folks have different security contexts (some api key, some basic auth, some oauth). Also, afiak, the only http span reporters that support auth only do so because their underlying libraries do.\nTo set this up for success, I'm pinging some folks who have custom http collectors for insight on what auth means for them (wrt at least collector, but perhaps ui if willing to share)\n@trustin and @anuraaga (armeria collector)\n@marcingrzejszczak (some folks do custom auth in sleuth, presumably via custom servers?)\n@kevinmdavis @denyska (stackdriver zipkin)\n@nicmunroe riposte\n@pavolloffay hawkular\nthen other folks who have requested auth, but not on this thread, yet (at least @jquatier)\nOne thing of note is that while you can add transports to the zipkin-server (like sqs, etc), you currently cannot modify the http configuration. I think this is an artificial constraint, which we can likely change. For example, zipkin-ui autoconfiguration exists independently, so it should be possible to change UI authorization without affecting the http transport. However, it should also be possible to change the http transport for reasons including auth (ex performance which I know others have done in the past).\n. Re-opening for @devinsba who has restarted an investigation on this (#1537). I'm adding some notes to help frame thing, and thankful for the help.\nThe current topic is custom auth (also how any instrumentation might produce it). This will be easier for something like sleuth (as it uses the same language and library), so particularly important to consider non-jvm instrumentation, too.\nTo frame things, I think the idea is to understand what's sustainable and sensible to support wrt auth either directly (some auth module that covers all http), or indirectly (swap out http ui or collector for something secured). This is important as folks have different security contexts (some api key, some basic auth, some oauth). Also, afiak, the only http span reporters that support auth only do so because their underlying libraries do.\nTo set this up for success, I'm pinging some folks who have custom http collectors for insight on what auth means for them (wrt at least collector, but perhaps ui if willing to share)\n@trustin and @anuraaga (armeria collector)\n@marcingrzejszczak (some folks do custom auth in sleuth, presumably via custom servers?)\n@kevinmdavis @denyska (stackdriver zipkin)\n@nicmunroe riposte\n@pavolloffay hawkular\nthen other folks who have requested auth, but not on this thread, yet (at least @jquatier)\nOne thing of note is that while you can add transports to the zipkin-server (like sqs, etc), you currently cannot modify the http configuration. I think this is an artificial constraint, which we can likely change. For example, zipkin-ui autoconfiguration exists independently, so it should be possible to change UI authorization without affecting the http transport. However, it should also be possible to change the http transport for reasons including auth (ex performance which I know others have done in the past).\n. FYI, (and not about SPIFFE, rather related to a gitter chat)\nWe all have different ideas on what authn/authz needs to be, I'd welcome folks to describe what they must have in order for this to be solved. Bear in mind that this may impact the http collector endpoint, but not others (ex kafka and rabbit have their own auth mechanisms). It is important to enumerate because we might end up otherwise burning time on a solution no-one can use and no sender supports.\nTL;DR; mention what you need or want, and how if at all it impacts your apps (ex do your apps need to share a token or key or password? who manages these?). Mention any workarounds you use today, if you can. Appreciated!. For those who really really want basic auth and no proxy, here's a way to bolt-on BASIC auth to an existing zipkin server without modifying it.\nOnce you have it together, the below shows how to change password (though there's an example too) https://docs.spring.io/spring-boot/docs/2.1.0.RELEASE/reference/htmlsingle/#boot-features-security\nThis requires that you have maven installed. You don't need to do any custom code as it is 100% packaging.\n```bash\nget normal zipkin server\n$ curl -sSL https://zipkin.io/quickstart.sh | bash -s\nbuild the security module\n$ mvn clean install\nrename the jar so it is easier\n$ mv ./target/security-1.0-SNAPSHOT-module.jar security.jar\nstart zipkin which now has BASIC auth on all endpoints\n$ java -Dloader.path='security.jar,security.jar!/lib' -cp zipkin.jar  org.springframework.boot.loader.PropertiesLauncher --spring.security.user.name=zipkin --spring.security.user.password=pipkin\n```\nHere's the only file you need:\n```xml\n\n4.0.0\nio.zipkin.custom\nsecurity\n1.0-SNAPSHOT\nExample module that adds basic auth to an existing Zipkin\n\n make sure this matches zipkin-server's spring boot version \n2.1.1.RELEASE\n\n\n\n\n This makes sure versions are aligned properly \norg.springframework.boot\nspring-boot-dependencies\n${spring-boot.version}\npom\nimport\n\n\n\n\n this is the thing that adds basic auth \n\norg.springframework.boot\nspring-boot-starter-security\n${spring-boot.version}\n\n zipkin already has this \n\norg.springframework.boot\nspring-boot-starter\n\n\norg.springframework\n*\n\n\n\n\n\n\n\nmaven-jar-plugin\n\n\ndefault-jar\n\n\n prevents huge pom file from also being added to the jar under META-INF/maven \nfalse\n\n\n\njar\n\n\n\n\n\norg.springframework.boot\nspring-boot-maven-plugin\n${spring-boot.version}\n\n\n\nrepackage\n\n\n\n\n\ncustom\n\nmodule\n\n\n\nio.zipkin.layout\nzipkin-layout-factory\n0.0.4\n\n\n\n\n\n\n```. For those who really really want basic auth and no proxy, here's a way to bolt-on BASIC auth to an existing zipkin server without modifying it.\nOnce you have it together, the below shows how to change password (though there's an example too) https://docs.spring.io/spring-boot/docs/2.1.0.RELEASE/reference/htmlsingle/#boot-features-security\nThis requires that you have maven installed. You don't need to do any custom code as it is 100% packaging.\n```bash\nget normal zipkin server\n$ curl -sSL https://zipkin.io/quickstart.sh | bash -s\nbuild the security module\n$ mvn clean install\nrename the jar so it is easier\n$ mv ./target/security-1.0-SNAPSHOT-module.jar security.jar\nstart zipkin which now has BASIC auth on all endpoints\n$ java -Dloader.path='security.jar,security.jar!/lib' -cp zipkin.jar  org.springframework.boot.loader.PropertiesLauncher --spring.security.user.name=zipkin --spring.security.user.password=pipkin\n```\nHere's the only file you need:\n```xml\n\n4.0.0\nio.zipkin.custom\nsecurity\n1.0-SNAPSHOT\nExample module that adds basic auth to an existing Zipkin\n\n make sure this matches zipkin-server's spring boot version \n2.1.1.RELEASE\n\n\n\n\n This makes sure versions are aligned properly \norg.springframework.boot\nspring-boot-dependencies\n${spring-boot.version}\npom\nimport\n\n\n\n\n this is the thing that adds basic auth \n\norg.springframework.boot\nspring-boot-starter-security\n${spring-boot.version}\n\n zipkin already has this \n\norg.springframework.boot\nspring-boot-starter\n\n\norg.springframework\n*\n\n\n\n\n\n\n\nmaven-jar-plugin\n\n\ndefault-jar\n\n\n prevents huge pom file from also being added to the jar under META-INF/maven \nfalse\n\n\n\njar\n\n\n\n\n\norg.springframework.boot\nspring-boot-maven-plugin\n${spring-boot.version}\n\n\n\nrepackage\n\n\n\n\n\ncustom\n\nmodule\n\n\n\nio.zipkin.layout\nzipkin-layout-factory\n0.0.4\n\n\n\n\n\n\n```. the above workaround was ported from earlier work by @devinsba .. thanks! https://github.com/openzipkin/zipkin/pull/1537. RestTemplate is one of many senders. people customizing post likely use the\nOkHttp one from the zipkin-reporter-java project\nAlso, we are moving the project to Armeria which has a customizer mechanism\nfor changing the server. When done, this could be used.\nIn any case, like Eirik mentioned, you can use a proxy or something like\nhotels.com pitchfork to separate out the collector tier\nOn Tue, Jan 15, 2019, 3:25 AM jbedalov <notifications@github.com wrote:\n\n@adriancole https://github.com/adriancole, we tested the auth\nconfigurations you recommended above and they work nicely but for only the\nGET APIs and WebUi... there are a issues with securing the POST APIs.\n1.\nThe zipkin server's POST APIs use undertow, so they are missed in the\n   Spring Security configuration\n   https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/java/zipkin2/server/internal/ZipkinHttpCollector.java\n   2.\nClients cannot inject any auth headers, they only add content type\n   https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin2/sender/RestTemplateSender.java#L125\n   .\n   3.\nZipkinRestTemplate is package private so it would be a bad code smell\n   to inject auth headers there.\nJust a heads up for those interested in pursuing an auth approach. For the\nmost part, there's no way to lock down POST without some sort of private\nnetwork approach. Maybe the other transports Kafka or Scribe may have a way\nto auth writing.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/782#issuecomment-454176525,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61yyNV-uGkR_csbR10niuXB01y_kAks5vDPzYgaJpZM4GSjgr\n.\n. RestTemplate is one of many senders. people customizing post likely use the\nOkHttp one from the zipkin-reporter-java project\n\nAlso, we are moving the project to Armeria which has a customizer mechanism\nfor changing the server. When done, this could be used.\nIn any case, like Eirik mentioned, you can use a proxy or something like\nhotels.com pitchfork to separate out the collector tier\nOn Tue, Jan 15, 2019, 3:25 AM jbedalov <notifications@github.com wrote:\n\n@adriancole https://github.com/adriancole, we tested the auth\nconfigurations you recommended above and they work nicely but for only the\nGET APIs and WebUi... there are a issues with securing the POST APIs.\n1.\nThe zipkin server's POST APIs use undertow, so they are missed in the\n   Spring Security configuration\n   https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/java/zipkin2/server/internal/ZipkinHttpCollector.java\n   2.\nClients cannot inject any auth headers, they only add content type\n   https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin2/sender/RestTemplateSender.java#L125\n   .\n   3.\nZipkinRestTemplate is package private so it would be a bad code smell\n   to inject auth headers there.\nJust a heads up for those interested in pursuing an auth approach. For the\nmost part, there's no way to lock down POST without some sort of private\nnetwork approach. Maybe the other transports Kafka or Scribe may have a way\nto auth writing.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/782#issuecomment-454176525,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61yyNV-uGkR_csbR10niuXB01y_kAks5vDPzYgaJpZM4GSjgr\n.\n. cc @adrianco I'll ping back when this is in a release\n. This is the scala implementation of https://github.com/openzipkin/zipkin-java/pull/35\n. docker release is out. if you clone https://github.com/openzipkin/docker-zipkin, you can do the same spigo instructions as above after running docker-compose up. Only change is that you need to use your docker-machine's IP as opposed to localhost to POST the spans. \n. can you describe how you are sending this? Ex. similar to this?\nhttps://github.com/openzipkin/brave/issues/102\n. maybe related to #783\n. thanks for reporting! sounds like a valid problem. personally, I'm offline\nfor a week or so. If anyone else can help with this, please do!\n. @liyichao If you'd like to have a stab at resolving your issue, this is open source: we'd welcome the hand.\n. @liyichao wrt \"why the graph initially collapsed\" the collapsed-ness has to do with the serviceName query. For example, if the serviceName parameter corresponds to the root span, it will show up collapsed. If it doesn't it will expand to the child that contains it. Feel free to spin off another issue on this if you feel that behavior isn't ideal cc @yurishkuro \n. wrt your original concern (about end time resetting) I see that this is certainly the case.\n\nFor example, if you start from scratch, ex. where there's no query params in the URL, you can select any time, and the page will return with the correct endTs query parameter.\nIf you already have an endTs query parameter, changing the endtime date or time fields has no effect.\nThe work around is to clear the URL when you want to change. I agree this is a bug.\n. Sounds odd but fixable. The connection pool should be a Singleton.\n. So you would have connections both from the collector and the query\nprocess, until we switch instrumentation to HTTP only. I forget the count\nof each pool. Will have a look\nOn Oct 22, 2015 7:12 AM, \"Jordi Polo Carres\" notifications@github.com\nwrote:\n\nStrange enough, it kind of hovered over 100 and stayed stable.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/787#issuecomment-150236953.\n. So the DB Pool is set to max 32 connections, so if you have a single query and a single collector, that should be 64 (potentially stale) connections https://github.com/openzipkin/zipkin/blob/master/zipkin-anormdb/src/main/scala/com/twitter/zipkin/storage/anormdb/DB.scala#L46\n\nThere are ways to tune this to prune out connections, but fastest way to control connection count is to obviate the collector. Then at least you only tune one place and connections are reused across query and store.\n. cc @jcarres-mdsol\n. All these little things make logic more complex. It would be better if the\ninstrumentation was corrected to log the 'sa'\n. PS I don't mind raising a pull request to do that. Is the thing that's\ncausing the 'cs' open source?\n. PS also we will eventually need the 'sa' anyway, for example querying by\ndestination address. The address on 'cs' is the local one.\n. All of this reminds me to update the thrift and associated doc on those\ncore annotations. All of these rules are very subtle.\n. Hi, there.\nWe've added substantial documentation updates, far more than existed\nbefore. Don't read too much into what's logged in old issues, but I will\nhave a look at it.\nWe now have verbose README files, detailing the minimum variables you need\nto set, and doc folders on both zipkin and also the docker-zipkin projects.\nUsers no longer have to build the project as we took great effort to\npublish binaries and also docker images.\nThe binaries are on bintray and docker images are on both quay and also\ndockerhub.\nMaybe if you can mention your specific use-case, we can update zipkin.io to\nbe more obvious.\nI know there's still work to do, but you are far better off as you don't\nhave to run sbt or gradle ;) Regardless, we are on gitter.im on\nopenzipkin/zipkin and folks deploy in many ways. Many use docker, which we\nsupport best. Some use chef or other tools.\nHope this helps and thanks for keeping us honest!\n. So we do have folks using non-docker, like Cloud Foundry and chef. It makes\nsense to make a \"recipe\" page or something, aggregating the content.\nI can try to contact Twitter to redirect the old site to the new one.\nShould be possible.\nAny particular config mgmt approach you are looking for? Or will basic\ncollection of where to get artifacts and how to start them work?\n. Great. which backend storage are you planning on? mysql is the simplest,\nbut cassandra is also well supported.\n. as this topic isn't concise enough to know when it is complete, I took what I could and made https://github.com/openzipkin/zipkin-redis/issues/2 out of it.\n. Good points, James. Teasing things out, though.. Having our collectors n\nquery processes not attempt to do DDL is probably a low hanging fruit, even\nif there be dragons like things you mentioned.\n. @jcarres-mdsol @yurishkuro @adrianco fyi\n@theatrus can you critique this, in case I misunderstood something?\n. ok updated based on feedback from @jcarres-mdsol @yurishkuro and finagle\n. got an ack from finagle that the docs are still valid. plan to merge this tomorrow\n. cc @danchia @eirslett @michaelsembwever @kristofa \n. tested locally against cassandra and mysql, both scala and java projects\n. yes, via SpanStore. However that happened in an earlier version as well.\nThis change is about how self-tracing is performed.\n. yes, via SpanStore. However that happened in an earlier version as well.\nThis change is about how self-tracing is performed.\n. Maybe you're right! Wanna give it a try?\n-A\n. Maybe you're right! Wanna give it a try?\n-A\n. taking a look now\n. taking a look now\n. If I access this via curl directly on localhost, there's no problem. Trying a public host..\n. If I access this via curl directly on localhost, there's no problem. Trying a public host..\n. Similarly, I can use a url directly against a public host. Ex. if you do a curl you'll see data in the response, like <span class=\"label label-default service-filter-label\" data-service-name=\"zipkin-jdbc\">zipkin-jdbc x1 8ms</span>\nCan you use chrome debugger and see what's actually being sent and received? Maybe paste the http envelopes (without the payload) here?\n. Similarly, I can use a url directly against a public host. Ex. if you do a curl you'll see data in the response, like <span class=\"label label-default service-filter-label\" data-service-name=\"zipkin-jdbc\">zipkin-jdbc x1 8ms</span>\nCan you use chrome debugger and see what's actually being sent and received? Maybe paste the http envelopes (without the payload) here?\n. cutting as the only change in 1.20.0\n. cutting as the only change in 1.20.0\n. agreed this is an issue!\n. agreed this is an issue!\n. LGTM: super-nit would be to not put the issue fixed on the summary line (as you can see it reads weird in a github issue)\nex. instead of single line\nempty the dropdown before adding service names. Fixes #803\nbreak like this\n```\nempties the dropdown before adding service names\nFixes #803\n```\n. LGTM: super-nit would be to not put the issue fixed on the summary line (as you can see it reads weird in a github issue)\nex. instead of single line\nempty the dropdown before adding service names. Fixes #803\nbreak like this\n```\nempties the dropdown before adding service names\nFixes #803\n```\n. Yes!\n. Yes!\n. Glad it might work. Just added some more detail around how I see this fitting in with local spans. PTAL, as it may make more sense within this context https://github.com/openzipkin/zipkin/issues/808\n. Glad it might work. Just added some more detail around how I see this fitting in with local spans. PTAL, as it may make more sense within this context https://github.com/openzipkin/zipkin/issues/808\n. Re-summarized in the thrift definition and WIP description. I removed my related redundant comments.\n. Re-summarized in the thrift definition and WIP description. I removed my related redundant comments.\n. Note I plan to start on this today, as soon as the supporting changes are released as 1.21.1\n. Note I plan to start on this today, as soon as the supporting changes are released as 1.21.1\n. ETA tomorrow!\n. ETA tomorrow!\n. expect an update tomorrow pacific AM. Just polishing tests\n. expect an update tomorrow pacific AM. Just polishing tests\n. ok. this is finally complete. Once merged, I'll raise a pull request for duration query.\n. ok. this is finally complete. Once merged, I'll raise a pull request for duration query.\n. at least in intellij, private constructor vs apply with same args often gets confused. feel free to raise a PR to try it out though!\n. at least in intellij, private constructor vs apply with same args often gets confused. feel free to raise a PR to try it out though!\n. next after this is cleaning up use of Span.timestamp/duration in the scala api before we backfill it with a thrift change\n. next after this is cleaning up use of Span.timestamp/duration in the scala api before we backfill it with a thrift change\n. tested with collector, query, tracegen and web\n. tested with collector, query, tracegen and web\n. took a while to finish, but not sure it is related. Thanks!\n. took a while to finish, but not sure it is related. Thanks!\n. NOTE: there's zero DDL change needed to support this. However, the interpretation of data is different.\n. NOTE: there's zero DDL change needed to support this. However, the interpretation of data is different.\n. thx for the review, @yurishkuro \njust going to pull in the duration thing.. so eta on merge is w/in the hour\n. thx for the review, @yurishkuro \njust going to pull in the duration thing.. so eta on merge is w/in the hour\n. for example, with cassandra tests enabled, a full build on my laptop takes 5 minutes and 40 seconds. With them disabled, it is less than a minute.\n. for example, with cassandra tests enabled, a full build on my laptop takes 5 minutes and 40 seconds. With them disabled, it is less than a minute.\n. @yurishkuro let me know, if you need a hand on cassandra support.\n. @yurishkuro let me know, if you need a hand on cassandra support.\n. @yurishkuro I am not aware of any intentional change like that.\n. @yurishkuro I am not aware of any intentional change like that.\n. Between 1.19 and now, mustache changed.. that's the template library. I\ndon't see any CSS changes, but maybe viewing source on both could identify\nwhat's up\ngit diff 1.19.0 1.22.1 zipkin-web/build.gradle\n. Between 1.19 and now, mustache changed.. that's the template library. I\ndon't see any CSS changes, but maybe viewing source on both could identify\nwhat's up\ngit diff 1.19.0 1.22.1 zipkin-web/build.gradle\n. Great progress!\nI think we'll eventually hit a point where we need to specify startTs. We\nalready need this for the dependency query. Should we add this now?\n. Great progress!\nI think we'll eventually hit a point where we need to specify startTs. We\nalready need this for the dependency query. Should we add this now?\n. cc @yurishkuro @eirslett @spencergibb @dsyer @kristofa @michaelsembwever \n. cc @yurishkuro @eirslett @spencergibb @dsyer @kristofa @michaelsembwever \n. Note that finagle instrumentation doesn't yet support Span.duration. The workaround until that's not the case is add a ClientRecv(\"cr\") annotation. That's because finagle needs some signal to know the span is complete. https://github.com/twitter/finagle/blob/develop/finagle-zipkin/src/main/scala/com/twitter/finagle/zipkin/thrift/DeadlineSpanMap.scala#L110\n. Note that finagle instrumentation doesn't yet support Span.duration. The workaround until that's not the case is add a ClientRecv(\"cr\") annotation. That's because finagle needs some signal to know the span is complete. https://github.com/twitter/finagle/blob/develop/finagle-zipkin/src/main/scala/com/twitter/finagle/zipkin/thrift/DeadlineSpanMap.scala#L110\n. Make sure you are setting Span.timestamp, duration? The example of\nbootstrap tracing displays...\n. Make sure you are setting Span.timestamp, duration? The example of\nbootstrap tracing displays...\n. rebased with a fix to a bug where the tracers weren't initializing.\n@yurishkuro @kristofa please take a look. I'd like to add support to this to brave, but want to make sure folks are on-board before raising that.\n. rebased with a fix to a bug where the tracers weren't initializing.\n@yurishkuro @kristofa please take a look. I'd like to add support to this to brave, but want to make sure folks are on-board before raising that.\n. last call for feedback, I'll merge in a couple hours and address anything else post-merge\n. last call for feedback, I'll merge in a couple hours and address anything else post-merge\n. good point. so we have a test that shows spanstore doesn't require annotations, but maybe there's some special casing in the web process subverting this. Should be possible to reproduce by making json and then posting it to the query server. I'll have a look.\n. @nishantshobhit feel free to hop on openzipkin/zipkin even if the questions are about brave.\nI have a working example of local tracer in the zipkin-java project. However, there's some important details about the implementation of span storage in brave you need to keep in mind. Look here for details: https://github.com/openzipkin/brave/blob/master/brave-core/src/main/java/com/github/kristofa/brave/LocalSpanState.java#L9\n. thanks!\n. thanks!\n. since this is documenting things actually the case in finagle, I'm gonna merge :P\n. since this is documenting things actually the case in finagle, I'm gonna merge :P\n. @yurishkuro this is the change we spoke about for optimizing buckets for duration query\n. @yurishkuro this is the change we spoke about for optimizing buckets for duration query\n. The only reason is that there are so many more uses of QueryRequest in\ntests vs uses of DependencyStore.getDependencies. Basically it would\n\"ugly up\" probably a hundred or so call-sites.\nHave a look and if you feel strongly about this, we can change after\nthe fact as it would break a lot of files in unrelated ways.\n. The only reason is that there are so many more uses of QueryRequest in\ntests vs uses of DependencyStore.getDependencies. Basically it would\n\"ugly up\" probably a hundred or so call-sites.\nHave a look and if you feel strongly about this, we can change after\nthe fact as it would break a lot of files in unrelated ways.\n. We literally introduced a \"lookback\" query flag for this https://github.com/openzipkin/zipkin/pull/825\nIt defaults to 7 days, and overridable via QUERY_LOOKBACK (in microseconds). I think @yurishkuro said he'd probably set this to 2 days. In docker, this would be QUERY_LOOKBACK=172800000000\nThis basically changes the default from looking back forever until a pre-canned value. For customizable values, I agree the UI needs to be changed. One question would be whether to specify start_date or lookback directly as the UI element. Thoughts?\nPS  I'll cut an interim release today so you can set  QUERY_LOOKBACK\n. We literally introduced a \"lookback\" query flag for this https://github.com/openzipkin/zipkin/pull/825\nIt defaults to 7 days, and overridable via QUERY_LOOKBACK (in microseconds). I think @yurishkuro said he'd probably set this to 2 days. In docker, this would be QUERY_LOOKBACK=172800000000\nThis basically changes the default from looking back forever until a pre-canned value. For customizable values, I agree the UI needs to be changed. One question would be whether to specify start_date or lookback directly as the UI element. Thoughts?\nPS  I'll cut an interim release today so you can set  QUERY_LOOKBACK\n. The sort happens before the javascript processes the spans for all span\nstores. Then, there's a javascript element to sort differently. I think the\nkey concern is that very old spans show up.. Easiest way out is to cut the\nQUERY_LOOKBACK change, as that will magically eliminate the month-old\nproblem.\n. default QUERY_LOOKBACK is an environment variable, so it isn't visible to\nusers anyway. This is similar to TTL, as most sites don't retain more than\n3-7 days trace data. You can feel free to set it to a year, for example.\nLet's separate the issue of the environment variable from how a UI presents\nthe query. Do you have a specific change in mind, even better a pull\nrequest for it? It is trivial to convert startTs into lookback as endTs is\na required parameter anyway.\n. Here's a summary from a gitter chat.\nUsers expect to see most relevant results by default.\nHere are some UI options I thought about:\n1. Add Lookback to the UI. User clicks or allows the date to be set how it is, then they choose 5minutes lookback.\n2. change the UI from basing on endTs (looking backwards) to being based on startTs (looking forwards)\nFor example, we choose a point in time, and we look for results newer than that.\n3. change the UI from basing on endTs (which is almost always current time) to being based on timestamp. Add a +- option. For example, user enters 9:55 +- 5minutes. This is easy to convert to endTs, lookback.\nRegardless, there's a different aspect, which is the order of trace results. If limit was subject to descending order of timestamp, they'd always get freshest by default, regardless of UI.\n. After addressing the only couple things. please squash and rebase into a single commit. http://rebaseandsqua.sh/\nHere's a better commit message I've crafted just for you!\n```\nMakes cassandra schema commands optional\nBefore, using cassandra wasn't possible without escalated privileges. \nThis was due to implicit use of schema commands. This change moves the\ndefault to assume schema is absent. To run with less privileges, set the\nenvironment variable CASSANDRA_ENSURE_SCHEMA=true. \nFor users customizing CassandraSpanStoreFactory directly, this maps to\nthe flag zipkin.store.cassandra.ensureSchema, which defaults to false.\nFixes #793\n```\n. After addressing the only couple things. please squash and rebase into a single commit. http://rebaseandsqua.sh/\nHere's a better commit message I've crafted just for you!\n```\nMakes cassandra schema commands optional\nBefore, using cassandra wasn't possible without escalated privileges. \nThis was due to implicit use of schema commands. This change moves the\ndefault to assume schema is absent. To run with less privileges, set the\nenvironment variable CASSANDRA_ENSURE_SCHEMA=true. \nFor users customizing CassandraSpanStoreFactory directly, this maps to\nthe flag zipkin.store.cassandra.ensureSchema, which defaults to false.\nFixes #793\n```\n. updated the comment as I noticed the environment variable defaults to true, which was going to be a question!\nOK Good job.. very little feedback for you. Thanks!\n. updated the comment as I noticed the environment variable defaults to true, which was going to be a question!\nOK Good job.. very little feedback for you. Thanks!\n. @pawelszymczyk typo.. I screwed up that sentence because I wrote it before realizing the env variable was opposite of the twitter flag. Please correct!\nPS if curious, default to true is mostly a laptop thing. Ex. our test docker image installs the schema directly.\n. LGTM will merge on green! Also, this change will go out today, probably in a few hours\n. @yurishkuro ptal\n. @yurishkuro PTAL\n. Here's the span that now works:\nbash\n$ curl -s localhost:9411/api/v1/spans -X POST -H 'Content-Type: application/json' -d '[\n  {\n    \"traceId\": \"5a4a9e596c08e861\",\n    \"name\": \"foo\",\n    \"id\": \"5a4a9e596c08e861\",\n    \"timestamp\": 1447092995596000,\n    \"duration\": 37000,\n    \"annotations\": [ ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"lc\",\n        \"value\": \"twitter-server\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-web\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      }\n    ]\n  }\n]'\n. cc @yurishkuro \n. manually verified also, by putting 30000 into the duration bin. with \"all\" names selected, bootstrap and get spans returned. selecting either refined as expected\n. Do you know what command results in a response of >12s? Ex. can you post a\nstack trace? That sounds like a long time.. I suspect we could update it or\npunch a hole, but it would be nice to know what is taking so long..\n. Do you know what command results in a response of >12s? Ex. can you post a\nstack trace? That sounds like a long time.. I suspect we could update it or\npunch a hole, but it would be nice to know what is taking so long..\n. so 12 seconds is a very long time. do you have some network problem where\nit is normal for 12 seconds is normal? How do you know 13 seconds or 30\nseconds would improve things? Or are you just interested in experimenting\nwhy you are having severe connectivity problems.\nMaybe we should fallback to a non-error state, like empty result, when this\noccurs?\nI want to avoid opening new flags for every one of the boundless scenarios\none could encounter. Reason is that our configuration story is quite bad\n(unlike zipkin-java where it is quite easy to do this). Ex we have a lot of\nwork and if you find that 12 seconds or 30 seconds you still have a connect\ntimeout, then that work stays in the codebase..\nI do think we need to consider how to do \"pass-through\" config to a driver.\nThis sort of pass-through was attempted in HTrace, for example in kafka\ntransport\nhttps://github.com/apache/incubator-htrace/blob/master/htrace-zipkin/README.md\n@michaelsembwever @yurishkuro do you have any ideas on this topic? Do you\nthink 12s timeout is unreasonably short? Should we add a fallback to empty\non this?\n. Sorry, I got distracted by the NoHostAvailableException in the log eventhough you mentioned the OperationTimedOutException underneath it. What parameter are you looking for specifically (just in case)?\nIt shouldn't take 12s to retrieve 10 traces (based on your log), especially in recent implementations. Ex. we have a default lookback parameter now, which you can use to reduce how far queries look back. In other words, if it is scanning days of data to get 10 traces, that sounds like a bug. What version are you running?\nIIRC, twitter kept 3 days of trace data (at some point it was high as 7). Uber is 2d? There are certainly going to be limits. ~15Gb/day is an interesting figure. Do you know how many traces/day that turns into?\n. @drax68 thanks for bearing with us.. do you mind turning debug logging on for org.twitter.zipkin.storage.cassandra.Repository ? It will emit which queries passed before blowing up. Ex. it would be nice to know if the blowup happened fetching ids or something else. A more recent stack trace will probably say most of that (the one in your log says 1.23.2-SNAPSHOT)\n. ps feel free to join gitter to discuss https://gitter.im/openzipkin/zipkin I really think there's a solvable problem here.\n. ps looks like this is the setting in question: http://docs.datastax.com/en/drivers/java/2.0/com/datastax/driver/core/SocketOptions.html#setReadTimeoutMillis-int-\n. @drax68 not sure how you are deploying, but can you control your environment? If so, I'd love if you can experiment with the following.\nquery-cassandra.scala is the file used (implicitly with the -f /query-cassandra.scala argument to java -jar)\nMake a copy of that, and replace the line object Factory extends App with CassandraSpanStoreFactory with this:\nscala\nobject Factory extends App with CassandraSpanStoreFactory {\n  override def createClusterBuilder(): Cluster.Builder = {\n    val result = super.createClusterBuilder()\n    result.getConfiguration.getSocketOptions.setReadTimeoutMillis(15000)\n    result\n  }\n}\nStart your server with a path to the alternate file. ex: -f /query-patched-cassandra.scala\nSee if you can find a timeout that works and report back?\n. Will join gitter in a bit. By didn't work, you mean the value didn't take?\nOr that 15s didn't help?\n. I must be completely blind as I still can't find the timeout value logged\nin the error message. Anyway, thanks for clarifying!\n. Oh never mind. The log timestamps indicate 12s duration. Don't mind me..\n. You guys are awesome. I'm really thankful we have a team earnestly trying\nto solve this. Keep up the good work! :clap:\n. I remember this burdensome default. You getting this from zipkin-web, right?\n. ps this smells like we aren't gzip encoding json. I'll look into that.\n. I recall this issue, as it has bit me multiple times. Not sure the best configuration answer. In the past I had to make a flag that override's finagle's default. Usually that was only used to set it to a sufficiently high value as someone never needed to touch it again.\nIn many cases, where this exception happened, apps weren't gzipping stringy payloads. The below should help a lot.\nhttps://github.com/openzipkin/zipkin/pull/835\n. Ex. in finagle, there's and implicit parameter thing, which controls this. Sometimes I wish our hands had more than 5 digits, so such defaults would be higher.\nscala\n    implicit object MaxResponseSize extends Stack.Param[MaxResponseSize] {\n      val default = MaxResponseSize(5.megabytes)\n    }\n. are we ok on no-op here?\n. @jcarres-mdsol Thanks tons for reporting this!\n. yep looks quite sensible.\n. yep. ./gradlew clean check on my laptop is now back to a tolerable 1.5mins\n. yep. ./gradlew clean check on my laptop is now back to a tolerable 1.5mins\n. Can you post JSON of the trace in question (taken from\n/api/v1/trace/your_id_here)?\n. Can you post JSON of the trace in question (taken from\n/api/v1/trace/your_id_here)?\n. so a span can be sent in parts, but by query time, they should be merged. That's a feature, not a bug.\nThe UI says that 3 spans include the service \"example\" and 2 include the service \"example2\", which seems to match the json. These are buttons that will select spans that include that service.\nIt is well known that the UI isn't intuitive, but I don't see a bug here.. do you?\n. so a span can be sent in parts, but by query time, they should be merged. That's a feature, not a bug.\nThe UI says that 3 spans include the service \"example\" and 2 include the service \"example2\", which seems to match the json. These are buttons that will select spans that include that service.\nIt is well known that the UI isn't intuitive, but I don't see a bug here.. do you?\n. IMHO, these [service xN] buttons and other confusing clutter on the screen might be better off deleted\n. IMHO, these [service xN] buttons and other confusing clutter on the screen might be better off deleted\n. Cool. thanks for clarifying the bug! I think I can squash this one.\n. Cool. thanks for clarifying the bug! I think I can squash this one.\n. SGTM\n. SGTM\n. @abesto if you're interested in this, lemme know.\n. @abesto if you're interested in this, lemme know.\n. The build is blowing up, guessing something around java8. I can help debug this later, if the cause isn't obvious.\nBesides that, I'm happy with helping to support this code. I'd like some clarity on the naming choices, but otherwise LGTM\n. The build is blowing up, guessing something around java8. I can help debug this later, if the cause isn't obvious.\nBesides that, I'm happy with helping to support this code. I'd like some clarity on the naming choices, but otherwise LGTM\n. mind squashing this into a single commit when you rebase on top of the travis JDK bump?\n. mind squashing this into a single commit when you rebase on top of the travis JDK bump?\n. I know you know how, but I like pasting this.. http://rebaseandsqua.sh/ :)\n. I know you know how, but I like pasting this.. http://rebaseandsqua.sh/ :)\n. I'll cherry-pick this in\n. I'll cherry-pick this in\n. closed via https://github.com/openzipkin/zipkin/commit/9603ca81f22d1b8e8ba17b9f0663603184d154dd\nThanks tons!\n. closed via https://github.com/openzipkin/zipkin/commit/9603ca81f22d1b8e8ba17b9f0663603184d154dd\nThanks tons!\n. appears to have a recent JDK, now.\n$ java -Xmx32m -version\njava version \"1.8.0_66\"\nJava(TM) SE Runtime Environment (build 1.8.0_66-b17)\nJava HotSpot(TM) 64-Bit Server VM (build 25.66-b17, mixed mode)\n. appears to have a recent JDK, now.\n$ java -Xmx32m -version\njava version \"1.8.0_66\"\nJava(TM) SE Runtime Environment (build 1.8.0_66-b17)\nJava HotSpot(TM) 64-Bit Server VM (build 25.66-b17, mixed mode)\n. UPDATE: did a lot of research and backfilling code docs, which will be where most of the rest docs are harvested from. Anyone needing insight into the model before this issue closes can look here: https://github.com/openzipkin/zipkin/blob/master/zipkin-thrift/src/main/thrift/com/twitter/zipkin/zipkinCore.thrift#L272\n. Update: I've reviewed tools and will spike this with apiblueprint https://apiblueprint.org/\nThis seems to fit the task of api portability and align with the fact that we have active contributors on markdown, who regularly get scared off when asked to look at scala or thrift files :package: \nMoreover, apiblueprint has a tool called dredd which can be used to test fixtures. I'm wondering if this could be leveraged to help test tracers...\nhttps://github.com/apiaryio/dredd\n@netmilk might ask you to help review something once I try this out. Short of it is that zipkin has 2 api server implementations at the moment, scala and java. We have clients (tracers) in many languages including scala, java, ruby, c#, python and some lesser maintained ones like node. Documenting the api is the first win, but a second one would be to use travis to test it.\ncc'ing folks who've expressed interest in or contributed to our docs: @abesto @kjoconnor @jfeltesse-mdsol @clehene @zfy0701 @dsyer @jamescway \n. here's a start, cherry-picking the easiest methods while I get used to things https://github.com/openzipkin/zipkin/pull/888\n. @netmilk thanks for the feedback.\nThere was a discussion on gitter about apiblueprint vs swagger:\nVoting for swagger:\n@prat0318 - has experience with swagger and prefers swagger doc integrated with query apis. Noted visual editor\n@karlunho - works at a company that makes OSS tooling for swagger. Notes higher adoption numbers\nAbstain:\n@yurishkuro - no experience with either apiblueprint vs swagger\n@abesto - limited experience with swagger, none with apiblueprint. Noted ttps://github.com/cachecontrol/hippie-swagger, which can be used to create tests.\nme - no experience with either, interested in testable, easy to write api docs. I'd like a solution that can be run with travis, and treats services as black boxes (ie tests go off spec as opposed to requiring frameworks on the server). apiblueprint/dredd seems to have that, but that doesn't mean hippie-swagger or otherwise don't. Regardless, I've not gotten to the point of even using dredd to validate the travis-black box ability.\n. PS it is totally my goal to re-use whatever framework we make for a v2 api.\nI'm using v1 as it is the only thing we have implemented, and what all the\ncurrent tracers are built on.\n. looks like swagger is now https://openapis.org/\n. @karlunho offered to take a stab (or faciliate one) w/ openapis.\nTo help, I'm making a google doc of curl commands https://docs.google.com/document/d/14Ucxk-K1JhWE60Xry2XZak5fHrBagJT-HCMbo_iou18/edit?usp=sharing\n. @karlunho offered to take a stab (or faciliate one) w/ openapis.\nTo help, I'm making a google doc of curl commands https://docs.google.com/document/d/14Ucxk-K1JhWE60Xry2XZak5fHrBagJT-HCMbo_iou18/edit?usp=sharing\n. cool.. glad you all are interested. if not I'd feel silly as making the google doc to prepare for swagger is slightly more work than writing the api blueprint format. This extra effort is worth it, if we're likely to have a more sustainable, shared outcome\n. cool.. glad you all are interested. if not I'd feel silly as making the google doc to prepare for swagger is slightly more work than writing the api blueprint format. This extra effort is worth it, if we're likely to have a more sustainable, shared outcome\n. I've updated the doc with all endpoints.. ready for swagger/apidocs conversion  https://docs.google.com/document/d/14Ucxk-K1JhWE60Xry2XZak5fHrBagJT-HCMbo_iou18/edit#\n. I've updated the doc with all endpoints.. ready for swagger/apidocs conversion  https://docs.google.com/document/d/14Ucxk-K1JhWE60Xry2XZak5fHrBagJT-HCMbo_iou18/edit#\n. https://github.com/openzipkin/zipkin-api !\n. https://github.com/openzipkin/zipkin-api !\n. @yurishkuro as noted inline, this is documentation, not a change. Basically, I've found much stronger signal that in zipkin root span is traceid = spanid, vs just absent parent id. Documenting how things work doesn't mean we can't change it.\nChanging it will certainly impact code anyway, at least ours, as we need to test things like tree construction don't depend on this. Moreover, I don't know off-hand why traceid=spanid, or if that was just to save a random number generator. If I were to start figuring it out, it would be like many things: searching through git logs of zipkin and finagle, asking people, then dealing with test breaks or missing tests. In other words, this type of issue is better dealt with as a top-level change, than a side bar on documentation polish :)\n@theatrus do you remember anything about traceid=spanid?\n. @yurishkuro as noted inline, this is documentation, not a change. Basically, I've found much stronger signal that in zipkin root span is traceid = spanid, vs just absent parent id. Documenting how things work doesn't mean we can't change it.\nChanging it will certainly impact code anyway, at least ours, as we need to test things like tree construction don't depend on this. Moreover, I don't know off-hand why traceid=spanid, or if that was just to save a random number generator. If I were to start figuring it out, it would be like many things: searching through git logs of zipkin and finagle, asking people, then dealing with test breaks or missing tests. In other words, this type of issue is better dealt with as a top-level change, than a side bar on documentation polish :)\n@theatrus do you remember anything about traceid=spanid?\n. ps fwiw I would like to understand traceid=spanid, too\n. ps fwiw I would like to understand traceid=spanid, too\n. PS parked the root span.id=traceid here: https://github.com/openzipkin/zipkin/issues/849\n. PS parked the root span.id=traceid here: https://github.com/openzipkin/zipkin/issues/849\n. @theatrus @eirslett @kristofa anybody have any insight (or memory) about this?\n. @theatrus @eirslett @kristofa anybody have any insight (or memory) about this?\n. Food for thought: dapper doesn't seem to imply a span.id=traceid restriction:\n\nSpans created without a parent id are known as root spans. All spans associated with a specific trace also share a common trace id.\n. A bit of a guess, but the traceid=spanid may have nothing to do with dapper, and more about the predecessor to the predecessor of zipkin: https://github.com/twitter/finagle/commit/e00c91709c1be6710d4aee102139b12cb1f837d2#diff-50e70bbc12edca0f2c7be3f34b53d88aR32\n. Also, seems htrace already have to set traceid differently, as their ids are 128bit! https://github.com/apache/incubator-htrace/blob/master/htrace-zipkin/src/main/java/org/apache/htrace/zipkin/HTraceToZipkinConverter.java#L112\n. I did a quick look through zipkin's scala code the only thing that seems impacted by this might be smoked out via DependencyStoreSpec. For instrumentation, probably just want to make sure none of core tracers indicate root span via parent_id=0 (vs null) or parent_id=trace_id\n. NEXT STEP: let others speak up about this\n. I'm +1 on this, too. I'll raise a PR with some test clarifications to support this\n. c* dependency tests weren't actually running. fixed that and backfilled tests to ensure we don't rely on this convention. https://github.com/openzipkin/zipkin/pull/851\n. Nope! no code change needed, so yeah glad about that.\n. Thanks. good start. I suspected there'd be a crapload of licenses (because crapload of dependencies).\n\nI think it is most important to know the licenses \"per binary\" like, collector, web, query.\nDo you mind running this and place the example html output in a comment, image a pastebin? That will lower the barrier to review for folks who don't have gradle setup. (Also, you could raise this as a openzipkin branch, too)\n. I'll raise a pull request to switch out mysql driver for mariadb. There's an exception to the GPL clause, but still unnecessarily scary: https://www.mysql.com/about/legal/licensing/foss-exception/\n. https://github.com/openzipkin/zipkin/pull/853 wrt killing the GPL dep\n. hehe one last whack-a-mole! \"verifyLicensesLicense \"LGPL-2.1\" is not on the list of allowed licenses. The dependencies using it: [mariadb-java-client-1.3.0.jar\"\n. sgtm. Let's merge this when you're ready.\n. corresponding issue in zipkin-java: https://github.com/openzipkin/zipkin-java/pull/44\n. @abesto mind rebasing the license audit over this?\n. Thanks, @yurishkuro!\n. how about adding the table before you upgrade?\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-cassandra-core/src/main/resources/cassandra-schema-cql3.txt#L58\n. good point. are you running a flag that prevents auto-addition perhaps?\n. ex. if you are using docker-containers, or the -service packages as an entrypoint (ex. query-cassandra.scala), then CASSANDRA_ENSURE_SCHEMA env is used, which defaults to true.\nIf you have a custom build, then the flag zipkin.store.cassandra.ensureSchema defaults to false. How are you running?\n. it should default to true.. what's weird is that this implies CREATE\nTABLE IF NOT EXISTS zipkin.span_duration_index... didn't work? which\ndoesn't make immediate sense to me.\n. ex. ensureExists in Repository.java isn't conditional on anything\nexcept for the flag, so the DDL should execute top-to-bottom.\n. by should I mean it does default to true, due to this line:\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-query-service/config/query-cassandra.scala#L27\n. ps hold on this one if you can :) there's a schema adjustment for that\ntable coming!\n. whew! Thanks for reporting back.. was a bit worried!\n. ps 1.25.0 adjusts the aforementioned table. hot off the press\n. @michaelsembwever @yurishkuro cassandra passes all tests again, now\n. Sounds fine to me, as long as we make it obvious in the code and DDL that these are millisecond granularity as opposed to micros.\n. I think we should apply this globally. ex change QueryRequest and the HTTP api to millis, so it is consistent. The logic is sound, and a change like this is better to do before we document the HTTP api.\n. manual testing of the web app works with queries and dependency graph\n. can you verify the field names are the same as others in the same file? If so, LGTM with only nits\n. nice. :shipit: \n. thx\n. -1 (first time ever in zipkin)\nI don't like this for a number of reasons:\n- The syntax is parsed and validated on the server side. This is an unnecessary round trip.\n- Web developers now need to learn scala to troubleshoot this parameter.\n- Alternative implementations of the query api now need to be compatible with scala duration parsing\n- It is a well-understood goal to move zipkin-web towards a pure-js app, and this is the opposite\nI'd suggest using moments.js or some other javascript library instead.\n. @danchia @yurishkuro @eirslett @nikhil2406 @gneokleo @synk Fishing for those who've touched flight components before :) Do any of you have some cycles to try this?\n. might not work, but I was thinking the 2 arg methods, which look like they\naccept a number and a unit? (ex  'm' or 'minutes')\nhttp://momentjs.com/docs/#/manipulating/add/\nhttp://momentjs.com/docs/#/utilities/\n. @danchia I hope this task didn't scare you from zipkin forever :P\n. @danchia I hope this task didn't scare you from zipkin forever :P\n. cc @spencergibb @dsyer I'll eventually reproduce this doc, in terms of spring boot, but in case this is interesting to you\n. So, the canonical serialization of zipkin objects are json or thrift, not java serialization. We have tests for the serialized for of json (used in query), for example, but that's also largely because case classes in scala are so hard to deal with.\nScrooge thrifts are marked as Serializable.. what would happen if you stored scrooge thrifts instead?\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-scrooge/src/main/scala/com/twitter/zipkin/conversions/thrift.scala#L156\n. sounds good. most important will be to update DependencyStoreSpec to use\nthe least amount of dummy data to reproduce this.\n. Thanks, yuri. If no-one else can integrate this into a PR, I can probably\ndo so before the end of the day.\n. Thanks for the test case!\n. > I would prefer if the UI could simply display the time interval for which\n\nthe aggregated results are shown.\nThat\n\nAdding multiple records together doesn't sound right to me - what if their\nstartTs-endTs intervals overlap or are disjoint? The aggregate numbers\nwouldn't really make sense. At least if we're showing a single aggregate\nrecord we can guarantee that the counts are consistent.\nThis sounds like edge case fishing. For example, this feature was\ndocumented as allowing backends to store a different granularity. Even to\nthe point of explaining what that meant, ex. How we expect to treat a look\nback that ends in-between grains. For example summing 2 days, if look back\nis 36hrs. This issue doesn't seem like a feature change to me, but your\nresponse to it does.\n\nAlso, summing may work for counts, but once we start adding more\nmeasures, like p99, we won't be able to aggregate across multiple time\nintervals (maybe we'll need to go back to 'moments').\nThis is really getting past the current feature. We have discussed that we\nmight add summable measures such as bucketed latency intervals. Average or\np99 is a distracting addition to this issue.\nIncidentally, the current Spark job impl leaves startTs/endTs==0, so the\nmaxBY above won't quite work.\nval dependencies: Dependencies = aggregates\n  .map { case ((parent: String, child: String), callCount: Long) =>\n    Dependencies(0, 0, Seq(DependencyLink(parent = parent, child =\nchild, callCount = callCount)))\n      }\n      .reduce( + ) // merge DLs under one Dependencies object, which\noverrides +\nThe job also doesn't filter traces by timestamps, it just reads\neverything from the table, so a single record already presents fully\naggregated result. If we want that job to generate data for adjacent time\nintervals, and considering that it clamps he timestamp to 24hrs, maybe it\nshould be doing a filter on the traces.\nstartTs = (ts floor 24hrs) - 24hr\nendTs = ts floor 24hrs\n\nThis response is hard because it is tail wagging the dog. The spark job is\none implementation of the dependency api, and as far as I remember doesn't\neven run the tests.\nLet's focus on the feature we have today, fill missing tests and fix jobs\nthat implement the feature. If we want a new or different feature that\ntries to sum P99, that's cool, but seems distracting to do that in the\nmiddle of a big report.\nIn other words can we please put this train back on the rails.\nI am willing to help fix the bug, but not if that means reworking zipkin's\ncode around a ramble of new requirements and UI changes.\nWe have 2 bugs: here we have a bug which we should be explainable in the\njavadoc on DependencyStore and a DependencyStoreSpec\nThe other bug is that the spark job has no tests. This leads to\nhypothetical discussion around features which could be avoided if it ever\nactually ran our interop test. We shouldn't kite any more feature work\nbased on the assumptions baked into untested code. So, that's bug 2.\nOnce we fix the bugs I think we should start thinking about future features\nbut honestly the Cassandra related codebase has some serious tech debt to\npay off.\n. Remember to cut a release, push a tag like 1.25.2\n. releasing now as 1.25.2 cc @prat0318 \n. are you failing to compile? or are you referring to the warning output.\n. Worthwhile\n. interesting. agreed.\nthough http://openzipkin.github.io/zipkin/ or better zipkin.io\n. @michaelsembwever @yurishkuro any thoughts?\n. agreed. zipkin v1 design uses span name similar to a dimensional modeling label. Its only intent is users to scroll through and should not be so big.\nAny future design could alter this, particularly if path-based things are implemented, but that's pie-in-the-cloud https://docs.google.com/document/d/1ixxEs9TvhiGjJObGbRSPhSna3zHdadoUTQIZ5JKgLzU/edit\n. +1 looks sensible to me even if the context to know to do this required a bit of experience!\n. build fixed via https://github.com/openzipkin/zipkin/pull/877\n. whoah.. just learned something!\nso we have 2 travis checkers on the pull request\nthe one ending in \"pr\" uses the travis config from that pull request\nthe one ending in \"push\" uses the previous travis config\nso .. if you raise a pull request, which affects travis config, you might end up with a christmas status\nthe \"pr\" passes\", but the \"push\" fails\nMerging because this actually makes sense, even though it is surprising enough to warrant revisiting. cc @abesto \n. yes 1.25.3\n. cc @michaelsembwever \n. cc @kristofa @eirslett @clehene @jamescway \n. looks like you have no data! this is a bug in our code, though.\n\"WHERE trace_id IN ()\"\n. FYI: @gneokleo you are the only recent contributor to this code, besides maintenance I've done.\ncc @dsyer @MatMoore and @skstronghold who have attempted to use this.\ncc @abesto @eirslett who have recently commented about this\n. I'm thinking add: MYSQL_USE_SSL as this is easy enough to make portable and is sensible as a top-level concern. This would add useSSL=true to the query line. sg?\n. started looking into this, but it seems awkward to test as most of the time you need to setup custom certs. Ex. if using RDS, you need to download a pem file, etc. \nhttp://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MariaDB.html#MariaDB.Concepts.SSLSupport\nIn other words there's more config than just the flag I think, and if we don't test this, then... well feels debty. Any thoughts on how to \"finish the job\" ex. patterns to stuff certs on disk in docker + self-signing one for mariadb? Or should we just open the flag and leave it to manual testing?\n. good point! I think this can work.\n. update: I've found what's needed to accomplish this, using spring-boot example for now. I've verified you only need 3 things:\n- append &useSSL=true to the mysql url\n- set javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword JVM arguments\nI used Amazon RDS and zipkin-java to verify. I'll raise a pull request both here and there to make this possible.\n``` bash\ndownload the region-specific cert in PEM format, and install it (default is $HOME/.keystore)\nhttp://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html\n$ keytool -import -alias mysqlServerCACertOR  -trustcacerts -file ~/Downloads/rds-ca-2015-us-west-2.pem\nTell the JVM you trust the cert, by specifying your trustStore and password.\n$ STORAGE_TYPE=mysql MYSQL_HOST=zipkin.abcdefg.us-west-2.rds.amazonaws.com MYSQL_USER=zipkin MYSQL_PASS=notetelling ./mvnw -pl zipkin-java-server spring-boot:run -Drun.jvmArguments=\"-Djavax.net.ssl.trustStore=/Users/acole/.keystore -Djavax.net.ssl.trustStorePassword=FOOBAR\"\ncheck to make sure you are actually able to get data\n$ curl http://localhost:9411/api/v1/services|jq .\n```\n. Closing this as the community is more interested in open api initiative/swagger v2\n. Closing this as the community is more interested in open api initiative/swagger v2\n. cc @yurishkuro \n. > Does this approach only work in deployments with a single collector\n\nprocess, i.e. it's not scalable to multiple collectors?\n\nI presume your question is about the adaptive sampler module in general,\nsince this change doesn't impact the topic.\nThe adaptive sampler works on multiple collectors as a leader is elected to\nmanage updates to the sampleRate based on the storeRate of each member. Ex.\nThere is code to review store rates under a zookeeper path and then\ncalculate what the sample rate should be.\nNext step is to add a test that proves this actually works ;) that said,\nthis code is used at Twitter with a dozen collector instances (or was half\na year ago), so I suspect it does.\n. releasing as 1.28.0\n. The tab is called dependencies and there will only be data if you are\nrunning MySQL or Cassandra with the zipkin-dependencies-spark job.\n. That dzone article is almost 2 years old and doesn't serve as\ndocumentation. The project has been rehosted to OpenZipkin and the UI\nchanged before that happened. The hadoop code to produce an aggregate view\nwas not merged when that article was written either. Long story short: we\ncan't change the past, so if your goal is to get started, why no engage\nwith us in the present?\nAs mentioned in the last thread, the MySQL storage type does query-time\naggregating, which isn't that scalable, but does work for small data sets.\nI have no idea which data store you are using, but assuming it is\nCassandra, you can use the spark job (as we no longer support hadoop)\nhttps://github.com/openzipkin/zipkin-dependencies-spark\nIf you'd like to discuss more, you can join\nhttps://gitter.im/openzipkin/zipkin\n. anytime!\n. No problemo\n. Good idea. I suppose  this is the difference  between unknown  and not know\nyet. Make sure the update also updates the thrift which says empty string\nis not allowed.\n. Hi, yuri. I'm in favor of this (ex making the de-facto label \"unknown\" something a user or instrumentation must opt into).\nThe changes you reverted were added for consistency, particularly to ensure that we don't end up with data that isn't queryable. In your experience, you've found it better to make \"\" unqueryable, which I can certainly see the logic in (basically this is effectively making this field optional)\nCan you add a summary here about the semantics of empty string? I know there are places in docs where we say empty is prohibited, so please do a quick sweep of serviceName accessors to ensure this change doesn't end up confusing ppl\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-thrift/src/main/thrift/com/twitter/zipkin/zipkinCore.thrift#L179\nAlternatively (and because this is in my timezone, and a branch on the main repo), I can make these changes and fold them into this. I expect my suggestions to be documentation affecting only (maybe another test, but yeah).\nwdyt?\n. I wanted to backfill tests before attempting to create the java port https://github.com/openzipkin/zipkin-java/issues/52\n. christmas build is unrelated\n. Thanks @joshlong @dsyer for finding the root problem.\n. christmas build is unrelated\n. cc @yurishkuro this is the minimum test I was looking for\n. cc kafka peeps @clehene @kristofa @espenkm @eirslett @jamescway \n. Here's the undebuggable error when trying to use collector w/0.9 against a 0.8.2.2 broker!\n04:58:52.721 [zipkin_c063683fd01b-1452056330741-96455429-leader-finder-thread] INFO  kafka.consumer.ConsumerFetcherThread - [ConsumerFetcherThread-zipkin_c063683fd01b-1452056330741-96455429-0-172315629], Shutting down\n04:58:52.723 [ConsumerFetcherThread-zipkin_c063683fd01b-1452056330741-96455429-0-172315629] INFO  kafka.consumer.ConsumerFetcherThread - [ConsumerFetcherThread-zipkin_c063683fd01b-1452056330741-96455429-0-172315629], Stopped\n04:58:52.724 [zipkin_c063683fd01b-1452056330741-96455429-leader-finder-thread] INFO  kafka.consumer.ConsumerFetcherThread - [ConsumerFetcherThread-zipkin_c063683fd01b-1452056330741-96455429-0-172315629], Shutdown completed\n04:58:52.755 [ConsumerFetcherThread-zipkin_c063683fd01b-1452056330741-96455429-0-172354147] WARN  kafka.consumer.ConsumerFetcherThread - [ConsumerFetcherThread-zipkin_c063683fd01b-1452056330741-96455429-0-172354147], Error in fetch kafka.consumer.ConsumerFetcherThread$FetchRequest@57ac9ba6. Possible cause: java.lang.IllegalArgumentException\n. Going to merge this so @prat0318 can test it via snapshot. If works, we'll cut a release fixing kafka.\n. use recently released zipkin 1.30.0 for the convenience of not having to override the classpath\n. coolio! cc @jcarres-mdsol as he's been interested in such a thing\n. coolio! cc @jcarres-mdsol as he's been interested in such a thing\n. cc @prat0318 \n. cc @prat0318 \n. took some surgery in the tests. basically we need to isolate topics from another\n. took some surgery in the tests. basically we need to isolate topics from another\n. @kristofa @jcarres-mdsol, this is the promised follow-up to https://github.com/spring-cloud/spring-cloud-sleuth/pull/115 I have an update readying for brave.\n. @kristofa @jcarres-mdsol, this is the promised follow-up to https://github.com/spring-cloud/spring-cloud-sleuth/pull/115 I have an update readying for brave.\n. Slashes were used in the common model doc, which a lot of this came out of,\nso that is how it started. Slashes fit nicely with URNs, for example, as a\nrelative path. Ex google cloud trace keys are exactly the same as these,\nbut have a different implied URN namespace.\nI didn't hear any resistance to slash encoding on the distributed-tracing\nthread that led to this, nor from Jordi, Kristof or others that saw the\npreperatory pull request. Maybe they weren't considering the URL encoding\naspect, or not considering it as a major concern. We should find out it if\nis widely shared before merging.\nSpecifically about URL encoding. You are right that this implies encoding\nthe slashes. That said, you'll notice though the query format of zipkin v1\nalready requires this due to \" and \" syntax. I didn't consider this a\ndriving factor and was hoping to be more common to be others on these keys.\nI didn't enumerate this tradeoff eventhough I thought about it. Thanks for\nbringing it up.\nAnyone else have an opinion about this?\n. Slashes were used in the common model doc, which a lot of this came out of,\nso that is how it started. Slashes fit nicely with URNs, for example, as a\nrelative path. Ex google cloud trace keys are exactly the same as these,\nbut have a different implied URN namespace.\nI didn't hear any resistance to slash encoding on the distributed-tracing\nthread that led to this, nor from Jordi, Kristof or others that saw the\npreperatory pull request. Maybe they weren't considering the URL encoding\naspect, or not considering it as a major concern. We should find out it if\nis widely shared before merging.\nSpecifically about URL encoding. You are right that this implies encoding\nthe slashes. That said, you'll notice though the query format of zipkin v1\nalready requires this due to \" and \" syntax. I didn't consider this a\ndriving factor and was hoping to be more common to be others on these keys.\nI didn't enumerate this tradeoff eventhough I thought about it. Thanks for\nbringing it up.\nAnyone else have an opinion about this?\n. Ex in google cloud trace\n\"http/status_code\" is implicitly\n \"trace.cloud.google.com/http/status_code\" and you can use either in their\nsystem.\n. Ex in google cloud trace\n\"http/status_code\" is implicitly\n \"trace.cloud.google.com/http/status_code\" and you can use either in their\nsystem.\n. @CodingFabian FYI, I have a WIP for the changes in brave, and in case I don't raise that before I fly out.\n. @CodingFabian FYI, I have a WIP for the changes in brave, and in case I don't raise that before I fly out.\n. There's no impact. Your tracer can store \"mary had a little lamb\" if it\nwanted to :P\nThese are suggested labels, with a goal that folks want to use them. For\nexample, if these are logged commonly (eventually), users don't need to\nremember different rules per language or framework. In many cases, there's\nonly one or two frameworks, this is more valuable when there are several\ndifferent tracers contributing to the same system.\n. There's no impact. Your tracer can store \"mary had a little lamb\" if it\nwanted to :P\nThese are suggested labels, with a goal that folks want to use them. For\nexample, if these are logged commonly (eventually), users don't need to\nremember different rules per language or framework. In many cases, there's\nonly one or two frameworks, this is more valuable when there are several\ndifferent tracers contributing to the same system.\n. I haven't heard anyone voice an opinion in favor of the slash delimited syntax. There have been two questions about the former syntax, and one request to retain it. The URN thing is contrived, as we don't have any requirement to support this. I'll see if I can smoke out any strong opinion for slashes, and if not, switch to dots.\n. I haven't heard anyone voice an opinion in favor of the slash delimited syntax. There have been two questions about the former syntax, and one request to retain it. The URN thing is contrived, as we don't have any requirement to support this. I'll see if I can smoke out any strong opinion for slashes, and if not, switch to dots.\n. I think summary in favor of '/' is that it is probably a better delimiter in isolation. However, this isn't truly green field. There's prior art in zipkin using dots. Plus, \"http/path=/foo/bar\" indeed looks worse than \"http.path=/foo/bar\", regardless of URL encoding.\nLong story short.. will change to dots.\n. I think summary in favor of '/' is that it is probably a better delimiter in isolation. However, this isn't truly green field. There's prior art in zipkin using dots. Plus, \"http/path=/foo/bar\" indeed looks worse than \"http.path=/foo/bar\", regardless of URL encoding.\nLong story short.. will change to dots.\n. OK I switched the convention to dots.\nI also added \"http.response.size\" and \"http.request.size\", convention borrowed from google cloud trace (except dots not slashes).\nWhile not widely supported in existing tracers, http size is widely supported in log-based analysis, including common logfile format (which is over 20years old at this point). It would be nice to show folks how to use zipkin instead.\nAny opinions? @yurishkuro @kristofa @jcarres-mdsol @prat0318 \n. OK I switched the convention to dots.\nI also added \"http.response.size\" and \"http.request.size\", convention borrowed from google cloud trace (except dots not slashes).\nWhile not widely supported in existing tracers, http size is widely supported in log-based analysis, including common logfile format (which is over 20years old at this point). It would be nice to show folks how to use zipkin instead.\nAny opinions? @yurishkuro @kristofa @jcarres-mdsol @prat0318 \n. Appears to be, yeah.. Those variables should never reach the repository!\n@abesto if no other reasons than timezone and I'm headed out of country\ntomorrow.. Mind having a look?\n. Appears to be, yeah.. Those variables should never reach the repository!\n@abesto if no other reasons than timezone and I'm headed out of country\ntomorrow.. Mind having a look?\n. Awesome turnaround! Will cut a new release shortly\n. Awesome turnaround! Will cut a new release shortly\n. Good eye!\n. Good eye!\n. I didn't look deeply at why the job fails. If you can find a way for the\ntest to pass meaningfully for the 2 dep stores and spark, please do!\nThanks for having a look.\n. thanks\n. made a comment in the PR. I think the flooring concern shouldn't be conflated with the \"today\" one.\ncassandra's impl will take an input date and floor it to a day. So, they can be retrieved, subject to the window which is a documented aspect of the dependency api. If the tests don't show that working, we can adjust them.\nWhat I'm concerned about is losing a test that shows we can process data older than a day. For example, if all of our tests look at today, we can easily make an impl that cannot process, for example, the data from 2 days ago. After looking at the test in question, I can tell that this aspect is non-obvious.\nThe test data should be explicitly a relative date such as 2 or 3 days ago (which avoids problems around any lookback defaults), as opposed to constants (which it has now). Also, we can rename the test to something like lookback3days which is doubley calls out the nature of the concern.\n. whoot!\n. ps deleted my above note, which worried about the {ParentAndChild, ParentOnly, ChildOnly}. I don't want to cause FUD. I think this is actually not risking the dependency link itself, and the use case of tracking sounds quite interesting.\nIt might make the aggregation work more complex (ex more in a SQL query, or another intermediate state), but as far as I can tell doesn't change the cardinality of dependencyLinks (which was my first concern).\n@yurishkuro mind bumping this into its own issue? I think I'm on board\n. @jcarres-mdsol to be clear, you are suggesting that the concern of reporting uninstrumented edges can be addressed in a different manner. Ex that the dependency graph doesn't need to hold this concern?\n. Why wouldn't the fix be to have the spark job store the data with\ntimestamps according to the spans? It seems like you are saying the data is\ntimestamped for when it was processed, which doesn't make sense at all.\n. Ex. here's the parameter.. it is clearly about the timestamps of the\nspans, not the time the job happened to run\n- @param endTs only return links from spans where\n  [[com.twitter.zipkin.common.Span.timestamp]]\n-              are at or before this time in epoch milliseconds.\n. Thanks, yuri\n. Thanks for keeping an eye, out, yuki. Whatever you find in anorm will\nlikely need to be repeated in zipkin-java (which I can do, if you aren't\ninto java)\nI can't comment too much on this today, but glad the both of you are\nlooking at it.\n. serveraddress was added to allow far-right resources to show up, for example, uninstrumented mysql servers, S3, etc.\nclientaddress was not as it explodes the far left\n. I don't have a strong opinion about who should win \"sa\" vs \"sr\", as long as\n\"sr\" isn't required.\nWould be nicer if we could resolve at collection time, but anyway..\nI don't have a problem with changing the precedence, I would just make a\ncomment in the test with your rationale.\nSeems this is an assumption that clients are more likely to make mistakes\nnaming the destination than servers naming themselves.. I buy that.\n. Yeah I think you understand my concern, which was about a potential\nexplosion on the left side if we named nodes based on clients (fear which\nmight not be substantiated).\nRegardless, you've clarified to me this isn't impacted at all.\n. @t-yuki I don't think you realize how much of a hero you are to me for helping with UI fixes.\n. Can you paste another image?\nSo the endTs and lookback is the same concept here as in the span query (main screen). Can you verify the fields are named the same and look similar? (for continuity's sake across the screens). Hard to tell, and they may already be.\nI'm quite excited as I was interested in being able to see dependencies from a period besides yesterday.\nI wouldn't necessarily say \"analyze dependencies\" as not all stores will do that online, but it is a minor point and I don't know a better word either!\n@jcarres-mdsol @eirslett @kristofa any thoughts?\n. Thanks.\nAs far as I'm concerned, let's merge.\nI did ping others, so will merge tomorrow unless feedback says otherwise.\n. 1.31.0 is on the way with this\n. interesting idea.. I think you mentioned something along these lines in the\npast.\nHow are you thinking this is signaled? We are assuming that the proxies are\ncreating core annotations and/or server address ones I guess, else they'd\nnot show up anyway. Would this be a field or attribute on the span ex. the\n SpanKind type of deal? or a well-known tag/binary annotation?\nOr would the result be specified (ex. indirect attribute on a link), but\nhow that's populated left to the dependency link parser/job?\n. >\n\nA year later, I'm wondering if you came to a solution here.\nsolutions usually aren't a function of time, rather hands available and\ninterest. Ex. elasticsearch hung around until it was implemented w/in a\ncouple weeks. Features need a champion as otherwise they don't get done.\nSeems to me that if you have a->haproxy->b, you could instrument haproxy\ndifferently. For example, it could set specialized annotations on the a->b\nspan instead of the standard cs/sr/ss/cr on its own span? Perhaps it could\nset proxy-receive and proxy-send? I think that would be sufficient to\nprevent the dependency graph from collapsing down as you've described.\nFirstly, I'm not sure if there is instrumentation for haproxy ... if there\nis, please tell! One approach is for proxies to be pass-through (though\nthat's really lack of approach) One of the more recent proxy-like things\nzipkinized is envoy which uses standard annotations. I think we have to\nassume that intermediaries will be instrumented, and changing their\ninstrumentation to fit the graph wouldn't help.\n\nOTOH, we could consider a code option. By the time the data gets to the UI\nor the api, it is too late I think. However, a dependency link job could\ncreating skip links based on a pattern like a service name. This wouldn't\nbe much code, and it doesn't require more data that is already pulled.\n. let's see if we can nail a design down for zipkin v1 model here: #1243\n. related issue: multiple parents aka linked traces https://github.com/openzipkin/zipkin/issues/1244\n. Nice catch.. Yeah byte buffers are easy to get wrong\n. Hi!\nThanks for thinking about this! Our new data stores are in separate github\nrepositories. If it becomes very popular and well maintained, we could\nconsider promoting it into the main repo, but certainly best to at least\nincubate it outside.\nIf you get to a point where you are closer.. Maybe suggest a name!\nPs Are you planning to write this in scala or java?\n. Hi, there.\nWe have 2 storage test cases, which if run will break when upstream changes\nclarify something.\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-cassandra/src/test/scala/com/twitter/zipkin/storage/cassandra\nLuckily, the rate of change is far less. Plus I can help with the repo\nsetup (and watching the tests). If you are interested in collaborating, we\nshould pick a name and work together to make sure the burden is least.\nmaybe zipkin-scala-elasticsearch or something?\nps the actual repo will be similar to\nhttps://github.com/openzipkin/zipkin-dependencies-spark or\nhttps://github.com/openzipkin/zipkin-mongodb\n. https://github.com/openzipkin/zipkin-scala-elasticsearch\n. https://github.com/openzipkin/zipkin-scala-elasticsearch\n. elasticsearch is being very quickly implemented here. It may move to the above repo at some point, probably with a different name as no scala is involved https://github.com/openzipkin/zipkin-java/pull/107\n. good idea.. just nuked it, as agreed it would be confusing to others.\n. good idea.. just nuked it, as agreed it would be confusing to others.\n. This topic comes up a lot.\nIn zipkin, canonical spans come in two forms [\"sr\", \"ss\"] and [\"cs\", \"sr\", \"ss\", \"cr\"], depending on whether the client is instrumented or not.\nFor example, zipkin (and dapper) examples usually do not have client-originated traces. So, the root span usually looks like  [\"sr\", \"ss\"]. If there's an outgoing call, it would look like this  [\"cs\", \"sr\", \"ss\", \"cr\"]. The latter \"sr\", \"ss\" annotations are contributed by the downstream services into the same span.\nWe did throughly review the intent of the address annotations, which are documented here https://github.com/openzipkin/zipkin/blob/master/zipkin-thrift/src/main/thrift/com/twitter/zipkin/zipkinCore.thrift\nIf you haven't seen this, have a look and let me know if there's any questions. No doubt that implementations (and spec data) have been out-of-sync. PS don't forget one primary trace instrumentation is openzipkin/brave.\n. cc @jcarres-mdsol\n. Long story on that one ;)\nIf the server has no propagated context Headers, it usually starts a new\nspan. If the headers are present, those ids are used. If a header says \"not\nsampled\" then no trace is created.\nHappy to chat more about this, too. Are you free to join the tracing\nworkshop Monday, Tuesday? We can review some of these things in a Tokyo\nfriendly time.\nhttps://docs.google.com/document/d/1CWiSc2PaKh9YAg5P5zE7S2q-RkpRv1kqQh_z-HefLng/edit?usp=sharing_eid&ts=563bfa38\n. I mentioned my opinion on another issue.\nHere's it again, I don't believe dependency links should be carrying information like this. It complicates the model for a niche, and introduces questions, like.. what if some links from service A->B are instrumented differently than others? Do we now have 2 links for the same path?\nZipkin primarily serves latency analysis, not software management. The fact that links don't have latency data on them is merely unfinished work. The more responsibilities and other ancillary work we add to the project, the less chance we have for finishing highly sought after, unfinished work.\nI don't think that dependency link is the only reporting structure, and I would love to hear why we assume it must be? The type of change we add to the project must represent highest need, if we aren't to make it a complicated mess again.\n. Here's an alternative: This report could be made in a separate repo and stored in a different table. Probably the UI could be written far cleaner than our current one. An advantage is that the next field of interest about tracers has a far more coherent place to go. The other advantage mentioned is that this leaves the current dependency model clear to actually complete! ie adding latency data, something mentioned multiple times.\nhttp://blog.codinghorror.com/rule-of-three/\nIf a reporting project is made, and no-one besides you use it, we'll know that it shouldn't have gone upstream anyway. We wouldn't have displaced effort from people who aren't interested. If on the other hand, it becomes popular, we've a new microservice of value in zipkin as opposed to an increasingly hard to maintain monolith. IOTW, interested people have a more nimble way to contribute to tracer inventory/adoption projects.\nRegardless, I'd encourage you to brief others, for example at our workshop next week. Maybe you've already met rule of three? Maybe lyft or coursera or someone else also wants this, and would feel stifled if this didn't happen immediately in the existing data structures? Worth finding out, imho\nhttps://docs.google.com/document/d/1CWiSc2PaKh9YAg5P5zE7S2q-RkpRv1kqQh_z-HefLng/edit#\n. is there anyone besides @yurishkuro interested in this?\n. Definitely agree sort in the test assertion vs the store. In zipkin-java we don't have this issue anyway as assertj has a means to express contents equal without implying order.\nI like this change. I think a version of your PR description should be javadoc on the test as it will be understandable later this way.\n. @t-yuki the side-discussion is not about your change, and isn't blocking it in any way\nto put this back on track, one thing I'd like to see is just a general comment on the test as even-though the pull request explains what this is doing, people looking at the test may not be able to guess that from the method name.\nIf you don't have time, I can add the comments and fix the order thing on the way in.\nps @jcarres-mdsol if you want to eye-ball the SQL feel free. I know that we've an outstanding concern, which is to have a solution for when the data set size gets bigger.. something that indexes may not be able to solve. We can leave that sort of concern to another pull request, IMHO (unless there's an obvious index you can think of)\n. ping when this is squashed and ready. I'll merge and release (also fix zipkin-java to do the same thing)\n. very nice work. squash and merge time!\n. @t-yuki about the default lookback.. right now, it is a single parameter that controls both the query and also the dependency tree. I agree that many sites only store 3 days of data. We can re-open the defaults question in another issue, and I don't mind re-summarizing how we got to 7 days\n. 1.32.0 on the way\n. OK, so to test this, we could publish 2 spans make two collectors consume\none each\n. Is updating the low mark something obvious in the api? (ex pastable). I've\nnot done this before. Regardless, good point.. we can have the test pass if\nthe second reads at least one span\n. sorry .. is this literally \"group.id\" in kafka?\n. sorry .. is this literally \"group.id\" in kafka?\n. Thank you for this. Tracegen's annotations certainly would have confused someone (and probably confused me when I started). Now, they won't!\n. This could be a google doc in the features folder, if interested enough: https://drive.google.com/drive/u/1/folders/0B0tSnQT3uGdAfmZaVzZfOXJRRzZPUmRDRFVGRUFHVm1fX2ZXenM1VGN3RElTQzc3azI3X1k\n. Lacking this, there's a fallback order at query time, but this code has proven itself high maintenance. If there was a way to hard-set a service name correctly before storage, this problem space could become smaller.\nex. https://github.com/openzipkin/zipkin/blob/master/zipkin-common/src/main/scala/com/twitter/zipkin/common/Span.scala#L76\nscala\n  /**\n   * Tries to extract the best name of the service in this span. This depends on annotations\n   * logged and prioritized names logged by the server over those logged by the client.\n   */\n  lazy val serviceName: Option[String] = {\n    // Most authoritative is the label of the server's endpoint\n    binaryAnnotations.find(_.key == Constants.ServerAddr).map(_.serviceName).filterNot(_.isEmpty) orElse\n      // Next, the label of any server annotation, logged by an instrumented server\n      serverSideAnnotations.headOption.map(_.serviceName).filterNot(_.isEmpty) orElse\n      // Next is the label of the client's endpoint\n      binaryAnnotations.find(_.key == Constants.ClientAddr).map(_.serviceName).filterNot(_.isEmpty) orElse\n      // Next is the label of any client annotation, logged by an instrumented client\n      clientSideAnnotations.headOption.map(_.serviceName).filterNot(_.isEmpty) orElse\n      // Finally is the label of the local component's endpoint\n      binaryAnnotations.find(_.key == Constants.LocalComponent).map(_.serviceName).filterNot(_.isEmpty)\n  }\n. eek.. you're right, though I think the better fix is to make the tracegen\nnot use scribe\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-tracegen/src/main/scala/com/twitter/zipkin/tracegen/Main.scala#L68\n. eek.. you're right, though I think the better fix is to make the tracegen\nnot use scribe\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-tracegen/src/main/scala/com/twitter/zipkin/tracegen/Main.scala#L68\n. thanks.. incremental progress is good!\n. thanks.. incremental progress is good!\n. here's an issue where binary annotations were a bit confusing (as they allow repeats on keys) https://github.com/openzipkin/brave/issues/136\n. The above concern on repeating keys is more comprehensively reviewed here: https://github.com/openzipkin/zipkin/issues/989#issuecomment-198825497\nIf the span included a copy of the endpoint, it would be possible to support this, though let's bear in mind this isn't a popular feature.\n. When we get to actually implementing this, we should be very explicit about the safety any issues that remain.\nFor example, even if a span is single endpoint, do we accept updates to it? If we do, there's a chance of high-maintenance concerns such as a client sending different names for the same span.\n. When we get to actually implementing this, we should be very explicit about the safety any issues that remain.\nFor example, even if a span is single endpoint, do we accept updates to it? If we do, there's a chance of high-maintenance concerns such as a client sending different names for the same span.\n. Here's my take of the zipkin v2 model. Simplified, focused on single-host spans, not including any new features, removing troublesome or unsupported features. Zipkin v3 could introduce features not present in our storage or rest api, but I believe it best to pare down first.\nHere's an example marked up in java.\n```java\nclass Span {\n  class TraceId {\n    long hi;\n    long lo;\n  }\nenum Kind {\n    CLIENT,\n    SERVER,\n    ONE_WAY\n  }\nclass Annotation {\n    long timestampMicros;\n    String value;\n  }\n@Nullable Kind kind;\nTraceId traceId;\n  @Nullable Long parentId;\n  long id;\nString name;\nlong timestampMicros;\n  long durationMicros;\nEndpoint localService;\n  @Nullable Endpoint peerService;\nList annotations;\n  Map tags;\nboolean debug;\n}\n```. @openzipkin/cassandra @openzipkin/elasticsearch @openzipkin/instrumentation-owners @openzipkin/core finally getting around to this. As mentioned in the previous comment, I don't think it is useful to try and make the model more fancy, rather the pare down to the smallest to support our most used features. Later models can be more fancy after we stabilize.\nThe model above can support our existing rest api, as well be heuristically derived from existing spans. For example, similar heuristics are already in place to export our current zipkin spans into google stackdriver trace, which as mentioned earlier has a similar single-host model.\nIf reactions to the proposal I put are mostly positive, then I can bump out an issue to sort details and any impact to storage, translation, propagation etc. This would be similar to the 128-bit thing where it would likely take a couple months to sort everything out.. @basvanbeek uhh.. click refresh :). >\n\nhow do you know if you should sample this trace?\nThe current model represents what is reported. Only sampled traces are\nreported.\nWould make sense for tags to have timestamp?\ntags currently don't have a timestamp, so adding a timestamp would actually\nbe a feature addition. I'd be quite hesitant in doing that as it has\nstorage impact and is also more complex than a dict\n\nOn Thu, Dec 1, 2016 at 7:45 AM, Jordi Polo Carres notifications@github.com\nwrote:\n\nhow do you know if you should sample this trace?\nWould make sense for tags to have timestamp?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/939#issuecomment-263989900,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xZCd2vLM1YtjXKwqKEbo5gfw5YIks5rDeBUgaJpZM4HS1vF\n.\n. >\nDoes it make sense to merge annotations and tags into a single structure?\nI feel tags should also be a list.\ntags used in search queries and since there's only one host in this data\nstructure, there's no need to have a list (which would allow duplicates).\nNo one should report multiple values for the same tag for the same span on\nthe same host.\n\ndict is a very easy data structure to work with in any language.\nannotations remain timestamped. Due to that, they need to be a list as you\ncan have multiple events logged at the same time.\n. Examples will help a lot. here's a literal translation of an existing client span into the proposed simplified format. Note: on subtle change is suffixing micros to our units. This helps with things like logstash etc which expect timestamps to be milllis or otherwise (meaning you can add a normal timestamp without clashing!)\ncurrent\njson\n{\n  \"traceId\": \"5af7183fb1d4cf5f\",\n  \"name\": \"query\",\n  \"id\": \"352bff9a74ca9ad2\",\n  \"parentId\": \"6b221d5bc9e6496c\",\n  \"timestamp\": 1461750040359000,\n  \"duration\": 5000,\n  \"annotations\": [\n    {\n      \"endpoint\": {\n        \"serviceName\": \"zipkin-server\",\n        \"ipv4\": \"172.19.0.3\",\n        \"port\": 9411\n      },\n      \"timestamp\": 1461750040359000,\n      \"value\": \"cs\"\n    },\n    {\n      \"endpoint\": {\n        \"serviceName\": \"zipkin-server\",\n        \"ipv4\": \"172.19.0.3\",\n        \"port\": 9411\n      },\n      \"timestamp\": 1461750040364000,\n      \"value\": \"cr\"\n    }\n  ],\n  \"binaryAnnotations\": [\n    {\n      \"key\": \"jdbc.query\",\n      \"value\": \"select distinct `zipkin_spans`.`trace_id` from `zipkin_spans` join `zipkin_annotations` on (`zipkin_spans`.`trace_id` = `zipkin_annotations`.`trace_id` and `zipkin_spans`.`id` = `zipkin_annotations`.`span_id`) where (`zipkin_annotations`.`endpoint_service_name` = ? and `zipkin_spans`.`start_ts` between ? and ?) order by `zipkin_spans`.`start_ts` desc limit ?\",\n      \"endpoint\": {\n        \"serviceName\": \"zipkin-server\",\n        \"ipv4\": \"172.19.0.3\",\n        \"port\": 9411\n      }\n    },\n    {\n      \"key\": \"sa\",\n      \"value\": true,\n      \"endpoint\": {\n        \"serviceName\": \"mysql\",\n        \"ipv4\": \"172.19.0.2\",\n        \"port\": 3306\n      }\n    }\n  ]\n}\nsimplified\njson\n{\n  \"kind\": \"CLIENT\",\n  \"traceId\": \"5af7183fb1d4cf5f\",\n  \"parentId\": \"6b221d5bc9e6496c\",\n  \"id\": \"352bff9a74ca9ad2\",\n  \"name\": \"query\",\n  \"timestampMicros\": 1461750040359000,\n  \"durationMicros\": 5000,\n  \"localEndpoint\": {\n    \"serviceName\": \"zipkin-server\",\n    \"ipv4\": \"172.19.0.3\",\n    \"port\": 9411\n  },\n  \"remoteEndpoint\": {\n    \"serviceName\": \"mysql\",\n    \"ipv4\": \"172.19.0.2\",\n    \"port\": 3306\n  },\n  \"tags\": {\n    \"jdbc.query\": \"select distinct `zipkin_spans`.`trace_id` from `zipkin_spans` join `zipkin_annotations` on (`zipkin_spans`.`trace_id` = `zipkin_annotations`.`trace_id` and `zipkin_spans`.`id` = `zipkin_annotations`.`span_id`) where (`zipkin_annotations`.`endpoint_service_name` = ? and `zipkin_spans`.`start_ts` between ? and ?) order by `zipkin_spans`.`start_ts` desc limit ?\"\n  }\n}. Hopefully it is visible in the example that bookend annotations \"cs\" \"cr\" are not needed in the simplified model, yet have the same signal (as you can tell by start, duration, type). ps details, but flexible with how the address annotations are mapped to top level.\nex. localService vs localEndpoint vs endpoint. >\n\n@adriancole https://github.com/adriancole I'm not sure what sort of\nappetite there is for breaking changes here, but ignoring compatibility, I\nwould suggest...\nYeah I had mentioned that this would be a simplification of the current\nmodel, not changing or removing features like debug.\n\nBasically, optimizing for single-host spans, something that can be\nmechanically done in the reporting tier without creating a new envelope\ntype etc.\nI'd like to defer actual changes to a model 3 as there has been no activity\nfor a year due to breaking scope. there's room for benefit now in other\nwords.\n. To put things in perspective, the biggest change needed for\ninstrumentation with the proposal is to not share span ids when\npropagating. I don't want to burden volunteers with unrelated things\nlike adding sample algos etc. we can always do that later.\nThis change doesn't require affecting B3, and wouldn't break use of\ndebug which some use today including the browser plugins. It doesn't\nrequire new data to be captured or presented differently. It doesn't\nrequire us to reshuffle logic or already exposed classes that\ncurrently use our endpoint type.\nBasically the goal is to pare down the model to the features actually\nsupported, which makes it something possible to deliver in a couple\nmonths, reducing cognitive and code complexity without introducing\nmuch work for the volunteers who are impacted by the changes. It is\naffordable and positive, basically.\n. TL;DR;  I've been thinking about how we don't break v3 in process of doing v2. If we change the tags to permit storage of string, number, bool values (as opposed to rejecting them), then the model can upgrade into a more featureful v3 system, particularly with advanced query support.\n\nI think there's definitely a need to keep a v3 in mind when doing v2. How we can eventually support things on our backlog such as analysis or linked traces is important. It is equally important that we can \"upgrade into\" future features (ie usually by adding fields, and at worst removing some). Being future conscious without paralysis or over-engineering today is very tough balance.\nI've know many full-bore systems keep mildly-typed tag values, for filter aggregation or analysis reasons.  For example, AWS X-Ray and google's trace proto have a 3-type value of String bool or number. These can be mapped into json without qualifiers (even though there are some limitations like quoting big numbers). As long as there's only 3 types, we can keep the tags map simple. Basic type is crucial as this allows tags to be a dict vs a list of tuples which is more arduous to map to standard types.\nFor example, eventhough we can't currently query on them, we can still permit basic type tags like so, notably allowing a number.\njson\n  \"tags\": {\n    \"jdbc.result_count\": 43,\n    \"jdbc.query\": \"select distinct `zipkin_spans`.`trace_id` from `zipkin_spans` join `zipkin_annotations` on (`zipkin_spans`.`trace_id` = `zipkin_annotations`.`trace_id` and `zipkin_spans`.`id` = `zipkin_annotations`.`span_id`) where (`zipkin_annotations`.`endpoint_service_name` = ? and `zipkin_spans`.`start_ts` between ? and ?) order by `zipkin_spans`.`start_ts` desc limit ?\"\n  }\nAnother analysis concern is not having to parse the annotations to figure out what they are. One way to accomplish that is to only send a map. However, this is difficult to reverse engineer a user-level description out of, and the lack of that leads to very poor zipkin experience of what I call \"json pooping\". Ex you have a message that is just a json blob. One neat thing I noticed about google's trace proto is that it supports both. For example, the annotation still has a primary description field.. and you can later add classification tags for use in filtering or whatnot. In other words, the feature is something you can \"upgrade into\" because the annotation type just gets an \"extra\" field of tags (as opposed to having its primary field replaced by tags)\nEx. In both v1 and v2, annotations look like this (notably v2 excludes the implicit endpoint):\njson\n{\"timestamp\":1478624611519000,\"value\":\"mr\"}\nin V3, we could add or parse classifying tags to it, without breaking existing code.\njson\n{\"timestamp\":1478624611519000,\"value\":\"mr\", \"tags\": { \"type\": \"messaging\", \"direction\": \"received\"}}. another update, I was working through brave4 and noticed all the conversion stuff can be done mechanically. This means reporters and collectors could easily be changed to support the simplified form now, even if storage query etc aren't updated for a while. In other words, we can do \"version 2\" now, without breaking api.\nThe only thing it requires is that instrumentation who use the new format don't share span ids across a network boundary. I think starting with instrumentation (well technically codec) is a nice way to ease in the change in a way that is in no way a big bang.\nThoughts?. ps I noted a while ago that stackdriver had a heuristic for splitting client+server spans into single host spans. That's here. Uber also have a heuristic for the same task here.\nWhen we get to the point of implementing v2 model, it will likely be the case that storage isn't upgraded, but the UI is. In such a case we can use a splitter similar to one of the above to represent traces in the new format even if the \"raw span\" is still the old model.. The mythical single-host span\nFrom a trace instrumentation point of view, spans have always been single-host.\nThat's because the host of the span is the tracer, and the tracer only reports\nits spans. With that point of view in mind, being required to add endpoint to\nall spans feels laborious and inefficient. This has led to people asking to\nchange the model. I've gone down this path, too, though in the process have \ngrown an appreciation for the existing model. Here's why:\n\n\nRPC spans require no navigation to see the parties involved\nIf you look at the span data presented by the Zipkin api, when RPC data is\ngrouped into a span, it is very easy to tell the timeline of activity between\nthe client and the server. The UI takes advantage of this implicitly. When you\nclick on a span, you automatically see the causal events in the same place:\na client request caused the server response. When client and server are reported\ninto different span IDs, there's a higher tooling burden. Ex I find it very hard\nto compare timestamps when they are nested into different json objects. Other\ntools like clock skew adjusters would have a different burden than today, too.\n\n\nRPC spans permit third parties\nWhen modeling ancillary behavior, such as an intermediary, the current span\nmodel offers more options. For example, a reverse proxy could note a passthrough\nevent with annotations as opposed to a new span. The code is more simple to do\nthis, and the result would end up in the same json/UI panel. On the other hand,\nmost tools are not written assuming there's N members in the same span: The\napproach of starting a new span for an instrumented proxy is still preferred,\nregardless of whether the proxy's span is propagated to the caller or not.\n\n\nRPC spans will exist for a while!\nEven if we move to single-host spans, and address tooling gaps, we have to\nexpect dual-host spans to coexist for a while. This means the canonical model\neither needs to synthesize fake spans from multiple host ones, or work with both\nmodels indefinitely. Synthesized Ids are mostly harmless, but not 100% harmless.\nIt is likely that someone will question a span ID that never existed.\n\n\nWhere to go from here?\nIt seems most sensible to formalize a reporting representation even if the real\nmodel needs to address multi-host for a while. The reporting representation can\nbe translated into the old one, and may eventually become its own model. In the\nmean time, there's a backlog of work needed to really support single-host spans.\nThis includes making the UI detect RPCs that span different IDs, backfilling\ntests and logic for clock skew correction and dependency linking, and probably\nother things. Once the whole system works with single-host spans, we could then\nconsider formalizing it as the \"one model\", and switch storage etc accordingly.. @jcarres-mdsol we can make a V2 api with any type of representation (ex if you are thinking about polishing things about the api, such as query parameters etc). This issue is more about the model. \nWe can add a v2 POST/PUT endpoint which has single-host json model/representation, and that could be used by instrumentation immediately (since they don't report timing information from multiple hosts anyway). The implementation would simply translate it to the existing model before sinking to storage.. @jcarres-mdsol \n\nI think I'm still confused on this one, is single host spans a better representation than shared spans among services?\n\n:) you asked it it is better representation. It is a better representation when you primarily deal with one host, which is the case with tracers.\nex. when we write a client tracing code, we might now do the following\nsetType(client)\nstart()\naddTag(\"http.path\", \"/foo\")\nsetRemoteService(remoteService)\nfinish()\nto map that to the existing span model, it becomes\naddAnnotation(startTimestamp, \"cs\", localService)\naddAnnotation(endTimestamp, \"cr\", localService)\naddBinaryAnnotation(\"http.path\", \"/foo\", localService)\naddBinaryAnnotation(\"sa\", true, remoteService)\nit is very awkward to explain ^^ in docs..\nusing the span model proposed it becomes\nsetType(client)\nsetLocalService(serviceOfTracer)\nsetRemoteService(remoteService)\nsetTimestamp(startTimestamp)\nsetDuration(endTimestamp - startTimestamp)\naddTag(\"http.path\", \"/foo\")\nI think the better is a better representation for tracers to use because...\n it is less jargony (tags vs \"binary annotations)\n it is simpler (string -> string, as opposed to list (string, blob, type, endpoint)\n it is less chatty (no needless repetition of the same endpoint on every annotation or binary annotation.\n it removes ambiguity (sometimes people use the wrong binary annotation type)\nAll these things feed into easier understanding and docs when writing tracers. That means less support time, and people setup for success as opposed to confusion. We'll have less people with bugs (ex where they leave out binaryannotation.host which happens), because such bugs are now impossible.\nBrown field tracers do not need to change. People who already learned the existing format can run with it. The winners here are new users and people who support zipkin. We make it easier for new tracers to get started. We offer them a simplified upload format, devoid of nuanced traps or cognitive burden.\nThe cost of doing this is a parser on the server-side. This is primarily collectors (ex we can say only http and kafka) which accept the simplified format. In the OpenApi/swagger spec, for example, we'd define an alternate supported representation, over the same endpoint or over a different one. It can coexist and I think it is worth the work if the only beneficiary was documentation and new users.. @jcarres-mdsol \n\nI think I'm still confused on this one, is single host spans a better representation than shared spans among services?\n\n:) you asked it it is better representation. It is a better representation when you primarily deal with one host, which is the case with tracers.\nex. when we write a client tracing code, we might now do the following\nsetType(client)\nstart()\naddTag(\"http.path\", \"/foo\")\nsetRemoteService(remoteService)\nfinish()\nto map that to the existing span model, it becomes\naddAnnotation(startTimestamp, \"cs\", localService)\naddAnnotation(endTimestamp, \"cr\", localService)\naddBinaryAnnotation(\"http.path\", \"/foo\", localService)\naddBinaryAnnotation(\"sa\", true, remoteService)\nit is very awkward to explain ^^ in docs..\nusing the span model proposed it becomes\nsetType(client)\nsetLocalService(serviceOfTracer)\nsetRemoteService(remoteService)\nsetTimestamp(startTimestamp)\nsetDuration(endTimestamp - startTimestamp)\naddTag(\"http.path\", \"/foo\")\nI think the better is a better representation for tracers to use because...\n it is less jargony (tags vs \"binary annotations)\n it is simpler (string -> string, as opposed to list (string, blob, type, endpoint)\n it is less chatty (no needless repetition of the same endpoint on every annotation or binary annotation.\n it removes ambiguity (sometimes people use the wrong binary annotation type)\nAll these things feed into easier understanding and docs when writing tracers. That means less support time, and people setup for success as opposed to confusion. We'll have less people with bugs (ex where they leave out binaryannotation.host which happens), because such bugs are now impossible.\nBrown field tracers do not need to change. People who already learned the existing format can run with it. The winners here are new users and people who support zipkin. We make it easier for new tracers to get started. We offer them a simplified upload format, devoid of nuanced traps or cognitive burden.\nThe cost of doing this is a parser on the server-side. This is primarily collectors (ex we can say only http and kafka) which accept the simplified format. In the OpenApi/swagger spec, for example, we'd define an alternate supported representation, over the same endpoint or over a different one. It can coexist and I think it is worth the work if the only beneficiary was documentation and new users.. note to all in the thread :)\nthis issue was created because zipkin v2 never happened as it had too much scope. This issue initially discussed tackling the complexity in the model. It is still that.\nAs this issue nears a year in duration with no activity beyond discussion, I've suggested we pare it down further:\nAllow a simpler representation format for tracers. Accept this format and convert to the existing one on collection.\nI think the V2 word is scarier than it needs to be. This is a feature, just like a propagation format would be for tracers. The party with the most responsibility in this feature is those who maintain Zipkin's docs and collector code. Not doing this means we continue to punt known complexity problem further down the road.\nIf folks feel strongly we shouldn't do this, those folks need to roll up sleeves to help when questions and concerns happen during onboarding. Ex contributing to docs,  gitter, and issue resolution when people get confused about what a binary annotation is and send the wrong data. If we prefer this route, please say so and help me with the burden of today.. note to all in the thread :)\nthis issue was created because zipkin v2 never happened as it had too much scope. This issue initially discussed tackling the complexity in the model. It is still that.\nAs this issue nears a year in duration with no activity beyond discussion, I've suggested we pare it down further:\nAllow a simpler representation format for tracers. Accept this format and convert to the existing one on collection.\nI think the V2 word is scarier than it needs to be. This is a feature, just like a propagation format would be for tracers. The party with the most responsibility in this feature is those who maintain Zipkin's docs and collector code. Not doing this means we continue to punt known complexity problem further down the road.\nIf folks feel strongly we shouldn't do this, those folks need to roll up sleeves to help when questions and concerns happen during onboarding. Ex contributing to docs,  gitter, and issue resolution when people get confused about what a binary annotation is and send the wrong data. If we prefer this route, please say so and help me with the burden of today.. >\n\nI am all for this to happen. I think I was confused by all this shared\nspans vs single-host, etc.\ngood point, this became clear to me too, that choosing one or the other is\npolarizing and wasn't going to result in a tactical gain. I think I failed\nto make that obvious before.\nWe are talking about simplifying the json sent to the http API. Then the\nearlier the better.\ngreat. I'll start carving something out.\n\none thing I noticed in practice is that it is more flexible and consistent\nto record a finish with a timestamp vs a duration. (especially as there's\nno granularity loss as both are microseconds). I'll open a new issue with\nthe proposed format (if for no other reason than to make it easier to read\n:))\n. >\n\nI am all for this to happen. I think I was confused by all this shared\nspans vs single-host, etc.\ngood point, this became clear to me too, that choosing one or the other is\npolarizing and wasn't going to result in a tactical gain. I think I failed\nto make that obvious before.\nWe are talking about simplifying the json sent to the http API. Then the\nearlier the better.\ngreat. I'll start carving something out.\n\none thing I noticed in practice is that it is more flexible and consistent\nto record a finish with a timestamp vs a duration. (especially as there's\nno granularity loss as both are microseconds). I'll open a new issue with\nthe proposed format (if for no other reason than to make it easier to read\n:))\n. https://github.com/openzipkin/zipkin/issues/1499\n. https://github.com/openzipkin/zipkin/issues/1499\n. A reminder that things are easier if we don't do mixed types for tags (ex map has value of type string or bool). Not only does this not work in elasticsearch mappings (first type wins), but also it is complex in json schema https://github.com/openzipkin/zipkin-api/issues/27. I think one problem is that the kafka receiver is literally writing span-at-a-time, while the span stores accept spans in bulk. I'm presuming the span stores are optimized for bulk, but we haven't benchmarked this.\nNote there's another related discussion going on w/ @yurishkuro and @danchia. For example, Yuri had a suggestion about intermediating here. https://github.com/openzipkin/zipkin/issues/961#issuecomment-182956930\n. Exactly. SpanStore doesn't require the bundle of spans it receives to be in\nthe same trace. A few tracers do send bundles at a time, subject to either\nspan count or bundle size.\nRight now, the Kafka receiver literally reads only one span from a message.\nIt doesn't matter if the span is 200bytes or 2megs.\nI think the first thing we can try is just allowing the receiver to accept\na bundle.. Not sure if we try to read a list and only one is present.. If\nthat works or not in thrift.\nOnce the receiver can accept N spans (aka a bundle) then instrumentation\ncan choose how much to send per message.\nMake sense?\n. Exactly. SpanStore doesn't require the bundle of spans it receives to be in\nthe same trace. A few tracers do send bundles at a time, subject to either\nspan count or bundle size.\nRight now, the Kafka receiver literally reads only one span from a message.\nIt doesn't matter if the span is 200bytes or 2megs.\nI think the first thing we can try is just allowing the receiver to accept\na bundle.. Not sure if we try to read a list and only one is present.. If\nthat works or not in thrift.\nOnce the receiver can accept N spans (aka a bundle) then instrumentation\ncan choose how much to send per message.\nMake sense?\n. I opened #979 about multiple spans. Still, there's a question about\n88milliseconds going unaccounted for\nCurrently, the collector itself isn't instrumented, so it is hard to get\nwhat could be the problem.\nIs it possible for you to run the collector against a different span store?\nIf, for example, you can run using collector-dev.scala, you could possibly\nnarrow any non-kafka-related delays.\n. we should close this issue with a pull request updating zipkin/zipkin-collector/kafka/README.md with notes on how to achieve best performance. there are some notes here: https://docs.google.com/document/d/1Px44fjZ37gr05lV7UFo8AfrWZCcJHCuv58290XCbDaw/edit#bookmark=id.eeozlmh0fxr\n. we should close this issue with a pull request updating zipkin/zipkin-collector/kafka/README.md with notes on how to achieve best performance. there are some notes here: https://docs.google.com/document/d/1Px44fjZ37gr05lV7UFo8AfrWZCcJHCuv58290XCbDaw/edit#bookmark=id.eeozlmh0fxr\n. Hmm I wonder if the CA and SA pointing to themselves is a configuration bug?\n. Hmm I wonder if the CA and SA pointing to themselves is a configuration bug?\n. Looking at the finagle code, it seems finagle indeed logs ClientAddr as the same service name of the local process.. It records the client-side of the socket.\nYeah seems if we look at \"CA\" (ex to track uninstrumented clients of the root span), we likely have to deal with this curious \"SA\" = \"CA\" cas of finagle..\n. Looking at the finagle code, it seems finagle indeed logs ClientAddr as the same service name of the local process.. It records the client-side of the socket.\nYeah seems if we look at \"CA\" (ex to track uninstrumented clients of the root span), we likely have to deal with this curious \"SA\" = \"CA\" cas of finagle..\n. I agree that supporting finagle and also supporting \"ca\" to resolve uninstrumented root spans requires work like this.\nI do feel we are digging a hole and more edge cases will make this SQL quite unwieldy. I'm cool merging for now as we can clean that up later. Ex I have alternative SQL (with client side joins) readying for a pull request soon on the java project.\n. I agree that supporting finagle and also supporting \"ca\" to resolve uninstrumented root spans requires work like this.\nI do feel we are digging a hole and more edge cases will make this SQL quite unwieldy. I'm cool merging for now as we can clean that up later. Ex I have alternative SQL (with client side joins) readying for a pull request soon on the java project.\n. Thanks for the help, Yuki!\n. Thanks for the help, Yuki!\n. So happy to see this! I can test manually in a couple hours.\n. So happy to see this! I can test manually in a couple hours.\n. Tests run alright, just it hangs on bin/web\n```\n$ bin/web \n:zipkin-common:compileJava UP-TO-DATE\n:zipkin-common:compileScala UP-TO-DATE\n:zipkin-common:processResources UP-TO-DATE\n:zipkin-common:classes UP-TO-DATE\n:zipkin-common:jar UP-TO-DATE\n:zipkin-web:compileJava UP-TO-DATE\n:zipkin-web:compileScala UP-TO-DATE\n:zipkin-web:processResources UP-TO-DATE\n:zipkin-web:classes UP-TO-DATE\n:zipkin-web:nodeSetup\n:zipkin-web:npmSetup UP-TO-DATE\n\nBuilding 78% > :zipkin-web:npmInstall\n``\n. Tests run alright, just it hangs onbin/web`\n\n```\n$ bin/web \n:zipkin-common:compileJava UP-TO-DATE\n:zipkin-common:compileScala UP-TO-DATE\n:zipkin-common:processResources UP-TO-DATE\n:zipkin-common:classes UP-TO-DATE\n:zipkin-common:jar UP-TO-DATE\n:zipkin-web:compileJava UP-TO-DATE\n:zipkin-web:compileScala UP-TO-DATE\n:zipkin-web:processResources UP-TO-DATE\n:zipkin-web:classes UP-TO-DATE\n:zipkin-web:nodeSetup\n:zipkin-web:npmSetup UP-TO-DATE\n\nBuilding 78% > :zipkin-web:npmInstall\n```\n. Thanks for the progress, Eirik. I still am having trouble locally, but\nmaybe I'm the only one. cc @abesto (in case your local setup works)\n\nOne thing I want to make sure is that the end distribution actually ends up\nworking.. I tried this and it seemed to be missing static content.\n$ ./gradlew :zipkin-web:clean :zipkin-web:shadowJar\n$ java -jar ./zipkin-web/build/libs/zipkin-web-*-all.jar\n. Thanks for the progress, Eirik. I still am having trouble locally, but\nmaybe I'm the only one. cc @abesto (in case your local setup works)\nOne thing I want to make sure is that the end distribution actually ends up\nworking.. I tried this and it seemed to be missing static content.\n$ ./gradlew :zipkin-web:clean :zipkin-web:shadowJar\n$ java -jar ./zipkin-web/build/libs/zipkin-web-*-all.jar\n. I can debug what's going on locally, but curiously the build succeeds yet\nwithout the static resources. Seems it should fail (ex we would want it to\nfail vs publish bad jar)\nIn my local env, the only way I can get it to hang is by doing bin/web. I\nwill look into it as I definitely want this change.\n. Progress, as bin/web now errors out :)\nWe should add zipkin-web/node_modules to the cache section in .travis.yml, right? cc @abesto \n. after upgrading my local node to 5.5 and nuking zipkin-web/node_modules.. bin/web works (albeit slower to start than before due to node processing things)!\n. ps another thought.. since node was crapping out due to version-things.. might want to check that travis' node doesn't crap out. right now, running tests doesn't actually invoke node, so a failure would wait until post merge. maybe we can do something to make node do things eagerly on pull request?\n. adding travis caching and will watch the build and merge on green (as looks like the other comments are different PRs). Great job @eirslett and thanks for the advice, @abesto!\n. not sure if that PR is the smoking gun or not.\nThe NPE suggests that the call to\nannotationKey(serviceName: String, annotation: String, value: Option[ByteBuffer]) had an option of null at the end.\nwould need to track calls to this to figure out what's passing null.\n. not sure if that PR is the smoking gun or not.\nThe NPE suggests that the call to\nannotationKey(serviceName: String, annotation: String, value: Option[ByteBuffer]) had an option of null at the end.\nwould need to track calls to this to figure out what's passing null.\n. NPEs are bugs! I'll fix it\n. NPEs are bugs! I'll fix it\n. logic sounds sensible. ack that it should have skipped (vs dropping the local spans) in the first place\n. logic sounds sensible. ack that it should have skipped (vs dropping the local spans) in the first place\n. > @adriancole currently the change is limited to LC spans. It breaks one\n\ntest in the span store spec that expects span-reported duration to be\noverwritten by the adjuster.\nAt first thought, I don't think duration should change, either... Either\nthere's a non-intuitive reason why duration is changing, or there's a bug\nin the adjuster.\nShould this actually apply to ALL spans? I.e. I can't think of a good\nreason for the adjuster to ever override the span's timestamp / duration\nbased on the annotations, it should only be allowed to shift the timestamp\nby the skew.\nI don't quite understand what you mean, but I want to. I agree that\nadjusting the duration sounds curious. Let's place duration aside for a sec.\n\nAre there conditions where the adjuster is incorrectly adjusting/overriding\nthe timestamp?\n. Wait.. If there is clock skew, where one span should be shifted right, its\nparent duration would be longer. I suppose what you are saying is that we\nshould only set the parent's duration if it was derived by annotations.\nThe problem is that there's a separate function to set duration from\nannotations. If we want to make sure duration is set properly, the clock\nskew correction has to happen before missing span.duration is filled in.\n. ok just looked at the code.. seems clock skew is applied before span.timestamp, duration is derived from annotations.\nSo, to make sure we don't botch instrumentation-assigned duration, we need to do the following..\nFirst, change the existing clock skew test to not set timestamp, duration on the parent and comment why. Basically, by leaving these unset, we can see if skewed, annotation-derived duration is set properly. The test impl should be the same.\nNext, add a skew test where a parent's duration is set explicitly, but children are derived from annotations. One way to do this is to take the spans from the above test and make them a child of a span with an explicitly-set duration. In this case the parent will have its timestamp shifted, so that children happen after it, but the parent's duration is left alone. The child tree should be affected the same as the existing test.\n@yurishkuro does ^^ capture your concern?\n. opened https://github.com/openzipkin/zipkin/issues/955 on the other topic\n. opened https://github.com/openzipkin/zipkin/issues/955 on the other topic\n. closed via #954\n. closed via #954\n. cc @yurishkuro \n. cc @t-yuki @yurishkuro this was hard to notice!\nthe local spans in this test were unqueryable in implementations that don't set a fallback span.timestamp\n. > @adriancole do you mean they were unqueryable because the storage did not\n\nindex them by the timestamp since it was missing?\nYep.\n. awesome.. this breaks the build :P Thanks for noticing, @yurishkuro \n\ncc @abesto looks like the main thing here is we should be careful to not duplicate yaml keys as travis silently chooses the last\n. cc c* folks @drax68 @michaelsembwever @yurishkuro @prat0318 @danchia\n. verified by running the snapshot jar\n. @yurishkuro do you have an opinion on this?\n. I have a similar opinion. will change the clock skewer to never adjust\nspan.duration\n. @michaelsembwever @yurishkuro @prat0318 @danchia any reason we shouldn't expose CASSANDRA_MAX_CONNECTIONS?\n. Here's an impl https://github.com/openzipkin/zipkin/pull/958\n. @danchia @yurishkuro booting the batch discussion to  #961 where it is easier for others to identify\n. fix pending on whether we want two properties (for local and remote), as this issue was brought up about remote.\n. Thanks for the fix! I'm curious why files are going into zipkin-web/src/main/resources/ vs target/something.. anyone know why?\n. I think the main idea would be that it could keep \"clean\" working\nconventionally in gradle world, where transient stuff is under build/ OTOH,\nmaybe the current location is more idiomatic outside gradle?\n. cc @yurishkuro \n. Harvested from comments on #956\n@danchia \n\none improvement I had planned to test was batching writes to Cassandra into small, unlogged batches, which I think would improvement performance.\n\n@yurishkuro\n\nI've been told quite the opposite by the team with more experience with Cassandra. A batch can span multiple nodes, so batching puts more load on the coordinating node, rather than that work to be done by a client with token-aware connection.\n\n@danchia \n\nI was basing it off: https://dzone.com/articles/efficient-cassandra-write, which could mean there is some benefit to be had if we have locality.\n\nI think the other thing that was motivating me to batch was that AFAIK we wait for the each trace to be stored before processing the next one, at least for Kafka. This greatly reduces potential throughput than say, if we allowed for up to X number of store operations in flight in the collector.\n@yurishkuro\n\nThanks for the link, Daniel. It seems the articles agrees with what I said, that fan-out from the client is more efficient then batching and sending to a coordination node. But if collector reads KAFKA and writes to C one span at a time, then yes it would be bad and batching might help. It's just you'd be optimizing the wrong end. We don't use collectors over KAFKA yet (actually working on it for another use case), but our collectors (in Go) have an internal queue which is feeding N go-routines, over a token-aware connection. The same thing can be easily done in Scala/Java collectors.\n\nAlso worth noting that Cassandra connections are multiplexing, I think up to 128 or 256 streams, but we haven't gone that high in the # of threads.\n. spot checked one with a missing span name.. and it was a local span.\n``` json\n  {\n    \"traceId\": \"1a6f929826b14a57\",\n    \"name\": \"async:ingredientsfetchcontroller#method=ingredients\",\n    \"id\": \"3dbcebf1dfa30d29\",\n    \"parentId\": \"f88ee39a5700b1db\",\n    \"timestamp\": 1.455188843323e+15,\n    \"duration\": 1000,\n    \"annotations\": [\n],\n\"binaryAnnotations\": [\n  {\n    \"key\": \"lc\",\n    \"value\": \"unknown\",\n    \"endpoint\": {\n      \"serviceName\": \"brewing\",\n      \"ipv4\": \"127.0.0.1\",\n      \"port\": 9992\n    }\n  }\n]\n\n}\n```\n. can't look deeper now, but next step is to pare down the data to include spans whose service name are not on the left. Then analyze them.\nEx. is there a common trait among these (or common traits?)\nlike.. spans missing core annotations? (\"cs\" \"sr\" \"ss\" \"sr\")? spans that are local? Is there something about the json itself that looks different than ones that display serviceNames? Does api/v1/services include the missing service name? Does api/v1/spans?serviceName=missing_name work?\nI think the rendering code is this, but we could probably narrow down a bit more, as it makes testing easier\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-web/src/main/resources/app/js/component_ui/trace.js\n. cc'ing some who might have an opinion: @marcingrzejszczak @yurishkuro @clehene\n. Is the attached json the input or output? Ex is this what was posted to the\nzipkin server or what was returned by the query? Reason I ask is that the\nquery response includes clock skew adjustment.\n. Couple things can help make it easier for people to pick up the sort of\nissues you are raising (me or otherwise).\nYou'll notice Yuri, Yuki and I usually write a failing test or otherwise.\nIt usually isn't a UI issue, rather a data one.\nI guess you believe this is a JavaScript problem or something in the\nmustache code, as you tagged this as a UI bug.\nCan annotate the image with an arrow or something pointing to where you\nthink something can be.  Please also pare down test data to only a few\nspans, and mention specifically the span data (ex timestamp on which span)\nyou think the UI is misunderstanding.\nThat makes issues like these easier to validate. If this isn't a UI issue\n(rather something around the api, it might need to go to another repo.\nThe more you can clarify, the easier it is for others to help you.\n. Nice progress.. Thanks!\n. Yuri, you mind adding/creating README on zipkin-web with this build note?\n. It is probably easy to blame the UI on this, but I would check the api\nfirst.\nDoes the api dependency query return data that is correct, but the UI\nmisrenders?\nAlso, please open only one issue per type of concern. I think you already\nopened one about service names, right?\n. The dependency data comes from the query server. If the UI doesn't render\ndata, it is a problem in the UI. If the query server doesn't return data,\nit is a data problem or a problem in the aggregation.\nPlease move this issue to zipkin-java as the way you have presented it, it\ndoesn't seem to be a UI issue.\nI don't want to distract people here on something that is likely not to be\nUI dependent.\nIf it were a UI issue, the api would be returning data that the UI didn't\nprocess. You can see for yourself by either hitting the api dependency\nendpoint yourself or turning on chrome debug etc.\n. LGTM\n. @yurishkuro good idea. I'm also guilty of not logging an overall \"what I'm working towards\" thing pretty often.\n@eirslett LGTM\n. cc @yurishkuro @joshlong\n. In my environment, gradle/idea-mark-config-dir-as-resource.gradle seems to have worked, as I don't have to do anything to have the resulting idea files correct.\nbash\n$ ./gradlew cleanIdea idea\n--snip--\n$ grep 'file://\\$MODULE_DIR\\$/config' */*.iml\nzipkin-collector-service/zipkin-collector-service.iml:      <sourceFolder url=\"file://$MODULE_DIR$/config\" isTestSource=\"false\" type=\"java-resource\"/>\nzipkin-query-service/zipkin-query-service.iml:      <sourceFolder url=\"file://$MODULE_DIR$/config\" isTestSource=\"false\" type=\"java-resource\"/>\n. @yurishkuro do you have any particular use case where the key and value is the same for different hosts in the same span, and knowing that would have helped solve a latency issue?\n. One thing to bear in mind here is that I don't think we've ever raised here is some UI changes imply cementing in aspects of the zipkin data model, which aren't universally popular and may limit future model choices.\nFor example, the binary annotation data structure is not popular. There exists a sentiment that they could be simpler, ex a String->String map. If we formally support reporting the originating host of a binary annotations, this limits model options to those that intentionally allow repeats. Zipkin v2 could solve this, if it stays single-host spans. If it doesn't, and a String->String map was chosen, then changing the UI to report the origin of a tag wouldn't be supportable.\nThis situation is not new, by the way, just solving it hasn't proved popular. Even here, multiple folks are working around this as opposed to asking for this to be changed. Ex filtering duplicates, or they have adjusted their tracers to avoid the ambiguity. Personally, I'm happy about that, as the act of \"choosing\" on conflict for most fields, particularly span name, has proven high-maintenance code. Leaving that undefined for routine binary annotations is less code to maintain.\nSince the time we introduced zipkin's http api, one could troubleshoot any issue like this by looking at the json for the trace. In fact, I noticed in a screen shot that Uber actually have \"json\" UI element, which could make navigation even easier. I'd recommend we sit on this and consider it for zipkin v2 vs investing further time into this topic, as rehashing this hurts the project by displacing time.\nIn the mean time, we might want to formalize a UI element to make getting to the raw json easier.\n. I'm over quota rehashing this, but trying again.\nbinaryannotation.host is sometimes set, so yes when it is present the UI\ncould display it.\nThat display choice will cost code, for example whether to log the same\npair twice when values agree or not. This type of code often breaks.\nOthers have mentioned on this thread that they filter out duplicates, just\nlike we do now. Also, knowing the host is a special case. Even if a feature\nis possible to make, that doesn't mean we should.\nFinally, if we do this and choose to make the model use string-string tags\n(which dont include host) as discussed in the past, we will break the\nfeature.\nRegardless, it is so unpopular that it doesn't deserve this kind of\nattention. Either someone should implement it in a supportable way, or\nclose it. No free lunch\n. Shorter why not:\nWidening the UI from key-value to (key, value, host) causes the following.\n- Clutter for those who perceive tags as unique per key. Ex multiple rows\n  saying http.method=GET for each host\n- More code to maintain if we choose to be less cluttery (ex suppress on\n  identical key/value, etc)\n- More difficult decisions later, especially around data structures to\n  represent tags. This is what i am moat concerned with.\nRight now, the path is clear as \"binary annotation\" and all its complexity\nisn't presented to the user in a way that would prevent us from moving to\nplain string-string.\nAs both someone who often has to fix bugs in code like this, yet also one\ninterested in actually improving our model to prevent them.. I really don't\nlike this feature as it will almost certainly cause burden for very little\ngain.\n. Thanks for thinking it through.. Bus factor increase!\n. FYI current workaround when people are sending the same key from multiple services is to look at the json. Looking at the json will be easier if anyone implements the json button https://github.com/openzipkin/zipkin/issues/1060\n. Here's some thoughts about choices we can make besides closing this issue.\nDisplay service name in binary annotations knowing what we know\nWe have discussed this issue, and know that many instrumentation projects dodge duplicating binary annotation key/value pairs in the same span. We also know the model won't likely support multimaps with endpoints in the future. We could choose to make this change, inverting the UI element to showing more detail as opposed to less, eventhough we know the constraints of this.\nIf we did something like this, we should explicitly agree. For example, multiple people. When it gets to implementation, we should make the UI element consistent, either by emulating the style of annotation rows, or changing the style of annotation rows. For this reason, we should be careful to review pull requests carefully. Ex. Incomplete screen shots can hide inconsistency.\nMake it easier to fork the UI\nThe UI no longer has any scala dependency etc. This particular change is not the only one that sites may want to customize. For example, they may want to change the titles, favicons etc. Rather than making everyone see an edge case, we can encourage forks, possibly by moving the UI into a separate repo. When we work in such a model, it means we can delay loaded topics like this until they become more widely requested.\nyour idea here\n. Thanks for the continued line of bug fixes @eirslett and thanks for the watchful eye and engaging comments, @yurishkuro!\n. last build before this was 6s faster, but most recent ones are 30s faster than this.\n. thinking we merge this to encourage test writing... I'm personally not deterred by the build time creep.\n. Ok so cool to merge this, of do you want to add more to it?\n. My guess is the most important  state is if the spanstore connection is\nOK.  I made a interface like this at Netflix and then exported it to the\nhealth check handler. It is better than just http and simpler than checking\nmany things.\nBasically we add a method span stores need to implement that returns true\nif OK.  Span stores use the cheapest valid way to do that. We can export\nthat to an http call (which load balancers etc can check)\nEx from a DNS project\nhttps://github.com/Netflix/denominator/blob/master/route53/src/main/java/denominator/route53/HostedZonesReadable.java\n. looks like there's an OK in the query server already:\nhttps://github.com/twitter/finatra/blob/master/inject/inject-server/src/main/scala/com/twitter/inject/server/TwitterServer.scala#L112\n$ curl localhost:9901/health\nOK\nDo we want to map health onto the main port? (9411)\n. ok, so localhost:9411/health?\n. cc @clehene @kristofa @prat0318 \nso after this.. instrumentation can choose to either TBinaryProtocol encode a single span, or a list of spans.\n. I doubt there is an existing benchmark for any Kafka zipkin reporter, much\nless single span vs list. I think the key here is that list gives more\nflexibility for instrumentation, including a means to benchmark and report\nback.\nPersonally, I am cool chasing instrumentation to have them switch to list,\nas list of one is only several bytes overhead. Kafka is still newish\n(except Ruby).\nAlso happy to add a \"log once\" message that notes the span id and endpoint\nof single-span Kafka messages. That could prevent logs from cluttering.\nI agree this is hacky just don't know a cheaper way to help folks move off\nsingle span without breaking them.\nAdding a key to the message would seem less hacky, but take more discussion\nfor example. I was hoping to save that energy for v2 (ack list-only\n.)\nI also think benchmarks would be helpful, but there's a chicken egg.\nIdeally, I would like Prat to respond back when he can try list.\nI'm unsurprised about meh reactions.. Anyone feel we shouldn't go down the\npeek path with above context in mind?\n. I've not heard any feedback against from a kafka transport user, and this clearly will help @prat0318 and move us in the right direction of moving towards lists as the defacto unit-of-transport.\nmerging\n. cc @jcarres-mdsol @eirslett @yurishkuro \n. @liyichao check this out.. it is confirmed (even if not necessarily the right thing to do) #2234. there was a bug actually.. https://github.com/openzipkin/zipkin/pull/2235\nOn Mon, 5 Nov 2018, 18:13 Li Yichao, notifications@github.com wrote:\n\nThen I misunderstand the meaning, which is not that straightforward. Close\nthe issue if you think it is not a problem.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/998#issuecomment-435822018,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61ykUWIMnEoDjkwoH0Ably1Ad4hQGks5usA80gaJpZM4HhYJm\n.\n. tracegen doesn't return the same data twice. to look into this, we'd need the data that you are seeing. you can grab it via http://your_host:9411/api/v1/trace/trace-id\n. fyi on this and other ui issues, I'd suggest holding off until zipkin-web is javascript only. There's a number of small issues, but sands are shifting. Once the big refactor is over, things will be easier. https://github.com/openzipkin/zipkin/pull/1004\n. absolutely agree. This is the older behavior. At some point, we should move this default to /config.js\n. this default was accidental for the dependencies view, so reverting to the previous default of 1 day wfm\n. I think gzip was added or verified some time back for GET requests. If it\ndoesn't work, definitely we should make it work.\n. Not an extremely intelligent algo (no threshold), but gzipped GET does work!\n\n``` bash\n\u2714 ~/oss/zipkin [master \u2193\u00b72|\u20261684\u2691 9] \n21:43 $ curl -v -s localhost:9411/api/v1/spans?serviceName=zipkin-query\n   Trying ::1...\n Connected to localhost (::1) port 9411 (#0)\n\nGET /api/v1/spans?serviceName=zipkin-query HTTP/1.1\nHost: localhost:9411\nUser-Agent: curl/7.43.0\nAccept: /\n< HTTP/1.1 200 OK\n< Content-Type: application/json; charset=utf-8\n< Server: Finatra\n< Date: Thu, 25 Feb 2016 13:43:22 +00:00\n< Content-Length: 19\n< \n Connection #0 to host localhost left intact\n[\"bootstrap\",\"get\"]\u2714 ~/oss/zipkin [master \u2193\u00b72|\u20261684\u2691 9] \n21:43 $ curl -v --compress -s localhost:9411/api/v1/spans?serviceName=zipkin-query -H \"Accept-Encoding: gzip\"\n   Trying ::1...\n Connected to localhost (::1) port 9411 (#0)\nGET /api/v1/spans?serviceName=zipkin-query HTTP/1.1\nHost: localhost:9411\nUser-Agent: curl/7.43.0\nAccept: /*\nAccept-Encoding: gzip\n< HTTP/1.1 200 OK\n< Content-Type: application/json; charset=utf-8\n< Server: Finatra\n< Date: Thu, 25 Feb 2016 13:43:25 +00:00\n< Content-Encoding: gzip\n< Content-Length: 45\n< \n* Connection #0 to host localhost left intact\n[\"bootstrap\",\"get\"]\u2714 ~/oss/zipkin [master \u2193\u00b72|\u20261684\u2691 9] \n```\n. cc @mohsen1 @eirslett @yurishkuro @dsyer @abesto @jcarres-mdsol \n. fyi /dependencies is in what I'm calling the \"query api\". basically the\ndocs are for everything under /api/v1. A request was made to host api\ndocumentation on the api root (vs 404)\n. I think the packaging step is helpful for derivative work. For example, the\nscala impl may expose static static assets differently that spring boot.\nFor example, spring boot might use \"web jars\".\n\nIn any case, the goal is the same, which is to add documentation to a\ncomplete implementation of the zipkin api. We put effort into getting docs\ntogether and this is a concrete way to put it in front of users.\n. #1959 . nice!\n. looks like a mild precision error in the assertion of durationStr.\nAssertionError: expected [ { traceId: '56bea38b40e95f83',\n    startTs: '02-29-2016T09:09:45.388+0800',\n    timestamp: 1456708185388000,\n    duration: 3948.889,\n    durationStr: '3948.889ms',\n    servicePercentage: 100,\n    spanCount: 1,\n    serviceDurations: [ [Object], remove: [Function] ],\n    width: 100 },\n  remove: [Function] ] to deeply equal [ { traceId: '56bea38b40e95f83',\n    startTs: '02-29-2016T09:09:45.388+0800',\n    timestamp: 1456708185388000,\n    duration: 3948,\n    durationStr: '3.948s',\n    servicePercentage: 100,\n    spanCount: 1,\n    serviceDurations: [ [Object], remove: [Function] ],\n    width: 100 },\n  remove: [Function] ]\n. Here's a longer one..\nUncaught AssertionError: expected [ { traceId: '56bea38b40e95f83',\n    startTs: '02-29-2016T09:09:45.388+0800',\n    timestamp: 1456708185388000,\n    duration: 3948.889,\n    durationStr: '3948.889ms',\n    servicePercentage: 100,\n    spanCount: 1,\n    serviceDurations: [ [Object], remove: [Function] ],\n    width: 100 },\n  { traceId: '249628f4fa8f23d3',\n    startTs: '02-29-2016T09:21:10.066+0800',\n    timestamp: 1456708870066000,\n    duration: 50,\n    durationStr: '50.000ms',\n    servicePercentage: 57,\n    spanCount: 8,\n    serviceDurations: [ [Object], [Object], remove: [Function] ],\n    width: 1 },\n  { traceId: '6d59dbe7ea141366',\n    startTs: '02-29-2016T09:21:10.066+0800',\n    timestamp: 1456708870066000,\n    duration: 50,\n    durationStr: '50.000ms',\n    servicePercentage: 57,\n    spanCount: 8,\n    serviceDurations: [ [Object], [Object], remove: [Function] ],\n    width: 1 },\n  { traceId: 'ef2ce5f890aee2dd',\n    startTs: '02-29-2016T09:21:21.907+0800',\n    timestamp: 1456708881907000,\n    duration: 16,\n    durationStr: '16.000ms',\n    servicePercentage: 87,\n    spanCount: 8,\n    serviceDurations: [ [Object], [Object], remove: [Function] ],\n    width: 0 },\n  { traceId: '57b9953167173b3f',\n    startTs: '02-29-2016T09:22:56.985+0800',\n    timestamp: 1456708976985000,\n    duration: 13,\n    durationStr: '13.000ms',\n    servicePercentage: 100,\n    spanCount: 5,\n    serviceDurations: [ [Object], [Object], remove: [Function] ],\n    width: 0 },\n  { traceId: 'fe33e75d70447b1d',\n    startTs: '02-29-2016T09:21:21.966+0800',\n    timestamp: 1456708881966000,\n    duration: 6,\n    durationStr: '6.000ms',\n    servicePercentage: 66,\n    spanCount: 8,\n    serviceDurations: [ [Object], [Object], remove: [Function] ],\n    width: 0 },\n  { traceId: '7318dcb23457300f',\n    startTs: '02-29-2016T09:21:21.966+0800',\n    timestamp: 1456708881966000,\n    duration: 6,\n    durationStr: '6.000ms',\n    servicePercentage: 66,\n    spanCount: 8,\n    serviceDurations: [ [Object], [Object], remove: [Function] ],\n    width: 0 },\n  { traceId: '2a47be612f5c86b7',\n    startTs: '02-29-2016T09:21:22.020+0800',\n    timestamp: 1456708882020000,\n    duration: 3,\n    durationStr: '3.000ms',\n    servicePercentage: 100,\n    spanCount: 5,\n    serviceDurations: [ [Object], [Object], remove: [Function] ],\n    width: 0 },\n  remove: [Function] ] to deeply equal [ { traceId: '56bea38b40e95f83',\n    startTs: '02-29-2016T09:09:45.388+0800',\n    timestamp: 1456708185388000,\n    duration: 3948,\n    durationStr: '3.948s',\n    servicePercentage: 100,\n    spanCount: 1,\n    serviceDurations: [ [Object], remove: [Function] ],\n    width: 100 },\n  { traceId: '6d59dbe7ea141366',\n    startTs: '02-29-2016T09:21:10.066+0800',\n    timestamp: 1456708870066000,\n    duration: 50,\n    durationStr: '50.000ms',\n    servicePercentage: 58,\n    spanCount: 4,\n    serviceDurations: [ [Object], [Object], remove: [Function] ],\n    width: 1 },\n  { traceId: '249628f4fa8f23d3',\n    startTs: '02-29-2016T09:21:10.066+0800',\n    timestamp: 1456708870066000,\n    duration: 50,\n    durationStr: '50.000ms',\n    servicePercentage: 58,\n    spanCount: 4,\n    serviceDurations: [ [Object], [Object], remove: [Function] ],\n    width: 1 },\n  { traceId: 'ef2ce5f890aee2dd',\n    startTs: '02-29-2016T09:21:21.907+0800',\n    timestamp: 1456708881907000,\n    duration: 16,\n    durationStr: '16.000ms',\n    servicePercentage: 87,\n    spanCount: 4,\n    serviceDurations: [ [Object], [Object], remove: [Function] ],\n    width: 0 },\n  { traceId: '57b9953167173b3f',\n    startTs: '02-29-2016T09:22:56.985+0800',\n    timestamp: 1456708976985000,\n    duration: 13,\n    durationStr: '13.000ms',\n    servicePercentage: 100,\n    spanCount: 3,\n    serviceDurations: [ [Object], [Object], remove: [Function] ],\n    width: 0 },\n  { traceId: '7318dcb23457300f',\n    startTs: '02-29-2016T09:21:21.966+0800',\n    timestamp: 1456708881966000,\n    duration: 6,\n    durationStr: '6.000ms',\n    servicePercentage: 66,\n    spanCount: 4,\n    serviceDurations: [ [Object], [Object], remove: [Function] ],\n    width: 0 },\n  { traceId: 'fe33e75d70447b1d',\n    startTs: '02-29-2016T09:21:21.966+0800',\n    timestamp: 1456708881966000,\n    duration: 6,\n    durationStr: '6.000ms',\n    servicePercentage: 66,\n    spanCount: 4,\n    serviceDurations: [ [Object], [Object], remove: [Function] ],\n    width: 0 },\n  { traceId: '2a47be612f5c86b7',\n    startTs: '02-29-2016T09:21:22.020+0800',\n    timestamp: 1456708882020000,\n    duration: 3,\n    durationStr: '3.000ms',\n    servicePercentage: 100,\n    spanCount: 3,\n    serviceDurations: [ [Object], [Object], remove: [Function] ],\n    width: 0 },\n  remove: [Function] ]\nHere's the associated json:\njson\n[{\"traceId\":\"56bea38b40e95f83\",\"name\":\"bootstrap\",\"id\":\"56bea38b40e95f83\",\"timestamp\":1456708185388000,\"duration\":3948889,\"annotations\":[{\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"192.168.99.1\",\"port\":9411},\"timestamp\":1456708185437062,\"value\":\"ApplicationStarted\"},{\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"192.168.99.1\",\"port\":9411},\"timestamp\":1456708185615720,\"value\":\"ApplicationEnvironmentPrepared\"},{\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"192.168.99.1\",\"port\":9411},\"timestamp\":1456708185720714,\"value\":\"ApplicationPrepared\"},{\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"192.168.99.1\",\"port\":9411},\"timestamp\":1456708189236645,\"value\":\"ContextRefreshed\"},{\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"192.168.99.1\",\"port\":9411},\"timestamp\":1456708189332465,\"value\":\"EmbeddedServletContainerInitialized\"},{\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"192.168.99.1\",\"port\":9411},\"timestamp\":1456708189336886,\"value\":\"ApplicationReady\"}],\"binaryAnnotations\":[{\"key\":\"lc\",\"value\":\"spring-boot\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"192.168.99.1\",\"port\":9411}}],\"debug\":false}]\nYou can post the json to your query server like so.. curl -s localhost:9411/api/v1/spans -X POST -H \"Content-Type: application/json\" -d ...\n. only found two last glitches:\napp.min.js:19480 Uncaught AssertionError: Expected to have \"127.0.0.1:0\" but got \"127.0.0.1:undefined\" at path \"/spansBackup/0/annotations/0/endpoint\".\nAnd this..\napp.min.js:19480 Uncaught AssertionError: Expected to have \"1+-0.001\" but got \"3\" at path \"/0/spanCount\".\nAbove for the following trace:\njson\n[{\"traceId\":\"d397ce70f5192a8b\",\"name\":\"get\",\"id\":\"d397ce70f5192a8b\",\"timestamp\":1457160374149000,\"duration\":2000,\"annotations\":[{\"timestamp\":1457160374149000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":9411}},{\"timestamp\":1457160374151000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":9411}}],\"binaryAnnotations\":[{\"key\":\"http.path\",\"value\":\"/api/v1/services\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\"}},{\"key\":\"srv/finagle.version\",\"value\":\"6.33.0\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\"}},{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":9411}},{\"key\":\"ca\",\"value\":true,\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":56828}}]}]\n. Thanks for your patience, Eirik. Found a couple more.\nfrom index:\napp.min.js:19480 Uncaught AssertionError: Expected to have \"e3a98be7d2f22205\" but got \"a25ceb4232891c35\" at path \"/5/traceId\".\nFrom trace detail:\napp.min.js:19480 Uncaught AssertionError: Expected to have \"Server Address\" but got \"sa\" at path \"/spansBackup/0/binaryAnnotations/2/key\".\nwith trace:\n[{\"traceId\":\"2480ccca8df0fca5\",\"name\":\"get\",\"id\":\"2480ccca8df0fca5\",\"timestamp\":1457186385375000,\"duration\":333000,\"annotations\":[{\"timestamp\":1457186385375000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":9411}},{\"timestamp\":1457186385708000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":9411}}],\"binaryAnnotations\":[{\"key\":\"http.path\",\"value\":\"/api/v1/trace/d397ce70f5192a8b\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\"}},{\"key\":\"srv/finagle.version\",\"value\":\"6.33.0\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\"}},{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":9411}},{\"key\":\"ca\",\"value\":true,\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":57602}}]}]\n. looks like the gradle test pattern needs to recurse directories:\n06 03 2016 13:35:51.753:WARN [watcher]: Pattern \"/Users/acole/oss/zipkin/zipkin-web/src/main/resources/app/test/*test.js\" does not match any file.\n. on the index page, maybe this is an ordering error?\nUncaught AssertionError: Expected to have \"6ff1c14161f7bde1\" but got \"9ed44141f679130b\" at path \"/5/traceId\".\nHere's the  two traces:\n[{\"traceId\":\"6ff1c14161f7bde1\",\"name\":\"get\",\"id\":\"6ff1c14161f7bde1\",\"timestamp\":1457186441657000,\"duration\":4000,\"annotations\":[{\"timestamp\":1457186441657000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":9411}},{\"timestamp\":1457186441661000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":9411}}],\"binaryAnnotations\":[{\"key\":\"http.path\",\"value\":\"/api/v1/trace/2480ccca8df0fca5\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\"}},{\"key\":\"srv/finagle.version\",\"value\":\"6.33.0\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\"}},{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":9411}},{\"key\":\"ca\",\"value\":true,\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":57603}}]}]\n[{\"traceId\":\"9ed44141f679130b\",\"name\":\"get\",\"id\":\"9ed44141f679130b\",\"timestamp\":1457186568026000,\"duration\":4000,\"annotations\":[{\"timestamp\":1457186568026000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":9411}},{\"timestamp\":1457186568030000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":9411}}],\"binaryAnnotations\":[{\"key\":\"http.path\",\"value\":\"/api/v1/trace/2480ccca8df0fca5\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\"}},{\"key\":\"srv/finagle.version\",\"value\":\"6.33.0\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\"}},{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":9411}},{\"key\":\"ca\",\"value\":true,\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\",\"port\":57603}}]}]\n\nlooks like a slight naming mismatch, too\napp.min.js:19480 Uncaught AssertionError: Expected to have \"Local Address\" but got \"Local Component\" at path \"/spansBackup/0/binaryAnnotations/1/key\".Assertion.assert @ app.min.js:19480(anonymous function) @ app.min.js:23380ctx.(anonymous function) @ app.min.js:19067success @ app.min.js:56958fire @ app.min.js:4755self.fireWith @ app.min.js:4885done @ app.min.js:10330(anonymous function) @ app.min.js:10696\nFor trace\n[{\"traceId\":\"f434dda9b8a7c3d8\",\"name\":\"bootstrap\",\"id\":\"f434dda9b8a7c3d8\",\"timestamp\":1457242094915000,\"duration\":301000,\"annotations\":[{\"timestamp\":1457242094915000,\"value\":\"init\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\"}},{\"timestamp\":1457242094915000,\"value\":\"warmup\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\"}},{\"timestamp\":1457242095216000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\"}}],\"binaryAnnotations\":[{\"key\":\"lc\",\"value\":\"finatra\",\"endpoint\":{\"serviceName\":\"zipkin-query\",\"ipv4\":\"127.0.0.1\"}}]}]\n. @eirslett I agree that the sort order thing isn't a useful signal as indeed any time I get that exception, the traces have the same duration.\nLGTM, so merge when ready! (I'm guessing the comparison calls are removed at this point)\n. can you post the span itself? ex http://zipkinhost:9411/api/v1/trace/span-id-in-hex\nthis will be easier to troubleshoot\n. @yurishkuro any thoughts on this?\n. @yurishkuro nothing in the CQL deserializes thrift. While it is true that we store these dates, they aren't used anywhere except to make epochDayMillis, which could be done directly.\nI'd like to separate complete model overhaul from the topic of extra fields, if possible.\n. @yurishkuro ps I opened #1009 on a thrift-less schema. If you don't mind, please jot your thoughts about what it would look like there, so the comments don't end up lost. cheers!\n. verified by running CassandraSpanStoreSpec\n[ScalaTest-run] WARN org.twitter.zipkin.storage.cassandra.Repository - Span 456_473519988_1241441653 in trace 123 had no timestamp. If this happens a lot consider switching back to SizeTieredCompactionStrategy for zipkin.traces\n. cc @abesto @kristofa @clehene @jcarres-mdsol \n. Thanks, Eirik!\n. sure.. just need to make sure finatra is compatible with it...\n. For those listening at home.. the following log from the datastax driver says which datacenter won\n17:17:01.645 [main] INFO  c.d.d.c.p.DCAwareRoundRobinPolicy - Using data-center name 'datacenter1' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)\n. pinging cassandra peeps @michaelsembwever @kristofa @danchia @yurishkuro @prat0318 @eirslett\n. Thanks for the review, folks!\n. Thanks for the review, folks!\n. I tidied up deletes and rebased on master. tested locally with a cassandra backend and still works.\n:shipit: \n. :shipit: thanks for the help!\n. :shipit: thanks for the help!\n. I agree with everything you've said. For example zipkin-java calls the\nserver simply zipkin-server. I will be writing the Kafka listener this\nweek, which would be an optional module. The same base code can be\nconfigured in clusters which have different roles, as necessary.\nEx it could be deployed to a \"web\" cluster or a \"collection\" cluster with\ndifferent heap args and options. The configuration story is easier to deal\nwith when there's a single entry point, and modularization at the component\nlevel keeps the codebase clean.\n. I agree with everything you've said. For example zipkin-java calls the\nserver simply zipkin-server. I will be writing the Kafka listener this\nweek, which would be an optional module. The same base code can be\nconfigured in clusters which have different roles, as necessary.\nEx it could be deployed to a \"web\" cluster or a \"collection\" cluster with\ndifferent heap args and options. The configuration story is easier to deal\nwith when there's a single entry point, and modularization at the component\nlevel keeps the codebase clean.\n. Incidentally, I think @yurishiko already bundles query and web in the same\nprocess!\n. Incidentally, I think @yurishiko already bundles query and web in the same\nprocess!\n. Here's a rough transition plan idea:\n- move zipkin-web's assets and tests to a separate repo (non-gradle). This\n  would make it more natural to maintain.\n- Once above is in, have the \"old zipkin-web\" import those assets, so that\n  it is drop-in, similarly do the same in zipkin-query (I suppose adding the\n  config endpoint?). we cut a release where the experience from a packaging\n  POV is the same.\n- Next release would be to drop the \"zipkin-web\" in scala, suggesting folks\n  just deploy zipkin-query over their old zipkin-web deployment (should they\n  still want 3-tier). This also affects our docker deployments.\n. Here's a rough transition plan idea:\n- move zipkin-web's assets and tests to a separate repo (non-gradle). This\n  would make it more natural to maintain.\n- Once above is in, have the \"old zipkin-web\" import those assets, so that\n  it is drop-in, similarly do the same in zipkin-query (I suppose adding the\n  config endpoint?). we cut a release where the experience from a packaging\n  POV is the same.\n- Next release would be to drop the \"zipkin-web\" in scala, suggesting folks\n  just deploy zipkin-query over their old zipkin-web deployment (should they\n  still want 3-tier). This also affects our docker deployments.\n. LGTM thanks for the polish!\n. LGTM thanks for the polish!\n. So the pivotal change in the recent commit was how to interpret a byte array.\nThere is state used at the moment, particulalry BinaryThriftStructSerializer reuses a transport which internally contains a buffer. @prat0318 do you have an example of the OOM involved?\n. So the pivotal change in the recent commit was how to interpret a byte array.\nThere is state used at the moment, particulalry BinaryThriftStructSerializer reuses a transport which internally contains a buffer. @prat0318 do you have an example of the OOM involved?\n. Thanks for the detailed notes.\nI think it makes sense that memory consumption would be directly related to\nthe amount of spans being processed.\nIt may be that we use too much memory, but the problem is not releasing\nmemory (via garbage collection).\n. Thanks for the detailed notes.\nI think it makes sense that memory consumption would be directly related to\nthe amount of spans being processed.\nIt may be that we use too much memory, but the problem is not releasing\nmemory (via garbage collection).\n. the code is significantly different now. try the docker image openzipkin/zipkin and see how it goes\n. the code is significantly different now. try the docker image openzipkin/zipkin and see how it goes\n. :+1:\n. :+1:\n. @prat0318 I don't think this has anything to do with your memory leak, but anyway...\n. @prat0318 I don't think this has anything to do with your memory leak, but anyway...\n. great. I like this mission\n. lemme know if you two want third eyes on this, otherwise, merge when you think it is ready!\n. @eirslett @abesto weird error fixed https://github.com/openzipkin/zipkin/commit/284cbbf7989fc4e96695966a6fb6b449968026b2\n. cc @yurishkuro who requested this and a few others I think might find this useful (instrumentation-peeps): @marcingrzejszczak @dsyer @kristofa @jcarres-mdsol @prat0318 \n. Incidentally, this came in handy troubleshooting a duration issue with @fedj today. By running the raw query, we were able to isolate input data that shows there's a duration bug!\n. Thanks to @fedj for reporting.\nI've used his test data below, which now correctly returns duration of 15000 when queried\n``` bash\n$ curl -s localhost:9411/api/v1/spans -X POST -H \"Content-Type: application/json\" -d '\n[\n   {\n      \"traceId\":\"a632d4ec12ca2535\",\n      \"name\":\"post\",\n      \"id\":\"6a0b0ee3fff90c9c\",\n      \"parentId\":\"6802a997a5ca68cd\",\n      \"timestamp\":1457596859492000,\n      \"duration\":11000,\n      \"annotations\":[\n         {\n            \"timestamp\":1457596859492000,\n            \"value\":\"sr\",\n            \"endpoint\":{\n               \"serviceName\":\"recoservice.test\",\n               \"ipv4\":\"10.5.4.152\"\n            }\n         },\n         {\n            \"timestamp\":1457596859503000,\n            \"value\":\"ss\",\n            \"endpoint\":{\n               \"serviceName\":\"recoservice.test\",\n               \"ipv4\":\"10.5.4.152\"\n            }\n         }\n      ],\n      \"binaryAnnotations\":[\n  ]\n\n},\n   {\n      \"traceId\":\"a632d4ec12ca2535\",\n      \"name\":\"get\",\n      \"id\":\"6802a997a5ca68cd\",\n      \"timestamp\":1457596859516000,\n      \"duration\":23000,\n      \"annotations\":[\n         {\n            \"timestamp\":1457596859516000,\n            \"value\":\"sr\",\n            \"endpoint\":{\n               \"serviceName\":\"infobanner\",\n               \"ipv4\":\"10.5.8.63\"\n            }\n         },\n         {\n            \"timestamp\":1457596859539000,\n            \"value\":\"ss\",\n            \"endpoint\":{\n               \"serviceName\":\"infobanner\",\n               \"ipv4\":\"10.5.8.63\"\n            }\n         }\n      ],\n      \"binaryAnnotations\":[\n  ]\n\n},\n   {\n      \"traceId\":\"a632d4ec12ca2535\",\n      \"name\":\"post\",\n      \"id\":\"6a0b0ee3fff90c9c\",\n      \"parentId\":\"6802a997a5ca68cd\",\n      \"timestamp\":1457596859524000,\n      \"duration\":15000,\n      \"annotations\":[\n         {\n            \"timestamp\":1457596859524000,\n            \"value\":\"cs\",\n            \"endpoint\":{\n               \"serviceName\":\"unknownservice\",\n               \"ipv4\":\"10.5.8.63\"\n            }\n         },\n         {\n            \"timestamp\":1457596859539000,\n            \"value\":\"cr\",\n            \"endpoint\":{\n               \"serviceName\":\"unknownservice\",\n               \"ipv4\":\"10.5.8.63\"\n            }\n         }\n      ],\n      \"binaryAnnotations\":[\n  ]\n\n}\n]'\n$ curl -s localhost:9411/api/v1/trace/a632d4ec12ca2535|jq '.[1].duration'\n15000\n```\n. javadoc and source jars don't make sense in terms of static asset jars.. I noticed this when browsing snapshot repo http://oss.jfrog.org/artifactory/oss-snapshot-local/io/zipkin/zipkin-ui/1.36.1-SNAPSHOT/\ncc @eirslett @abesto \n. merging to test (by looking at jfrog)\n. reverted by https://github.com/openzipkin/zipkin/commit/4f75efe852319c49bc34ecbba2b6ec38bb0a109b\nBasically, this is an unnecessary thing required by maven central policy. Easiest thing is just to ignore the extra assets. http://central.sonatype.org/pages/requirements.html#supply-javadoc-and-sources\n. cc @danchia @yurishkuro \n. cc @abesto @eirslett \n. I think the two current consumers of this are already using maven\ndependencies, and that simplifies the build quite a bit. I'd vote for a jar\nwith \"/static\" in it, as a published artifact (at least for now).\nI've really no experience with webjars or webpack\n. ran the UI, clicked around and seems good to me.\n:shipit:\n. I was going to suggest this! Thanks for implementing before I had the\nchance to.\n. Do we want to call this config.json or just config? I can see the case for the former, as it looks like a file and could be implemented by one. The latter looks more like other things like /healthcheck (assuming content-type is correct)\nThoughts?\n. Whoot!\n. To kill an ostrich.. This is what you seek?\n. I'm cool with the peek option. We already have to peek anyway. When we move\nto a different model, we'd probably want to use a message key instead, and\nit might look like a media type.\nex. application/vnd.zipkin2.tracelist+json\n. cc @eirslett @abesto \nI tested this locally with both explicit paths and bundled ones.. the flag is compatible with before, but better worded and with a better error message.\n. no more zipkin-query\n. only things needed are removal from docker (https://github.com/openzipkin/docker-zipkin/issues/98) and then doc updates\n. @dragontree101 starting with zipkin 1.1, ttls are moved to schema defaults. So, you can change the schema to whatever your preferred expiration is.\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-storage/cassandra/src/main/resources/cassandra-schema-cql3-upgrade-1.txt\n. released in 1.38.1 please give it a shot!\n. No duration if span.duration wasnt set by instrumentation and any of the\nfollowing are true:\n- there are no annotations\n- all annotations have the same timestamp\n- there's a bug in the server ;)\n. releasing a subminor as this bug hits very hard when present\n. there's something to consider in the above logic, which is how to detect if we are serving the content of index.html or not. if we are just going on the path name, it is simpler.\nresp.cacheControl =  if (isIndex) 365.days else 1.minute\n. there's something to consider in the above logic, which is how to detect if we are serving the content of index.html or not. if we are just going on the path name, it is simpler.\nresp.cacheControl =  if (isIndex) 365.days else 1.minute\n. this is also hard to test, as ZipkinQueryServerFeatureTest doesn't have a classpath reference to the resources built by zipkin-ui. Ex. even if you set the following, it 404s\n``` scala\n  override val server = new EmbeddedHttpServer(\n    new ZipkinQueryServer(spanStore, dependencyStore), extraArgs = Seq(\n      // Zipkin's web assets are under the path /zipkin-ui\n      \"-doc.root\", \"zipkin-ui\",\n      // We are using fake times, so make sure default lookback doesn't interfere\n      \"-zipkin.queryService.lookback\", lookbackOverride.toString\n    )\n  )\n```\n. this is also hard to test, as ZipkinQueryServerFeatureTest doesn't have a classpath reference to the resources built by zipkin-ui. Ex. even if you set the following, it 404s\n``` scala\n  override val server = new EmbeddedHttpServer(\n    new ZipkinQueryServer(spanStore, dependencyStore), extraArgs = Seq(\n      // Zipkin's web assets are under the path /zipkin-ui\n      \"-doc.root\", \"zipkin-ui\",\n      // We are using fake times, so make sure default lookback doesn't interfere\n      \"-zipkin.queryService.lookback\", lookbackOverride.toString\n    )\n  )\n```\n. opened #1062 with a work-in-progress. If anyone feels like adding commits, go for it!\n. opened #1062 with a work-in-progress. If anyone feels like adding commits, go for it!\n. #1062 is ready to go\n. #1062 is ready to go\n. indeed. thanks!\n. indeed. thanks!\n. Thx for speaking up, folks!\n. Thx for speaking up, folks!\n. a quick could be to port the code from the dependencies screen that already does this. This should be very easy as the query parameters are the same (ex endTs and lookback) cc @t-yuki \n\n. a quick could be to port the code from the dependencies screen that already does this. This should be very easy as the query parameters are the same (ex endTs and lookback) cc @t-yuki \n\n. closed via #1067\n. closed via #1067\n. Valid point.\n. Valid point.\n. this should do it https://github.com/openzipkin/zipkin/pull/1070\n. this should do it https://github.com/openzipkin/zipkin/pull/1070\n. JAVA_HOME needs to be set to a JDK ( not a JRE )\n. JAVA_HOME needs to be set to a JDK ( not a JRE )\n. what is the value of $JAVA_HOME\n. what is the value of $JAVA_HOME\n. cc @mjbryant @t-yuki @eirslett @gena01\n. cc @mjbryant @t-yuki @eirslett @gena01\n. closed via  #1069\n. closed via  #1069\n. These are valid concerns, though I'd like to mention a few things that all\nmight not know. They impact the perception of supportability:\n- zipkin's scala code has had numerous significant bugs, yet very few have\n  contributed fixes to it.\n- very little of the scala code is actually used inside Twitter. For\n  example, twitter doesn't use cassandra.\n- there is little evidence of heavy tuning in the source history of the\n  scala code\n- benchmarks in scala that have happened have been ad-hoc. For example,\n  pyramid-zipkin adjusting parameters for kafka (these parameters exist in\n  both java and scala projects)\nThe scala+finagle aspect has actually led directly to people not\nparticipating in the project, and also delayed features for extreme amounts\nof time. For example, elasticsearch was requested years ago, and had some\nfalse-starts. Elasticsearch was implemented in java in less than a week by\nsomeone who formerly had no experience with zipkin. There's a lot of\nevidence we can dig up about this, and this was a primary motivation for\nthe java project itself.\nWhile both the scala and java processes can enable a feature like kafka,\nthere's no reason to believe that either choice means a resilience fail.\nBefore, we had no choice for an all-in-one process: that significantly\ndamaged the ability for people to learn zipkin quickly. IOTW, let's not\nmistake choice for a topology mandate. It is already the case that folks\nhave collectors in golang and whatnot. There's nothing preventing someone\nfrom making a zipkin server that is only a collector. in fact, there\nalready is one in spring-cloud-sleuth for rabbit.\nzipkin-java had a scribe transport, but it was deleted because scribe is\narchived and folks involved in zipkin had bad experience with scribe. We can\nchoose to re-enable that feature in java using facebook swift (history\nlesson: facebook made scribe and thrift, so this choice shouldn't be scary)\nRegardless of whether folks want a scala+finagle thing or not, it is\nimportant that zipkin lives. Right now, I spend a great deal of time on\nundifferentiated work in scala eventhough I get lots of help in java. There\nis a burden to maintaining zipkin and with the amount of people we have, it\nsurely suffers from having to carry excess weight in relatively unknown\nframeworks.\n. These are valid concerns, though I'd like to mention a few things that all\nmight not know. They impact the perception of supportability:\n- zipkin's scala code has had numerous significant bugs, yet very few have\n  contributed fixes to it.\n- very little of the scala code is actually used inside Twitter. For\n  example, twitter doesn't use cassandra.\n- there is little evidence of heavy tuning in the source history of the\n  scala code\n- benchmarks in scala that have happened have been ad-hoc. For example,\n  pyramid-zipkin adjusting parameters for kafka (these parameters exist in\n  both java and scala projects)\nThe scala+finagle aspect has actually led directly to people not\nparticipating in the project, and also delayed features for extreme amounts\nof time. For example, elasticsearch was requested years ago, and had some\nfalse-starts. Elasticsearch was implemented in java in less than a week by\nsomeone who formerly had no experience with zipkin. There's a lot of\nevidence we can dig up about this, and this was a primary motivation for\nthe java project itself.\nWhile both the scala and java processes can enable a feature like kafka,\nthere's no reason to believe that either choice means a resilience fail.\nBefore, we had no choice for an all-in-one process: that significantly\ndamaged the ability for people to learn zipkin quickly. IOTW, let's not\nmistake choice for a topology mandate. It is already the case that folks\nhave collectors in golang and whatnot. There's nothing preventing someone\nfrom making a zipkin server that is only a collector. in fact, there\nalready is one in spring-cloud-sleuth for rabbit.\nzipkin-java had a scribe transport, but it was deleted because scribe is\narchived and folks involved in zipkin had bad experience with scribe. We can\nchoose to re-enable that feature in java using facebook swift (history\nlesson: facebook made scribe and thrift, so this choice shouldn't be scary)\nRegardless of whether folks want a scala+finagle thing or not, it is\nimportant that zipkin lives. Right now, I spend a great deal of time on\nundifferentiated work in scala eventhough I get lots of help in java. There\nis a burden to maintaining zipkin and with the amount of people we have, it\nsurely suffers from having to carry excess weight in relatively unknown\nframeworks.\n. updated the description a bit as I noticed people were reacting to the idea that java had to be single-process when there's not actually anything about it that constrains this.\n. updated the description a bit as I noticed people were reacting to the idea that java had to be single-process when there's not actually anything about it that constrains this.\n. > work has gone into it to make sure that unexpected increases in span write\n\nvolume doesn't overload our storage backend, which doesn't seem to be there\nyet in zipkin-java.\nI think you are referring to the adaptive sampler, if I'm not mistaken. It\nsat broken for a while because it had almost no test coverage, and no users\noutside twitter complaining. I think that feature should be spun out, as it\nhas a lot of code that hasn't been touched in years.\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-sampler\n. thanks for the interest, Kevin. For starters, zipkin-java's docker image\nhttps://github.com/openzipkin/docker-zipkin-java was written with\ntransition in mind: except for scribe (which we can chase up), it responds\nto all environment variables from upstream. This means that if using\ndocker, one can do performance comparisons, etc with http, kafka, mysql,\ncassandra, etc by swapping the images.\n\nOne of the reasons for wanting to drop the scala project is that we are\nswamped with maintenance. That hints that things like rigorous performance\ncomparisons aren't likely to happen without volunteers, particularly as\nthere's no benchmark of any sort in the scala project. Anyway, I started this\nissue https://github.com/openzipkin/zipkin-java/issues/45 on the\ngeneralities of things that might be helpful.\n. added a blocking issue to add the scribe transport to zipkin-java. https://github.com/openzipkin/zipkin-java/issues/150\n. broke out a topic on zipkin-sampler https://github.com/openzipkin/zipkin/issues/1078\n. I've resurrected scribe in the java project. once https://github.com/openzipkin/zipkin-java/pull/152 is released, I'll scrub it from the pro/con list in the description\n. fyi zipkin-java 0.12.1 has working scribe and available in docker. cc @klingerf @jtrobec @gena01\n. ps once we drop the scala server, we can avoid the routine build breaks due to twitter dependencies like this.\nGET 'https://maven.twttr.com/com/twitter/common/stats-util/0.0.58/stats-util-0.0.58.jar'. Received status code 503 from server: Service Temporarily Unavailable\n. zipkin-java 0.13.0 has both scribe and sampler-zookeeper (the adaptive one). there's still probably a week or two's work before 1.0 is realistic.\n. Just rewrote the adaptive sampler to remove the scala dependency. https://github.com/openzipkin/zipkin-java/pull/167\n. zipkin-java is headed towards the finish line, we've had no unresolved objections, and a dozen votes in favor.\nI think we should focus on logistics at this point, for example explicitly suggest new projects and those with flexibility to use zipkin-java, mention more formally the bug-fix nature of this project, make explicit logistics of non-scala assets such as the thrift files and javascript. Mention the current plan which is to replace master with the assets that are currently in the zipkin-java repo. Setup an answer for the question of who's already using it, etc.\nTo do all this, we could either re-title/ repurpose this, or close it and make a new logistics issue. Any other ideas?\nPS here's latest release notes from the java project: https://github.com/openzipkin/zipkin-java/releases/tag/0.20.0\n. closed via #1119\n. @eirslett mind helping on this one? I'm not quite sure the best way to early exit on this..\n. there's no use case for non-shadowed -service jars, so yes they can be\nremoved.\naside: in the java project (zipkin-server), there is a use case, which is\nthat you can build a smaller or extended zipkin server (ex one that doesn't\ninclude kafka, or that includes 3rd party transports or storage).\n. there's no use case for non-shadowed -service jars, so yes they can be\nremoved.\naside: in the java project (zipkin-server), there is a use case, which is\nthat you can build a smaller or extended zipkin server (ex one that doesn't\ninclude kafka, or that includes 3rd party transports or storage).\n. Looks like you'll need to add some things to gradle.properties (or maybe\nyou can make local.properties file)\nhttps://docs.gradle.org/current/userguide/build_environment.html\n. applies to zipkin-web, too. ps once we merge we need to adjust the\ndockerfiles\nhttps://jcenter.bintray.com/io/zipkin/zipkin-web/1.39.0/\n. nevermind me.. as long as the fat jar artifact name doesn't change, which\nseems the case\n. oops.. looks like release needs an update?\nCaused by: org.gradle.api.GradleException: Could not sync 'openzipkin/maven/zipkin/1.39.1' to Maven Central: HTTP/1.1 400 Bad Request [messages:[Missing: no main jar artifact found in folder '/io/zipkin/zipkin-web/1.39.1', Missing: no main jar artifact found in folder '/io/zipkin/zipkin-collector-service/1.39.1', Missing: no main jar artifact found in folder '/io/zipkin/zipkin-query-service/1.39.1', Dropping existing partial staging repository.], status:Validation Failed]\nhttps://travis-ci.org/openzipkin/zipkin/builds/119740137\n. I think renaming the jar to be the primary artifact would be a great\noption, as polishing the docker files is pretty easy. I don't know how easy\nthis is in gradle.\n. reverted until we sort out a solution that works\n. Implementation aside, the integration with a span consumer is fairly\nsimple, a filter.\nThis would work with both the scala and the java interfaces, and I can put\nexample code showing this.\n. One thing I'd clarify is that even if I can demonstrate get this to\nwork with another collection filter, this isn't a commitment in any\nway to support it. There's no such commitment today by any current\ncontributor of zipkin, nor any history of it by past committers.\nWhen this goes to its own repository, its fate will be in the hands of\nthose interested enough to maintain several year old code, or those\nwho wish to rewrite it.\n. Here's the issue that adds basic build infra https://github.com/openzipkin/zipkin-adaptive-sampler/pull/1\n. Had an offline chat w/ @eirslett and I think it isn't worth it to move the sampler to a separate repository. Instead, we can add support in zipkin-java and leave this here until zipkin-java can replace the code here. https://github.com/openzipkin/zipkin-java/issues/164\n. No-one is using the zookeeper sampler. it has been just a maintenance burden for years. I'd like to move it to openzipkin-attic\n@openzipkin/core anyone disagree?. they'd only be using it directly if they moved to the java port of the server. In any case it isn't actually connected to the server, so this is why I don't think anyone is using it.. Thanks @mosesn if somehow this is in use, it is also ok to make it a non-attic repo (ex contrib), just it pulls zk dependencies takes a while to run tests and has had near zero feedback for the last couple years. Would like to keep the core repo focused. cc @klingerf \n. Note that if the underlying concern is overload, other work may be harder hitting.\nFor example, Kafka transport was easier to tune when it could accept multiple spans per message (and instrumentation sent multiple spans per message).\nAlso, the Cassandra layer currently works on one span at a time. This could be measured to see if batching helps reduce load or not.\n. current code is disconnected\n. current code is disconnected\n. ex. we literally have a test to try later on NullPointerException! Would we expect this to not be a NPE later?!\nassert(log.get.trim === \"Sending TryLater due to NullPointerException(foo was null)\")\n. whitelisting transient could be tricky, as for example NoHostAvailableException isn't an IOException, and it is a cassandra-specific class. One way could be to enforce that AsyncSpanConsumer implementations must route an transient scenario to an IOException. The problem with this approach is that it is a whack-a-mole and very sensitive to library updates.\nAnother option is to simply not support tryLater.\n. current code doesn't do this\n. current code doesn't do this\n. PS the reason this works is cassandra's repository is already lazy wired. the only thing dereferencing it at bootstrap was self-tracing.\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-cassandra/src/main/scala/com/twitter/zipkin/cassandra/CassandraSpanStoreFactory.scala#L47\n. @prat0318 so I think this is a bug if the above span has no parent (ie is a root span). Ex setting to 0 is invalid, whereas leaving the parentId field unset is\n. since ids are basically byte arrays packed into a long, we shouldn't do things like interpret them. My concern about conventions like 0 is that a small bug in a custom collector's thrift codec will send the zero across. Basically, do things like that at one's own risk.\n. back to this issue.. we do have a bug in the UI, IMHO\na headless trace shouldn't cause problems. Ex. it can be the case that the root span is missing, but everything else is present. The data here is similar in shape to a headless trace (i.e. the root-most span's parent is missing)\n. @prat0318 I cleaned up the control characters out of your json and posted it locally. works.. can you try on your end?\njson\n[\n  {\n    \"traceId\": \"a83668f5b355bd70\",\n    \"name\": \"post\",\n    \"id\": \"0000000000000001\",\n    \"parentId\": \"0000000000000000\",\n    \"timestamp\": 1459723138170202,\n    \"duration\": 604053,\n    \"annotations\": [\n      {\n        \"timestamp\": 1459723138170202,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"yelp\",\n          \"ipv4\": \"10.56.5.4\",\n          \"port\": 80\n        }\n      },\n      {\n        \"timestamp\": 1459723138774255,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"yelp\",\n          \"ipv4\": \"10.56.5.4\",\n          \"port\": 80\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"servlet\",\n        \"value\": \"connect\",\n        \"endpoint\": {\n          \"serviceName\": \"yelp\",\n          \"ipv4\": \"10.56.5.4\",\n          \"port\": 80\n        }\n      },\n      {\n        \"key\": \"http.uri\",\n        \"value\": \"register\",\n        \"endpoint\": {\n          \"serviceName\": \"yelp\",\n          \"ipv4\": \"10.56.5.4\",\n          \"port\": 80\n        }\n      },\n      {\n        \"key\": \"http.uri.qs\",\n        \"value\": \"register\",\n        \"endpoint\": {\n          \"serviceName\": \"yelp\",\n          \"ipv4\": \"10.56.5.4\",\n          \"port\": 80\n        }\n      }\n    ]\n  }\n]\n. ok narrowing this down, it couldn't have happened before 1.38 (and be run on the query service), because query didn't host static assets until 1.38\n. While I can't get the waiting screen to come up, the error is repeatable given the json I posted in version 1.38.0\n``` bash\nin one tab...\n$ java -jar ~/Downloads/zipkin-query-service-1.38.0-all.jar -f /query-dev.scala\nin another..\n$ curl -X POST -H 'Content-Type: application/json' -s localhost:9411/api/v1/spans -d @/tmp/foo\nin a browser\nhttp://localhost:9411/traces/a83668f5b355bd70\n```\n. above is also reproducible with latest 1.39.3\n. ok the patch is to guard.. now figuring out if I can get this to be testable..\njavascript\n        /* Parent may not be found for a number of reasons. For example, the\n        trace id may not be a span id. Also, it is possible to lose the root\n        span data (i.e. a headless trace) */\n        if (self.spans[pId]) {\n          toShow[pId] = true;\n          self.spans[pId].openChildren += 1;\n        }\n. took me a long time to figure out how to test this :P If anyone has better ideas, please comment here https://github.com/openzipkin/zipkin/pull/1089\n. This question seems more about how to POST data vs an issue in zipkin.\nDebugging custom http clients would be outside the scope of this project.\nFor example, the following works.\n$ curl -X POST -H'Content-Type: application/json' -s\nlocalhost:9411/api/v1/spans -d @your_file\nYou might also look at this:\nhttps://github.com/openzipkin/brave/tree/master/brave-spancollector-http\n. nope 9410 is scribe (thrift rpc protocol) 9411 is http.\n. Mind re-opening this against the doc repo? There's no code in this repo\nrelated to this question, but it is a nice topic to clarify.\nack that the name of this repo could lead it to be a catch-all for all\nthings including instrumentation, just saying there's nothing to change\nhere as a result of this topic.\nTo answer, I think most instrumentation use 0 or 1 for that header, but we\ncould clarify it as a choice of true/false or 1/0. We need to check that\nother tracers accept that,  though, particularly those in the openzipkin\norganization) brave and zipkin-tracer (ruby)). Great example of where\ninteroperability testing  could help\nhttps://github.com/opentracing/opentracing.github.io/issues\n. This issue should be opened on OpenZipkin/brave as it doesn't have much to\ndo with this repository.\nI used the below successfully with various brave collectors including HTTP,\nwhich is the easiest.\nI'd recommend using HTTP against a  zipkin-java server.\nhttps://github.com/kristofa/brave-resteasy-example/pull/6\n. closing this, as it makes more sense to keep code here until zipkin-java replaces it.\n. @eirslett @abesto @kristofa @mosesn Once this is merged, I can add adaptive sampler to the java project.\n. refactored to hide the custom ZKClient from java. This will allow us to swap it out with something else without breaking callers.\n. > We could also implement an adaptive sampler that polls zipkin-server for a\n\ntrace frequency every few seconds.\nyeap. there's some coordination concern, but sure! probably would be more\nmeaningful for non-jvm collectors, ex an adaptive sampler buddy/ sidecar\naddition\n. Thanks for the sample trace. Not that it matters, but is this the \"raw\"\noutput? Also I assume this is not zipkin-java (and using mysql storage type)\n. this is a bug when there's no queryName parameter. Ex. the raw trace by id, like this: http://localhost:9411/traces/bd8283279c35f5e1, when you click expand all, the console shows this..\n\n\n. ok I think this does it.. https://github.com/openzipkin/zipkin/pull/1094\n. neat idea. One way could be to save them off to a user profile (even\nbrowser storage), since traces are so small. Ex,\nzipkin/api/v1/trace/trace_id becomes searchable locally. Just an idea.\nPS this is only an issue in cassandra at the moment, since others don't\nsupport TTLs anyway. It might be tricky to do this on the storage side\nportably, since we don't have a portable feature for TTL. That doesn't\npreclude custom logic, just explanation.\n. > We already have the JSON button, so Save As is there, but we don't have a\n\nLoad function in the UI.\n\"We\" are a privileged minority until #1060 :P\n. PS there's now a download button on a trace.\n. PS there's now a download button on a trace.\n. We had a note in #1222 about just making a separate non-bucketed index for elasticsearch and just move docs to it. There's no TTL support in mysql anyway, so noop there. Cassandra might take some thinking.\n\nWe can address this by making it possible to query the \"infinite\" index routinely, and we could also make a \"request save\" api, which either moves the trace there or returns a message if unsupported.\ncc @openzipkin/elasticsearch @openzipkin/cassandra \n. also in mysql I suppose we could double the table-count to provide an \"infinite\" index separate from the routine one (cc @jcarres-mdsol)\n. thx @michaelsembwever \nWe'd likely want some signal to imply that the trace is special. One way is to add a binary annotation (tag) to the saved trace, like..\n\"representative\" -> \"fastestest\"\n\"representative\" -> \"discovery name failure\"\nWe wouldn't care what the values are, but it allows zipkin query of \"representative\" to include them, and also would allow any UI to distinguish them from something else (cc @rogeralsing)\nThis would also permit those doing modeling or analysis research to ask folks for representative traces in some easy-to-grab fashion (cc @adrianco @rfonseca)\n. I thought about this a bit over the weekend. Here's what that ended up as.\n- once we start worrying about retention policy for \"saved tweets\" we approach the complexity of the span store (again)\n- this is compounded by a need for an api change, which would break folks to support a feature that hasn't been requested widely\n- the more complexity we add our component apis, the harder future changes get\nI've an alternative proposal: do it all on the client\nInstead of creating a secondary tier storage in our api, simply deploy twice. Ex use a keyspace/DB for the transient trace depot and another for the \"permanent\" one.\nEx. index=zipkin (for transient) and index=zipkin-4ever for permanent.\nThe second is fronted by vanilla zipkin-servers that don't run any collectors except http. The act of \"saving a trace\" is just taking the json from the transient one and POSTing it to the permanent one.\nSomeone could later change the zipkin-ui (or as some sort of plugin) to query across both and/or create a automatic flow (such as a button which clicked posts to the \"permanent\" zipkin). cc @rogeralsing @eirslett \nThis automatically solves any future needs around retention, as the same mechanics can be used. The only difference is that in the case of cassandra, the keyspace should be affected prior to use, notably to remove the TTL (or set it to a very long value). The best win is that there's no code impact on server components. They remain simple and probably more \"microservice\" as a result.\nthoughts?\n. I thought about this a bit over the weekend. Here's what that ended up as.\n- once we start worrying about retention policy for \"saved tweets\" we approach the complexity of the span store (again)\n- this is compounded by a need for an api change, which would break folks to support a feature that hasn't been requested widely\n- the more complexity we add our component apis, the harder future changes get\nI've an alternative proposal: do it all on the client\nInstead of creating a secondary tier storage in our api, simply deploy twice. Ex use a keyspace/DB for the transient trace depot and another for the \"permanent\" one.\nEx. index=zipkin (for transient) and index=zipkin-4ever for permanent.\nThe second is fronted by vanilla zipkin-servers that don't run any collectors except http. The act of \"saving a trace\" is just taking the json from the transient one and POSTing it to the permanent one.\nSomeone could later change the zipkin-ui (or as some sort of plugin) to query across both and/or create a automatic flow (such as a button which clicked posts to the \"permanent\" zipkin). cc @rogeralsing @eirslett \nThis automatically solves any future needs around retention, as the same mechanics can be used. The only difference is that in the case of cassandra, the keyspace should be affected prior to use, notably to remove the TTL (or set it to a very long value). The best win is that there's no code impact on server components. They remain simple and probably more \"microservice\" as a result.\nthoughts?\n. Thanks for the feedback. I have one question on your comment.\n\nHowever, I prefer making this a feature of the current backend instead of\nhaving separate clusters though. I think that way the backend would be\neasier to operate. Multiple clusters increase operational overhead in large\norganizations.\nBy backend, if you mean storage, I think this is already possible because\nyou can use different index in the same cluster.\n\nIf by backend you mean zipkin servers that would really complicate\nconfiguration as currently they are designed for a single storage\ncomponent. I don't think this feature is worth complexifying that as it is\nquite easy to spin up api servers.\n. Thanks for the feedback. I have one question on your comment.\n\nHowever, I prefer making this a feature of the current backend instead of\nhaving separate clusters though. I think that way the backend would be\neasier to operate. Multiple clusters increase operational overhead in large\norganizations.\nBy backend, if you mean storage, I think this is already possible because\nyou can use different index in the same cluster.\n\nIf by backend you mean zipkin servers that would really complicate\nconfiguration as currently they are designed for a single storage\ncomponent. I don't think this feature is worth complexifying that as it is\nquite easy to spin up api servers.\n. FYI \"permanent traces\" will eventually clash, even if unlikely for some. While not a strict dependency, this is certainly related to the 128bit trace id work https://github.com/openzipkin/zipkin/issues/1262\n. FYI \"permanent traces\" will eventually clash, even if unlikely for some. While not a strict dependency, this is certainly related to the 128bit trace id work https://github.com/openzipkin/zipkin/issues/1262\n. ps the original version of zipkin had a \"favorite\" button (trivia). ps the original version of zipkin had a \"favorite\" button (trivia). This is a side effect of gradle. You'll notice that the server logged that\nit started. Ignore the 97%, basically\n. interesting.. id as hex is documented..\nhttps://github.com/openzipkin/zipkin-api/blob/master/zipkin-api.yaml#L141\nbut.. the website doesn't seem to render that description:\nhttp://zipkin.io/zipkin-api/#/paths/%252Ftrace%252F%257BtraceId%257D\n@mohsen1 any ideas why?\n. cc @mosesn \n. cc @jtrobec\n. looks reasonable to me!\n. will merge on green and cut a patch release\n. going out as 1.39.7\n. Hi. so the docs actually indicate a megabyte is the max size per message.\nRegardless, let me see if there's a way for us to constrain this portably.\nIf would be nice if we could have one collector parameter for this, as\nlarge messages can become problematic.\nhttp://kafka.apache.org/documentation.html\n. which reporter are you using to send the messages? I noticed that htrace compresses the messages like so..\nhttps://github.com/apache/incubator-htrace/blob/master/htrace-zipkin/src/main/java/org/apache/htrace/impl/KafkaTransport.java#L105\n. so kafka is tuned for smaller message sizes, and 1MB with compression can hold a fair amount of spans. Can you try compression first? I've opened an issue in zipkin-java to add metrics on message size https://github.com/openzipkin/zipkin-java/issues/165\n. OK if that's the case, then this needs to be a kafka-specific parameter, not conflated with span message size\n. zipkin doesn't have logs. I presume you mean to say annotations where\nsomeone encoded json as the value.. first check to see if that is valid.\nfor example one thing that causes this is OpenTracing, but as far as I\nrecall, \"logs\" are encoded in annotation as k1=v1,k2=v2 which is\nsimultaneously easier to read (in one line) and also not json.\n. thx for following up\n. can you clarify if there's any action desired on this?\n. I think this was probably fixed by now. updated the travis things and links https://github.com/openzipkin/zipkin/commit/2b50556dbb940d39fb6b4ae6dfc04a1c042ad143\n. moved release tags for 1.0.0 and 1.1.0 and backfilled release notes\n. @gregwhitaker @anuraaga note! what was zipkin-java is now here. please update your refs accordingly\n. created a branch on zipkin-java called cutover and force pushed a README that redirects here\n. done!\n. done!\n. thanks!\nmind putting in the README how to connect to an existing zipkin server (ex via docker or some other specified ip)? \n. Thanks! \n. What one  can synchronise before a flight is indeed important!\n. nice once :)\n. Zipkin-Web is deprecated, but listens by default on port 8080 (bin/web)\n. zipkin-web is a dumb proxy and unnecessary.\nYou don't need to run it because the javascript is now hosted on the query\nserver which defaults to port 9411. This port can be changed with the\nQUERY_PORT variable.\nPer various communication, the code here is not going to be maintained, so\nI would recommend transitioning to the new server. You will get better\nsupport from me and others, as well better documentation and a better\nserver.\nhttps://github.com/openzipkin/zipkin-java#quick-start\nhttps://github.com/openzipkin/zipkin-java/tree/master/zipkin-server#environment-variables\n. Yep! Plus, depending on what you are doing you don't even need collector.\n. Glad things are working for you!\n. please re-open if the new codebase has this issue (I think it won't)\n. please re-open if the new codebase has this issue (I think it won't)\n. just opened this as a constrained proposal to only address the trace id as 128bit https://github.com/openzipkin/zipkin/issues/1262\n. just opened this as a constrained proposal to only address the trace id as 128bit https://github.com/openzipkin/zipkin/issues/1262\n. Thanks for the effort and review folks. This is very popular!\n. running it locally and will merge (and release) after!\n. ah ok thx for the heads-up. this is a different PR, right?\n. fyi here are some screen shots:\n\n\n. @yurishkuro you have this feature already.. do you default to raw json? any thoughts about what the default should be?\n. interesting. search works fine with the pop-up, but save-as doesn't. It is\na common support case... @virtuald https://github.com/virtuald thoughts\non save-as etc?\n. \nHere's a thought. you can add a save or download link to the pop-up. this could point to /api/v1/trace/id for example.\n. yay json button!\n. can you post an example screen shot of config missing and also bad request?\n. ps great work!\n. :thumbsup:\n. needs a rebase, then ready to go\n. pink is the new black, I guess\n. TL;DR I feel your pain, as I've mentioned this myself a couple times. However, the route isn't to ask the entire zipkin ecosystem to change to adopt this format. The following issue is tracking this https://github.com/spring-cloud/spring-cloud-sleuth/issues/253\nThere are good reasons for spring-cloud-stream to envelope data, because it is trying to do things portable across different messaging transports. This isn't a good enough reason to muddy zipkin's data format.\nFor example, this data has nothing to do with the payload, intentionally! There's also no cross-language tool that shares this format. In other words, we'd be special-casing spring-cloud-stream, but impacting several languages who don't have a tool that reads this, and probably couldn't be runtime portable with spring-cloud-stream. In other words, this is the wrong way to look at the problem.\nI'd explore some way of making sleuth/stream send zipkin formatted messages to a dedicated zipkin topic. For example, sending in a way that doesn't require affecting the payload. If that isn't possible, another option would be to add a KafkaZipkinSpanReporter, referencing the one in brave.\nhttps://github.com/openzipkin/brave/blob/master/brave-spancollector-kafka/src/main/java/com/github/kristofa/brave/kafka/KafkaSpanCollector.java\nKafka reporters in any language are <100 lines of code, and it is normal case in zipkin to expect transports such as http, scribe and kafka to not share the actual transport code or metadata. What does work is sharing a data format, so we should focus on how to achieve that.\n. The issue you are getting is because you are running an older version\nof cassandra. Please update per below:\nhttps://groups.google.com/d/msg/zipkin-user/s6NZctGdNJM/VpytyTsICQAJ\nFYI The scala code is no longer supported, and the zipkin-java code\nthat replaces it has a workaround to this issue. Shortly this code\nwill replace the master branch per #1119\n. closing as the current code doesn't throw this exception\n. closing as the current code doesn't throw this exception\n. note if you are running an unpatched, or not recently patched version of cassandra 2.1 you'll want to set CASSANDRA_ENSURE_SCHEMA=false until below. For example, I've adhoc-tested 2.1.14 and that works in degraded mode, but 2.1.1 does not.\nhttps://groups.google.com/forum/#!topic/zipkin-user/s6NZctGdNJM\n. note if you are running an unpatched, or not recently patched version of cassandra 2.1 you'll want to set CASSANDRA_ENSURE_SCHEMA=false until below. For example, I've adhoc-tested 2.1.14 and that works in degraded mode, but 2.1.1 does not.\nhttps://groups.google.com/forum/#!topic/zipkin-user/s6NZctGdNJM\n. ps the error would bubble as Cannot read keyspace metadata for give keyspace, and before this change, the only way out would be to set CASSANDRA_ENSURE_SCHEMA=false\n. ps the error would bubble as Cannot read keyspace metadata for give keyspace, and before this change, the only way out would be to set CASSANDRA_ENSURE_SCHEMA=false\n. fyi @michaelsembwever I'm going to merge this as it is not safe to assume folks have installed the keyspace\n. fyi @michaelsembwever I'm going to merge this as it is not safe to assume folks have installed the keyspace\n. This issue was moved to openzipkin/docker-zipkin#112\n. we can change the parser, too. To be clear, it is sending \"parentId\": null right?\n. we can change the parser, too. To be clear, it is sending \"parentId\": null right?\n. some relevant notes here https://github.com/openzipkin/zipkin-api/pull/17\n. duration query needs some major work and was previously tracked in\nhttps://github.com/openzipkin/zipkin-java/issues/200\nwe should probably migrate some of the issues to the new repo\n. Sounds fine to me..  implementation would be to add @CrossOrigin onto ZipkinQueryApiV1\n@gena01 @marcingrzejszczak @shakuzen unless you know a reason not to, let's make this change\n. pretty sure this means that of the services participating in the\ntrace, mysql-ycfsolr_inno\naccounted for 88% of the critical path duration.\nto prove it I'd need to look deeper as I've not looked closely at the code\ninvolved\n. look at the json (click json) associated with this trace.. it might show\nthe timestamps for these spans are the same?\n. closing for now.  to sort this kind of problem means looking at json usually. we'd need a heuristic for error status (or a new field on Span), as you\nmentioned..\n@yurishkuro you have an opinion on this?\n. both sound reasonable\n. @virtuald you think you might be able to help make this come to life? Even in zipkin's internal troubleshooting, I could find value in quickly identifying an error span.\n. > Let's clarify what we would actually do then in zipkin terms:\n\n\nIf an annotation named error is present in a span, then the dot\n  associated with the annotation is colored red. Perhaps the entire span is\n  also colored yellow, or some other color that draws your attention to it?\n\nHere's 2p: If there is only one service participating in the span, it is\nunambiguously error, so red. When there's 2 aka RPC (client+server), it\ncould be yellow. Also fine with red regardless of how many services\nparticipate.\n- If a binary annotation with the keyname error is present in a span,\n  then the entire span is colored red\nSG\nSound good?\nyeap sounds good! I'll raise a PR to add annotations to the constants file\nbase on OpenTracing's (simply the word \"error\")\n. Here's documentation for the error https://github.com/openzipkin/zipkin-api/pull/18\n. there's a \"/metrics\" endpoint. It would show how many spans were accepted by kafka on the zipkin-server side.  can you send relevant output from that?\n\nalso, I don't understand what you mean by\n\n\"ffffffffffffffff\" was replaced in \"1-5000000\";\n\ndo you mean that when you post a span where the id is all bits set, it comes back not in hex?\n. wondering if this is to do with the data being older? Ex. if I post this, I have to set lookback to a relatively high value to see it.\nbash\n$ curl -s localhost:9411/api/v1/spans -X POST -H \"Content-Type: application/json\" --data '[\n  {\n    \"traceId\": \"ffffffffffffffff\",\n    \"name\": \"fermentum\",\n    \"id\": \"ffffffffffffffff\",\n    \"annotations\": [\n      {\n        \"timestamp\": 1466386794757000,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"semper\",\n          \"ipv4\": \"113.29.89.129\",\n          \"port\": 2131\n        }\n      },\n      {\n        \"timestamp\": 1466386794757000,\n        \"value\": \"sagittis\",\n        \"endpoint\": {\n          \"serviceName\": \"semper\",\n          \"ipv4\": \"113.29.89.129\",\n          \"port\": 2131\n        }\n      },\n      {\n        \"timestamp\": 1466386794758000,\n        \"value\": \"montes\",\n        \"endpoint\": {\n          \"serviceName\": \"semper\",\n          \"ipv4\": \"113.29.89.129\",\n          \"port\": 2131\n        }\n      },\n      {\n        \"timestamp\": 1466386794758000,\n        \"value\": \"augue\",\n        \"endpoint\": {\n          \"serviceName\": \"semper\",\n          \"ipv4\": \"113.29.89.129\",\n          \"port\": 2131\n        }\n      },\n      {\n        \"timestamp\": 1466386794759000,\n        \"value\": \"malesuada\",\n        \"endpoint\": {\n          \"serviceName\": \"semper\",\n          \"ipv4\": \"113.29.89.129\",\n          \"port\": 2131\n        }\n      },\n      {\n        \"timestamp\": 1466386794760000,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"semper\",\n          \"ipv4\": \"113.29.89.129\",\n          \"port\": 2131\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"mollis\",\n        \"value\": \"hendrerit\",\n        \"endpoint\": {\n          \"serviceName\": \"semper\",\n          \"ipv4\": \"113.29.89.129\",\n          \"port\": 2131\n        }\n      }\n    ]\n  }\n]'\n$ curl -s 'localhost:9411/api/v1/traces?serviceName=semper&lookback=5000000000'|jq .\n. I started a local server against elasticsearch and got the same output. ex the trace id returned the same as if it were not using elasticsearch\nbash\n$ SELF_TRACING_ENABLED=false STORAGE_TYPE=elasticsearch java -jar zipkin.jar\n. @liangman we need some way of reproducing the problem, so maybe verify versions and see if you can reproduce something using POST like above. Our tests run latest zipkin against elasticsearch 2.2.1\n. great... glad to see progress. First step is to make sure that when\nyou say POST you mean POST + ES. That way, there's only one variable\nchanging, the transport. After you run your scenario, store the\ncollector metrics, which should show how many messages were sent, if\nany were dropped etc.\nex. http://localhost:9411/metrics\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-server#collector\nOnce you verify this, change only the transport variable and run the\nsame scenario (i.e. report using Kafka, not HTTP).\nlook at the /metrics endpoint and compare the kafka stats with the\nhttp stats from the prior pass. you can also run the server with the\nargument --logging.level.zipkin=DEBUG commandline argument\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-server#logging\nYou might see dropped spans or dropped messages in the metrics output,\nand you might see exceptions in the log output. This is how we can\nstart progressing from here.\n. looking at your metrics output, it seems you aren't running the latest\nversion of zipkin (counter.zipkin_collector.bytes should have read\ncounter.zipkin_collector.bytes.http).\nI don't think this impacts your issue, but it would be less distracting to\nuse the same version of code (latest is 1.1.5).\nOne thing that seems odd is that the cumulative bytes collected from\nhttp(157k)  are less than the cumulative bytes collected from kafka (407k).\nAre you using the same encoding for both? the byte count is after any\ndecompression, so I'd expect figures to be similar...\nRegardless, if a scenario of only 100 spans can create the concern, it\nseems small enough to be something myself or someone else could run with\nease.\ndo you mind posting your script somewhere so that I can try it?\n. maybe you can send the same json you send via http using something like\nthis?\nbash\n$ kafka-console-producer.sh --broker-list $ADVERTISED_HOST:9092 --topic zipkin\n[{\"traceId\":\"1\",\"name\":\"bang\",\"id\":\"2\",\"timestamp\":1234,\"binaryAnnotations\":[{\"key\":\"lc\",\"value\":\"bamm-bamm\",\"endpoint\":{\"serviceName\":\"flintstones\",\"ipv4\":\"127.0.0.1\"}}]}]\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-collector/kafka#json\n. OK so I've verified that with the following setup, I get readbacks between 58 and 100 spans when using the kafka script vs the http one which routinely reads back 100.\nwhat I do, is run the scenarios below while kafka is left up, but elasticsearch is cleaned between runs\n\nWhere below instructions run kafka and ES\n``` bash\nstart kafka\n$ curl -SL http://www.us.apache.org/dist/kafka/0.8.2.2/kafka_2.11-0.8.2.2.tgz | tar xz\n$ nohup bash -c \"cd kafka_ && bin/zookeeper-server-start.sh config/zookeeper.properties >/dev/null 2>&1 &\"\n$ nohup bash -c \"cd kafka_ && bin/kafka-server-start.sh config/server.properties >/dev/null 2>&1 &\"\nstart ES\n$ curl -SL https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.2.1/elasticsearch-2.2.1.tar.gz | tar xz\n$ elasticsearch-*/bin/elasticsearch -d > /dev/null\n```\nAnd.. I start zipkin-server like so..\nbash\nSELF_TRACING_ENABLED=false KAFKA_ZOOKEEPER=localhost:2181 STORAGE_TYPE=elasticsearch java -jar zipkin.jar --logging.level.zipkin=DEBUG\nHTTP\nWhen I run the HTTP test like this:\nbash\nfor i in {1..100}; do ./senddata.sh `printf \"%x\" $i`; done\nI get these collector metrics:\n\"gauge.zipkin_collector.message_spans.http\": 1,\n  \"counter.zipkin_collector.spans.http\": 100,\n  \"gauge.zipkin_collector.message_bytes.http\": 1569,\n  \"counter.zipkin_collector.messages.http\": 100,\n  \"counter.zipkin_collector.bytes.http\": 156870,\nAnd the api count looks correct:\nbash\n$ curl -s 'localhost:9411/api/v1/traces?lookback=500000000&limit=100'|jq '. | length'\n100\nKafka\nWhen I run the Kafka test like this:\nbash\n$ python senddatatokafka.py \nJava HotSpot(TM) 64-Bit Server VM warning: Option UseParNewGC was deprecated in version 9.0 and will likely be removed in a future release.\n[2016-06-25 14:11:06,504] WARN Property topic is not valid (kafka.utils.VerifiableProperties)\nfinsh!\n[2016-06-25 14:11:06,676] WARN Error while fetching metadata [{TopicMetadata for topic zipkin -> \nNo partition metadata for topic zipkin due to kafka.common.LeaderNotAvailableException}] for topic [zipkin]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)\n[2016-06-25 14:11:06,684] WARN Error while fetching metadata [{TopicMetadata for topic zipkin -> \nNo partition metadata for topic zipkin due to kafka.common.LeaderNotAvailableException}] for topic [zipkin]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)\n[2016-06-25 14:11:06,684] ERROR Failed to collate messages by topic, partition due to: Failed to fetch topic metadata for topic: zipkin (kafka.producer.async.DefaultEventHandler)\nI get these collector metrics:\n\"gauge.zipkin_collector.message_bytes.kafka\": 903,\n  \"counter.zipkin_collector.bytes.kafka\": 90270,\n  \"counter.zipkin_collector.spans.kafka\": 100,\n  \"gauge.zipkin_collector.message_spans.kafka\": 1,\n  \"counter.zipkin_collector.messages.kafka\": 100,\nAnd the api count looks correct sometimes, and not others (always the stats look the same):\nbash\n$ curl -s 'localhost:9411/api/v1/traces?lookback=500000000&limit=100'|jq '. | length'\n100\n. NEXT STEP:\none difference between in-memory storage and ES storage is that the former doesn't do anything asynchronously. We should validate that this scenario against Cassandra, too (as it also uses guava futures).\n. Might be an issue in cassandra, too, but looks like #1142 is blocking my ability to use the normal readback (only returns 10-16)\n. @liangman I edited the comment for the http script. can you edit the one for kafka and make sure that timestamps are reset each time (using epoch micros)?\n. NEXT STEP:\nSee the behavior when the kafka script reports spans with unique timestamps. For example TIMESTAMP=$(node -e 'console.log(new Date().getTime())')000. I don't really expect this to make a difference, but we ought to be consistent.\nA step after that would be to instrument ElasticsearchSpanConsumer in some way that we can track the futures (possibly ensuring the result has the correct numberOfActions() etc). This might be hard to track down, but at least the scenario is repeatable.\nps I'm offline likely the rest of the day, but might look into this tomorrow.\n. sure.. I'd like you to edit your comment here https://github.com/openzipkin/zipkin/issues/1141#issuecomment-228511204\nin the span you are generating in python, please make it have timestamps according to current system time.\nThat reduces the work needed when querying and also ttl considerations.  For example, you can look at how I edited the http script.\nhttps://github.com/openzipkin/zipkin/issues/1141#issuecomment-228507867\nafter that you can try to troubleshoot ElasticsearchSpanConsumer by customizing the class, building and running locally. https://github.com/openzipkin/zipkin/tree/master/zipkin-server#running-locally\nFor example, you could add print statements etc.\nIf you aren't familiar enough to do that, you'd likely need to wait until I have more time to help (or someone else does).\n. @luoyongjiee update. I've reproduced this problem in a unit test (important so it doesn't creep back in). Yuri's suggestion works fine, but the index needs to be created. We'll have a release with the fix out by tomorrow.\n. Reverted 0d51d90 as it needs more work. We need the query to return only unique trace ids, or repeat the query up to limit.\nEx.\n```\ncqlsh:zipkin> SELECT bucket,blobAsBigint(timestampAsBlob(ts)),trace_id FROM service_name_index WHERE bucket in (0,1,2,3,4,5,6,7,8,9) and service_name='zipkin-server' limit 10;\nbucket | system.blobasbigint(system.timestampasblob(ts)) | trace_id\n--------+-------------------------------------------------+----------------------\n      0 |                                   1467174838732 |  4603096813731895486\n      0 |                                   1467174836587 | -1336993720941103457\n      0 |                                   1467174787202 |  6361320438414089915\n      0 |                                   1467174787201 |  6361320438414089915\n      0 |                                   1467174787201 |  9078688442428604055\n      0 |                                   1467174781499 |  5991165789192187628\n      0 |                                   1467174778689 | -8155071232048349133\n      0 |                                   1467174778448 |  -395579676103804384\n      0 |                                   1467174777530 |  3288432430321170335\n      0 |                                   1467174776014 |  1836663428203143817\n(10 rows)\n``\n. so.. I'll wait for experts to get an idea.. I want to getdistinct(trace_id)`, as opposed to a row for every span reported in the trace (often 2 rows per span). @michaelsembwever @yurishkuro @danchia rescue request :)\n. I think this is the last update I have on this issue.\nQueryRequest.limit always is higher than results from Cassandra queries, regardless of the schema adjustment we made (though the schema change does actually help with precision). That's because limit is applied to redundant index entries, which are deduped client side. The redundant index count is related to span count per trace.\nIt is a fools errand to attempt to deduplicate trace ids on the (cassandra) server side because trace ids aren't a partition key. We can only do distinct clauses on partition keys. The only way left is to compensate on the (cassandra) client side: zipkin in this case.\nHere's two concrete proposals:\nChange CassandraSpanConsumer to cache trace id indexes locally\nBy caching trace id indexes locally, we can ensure that at least in the same collector shard, we don't write the same unique input more than once per trace. This will be most effective for those who run a single collector or consistently route trace ids to collector instances. Even randomly routed collectors should see smaller indexes, when spans in the same trace are bundled when reported from tracers.\nChange CassandraSpanStore to fetch more trace ids than QueryRequest.limit\nThe trace id query returns very little data: trace id and timestamp. One option is to just prefetch more than limit and dedupe client side. Since this issue is amplified at low trace counts, we can simply make a floor of 100 and dedupe to QueryRequest.limit. This could be a first step before we do something like multiple expanding queries.\n. No required changes on my end, except linking to the server's README (to make it more discoverable)\nnice work!\n. nice work. squashed on entry\n. hi all. I just noticed that prometheus' java client now ships with spring boot support. any thoughts on whether we should switch? https://github.com/prometheus/client_java/tree/master/simpleclient_spring_boot/src/main/java/io/prometheus/client/spring/boot\ncc @shakuzen . ping @anuraaga \n. \ud83d\udc4d \n. LGTM\n. thinking about jbender for this http://blog.paralleluniverse.co/2016/03/30/http-server-benchmark/ https://github.com/pinterest/jbender\n. thinking about jbender for this http://blog.paralleluniverse.co/2016/03/30/http-server-benchmark/ https://github.com/pinterest/jbender\n. if you can, please add ERROR to this and use the constant. we can change the java, thrift sides in a different PR.\n. this :) https://github.com/openzipkin/zipkin/blob/master/zipkin-ui/js/component_ui/traceConstants.js\n. something to consider.. if we think of an \"error\" event as/annotation transient, we might want to stain the span yellow instead. Presence of the binary annotation could imply complete failure (something the client would be responsible for in an RPC span, or the server is responsible for in a server-only root span).\nSee https://github.com/openzipkin/zipkin-api/pull/18/files/1dc521c1994c104211f9a72400e48eae5e948549#r68687632 from @yurishkuro \n. something to consider.. if we think of an \"error\" event as/annotation transient, we might want to stain the span yellow instead. Presence of the binary annotation could imply complete failure (something the client would be responsible for in an RPC span, or the server is responsible for in a server-only root span).\nSee https://github.com/openzipkin/zipkin-api/pull/18/files/1dc521c1994c104211f9a72400e48eae5e948549#r68687632 from @yurishkuro \n. for example, I think finagle logs \"finagle.retry\" which is a stack-specific way of saying there was a transient error\n. for example, I think finagle logs \"finagle.retry\" which is a stack-specific way of saying there was a transient error\n. all your help has been appreciated, Dustin!\n. all your help has been appreciated, Dustin!\n. great. will try it out soon!\n. great. will try it out soon!\n. tried and looks good. thanks!\n. tried and looks good. thanks!\n. This came to mind when troubleshooting #1141\n@anuraaga PTAL, particularly on the descriptions.\n. This came to mind when troubleshooting #1141\n@anuraaga PTAL, particularly on the descriptions.\n. updated with better comments thx to @anuraaga \n. updated with better comments thx to @anuraaga \n. Releasing 1.2.1 which reverts this change due to https://github.com/openzipkin/zipkin/issues/1142#issuecomment-229254901\n. Releasing 1.2.1 which reverts this change due to https://github.com/openzipkin/zipkin/issues/1142#issuecomment-229254901\n. we don't run tests on 3.7, but I don't know a reason why it wouldn't work.\n. we don't run tests on 3.7, but I don't know a reason why it wouldn't work.\n. I mean proceed, and report any issues!\n. I mean proceed, and report any issues!\n. One neat side-effect is that it also logs the DDL stuff, which can explain large pauses on startup.\nEx.\n2016-06-30 15:39:14.267 TRACE 29485 --- [r2-nio-worker-1] c.d.driver.core.QueryLogger.NORMAL       : [cluster2] [localhost/127.0.0.1:9042] Query completed normally, took 50 ms: [0 bound values] CREATE TABLE IF NOT EXISTS zipkin.traces (\n    trace_id  bigint,\n    ts        timestamp,\n    span_name text,\n    span      blob,\n    PRIMARY KEY (trace_id, ts, span_name)\n)\n    WITH compaction = {'class': 'org.apache.cassandra.db.compaction.DateTieredCompactionStrategy', 'max_window_size_seconds': '86400'}\n    AND default_time_to_live =  604800;\n. One neat side-effect is that it also logs the DDL stuff, which can explain large pauses on startup.\nEx.\n2016-06-30 15:39:14.267 TRACE 29485 --- [r2-nio-worker-1] c.d.driver.core.QueryLogger.NORMAL       : [cluster2] [localhost/127.0.0.1:9042] Query completed normally, took 50 ms: [0 bound values] CREATE TABLE IF NOT EXISTS zipkin.traces (\n    trace_id  bigint,\n    ts        timestamp,\n    span_name text,\n    span      blob,\n    PRIMARY KEY (trace_id, ts, span_name)\n)\n    WITH compaction = {'class': 'org.apache.cassandra.db.compaction.DateTieredCompactionStrategy', 'max_window_size_seconds': '86400'}\n    AND default_time_to_live =  604800;\n. ok this works and is thoroughly unit tested\n. ok this works and is thoroughly unit tested\n. This is a bit large a topic to address in this repo, and wouldn't likely affect any code inside of it. Open-ended questions are better for gitter or zipkin-user, probably best gitter.\nhttps://gitter.im/openzipkin/zipkin\nhttps://groups.google.com/forum/#!forum/zipkin-user\nSince what you are trying to do has multiple aspects, you might want to create a github repo on your research project and add an issues to it there. For example, how to accept trace identifiers.. how to report json (or otherwise) back to zipkin. Dividing concerns can help make helping more approachable, as people often don't have time to address several topics at the same time.\nWhat would also make things more approachable is not doing this in Oracle. For example, see if you can create a scenario similar enough in something like mysql or postgresql. Then, port your approach to Oracle.\nBack to specifics, I don't think there's any existing SQL database accepting traces propagated from a zipkin-instrumented caller. There is one for cassandra https://github.com/thelastpickle/cassandra-zipkin-tracing\nThere may be some topics on our documentation site that need added to or elaborated. Here are some notes that may be interesting to you:\nhttps://github.com/openzipkin/openzipkin.github.io/issues/31\nhttps://github.com/openzipkin/openzipkin.github.io/issues/11\nfeel free to ask more questions here https://gitter.im/openzipkin/zipkin and good luck\n. This is a bit large a topic to address in this repo, and wouldn't likely affect any code inside of it. Open-ended questions are better for gitter or zipkin-user, probably best gitter.\nhttps://gitter.im/openzipkin/zipkin\nhttps://groups.google.com/forum/#!forum/zipkin-user\nSince what you are trying to do has multiple aspects, you might want to create a github repo on your research project and add an issues to it there. For example, how to accept trace identifiers.. how to report json (or otherwise) back to zipkin. Dividing concerns can help make helping more approachable, as people often don't have time to address several topics at the same time.\nWhat would also make things more approachable is not doing this in Oracle. For example, see if you can create a scenario similar enough in something like mysql or postgresql. Then, port your approach to Oracle.\nBack to specifics, I don't think there's any existing SQL database accepting traces propagated from a zipkin-instrumented caller. There is one for cassandra https://github.com/thelastpickle/cassandra-zipkin-tracing\nThere may be some topics on our documentation site that need added to or elaborated. Here are some notes that may be interesting to you:\nhttps://github.com/openzipkin/openzipkin.github.io/issues/31\nhttps://github.com/openzipkin/openzipkin.github.io/issues/11\nfeel free to ask more questions here https://gitter.im/openzipkin/zipkin and good luck\n. kafka partitioning shouldn't work differently per storage backend. I think\nyou are running into the limit problem (#1142)\n@prat0318 do you have any general advice on partitioning to share (I can\ntry to include it in docs)\n. kafka partitioning shouldn't work differently per storage backend. I think\nyou are running into the limit problem (#1142)\n@prat0318 do you have any general advice on partitioning to share (I can\ntry to include it in docs)\n. related to #903\n. ps there are notes about Yelp's infrastructure (which uses kafka) here https://docs.google.com/document/d/1Px44fjZ37gr05lV7UFo8AfrWZCcJHCuv58290XCbDaw/edit#\n. @danchia @eirslett @yurishkuro @michaelsembwever blast from the past\n. @danchia @eirslett @yurishkuro @michaelsembwever blast from the past\n. going to merge this as I'd like to do some cleanups on top. if there are any comments, happy to change the docs post commit\n. Thanks for answering!\n. When we released the new server, we clarified this topic on the mailing list, too, which you are free to join, too!  https://groups.google.com/forum/#!topic/zipkin-user/VtyR3GD-0Fo\n. yes, this is related to your other question, where the answer implied you can run multiple instances of zipkin #1159\nWhen we released the new server, we clarified this topic on the mailing list, too, which you are free to join https://groups.google.com/forum/#!topic/zipkin-user/VtyR3GD-0Fo\n. @dragontree101 there's not much point in creating a separate JVM for the UI, as the UI is just javascript and configuration. Some host the UI on nginx, but we don't support this at the moment.\nConfiguration aside, there's no state shared between the api/ui components and the collector ones.\nThis means yes, you can choose to send api/ui traffic to different servers than you send spans to.\n. yep. they don't coordinate or share state between each other. (ps use a\nlater version like 1.2.1 :) )\nthink of them like you would a normal stateless api server.\n. OK I'm pretty done with this (and out of time). Unless there are other comments, I'll merge soon.\n. first adjustment is to zipkin-api https://github.com/openzipkin/zipkin-api/pull/19\n. cc @eirslett @abesto \n. 4.5K lines of this change is comparing w/libthrift (checking in generated files as opposed to introducing a build step)\n. another option I thought about, which might be less maintenance..\n- always merge zipkin-api first (zipkin impl second)\n  - This corrects the race condition, and is manageable as thrift changes are rare.\n- re-add libthrift codec comparisons to benchmarks\n  - when we add a new field, we should update the benchmarks anyway\n  - benchmarks are already explicitly skipped from publishing\n  - we can use the git raw url to get the thrift src\n. This came up in troubleshooting with @liyichao. Basically, we can eliminate potential sources of memory problems by making the disable setting easier to find.\n. cc @eirslett @kristofa @prat0318 @liyichao @anuraaga @basvanbeek @abesto thoughts?\n. For example, right now, if too many writes go to Elasticsearch, we get logs like this. Seems that in Kafka, we could have an opportunity to just read less..\norg.elasticsearch.common.util.concurrent.EsRejectedExecutionException: rejected execution of org.elasticsearch.transport.TransportService$4@3b438fe4 on EsThreadPoolExecutor[index, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@6ed30c21[Running, pool size = 8, active threads = 8, queued tasks = 200, completed tasks = 72]]\n. @prat0318 exactly\n. Here's the current thinking.\nSo Kafka implies running zookeeper. We have an adaptive sampler which uses zookeeper to store the target rate that the storage layer is capable of. We could re-use this code to rate limit kafka consumers (and/or drop), without adding a new service dependency (because kafka already needs zookeeper).\nThere's a problem we might have, which is that kafka marks slow consumers dead (I hear).\nthoughts?\n. nows a good time to start working on this, since the elasticsearch code is stabilizing cc @openzipkin/elasticsearch\n. easiest dead simple start on this is to make storage block the kafka stream loop\n. If you look at zipkin-aws and zipkin-azure in both cases you can add a\ncustom collector. So yes you could make a third party kafka collector that\ndoes backpressure nicely. However it would be even nicer to contribute such\na change upstream as all we are missing is hands to help!\nOn 14 Mar 2017 10:33, \"\u5b8b\u946b\" notifications@github.com wrote:\n\nIs there something 3rd lib can support Kafka with Zipkin?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1181#issuecomment-286354335,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610BJdzbqwIHbf0JVZerha96nhgr_ks5rllDegaJpZM4JKK1i\n.\n. If you look at zipkin-aws and zipkin-azure in both cases you can add a\ncustom collector. So yes you could make a third party kafka collector that\ndoes backpressure nicely. However it would be even nicer to contribute such\na change upstream as all we are missing is hands to help!\n\nOn 14 Mar 2017 10:33, \"\u5b8b\u946b\" notifications@github.com wrote:\n\nIs there something 3rd lib can support Kafka with Zipkin?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1181#issuecomment-286354335,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610BJdzbqwIHbf0JVZerha96nhgr_ks5rllDegaJpZM4JKK1i\n.\n. This was another subtle bug found by @liyichao!\n\n```\n2016-07-12 20:19:30.317  WARN 14973 --- [2-nio-worker-57] io.netty.channel.DefaultChannelPipeline  : An exception was thrown by a user handler's exceptionCaught() method while handling the following exception:\njava.lang.IllegalStateException: select-trace-ids-by-span-duration not in the cache eventhough tracing is on\n    at com.google.common.base.Preconditions.checkState(Preconditions.java:197) ~[guava-18.0.jar!/:na]\n``\n. ps this still ends up a warning in the log, just not as severe as it isn't relegated toexceptionCaught()handler\n. tested withSELF_TRACING_SAMPLE_RATE=0.01(exact rate) andSELF_TRACING_SAMPLE_RATE=0.001`(probabilistic rate)\n. @yurishkuro FYI\n. by \"only works on daily buckets\" I mean that in cassandra and also elasticsearch, we store links in daily buckets. when we get to pre-aggregating mysql, it would be simplest to follow that pattern\n. the http port is 9411, not 9410. try that?\n. I logged an issue to disable port 9410 by default. If you think this would be helpful, place a thumbs up on https://github.com/openzipkin/zipkin/issues/1192\n. https://github.com/openzipkin/zipkin/pull/1540 will disable scribe by default. https://github.com/openzipkin/zipkin/pull/1540 will disable scribe by default. cc @anuraaga ps I tried to get the aggregation query working, but somehow it was double-counting. I punted by doing client side, as at the moment less moving parts is helpful.\n. cc @anuraaga another thing noticed :)\n. thanks! real pull request coming in a minute in the spark repo\n. thanks! real pull request coming in a minute in the spark repo\n. To address the clock skew thing, pretend the producer action is a client\naction and the consumer action is a server one. these represent the\nboundaries of your two services, with kafka etc in the middle.\nThe problem is that there is kafka in the middle :) That said, the clock\nskew will be shifted anyway as regardless of that the consumer shouldn't\nreceive a message before it is produced.\nAnother (more dramatic approach) to clock skew could be to keep track of\nthings on the instrumentation side. For example, if you know the time of\nthe kafka server, you can shift to that before reporting. This is different\nthan NTP as you'd just shift for the purposes of sending trace data as\nopposed to the whole VM/instance.\n. To address the clock skew thing, pretend the producer action is a client\naction and the consumer action is a server one. these represent the\nboundaries of your two services, with kafka etc in the middle.\nThe problem is that there is kafka in the middle :) That said, the clock\nskew will be shifted anyway as regardless of that the consumer shouldn't\nreceive a message before it is produced.\nAnother (more dramatic approach) to clock skew could be to keep track of\nthings on the instrumentation side. For example, if you know the time of\nthe kafka server, you can shift to that before reporting. This is different\nthan NTP as you'd just shift for the purposes of sending trace data as\nopposed to the whole VM/instance.\n. @AndrewWang996 had a question about this in Brave. Basically how do you deal with a one-way system. He was concerned about half-open spans (\"cs\" and \"sr\" only), although I don't know what the specific issue with that was. Guessing dependencies view? or maybe lack of duration?\n- Zipkin doesn't care about these annotations except for a few scenarios\n  - clock skew correction (ex receiver happens before send)\n  - duration calculation (only relevant when Span.timestamp and Span.duration aren't set)\n  - dependencies view (to show the link between services\nI mentioned similar things to here..\n- You can treat the act of sending the message as \"cs\", \"cr\" and the act of receiving as \"sr\", \"ss\". Ex. close the span after you've sent the message (ignoring that there was no response received).\n- You can close the span with something else (ex Brave's LocalTracer), which adds span duration without using RPC annotations. You should add \"ca\" (producer) and \"sa\" (consumer) on the span (to ensure the dependencies graph can show things).\nThings we could do here is investigate clock skew when no RPC spans exist in a trace. That could be bumped out as a separate issue, but I think it would only work if \"ca\" and \"sa\" are logged.\n. @AndrewWang996 had a question about this in Brave. Basically how do you deal with a one-way system. He was concerned about half-open spans (\"cs\" and \"sr\" only), although I don't know what the specific issue with that was. Guessing dependencies view? or maybe lack of duration?\n- Zipkin doesn't care about these annotations except for a few scenarios\n  - clock skew correction (ex receiver happens before send)\n  - duration calculation (only relevant when Span.timestamp and Span.duration aren't set)\n  - dependencies view (to show the link between services\nI mentioned similar things to here..\n- You can treat the act of sending the message as \"cs\", \"cr\" and the act of receiving as \"sr\", \"ss\". Ex. close the span after you've sent the message (ignoring that there was no response received).\n- You can close the span with something else (ex Brave's LocalTracer), which adds span duration without using RPC annotations. You should add \"ca\" (producer) and \"sa\" (consumer) on the span (to ensure the dependencies graph can show things).\nThings we could do here is investigate clock skew when no RPC spans exist in a trace. That could be bumped out as a separate issue, but I think it would only work if \"ca\" and \"sa\" are logged.\n. If your goal is to measure latency of async spans, I think we can find\nsomething to work.\nIf your goal is to make a span with only \"cs\" and \"sr\" in it, I'm not\ninterested in helping as it will produce bugs elsewhere we'd have to clean\nup. The core RPC annotations are made to be used together, so you'd be\nbetter off being more flexible about this point.\n. ps spent all my battery on this topic on the flight :)\nso I think there are a few ways to skin this cat. I tried a couple\nwith Kafka (stuffing SpanId into the message key, and using callbacks\nto close the producer span)\n- represent sending to the broker as a cs/cr and receiving from the\n  broker as sr/ss (in the same span)\n  - time in transport is the interval between cr and sr (noting you\n    can also log \"ws\" \"wr\" to show wire send attempts)\n  - plus is that you can do this with all instrumentation\n  - minus is the \"ss\" and \"cr\" parts are forged (ex you are really\n    capturing the overhead of send and receipt)\n- make an annotation \"mp\"(message produced) \"mc\"(message consumed) in\n  the same span (annotation doesn't matter, just that there are a pair).\n  - time in transport is the interval between these (noting you can\n    also log \"ws\" \"wr\" to show wire send attempts)\n  - plus is that it doesn't conflate with client/server annotations\n  - minus requires you to flush an incomplete span on both sides (not\n    all instrumentation allow this)\n- make a separate span for the consumer side\n  - time in transport is the interval between the end of the producer\n    span and the beginning of the consumer span\n  - plus is the diagram more clearly separates the producer from the\n    consumer (they are in different spans)\n  - this means you propagate the \"next span id\" to the other side\nI can share my kafka code if someone is interested (written in brave)\n. here's the kafka spike https://github.com/openzipkin/brave/pull/212\n. let's see if we can nail a design down for zipkin v1 model here: https://github.com/openzipkin/zipkin/issues/1243\n. cc @anuraaga\n. cc @openzipkin/cassandra\n. @michaelsembwever noted that the highest impact is more about the key count, not the length. Ex there's more benefit to reducing the (irrelevant) cardinality in the table vs the length.\n@danchia for example, I'm not sure anyone would do a key lookup by md5 of a long url :)\nThis indirectly reduces count by stripping things that are presumably volatile from being long. Ex query parameters in a signature tend to be (but are not always) long, so stripping them on account of length, may very likely reduce the count as well.\nAnother option could be to make a blacklist, ex Signature. Pro is that this could weed out urls unlikley to be search keys. Problem is that this requires special knowledge of the namespace, and adds maintenance the zipkin project can't likely help with...\n. I'm going to merge this as I see more meh or in favor than against. thanks for the feedback!\n. https://github.com/openzipkin/zipkin/pull/1540. The only way to do this is somewhat basic.. put said file in front of the\nclasspath!\n. in the case of docker you'd overwrite the file at\n/zipkin/cassandra-schema-cql3.txt\ndoing arbitrary upgrades could be dodgy. there's careful logic about\nthe upgrade, and it checks for very certain things because CQL can't\ndo everything. A log message might be misleading if we used this\ncheck, but did something else.\nex. \"/cassandra-schema-cql3-upgrade-1.txt\" has this check\nstatic boolean hasUpgrade1_defaultTtl(KeyspaceMetadata keyspaceMetadata) {\n  // TODO: we need some approach to forward-check compatibility as well.\n  //  backward: this code knows the current schema is too old.\n  //  forward:  this code knows the current schema is too new.\n  return keyspaceMetadata.getTable(\"traces\").getOptions().getDefaultTimeToLive()\n\n0;\n}\n\nWe have tests to show the effects of this work etc, but arbitrary\nthings aren't something we could promise and therefore unlikely to be\nable to support.\nI'd recommend only replacing the semantic contents of the existing\nschema files for this reason. Also, there's a lot of folks who use\ncassandra.. maybe there are other tools available to keep schema up to\ndate which don't require zipkin's ENSURE_SCHEMA feature?\n. @jcarres-mdsol @marcingrzejszczak @shakuzen @eirslett need a second opinion on leaving the \"compute on demand\" around in mysql. Pro is that demos will show the aggregation without running any spark job. Con is that the data gets too big to return quickly.\n. Current approach is to aggregate on demand only if a zipkin-dependencies job has never been run. This means the both have to be true...\n- zipkin_dependencies table exists\n- zipkin_dependencies table has data\n. ok this will end up as zipkin 1.5 after I make sure the dependencies job works fine, etc\n. I was able to reproduce this bug. will cut a version shortly. Thanks for\nthe test case!\n. please verify with v1.4.4\n. thanks!\n. copying some people of interest, though it affects all\n@eirslett @virtuald @yurishkuro @dsyer @marcingrzejszczak \n. > Oh this would fix us needing to do the \"fake span\" on the server side in\n\nSleuth, wouldn't it?\nyeah, that's what I was thinking\n. I'd prefer the changes to the old code to stay separate from adding a new model. It will keep review easier as there are some unanswered things here that would be burdensome to move to #1252. Hopefully, the latter can just focus on adding the new model and therefore not need any changes from here.\n. re-opening as the root question was about searching by dependency link\n\nhttps://docs.google.com/document/d/1QulozaBhJemNgy4Db8uIc_1ycSLZufLfNeGW8QQ42vg/edit#heading=h.6bva3qvc4wz1 is a design in progress. Since last time we discussed how to deal with bootstrapping third parties and now, there's something brewing from @dsyer called https://github.com/dsyer/spring-boot-thin-launcher\nThe idea is that you could mix and match components this way, for example, adding the core zipkin server with elasticsearch storage and azure collector autoconfiguration deps.\nA stable docker image could still be created by doing a \"dry run\" which just downloads the dependencies listed in the manifest during the build.\nWhat do you all think about trying this? @aliostad @shakuzen @marcingrzejszczak @llinder @devinsba @anuraaga @kevinmdavis @denyska. one subtle plus to the thin launcher is that it would not have the problem\nof composing fat jars (classpath duplication). Ex unless I misunderstand,\nthe classpath would resolve to a single version on conflict, so you don't\naccidentally have mixed version of zipkin core jars.\n. copied from #1488\nUpdate: figured out how to get modularity working. It is a little constrained, but it works now.\nHere's how.\nStep 1: Add spring boot plugin to your autoconfig module, adding a \"module\" jar\nThe classifier name \"module\" is arbitrary, but I think it works. Take extreme care to not duplicate jars already in zipkin-server's exec jar. Here's an example configuration for the SQS collector:\n```xml\n  \n\n\n Import dependency management from Spring Boot \norg.springframework.boot\nspring-boot-dependencies\n${spring-boot.version}\npom\nimport\n\n\n\n\n\n\norg.springframework.boot\nspring-boot-maven-plugin\n\n\n\nrepackage\n\n\n\n\nMODULE\nmodule\n https://github.com/spring-projects/spring-boot/issues/3426 transitive exclude doesn't work \nio.zipkin.java,org.springframework.boot,org.springframework,commons-codec,com.fasterxml.jackson.core,com.fasterxml.jackson.dataformat,org.apache.httpcomponents,commons-logging,joda-time,software.amazon.ion\n already packaged in zipkin-server \naws-java-sdk-core,aws-java-sdk-sts,jmespath-java\n\n\n\n\n```\nThis will make a file like.. autoconfigure/collector-sqs/target/zipkin-autoconfigure-collector-sqs-0.0.4-SNAPSHOT-module.jar\nStep 2: Extract this module jar when composing a server layer\nI cannot get the PropertiesLauncher configuration to accept the module-jar directly. However, the following does work.\n\nextract your module jar somewhere (Ex to an extensions folder)\nexplicitly start zipkin using PropertiesLauncher with loader.path set to include that directory\n\nExample:\nbash\n$ java  -Dloader.path=/tmp/extensions -cp zipkin.jar org.springframework.boot.loader.PropertiesLauncher --zipkin.collector.sqs.queue-url=http://foobar\nNote: the module jar is self-contained.. when doing your devopsian things, feel free to just curl it from jcenter, like we do for the server jar.. copied from #1488\nUpdate: figured out how to get modularity working. It is a little constrained, but it works now.\nHere's how.\nStep 1: Add spring boot plugin to your autoconfig module, adding a \"module\" jar\nThe classifier name \"module\" is arbitrary, but I think it works. Take extreme care to not duplicate jars already in zipkin-server's exec jar. Here's an example configuration for the SQS collector:\n```xml\n  \n\n\n Import dependency management from Spring Boot \norg.springframework.boot\nspring-boot-dependencies\n${spring-boot.version}\npom\nimport\n\n\n\n\n\n\norg.springframework.boot\nspring-boot-maven-plugin\n\n\n\nrepackage\n\n\n\n\nMODULE\nmodule\n https://github.com/spring-projects/spring-boot/issues/3426 transitive exclude doesn't work \nio.zipkin.java,org.springframework.boot,org.springframework,commons-codec,com.fasterxml.jackson.core,com.fasterxml.jackson.dataformat,org.apache.httpcomponents,commons-logging,joda-time,software.amazon.ion\n already packaged in zipkin-server \naws-java-sdk-core,aws-java-sdk-sts,jmespath-java\n\n\n\n\n```\nThis will make a file like.. autoconfigure/collector-sqs/target/zipkin-autoconfigure-collector-sqs-0.0.4-SNAPSHOT-module.jar\nStep 2: Extract this module jar when composing a server layer\nI cannot get the PropertiesLauncher configuration to accept the module-jar directly. However, the following does work.\n\nextract your module jar somewhere (Ex to an extensions folder)\nexplicitly start zipkin using PropertiesLauncher with loader.path set to include that directory\n\nExample:\nbash\n$ java  -Dloader.path=/tmp/extensions -cp zipkin.jar org.springframework.boot.loader.PropertiesLauncher --zipkin.collector.sqs.queue-url=http://foobar\nNote: the module jar is self-contained.. when doing your devopsian things, feel free to just curl it from jcenter, like we do for the server jar.. this works fine now. both in aws and azure. we just need to polish instructions. From @eirslett on May 30, 2016 22:7\nOr maybe openzipkin...\n. cc @eirslett @kristofa @abesto \n. I'd conserve your internet for No Man's Sky :)\n. @anuraaga thoughts?\n. From @yurishkuro on May 26, 2016 15:27\nOne thing I am not sure about is whether there are actually any tools in ES to auto-expire the named indices that are outside of the retention window. I believe ES does not provide that out of the box and people are implementing adhoc devops solutions for that, which is sad.\n. From @anuraaga on May 27, 2016 3:47\nElasticsearch has a tool called curator which makes this pretty easy - I guess it's somewhat official. Yeah, it is a cron job somewhere so a bit of complexity but usage is really easy, you can specify a prefix of indices to delete, when they are older than n days. e.g. \ncurator delete indices --prefix zipkin- --older-than 7 --time-unit days --timestring %Y-%m-%d\nhttps://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html\nI was using this initially but we actually use the same elasticsearch for metrics as well, for which we needed a more complex custom cleanup job written in Java, so I ended up just adding the zipkin cleanup to that job as well but when this isn't the case, curator is great.\n. > I'd be willing to help design / implement this, but would like to get a\n\ndiscussion going.\nsgtm would be interested in other's thoughts, too. ps here's the last\nrequest for saving a trace: https://github.com/openzipkin/zipkin/issues/1093\n. > I think having a path that can save to an index named something like\n'.zipkin-infinite', which is not a daily bucketed index, would be pretty\ntrivial to implement and allow saving traces.\nThis is a great idea and also super-easy in elasticsearch as we don't need\nto do anything except move documents. It might take some thinking on other\nbackends.\n. Not currently keen on this being a span store method because eviction is\nlow priority and can be handled more bluntly in ES (drop the whole daily\nindex after a point) right?\n. > I was trying to consolidate the ideas of individual trace retention along\nwith default trace eviction - though I don't like it on the grounds of\nbackwards-incompatibility.\nahh thinking ahead :)\n\nSo, in some span stores, there's no eviction at all. Are you anticipating\nstorage usage being a problem on this feature? Wondering if we can separate\nsave-a-trace from retention policy of saved traces at least initially.  PS\nshould probably move this discussion to #1093\n. #228 relates to this. for example, if you get services names across 30k traces and 5M rows in the annotations table, it will take over 2 seconds to return\n. From @basvanbeek on May 25, 2016 14:28\nJust a note so we won't forget, but if we look into altering table structure for the MySQL option it might be wise to also test against the TokuDB engine. This DB engine uses fractal tree indexes for much faster insertions and speedy searches on tables with very large row sets. I think this engine might be very inline with Zipkin's database usage pattern and make the MySQL option scale better.\n. the refactor intentionally named the artifact with v1 suffix to allow for a\nnew impl in mysql or even postgres which is frequently asked about. since\nwe update the README I think this is closeable thanks!\n. the refactor intentionally named the artifact with v1 suffix to allow for a\nnew impl in mysql or even postgres which is frequently asked about. since\nwe update the README I think this is closeable thanks!\n. From @yurishkuro on May 6, 2016 2:17\nI seem to recall that the reason I didn't do it was because limit on a single time range isn't accurate since the results can be further filtered down by intersecting with results of another index query. This is a general problem with the current implementation of Cassandra storage, that each index is consulted individually with the final limit.\n. From @michaelsembwever on May 6, 2016 5:7\nI vaguely remember this, does  \"CASSANDRA-7017 \u2013 allow per-partition LIMIT clause in cql\" help?\n. From @michaelsembwever on May 6, 2016 5:13\nIf those ten requested results are a few days back, you're going to sequentially go through a lot of requests to get to them.\nBut an approach that dispatches one day (24 requests) at a time. That's going to still significantly reduce the number of requests from 168. And the latency increase in the worse case is 7-fold, which I suspect we can live with (when the results are so old).\n. Thanks for the tip.. we can work with this!\n. well, the way we've defined the request allows us flexibility. The response\nis those things matching in reverse order by timestamp, limited to a count.\nThis means we are really looking for the \"most recent traces\" matching\ncriteria, one of which is a duration range.\nThat in mind.. we should be able to use tips mentioned here and be pretty\nefficient!\n. From @danchia on May 6, 2016 5:22\n+1 to @michaelsembwever suggestion. I think batching the lookbacks into windows will probably help.\n. Here's what a duration query looks like right now\n\n. From @eirslett on May 7, 2016 11:56\nLucene does searches like this exponentially; first get 10 documents, then 20, then 40 etc. until the desired number of matches is found. We could do something like that to optimize the time windows. First look back one hour, then two hours, then four etc.\n. From @michaelsembwever on July 28, 2016 4:55\nThe simplest improvement here is to increase CassandraUtil.DURATION_INDEX_BUCKET_WINDOW_SECONDS from one hour to 6 hours.\nBut that means though the span_duration_index table needs to be wiped when upgraded.\nLooking into the requests it seems that the request is more typically restricted by a time range than it is a minimum duration.  If that is the case then the clustering keys are in the wrong order.\n. From @michaelsembwever on July 28, 2016 10:43\nLooking into this more it looks like these three tables can be merged into one.\n- service_name_index\n- service_span_name_index\n- span_duration_index\nby using the following table:\nCREATE TABLE zipkin.service_span_name_index (\n    service_name  text,      // service name\n    span_name     text,      // span name, or blank for queries without span name\n    bucket        int,       // time bucket, calculated as ts/interval (in microseconds), for some pre-configured interval like 1 day.\n    ts            timeuuid, // start timestamp of the span, truncated to millisecond precision\n    trace_id      bigint,    // trace ID. Included as a clustering column to avoid clashes (however unlikely)\n    duration      bigint,    // span duration, in microseconds\n    PRIMARY KEY ((service_name, span_name, bucket), ts)\n)\nActual query constraints against a duration range would no longer be possible, but I suspect that doing this at run-time is not going to be worse than what we have today, especially if we can reduce so much writes, and put more read queries into the cache.\n. From @michaelsembwever on July 28, 2016 10:45\nI'm working on a patch\u2026\n. From @michaelsembwever on July 28, 2016 12:21\nhttps://github.com/openzipkin/zipkin/pull/1204 is open, but I still need to fix one test.\n. one thing we can do in the mean time is to change the default ui lookback to 1 hour instead of 7 days.\nIf we default to looking back 1 hour, this matches the existing duration index bucket, which will absolutely help with this issue.\nhttps://github.com/openzipkin/zipkin/issues/1210\n. I know where you are going with this, just that features like this are\npretty expensive without changing schema. For example, to do this in JDBC\n(which uses the same schema as openzipkin/zipkin), we'd need to store\nsomething to indicate which side it is on. For example, I pull this\nintermediate state here\nhttps://github.com/openzipkin/zipkin-java/blob/master/zipkin/src/main/java/zipkin/internal/DependencyLinkSpan.java\nI wouldn't object to best-efforts to do this, as long as it doesn't add\ngnarliness. I know there are some approaches folks are using, ex using\nempty span names on clients (which makes the server win)\nRegardless, this should be a focus area here\nhttps://github.com/openzipkin/zipkin/issues/939\n. #55 is a start, benchmarking the sampling algo\n. related work:\nzipkin-reporter has a micro(ish)benchmark which tests the throughput of senders like Kafka (not directly to a zipkin server, but could be rejigged to) \nEx. https://github.com/openzipkin/zipkin-reporter-java/blob/master/benchmarks/src/main/java/zipkin/reporter/HttpSenderBenchmarks.java\nThe sender itself is used by many instrumentation projects (like brave, spring etc), but it was also made to facilitate benchmarking a real setup. The idea was to use something like jbender and hook it up to a reporter and a generator. It would need a plugin to monitor backup etc.\nThere was also an idea to use spigo to simulate loads. This stopped working at some point and also needs help to route through our go library as opposed to creating dump files.\nAnother idea was to use storage-specific benchmarks, such as Rally for elasticsearch #1354 or stress tool for cassandra. While not flow tests, these could help isolate storage performance concerns (like indexing)\nWe've also had a number of ad-hoc tests where people make python scripts to test kafka. Here's an example of such a script: https://github.com/openzipkin/zipkin/issues/1141#issuecomment-228514983\nAnother way you could test is shoving traffic through an existing system as a black box. For example, you could look at polyglot-zipkin and drive traffic through that (via grinder or similar) and watch side-effects on the storage system or collector metrics of zipkin\n. used a quick and dirty wrk benchmark to measure overhead reduction of tomcat -> undertow https://github.com/openzipkin/zipkin/pull/1806. >\n\nThis feature would be a nice to have.\nThanks for the summary. Would changing the default context root to\n\"/zipkin\" suffice? or would you need to control it.\n. K was mainly suggesting /zipkin as it seemed easier due to eirik's\ncomments, letting best not be enemy of good etc.\n. Currently seems easiest+most acceptable way to implement this is by way of\n/zipkin\n\nZipkin is mostly volunteers, waiting for one on this!\n. To save you effort it isn't so much the index.html as the bundled\njavascript referenced by it. There are other things, too. Probably the more\nstraightforward way out is to address the redirect issue\n\n. @gurinderu good luck! and post back and/or let us know on gitter what you find https://gitter.im/openzipkin/zipkin\n\nps notice you're in st petersburg. I'll be there in oct for https://devoops.ru/. From @eirslett on March 21, 2016 10:21\nCould it be supported as an external/optional module? I'm not sure it's a good idea to reuse the JDBC code, even if that's tempting.\n. From @Koizumi85 on March 21, 2016 10:28\nActually i think with postgresql 9.5 the actual code could possibly work if there were a possibility to switch the database type in Jooq..\nBackground: psql9.5 supports \"upsert\" semantics which are heavily used here.\nBut postgresql 9.5 is fairly new and is not hosted in the repositories of Ubuntu LTS and other linux distributions and there are a lot of sysadmins who don't want to take the postgresql repositories for LTS reasons.\n. From @eirslett on March 21, 2016 12:11\n9.1 or 9.3 are the safest choices, I think.\n. From @Koizumi85 on March 21, 2016 13:10\n9.1 and 9.3 should be almost identically unless the json column type of 9.3. but i don't think we would need this here, or?\nSo I think a 9.1 compatibility would be nice.\n. From @eirslett on March 21, 2016 13:17\nSounds good to me. The JSON query capabilites would be nice to have (for things like annotations), but it wasn't added in MySQL until 5.7.8, I think many linux distros are at MySQL 5.5 or so. And the existing SQL span store doesn't use JSON.\n. your are lucky number 3 person to be interested in this! https://blog.codinghorror.com/rule-of-three/\nif we started this, would you help maintain and/or test it? Which version of postgresql can you use?\n. your are lucky number 3 person to be interested in this! https://blog.codinghorror.com/rule-of-three/\nif we started this, would you help maintain and/or test it? Which version of postgresql can you use?\n. ooo this sounds suspiciously like an offer to help :) @dwelch2344 would you be up to designing this?\nps the version discussion is dated. ex 9.4 is the current supported version https://www.postgresql.org/download/linux/ubuntu/\n@Koizumi85 @eirslett @klette @mansu @jcarres-mdsol any thoughts on this one?\n. no one is actively working on it quite yet, hoping someone can pick it up. Regardless, it is probably best started as a separate repo I think.\n. best as a separate repo as we are pretty out of resources on this one. adding another DB might hurt our memory and bump build execution past 20m. that said, maybe circleci might make that better.. wdyt @abesto \n. First, we need to pick a minimum version Ex 9.5 supports upsert and if we use the existing schema from mysql, it would be the easiest to port. On that note, we should decide if we are just porting, or looking to design a high performance schema.\nIOTW is the motivation just using postgresql? or are we trying to make a high-performance alternative that uses postgresql?\n. > I think most companies don't really need Cassandra on the backend. I\n\nsuspect a high-performance alternative for medium-size data will something\nthat will appeal to this audience. What do you think?\nno insight into deployments of cassandra vs elasticsearch vs postgresql to\nbe honest\n\nOn Wed, Oct 5, 2016 at 10:25 PM, Hwasung Lee notifications@github.com\nwrote:\n\nI think most companies don't really need Cassandra on the backend. I\nsuspect a high-performance alternative for medium-size data will something\nthat will appeal to this audience. What do you think?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1232#issuecomment-251690230,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61y6FX0wM0026fk-Ge_huEIJ17lvHks5qw7NzgaJpZM4Jgz3S\n.\n. work here needs champions to run it through. If you like, you can look at\nthe work-on-progress voltdb experiment as an example, though in the schema\nlikely completing normalization of the data is preferred (unclear on this\nreally)\nhttps://github.com/adriancole/zipkin-voltdb\n\nAnyway, you can create a repo based on this to start toying with it. Right\nnow, no one has worked on postgres\n. >\n\nCame across this :\nhttps://github.com/IAMTJW/zipkin/tree/master/zipkin-storage/postgresql\nAny chance of this getting merged(accepted) ?\n\n\nfirst, the author would need to volunteer to contribute the code.\ntechnically though, this looks like a port of the mysql code, which isn't a\ngood schema to emulate. so, in its form I dont expect that specific code to\nbe merged.\n\n. This came out of a discussion with @postwait. Depending on how many folks are interested in this, it could end up in a separate repo or in the monorepo (this repo)\n. From @eirslett on May 26, 2016 21:58\n\nWhat isn't high-performant about Kafka?\n. @eirslett well when they last used zipkin, it only supported scribe. I don't recall a performance comparison with kafka, although less complexity was mentioned (ex no zookeeper dep)\n. From @postwait on May 26, 2016 22:10\nNot sure how a performance benchmark is relevant to the discussion? Fq can\neasily saturate multiple 10Gbe links,  but that isn't the point. The two\nsystems have different use cases and semantics. Fq is open,  tiny,  wildly\nefficient, has very few dependencies and fits the use case for trace\ndissemination quite well.\nAdditionally,  I don't think anyone was suggesting the removal of Kafka -\nthat would be nuts.\n. Summary so far:\nWith some relative ease one could make a proof of concept which employs FQ as a custom collector. \nIt doesn't look viable to do more than that until fq has binaries and a jar published to artifact repositories. Our build and dev story is far easier to manage when we don't need to compile c as a prerequisite to running tests.\ndetails..\n\nSteps to create fq are like below main things is that it creates a fqd and friends, as well a jar file for the client connector. It wasn't hard to get it started though I haven't tested it, yet\nI think the challenge we'd have with this is we usually get dependencies from packages or otherwise.\nTesting would be easier to integrate if there were a current fq homebrew and apt (for our CI server) package. Otherwise we'd end up with what could be brittle pre-install routines that build the code.\nThere's also the java side, where we typically consume published jars that include some version of whatever the connector is. The fq code has no dependencies, so it could be feasible to build locally (ie source import), but my concern would be about support issues around that. Ex we don't have experience or a means to run unit tests against it.\n``` sh\nbuild and install ck\n$ git clone https://github.com/concurrencykit/ck.git; cd ck/\n$ ./configure && make && make install\n$ cd ..\nbuild and install jlog\n$ git clone https://github.com/omniti-labs/jlog.git; cd jlog/\n$ autoconf; ./configure; make; make install\n$ cd ..\nbuild and install fq\n$ git clone https://github.com/circonus-labs/fq.git; cd fq/\n$ make; make install\n```\n. From @postwait on May 27, 2016 1:5\nWe'll look at publishing the fq java client jars to the maven central repo.\nOn Thu, May 26, 2016 at 6:58 PM, Adrian Cole notifications@github.com\nwrote:\n\nSummary so far:\nWith some relative ease one could make a proof of concept which employs FQ\nas a custom collector.\nIt doesn't look viable to do more than that until fq has binaries and a\njar published to artifact repositories. Our build and dev story is far\neasier to manage when we don't need to compile c as a prerequisite to\nrunning tests.\ndetails..\nSteps to create fq are like below main things is that it creates a fqd and\nfriends, as well a jar file for the client connector. It wasn't hard to get\nit started though I haven't tested it, yet\nI think the challenge we'd have with this is we usually get dependencies\nfrom packages or otherwise.\nTesting would be easier to integrate if there were a current fq homebrew\nand apt (for our CI server) package. Otherwise we'd end up with what could\nbe brittle pre-install routines that build the code.\nThere's also the java side, where we typically consume published jars that\ninclude some version of whatever the connector is. The fq code has no\ndependencies, so it could be feasible to build locally (ie source import),\nbut my concern would be about support issues around that. Ex we don't have\nexperience or a means to run unit tests against it.\nbuild and install ck\n$ git clone https://github.com/concurrencykit/ck.git; cd ck/\n$ ./configure && make && make install\n$ cd ..\nbuild and install jlog\n$ git clone https://github.com/omniti-labs/jlog.git; cd jlog/\n$ autoconf; ./configure; make; make install\n$ cd ..\nbuild and install fq\n$ git clone https://github.com/circonus-labs/fq.git; cd fq/\n$ make; make install\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin-java/issues/252#issuecomment-222034932,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AAUfhCGmAJrgqqET8kgdTLictJOGAbZAks5qFkGqgaJpZM4In-Hj\n.\n. Sounds good. So, I took a few stabs while in the air and probably didn't\nconcentrate hard enough.\n\nA sample using the java api in a producer consumer context would be nice,\neven better if it models how libmtev sends spans. Super kudos for a\nhealth-check snippet. This sort of sample is something I could help put a\ncollector and test around.\nWithout a sample, it is harder (at least for me) to understand which\nmethods in the client interface I should be handling or paying attention to.\n. From @eirslett on May 27, 2016 8:39\nI wasn't saying it's either Kafka or fq; I was just pointing out we already have ~~Scribe~~Fluentd and Kafka transports, both are widely known to be high-performant and fit the bill well, so I don't agree with @adriancole's claim. I think the primary reason one would use fq instead of Fluentd or Kafka, would be that you already have fq running on the servers, and don't have any of the other transports present?\nI'm all for adding a transport for fq if people are willing to contribute one (adriancole sounds really motivated) but we should handle it the same way we handle zipkin-hbase, which is similar, community/ecosystem-wise.\n. Sheesh sorry about mis-introducing this.\nThere are many who have had problems with scribe due to its synchronous\nprotocol and often mention the double encoding in base64 as a performance\nconcern. At craftconf last year, these topics were brought up comparing\nscribe to fq. I didn't expect that to be contentious, but seems to have\nhijacked this conversation. Apologies  for the distraction.\n. From @postwait on May 27, 2016 23:24\nBacks away slowly and exits the room.\nOn May 27, 2016 7:03 PM, \"Adrian Cole\" notifications@github.com wrote:\n\nSheesh sorry about mis-introducing this.\nThere are many who have had problems with scribe due to its synchronous\nprotocol and often mention the double encoding in base64 as a performance\nconcern. At craftconf last year, these topics were brought up comparing\nscribe to fq. I didn't expect that to be contentious, but seems to have\nhijacked this conversation. Apologies for the distraction.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin-java/issues/252#issuecomment-222271344,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AAUfhGBnyzRF_8nCecBo7CzRzkEFacq1ks5qF3hAgaJpZM4In-Hj\n.\n. Ps to be transparent about this, I looked into this because I met circonus and we chatted :) It was interesting enough for me to want to take a look to see if I could help them move off a fork.\n\nSpikes are important. I get swamped with other things and I don't usually have the energy to try lots of things simultaneously. I opened this issue to log thoughts so the energy I spent might become helpful to circonus.\nFrankly, I am out of energy and already focusing on other things. That said, if we get a code snippet, I am interested enough to help sketch a proof of concept that Circonus might be able to use.\nSo motivation on my end is that I am motivated to finish the exploration, but I wouldn't conflate that with a priority of a somewhat niche transport being higher than our more requested backlog :)\n. Sorry about this, Theo. I'll close this issue as it has become a mess.\nI'm still keen to help, but I think it would be less distracted to explore this out of the mono repo, as it would be less distracting.\n. From @postwait on June 7, 2016 14:28\n@adriancole fqclient artifact is now published via maven http://search.maven.org/#artifactdetails%7Ccom.circonus%7Cfqclient%7C0.0.1%7Cjar\nA default program for zipkin thrift-encoded messages would be: \"prefix:\\\"zipkin.thrift.\\\"\"\nThe program we use to bridge fq onto scribe is here:\nhttps://github.com/circonus-labs/fq2scribe\nSo a fully functional fq subscription to zipkin thrift data lies within:\nhttps://github.com/circonus-labs/fq2scribe/blob/master/src/main/java/com/circonus/fq2scribe/Fq2Scribe.java\n. From @eirslett on June 7, 2016 16:35\nGreat work! I guess fq2scribe could be ported to c or go, for an additional performance boost?\n. From @postwait on June 7, 2016 16:40\nIt performs okay today, but it quite Rube Goldberg. If performance is\nimportant,the next logical step would be to have openzipkin act directly as\nan fq client...\nOn Tue, Jun 7, 2016 at 12:35 PM, Eirik Sletteberg notifications@github.com\nwrote:\n\nGreat work! I guess fq2scribe could be ported to c or go, for an\nadditional performance boost?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin-java/issues/252#issuecomment-224338296,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AAUfhEXF4jlcfVtMEhShBITUYB7-Wqcgks5qJZ27gaJpZM4In-Hj\n.\n. From @eirslett on June 7, 2016 17:8\n\nLatency is not a big deal (1 second is fine end-to-end), but high throughput is critical.\n. From @postwait on June 7, 2016 17:10\nThe avoiding scribe itself is critical as it is RPC. Integrating an\nfqclient directly makes even more sense.\nOn Jun 7, 2016 1:08 PM, \"Eirik Sletteberg\" notifications@github.com wrote:\n\nLatency is not a big deal (1 second is fine end-to-end), but high\nthroughput is critical.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin-java/issues/252#issuecomment-224347882,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AAUfhMSgUb4Y13A7wi3XAdTDjYMFA9QMks5qJaV2gaJpZM4In-Hj\n.\n. re-opening as the topic has been gratefully resurrected\n. Thanks, theo!\n. Is @CrossOrigin(*) the way to do this, or is there a parameterized way to do this in config that would allow it to be tightened? cc @shakuzen @dsyer \n. any news here? will probably cut a release in the next day or so\n. rebase against latest master and you should be ok on this test failure\n\nOn Tue, Aug 16, 2016 at 9:43 PM, Ho Yan Leung notifications@github.com\nwrote:\n\n@adriancole https://github.com/adriancole I updated the PR based on\n@shakuzen https://github.com/shakuzen's feedback - but still can't get\nthe PR build to pass.\nTests in error:\n  multithreaded(zipkin.storage.cassandra.DeduplicatingExecutorTest): test timed out after 2000 milliseconds\nAs far as I can tell, the build is ok locally - maybe there's something\nelse I should check? (I'm just running ./mvnw clean install)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1234#issuecomment-240105428,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6199FMeya8wDTSqJL_2jCelZ_LEmfks5qgb56gaJpZM4JhLqg\n.\n. Travis has been more flakey lately on heavy tests. I will kick it\n\nOn 17 Aug 2016 3:42 am, \"Ho Yan Leung\" notifications@github.com wrote:\n\n@adriancole https://github.com/adriancole hmm...I rebased, pretty sure\nat the moment I'm up to date vs. upstream getting:\nTests in error:\n  zipkin.collector.kafka.KafkaCollectorTest: test timed out after 10 seconds\nIn any case, I've made the changes discussed above and adding a little\nblurb to the README\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1234#issuecomment-240214751,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61-_8FH0PQEF4nITklM39PH7bl9U4ks5qghKIgaJpZM4JhLqg\n.\n. @hyleung ps src/it I suppose is optional as long as you've a test in ZipkinServerIntegrationTests\n. very good. pretty much ready to merge! If you aren't in a position to easy address the last nits, I can do them on the way in. say the word!\n. nice work!\n. @shakuzen thanks for the review!\n. Hi, Janusz.\n\nnot sure if either of the frameworks you mentioned have been instrumented,\nyet, but they'd be done in https://github.com/openzipkin/zipkin-js for the\nfrontend and https://github.com/openzipkin/brave for the backend.\nThere's a brave example here if it helps..\nhttps://github.com/openzipkin/brave-resteasy-example\nfeel free to join gitter, too https://gitter.im/openzipkin/zipkin\n. The example wasn't how to instrument Java Spark, it was how to run a java setup in general. Glad you were able to do that.\nFor example, to instrument the Spark framework, something like below would be needed:\nhttps://github.com/openzipkin/brave/blob/fd3cdb921964a0d7164c29141fe6aac8099ae4cf/brave-spring-web-servlet-interceptor/src/main/java/com/github/kristofa/brave/spring/ServletHandlerInterceptor.java\nmaybe using this? https://github.com/perwendel/spark/blob/master/src/main/java/spark/Filter.java\nHere are details on brave's hooks:  https://github.com/openzipkin/brave/tree/master/brave-http\nPopular frameworks end up in Brave, the first step in making something popular is opening an issue. I'd open one for the sparkjava\nhttps://github.com/openzipkin/brave/issues\nIf you can't use gitter, then probably best to move this to a zipkin-user email since this is an open-ended conversation (as opposed to something we can fix in this repo). If you do, I'll try to help you there. https://groups.google.com/forum/#!forum/zipkin-user\n. Motivation for this is to lower barrier for tracers to include zipkin's standard library without resorting to extracting a separate \"model jar\"\n. I rewrote a cassandra test that was timing out to use mocks and the build went green. I'm assuming that we're bumping limits of our slave, and that becomes more obvious when travis is degraded.\n. Had to rewrite another test, mainly as something in the travis image ended up consistently changing a failure case behavior. Note that this isn't something we'd necessarily avoid in circleci.\n. great news.. taking a look at that test now.. maybe it is time sensitive.\n. pushed a commit with my suspicion about the test. If this doesn't\nwork, there's a lag involved in the runtime that might require some\nreal surgery.\nOn Fri, Aug 19, 2016 at 3:28 PM, Adrian Cole adrian.f.cole@gmail.com wrote:\n\ngreat news.. taking a look at that test now.. maybe it is time sensitive.\n. pushed a commit with my suspicion about the test. If this doesn't\nwork, there's a lag involved in the runtime that might require some\nreal surgery.\n\nOn Fri, Aug 19, 2016 at 3:28 PM, Adrian Cole adrian.f.cole@gmail.com wrote:\n\ngreat news.. taking a look at that test now.. maybe it is time sensitive.\n. ok that's done.. now we just need a mysql tweak...\n\nOn Fri, Aug 19, 2016 at 3:46 PM, Adrian Cole adrian.f.cole@gmail.com wrote:\n\npushed a commit with my suspicion about the test. If this doesn't\nwork, there's a lag involved in the runtime that might require some\nreal surgery.\nOn Fri, Aug 19, 2016 at 3:28 PM, Adrian Cole adrian.f.cole@gmail.com wrote:\n\ngreat news.. taking a look at that test now.. maybe it is time sensitive.\n. ok that's done.. now we just need a mysql tweak...\n\n\nOn Fri, Aug 19, 2016 at 3:46 PM, Adrian Cole adrian.f.cole@gmail.com wrote:\n\npushed a commit with my suspicion about the test. If this doesn't\nwork, there's a lag involved in the runtime that might require some\nreal surgery.\nOn Fri, Aug 19, 2016 at 3:28 PM, Adrian Cole adrian.f.cole@gmail.com wrote:\n\ngreat news.. taking a look at that test now.. maybe it is time sensitive.\n. fingers crossed\n. fingers crossed\n. Last mile: timeout on KafkaCollectorTest\n. Last mile: timeout on KafkaCollectorTest\n. TODO: needs README and integration test (on the actual index)\n\n\nalso needs manual testing\n. Manually tested with Kibana. timestamp_millis automagically showed up, and with no effort graphs worked.\n\n. added tests and docs\n. note: this is easy to reproduce if you use JMH or similar to pummel the elasticsearch server :)\n. seems related https://github.com/elastic/elasticsearch/issues/12366\n. build failure unrelated.. rebase would fix it\n. maybe update RELEASE.md? Otherwise LGTM. Thanks for the help!\n. Also, probably putting a javadoc link reference to http://zipkin.io/zipkin/ at the bottom of the README with the other things.\n. Thanks tons!\n. hit a snag on a release tag:\nhttps://travis-ci.org/openzipkin/zipkin/builds/153257593\n+javadoc_to_gh_pages\n++print_project_version\n++./mvnw help:evaluate -N -Dexpression=project.version\n++grep -v '\\['\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=192m; support was removed in 8.0\n+version=1.7.1-SNAPSHOT\n+rm -rf javadoc-builddir\n+builddir=javadoc-builddir/1.7.1-SNAPSHOT\n++find . -name '*1.7.1-SNAPSHOT-javadoc.jar'\n+git checkout gh-pages\nerror: pathspec 'gh-pages' did not match any file(s) known to git.\nThe command \"./travis/publish.sh\" exited with 1.\nstore build cache\n. ps thanks for making the step last! wise\n. little red here, too.. https://travis-ci.org/openzipkin/zipkin/builds/153262205\n. local-repo is for integration testing the jar you just made. so, you\ncan safely ignore that path.\n. local-repo is for integration testing the jar you just made. so, you\ncan safely ignore that path.\n. :facepalm: ok will fix\nOn Tue, Aug 16, 2016 at 11:53 PM, bedag-moo notifications@github.com\nwrote:\n\nWhen compiling Zipkin in eclipse, compilation fails with\nzipkin/src/main/java/zipkin/collector/InMemoryCollectorMetrics.java:14\nThe declared package \"zipkin\" does not match the expected package\n\"zipkin.collector\"\n(eclipse enforces that a source file resides in the directory that\ncorresponds to its package)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1241, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD614V1wZJAbGb5csigqeG8KN-WQgQVks5qgd0OgaJpZM4JlkK7\n.\n. related issue: multiple parents aka linked traces https://github.com/openzipkin/zipkin/issues/1244\n. had a chat with @mbogoevici from spring-cloud-stream.\n\nWe talked about a special case, where you might want to visualize the trace as being \"open\" the same way that a root span normally works in RPC. IE the critical path of the pipeline continues until a terminal event occurs. This allows you to see the entire duration clearly.\nThis implies the root span isn't closed immediately when it sent something to a broker, rather it is left open until a terminal event occurs. Something down the line will close it.\n1. Report a start annotation in the root span like \"pipeline.begin\" or similar\n2. When going across a broker, propagate the root span id as baggage and report \"ms\" in the current span.\n3. When receiving from a broker, extract the current span and report \"mr\"; extract the root span id from baggage and propagate in process\n4. repeat 2, 3 as necessary\n5. When you've hit a terminal point for the pipeline, report \"pipeline.end\" or similar using the root span id you propagated\ncc @dsyer @marcingrzejszczak\n. >\n\n@adriancole https://github.com/adriancole when publish zipkin 2 release\nwe usually discuss when features will land vs certain version numbers. What\nfeatures are you interested in?\n. >\nWould be nice if both push (notification/topic) and pull (queue)\ncommunication was covered.\nOne tricky thing, which would also be nice to be covered as well, is\nbatching support in both produce (publish), and consume side (push, pull).\nE.g. one can have multiple publish requests which end up consumed in one\npull request. Also, single publish request could publish multiple messages,\nwhich get delivered with multiple independent push requests.\ngood point. we've actually another issue on that topic, as it is trickier\nthan one-way single producer single consumer: linked traces\nhttps://github.com/openzipkin/zipkin/issues/1244\n. >\n@adriancole https://github.com/adriancole The path of least resistance\nfor async (half-duplex, or messaging), is to follow this model. This\nprevents us from needing to wait for a big-bang zipkin 2 release of the\nentire ecosystem.\ngotcha.. no big bangs here (though there are some features that take a\nwhile!) :)\n\nthe effort to add messaging span is very little. It is defining constants\nand adjusting some things in the UI and clock skew checker. Main reason I\ndelayed this one was rule-of-three. I think we have enough interested at\nthis point to move forward with implementing \"ms\" \"mr\"\n. I'll update this with a pull request in the next couple days latest by\nmonday. In the mean time, you can always use the \"ms\" and \"mr\" annotations\nyourself, they just won't be interpreted by the UI until we upgrade.\n. @lijunyong ps after thinking about it.. I am not going to raise this PR  yet. @mbogoevici's work is new information. We should see if that is extending this or an alternative. Plus I'd like to get a tally of at least a couple folks who aren't primarily interested in JVM environments. Finally, we need to vet this against single-host span pattern.\nIf there's anyone on this thread tracing non java environments with opinions on one-way tracing please speak up (cc @jcarres-mdsol @ys @mjbryant @hammett)!\nBetween now and when pull requests to various repos are reviewed and merged, feel free to just use the conventions described for one-way messages. Ex. send \"ms\" on the way out and \"mr\" when receiving (just like client+server, except no response is carried back). There's nothing in zipkin that prevents experimenting with this.. @lijunyong @jcarres-mdsol here's a repaste of a comment on brave which I hope will clarify both of your questions. Please let me know if it doesn't.\nHere's an example scenario asked by @coolbetm\n\nConsider the following simplish example:\nSuppose service A receives a post to its REST\nendpoint and produces the body of the post to a kafka\ntopic. Service B is tailing on that topic and consumes\nit, parses it, and produces a derived message to second topic.\nService C is tailing on the second topic, receives the derived\nmessage, and responds by executing a database update.\nWhat do you imagine the zipkin trace should look like (what spans are there\nand in what order do they start/end)?\n\nService A receives a POST and sends a message to kafka\n{traceid=1, parent=_, spanid=1 [sr=serviceA, ss=serviceA]} // server span\n{traceid=1, parent=1, spanid=2 [\"make message\", lc=serviceA]} // local span\n{traceid=1, parent=2, spanid=3 [ms=serviceA]} // messaging span sent\nService B consumes the message and puts a message on a second topic\n{traceid=1, parent=2, spanid=3 [mr=serviceB]} // messaging span received  (same span id)\n{traceid=1, parent=3, spanid=4 [\"process and make another message\", lc=serviceB]} // local span\n{traceid=1, parent=4, spanid=5 [ms=serviceB]} // messaging span sent\nService C consumes the message and saves to a database\n{traceid=1, parent=4, spanid=5 [mr=serviceC]} // messaging span received (same span id)\n{traceid=1, parent=5, spanid=6 [\"process message\", lc=serviceC]} // local span\n{traceid=1, parent=6, spanid=7 [cs=serviceC, cr=serviceC, sa=database]} // client span\n. >\n\nWhy the last CS/CR to itself?\nThis is a call to a database. I'm assuming the database doesn't speak\nzipkin, so we just log \"sa\" in the client span to indicate the remote side.\n. >\nIf serviceC finish processing the message but does not generate any other\nzipkin information after mr (for instance we never trace calls to DBs)\nhow will we know the trace has in fact ended?\nSince \"mr\" closes the messaging span, all spans are closed.. so it is the\nsame situation as today. A trace is effectively ended when all spans finish.\n. >\nI thought mr was emitted when the worker got the message from the queue.\nPerformance wise I would want to know two things:\n\nHow long the message was in the messaging bus\n\nAfter \"ms\" we can use the normal \"ws\" to indicate when it went to the bus.\nSimilarly, we can use \"wr\" to tell the difference between when we got the\nmessage vs started to process it.\n\nHow long it took the worker to process the message\n\nThe messaging span in my mind really should be transit only (don't record\nmessage processing in the span). That's why in my example, I have listed a\nchild span to process the message. Timewise, this directly follows the\nmessaging span, so its (start) timestamp will very close or exactly the\nsame as \"mr\" of its parent (the messaging span)\nTo do both we need to tell at least one more annotation, no?\nyeah I think probably I didn't highlight \"ws\" \"wr\" (which incidentally are\ndefined also for client and server spans, just not widely used)\n. >\nAh, ok, then ServiceC would also use lc to specify how long its\nprocessing took\nheh.. whoops. guess I left that out of the span list. fixing!\n. ok updated https://github.com/openzipkin/zipkin/issues/1243#issuecomment-266359841 .. I understand why it was unclear now! thanks @jcarres-mdsol . >\n@adriancole https://github.com/adriancole should I add span(traceid,\nparent, spanid) in message header,then send the json data to zipkin by\nspanCollector(HttpSpanCollector)?\nYes, you can use the same propagation and reporting approach as\nclient/server, just where you encode the trace identifiers will be\nmessaging specific. Some are using message headers exactly as they would if\nthey were http (spring-cloud-sleuth does this, for example).\n. >\nThe other thing I can think of is identifying the message system as a\nseparate endpoint - i.e. using the broker as the endpoint (e.g. 'kafka' or\nsomething else - for Stream we can adopt the logical binder name, in other\ncases it could be the broker name) and the logical destination as the span\nname, e.g.\n{traceid=1, parent=_, spanid=1 [sr=serviceA, ss=serviceA]} // server span\n{traceid=1, parent=1, spanid=2 [\"make message\", lc=serviceA]} // local span\n{traceid=1, parent=2, spanid=3 [ms=kafka]} // messaging span sent\nService B consumes the message and puts a message on a second topic\n{traceid=1, parent=2, spanid=3 [mr=kafka]} // messaging span received\n(same span id)\n{traceid=1, parent=3, spanid=4 [\"process and make another message\",\nlc=serviceB]} // local span\n{traceid=1, parent=4, spanid=5 [ms=kafka]} // messaging span sent\nService C consumes the message and saves to a database\n{traceid=1, parent=4, spanid=5 [mr=kafka]} // messaging span received\n(same span id)\n{traceid=1, parent=5, spanid=6 [\"process message\", lc=serviceC]} // local\nspan\n{traceid=1, parent=6, spanid=7 [cs=serviceC, cr=serviceC, sa=database]} //\nclient span\nThen for instance you could search for spans involving 'kafka'. WDYT?\nAgreed on the concern, but we still need to set mr's endpoint to the local\nservice receiving from kafka.\n\nOne way is to add \"ma\" which is similar to client address and server\naddress, except it represents the broker (using m for congruence with the\ntimestamp annotations).\nIn zipkin v2 it would simply be the remote address (no extra type info\nneeded)\n. >\n\nI suppose 'ms' then gets set to the sender, then ?\nyep. all timestamp annotations are attributed to the host that caused the\nactivity\nGreat - I think it would be nice to emphasize that those spans are\ntraversing the broker (e.g. Kafka) as opposed to belonging to the sending\nand receiving service.\nagreed!\n. first step: adding constants to the thrift https://github.com/openzipkin/zipkin-api/pull/24. During https://github.com/openzipkin/zipkin-api/pull/24 we noticed this can be complicated when the messaging system is also RPC (such as pubsub or Amazon SQS which are both http services). For example, how would they know if they should add a \"ms\" or \"cs\" considering the production of a message is literally an rpc (usually http) call? I noted that we still have the option to not do this change, or to instead repurpose \"cs\" \"sr\" to address one-way concerns. For example, thrift and other RPC systems have a one-way specialization.\n\nIt is important to think about things like this, but also understand tactical vs strategic change. For example, any annotations like this are tactical in my view. I'll try to recap some of this (which came from a gitter chat with @jcarres-mdsol)\n\nI think that linked traces are a better fit for messaging than these annotations. The issue with linked traces is we need to change schema (which isn't a problem, just something to get through)\nThe \"ms\" \"mr\" was a way to identify a means to handle one-way for those in the brown field, because it requires very little change (similar to if we decided to treat \"cs\" \"sr\" missing response, as a special one-way). So, presence of some heuristic for one-way doesn't preclude linked traces, or even handle it better in any way. It is merely a stepping stone.\nHere are the steps I'm thinking of that lead us forward with regards to messaging:\n implement some way to address one-way for current model\n implement single-host span model (aka zipkin model 2 which I think isn't a big bang)\nBoth of the above are independent, but once the latter is in, we can add linked traces to it much easier than trying to add to the multi-host span model we have today.\nBack to the effort here, as far as I can tell, we either proceed as noted or try to repurpose the \"cs\" and \"sr\" similarly. Another option is to not do this and head directly towards links.\nI'll give another couple days to make sure people have seen this and can weigh in if any of this context changes their mind.. I think we should make a decision as perpetually incomplete work is haunting, particularly as we've discussed this topic for a very long time now, and there are people interested in it. That, or we should make visible what must happen before a decision can be made. We don't want a worst of show, where we've spent lots of time discussing, and gathering interest, but lose them in analysis paralysis.\nA valid question is what to do when an RPC system is used to carry what is semantically a one-way message. I'd flatten this to the generic topic of one-way RPC, which exists in many systems (regardless if you pay attention to acks or not).\nThis is separate from the concern that led to this issue, which is that a non-RPC system is used to carry single-producer single-consumer messages. The latter concern is addressed by ad-hoc users today, using annotations which may or may not be named \"ms\" and \"mr\" :)\nIf we feel those in the latter camp should continue to work ad-hoc, that's a valid thing, but I think people should be more explicit about the desire to halt this, and what, if anything it would be replaced with. We'd then close the issue as won't fix and stop spending time on it.\nIf we feel there's something that must be addressed, let's make test cases for that. They can be simple descriptions. Regardless, we need to find a way to progress this topic.. Here's a diagram of the latest revision of this, based on a pre new-year chat with @jcarres-mdsol \nBy not defining a new address annotation for the message broker, a lot of code will be similar to today. Should we decide to use new annotations, they would be exactly like existing, except clarifying one-way semantics.\nthat reduces the explanation to...\n\"ms\" is like \"cs\" except there's no response anticipated\n\"mr\" is like \"sr\" except we aren't sending a response\nProducer Tracer                                    Consumer Tracer     \n+------------------+                               +------------------+\n| +--------------+ |     +-----------------+       | +--------------+ |\n| | TraceContext |======>| Message Headers |========>| TraceContext | |\n| +--------------+ |     +-----------------+       | +--------------+ |\n+--------||--------+                               +--------||--------+\n   start ||                                                 ||\n         \\/                                          finish ||\nspan(context).annotate(\"ms\")                                \\/\n             .address(\"sa\", broker)          span(context).annotate(\"mr\")\n                                                          .address(\"ca\", broker). Here's a diagram of the latest revision of this, based on a pre new-year chat with @jcarres-mdsol \nBy not defining a new address annotation for the message broker, a lot of code will be similar to today. Should we decide to use new annotations, they would be exactly like existing, except clarifying one-way semantics.\nthat reduces the explanation to...\n\"ms\" is like \"cs\" except there's no response anticipated\n\"mr\" is like \"sr\" except we aren't sending a response\nProducer Tracer                                    Consumer Tracer     \n+------------------+                               +------------------+\n| +--------------+ |     +-----------------+       | +--------------+ |\n| | TraceContext |======>| Message Headers |========>| TraceContext | |\n| +--------------+ |     +-----------------+       | +--------------+ |\n+--------||--------+                               +--------||--------+\n   start ||                                                 ||\n         \\/                                          finish ||\nspan(context).annotate(\"ms\")                                \\/\n             .address(\"sa\", broker)          span(context).annotate(\"mr\")\n                                                          .address(\"ca\", broker). NEXT STEP: Experiment with this design both in server and instrumentation projects\nOne danger area I think there is will be the code involved to handle \"ms\", \"mr\" neatly vs just allowing \"cs\" and \"sr\". The code impact of this will be present in instrumentation as well as the server (and its UI). For example, instrumentation will amplify this work, as a dozen different tracers may need new if statements etc.\nWe need to compare implementing one-way/unary/messaging as..\n\n\"cs\" on sender (explicit flush), \"sr\" on receiver (explicit flush)\nvs  \n\"ms\" on sender (implicit flush), \"mr\" on receiver (implicit flush)\n\nIn this repo, we can open a branch/pull request would include test cases that include one-way spans, modeling pubsub with pubsub RPC on either side, checking the dependency linker and the clock skew adjuster. This pull request should note experiences that led to either option being preferable.\nIn one or more instrumentation repos, we'd check the two modeling approaches (cs/sr with flush vs ms/mr implicit flush), and in their corresponding pull request note experiences.\nOnce we have the bigger picture, I think we can lock-in and roll out what works best. I'm glad the choices are quite narrow at this point.\n@openzipkin/instrumentation-owners anyone have time to help experiment on this?. NEXT STEP: Experiment with this design both in server and instrumentation projects\nOne danger area I think there is will be the code involved to handle \"ms\", \"mr\" neatly vs just allowing \"cs\" and \"sr\". The code impact of this will be present in instrumentation as well as the server (and its UI). For example, instrumentation will amplify this work, as a dozen different tracers may need new if statements etc.\nWe need to compare implementing one-way/unary/messaging as..\n\n\"cs\" on sender (explicit flush), \"sr\" on receiver (explicit flush)\nvs  \n\"ms\" on sender (implicit flush), \"mr\" on receiver (implicit flush)\n\nIn this repo, we can open a branch/pull request would include test cases that include one-way spans, modeling pubsub with pubsub RPC on either side, checking the dependency linker and the clock skew adjuster. This pull request should note experiences that led to either option being preferable.\nIn one or more instrumentation repos, we'd check the two modeling approaches (cs/sr with flush vs ms/mr implicit flush), and in their corresponding pull request note experiences.\nOnce we have the bigger picture, I think we can lock-in and roll out what works best. I'm glad the choices are quite narrow at this point.\n@openzipkin/instrumentation-owners anyone have time to help experiment on this?. In brave 4, I think it is easier overall to use \"cs\" and \"sr\". I made an example with Kafka under this assumption https://github.com/openzipkin/brave/pull/300/files#diff-0f684bf482d2b5acd65765f8d6f8f24eR35. In brave 4, I think it is easier overall to use \"cs\" and \"sr\". I made an example with Kafka under this assumption https://github.com/openzipkin/brave/pull/300/files#diff-0f684bf482d2b5acd65765f8d6f8f24eR35. moved the kafka example to a gist as I think it is too heavyweight to be in the normal test tree https://gist.github.com/adriancole/76d94054b77e3be338bd75424ca8ba30. moved the kafka example to a gist as I think it is too heavyweight to be in the normal test tree https://gist.github.com/adriancole/76d94054b77e3be338bd75424ca8ba30. I think it is much better to formalize one-way/async as client+server without a response. It is feasible and it is more correct to say one-way/async than say \"messaging\" knowing we only handle single-producer single-consumer anyway. The linked spans work could pick up from there.\nI've added a demo in Brave: https://github.com/openzipkin/brave/tree/master/brave/src/test/java/brave/features/async\nAnd I've also verified the dependency graph pretty-much works as-is:\nhttps://github.com/openzipkin/zipkin/pull/1478\nI know the clock skew adjuster will change, but it will be less work than adding new annotations, as it already special cases skew found when \"sr\" happens before \"cs\".\nI thought that it might break single-host spans in the dependency graph until I realized that single-host spans are already broken! In other words, regardless of one or two-way, the dependency graph would need code anyway.\nSo summary is that we should forget about \"ms\"->\"mr\" and go with \"cs\"->\"sr\", which reduces work on all parties to support this feature. If anyone feels against this, please make it known!. I think it is much better to formalize one-way/async as client+server without a response. It is feasible and it is more correct to say one-way/async than say \"messaging\" knowing we only handle single-producer single-consumer anyway. The linked spans work could pick up from there.\nI've added a demo in Brave: https://github.com/openzipkin/brave/tree/master/brave/src/test/java/brave/features/async\nAnd I've also verified the dependency graph pretty-much works as-is:\nhttps://github.com/openzipkin/zipkin/pull/1478\nI know the clock skew adjuster will change, but it will be less work than adding new annotations, as it already special cases skew found when \"sr\" happens before \"cs\".\nI thought that it might break single-host spans in the dependency graph until I realized that single-host spans are already broken! In other words, regardless of one or two-way, the dependency graph would need code anyway.\nSo summary is that we should forget about \"ms\"->\"mr\" and go with \"cs\"->\"sr\", which reduces work on all parties to support this feature. If anyone feels against this, please make it known!. ps I chatted with @fedj and don't think we can help correct clock skew on async-spans. The reason is that the current approach uses the two views of duration in the same span to interpret skew. Since async would have only a single view of latency, it won't work. This means either people need to keep their clocks in sync, or they need to resolve skew as a part of the reporting pipeline somehow.\nThe problem will be the same regardless of which annotations are used except in the case where messaging is implemented under the covers as normal RPC. In that case, all 4 annotations will be present and then skew can correct automatically.. Another topic of interest when we get to documenting. If someone is trying to model an \"ack\", they should be careful not to use \"cr\" since it isn't semantically the same (as cr implies a round trip of an operation). When modeling ack, it would be better to use \"ws\" wire send or just make up another annotation (ex. kafka.ack).. added #1497 to make one-way spans via cs -> sr work. added #1497 to make one-way spans via cs -> sr work. docs https://github.com/openzipkin/openzipkin.github.io/pull/76\n. fyi, those who need to trace kafka via one-way approach can likely do so using 0.11 Brave implementation may follow here https://github.com/openzipkin/brave/issues/211. In a recent tracing workshop, we were able to elaborate a bit on async tracing. In many cases, new traces are not needed. Ex if propagation does not imply sharing spans, you can do single producer multiple consumer as normal spans in the same trace. Here's a diagram describing most concerns\n```\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   RPC with Async Callbacks   \u2502             \u2502                 Messaging                 \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524             \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502                              \u2502             \u2502                                           \u2502\n    \u2502 request-response             \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 Single Producer Single Consumer           \u2502\n    \u2502                              \u2502      \u2502      \u2502                                           \u2502\n    \u2502   deferring completion       \u2502      \u2502      \u2502 No guarantee of only one consumer from    \u2502\n    \u2502    onto a different thread   \u2502      \u2502      \u2502 the POV of the producer. No guarantee     \u2502\n    \u2502                              \u2502      \u2502      \u2502 of the scale of latency between producer  \u2502\n    \u2502                              \u2502      \u2502      \u2502 and consumer. Can consider collapsing     \u2502\n    \u2502                              \u2502      \u2502      \u2502 part of the timeline.                     \u2502\n    \u2502 one-way                      \u2502      \u2502      \u2502                                           \u2502\n    \u2502                              \u2502      \u2502      \u2502                                           \u2502\n    \u2502   breaks assumptions of      \u2502      \u2502      \u2502 Single Producer Multiple Consumer         \u2502\n    \u2502   caller knowing when the    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u25b6      \u2502                                           \u2502\n    \u2502   request finished           \u2502             \u2502 Breaks shared span model, ex each consumer\u2502\n    \u2502                              \u2502             \u2502 may accidentally use the same span ID     \u2502\n    \u2502   depending on POV knowing   \u2502             \u2502 Harder to visualize if there are varied   \u2502\n    \u2502   a request finished may     \u2502             \u2502 time scales per consumer. That said, each \u2502\n    \u2502   be less important than if  \u2502             \u2502 consumer can still work as a child span   \u2502\n    \u2502   a caller was blocked       \u2502             \u2502                                           \u2502\n    \u2502                              \u2502             \u2502                                           \u2502\n    \u2502                              \u2502             \u2502                                           \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            Batching             \u2502           \u2502        Distributed Flow Processing        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524           \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                 \u2502           \u2502                                           \u2502\n\u2502 Examples are coallesced writes  \u2502           \u2502 Inter-process Streams (like Spark)        \u2502\n\u2502 or flushing. A batch can be a   \u2502           \u2502                                           \u2502\n\u2502 separate trace with links added \u2502           \u2502 The user can define a join/zip/etc which  \u2502\n\u2502 for each enclosed element. The  \u2502           \u2502 may combine work started in multiple      \u2502\n\u2502 root is the batch. Each inserted\u2502           \u2502 traces. Phases tend to be coarse grained  \u2502\n\u2502 span is a direct child.         \u2502           \u2502 which maps to fewer spans. Challenge is   \u2502\n\u2502                                 \u2502           \u2502 if we can detect or assist in how to      \u2502\n\u2502 Representing the batch explains \u2502           \u2502 represent this (ex like we do in batching)\u2502\n\u2502 worst case latency like if there\u2502           \u2502                                           \u2502\n\u2502 was a long write ahead of you   \u2502           \u2502 How do we address latency of a data flow? \u2502\n\u2502                                 \u2502           \u2502                                           \u2502\n\u2502 Also could use correlation id   \u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502 for the batch or another parent \u2502                              \u2502\n\u2502                                 \u2502                              \u2502\n\u2502                                 \u2502                              \u2502\n\u2502                                 \u2502                              \u2502\n\u2502                                 \u2502                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2502\n                                                                 \u2502\n                                                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502                                    \u2502\n\u2502   Intra-process Streams (like Reactor)    \u2502           \u2502Apis look the same, but challenges  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524           \u2502can be different. For example,      \u2502\n\u2502                                           \u2502           \u2502reactive streams define threading   \u2502\n\u2502 Intra-process Streams (like Reactor)      \u2502           \u2502semantics which can be helpful in   \u2502\n\u2502 Automatic instrumentation challenges:     \u2502           \u2502understanding how an application    \u2502\n\u2502 Granularity challenges include small as   \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2524behaves. Data flows have coarser    \u2502\n\u2502 clock ticks. Infinite sequences exist.    \u251c\u2500\u2500\u2500\u2500\u2500\u2518     \u2502granularity which maps easier to    \u2502\n\u2502 No way to tell by type if a flux will end \u2502           \u2502separate spans.                     \u2502\n\u2502 and many operators dont have metadata to  \u2502           \u2502                                    \u2502\n\u2502 indicate if it is a grouping command or   \u2502           \u2502Both break models or visualizations \u2502\n\u2502 not. Notable exception is parallel flux.  \u2502           \u2502that assume a tree shape (ex joins  \u2502\n\u2502                                           \u2502           \u2502are possible in either). Both can be\u2502\n\u2502 In cases where you create the source, ex  \u2502           \u2502infinite, though each process w/in  \u2502\n\u2502 web flux, you can know for sure it is     \u2502           \u2502the flow has a beginning and end.   \u2502\n\u2502 finite and how model spans on behalf of   \u2502           \u2502                                    \u2502\n\u2502 requests.                                 \u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502                                           \u2502\n\u2502 Manual instrumentation is possible where  \u2502\n\u2502 you can materialize a context which will  \u2502\n\u2502 be available at any point in the flux.    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n. closing as async one-way is done. lets move to https://github.com/openzipkin/zipkin/issues/1654 for multiple consumers. closing as async one-way is done. lets move to https://github.com/openzipkin/zipkin/issues/1654 for multiple consumers. here's an example model of links https://github.com/googleapis/googleapis/blob/master/google/tracing/trace.proto#L163. Here is a convenience paste of notes on linked traces from a recent workshop. Notably they aren't used primarily for async (discussion about async is here)\nLinked traces from Tracing Workshop Notes\nMost common use case is how to represent batching (like flushing to disk). Could this be multiple parents? In this case the batch is an independent operation that precedes what it is batching. This consolidating relationship exists, but is not strictly parent/child. The original purpose of links is similar in concept to the reference concept in OpenTracing. Look for this in BigTable client (soon :))\nLater use cases of links included how to handle multi-tenancy. For example, how do you handle a request that crosses multiple organizations (Ex tenant and the cloud itself). If you render a normal parent/child, a malicious client could send a constant trace ID and mess up all the traces in google\u2019s trace repository. Another abuse case is injecting traffic and observing the exit points, allowing them to understand some internals of the architecture. Linking can obscure the \u201cinternal trace\u201d while still allowing causality. In implementation, a trace is restarted entering one network, but linked back to the requestors ID. The way this usually works is that you can see that there\u2019s a link to another trace, but not necessarily see the data itself. \nAnother use case is long queries. How do you deal with pipelines that could take days to complete? Using links If you encode timestamp into a trace ID, you cannot determine at runtime if there\u2019s a long trace in progress.\nSergey mentioning some customers are quite large and may be tempted to use links any time they exit their network. Bogdan replying that overuse is possible, but not sure a solution. Documentation needs to be good with advice about when to use links and why, perhaps advising links as an edge case.\nSergey ask about sampling and links, particularly sampling based on trace IDs. Bogdan says by propagating the sampling decision you don\u2019t need to look at the value of the trace ID afterwards. By default accept the decision from the caller, with some thought about abuse in the future (maybe throttling). S: How does batching apply to sampling? B: if one trace in the batch is sampled, the batch is sampled.\nSome linking policy have to do with trust level, which is likely higher for paying users than anonymous ones.\n(Trace, span, link, attributes) is an example format\nThe reason for a trace ID is to identify all spans. Now, you have 10 requests coming with 10 different trace IDs, you can\u2019t make them one.\nOne of the problems with tracing (for service owners) is how to know when a trace is complete (and ready for analysis). Use of links can help break up longer work into units that can be analyzed, links being the part that can combine \u201cfinished\u201d work.\nOne guiding policy is to not put into the client what can be done on the server. In google, they defer to the latter to reduce critical path overhead.\nOne approach to visualizing a batch is to chop the heads off the traces linked to the batch, so they appear as children of the batch (ignoring span in each trace preceding the batch)\n. This one focuses on limitations of normal parent/child relationship (as complicated by multiple origins or coalescing) less so on async programming.. cc @ivansenic\n. @eirslett so found that gson is still actively supporting java 6, so wasn't bad after using that instead of moshi.\n. OK.. done toying around. The new implementation uses gson instead of moshi, and a normal buffer instead of okio.\nI ran benchmarks before and after:\n- Thrift reads were unaffected. Thrift writes (which used to use okio), are slightly faster with small spans and the same with spans with several annotations.\n- Json reads (which now use gson instead of moshi) are 20-35% faster, but writes are about half as fast.\nWhile I'm sure I'll nerd out on json writes, in the future, I think this performance is fine. Those concerned with performance should be using thrift anyway, as it is an order of magnitude faster than json before and after this change.\n. @nicmunroe you might also be interested in this option. Especially when using a fast encoder like thrift (~1us to encode a span), spans could be encoded on the calling thread, or you could incrementally decode spans coming off a queue until a certain size threshold where you send them.\n. @nicmunroe you might also be interested in this option. Especially when using a fast encoder like thrift (~1us to encode a span), spans could be encoded on the calling thread, or you could incrementally decode spans coming off a queue until a certain size threshold where you send them.\n. merged into https://github.com/openzipkin/zipkin-reporter-java/pull/6\n. merged into https://github.com/openzipkin/zipkin-reporter-java/pull/6\n. Thanks for putting this together. I've a head-start on this as toyed a\nlittle last couple days. Here's one thought..\nseparate out the generic changes that make tests not flake into a commit.\nseparate out the non-publish changes into a separate commit (ex ones that\nmake circle work)\nadd a new PR for the publishing things.. (or separate ^^ into a different\nPR and keep this one alive)\nThought process is that we can merge having dual pull request checkers\n(existing one and now circleci). We could then carry on a separate\ndiscussion around the publish oriented things, including the more crude\ncredential tooling etc.\nWe didn't find travis to be a problem until time passed. My overall concern\nis quickly moving without experience in CircleCI, we end up worse than\ntravis. One way we could address this is have pull request checkers here,\nbut use a less critical repo for guinea-pig testing release (ex\nzipkin-dependencies, which doesn't change that often)\nThoughts?\n. Thanks for putting this together. I've a head-start on this as toyed a\nlittle last couple days. Here's one thought..\nseparate out the generic changes that make tests not flake into a commit.\nseparate out the non-publish changes into a separate commit (ex ones that\nmake circle work)\nadd a new PR for the publishing things.. (or separate ^^ into a different\nPR and keep this one alive)\nThought process is that we can merge having dual pull request checkers\n(existing one and now circleci). We could then carry on a separate\ndiscussion around the publish oriented things, including the more crude\ncredential tooling etc.\nWe didn't find travis to be a problem until time passed. My overall concern\nis quickly moving without experience in CircleCI, we end up worse than\ntravis. One way we could address this is have pull request checkers here,\nbut use a less critical repo for guinea-pig testing release (ex\nzipkin-dependencies, which doesn't change that often)\nThoughts?\n. > Side question: the flakes that we had on Circle, are those things our\n\nusers would experience in their (non-test) environments?\nThe flake fixes for things that happen in travis and circle might be OS\nspecific or resource specific. OSX laptop builds don't fail for me often.\n. > Side question: the flakes that we had on Circle, are those things our\nusers would experience in their (non-test) environments?\nThe flake fixes for things that happen in travis and circle might be OS\nspecific or resource specific. OSX laptop builds don't fail for me often.\n. oops should have merged this :) thanks!\n. oops should have merged this :) thanks!\n. fyi just had a zombie build today. even with nothing else in the org running, the below job hung for >15m waiting to be scheduled https://circleci.com/gh/openzipkin/zipkin/347\n\nso far seems canceling and retrying works. Only mentioning here as it seems that this topic comes up sometimes https://discuss.circleci.com/t/indefinitely-queued-build/1584/33\n. PS circleci has been extremely flakey, moreso than travis who is doing more work (ex running docker tests). There's a rumor that circle 2.0 might make things less flakey. I'd recommend we continue to not push for more things on circle until at least normal builds pass routinely. PS circleci has been extremely flakey, moreso than travis who is doing more work (ex running docker tests). There's a rumor that circle 2.0 might make things less flakey. I'd recommend we continue to not push for more things on circle until at least normal builds pass routinely. closing as very over year old :). I noticed this, which is a bit out-of-date:\nhttps://github.com/frodenas/zipkin-boshrelease\nAlso, since zipkin server is a spring boot app, there's probably means to\ndeploy it in cloud foundry as a normal java app.\niirc @Arghanil might have deployed zipkin in cloud foundry..\n. I noticed this, which is a bit out-of-date:\nhttps://github.com/frodenas/zipkin-boshrelease\nAlso, since zipkin server is a spring boot app, there's probably means to\ndeploy it in cloud foundry as a normal java app.\niirc @Arghanil might have deployed zipkin in cloud foundry..\n. bosh used to..\nhttps://github.com/frodenas/zipkin-boshrelease/tree/master/packages\ndefinitely would need some cleanup. Also, no idea if bosh-lite makes this\neasier now. https://github.com/cloudfoundry/bosh-lite\nOn Tue, Aug 23, 2016 at 10:38 PM, Eirik Sletteberg <notifications@github.com\n\nwrote:\nWould it be able to setup a transitive dependency on cassandra as well?\nEase of deployment is always a good thing :)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1251#issuecomment-241752670,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61w8yBBxSgjaxn8K67kkow9FCNgSKks5qiwXTgaJpZM4JqfUV\n.\n. bosh used to..\nhttps://github.com/frodenas/zipkin-boshrelease/tree/master/packages\n\ndefinitely would need some cleanup. Also, no idea if bosh-lite makes this\neasier now. https://github.com/cloudfoundry/bosh-lite\nOn Tue, Aug 23, 2016 at 10:38 PM, Eirik Sletteberg <notifications@github.com\n\nwrote:\nWould it be able to setup a transitive dependency on cassandra as well?\nEase of deployment is always a good thing :)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1251#issuecomment-241752670,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61w8yBBxSgjaxn8K67kkow9FCNgSKks5qiwXTgaJpZM4JqfUV\n.\n. not sure if this helps, but pivotal web services (powered by cloud foundry) has been successfully used in sleuth\n\nhttps://github.com/spring-cloud/spring-cloud-sleuth/blob/2f93bd955a3399224a3cef5d799064342a28e0e7/docs/src/main/asciidoc/spring-cloud-sleuth.adoc#running-examples\n@marcingrzejszczak @dsyer do you have any instructions you can paste links to (or just c&p here?)\n. not sure if this helps, but pivotal web services (powered by cloud foundry) has been successfully used in sleuth\nhttps://github.com/spring-cloud/spring-cloud-sleuth/blob/2f93bd955a3399224a3cef5d799064342a28e0e7/docs/src/main/asciidoc/spring-cloud-sleuth.adoc#running-examples\n@marcingrzejszczak @dsyer do you have any instructions you can paste links to (or just c&p here?)\n. @shakuzen mentioned we could also look at porting from a similar guide, like this one http://blog.hazelcast.com/cloud-foundry/\n. @shakuzen mentioned we could also look at porting from a similar guide, like this one http://blog.hazelcast.com/cloud-foundry/\n. kudos shoutout to @mminella and team on that deployment niceness. maybe we could open a repo for something similar for zipkin, if someone's up to task (pun intended)\n. kudos shoutout to @mminella and team on that deployment niceness. maybe we could open a repo for something similar for zipkin, if someone's up to task (pun intended)\n. @tushar-dadlani if you know of any place users can go to when asking about CF, please note here!\n. For those interested, the next OSS Cloud Foundry release (cf-245) will introduce the first phase of Zipkin support. \nOnce this is out, it should probably be communicated more broadly via our existing implementations list, zipkin-user and twitter.\nYou can stalk https://github.com/cloudfoundry/cf-release/releases for details, but here's a sneak preview from @shalako (who's to thank for driving this feature).\n\nZipkin is an optional feature that operators enable by specifying a configuration property in their BOSH deployment manifest:\nrouter.tracing.enable_zipkin:true\nThis is support for developer correlation via logs only. When enabled the CF HTTP router (gorouter) will:\n- initiate trace id when trace id and span id are not present in the received request: forward X-B3-TraceId with requests to apps and log x_b3_traceid in access log\n- always initiate a span id: forward X-B3-SpanId with requests to apps and log x_b3_spanid in access log\n- initiate parent span id when trace id and span id are present: forward X-B3-ParentSpan with requests to apps and log x_b3_parentspanid in access log\n. @shalako I noticed that 245 is out, but there's no mention of this feature in the main or router release notes https://github.com/cloudfoundry/cf-release/releases/tag/v245 can you ping us when the release notes are updated? cc @tushar-dadlani\n. @shalako I noticed that 245 is out, but there's no mention of this feature in the main or router release notes https://github.com/cloudfoundry/cf-release/releases/tag/v245 can you ping us when the release notes are updated? cc @tushar-dadlani\n. Thanks for the sleuthing, Shannon!\n. Thanks for the sleuthing, Shannon!\n. It's too bad cf release notes don't summarize this, but hey. in the\nmean time, zipkin just tweeted to point people where to look\nhttps://twitter.com/zipkinproject/status/786761813698289664\n. It's too bad cf release notes don't summarize this, but hey. in the\nmean time, zipkin just tweeted to point people where to look\nhttps://twitter.com/zipkinproject/status/786761813698289664\n. looks like zipkin is mentioned in recent pivotal cloud foundry notes: https://twitter.com/making/status/800708459041722368. Very excited about this. Thanks for putting the effort in! Once you get to the point of having java code that does this, I can help with wiring etc to do the following if you are keen on it.\n\nWhen we get to the point, let's put a new StorageComponent in the existing module called Cassandra3StorageComponent (or a better name). Having this in the same jar will make it easier for people to switch, and keeps the clutter down. Being in the same jar shouldn't be an issue as for example, they both use the same version of the underlying storage driver.\nCassandra3StorageComponent can share the lazy connection stuff etc that already exists and is tested, but employ new types Cassandra3SpanConsumer and Cassandra3SpanStore which know about the new model. This shouldn't require any forward or backward changes in the old codebase, which helps simplify this PR to only dealing with the new model (in other words, let's drop the indexing changes as there's already a PR about that.)\nWe can add MigratingCassandraStorageComponent which would write to Cassandra3SpanConsumer, but read from both the old and new impl and can share a connection for example. This would allow existing things like health checks to work simply, and also make things simple on users.\n. Very excited about this. Thanks for putting the effort in! Once you get to the point of having java code that does this, I can help with wiring etc to do the following if you are keen on it.\n\nWhen we get to the point, let's put a new StorageComponent in the existing module called Cassandra3StorageComponent (or a better name). Having this in the same jar will make it easier for people to switch, and keeps the clutter down. Being in the same jar shouldn't be an issue as for example, they both use the same version of the underlying storage driver.\nCassandra3StorageComponent can share the lazy connection stuff etc that already exists and is tested, but employ new types Cassandra3SpanConsumer and Cassandra3SpanStore which know about the new model. This shouldn't require any forward or backward changes in the old codebase, which helps simplify this PR to only dealing with the new model (in other words, let's drop the indexing changes as there's already a PR about that.)\nWe can add MigratingCassandraStorageComponent which would write to Cassandra3SpanConsumer, but read from both the old and new impl and can share a connection for example. This would allow existing things like health checks to work simply, and also make things simple on users.\n. okie dokie. thx for the update\n. okie dokie. thx for the update\n. This change reminded me of the 128bit trace id concern which pops up quite often. If there's support to switch towards that, I'd suggest changing the trace id lookup fields to be something big enough to hold 128bits. #1262\n. This change reminded me of the 128bit trace id concern which pops up quite often. If there's support to switch towards that, I'd suggest changing the trace id lookup fields to be something big enough to hold 128bits. #1262\n. just made the java packages \"zipkin.storage.cassandra3\" which will allow us to define STORAGE_TYPE=cassandra3 and also have both jars in the classpath without overlapping\nNEXT STEP: add auto-configuration stuff so that the server can use this.\n. actually I need to sort out something around the UDTs first.. on it\n. pushed some work in progress on the UDT bootstrap problem. still some kinks.. I'll work on it again tomorrow.\n. thanks. I'll keep picking at the UDT problem today, as I really don't want\nto have to support or answer questions on a keyspace that is only to work\naround a deficiency in datastax driver's reflective mapping thing. I'll\nraise an issue on their jira if you haven't already.\n. ps congrats on travis green! even works \"on my laptop\"\n. circleci fail is because its cassandra version isn't recent enough.. will fix\n. it isn't ready to merge, yet anyway.. we still have to make this usable on the server. give me a day to help!\n. just pushed an update to do the following:\n- removes UDT commit I added yesterday\n- adds circleci config\n- dodges test keyspace collision with old cassandra (ex use test-zipkin3 not test-zipkin)\n. next step:\n- add autoconfig so that it can be used on the server.. test server manually\n. woot circleci passed\n. OK server works now. Even self-tracing. I need to look into putting an index-ttl based floor on lookback, then test with docker and zipkin-dependencies\nbash\nSELF_TRACING_ENABLED=true STORAGE_TYPE=cassandra3 java -jar ./zipkin-server/target/zipkin-server-*exec.jar\n\n. OK server works now. Even self-tracing. I need to look into putting an index-ttl based floor on lookback, then test with docker and zipkin-dependencies\nbash\nSELF_TRACING_ENABLED=true STORAGE_TYPE=cassandra3 java -jar ./zipkin-server/target/zipkin-server-*exec.jar\n\n. OK fixed oldest data thing.\nNEXT STEPS:\n- test with zipkin-dependencies\n- test with docker\n. OK fixed oldest data thing.\nNEXT STEPS:\n- test with zipkin-dependencies\n- test with docker\n. ok I have a docker image update ready to go.. works with both cassandra and cassandra3\nnext step: validate zipkin-dependencies\n. ok I have a docker image update ready to go.. works with both cassandra and cassandra3\nnext step: validate zipkin-dependencies\n. I have some doubts about \"all_annotations\" and whether we really need full text search to accomplish the goal, especially as annotations are easy to model as key=\"f\" or key=\"f\" and value=\"b\". It feels like a LIKE search is a bit loose.\n. I have some doubts about \"all_annotations\" and whether we really need full text search to accomplish the goal, especially as annotations are easy to model as key=\"f\" or key=\"f\" and value=\"b\". It feels like a LIKE search is a bit loose.\n. We should highlight that it is no longer possible to store or retrieve the \"raw span\" sent from instrumentation. Ex spark jobs etc need to parse the table as opposed to retrieve the span blob as they were earlier.\n. We should highlight that it is no longer possible to store or retrieve the \"raw span\" sent from instrumentation. Ex spark jobs etc need to parse the table as opposed to retrieve the span blob as they were earlier.\n. ex zipkin-dependencies will need some hours of effort to redo\n. ex zipkin-dependencies will need some hours of effort to redo\n. @michaelsembwever I just pushed an update which makes this possible to use on versions of cassandra that are easy to get a handle on (3.0.8/3.8+). This came at the cost of removing the full-text search on annotations (something the api doesn't need) and also opting out of SASI for the duration query.\nI'd much rather do something like this than spend effort maintaining a storage layer no-one can use. For example, @schrepfler asked about this, and honestly it will be a year before 3.9 is supported on DSE.\nIf the duration query is far better off with SASI PREFIX vs ALLOW FILTERING, I'd rather make the index setup conditional on cassandra version than prevent the model from working on versions of cassandra below 3.9\n@openzipkin/cassandra any feelings on this?\n. @michaelsembwever I just pushed an update which makes this possible to use on versions of cassandra that are easy to get a handle on (3.0.8/3.8+). This came at the cost of removing the full-text search on annotations (something the api doesn't need) and also opting out of SASI for the duration query.\nI'd much rather do something like this than spend effort maintaining a storage layer no-one can use. For example, @schrepfler asked about this, and honestly it will be a year before 3.9 is supported on DSE.\nIf the duration query is far better off with SASI PREFIX vs ALLOW FILTERING, I'd rather make the index setup conditional on cassandra version than prevent the model from working on versions of cassandra below 3.9\n@openzipkin/cassandra any feelings on this?\n. ps it is probably obvious that I'd prefer this targeting 3.0.8 here's why:\n- 3.0.8 can be used today\n  - 3.0.8 requires no significant change to docker or anyone else's scripts\n  - 3.0.8 can be used by those who run DSE\n  - you can't even download 3.9 right now\n- we are swamped with maintenance, so each storage service needs to count.\n  - introducing this for a supported version means we can help people transition off the old faster\n  - introducing this for a future version of cassandra means we have now another module to support, but not many users of it!\nPersonal reasons why:\nWe don't have many people regularly fixing bugs or answering questions about cassandra. However we have a lot of support problems with it. The current cassandra impl has taken considerable time for me to support, usually at the cost of evenings and weekends, and usually with complaints from users. The small changes in the commit I added (or similar ones if I make a mistake) seems to convert this PR to a generally useful tool, and gives me hope that I don't have to maintain the old one forever.\n. ps it is probably obvious that I'd prefer this targeting 3.0.8 here's why:\n- 3.0.8 can be used today\n  - 3.0.8 requires no significant change to docker or anyone else's scripts\n  - 3.0.8 can be used by those who run DSE\n  - you can't even download 3.9 right now\n- we are swamped with maintenance, so each storage service needs to count.\n  - introducing this for a supported version means we can help people transition off the old faster\n  - introducing this for a future version of cassandra means we have now another module to support, but not many users of it!\nPersonal reasons why:\nWe don't have many people regularly fixing bugs or answering questions about cassandra. However we have a lot of support problems with it. The current cassandra impl has taken considerable time for me to support, usually at the cost of evenings and weekends, and usually with complaints from users. The small changes in the commit I added (or similar ones if I make a mistake) seems to convert this PR to a generally useful tool, and gives me hope that I don't have to maintain the old one forever.\n. ps regardless of version, implementation-wise we are pretty clear. here's what's left\n- if we go with 3.9, there's no stable download, so docker needs to point to a snapshot. some might complain, but it does work.\n- regardless of version, the dependencies job needs to be rewritten. I can help with that.. looks like 4-8hrs of work.\n- I'd like to not install a fake keyspace just to satisfy the @UDT annotation processor. I didn't have time to look into it, but if we merge without this, I'll create a separate issue to track getting rid of that.\n. ps regardless of version, implementation-wise we are pretty clear. here's what's left\n- if we go with 3.9, there's no stable download, so docker needs to point to a snapshot. some might complain, but it does work.\n- regardless of version, the dependencies job needs to be rewritten. I can help with that.. looks like 4-8hrs of work.\n- I'd like to not install a fake keyspace just to satisfy the @UDT annotation processor. I didn't have time to look into it, but if we merge without this, I'll create a separate issue to track getting rid of that.\n. one way forward is to do what I mentioned about SASI indexing...\n- make it possible to use 3.9+ SASI indexing\n- but don't make it impossible to use an enterprise supported version of cassandra (3.0.8)\niotw, let's not cut off our nose to spite our face. I don't think index strategy is a model. it is an implementation detail which is similar to other things we've done in the past.\n. one way forward is to do what I mentioned about SASI indexing...\n- make it possible to use 3.9+ SASI indexing\n- but don't make it impossible to use an enterprise supported version of cassandra (3.0.8)\niotw, let's not cut off our nose to spite our face. I don't think index strategy is a model. it is an implementation detail which is similar to other things we've done in the past.\n. here's a list of various things and where they are at wrt cassandra:\nPackages\n- cassandra's direct download\n  - 3.0.8 3.7\n- cassandra's debian repo\n  - 3.0.8 3.7\n- datastax DSE\n  - 3.0.8\n- datastax community (windows, red hat, ...)\n  - 3.7\nPlatforms\n- docker\n  - 3.0.8 3.7\n- circleci\n  - 3.4\n- travis\n  - 2.1.8\nLast time we upgraded cassandra was 3 months ago, to version 2.2. I was immediately asked by multiple people on backchannel or on gitter (or both) to make it work with earlier versions as their support team couldn't upgrade that fast. That resulted in a PSA to my PSA below.\nhttps://groups.google.com/forum/#!searchin/zipkin-user/cassandra%7Csort:date/zipkin-user/s6NZctGdNJM/DjIbB3LcPwAJ\nThe usual counterpart to a statement around using a more conservative version like 3.0.8 is support. For example apache only supports 3.0.8 until next May 2017. I don't know DataStax policy (if it is the same or not).\nThe commit prior to mine doesn't work on 3.7 incidentally, due to missing org.apache.cassandra.db.compaction.TimeWindowCompactionStrategy. Let's not forget the lesson we learned in #706 where @liyichao raised an issue because of an awkward error. If we insist on bleeding edge, we should query the server or rewrite the error to help people who don't realize the minimum version requirements.\nAnyway, all this in mind.. I feel confident that the \"model\" can work across both 3.0.8 and 3.9+. As long as we can identify via metadata or exception the version, we can also setup SASI on the duration field transparently. I'm not sure about the \"annotation_keys     set\" field, as I don't know if you can SASI a collection, but then again, we don't have a full-text search api either.\nFood for thought.\n. here's a list of various things and where they are at wrt cassandra:\nPackages\n- cassandra's direct download\n  - 3.0.8 3.7\n- cassandra's debian repo\n  - 3.0.8 3.7\n- datastax DSE\n  - 3.0.8\n- datastax community (windows, red hat, ...)\n  - 3.7\nPlatforms\n- docker\n  - 3.0.8 3.7\n- circleci\n  - 3.4\n- travis\n  - 2.1.8\nLast time we upgraded cassandra was 3 months ago, to version 2.2. I was immediately asked by multiple people on backchannel or on gitter (or both) to make it work with earlier versions as their support team couldn't upgrade that fast. That resulted in a PSA to my PSA below.\nhttps://groups.google.com/forum/#!searchin/zipkin-user/cassandra%7Csort:date/zipkin-user/s6NZctGdNJM/DjIbB3LcPwAJ\nThe usual counterpart to a statement around using a more conservative version like 3.0.8 is support. For example apache only supports 3.0.8 until next May 2017. I don't know DataStax policy (if it is the same or not).\nThe commit prior to mine doesn't work on 3.7 incidentally, due to missing org.apache.cassandra.db.compaction.TimeWindowCompactionStrategy. Let's not forget the lesson we learned in #706 where @liyichao raised an issue because of an awkward error. If we insist on bleeding edge, we should query the server or rewrite the error to help people who don't realize the minimum version requirements.\nAnyway, all this in mind.. I feel confident that the \"model\" can work across both 3.0.8 and 3.9+. As long as we can identify via metadata or exception the version, we can also setup SASI on the duration field transparently. I'm not sure about the \"annotation_keys     set\" field, as I don't know if you can SASI a collection, but then again, we don't have a full-text search api either.\nFood for thought.\n. OK I just looked. Here's the jist of the issue..\nIn order to support the current api, we need to search based on annotation key/value. The last commit I added does that.\nThe only thing that can't be done from the commit prior is to full-text search on that annotationKeys field. This is an accepted feature here https://issues.apache.org/jira/browse/CASSANDRA-11182\nWhen the CASSANDRA-11182 is in, there's literally no modeling change needed to support the full-text search features of the prior commit. I'd recommend putting weight on that Jira, if full-text support is important. That way, it will eventually work with no model break!\nI will add a commit to conditionally use SASI on the duration field in the mean time. Duration query is by far the most important reason to upgrade. I don't know the performance difference from ALLOW_FILTERING to SASI, but at least we can measure it.\nSound fair? \n. OK I just looked. Here's the jist of the issue..\nIn order to support the current api, we need to search based on annotation key/value. The last commit I added does that.\nThe only thing that can't be done from the commit prior is to full-text search on that annotationKeys field. This is an accepted feature here https://issues.apache.org/jira/browse/CASSANDRA-11182\nWhen the CASSANDRA-11182 is in, there's literally no modeling change needed to support the full-text search features of the prior commit. I'd recommend putting weight on that Jira, if full-text support is important. That way, it will eventually work with no model break!\nI will add a commit to conditionally use SASI on the duration field in the mean time. Duration query is by far the most important reason to upgrade. I don't know the performance difference from ALLOW_FILTERING to SASI, but at least we can measure it.\nSound fair? \n. OK just changed the impl to conditionally apply SASI when available.\n. OK just changed the impl to conditionally apply SASI when available.\n. @michaelsembwever to be clear about this.. if you want to merge strictly requiring 3.9+, you are now the sole owner of this change and its support. I'm no longer investing my time on it.\nThis means you or someone else needs to..\n- rewrite the dependency graph\n- redo docker to support this\n- support users when they come in asking about how this works\nThis isn't me being mean, just standing up for myself: I'm not slave labor for change I don't agree with.\nIf this merges strictly 3.9+, it does imply other work for me.. which I accept. I will backport some of the features here to the existing cassandra to help get it out of its mess\nAssuming the most committers remain quiet, I guess it is your call.\n. @michaelsembwever to be clear about this.. if you want to merge strictly requiring 3.9+, you are now the sole owner of this change and its support. I'm no longer investing my time on it.\nThis means you or someone else needs to..\n- rewrite the dependency graph\n- redo docker to support this\n- support users when they come in asking about how this works\nThis isn't me being mean, just standing up for myself: I'm not slave labor for change I don't agree with.\nIf this merges strictly 3.9+, it does imply other work for me.. which I accept. I will backport some of the features here to the existing cassandra to help get it out of its mess\nAssuming the most committers remain quiet, I guess it is your call.\n. The only flaw in this logic is the assumption that 2.2 is supportable. It\nisnt. It has become a maintenance pit.\n. The only flaw in this logic is the assumption that 2.2 is supportable. It\nisnt. It has become a maintenance pit.\n. I'm taking back my assertion that this code can operate the same with or without c* 3.9. @michaelsembwever pointed out a place that implies a full table scan unless SASI is used (obviously something I'd have preferred pointed out well before this point in the conversation!).\nanyway.. he's right that the code is simpler or less hairy, by not adding another \"if statement\" below:\njava\n      for (String serviceName : serviceNames) {\n        for (Integer bucket : buckets) {\n          BoundStatement bound = CassandraUtil\n              .bindWithName(\n                  withDuration ? selectTraceIdsByServiceSpanNameAndDuration\n                      : selectTraceIdsByServiceSpanName,\n                  \"select-trace-ids-by-service-name\")\nThis is a sad day for me because I had hoped we could have a model that would buy us out of the current cassandra situation, and also buy us into a simpler future (with 3.9). The way this model is designed, it doesn't optimize for both. We already have one hairy cassandra module, and it would be worse to have two.\nLong story short: this should incubate in efforts to make cassandra of the future better one day, even if it now it will only be usable for users who can install and run their own c* 3.9+\n. I'm taking back my assertion that this code can operate the same with or without c* 3.9. @michaelsembwever pointed out a place that implies a full table scan unless SASI is used (obviously something I'd have preferred pointed out well before this point in the conversation!).\nanyway.. he's right that the code is simpler or less hairy, by not adding another \"if statement\" below:\njava\n      for (String serviceName : serviceNames) {\n        for (Integer bucket : buckets) {\n          BoundStatement bound = CassandraUtil\n              .bindWithName(\n                  withDuration ? selectTraceIdsByServiceSpanNameAndDuration\n                      : selectTraceIdsByServiceSpanName,\n                  \"select-trace-ids-by-service-name\")\nThis is a sad day for me because I had hoped we could have a model that would buy us out of the current cassandra situation, and also buy us into a simpler future (with 3.9). The way this model is designed, it doesn't optimize for both. We already have one hairy cassandra module, and it would be worse to have two.\nLong story short: this should incubate in efforts to make cassandra of the future better one day, even if it now it will only be usable for users who can install and run their own c* 3.9+\n. Since most people won't be able to use this anyway, I'm less concerned about the dependencies job. I'll revert my commit and merge so that @michaelsembwever can work on the future of this. I'll take on docker since already know how to do that anyway.\n. Since most people won't be able to use this anyway, I'm less concerned about the dependencies job. I'll revert my commit and merge so that @michaelsembwever can work on the future of this. I'll take on docker since already know how to do that anyway.\n. added a \"don't use this in production\" disclaimer to the cassandra3 readme and intentionally left this out of the top-level README until it matures and people have a good chance of being able to install it.\n. @michaelsembwever @abesto so I'm going to do a \"soft release\" on this. in other words, not widely announce the new cassandra3 thing as we have no experience with it. If you like, probably deserves an email to zipkin-dev google group with instructions on how power users can help beta test this.\nAll of us will watch for c* 3.9 download to actually happen. when it does, please update circleci and .travis files so that we don't get a late break. I'll file a low priority issue on the annotation processing thing and also an issue in zipkin-dependencies to add support. I'll do docker-zipkin as soon as the soft-release happens. That will work so long as we don't rename the schema file :)\n. soft-releasing 1.9.0\n. https://github.com/openzipkin/zipkin-dependencies/issues/44\n. https://github.com/openzipkin/zipkin/issues/1276\n. https://github.com/openzipkin/docker-zipkin/pull/111\n. @michaelsembwever thanks for the efforts here and good luck!\n. Sounds great, Mick. Thanks for the writeup.\nWe will also need to keep an eye on the traceid field. Not sure, but i\nthink it will be better as bytes than a number since we will soon be\npushing two longs together from the thrift. Having one bytes field in the\ndb might eliminate any confusion about if a negative number was a 64 or\n128bit id during transition. Food for thought.\n. Unless the libraries change or the code ends up way different, I would make\nit a flag or similar. I presume there's feature parity across the two?\nOn 25 Aug 2016 05:07, \"Brian Devins\" notifications@github.com wrote:\n\nIn order to use the AWS hosted Elasticsearch service we will need to\nsupport the REST api instead of the native transport API.\nI'm wondering if this should be a separate module or a flag in the\nElasticSearch storage builder if I were to create a PR for this.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1253, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61zuNhsgbe4IXeKyq6p49gbpxtwrbks5qjLKfgaJpZM4JsdFY\n.\n. Unless the libraries change or the code ends up way different, I would make\nit a flag or similar. I presume there's feature parity across the two?\n\nOn 25 Aug 2016 05:07, \"Brian Devins\" notifications@github.com wrote:\n\nIn order to use the AWS hosted Elasticsearch service we will need to\nsupport the REST api instead of the native transport API.\nI'm wondering if this should be a separate module or a flag in the\nElasticSearch storage builder if I were to create a PR for this.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1253, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61zuNhsgbe4IXeKyq6p49gbpxtwrbks5qjLKfgaJpZM4JsdFY\n.\n. In the mean time, pinged on twitter about this as this is a feature requested for them many times. It would be nice to understand if this is a backlog item on the product or not.\n\nhttps://twitter.com/adrianfcole/status/768616436948271104\nI noticed that maggieaws is the one doing product launch announcements on the forum:\nhttps://forums.aws.amazon.com/ann.jspa?annID=3946\nThis might be the correct maggie, or maybe just there are multiple AWS product managers named maggie\nhttps://www.linkedin.com/in/yuanzhuang\nIf someone knows the product team, please reach out and see if this is on the roadmap as rewriting would be fruitless if it is.\n. In the mean time, pinged on twitter about this as this is a feature requested for them many times. It would be nice to understand if this is a backlog item on the product or not.\nhttps://twitter.com/adrianfcole/status/768616436948271104\nI noticed that maggieaws is the one doing product launch announcements on the forum:\nhttps://forums.aws.amazon.com/ann.jspa?annID=3946\nThis might be the correct maggie, or maybe just there are multiple AWS product managers named maggie\nhttps://www.linkedin.com/in/yuanzhuang\nIf someone knows the product team, please reach out and see if this is on the roadmap as rewriting would be fruitless if it is.\n. PS elastic just mentioned that their cloud (which runs on AWS) supports transport client. So, that's one way to get collocated data in AWS. https://twitter.com/elastic/status/768621013063995392\n. PS elastic just mentioned that their cloud (which runs on AWS) supports transport client. So, that's one way to get collocated data in AWS. https://twitter.com/elastic/status/768621013063995392\n. there's a new impl in town, via @sethp-jive https://github.com/openzipkin/zipkin/pull/1302\n. there's a new impl in town, via @sethp-jive https://github.com/openzipkin/zipkin/pull/1302\n. Will be released in zipkin 1.12\n``` bash\nmake sure your cli credentials are setup as zipkin will read them\n$ aws es describe-elasticsearch-domain --domain-name mydomain|jq .DomainStatus.Endpoint\n\"search-mydomain-2rlih66ibw43ftlk4342ceeewu.ap-southeast-1.es.amazonaws.com\"\n$ STORAGE_TYPE=elasticsearch ES_HOSTS=https://search-mydomain-2rlih66ibw43ftlk4342ceeewu.ap-southeast-1.es.amazonaws.com java -jar zipkin.jar\nor if you don't know the url already\n$ STORAGE_TYPE=elasticsearch ES_AWS_DOMAIN=mycluster ES_AWS_REGION=ap-southeast-1 java -jar zipkin.jar\n```\n. Try this?\n``` java\nimport com.esotericsoftware.kryo.Kryo;\nimport com.esotericsoftware.kryo.Serializer;\nimport com.esotericsoftware.kryo.io.Input;\nimport com.esotericsoftware.kryo.io.Output;\nimport com.esotericsoftware.kryo.serializers.DefaultArraySerializers.ByteArraySerializer;\nimport java.io.ByteArrayOutputStream;\nimport java.io.IOException;\nimport org.junit.Test;\nimport zipkin.Codec;\nimport zipkin.Span;\nimport zipkin.TestObjects;\nimport static org.assertj.core.api.Assertions.assertThat;\npublic final class KryoTest {\nstatic final class SpanSerializer extends Serializer {\n    final Codec delegate = Codec.THRIFT;\n    final ByteArraySerializer bytesSerializer = new ByteArraySerializer();\n@Override\npublic void write(Kryo kryo, Output output, Span span) {\n  bytesSerializer.write(kryo, output, delegate.writeSpan(span));\n}\n\n@Override\npublic Span read(Kryo kryo, Input input, Class<Span> type) {\n  byte[] bytes = bytesSerializer.read(kryo, input, byte[].class);\n  return delegate.readSpan(bytes);\n}\n\n}\n@Test\n  public void spanSandwich() throws IOException {\n    Kryo kryo = new Kryo();\n    kryo.register(Span.class, new SpanSerializer());\nByteArrayOutputStream baos = new ByteArrayOutputStream();\nOutput output = new Output(baos);\n\n// let's make a span sandwich\nkryo.writeObject(output, \"bread\");\nkryo.writeObject(output, TestObjects.TRACE.get(0));\nkryo.writeObject(output, \"bread\");\n\noutput.flush();\nInput input = new Input(baos.toByteArray());\n\nassertThat(kryo.readObject(input, String.class))\n    .isEqualTo(\"bread\");\nassertThat(kryo.readObject(input, Span.class))\n    .isEqualTo(TestObjects.TRACE.get(0));\nassertThat(kryo.readObject(input, String.class))\n    .isEqualTo(\"bread\");\n\n}\n}\n```\n. Try this?\n``` java\nimport com.esotericsoftware.kryo.Kryo;\nimport com.esotericsoftware.kryo.Serializer;\nimport com.esotericsoftware.kryo.io.Input;\nimport com.esotericsoftware.kryo.io.Output;\nimport com.esotericsoftware.kryo.serializers.DefaultArraySerializers.ByteArraySerializer;\nimport java.io.ByteArrayOutputStream;\nimport java.io.IOException;\nimport org.junit.Test;\nimport zipkin.Codec;\nimport zipkin.Span;\nimport zipkin.TestObjects;\nimport static org.assertj.core.api.Assertions.assertThat;\npublic final class KryoTest {\nstatic final class SpanSerializer extends Serializer {\n    final Codec delegate = Codec.THRIFT;\n    final ByteArraySerializer bytesSerializer = new ByteArraySerializer();\n@Override\npublic void write(Kryo kryo, Output output, Span span) {\n  bytesSerializer.write(kryo, output, delegate.writeSpan(span));\n}\n\n@Override\npublic Span read(Kryo kryo, Input input, Class<Span> type) {\n  byte[] bytes = bytesSerializer.read(kryo, input, byte[].class);\n  return delegate.readSpan(bytes);\n}\n\n}\n@Test\n  public void spanSandwich() throws IOException {\n    Kryo kryo = new Kryo();\n    kryo.register(Span.class, new SpanSerializer());\nByteArrayOutputStream baos = new ByteArrayOutputStream();\nOutput output = new Output(baos);\n\n// let's make a span sandwich\nkryo.writeObject(output, \"bread\");\nkryo.writeObject(output, TestObjects.TRACE.get(0));\nkryo.writeObject(output, \"bread\");\n\noutput.flush();\nInput input = new Input(baos.toByteArray());\n\nassertThat(kryo.readObject(input, String.class))\n    .isEqualTo(\"bread\");\nassertThat(kryo.readObject(input, Span.class))\n    .isEqualTo(TestObjects.TRACE.get(0));\nassertThat(kryo.readObject(input, String.class))\n    .isEqualTo(\"bread\");\n\n}\n}\n```\n. @ivansenic fyi\n. @ivansenic fyi\n. \ud83d\udc4d \n. \ud83d\udc4d \n. fyi https://github.com/openzipkin/zipkin-spark-streaming. This issue isn't descriptive enough to take action on. We had a fix recently for @bedag-moo who was using eclipse. If you can be more specific on what zipkin needs to do in order to support eclipse, let us know.\n. This issue isn't descriptive enough to take action on. We had a fix recently for @bedag-moo who was using eclipse. If you can be more specific on what zipkin needs to do in order to support eclipse, let us know.\n. @lijunyong the error you pasted has nothing to do with the IDE. it means you haven't installed npm https://www.npmjs.com/package/npm\nI use intellij\n. @lijunyong the error you pasted has nothing to do with the IDE. it means you haven't installed npm https://www.npmjs.com/package/npm\nI use intellij\n. OK, here's the deal on eclipse..\nIf we import with this, language level isn't set properly and so you have to go through and reset everything to java 8. \nbash\n$ ./mvnw eclipse:eclipse\nThere's probably other things, too, as eclipse is more strict by default than intellij.\n. OK, here's the deal on eclipse..\nIf we import with this, language level isn't set properly and so you have to go through and reset everything to java 8. \nbash\n$ ./mvnw eclipse:eclipse\nThere's probably other things, too, as eclipse is more strict by default than intellij.\n. @lijunyong no prob\n. @lijunyong no prob\n. PS recent versions of intellij are now harder to use as they have module-specific compiler settings, but not the ability to override them globally. In a sense, this is similar to the problems in Eclipse.\nhttps://youtrack.jetbrains.com/issue/IDEA-161060\nHere's an OS/X workaround for when your IDE makes everything use the main tree source level (which makes the test tree fail to compile)\nfind . -name '*.iml' -print0 | xargs -0 sed -i '' 's/JDK_1_6/JDK_1_8/g'\nfind . -name '*.iml' -print0 | xargs -0 sed -i '' 's/JDK_1_7/JDK_1_8/g'\nsed -i '' 's/target=\"1.7\"/target=\"1.8\"/g' .idea/compiler.xml\nsed -i '' 's/target=\"1.6\"/target=\"1.8\"/g' .idea/compiler.xml\n. PS recent versions of intellij are now harder to use as they have module-specific compiler settings, but not the ability to override them globally. In a sense, this is similar to the problems in Eclipse.\nhttps://youtrack.jetbrains.com/issue/IDEA-161060\nHere's an OS/X workaround for when your IDE makes everything use the main tree source level (which makes the test tree fail to compile)\nfind . -name '*.iml' -print0 | xargs -0 sed -i '' 's/JDK_1_6/JDK_1_8/g'\nfind . -name '*.iml' -print0 | xargs -0 sed -i '' 's/JDK_1_7/JDK_1_8/g'\nsed -i '' 's/target=\"1.7\"/target=\"1.8\"/g' .idea/compiler.xml\nsed -i '' 's/target=\"1.6\"/target=\"1.8\"/g' .idea/compiler.xml\n. retrolambda works around the problem in a different way https://github.com/openzipkin/zipkin/pull/1425. libthrift doesn't have a \"sizeOf\" feature, so doing this automatically\nwould need a change to the compiler. This might be harder to do when you\nhave an open set of fields, though not impossible. Filed an issue\nregardless https://issues.apache.org/jira/browse/THRIFT-3913\nOn Wed, Aug 31, 2016 at 6:17 AM, Eirik Sletteberg notifications@github.com\nwrote:\n\nCould this be merged upstream to libthrift as well? Or is it too\nZipkin-specific?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1259#issuecomment-243599342,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD619stIGZFxambvMNenkD0KQL0CRRiks5qlKvsgaJpZM4JwPRy\n.\n. libthrift doesn't have a \"sizeOf\" feature, so doing this automatically\nwould need a change to the compiler. This might be harder to do when you\nhave an open set of fields, though not impossible. Filed an issue\nregardless https://issues.apache.org/jira/browse/THRIFT-3913\n\nOn Wed, Aug 31, 2016 at 6:17 AM, Eirik Sletteberg notifications@github.com\nwrote:\n\nCould this be merged upstream to libthrift as well? Or is it too\nZipkin-specific?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1259#issuecomment-243599342,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD619stIGZFxambvMNenkD0KQL0CRRiks5qlKvsgaJpZM4JwPRy\n.\n. ok wrote an ipv6 encoder.. now json is faster than libthrift!\n. ok wrote an ipv6 encoder.. now json is faster than libthrift!\n. thanks for starting on this.. I got sidetracked, but understand this is a big deal, not just AWS, but also for ElasticSearch 5. I'll try to dedicate some time towards this, as we really need a reliable and performant backend that most people can use.\n. thanks for starting on this.. I got sidetracked, but understand this is a big deal, not just AWS, but also for ElasticSearch 5. I'll try to dedicate some time towards this, as we really need a reliable and performant backend that most people can use.\n. with my current backlog, I think earliest I can start is this weekend.\n\nOn Wed, Sep 7, 2016 at 11:32 PM, Brian Devins notifications@github.com\nwrote:\n\nAwesome. I might get some time for this today too if you need a rubber\nduck for anything\nOn Tue, Sep 6, 2016 at 11:28 PM -0400, \"Adrian Cole\" \nnotifications@github.com wrote:\nthanks for starting on this.. I got sidetracked, but understand this is a\nbig deal, not just AWS, but also for ElasticSearch 5. I'll try to dedicate\nsome time towards this, as we really need a reliable and performant backend\nthat most people can use.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1261#issuecomment-245319624,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD614vqv6p_Tg0LkOWDKAIidrsE6qDyks5qntkcgaJpZM4JwyOT\n.\n. ok I'll start on this now. Similar to you, I think I'll start with the collector side.\n. ok I'll start on this now. Similar to you, I think I'll start with the collector side.\n. thanks for the tips. I started this on a shared branch https://github.com/openzipkin/zipkin/pull/1294\n. thanks for the tips. I started this on a shared branch https://github.com/openzipkin/zipkin/pull/1294\n. spamming some folks of interest, of course not exhaustive @nicmunroe @felixbarny @shakuzen @yurishkuro @jcarres-mdsol @abesto @eirslett @kristofa @michaelsembwever @kevinoliver @mosesn @anuraaga @marcingrzejszczak @prat0318 @devinsba @basvanbeek @schlosna @ewhauser @klingerf @bogdandrutu @clehene\n. spamming some folks of interest, of course not exhaustive @nicmunroe @felixbarny @shakuzen @yurishkuro @jcarres-mdsol @abesto @eirslett @kristofa @michaelsembwever @kevinoliver @mosesn @anuraaga @marcingrzejszczak @prat0318 @devinsba @basvanbeek @schlosna @ewhauser @klingerf @bogdandrutu @clehene\n. One idea to bridge this in the current model is to add another u64 field: traceIdHigh\n\nThis would be somewhat easy to implement, and we can default to zero. When 128 bit ids are used, they would be split between traceIdHigh and traceId (in data structures). In storage, they can either be concatenated or stored separately.\n. actually, I don't like traceIdHigh as it would leave identifiers in a transitional state (and make querying awkward). I think it would be best to obviate the old traceId field with a wider one rather than append to it.\n. > actually, I don't like traceIdHigh as it would leave identifiers in a\n\ntransitional state (and make querying awkward). I think it would be best to\nobviate the old traceId field with a wider one rather than append to it.\n@adriancole https://github.com/adriancole do you have an alternative\nproposal for thrift? binary?\n\nI didn't mean to imply not using thrift. I meant adding another field with\nsome notes about how to use it. Similar to how we added ipv6\n11: optional binary traceId128\n. > I like this\u2013in practice we run into collisions frequently. I'm not sure I\n\nlike increasing the size of the thrift frame. What would you estimate the\noverall increase in storage size would be for TBinary and for TCompact?\nSo, there's two places of concern to finagle.\n\nFor example, the trace context (SpanId) field would need to look at the\nsize before deserializing. This would grow from 32 to 40bytes per span,\nplus whatever encoding overhead. This assumes the simplest option, where we\njust write more bytes to the first field.\nAt the moment, we only use big-endian TBinaryProtocol. This simplifies a\nlot of reporters, particularly kafka which doesn't have a field to carry\nwhat the encoding type is. As long as we keep the scope to only trace ids,\nthis is a fixed overhead of how to encode the extra 64 bits.\nIf we go for a separate \"binary traceId128\" field, we add 26 bytes per thrift (3 /\nfor TType.STRING field declaration / + 4 / for length prefix / + 16 / for id\n/)\nIf we go for an additional \"i64 traceIdHigh\" field, we add 11 bytes per thrift (3\n/ for TType.I64 field declaration / + 8 / for high bytes /)\nSince no-one is likely indexing on thrift offsets, traceIdHigh might not be\nas bad as I thought. Ex we could still have json just use a wider amount of\nhex characters in its traceId field.\nAssuming we'd need a separate index (minimally +16bytes), I'd say the\nstorage costs are spanCount * at least 27-42 bytes per span.\non the topic of TBinaryProtocol vs TCompactProtocol\nif we are looking to optimize serialized size, I'm not sure I'd use either\nsince other options are now available, and also we can rejig the model,\ntoo. So, leaving that out-of-scope for this talk.\n. incidentally, just peeked at a work-in-progress for GRPC trace propagation.\nThey are using 2 64bit fields to capture the 128bit trace id.\n// A TraceId uniquely represents a single Trace. It is a 128-bit nonce.\nmessage TraceId {\n  fixed64 hi = 1;\n  fixed64 lo = 2;\n}\nAn aside, but they don't share the same span id across RPC calls, so only need to propagate the parent (as opposed to the parent and the current id)\n// Tracing information that is propagated with RPC's.\nmessage TraceContext {\n  // Trace identifer. Must be present.\n  TraceId trace_id = 1;\n  // ID of parent (client) span. Must be present.\n  fixed64 span_id = 2;\n  // true if this trace is sampled.\n  bool is_sampled = 3;\n}\n. @yurishkuro there is no encoded string type in thrift. string (TType 11) fields are binary\n. ps on high-low, unless I'm missing something, there's effectively no difference between traceIdHigh = 0 and traceIdHigh = null, so even if the field is optional in IDL, I'd just make it default to zero (ex in java). Reason being is that it only is used when traceId is set, and if you are packing 128 bits shifting 0 on the high side is the same as shifting nothing into the high side.\n. @yurishkuro didn't mean to distract the issue. you are right that in thrift IDL there's a \"binary\" compiler hint that says the STRING field is being used for opaque bytes. I changed my example above.\n. cool thing in thrift is we can actually rename traceId to lo without\nbreaking anything (on the wire anyway) :)\nOn Thu, Sep 1, 2016 at 10:50 PM, Moses Nakamura notifications@github.com\nwrote:\n\nYeah, I'm beginning to lean toward hi/lo.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1262#issuecomment-244103407,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610M7MBbJPPnxTdr58_EchGfgsuFAks5qluYlgaJpZM4Jxav9\n.\n. I haven't seen anyone against this, so here's a proposal:\n\nIn thrift and zipkin.Span, add a field.. this can happen immediately\n- add traceIdHigh, default to zero\nMake all of the below that accept trace ids, check length and prefer lower 64 bits\n- http servers that receive \"X-B3-TraceId\" check length and throw away higher ones\n- Finagle TraceId and Brave SpanId check buffer length. If 40, there's a 128 bit trace id in there\n- Json decoders (probably only zipkin.Codec and zipkin UI) check length and throw away higher bits of traceId\nChange all of the storage backends to use a 128 bit traceId index. Pad 64 bit trace ids to 128 bit on write. Toss higher bits on read.\nAfter all the above is complete.. we can start writing the longer ids as a matter of course.\n- change encoders to write 128 bit for B3 Headers and B3 Binary, and Json as opposed to tossing the high bits\n. Note this is for 128 bit ids not uuid (which are not 128bits of randomness\nand also longer due to hyphens). Ex all lowercase hex chars.\nThis design is focused on permiting 128bit fully random ids, not the shape\nof uuid.\nZipkin has binary annotations (key,value) where different shaped ids can be\nplaced. For text search and indexing, all lowercase hex 128bit is what this\nis proposing, which is not only a more straightforward transition from our\ncurrent 64bit lowerhex, but incidentally easier to copy paste than uuids.\n. Probably worth calling out in general that the text form of ids are almost\nalways different than storage, except where the store is json (ex\nelasticsearch)\nSo when we talk about traceIdHigh, it is thrift binary form (which most\nwont ever see) and is far smaller. The second field is better for\ncompatibility. Also, many dbs will store keys as fixed width binary and\ndefer to functions to render them in hex as needed.\nIn http and json world, most will only know about the lowerhex form, being\nblissfully unaware of anything else. They will see a lowerhex id like they\ndo now, and later one up to twice as long.\nThats the working design anyway.\n. One more thing as the question of log correlation always comes up.\nFor offline, any different shape id or simply different id, needs to be\nwritten once per trace. B3 doesnt define a log correlation key name.\nFor example, say I am a frontend and I have chosen to correlate logs with\nthe name cfTraceId and a value of mostly 128bit in UUID form.\nI would write that field as is once to storage.. maybe I also write the\nsampling method, too.\nEx span.addBinaryAnnotation(\"cfTraceId\", \"abcdef-....\")\nspan.addBinaryAnnotation(\"sampleRate\", \"0.1\")\nUsers dont interact with B3, and shouldnt know it intimately if we do our\njobs right. In fact a future B4 will almost certainly not separate traceid\nas a separate header!\nAnyway, in my user manual, I tell users to search for cfTraceId if I want\nthem to search with UUIDs as opposed to the raw zipkin traceid. Ex in\nzipkin search panel cfTraceId=abcded-... In my impl I make sure logs are\nusing that key in their MDC etc.\nAs long as this is logged once, it will return unambiguously a single trace.\n. So I will open a new issue on Monday for implementation, and at that point\nI will summarize things I've mentioned for docs.\nHere's one thing that might not be obvious. Currently, zipkin trace ids are\nlogged as fixed 16 lowerhex in json.\nWhen you copy paste this id into something with full text search, it comes\nback. When we widen that to 128bit, and start encoding, we might want to\nstill encode ids with high bits unset as 16 chars as opposed to padding to\n32.\nEx. Instead of one with traceId '12ab3..4' and an upgraded one with traceId\n'0000...12ab..34' use 16char lowerhex encoding. That way, those who\ncurrently do full text search on zipkin ids are likely to copy/paste a\nvalue that works even in a system not fully upgraded.\nIf we did this, the cool part is that the edge (originator of a trace aka\nthe tracer that makes a root trace) makes the final decision on whether or\nnot 128bit ids are supported. Ex if it only sends 64bits of id, the system\nwill work on the brown field. When ready, that edge tracer flips a switch\nand now 128bit ids are used.\nThoughts?\n. Sorry one more thing I should have said in the beginning.. the audience of\nthis change!\nKeep in mind that while most who want to integrate with Zipkin use B3\npropagation, not everyone does. B3 is the dominant style, but 128 bit will\nbe used even when systems are propagating with something unlike B3.\nFor example, Jaeger can log to zipkin, but uses a single concatenated\nheader. Once we support 128bit, grpc will too, but also they wont use B3.\nIt is important to note that while we are discussing intimate details of\nB3, it is not to preclude someone from encoding propagation differently,\nespecially as that's already the case today.\nSo, there are three audiences for this change..\n- those using B3 and Zipkin\n- those integrating with B3 but not Zipkin (ex to reuse existing tracers\n  but a different system)\n- those integrating with Zipkin, but not using B3 (ex to reuse storage and\n  correlation)\nThere is a sub-group of above who want to know how to do log correlation,\ntoo.\n. here's a work-in-progress for a java library that does 128bit B3 propagation based on finagle/brave. It doesn't yet do http headers. Note that thrift or json serialization isn't in scope of propagation.\nhttps://github.com/openzipkin/b3-propagation/pull/2\n. Added some notes about how 128bit ids will look in practice https://github.com/openzipkin/b3-propagation/issues/5\n. Added some notes about how 128bit ids will look in practice https://github.com/openzipkin/b3-propagation/issues/5\n. moving to implementation in #1298\n. moving to implementation in #1298\n. fyi, there's a tentative understanding that sampling (when traceId is a function) will only consider the lower 64-bits. The assumption is that the resolution is good enough and it prevents us from needing to redefine samplers. (recommended by @basvanbeek and I agree)\n. fyi, there's a tentative understanding that sampling (when traceId is a function) will only consider the lower 64-bits. The assumption is that the resolution is good enough and it prevents us from needing to redefine samplers. (recommended by @basvanbeek and I agree)\n. throwing something out there.. what if we adopted a default behavior to make the 128bit trace ID exactly how Amazon X-Ray does? In this case, it would be possible to parse into an X-Ray ID, and still not impact us (as folks only sample lower 64 anyway).\nEx. first 8 hex are epoch seconds, next 24 are random.\nI suspect the cost would be time to get the epoch secs. Also, we'd not know if the trace was generated like this (in order to parse it), but anyway, there'd be a chance. Worth it? Not worth it?\nhttp://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-request-tracing.html. In the case of Zipkin, there is a large amount of users in Amazon, and a lot of the time we generate 128-bit w/o any consistency anyway. Some copy/paste the 64bit, etc. The rationale I was told from Abhishek is that the backend will reject the trace. The timestamp is used to store the data and enable even partitions in the backend. So, basically not using 32 of the 128 for the timestamp has a severe penalty if trying to use X-Ray propagation. 96 bits of random is great for probably 99.9% of deployments, and also doesn't impact sampling as most only look at lower 64-bit anyway.\nIf we had great support for a different way of 128-bit trace ID generation, I'd not suggest something like this, but as we don't, seems a very good reason to recommend and track one. Of course folks can make trace ID generation pluggable, but I think the tradeoff wins in favor of making things align with the largest cloud.\nTaking this forward, for example, I'd open a ticket on B3 with a suggested generation format for 128-bit trace IDs, with this rationale, and whoever wants to do this could originate trace IDs that Amazon won't toss. Remember, it isn't just X-Ray, it is also ALB, Api Gateway and other products that use the same ID format (if only for log correlation).\nThis means even if users never use X-Ray storage (which some sites can choose to as it is possible to convert zipkin data to X-Ray data), at least propagation will pass and you can ID correlate and pull data in a pinch.\nI think of this as tactical as it is easy to do and can be done even today. It allows us to tunnel AWS IDs through clients written for B3. Trace-Context stuff is more strategic as there's more going on there. This sort of integration shows almost exactly the pattern needed for trace-context, too.\nAnyone don't want me to suggest why a tracer might put timestamp in first 32 of 128? If I did, I'd mention it isn't fully random etc, and also maybe some sanity check tricks. Personally, I think not telling people about this is more damaging than telling them, as there's no easier path to Amazon integration (at least that I can think of).. @jcarres-mdsol so I have been toying around and hopefully by the end of the day I can verify zipkin and x-ray interop works (prereq to actually suggesting this in more than a hypothetical way). Will share as soon as done, but I see no road-blocks.\nOther notes:\nAmazon actually have the same sampling approach as we do. They support a \"deferred\" decision (same as absence of B3-Sampled flag). Only difference is that in this case the http response contains the result of that decision. However, I checked and this behavior isn't mandatory.\nSome concern raised by @jcchavezs about encoding sensitivity. Here's the response to this. Amazon's format is version stamped. If the version isn't one, we can't continue the trace anyway (until that version is understood). We have choices including adding a tag about the linked ID before the trace is restarted. Amazon have a very good api compat track record. I am in no way concerned about X-Ray trace format v1 disappearing next year. We don't know what v2 will be anyway, and who knows, it might be the trace-context format. Anyway TL;DR; is that in my playground, I'll verify the version, and when processing inbound headers, we can think about how to restart a trace if we are in strict AWS compat mode (implies restarting a trace if the first 32 bits could not have been epoch seconds).\nSo, basically there are two parts.. the easy one is just using a different algo for root IDs. The second part is for AWS interop... if a library must use AWS-compat IDs, then it needs to restart traces which don't include valid epoch seconds. The second part only impacts sites running on AWS, and can be handled by an optional tracer plugin.. >\n\n@adriancole https://github.com/adriancole so what kind of interop are\nyou targeting, a Zipkin-instrumented system talking to AWS SaaS?\nmoving the conversation here now. hope you don't mind!\nhttps://github.com/openzipkin/zipkin/issues/1754\n. Thanks for finding the bug. Are you up to trying a fix?\n. Thanks for finding the bug. Are you up to trying a fix?\n. it's ok. I'll ping you with a fix and then you'll know more!\n. https://github.com/openzipkin/zipkin/pull/1264\n. 1.8.1 on the way. sorry folks. I caused this bug\n. ok 1.8.1 is out to docker, now.\n. also manually tested with the following spans, then went into the UI to check..\n\nbash\n$ curl -s localhost:9411/api/v1/spans -H'Content-Type: application/json' -d '[{\"traceId\":\"1\",\"name\":\"no-annotations\",\"id\":\"1\",\"timestamp\":1234,\"duration\":100,\"binaryAnnotations\":[{\"key\":\"lc\",\"value\":\"bamm-bamm\",\"endpoint\":{\"serviceName\":\"flintstones\",\"ipv4\":\"127.0.0.1\"}}]}]'\n$ curl -s localhost:9411/api/v1/spans -H'Content-Type: application/json' -d '[{\"traceId\":\"2\",\"name\":\"no-binary-annotations\",\"id\":\"2\",\"timestamp\":1234,\"duration\":100,\"annotations\":[{\"timestamp\":1234,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"flintstones\",\"ipv4\":\"127.0.0.1\"}}]}]'\n. @prat0318 for api design, there's two paths that are repeated\n- Zipkin Tracer - Many zipkin apis, including finagle, ruby and javascript use the same approach. Ex Trace.record(\"event\"). These apis only offer zipkin features, so are more straightforward to support in zipkin.\n- OpenTracing Span - ex https://github.com/uber/jaeger-client-python is an opentracing api, even using zipkin thrifts, but lacking B3 and zipkin transports. Ex I changed their java client to support zipkin https://github.com/uber/jaeger-client-java/pull/34. OpenTracing is a generic API, but has some features that aren't supported in zipkin/B3 (like arbitrary attachments and baggage).\n. whoot! https://github.com/Yelp/py_zipkin\n. for those interested, I just made an example project https://github.com/openzipkin/pyramid_zipkin-example/pull/1\n. ok this will teach me to switch between too many types of work. The change to fix every thing except the problem you reported was merged. the fix for binary annotation.value was in a git stash.\nhttps://github.com/openzipkin/zipkin/pull/1275\n. 1.8.4 on the way. sorry about the confusion I created\n. ok tested by downloading from maven and doesn't blow up!\n. here's for wingtips https://github.com/Nike-Inc/wingtips/pull/24\n. awesome turnaround, nic\nOn Tue, Sep 6, 2016 at 10:29 AM, Nic Munroe notifications@github.com\nwrote:\n\n@jpiechowka https://github.com/jpiechowka Merged, built, and deployed\n@adriancole https://github.com/adriancole's wingtips PR as wingtips\nversion 0.11.1. Thanks guys!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1272#issuecomment-244835874,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61-PvDTRGO1Jps6QWJ5b78N7Azpveks5qnNAmgaJpZM4J04zi\n.\n. Whew :) thanks for replying back\n\nOn 6 Sep 2016 15:47, \"Janusz Piech\u00f3wka\" notifications@github.com wrote:\n\nTested the new Zipkin version right now and it works well. Thanks\n@adriancole https://github.com/adriancole\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1272#issuecomment-244874762,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61_vlPvuqWVgGhd6B7w_FC8RFQzGpks5qnRp5gaJpZM4J04zi\n.\n. Whew :) thanks for replying back\n\nOn 6 Sep 2016 15:47, \"Janusz Piech\u00f3wka\" notifications@github.com wrote:\n\nTested the new Zipkin version right now and it works well. Thanks\n@adriancole https://github.com/adriancole\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1272#issuecomment-244874762,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61_vlPvuqWVgGhd6B7w_FC8RFQzGpks5qnRp5gaJpZM4J04zi\n.\n. The UI is bound to the zipkin api, not the storage driver, so you can use\nany of the storage choices.\n. I'd say use what you are good at.\n\nElasticsearch has less experience, but there are users on it now. We will\nlikely need to revisit it soon to make it possible to be REST-only\nhttps://github.com/openzipkin/zipkin/pull/1261\nWe are in the process of reworking the cassandra model, too (for cassandra\n3.9+) https://github.com/openzipkin/zipkin/pull/1252\nIn the next months, we'll do more dilligence benchmarking both\nelasticsearch and cassandra, but we don't expect either to be a bad choice.\n. when a zipkin server is started, it will by default create the schema\nassociated with UDTs on first usage. This is done programmatically in java.\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-storage/cassandra3/src/main/java/zipkin/storage/cassandra3/DefaultSessionFactory.java#L86\n. not saying you'll love this option, but you can bring up the zipkin server,\nhit its health check, then bring it down.\nex. localhost:9411/health\n^^ will create the keyspace and the whole thing won't take too long\n(seconds)\n. cc @openzipkin/cassandra for other opinions on eagerly creating cassandra3 keyspace w/o java. in zipkin v1 we have to have at least one annotation or binary annotation as that's where the servicename is sourced from. If you could help with model v2 support we could get it in much quicker.. lemme know https://github.com/openzipkin/zipkin/issues/939#issuecomment-264076909. ps (does this literally crash or just doesn't show up?). I was going to ask. Seems this issue wss opened on the wrong repository as\nthe new UI code is not here.\nMoreover, we are looking to renovate the \"old UI\" because no one is\ncontributing to the angular one. If you know anyone who can help, do\nmention.\nOn 4 May 2017 12:19 am, \"Bart van Kleef\" notifications@github.com wrote:\n\n@basvanbeek https://github.com/basvanbeek thanks a lot for your\nfeedback. We will look into it.\nIMHO it's still a bad thing that the new Zipkin UI crashes (only\nservices are displayed in \"Service\" dropdown but no traces are shown while\nthey are visualized in old UI) when annotations is missing in JSON\nresponse. Especially because old UI can handle this.\nhttps://camo.githubusercontent.com/cdf93e1e6377f9d5892996e07f3a7b43d5a72934/68747470733a2f2f7075752e73682f76456d4b6d2f633638653661363233632e706e67\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1278#issuecomment-298961247,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD618RewGxvR9WIWGwZjtrAdGwJ4qjCks5r2KkZgaJpZM4J4ZR4\n.\n. I was going to ask. Seems this issue wss opened on the wrong repository as\nthe new UI code is not here.\n\nMoreover, we are looking to renovate the \"old UI\" because no one is\ncontributing to the angular one. If you know anyone who can help, do\nmention.\nOn 4 May 2017 12:19 am, \"Bart van Kleef\" notifications@github.com wrote:\n\n@basvanbeek https://github.com/basvanbeek thanks a lot for your\nfeedback. We will look into it.\nIMHO it's still a bad thing that the new Zipkin UI crashes (only\nservices are displayed in \"Service\" dropdown but no traces are shown while\nthey are visualized in old UI) when annotations is missing in JSON\nresponse. Especially because old UI can handle this.\nhttps://camo.githubusercontent.com/cdf93e1e6377f9d5892996e07f3a7b43d5a72934/68747470733a2f2f7075752e73682f76456d4b6d2f633638653661363233632e706e67\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1278#issuecomment-298961247,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD618RewGxvR9WIWGwZjtrAdGwJ4qjCks5r2KkZgaJpZM4J4ZR4\n.\n. This issue was moved to openzipkin/zipkin-ui#40. This issue was moved to openzipkin/zipkin-ui#40. adding help-wanted as I would like this, too, but don't know how to make it happen.\n. @joel-airspring thx.. for laziness it is easier to review code on a pull request than clicking on commits. Can you raise a PR with those 2?. @joel-airspring thx.. for laziness it is easier to review code on a pull request than clicking on commits. Can you raise a PR with those 2?. Hi, Roger.\n\nCan you click the JSON button and add that to the issue? It will help us\nfigure out what's up.\n-A\n. nit: the service logging \"sr\", \"ss\" is called \"client\". That means \"client\" is a server that received a request, and sent a response. I think that's probably wrong.\n. the middle span 48c898fd24ab75a4 is very strange compared to its parent. It is unlikely a server can receive a request and send an outbound one with no overhead w/in the same microsecond. if it can.. I want one!\n. so far.. I think there is a bug here. the span looks valid to me (even if unrealistic for a few spots, looks ok). I'm going to dig deeper into why the parent is offset.\n. ahh I think I found it. The timestamp is wrong on the second span.\n. Span.timestamp is when the span began. This says that the first annotation (\"cs\") was written 11ms before the span began! You'll notice if you set the timestamp to the minimum, it renders properly\n{\n    \"traceId\": \"48c1b3160196c098\",\n    \"id\": \"48c898fd24ab75a4\",\n    \"name\": \"get /customers\",\n    \"parentId\": \"48c1b3160196c098\",\n    \"timestamp\": 1473400356318643,\n    \"duration\": 199117,\n    \"annotations\": [\n      {\n        \"timestamp\": 1473400356307624,\n. in general, span.timestamp and span.duration should be added when the host is authoritative for that span, ie. they started it.\nFor example, if you start a root span, then setting timestamp, duration is a good idea. It is also a good idea when you start a client span. However, if you are a server receiving a span that was started by a client, setting Span.timestamp, duration is likely invalid.\n. added docs here, which should hopefully help others with the same problem https://github.com/openzipkin/openzipkin.github.io/pull/46\n. @rogeralsing thanks for looking into this. I'll check the logic on the server-side to see if it could have caused this.\n. I believe it was a server problem in this case. https://github.com/openzipkin/zipkin/issues/1281\n. Can you try with zipkin 1.10? seems to calculate properly when instrumentation don't set timestamp/duration. Please re-open if it is a problem, still.\nbash\n$ curl -vs 192.168.99.100:9411/api/v1/spans -H'Content-type: application/json' -H 'Expect:' -d '[\n  {\n    \"traceId\": \"48c1b3160196c098\",\n    \"id\": \"48c1b3160196c098\",\n    \"name\": \"main\",\n    \"annotations\": [\n      {\n        \"timestamp\": 1473400356307624,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"client\",\n          \"ipv4\": \"12.123.12.169\",\n          \"port\": 80\n        }\n      },\n      {\n        \"timestamp\": 1473400356506741,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"client\",\n          \"ipv4\": \"12.123.12.169\",\n          \"port\": 80\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"48c1b3160196c098\",\n    \"id\": \"48c898fd24ab75a4\",\n    \"name\": \"get /customers\",\n    \"parentId\": \"48c1b3160196c098\",\n    \"annotations\": [\n      {\n        \"timestamp\": 1473400356307624,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"client\",\n          \"ipv4\": \"12.123.12.169\",\n          \"port\": 80\n        }\n      },\n      {\n        \"timestamp\": 1473400356318643,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473400356493766,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473400356506741,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"client\",\n          \"ipv4\": \"12.123.12.169\",\n          \"port\": 80\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"Request\",\n        \"value\": \"some payload\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"key\": \"Result\",\n        \"value\": \"some result\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"48c1b3160196c098\",\n    \"id\": \"406faffcd864013d\",\n    \"name\": \"select orders\",\n    \"parentId\": \"48c898fd24ab75a4\",\n    \"annotations\": [\n      {\n        \"timestamp\": 1473400356441667,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473400356452666,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"orderdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473400356483168,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"orderdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473400356493766,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"sql.query\",\n        \"value\": \"select * from Orders where OrderId = 123\",\n        \"endpoint\": {\n          \"serviceName\": \"orderdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"48c1b3160196c098\",\n    \"id\": \"46320d346edc5a84\",\n    \"name\": \"select customers\",\n    \"parentId\": \"48c898fd24ab75a4\",\n    \"annotations\": [\n      {\n        \"timestamp\": 1473400356318643,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473400356329640,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"customerdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473400356430668,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"customerdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473400356441667,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"sql.query\",\n        \"value\": \"select * from Customers where CustomerId = 123\",\n        \"endpoint\": {\n          \"serviceName\": \"customerdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      }\n    ]\n  }\n]'\n. Whew.. thanks for checking back!\nOn 10 Sep 2016 23:56, \"Roger Johansson\" notifications@github.com wrote:\n\nIt looks correct now, thanks!\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1280#issuecomment-246119439,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6183FqN2Aay5GdzLEERqBf2cNwkf0ks5qotMPgaJpZM4J4hpJ\n.\n. PS the impact to this is that if instrumentation projects don't update, the duration query won't work. This is a better problem than changing spans to have the wrong timestamp, especially as instrumentation have had 10 months time to start updating.\n. this is crappier than I thought since when spans don't set timestamp, and we don't derive one, we could end up not indexing them at all. Dirty as it is, we may have to use an imperfect heuristic until instrumentation projects log span.timestamp\n. fix on the way\n. here's the fix https://github.com/openzipkin/zipkin/pull/1282\n. yep\n\nOn Fri, Sep 9, 2016 at 7:37 PM, Roger Johansson notifications@github.com\nwrote:\n\nIf there is a \"cs\", use that (because client always is authoritative)\nDoes this mean that if client sends a request at t0, and the server starts\nreceiving at t100, then the span will be rendered from t0?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1282#issuecomment-245889615,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61-r4eR_oyQBBm6FQtIaKvburwHqTks5qoUTcgaJpZM4J48Jf\n.\n. one broken test.. looking into it.\n. ok I think I got the fix now.\n. alright I'll plan on merging this and cutting a release tomorrow morning my time (+~12hrs) unless someone says don't.\n. @openzipkin/cassandra I applaud the efforts, but this feature has been non-stop support, and most people can't use it. Let's move on and put efforts behind cassandra3 instead?\n. merging due to the history of this issue, comments here and also an out-of-band thumbs up from @michaelsembwever a few days ago.\n. I was wondering about why dots dont show up for core annotations :) thanks,\neirik! (I will push a comment commit)\n. ps there are some related annotations that are less common you'd want to consider. For example, wire send (\"ws\") which is there to help show actual network time. Here's an example to consider.\n\njson\n[\n  {\n    \"traceId\": \"cfe7bb9e10d37dac\",\n    \"name\": \"get\",\n    \"id\": \"cfe7bb9e10d37dac\",\n    \"timestamp\": 1473723216351000,\n    \"duration\": 5000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1473723216351000,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 8081\n        }\n      },\n      {\n        \"timestamp\": 1473723216356000,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 8081\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"ca\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 59472\n        }\n      },\n      {\n        \"key\": \"http.uri\",\n        \"value\": \"/\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      },\n      {\n        \"key\": \"sa\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 8081\n        }\n      },\n      {\n        \"key\": \"srv/finagle.version\",\n        \"value\": \"6.38.0\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"cfe7bb9e10d37dac\",\n    \"name\": \"get\",\n    \"id\": \"91f864f045d8a648\",\n    \"parentId\": \"cfe7bb9e10d37dac\",\n    \"timestamp\": 1473723216352000,\n    \"duration\": 3000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1473723216352000,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      },\n      {\n        \"timestamp\": 1473723216352000,\n        \"value\": \"ws\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      },\n      {\n        \"timestamp\": 1473723216353000,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"backend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473723216354000,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"backend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473723216355000,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      },\n      {\n        \"timestamp\": 1473723216355000,\n        \"value\": \"wr\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"ca\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"backend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 59473\n        }\n      },\n      {\n        \"key\": \"ca\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 59473\n        }\n      },\n      {\n        \"key\": \"clnt/finagle.version\",\n        \"value\": \"6.38.0\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      },\n      {\n        \"key\": \"http.uri\",\n        \"value\": \"/api\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      },\n      {\n        \"key\": \"http.uri\",\n        \"value\": \"/api\",\n        \"endpoint\": {\n          \"serviceName\": \"backend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      },\n      {\n        \"key\": \"sa\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"key\": \"sa\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"backend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"key\": \"srv/finagle.version\",\n        \"value\": \"6.38.0\",\n        \"endpoint\": {\n          \"serviceName\": \"backend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      }\n    ]\n  }\n]\n. ps there are some related annotations that are less common you'd want to consider. For example, wire send (\"ws\") which is there to help show actual network time. Here's an example to consider.\njson\n[\n  {\n    \"traceId\": \"cfe7bb9e10d37dac\",\n    \"name\": \"get\",\n    \"id\": \"cfe7bb9e10d37dac\",\n    \"timestamp\": 1473723216351000,\n    \"duration\": 5000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1473723216351000,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 8081\n        }\n      },\n      {\n        \"timestamp\": 1473723216356000,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 8081\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"ca\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 59472\n        }\n      },\n      {\n        \"key\": \"http.uri\",\n        \"value\": \"/\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      },\n      {\n        \"key\": \"sa\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 8081\n        }\n      },\n      {\n        \"key\": \"srv/finagle.version\",\n        \"value\": \"6.38.0\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"cfe7bb9e10d37dac\",\n    \"name\": \"get\",\n    \"id\": \"91f864f045d8a648\",\n    \"parentId\": \"cfe7bb9e10d37dac\",\n    \"timestamp\": 1473723216352000,\n    \"duration\": 3000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1473723216352000,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      },\n      {\n        \"timestamp\": 1473723216352000,\n        \"value\": \"ws\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      },\n      {\n        \"timestamp\": 1473723216353000,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"backend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473723216354000,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"backend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473723216355000,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      },\n      {\n        \"timestamp\": 1473723216355000,\n        \"value\": \"wr\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"ca\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"backend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 59473\n        }\n      },\n      {\n        \"key\": \"ca\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 59473\n        }\n      },\n      {\n        \"key\": \"clnt/finagle.version\",\n        \"value\": \"6.38.0\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      },\n      {\n        \"key\": \"http.uri\",\n        \"value\": \"/api\",\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      },\n      {\n        \"key\": \"http.uri\",\n        \"value\": \"/api\",\n        \"endpoint\": {\n          \"serviceName\": \"backend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      },\n      {\n        \"key\": \"sa\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"frontend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"key\": \"sa\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"backend\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"key\": \"srv/finagle.version\",\n        \"value\": \"6.38.0\",\n        \"endpoint\": {\n          \"serviceName\": \"backend\",\n          \"ipv4\": \"127.0.0.1\"\n        }\n      }\n    ]\n  }\n]\n. Hehe nice commentary. I suppose sleepABit is a chummier alternative to 5\ntries, except in this case it probably does flake more.\nOn 11 Sep 2016 17:43, \"Zolt\u00e1n Nagy\" notifications@github.com wrote:\n\nSmells like a race condition to me (never dreamed I'd see sleepABit in\nthe public API of a library. And it doesn't even specify how long \"a bit\"\nis :D). Since we run the same version and config of ZK on CircleCI as we do\non Travis, first guess is less power in the build node. Or maybe I'm\nmissing something, and we know \"a bit\" must be enough (maybe waits for some\nlocks or something)?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1285#issuecomment-246171430,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6148fDiHD94Hnv4_GeEoivhOilTsvks5qo81dgaJpZM4J5ln1\n.\n. this is a bit of a hammer, but seems to work (until it doesn't)\n\nhttps://github.com/openzipkin/zipkin/pull/1340/commits/101fa042b505d1efb089b6ce85ad292839bf3d8d\n. this is a bit of a hammer, but seems to work (until it doesn't)\nhttps://github.com/openzipkin/zipkin/pull/1340/commits/101fa042b505d1efb089b6ce85ad292839bf3d8d\n. cc @anuraaga I think this is squashed.. hitting a fresh ES host tons of times per second recreates the problem, and I can't repro it anymore now :)\nThe root issue was a race condition of who indexed first, a boolean or string value binary annotation.\n. cc @anuraaga\n. Right now, this is a server-centric (and rpc centric) diagram, so you are\nright to say that client entrypoints aren't listed.\nIn your case, do you expect the client-originated traces to be instrumented\n(ex trace starts with \"cs\") or only in this example?\nA common problem raised in client-orginated traces is often they arent\naware of their service name. Can you describe a little about your arch\nwhere client names can be trusted? It would help to capture the use case,\nand this will hell me or others make sure this is implemented properly.\nOn 10 Sep 2016 23:49, \"Roger Johansson\" notifications@github.com wrote:\n\nGiven the following json, I would have expected to see that the client3\nis a consumer of fakewebapi.\nWhich it doesnt, adding a SR SS span at the root changes that, but it\nseems odd that consumers are not listed?\n[\n  {\n    \"traceId\": \"48485a3953bb6124\",\n    \"id\": \"48485a3953bb6124\",\n    \"name\": \"main\",\n    \"timestamp\": 1473522129944953,\n    \"duration\": 209000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1473522129944953,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"client3\",\n          \"ipv4\": \"12.123.12.169\",\n          \"port\": 80\n        }\n      },\n      {\n        \"timestamp\": 1473522129956953,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473522130142954,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473522130153953,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"client3\",\n          \"ipv4\": \"12.123.12.169\",\n          \"port\": 80\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"Request\",\n        \"value\": \"some payload\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"key\": \"Result\",\n        \"value\": \"some result\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"48485a3953bb6124\",\n    \"id\": \"42e1e27066118385\",\n    \"name\": \"select customers\",\n    \"parentId\": \"48485a3953bb6124\",\n    \"timestamp\": 1473522129958968,\n    \"duration\": 130989,\n    \"annotations\": [\n      {\n        \"timestamp\": 1473522129958968,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473522129969954,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"customerdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473522130070957,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"customerdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473522130089957,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"sql.query\",\n        \"value\": \"select * from Customers where CustomerId = 123\",\n        \"endpoint\": {\n          \"serviceName\": \"customerdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"48485a3953bb6124\",\n    \"id\": \"463ac35c9f6413ad\",\n    \"name\": \"select orders\",\n    \"parentId\": \"48485a3953bb6124\",\n    \"timestamp\": 1473522130089957,\n    \"duration\": 52997,\n    \"annotations\": [\n      {\n        \"timestamp\": 1473522130089957,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473522130100955,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"orderdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473522130131954,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"orderdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473522130142954,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"sql.query\",\n        \"value\": \"select * from Orders where OrderId = 123\",\n        \"endpoint\": {\n          \"serviceName\": \"orderdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      }\n    ]\n  }\n]\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1288, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD615oJs9yYZm2a3q98yc-ArhT5c9PLks5qotGagaJpZM4J5wFe\n.\n. actually I think we are addressing this already with the \"ca\" annotation, but not the \"cs\". I'll have a go at this. thanks for the json.\n. on it!\n. here's the fix! https://github.com/openzipkin/zipkin/pull/1293\n. going out as release-1.11.0\n\nI'll update zipkin-dependencies as soon as ^^ is out. Thanks for the report!\n. verified all the way to docker. Lemme know if you have any trouble\n. Will take a look.\nOn 11 Sep 2016 20:00, \"mck\" notifications@github.com wrote:\n\nThis looks like a good idea. But I don't have much experience on how\nannotation search works (and is typically used) in the UI layer.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1289#issuecomment-246176647,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD615aZhGABaHeonKxD15ERmCbtLCYpks5qo-1xgaJpZM4J5_ci\n.\n. I'd really like to not use defensive list add, and instead go back to the set approach to ensure unique keys. it is easier to not have to think about pairing if-add, and instead just assert a set add.\n\nWRT the change itself.. honestly, I'm pleasantly surprised it works! Either our SpanStoreTest isn't good enough to show this doesn't work, or it works and it is a far more elegant index. Regardless, I'm inclined to go with it until someone screams.\n. yeah this is a good change.. I'll timebox an hour to see if I can break it (ex if we are simply missing a test). otherwise, will merge :)\n. sorry to report, but we were missing a test... will raise a PR with it later.\n. here's the missing test, in case it helps you between now and when I fix the in-memory storage which this fails on (old cassandra passes) https://github.com/openzipkin/zipkin/pull/1292\n. so good news and bad news.\nGood news is that we didn't need to index binaryAnnotation.key independently, so in any case, writes should be less (only one per annotation/binaryAnnotation+service)\nBad news is that the search is imprecise without the service name. I've changed delimiters of binary annotations to ';' to avoid a LIKE clash with annotations (which are delimited with ':')\n. gonna close this as after looking closely and adding a missing test, I don't think it will work as-is. Feel free to re-open if this is not the case.\n. gonna close this as after looking closely and adding a missing test, I don't think it will work as-is. Feel free to re-open if this is not the case.\n. Thanks for putting this together. It made me think! long thoughts about the thing in general\nClient-tracing in C* is not yet transparent\nRegardless of how we coordinate, we need to coordinate. Ex you introduce a special binary annotation, before we also had to do work to control names (especially as datastax nixed the ability to have client-side metadata). I needed to use a proxy to make it drop-in (without collaboration code from the caller). This is a similar problem in Kafka, except easier in Cassandra as at least there's a server-hook.\nI should have written tests on server tracing\nA lot of things had to happen before the storage layer could be decoupled from the trace layer explicitly, with short, low-cardinality span names and without async bugs. It took more than a couple days to get this working. When thinking about tests, I realized it would likely take a few days to create them (as it needs to read back and forth to figure out things). I special-cased with flags, smaller entry scope instead, and zero coordination instead of investing more time to make this a real tracer. If I wrote tests, we'd know that this is a drop-in or not. Now, we don't.. at least not without eye-balling spans and running manually.\nEnd notes\nRegardless of which approach is used, it is probably mutually obvious that the code involved is probably better off as a library, particularly when it aims to act generically (ex work on any Statement). If such a library could drive changes in the datastax api to make it more transparent, far more than self-tracing would benefit.\n. gonna close this as from your description you seemed to be most interested in feedback. I'd like to keep the PR queue clean as possible. If I closed this too early, please re-open!\n. gonna close this as from your description you seemed to be most interested in feedback. I'd like to keep the PR queue clean as possible. If I closed this too early, please re-open!\n. cc @eirslett \n. Thanks to @rogeralsing for capturing json. It makes things easy to solve offline.\nbash\n$ curl -s localhost:9411/api/v1/spans -H'Content-Type: application/json' -d '\n[\n  {\n    \"traceId\": \"48485a3953bb6124\",\n    \"id\": \"48485a3953bb6124\",\n    \"name\": \"main\",\n    \"timestamp\": 1473522129944953,\n    \"duration\": 209000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1473522129944953,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"client3\",\n          \"ipv4\": \"12.123.12.169\",\n          \"port\": 80\n        }\n      },\n      {\n        \"timestamp\": 1473522129956953,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473522130142954,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473522130153953,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"client3\",\n          \"ipv4\": \"12.123.12.169\",\n          \"port\": 80\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"Request\",\n        \"value\": \"some payload\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"key\": \"Result\",\n        \"value\": \"some result\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"48485a3953bb6124\",\n    \"id\": \"42e1e27066118385\",\n    \"name\": \"select customers\",\n    \"parentId\": \"48485a3953bb6124\",\n    \"timestamp\": 1473522129958968,\n    \"duration\": 130989,\n    \"annotations\": [\n      {\n        \"timestamp\": 1473522129958968,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473522129969954,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"customerdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473522130070957,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"customerdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473522130089957,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"sql.query\",\n        \"value\": \"select * from Customers where CustomerId = 123\",\n        \"endpoint\": {\n          \"serviceName\": \"customerdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"48485a3953bb6124\",\n    \"id\": \"463ac35c9f6413ad\",\n    \"name\": \"select orders\",\n    \"parentId\": \"48485a3953bb6124\",\n    \"timestamp\": 1473522130089957,\n    \"duration\": 52997,\n    \"annotations\": [\n      {\n        \"timestamp\": 1473522130089957,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473522130100955,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"orderdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473522130131954,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"orderdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473522130142954,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"sql.query\",\n        \"value\": \"select * from Orders where OrderId = 123\",\n        \"endpoint\": {\n          \"serviceName\": \"orderdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      }\n    ]\n  }\n]'\n\n. note: this will need a slight adjustment on zipkin-dependencies (spark job), too. I'll follow-up post-release\n. implemented storage flush and wired in a delegate for span store.\nNext step: incrementally implement SpanStore, probably easiest ones first :)\n. implemented storage flush and wired in a delegate for span store.\nNext step: incrementally implement SpanStore, probably easiest ones first :)\n. using a lightly wrapped apachehc to implement a json api is too much work. I'm switching to okhttp.\n. using a lightly wrapped apachehc to implement a json api is too much work. I'm switching to okhttp.\n. switched to OkHttp+Moshi and implemented the first read api: getRawTrace\n. switched to OkHttp+Moshi and implemented the first read api: getRawTrace\n. obviated by #1302\n. obviated by #1302\n. cc @anuraaga \n. cc @anuraaga \n. cc @yurishkuro fyi @dominikh\n. cc @yurishkuro fyi @dominikh\n. cc @gena01\n. cc @gena01\n. Most libraries now accommodate 128bit trace ids. Next step is to make it possible to store them\n. Most libraries now accommodate 128bit trace ids. Next step is to make it possible to store them\n. @openzipkin/elasticsearch we've a bit of a crossroads wrt how to handle json. Right now, we do an equals query on .traceId. There's a prefix query, but no suffix one. What we want for mixed-deployments is right-most 16characters to equal an input. Any ideas on the index template or otherwise options for this?\n. @openzipkin/elasticsearch we've a bit of a crossroads wrt how to handle json. Right now, we do an equals query on .traceId. There's a prefix query, but no suffix one. What we want for mixed-deployments is right-most 16characters to equal an input. Any ideas on the index template or otherwise options for this?\n. on elasticsearch.. I think we could get by in the interim with a tokenizer that only captures lower bits, then use the existing equals terms query. Something like this..\njson\n        \"traceId_tokenizer\": {\n          \"type\": \"pattern\",\n          \"pattern\": \"([0-9a-f]{1,16})$\",\n          \"group\": 1\n        }\n. on elasticsearch.. I think we could get by in the interim with a tokenizer that only captures lower bits, then use the existing equals terms query. Something like this..\njson\n        \"traceId_tokenizer\": {\n          \"type\": \"pattern\",\n          \"pattern\": \"([0-9a-f]{1,16})$\",\n          \"group\": 1\n        }\n. Added much more detail to the plan and \"wave 2\" implementation https://github.com/openzipkin/zipkin/pull/1353\n. Added much more detail to the plan and \"wave 2\" implementation https://github.com/openzipkin/zipkin/pull/1353\n. > Are we doing this on the read path or the write path?\n\nAny settings like this would be in the index template, which would be on\nthe write path.\n. > Are we doing this on the read path or the write path?\nAny settings like this would be in the index template, which would be on\nthe write path.\n. here's the final cut on elasticsearch. This will create 2 tokens for trace ids who are 128-bit and one if 64-bit. This allows existing tools which only support 64-bit ids (like SpanStore) to work, without limiting future tools from doing exact match on 128-bit. The cost is 2x the trace id tokens in sites who are 100% 128-bit. Since this is on a daily index basis, if the latter becomes a problem, one can either revise the index template manually, or we can as a general course. However, I suspect mixed-mode will exist for at least a year in many sites.\n\nEx this is the snipped of the analyzer in #1353 which supports mixed mode via custom tokenization. When the id is larger that 16 characters, it will end up creating 2 tokens (one for the original text and one for the truncated one). When the id is not larger than 16 characters, only one token is created for search purposes.\njson\n--snip--\n        \"traceId_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"keyword\",\n          \"filter\": \"traceId_filter\"\n        }\n      },\n      \"filter\": {\n        \"traceId_filter\": {\n          \"type\": \"pattern_capture\",\n          \"patterns\": [\"([0-9a-f]{1,16})$\"],\n          \"preserve_original\": true\n        }\n      }\n--snip--\n. here's the final cut on elasticsearch. This will create 2 tokens for trace ids who are 128-bit and one if 64-bit. This allows existing tools which only support 64-bit ids (like SpanStore) to work, without limiting future tools from doing exact match on 128-bit. The cost is 2x the trace id tokens in sites who are 100% 128-bit. Since this is on a daily index basis, if the latter becomes a problem, one can either revise the index template manually, or we can as a general course. However, I suspect mixed-mode will exist for at least a year in many sites.\nEx this is the snipped of the analyzer in #1353 which supports mixed mode via custom tokenization. When the id is larger that 16 characters, it will end up creating 2 tokens (one for the original text and one for the truncated one). When the id is not larger than 16 characters, only one token is created for search purposes.\njson\n--snip--\n        \"traceId_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"keyword\",\n          \"filter\": \"traceId_filter\"\n        }\n      },\n      \"filter\": {\n        \"traceId_filter\": {\n          \"type\": \"pattern_capture\",\n          \"patterns\": [\"([0-9a-f]{1,16})$\"],\n          \"preserve_original\": true\n        }\n      }\n--snip--\n. added a couple notes, notably instrumentation will eventually need a flag to generate 128-bit traces (which implies sampling on 128-bits vs 64). Also, this implies adjusting the collector sampler so that it is consistent on 128bit as opposed to just 64. cc @basvanbeek\n. added issue https://github.com/twitter/finagle/issues/564 to track finagle\n. added issue https://github.com/twitter/finagle/issues/564 to track finagle\n. @basvanbeek I agree. removed updating sampling to 128-bit from the TODO section\n. @basvanbeek I agree. removed updating sampling to 128-bit from the TODO section\n. working on the final lap in #1385 going under the assumption that we add an overloaded method of SpanStore.getTrace(long traceIdHigh, long traceId). We also need a parameter to tell control group-by, currently named groupByTraceIdHigh. None of this impacts the rest api or changes already made to codec.\nIf anyone has any style or naming guidance for these changes, do speak up!\n. Here's the other supporting change, which is a builder for StorageComponent. This is needed to move the \"compatibility mode\" parameter to where it can be generically assigned. This is important because in most storage engines, there's an additional cost to indexing on half of the 128-bit ID, so it should be possible to turn this off\n``` java\ninterface Builder {\n/**\n * Zipkin historically had 64-bit {@link Span#traceId trace IDs}, but it now supports 128-bit\n * trace IDs via {@link Span#traceIdHigh}, or its 32-character hex representation.\n *\n * <p>This setting allows you to look up traces who have 128-bit trace ids by the lower-64 bits\n * using {@link SpanStore#getTrace(long)}. In most implementations, this implies a second index\n * which can be more expensive in terms or storage or memory than only supporting 128-bit\n * lookups via {@link SpanStore#getTrace(long, long)}\n *\n * <p>This should be enabled until all instrumentation report 128-bit trace IDs consistently,\n * and {@link SpanStore#getTrace(long)} is no longer in use.\n */\nBuilder mixedTraceIdLength(boolean mixedTraceIdLength);\n\nStorageComponent build();\n\n}\n```\n. I've updated the name and the description of this setting. Please review as soon as you can, as the final wave of this effort is nearly complete.\nParticularly, @michaelsembwever @mansu @basvanbeek and @shakuzen please review the below if you can, and let me know if there's anything about the name or description that seems confusing.\n\nStorageComponent.Builder.upgradingTo128BitTraceId(boolean upgradingTo128BitTraceId)\nZipkin supports 64 and 128-bit trace identifiers, typically serialized as 16 or 32 character hex strings. When true, this setting only considers the low 64-bits (right-most 16 characters) of a trace ID when grouping or retrieving traces. This should be enabled when transitioning from 64 to 128-bit trace IDs and disabled after the transition is complete.\nDetails\nZipkin historically had 64-bit trace IDs, but it now supports 128-bit trace IDs via Span.traceIdHigh, or its 32-character hex representation. While instrumentation update to propagate 128-bit IDs, it can be ambiguous whether a 64-bit trace ID was sent intentionally, or as an accident of truncation. This setting allows Zipkin to be usable until application instrumentation are upgraded to support 128-bit trace IDs.\nHere are a few trace IDs the help explain this setting.\n- Trace ID A: 463ac35c9f6413ad48485a3953bb6124\n- Trace ID B: 48485a3953bb6124\n- Trace ID C: 463ac35c9f6413adf1a48a8cff464e0e\n- Trace ID D: 463ac35c9f6413ad\nIn the above example, Trace ID A and Trace ID B might mean they are in the same trace, since the lower-64 bits of the IDs are the same. This could happen if a server A created the trace and propagated it to server B which ran an older tracing library. Server B could have truncated the trace ID to lower-64 bits. When upgradingTo128BitTraceId == true, spans matching either trace ID A or B would be returned in the same trace when searching by ID A or B. Spans with trace ID C or D wouldn't be when searching by ID A or B because trace IDs C and D don't share lower 64-bits (right-most 16 characters) with trace IDs A or B.\nIt is also possible that all servers are capable of handling 128-bit trace identifiers, but are configured to only send 64-bit ones. In this case, if upgradingTo128BitTraceId == true trace ID A and B would clash and be put into the same trace, causing confusion. Moreover, there is overhead associated with indexing spans both by 64 and 128-bit trace IDs. When a site has finished upgrading to 128-bit trace IDs, they should disable this setting.\nSee https://github.com/openzipkin/b3-propagation/issues/6 for the status of known open source libraries on 128-bit trace identifiers.\n. I've updated the name and the description of this setting. Please review as soon as you can, as the final wave of this effort is nearly complete.\nParticularly, @michaelsembwever @mansu @basvanbeek and @shakuzen please review the below if you can, and let me know if there's anything about the name or description that seems confusing.\n\nStorageComponent.Builder.upgradingTo128BitTraceId(boolean upgradingTo128BitTraceId)\nZipkin supports 64 and 128-bit trace identifiers, typically serialized as 16 or 32 character hex strings. When true, this setting only considers the low 64-bits (right-most 16 characters) of a trace ID when grouping or retrieving traces. This should be enabled when transitioning from 64 to 128-bit trace IDs and disabled after the transition is complete.\nDetails\nZipkin historically had 64-bit trace IDs, but it now supports 128-bit trace IDs via Span.traceIdHigh, or its 32-character hex representation. While instrumentation update to propagate 128-bit IDs, it can be ambiguous whether a 64-bit trace ID was sent intentionally, or as an accident of truncation. This setting allows Zipkin to be usable until application instrumentation are upgraded to support 128-bit trace IDs.\nHere are a few trace IDs the help explain this setting.\n- Trace ID A: 463ac35c9f6413ad48485a3953bb6124\n- Trace ID B: 48485a3953bb6124\n- Trace ID C: 463ac35c9f6413adf1a48a8cff464e0e\n- Trace ID D: 463ac35c9f6413ad\nIn the above example, Trace ID A and Trace ID B might mean they are in the same trace, since the lower-64 bits of the IDs are the same. This could happen if a server A created the trace and propagated it to server B which ran an older tracing library. Server B could have truncated the trace ID to lower-64 bits. When upgradingTo128BitTraceId == true, spans matching either trace ID A or B would be returned in the same trace when searching by ID A or B. Spans with trace ID C or D wouldn't be when searching by ID A or B because trace IDs C and D don't share lower 64-bits (right-most 16 characters) with trace IDs A or B.\nIt is also possible that all servers are capable of handling 128-bit trace identifiers, but are configured to only send 64-bit ones. In this case, if upgradingTo128BitTraceId == true trace ID A and B would clash and be put into the same trace, causing confusion. Moreover, there is overhead associated with indexing spans both by 64 and 128-bit trace IDs. When a site has finished upgrading to 128-bit trace IDs, they should disable this setting.\nSee https://github.com/openzipkin/b3-propagation/issues/6 for the status of known open source libraries on 128-bit trace identifiers.\n. one more bikeshed concern is the name of the parameter being described, particularly as we need to describe it as configuration potentially to non-english speakers.\n- upgradingTo128BitTraceId - describes well the act, but it is long and is lengthy converted to snake or upper underscore case\n- stepDownTraceIds - implies \"stepping down\" from 128 to 64-bit, which is a bit jargony. Ok to encode as an environment variable.\n- mixedTraceIdLength - discusses the problem itself, but not the context. Ex mixed length ids are fine regardless, provided they are in separate traces. Easy to encode as an environment variable.\n- ignoreTraceIdHigh - says what's happening, but not why or when. Easy to encode as an environment variable.\ncc @abesto in case you have thoughts or other ideas..\n. one more bikeshed concern is the name of the parameter being described, particularly as we need to describe it as configuration potentially to non-english speakers.\n- upgradingTo128BitTraceId - describes well the act, but it is long and is lengthy converted to snake or upper underscore case\n- stepDownTraceIds - implies \"stepping down\" from 128 to 64-bit, which is a bit jargony. Ok to encode as an environment variable.\n- mixedTraceIdLength - discusses the problem itself, but not the context. Ex mixed length ids are fine regardless, provided they are in separate traces. Easy to encode as an environment variable.\n- ignoreTraceIdHigh - says what's happening, but not why or when. Easy to encode as an environment variable.\ncc @abesto in case you have thoughts or other ideas..\n. strictTraceId is another option\n. strictTraceId is another option\n. okie dokie \nzipkin.storage.strict-trace-id as the config property, mapped to..\nSTRICT_TRACE_ID as the env variable (because we usually provide unscoped ones to make run statements less heinous).\ndefault will be true as that will be cheaper.\nunless I hear otherwise in the next day, these will be so!\n. okie dokie \nzipkin.storage.strict-trace-id as the config property, mapped to..\nSTRICT_TRACE_ID as the env variable (because we usually provide unscoped ones to make run statements less heinous).\ndefault will be true as that will be cheaper.\nunless I hear otherwise in the next day, these will be so!\n. going out as 1.15!\n. going out as 1.15!\n. Span detail view could show this I guess\n. Span detail view could show this I guess\n. \"67e1383c1e7f\" isn't an IP address. what is sending this?\nOn Thu, Sep 15, 2016 at 8:40 PM, Pavol Loffay notifications@github.com\nwrote:\n\nEnv:\nZipkin master/head\nopenjdk version \"1.8.0_102\"\nOpenJDK Runtime Environment (build 1.8.0_102-b14)\nOpenJDK 64-Bit Server VM (build 25.102-b14, mixed mode)\nException:\n2016-09-15 14:19:49.649  WARN 18326 --- [nio-9411-exec-2] zipkin.server.ZipkinHttpCollector        : Cannot decode spans due to IllegalArgumentException(For input string: \"67e1383c1e7f\" reading List from json: [{\"name\":\"get\",\"traceId\":\"ff6b08e210acd24b\",\"id\":\"ff6b08e210acd24b\",\"annotations\":[{\"value\":\"sr\",\"timestamp\":1473941989594730,\"endpoint\":{\"ipv4\":\"67e1383c1e7f\",\"port\":9292,\"serviceName\":\"roda\"}},{\"value\":\"ss\",\"timestamp\":1473941989594846,\"endpoint\":{\"ipv4\":\"67e1383c1e7f\",\"port\":9292,\"serviceName\":\"roda\"}}],\"binaryAnnotations\":[{\"key\":\"http.uri\",\"value\":\"/roda/hello\",\"endpoint\":{\"ipv4\":\"67e1383c1e7f\",\"port\":9292,\"serviceName\":\"roda\"}}],\"timestamp\":1473941989594695,\"duration\":158,\"debug\":false}])\njava.lang.IllegalArgumentException: For input string: \"67e1383c1e7f\" reading List from json: [{\"name\":\"get\",\"traceId\":\"ff6b08e210acd24b\",\"id\":\"ff6b08e210acd24b\",\"annotations\":[{\"value\":\"sr\",\"timestamp\":1473941989594730,\"endpoint\":{\"ipv4\":\"67e1383c1e7f\",\"port\":9292,\"serviceName\":\"roda\"}},{\"value\":\"ss\",\"timestamp\":1473941989594846,\"endpoint\":{\"ipv4\":\"67e1383c1e7f\",\"port\":9292,\"serviceName\":\"roda\"}}],\"binaryAnnotations\":[{\"key\":\"http.uri\",\"value\":\"/roda/hello\",\"endpoint\":{\"ipv4\":\"67e1383c1e7f\",\"port\":9292,\"serviceName\":\"roda\"}}],\"timestamp\":1473941989594695,\"duration\":158,\"debug\":false}]\n    at zipkin.internal.JsonCodec.exceptionReading(JsonCodec.java:643) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec.readList(JsonCodec.java:597) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec.readSpans(JsonCodec.java:432) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.collector.Collector.acceptSpans(Collector.java:93) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.server.ZipkinHttpCollector.validateAndStoreSpans(ZipkinHttpCollector.java:89) [classes!/:na]\n    at zipkin.server.ZipkinHttpCollector.uploadSpansJson(ZipkinHttpCollector.java:63) [classes!/:na]\n    at sun.reflect.GeneratedMethodAccessor84.invoke(Unknown Source) ~[na:na]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_102]\n    at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_102]\n    at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:136) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:114) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:963) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:897) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:648) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:729) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:230) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) [tomcat-embed-websocket-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55) [spring-boot-1.4.0.RELEASE.jar!/:1.4.0.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:105) [spring-boot-actuator-1.4.0.RELEASE.jar!/:1.4.0.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:107) [spring-boot-actuator-1.4.0.RELEASE.jar!/:1.4.0.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:198) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:108) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:522) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:140) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:87) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:349) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:1110) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:785) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1425) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_102]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_102]\n    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102]\nCaused by: java.lang.NumberFormatException: For input string: \"67e1383c1e7f\"\n    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[na:1.8.0_102]\n    at java.lang.Integer.parseInt(Integer.java:580) ~[na:1.8.0_102]\n    at java.lang.Integer.parseInt(Integer.java:615) ~[na:1.8.0_102]\n    at zipkin.internal.JsonCodec$1.fromJson(JsonCodec.java:76) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec$1.fromJson(JsonCodec.java:63) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec$2.fromJson(JsonCodec.java:145) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec$2.fromJson(JsonCodec.java:133) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec$4.fromJson(JsonCodec.java:331) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec$4.fromJson(JsonCodec.java:309) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec.readList(JsonCodec.java:592) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    ... 69 common frames omitted\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1301, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61_u98MPI6E5dqdP-q0VIwah1VpkAks5qqTzVgaJpZM4J90dL\n.\n. \"67e1383c1e7f\" isn't an IP address. what is sending this?\n\nOn Thu, Sep 15, 2016 at 8:40 PM, Pavol Loffay notifications@github.com\nwrote:\n\nEnv:\nZipkin master/head\nopenjdk version \"1.8.0_102\"\nOpenJDK Runtime Environment (build 1.8.0_102-b14)\nOpenJDK 64-Bit Server VM (build 25.102-b14, mixed mode)\nException:\n2016-09-15 14:19:49.649  WARN 18326 --- [nio-9411-exec-2] zipkin.server.ZipkinHttpCollector        : Cannot decode spans due to IllegalArgumentException(For input string: \"67e1383c1e7f\" reading List from json: [{\"name\":\"get\",\"traceId\":\"ff6b08e210acd24b\",\"id\":\"ff6b08e210acd24b\",\"annotations\":[{\"value\":\"sr\",\"timestamp\":1473941989594730,\"endpoint\":{\"ipv4\":\"67e1383c1e7f\",\"port\":9292,\"serviceName\":\"roda\"}},{\"value\":\"ss\",\"timestamp\":1473941989594846,\"endpoint\":{\"ipv4\":\"67e1383c1e7f\",\"port\":9292,\"serviceName\":\"roda\"}}],\"binaryAnnotations\":[{\"key\":\"http.uri\",\"value\":\"/roda/hello\",\"endpoint\":{\"ipv4\":\"67e1383c1e7f\",\"port\":9292,\"serviceName\":\"roda\"}}],\"timestamp\":1473941989594695,\"duration\":158,\"debug\":false}])\njava.lang.IllegalArgumentException: For input string: \"67e1383c1e7f\" reading List from json: [{\"name\":\"get\",\"traceId\":\"ff6b08e210acd24b\",\"id\":\"ff6b08e210acd24b\",\"annotations\":[{\"value\":\"sr\",\"timestamp\":1473941989594730,\"endpoint\":{\"ipv4\":\"67e1383c1e7f\",\"port\":9292,\"serviceName\":\"roda\"}},{\"value\":\"ss\",\"timestamp\":1473941989594846,\"endpoint\":{\"ipv4\":\"67e1383c1e7f\",\"port\":9292,\"serviceName\":\"roda\"}}],\"binaryAnnotations\":[{\"key\":\"http.uri\",\"value\":\"/roda/hello\",\"endpoint\":{\"ipv4\":\"67e1383c1e7f\",\"port\":9292,\"serviceName\":\"roda\"}}],\"timestamp\":1473941989594695,\"duration\":158,\"debug\":false}]\n    at zipkin.internal.JsonCodec.exceptionReading(JsonCodec.java:643) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec.readList(JsonCodec.java:597) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec.readSpans(JsonCodec.java:432) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.collector.Collector.acceptSpans(Collector.java:93) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.server.ZipkinHttpCollector.validateAndStoreSpans(ZipkinHttpCollector.java:89) [classes!/:na]\n    at zipkin.server.ZipkinHttpCollector.uploadSpansJson(ZipkinHttpCollector.java:63) [classes!/:na]\n    at sun.reflect.GeneratedMethodAccessor84.invoke(Unknown Source) ~[na:na]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_102]\n    at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_102]\n    at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:136) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:114) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:963) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:897) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:648) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:729) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:230) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) [tomcat-embed-websocket-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55) [spring-boot-1.4.0.RELEASE.jar!/:1.4.0.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:105) [spring-boot-actuator-1.4.0.RELEASE.jar!/:1.4.0.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:107) [spring-boot-actuator-1.4.0.RELEASE.jar!/:1.4.0.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.2.RELEASE.jar!/:4.3.2.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:198) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:108) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:522) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:140) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:87) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:349) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:1110) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:785) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1425) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_102]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_102]\n    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-embed-core-8.5.4.jar!/:8.5.4]\n    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102]\nCaused by: java.lang.NumberFormatException: For input string: \"67e1383c1e7f\"\n    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[na:1.8.0_102]\n    at java.lang.Integer.parseInt(Integer.java:580) ~[na:1.8.0_102]\n    at java.lang.Integer.parseInt(Integer.java:615) ~[na:1.8.0_102]\n    at zipkin.internal.JsonCodec$1.fromJson(JsonCodec.java:76) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec$1.fromJson(JsonCodec.java:63) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec$2.fromJson(JsonCodec.java:145) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec$2.fromJson(JsonCodec.java:133) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec$4.fromJson(JsonCodec.java:331) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec$4.fromJson(JsonCodec.java:309) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    at zipkin.internal.JsonCodec.readList(JsonCodec.java:592) ~[zipkin-1.9.1-SNAPSHOT.jar!/:na]\n    ... 69 common frames omitted\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1301, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61_u98MPI6E5dqdP-q0VIwah1VpkAks5qqTzVgaJpZM4J90dL\n.\n. shouldn't matter, but do you mind trying to explicitly update Brave to 3.11? the json writer was changed in this version, though I've never seen anything like this before.\n\n<dependency>\n    <groupId>io.zipkin.brave</groupId>\n    <artifactId>brave</artifactId>\n    <version>3.11.0</version>\n</dependency>\n. shouldn't matter, but do you mind trying to explicitly update Brave to 3.11? the json writer was changed in this version, though I've never seen anything like this before.\n<dependency>\n    <groupId>io.zipkin.brave</groupId>\n    <artifactId>brave</artifactId>\n    <version>3.11.0</version>\n</dependency>\n. ps not that it should matter, but zipkin-1.9.1-SNAPSHOT isn't head.. 1.11.2-SNAPSHOT is\n. ps not that it should matter, but zipkin-1.9.1-SNAPSHOT isn't head.. 1.11.2-SNAPSHOT is\n. This issue was moved to openzipkin/zipkin-tracer#75\n. holy moly! this is quite a bit of progress!\n. @sethp-jive thanks for this work! I'll stop my work and help yours.\nI think the next step is to modularize. Right now, there are a number of classpath problems we have already, so adding two new dep trees (especially with guava etc), is something we should avoid forcing. Here's the idea.\ncreate elasticsearch-jest (which has the jest dependency)\ncreate elasticsearch-aws (which depends on elasticsearch-jest and aws sdk)\nThis implies exposing Client, probably as InternalElasticsearchClient extending LazyCloseable. The Internal thing implies that it is internal eventhough it is exposed public. place the jest and aws impls in a separate package like zipkin.elasticsearch.jest and zipkin.elasticsearch.aws. These classes that extend InternalElasticsearchClient should have builders that look like other builders, and be an arg to ElasticSearchStorage.builder without any prior knowledge (ex  ElasticSearchStorage has no registry)\nIn those modules, we would have any unit tests they need, but at the very least copies of the DependenciesTest and SpanStoreTest so that they can be tested independently. The AWS variant would look for AWS access key or similar. If these args are present, the AWS variant runs or silently ignores (because its TestGraph is written to). Avoid creating a elasticsearch test jar unless it really really really helps :)\nOnce that's together we can work on auto-configuration (via spring). No magic until then. ex please don't detect anything :) magic is for auto-configuration, storage components should have smart defaults, but no magic.\n. pps please limit use of guava in InternalElasticsearchClient, and if it is really required, limit it to signatures present in 18.0\n. incidentally, aws could probably be done as an autoconfigure module. It doesn't seem to need to affect anything else, so I think we'd have..\nmodify: zipkin-storage/elasticsearch\nrefactored to expose zipkin.elasticsearch.InternalElasticsearchClient and accept it\nadd: zipkin-storage/elasticsearch-jest\ndepends on ^^ and jest, implements  zipkin.elasticsearch.jest.JestElasticsearchClient\nadd: zipkin-autoconfigure/storage-elasticsearch-jest\nmaps spring properties for http endpoints and instantiates JestElasticsearchClient via classes in package zipkin.autoconfigure.storage.elasticsearch.jest\nadd: zipkin-autoconfigure/storage-elasticsearch-aws\nuses AWS sdk to provide overrideable defaults for http endpoint and authentication and instantiates JestElasticsearchClient via classes in package zipkin.autoconfigure.storage.elasticsearch.aws\n. ps wrt commits, it is probably nicest to merge the refactor one first, as it is less code.\nThen a PR for:\nzipkin-storage/elasticsearch-jest, zipkin-autoconfigure/storage-elasticsearch-jest (and changes to zipkin-server including property/env variable documentation)\nThen, a PR for:\nzipkin-autoconfigure/storage-elasticsearch-aws (and changes to zipkin-server including property/env variable documentation)\nI'm ok reviewing everything as one pull request, just this ^^ is one way to break it up.\n. This is a summarising reply :)\nI can help with the magic stuff and also some of the test setup issues.\nLet's go with jest for now (vs ground up http or es 5 client) as we can\nalways optimize later. If you run into trouble with modularisation or\nmagic, just note in the pull request and I can take some efforts to tidy\nthings.\nI suspect that we can handle most of this in spring by looking at the value\nof the elasticsearth hosts. Ex if they are amazon, assume that. If the are\nnot port 9300 assume http. We can have something like ES_CLIENT_TYPE=http\nor native or aws to force a decision if need be.\nOn 17 Sep 2016 03:13, \"sethp-jive\" notifications@github.com wrote:\n\nOh, cool, I had totally missed that this was already in progress, I'm\nsorry for dropping in unannounced. Thanks for the feedback!\nI'll get started teasing this stuff out into modules as you described, but\nI have a few questions for you:\n1. Jest's dependency on Elasticsearch core (and its drag) is optional.\n   I've just kept it around it for the query builders really \u2013 if you're\n   comfortable with embedding those in the source as JSON instead of objects,\n   we could have a zipkin-storage/elasticsearch, zipkin-storage/elasticsearch-transport,\n   zipkin-storage/elasticsearch-http (or Jest) tree?\n2. As far as I know (which is not very far, admittedly), the\n   difference between the Transport client and Jest is largely of Java API (+\n   discovery entrypoint, naturally) \u2013 do we want to consider simplifying\n   things by just replacing the elasticsearch implementation with one over\n   HTTP instead of the binary transport? So just zipkin-storage/elasticsearch\n   and zipkin-storage/elasticsearch-http? I didn't go that route\n   initially because it would be a breaking change for existing\n   configurations, but I thought I should mention it.\n3. I'm not very familiar with spring-boot or the autoconfiguration\n   stuff, but I think I can figure it out if you can help me understand the\n   final vision for how this stuff gets driven. Is the current (user-facing)\n   magic regarding URL parsing and host-pattern-matching acceptable? e.g. one\n   STORAGE_TYPE=elasticsearch and switching on the value in ES_HOSTS, or\n   are we aiming for a suite of storage options like\n   STORAGE_TYPE=elasticsearch, STORAGE_TYPE=elasticsearch-http,\n   STORAGE_TYPE=elasticsearch-aws?\n4. Regarding testing the AWS integration \u2013 I don't know if it was\n   clear, but the AWS code makes identical requests to the normal HTTP client\n   with the addition of a pair of magic AWS headers. Regrettably, the contents\n   of those headers are tough to validate except against a running AWS ES\n   instance (I have a gnarly story about just how many URL encoding rounds it\n   accepts for commas, if you want to hear it). I think that means just\n   detecting on the presence of credentials is not quite enough \u2013 we also need\n   a \"domain\" (what AWS calls an ES cluster) to connect to. How would you\n   imagine this working in an environment that's not just my machine (with\n   access to my AWS account)? i.e. should I be looking in an environment\n   variable for that domain? I could code the tests to try creating a domain\n   with the credentials at hand, but regrettably that takes quite a few\n   minutes.\nI think if you're alright with it, I'd like to keep this PR as a kind of\n\"tracking\" issue and for the final zipkin-storage/elasticsearch-aws\nmodules when we get there, and I can send you other PRs for the steps along\nthe way.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1302#issuecomment-247683391,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wpI1hXUc7JaLlUx1tdQySWRhGzXks5qqupSgaJpZM4J-Wai\n.\n. looks like we are on final lap. I'd like to cut a release from master sometime in the next day or two as there's a method I'd like to deprecate and get a replacement out for. do you think you'll have time for the aws autoconfigure in the next day or two? If so, it would be nice to make a general announcement for elasticsearch http+aws vs separately.\n. @sethp-jive great. once you have this is, I'll take a look at it and possibly modify it to use https://github.com/spring-cloud/spring-cloud-aws\n\nReason being is there's a standard way to get AWS credentials in spring boot, I can port the approach if you don't have cycles to look into it.\ncc @dsyer \n. > Hmm, my only concern about that spring-aws module is that it looks like it\n\nmight establish a different set of conventions from the pre-existing AWS\nclient stuff (a rich set of system properties, environment variables,\nand/or config files that parameterize a bunch of different entrypoints into\nthings like AWS credentials).\nFor example, in local development it's really convenient to use\nAWS_PROFILE=zipkin and have a role defined that I can switch into that\nemulates a restricted permission user comparable to what'd be running in\nproduction. Do you think that module will match pre-existing expectations?\nNot sure if it will be a problem or not. Sounds like a valid concern.\n. no worries. I'm only sensitive as I've broken JRE compat in two related\nzipkin libraries accidentally in the last year :P\n\nOn Fri, Sep 23, 2016 at 11:09 AM, sethp-jive notifications@github.com\nwrote:\n\n@sethp-jive commented on this pull request.\nIn pom.xml https://github.com/openzipkin/zipkin/pull/1302:\n\n@@ -40,7 +40,8 @@\n     UTF-8\n\n<!-- default bytecode version for src/main -->\n-    1.7\n-     IntelliJ isn't clever enough to have different language levels for tests vs. main code \n-    1.8\nyeah, I'm sorry, I'd been good about keeping this out of the commit (I\nknow it's not going in), but an errant git commit -a seems to have undone\nthat effort.\nThanks for the JIRA and the workaround!\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1302, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6121oViaGwyLnlW8dns1E6eC_enUMks5qs0LjgaJpZM4J-Wai\n.\n. Oh wait.. I forgot :) the intent of the naming convention here is to relate\nto the path of properties  what you had was right\n\nOn 23 Sep 2016 10:11 am, \"sethp-jive\" notifications@github.com wrote:\n\n@sethp-jive commented on this pull request.\nIn zipkin-autoconfigure/storage-elasticsearch-http/src/main/\njava/zipkin/autoconfigure/storage/elasticsearch/http/\nZipkinHttpElasticsearchStorageAutoConfiguration.java\nhttps://github.com/openzipkin/zipkin/pull/1302#pullrequestreview-1262349\n:\n\n@Configuration\n-@EnableConfigurationProperties(ZipkinElasticsearchHttpStorageProperties.class)\n+@EnableConfigurationProperties(ZipkinHttpElasticsearchStorageProperties.class)\n\nI think this is more conventional this way? I changed it to mirror your\nchange to the HTTP autoconfiguration.\nIn zipkin-autoconfigure/storage-elasticsearch-aws/src/main/\njava/zipkin/storage/elasticsearch/LazyDomainLookup.java\nhttps://github.com/openzipkin/zipkin/pull/1302#pullrequestreview-1262349\n:\n\n\n}\n  +\n@Override public InternalElasticsearchClient.Builder cluster(String cluster) {\nthrow new UnsupportedOperationException();\n}\n  +\n@Override public InternalElasticsearchClient.Builder hosts(List hosts) {\nthrow new UnsupportedOperationException();\n}\n  +\n@Override public InternalElasticsearchClient.Builder flushOnWrites(boolean flushOnWrites) {\nthrow new UnsupportedOperationException();\n}\n  +\n@Override public InternalElasticsearchClient.Factory buildFactory() {\nreturn new InternalElasticsearchClient.Factory() {\n\n\nSince the endpoint URLs that get dumped out of the AWS ES service aren't\nparticularly friendly (e.g. https://search-name-\nwvkjy7wgk6j7iihjqcqemhhfju.us-west-2.es.amazonaws.com ) I found it useful\nto have Zipkin do the lookup for me to map from the name of the domain to\nthe endpoint for a few different testing purposes. I also thought it might\nbe nice to have a second more explicit path in just in case the AWS \"magic\"\nparsing of the URLs fails.\nIn zipkin-server/src/main/java/zipkin/server/ZipkinServer.java\nhttps://github.com/openzipkin/zipkin/pull/1302#pullrequestreview-1262349\n:\n\nimport org.springframework.boot.builder.SpringApplicationBuilder;\n import zipkin.server.brave.BootstrapTrace;\n\n-@SpringBootApplication\n+@SpringBootApplication(exclude = JestAutoConfiguration.class)\nI'm sorry about this, but it was the only way I could find to prevent\nSpring from (eagerly) spinning up an unused JestClient in the application\ncontext (with pursuant confusing log messages about its initialization). I\nsuppose we could have found a way to use that configuration, but it doesn't\nquite meet our needs \u2013 there's no hook for adding the signing interceptor,\nfor example. I'd also suggest that makes more sense to have the HttpClient\nown its own connectivity rather than having it injected, similarly to how\nthe NativeClient handles things.\nIn zipkin-storage/elasticsearch-http/src/main/java/zipkin/\nstorage/elasticsearch/http/HttpClient.java\nhttps://github.com/openzipkin/zipkin/pull/1302#pullrequestreview-1262349\n:\n\n@@ -289,23 +322,18 @@ public void clear(String index) {\n     checkState(!\"RED\".equalsIgnoreCase(status), \"Health status is RED\");\n   }\n-  private static final class IndicesHealth extends GenericResultAbstractAction {\n-  private static final class IndicesHealth extends Health {\n\nI had this change locally because, as I'm sure you noted, my previous\nattempt to move the method-local class out didn't quite work (the\nconstructor needed the additional setURI call).\nThe worst part about this is that I think it means the ES tests didn't run\nfor my last PR \u2013 all the tests are passing consistently for me locally, but\nmaybe having some trouble in CI. I might need your help to dig into that if\nit persists.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1302#pullrequestreview-1262349,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610knqG7ubV_VwxsC1OVDzuZPVOIbks5qszVTgaJpZM4J-Wai\n.\n. > Was there a race condition? I only saw a janky ordering thing (with the constructor calling setURI out of order with its descendant) but it was limited to the constructing single thread (no leaking this outside the class that I saw, at least).\nsorry I meant an ordering problem, which robbed me of time when\ninnocently attempting to make the class static. Stuff like that is dodgy to\nme. If a class isn't designed for extension (because of method calls in the\nconstructor), it should be marked final.\nI'll swap the base class out and fix up LenientSearch (which has sorta the\nsame problem, except it has no constructor parameters for now). In that\ncase, though, it seems I just misunderstood the Jest API and that class can\ngo away entirely.\nk. lemme know when you are finished polishing I probably won't pull this\ndown for an hour or so anyway. Cheers\n. merging to keep trunk shippable and @sethp-jive unblocked\n\n@sethp-jive feel free to change the internal interface as you need to in your next PR\n. This one is my fault I think. So is the best behaviour to copy/paste a\none-liner?\nOn 20 Sep 2016 12:36 pm, \"Tommy Ludwig\" notifications@github.com wrote:\n\n@shakuzen commented on this pull request.\nIn zipkin-autoconfigure/storage-elasticsearch-jest/src/main/\njava/zipkin/autoconfigure/storage/elasticsearch/jest/\nZipkinElasticsearchJestStorageProperties.java\nhttps://github.com/openzipkin/zipkin/pull/1306#pullrequestreview-676674:\n\n\n* Unless required by applicable law or agreed to in writing, software distributed under the License\n* is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n* or implied. See the License for the specific language governing permissions and limitations under\n* the License.\n*/\n  +package zipkin.autoconfigure.storage.elasticsearch.jest;\n  +\n  +import java.util.Collections;\n  +import java.util.List;\n  +import org.springframework.boot.context.properties.ConfigurationProperties;\n  +import zipkin.storage.elasticsearch.ElasticsearchStorage;\n  +import zipkin.storage.elasticsearch.jest.RestClient;\n  +\n  +@ConfigurationProperties(\"zipkin.storage.elasticsearch\")\n  +public class ZipkinElasticsearchJestStorageProperties {\n/* @see ElasticsearchStorage.Builder#hosts(List) /\n\n\nPer convention, JavaDoc comments on @ConfigurationProperties fields are\nplain text (don't use JavaDoc keywords like @see) because they are used\nby the configuration processor and then by IDEs as descriptions of the\nfields during auto-completion, for instance.\nIn zipkin-storage/elasticsearch-jest/src/main/java/zipkin/\nstorage/elasticsearch/jest/RestClient.java\nhttps://github.com/openzipkin/zipkin/pull/1306#pullrequestreview-676674:\n\n\nprivate static final int MAX_INDICES = 100;\n  +\npublic static final class Builder implements InternalElasticsearchClient.Builder {\nList hosts = Collections.singletonList(\"http://localhost:9200\");\nboolean flushOnWrites;\n  +\n/**\n* Unnecessary, but we'll check it's not null for consistency's sake.\n*/\n@Override public Builder cluster(String cluster) {\ncheckNotNull(cluster, \"cluster\");\nreturn this;\n}\n  +\n/**\n* A comma separated list of elasticsearch hostnodes to connect to, in http://host:port or\n\n\nNot a comma-separated list but an actual List\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1306#pullrequestreview-676674,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD612Kovuw4gMyVDD5fV63nN5nfusTrks5qr2LXgaJpZM4KBJdk\n.\n. cherry-picked in. take note of a few conventional file renames (also it is now plumbed to zipkin-server), so the following works:\n\nbash\n$ STORAGE_TYPE=elasticsearch ES_HOSTS=http://localhost:9200 java -jar zipkin-server/target/zipkin-server-1.11.2-SNAPSHOT-exec.jar\n. interested, as I'm interested in decommissioning the \"cassandra\" module, but I don't think this gets us there, as not everyone uses DSE. Basically, we end up with 3 codebases for cassandra that don't share code.\nI know @schrepfler asked about DSE, but not sure if he uses zipkin at the moment or not. Do we have a good gauge of how many use DSE?\nI'm concerned about the maintenance problem of bringing this into the core repo, particularly as it is getting bloated and it doesn't seem obvious how to test commercial variants like this. There's also bus factor.. we're lucky we have you, but there's few others working on cassandra, almost none. Can we afford to have a third codebase? Finally, we already blow up on memory often as it is and our builds are approaching 20minutes.. Feels like it should be a separate repo.\n@openzipkin/cassandra @abesto What are your thoughts?\n. @michaelsembwever I noticed this has a dep on cassandra3. seems CassandraSpanConsumer is the same type, but it has a different front-end (SpanStore) and schema.\nAm I right to think that this is \"effectively\" the same schema? Else, I'd assume you wouldn't be able to use cassandra3's CassandraSpanConsumer\nWhile this PR is >2K lines, it seems some of it is benign (ex UDT related cruft) or some things that have to be copy/pasted. The heaviest class seems to be the SpanStore variant. Is that a fair assessment?\n. > Personally I would rather not include the code into master, for the same\n\nreasons you describe @adriancole https://github.com/adriancole\nBut I did want to at least throw the code out here, so that others could\ntake it (if only the idea) and run if they wanted to.\nHistorically, most of the repos we spun out from concerns about interest\ndidn't turn into anything. Ex mongo and redis went to separate repos to\ndie. However, I think it is more reasonable that this could live. I'd offer\neffort to carve out a repo for this.\n\nWe could attract attention to it, saying that once cassandra3 is done,\nwe'll deprecate \"cassandra\" for removal at some point. Those who can't\nupgrade on account of DSE should look to the zipkin-storage-dse5 repo. If\nno-one is interested enough even to comment there, then there's not much\nelse we can do... worst case, it dies.\n. Thanks for answering my questions, @michaelsembwever! I'll shut up and see what others think\n. cc @devinsba\nI'll make a similar change to brave, too\n. PS in hindsight I wouldn't have defined port as a short, rather left that to codec.\nThis class used to be a literal struct, and latter changed to decouple from field lengths. In zipkin 2 we could break api and change port accordingly, if we felt the need to. For now, this change should help mitigate the concern.\n. PS in hindsight I wouldn't have defined port as a short, rather left that to codec.\nThis class used to be a literal struct, and latter changed to decouple from field lengths. In zipkin 2 we could break api and change port accordingly, if we felt the need to. For now, this change should help mitigate the concern.\n. going to raise a pull request to fix something I messed up, more importantly carving out a unit test as doing the first one is usually the hard part.\n. cc @openzipkin/elasticsearch \n. To validate this, I put the wrong url in (leaving out the domain) and got a more helpful message back:\nbash\n$ curl -s localhost:9411/health|jq .\n{\n  \"status\": \"DOWN\",\n  \"zipkin\": {\n    \"status\": \"DOWN\",\n    \"ScribeCollector\": {\n      \"status\": \"UP\"\n    },\n    \"ElasticsearchStorage\": {\n      \"status\": \"DOWN\",\n      \"error\": \"java.net.UnknownHostException: ap-southeast-1.es.amazonaws.com: unknown error\"\n    }\n  },\n  \"diskSpace\": {\n    \"status\": \"UP\",\n    \"total\": 498920849408,\n    \"free\": 377195024384,\n    \"threshold\": 10485760\n  }\n}\nwhich is nicer than\n\"ElasticsearchStorage\": {\n      \"status\": \"DOWN\",\n      \"error\": \"com.google.common.util.concurrent.UncheckedExecutionException: java.net.UnknownHostException: ap-southeast-1.es.amazonaws.com: unknown error\"\n    }\n. cc @openzipkin/elasticsearch ps I've started resurrecting my old okhttp work towards this. The client design that @sethp-jive made makes it much lighter effort.\nI'm personally interested in an okhttp option for a number of reasons:\n- We want a single library for ES http 2-5 if possible. Each dependency makes life difficult, and I'm pretty sure ES 2 will be a problem in the same classpath as ES 5.\n- Similar, but focused on Jest. Library compatibility bites me in every project used as platform code and jest uses many of the usual suspects of api compat problems: guava, gson and apachehc. OkHttp (and similar like moshi) have the same dependency policy: switch package names when you break api. \n- I really don't like how inefficient the apachehc stuff is, especially when using the AWS interceptor, it literally copies the data when we don't need to. While less concerning for query, we should be careful about performance on the write side.\n- We want self-tracing for ES performance and behavior debugging. OkHttp already has a brave integration, as well a way to tag calls.\n- And should be said.. I like to enjoy coding and OkHttp is much more fun than worrying about AWS SDK library gotchas, terrible apachehc interceptors, things like constructor order in Jest, or other indirection.\n. I've completed the okhttp storage layer for elasticsearch 2.x (haven't looked into 5.x portability, yet). I've also written a tracing interceptor so that we can debug the performance. \nhttps://github.com/openzipkin/brave/pull/242\nAfter I get an okhttp aws signature v4 interceptor together, I'll raise a pull request. should be tomorrow or next day. here's a peek for now.\n\n. I'm looking into this today, since ES is out\n. I'm looking into this today, since ES is out\n. currently, the biggest problem is an artificial one (the default lookback being equal to endTs).\nIn the  current implementation, daily buckets are listed individually, so during tests this ends up looking for 15K daily indexes, which pretty much crashes the ES server.\n. currently, the biggest problem is an artificial one (the default lookback being equal to endTs).\nIn the  current implementation, daily buckets are listed individually, so during tests this ends up looking for 15K daily indexes, which pretty much crashes the ES server.\n. index problem is sorted. next up is how we do aggregations when using 128-bit trace IDs, considering ES5 has caveats on string fields https://github.com/openzipkin/zipkin/issues/1379\n. cc @openzipkin/elasticsearch ps this is net less main code, the increase in lines is due to backfilling tests on the aggregation bucket code which had a bug\n. @shakuzen thx for the review!\n. we'll need the json for the spans that work in the in-mem, but not in cassandra. can you paste that?\n. ps @guilespi @aliceliang @strongh if any of you are interested in continuing work on clj-zipkin, let us know. for example, we've moved some language specific repositories into the openzipkin or so others can help maintain them. would be nice to keep the fire burning.\n. ps the ip is very invalid.. that's why it isn't showing up in the json\n. raised this to help get you along\nhttps://github.com/sachinpkale/clj-zipkin/pull/1\n. whoo hoo!\n. this storage type should also help people answer the question @dan-tr asked about\n\nCan anyone offer any guidance on the number and type of servers to use to run Zipkin-Server? We are expecting roughly 10,000 spans per second in our production enviornment?\n. > This might be a separate request, but one item I haven't had a chance to\nlook into would be exposing something along the lines of a span predicate\nfilter that would allow dropping spans based on specific logic -- say drop\nspans with duration less than 1 millisecond, or in noop case drop\neverything.\nyeah worth calling out separately, I think. in that issue, we'd have to\ndiscuss the impact which is partial trace data. this has historically been\ntroublesome in collectors and why they consistently drop based on trace id\nvs content of one span in a trace.\n\non the other hand.. you raise a very interesting point! if we set\ncollection sample rate to zero, we have effectively noop storage :P\n. COLLECTOR_SAMPLE_RATE=0.0 is essentially noop storage and requires no effort!\n. we could add this, but it will be the third timestamp field, which makes me\nreluctant.\nWould metrics be reasonable way to track the last write attempt?\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-server#collector\n. ps @openzipkin/elasticsearch this topic is a pretty opsy one. anyone doing something similar and have opinions?\nex. timestamp(_millis) is a semantic field for the span. If someone wanted to know the created_at, well elasticsearch doesn't do that. The question here, is should we? if so, how can we label such a field so that it isn't confused with the timestamp reported by the apps?\n. I think the key thing here is that Dan is looking for a per-message\naccounting of latency vs an aggregate one (ex via metrics). This is the\nfirst time such a request has come up (ex hasn't happened on any other\nstorage system). That may be because spans are lossy and so not guaranteed.\nMaybe people aren't looking at spans one-at-a-time knowing that. I can\ncertainly understand that when troubleshooting or setting up the system for\nthe first time, something like this could be handy!\nTo answer the question, the server's /metrics endpoint is json and there's\nalso a prometheus metrics endpoint exposed. Regardless of monitoring system\nused, these metrics would need to be collected to be useful in any way.\n. I think that we should add (and define) latency metrics as an initial step, since this would be useful regardless of the storage backend. Will keep this issue open about the forensic investigation on a span-by-span basis.\nps opened this to help showcase metrics in use https://github.com/openzipkin/docker-zipkin/issues/121\n. Since this is not supported in other storage layers, and others don't seem\ninterested. I'm hesitant about adding an if statement, environment variable\nand another hack.\nI also don't like how close the variable name is to the other two\ntimestamps. It really feels like tech debt.\nSince this is for manual troubleshooting, what about encoding the stored\ntimestamp into the id of the document? This is done with cassandra iirc and\nwouldn't clutter the json with another similar field. We would assign an id\nthat encodes the timestamp instead of having elasticsearch generate one.\nWould that work for you?\n. here's a concrete counter-offer :)\nsince we don't change spans after writing them to ES, we can use a timestamp convention for the document ids.\nEx. something like $epochmillis-$traceId.$spanId. The trace and span id would help ensure against collision. It is unlikely a span will be reported twice in the same millisecond.\nSo, if I wrote the same span successively (for client and server), it would look like the following rows:\n\"1475335327601-0000000000000001.0000000000000003\"\n\"1475335439449-0000000000000001.0000000000000003\"\nThis would be a longer id than the normal ones in ES, and not good for any automated analysis, could be helpful to answer your question about when the span was stored without defining another field. Is this worth doing?\n. > Hum... To me that feels like we would be overloading a data field for two\n\ndifferent purposes (for timestamping and identification).\nindeed that's what it would be :)\nI think for now I would prefer to just let this issue \"percolate\" for a\nwhile and see anyone else asks for it.\nNo point making a change like this just for me \ud83d\ude03\nyeah usually issues like this end up with multiple requestors, thanks for\ndeferring until then\n. Thanks, suman. Can someone cite the practice with a link? busy-work I know,\nbut I'm fine with doing this, just want to borrow this rationale in code\ncomments.\n. I did some digging and I don't like using @timestamp.\n\nFirstly, seems logstash format it in iso8601 which is unnecessary overhead and distraction (to add a formatter to our code). https://github.com/elastic/logstash/blob/master/logstash-core-event/lib/logstash/timestamp.rb\nNext iso8601 doesn't help you compare lag using other timestamps which are in epoch millis (and also epoch micros), just makes busy work to convert one or the other.\nFinally, it is a special field for logstash. I've seen some references suggesting even that it is internal. It seems they have changed formatting before. I don't like the idea of misleading people into thinking this is logstash interop and then that accidentally breaking.\nMy thought is that if we are to do this, be specific in a way where we can inform people what to do with this. Ex. collector_timestamp (because we don't have a component named ingestor), and make it epoch millis as that's the least overhead and burden for those who will never read this.\nWDYT?\n. here's the fix https://github.com/openzipkin/zipkin/pull/1328\n. In testing the new ES 5 support, I found that our hypothesis, which is that the new ingest node functionality can address this.. well it is supported.\nThe following stuff works like a champ and requires no zipkin change except to punch a hole to specify a pipeline id\nPUT _ingest/pipeline/my-pipeline-id\n{\n  \"description\" : \"add collector_timestamp_millis\",\n  \"processors\" : [\n    {\n      \"set\" : {\n        \"field\": \"collector_timestamp_millis\",\n        \"value\": \"{{_ingest.timestamp}}\"\n      }\n    }\n  ]\n}\n. In testing the new ES 5 support, I found that our hypothesis, which is that the new ingest node functionality can address this.. well it is supported.\nThe following stuff works like a champ and requires no zipkin change except to punch a hole to specify a pipeline id\nPUT _ingest/pipeline/my-pipeline-id\n{\n  \"description\" : \"add collector_timestamp_millis\",\n  \"processors\" : [\n    {\n      \"set\" : {\n        \"field\": \"collector_timestamp_millis\",\n        \"value\": \"{{_ingest.timestamp}}\"\n      }\n    }\n  ]\n}\n. @dan-tr so this is used to implement timestamp and lookback. Ex. we don't want to ask elasticsearch to scan more indexes than needed. For example, if people don't setup a job to purge old data, a wildcard zipkin-* would search indexes every document ever sent, even if the api request was for the last few days.\nI noticed one way we could compress this is using partial wildcards. For example, instead of literally sending 2016-01-01, 2016-01-02 ..  2016-04-01, 2016-04-02, we could send  2016-01-*, 2016-01-*, 2016-03-*, 2016-04-01, 2016-04-02. In the example given, of spans this year up to april 2nd, it would reduce the date expressions sent from over a hundred to 5. I'll open an issue for this\n. > I was just made aware of the new feature in ES 5:\n\nhttps://www.elastic.co/blog/new-way-to-ingest-part-1\nWith the new processors we may be able to have ES add an ingest date for\nus.\nwould you be ok running ES 5, then? We've an open issue to support it\nanyway (doesn't work now). We can test through that.\n. deal!\n\nOn Tue, Oct 11, 2016 at 8:34 PM, dan-tr notifications@github.com wrote:\n\nWe're not running ES 5 yet, but hope to be soon (Next 6 months \ud83d\ude04 ), but\nI am willing to table this issue until after we get a chance to see if ES 5\nwill solve the problem.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1328#issuecomment-252904203,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD611OQFaDRe6No3ZBwSCmwt8Ujae45ks5qy4JlgaJpZM4KN1EO\n.\n. verified that the below works with ES 5, so we can close this in favor\n\nPUT _ingest/pipeline/my-pipeline-id\n{\n  \"description\" : \"add collector_timestamp_millis\",\n  \"processors\" : [\n    {\n      \"set\" : {\n        \"field\": \"collector_timestamp_millis\",\n        \"value\": \"{{_ingest.timestamp}}\"\n      }\n    }\n  ]\n}\n. here's another https://github.com/whitemerry/phpkin. @jcchavezs has continued efforts with symfony integration https://github.com/jcchavezs/zipkin-instrumentation-symfony. His tracer is very feature rich and easy to understand (for me anyway).\nOne thing I'd like to see is more folks collaborating on whatever solution ends up being the formal one for zipkin, if such ever happens. zipkin-php is doing well, folks opening issues and commenting, but still single-author. Is anyone available to partner up with more hands-on with @jcchavezs?. By the way, while it is possible to overrun the cluster using the native transport, I found that if using http, it doesn't.\nEx. if I use ES_HOSTS=http://localhost:9200 (on latest zipkin), backlogging even 100K messages doesn't result in any memory or overload I can reproduce.\nCan you try to make a reproducible script, such as below, so that myself or someone else can help debug this? By scripting, we don't have to guess so much. Please include the parameters you are using to run the server.\nhttps://github.com/openzipkin/zipkin/issues/1141#issuecomment-228514983\n. and by \"overrun the cluster\" I mean it is possible for us to send so many messages that we exceed the worker queue in elasticsearch, resulting in errors like this:\norg.elasticsearch.common.util.concurrent.EsRejectedExecutionException: rejected execution of org.elasticsearch.transport.TransportService$4@6ab048e0 on EsThreadPoolExecutor[index, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@1cb4322a[Running, pool size = 8, active threads = 8, queued tasks = 200, completed tasks = 243]]\n. >\n\n@adriancole https://github.com/adriancole, it looks like I need to\nrevive this case. We are currently running zipkin 1.28.0 and using\nElasticsearch as our storage component using HTTP all the way from\nJava/Brave down through zipkin to Elasticsearch (so no Kafka) and are\nhaving this issue.\nOOM is too generic to try to address in one issue about kafka. I did add a\nnote about exceptions here, which is probably why you added details about\nelasticsearch. Can you describe your issue on the backpressure issue? #1741\nThe problem is that when ES is backlogged, requests don't complete. We may\nbe able to tactically put a request timeout on okhttp side so that it\ndoesn't linger when the server is not being nice. Another option is to rate\nlimit, but the other issue is more specific to ES, so probably best to\ncarry-forward there. Thanks for the offer to run tests!\n. Especially for those using ES, use latest brave and zipkin and have brave\nuse v2 reporter against /api/v2\n\nThis will reduce gc pressure on both sides as ES is v2 native and using v1\nendpoints means converting every span\n. >\n\nAs it stands, I'm also not sure \"backpressure\" is entirely the issue we're\nhaving. The second instance we had an OOM, for example, was because\nElasticsearch ran out of disk space. Something obviously easy to remedy,\nbut in that case I wouldn't expect zipkin-server to lock up and die like it\ndid. Instead, imo, it should try to push what data it gets and drop\nwhatever Elasticsearch says it can't store; assuming Elasticsearch has the\nability to say it can't store it. To some extent, I'm curious why the\nES_HTTP_READ_TIMEOUT property doesn't resolve this issue. I'm guessing it's\nbecause OkHttp3 may not be able to establish the connection to\nElasticsearch and therefore isn't getting to a point where it's waiting on\na read?\nI think this is as you say not really a backpressure so much as how to keep\nzipkin from crashing when ES is not happy. However the solution might be\nrelated. I would still upgrade even if it yeah won't help from crashing\nwhen ES it dead (maybe only slow it).. pretty sure we updated OkHttp as\nwell lately. I've been thinking about maybe maxRequests=64 could be\nproblematic depending on how big the in-flight requests are. I've also\nwondered if we shouldn't drop sooner, perhaps with a custom http listener\n(undertow or similar) so that we don't buffer any data when the backend is\ndead. Maybe we can retitle https://github.com/openzipkin/zipkin/issues/1741\ntowards ES resilience or open one on that point. Any help you can give\nwould be appreciated.. apologies the work in progress on v2 made that a bit\nof shifting sands.\n. Are you sending this data via a homegrown or existing library? I know census has support for pruning spans as for example in last workshop there was no tool that could handle over 10k.. many drop far before that, for example using statistics. We have not explored this deeply as usually very large traces are an error.\n\nPs okhttp does have max connections but i dont think that limits backlog. We can verify.. Are you sending this data via a homegrown or existing library? I know census has support for pruning spans as for example in last workshop there was no tool that could handle over 10k.. many drop far before that, for example using statistics. We have not explored this deeply as usually very large traces are an error.\nPs okhttp does have max connections but i dont think that limits backlog. We can verify.. >\n\nQuick review of the documentation indicates that the max connections only\naffects the size of the pool they use; which probably means everything else\nwould be queued and backlogged.\nAgain, not knowing OkHttp that well, I wonder if you can inspect the\nDispatcher queue and cancel anything in the queue if you exceed a certain\nthreshold?\nRef: https://gist.github.com/RyanRamchandar/64c5863838940ec67f03\nappreciate your persistence.. it is working :) will take a look thanks!\n. >\nQuick review of the documentation indicates that the max connections only\naffects the size of the pool they use; which probably means everything else\nwould be queued and backlogged.\nAgain, not knowing OkHttp that well, I wonder if you can inspect the\nDispatcher queue and cancel anything in the queue if you exceed a certain\nthreshold?\nRef: https://gist.github.com/RyanRamchandar/64c5863838940ec67f03\nappreciate your persistence.. it is working :) will take a look thanks!\n. moved okhttp related discussion to a new issue as this topic is tangentially related https://github.com/openzipkin/zipkin/issues/1760. moved okhttp related discussion to a new issue as this topic is tangentially related https://github.com/openzipkin/zipkin/issues/1760. cc @abesto and @shakuzen (the README whisperer)\n. cc @abesto and @shakuzen (the README whisperer)\n. I agree (though it should be in the zipkin-aws repo)\n\nFor next steps: what to do with such a jar? It needs to be in zipkin's\nclasspath to auto-configure.\nIf you look at under the covers, there's optional dependencies in\nzipkin-server (for the autoconfiguration we've defined already) So if we\nwanted something literally in the exec jar, we'd add it as an optional\ndependency.\nOtherwise, we or look at alternative ways to do modularity (such as\ninstructions in projects to add it to the server classpath and/or having\ndocker download the new modules automatically during image creation).\nThere will come a point where we'd not want to keep stuffing things into\nthe exec jar, though I'm not sure if we are there not.\n. I agree (though it should be in the zipkin-aws repo)\nFor next steps: what to do with such a jar? It needs to be in zipkin's\nclasspath to auto-configure.\nIf you look at under the covers, there's optional dependencies in\nzipkin-server (for the autoconfiguration we've defined already) So if we\nwanted something literally in the exec jar, we'd add it as an optional\ndependency.\nOtherwise, we or look at alternative ways to do modularity (such as\ninstructions in projects to add it to the server classpath and/or having\ndocker download the new modules automatically during image creation).\nThere will come a point where we'd not want to keep stuffing things into\nthe exec jar, though I'm not sure if we are there not.\n. ahh nice, but we should put this in the zipkin-aws repo (since that's where\nthe code lives, it will require less coordination. Maybe make an\nautoconfigure folder there?\n. ahh nice, but we should put this in the zipkin-aws repo (since that's where\nthe code lives, it will require less coordination. Maybe make an\nautoconfigure folder there?\n. @Dreampie I'm wondering if there's anything special about the trace in question. Can you post the JSON for it? http://your_host:9411/api/v1/trace/hex-id\n. @Dreampie I'm wondering if there's anything special about the trace in question. Can you post the JSON for it? http://your_host:9411/api/v1/trace/hex-id\n. so using latest zipkin, I can view that trace after posting it like so..\nbash\ncurl -H'Content-Type: application/json' -s localhost:9411/api/v1/spans -d '{..your json..}'\n. so using latest zipkin, I can view that trace after posting it like so..\nbash\ncurl -H'Content-Type: application/json' -s localhost:9411/api/v1/spans -d '{..your json..}'\n. tried in chrome and safari at least.\n. tried in chrome and safari at least.\n. yeah that curl command would just upload to zipkin. I then viewed like this: http://localhost:9411/traces/8353ecbd3f082ee4?serviceName=user-api-provider\n. yeah that curl command would just upload to zipkin. I then viewed like this: http://localhost:9411/traces/8353ecbd3f082ee4?serviceName=user-api-provider\n. well createSpanTreeEntry is a recursive method. Not sure why you can get this consistently where I can't. If it were consistent, then it would be easier to debug why (as opposed to guessing).\nIf you are up to it, you could try putting a breakpoint at line 29 or so and advancing. If it is endless, the values of span, trace or idx would likely explain the loop. \n. well createSpanTreeEntry is a recursive method. Not sure why you can get this consistently where I can't. If it were consistent, then it would be easier to debug why (as opposed to guessing).\nIf you are up to it, you could try putting a breakpoint at line 29 or so and advancing. If it is endless, the values of span, trace or idx would likely explain the loop. \n. One thing I've noticed is that the instrumentation is wrong. It is setting parentId to the trace id in the root span. parentId should not present on the root span.\nbash\n$ curl -s http://localhost:9411/api/v1/trace/8353ecbd3f082ee4 |jq '.[]|{traceId: .traceId,parentId: .parentId,id: .id}'\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"9cfc718eaa9ac68a\",\n  \"id\": \"b31e7ba035fa7863\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"b31e7ba035fa7863\",\n  \"id\": \"085bed5c66734fbf\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"e7afbf720c4c2afe\",\n  \"id\": \"a1ebd26feb6368ed\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"05c9ccf0a7207624\",\n  \"id\": \"e7afbf720c4c2afe\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"b31e7ba035fa7863\",\n  \"id\": \"b9243445b0abe1f3\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"b9243445b0abe1f3\",\n  \"id\": \"6eab4d68da610379\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"b31e7ba035fa7863\",\n  \"id\": \"05c9ccf0a7207624\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"05c9ccf0a7207624\",\n  \"id\": \"164fb5ad724af35f\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"8353ecbd3f082ee4\",\n  \"id\": \"8353ecbd3f082ee4\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"8353ecbd3f082ee4\",\n  \"id\": \"cd79e9a448961760\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"164fb5ad724af35f\",\n  \"id\": \"4ffd423b8b006578\"\n}\n. One thing I've noticed is that the instrumentation is wrong. It is setting parentId to the trace id in the root span. parentId should not present on the root span.\nbash\n$ curl -s http://localhost:9411/api/v1/trace/8353ecbd3f082ee4 |jq '.[]|{traceId: .traceId,parentId: .parentId,id: .id}'\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"9cfc718eaa9ac68a\",\n  \"id\": \"b31e7ba035fa7863\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"b31e7ba035fa7863\",\n  \"id\": \"085bed5c66734fbf\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"e7afbf720c4c2afe\",\n  \"id\": \"a1ebd26feb6368ed\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"05c9ccf0a7207624\",\n  \"id\": \"e7afbf720c4c2afe\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"b31e7ba035fa7863\",\n  \"id\": \"b9243445b0abe1f3\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"b9243445b0abe1f3\",\n  \"id\": \"6eab4d68da610379\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"b31e7ba035fa7863\",\n  \"id\": \"05c9ccf0a7207624\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"05c9ccf0a7207624\",\n  \"id\": \"164fb5ad724af35f\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"8353ecbd3f082ee4\",\n  \"id\": \"8353ecbd3f082ee4\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"8353ecbd3f082ee4\",\n  \"id\": \"cd79e9a448961760\"\n}\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"164fb5ad724af35f\",\n  \"id\": \"4ffd423b8b006578\"\n}\n. also, this span is missing its parent\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"9cfc718eaa9ac68a\",\n  \"id\": \"b31e7ba035fa7863\"\n}\n. also, this span is missing its parent\n{\n  \"traceId\": \"8353ecbd3f082ee4\",\n  \"parentId\": \"9cfc718eaa9ac68a\",\n  \"id\": \"b31e7ba035fa7863\"\n}\n. wild guess is that this is spring-cloud-sleuth. I think probably best to update to latest sleuth and see if it persists as there are problems in this trace.\n. wild guess is that this is spring-cloud-sleuth. I think probably best to update to latest sleuth and see if it persists as there are problems in this trace.\n. latest sleuth is 1.0.9.RELEASE and latest zipkin is 0.13.1\n. latest sleuth is 1.0.9.RELEASE and latest zipkin is 0.13.1\n. @Dreampie so that trace is invalid, but unfortunately I cannot reproduce the error you are getting.\nSince the trace is definitely invalid, I can't budget time to dig into this with what I have available this week. It is better to see if the bug that resulted in an invalid trace has been fixed.\n. @Dreampie so that trace is invalid, but unfortunately I cannot reproduce the error you are getting.\nSince the trace is definitely invalid, I can't budget time to dig into this with what I have available this week. It is better to see if the bug that resulted in an invalid trace has been fixed.\n. In other words, if this happens again (different trace), then best to raise a ticket for why it happened (ex what's causing the invalid trace). If it becomes repeatable here, then we can look into it, but at the moment, it is too expensive to try and figure out since I cannot reproduce it.\n. In other words, if this happens again (different trace), then best to raise a ticket for why it happened (ex what's causing the invalid trace). If it becomes repeatable here, then we can look into it, but at the moment, it is too expensive to try and figure out since I cannot reproduce it.\n. often when you have one trace with a lot of repeated sometimes unrelated\nspans in it, it is the result of a \"context leak\" In java, this could be\ncaused by inheritable thread local when thread pools that can grow are in\nuse in your application.  could this be the case?\nmeanwhile troubleshooting via an old issue isn't best. try gitter instead\nhttps://gitter.im/openzipkin/zipkin\n. I'm pretty sure latest UI code handles when there are multiple spans that look like they are the root. please re-open if not. I'm pretty sure latest UI code handles when there are multiple spans that look like they are the root. please re-open if not. yes. annotations and binary annotations are only queryable when they have\nan endpoint associated with them.\njaeger should be corrected to backfill endpoints prior to reporting vs only\npopulating a few.\n. yes. annotations and binary annotations are only queryable when they have\nan endpoint associated with them.\njaeger should be corrected to backfill endpoints prior to reporting vs only\npopulating a few.\n. So, in our thrift we tried to suggest that endpoint is used for querying along with service name https://github.com/openzipkin/zipkin-api/blob/master/thrift/zipkinCore.thrift#L297\nWhile Annotation.host and BinaryAnnotation.host fields are marked \"optional\", that's only a side effect of the original thrift. The description states that they are used both for query and also dependency links.\nMy guess is \"host\" (aka endpoint) was originally marked optional as it was lazy applied right before reporting the span out of process.\nhttps://github.com/twitter/finagle/blob/develop/finagle-zipkin-core/src/main/scala/com/twitter/finagle/zipkin/core/Span.scala#L61\nIn practice, it is sent with all instrumentation I've seen except Jaeger (which is either an oversight or an intentional incompatibility). That's because there can be multiple hosts in the same span. How do you know which was responsible for that annotation?\nSo, if you are querying by serviceName, you'll need to report spans with endpoint set. If this is about query by annotation when serviceName a query parameter, I know that isn't fully supported. We'd want a separate issue for that.\nThis issue seems to show api queries with serviceName present, so the action to take is to fix the incompatibility in jaeger. If the incompatibility is intentional, you could get out by adjusting the ZipkinSender to backfill missing endpoint.host prior to encoding. That way, folks using Zipkin can work regardless of the policy in jaeger's core code.\nhttps://github.com/uber/jaeger-client-java/blob/master/jaeger-zipkin/src/main/java/com/uber/jaeger/senders/zipkin/ZipkinSender.java#L97\n. So, in our thrift we tried to suggest that endpoint is used for querying along with service name https://github.com/openzipkin/zipkin-api/blob/master/thrift/zipkinCore.thrift#L297\nWhile Annotation.host and BinaryAnnotation.host fields are marked \"optional\", that's only a side effect of the original thrift. The description states that they are used both for query and also dependency links.\nMy guess is \"host\" (aka endpoint) was originally marked optional as it was lazy applied right before reporting the span out of process.\nhttps://github.com/twitter/finagle/blob/develop/finagle-zipkin-core/src/main/scala/com/twitter/finagle/zipkin/core/Span.scala#L61\nIn practice, it is sent with all instrumentation I've seen except Jaeger (which is either an oversight or an intentional incompatibility). That's because there can be multiple hosts in the same span. How do you know which was responsible for that annotation?\nSo, if you are querying by serviceName, you'll need to report spans with endpoint set. If this is about query by annotation when serviceName a query parameter, I know that isn't fully supported. We'd want a separate issue for that.\nThis issue seems to show api queries with serviceName present, so the action to take is to fix the incompatibility in jaeger. If the incompatibility is intentional, you could get out by adjusting the ZipkinSender to backfill missing endpoint.host prior to encoding. That way, folks using Zipkin can work regardless of the policy in jaeger's core code.\nhttps://github.com/uber/jaeger-client-java/blob/master/jaeger-zipkin/src/main/java/com/uber/jaeger/senders/zipkin/ZipkinSender.java#L97\n. @yurishkuro @vprithvi fyi\nthe adjustment to jaeger would be similar to finagle.. backfill missing endpoints prior to send. Assuming the incompatibility is intentional, it can be done in the ZipkinSender module, except the ZipkinSender doesn't know what the local endpoint is. You'd need to look it up.\njava\n  public int append(Span span) throws SenderException {\n    Endpoint localEndpoint = findLocalEndpoint(span);\n    // look for any missing Annotation.host, BinaryAnnotation.host and fill with localEndpoint\n    span = backfillEndpoints(span, localEndpoint);\n    byte[] next = encoder.encode(span);\n    ...\n. @yurishkuro @vprithvi fyi\nthe adjustment to jaeger would be similar to finagle.. backfill missing endpoints prior to send. Assuming the incompatibility is intentional, it can be done in the ZipkinSender module, except the ZipkinSender doesn't know what the local endpoint is. You'd need to look it up.\njava\n  public int append(Span span) throws SenderException {\n    Endpoint localEndpoint = findLocalEndpoint(span);\n    // look for any missing Annotation.host, BinaryAnnotation.host and fill with localEndpoint\n    span = backfillEndpoints(span, localEndpoint);\n    byte[] next = encoder.encode(span);\n    ...\n. cc @llinder fyi\n. cc @llinder fyi\n. cc folks who might be curious @llinder @sethp-jive and @shakuzen who mentioned to me the list approach\n. cc folks who might be curious @llinder @sethp-jive and @shakuzen who mentioned to me the list approach\n. whoohoo no test flake\n. whoohoo no test flake\n. thanks!\n. thanks!\n. This issue was moved to openzipkin/zipkin-ui#16\n. This issue was moved to openzipkin/zipkin-ui#16\n. moved because there are similar concerns in the zipkin-ui repo\n. Thanks for raising this. It would be interesting to understand if such a thing would be possible to do.\nLet's keep in mind that assuming that whitespace is network delay can be problematic. I made some comments on your issue about improving how the UI renders spans such that network timing can be more obvious https://github.com/openzipkin/zipkin-ui/issues/16\nAlso, analyzing the quality of traces isn't the job of the UI code. Ex, we don't buy a new TV because the news reports have bad signal.. maybe we just change the channel :)\nI'd suggest that analysis (and correction) should be an offline or stream processing job, similar to how it was at Twitter back when hadoop jobs were used for quality concern. Such a thing would be awesome, no doubt, but better fodder for a separate repo, as the concern is vast and primarily focuses on instrumentation.\nUnfortunately, not everyone is open about their pipeline or analysis code, so I don't have a lot of examples of similar things. Here are a couple that are at least in the same ballpark of analysis\nhttps://github.com/m3dev/Zipkin-Interrogator\nhttps://github.com/yanowitz/aggregate-trace-spike/blob/master/tracecollapse.rb\nThe thinking here is that by using some analysis techniques, you'd be able to detect problems in trace data, and possibly correct them, or maybe even make a linter.\nIf you are up to doing this, I'd be happy to open a repo for you!\n. the duration query depends a lot on how your apps are instrumented and\nwhich storage is in use. For example, we no longer support duration query\non the \"cassandra\" storage type.\n. The Endpoint class (and similar) originally used fields with the same size as the thrift structs. The size of port is i16, yet there's no unsigned 16bit field in java. So, a high port will overflow and be read as signed (negative) unless you do something like this: int port = endpoint.port & 0xffff\nIn hindsight, and with the codecs we have now (especially considering json is often used as thrift), we might just prefer the java type being an int and bounding it to only 16 unsigned bits. However, this would be a significant api break, so not likely worth it by itself.\n. if there's a bug, we should indeed fix it! I'll look at that one you mentioned\n. thanks for the help. I think I got stuck on the word \"query error\" earlier and didn't realize how clearly you pointed to the code bug https://github.com/openzipkin/zipkin/pull/1358\n. added an example to the javadoc that mentioned this https://github.com/openzipkin/zipkin/pull/1351\n. cc @openzipkin/core @openzipkin/elasticsearch \n. chose to add new methods to SpanStore for 128-bit support. Since no one has screamed, I'm assuming we're ok with this variation of semver!\n. cc @openzipkin/core @openzipkin/elasticsearch @jcarres-mdsol \n. revised elasticsearch so that it tokenizes both 64 and 128bit lengths. This will allow future apis to support exact retrieval on trace id without preventing retrieval on 64-bit (interop mode).\n. relates to #1226\n. possibly!\nit takes 15m for every attempt to sort this in travis, and quite a few\nvarious attempts done today, so not that interested in trying again.\ncircleci is currently stable on cassandra with the extra configuration\napplied to it. If that circle starts failing again, will get out the ulimit\nstick :)\nhttps://github.com/openzipkin/zipkin/commit/4597c8b6ca6cb6ffc7284008c32047756483ce19\n. cc @basvanbeek this uses code generated from the latest merge in zipkin-api\n. one thing this implies is cutting in two-steps, ex fixing here, releasing, then fixing elsewhere. Alternatively, we could make the lazy type not internal, and just design the new one like that.\nNote: another option is to generate the lazy closable by an auto-value extension. For example, the latest auto-value includes a lazy generator. This would allow us to not have to share libraries to get behavior like this. https://github.com/google/auto/tree/master/value/src/main/java/com/google/auto/value/extension/memoized\n. we'd assume 3) doubles the storage right?\n. Separate issue, but related. This reminds me that there is a bit of a quirk\nwhen representing the 128-bit trace id with a single number (as opposed to\ntwo longs or a byte array). Basically, it is harder to tell if the high\nbits are unset (since it is a signed variable length number). In hindsight\nI'd prefer some fixed length binary, or dual-long holder form.\n. sorry earlier I said I like 1, when I meant 2.. corrected my comment\n. hey, mick I like the idea of 3) particularly from the schema impact\nperspective.\nThe extra cost of storage should be ok for a lot of sites, but we need to\nbe careful about things that merge or aggregate. (ex. we don't want to\ncreate duplicate dependency links, mess up limit results etc). I have some\nother tests in progress locally and certainly will keep this point in mind.\nI think the most precise is to add another field to index on, but I agree\nit makes the schema more gnarly long term.. in other words, I think you've\nprobably the right choice here, but we should look at its impact closely.\nIn the mean time, I've pulled out most of the change, just as one to switch\nids to fixed length(high/low). Regardless of what we choose here, I think\nwe should make the move to fixed length trace identifiers.\nhttps://github.com/openzipkin/zipkin/pull/1394\n. hey, mick I like the idea of 3) particularly from the schema impact\nperspective.\nThe extra cost of storage should be ok for a lot of sites, but we need to\nbe careful about things that merge or aggregate. (ex. we don't want to\ncreate duplicate dependency links, mess up limit results etc). I have some\nother tests in progress locally and certainly will keep this point in mind.\nI think the most precise is to add another field to index on, but I agree\nit makes the schema more gnarly long term.. in other words, I think you've\nprobably the right choice here, but we should look at its impact closely.\nIn the mean time, I've pulled out most of the change, just as one to switch\nids to fixed length(high/low). Regardless of what we choose here, I think\nwe should make the move to fixed length trace identifiers.\nhttps://github.com/openzipkin/zipkin/pull/1394\n. 1.15 does the simple option which is to store the span twice. Besides the storage impact (which is only one during the transition), the only thing people might notice is that the json only includes 64-bit trace ids (even when instrumentation sent 128-bit ones)\nfrom CassandraStrictTraceIdFalseTest\njava\n  /**\n   * When {@link StorageComponent.Builder#strictTraceId(boolean)} is true and {@link\n   * Span#traceIdHigh} is not zero, the span is stored a second time, with {@link Span#traceId}\n   * zero. This allows spans to be looked up by the low bits of the trace ID at the cost of extra\n   * storage. When spans are retrieved by {@link Span#traceIdHigh} as zero, they are returned as\n   * {@link Span#traceIdHigh} zero because unlike the old schema, the original structs are not\n   * persisted.\n   */\n  @Test\n  @Override\n  public void getTrace_retrieves128bitTraceIdByLower64Bits_mixed() {\n. 1.15 does the simple option which is to store the span twice. Besides the storage impact (which is only one during the transition), the only thing people might notice is that the json only includes 64-bit trace ids (even when instrumentation sent 128-bit ones)\nfrom CassandraStrictTraceIdFalseTest\njava\n  /**\n   * When {@link StorageComponent.Builder#strictTraceId(boolean)} is true and {@link\n   * Span#traceIdHigh} is not zero, the span is stored a second time, with {@link Span#traceId}\n   * zero. This allows spans to be looked up by the low bits of the trace ID at the cost of extra\n   * storage. When spans are retrieved by {@link Span#traceIdHigh} as zero, they are returned as\n   * {@link Span#traceIdHigh} zero because unlike the old schema, the original structs are not\n   * persisted.\n   */\n  @Test\n  @Override\n  public void getTrace_retrieves128bitTraceIdByLower64Bits_mixed() {\n. issues like this are somewhat hard for us to help you with. that error says that the process is running garbage collection too much. This doesn't necessarily mean the thread reported is the cause of the garbage. You can also simply increase your heap size as an interim measure.\nhttps://docs.oracle.com/javase/8/docs/technotes/guides/troubleshoot/memleaks002.html\nIn general, you'd need to investigate what was in the heap when such a thing occurred and use a heap dump or profiling tool to identify what's going on. It may or may not be related to the data retrieved by MySQL.\nIf you find something that we can take action on, then go ahead and open a case at that point (or ask to re-open this one).\n. issues like this are somewhat hard for us to help you with. that error says that the process is running garbage collection too much. This doesn't necessarily mean the thread reported is the cause of the garbage. You can also simply increase your heap size as an interim measure.\nhttps://docs.oracle.com/javase/8/docs/technotes/guides/troubleshoot/memleaks002.html\nIn general, you'd need to investigate what was in the heap when such a thing occurred and use a heap dump or profiling tool to identify what's going on. It may or may not be related to the data retrieved by MySQL.\nIf you find something that we can take action on, then go ahead and open a case at that point (or ask to re-open this one).\n. releasing now in 1.14.2\n. duplicate\n. duplicate\n. Hi, zoltan.\nzipkin-ui's purpose is to allow someone to \"add\" the ui to a customized\nzipkin-server process. It isn't a replacement for zipkin-web.\nWe aren't supporting the zipkin web (proxy) use case anymore, though we\nhave made an example you can look at.\nhttps://github.com/openzipkin/docker-zipkin/tree/master/zipkin-ui\n. Hi, zoltan.\nzipkin-ui's purpose is to allow someone to \"add\" the ui to a customized\nzipkin-server process. It isn't a replacement for zipkin-web.\nWe aren't supporting the zipkin web (proxy) use case anymore, though we\nhave made an example you can look at.\nhttps://github.com/openzipkin/docker-zipkin/tree/master/zipkin-ui\n. sounds testable. could someone with experience raise a PR? I assume yarn\ncan still publish to npm repo right?\n. sounds testable. could someone with experience raise a PR? I assume yarn\ncan still publish to npm repo right?\n. yeah please don't use types besides string (default). They are only present\nas it was in the original thrift definition. However they are not\nsupported. When logging binary annotations for key lookup etc, stringify\nthem\n. > I suspect this problem is on the zipkin side.\n\ndepends on the level of abstraction we define for this :) we've not\n\"supported\" anything besides string for user-defined binary annotations and\nalmost certainly future formats won't support it. When people file issues\nlike this we should probably let them know that this is not a feature that\nworks (ex search won't work etc).\nThe JSON that comes from the Java server could be bad. If it sends the IDs\nas numbers instead of strings? Or are they sent as strings from the Zipkin\nserver?\nIds are sent as strings, but this is a binary annotation. We could define a\nstring encoding for when people use the unsupported I64 type, just I'm not\nsure how productive it is to invest in that.\n. I guess it is probably this..\n\ndef recordBinary(key: String, value: Any): Unit = {\n    record(Annotation.BinaryAnnotation(key, value))\n  }\nMeh.. ok :) I'll raise a PR since yeah only thing we could do to help\nis change zipkin-finagle to re-type the thing.\n. https://github.com/openzipkin/zipkin/pull/1373\n. fix going out now in 1.14.2\n. cc @nollbit @eirslett \n. merging tomorrow unless someone says not too cc @openzipkin/cassandra \n. @michaelsembwever no problemo\n. I don't think we should be dealing with upgrades at this point. One of the reasons we merged earlier was under the assumption that this is experimental. With that in mind, there should be no expectations around schema upgrade logic (that tends to very much clutter the code when in place)\n. @llinder @michaelsembwever @abesto I think there's something wrong with our circleci or something as the cassandra3 tests skip. This led me to merge on false-green. Today, I noticed locally that the tests fail, which is bad, so I reverted this commit and the associated cleanups.\nhttps://github.com/openzipkin/zipkin/commit/10b855231a6736552cf1e01064fdea3afa9704bc\nsorry about the confusion. Once it is working locally, please raise another PR. Probably best to start by reverting the revert commit above, so you can start with cleanups applied.\n. thanks. fixed!\n. cc @openzipkin/elasticsearch \n. Here's an example of the compression in action\n\n. It will be hard to answer your question without detail, but generally\nspeaking the zipkin-ui only shows data in zipkin storage. The only log-like\ndata in zipkin are annotations (timestamped events in a span). All tracers\nsupport annotations, so that's the closest it gets. if you want to\nfollow-up please use gitter as open-ended questions aren't great for github\nissues https://gitter.im/openzipkin/zipkin\n. cc @openzipkin/elasticsearch \n. I started working on this and found it became a bit of a mess to implement. In efforts to manually tokenize, we have much more complex queries as we double the aggregations needed to do searching. Moreover, even search by id queries are complex. It would be a more sustainable option to go with how we are doing this today.\nIf later we get complaints about memory usage or other knock-on effects, we can recommend those sites to keep the transition period between 64 and 128bit ids to a minimum. Once traces are 128-bit, there's no need for any tokenization anymore and at that point, normal keywords would work. Alternatively, such a site could choose to only store 64bit ids, which would have the same effect of not requiring mixed lookups (and by extension tokenization to support them)\n. note: not sure exactly what caused this in travis, but it appears to have recently picked up mysql 5.6.33\n. merging as needed to unbreak travis\n. agreed. we already take care to sanitize json\n. I don't expect the outcome of this issue to end up with a change to this\nrepo. Usually it is better to start exploratory troubleshooting with an\nissue closer to the repo that you are using (or even better start with\nhttps://gitter.im/openzipkin/zipkin). For example, it is unclear what your\ndeployment consists of.. ex is it an unmodified zipkin-server, or something\nelse? what's the storage type? what's the version?\nif unsure, paste the output of the following\ncurl -s your_zipkin_host:9411/info|jq .\ncurl -s your_zipkin_host:9411/health|jq .\nAlso, the drop-down behavior sounds like different applications are in use\nas the zipkin deployment shouldn't impact what the applications report. You\ncould grab an example trace to get to the bottom of this.\n. wondering if there's something about the version you are using? just noticing more formal support of PCF making its way into the codebase https://twitter.com/making/status/800708459041722368\nfeel free to open again if after bumping latest or otherwise, you are able to give more feedback on this. There's an unlikely scenario where the conditions searched returned results with traces that include exactly the same low bits, but have different and non-zero high bits.\nIt is possible that one of the traces didn't actually match the predicate. Accordingly, we have to filter them client-side to ensure we don't return false-positives.\nThis particular scenario affects both cassandra and elasticsearch, so extracting common code to deal with it. This code should be up tomorrow.\n. Still quite a lot of work left, particularly around testing this and making it configurable. Probably won't get a revision up until Monday as I'm flying most of tomorrow.\n. At the moment, this is what I'm going with for the storage component parameter. This would be mapped to an environment variable in the server/docker...\n```\ninterface Builder {\n/**\n * Zipkin historically had 64-bit {@link Span#traceId trace IDs}, but it now supports 128-bit\n * trace IDs via {@link Span#traceIdHigh}, or its 32-character hex representation.\n *\n * <p>This setting allows you to look up traces who have 128-bit trace ids by the lower-64 bits\n * using {@link SpanStore#getTrace(long)}. In most implementations, this implies a second index\n * which can be more expensive in terms of storage or memory than only supporting 128-bit\n * lookups via {@link SpanStore#getTrace(long, long)}\n *\n * <p>This should be enabled until all instrumentation report 128-bit trace IDs consistently,\n * and {@link SpanStore#getTrace(long)} is no longer in use.\n */\nBuilder mixedTraceIdLength(boolean mixedTraceIdLength);\n\nStorageComponent build();\n\n}\n```\nParticularly, this should allow sites who have transitioned to 128-bit ids to opt-out of tokenizing the traceId in elasticsearch or possibly another column in cassandra3.\n. pushed a code update which needs cleanup (particularly in MySQL which is the nastiest).\n. pushed a code update which needs cleanup (particularly in MySQL which is the nastiest).\n. changed flag to strictTraceId\n. changed flag to strictTraceId\n. wow.. actually green, huh\n. wow.. actually green, huh\n. pretty happy with docs and config. test backfilling and some code polishing needed, readying for merge\n. pretty happy with docs and config. test backfilling and some code polishing needed, readying for merge\n. added config integration tests.\nNEXT STEP:\n- see if I can make ES template opt-out of tokenization.\n- mysql cleanup\n- more test backfills\nETA sometime tomorrow\n. added config integration tests.\nNEXT STEP:\n- see if I can make ES template opt-out of tokenization.\n- mysql cleanup\n- more test backfills\nETA sometime tomorrow\n. got ES to conditionally tokenize and cleaned up some of the code. want to do one last cleanup and double-check the test cases are named and documented well. Will merge tomorrow \n. got ES to conditionally tokenize and cleaned up some of the code. want to do one last cleanup and double-check the test cases are named and documented well. Will merge tomorrow \n. ok only thing I'm doing between now and merge is backfilling tests for GroupByTraceId... so expect this in by tomorrow. I'll cut a version following\n. ok only thing I'm doing between now and merge is backfilling tests for GroupByTraceId... so expect this in by tomorrow. I'll cut a version following\n. thanks!\n. The current configuration for 5.7+ is noted in the readme.\nIf using MySQL 5.7, you'll need to disable ONLY_FULL_GROUP_BY\n$ mysql -uroot -e \"SET GLOBAL sql_mode=(SELECT\nREPLACE(@@sql_mode,'ONLY_FULL_GROUP_BY',''))\"\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-storage/mysql#applying-the-schema\nThere's also another way, which is to empty out sql mode like this;\nhttps://github.com/fabric8io/kubernetes-zipkin/commit/454f5b1ad86bcaa1e8749fcfa2a716c25e0f21bd\nA PR would be welcome to address this without configuration, just make sure\nthat the tests pass.\nex MYSQL_USER=root ./mvnw clean install -p zipkin-storage/mysql --am\n. cool beans.. all of the snapshot repos should be working, so please ping back how it goes.\n. fyi this (and other related change) is going out as 1.14.4\n. Great stuff. do you mind removing the workaround from the README and circle.yml? Once the latter is done, you can assume the README is no longer needed (as circleci uses mysql 5.7 and travis 5.6)\ncc @iocanel\n. Nice work!\n. fyi this (and other related change) is going out as 1.14.4\n. @ajacques @jcarres-mdsol not sure if you've updated to zipkin_spans.trace_id_high, yet, but if not you'll also need to add zipkin_annotations.trace_id_high\n. tested locally.. all good!\n. tested locally.. all good!\n. thanks for resurrecting this, @michaelsembwever \n. thanks for resurrecting this, @michaelsembwever \n. cc @michaelsembwever so I removed the indexing change from this commit just to separate the changes. I've not thought through the double-store thing, but I do believe that regardless of how we handle get by lower 64, this PR part is good change.\n. cc @michaelsembwever so I removed the indexing change from this commit just to separate the changes. I've not thought through the double-store thing, but I do believe that regardless of how we handle get by lower 64, this PR part is good change.\n. @openzipkin/cassandra fyi, this primarily changes the experimental cassandra3 impl to use fixed-length trace id in storage, particularly with the same high/low approach used in thrift and mysql.\n. @openzipkin/cassandra fyi, this primarily changes the experimental cassandra3 impl to use fixed-length trace id in storage, particularly with the same high/low approach used in thrift and mysql.\n. tested locally\n. tested locally\n. Let's layer any further work on #1364 over this change\n. Let's layer any further work on #1364 over this change\n. @shakuzen good point, though I think normally we don't create tests ending in Tests.. I think there was some C&P to blame here :P\n. @shakuzen good point, though I think normally we don't create tests ending in Tests.. I think there was some C&P to blame here :P\n. thanks for opening up the topic for conversation. good start and helpful.\nSo, at the component abstraction, we don't use anything that binds environment variables, as configuration can vary wildly depending on the framework used. Firstly, I'm curious if things \"just work\" if the java system provided trust store is present.. ex. -Djavax.net.ssl.trustStore\nIf not, we'd want to do the following:\n- identify the surface area we need for SSL configuration needed\n- figure out how to make it possible to configure\n  - ex via a direct option, or exposing or accepting a Cluster.Builder\ncc @openzipkin/cassandra \n. thanks for opening up the topic for conversation. good start and helpful.\nSo, at the component abstraction, we don't use anything that binds environment variables, as configuration can vary wildly depending on the framework used. Firstly, I'm curious if things \"just work\" if the java system provided trust store is present.. ex. -Djavax.net.ssl.trustStore\nIf not, we'd want to do the following:\n- identify the surface area we need for SSL configuration needed\n- figure out how to make it possible to configure\n  - ex via a direct option, or exposing or accepting a Cluster.Builder\ncc @openzipkin/cassandra \n. @michaelsembwever any thoughts on this? do you know if the default trust store works, or if configuration here is inevitable for TLS client auth\n. @michaelsembwever any thoughts on this? do you know if the default trust store works, or if configuration here is inevitable for TLS client auth\n. I think most are interested in the other PR. Let's go with that for now! Thanks for starting the gears turning, @jroes \n. I think most are interested in the other PR. Let's go with that for now! Thanks for starting the gears turning, @jroes \n. this looks like an accident\n. this looks like an accident\n. Good explanation and thanks for the help.\nps guessing long term stuff should be things to keep in mind here: https://github.com/openzipkin/zipkin-ui as once this is done\n. Good explanation and thanks for the help.\nps guessing long term stuff should be things to keep in mind here: https://github.com/openzipkin/zipkin-ui as once this is done\n. @openzipkin/elasticsearch @openzipkin/cassandra PTAL: this is documenting and enforcing things that hopefully are not surprising\n. @openzipkin/elasticsearch @openzipkin/cassandra PTAL: this is documenting and enforcing things that hopefully are not surprising\n. try testing with the build instructions at the bottom of zipkin-server/README if it works, add the property to the others in that file. I presume this is only about encryption, not auth? or is there truststore (via sys props or otherwise) that would handle this?\n. try testing with the build instructions at the bottom of zipkin-server/README if it works, add the property to the others in that file. I presume this is only about encryption, not auth? or is there truststore (via sys props or otherwise) that would handle this?\n. looks good. still needs to be added to zipkin-server/README (with the other cassandra settings), then merge time!\n. looks good. still needs to be added to zipkin-server/README (with the other cassandra settings), then merge time!\n. ps we'll likely need a cassandra3 port of this in the future. if you feel up to it, go ahead and put it in. Otherwise, someone else will need to replicate this work there\n. ps we'll likely need a cassandra3 port of this in the future. if you feel up to it, go ahead and put it in. Otherwise, someone else will need to replicate this work there\n. @ys hate to be a nag, but there's still no update to zipkin-server/README here.. do you mind adding the new env variables to there (for both cassandra and cassandra3)? that finishes the job!\n. @ys hate to be a nag, but there's still no update to zipkin-server/README here.. do you mind adding the new env variables to there (for both cassandra and cassandra3)? that finishes the job!\n. \ud83d\udc4d \n. \ud83d\udc4d \n. cc @openzipkin/elasticsearch and @dragontree101 @dan-tr @shakuzen and @mansu particularly (as they had a lot of interest here) \n. cc @openzipkin/elasticsearch and @dragontree101 @dan-tr @shakuzen and @mansu particularly (as they had a lot of interest here) \n. circleci died trying to start elasticsearch 5, so swapped and have travis running it (which has more resources)\n. circleci died trying to start elasticsearch 5, so swapped and have travis running it (which has more resources)\n. here's the playbook I used to test this.\nInstall elasticsearch 5 by downloading and running it\nbash\n$ curl -SL https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.0.0.tar.gz | tar xz\n$ elasticsearch-*/bin/elasticsearch\nAdd a custom pipeline to it\nbash\n$ curl -X PUT -s localhost:9200/_ingest/pipeline/zipkin -d '{\n  \"description\" : \"add collector_timestamp_millis\",\n  \"processors\" : [\n    {\n      \"set\" : {\n        \"field\": \"collector_timestamp_millis\",\n        \"value\": \"{{_ingest.timestamp}}\"\n      }\n    }\n  ]\n}'\nStart zipkin, pointed at that pipeline\nbash\n$ STORAGE_TYPE=elasticsearch ES_HOSTS=http://localhost:9200 ES_PIPELINE=zipkin java -jar zipkin-server/target/zipkin-*exec.jar\nAdd some traces (ex using one of the examples), then check the resulting index\nbash\n$ curl -s localhost:9200/zipkin-2016-11-15/_search|jq '.hits.hits[]._source.collector_timestamp_millis'\n. here's the playbook I used to test this.\nInstall elasticsearch 5 by downloading and running it\nbash\n$ curl -SL https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.0.0.tar.gz | tar xz\n$ elasticsearch-*/bin/elasticsearch\nAdd a custom pipeline to it\nbash\n$ curl -X PUT -s localhost:9200/_ingest/pipeline/zipkin -d '{\n  \"description\" : \"add collector_timestamp_millis\",\n  \"processors\" : [\n    {\n      \"set\" : {\n        \"field\": \"collector_timestamp_millis\",\n        \"value\": \"{{_ingest.timestamp}}\"\n      }\n    }\n  ]\n}'\nStart zipkin, pointed at that pipeline\nbash\n$ STORAGE_TYPE=elasticsearch ES_HOSTS=http://localhost:9200 ES_PIPELINE=zipkin java -jar zipkin-server/target/zipkin-*exec.jar\nAdd some traces (ex using one of the examples), then check the resulting index\nbash\n$ curl -s localhost:9200/zipkin-2016-11-15/_search|jq '.hits.hits[]._source.collector_timestamp_millis'\n. no worries. nice work.\n. no worries. nice work.\n. ps according to confluent, it is still fine to use kafka 0.8 consumers and producers http://docs.confluent.io/3.0.0/upgrade.html\nSince zipkin-server is standalone, it might not need to change. However, zipkin-reporter-java might want a kafka010 module since it shares the classpath with production apps (who might want to upgrade)\n. ps according to confluent, it is still fine to use kafka 0.8 consumers and producers http://docs.confluent.io/3.0.0/upgrade.html\nSince zipkin-server is standalone, it might not need to change. However, zipkin-reporter-java might want a kafka010 module since it shares the classpath with production apps (who might want to upgrade)\n. How does this impact you? Most of the time zipkin is a standalone server,\nso the library version conflict is of no concern (black box)\nAre you running a custom server, or is there some feature in 0.10 which you\nneed zipkin to use?\n. How does this impact you? Most of the time zipkin is a standalone server,\nso the library version conflict is of no concern (black box)\nAre you running a custom server, or is there some feature in 0.10 which you\nneed zipkin to use?\n. @StephenWithPH Sorry, you said \"newer versions of the consumer library use Kafka itself for offset tracking\" (still curious about custom build or not as that impacts the direction here)\nCan you elaborate on this? Is this an automatic feature or would we need to expose configuration for it. Code example would be best.\nAlso, are you saying that now brokers self-discover? so you pass a broker url and that's it?\nI'm also interested in progressing this, but need to pick your brain a bit. I think we'll need to do both 0.8 and 0.10, so this is the tricky part.. @StephenWithPH Sorry, you said \"newer versions of the consumer library use Kafka itself for offset tracking\" (still curious about custom build or not as that impacts the direction here)\nCan you elaborate on this? Is this an automatic feature or would we need to expose configuration for it. Code example would be best.\nAlso, are you saying that now brokers self-discover? so you pass a broker url and that's it?\nI'm also interested in progressing this, but need to pick your brain a bit. I think we'll need to do both 0.8 and 0.10, so this is the tricky part.. pps are you using docker? If so, this gives us some more options on how to address this... pps are you using docker? If so, this gives us some more options on how to address this... incidentally, 0.10 has been more requested than 0.9. I'd prefer not to\nthink about 0.9 if we can. Fine by you? :)\nThanks for the details! I think we can sort something out for sure.\n. incidentally, 0.10 has been more requested than 0.9. I'd prefer not to\nthink about 0.9 if we can. Fine by you? :)\nThanks for the details! I think we can sort something out for sure.\n. First step would be to make a kafka10 collector impl, similar to what we\ndid in reporter\nhttps://github.com/openzipkin/zipkin-reporter-java/tree/master/kafka10\nThis would need to also have a corresponding auto-configuration module.\nThe last yard would be how to stitch it in. we can resolve that one way or\nanother. For example, in docker we can see if KAFKA_BROKERLIST (or whatever\nname is used) is set while KAFKA_ZOOKEEPER is not. flipping on that, we can\nadjust the server classpath accordingly.\nThis would allow old versions to continue w/ 0.8 and opt-in to 0.10\nDo you have cycles to carve a kafka10 collector impl?\n. First step would be to make a kafka10 collector impl, similar to what we\ndid in reporter\nhttps://github.com/openzipkin/zipkin-reporter-java/tree/master/kafka10\nThis would need to also have a corresponding auto-configuration module.\nThe last yard would be how to stitch it in. we can resolve that one way or\nanother. For example, in docker we can see if KAFKA_BROKERLIST (or whatever\nname is used) is set while KAFKA_ZOOKEEPER is not. flipping on that, we can\nadjust the server classpath accordingly.\nThis would allow old versions to continue w/ 0.8 and opt-in to 0.10\nDo you have cycles to carve a kafka10 collector impl?\n. first feedback is thanks!\nSecond is I wouldn't prefix these properties with kafka10. Not only will\nthese not be in the same classpath together, but also the detection\nproperty KAFKA_BOOTSTRAP_SERVERS doesn't exist yet (so you can't clash with\nit).\nThis property is used in spark-streaming, and wouldn't be something we\nwould change for kafka11\n. Hi, Dan. Let's move this to a pull request? it is a lot easier to deal with\ndifferent threads of discussion this way.\nwrt how we address integration, I think the best way is to act like sqs,\nwhere we package a module jar (and this includes a zipkin-server-kafka file\nlike\nhttps://github.com/openzipkin/zipkin-aws/blob/master/autoconfigure/collector-sqs/src/main/resources/zipkin-server-sqs.yml\n)\nThe pom would include a module file as well, like this, except different\nexcludes:\nhttps://github.com/openzipkin/zipkin-aws/blob/master/autoconfigure/collector-sqs/pom.xml\nRunning will be a little messier than this because of classpath\nhttps://github.com/openzipkin/zipkin-aws/tree/master/autoconfigure/collector-sqs#running\nFor example, we'll probably have to customize the docker image based on\npresence of the KAFKA_BOOTSTRAP_SERVERS variable, and move old jars out of\nthe way.. we may end up lucky enough to not need to do that. Basically,\nonce there's a module dir the next step is seeing how lucky we are.. in\nbest case classpath order works and we don't need to hack the all jar to\ntake out the kafka08 stuff.\nIn the future, we could make a thin launcher\nhttps://github.com/dsyer/spring-boot-thin-launcher as the default image\n. PS rationale is that unlike SQS, we have existing deployments with Kafka.\nUpgrading these sites should be easy a possible (vs creating a new image),\nso let's try to get the docker image working.. better to try and fail than\nnot try and have people upset at now needing to think about multiple images\nwhen they formerly didn't.\n. will fix immediately. thanks\n. will fix immediately. thanks\n. this looks like it is in brave...\n. this looks like it is in brave...\n. so when this is sorted, we'll need a bump in brave.\n. so when this is sorted, we'll need a bump in brave.\n. Here's a failing test:\n``` java\n  static final class Foo {\n    @Override\n    public String toString() {\n      return new String(JsonCodec.write(new Buffer.Writer() {\n        @Override public int sizeInBytes(Foo value) {\n          return 1;\n        }\n    @Override public void write(Foo value, Buffer buffer) {\n      throw new RuntimeException(\"buggy encoder\");\n    }\n  }, this), UTF_8);\n}\n\n}\n@Test\n  public void doesntLoopOnToStringError() {\n    new Foo().toString();\n  }\n```\n. I think an encoding bug was fixed between the version of zipkin brave is using and now, but here's a permanent fix https://github.com/openzipkin/zipkin/pull/1407\n. releasing fix as zipkin 1.16.2 and will propagate this into reporter, brave etc patch releases\n. zipkin-reporter 0.6.9 on the way (just dependency change to zipkin 1.16.1)\n. cutting brave 3.15.3, zipkin-finagle 0.3.2 (pinned to zipkin 1.16.2, zipkin-reporter 0.6.9)\n. NP this class of bug is insidious and needs to be killed immediately!\n. duplicate of #1159 please take a look there and comment as needed!\n. we run 2 CI jobs right now, travis and also circle ci. If you pick one of them in this PR and instead of starting ES, start docker, we can get experience. We'd still be covered as the other is running ES normally. sg?\n. @bsideup that's a flake that's unrelated. I'll see about fixing it\n. so looks like progress for sure. I particularly like the enclosed runner and think regardless of testcontainers we should switch to that. Really excited about this.\nAs you guessed I'm sure, I'm not keen on publishing a new test jar to avoid copy/pasting a 100 line text fixture. Copy/paste is fine during experimentation. Can you please reverse that part?\nDetails of why: -http and not already have setup due to the clients involved (ex sometimes I add a logging interceptor to the http one). Also, if this experiment proves itself, we'd put the testcontainers fixture in the existing zipkin test jar, not in the elasticsearch one. That would obviate a lot of the duplication. TL;DR; I really don't want to taint our public maven namespace for tactical reasons. Let's wait it out and see.\nAlso, I noticed the test duration went up by 2+ minutes, which is a significant bump. Wondering if this is mostly downloading the elasticsearch image or not. 2 minutes is a big enough issue to dig into as if we are to use testcontainers generically, we might be significantly increasing our build time as we port this to more things. Let me know what the source of the lag is (if you notice where it is). For example, a travis cache config might help (or might not)\n. ahh right. of course. we are running both now. geez we need to instrument\nmaven and surefire with zipkin, right?\nThanks for the analysis!\n. good to know. thanks!\n. Thanks, for whack-a-mole'ing!\n. we aren't setup to support Kibana as a first class citizen, since there's\nreally no direct relationship here. We are focused on supporting the zipkin\nUI.\nYou might have more luck going through normal Kibana support channels and\nreporting back anything interesting you find.\n. just in case.. @openzipkin/elasticsearch does anyone happen to know what this might imply? I know that in zipkin itself annotation searches are nested terms queries. @mansu well the point in suggesting reliable channels for Kibana questions isn't a question about if it is a valid use case, or not. Rather, we haven't yet established anything near reliability for a place to come to for Kibana support. We neither document, run or test it for starters! Only thing we have right now is a note about timestamps (and an ad-hoc test I did once to see if maybe it worked or not). Even if we did support Kibana, we'd want the community to know where to get more help, or help when one of those who know Kibana aren't looking.\nAnyway...\nI'm glad you're interested in helping, as that'd be the first step towards supporting something sustainably. If you come up with something, let's put it in the README in a section for Kibana. Then, maybe @bsideup and/or I can help make sure it is tested. That way, we don't accidentally break it, which would be sad.\nSound good!. well, it could only be better if you use elasticsearch :) ps whatever isn't good should at least be noted for improvement in with a screen shot of what's better (since good is subjective and not always guessable) https://github.com/openzipkin/zipkin-ui/issues\nAnyway, in case it helps, here's a complex query that the http implementation sends on the wire. It might help you figure out what to do (or not).\nex.\njava\nQueryRequest.builder().serviceName(\"service\")\n    .spanName(\"methodcall\")\n    .addAnnotation(\"custom\")\n    .addBinaryAnnotation(\"BAH\", \"BEH\").build();\nbecomes this http request:\n```\nPOST zipkin/span/_search?allow_no_indices=true&expand_wildcards=open&ignore_unavailable=true\n...\n{\n  \"size\" : 0,\n  \"query\" : {\n    \"bool\" : {\n      \"must\" : {\n        \"match_all\" : { }\n      },\n      \"filter\" : {\n        \"bool\" : {\n          \"must\" : [ {\n            \"range\" : {\n              \"timestamp_millis\" : {\n                \"from\" : 0,\n                \"to\" : 1479693481467,\n                \"include_lower\" : true,\n                \"include_upper\" : true\n              }\n            }\n          }, {\n            \"bool\" : {\n              \"should\" : [ {\n                \"nested\" : {\n                  \"query\" : {\n                    \"term\" : {\n                      \"annotations.endpoint.serviceName\" : \"service\"\n                    }\n                  },\n                  \"path\" : \"annotations\"\n                }\n              }, {\n                \"nested\" : {\n                  \"query\" : {\n                    \"term\" : {\n                      \"binaryAnnotations.endpoint.serviceName\" : \"service\"\n                    }\n                  },\n                  \"path\" : \"binaryAnnotations\"\n                }\n              } ]\n            }\n          }, {\n            \"term\" : {\n              \"name\" : \"methodcall\"\n            }\n          }, {\n            \"nested\" : {\n              \"query\" : {\n                \"bool\" : {\n                  \"must\" : [ {\n                    \"term\" : {\n                      \"annotations.value\" : \"custom\"\n                    }\n                  }, {\n                    \"term\" : {\n                      \"annotations.endpoint.serviceName\" : \"service\"\n                    }\n                  } ]\n                }\n              },\n              \"path\" : \"annotations\"\n            }\n          }, {\n            \"nested\" : {\n              \"query\" : {\n                \"bool\" : {\n                  \"must\" : [ {\n                    \"term\" : {\n                      \"binaryAnnotations.key\" : \"BAH\"\n                    }\n                  }, {\n                    \"term\" : {\n                      \"binaryAnnotations.value\" : \"BEH\"\n                    }\n                  }, {\n                    \"term\" : {\n                      \"binaryAnnotations.endpoint.serviceName\" : \"service\"\n                    }\n                  } ]\n                }\n              },\n              \"path\" : \"binaryAnnotations\"\n            }\n          } ]\n        }\n      }\n    }\n  },\n  \"aggregations\" : {\n    \"traceId_agg\" : {\n      \"terms\" : {\n        \"field\" : \"traceId\",\n        \"size\" : 10,\n        \"order\" : {\n          \"timestamps_agg\" : \"desc\"\n        }\n      },\n      \"aggregations\" : {\n        \"timestamps_agg\" : {\n          \"min\" : {\n            \"field\" : \"timestamp_millis\"\n          }\n        }\n      }\n    }\n  }\n}\n```\n. >\n\n@adriancole https://github.com/adriancole , thanks for the sample\nelasticsearch query. I was having trouble querying the serviceName too,\nuntil I realized from your code snippet that the annotations are stored as\na nested object in elasticsearch, which requires a different type of query.\nOne question I had though was why the serviceName is stored for every\nannotation. I would assume serviceName is applicable to the whole span, and\nshould be stored as a field on the span. Under what situations can you have\ndifferent serviceNames for the same span?\nIn an RPC span, you have a different serviceName for the client and the\nserver. Also, there's a special case for \"ca\"(Client Address) and \"sa\"\n(Server Address) binary annotations. our api contract is such that service\nname is a primary filter, so it is like.. I want a trace where service A\nadded annotation foo=bar.\n\nhttp://zipkin.io/zipkin-api/#/paths/%252Ftraces\nHope that answers the question.\n. this is historical See https://github.com/openzipkin/zipkin/issues/939 for\na proposal to use a new model.\n. closing as it is antiquated especially now we have a different model. ps it will be non-storage, too. ex we ought to do the same thing for Kafka.\n. cool start! wanna add a screen shot?\ncc @openzipkin/elasticsearch @rogeralsing @eirslett @basvanbeek . cc'ing other folks who have interest in zipkin + kibana @marcingrzejszczak @dragontree101 @fedj @dan-tr @C-Otto . @haf what do you think about something like this and logary?. @fedj mind sharing a screen shot of the logs detail? is it a pop-out or a new tab?. thanks for the help on this. It will give folks a way to experiment with log correlation without visible impact to those who aren't interested.. if you don't mind, please verify the snapshot exec jar acts as expected!\nhttp://oss.jfrog.org/oss-snapshot-local/io/zipkin/java/zipkin-server/1.16.3-SNAPSHOT/\noh and probably we need to update zipkin-server/README.md with details on this feature.. yeah noticed that post-merge, too. we need to add the description of this PR or similar to zipkin-server/README.md\n. actually, the detailed stuff from the description should go to zipkin-ui/README.md and only description of the env variable to zipkin-server/README.md that way the latter doesn't grow endlessly\n. :thumbsup: thanks for the follow-up\n. shortly, brave will include SLF4J support which could integrate with this https://github.com/openzipkin/brave/pull/389. shortly, brave will include SLF4J support which could integrate with this https://github.com/openzipkin/brave/pull/389. dupe of #1229. the code @tramchamploo mentions that limits the result count in elasticsearch to 1024 is here https://github.com/openzipkin/zipkin/blob/master/zipkin-storage/elasticsearch/src/main/java/zipkin/storage/elasticsearch/ElasticsearchSpanStore.java#L140. @tramchamploo ps can you check with latest zipkin 1.20+ There's a query-limit relating change there? (Our tests now look for 1000 dependency links.. maybe I can change that to 1025 :) ) cc @lijunyong. @hvandenb so far I've seen nothing open sourced on this topic, rather a few people experimenting on their own.\n@tramchamploo  @naoman, @mansu do any of you have code to share around vizceral? I know that has been a hot topic and something clearly wanted (even if the impl is imperfect due to limited data in zipkin).\nBy sharing with others, someone might be able to progress this as it has been stuck for over 3 months now. This could be as simple as a gist or code paste.\n. This issue was moved to openzipkin/zipkin-spark-streaming#1. Specifically, the ids are opaque,, 64 or 128 bit for the trace id and 64\nbit for span ID. in json and http headers, they are encoded as strings\nhttps://github.com/openzipkin/b3-propagation\n. http://zipkin.io/zipkin-api/#/paths/%252Ftrace%252F%257BtraceId%257D\nis an example for the http api\n. One of the main reasons our api supports key/value lookup is for\ncorrelation. For example, when creating a root span, you can add a binary\nannotation of some external ID. After that is in place, you can lookup\ntraces given it. This is how a lot of people address the use case.\nZipkin isn't able to reuse all types of ids as a trace id, and that isn't\nits goal. It isn't practical to try to convert all storage and propagation\nformats to variably shaped ids. That said, many types of ids can be encoded\nas 64 or 128-bits and therefore encodable as a trace ID. Some are doing it\n(for example, take out the hyphens of a UUID to make it a 128-bit ID).\nThere are a number of related issues on correlating ids you might want to\nlook at:\nhttps://github.com/openzipkin/openzipkin.github.io/issues/48\nhttps://github.com/openzipkin/b3-propagation/issues/4\n. @xiaoshuang-lu We already have a solution for storing random lookup data. It is called a binary annotation, and you can search on it using the api.\nfor example, I can add \"external-id\" -> \"really long variable string even with unicode\" into the root span, and search on it using the zipkin api http://zipkin.io/zipkin-api/#/paths/%252Ftraces/get/parameters/annotationQuery\nWe will not rewrite the entire storage layer, design a new propagation format and also break all libraries just because someone doesn't want to use the binary annotation feature. The value just isn't there.\nMany systems do not choose to use variable length strings as a primary id, particularly those who are metrics in nature, but also many who are not. For example, span storage is directly related to the size of the data sent. Fixed length ids as exist today use less space, and have fixed overhead when propagated across process boundaries. Right now, you can propagate a zipkin context in binary in a fixed-width payload 32 bytes, and someone can portably unmarshal that because they don't have to be concerned with variable length strings. There are various other reasons including index optimization.\nI'm going to close this issue, but feel free to follow-up on gitter if you'd like to continue to ask questions about the design https://gitter.im/openzipkin/zipkin. lol read more about the design defect here :)\nhttp://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36356.pdf\npeace\n. Why do you need a 128bit number? If you want to use a UUID as hex, take our\nthe hyphens.\nIf you have some concrete use case you are trying to get to work, please\nmention (ex language library and if you are trying to make headers or a\nspan or what)\nOn 5 May 2017 4:27 pm, \"Martin Ambrus\" notifications@github.com wrote:\n\nhi guys, could anyone point me to the right direction as to how to convert\nstring into a 128 bit integer, please? I've been searching for a day and\nstill have no clue how to convert uuids to usable traceIds. thanks in\nadvance :)\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1418#issuecomment-299409894,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wkFVy5CW_F1YaBSJ_bpCC6AG7-yks5r2t1tgaJpZM4K6gZw\n.\n. Why do you need a 128bit number? If you want to use a UUID as hex, take our\nthe hyphens.\n\nIf you have some concrete use case you are trying to get to work, please\nmention (ex language library and if you are trying to make headers or a\nspan or what)\nOn 5 May 2017 4:27 pm, \"Martin Ambrus\" notifications@github.com wrote:\n\nhi guys, could anyone point me to the right direction as to how to convert\nstring into a 128 bit integer, please? I've been searching for a day and\nstill have no clue how to convert uuids to usable traceIds. thanks in\nadvance :)\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1418#issuecomment-299409894,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wkFVy5CW_F1YaBSJ_bpCC6AG7-yks5r2t1tgaJpZM4K6gZw\n.\n. I've been meaning to move the component based config to their corresponding autoconfigure modules, but didn't know how. would profiles be the answer to that as well? cc @shakuzen . that said, if we moved them, it might be harder to figure out where to look for settings.. ps I think we'd need to look for a README link to change as it is... I think the minimum change needed here is to change the README links.. I'll do that on the way in.. closed via 890bf402c01bc72197f295c4760f124317b9f52c. Thanks for opening the issue! This is a bit of a known issue. I assume you\ncannot create a DNS A record set for those IPs?\n. as long as you are treating the same as if they were in an A record set, we\ncan accommodate this.\n. https://github.com/openzipkin/zipkin/pull/1422. thanks @nicolai86 for reporting this. cc @openzipkin/elasticsearch . tested locally, by setting ES_HOSTS=http://1.1.1.1:9200,http://localhost:9200\n\nThen, I did curl -s localhost:9411/api/v1/services which succeeded.\nHere's what self-tracing showed (note.. it doesn't show the connection failure, but that's a Brave/OkHttp tracing issue)\n\n. revised example which shows a slow trace where the connection auto-recovered. will be in zipkin 1.17.1. thanks tons!. I'll try to help with this, or you can raise a PR on the zipkin-docker repo to make something more configurable. It is a step backwards to rely on a big image even if travis can pull it down quickly enough. I'm often in slow networks and am usually the one maintaining this code. \nWeird that testcontainers can't use mariadb.. can this be addressed? I'd like to not introduce any more dependencies that we'd have to track separately.. for example, if you need a more powerful user or something in order to drop schema, you could add it here: https://github.com/openzipkin/docker-zipkin/blob/master/mysql/install.sh#L35\nIn other words, let's try to make this all coherent. We use the same test images in our tests and ad-hoc docker testing. One docker image version... one base layer.\nZipkin's complex enough without having to remember a matrix of driver and docker images :P\n-- all this said, I appreciate that if this were a project that didn't publish its own images for reasons described, I'd also be tempted to use whatever's the standard image was.. thanks for the tenacity sergei!\n. great! looks like it worked fine. 2 small things then merge time!\n change description of the PR to reflect today :)\n unhook circleci as I think that's still running the old mysql manually\n. thanks for checking me. yeah, let's keep them, though we should probably add a comment to the circle ci reminding us that we aren't using docker for services intentionally (to ensure docker-free tests work). yeah just a reminder of why we aren't adding docker in case we forget.\nIIRC we are not yet relying completely on it, nor ask those developing local to always have docker installed. By running on circle w/o docker we have a backup (in case we goofed in our new tests) and also implicitly test the non-docker way.\nAt some point we may yank the non-docker way of course.. thanks again!. @shakuzen good catch. I opted out of retrolambda for modules that actually need java 8 (as opposed to lazily using it just for lambdas). this was fixed in a more recent version of zipkin. can you upgrade?. here's the related fix https://github.com/openzipkin/zipkin/pull/1390. no problem. glad you have it working now!\n. We probably need something like this, which is from the zipkin-reporter-java producer\njava\n    /**\n     * By default, a producer will be created, targeted to {@link #bootstrapServers(String)} with 0\n     * required {@link ProducerConfig#ACKS_CONFIG acks}. Any properties set here will affect the\n     * producer config.\n     *\n     * <p>For example: Reduce the timeout blocking on metadata from one minute to 5 seconds.\n     * <pre>{@code\n     * Map<String, String> overrides = new LinkedHashMap<>();\n     * overrides.put(ProducerConfig.METADATA_FETCH_TIMEOUT_CONFIG, \"5000\");\n     * builder.overrides(overrides);\n     * }</pre>\n     *\n     * @see ProducerConfig\n     */\n    public final Builder overrides(Map<String, String> overrides) {\n      properties().putAll(checkNotNull(overrides, \"overrides\"));\n      return this;\n    }. https://github.com/openzipkin/zipkin/pull/1429. some are using a messaging span approach discussed here https://github.com/openzipkin/zipkin/issues/1243. you can try something like we have in kafka, except rabbit has message properties. This means you can inject/extra the trace ID from headers instead of the kafka key.\nhttps://gist.github.com/adriancole/76d94054b77e3be338bd75424ca8ba30\nCheck with spring-cloud-sleuth (if you are using that) or brave for related functionality (raise an issue if one doesn't exist). >\n\nIt makes me feel strange that only few information can be found for the\nintegration of zipkin(brave) and mq.\nNot strange, just we don't develop features until they are requested is\nall. If you look at the issues there has been no-one requesting rabbit\nbefore you (at least not here). There have been a lot of folks asking in\ngeneral on zipkin, and that's how the one-way span thing came to be.\n\nAnyway, if you need a hand, feel free to hop on\nhttps://gitter.im/openzipkin/zipkin If others end up interested in this, it\ncould become a feature.\n. starting this now.. I suspect it will take all week to move off the old library https://github.com/openzipkin/zipkin/pull/1495. think I've finally an idea for what to do next..\nwe can either say \"More info\" instead of \"Debug info\", since the ids aren't necessarily debug in nature, or.. make this a debug toggle.\nFor example, occasionally we have overlap in tags (since client+server spans are the norm). The debug toggle could add the host information which could clarify these.\n\njust thoughts. @cburroughs @jcarres-mdsol wdyt?\n. nice! thanks for the help\n\nIf I understand it right, with v2/simplified formats, having data from\nmultiple addresses on a single span is going to become even less common,\nand I'm wary of dynamically mucking with the number of table columns.\nfair point. on conflict, there's always the json\n. if you use the \"cs\" \"sr\" practice of one-way spans, the UI should work properly, now. Here's an example https://github.com/openzipkin/brave/tree/master/brave/src/test/java/brave/features/async\n\nI don't expect us to continue with \"ms\" \"mr\". thanks so much for sharing!\n. Zipkin will not compete with Splunk and ELK. Some companies may have the\nstaff to take on such a burden, but that isn't the case here. Scope is\nextremely important, and taking on the responsibility of a logging stack\n(and replacement for SLF4J etc) is way beyond reasonable for this project.\nThose wanting to log events can use annotations, but that's far different\nthan replacing logging.\n. Closing because the question has been answered. Don't expect Zipkin to assume the responsibility of a full-bore logging system.\nThe log-like features that exist in zipkin will continue, though, and we will continue helping people integrate more tightly with logging systems, for example the recent change to support log urls.\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-server#configuration-for-the-ui\nZipkin's scope is different than OpenTracing, which has recently started entering the general purpose logging space. For example, Zipkin is a system and OpenTracing is a library api. Some providers of OpenTracing api have logging features and will encourage their use. That doesn't imply Zipkin will. These are different projects, and the logging aspect of OpenTracing isn't universally popular.. Closing because the question has been answered. Don't expect Zipkin to assume the responsibility of a full-bore logging system.\nThe log-like features that exist in zipkin will continue, though, and we will continue helping people integrate more tightly with logging systems, for example the recent change to support log urls.\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-server#configuration-for-the-ui\nZipkin's scope is different than OpenTracing, which has recently started entering the general purpose logging space. For example, Zipkin is a system and OpenTracing is a library api. Some providers of OpenTracing api have logging features and will encourage their use. That doesn't imply Zipkin will. These are different projects, and the logging aspect of OpenTracing isn't universally popular.. how about grabbing a slot here?\nhttps://docs.google.com/document/d/16qNbn6IU43FuNMIie0rgWswIjWUCZUU0yNHxeR1vlRk/edit#\nwe can do a remote talk (bear in mind that Nic is in pacific time iirc)\n. I've added world write to this (until people give me email addresses\nto do a better job). I hope folks can help outline and clarify what's\nbeen captured here. In many cases, it is copy/paste from this issue.\nhttps://docs.google.com/document/d/1TdgBjMI0KhyXPmlrfVQ_d3wbtbydWQh5dcNzYe4oD2Y/edit\n. I'm guessing you are using scribe as opposed to kafka? Do you have a scribe\nsetup or are using that because htrace doesn't yet support http?\n. My guess is that this has to do with url encoding. Can you patch htrace to\nuse encodeBase64URLSafeString instead of encodeBase64String?\n. yeah it would be a buffer/pipeline issue. you could buffer more in instrumentation or introduce buffering on the server.\nEx in kafka it is safe to try to collect more because the backlog is persistent. I think in zipkin-aws there's another buffering layer used for SQS for a similar problem.\ncc @llinder @mansu for thoughts.. I think we've had a discussion about when Kafka is present, we can easily\ndo this (since kafka is persistent and we don't need to worry about losing\ndata)\nIf you are referring to the normal HTTP endpoint, it would be a little more\ncomplex because we'd have to decide where to pool the data (so that N http\nrequests don't mean N buffers which would blow up memory)\nI know that many tracers have a buffer configuration which in a pinch can\nhelp (provided they can be adjusted)\n. I think we've had a discussion about when Kafka is present, we can easily\ndo this (since kafka is persistent and we don't need to worry about losing\ndata)\nIf you are referring to the normal HTTP endpoint, it would be a little more\ncomplex because we'd have to decide where to pool the data (so that N http\nrequests don't mean N buffers which would blow up memory)\nI know that many tracers have a buffer configuration which in a pinch can\nhelp (provided they can be adjusted)\n. thanks for the notes. we do need to make a deployment guide, and this stuff\nwill need to become a part of it.\n. thanks for the notes. we do need to make a deployment guide, and this stuff\nwill need to become a part of it.\n. These are great notes. Thanks for sharing.\n. mind making a before and after screen shot? I suspect 4 spaces is ok vs 2... @eirslett any strong opinion?. looks better! do you mind having a go at a unit test?\non spacing, I don't care as they both look fine.. ok I'd remove the first part (which looks the same, but is not), and then I'll merge. Scout's honor I will create a test class afterwords sometime and ping you for review. cheers!. ok I'd remove the first part (which looks the same, but is not), and then I'll merge. Scout's honor I will create a test class afterwords sometime and ping you for review. cheers!. thanks @jakubhava!. thanks @jakubhava!. >\n\nwe are trace is very big, and have a lot of spans,\ndefine big? spans are typically no more than 1KiB in size (often far less\nthan that, like hundreds of bytes)\nso zipkin's ui very slow when i choose span name or into some big trace.\ncan you show a profile from Chrome debug tools?\ni use zipkin 1.16 and use es store?\nok\ndoes have any way to speed ui's performance?\nprobably, but we'd need to try and understand what we are optimizing for.\n. >\nwe are trace is very big, and have a lot of spans,\ndefine big? spans are typically no more than 1KiB in size (often far less\nthan that, like hundreds of bytes)\nso zipkin's ui very slow when i choose span name or into some big trace.\ncan you show a profile from Chrome debug tools?\ni use zipkin 1.16 and use es store?\nok\ndoes have any way to speed ui's performance?\nprobably, but we'd need to try and understand what we are optimizing for.\n. yeah I think this could be optimized. @cburroughs are you interested in cracking knuckles on some performance work?. yeah I think this could be optimized. @cburroughs are you interested in cracking knuckles on some performance work?. @cburroughs based on the screen shot.. looks like roughly 10k spans at a maximum depth of 7. dunno about the size of the json @dragontree101 ?. @cburroughs based on the screen shot.. looks like roughly 10k spans at a maximum depth of 7. dunno about the size of the json @dragontree101 ?. The \"10k span problem\" was discussed at the most recent tracing workshop. We are not alone, for example even Google's UI doesn't render with thousands of spans per trace.\n\nHere are some options\n aggregating the spans and putting in timing, possibly on the collection side.\n  * basically adding a tag that says spans were dropped\n drop span and replace with annotation\n  * have the UI code convert large amounts of spans into annotations before they are rendered\n  * note large orders of annotations could also crash the UI so should be tested\n add a control to collapse parts of a trace that are too large\n  * leave the span data alone, but collapse spans which have over a certain amount of children\n add a dependency graph per trace\n  * when there are a lot of spans, a trace ID scoped dependency graph could be useful. Thanks for the tips, folks. I don't think many have mentioned concrete\nways to improve UI performance. Lucky for us though\n@igorwwwwwwwwwwwwwwwwwwww has been active in related performance work\nand who knows... might be able to carry this forward.\nThe most concrete way to improve the UI is available, capable hands.\nWe don't have fulltime UI staff, so appreciate volunteers who offer\nadvice or take time to fix things. Thank you.\n. Thanks for the tips, folks. I don't think many have mentioned concrete\nways to improve UI performance. Lucky for us though\n@igorwwwwwwwwwwwwwwwwwwww has been active in related performance work\nand who knows... might be able to carry this forward.\nThe most concrete way to improve the UI is available, capable hands.\nWe don't have fulltime UI staff, so appreciate volunteers who offer\nadvice or take time to fix things. Thank you.\n. @igorwwwwwwwwwwwwwwwwwwww don't have a prebaked utility to make large traces, but this lua script could probably be modified to do so.. https://github.com/openzipkin/zipkin/issues/1226#issuecomment-346745547. not sure this will change anything but next zipkin release will update jquery lib and we should retry https://github.com/openzipkin/zipkin/pull/1954. cc @openzipkin/cassandra funny we didn't notice this before! (ps I've verified this is a problem and the fix). cc @openzipkin/cassandra funny we didn't notice this before! (ps I've verified this is a problem and the fix). Currently, the query for service names looks at all indexes (it also sorts which is redundant). So, it probably affects more than just today's data.. Currently, the query for service names looks at all indexes (it also sorts which is redundant). So, it probably affects more than just today's data.. @openzipkin/elasticsearch I'm suspicious of using catch-all for service and span names. On one hand we want to make sure data is readable, but on the other hand do you think we need to search all indexes for service and span names? what if we cut-off at a particular date?. @openzipkin/elasticsearch I'm suspicious of using catch-all for service and span names. On one hand we want to make sure data is readable, but on the other hand do you think we need to search all indexes for service and span names? what if we cut-off at a particular date?. elasticsearch caching is enabled by default by our schema. Have you looked\ninto this and perhaps what it would take to better tune the service and\nspan name cache (or at least see if it is or isn't working)?\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/shard-request-cache.html\nIdeally, we can find a solution that works well at ES level since not\neveryone shares libraries to access it.\n. elasticsearch caching is enabled by default by our schema. Have you looked\ninto this and perhaps what it would take to better tune the service and\nspan name cache (or at least see if it is or isn't working)?\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/shard-request-cache.html\nIdeally, we can find a solution that works well at ES level since not\neveryone shares libraries to access it.\n. notes from gitter\n@tramchamploo \n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-search-speed.html\nnested can make queries several times slower and parent-child relations can make queries hundreds of times slower. So if the same questions can be answered without joins by denormalizing documents, significant speedups can be expected.\n...\nI think that\u2019s suggesting flatting the span structure would be fine.\n. @sammypbaird @mansu without more drastic measures such as indexing service names manually or flattening the span's data structure... we could parameterize the \"lookback\" of service/span names, changing it from forever to one or two days?. @sammypbaird incidentally, I was also thinking about a \"special field\" for indexing service names. Note it would have to be more than one (terms vs a term query to cover client+server spans), but I still think ES would optimize it much better than current nested queries. That said, the follow-up span names query might remain a problem if unbounded.. ok, so there's two paths to take.\n\nthere's an obvious quick win by limiting the lookback on service names. I think that should be done as it has zero schema implications.\non optimizing other parts, remember that if we optimize for a separate doc etc for service name caching, we end up with a big \"if statement\" in the codebase, which is tech debt that might last quite a while. For example, the query syntax for old data and new data will be different, and in some cases you may be querying across both.. @sammypbaird here's one thing I think we can do which should be easy and introduces no new variables.\nWe already have a variable QUERY_LOOKBACK, which controls the default amount of data one searches for. We could reuse this in elasticsearch, for service names, which would default to 24hrs back (usually 2 days because ES indexes are midnight based). So, by default, ES service names would only query 2 days (today and yesterday). Someone can extend that by affecting QUERY_LOOKBACK variable.\n@Autowired\n  @Value(\"${zipkin.query.lookback:86400000}\")\n  int defaultLookback = 86400000; // 1 day in millis\nIf you agree, this would be a simple change and not increase tech debt we'd have to revert when addressing the topic more fundamentally.. @sammypbaird here's one thing I think we can do which should be easy and introduces no new variables.\nWe already have a variable QUERY_LOOKBACK, which controls the default amount of data one searches for. We could reuse this in elasticsearch, for service names, which would default to 24hrs back (usually 2 days because ES indexes are midnight based). So, by default, ES service names would only query 2 days (today and yesterday). Someone can extend that by affecting QUERY_LOOKBACK variable.\n@Autowired\n  @Value(\"${zipkin.query.lookback:86400000}\")\n  int defaultLookback = 86400000; // 1 day in millis\nIf you agree, this would be a simple change and not increase tech debt we'd have to revert when addressing the topic more fundamentally.. Currently it is used when someone makes an api call to get traces (but\nleave off the lookback parameter). No worries on reusing for span names.\nOn 9 Mar 2017 17:42, \"sammypbaird\" notifications@github.com wrote:\n\nThat sounds great. The only reason I chose 2 days for the indices was to\nnot have a small sample early in the morning - 24 hours should address\nthis. I'm not sure however how overloading this variable will impact the\ncurrent system. What does it currently control and how is it currently\nused? As long as it doesn't restrict some other part of the system, I'm\nhappy with that idea. I would want it to also affect the spanName lookup\ntoo (which is also unbounded right now).\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1462#issuecomment-285387445,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD616beyRmcdN9X6Ifz1XT3jZFwZjEpks5rkB3fgaJpZM4LYE23\n.\n. Currently it is used when someone makes an api call to get traces (but\nleave off the lookback parameter). No worries on reusing for span names.\n\nOn 9 Mar 2017 17:42, \"sammypbaird\" notifications@github.com wrote:\n\nThat sounds great. The only reason I chose 2 days for the indices was to\nnot have a small sample early in the morning - 24 hours should address\nthis. I'm not sure however how overloading this variable will impact the\ncurrent system. What does it currently control and how is it currently\nused? As long as it doesn't restrict some other part of the system, I'm\nhappy with that idea. I would want it to also affect the spanName lookup\ntoo (which is also unbounded right now).\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1462#issuecomment-285387445,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD616beyRmcdN9X6Ifz1XT3jZFwZjEpks5rkB3fgaJpZM4LYE23\n.\n. My plan would be to add a field to ElasticsearchHttpStorage.Buider of\nnamesLookback\n\nIn the autoconfiguration method, add a Value parameter which binds\nzipkin.query.lookback to the parameter namesLookback and pass that to the\nbuilder. In other words dont add it to a properties class. Instead use it\ndirectly in the Bean method\nOn 10 Mar 2017 12:44, \"tramchamploo\" notifications@github.com wrote:\nI'm trying to implement this, and defaultLookback goes a long way through\nZipkinElasticsearchHttpStorageProperties -> ZipkinElasticsearchHttpStorage\nAutoConfiguration -> ElasticsearchHttpStorage -> ElasticsearchHttpSpanStore.\nAlso in ZipkinElasticsearchHttpStorageProperties there is\n@Autowired\n  @Value(\"${zipkin.query.lookback:86400000}\")\n  private int defaultLookback = 86400000; // 1 day in millis\nany idea a better way to implement this?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1462#issuecomment-285637183,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6190-YIzvYHsE2R7mRkbHm1MKtH1Dks5rkSmBgaJpZM4LYE23\n.\n. My plan would be to add a field to ElasticsearchHttpStorage.Buider of\nnamesLookback\nIn the autoconfiguration method, add a Value parameter which binds\nzipkin.query.lookback to the parameter namesLookback and pass that to the\nbuilder. In other words dont add it to a properties class. Instead use it\ndirectly in the Bean method\nOn 10 Mar 2017 12:44, \"tramchamploo\" notifications@github.com wrote:\nI'm trying to implement this, and defaultLookback goes a long way through\nZipkinElasticsearchHttpStorageProperties -> ZipkinElasticsearchHttpStorage\nAutoConfiguration -> ElasticsearchHttpStorage -> ElasticsearchHttpSpanStore.\nAlso in ZipkinElasticsearchHttpStorageProperties there is\n@Autowired\n  @Value(\"${zipkin.query.lookback:86400000}\")\n  private int defaultLookback = 86400000; // 1 day in millis\nany idea a better way to implement this?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1462#issuecomment-285637183,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6190-YIzvYHsE2R7mRkbHm1MKtH1Dks5rkSmBgaJpZM4LYE23\n.\n. https://github.com/openzipkin/zipkin/pull/1538. https://github.com/openzipkin/zipkin/pull/1538. ps not merging https://github.com/openzipkin/zipkin/pull/1538#discussion_r105528463 until someone weighs in on whether we should use QUERY_LOOKBACK, or introduce ES_NAMES_LOOKBACK instead. I'm ok with either, just noticing the work is roughly the same for either. ps not merging https://github.com/openzipkin/zipkin/pull/1538#discussion_r105528463 until someone weighs in on whether we should use QUERY_LOOKBACK, or introduce ES_NAMES_LOOKBACK instead. I'm ok with either, just noticing the work is roughly the same for either. Can you quantify \"much\" in terms of improvement and how far back your data\ngoes?\nI think one thing lost is that this isnt a solution for those doing 100%\nsampling. Sampling is still required as is watching the amount of data in\ntraces. This change will not \"fix\" this part, rather help bound the service\nname lookup problem in general for those sampling.\nOn 13 Mar 2017 10:01, \"tramchamploo\" notifications@github.com wrote:\n\nJust merge limit-es-names, seems the speed does not much improve. 100GB\ndata every day with 12 hosts cluster, 64GB memory per host.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1462#issuecomment-286038101,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD611Hb1vlaJaTMxg3hWPSnMCIUSYP2ks5rlPfEgaJpZM4LYE23\n.\n. Can you quantify \"much\" in terms of improvement and how far back your data\ngoes?\n\nI think one thing lost is that this isnt a solution for those doing 100%\nsampling. Sampling is still required as is watching the amount of data in\ntraces. This change will not \"fix\" this part, rather help bound the service\nname lookup problem in general for those sampling.\nOn 13 Mar 2017 10:01, \"tramchamploo\" notifications@github.com wrote:\n\nJust merge limit-es-names, seems the speed does not much improve. 100GB\ndata every day with 12 hosts cluster, 64GB memory per host.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1462#issuecomment-286038101,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD611Hb1vlaJaTMxg3hWPSnMCIUSYP2ks5rlPfEgaJpZM4LYE23\n.\n. In your case i am surprised it makes any difference at all! The parameter\nused limits the names query to lookback 1 day (which in your case is the\nsame as the amount of total data you have). I would expect no difference\nsince the data searched is the same.\n\nyou could try to change the lookback to something less than a day.. like an\nhour? Just as an experiment.. again something more significant will likely\nbe needed in your case.\nThanks for reporting back.\nOn 15 Mar 2017 06:23, \"tramchamploo\" notifications@github.com wrote:\nI save data for just one day, a service name query takes about more than 5\nsecs. After limitation I see a about 1s improvement or just nothing.\nTimeout sometimes happens.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1462#issuecomment-286637345,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD615ja1Dk3ALOSVLbl7Urvg7PKxqniks5rl2fTgaJpZM4LYE23\n.\n. In your case i am surprised it makes any difference at all! The parameter\nused limits the names query to lookback 1 day (which in your case is the\nsame as the amount of total data you have). I would expect no difference\nsince the data searched is the same.\nyou could try to change the lookback to something less than a day.. like an\nhour? Just as an experiment.. again something more significant will likely\nbe needed in your case.\nThanks for reporting back.\nOn 15 Mar 2017 06:23, \"tramchamploo\" notifications@github.com wrote:\nI save data for just one day, a service name query takes about more than 5\nsecs. After limitation I see a about 1s improvement or just nothing.\nTimeout sometimes happens.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1462#issuecomment-286637345,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD615ja1Dk3ALOSVLbl7Urvg7PKxqniks5rl2fTgaJpZM4LYE23\n.\n. @sammypbaird are you able to test the outstanding PR? we should do a release for other reasons in the next day or two.. @sammypbaird are you able to test the outstanding PR? we should do a release for other reasons in the next day or two.. I'll merge as-is as we can always add an ES specific property later.\n@StephenWithPH sorry we don't have a docker process for snapshots at the moment cc @openzipkin/devops-tooling in case someone is bored and wants to help make that happen :P. I'll merge as-is as we can always add an ES specific property later.\n@StephenWithPH sorry we don't have a docker process for snapshots at the moment cc @openzipkin/devops-tooling in case someone is bored and wants to help make that happen :P. fyi @openzipkin/elasticsearch probably nothing. fyi @openzipkin/elasticsearch probably nothing. you'd have to use charles proxy or something to capture the post traffic,\nor try adjusting your server logs as I can't dig into this at the moment.\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-slowlog.html\nRegardless of performance, we don't need to ask server to do this.\n. you'd have to use charles proxy or something to capture the post traffic,\nor try adjusting your server logs as I can't dig into this at the moment.\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-slowlog.html\nRegardless of performance, we don't need to ask server to do this.\n. is it possible to read this prefix from config.json (and parameterize all\nother paths based on that value)? It would be nice to not have to play\ngames for the base case where the url space is root.\n. is it possible to read this prefix from config.json (and parameterize all\nother paths based on that value)? It would be nice to not have to play\ngames for the base case where the url space is root.\n. I like this idea\nOn Sat, Dec 31, 2016 at 8:21 PM, Eirik Sletteberg notifications@github.com\nwrote:\n\nWe could try to heuristically detect whether we're mounted under root or\nnot:\n// inside the web UI code:\n// check if pathname starts with /zipkin/, and if it does,// assume that we're mounted under a /zipkin/ context pathconst isMountedAtRoot = window.location.pathname.indexOf('/zipkin/') !== 0;const basePath = isMountedAtRoot ? '/' : '/zipkin/';\nThat is assuming we don't have any request paths in our application that\nstart with /zipkin/, but I don't think we have.\nWe would then support two use cases: 1) mounting zipkin on root, as we do\ntoday, and 2) mounting zipkin under /zipkin/.\nThe case we would not support is 3), to mount zipkin under an arbitrary\ncontext path, but that would be an edge case. I don't think we should take\non the burden of supporting it. (Why would somebody want to mount an app\nunder /zipkin/ that is not zipkin?)\nThe context path mapping itself can either be done on the zipkin server\nside (MOUNT_CONTEXT_PATH=true) or in a proxy setup (apache 2 or nginx)\nrunning in front of the server.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1464#issuecomment-269862729,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD618Azpk-fIKhH8WRpjb9Ybo5ludS7ks5rNkjYgaJpZM4LYNvh\n.\n. I like this idea\n\nOn Sat, Dec 31, 2016 at 8:21 PM, Eirik Sletteberg notifications@github.com\nwrote:\n\nWe could try to heuristically detect whether we're mounted under root or\nnot:\n// inside the web UI code:\n// check if pathname starts with /zipkin/, and if it does,// assume that we're mounted under a /zipkin/ context pathconst isMountedAtRoot = window.location.pathname.indexOf('/zipkin/') !== 0;const basePath = isMountedAtRoot ? '/' : '/zipkin/';\nThat is assuming we don't have any request paths in our application that\nstart with /zipkin/, but I don't think we have.\nWe would then support two use cases: 1) mounting zipkin on root, as we do\ntoday, and 2) mounting zipkin under /zipkin/.\nThe case we would not support is 3), to mount zipkin under an arbitrary\ncontext path, but that would be an edge case. I don't think we should take\non the burden of supporting it. (Why would somebody want to mount an app\nunder /zipkin/ that is not zipkin?)\nThe context path mapping itself can either be done on the zipkin server\nside (MOUNT_CONTEXT_PATH=true) or in a proxy setup (apache 2 or nginx)\nrunning in front of the server.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1464#issuecomment-269862729,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD618Azpk-fIKhH8WRpjb9Ybo5ludS7ks5rNkjYgaJpZM4LYNvh\n.\n. sure thing\n. sure thing\n. looks good. I think this is more straightforward. thanks!. looks good. I think this is more straightforward. thanks!. thanks for the help @kojilin . thanks for the help @kojilin . Gracias, @josetesan y bravo!. Gracias, @josetesan y bravo!. @jcarres-mdsol fyi!. @jcarres-mdsol fyi!. cc @cburroughs in case you have time for this. cc @cburroughs in case you have time for this. https://github.com/openzipkin/zipkin/pull/1475. https://github.com/openzipkin/zipkin/pull/1475. \n\ncc @jakubhava @cburroughs @eirslett . \ncc @jakubhava @cburroughs @eirslett . @jakubhava no problemo. I learned this stuff from @eirslett, and now the virus spreads to you :). @jakubhava no problemo. I learned this stuff from @eirslett, and now the virus spreads to you :). cc @jakubhava @cburroughs @eirslett. cc @jakubhava @cburroughs @eirslett. \n\nUsing this data:\nbash\ncurl -H \"Content-Type: application/json\" -s localhost:9411/api/v1/spans -d '[\n  {\n    \"traceId\": \"4e441824ec2b6a44\",\n    \"id\": \"4e441824ec2b6a44\",\n    \"name\": \"get\",\n    \"timestamp\": 1478624611519000,\n    \"annotations\": [\n    {\n      \"timestamp\": 1478624611519000,\n      \"value\": \"Server Send Error: TimeoutException: socket timed out\",\n      \"endpoint\": {\n        \"serviceName\": \"zipkin-server\",\n        \"ipv4\": \"192.168.99.1\",\n        \"port\": 9411\n      }\n    },\n    {\n      \"timestamp\": 1478624611519001,\n      \"value\": \"{\\\"foo\\\": \\\"bar\\\"}\",\n      \"endpoint\": {\n        \"serviceName\": \"zipkin-server\",\n        \"ipv4\": \"192.168.99.1\",\n        \"port\": 9411\n      }\n    }],\n    \"binaryAnnotations\": [\n    {\n      \"key\": \"bar\",\n      \"value\": \"bar\",\n      \"endpoint\": {\n        \"serviceName\": \"zipkin-server\",\n        \"ipv4\": \"192.168.99.1\",\n        \"port\": 9411\n      }\n    },\n      {\n        \"key\": \"foo\",\n        \"value\": \"{\\\"foo\\\": \\\"bar\\\"}\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ]\n  }\n  ]'. \n\nUsing this data:\nbash\ncurl -H \"Content-Type: application/json\" -s localhost:9411/api/v1/spans -d '[\n  {\n    \"traceId\": \"4e441824ec2b6a44\",\n    \"id\": \"4e441824ec2b6a44\",\n    \"name\": \"get\",\n    \"timestamp\": 1478624611519000,\n    \"annotations\": [\n    {\n      \"timestamp\": 1478624611519000,\n      \"value\": \"Server Send Error: TimeoutException: socket timed out\",\n      \"endpoint\": {\n        \"serviceName\": \"zipkin-server\",\n        \"ipv4\": \"192.168.99.1\",\n        \"port\": 9411\n      }\n    },\n    {\n      \"timestamp\": 1478624611519001,\n      \"value\": \"{\\\"foo\\\": \\\"bar\\\"}\",\n      \"endpoint\": {\n        \"serviceName\": \"zipkin-server\",\n        \"ipv4\": \"192.168.99.1\",\n        \"port\": 9411\n      }\n    }],\n    \"binaryAnnotations\": [\n    {\n      \"key\": \"bar\",\n      \"value\": \"bar\",\n      \"endpoint\": {\n        \"serviceName\": \"zipkin-server\",\n        \"ipv4\": \"192.168.99.1\",\n        \"port\": 9411\n      }\n    },\n      {\n        \"key\": \"foo\",\n        \"value\": \"{\\\"foo\\\": \\\"bar\\\"}\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ]\n  }\n  ]'. @kevinoliver @mosesn better late than never :P. @kevinoliver @mosesn better late than never :P. (ps ignore the escaped json in the spans.. that's unrelated). (ps ignore the escaped json in the spans.. that's unrelated). cc @openzipkin/devops-tooling . cc @openzipkin/devops-tooling . cc @openzipkin/devops-tooling we did the same in brave via https://github.com/openzipkin/brave/pull/326. cc @openzipkin/devops-tooling we did the same in brave via https://github.com/openzipkin/brave/pull/326. I've not heard of anyone making an apache module as yet. right now, linkerd might be the only known reverse proxy that has builtin zipkin support.. I've not heard of anyone making an apache module as yet. right now, linkerd might be the only known reverse proxy that has builtin zipkin support.. cc @openzipkin/instrumentation-owners @fedj @openzipkin/core @cburroughs @rogeralsing. cc @reta. one can model single producer multiple consumer messages with a parent-child relationship similar to single-host spans in RPC. For example, by using X-B3-SpanId as the parent id of the consumer span, many consumers will end up as different spans. So, this pattern for single-host spans also applies to SPMC\nhttps://github.com/openzipkin/b3-propagation#why-is-parentspanid-propagated. added https://github.com/openzipkin/zipkin/pull/1656 to fix dependency linking. Noticed this when implementing the simple json thing. added https://github.com/openzipkin/zipkin/pull/1656 to fix dependency linking. Noticed this when implementing the simple json thing. please take a look at https://github.com/openzipkin/zipkin/pull/2302 and let me know if this is still desirable! I think this is the last step cc @ivantopo. please take a look at https://github.com/openzipkin/zipkin/pull/2302 and let me know if this is still desirable! I think this is the last step cc @ivantopo. thanks for writing this up @fedj. I think if the goal is to keep the entire value space in an aggregatable form, something like hdrhistogram might do the trick http://psy-lob-saw.blogspot.my/2015/02/hdrhistogram-better-latency-capture.html It might not end up with the exact values, but it will be.. well.. high resolution :P\nin this issue you are collecting a histogram of linksPerTrace. That might end up as the json field corresponding to the histogram's representation. You might want to add traceCount as opposed to removing callCount.. not sure..\nps. something more commonly requested has been latency across the link (which hdr histogram could do as well)\nOne trick is that the value needs to be serialized (both in thrift and also in json). I've not done research to suggest how people are reading hdr histograms from the browser... for example, are they converting them to percentiles or rendering them directly.\ncc @eirslett for more ideas. cc @fedj @marcingrzejszczak I'm taking a look now.... actually span 2dcfafadb8f566bb is invalid..\n the server send is 1484121797501000\n the client receive is 1484121797488000 (13ms before the server sent the request!)\nstill, we shouldn't adjust because the endpoints are of the same host. https://github.com/openzipkin/zipkin/pull/1483. after dinner I'll look even more closely to see if there's a way to detect this (without cheating like assuming bugs only happen on loopback spans :P). cc @cburroughs . I think the problem is easier to take action on in the client side, not\nonly because there are non-UI users of the API, but also purely from a\n\"what action to take\" standpoint. I suspect there are more ways to help\nvisually vs trying to encode in a message format. That said, there's\nprobably prior art somewhere that does a good job with either way.\n. thanks, lance. will look into it.\n. out in 1.19.2 (and 1 I think)\n. out in 1.19.2 (and 1 I think)\n. Thanks for digging into this.\nso before, people used /api/v1/trace/my-id?raw to debug data (as that skips adjustment). The concept of \"root-most-span\" has been around a while in various forms, seemed to be a usability thing, for the UI. Can you show a screen shot of how the UI behaves before and after.. with a corrupt span like this?\nRegardless, I like making it obvious that a trace is malformed, and allowing multi-roots through should be a signal of that! (maybe we can use it to yell at a user one day so they don't get confused). looking at this deeper with sample data (ex delete parentId from a span in the trace). looking at this deeper with sample data (ex delete parentId from a span in the trace). I ran this locally and I think it is better than before. We should follow-up with #1484 to see if there's a way to visualize problems better.\nBefore:\n\nAfter:\n\nData:\nbash\ncurl -s localhost:9411/api/v1/spans -H'Content-Type: application/json' -d '[\n  {\n    \"traceId\": \"d91d3ff6ac8d3ee0\",\n    \"id\": \"d91d3ff6ac8d3ee0\",\n    \"name\": \"get\",\n    \"timestamp\": 1484560599062664,\n    \"duration\": 9880,\n    \"annotations\": [\n      {\n        \"timestamp\": 1484560599062664,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"timestamp\": 1484560599072544,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"ca\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"\",\n          \"port\": 50278,\n          \"ipv6\": \"::1\"\n        }\n      },\n      {\n        \"key\": \"http.status_code\",\n        \"value\": \"200\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"key\": \"http.url\",\n        \"value\": \"/api/v1/traces\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"d91d3ff6ac8d3ee0\",\n    \"id\": \"7e88135971d84c28\",\n    \"name\": \"get-traces\",\n    \"timestamp\": 1484560599063323,\n    \"duration\": 8261,\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"lc\",\n        \"value\": \"CassandraStorage\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"key\": \"request\",\n        \"value\": \"QueryRequest{serviceName=zipkin-server, spanName=get-span-names, annotations=[], binaryAnnotations={foo=bar, baz=qux}, minDuration=null, maxDuration=null, endTs=1484560580968, lookback=3600000, limit=10}\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"d91d3ff6ac8d3ee0\",\n    \"id\": \"918ae5aa60724d7b\",\n    \"name\": \"select-trace-ids-by-span-name\",\n    \"parentId\": \"7e88135971d84c28\",\n    \"timestamp\": 1484560599063688,\n    \"duration\": 5128,\n    \"annotations\": [\n      {\n        \"timestamp\": 1484560599063688,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"timestamp\": 1484560599068816,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"cql.query\",\n        \"value\": \"SELECT ts,trace_id FROM service_span_name_index WHERE service_span_name=:service_span_name AND ts>=:start_ts AND ts<=:end_ts ORDER BY ts DESC LIMIT :limit_;\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"d91d3ff6ac8d3ee0\",\n    \"id\": \"b05692b601e64647\",\n    \"name\": \"select-trace-ids-by-annotation\",\n    \"parentId\": \"7e88135971d84c28\",\n    \"timestamp\": 1484560599064213,\n    \"duration\": 7313,\n    \"annotations\": [\n      {\n        \"timestamp\": 1484560599064213,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"timestamp\": 1484560599071526,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"cql.query\",\n        \"value\": \"SELECT ts,trace_id FROM annotations_index WHERE annotation=:annotation AND bucket IN :bucket AND ts>=:start_ts AND ts<=:end_ts ORDER BY ts DESC LIMIT :limit_;\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"d91d3ff6ac8d3ee0\",\n    \"id\": \"618c29891541be7f\",\n    \"name\": \"select-trace-ids-by-annotation\",\n    \"timestamp\": 1484560599064714,\n    \"duration\": 5475,\n    \"annotations\": [\n      {\n        \"timestamp\": 1484560599064714,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"timestamp\": 1484560599070189,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"cql.query\",\n        \"value\": \"SELECT ts,trace_id FROM annotations_index WHERE annotation=:annotation AND bucket IN :bucket AND ts>=:start_ts AND ts<=:end_ts ORDER BY ts DESC LIMIT :limit_;\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ]\n  }\n]'\n. I ran this locally and I think it is better than before. We should follow-up with #1484 to see if there's a way to visualize problems better.\nBefore:\n\nAfter:\n\nData:\nbash\ncurl -s localhost:9411/api/v1/spans -H'Content-Type: application/json' -d '[\n  {\n    \"traceId\": \"d91d3ff6ac8d3ee0\",\n    \"id\": \"d91d3ff6ac8d3ee0\",\n    \"name\": \"get\",\n    \"timestamp\": 1484560599062664,\n    \"duration\": 9880,\n    \"annotations\": [\n      {\n        \"timestamp\": 1484560599062664,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"timestamp\": 1484560599072544,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"ca\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"\",\n          \"port\": 50278,\n          \"ipv6\": \"::1\"\n        }\n      },\n      {\n        \"key\": \"http.status_code\",\n        \"value\": \"200\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"key\": \"http.url\",\n        \"value\": \"/api/v1/traces\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"d91d3ff6ac8d3ee0\",\n    \"id\": \"7e88135971d84c28\",\n    \"name\": \"get-traces\",\n    \"timestamp\": 1484560599063323,\n    \"duration\": 8261,\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"lc\",\n        \"value\": \"CassandraStorage\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"key\": \"request\",\n        \"value\": \"QueryRequest{serviceName=zipkin-server, spanName=get-span-names, annotations=[], binaryAnnotations={foo=bar, baz=qux}, minDuration=null, maxDuration=null, endTs=1484560580968, lookback=3600000, limit=10}\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"d91d3ff6ac8d3ee0\",\n    \"id\": \"918ae5aa60724d7b\",\n    \"name\": \"select-trace-ids-by-span-name\",\n    \"parentId\": \"7e88135971d84c28\",\n    \"timestamp\": 1484560599063688,\n    \"duration\": 5128,\n    \"annotations\": [\n      {\n        \"timestamp\": 1484560599063688,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"timestamp\": 1484560599068816,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"cql.query\",\n        \"value\": \"SELECT ts,trace_id FROM service_span_name_index WHERE service_span_name=:service_span_name AND ts>=:start_ts AND ts<=:end_ts ORDER BY ts DESC LIMIT :limit_;\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"d91d3ff6ac8d3ee0\",\n    \"id\": \"b05692b601e64647\",\n    \"name\": \"select-trace-ids-by-annotation\",\n    \"parentId\": \"7e88135971d84c28\",\n    \"timestamp\": 1484560599064213,\n    \"duration\": 7313,\n    \"annotations\": [\n      {\n        \"timestamp\": 1484560599064213,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"timestamp\": 1484560599071526,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"cql.query\",\n        \"value\": \"SELECT ts,trace_id FROM annotations_index WHERE annotation=:annotation AND bucket IN :bucket AND ts>=:start_ts AND ts<=:end_ts ORDER BY ts DESC LIMIT :limit_;\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"d91d3ff6ac8d3ee0\",\n    \"id\": \"618c29891541be7f\",\n    \"name\": \"select-trace-ids-by-annotation\",\n    \"timestamp\": 1484560599064714,\n    \"duration\": 5475,\n    \"annotations\": [\n      {\n        \"timestamp\": 1484560599064714,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"timestamp\": 1484560599070189,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"cql.query\",\n        \"value\": \"SELECT ts,trace_id FROM annotations_index WHERE annotation=:annotation AND bucket IN :bucket AND ts>=:start_ts AND ts<=:end_ts ORDER BY ts DESC LIMIT :limit_;\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 9411\n        }\n      }\n    ]\n  }\n]'\n. At first, I was a bit hesitant, but after seeing this in action, I think it would be silly not to merge. Thanks, @cburroughs!. At first, I was a bit hesitant, but after seeing this in action, I think it would be silly not to merge. Thanks, @cburroughs!. Thanks for the ping, Ali! We sometimes get people asking about various\naspects of azure. Ex\nhttps://groups.google.com/d/msg/zipkin-dev/bTBKUEjLtKg/yLbvVkFYAQAJ for\nstorage\nThere are two alternate collector projects:\nhttps://github.com/openzipkin/zipkin-aws < for amazon components, notable\nSQS\nhttps://github.com/GoogleCloudPlatform/stackdriver-zipkin <- actually more\na storage bridge to Google Stackdriver\non SDK vs small internal library\nZipkin-aws uses the Amazon SDK to access SQS, though that may not always be\nthe case. There are pros and cons to using an SDK for something like\npulling from a queue. For example, usually the SDKs have quite a lot of\ndependencies, and the act of pulling from a queue is a simple rest call.\nMoreover, the rest calls involved in collections are usually extremely\nstable apis. Additional dependencies can often complicate interactions with\nexisting code, and they typically not easy to instrument, for example\ninternal debugging we call \"self-tracing\". On the other hand, sometimes\nSDKs are directly integrated in Spring Boot, which could cut down work.\nAlso, SDKs often help with platform-specific concerns. For example,\nAmazon's includes the ability to refresh credentials which is not\ntechnically difficult, but work better deferred until it has to be\nchanged.  In the zipkin-aws project, code currently seems better off using\nAmazon's SDK than not, so that's why it does!\nback to the point..\nAnyway, when making a zipkin-azure, I'd make the SDK an internal detail, in\ncase it needs to change later, and follow the same pattern as zipkin-aws as\nthat's almost exactly the same interaction you've mentioned. You can\nprobably ping @llinder for advice, too. In both projects mentioned, there's\na separate docker image at the moment, though we've regularly discussed a\n\"layer\" approach which could make it possible to add something like Azure\nEventHub to an existing container.\nnext steps (imho)..\nThe key next steps would be to look at the existing projects, and introduce\nyourself in gitter, as that's where we chat\nhttps://gitter.im/openzipkin/zipkin\nThen, decide whether you want to make this third-party or not. For example,\nzipkin-aws is released in the openzipkin org, which means a few of us can\nrelease it. Outside the zipkin org, you would be responsible for\ndetermining your own path, for example, Google have their own continuous\nintegration and release setup (which seems to work well). Regardless, you'd\nwant to make sure that there's some user who can vet that the result works.\nFor example, one common problem with new code is that it is only used by\nthe author. If you find a user or another stakeholder, the work will be\nmore sustainable.\nHope this helps!\n-A\n. ps totally missed that there was a link in your description!\nhttps://github.com/aliostad/zipkin-collector-eventhub\ncool that the collector work is in progress. If I were you, I'd try to\nadd offline tests to it (this allows others to help even if they don't\nhave a cloud credential). We often use MockWebServer to do things like\nthis, though google created a fake server of their own (since it was\nmore gRPC than plain http). The key would be getting the SDK to allow\nyou to override the URL to azure so that you can fake responses.\n. I pinged twitter to hopefully interest others in contributing or helping https://twitter.com/zipkinproject/status/820801744833691651. looking forward to it!\n. looking forward to it!\n. hi, @aliostad Thanks for the update.\nRight now, people are working with custom images, though there is a means to change how things work by using \"zip\" layout on the server, which allows use of the more customizable property launcher\nThere's another approach, too, a thin launcher.. I mention things about it here: https://github.com/openzipkin/zipkin/issues/1219#issuecomment-273670246\nIt is high time we had a suggested and testable way to compose server binaries.. let's follow-through on this.. hi, @aliostad Thanks for the update.\nRight now, people are working with custom images, though there is a means to change how things work by using \"zip\" layout on the server, which allows use of the more customizable property launcher\nThere's another approach, too, a thin launcher.. I mention things about it here: https://github.com/openzipkin/zipkin/issues/1219#issuecomment-273670246\nIt is high time we had a suggested and testable way to compose server binaries.. let's follow-through on this.. I don't think this exists in zipkin-aws at the moment.. there is a\nversion of what was originally to be merged here:\nhttps://github.com/openzipkin/zipkin-aws/pull/26/files#diff-67f9e3155157d7189b9915f13d8a3669\nthe spring-boot-maven-plugin makes the fat jar,\n. I don't think this exists in zipkin-aws at the moment.. there is a\nversion of what was originally to be merged here:\nhttps://github.com/openzipkin/zipkin-aws/pull/26/files#diff-67f9e3155157d7189b9915f13d8a3669\nthe spring-boot-maven-plugin makes the fat jar,\n. actually, the thin plugin is available, you just need to use snapshot repo\nex.\nhttp://repo.spring.io/libs-snapshot/org/springframework/boot/experimental/spring-boot-thin-maven-plugin/. actually, the thin plugin is available, you just need to use snapshot repo\nex.\nhttp://repo.spring.io/libs-snapshot/org/springframework/boot/experimental/spring-boot-thin-maven-plugin/. Update: figured out how to get modularity working. It is a little constrained, but it works now.\nHere's how.\nStep 1: Add spring boot plugin to your autoconfig module, adding a \"module\" jar\nThe classifier name \"module\" is arbitrary, but I think it works. Take extreme care to not duplicate jars already in zipkin-server's exec jar. Here's an example configuration for the SQS collector:\n```xml\n  \n\n\n Import dependency management from Spring Boot \norg.springframework.boot\nspring-boot-dependencies\n${spring-boot.version}\npom\nimport\n\n\n\n\n\n\norg.springframework.boot\nspring-boot-maven-plugin\n\n\n\nrepackage\n\n\n\n\nMODULE\nmodule\n https://github.com/spring-projects/spring-boot/issues/3426 transitive exclude doesn't work \nio.zipkin.java,org.springframework.boot,org.springframework,commons-codec,com.fasterxml.jackson.core,com.fasterxml.jackson.dataformat,org.apache.httpcomponents,commons-logging,joda-time,software.amazon.ion\n already packaged in zipkin-server \naws-java-sdk-core,aws-java-sdk-sts,jmespath-java\n\n\n\n\n```\nThis will make a file like.. autoconfigure/collector-sqs/target/zipkin-autoconfigure-collector-sqs-0.0.4-SNAPSHOT-module.jar\nStep 2: Extract this module jar when composing a server layer\nI cannot get the PropertiesLauncher configuration to accept the module-jar directly. However, the following does work.\n\nextract your module jar somewhere (Ex to an extensions folder)\nexplicitly start zipkin using PropertiesLauncher with loader.path set to include that directory\n\nExample:\nbash\n$ java  -Dloader.path=/tmp/extensions -cp zipkin.jar org.springframework.boot.loader.PropertiesLauncher --zipkin.collector.sqs.queue-url=http://foobar\nNote: the module jar is self-contained.. when doing your devopsian things, feel free to just curl it from jcenter, like we do for the server jar.. Update: figured out how to get modularity working. It is a little constrained, but it works now.\nHere's how.\nStep 1: Add spring boot plugin to your autoconfig module, adding a \"module\" jar\nThe classifier name \"module\" is arbitrary, but I think it works. Take extreme care to not duplicate jars already in zipkin-server's exec jar. Here's an example configuration for the SQS collector:\n```xml\n  \n\n\n Import dependency management from Spring Boot \norg.springframework.boot\nspring-boot-dependencies\n${spring-boot.version}\npom\nimport\n\n\n\n\n\n\norg.springframework.boot\nspring-boot-maven-plugin\n\n\n\nrepackage\n\n\n\n\nMODULE\nmodule\n https://github.com/spring-projects/spring-boot/issues/3426 transitive exclude doesn't work \nio.zipkin.java,org.springframework.boot,org.springframework,commons-codec,com.fasterxml.jackson.core,com.fasterxml.jackson.dataformat,org.apache.httpcomponents,commons-logging,joda-time,software.amazon.ion\n already packaged in zipkin-server \naws-java-sdk-core,aws-java-sdk-sts,jmespath-java\n\n\n\n\n```\nThis will make a file like.. autoconfigure/collector-sqs/target/zipkin-autoconfigure-collector-sqs-0.0.4-SNAPSHOT-module.jar\nStep 2: Extract this module jar when composing a server layer\nI cannot get the PropertiesLauncher configuration to accept the module-jar directly. However, the following does work.\n\nextract your module jar somewhere (Ex to an extensions folder)\nexplicitly start zipkin using PropertiesLauncher with loader.path set to include that directory\n\nExample:\nbash\n$ java  -Dloader.path=/tmp/extensions -cp zipkin.jar org.springframework.boot.loader.PropertiesLauncher --zipkin.collector.sqs.queue-url=http://foobar\nNote: the module jar is self-contained.. when doing your devopsian things, feel free to just curl it from jcenter, like we do for the server jar.. >\n\nSo unless I am missing something, this is not launcher, no?\nNo, this isn't https://github.com/dsyer/spring-boot-thin-launcher it is\nusing a modularity feature that's been in spring boot for a while\nSo do you think it is worth to try out that (I was trying to get my head\naround it and made some progress) or this is the canonical way to go?\nSince the thin launcher is still experimental, we'd have a race condition\non it finishing or us :) There's also matters like docs support etc which\nare trickier on experimental features.\n\nI'd say that the best way forward for today is to use the module approach,\nand then have an experimental (or even alternate docker image) for the thin\nlauncher. In other words, module for tactical and short term, and thin\nlauncher as an experiment (possible future replacement)\nmy 2p\n. @aliostad so keep in mind this is using the default zipkin exec jar (which has the start class). You would be making the module that adds to it.\nEx. you should be able to download and use an unmodified zipkin-server jar like below.. (which was zipkin.jar in my example)\nhttps://github.com/openzipkin/zipkin#quick-start\nalso feel free to hop on https://gitter.im/openzipkin/zipkin if you get stuck! Thanks for the help on this. >\n\nSure, I will go to gitter. Just to clarify I WAS using the original\nzipkin.jar but as I said, it was not working with -cp.\nsorry about that. It might be best to push what you have to your repo (or\nsomewhere), and add instructions for how you get the error. Then, myself or\nsomeone else could try. It might be better than guessing what's up\n. >\nOK, this is working now \ud83d\udc4d\nyay\nImpossible without your help. I am closing this now, I have everything I\nneed.\nhah.. sorry about that. you caught us at a good time, though as we were\nvetting ideas on how to do modularity!\n. cc @llinder ps something relating to self-tracing, but not related to this noticed .. another fix coming up!. fyi @cburroughs . This fixes a situation where self-tracing mistook localhost for an embedded ipv4 address \"0.0.0.1\"\n\nhere's a screenshot of the fix:\n\n. This fixes a situation where self-tracing mistook localhost for an embedded ipv4 address \"0.0.0.1\"\nhere's a screenshot of the fix:\n\n. here's the before and after:\n\n\n. here's the before and after:\n\n\n. >\n\n\nCS-SR-SS-CR with same end point (not sure it exists but it could be\n   earlier version of local component)\n\nI'd clarify this as loopback. In some cases, a node makes a remote call to\nitself, which can actually be a latency bug!\nDid I miss any use case?\npretty good list! I'd guess that in some cases there will be no-op related\nto skew on one side or the other.\n. >\n\nCS-SR-SS-CR with same end point (not sure it exists but it could be\n   earlier version of local component)\n\nI'd clarify this as loopback. In some cases, a node makes a remote call to\nitself, which can actually be a latency bug!\nDid I miss any use case?\npretty good list! I'd guess that in some cases there will be no-op related\nto skew on one side or the other.\n. >\n@adriancole https://github.com/adriancole, @fedj\nhttps://github.com/fedj, any progress made on dealing with clock skew\nwhen spans come from different hosts? Even when using ntpd, the clocks are\nvery close, but evidently not close enough:\nIt appears as if you might not be using shared RPC spans.\nhttps://github.com/openzipkin/zipkin/issues/1480 tracks some issues around\nthis.\n\nThe current algorithm can correct obvious skew, where the child occurs\nbefore the parent.  This does not, it is just far right.\nDefinitely this can be corrected is that the child cannot start after the\nparent completed. This would be better, as it would at least shift the\nchild left to the end of getId. We could make an assumption that when clock\nskew is present that the RPC child should be contained (or at least\nattempted to be contained)\nopen to other ideas, too.\n. TODO here is look if there are any missing cases that aren't in our unit tests yet. they are all javascript now.. cc @openzipkin/core note I duplicated this as https://github.com/openzipkin/brave/pull/345 and will propagate it to other repos once merged.. cc @openzipkin/core note I duplicated this as https://github.com/openzipkin/brave/pull/345 and will propagate it to other repos once merged.. cc @dragontree101. cc @dragontree101. Sending one side separately from the other..\nbash\n$ curl -s localhost:9411/api/v1/spans -H'Content-type: application/json' -d '[\n  {\n    \"traceId\": \"77047bf9cf04ed5d\",\n    \"id\": \"77047bf9cf04ed5d\",\n    \"name\": \"get\",\n    \"timestamp\": 1484722951945904,\n    \"duration\": 156470,\n    \"annotations\": [\n      {\n        \"timestamp\": 1484722951945904,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"client\"\n        }\n      },\n      {\n        \"timestamp\": 1484722952102374,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"client\"\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"77047bf9cf04ed5d\",\n    \"id\": \"40ef4e93ab3b1507\",\n    \"name\": \"get\",\n    \"parentId\": \"77047bf9cf04ed5d\",\n    \"annotations\": [\n      {\n        \"timestamp\": 1484722952100852,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"client\"\n        }\n      }\n    ]\n  }\n]'\n$ curl -s localhost:9411/api/v1/spans -H'Content-type: application/json' -d '[\n  {\n    \"traceId\": \"77047bf9cf04ed5d\",\n    \"id\": \"40ef4e93ab3b1507\",\n    \"parentId\": \"77047bf9cf04ed5d\",\n    \"annotations\": [\n      {\n        \"timestamp\": 1484722952121651,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"server\"\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"77047bf9cf04ed5d\",\n    \"id\": \"c8a3701afd819d33\",\n    \"name\": \"process\",\n    \"parentId\": \"40ef4e93ab3b1507\",\n    \"timestamp\": 1484722952121748,\n    \"duration\": 656,\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"lc\",\n        \"value\": \"message-processor\",\n        \"endpoint\": {\n          \"serviceName\": \"server\"\n        }\n      }\n    ]\n  }\n]'\nwe get this due to backfilling of timestamp and duration of the middle one-way span\n\n. Sending one side separately from the other..\nbash\n$ curl -s localhost:9411/api/v1/spans -H'Content-type: application/json' -d '[\n  {\n    \"traceId\": \"77047bf9cf04ed5d\",\n    \"id\": \"77047bf9cf04ed5d\",\n    \"name\": \"get\",\n    \"timestamp\": 1484722951945904,\n    \"duration\": 156470,\n    \"annotations\": [\n      {\n        \"timestamp\": 1484722951945904,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"client\"\n        }\n      },\n      {\n        \"timestamp\": 1484722952102374,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"client\"\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"77047bf9cf04ed5d\",\n    \"id\": \"40ef4e93ab3b1507\",\n    \"name\": \"get\",\n    \"parentId\": \"77047bf9cf04ed5d\",\n    \"annotations\": [\n      {\n        \"timestamp\": 1484722952100852,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"client\"\n        }\n      }\n    ]\n  }\n]'\n$ curl -s localhost:9411/api/v1/spans -H'Content-type: application/json' -d '[\n  {\n    \"traceId\": \"77047bf9cf04ed5d\",\n    \"id\": \"40ef4e93ab3b1507\",\n    \"parentId\": \"77047bf9cf04ed5d\",\n    \"annotations\": [\n      {\n        \"timestamp\": 1484722952121651,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"server\"\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"77047bf9cf04ed5d\",\n    \"id\": \"c8a3701afd819d33\",\n    \"name\": \"process\",\n    \"parentId\": \"40ef4e93ab3b1507\",\n    \"timestamp\": 1484722952121748,\n    \"duration\": 656,\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"lc\",\n        \"value\": \"message-processor\",\n        \"endpoint\": {\n          \"serviceName\": \"server\"\n        }\n      }\n    ]\n  }\n]'\nwe get this due to backfilling of timestamp and duration of the middle one-way span\n\n. I can't see your example, but this change will likely fix the UI. That's\nbecause the old duration logic did not expect a span with only \"cs\" and \"sr\"\n. I can't see your example, but this change will likely fix the UI. That's\nbecause the old duration logic did not expect a span with only \"cs\" and \"sr\"\n. simplified code and updated description. key change here is making sure there's a timestamp added (which stops the UI from crashing). simplified code and updated description. key change here is making sure there's a timestamp added (which stops the UI from crashing). PS this is a screenshot of the in-flight span. duration absent appearing as NaN is a separate issue, which I'll raise separately.\n\n. PS this is a screenshot of the in-flight span. duration absent appearing as NaN is a separate issue, which I'll raise separately.\n\n. cc @dragontree101 @cburroughs . cc @dragontree101 @cburroughs . cc @openzipkin/instrumentation-owners @openzipkin/core . cc @openzipkin/instrumentation-owners @openzipkin/core . >\n\n@adriancole https://github.com/adriancole what does Absent mean in\n\"kind\"? What kind of use case it stands for? Will it mean that we put\n'Absent' as the value or put nothing at all?\nsorry I was trying to hint you can either leave out the field \"key\" or set\nit to null.\n\nKind is used to indicate the span is a client or server RPC span. This\nhelps in translation to the old format, and means the user doesn't need do\nadd annotations like \"cs\" and \"cr\". It also makes obvious what\nremoteAddress is (if client, the remoteAddress is server)\nlocal spans (ex ones that never leave the host), would leave kind out.\n. >\n\n@adriancole https://github.com/adriancole what does Absent mean in\n\"kind\"? What kind of use case it stands for? Will it mean that we put\n'Absent' as the value or put nothing at all?\nsorry I was trying to hint you can either leave out the field \"key\" or set\nit to null.\n\nKind is used to indicate the span is a client or server RPC span. This\nhelps in translation to the old format, and means the user doesn't need do\nadd annotations like \"cs\" and \"cr\". It also makes obvious what\nremoteAddress is (if client, the remoteAddress is server)\nlocal spans (ex ones that never leave the host), would leave kind out.\n. > We were talking about async traces with @jcarres-mdsol . Is it safe to assume that if finishTimestamp is empty with kind as Client, an async process is being started with that span?\nThe semantics of one-way spans are currently the same as RPC, except there's no response. This was the least efforts means we could come up with. There's an example of this in a recent pull request #1497\nTo do this in using the new format in a backwards compatible way, the key is being able to flush a span (report it to zipkin without attempting to calculate duration locally).\nex. span.kind(client).start() --> translates into a \"cs\" annotation if you flush at this point\n-- propagated to the server then.. \nex. span.kind(server).start() --> translates into an \"sr\" annotation if you flush at this point\nLet me know if this doesn't answer your question. > We were talking about async traces with @jcarres-mdsol . Is it safe to assume that if finishTimestamp is empty with kind as Client, an async process is being started with that span?\nThe semantics of one-way spans are currently the same as RPC, except there's no response. This was the least efforts means we could come up with. There's an example of this in a recent pull request #1497\nTo do this in using the new format in a backwards compatible way, the key is being able to flush a span (report it to zipkin without attempting to calculate duration locally).\nex. span.kind(client).start() --> translates into a \"cs\" annotation if you flush at this point\n-- propagated to the server then.. \nex. span.kind(server).start() --> translates into an \"sr\" annotation if you flush at this point\nLet me know if this doesn't answer your question. >\n\nRemember, 64 bit integers are tricky in JavaScript, which doesn't have\nthem.\nInterestingly, it's possible to represent the number of milliseconds since\n1970 in only ~52 bits, while JavaScript can represent whole numbers up to\n53 bits. (Just a coincidence) So it will silently work until we eventually\nencounter a \"year 2000 problem\".\nIt might be safer to store timestamps as strings (possibly\ncompact/encoded), even if that could impact performance?\nI hear you, and thanks for reminding about this. String dates are indeed\nidiomatic. Worth re-stating the rationale, since this issue is fairly\nspecial case. For example, it is making a shortcut form of the existing\nmodel (and would need to port without lossiness to the old one)\n\nHere's some things to think about.\nEverything currently uses the epoch micros, so it directly translates\nacross. For example, when querying back (or looking at stored data) you\nwould see the exact numeric form reported).\nDate formats usually don't have microsecond precision, either.\nUsing a date format opens up to people in the N languages with\nuncoordinated tracers putting in an incorrect format (which we would\nimmediately unwind it back into an epoch time). We'd get all the fun with\npeople accidentally putting wrong timezone etc which I'm sure many of us\nare familiar with.\nOn 53bits. that's a known constraint wrt timestamps (our ids don't use\nnumeric form for this reason). Ex our current json parser guards on 53bits\nwhich if I'm not mistaken puts the Y2K problem at the year 2255. While\nthat's a punt for sure, since data is usually retained at a few days,\nsometime between now and 2255 I'm sure we'd have a new overall format.\nStatus quo does include the typical problem of people making ad-hoc or new\ntracers accidentally put epochMillis where they meant epochMicros. This is\nvery easy to detect and fix. Heck we could probably even log it since when\nthis mistake occurs the timestamps would look like they are from 1970. In\nother words, we can help make developing easier by making a development\nmode of zipkin which would crash POST on upload lint problems, such as 1970\ndates.\nThoughts?\n. >\n\nRemember, 64 bit integers are tricky in JavaScript, which doesn't have\nthem.\nInterestingly, it's possible to represent the number of milliseconds since\n1970 in only ~52 bits, while JavaScript can represent whole numbers up to\n53 bits. (Just a coincidence) So it will silently work until we eventually\nencounter a \"year 2000 problem\".\nIt might be safer to store timestamps as strings (possibly\ncompact/encoded), even if that could impact performance?\nI hear you, and thanks for reminding about this. String dates are indeed\nidiomatic. Worth re-stating the rationale, since this issue is fairly\nspecial case. For example, it is making a shortcut form of the existing\nmodel (and would need to port without lossiness to the old one)\n\nHere's some things to think about.\nEverything currently uses the epoch micros, so it directly translates\nacross. For example, when querying back (or looking at stored data) you\nwould see the exact numeric form reported).\nDate formats usually don't have microsecond precision, either.\nUsing a date format opens up to people in the N languages with\nuncoordinated tracers putting in an incorrect format (which we would\nimmediately unwind it back into an epoch time). We'd get all the fun with\npeople accidentally putting wrong timezone etc which I'm sure many of us\nare familiar with.\nOn 53bits. that's a known constraint wrt timestamps (our ids don't use\nnumeric form for this reason). Ex our current json parser guards on 53bits\nwhich if I'm not mistaken puts the Y2K problem at the year 2255. While\nthat's a punt for sure, since data is usually retained at a few days,\nsometime between now and 2255 I'm sure we'd have a new overall format.\nStatus quo does include the typical problem of people making ad-hoc or new\ntracers accidentally put epochMillis where they meant epochMicros. This is\nvery easy to detect and fix. Heck we could probably even log it since when\nthis mistake occurs the timestamps would look like they are from 1970. In\nother words, we can help make developing easier by making a development\nmode of zipkin which would crash POST on upload lint problems, such as 1970\ndates.\nThoughts?\n. ps will change proposal to 53bit as that's a very good point regardless!\n. ps will change proposal to 53bit as that's a very good point regardless!\n. @eirslett and anyone else just circling back.. would any of the below help on the date thing?\nIt is certainly the case that \"timestamp\" implies seconds since epoch. Would replacing \"timestamp\" with \"micros\" or adding a suffix clear up enough ambiguity to be worth its characters?\nrename startTimestamp to startMicros or startTimestampMicros\nrename finishTimestamp to finishMicros or finishTimestampMicros. ps I had an offline request somewhere about the size impact of this. Spans that include more than one annotation or tag will be smaller than before due to de-duplication of the local endpoint.\nIt won't be as small as using single-character field names, but it isn't optimizing for that anyway.\nThe compression of json vs the same data compressed in thrift is an exercise we could do, but I don't think size is the major feature here, rather simplicity.. thinking about picking this up. >\n\nOne thing to think about... maybe do this format in proto3 as it supports\na canonical encoding in JSON so we have a simple roll your own JSON model\nbut at the same time can handle higher performance binary encoding/decoding.\ninteresting idea. regardless we'd want a json schema, too (ex as we sortof\ndo in the openapi spec)\n. So first wave will be implementing the collection side, and a storage impl for elasticsearch. This can flex as we review the implementation.\n\nNote: We aim to reuse hex encoding of IDs as they exist today (in lower-hex). This makes sure existing log integration patterns don't break (ex logging usually adds trace ID to context and that is lower-hex which is same as json and headers). This is particularly useful in storage backends that contain both logging and tracing data (ex Elasticsearch).\nNote part deux: usually, collectors receive single-host spans, but we'll need to address shared spans sooner or later. Kafka pipelines exist (theoretically) that send merged traces directly to storage (ex via zipkin-sparkstreaming). Also, people sometimes save off json (which has merged  spans) and POST it later. While not a data format issue, it is an integration issue that will bite us similarly to how it bit stackdriver when a storage layer like ES stores single-host spans. https://github.com/GoogleCloudPlatform/stackdriver-zipkin/blob/master/translation/src/main/java/com/google/cloud/trace/zipkin/translation/SpanTranslator.java\nI'll do my best to keep all operational notes like above close to the code, so that nothing is surprising.. >\n\nWhat do you think of making this JSON two-layered - envelope that has a\nbasic correlation fields and versioned&typed payload. Something along the\nlines:\nFirst thoughts were that this design was to reduce nesting, not add to it\n:) That said, at least this nesting will not introduce a collection to\nsearch through. Will comment more below, and regardless appreciate your\ninterest.\nThis way you can solve some scenarios like:\n\nYou can send annotation separately from span without the need to\n   specify the end time:\n\nin-flight work has both timestamps optional, as it was an oversight to\nmake them mandatory (for tags, not annotations, but anyway yeah). Reason is\nthat there are use cases where data happens after the fact (even if it is\nexceptional and usually related to timeouts). Adding another layer of\nnesting doesn't make this easier or harder, imho.\n\nSome annotations can be forced out of span in future so they could\n   be indexed and queried easier\n\nyeah that was an oversight in the sketch, not the impl. It doesn't require\na data envelope to handle.\n\nVersioning of span types may be easier in future. Just encode\n   version in kind field\n\nThe kind field is a shortcut currently used to eliminate bookend\nannotations of \"cs\" \"sr\" etc. That's why it is an enum, not a open type.\nEncoding other data into that field would not be supported. For versioning,\nI think it is useful to define a schema and possibly a proto3 mapping.\n\nOther types like traces/exceptions/etc. may be supported as a\n   payload in future.\n\nWe wouldn't remove fields, though possibly add ones. I would be\nunsurprised if we had an exception type at some point (in the span).\nThere's no likelihood of defining a separate \"trace\" type in zipkin (as\nopposed to a list of spans). Regardless, there's no issue tracking future\nchanges in json schema and/or proto3.\n\nThose types will be correlated with traces without the need to\n   change UI and tools. If type is unknown - tool can simply show JSON\n\nThere are clear benefits for having this a straight-forward data type,\nincluding natural objects when code is generated from it. Unknown fields\nwithin this type can be skipped, as they are now, but changing this from a\ndata type to be a generic carrier is beyond the intent.\n\nData type currently fits the model of existing tracers who post some or all\nof a span as a unit. There's no expectation that they will start sending\narbitrary data, and it is a bit early to guess how they would. Meanwhile,\nescalating this change from a data type to a trace-stained-carrier will put\nit at risk of not completing (again).\nTL;DR; let's keep this as a data type and fork any discussion off about a\ncoordinated trace-identifier scoped carrier as a separate issue or thread.\nLet's make sure there any evaluation of such is diversely bought-into as we\ndon't have enough hands to maintain things that aren't.\n. >\n\n@adriancole https://github.com/adriancole thanks for clarification. My\nunderstanding of schema was wrong I thought only these fields marked\n\"Absent\" can be omitted:\nheh you are right about the absent part, just that I didn't update the\nissue after working on it yesterday and noticing the missing absent on the\ntimestamps. Will do now!\n. updated about absent. good point. we can accept absent span name and use the same semantics as what we currently do for empty (which is defer the naming choice). updated. ok made a tracking issue for this https://github.com/openzipkin/zipkin/issues/1644. ran into a couple things in java binding.\n 64-bit trace IDs are in heavy use, so assuming only 128-bit encoding might be a mistake\n  * For example, many using B3 still use 64-bit (as the spec defines two encodings). Assuming 128-bit padding knowing this can interfere with tokenization defined there.\n  * Once other specs like Trace-Context exist, we can reconsider this\n In java, code in most places use uint64s (longs) for convenience, we should keep this convention.\n  * if we switched to string for IDs in the java/structs we need to explicitly check length/format of the string.\n  * Also we'd have to modify code that currently exists for random generation, span processing, or trace contexts.\n\nSo, while the representation of IDs will be hex like before, we shouldn't arbitrarily pad left, at least not now. We should also keep the ID layout in java/structs as uint64s as this facilitates interop with existing code.. I found another glitch. We currently still support sharing spans between the client and the server. Unless we store the fact that the span was shared, we break duration logic (because client duration is authoritative zipkin.io/pages/instrumenting.html). I'm tentatively including the shared status so that we don't lose this signal.. So, coming up for air on #1651\nSimpleSpan to existing Span is relatively straightforward (due to prior work). Splitting existing Span into SimpleSpan is very complex due to many things including:\n looseness of Endpoint type (which permits null eventhough it isn't really permitted)\n Endpoint instances sometimes mismatch in the same process (one has port one doesn't)\n* Sometimes by mistake people end up with more than 2 hosts in one span.\n\"simplespan native\" storage (ex elasticsearch w/o nested queries) depends on us being able to split old spans into simplified ones, so the impact of this complexity is delay on this milestone. We can't reasonably expect to have sites with only \"simplespan\" reporter/collectors, so this is a reality vs a nice to have.\nTL;DR; more effort to get through this, hopefully another day or two. . another glitch :) so existing span store tests allow spans to be written with no annotation or endpoint (usually the result of a late flush or a bug, but still possible). Moreover, the thrift defined endpoint fields optional (eventhough it botches search). Long story short, we need to make \"localEndpoint\" optional, if nothing else but to help track down instrumentation bugs.. another glitch :) so existing span store tests allow spans to be written with no annotation or endpoint (usually the result of a late flush or a bug, but still possible). Moreover, the thrift defined endpoint fields optional (eventhough it botches search). Long story short, we need to make \"localEndpoint\" optional, if nothing else but to help track down instrumentation bugs.. @openzipkin/core calling attention to something as it is important in solidifying the new simpler model. I'd like to consider reverting to span.timestamp/duration just like the existing one does.\nAs far as I can research, the idea of changing span.timestamp/duration as we have now to span.startTimestamp/finishTimestamp came from me. When working on brave v4 one of things I ran into was that when recording, it is sometimes better to have two timestamps instead of one and a duration.\n from an api pov, all numeric values are timestamps (so accidentally putting duration where timestamp should go is impossible)\n from a data pov, you could receive partial information, like start/end in different patches (as done in async span)\nWhat I didn't think about was that we have a duration query. If we choose to store two timestamps instead of a single timestamp and a duration (as we do now), then the duration query is more effortful. For example, you need to sneak a field or make a custom index from date math.\nSo, in hindsight, even if folks support a two timestamp (start/finish) approach for the api, it might be better to report a duration field like we do now. This makes duration query dead simple in document stores like Elasticsearch, and from a code POV subtracting timestamps before writing json is easy. Moreover, the two-host async span sharing a single span ID thing will eventually go away anyway.. @openzipkin/core calling attention to something as it is important in solidifying the new simpler model. I'd like to consider reverting to span.timestamp/duration just like the existing one does.\nAs far as I can research, the idea of changing span.timestamp/duration as we have now to span.startTimestamp/finishTimestamp came from me. When working on brave v4 one of things I ran into was that when recording, it is sometimes better to have two timestamps instead of one and a duration.\n from an api pov, all numeric values are timestamps (so accidentally putting duration where timestamp should go is impossible)\n from a data pov, you could receive partial information, like start/end in different patches (as done in async span)\nWhat I didn't think about was that we have a duration query. If we choose to store two timestamps instead of a single timestamp and a duration (as we do now), then the duration query is more effortful. For example, you need to sneak a field or make a custom index from date math.\nSo, in hindsight, even if folks support a two timestamp (start/finish) approach for the api, it might be better to report a duration field like we do now. This makes duration query dead simple in document stores like Elasticsearch, and from a code POV subtracting timestamps before writing json is easy. Moreover, the two-host async span sharing a single span ID thing will eventually go away anyway.. @anuraaga status quo async span is where \"cs\" and \"sr\" are annotations in different docs with the same span ID (one in the sender and the other in the receiver).\nModeling that to \"simplespan with duration\" it would be one doc with a started, but unfinished kind=client span, and another doc with a started, but unfinished kind=server span both sharing the same span ID. unfinished means it has no duration set.. ps on one-way I don't see any (more) impact than we have today. Right now, you can't tell the difference between something that is one-way vs an RPC that hasn't yet finished (except that good clients shouldn't return things unfinished!). Essentially, it is a nuance we could clarify later if needed, as extra data, and separate from this change.. next one is for @openzipkin/elasticsearch \nTag keys usually have dots in them. In the current model it might look like this.\n\"tags\" {\n    \"http.method\": \"GET\"\n  }\nWe claim to support Elasticsearch 2.x and 5.x. Elasticsearch 2.0-2.3 do not support dots in a name, though 2.4, 5.x do.\n  * 2.4 was released Aug 2016 and works if run with ES_JAVA_OPTS=-Dmapper.allow_dots_in_name=true\nThis simple json format looks otherwise ideal for elasticsearch. For example, there are no nested queries needed anymore and possibly not even a special type for servicespan. Placing 2.0-2.3 out of scope for use of simplespan-native storage seems more reasonable to me than rewriting the json structure to work with that version range. I think we'll need to read the old format for a while anyway.\nWhat do you think? Should we keep the above format knowing it doesn't work with ES 2.0-2.3? When thinking of this, note that Amazon's Elasticsearch 2.x is not at 2.4, yet (though it has a couple 5.x versions). sent a note to users, too as many won't otherwise see this note https://twitter.com/zipkinproject/status/888696518735052801. had a quick chat esp w/ @basvanbeek on gitter about tags especially string->string. things still seem sensible, especially as we are optimizing for simple and what works today.\nEx we currently have no aggregation that requires other data types, and even if we did, this could be mitigated by parsing consumer side. Further, anyOf values don't have wide support and when they do, they often make wrapper types which can be troublesome to serialize. Keeping the course of map keeps focus on the job and away from things like this.. also settling on timestamp/duration as we have today in normal span. It makes it easy to do duration, and doesn't prevent arithmetic to get finish timestamp. Turns out that async span is not easier or harder with this decision.. OK the current work in progress proves the new format can not only work for instrumentation, but also as an input for dependency linking (before, we had an internal DependencyLinkSpan type).\nSo, right now, we have zipkin.simplespan.SimpleSpan\nI'm inclined to make a new package called zipkin2 and rename SimpleSpan to zipkin2.Span etc. If we did that, we'd have the artifact \"zipkin\" depend on \"zipkin2\", and host converters in the former.  The rationale is that it would be easier to explain that what \"simplespan\" means, and also encourage its use moving forward.\nFYI: is exactly the same way we moved Brave to a new api (in pieces, doing small parts at a time instead of big bang). For example, we added apis to Brave 4 over the course of many minor releases, and we still release the old artifacts today.\nBasically, the choices are zipkin.simplespan.SimpleSpan or  zipkin2.Span for the single-host variant. Does anyone feel strongly about this? thumbsup means you like zipkin2.Span\ncc @openzipkin/core @shakuzen @devinsba @llinder @reta @ImFlog @semyonslepov @sirtyro. OK the current work in progress proves the new format can not only work for instrumentation, but also as an input for dependency linking (before, we had an internal DependencyLinkSpan type).\nSo, right now, we have zipkin.simplespan.SimpleSpan\nI'm inclined to make a new package called zipkin2 and rename SimpleSpan to zipkin2.Span etc. If we did that, we'd have the artifact \"zipkin\" depend on \"zipkin2\", and host converters in the former.  The rationale is that it would be easier to explain that what \"simplespan\" means, and also encourage its use moving forward.\nFYI: is exactly the same way we moved Brave to a new api (in pieces, doing small parts at a time instead of big bang). For example, we added apis to Brave 4 over the course of many minor releases, and we still release the old artifacts today.\nBasically, the choices are zipkin.simplespan.SimpleSpan or  zipkin2.Span for the single-host variant. Does anyone feel strongly about this? thumbsup means you like zipkin2.Span\ncc @openzipkin/core @shakuzen @devinsba @llinder @reta @ImFlog @semyonslepov @sirtyro. Messaging implies a boatload more work to elaborate than guessing we solve\nit with an enum choice.\nI would suggest decoupling such an idea to a minor bump because it isnt\nrequired to rename a type to add another field or enum choice.\nFor example, this type now has direction where formerly we didnt. Ex\nlocalEndpoint and remoteEndpoint even if type is null can tell you the\ndirection of communication.\nWhat I mean to say is I don't think it is required to change the format\nincompatibly to handle messaging. On the other hand to formalize messaging\nstill requires practice and whats out there now doesnt.\nAre you flexible on this point? If not then we will need another set of\nconverters (just for packaging) when we do decide it is ok to increment our\nmajor version number. This may lead to it never happening as has happened\nprior issues.\nOn 27 Jul 2017 12:35 am, \"Brian Devins\" notifications@github.com wrote:\n\nBased on the amount of discussion lately on messaging, my gut feeling is\nthat if we go the zipkin2.Span route that we make messaging concerns a\nfirst class citizen of the data model if needed. Specifically around the\nsingle producer multiple consumer case. Maybe there should be a Span.Kind\nthat makes this relationship explicit and a defined practice around it.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1499#issuecomment-318110087,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wSCoudCPnySgLv7Jm3vsrQpxU8xks5sR2rrgaJpZM4Ln26F\n.\n. Messaging implies a boatload more work to elaborate than guessing we solve\nit with an enum choice.\n\nI would suggest decoupling such an idea to a minor bump because it isnt\nrequired to rename a type to add another field or enum choice.\nFor example, this type now has direction where formerly we didnt. Ex\nlocalEndpoint and remoteEndpoint even if type is null can tell you the\ndirection of communication.\nWhat I mean to say is I don't think it is required to change the format\nincompatibly to handle messaging. On the other hand to formalize messaging\nstill requires practice and whats out there now doesnt.\nAre you flexible on this point? If not then we will need another set of\nconverters (just for packaging) when we do decide it is ok to increment our\nmajor version number. This may lead to it never happening as has happened\nprior issues.\nOn 27 Jul 2017 12:35 am, \"Brian Devins\" notifications@github.com wrote:\n\nBased on the amount of discussion lately on messaging, my gut feeling is\nthat if we go the zipkin2.Span route that we make messaging concerns a\nfirst class citizen of the data model if needed. Specifically around the\nsingle producer multiple consumer case. Maybe there should be a Span.Kind\nthat makes this relationship explicit and a defined practice around it.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1499#issuecomment-318110087,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wSCoudCPnySgLv7Jm3vsrQpxU8xks5sR2rrgaJpZM4Ln26F\n.\n. @devinsba ps if a gun was to my head I would choose producer and consumer as the messaging kinds :) that's because it is single sided and knowing which side it is helps with dependency linking. We can alias this to client/server on ambiguity (ex like discussed before one side thinks messaging other rpc)\n\nEven if messaging would be more work than this to complete, adding producer/consumer seem harmless and can clarify intent a bit better cc @Imflog @jcarres-mdsol @fedj\nSo counter proposal is :) add messaging enums, if nothing more to better express intent. Test them on the existing WIP branch for known confusion cases (ex pubsub where one side is rpc other is messaging). If that works, \"rough in\" messaging with these enums but save most work (UI etc) for later PR.. @devinsba ps if a gun was to my head I would choose producer and consumer as the messaging kinds :) that's because it is single sided and knowing which side it is helps with dependency linking. We can alias this to client/server on ambiguity (ex like discussed before one side thinks messaging other rpc)\nEven if messaging would be more work than this to complete, adding producer/consumer seem harmless and can clarify intent a bit better cc @Imflog @jcarres-mdsol @fedj\nSo counter proposal is :) add messaging enums, if nothing more to better express intent. Test them on the existing WIP branch for known confusion cases (ex pubsub where one side is rpc other is messaging). If that works, \"rough in\" messaging with these enums but save most work (UI etc) for later PR.. PS technically the new \"shared\" flag for the receiver (server or consumer) helps with messaging as you can tell instrumentation error (more than one span with shared set for same span ID). This is important for SPMC but not well discussed in this thread.. PS technically the new \"shared\" flag for the receiver (server or consumer) helps with messaging as you can tell instrumentation error (more than one span with shared set for same span ID). This is important for SPMC but not well discussed in this thread.. so here's next thing to vote while I experiment. thumbsup means you are cool with adding PRODUCER/CONSUMER to the kind enum in zipkin 2's (single host) span type. if frownie face, reply back why por favor\nI offer labor to document what this means etc, as it is less than writing converters and then documenting them later :)\ncc @markpollack @marcingrzejszczak who also have asked for this. so here's next thing to vote while I experiment. thumbsup means you are cool with adding PRODUCER/CONSUMER to the kind enum in zipkin 2's (single host) span type. if frownie face, reply back why por favor\nI offer labor to document what this means etc, as it is less than writing converters and then documenting them later :)\ncc @markpollack @marcingrzejszczak who also have asked for this. here's the first piece, which adds an internal copy of span2 (currently without messaging types which will go in later) #1669 . here's the first piece, which adds an internal copy of span2 (currently without messaging types which will go in later) #1669 . #1670: conversion utility between the original and new model from Span2 (splitting as necessary)\n. #1670: conversion utility between the original and new model from Span2 (splitting as necessary)\n. #1671: json codec for Span2. #1671: json codec for Span2. #1673 Drops internal DependencyLinkSpan for Span2. #1673 Drops internal DependencyLinkSpan for Span2. #1651 Elasticsearch uses Span2 internally. #1651 Elasticsearch uses Span2 internally. PS on messaging span, it is independent of this issue as we definitely can't assume all parts of the infra use span2 (we need conversion for the foreseeable future). Here's an approach I think will work https://github.com/openzipkin/zipkin/issues/1654#issuecomment-318546734. PS on messaging span, it is independent of this issue as we definitely can't assume all parts of the infra use span2 (we need conversion for the foreseeable future). Here's an approach I think will work https://github.com/openzipkin/zipkin/issues/1654#issuecomment-318546734. Just added #1684 which adds transport integration. This allows instrumentation to start sending in the new format eventhough the server will convert it to the old format regardless. Later change can make \"zipkin2\" components, but this isn't urgent.. received some feedback from @sokac who is doing early testing on this format with Elasticsearch. There's a problem because some trace instrumentation are creating keys with more than one dot:\nEx pyramid_zipkin adds http.uri.qs cc @kaisen \nAs far as I know, this won't work in Elasticsearch when there's another key with a substring. Ex in pyramid_zipkin, http.uri is also tagged.\nSo, we have a decision to make..\n Workaround the issue specific to Elasticsearch\n  * map key names converting later dots to underscores or similar\n  * drop keys with multiple dots\n change the json format to not use a dict (instead a list of KV pairs)\n* something else...\ncc @openzipkin/elasticsearch . >\n\nIMO, I think it's fine to have restrictions on names for the new format as\nlong as we are explicit and clear about violations. It seems reasonable to\ndisallow dots entirely in the keys for a json format.\nIn the past, we've discussed using slashes instead of dots, due to some\nconventions used in stackdriver. Incidentally, the successor to that still\nuses 1 dot!\nhttps://github.com/census-instrumentation/opencensus-java/blob/master/core/src/main/java/io/opencensus/stats/RpcMeasureConstants.java\nOut of curiosity, would this also fail if you had a keys like \"foo\" and\n\"foo.bar\"?\nI think it would fail for any nesting, but running tests now\n. For elasticsearch the problem isn't so much in dots, just an\ninconsistent number of them from a nesting pov. Since this applies to\nthe entire index it is impossible to tell if there will be a nesting\nproblem later. I think the OpenTracing convention of using\n\"error.kind\" and \"error.object\" will interfere with our tag which is\nsimply \"error\" https://github.com/opentracing/specification/blob/a8103e8e881c84425e4febb9df169f858d59a193/semantic_conventions.yaml\n\nIn other words, forbidding dots in tag names might be a fools errand,\nas we can't control instrumentation. Probably we just need to decide\nwhether to map dots or change the json structure (by default or\nspecifically for ES)\n. meh.. the opentracing thing isn't exactly the same because error.kind\nis a \"log\" in their world, which is annotation in ours. but anyway..\n. FWIW, I think if we choose to set a policy, it would affect tags which are\nindexed, not necessarily data retained. I'm hoping to not re-introduce\nnested queries, which was one of the motivations of this work. We don't\nneed to forbid dots to achieve that, however we would need rules to handle\nthis.\n. OTOH, the nested part which was a problem before was about service and\nspan names. This is already sorted by top-leveling the endpoints. If\nwe switch to \"tags\" = [{\"key\": \"http.path\", \"value\": \"/api\"}], we\ndon't lose this.. it is just a bit expensive and awkward to do\nkey=value search, but not more expensive or awkward than before.\nStill, I'd like to see if there's a way to avoid any nested querying\nand feel there's likely a way out here somehow..\n. Another datapoint. We are far from the only site to have concerns\naround dots. There is standard tooling to dedot fields, for example\nvia logstash or on ingest.\nhttps://github.com/elastic/elasticsearch/issues/15951\nOne way out would be to change our standard conventions (which are not\ninconsistent wrt dot count), to use slashes instead (similar to\ncensus). Then moving forward, use the dedot approach when passed\ndotted tag names.\nThis would make a convention which is not fragile and not create a new\ntype of maintenance burden, especially considering we rarely get help\nwith maintenance.\n@felixbarny I noticed you had similar problems with elasticsearch. Are\nwe missing options?\n. For example, I wonder if we can change via dynamic mapping to tokenize\nwith underscores while leaving the field (with dots) alone. This would\nmake search a lot easier (ex at query time replace dots with\nunderscores) cc @xeraa @anuraaga\n. one way out I'm thinking about is to do manual indexing (in elasticsearch\nonly because this is where the problem is limited to).\nFor example,\n * make a list field similar to cassandra which tokenizes the possible\nsearch queries, like \"_query\": [\"http.uri=/path\", \"http.uri\"...\n    * note we can also in this way choose to not index long fields,\nsimilarly as how we do in cassandra.\n * exclude this list field from _source\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-source-field.html#_disabling_the_literal__source_literal_field,\nso it is only used for indexing.\n * when doing an annotation query, simply pass the query directly to this\nindexing field.\n * disable indexing of tags and annotations as the above substitutes\nUsers will never know the difference I think.\n. The last approach works, and also helps because we no longer need a special property or version range for ES 2.x https://github.com/openzipkin/zipkin/pull/1685. 1.29.3 is on the way out, addressing the ES indexing problem. I think we should leave the format alone.. started on trace identifiers as string here https://github.com/openzipkin/zipkin/pull/1721. java api change to validated strings for IP and IDs resulted in 2x efficiency gains in benchmarks. https://github.com/openzipkin/zipkin/pull/1721 It is incidentally also easier to write your own codec for. However, when we write to storage, we should still use the formal library as it normalizes IPv6, which makes string comparison more likely to work.. thanks @dragontree101 for reporting. @ahxm do you have any errors from the console? I assume you are running java jar zipkin-server.jar as noted in the README?\n@smartthought @gena01 @Horusiath @theoptimiste I think each of you have run zipkin locally. Did you have to do anything special in order to do so? We are sorely missing a windows section on readme.\n@eirslett @abesto maybe we should setup a CI worker to use free windows VM to test zipkin can be started on windows (if not also that it can be built)\n. FWIW the zipkin-ui readme does not suggest to run zipkin server in\nintellij. Did you try what it suggests?\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-ui#whats-the-easiest-way-to-develop-against-this-locally\nIt would be helpful to figure out what works at all for you.. for example,\nif you failed to use the normal instructions and then used intellij for\nsome reason..\n. per #1532 folks who are trying to build on windows may need to avoid building the zipkin-ui project.\nex. mvnw -DskipTests --also-make -pl -:zipkin-ui,zipkin-server clean install. I'm worried this might be something strange about git or github. this popped up in circleci just now (but only my pull request, not the one from @devinsba)\nhttps://circleci.com/gh/openzipkin/zipkin/681?utm_campaign=build-failed&utm_medium=email&utm_source=notification. The .net topic is a pretty important one. There have been several occurrences where someone has entered the gitter channel asking about .net tracing and we are stuck between a rock and a hard place: Mention the incomplete zipkin-csharp project or mention one of the two full featured second-party projects such as criteo and mdsol's. Unless I hear strong objections from folks in @openzipkin/core, we'll move forward with setting aside zipkin-csharp in favor of what's currently criteo/zipkin4net\nOn stopping the zipkin-csharp experiment\nI've initiated an effort to move zipkin-csharp either to the attic or under @aliostad's personal account. You can follow that there, but suffice to say it is stalled and not healthy compared to other options. I'm asking folks to comment on this issue if you have strong feelings about zipkin-csharp: https://github.com/openzipkin/zipkin-csharp/issues/36\nOn Criteo's tracer going into OpenZipkin\nOn moving forward, I've informally asked @openzipkin/core about .net or at least csharp. There's unsurprisingly still support to have a full-featured tracer available in the OpenZipkin repository. While great work exists both in MdSol and Criteo repositories, Criteo have extra resources available to carry forward a transition. \nWhy Criteo?\nCode aside, I've paid attention to folks at criteo. Like MdSol, Criteo actively participate in distributed tracing workshop and tough problems here in Zipkin. @fedj takes time to address requests made by others or those that make it fit well with projects like zipkin-js. He's also one of the few people who volunteered to help do Zipkin builds, and has fixed bugs on the server. Their site is one of the highest volume sites Zipkin has, so keeping things working is of mutual benefit. Finally, it is more than reasonable to have a tracer written by one of our core team members be the official one. Knowing our other core teammates MdSol are supportive of this is equally important.\nBut what about fsharp etc?\nIf we are honest, it is not the right time to hold back a healthy csharp option in favor of one that doesn't exist. There is no project that covers all languages in dotnet, but if that happens, we can revisit it. It is encouraging that Criteo have taken time to address things they may not need themselves, such as .Net Standard 1.5 compat.\nBut what about OpenTracing?\n@fedj has also raised work towards a bridge for those who want to use opentracing. That's currently a pull request, but could be pulled out as a separate repo to address version drift if needed https://github.com/criteo/zipkin4net/pull/74\n. Thanks very interesting. You have very few but large tags. Is it safe to do\nmath to extrapolate span count from this?\nOn 10 Apr 2017 11:42 pm, \"Semyon Slepov\" notifications@github.com wrote:\n\nThere are some statistics about our \"bad\" experience:\n\n~6 Gb of traces in each of daily indices (keeping 5 last indices)\n~1 Kb per span\n3-4 tags per span\n\n( - running on m3.medium.elasticsearch in AWS, if it matters)\nThe main workload is writing and some users accidentally come to a\nweb-interface for looking their traces.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1526#issuecomment-292989310,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD615yRdvh0JxcWakH5BoLoEb4AkJwrks5ruk3lgaJpZM4MKsNW\n.\n. I have been thinking about this and believe adding a new index for service and span name is the best option. This would be just like cassandra where we keep a separate document for service/span lookup:\n\nCREATE TABLE IF NOT EXISTS zipkin3.span_name_by_service (\n    service_name text,\n    span_name    text,\n    PRIMARY KEY (service_name, span_name)\n)\nBackfilling this index would be relatively simple and could be done a myriad of ways. Unlike adding new data to the span index, this approach doesn't risk people accidentally clobbering their spans to support it. It also doesn't depend on any specific span representation (current or future).\nIn implementation, easiest would be a daily index like we use for dependency links. We could set the document ID to service and span name so that the data doesn't grow unbounded.  If daily is too course granularity, we could make the ID naming convention include the hour of the day or minute of the hour and still significantly bound the data size.\nWe could make the read side look at both, and only execute the slow nested query when the ideal index contains no data. This means users don't have to do anything to start using it.\nOperationally, I like this approach as being similar to cassandra means less cognitive overhead for cross-storage maintainers.\n@openzipkin/elasticsearch @semyonslepov wdyt?. >\n\n\nWhen is this new index populated? Is it when a span is written to\n   storage?\n\nyes when written, although we can also provide a script to apply to old\ndata\n\nHaving the ability to set the granularity of the index name would be\n   awesome. So, it would be nice to set the granularity to weekly or monthly\n   also. That way, we may not need to clean up this index if we so desire.\n\nIt is important to note that all of this assumes the index is the same as\nthe others.. this is just another document type in the existing index\n(another beyond span and dependencyLink). The current strategy for cleaning\nup any daily index would be curator or similar, and through that you can\nchoose to retain as much as you like (caveat being performance will\neventually degrade)\n\nSo the naming pattern I'm referring to is on the document id .. and doing\nsomething different would only be needed if we want less than a day\ngranularity on queries.\nex. The proposal is that inside the zipkin-2017-04-18/serviceSpan\nindex/type, there would be documents with ids like.. accounting|get-users\naccounting|update-users (this is the exact approach used for dependencyLink)\nIf this is good enough, we should leave it at that. If for some reason less\nthan daily is needed, we could add a timestamp, though it will make query\nmore complex and also visually looking at the data will be annoying. In\ncase you don't know.. I don't like this option :)\nex. for hourly\n00-accounting|get-users,\n01-accounting|get-users,  01-accounting|update-users,\n02-accounting|get-users, ... 23-accounting|get-users\n    * to do this, we'd need to store a timestamp in the json, too, which\nwould make the query a bit more expensive because if we go daily we don't\nneed to do anything except grab everything in the daily bucket.\n. np and thanks for your feedback! will wait for other feedback before\nheading towards impl.\n. >\n\nAbout indexes: sounds good and transparent for users. Am I right that we\nwill use the same index for spans' names lookup and it's going to be faster\ntoo?\nyes this index will support both. basically the query will be a lot\nsimpler, so it should be much faster. What won't be faster are trace\nqueries, as they still use nested stuff.\n\nI won't be able to benchmark as I don't know how besides laptop test which\nisn't likely very realistic.. hopefully you can help let us know how much\nbetter it goes. is that ok?\n. >\n\n+1 for a new index, but is there any payload that is from existence check\nwhen persisting service/span names?\nyes, there would have to be a payload so we can do the span name query. it\nwould be a document including the service name and the span name\n\nex. for id = accounting|update-users\nthe document would be:\n{ \"serviceName\": \"accounting\", \"spanName\": \"update-users\" }\nThat way, when we do a span name query, we can retrieve the spanName keys\nwith a term query where serviceName is the input.\nThis is needed because you cannot do a query against an id field.\nmake sense?\n. ok moving to impl as no one is against. thx for feedback, folks\n. will take a bit to refactor properly, but the more important code change at query time will look something like this...\nservice name query\ndiff\n-    SearchRequest.Filters filters = new SearchRequest.Filters();\n-    filters.addRange(\"timestamp_millis\", beginMillis, endMillis);\n-    SearchRequest request = SearchRequest.forIndicesAndType(indices, SPAN)\n-        .filters(filters)\n-        .addAggregation(Aggregation.nestedTerms(\"annotations.endpoint.serviceName\"))\n-        .addAggregation(Aggregation.nestedTerms(\"binaryAnnotations.endpoint.serviceName\"));\n+    SearchRequest request = SearchRequest.forIndicesAndType(indices, SERVICE_SPAN)\n+        .addAggregation(Aggregation.terms(\"serviceName\", Integer.MAX_VALUE));\nspan name query\ndiff\n-    SearchRequest.Filters filters = new SearchRequest.Filters();\n-    filters.addRange(\"timestamp_millis\", beginMillis, endMillis);\n-    filters.addNestedTerms(asList(\n-        \"annotations.endpoint.serviceName\",\n-        \"binaryAnnotations.endpoint.serviceName\"\n-    ), serviceName.toLowerCase(Locale.ROOT));\n-    SearchRequest request = SearchRequest.forIndicesAndType(indices, SPAN)\n-        .filters(filters)\n-        .addAggregation(Aggregation.terms(\"name\", Integer.MAX_VALUE));\n+    SearchRequest request = SearchRequest.forIndicesAndType(indices, SERVICE_SPAN)\n+        .term(\"serviceName\", serviceName.toLowerCase(Locale.ROOT))\n+        .addAggregation(Aggregation.terms(\"spanName\", Integer.MAX_VALUE));. https://github.com/openzipkin/zipkin/pull/1560 is ready, just looking to make a backport script or at least pseudocode for adding \"servicespan\" into old indexes. If I can get a shipit or change requests on #1562 and #1560, I will release the newly optimized stuff tomorrow.. @semyonslepov @tramchamploo @mansu @devinsba @jcarres-mdsol can any of you try zipkin 1.23? I'd like to get someone to use it in real life before propagating to other projects.. if any of you do get a chance, along with response time on /api/v1/services and /api/v1/spans please also report back any differences in storage usage, too. A lot of mapping options are now turned off, so it should be less though I don't know what percentage real life usage will end up as.. @devinsba try 0.2.2 :). @semyonslepov thanks for the feedback! fair to say overall better? :)\nyeah I wouldn't expect zipkin CPU to go down based on this change, though I would expect elasticsearch's CPU to go down. zipkin is actually doing slightly more, but if its CPU load when consuming becomes an issue we can probably profile a bit.. Ahh.. i think i know what might increase the load on ES. It is probably\nless about finer tuned indexing on spans, rather more about having to index\nservice and span name separately as it wasnt before. If this becomes an\nissue or we just want to sort it out.. we can employ a deduping approach\nused in cassandra. Essentially we dont write the same names twice from the\nsame node (rather than overwriting). If your tests are clean enough (as\nthey seem to be) we could test that deduping brings load back down..\n. The duration query was made for this type of search. Can you use this for\nnow? (understood that you need to know the value you want to constrain\nbefore you search)\n. >\n\ni think the ui can increase some function\uff0csuch as find the real longest\ntrace in some time , and after search, the search condition is lost ,and it\nshould not refresh all the page ,just refresh the search result,I think it\nis not friendly to users\nThe UI already has an option to sort results by duration.\n\nThe problem you are referring to is the backend. If I understand you right,\nyou want the server to return you the longest duration traces constrained\nby other parameters. For example, if there were 2 million traces between\ntoday and yesterday, return the 10 that had the longest duration. Correct?\n. So, the biggest change to support this would be related to indexing. For example, it is not realistic to continually search through a very large amount of traces even if a small amount could be done without noticing. It also is unlikely to work client-side for many sites as the data is too big.\nWe've some similar work in place to support the duration query, but it is a big deal feature and would  require different work for each data store (ex the solution for MySQL will be different than Cassandra, which is different than Elasticsearch).\nSo, the main concern is that we would need some relatively large effort to accomplish this feature.. @zengzonghou can you move the above comment to an issue on brave? I don't know which and this is unrelated to the duration query. It will help us make sure we solve each problem independently. Go for simplest auto-purge based on oldest data. I suspect span or trace id\nis likely easier to implement, but happy to review whatever proves easiest.\nOne knob is ideal because we dont want to start encouraging use of mem\nstorage by adding too many features to it.\n. on purge, I'd steal time from the accept method. not worth making any\nmachinery imho, as it will still be faster than i/o based span stores.\n. >\n\nThe min duration filter does not work when using Cassandra storage.\nThe duration filter requires that spans are sent with duration set. Which\nlibrary is creating the spans? We have tracked some of them here:\nhttps://github.com/openzipkin/openzipkin.github.io/issues/49\n. Sorry, I should have been more specific and the old readme should have been updated.\n\nif you are using \"cassandra\" this won't work as the indexing scheme used for duration caused a lot of problems including excess load. \"cassandra3\" supports duration query\nEx. this from CassandraSpanStoreTest\njava\n  @Test\n  @Override\n  public void getTraces_duration() {\n    throw new AssumptionViolatedException(\"Upgrade to cassandra3 if you want duration queries\");\n  }. Sorry, I should have been more specific and the old readme should have been updated.\nif you are using \"cassandra\" this won't work as the indexing scheme used for duration caused a lot of problems including excess load. \"cassandra3\" supports duration query\nEx. this from CassandraSpanStoreTest\njava\n  @Test\n  @Override\n  public void getTraces_duration() {\n    throw new AssumptionViolatedException(\"Upgrade to cassandra3 if you want duration queries\");\n  }. updated readme 9f2004e. updated readme 9f2004e. This is a duplicate of #1501\nnotably, the javascript project requires npm, which isn't installed on your machine and can't be auto-installed. you can try the following to skip building the ui\nmvnw -DskipTests --also-make -pl -:zipkin-ui,zipkin-server clean install. @openzipkin/cassandra PS does anyone know how to adjust the TTL after the schema has been installed?\n@pdraper tried this..\nbash\ncqlsh> alter table zipkin.traces with default_time_to_live = 60480;\nServerError: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError. @michaelsembwever ideas on changing the default ttl on an existing schema?. @michaelsembwever ideas on changing the default ttl on an existing schema?. opened another issue for how to modify ttl after the fact. This has been in place in some fashion since zipkin was around iirc. A\ndifferent ttl for data vs indexes supports retrieval by id for a longer\nperiod without impacting the size or performance of search requests\n(indexes). Runbooks in twitter had info go get trace by id when they were\nin log files for example. This wouldnt need a search to work. Not sure if\nthis split is popular anymore though.\nOn 30 Mar 2017 10:20 am, \"Yuri Shkuro\" notifications@github.com wrote:\n\nCurious as to the reasoning behind different TTLs\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1533#issuecomment-290282680,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD612bLYJeXE_rKDzG0xabWbfw9xLdNks5rqxFVgaJpZM4MUUEE\n.\n. sounds reasonable.. can you add a test in spanPanel.test.js? At the bottom there are similar ones for the other cases like this.\n\nThanks for the help. well done!. can you check to see what your git believes is the last update time on\nLazyGuavaStorageComponent?\nex.\ngit log C:/zipkin/zipkin-guava/src/main/java/zipkin/storage/guava/\nLazyGuavaStorageComponent.java\n. can you check to see what your git believes is the last update time on\nLazyGuavaStorageComponent?\nex.\ngit log C:/zipkin/zipkin-guava/src/main/java/zipkin/storage/guava/\nLazyGuavaStorageComponent.java\n. windows is not currently a supported build environment (because no-one has\ngotten a solution together for #1501).\nI raised https://github.com/openzipkin/zipkin/pull/1536 which might help\n(assuming line endings are the reason for the license glitch, which might\nnot be the case). If you can test that, it would be helpful.\n. windows is not currently a supported build environment (because no-one has\ngotten a solution together for #1501).\nI raised https://github.com/openzipkin/zipkin/pull/1536 which might help\n(assuming line endings are the reason for the license glitch, which might\nnot be the case). If you can test that, it would be helpful.\n. Yes most of use linux or osx\nOn 9 Mar 2017 11:06, \"tomgithub2016\" notifications@github.com wrote:\nYou mean if with Linux and modify the local source code,there will be\ncompile successfully?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1535#issuecomment-285294745,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD615U_3BX2jQAImpEC22H6IfPgkRt5ks5rj8ENgaJpZM4MXvhF\n.\n. Yes most of use linux or osx\nOn 9 Mar 2017 11:06, \"tomgithub2016\" notifications@github.com wrote:\nYou mean if with Linux and modify the local source code,there will be\ncompile successfully?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1535#issuecomment-285294745,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD615U_3BX2jQAImpEC22H6IfPgkRt5ks5rj8ENgaJpZM4MXvhF\n.\n. If you modify files it could invalidate the license header (which includes\na year of modification) look at CONTRIBUTING.md which shows the command to\nre-apply license headers (which would change a file you edited from 2016 to\n2016-2017)\nOn 9 Mar 2017 15:50, \"tomgithub2016\" notifications@github.com wrote:\n\nI change to Linux,and git clone \"https://github.com/openzipkin/zipkin/git\"\nIf I do not modify any file,compile successful\nIf I modify any files compile failed.\nIf I understand is wrong, should be use git checkout?\nI only want to download the source code, modify the source code, and\ncompile successful\nHow to do?\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Guava support library 1.20.2-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ zipkin-guava ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /root/zipkin/zipkin-guava/src/main/resources\n[INFO]\n[INFO] --- maven-compiler-plugin:3.6.0:compile (default-compile) @ zipkin-guava ---\n[INFO] Changes detected - recompiling the module!\n[INFO] Compiling 10 source files to /root/zipkin/zipkin-guava/target/classes\n\u6ce8: /root/zipkin/zipkin-guava/src/main/java/zipkin/storage/guava/InternalGuavaToAsyncSpanStoreAdapter.java\u4f7f\u7528\u6216\u8986\u76d6\u4e86\u5df2\u8fc7\u65f6\u7684 API\u3002\n\u6ce8: \u6709\u5173\u8be6\u7ec6\u4fe1\u606f, \u8bf7\u4f7f\u7528 -Xlint:deprecation \u91cd\u65b0\u7f16\u8bd1\u3002\n[INFO]\n[INFO] --- license-maven-plugin:3.0:check (default) @ zipkin-guava ---\n[INFO] Checking licenses...\n[WARNING] Missing header in: /root/zipkin/zipkin-guava/src/main/java/zipkin/storage/guava/GuavaStorageAdapters.java\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 2.229 s\n[INFO] Finished at: 2017-03-09T21:28:21+08:00\n[INFO] Final Memory: 28M/266M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal com.mycila:license-maven-plugin:3.0:check (default) on project zipkin-guava: Some files do not have the expected license header -> [Help 1]\n[ERROR]\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1535#issuecomment-285355831,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61zTgffKJHlq8jrHrJx7EJFl3PS1wks5rkAOwgaJpZM4MXvhF\n.\n. If you modify files it could invalidate the license header (which includes\na year of modification) look at CONTRIBUTING.md which shows the command to\nre-apply license headers (which would change a file you edited from 2016 to\n2016-2017)\n\nOn 9 Mar 2017 15:50, \"tomgithub2016\" notifications@github.com wrote:\n\nI change to Linux,and git clone \"https://github.com/openzipkin/zipkin/git\"\nIf I do not modify any file,compile successful\nIf I modify any files compile failed.\nIf I understand is wrong, should be use git checkout?\nI only want to download the source code, modify the source code, and\ncompile successful\nHow to do?\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Guava support library 1.20.2-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ zipkin-guava ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /root/zipkin/zipkin-guava/src/main/resources\n[INFO]\n[INFO] --- maven-compiler-plugin:3.6.0:compile (default-compile) @ zipkin-guava ---\n[INFO] Changes detected - recompiling the module!\n[INFO] Compiling 10 source files to /root/zipkin/zipkin-guava/target/classes\n\u6ce8: /root/zipkin/zipkin-guava/src/main/java/zipkin/storage/guava/InternalGuavaToAsyncSpanStoreAdapter.java\u4f7f\u7528\u6216\u8986\u76d6\u4e86\u5df2\u8fc7\u65f6\u7684 API\u3002\n\u6ce8: \u6709\u5173\u8be6\u7ec6\u4fe1\u606f, \u8bf7\u4f7f\u7528 -Xlint:deprecation \u91cd\u65b0\u7f16\u8bd1\u3002\n[INFO]\n[INFO] --- license-maven-plugin:3.0:check (default) @ zipkin-guava ---\n[INFO] Checking licenses...\n[WARNING] Missing header in: /root/zipkin/zipkin-guava/src/main/java/zipkin/storage/guava/GuavaStorageAdapters.java\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 2.229 s\n[INFO] Finished at: 2017-03-09T21:28:21+08:00\n[INFO] Final Memory: 28M/266M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal com.mycila:license-maven-plugin:3.0:check (default) on project zipkin-guava: Some files do not have the expected license header -> [Help 1]\n[ERROR]\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1535#issuecomment-285355831,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61zTgffKJHlq8jrHrJx7EJFl3PS1wks5rkAOwgaJpZM4MXvhF\n.\n. there are no more detailed steps.. you paste the below command, which will\nreformat the license header of the file you changed.\n\n./mvnw com.mycila:license-maven-plugin:format -pl -:zipkin-ui\nsee\nhttps://github.com/openzipkin/zipkin/blob/master/.github/CONTRIBUTING.md#license\nfor more\nOn Fri, Mar 10, 2017 at 9:02 AM, tomgithub2016 notifications@github.com\nwrote:\n\nI probably understand what you mean,but how to edited a file from 2016 to\n2016-2017(which file?)\nand Is to run this command mvnw com.mycila:license-maven-plugin:format\n-pl -:zipkin-ui\nCan you give me more detail steps? thank you.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1535#issuecomment-285540576,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xrkU0rgjhmeEvn57QFnJRWrAVUbks5rkKEhgaJpZM4MXvhF\n.\n. there are no more detailed steps.. you paste the below command, which will\nreformat the license header of the file you changed.\n\n./mvnw com.mycila:license-maven-plugin:format -pl -:zipkin-ui\nsee\nhttps://github.com/openzipkin/zipkin/blob/master/.github/CONTRIBUTING.md#license\nfor more\nOn Fri, Mar 10, 2017 at 9:02 AM, tomgithub2016 notifications@github.com\nwrote:\n\nI probably understand what you mean,but how to edited a file from 2016 to\n2016-2017(which file?)\nand Is to run this command mvnw com.mycila:license-maven-plugin:format\n-pl -:zipkin-ui\nCan you give me more detail steps? thank you.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1535#issuecomment-285540576,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xrkU0rgjhmeEvn57QFnJRWrAVUbks5rkKEhgaJpZM4MXvhF\n.\n. No problem!\n\nOn 10 Mar 2017 11:00, \"tomgithub2016\" notifications@github.com wrote:\n\n@adriancole https://github.com/adriancole thank you.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1535#issuecomment-285614581,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD611Aqk3Upi0oP3Ztt1NaJek2XO9qLks5rkREygaJpZM4MXvhF\n.\n. No problem!\n\nOn 10 Mar 2017 11:00, \"tomgithub2016\" notifications@github.com wrote:\n\n@adriancole https://github.com/adriancole thank you.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1535#issuecomment-285614581,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD611Aqk3Upi0oP3Ztt1NaJek2XO9qLks5rkREygaJpZM4MXvhF\n.\n. I like the idea of finding a lightweight manner to progress #782 (which I think has been discussed offline as well, including using either custom auto-config or nginx proxy for the same).\n\nUntil reading further, it seems that even shared password would have drawbacks, notably that no instrumentation projects support it. That means if it was turned on, you could no longer use any instrumentation projects who use http transport, right? There's also the matter of how reusable this is to others.. I can't pretend to know that, so we should just ping folks.\nI don't know the dependencies implied by spring-boot-starter-security, so it is probably more than just adding one existing jar and a config file layer.. have you tried this way?. I like the idea of finding a lightweight manner to progress #782 (which I think has been discussed offline as well, including using either custom auto-config or nginx proxy for the same).\nUntil reading further, it seems that even shared password would have drawbacks, notably that no instrumentation projects support it. That means if it was turned on, you could no longer use any instrumentation projects who use http transport, right? There's also the matter of how reusable this is to others.. I can't pretend to know that, so we should just ping folks.\nI don't know the dependencies implied by spring-boot-starter-security, so it is probably more than just adding one existing jar and a config file layer.. have you tried this way?. ps I don't see any tests, but we'd check the impact to see if this affects both the api and the UI (presumably it affects both). ps I don't see any tests, but we'd check the impact to see if this affects both the api and the UI (presumably it affects both). as a conversation starter, I think it succeeded :) To continue it, I'd\nprobably ping folks on issues who have formerly mentioned auth\nrequirements. Sleuth is one of many libraries, so if it were me, I'd try to\nsee what folks from other languages are interested in, or how other similar\nprojects secure their http transports for telemetry data.\nI'm glad you started this.. if you can keep it up, please summarize updates\nto #782 https://github.com/openzipkin/zipkin/issues/782 I'll make some\nnotes there, which might help.\n. as a conversation starter, I think it succeeded :) To continue it, I'd\nprobably ping folks on issues who have formerly mentioned auth\nrequirements. Sleuth is one of many libraries, so if it were me, I'd try to\nsee what folks from other languages are interested in, or how other similar\nprojects secure their http transports for telemetry data.\nI'm glad you started this.. if you can keep it up, please summarize updates\nto #782 https://github.com/openzipkin/zipkin/issues/782 I'll make some\nnotes there, which might help.\n. I dont think anything has progressed. It is important we dont add\nspeculative features. For example, none of the open source zipkin reporters\nwould work out of the box as they dont expect to authenticate.\nBesides comments above and others made in the past, we need to make sure\nthis isnt a \"tease feature\" (looks neat but no one in the ecosystem\nsupports it)\n. Are you in a position to share your new requirements? In back of mind was\nthinking we can re-eval some things about modularity when we go spring boot\n2\n. Are you in a position to share your new requirements? In back of mind was\nthinking we can re-eval some things about modularity when we go spring boot\n2\n. also obviated by this workaround https://github.com/openzipkin/zipkin/issues/782#issuecomment-433306013. circleci is freaking out on license header dates (which was a problem we had in travis prior). circleci is freaking out on license header dates (which was a problem we had in travis prior). sounds like a bug. mind sending a PR to fix this (along with a unit test to\nshow it working)?\nnice find!\n. fantastic work! cc @dsyer @shakuzen \nthis is per notes here https://github.com/openzipkin/zipkin/commit/568e3e3ffdd9f319c877692b2d268a894f95bef0#commitcomment-21300483\nand this is in efforts to reduce the load on the zipkin server. fantastic work! cc @dsyer @shakuzen \nthis is per notes here https://github.com/openzipkin/zipkin/commit/568e3e3ffdd9f319c877692b2d268a894f95bef0#commitcomment-21300483\nand this is in efforts to reduce the load on the zipkin server. yeah I'm mainly only concerned with a bean present and not (ex javaconfig\nfor counterbuffers/guagebuffers and another when they aren't there). you\ncan assert on the spring factory as a reality check (ex to verify\ncounterbuffers are or aren't there) before asking for collectormetrics\n. yeah I'm mainly only concerned with a bean present and not (ex javaconfig\nfor counterbuffers/guagebuffers and another when they aren't there). you\ncan assert on the spring factory as a reality check (ex to verify\ncounterbuffers are or aren't there) before asking for collectormetrics\n. released in 1.21.0. released in 1.21.0. leaving this open for a day or two in case someone isn't looking. leaving this open for a day or two in case someone isn't looking. @cburroughs @virtuald either of you bored? :). One thing surprising in that span is that the client and server annotations\nhave the same endpoint.. was that a loopback?\nOn 24 Mar 2017 18:16, \"Yannick Schutz\" notifications@github.com wrote:\n@adriancole https://github.com/adriancole I can try to take a look. Am I\ncorrect seeing this broken or am I doing something wrong?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1541#issuecomment-288984047,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61yyrGrD89lsXWfC4JEtqU2k59jX5ks5ro5gRgaJpZM4MljZP\n.\n. If it doesnt make a remote call, maybe best to not add cs sr in that span.\nIt is possible there is some ui logic that is looking at that. This is just\na guess\nOn 24 Mar 2017 20:05, \"Yannick Schutz\" notifications@github.com wrote:\n\nIt is a sidekiq background job.\nSo yeah async tasks on same app.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1541#issuecomment-289005890,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6124khNW-aWrUkHKzshiht0dN4L-Pks5ro7GSgaJpZM4MljZP\n.\n. Existing code (zipkin-js and finagle) render redis as a client span (cs\ncr). There wouldnt be a 'sr' until redis itself has zipkin support. cr is\nthe end of a span.. if theres something later that happens as a callback\nfrom the redis command.. probably a child span?\n\nAscii art flow of what is going on in real life might help. Usually the\nboxes in a flow diagram end up as spans.\nOn 24 Mar 2017 9:15 pm, \"Yannick Schutz\" notifications@github.com wrote:\n\nThis enqueue the job into redis.\nSide question is how would you render that ?\nIt is async jobs with a queue that is redis.\nDifferent instances\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1541#issuecomment-289019342,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD612Ao1upv4YA-IpfQM6v933InnuyNks5ro8HegaJpZM4MljZP\n.\n. here's my 2p:\n\nso I think the client enquing the span would be a one-way \"cs\"\nthe worker pulling the message extracts trace ids and adds \"sr\" to the same\nspan as the client.\na child span of that should be made for the processing of that message\n. PS we have messaging spans now.. do you like things like this?\n\n. good capture of the problem.. should be reproducible in a test\n. good capture of the problem.. should be reproducible in a test\n. Thanks for the start.\nI'm not sure how this is supposed to work. Can you mention the steps that would preserve order?\nI go and find traces, set the order to oldest, then click on a trace. Regardless of how I navigate to the search screen (back button, clicking on find trace etc), the order isn't kept.\nthis is using chrome, in case this helps.. The trouble I have here is that I don't know how natural it is for someone to change the sort order then click search, and the problem is only sorted (no pun intended) when that's the case. It sounds fragile, because it requires someone to know the way to not make it break :)\n@eirslett other ideas?. sounds ok to me..\nOn Thu, Mar 30, 2017 at 9:14 PM, joel-airspring notifications@github.com\nwrote:\n\nIf the {{#queryWasPerformed}} section of index.mustache was always\nincluded, then the problem goes away. The only thing that would change\nwould be the default view from 0 traces showing.\nFrom:\n[image: current]\nhttps://cloud.githubusercontent.com/assets/4552139/24505570/529d3898-1528-11e7-96ee-1d4da15f7b27.png\nTo:\n[image: always]\nhttps://cloud.githubusercontent.com/assets/4552139/24505582/5fde1f22-1528-11e7-9645-965a11ea5b8f.png\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1543#issuecomment-290406757,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6172PAzPmDOvuyDWmi9T0_KWokG6Aks5rq6q0gaJpZM4MmdeD\n.\n. I will take a look tomorrow.. if works and needs no changes, and no one objects, it will get in the next cut this weekend. thx for the help!. is your job air spring mattress or air spring software? :D. works for me. if anyone complains, we'll revisit. It looks like the 5 spans is relating to the amount of service * spans in the trace (ex if you look at the image, clientapp.com X2 + desktop-prbarl *2 + localhost * 1 = 5\n\na unit test could probably prove this out https://github.com/openzipkin/zipkin/blob/master/zipkin-ui/test/component_ui/traceSummary.test.js#L329. As suspected by @prbarl this looks like a dupe of #842. I'm answering eventhough this logic predated me by quite a while.\nthe search screen is primarily partitioned by service. a guess is that this was originally Span count (in service) as opposed to Total Spans.\nIn that case, span count in this screen is implicitly count of spans that passed through the selected service. For example, there's a percentage of time in the same panel which is also \"current service\" specific.\nIf it were more clear, it could be a fraction ex. \"Span count: 5/10\"\nThe trace detail screen isn't scoped to a specific service by default, and the \"Total spans\" there should be in fact the total. The \"per service\" boxes below it should be accurate. For example, if you click on a span which had the service \"foo\" selected in the search screen, then the Span count on the search screen should match \"foo x NNN\" on the detail screen.. The following trace can reproduce the issue.. the search page is reporting service * span (I think) as opposed to total spans as span count\njson\n[\n  {\n    \"traceId\": \"5c53ab5f3f2ac9ff\",\n    \"id\": \"5c53ab5f3f2ac9ff\",\n    \"name\": \"get\",\n    \"timestamp\": 1491286274031591,\n    \"duration\": 37217,\n    \"annotations\": [\n      {\n        \"timestamp\": 1491286274031591,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"172.18.0.4\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"timestamp\": 1491286274068808,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"172.18.0.4\",\n          \"port\": 9411\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"ca\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"\",\n          \"ipv4\": \"192.168.99.1\",\n          \"port\": 59656\n        }\n      },\n      {\n        \"key\": \"http.status_code\",\n        \"value\": \"200\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"172.18.0.4\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"key\": \"http.url\",\n        \"value\": \"/api/v1/spans\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"172.18.0.4\",\n          \"port\": 9411\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"5c53ab5f3f2ac9ff\",\n    \"id\": \"331fc2da6e9d74cf\",\n    \"name\": \"get-span-names\",\n    \"parentId\": \"5c53ab5f3f2ac9ff\",\n    \"timestamp\": 1491286274031937,\n    \"duration\": 35797,\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"lc\",\n        \"value\": \"AutoValue_ElasticsearchHttpStorage\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"172.18.0.4\",\n          \"port\": 9411\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"5c53ab5f3f2ac9ff\",\n    \"id\": \"fcd1101cae5fc528\",\n    \"name\": \"aggregation\",\n    \"parentId\": \"331fc2da6e9d74cf\",\n    \"timestamp\": 1491286274037720,\n    \"duration\": 29791,\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"lc\",\n        \"value\": \"okhttp\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"172.18.0.4\",\n          \"port\": 9411\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"5c53ab5f3f2ac9ff\",\n    \"id\": \"a69778605c1e1a00\",\n    \"name\": \"post\",\n    \"parentId\": \"fcd1101cae5fc528\",\n    \"timestamp\": 1491286274043711,\n    \"duration\": 23671,\n    \"annotations\": [\n      {\n        \"timestamp\": 1491286274043711,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"172.18.0.4\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"timestamp\": 1491286274067382,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"172.18.0.4\",\n          \"port\": 9411\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"http.url\",\n        \"value\": \"http://elasticsearch:9200/zipkin-2017-04-03,zipkin-2017-04-04/span/_search?allow_no_indices=true&expand_wildcards=open&ignore_unavailable=true\",\n        \"endpoint\": {\n          \"serviceName\": \"zipkin-server\",\n          \"ipv4\": \"172.18.0.4\",\n          \"port\": 9411\n        }\n      },\n      {\n        \"key\": \"sa\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"elasticsearch\",\n          \"ipv4\": \"172.18.0.2\",\n          \"port\": 9200\n        }\n      }\n    ]\n  }\n]. so you see something like this.. note that the number 5 matches the count summed below it.\n\nWe either need to change the name to something besides \"spans\" or replace the value with totalSpans. https://github.com/openzipkin/zipkin/pull/1550. this should be fixed now.. try latest (currently 1.23.0). it looks like your zipkin versions are not aligned. I'd set everything under the io.zipkin.java group id to the same version. gradle can do this I think.. I assigned mick (as a polite request) but if anyone @openzipkin/cassandra can help, would be great. One way to help is to ask on one of the cassandra forums as I don't expect this to be zipkin specific (except the table names). from gitter @maryoush Mar 29 23:51\n\nin my case only what effectively allows me to get rid of old traces is to run globally script on cassandra to adjust the ( gc_grace_seconds , default_time_to_live ). cc @openzipkin/elasticsearch . lets try to wrap this up today, as I'd like to cut a release that sorts out the elasticsearch stuff. I went ahead and resolved conflicts.. will merge on green!. indeed. sorry if I messed up anything for you. cc @felixbarny . have you tried https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md#logging\n\nif you have more problems try https://gitter.im/openzipkin/zipkin or worst case stackoverflow thx!. Here are two related issues on this topic to review before we talk about\nnew code and where:\ncache control:\nhttps://github.com/openzipkin/zipkin/issues/718#issuecomment-264052551\nelasticsearch performance: https://github.com/openzipkin/zipkin/issues/1526\ndo you mind commenting inside each, particularly in #1526 note how much you\nare ingesting and if you are doing any sampling.\n. one last thing to verify. we recently compensated for this in #1538 \nare you running latest?. >\n\nYes, we're already using it, it's a great thing and helps a lot! But I\nsuppose if we increase our workload significantly (and yes, we will do it),\nwe will again face the same problem (and we won't be able to decrease\nQUERY_LOOPBACK because it's not affordable for our users).\ncool. just covering the bases..\n\nI think we'll end up with a decision about where to introduce a cache if we\ndo (in UI code vs in the ES impl).\nAlso, if that cache needs to be managed like it is here (the response is so\nslow that you can't rely on users to ever succeed). I kindof prefer not\nhaving machinery in-process, as it would be handy regardless of the cache\nimpl for it to be user scheduled. maybe there's something we can do in the\nUI to not block, but not make repeated calls, when a user first calls for\nservice and span names?\nthen there's also the chance we toy with data format.. I forget which issue\nis tracking that one, but flattening service+span names inside ES might end\nup being a way out.\n. moved my last comment to #1526 which was the issue I thought I was on!. @semyonslepov assuming w/current perf we might be able to close this?. I would love to hear if you end up with hundreds of users who all refresh\nbrowser cache/expire these headers :) would be a sign of a very effective\ndeployment.\nI do think we will get better and probably simplest might be an caching\nintermediary as it could be applied to all and invalidation isnt terribly\nimportant with service names. Simpler than changing storage (plus not\npositive of perf differences here or cassandra either). We could also\nconsider an optional caching decorator storagecomponent.\nI do think we will change the format stored in ES by year end though this\nwill likely only help with trace queries as service span are now simple as\npossible.\nMeanwhile thanks for all the feedback. You have been very helpful in this!\n. would be interesting to see what the trace had when it had no timestamp. possibly could be a one-way span which hasn't seen its caller yet (in-flight). if you have a trace that results in a blank page, possibly anonymizing that trace could lead to test data to put in the unit test for this.. heh looks like self-tracing is broke :) there's a brave.flush annotation!\nOn Tue, Apr 11, 2017 at 4:20 PM, Semyon Slepov notifications@github.com\nwrote:\n\nThere is an example of such trace (notice again that it is self-trace of\nZipkin, haven't found such traces in other places yet):\n{\n  \"traceId\": \"03ac7fee421c76a0\",\n  \"id\": \"6c353b6ed9baabf1\",\n  \"name\": \"get-service-names\",\n  \"parentId\": \"2cb3257011291dfe\",\n  \"annotations\": [\n    {\n      \"timestamp\": 1491454307215328,\n      \"value\": \"brave.flush\",\n      \"endpoint\": {\n        \"serviceName\": \"zipkin-server\",\n        \"ipv4\": \"X.X.X.X\",\n        \"port\": 9411\n      }\n    }\n  ],\n  \"binaryAnnotations\": [\n    {\n      \"key\": \"lc\",\n      \"value\": \"AutoValue_ElasticsearchHttpStorage\",\n      \"endpoint\": {\n        \"serviceName\": \"zipkin-server\",\n        \"ipv4\": \"X.X.X.X\",\n        \"port\": 9411\n      }\n    }\n  ]\n},\n{\n  \"traceId\": \"03ac7fee421c76a0\",\n  \"id\": \"21e299b050cd0a85\",\n  \"name\": \"get-service-names\",\n  \"parentId\": \"5ecae66e040c3a35\",\n  \"timestamp\": 1491436907215516,\n  \"duration\": 13820,\n  \"binaryAnnotations\": [\n    {\n      \"key\": \"lc\",\n      \"value\": \"AutoValue_ElasticsearchHttpStorage\",\n      \"endpoint\": {\n        \"serviceName\": \"zipkin-server\",\n        \"ipv4\": \"X.X.X.X\",\n        \"port\": 9411\n      }\n    }\n  ]\n},\n{\n  \"traceId\": \"03ac7fee421c76a0\",\n  \"id\": \"01944d480bd78a91\",\n  \"name\": \"get-service-names\",\n  \"parentId\": \"21e299b050cd0a85\",\n  \"timestamp\": 1491437207215343,\n  \"duration\": 17292,\n  \"binaryAnnotations\": [\n    {\n      \"key\": \"lc\",\n      \"value\": \"AutoValue_ElasticsearchHttpStorage\",\n      \"endpoint\": {\n        \"serviceName\": \"zipkin-server\",\n        \"ipv4\": \"X.X.X.X\",\n        \"port\": 9411\n      }\n    }\n  ]\n},\n// other spans with timestamps \\\nWhat really surprises me is the fact that we have a timestamp in\nannotation of the first span, but not in the span.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1555#issuecomment-293185990,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61_BKBbjtfCtNX0NPPq2cC4k88HJTks5ruzfjgaJpZM4M4trJ\n.\n. on my honor, I will add backfill a test later :). in the javadoc, you'll see that the \"target storage rate in spans/minute\"\nthe updateFrequency is documented as \"Frequency in seconds which to update the store and sample rate\"\n\nso 60 here is seconds in a minute. to be more specific, the calculation you copy/pasted is how to get the storage rate in spans/minute, given spans since updateFrequency (defaults to every 30s).. good question. honestly porting this code taught me a lot.\nSo discounted average is in play to give preference to certain types of\nnumbers.\nfor example, if your rate is 10000 spans/second and then you get an\naccidental reading of zero spans/second (because a machine malfunctioned\nbriefly), if would skew an average quite a bit.\nThe default used adjusts for this, which you can see in play in the unit\ntests. For example, small numbers don't impact the discounted average like\nthey would a normal average. This helps make the adaptive part less fickle,\nas it doesn't change the rate as wildly when things happen such as server\nrestarts.\n@Test public void discountedAverage() {\n  assertThat(SampleRateCalculator.discountedAverage(asList(10, 5, 0), 1.0f))\n      .isEqualTo(5); // normal average\nassertThat(SampleRateCalculator.discountedAverage(asList(10, 5, 0), 0.09f))\n      .isEqualTo(9); // smaller discount rate prefers larger numbers\nassertThat(SampleRateCalculator.discountedAverage(asList(1000, 5, 0), 0.09f))\n      .isEqualTo(911);\n}\n. ok code now sends service/span indexes in the same batch command. This should help with AWS transfer charges, especially since we gzip requests. cc @devinsba @semyonslepov  \nstill need some code cleanup and other notes in the TODO. backport logic in place. going to give a stab at a backport script as particularly serviceNames being current helps.. I think the best way to backport is to sneak hidden feature into zipkin-dependencies (as it is already a spark job and can do everything). If someone wants to migrate old data, they'd need to do the following.\nFor each span in an index (ex zipkin-2017-02-14/span)\nIf span.name isn't empty, create a document like this for each service name in span.annotations[].endpoint.serviceName, span.binaryAnnotations[].endpoint.serviceName, \nid = myServiceName|mySpanName\n{ \"serviceName\":\"myServiceName\",  \"spanName\":\"mySpanName\", \nsave the unique set of documents to the \"servicespan\" type  in the same index (ex zipkin-2017-02-14/servicespan). depends on #1562. fixed a very silly test setup bug which consumed a lot of my last night.. will be ready to go on green\nPS ignoring circleci because it thinks the date is different than it is (a routine problem unrelated to this code). cc @openzipkin/elasticsearch @jcarres-mdsol . Eventhough 5k sounds like a lot. This is deduping client side right? In\nyour site what is the amount of names returned vs unique?\n. >\n\nThe case where we hit this the most was on getSpansByTraceIds. If we put\na high number, sometimes even 1000, in the UI trace limit field it can\ntrigger the error. For other read paths such as getSpanNames we didn't\nhit this error. Even though we didn't encounter this in other read paths,\nthe pattern seemed easy enough to generalize that it could be applied to\nall areas and not just getSpansByTraceIds.\nah thx for the insight. so basically we should be able to find some command\nthat creates this error, even if it is a sneaky test setup.\n. Mind trying maybe with TestObjects.LOTS_OF_SPANS sliced at 5001\n(instead of the old code that replicates LOTS_OF_SPANS)? If it doesn't\ndramatically increase the test duration, should be fine.\n. merging this as cassandra3 is experimental and leaving this unmerged keeps the code broken. Will try and merge in the other things so we can follow-up with tests.. If this turns out to work without dogging the test time, lets comment on\n6000 that it intends to trigger pagination\n. Ps might end up wanting to chop this into slices ex maybe 100 accept\ncommands with 60 spans each (guessing at cardinality that is best). This\nimplies mild join on async work, but could considerably speed up test\nexecution.. reopened as circleci was turned off for forks!. so my advice would be to use the least amount to trip the test (document the count).\n\nThen, you can cherry-pick this commit into the one that fixes it (essentially this broken test doesn't merge until there's a fix)\nsg?. uh-oh.. passed circleci!!. circle is running c* 3.10 iirc.. wonder if is version-specific, the\nparameter?\nOn Sun, Apr 16, 2017 at 11:56 AM, Lance Linder notifications@github.com\nwrote:\n\nLocally I've been able to trigger this error with as few as 2000 traces.\nAfter some investigation I see that the Cassandra tests are skipped in the\nTravis PR builds which is why this PR didn't fail.\nAlso doubling the trace count didn't have a noticeable impact on test time\nso I'm wondering if its worth the effort to fan out the accept calls. I can\nstill add that if it seems valuable though.\nRight now I'm not sure how to trigger the failure in a pull request\nwithout merging it which will surely break the build :)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1565#issuecomment-294332050,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD616gGXSMDLyn22XDku-bqzFws-Yx5ks5rwZFigaJpZM4M-hOf\n.\n. Thx for the tip. Makes sense\n\nOn 17 Apr 2017 05:49, \"Lance Linder\" notifications@github.com wrote:\n\nLooks like CircleCI is skipping the C tests as well. My suspicion is that\nC is failing to fully start. When I get a chance I will SSH into the build\nmachine and look at the logs.\n21:10:37.900 [main] INFO  c.datastax.driver.core.ClockFactory - Using native clock to generate timestamps.\n21:10:37.952 [main] INFO  com.datastax.driver.core.NettyUtil - Did not find Netty's native epoll transport in the classpath, defaulting to NIO.\nTests run: 41, Failures: 0, Errors: 0, Skipped: 41, Time elapsed: 3.571 sec\nResults :\nTests run: 60, Failures: 0, Errors: 0, Skipped: 41```\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1565#issuecomment-294376351,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wb4Ol8HpNaWKMt4OZ1EfxYxmKxWks5rwozWgaJpZM4M-hOf\n.\n. trying this after noticing the process crash locally\n\nhttps://circleci.com/gh/openzipkin/zipkin/761\n. ok I think this failure is legit. we probably need to dockerize\ncassandra tests and quit trying to run them in circleci (because\ncircle starts cassandra even if it is never used and it is really\nfragile)\nhttps://circleci.com/gh/openzipkin/zipkin/tree/cassandra-tests\nOn Mon, Apr 17, 2017 at 10:16 AM, Adrian Cole adrian.f.cole@gmail.com wrote:\n\ntrying this after noticing the process crash locally\nhttps://circleci.com/gh/openzipkin/zipkin/761\n. Can you anonymize the trace json? (Click json button on it)\nAlso note a screen grab of the ui when search works (or otherwise capture\nthe parameters the UI uses for the /api/v1/traces endpoint)\n. lol mrtask huh?\n. thx will look into this today\n. first thing I noticed is the duration is invalid. 28367750927 is being stored as the duration (which is microsecond), which would be almost 8 hours. \n\nnext, the timestamps are odd ex. 28395517671 which isn't a valid epoch microsecond.\n28395517671 <- your data\n1492500435180000 <- current epoch microseconds\nSo, I'd look at the time and duration code, first. That will probably solve the search thing. There are tips here: http://zipkin.io/pages/instrumenting.html Alternatively, you could consider basing your custom instrumentation on brave's api instead, which handles timestamps etc https://github.com/openzipkin/brave/tree/master/brave. no worries\n. troubleshooting is a bit easier on gitter https://gitter.im/openzipkin/zipkin\nthat said, it is interesting this pattern only exists in mysql so far (you could also try aws elasticsearch, but still we should figure out what's up).\ntry setting SELF_TRACING_ENABLED=true and see if zipkin traces make it even if yours don't? This might help narrow things down. here's an example query which might help, too https://github.com/openzipkin/zipkin/pull/1568. Sorry about timezones making this troubleshooting more difficult. So are\nyou saying that you saw internal traces but not app ones after 30m or that\nyou saw no traces? Seems like none if the sql query returned none. You\ncould check the /metrics endpoint to ensure you are receiving messages.\nYep es or c* are better long term anyway although i would like to\nunderstand what you are bumping into.. guessing theres an error receiving\nor writing data which is somehow not in you logs. Seeing metrics will at\nleast show you spans incoming and if these are plotted with something you\nmight see a correlation between the incoming messages counted and the count\nin storage. If you look at the docker-zipkin repo theres a pull request\nregarding metrics and prometheus if you dont have a different tool (or just\ncalling /metrics isnt enough)\n. closing as a question as there's no clear single change to the codebase this is about. If we need to revisit, please chat on gitter! https://gitter.im/openzipkin/zipkin. closing as a question as there's no clear single change to the codebase this is about. If we need to revisit, please chat on gitter! https://gitter.im/openzipkin/zipkin. hmm that sql file isn't supposed to be executed automatically. what is\ncausing that?\n. >\n\nI copy/pasted that from an example....\nyeah sorry. maybe someone made an example to do that, but we haven't\nwritten it to do incremental updates. It is probably best to not rely on\nthat.\n. @sanamaghraoui please use https://gitter.im/openzipkin/zipkin instead of posting open-ended questions into github issues. thank you!. Congrats on the new failures :) knowing is half the battle\n. ok I rejigged a little on the tests. One thing I had to do was throttle the tests not to write so much at a time to the docker cassandra, as it makes it all crash and burn with BusyPoolException. There are two failing tests now, which we can take care of afterwards.. I think this was also noticed by @felixbarny. You can verify that it is indeed storing in MySQL by looking at the /health\nendpoint. Also, maybe check /metrics to see if you are getting storage\nerrors?\n. ok if you have no errors and increasing memory it seems there is a queue backing up. you can look at threads to see if any are stuck. if pending spans are valuable you can add another zipkin server and stop routing traffic to the clogged one. otherwise bounce the clogged once you see which threads are stuck. @shakuzen @devinsba @denyska so we seem to be accepting unbounded POSTs, and the memory eats up before the collector component gets involved (hence no drop metrics). wonder if theres a way to control in-flight accepts better.. . cc @dsyer @marcingrzejszczak I suspect there's a way to address this cheaply.. I think there's at least 2 paths\n\nCurrently ZipkinHttpCollector asks for buffered content like so..\njava\n  @RequestMapping(value = \"/api/v1/spans\", method = POST)\n  public ListenableFuture<ResponseEntity<?>> uploadSpansJson(\n      @RequestHeader(value = \"Content-Encoding\", required = false) String encoding,\n      @RequestBody byte[] body\n  )\nThis is unzipped on the calling thread then plopped into a collector async.\nThere's a chance for us to drop instead of plop, if we know more about the queue. For example, an in-flight control could be made and if we have a fixed-length queue, we could drop and reply back with a try-later http code.\nThere's another issue which is that we are asking for byte[] as a parameter. This means that some layer before us is buffering, and before we even process a surge of writes could OOM things.\nI think the most important and easiest thing would be to add an in-flight limit either here or in the MySQL impl (notably elasticsearch and cassandra already have in-flight limit). Other ideas welcome, too. cc @dsyer @marcingrzejszczak I suspect there's a way to address this cheaply.. I think there's at least 2 paths\nCurrently ZipkinHttpCollector asks for buffered content like so..\njava\n  @RequestMapping(value = \"/api/v1/spans\", method = POST)\n  public ListenableFuture<ResponseEntity<?>> uploadSpansJson(\n      @RequestHeader(value = \"Content-Encoding\", required = false) String encoding,\n      @RequestBody byte[] body\n  )\nThis is unzipped on the calling thread then plopped into a collector async.\nThere's a chance for us to drop instead of plop, if we know more about the queue. For example, an in-flight control could be made and if we have a fixed-length queue, we could drop and reply back with a try-later http code.\nThere's another issue which is that we are asking for byte[] as a parameter. This means that some layer before us is buffering, and before we even process a surge of writes could OOM things.\nI think the most important and easiest thing would be to add an in-flight limit either here or in the MySQL impl (notably elasticsearch and cassandra already have in-flight limit). Other ideas welcome, too. @eirslett wonder if you still have a stash of some of it, or if it is possible to raise a partial migration (Ex the service/span name dropdown to react+dedux). I might be able to follow-the-leader from there (or at least try..). @rondevera any tips on migrating from flightjs to react?. @zeagord I think you've some work towards this. @zeagord I think you've some work towards this. there's now a UI Renovation project as folks are able to progress this. there's now a UI Renovation project as folks are able to progress this. cc @openzipkin/elasticsearch this is a pretty important bugfix, so will roll out immediately\nThanks to @sirtyro for noticing this!. also cc @semyonslepov. thx for the start! I think it doesn't need to be configurable at the moment. we have many things that aren't configurable.. thx for the start! I think it doesn't need to be configurable at the moment. we have many things that aren't configurable.. I rebased and squashed this for you. need to do another pass on review as not sure where we left off. ah ok.. well anyway when you are ready for it, make github line comments or\notherwise to explain tests removed or repurposed.\n. test order can bite us later perhaps. maybe we should expose a way to clean\nthe cache (package private access) and make sure it is invoked in a Before\nblock?\n. Do you have docker running on your laptop? These tests conditionally run if\ndocker (like docker machine or such) is installed\nOn 12 Jul 2017 3:49 pm, \"Semyon Slepov\" notifications@github.com wrote:\n\nStill don't understand why these ES integration tests are failing...\nExecuting the same thing with the same parameters on my notebook and it\npasses...\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1579#issuecomment-314684146,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61zgOyRT-J4bvrRSadVb3ZPKRQ1rIks5sNHp2gaJpZM4NMmrV\n.\n. Glad you got to the bottom of it and 100thx for putting in effort to fix it\non your own. Does this mean you think this is ready for a \"final review\"?\n\nOn 18 Jul 2017 2:52 am, \"Semyon Slepov\" notifications@github.com wrote:\n\nYeah, it was docker issue because I ran tests on my MacBook and it's a bit\ntricky sometimes to run Docker properly there.\nEnabled Travis for my forked repo and finally fixed the cause.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1579#issuecomment-315847285,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD617d0crUNLSK4H0IkP-cGjVrcleu_ks5sO61xgaJpZM4NMmrV\n.\n. all things in quotes aren't final :) but cool.. will do a pass in a bit\n. gonna close this as we should re-evaluate with the new single-type index. If needed, we can re-open. Thanks Gary. Of course this change is welcome for anyone I dont mind\nhogging it ;). Thanks Gary. Of course this change is welcome for anyone I dont mind\nhogging it ;). ok this actually breaks for everything except mysql and mem. ok this actually breaks for everything except mysql and mem. in cassandra, this seems to be related to the server not setting timestamp (which it shouldn't). in cassandra, this seems to be related to the server not setting timestamp (which it shouldn't). nice find.. so far seems legit. I'll probably make sure this works for all or is understood why not.. nice find.. so far seems legit. I'll probably make sure this works for all or is understood why not.. @garyd203 @sirtyro I added a commit to fix it. @garyd203 @sirtyro I added a commit to fix it. cc @kellabyte @basvanbeek @fedj @garyd203 @sirtyro @semyonslepov . cc @kellabyte @basvanbeek @fedj @garyd203 @sirtyro @semyonslepov . merging based on thumbs present!. merging based on thumbs present!. going out as 1.24. going out as 1.24. can you recommend if this is equally valid to non-json values with newlines?. Ok can you update the test class?\n\nOn 5 May 2017 11:12 pm, \"dmitry-prokopchenkov\" notifications@github.com\nwrote:\n\nYes, this fix works for any texts.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1583#issuecomment-299491881,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD611jmeXUwM42CZqvKi_Kj6-Zx07lIks5r2zxogaJpZM4NR8OH\n.\n. ps tests are in ./zipkin-ui/test/component_ui/spanPanel.test.js\n\nthey aren't tough to write.. wanna give it a shot?. cherry-picked after backfilling tests. cherry-picked after backfilling tests. Looks like you cannot access maven central. Are you behind a proxy? If so,\nyou can google for how to use proxies with Maven. Also, you do not have to\nbuild from scratch. You can use the docker image or download from bintray.\n. Thanks for getting this started!\nHere are some notes as most looks picture perfect!\n\ntoss the extra logback config in test sources. Add a test dep of a slf4j\nto log4j2 bridge if somehow missing\nif slf4j is the implied logging api of kafka, make a comment in the pom\nof why we are binding to it. Otherwise switch to whatever kafka10 uses and\nsimilarly make a comment\nyou have a commented out test. You should be able to test this by\nsneakily using package access to the kafkaConsumers field. Try it!\nmove/rework the kafka properties description from zipkin-server to the\nautoconfig README. you can look at zipkin-sparkstreaming or zipkin-aws for\nexampkes.. mainly we should link in nice bold in the zipkin-server readme\nto the right place. (This is different than othe4s but a good time to break\nfrom sprawling)\n\nAll above are small things.. good work!\n. >\n\nI made some changes to clean up logging for the unit tests, but I'm not\nsure if I went in the direction you intended.\nthis is fine. I forgot we hadn't switched tests in the main repo to log4j2,\nyet. my mistake!\n. great. do you want to try and get conditional loading working in the docker\nimage? There might be a chicken/egg problem as the docker image pulls\nrelease images..\n. Thanks. How do you feel about this PR btw. Anything left?\n\nOn 16 May 2017 1:49 pm, \"Dan Grabowski\" notifications@github.com wrote:\n\nPull request for the initial attempt a docker changes is now created.\nopenzipkin/docker-zipkin#142\nhttps://github.com/openzipkin/docker-zipkin/pull/142\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1586#issuecomment-301682169,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wy0xI5jfwpYtk7rUlePhgQKs0Fnks5r6Tj_gaJpZM4NUsrX\n.\n. Thanks. How do you feel about this PR btw. Anything left?\n\nOn 16 May 2017 1:49 pm, \"Dan Grabowski\" notifications@github.com wrote:\n\nPull request for the initial attempt a docker changes is now created.\nopenzipkin/docker-zipkin#142\nhttps://github.com/openzipkin/docker-zipkin/pull/142\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1586#issuecomment-301682169,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wy0xI5jfwpYtk7rUlePhgQKs0Fnks5r6Tj_gaJpZM4NUsrX\n.\n. >\nI think I'll need to replace the forEach() call here to maintain Java 7\ncompatibility.\ncool. I think one for loop is reasonable. If it had higher impact, I'd\nrecommend retrolambda\n. >\nI think I'll need to replace the forEach() call here to maintain Java 7\ncompatibility.\ncool. I think one for loop is reasonable. If it had higher impact, I'd\nrecommend retrolambda\n. >\nI ran out of steam before getting to this. If anybody feels strongly about\nusing autovalue here, let me know and I will circle back around to it.\nnot important!\n. >\nI ran out of steam before getting to this. If anybody feels strongly about\nusing autovalue here, let me know and I will circle back around to it.\nnot important!\n. I don't think anyone else is using your branch, so you can force push it.\nThe commits will be squashed on merge anyway.\n. I don't think anyone else is using your branch, so you can force push it.\nThe commits will be squashed on merge anyway.\n. ok will merge on green, which would then put it into the snapshot repo. ok will merge on green, which would then put it into the snapshot repo. Grand work, btw \ud83d\udc4d . Grand work, btw \ud83d\udc4d . With the type of UI skills I have, I can round up :) might be more work, even if not much do do other things it would require a bit more understanding. With the type of UI skills I have, I can round up :) might be more work, even if not much do do other things it would require a bit more understanding. just pinged about limit here as different topic https://github.com/openzipkin/zipkin/pull/443. just pinged about limit here as different topic https://github.com/openzipkin/zipkin/pull/443. @schlosna if you want a relatively straightforward one to help with, here 'tis!. @schlosna if you want a relatively straightforward one to help with, here 'tis!. you have a mismatch with the version of jooq. This is one of the troubles\nof making custom builds. You'll need to correct the versions to align\n. there might be a propagation bug in brave-cassandra, as this trace looks like it has spans for other traces inside..\n\n\nstarted via STORAGE_TYPE=cassandra3 SELF_TRACING_ENABLED=true java -jar ./zipkin-server/target/zipkin-server-*exec.jar cc @llinder . Oops.. yeah we should have a flag for this.\nPS This probably applies to any \"collector-only\" deployment, whether that's a standard build or something like stackdriver-zipkin or sleuth's stream server. oh.. you mean to say if the schema property has been overwritten, this\nerror is misleading?\n. good news is that only the health check is messed up :). https://github.com/openzipkin/zipkin/pull/1595. polished up thx to @shakuzen for the advice. Ok it must be the /api/v1/spans endpoint as that's the only thing that uses a label \"spanName\". still trying to repro.. trying this now..\nbash\n$ docker run -p 9200:9200 -e \"http.host=0.0.0.0\" -e \"transport.host=127.0.0.1\" docker.elastic.co/elasticsearch/elasticsearch:5.4.0\nbash\n$ ES_USERNAME=elastic ES_PASSWORD=changeme ES_HOSTS=http://192.168.99.100:9200 STORAGE_TYPE=elasticsearch java -jar ./zipkin-server/target/zipkin-server-*exec.jar. I'm having a hard time reproducing this, sorry to say. Even using the same version of zipkin once I load some traces, I can get data without a problem.. I don't get errors in ES, or zipkin or in the console.\nbash\n$ curl -s http://localhost:9411/api/v1/spans?serviceName=brave-webmvc-example\n[\"get\"]\nCan you help find a way to reproduce this so that I can try to fix this in a way it stays fixed? I don't want to guess if it can be avoided.. can you GET /info on the zipkin endpoint and / on the elastic one to\ntriple-check versions?\n. ok I will have to resort to fixing blind.. there must be a non-default\nconfiguration somewhere that applies and that I'm not getting.\nluckily the error is explanatory\n. ok looking closer.. these should not have the type \"text\" as the index template sets them to \"keyword\" (which is the documented remedy for this problem). I wonder why you aren't getting the normal index template. Are you using zipkin-server to write your spans (collect) or only to read them?. ok, so normal zipkin-server (the stock image) is writing the spans to ES?. can you grab /_template from elasticsearch?. ok zipkin isn't creating any index template.. are permissions locked down?. I would expect \"zipkin_template\" to be there, as the storage component creates that if it doesn't exist. (ps I am deleting your index template comments as we've covered this now) Are you able to join https://gitter.im/openzipkin/zipkin?. (ps I am deleting your index template comments as we've covered this now) Are you able to join https://gitter.im/openzipkin/zipkin?. one thing you can do in the mean time is to manually add the index template using curl. Ex. here's the one you should end up with. Once this is in, you can re-index your spans to fix the problem.\nbash\ncurl -s localhost:9200/_template/zipkin_template|jq .\n{\n  \"zipkin_template\": {\n    \"order\": 0,\n    \"template\": \"zipkin-*\",\n    \"settings\": {\n      \"index\": {\n        \"mapper\": {\n          \"dynamic\": \"false\"\n        },\n        \"requests\": {\n          \"cache\": {\n            \"enable\": \"true\"\n          }\n        },\n        \"analysis\": {\n          \"filter\": {\n            \"traceId_filter\": {\n              \"type\": \"pattern_capture\",\n              \"preserve_original\": \"true\",\n              \"patterns\": [\n                \"([0-9a-f]{1,16})$\"\n              ]\n            }\n          },\n          \"analyzer\": {\n            \"traceId_analyzer\": {\n              \"filter\": \"traceId_filter\",\n              \"type\": \"custom\",\n              \"tokenizer\": \"keyword\"\n            }\n          }\n        },\n        \"number_of_shards\": \"5\",\n        \"number_of_replicas\": \"1\"\n      }\n    },\n    \"mappings\": {\n      \"_default_\": {\n        \"_all\": {\n          \"enabled\": false\n        }\n      },\n      \"dependencylink\": {\n        \"enabled\": false\n      },\n      \"span\": {\n        \"properties\": {\n          \"traceId\": {\n            \"norms\": false,\n            \"ignore_above\": 256,\n            \"type\": \"keyword\"\n          },\n          \"duration\": {\n            \"type\": \"long\"\n          },\n          \"binaryAnnotations\": {\n            \"dynamic\": false,\n            \"type\": \"nested\",\n            \"properties\": {\n              \"endpoint\": {\n                \"dynamic\": false,\n                \"type\": \"object\",\n                \"properties\": {\n                  \"serviceName\": {\n                    \"norms\": false,\n                    \"ignore_above\": 256,\n                    \"type\": \"keyword\"\n                  }\n                }\n              },\n              \"value\": {\n                \"norms\": false,\n                \"ignore_above\": 256,\n                \"type\": \"keyword\"\n              },\n              \"key\": {\n                \"norms\": false,\n                \"ignore_above\": 256,\n                \"type\": \"keyword\"\n              }\n            }\n          },\n          \"timestamp_millis\": {\n            \"format\": \"epoch_millis\",\n            \"type\": \"date\"\n          },\n          \"name\": {\n            \"norms\": false,\n            \"ignore_above\": 256,\n            \"type\": \"keyword\"\n          },\n          \"annotations\": {\n            \"dynamic\": false,\n            \"type\": \"nested\",\n            \"properties\": {\n              \"endpoint\": {\n                \"dynamic\": false,\n                \"type\": \"object\",\n                \"properties\": {\n                  \"serviceName\": {\n                    \"norms\": false,\n                    \"ignore_above\": 256,\n                    \"type\": \"keyword\"\n                  }\n                }\n              },\n              \"value\": {\n                \"norms\": false,\n                \"ignore_above\": 256,\n                \"type\": \"keyword\"\n              }\n            }\n          }\n        }\n      },\n      \"servicespan\": {\n        \"properties\": {\n          \"serviceName\": {\n            \"norms\": false,\n            \"ignore_above\": 256,\n            \"type\": \"keyword\"\n          },\n          \"spanName\": {\n            \"norms\": false,\n            \"ignore_above\": 256,\n            \"type\": \"keyword\"\n          }\n        }\n      }\n    },\n    \"aliases\": {}\n  }\n}. one thing you can do in the mean time is to manually add the index template using curl. Ex. here's the one you should end up with. Once this is in, you can re-index your spans to fix the problem.\nbash\ncurl -s localhost:9200/_template/zipkin_template|jq .\n{\n  \"zipkin_template\": {\n    \"order\": 0,\n    \"template\": \"zipkin-*\",\n    \"settings\": {\n      \"index\": {\n        \"mapper\": {\n          \"dynamic\": \"false\"\n        },\n        \"requests\": {\n          \"cache\": {\n            \"enable\": \"true\"\n          }\n        },\n        \"analysis\": {\n          \"filter\": {\n            \"traceId_filter\": {\n              \"type\": \"pattern_capture\",\n              \"preserve_original\": \"true\",\n              \"patterns\": [\n                \"([0-9a-f]{1,16})$\"\n              ]\n            }\n          },\n          \"analyzer\": {\n            \"traceId_analyzer\": {\n              \"filter\": \"traceId_filter\",\n              \"type\": \"custom\",\n              \"tokenizer\": \"keyword\"\n            }\n          }\n        },\n        \"number_of_shards\": \"5\",\n        \"number_of_replicas\": \"1\"\n      }\n    },\n    \"mappings\": {\n      \"_default_\": {\n        \"_all\": {\n          \"enabled\": false\n        }\n      },\n      \"dependencylink\": {\n        \"enabled\": false\n      },\n      \"span\": {\n        \"properties\": {\n          \"traceId\": {\n            \"norms\": false,\n            \"ignore_above\": 256,\n            \"type\": \"keyword\"\n          },\n          \"duration\": {\n            \"type\": \"long\"\n          },\n          \"binaryAnnotations\": {\n            \"dynamic\": false,\n            \"type\": \"nested\",\n            \"properties\": {\n              \"endpoint\": {\n                \"dynamic\": false,\n                \"type\": \"object\",\n                \"properties\": {\n                  \"serviceName\": {\n                    \"norms\": false,\n                    \"ignore_above\": 256,\n                    \"type\": \"keyword\"\n                  }\n                }\n              },\n              \"value\": {\n                \"norms\": false,\n                \"ignore_above\": 256,\n                \"type\": \"keyword\"\n              },\n              \"key\": {\n                \"norms\": false,\n                \"ignore_above\": 256,\n                \"type\": \"keyword\"\n              }\n            }\n          },\n          \"timestamp_millis\": {\n            \"format\": \"epoch_millis\",\n            \"type\": \"date\"\n          },\n          \"name\": {\n            \"norms\": false,\n            \"ignore_above\": 256,\n            \"type\": \"keyword\"\n          },\n          \"annotations\": {\n            \"dynamic\": false,\n            \"type\": \"nested\",\n            \"properties\": {\n              \"endpoint\": {\n                \"dynamic\": false,\n                \"type\": \"object\",\n                \"properties\": {\n                  \"serviceName\": {\n                    \"norms\": false,\n                    \"ignore_above\": 256,\n                    \"type\": \"keyword\"\n                  }\n                }\n              },\n              \"value\": {\n                \"norms\": false,\n                \"ignore_above\": 256,\n                \"type\": \"keyword\"\n              }\n            }\n          }\n        }\n      },\n      \"servicespan\": {\n        \"properties\": {\n          \"serviceName\": {\n            \"norms\": false,\n            \"ignore_above\": 256,\n            \"type\": \"keyword\"\n          },\n          \"spanName\": {\n            \"norms\": false,\n            \"ignore_above\": 256,\n            \"type\": \"keyword\"\n          }\n        }\n      }\n    },\n    \"aliases\": {}\n  }\n}. #1870 covers discovery fyi. cc @openzipkin/elasticsearch @sirtyro I suspect y'all will find this super handy . cc @openzipkin/elasticsearch @sirtyro I suspect y'all will find this super handy . ps this was originally a desperate option to try and help debug #1594 glad it would be generally helpful!. ps this was originally a desperate option to try and help debug #1594 glad it would be generally helpful!. added env mapping will merge on green. added env mapping will merge on green. This issue was moved to openzipkin/zipkin-aws#45. moved to zipkin-aws. generally speaking we don't do something as involved as adding another storage option until there is more than one person interested. There's significant maintenance effort involved in storage layer vs simpler components such as collectors.. Zipkin collects metrics on span count and size (including message they came\nin)\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md#metrics\nThere's a work-in-progress for a dashboard on this\nhttps://github.com/openzipkin/docker-zipkin/pull/135\nYou can use these tools to measure your throughput, and possibly use them\nas feedback loops in a load testing tool.\n. looks like a change here:\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/java/zipkin/server/ZipkinServerConfiguration.java#L84\nwanna try (also adding a unit test)?\n. ping! weeks do get away from us, sending one nag just in case the next one doesn't :D. >\n\nI gave it a look, but I'm to unfamiliar with Java to get my head around\nthis.\nif unfamiliar, advice would be to box a little time to try, and speak up if\nyou hit a wall and would like help.\n. Oh sorry I read you wrong... you already gave it a look. No biggie. ping me\nif I don't get this in the next week.\n. https://github.com/openzipkin/zipkin/pull/1759 !. https://github.com/openzipkin/zipkin/pull/1759 !. https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/java/zipkin2/server/internal/ZipkinServerConfiguration.java\n\nOn Tue, Jan 8, 2019, 10:42 AM helloGit <notifications@github.com wrote:\n\nlooks like a change here:\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/java/zipkin/server/ZipkinServerConfiguration.java#L84\nwanna try (also adding a unit test)?\npage not found.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1601#issuecomment-452155353,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61_bpUxVYtRQNi6PXCkp0z0wAV0GCks5vBAWGgaJpZM4NqKqo\n.\n. https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/java/zipkin2/server/internal/ZipkinServerConfiguration.java\n\nOn Tue, Jan 8, 2019, 10:42 AM helloGit <notifications@github.com wrote:\n\nlooks like a change here:\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/java/zipkin/server/ZipkinServerConfiguration.java#L84\nwanna try (also adding a unit test)?\npage not found.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1601#issuecomment-452155353,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61_bpUxVYtRQNi6PXCkp0z0wAV0GCks5vBAWGgaJpZM4NqKqo\n.\n. ok, so would this be similar to the centralsync maven plugin, ex a decoupled artifact (solving a weird dependency loop of the build on a project it encloses.. so for repo name.. zipkin-layout-factory? or\nzipkin-spring-boot-loader-tools? (based on the implied jar)\n\ncc @shakuzen @llinder\n. https://github.com/openzipkin/zipkin-layout-factory\n\n. https://github.com/openzipkin/zipkin-layout-factory\n. Thanks @eirslett!. I will help merge this in. thanks for raising this!. Thanks for showing the bug, I merged your test in and https://github.com/openzipkin/zipkin/pull/1640 fixes it. ftr I recommended setting ES_HTTP_LOGGING=BODY next time this happens. That way we can see the error amazon sent with the 403. Sometimes that's telling. . What happens if you try one of those sql queries manually? Wondering if\nthere is something in this syntax aurora doesnt like.\n. edited issue title. once we get to the bottom of this, we can figure out whether to do detection (like we do with AWS elasticsearch vs normal elasticsearch) or if we can get by with just a small adjustment or some library compat stuff here.. This should be sorted in 1.26.1 which has corrected the mysql dialect setting in the client. If doesn't work, please re-open!. This should be sorted in 1.26.1 which has corrected the mysql dialect setting in the client. If doesn't work, please re-open!. might be worth making some sample traffic against this to ensure that collector metrics show up (they are probably the most important metric). ex run one of the example projects like https://github.com/openzipkin/pyramid_zipkin-example. >\nthey are probably the most important metric\nOh! Then I guess I should add them to the Grafana dashboard!\nyes I had a nag in the back of my head to nag you on the other issue. By\nnaming convention substitute http for kafka (sqs scribe) etc as not\neveryone sends spans via http transport.\n\nmessages, spans and bytes are interesting metrics, especially if\ncoordinated client-side. drop metrics are something that one would alert on\nat some point.\n. >\n\nAssumption: not a lot of people are currently using these metrics.\nProposal: let's drop as much custom logic as we can, and break the metric\nnames now, before publishing the dashboard using the metric names.\nI'd look at changelog for the directory and ping any implicated. After that\ngo for it\n\nOn Mon, Jun 12, 2017 at 5:40 PM, Zolt\u00e1n Nagy notifications@github.com\nwrote:\n\nImportant note, just realized: this change does actually break the\ncurrently used metric names. The Zipkin implementation trims the counter_\nand gauge_ prefixes (and sets the Prometheus metric type accordingly),\nwhile the upstream one does not trim those prefixes, and makes\neverything a gauge in Prometheus (see #1609 (comment)\nhttps://github.com/openzipkin/zipkin/pull/1609#issuecomment-307608094).\nAssumption: not a lot of people are currently using these metrics.\nProposal: let's drop as much custom logic as we can, and break the metric\nnames now, before publishing the dashboard using the metric names.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1609#issuecomment-307740186,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xuKt-aXNt6rHIwJ_eeNonZcx50Tks5sDQeZgaJpZM4N2F3H\n.\n. @klette nagtime!. gracias!. gracias!. the mystery of why CI is skipping license check is here: #1512. @swankjesse @jakewharton @brianm do y'all know of a friendly little library which lets you fan-out okhttp across multiple TLS connections such as you can with plain text connections by hacking okhttp3.Dns? I'm trying to not add many deps for our native Elasticsearch thing.. @swankjesse @jakewharton @brianm do y'all know of a friendly little library which lets you fan-out okhttp across multiple TLS connections such as you can with plain text connections by hacking okhttp3.Dns? I'm trying to not add many deps for our native Elasticsearch thing.. fan-out is a poor choice of terms, I think transparent round-robin and fail-over is more precisely what's needed here.. fan-out is a poor choice of terms, I think transparent round-robin and fail-over is more precisely what's needed here.. I guess what I mean is to have different TLS endpoints, like\nhttps://foo.com https://bar.com\nI suspect that the different TLS identities between foo.com and\nbar.com would prevent me from using the dns trick.\n. I guess what I mean is to have different TLS endpoints, like\nhttps://foo.com https://bar.com\nI suspect that the different TLS identities between foo.com and\nbar.com would prevent me from using the dns trick.\n. Wrote a failing test here #1682 cc @dgrabows who was most recently asking for this. > In the mean time, consider raising the log level of this trace from Debug to Error to help people understand what's wrong:\n\nyour comment was about the zipkin-reporter project, but I'll answer here. raising this to error will cause logs to fill in a high-throughput service. tracing shouldn't cause apps to crash which is why this is debug level. Other approaches are to monitor the dropped message count. > In the mean time, consider raising the log level of this trace from Debug to Error to help people understand what's wrong:\nyour comment was about the zipkin-reporter project, but I'll answer here. raising this to error will cause logs to fill in a high-throughput service. tracing shouldn't cause apps to crash which is why this is debug level. Other approaches are to monitor the dropped message count. satisfies the rule-of-three. merge time!. hmm can you be more specific on what error you are receiving?. hmm.. well indeed it is hard to promise forward compatibility when data formats change on a minor version!\n5.3 was working, maybe this is something new in 5.4? cc @openzipkin/elasticsearch . @xiaoxing598 how are you creating spans. can you create a failing test? (ex I can upgrade my ES to 5.4, but just in case it doesn't break). @xiaoxing598 I ran our whole test suite against ES 5.4.1 and also an ad-hoc test as well.\nHow are you creating spans? Do you have sample JSON that causes this?. I also added a unit test with newlines wondering if this was a cause, but it wasn't as it passed fine..\n```java\n @Test\n  public void dataIncludesNewlines() {\n    Span withNewlines = span1.toBuilder()\n      .addAnnotation(ann1.toBuilder().value(\"h\\nello\").build())\n      .addBinaryAnnotation(BinaryAnnotation.create(\"hello\", \"w\\norld\", ep)).build();\naccept(withNewlines);\n\nassertThat(store().getTraces(QueryRequest.builder().build()))\n  .flatExtracting(t -> t)\n  .contains(withNewlines);\n\n}\n``. probably easiest way is to setES_HTTP_LOGGING=BODY` to get the json. >\n\nI see this frame on the official website, which is your recommended PHP\nframework, so I used it.\nphpkin is under the community support section, and it is very new. The\nauthor added it to the list, though there are other libraries, too\nhttps://github.com/openzipkin/zipkin/issues/1330\n\nRegardless, if you capture the http body we can tell for sure what is wrong.\n. hah.. so the tracer is sending json as a span name! that's probably what\nthe issue is. That's a bug in the tracer or how it is used. span names\nshould be low cardinality such as \"event-list\", not a json body.\n. I think to make this easier to troubleshoot, we should probably escape\njson in span names (in ES) for now. Otherwise it is hard for people to\ntroubleshoot. anyone think otherwise?\n. so turns out that this isn't a problem specific to an ES version, as plopping in this integration test also breaks ES 2.x\n```java\n  @Test\n  public void spanNameIsJson() {\n    String json = \"{\\\"foo\\\":\\\"bar\\\"}\";\n    Span withJsonSpanName = span1.toBuilder().name(json).build();\naccept(withJsonSpanName);\n\nassertThat(store().getTraces(QueryRequest.builder().spanName(json).build()))\n  .flatExtracting(t -> t)\n  .contains(withJsonSpanName);\n\n}\n```. it doesn't break on cassandra or mysql, so I think we can localize a fix. https://github.com/openzipkin/zipkin/pull/1616 fixes this. here's a follow-up on the overall problem of span data https://github.com/openzipkin/zipkin/issues/1617. Thanks for finally raising this! I'd like to see a guage of sites\ninterested and also those who have rabbit experience or otherwise could\nhelp maintain this.\nOn data format, it is best to go with what's supported on the others imho.\nSerialized json or thrift lists. This allows the plumbing to be swappable.\nAnecdotally, I've heard one reason for popularity is cloud foundry\ndeployments which usually or always have rabbit.\nAnother thing to note here is the version range if any to indicate support\nof.\nFinally, and more of a note, spring-cloud-sleuth stream collector indeed\nsupports rabbit. However, the data format is sleuth-specific which would\ninterfere especially with non java apps who want to share the transport (as\nthere is no thrift or schema for that and also it is bespoke)\n. so.. folks interested, please click thumbs up or otherwise note interest! Pretty sure there is interest, but we'd like to make sure this is easy to see (as we can ping you when any feature comes out). I'm interested in this because very often folks pasting problems with classpath are those trying to workaround lack of RabbitMQ support and instead using Spring Cloud Stream. Here's a recent example, but not the only one: https://stackoverflow.com/questions/44997050/sleuth-zipkins-with-elasticsearch\nHaving RabbitMQ out of the box can help avoid using our time for undifferentiated maven/eclipse support and focus it on things about zipkin instead.. you could do it that way, which is more an adapter model. A more direct way\nwould be to make a java component which reads off the queue and writes to\nstorage (similar to SQS in zipkin-aws project). On the sender side, it\ndepends on the language, but if java for example a part of the\nzipkin-reporter project. The spans themselves should be encoded the same\nway as http body. Regardless, I think we'd need to settle any minimum\nversion of RMQ.\n. >\n\nThanks for the explanation and pointer to zipkin-aws. In my case the\nservices are non-Java (Python, golang, PHP).\ngotcha, so for now, you can indeed write an adapter and write the py, go,\nphp producers to send in zipkin's http format (json or thrift). At some\npoint, some server will directly implement rabbit obviating your RMQ ->\nHTTP or otherwise adapter.\n\nThe encoding is here (don't assume \"legacy encoding\" as that isn't\nnecessary for new work)\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-collector/kafka10#encoding-spans-into-kafka-messages\nNote we will soon have a simpler encoding, too, which is one reason I've\nnot pushed on this work as yet.\nhttps://github.com/openzipkin/zipkin/issues/1644\n. encoding prep work is done. We need to pick a dependency light or free rabbitmq java driver (ex one that doesn't pin to a spring version), and also a version of rabbit MQ to test against.. zipkin-server 2.1 supports rabbitmq \ntracers who want to support this will need to add a transport/reporter which produces messages (by default to the queue named \"zipkin\") which are encoded lists (ex a json list of v1 or v2 span format). Sounds reasonable. cc @sityro\n. sorry meant to cc @sirtyro. Looks good. On the env part, spring boot already avails upper underscore\nenv for correspinding properties. Here and elsewhere we are just making\nshorter ones available. At any rate, I think this is good. Thanks!\n. thank you!. here's a follow-up on the overall problem of span data https://github.com/openzipkin/zipkin/issues/1617. gonna merge and see if this helps with aws aurora. gonna merge and see if this helps with aws aurora. In between now and then you should be able to configure go to use the same\nspan ID. Iirc it is something like ClienServerSameId or similary named (in\nzipkin-go-opentracing)\n. started on this in #1663. if you enable debug, you'll see some validation warnings like so:\nskipping redundant root span: traceId=0000000000000001, rootSpanId=0000000000000001, spanId=ffffffffffffffff\nskipping clock skew adjustment due to data errors: traceId=0000000000000001\nMore work to do and pull requests welcome. Custom fields could be added using elasticsearch 5+ pipeline feature\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/pipeline.html\nSorry I dont have an example but this is what you can try\n. Custom fields could be added using elasticsearch 5+ pipeline feature\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/pipeline.html\nSorry I dont have an example but this is what you can try\n. no there isn't any supported way in 2.4\n. no there isn't any supported way in 2.4\n. Set the version of io.zipkin.java:zipkin\nI will raise something to update spring cloud sleuth's deps which might be\nsetting an earlier version (guess)\n. Set the version of io.zipkin.java:zipkin\nI will raise something to update spring cloud sleuth's deps which might be\nsetting an earlier version (guess)\n. seems handy, but would need a test and updates on the config docs. Also, we should make sure that this is something others agree with as I don't know the support implications of always using ConsumerRebalanceListener.\nSorry we don't have a kafka mailer at the moment. pinging @dgrabows as he touched this recently. seems handy, but would need a test and updates on the config docs. Also, we should make sure that this is something others agree with as I don't know the support implications of always using ConsumerRebalanceListener.\nSorry we don't have a kafka mailer at the moment. pinging @dgrabows as he touched this recently. ok great. @danielkwinsor mind adding a test? (can be as simple as checking that split worked)\nlemme know if you need help with this. looks like you might be able to write a simple test that uses package access to check kafkaConsumer.subscription()\nbasically read-back that to show a simple split works. wanna try?. k very least this needs an update to the docs KafkaCollector.Builder and in zipkin-server/README like..\n* `KAFKA_TOPIC`: Topic zipkin spans will be consumed from. Defaults to \"zipkin\"\n      * When Kafka 0.10 is in use, multiple topics may be specified if comma delimited. All good or did I scare you off? :)\n. Thanks very much @danielkwinsor! (and thanks for the timeboxed attempt on tests.. I'll copy you on a test commit). @eirslett @basvanbeek know anyone w/xss mitigation experience?. @eirslett @basvanbeek know anyone w/xss mitigation experience?. Good points. Responses inline!\nI would imagine you can attack it in two ways. When you receive data, you\nsanitise it.\nThis is one use case of zipkin-sparkstreaming, though people making custom\nbuilds of zipkin server can likely intercept the Collector or Storage\ncomponent to do the same.\nand when you render it, you should be able to ask your framework to\nsanitise the data. What's the UI framework you're using\nWe are using flightjs and crossroads\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-ui/package.json\nThere is not yet started renovation idea towards react redux\nhttps://github.com/openzipkin/zipkin/issues/1577#issuecomment-297876694\n. Good points. Responses inline!\nI would imagine you can attack it in two ways. When you receive data, you\nsanitise it.\nThis is one use case of zipkin-sparkstreaming, though people making custom\nbuilds of zipkin server can likely intercept the Collector or Storage\ncomponent to do the same.\nand when you render it, you should be able to ask your framework to\nsanitise the data. What's the UI framework you're using\nWe are using flightjs and crossroads\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-ui/package.json\nThere is not yet started renovation idea towards react redux\nhttps://github.com/openzipkin/zipkin/issues/1577#issuecomment-297876694\n. fwiw here's a span with the trace data in it, wrapped in an array for easy posting to zipkin.\nbash\ncurl -s localhost:9411/api/v1/spans -H'Content-Type: application/json' -d '[{\n  \"traceId\": \"6add063375594175\",\n  \"id\": \"6add063375594175\",\n  \"name\": \"\",\n  \"timestamp\": 1498043010965929,\n  \"duration\": 6650,\n  \"annotations\": [\n    {\n      \"timestamp\": 1498043010971277,\n      \"value\": \"<script>alert(1)</script>\",\n      \"endpoint\": {\n        \"serviceName\": \"unknown\",\n        \"ipv4\": \"192.168.182.54\"\n      }\n    },\n    {\n      \"timestamp\": 1498043010971480,\n      \"value\": \"Annotation 2</td><script>alert(2)</script><td>\",\n      \"endpoint\": {\n        \"serviceName\": \"unknown\",\n        \"ipv4\": \"192.168.182.54\"\n      }\n    },\n    {\n      \"timestamp\": 1498043010971489,\n      \"value\": \"<img src=x onerror=alert(3) />\",\n      \"endpoint\": {\n        \"serviceName\": \"unknown\",\n        \"ipv4\": \"192.168.182.54\"\n      }\n    }\n  ],\n  \"binaryAnnotations\": [\n    {\n      \"key\": \"<img src=x onerror=alert(4) />\",\n      \"value\": \"\\\"quotes are great two\\\"\",\n      \"endpoint\": {\n        \"serviceName\": \"unknown\",\n        \"ipv4\": \"192.168.182.54\"\n      }\n    },\n    {\n      \"key\": \"Tag key 1</td><script>alert(tag key 5)</script><td>\",\n      \"value\": \"<script>alert(\\\"tag value 1\\\")</script>\",\n      \"endpoint\": {\n        \"serviceName\": \"unknown\",\n        \"ipv4\": \"192.168.182.54\"\n      }\n    },\n    {\n      \"key\": \"Tag key 2</td><!--\",\n      \"value\": \"\\\"quotes are great two\\\"\",\n      \"endpoint\": {\n        \"serviceName\": \"unknown\",\n        \"ipv4\": \"192.168.182.54\"\n      }\n    }\n  ]\n}]'. I'm looking into this. thanks @eirslett for the headstart!. Thanks for this. umm we currently only use this in tests, so we can go with as-is. going out as 1.26.2. ok there was one miss which was about binary annotation key. no more popups\n\n. @sirtyro can you give the OK on this? if works, I'll monkey in some tests. ayup!\n. fixed tests and made sure json literals still work. Thanks for raising. will take a look.\nIs it possible for you to update your ELB to forward the path / to /zipkin?\nhttp://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#listener-rules\n. failing test https://github.com/openzipkin/zipkin/pull/1723\nOn Sat, Sep 2, 2017 at 9:14 AM, Adrian Cole adrian.f.cole@gmail.com wrote:\n\nThanks for raising. will take a look.\nIs it possible for you to update your ELB to forward the path / to /zipkin?\nhttp://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#listener-rules\n. @naoman next release will be default, but anyway you can set --server.use-forward-headers=true in the mean time, if you can't change the ELB config. I'm guessing by the fact that influx folks integrated with jaeger that nothing is planned here by them. Happy to be wrong. \n\nIf community members are interested in moving this forward, pull requests welcome. The modeling job is much easier here as we have a simpler v2 json format which doesn't have the complexity present in the v1 model as baked into the telegraph plugin.. @goller ok cool. Yeah was just weird to see interest drummed up here then a blog post on zipkin showing how to use jaeger. . >\n\nThat's why we are using both. It probably creates a bit of confusion and\nwe are sorry about this. But you know how it works :D At some point\neverything will be ready.\nyeah in hindsight if it were a resource problem, probably better to ask for\nhelp because I've helped with others for example. For example, I suspect\nthe schema of \"zipkin\" in your telegraph plugin is now v1 format, which is\nunfortunate as it is likely more complex than needed and would introduce\ncomplexity to migrate if ever to v2. If in hindsight asked for help, I\ncould have helped you with the java part so that the schema could be\nsimple. Also, we wouldn't have the awkward situation where a team who\nintentionally compete with zipkin's community (jaeger) are promoted with\n\"zipkin\" blog posts.\nBtw as you said if there is some java dev happy to help here let us know!\nIdeally, if zipkin could own the format of the zipkin plugin that could\nmake things right. Personally I spent a good deal of effort in attempts to\nroll it out in support  of your plugin before you guys dropped off the face\nof the earth :) Are you up to at least changing the schema to v2? I would\nhelp with the java side if so.\n. >\n@adriancole https://github.com/adriancole Telegraf is modular, we can\nwrite a new plugin called zipkin2 at some point. This is not a problem at\nall. We wrote the plugin because we were looking to build a data flow to\ntest InfluxDB with traces and for us was more comfortable to write a\ntelegraf plugin because we know the code better.\nNot sure how things work, but for example if the schema is the same between\nthe both, sounds good. It would be a pain if you'd have to read two\ndifferent formats from influx. Less a pain if the telegraf converted\nzipkin1 format to zipkin2 on the way in (so that the storage is coherent).\nWe spoke internally about this and I agree with you, we created a small\nchaos but only because we are really engaged with tracing. I am sorry about\nthat.\nappreciate you saying this.\nWhat I am trying to say is that Telegraf is not related to this issue,\nwhat I would like to have is influxdb as backend in zipkin. Do you think we\nshould re-start the integration with zipkinv2? People that are using zipkin\nnow will be able to use the influxdb backend in a easy way (just updating\nzipkin and configuring it properly) ? Or the migration path from zipkin1 to\nzipkin2 is more complicated?\nthe main thing is that the schema used in telegraf is v2 (it is simpler to\ndesign it this way and simpler for long-term). For example, we convert v1\nto v2 format so that queries on data are always in v2. User apps don't need\nto change. So basically, yeah I'd restart the effort in v2 format. Thanks\nfor asking!\n. >\nOk, thank you for your clarification. At this point, we can speak\ninternally about how to proceed in order to open a PR here with the new\ninfluxdb backend for zipkin2.\nsounds good! ps https://github.com/openzipkin/zipkin-go will very soon have\na zipkin2 model in go. You might want to watch the repo or #1778 (most\nlikely a go host agent) in case any code there becomes helpful to telegraf\nin the future.\n. PS please do add me if you make a repo, I can help review etc\n. just tweeted in efforts to call attention to the offer for help on this. looking forward to what comes of it.. Thanks for giving a swing and impressive getting all the tests to pass.\n\nDid a quick look and have the following thoughts\n\nis it possible to not use XML aka check in XML files? I suspect java\nconfig will be less indirect for the storage component, and also reduce\nconfusion when binding to property based configuration in spring. Can we\nuse java config to supply defaults instead of XML?\nI'd advise against doing merge logic on consumption (implying\nread-backs). Typically we merge at query time, and in doing so\nfunctionality such as \"raw trace\" (what instrumentation sent) is retained.\nThis also allows certain things like timestamps to remain authoritative vs\nguessing.\nOn timestamps, I'd suggest adding things with an ingest timestamp if one\nis missing from the span (but leaving the span in-tact). You'll notice\nother storage stuff walking around accidentally setting an improper\ntimestamp on a span.\n. Regardless, it is probably best to start with a separate repo as there's still no user interest noted here despite tweeting, mentioning this in release notes, etc. We have to conserve energy in this repo for diversely needed change.\n\nI'd suggest making that repo integrated as a library with dependencies vs forking this repo. If you want a special server to make testing easier in the short term, you could consider making something similar to stackdriver-zipkin. Otherwise, you can do a pure library integration like done in zipkin-aws or zipkin-azure.\nThis repo should be in your account or org; in github it is easy to move repos when the time comes to consider such. Once there, you can feel free to add me or tommy who can check out things as we can.. The default is intentional (not java 8) for a couple reasons, some are just\nbackground.\nWe dont want to enforce more constraints than the libraries we use. Ex jooq\nrequires JRE8, so no problem. Though cassandra does not. Not escalating\ncode level prevents accidents that limit library reuse.\nSome are using \"local reporters\" for example reporting to zipkin without a\nseparate transport tier. Taking the least dep on JRE level interferes least.\nAnother non server user is the dependency linker spark job. As recent as\nlast year the job needed to be pinned to JRE 7 due to some hadoop reasons\nwhich was another reason why we had to take a second look at versions here.\nThe core library is not only used in spark, but also application code ...\nand also java agents. Java agents are used to instrument legacy systems and\nsometimes terribly so, but sometimes these use JRE6. Moreover, it was\nintended to be used in android and depending on level this is kindof like\njava 6.5. Long story short.. the core library is most important to remain\nJRE 6.\nThere are some annoyances, such as copy pasting long compare and lack of\nthings like threadlocalrandom, but in core, this is a limited impact. The\nanimal sniffer did its job in this case, preventing us from breaking our\ncontract.\n. There's a longer explanation held hostage until my phone reconnects to the internet :) suffice to say this is very intentional, especially the core version at JRE 6. Feel free to ping on gitter if you have any q's about approaches considering the limitation. thanks for fixing these broken windows!. Hmm wonder why it is silently failing (as opposed to loudly failing).\n. thanks!. I suspect there is no value in a list here. I will switch and if tests\nstill pass, merge it as a set. Nice catch.\nOn 30 Jun 2017 5:10 pm, \"joel-airspring\" notifications@github.com wrote:\n\n@joel-airspring commented on this pull request.\nI like this refactor, and will base my changes on them.\nQuick question: What's the use case for this being a list:\n\nprivate final SortedMultimap> traceIdToTraceIdTimeStamps =\nnew LinkedListSortedMultimap<>(Long::compareTo);\n\ninstead of a LinkedHashSetMultiMap?\nAre you anticipating cases where you want to know that\naaaa --> [, , ]\nas opposed to just\naaaa --> (, )\n(Notice that the first example can and does contain duplicates because\nit's a linked list.)\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1636#pullrequestreview-47417599,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wiVQlEOGGm-rEzMFr3tG4IJYtovks5sJQ_TgaJpZM4OHv77\n.\n. @joel-airspring keen eye: LinkedListSortedMultimap would not have helped before or after this change. oops killing linkedlist broke a test :) It supported reading back raw spans (which helps show when instrumentation sent the same spans twice). >\nBummer! I had run the zipkin/zipkin tests, but not those\nzipkin/zipkin-junit tests that failed. Thanks for checking. I'll keep\nrolling with the existing linked list implementation.\nNote: I moved the linkedlist to the primary data source (as opposed to the\nindex where it was in the last diff you saw)\n. can you paste before and after of the entire trace detail? Ex does removing this end up affecting hiding spans that start early?. tell you what... I'm going to take a chance and merge this. If it was a bad choice, we will hear about it  and revert. Thanks!. thanks!. thx again!. ty again!. thanks so much for this.. NaN is not a great experience.. Hi, ted.\n\nThanks for the updates. This sounds like great progress\nOne thing quite important is having multiple folks on a project as we have\nbeen bit in the past. There are abandoned or barely breathing works and\nthese confuse and annoy people. Diversity is missing in these scenarios.\nFor example eventhough criteo/zipkin4net is behind one of the largest\nzipkin sites (2.5M req/second on burst for a service), and also well\nmaintained, we are waiting for more buy in before moving the project here.\nCan you do me a favor and ping the authors of the original zipkin c++driver\nabout the repo you have? That might raise attention. I will look at it,\ntoo, and see if I can find others. Maybe flier will reply back.\nIf we have a couple contributors who dont work at the same place on board\nto help, then seems a good idea to consider.\nHope it makes sense, but if not ping back! Regardless, good work.\n. notified folks:\nhttps://groups.google.com/d/msg/zipkin-user/IZr450uTISA/TjhUgsykAQAJ\nhttps://twitter.com/zipkinproject/status/882140150632808448. here's the first work on the java binding and codec https://github.com/openzipkin/zipkin/pull/1651. Update: all conversion tests work. I'm going to do a POC on elasticsearch to make sure it works. Once it does, I'll chop up #1651 into several smaller works. Update: all conversion tests work. I'm going to do a POC on elasticsearch to make sure it works. Once it does, I'll chop up #1651 into several smaller works. @xeraa quick question. Is there an option besides spark to do a data migration?\nEx. I'd like to walk through documents grouped by traceID then write them back modified to a different index.. @xeraa quick question. Is there an option besides spark to do a data migration?\nEx. I'd like to walk through documents grouped by traceID then write them back modified to a different index.. thx for the help @xeraa . thx for the help @xeraa . https://github.com/openzipkin/zipkin/pull/1700 makes the write api span2 native. I'll make a later pull request for the read side. Notably, I'm not going to carry over the merging/clock skew adjustment logic currently copy-pasted into every impl. This simplifies the storage contract which is raw by default now. The merging/clock-skew can be done in an api decorator and/or javascript.. #1705 hones the way we'll address callbacks (ex span consumption) and synchronous calls (ex span name lists). This will reduce the effort in implementing v2 interfaces. draft of the v2 java read api https://github.com/openzipkin/zipkin/pull/1709. openzipkin/zipkin#1711 << elasticsearch uses spanstore2 api\nopenzipkin/zipkin#1710 << expose spanstore2 http api. https://github.com/openzipkin/zipkin/pull/1726 << finishes the v2 model (and codec ops required). This doesn't make a v2 \"storage component\" type, yet, as this can be done later. https://github.com/openzipkin/zipkin/pull/1729 << adds the v2 storage component. https://github.com/openzipkin/zipkin-api/pull/47 starts on the proto3 encoding. think we're all done. cc @joel-airspring so this fixes my curl-in-a-for loop. Mind checking before we cut a release?. here's a quick test case:\n```bash\nrun with self-tracing enabled, and max-spans set lower\n$ SELF_TRACING_ENABLED=true java -Dzipkin.storage.mem.max-spans=500 -jar ./zipkin-server/target/zipkin-server-*exec.jar\nin another window, do this for a while\n$ while true; do curl -s localhost:9411/api/v1/services;done\nthen, check to see the count is less than or equal to what you set it to (default is really high)\n$ curl -s localhost:9411/api/v1/traces?limit=1000000|jq '.[]|.[]|.id'|wc -l\n```. Thanks for testing! Will release your work shortly. Sounds possible to remedy, but it also is easy to miss a detail.\nTo make sure we are clear, can you post a simplified json with a missing\nspan and the links you expect and what heuristic you would use to decide\nhow to create the pseudo link? Even better a pull request.\nOn 8 Jul 2017 07:11, \"jprateekvmware\" notifications@github.com wrote:\n\nSo in that case we need to have a fix in zipkin dependencies, as it\nshowing traces from M1 -->M3, when some of the M2 traces have got dropped ,\nit should take into account the parent span id of M3 and whether it exist\nin system , before defining the dependencies, else it gets a wrong picture\nprojected\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1648#issuecomment-313813507,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wEmTCL4eDbFSlT96BnOn-uLTSdWks5sLrsIgaJpZM4ORhDs\n.\n. please don't re-open the issue there.. the tests are here, and likely any code around it will be, too.. @jprateekvmware I've changed the title of this to \"Skip dependency links for missing intermediates\" as that's the best I can understand your scenario so far.\n\nCan you explain why you are creating child spans in an intermediary, but not reporting them to Zipkin. It is one thing for this to be an exception, but it is certainly unexpected to do routinely. Help me understand?. ps you can also hop on gitter to chat about this, too https://gitter.im/openzipkin/zipkin. Viewing JSON will reduce the round-trips on this issue (and spam to folks subscribed), please attach a pasted link to json of a trace that matches this scenario before proceeding further.. @jprateekvmware There are a large number of data errors in your trace. Make sure you are using latest versions of whatever instrumentation you are using, then try again. I've parsed the easy to spot problems below:\nspan 94aa5c0443cc5582: there's an instrumentation bug as a span should have at most one core annotation (core annotation is cs, sr, ss, cr). Yours has 2 servers responding to a single client.\nspan b963390959854a97 b49392870c173a9f: these are client spans, but have their endpoint (the one on cs, cr) marked as the same as its remote (the one on sa). This seems like a bug unless they are supposed to be calling themselves? They are also missing http tags, which are placed on their parents. more on that below:\nspan fe70d9705ea17279 f5f9dcc8edb20ae7: these look a bit distracting as they have http tags (binary annotations), yet are marked local. These will be treated as process local spans, but they seem to be half-the-data of their children.. OK please get back once things are upgraded as sleuth fixes a lot of\nthings. By not troubleshooting until you have things up to date, it also\nreduces the amount of times folks following this repo are spammed with\ncomments.\n. thanks @a86c6f7964 for reporting cc @shakuzen . added a commit for thrift doing the same. will later run benchmarks there. the \"readTen\" benchmarks are important to show optimizations when decoding lists which nowadays are often much larger than 10 spans. For example, most new tracers batch and can send quite a lot of spans per message (easily hundreds in some scenarios). added a \"shared\" field to help with status quo where libraries on the client and server side usually share a span ID. https://github.com/openzipkin/zipkin/issues/1499#issuecomment-315293307\nMade one of the converters, but lacking tests at the moment. Once there's 2-way conversion, I'll open another PR trying out ES or similar. lack of tests was driving me bonkers. Backfilled tests so that next change (which includes conversion code) is more sane. I've a working span converter, but still needs a lot more tests as there are many ways data can come in (and come in unexpectedly). I've a working span converter, but still needs a lot more tests as there are many ways data can come in (and come in unexpectedly). to move this forward, I'm first temporarily converting the internals of InMemorySpanStore to use SimpleSpan (and the converter). This will help break tests and show bugs before trying in elasticsearch.. to move this forward, I'm first temporarily converting the internals of InMemorySpanStore to use SimpleSpan (and the converter). This will help break tests and show bugs before trying in elasticsearch.. ok have almost all storage tests working.. should finish it tomorrow and move to ES. ok have almost all storage tests working.. should finish it tomorrow and move to ES. update: all spanstore tests work, but some dependency linker tests don't probably missing some edge cases. update: all spanstore tests work, but some dependency linker tests don't probably missing some edge cases. ok almost there on the data conversion. a couple more tests to go. ok almost there on the data conversion. a couple more tests to go. all tests are good with local span store.\nI moved onto @openzipkin/elasticsearch, but ran into a glitch and need feedback on the idea or reverting to span.timestamp/duration (so that duration query is effortless) https://github.com/openzipkin/zipkin/issues/1499#issuecomment-316925849. all tests are good with local span store.\nI moved onto @openzipkin/elasticsearch, but ran into a glitch and need feedback on the idea or reverting to span.timestamp/duration (so that duration query is effortless) https://github.com/openzipkin/zipkin/issues/1499#issuecomment-316925849. tentatively changed from startTimestamp/finishTimestamp to timestamp/duration to see if elasticsearch becomes easier (almost certainly will). @openzipkin/elasticsearch I added a commit for elasticsearch which uses the new format internally (no change to the format collectors use). Can I get a guinea pig to test this out by building this branch and throwing traffic at it? Bear in mind it requires ES 2.4+. ps thanks @ImFlog and @anuraaga for the help getting ES to work. Would have been a lot harder without you!. FYI working on dependency link now. deferring ES as I expect that it might be effortful for folks to test a branch. dependency linking (killing DependencyLinkSpan for SimpleSpan) went well enough except one-way spans. I might have to re-jig the converters a bit to keep span store tests passing and also pass on dependency links. Glad I hit a snag as this stuff really needs to be solid.. cleanups all done and provably we won't need DependencyLinkSpan after this. We can use the same span type for input, storage, query and aggregation. To that end, I've raised a comment whether to rename SimpleSpan to zipkin2.Span.\nhttps://github.com/openzipkin/zipkin/issues/1499#issuecomment-318061873\nWill proceed in whichever direction following a couple people feeding back. Regardless, it will be chopping this PR into smaller pieces.. cleanups all done and provably we won't need DependencyLinkSpan after this. We can use the same span type for input, storage, query and aggregation. To that end, I've raised a comment whether to rename SimpleSpan to zipkin2.Span.\nhttps://github.com/openzipkin/zipkin/issues/1499#issuecomment-318061873\nWill proceed in whichever direction following a couple people feeding back. Regardless, it will be chopping this PR into smaller pieces.. Here's a thought about order\nFirst is adding the type SimpleSpan or v2 span whatever we decide and\nconverters.\nThen add the json serialisation code\nIn any order we add content type handler for post, kafka etc transports\nAlso in any order we add ElasticSearch support (storage format). We can add\nthis as optional flag especially as indexing needs to be looked at\ncarefully.\nAll of the above can be done pretty quickly once we make a decision on\nnaming, but I would probably aim for elasticsearch even before transports\nbecause there are a lot of wins there.\nOn 27 Jul 2017 5:53 am, \"Jordi Polo Carres\" notifications@github.com\nwrote:\n\nWhat's the timeline? First changes to internal storage then API or all\ntogether?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1651#issuecomment-318193923,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD617t5ul8HK71Svx5IWNWZmMTyuqtkks5sR7U8gaJpZM4OVQvn\n.\n. Here's a thought about order\n\nFirst is adding the type SimpleSpan or v2 span whatever we decide and\nconverters.\nThen add the json serialisation code\nIn any order we add content type handler for post, kafka etc transports\nAlso in any order we add ElasticSearch support (storage format). We can add\nthis as optional flag especially as indexing needs to be looked at\ncarefully.\nAll of the above can be done pretty quickly once we make a decision on\nnaming, but I would probably aim for elasticsearch even before transports\nbecause there are a lot of wins there.\nOn 27 Jul 2017 5:53 am, \"Jordi Polo Carres\" notifications@github.com\nwrote:\n\nWhat's the timeline? First changes to internal storage then API or all\ntogether?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1651#issuecomment-318193923,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD617t5ul8HK71Svx5IWNWZmMTyuqtkks5sR7U8gaJpZM4OVQvn\n.\n. So everything mentioned including ES internal in next release is a\nproposal. Next minor from that would improve or formalize that and add\ntransport (http kafka) support.\n\nOn 27 Jul 2017 8:42 am, \"Adrian Cole\" adrian.f.cole@gmail.com wrote:\n\nHere's a thought about order\nFirst is adding the type SimpleSpan or v2 span whatever we decide and\nconverters.\nThen add the json serialisation code\nIn any order we add content type handler for post, kafka etc transports\nAlso in any order we add ElasticSearch support (storage format). We can\nadd this as optional flag especially as indexing needs to be looked at\ncarefully.\nAll of the above can be done pretty quickly once we make a decision on\nnaming, but I would probably aim for elasticsearch even before transports\nbecause there are a lot of wins there.\nOn 27 Jul 2017 5:53 am, \"Jordi Polo Carres\" notifications@github.com\nwrote:\n\nWhat's the timeline? First changes to internal storage then API or all\ntogether?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1651#issuecomment-318193923,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD617t5ul8HK71Svx5IWNWZmMTyuqtkks5sR7U8gaJpZM4OVQvn\n.\n\n\n. So everything mentioned including ES internal in next release is a\nproposal. Next minor from that would improve or formalize that and add\ntransport (http kafka) support.\n\nOn 27 Jul 2017 8:42 am, \"Adrian Cole\" adrian.f.cole@gmail.com wrote:\n\nHere's a thought about order\nFirst is adding the type SimpleSpan or v2 span whatever we decide and\nconverters.\nThen add the json serialisation code\nIn any order we add content type handler for post, kafka etc transports\nAlso in any order we add ElasticSearch support (storage format). We can\nadd this as optional flag especially as indexing needs to be looked at\ncarefully.\nAll of the above can be done pretty quickly once we make a decision on\nnaming, but I would probably aim for elasticsearch even before transports\nbecause there are a lot of wins there.\nOn 27 Jul 2017 5:53 am, \"Jordi Polo Carres\" notifications@github.com\nwrote:\n\nWhat's the timeline? First changes to internal storage then API or all\ntogether?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1651#issuecomment-318193923,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD617t5ul8HK71Svx5IWNWZmMTyuqtkks5sR7U8gaJpZM4OVQvn\n.\n\n\n. @jcarres-mdsol I think I found a way to tease out a lot of the code safely.\n\nIf I start with swapping the internal DependencyLinkSpan for an internal copy of what's now SimpleSpan, we can get that code out there and make sure it works. Later will be a simple import change to formalize the type.. @jcarres-mdsol I think I found a way to tease out a lot of the code safely.\nIf I start with swapping the internal DependencyLinkSpan for an internal copy of what's now SimpleSpan, we can get that code out there and make sure it works. Later will be a simple import change to formalize the type.. first thing splitting off is the span model type (no codec or converters) #1669. first thing splitting off is the span model type (no codec or converters) #1669. All the rejigging PRs are green. I plan to merge the ones leading up to this (Elasticsearch spike) tomorrow. If folks are keen on the new span kinds for producer/consumer, I'll add a PR for that.\nOnce the above is done, I'll look at how hard it is to make this have an internal flag to use the \"span2\" type conditionally (right now, this branch won't read old data only new data). If it is straightforward enough, I'll push that out with a minor bump adding instructions here on how to enable it.\nIn the mean time, anyone can try this branch on their own. It writes to the same indexes, but using the \"span2\" type and no \"servicespan\" index.. All the rejigging PRs are green. I plan to merge the ones leading up to this (Elasticsearch spike) tomorrow. If folks are keen on the new span kinds for producer/consumer, I'll add a PR for that.\nOnce the above is done, I'll look at how hard it is to make this have an internal flag to use the \"span2\" type conditionally (right now, this branch won't read old data only new data). If it is straightforward enough, I'll push that out with a minor bump adding instructions here on how to enable it.\nIn the mean time, anyone can try this branch on their own. It writes to the same indexes, but using the \"span2\" type and no \"servicespan\" index.. heh.. something's up as github won't let me re-open this. heh.. something's up as github won't let me re-open this. carried forward in #1674. carried forward in #1674. Lazy question.. is it possible for hitch do redirect / -> /zipkin?\n. @shakuzen @devinsba do either of you know any spring boot/MVC built-ins to redirect scheme aware?. ps this might be helpful in figuring out what to do https://github.com/varnish/hitch/wiki/FAQ#varnish-request-coming-from-hitch-or-not. FYI, there's a proposal to add producer and consumer span kinds on the new zipkin model. thumbsup on that issue if you feel it should be the case, or reply back if you feel against it. https://github.com/openzipkin/zipkin/issues/1499#issuecomment-318230305. FYI, there's a proposal to add producer and consumer span kinds on the new zipkin model. thumbsup on that issue if you feel it should be the case, or reply back if you feel against it. https://github.com/openzipkin/zipkin/issues/1499#issuecomment-318230305. ## Proposal: add span.kind = PRODUCER or CONSUMER\nwhen PRODUCER:\n  span.timestamp = the time the user produced the message\n  span.duration = if present, the delay between the user producing the message and it actually being sent\nwhen CONSUMER\n  span.timestamp = the time the consumer received the message\n  span.duration = if present, the processing delay (ex decoding it etc)\nIn Span2 model (#1499), this is straightforward. However, how we map these to annotations is really important especially as they can be used in single-host mode. Here's the idea:\nAllocating PRODUCER or CONSUMER to annotations\nDefine \"ms\" and \"mr\", as discussed in #1243. Notably, these annotations are on separate endpoints, the producer and consumer respectively. Here's how to allocate span.kind = PRODUCER or CONSUMER to the span v1 model:\n\nIf PRODUCER\nif duration is unset or <1us, span.timestamp -> \"ms\"\n\nif duration is >=1us,  span.timestamp -> \"ms\", span.timestamp+duration -> \"ws\"\n\n\nIf CONSUMER\n\nif duration is unset or <1us, span.timestamp -> \"mr\"\nif duration is >=1us,  span.timestamp -> \"wr\", span.timestamp+duration -> \"mr\"\n\nMost importantly, unset span.timestamp and duration if allocating to annotations.\nExamples:\nIn SPSC, the timeline is OK in both single-host spans, and potentially shared spans. in SPMC shared spans don't work anyway (each consumer needs to fork a span).\nSender and receiver have no queuing delay or knowledge of it.\nspans: [kind = PRODUCER, timestamp = 1L, service=sender], [kind = CONSUMER, timestamp = 3L, service=receiver]\nannotations: [(1L, ms, sender), (3L, mr, receiver)]\nReceiver has some processing delay\nspans: [kind = PRODUCER, timestamp = 1L, service=sender], [kind = CONSUMER, timestamp = 2L, duration=1L, service=receiver]\nannotations: [(1L, ms, sender), (2L, wr, receiver), (3L, mr, receiver)]\nSender has some queuing delay\nspans: [kind = PRODUCER, timestamp = 1L, duration=1, service=sender], [kind = CONSUMER, timestamp = 3L, service=receiver]\nannotations: [(1L, ms, sender), (2L, ws, sender), (3L, mr, receiver)]\nBoth sides are delayed\nspans: [kind = PRODUCER, timestamp = 1L, duration=1, service=sender], [kind = CONSUMER, timestamp = 3L, duration = 1L, service=receiver]\nannotations: [(1L, ms, sender), (2L, ws, sender), (3L, wr, receiver), (4L, mr, receiver)]\nSender thinks it is a RPC client, receiver with queuing delay thinks it is a consumer\nspans: [kind = CLIENT, timestamp = 1L, duration=3L, service=sender], [kind = CONSUMER, timestamp = 2L, duration=20L, service=receiver]\nannotations: [(1L, cs, sender), (2L, wr, receiver), (3L, cr, sender), (22L, mr, receiver)]\nBoth sides have delay\nspans: [kind = PRODUCER, timestamp = 1L, service=sender], [kind = SERVER, timestamp = 2L, duration=1L, service=receiver]\nannotations: [(1L, cs, sender), (2L, sr, receiver), (3L, ss, receiver)]\nInconsistent instrumentation\nWhen one side thinks it is an RPC client or server, things look ok with some gotchas:\nWhen a sender in a shared span thinks it is a client, the critical path of span will not match cr-cs.\n  * this can be corrected at query time if the receiver is instrumented as a consumer (mr).\nWhen a receiver in a shared span thinks it is a server, (sr, ss) are at the end of the span, not the middle.\n  * this can be corrected at query time if the sender instrumented as a producer.\nBasically, there are some impacts, but mostly when both sides are instrumented incorrectly, eg both producer and consumer are mis-modeled as client and server. In the latter case, we can't do anything to fix it.\nSender thinks it is a RPC client, receiver thinks it is a consumer\nspans: [kind = CLIENT, timestamp = 1, duration=3, service=sender], [kind = CONSUMER, timestamp = 5, service=receiver]\nannotations: [(1, cs, sender), (4, cr, sender), (5, mr, receiver)]\nSender thinks it is a RPC client, receiver with queuing delay thinks it is a consumer\nspans: [kind = CLIENT, timestamp = 1L, duration=3L, service=sender], [kind = CONSUMER, timestamp = 5L, duration=1L, service=receiver]\nannotations: [(1, cs, sender), (4, cr, sender), (5, wr, receiver), (6L, mr, receiver)]\nSender thinks it is a Producer, receiver thinks it is an RPC server\nspans: [kind = PRODUCER, timestamp = 1, service=sender], [kind = CONSUMER, timestamp = 5L, duration=1, service=receiver]\nannotations: [(1, ms, sender), (5L, sr, receiver), (5L, sr, receiver)]\nSender with queuing delay thinks it is a Producer, receiver thinks it is an RPC server\nspans: [kind = PRODUCER, timestamp = 1, duration=3, service=sender], [kind = SERVER, timestamp = 5, duration=1, service=receiver]\nannotations: [(1, ms, sender), (4, ws, sender), (5, sr, receiver), (6L, ss, receiver)]\nSender thinks it is a RPC client, receiver thinks it is a RPC server\nspans: [kind = CLIENT, timestamp = 1, duration=3, service=sender], [kind = SERVER, timestamp = 5, duration=1, service=receiver]\nannotations: [(1, cs, sender), (4, cr, sender), (5, sr, receiver), (6L, ss, receiver)]\n. ## Proposal: add span.kind = PRODUCER or CONSUMER\nwhen PRODUCER:\n  span.timestamp = the time the user produced the message\n  span.duration = if present, the delay between the user producing the message and it actually being sent\nwhen CONSUMER\n  span.timestamp = the time the consumer received the message\n  span.duration = if present, the processing delay (ex decoding it etc)\nIn Span2 model (#1499), this is straightforward. However, how we map these to annotations is really important especially as they can be used in single-host mode. Here's the idea:\nAllocating PRODUCER or CONSUMER to annotations\nDefine \"ms\" and \"mr\", as discussed in #1243. Notably, these annotations are on separate endpoints, the producer and consumer respectively. Here's how to allocate span.kind = PRODUCER or CONSUMER to the span v1 model:\n\nIf PRODUCER\nif duration is unset or <1us, span.timestamp -> \"ms\"\n\nif duration is >=1us,  span.timestamp -> \"ms\", span.timestamp+duration -> \"ws\"\n\n\nIf CONSUMER\n\nif duration is unset or <1us, span.timestamp -> \"mr\"\nif duration is >=1us,  span.timestamp -> \"wr\", span.timestamp+duration -> \"mr\"\n\nMost importantly, unset span.timestamp and duration if allocating to annotations.\nExamples:\nIn SPSC, the timeline is OK in both single-host spans, and potentially shared spans. in SPMC shared spans don't work anyway (each consumer needs to fork a span).\nSender and receiver have no queuing delay or knowledge of it.\nspans: [kind = PRODUCER, timestamp = 1L, service=sender], [kind = CONSUMER, timestamp = 3L, service=receiver]\nannotations: [(1L, ms, sender), (3L, mr, receiver)]\nReceiver has some processing delay\nspans: [kind = PRODUCER, timestamp = 1L, service=sender], [kind = CONSUMER, timestamp = 2L, duration=1L, service=receiver]\nannotations: [(1L, ms, sender), (2L, wr, receiver), (3L, mr, receiver)]\nSender has some queuing delay\nspans: [kind = PRODUCER, timestamp = 1L, duration=1, service=sender], [kind = CONSUMER, timestamp = 3L, service=receiver]\nannotations: [(1L, ms, sender), (2L, ws, sender), (3L, mr, receiver)]\nBoth sides are delayed\nspans: [kind = PRODUCER, timestamp = 1L, duration=1, service=sender], [kind = CONSUMER, timestamp = 3L, duration = 1L, service=receiver]\nannotations: [(1L, ms, sender), (2L, ws, sender), (3L, wr, receiver), (4L, mr, receiver)]\nSender thinks it is a RPC client, receiver with queuing delay thinks it is a consumer\nspans: [kind = CLIENT, timestamp = 1L, duration=3L, service=sender], [kind = CONSUMER, timestamp = 2L, duration=20L, service=receiver]\nannotations: [(1L, cs, sender), (2L, wr, receiver), (3L, cr, sender), (22L, mr, receiver)]\nBoth sides have delay\nspans: [kind = PRODUCER, timestamp = 1L, service=sender], [kind = SERVER, timestamp = 2L, duration=1L, service=receiver]\nannotations: [(1L, cs, sender), (2L, sr, receiver), (3L, ss, receiver)]\nInconsistent instrumentation\nWhen one side thinks it is an RPC client or server, things look ok with some gotchas:\nWhen a sender in a shared span thinks it is a client, the critical path of span will not match cr-cs.\n  * this can be corrected at query time if the receiver is instrumented as a consumer (mr).\nWhen a receiver in a shared span thinks it is a server, (sr, ss) are at the end of the span, not the middle.\n  * this can be corrected at query time if the sender instrumented as a producer.\nBasically, there are some impacts, but mostly when both sides are instrumented incorrectly, eg both producer and consumer are mis-modeled as client and server. In the latter case, we can't do anything to fix it.\nSender thinks it is a RPC client, receiver thinks it is a consumer\nspans: [kind = CLIENT, timestamp = 1, duration=3, service=sender], [kind = CONSUMER, timestamp = 5, service=receiver]\nannotations: [(1, cs, sender), (4, cr, sender), (5, mr, receiver)]\nSender thinks it is a RPC client, receiver with queuing delay thinks it is a consumer\nspans: [kind = CLIENT, timestamp = 1L, duration=3L, service=sender], [kind = CONSUMER, timestamp = 5L, duration=1L, service=receiver]\nannotations: [(1, cs, sender), (4, cr, sender), (5, wr, receiver), (6L, mr, receiver)]\nSender thinks it is a Producer, receiver thinks it is an RPC server\nspans: [kind = PRODUCER, timestamp = 1, service=sender], [kind = CONSUMER, timestamp = 5L, duration=1, service=receiver]\nannotations: [(1, ms, sender), (5L, sr, receiver), (5L, sr, receiver)]\nSender with queuing delay thinks it is a Producer, receiver thinks it is an RPC server\nspans: [kind = PRODUCER, timestamp = 1, duration=3, service=sender], [kind = SERVER, timestamp = 5, duration=1, service=receiver]\nannotations: [(1, ms, sender), (4, ws, sender), (5, sr, receiver), (6L, ss, receiver)]\nSender thinks it is a RPC client, receiver thinks it is a RPC server\nspans: [kind = CLIENT, timestamp = 1, duration=3, service=sender], [kind = SERVER, timestamp = 5, duration=1, service=receiver]\nannotations: [(1, cs, sender), (4, cr, sender), (5, sr, receiver), (6L, ss, receiver)]\n. here's the thrift update https://github.com/openzipkin/zipkin-api/pull/29. here's the thrift update https://github.com/openzipkin/zipkin-api/pull/29. PS on the above, I explicitly said messaging spans should always be single-host (even in the multi-host model). What this means is that if someone wants one-way (in the same span), they use \"cs\" -> \"sr\" not \"ms\" -> \"mr\". \"ms\" is the parent of \"mr\" not the same span. This is because we may never know if there are multiple recipients or not.. PS on the above, I explicitly said messaging spans should always be single-host (even in the multi-host model). What this means is that if someone wants one-way (in the same span), they use \"cs\" -> \"sr\" not \"ms\" -> \"mr\". \"ms\" is the parent of \"mr\" not the same span. This is because we may never know if there are multiple recipients or not.. here's the work in progress https://github.com/openzipkin/zipkin/pull/1677. here's the work in progress https://github.com/openzipkin/zipkin/pull/1677. https://github.com/openzipkin/zipkin/pull/1677 is nearly done. will test it via https://github.com/openzipkin/brave/pull/461 then polish for merge. https://github.com/openzipkin/zipkin/pull/1677 is nearly done. will test it via https://github.com/openzipkin/brave/pull/461 then polish for merge. Here is what work in progress looks like for a single producer, single consumer POV where the remote endpoint (broker) is set to kafka:\nbash\n$ curl -X POST -s localhost:9411/api/v1/spans -H 'Content-Type: application/json' -d '[{\"traceId\":\"040c464a2b70a87a\",\"id\":\"040c464a2b70a87a\",\"name\":\"\",\"timestamp\":1501654414312417,\"annotations\":[{\"timestamp\":1501654414312417,\"value\":\"ms\",\"endpoint\":{\"serviceName\":\"producer\",\"ipv4\":\"192.168.1.10\"}}],\"binaryAnnotations\":[{\"key\":\"kafka.key\",\"value\":\"foo\",\"endpoint\":{\"serviceName\":\"producer\",\"ipv4\":\"192.168.1.10\"}},{\"key\":\"kafka.topic\",\"value\":\"myTopic\",\"endpoint\":{\"serviceName\":\"producer\",\"ipv4\":\"192.168.1.10\"}},{\"key\":\"ma\",\"value\":true,\"endpoint\":{\"serviceName\":\"kafka\"}}]}]'\n$ curl -X POST -s localhost:9411/api/v1/spans -H 'Content-Type: application/json' -d '[{\"traceId\":\"040c464a2b70a87a\",\"id\":\"b169190ffec10531\",\"name\":\"\",\"parentId\":\"040c464a2b70a87a\",\"timestamp\":1501654414816112,\"annotations\":[{\"timestamp\":1501654414816112,\"value\":\"mr\",\"endpoint\":{\"serviceName\":\"consumer\",\"ipv4\":\"192.168.1.10\"}}],\"binaryAnnotations\":[{\"key\":\"ma\",\"value\":true,\"endpoint\":{\"serviceName\":\"kafka\"}}]}]'\n\n\n. Here is what work in progress looks like for a single producer, single consumer POV where the remote endpoint (broker) is set to kafka:\nbash\n$ curl -X POST -s localhost:9411/api/v1/spans -H 'Content-Type: application/json' -d '[{\"traceId\":\"040c464a2b70a87a\",\"id\":\"040c464a2b70a87a\",\"name\":\"\",\"timestamp\":1501654414312417,\"annotations\":[{\"timestamp\":1501654414312417,\"value\":\"ms\",\"endpoint\":{\"serviceName\":\"producer\",\"ipv4\":\"192.168.1.10\"}}],\"binaryAnnotations\":[{\"key\":\"kafka.key\",\"value\":\"foo\",\"endpoint\":{\"serviceName\":\"producer\",\"ipv4\":\"192.168.1.10\"}},{\"key\":\"kafka.topic\",\"value\":\"myTopic\",\"endpoint\":{\"serviceName\":\"producer\",\"ipv4\":\"192.168.1.10\"}},{\"key\":\"ma\",\"value\":true,\"endpoint\":{\"serviceName\":\"kafka\"}}]}]'\n$ curl -X POST -s localhost:9411/api/v1/spans -H 'Content-Type: application/json' -d '[{\"traceId\":\"040c464a2b70a87a\",\"id\":\"b169190ffec10531\",\"name\":\"\",\"parentId\":\"040c464a2b70a87a\",\"timestamp\":1501654414816112,\"annotations\":[{\"timestamp\":1501654414816112,\"value\":\"mr\",\"endpoint\":{\"serviceName\":\"consumer\",\"ipv4\":\"192.168.1.10\"}}],\"binaryAnnotations\":[{\"key\":\"ma\",\"value\":true,\"endpoint\":{\"serviceName\":\"kafka\"}}]}]'\n\n\n. Hi Sridhar. The current version of zipkin purges traces, try 1.28.1 (latest)\nhttps://github.com/openzipkin/zipkin/releases/tag/1.28.0\n-A\n. Hi Sridhar. The current version of zipkin purges traces, try 1.28.1 (latest)\nhttps://github.com/openzipkin/zipkin/releases/tag/1.28.0\n-A\n. merging bc this is blocking the simplejson thing (and generally useful towards single-host spans). Happy to accept any feedback post commit.. merging bc this is blocking the simplejson thing (and generally useful towards single-host spans). Happy to accept any feedback post commit.. https://github.com/openzipkin/zipkin-js/issues/113 is one request. https://github.com/openzipkin/zipkin-js/issues/113 is one request. FYI circleci is busted due to https://discuss.circleci.com/t/connection-resets-to-maven-central/14563 cc @openzipkin/devops-tooling . FYI circleci is busted due to https://discuss.circleci.com/t/connection-resets-to-maven-central/14563 cc @openzipkin/devops-tooling . starting on this. realize this isn't probably a great first PR since it touches a lot of things!. starting on this. realize this isn't probably a great first PR since it touches a lot of things!. https://github.com/openzipkin/zipkin/pull/1675. https://github.com/openzipkin/zipkin/pull/1675. PS rationale to drop here is that this particular pattern we are avoiding is only possible on buggy instrumentation (ex that dole out incorrect IDs).\nCommon usage errors, like putting the remote service name on client annotations.. code for this is kept at the moment.. thx for the feedback @fedj will sort this on the way in.. Added a bunch of logging and logging tests. The logging is incidentally a start of #1619. @fedj commented on this pull request.\nFAR: Shouldn't we put into the README the strictTraceId feature\nhttps://github.com/openzipkin/zipkin/blob/7cd7b687aebe52db2923fdd0a7d827df1940ca8d/zipkin/src/main/java/zipkin/storage/StorageComponent.java#L35\n?\nYeah.. surprised it isnt there. Will add it.\nOPT: By extracting the traceId in a constant, it would emphasize that the\ntrace is supposed to be considered the same\nGood idea. Thx\n. circleci is fussy for reasons unrelated. there are troubles downloading dependencies (happened last week, too). circleci is fussy for reasons unrelated. there are troubles downloading dependencies (happened last week, too). You can use google stackdriver as a test server, though you would still\nneed run the proxy collector..\n. In general, agree that quickstart is current easiest esp now that in mem\nexpires data.\nIt could be possible to add zipkin to the catalog of a cloud service like\namazon to make it ever so slightly easier.\n. rationale for this is I'm trying to squeeze out the internal DependencyLinkSpan type (for #1499), and this simplifies the code that depends on it.. cc @shakuzen @hfgbarrigas . #1670 follows this. followed by #1671. followed by #1673. we started discussing this in gitter. One of the issues we have is that ES 6 does not support multiple types. One way to roll this out is to store the new span (and dependency link) in indexes that are not the same as the old (ex with a delimited -span in the index name).\nThis means that an old server can still read the old data, but a new server will store and read the new format. IOTW, you'd cut traffic over, maybe leaving an old server running for a couple days with special instructions to go there.\nFor those who want to migrate old data to the new index space, we can write a job, maybe spark, to do that. Thoughts?. ps this branch needs a bit of polish wrt ES 6. another way (more work) would be to make this code support querying the old\nformat (by issuing redundant commands for each query). This could be\nremoved at some point in the future.\n. >\n\nI like that the second item, maybe a configuration item so you can opt in\nthis new format at some point when you are ready? Or things are getting too\ncomplicated?\nopting into writes delays an inevitable which is that the index template\nwill have to change else ES 6 will break. I suppose we can try to detect\nversions accordingly if we keep both indexing strategies..\n. @jcarres-mdsol I don't think it will be feasible to keep the code writing both versions concurrently (even if we could maintain code to read both). It is really a lot of work and we don't have that many hands participating on this sort of thing. ES 6 will soon start hitting us, so doing this at the same time makes most sense.\n\nIf you want to remain writing the old version, you could leave a node running on a prior release and keep POST traffic going there until you are ready to cutover. Reads could use the new version regardless (once dual reads are woven back in). Is this acceptable?. NEXT STEP: make this work with ES 6 w/o breaking ES 2 and 5. OK the code now works with Elasticsearch 2.4+, 5.x and 6.0 alpha\n2.4 was released Aug 2016 and works if run with ES_JAVA_OPTS=-Dmapper.allow_dots_in_name=true\n. NEXT STEP: have @garyd203 or any other willing user give this branch a spin. Meanwhile see how hard it is to simultaneously support the old format (tomorrow). @jcarres-mdsol PS I think you are right about the next version using this format opt-in (via an experimental prop) and a following release default. I think I can find a way to do this neatly enough now that the index templates no longer clash with eachother. With luck can-do over the weekend.. FYI I have code that works portably with the old and new format (enabled by a flag). just testing last bits of it.. The new single-type indexing is setup by default when using ES 6.x. For other versions, we need to set ES_EXPERIMENTAL_SPAN2=true. When set and <6.x fan-out reads occur.\nI'll be polishing this up a bit more and adding more tests.. @hfgbarrigas can you take this for a spin?. ok last tests are in. I'm not planning to add unit tests for the javascript as there aren't similar ones in for the classes effected. If folks have strong feelings, I can backfill some in a week or two (takes me a while to do these, but I don't want to hold up #1499 over it).. >\n\nCould we make the red and yellow thresholds configurable?\nI guess maybe something like highErrorRate (red) and lowErrorRate (yellow)\ninstead of the hard-coded values?\nyes. we can read them from /config.json I think\n. @shakuzen added a commit to parameterize error rates. @sirtyro does this look of interest to you?. @sirtyro does this look of interest to you?. Thanks for thinking this through, Tommy. I think the original names are\nbest and probably best (dot)nested names\n. Thanks for thinking this through, Tommy. I think the original names are\nbest and probably best (dot)nested names\n. @shakuzen (and others). Should we say a rate of zero disables the color? For example, if \"low\" is zero, no yellow lines occur. If both low and high are zero, red lines don't show up either.. @shakuzen (and others). Should we say a rate of zero disables the color? For example, if \"low\" is zero, no yellow lines occur. If both low and high are zero, red lines don't show up either.. @shakuzen I meant using zero as a signal and change code to check for zero.\nWe could say use greater than 1 and not add code.. didnt consider it,\nsounds fine to me!\n. @shakuzen I meant using zero as a signal and change code to check for zero.\nWe could say use greater than 1 and not add code.. didnt consider it,\nsounds fine to me!\n. You do!\n\nOn 15 Aug 2017 06:15, \"Simeon Ross\" notifications@github.com wrote:\n\n@adriancole https://github.com/adriancole, sorry for the lag. It\ncertainly looks interesting. If we bump to latest zipkin, do we get it for\nfree?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1675#issuecomment-322324958,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD616PMb35EtLcuZwz-KQK1XsFcsuBqks5sYMcHgaJpZM4OmcNi\n.\n. polish.. maybe since it is /dependency in the UI and api zipkin-dependencyLink-2017-08-01 -> zipkin-dependency-2017-08-01 (the link part isn't accurate anyway as it is links :) ). polish.. maybe since it is /dependency in the UI and api zipkin-dependencyLink-2017-08-01 -> zipkin-dependency-2017-08-01 (the link part isn't accurate anyway as it is links :) ). PS I'm going to make a test elasticsearch 6 image in our docker namespace so we can prove this stuff out (same rationale as our other test images: very few layers which makes things fast). PS I'm going to make a test elasticsearch 6 image in our docker namespace so we can prove this stuff out (same rationale as our other test images: very few layers which makes things fast). I think we need to bust the old index template match\n\nex our old was \"template\": \"${__INDEX__}-*\", which would match zipkin-2017-08-01, but also zipkin-span-2017-08-01. If an old template matches, we'll index improperly.\nI think we should break it by using a different delimiter, perhaps a colon (like jaeger do, possibly for similar reasons)\nex \"template\": \"${__INDEX__}:span-*\", which would match zipkin:span-2017-08-01, but leave the old data alone (because old data in zipkin-2017-08-01 doesn't have a prefix of zipkin:span-)\n. I think we need to bust the old index template match\nex our old was \"template\": \"${__INDEX__}-*\", which would match zipkin-2017-08-01, but also zipkin-span-2017-08-01. If an old template matches, we'll index improperly.\nI think we should break it by using a different delimiter, perhaps a colon (like jaeger do, possibly for similar reasons)\nex \"template\": \"${__INDEX__}:span-*\", which would match zipkin:span-2017-08-01, but leave the old data alone (because old data in zipkin-2017-08-01 doesn't have a prefix of zipkin:span-)\n. https://github.com/openzipkin/zipkin/pull/1674 implements the new span2 model and also supports ES 6. I'll try to get this to work in such a way that the new model is optional for writes.. https://github.com/openzipkin/zipkin/pull/1674 implements the new span2 model and also supports ES 6. I'll try to get this to work in such a way that the new model is optional for writes.. FYI: here's an approach to consider now or later when the storage implementation dramatically differs from a prior version https://github.com/openzipkin/zipkin/issues/1679. FYI: here's an approach to consider now or later when the storage implementation dramatically differs from a prior version https://github.com/openzipkin/zipkin/issues/1679. everything is all ready to go in https://github.com/openzipkin/zipkin/pull/1674. Folks can play with ES 6.x using 1.29.2 which is on the way out.\nIf you want to use the same indexing with 2.4+, you have to set ES_EXPERIMENTAL_SPAN2=true as detailed in #1674. We'll formalize the mechanism for using single-type indexes in ES <6 prior to zipkin 1.30. ok well ES 6.x can't work in zipkin-dependencies, yet due to elastic/elasticsearch-hadoop#1032\nHowever, spans created with ES_EXPERIMENTAL_SPAN2=true on ES 2.4+ -> 5.x can once https://github.com/openzipkin/zipkin-dependencies/pull/83 is released. ran into a snag with dots.. https://github.com/openzipkin/zipkin/issues/1499#issuecomment-321719394. Snag with dots resolved by manually indexing queries with a non-storage index field _q\nStarting with zipkin 1.29.3, all should work fine, notably without version constraints. For example, you no longer need minimum ES 2.4 as earlier versions of 2.0 should work fine\nInternally, it just pre-encodes the valid query strings like we do with cassandra, except using equals as a delimiter as opposed to colon. Incidentally, curl queries work still, just with a little inception:\ncurl -s localhost:9200/zipkin:span-2017-08-11/_search?q=_q:error=500\nThanks very very much to @sokac for testing and finding the problem!. @devinsba @jcarres-mdsol @mansu some of you requested that we take care to make formats compatible and opt-in. I took several person days effort to do that, but I haven't heard from anyone who asked if it works or not.\nI'm wondering if one or more of you can test the ES_EXPERIMENTAL_SPAN2=true flag so that we can progress this issue?. another bomb coming in ES 7 https://github.com/openzipkin/zipkin/issues/2219. ok dependency linking is done. I'll look at UI next. ok dependency linking is done. I'll look at UI next. cc @openzipkin/elasticsearch @openzipkin/cassandra @openzipkin/core for input on this. cc @openzipkin/elasticsearch @openzipkin/cassandra @openzipkin/core for input on this. Note this technique can also be used to address a transition from one storage type to another. for example, mysql to cassandra or elasticsearch. Note this technique can also be used to address a transition from one storage type to another. for example, mysql to cassandra or elasticsearch. PS another way out would be to write a read api merging proxy. This would work, but it would add another process to manage and upgrade. Yet another way out would be to do the fan-out in javascript in the UI code. This would be a bit complicated especially due to cross origin requests.. PS another way out would be to write a read api merging proxy. This would work, but it would add another process to manage and upgrade. Yet another way out would be to do the fan-out in javascript in the UI code. This would be a bit complicated especially due to cross origin requests.. >\n\nHow will it work if there is an index with the same date in old and new\nstorage? Scan for traceId until the first match is found?\nIt would speculatively query the old one with the same parameters, and\nmerge any results (which would delay quests a bit)\nP.S. In general sounds good for me.\ncool!\n. >\nHow will it work if there is an index with the same date in old and new\nstorage? Scan for traceId until the first match is found?\nIt would speculatively query the old one with the same parameters, and\nmerge any results (which would delay quests a bit)\nP.S. In general sounds good for me.\ncool!\n. >\nCould it also be a way to migrate from a Span V1 => V2 in the same storage\n?\nYes, for example I have this in mind for the transition to Span V2 (which\nis the same as Elasticsearch 6.x)\nOr any breaking change in general (even if it only happens once a in a\nvery while) ?\nIndeed\n. >\nCould it also be a way to migrate from a Span V1 => V2 in the same storage\n?\nYes, for example I have this in mind for the transition to Span V2 (which\nis the same as Elasticsearch 6.x)\nOr any breaking change in general (even if it only happens once a in a\nvery while) ?\nIndeed\n. As a part of #1674 I'm using an internal type like this which is working so far. This could likely be used for a generic one (where right is an http api span store):\n\n```java\n/*\n * This makes redundant read commands, concatenating results if two answers come back, or accepting\n * one if there's an error on the other.\n /\npublic final class LenientDoubleAsyncSpanStore implements AsyncSpanStore {\n  final AsyncSpanStore left;\n  final AsyncSpanStore right;\n--snip--\n  @Override public void getTrace(long traceIdHigh, long traceIdLow, Callback> callback) {\n    GetTraceDoubleCallback doubleCallback = new GetTraceDoubleCallback(callback);\n    left.getTrace(traceIdHigh, traceIdLow, doubleCallback);\n    right.getTrace(traceIdHigh, traceIdLow, doubleCallback);\n  }\nstatic final class GetTraceDoubleCallback extends LenientDoubleCallback> {\n    static final Logger LOG = Logger.getLogger(GetTraceDoubleCallback.class.getName());\nGetTraceDoubleCallback(Callback<List<Span>> delegate) {\n  super(LOG, delegate);\n}\n\n@Override List<Span> merge(List<Span> v1, List<Span> v2) {\n  List<Span> result = new ArrayList<>(v1);\n  result.addAll(v2);\n  return MergeById.apply(result);\n}\n\n}\n--snip--\n/* Callback that succeeds if at least one value does. The first error is logged. /\nabstract class LenientDoubleCallback implements Callback {\n  final Logger log;\n  final Callback delegate;\nV v;\n  Throwable t;\nLenientDoubleCallback(Logger log, Callback delegate) {\n    this.log = log;\n    this.delegate = delegate;\n  }\nabstract V merge(V v1, V v2);\n@Override synchronized final public void onSuccess(V value) {\n    if (t != null) {\n      delegate.onSuccess(value);\n    } else if (v == null) {\n      v = value;\n    } else {\n      delegate.onSuccess(merge(v, value));\n    }\n  }\n@Override synchronized final public void onError(Throwable throwable) {\n    if (v != null) {\n      delegate.onSuccess(v);\n    } else if (t == null) {\n      log.log(Level.INFO, \"first error\", throwable);\n      t = throwable;\n    } else {\n      delegate.onError(throwable);\n    }\n  }\n}\n```. As a part of #1674 I'm using an internal type like this which is working so far. This could likely be used for a generic one (where right is an http api span store):\n```java\n/*\n * This makes redundant read commands, concatenating results if two answers come back, or accepting\n * one if there's an error on the other.\n /\npublic final class LenientDoubleAsyncSpanStore implements AsyncSpanStore {\n  final AsyncSpanStore left;\n  final AsyncSpanStore right;\n--snip--\n  @Override public void getTrace(long traceIdHigh, long traceIdLow, Callback> callback) {\n    GetTraceDoubleCallback doubleCallback = new GetTraceDoubleCallback(callback);\n    left.getTrace(traceIdHigh, traceIdLow, doubleCallback);\n    right.getTrace(traceIdHigh, traceIdLow, doubleCallback);\n  }\nstatic final class GetTraceDoubleCallback extends LenientDoubleCallback> {\n    static final Logger LOG = Logger.getLogger(GetTraceDoubleCallback.class.getName());\nGetTraceDoubleCallback(Callback<List<Span>> delegate) {\n  super(LOG, delegate);\n}\n\n@Override List<Span> merge(List<Span> v1, List<Span> v2) {\n  List<Span> result = new ArrayList<>(v1);\n  result.addAll(v2);\n  return MergeById.apply(result);\n}\n\n}\n--snip--\n/* Callback that succeeds if at least one value does. The first error is logged. /\nabstract class LenientDoubleCallback implements Callback {\n  final Logger log;\n  final Callback delegate;\nV v;\n  Throwable t;\nLenientDoubleCallback(Logger log, Callback delegate) {\n    this.log = log;\n    this.delegate = delegate;\n  }\nabstract V merge(V v1, V v2);\n@Override synchronized final public void onSuccess(V value) {\n    if (t != null) {\n      delegate.onSuccess(value);\n    } else if (v == null) {\n      v = value;\n    } else {\n      delegate.onSuccess(merge(v, value));\n    }\n  }\n@Override synchronized final public void onError(Throwable throwable) {\n    if (v != null) {\n      delegate.onSuccess(v);\n    } else if (t == null) {\n      log.log(Level.INFO, \"first error\", throwable);\n      t = throwable;\n    } else {\n      delegate.onError(throwable);\n    }\n  }\n}\n```. @openzipkin/core @bplotnick @llinder @aliostad @devinsba @flier @ellispritchard @mosesn @pavolloffay @clguimanMSFT @neilstevenson @goller @SergeyKanzhelev\nSo this change is pretty significant as it means folks can start using the new simplified json format defined in #1499 as soon as this is released (likely zipkin 1.30). In reality a lot of sites won't automatically upgrade to this, but it is important to know anyway.\nConcretely, this means tracers can start offering the ability to send in the new format, ex via kafka (sqs azure hub etc) or POST /api/v2/spans. This format is the same as what we're using internally in Elasticsearch 6.x (#1674), and will be the default choice for document stores.\nI'm cc'ing you for heads up in case you'd like to try using this, or update your libraries to support it. In the next few weeks a lot of stuff around this will be landing tracked in #1644. Watch or comment on that issue if you want others to know your status.. >\n\nWhat is the format for annotations?\nWe will publish an open api spec and it is noted in #1499, but annotations\nare simple. It is the same as before except no endpoint.\n\n{\n  \"traceId\": \"86154a4ba6e91385\",\n  \"parentId\": \"86154a4ba6e91385\",\n  \"id\": \"4d1e00c0db9010db\",\n  \"kind\": \"CLIENT\",\n  \"name\": \"get\",\n  \"timestamp\": 1472470996199000,\n  \"duration\": 207000,\n  \"localEndpoint\": {\n    \"serviceName\": \"frontend\",\n    \"ipv4\": \"127.0.0.1\"\n  },\n  \"remoteEndpoint\": {\n    \"serviceName\": \"backend\",\n    \"ipv4\": \"192.168.99.101\",\n    \"port\": 9000\n  },\n  \"annotations\": [\n    {\n      \"timestamp\": 1472470996238000,\n      \"value\": \"ws\"\n    },\n    {\n      \"timestamp\": 1472470996403000,\n      \"value\": \"wr\"\n    }\n  ],\n  \"tags\": {\n    \"http.path\": \"/api\",\n    \"clnt/finagle.version\": \"6.45.0\"\n  }\n}\n. not merging this, yet due to https://github.com/openzipkin/zipkin/issues/1499#issuecomment-321719394. ok ES indexing problem has been resolved. will merge on green. this gets rid of empty service names in ES 6.x. all warnings are gone except zipkin-ui and spring boot about layoutfactory which will be solved in #1602 \nhere's the zipkin-ui warnings remaining\n[WARNING] npm WARN zipkin-ui@0.0.0 No repository field.\n[INFO] 16 08 2017 11:11:20.532:WARN [watcher]: Pattern \"/Users/acole/oss/zipkin/zipkin-ui/test/*test.js\" does not match any file.\n[INFO] 16 08 2017 11:11:20.543:WARN [watcher]: Pattern \"/Users/acole/oss/zipkin/zipkin-ui/test/*test.js\" does not match any file.\nI'll see if I can kill the ui ones... \"No repository field\" sorted. the other would be sorted if we wrote more unit tests :P. https://github.com/openzipkin/zipkin/pull/1637 is at odds with this..\nI don't think it expected service names so long, and generally service\nnames shouldn't have IPs in them as it subverts aggregation..\nThat said, there could well be long service names even if there\nweren't variables in them. I don't know what the best action to take\nwould be.. should we push the timeline to the right? or truncate the\nservice names like we did before.. or..\ncc @eirslett @fgcui1204 @klingerf @esbie for brainstorming\n. note annotation_query would be shorter per span as it only needs to concat annotation and tag pairs (as opposed to also putting in the service name, because the local_endpoint covers this part). > i don't get this. when we search against that index we still are searching for local_serviceName:annotation or local_serviceName;tag_key;tag_value.\nI mean to say that the serviceName can be refined independent of the\nball of tags. for example, span.local_serviceName = foo filtered\nbefore annotationQuery. So instead of redundantly encoding the service\nname in the annotationQuery concatenation, access it as a field.\nMy assumption is that this could be more efficient, but might be\nwrong. At any rate, it would be easier to read.\nMake sense?\n. cc @openzipkin/elasticsearch . here's the double-conversion which is easier to obviate when we know how storage happens by type https://github.com/openzipkin/zipkin/pull/1700. thanks for raising this! one note to bear in mind that unless we change the\nentire layout a \"follows from\" that crosses services (aka a\nproducer/consumer) would be indistinguishable from a step child. that's\nbecause depth also is how we tag a span with a service.\nAnother side note is that producer/consumer span kind in zipkin is imho a\nbit more semantically useful than \"follows from\", as it implies the same\nwhich is that the producer doesn't rely on the outcome of the consumer.\nthere are probably hairs to split here, but anyway worth mentioning how we\nhandle things.\n. Spoke with @garyd203 and @sirtyro here at Tyro. Right now, in lieu of this, they are cleverly propagating send timestamp, then on the receiver side adding that as an annotation on the child span. Ends up looking like a dot followed by a bar thereafter. Being on the same line helps you know visibly where it started.\nIn mapping producer/consumer, still think single-line could work. If consumer span is solely for consuming (not processing), the processor child could also be on the same line.\nMight also be nice if you can click on possibly a dashed line or something between producer and consumer to give a view similar to the span detail pane (except showing all points of time between the producer and consumer). Spoke with @garyd203 and @sirtyro here at Tyro. Right now, in lieu of this, they are cleverly propagating send timestamp, then on the receiver side adding that as an annotation on the child span. Ends up looking like a dot followed by a bar thereafter. Being on the same line helps you know visibly where it started.\nIn mapping producer/consumer, still think single-line could work. If consumer span is solely for consuming (not processing), the processor child could also be on the same line.\nMight also be nice if you can click on possibly a dashed line or something between producer and consumer to give a view similar to the span detail pane (except showing all points of time between the producer and consumer). This change still needs polish but raising for early feedback cc @anuraaga @shakuzen @llinder . Did significant polishing on flight, but needs unit tests. Will play with that in next change to the file. Thx for the tip!\nOn 27 Aug 2017 2:15 pm, \"Anuraag Agrawal\" notifications@github.com wrote:\n\n@anuraaga commented on this pull request.\nIn zipkin/src/test/java/zipkin/internal/v2/CallTest.java\nhttps://github.com/openzipkin/zipkin/pull/1705#discussion_r135403529:\n\n+\n+    try {\n+      call.enqueue(callback);\n+      failBecauseExceptionWasNotThrown(IllegalStateException.class);\n+    } catch (IllegalStateException e) {\n+\n+    }\n+  }\n+\n+  @Test public void enqueuesOnce() throws Exception {\n+    Call call = Call.create(null);\n+    call.enqueue(callback);\n+\n+    try {\n+      call.enqueue(callback);\n+      failBecauseExceptionWasNotThrown(IllegalStateException.class);\n\nSince you're using assertj, just wondering any reason not to use\nassertThatThrownBy?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1705#pullrequestreview-58814941,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD613YFk-rY-ILSdphtxFkM7Vw1omEUks5scQmYgaJpZM4PBfRH\n.\n. here's follow-ups needed to compete the read side of elasticsearch storage https://github.com/openzipkin/zipkin/pull/1708. The UI element is sorting results client side. Are you saying the client\nside sort is unreliable? Or that you want all storage backends to sort data\ndifferently before applying filters. The latter is hard due to the amount\nof data involved so unlikely to land soon or for all storage options.\n. If you are up to making this work for all storage backends with gigabytes\norders of magnitude more data, feel free to propose a solution to\narbitrarily change order of sort and other conditions.\n\nYou can also propose to remove the client side sort.\nFinally, if you know which backend you are using there may be a way to\nquery more arbitrarily.\nMeanwhile, thanks for letting us know. All the changes we do are based on\ngreatest need and hands available. Knowing another person is interested in\nsomething helps steer things.\n. closing in favor of a related issue which is still open but has a better explanation imho #1218. @Jaware I guess I tried to say earlier that sorting as a server-side function would need to be done consistently for all backends. I'm in favor of removing the client-side sort if it is annoying you. As mentioned earlier, let's keep discussion on the older issue https://github.com/openzipkin/zipkin/issues/1218. There is an existing issue about this you can look. cassandra3 is\nincomplete, though a workaround exists.\n. thx @anuraaga . PS once this is in, the v2 http api (openapi/swagger etc) can be implemented. Later, we can formalize and make the java libraries not-internal.. ok all apis are here, and I'll raise two other PRs: one to add the http api and another to implement with elasticsearch. There's still a solid amount of work on this PR, yet, which is to backfill tests and make a v2 replacement for SpanStoreTest\nOne thing you'll notice is I made a copy of the bridging read api from elasticsearch to here. This is to decouple change. The elasticsearch copy will be deleted later.\ncc @anuraaga. OK backfilled almost all tests. nearly there. closing and will delete the branch later. no offense, just stale PR queue isn't awesome. This is the last function needed to complete the v2 http api (the junit rule tests the api impl with okhttp) cc @anuraaga @shakuzen . This is the last function needed to complete the v2 http api (the junit rule tests the api impl with okhttp) cc @anuraaga @shakuzen . will take feedback post-commit, even renames and such. won't release this immediately, but need the function to proceed.. will take feedback post-commit, even renames and such. won't release this immediately, but need the function to proceed.. @SimenB up for updating the plugin?. @SimenB up for updating the plugin?. you could check /zipkin/config.json first and if 404 try /config.json. you could check /zipkin/config.json first and if 404 try /config.json. cc @naoman who raised the issue. I'm not sure if it is a simple workaround to have ELB do the path forwarding or not.\nAny @shakuzen @wilkinsona do you know latest on X-Forwarded-For on spring boot? we are using default container on 1.5.6.RELEASE. cc @naoman who raised the issue. I'm not sure if it is a simple workaround to have ELB do the path forwarding or not.\nAny @shakuzen @wilkinsona do you know latest on X-Forwarded-For on spring boot? we are using default container on 1.5.6.RELEASE. fwiw I tried adding the following to no avail:\nserver:\n  tomcat:\n    protocol-header: X-Forwarded-Proto\n    remote-ip-header: X-Forwarded-For\n    port-header: X-Forwarded-Port\n    protocol-header-https-value: https\nHere's the redirect controller:\njava\n  /** Make sure users who aren't familiar with /zipkin get to the right path */\n  @RequestMapping(value = \"/\", method = GET)\n  public ModelAndView redirectRoot() {\n    return new ModelAndView(\"redirect:/zipkin/\");\n  }. fwiw I tried adding the following to no avail:\nserver:\n  tomcat:\n    protocol-header: X-Forwarded-Proto\n    remote-ip-header: X-Forwarded-For\n    port-header: X-Forwarded-Port\n    protocol-header-https-value: https\nHere's the redirect controller:\njava\n  /** Make sure users who aren't familiar with /zipkin get to the right path */\n  @RequestMapping(value = \"/\", method = GET)\n  public ModelAndView redirectRoot() {\n    return new ModelAndView(\"redirect:/zipkin/\");\n  }. @wilkinsona thanks.. almost there.. just X-Forwarded-Host isn't being picked up.. not really sure which setting would accomadate that.\nserver:\n  use-forward-headers: true\n  tomcat:\n    protocol-header: X-Forwarded-Proto\n    remote-ip-header: X-Forwarded-For\n    port-header: X-Forwarded-Port\n    protocol-header-https-value: https\norg.junit.ComparisonFailure: \nExpected :\"https://zipkin.com:443/zipkin/\"\nActual   :\"https://localhost/zipkin/\". @wilkinsona thanks.. almost there.. just X-Forwarded-Host isn't being picked up.. not really sure which setting would accomadate that.\nserver:\n  use-forward-headers: true\n  tomcat:\n    protocol-header: X-Forwarded-Proto\n    remote-ip-header: X-Forwarded-For\n    port-header: X-Forwarded-Port\n    protocol-header-https-value: https\norg.junit.ComparisonFailure: \nExpected :\"https://zipkin.com:443/zipkin/\"\nActual   :\"https://localhost/zipkin/\". I've removed X-Forwarded-Host from the test as Host is more typical anyway.. I've removed X-Forwarded-Host from the test as Host is more typical anyway.. Thanks @wilkinsona!. Api allows this, so probably it is a matter of allowing an \"all\" keyword\nlike we use in span name\n. added the help-wanted tag as I don't know why the following isn't running a trace query as I'd expect it to. Seems I still have to choose a service, so missing something..\n```diff\ndiff --git a/zipkin-ui/js/component_data/default.js b/zipkin-ui/js/component_data/default.js\nindex 3ce6708c4..be0322c33 100644\n--- a/zipkin-ui/js/component_data/default.js\n+++ b/zipkin-ui/js/component_data/default.js\n@@ -13,6 +13,12 @@ export function convertToApiQuery(windowLocationSearch) {\n     }\n     delete query.startTs;\n   }\n+  if (query.serviceName === 'all') {\n+    delete query.serviceName;\n+  }\n+  if (query.spanName === 'all') {\n+    delete query.spanName;\n+  }\n   return query;\n }\ndiff --git a/zipkin-ui/js/component_ui/serviceName.js b/zipkin-ui/js/component_ui/serviceName.js\nindex e94c4dc41..9c8487e95 100644\n--- a/zipkin-ui/js/component_ui/serviceName.js\n+++ b/zipkin-ui/js/component_ui/serviceName.js\n@@ -16,6 +16,8 @@ export default component(function serviceName() {\nthis.updateServiceNameDropdown = function(ev, data) {\n     $('#serviceName').empty();\n+    this.$node.append($($.parseHTML('all')));\n+\n     $.each(data.names, (i, item) => {\n       $('').val(item).text(item).appendTo('#serviceName');\n     });\ndiff --git a/zipkin-ui/test/component_data/default.test.js b/zipkin-ui/test/component_data/default.test.js\nindex c091ce9e6..109487655 100644\n--- a/zipkin-ui/test/component_data/default.test.js\n+++ b/zipkin-ui/test/component_data/default.test.js\n@@ -3,6 +3,18 @@ import {convertToApiQuery} from '../../js/component_data/default';\n describe('convertToApiQuery', () => {\n   const should = require('chai').should();\n\nit('should clear spanName all', () => {\nconst parsed = convertToApiQuery('?spanName=all&endTs=1459169770000');\n+\nshould.not.exist(parsed.spanName);\n});\n+\nit('should clear serviceName all', () => {\nconst parsed = convertToApiQuery('?serviceName=all&endTs=1459169770000');\n+\nshould.not.exist(parsed.spanName);\n});\n+\n   it('should not require startTs', () => {\n     const parsed = convertToApiQuery('?endTs=1459169770000');\n```. https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md#mysql-storage\n\nFeel free to ask more on gitter. I will close this issue as we don't use\nissues for q&a\nhttps://gitter.im/openzipkin/zipkin\n. >\n\nI believe the backend for the UI in this setup is the Zipkin backend,\nso we should be able to conditionally inject the HTML  tag based on\nan ENV var. @adriancole https://github.com/adriancole am I making any\nwrong assumptions here / messing up the conclusion?\nYes I think we are at a point where we need to rewrite HTML to accomplish\nthe goal. We had instructions for this when using NGINX, but there's likely\na \"cheap way\" to do this either via spring boot, or worst case in the\ndocker container at startup (ex literally rewrite the files)\n(Meta: should we move this discussion to a separate issue?)\nagreed. we now have a \"boiled down\" issue, and it is a matter of how to\nmove it forward.  We can either re-title the issue or we can close and open\na new one about the task.\n. >\n@adriancole https://github.com/adriancole does it work locally if you\nstop your local RabbitMQ and run tests with Docker? I have a feeling that\nsome tests use local RabbitMQ (it explains why it passes locally)\nvery good guess. sadly after killing my rabbit still passes. I added a log\nstatement to verify\n. pushed a commit to use travis rabbitmq service (to see if local passes). nice work, @shakuzen!. The next zipkin version will support rabbitmq\nhttps://github.com/openzipkin/zipkin/pull/1742\n\nThe problem you are experiencing has more to do with spring-cloud-sleuth,\nso better to join their gitter for support.\n. zipkin-server 2.1 supports rabbitmq (not sleuth stream variant). are you in a position where you cant upgrade whatever is doing that? seems\na rather serious bug which would make the trace useless even if it didnt\ncrash.\nwhat is tracing your boot app?\n. I think you are probably sending spans to the wrong endpoint perhaps? check\nthat first as 2.0 writes v2 format by default and expects to go to the\n/api/v2 endpoint\n. maybe you can paste the trace json (cleaning any private info) as it might\nhelp identify what library is actually setting those things to all the same\nID? Let's carry-on here https://gitter.im/spring-cloud/spring-cloud-sleuth\nand please also mention when you are on gitter if there are any\ncustomizations we should know about.\n. let's use gitter as this issue isn't a good place for chat. PS the lack of parent ID is fine, and shouldn't cause a circular reference. It would if the parent ID was there and was also the same as the span ID. If you have questions about the data model (which isn't sleuth related) you can hop on the normal zipkin channel here https://gitter.im/openzipkin/zipkin. https://github.com/openzipkin/zipkin/pull/2109 should end up killing this in the next release. https://github.com/openzipkin/zipkin/pull/2109 should end up killing this in the next release. Here's an example of this.. in the census library, the SpanContext type does not include the parent-id, so it can't propagate it (not without changing to somehow capture it).\nToday, in Brave, we could make a function like supportsJoin = false similar to ClientServerSameSpan in zipkin-go, which we set in any environment which can contain a node which might not send the parent ID. This issue is more about figuring out the \"what if\" ex knowing what we can resolve server-side from what we can't.\nFor example, dependency graph will be the worst off in a case like this, but we could at least differentiate a root span from one missing parent conventionally. For example, if the span ID is the same as the trace ID, conventionally we know this one is more \"root like\" even if it isn't 100% accurate.\ncc also @jcchavezs since you are now supporting a zipkin tracer :). added a commit to make mysql backfill this (might be the only storage impacted)\ndependency linker will need more smarts to detect this.. not yet sorted.. note to folks who might use this to say \"use my tracing system instead!\"\nB3 is a format in wide use, and so we have some shared stake investigating how this works. Obviously zipkin could drop support for shared spans, but that would orphan the ecosystem. That's precisely why I take time on issues like this.. PS I just looked deeply into Amazon X-Ray. It is mostly compatible with zipkin from a trace ID storage POV.\nThe trace ID includes a version bit, 32bits of epoch seconds and 96 bits of random. This means it is 128bits. Most zipkin systems only sample on lower 64-bits anyway, so supporting X-Ray compatible trace IDs mean changing generation to be the same approach. Span IDs are 64bit, so exact match zipkin.\nLike trace-context only the calling ID is propagated (they call it Parent, we call it X-B3-SpanId), so fixing this issue will also also align with X-Ray integration.\ncc @devinsba. >\n\n@adriancole https://github.com/adriancole nice discovery. You still\nneed to fix to not share the span between client and server I think,\ncorrect?\nwell \"fix\" is relative :) we have to be able to express this is some way\nbecause B3 once upon a time implied sharing. Here's how we do this in brave\nhttps://github.com/openzipkin/brave/blob/master/brave/src/main/java/brave/propagation/Propagation.java#L38\n. @sokac https://github.com/openzipkin/zipkin/pull/1945 on yours. self-note: merge or close. You have hit max connections. You might want to monitor zipkin metrics and\npossibly increase the connection count above 10\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md if\nthis is a volume problem you might habe a separate instance for query vs\ncollector (same code)\n\nHard to tell what the connections are doing. Some normal practice such as\nthread dumps could tell. Check to see if messages are being dropped or\nthreads are hung. Restart is likely needed.\nOn 26 Sep 2017 09:26, \"eshujiushiwo\" notifications@github.com wrote:\n\nHey guys\ni`ve met a problem when i trying get data from the zipkin-ui.\nCannot store spans [34ddfb7bbb28d076.67dc18dedd736d2b<:67dc18dedd736d2b]\ndue to RuntimeException(java.sql.SQLTransientConnectionException:\nHikariPool-1 - Connection is not available, request timed out after\n30003ms.)\nThe logs are below\uff1a\nzipkin.log https://github.com/openzipkin/zipkin/files/1331397/zipkin.log\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1746, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61_uszC9HXHuOmKircNRt8JtFhAu9ks5smFLNgaJpZM4PjjKh\n.\n. maybe hop onto https://gitter.im/openzipkin/zipkin ?\n\nthere are different storage options like cassandra and elasticsearch, and\nchoices depend on what you are comfortable running. You can try different\nones but you also might need to look at your ingest rate and if you need to\nadjust sampling etc.\n. Add screen shot?\nPS: use v2 api as this is a new feature? Easier than having to do a migration of this to v2 later. I don't want to make an api like this as a part of the server api, especially not under v1 namespace. it doesn't impact the UI to do the blocking here or there.\nIn order to deploy this UI change, you will inherit the v2 api anyway, unless you are somehow hosting static assets independently.. is that the case? . > Not sure what you mean. What I'm thinking is that UI/Javascript will read raw spans for a trace using /api/v1/trace/ and post them to http:///api/v1/spans\nI don't want to introduce new features that depend on v1 api. That's\nwhy I mentioned if your concern is about not having v2 api, yet, you\nwould as a side-effect of deploying this update! So basically we\nshouldn't make a long term decision based on a short term upgrade\nconcern.\nEx save /api/v2/trace/ to  POST\n not using /api/v2/spans explicitly in\nthe config entry as some will mount it via proxy maybe to the same\norigin under a different path. Make sense?\n. Sorry to not think of this earlier, but would this experience be ok? Ex we\nhave a saved trace, but this UI wont be able to find saved traces.. Is that\nok? If so, why? If not, why not?\n. Thanks for the info. I kindof like the archive term, as I don't want\nus necessarily to try to make the UI a fan-out system at the moment.\nMaybe we align the property name as such? like archiveEndpoint? Also,\nfolks deploying an archive instance of zipkin can set the property\nzipkin.ui.environment to archive to know that's what they are\nlooking at.\nI basically would like this feature to be easy to explain with some\ncoherent suggestions on setup... and so far I think archive is it. do\nyou agree?\n. you'll want to rebase this on master, as the thing that allows proxies to work was merged. maybe add\n Prevents error in XXXTest\njava.lang.NoClassDefFoundError:\norg/springframework/util/concurrent/SettableListenableFuture\nat zipkin.server.ZipkinHttpCollector.validateAndStoreSpans(ZipkinHttpCollector.java:88)\n\nas an xml comment above the dependency? That way, we can figure out\nlater what's up\n. I can fix the rebase stuff on the way in, no worries. looks like the test is failing:\n[INFO] JS 2.1.1 (Linux 0.0.0) traceToMustache should show archiveReadEndpoint FAILED\n[INFO]  undefined is not an object (evaluating 'modelview.archiveEndpoint.should')\n[INFO]  /home/travis/build/openzipkin/zipkin/zipkin-ui/test/component_ui/traceToMustache.test.js:110:31 <- webpack:///test/component_ui/traceToMustache.test.js:70:30\n[INFO] PhantomJS 2.1.1 (Linux 0.0.0): Executed 82 of 93 (1 FAILED) (0 secs / 0.041 secs). looks like the test is failing:\n[INFO] JS 2.1.1 (Linux 0.0.0) traceToMustache should show archiveReadEndpoint FAILED\n[INFO]  undefined is not an object (evaluating 'modelview.archiveEndpoint.should')\n[INFO]  /home/travis/build/openzipkin/zipkin/zipkin-ui/test/component_ui/traceToMustache.test.js:110:31 <- webpack:///test/component_ui/traceToMustache.test.js:70:30\n[INFO] PhantomJS 2.1.1 (Linux 0.0.0): Executed 82 of 93 (1 FAILED) (0 secs / 0.041 secs). @naoman circle has been hell lately. just ignore it for now as I can't dedicate time towards it. I do spend probably 10hrs a month on various build issues... just need to get some work done.. @naoman circle has been hell lately. just ignore it for now as I can't dedicate time towards it. I do spend probably 10hrs a month on various build issues... just need to get some work done.. @naoman can you confirm that the archive button is absent when there's no url present?. @naoman can you confirm that the archive button is absent when there's no url present?. @jcarres-mdsol so we have had a number of requests for how to achieve permanent storage w/o complicating other things. I've not found a simpler way than this..\nThe notes would be that if you want permanent storage, setup zipkin somewhere (or a zipkin proxy like stackdriver), then point env variables to that. If using a normal zipkin server, the archival one should probably set the environment name to \"archive\" (something you can configure on the UI).\nDoes this clarify the setup? indeed it would be new for a lot of folks, but fairly easy to provision (add a permanent namespace to storage, keep a server running which needn't deal with high volume as it is only archival). @jcarres-mdsol so we have had a number of requests for how to achieve permanent storage w/o complicating other things. I've not found a simpler way than this..\nThe notes would be that if you want permanent storage, setup zipkin somewhere (or a zipkin proxy like stackdriver), then point env variables to that. If using a normal zipkin server, the archival one should probably set the environment name to \"archive\" (something you can configure on the UI).\nDoes this clarify the setup? indeed it would be new for a lot of folks, but fairly easy to provision (add a permanent namespace to storage, keep a server running which needn't deal with high volume as it is only archival). I'm not sure how much impact there is to code hygiene to present a link to\nthe archive repo. It does feel like it is more a feature if one has the\nability to know where the archive repo is without saving a trace to figure\nit out.\nThere's currently code that conditionally shows the environment label when\npresent in configuration. How bad would the code look if a similar approach\nwas used to place a link to the archives where people can see it?\n. I'm not sure how much impact there is to code hygiene to present a link to\nthe archive repo. It does feel like it is more a feature if one has the\nability to know where the archive repo is without saving a trace to figure\nit out.\nThere's currently code that conditionally shows the environment label when\npresent in configuration. How bad would the code look if a similar approach\nwas used to place a link to the archives where people can see it?\n. Naoman I dont think you understand. This change is for the community and a\nvalid request was made. We dont merge single user stuff, so consider it\nlucky someone else is interested!\n. Naoman I dont think you understand. This change is for the community and a\nvalid request was made. We dont merge single user stuff, so consider it\nlucky someone else is interested!\n. >\n\nLet me try to explain one more time. While the request to add an\nadditional url link is valid, its an add-on feature for this change, and\ndoes not block it.\n\nThis is a community project. One of the user/core team members felt this\nwas hacky. Another user asked for an adjustment to the change. No one so\nfar has actually said they want the change actively. We have to be careful\nto make sure code is actually useful as often non-core people disappear\nafter they land something.\nHistorically, UI code is write once, very few go back and maintain changes\nlater. I don't have enough experience with you personally to know if you\nwould actually go back later and do a pull request.\nThe time you have taken to intentionally not answer the question has rid me\nof effort I can spend elsewhere and also yourself as looking at the\nsuggestion wouldn't have been costly. Please be more considerate especially\nif you want others to help merge an unpopular change.\n. >\n\nLet me try to explain one more time. While the request to add an\nadditional url link is valid, its an add-on feature for this change, and\ndoes not block it.\n\nThis is a community project. One of the user/core team members felt this\nwas hacky. Another user asked for an adjustment to the change. No one so\nfar has actually said they want the change actively. We have to be careful\nto make sure code is actually useful as often non-core people disappear\nafter they land something.\nHistorically, UI code is write once, very few go back and maintain changes\nlater. I don't have enough experience with you personally to know if you\nwould actually go back later and do a pull request.\nThe time you have taken to intentionally not answer the question has rid me\nof effort I can spend elsewhere and also yourself as looking at the\nsuggestion wouldn't have been costly. Please be more considerate especially\nif you want others to help merge an unpopular change.\n. @openzipkin/core I personally don't want to go around in explanation circles on this. I actually am ok with the change in general, but not motivated to continue this line of inquiry. If someone else is ok driving this, please do. I'm going to unsubscribe for the time being. @openzipkin/core I personally don't want to go around in explanation circles on this. I actually am ok with the change in general, but not motivated to continue this line of inquiry. If someone else is ok driving this, please do. I'm going to unsubscribe for the time being. can you join https://gitter.im/openzipkin/zipkin ?\nin short, you replace STORAGE_TYPE=mysql for STORAGE_TYPE=elasticsearch\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md#elasticsearch-storage\n. URLs tend to be a bad choice for span names as they end up with variable\nnames in them (which explodes the cardinality). Do you have very long span\nnames that don't have variables encoded? In general, we default to method\nname, or where supported the associated RPC name (ex method name)\n. meant \"RPC name (ex function name)\"\n. This issue was moved to spring-cloud/spring-cloud-sleuth#723. sleuth has means to change the naming convention, you can peek at docs or hop on gitter to ask about it https://gitter.im/spring-cloud/spring-cloud-sleuth. circleci is busted for a different reason (it is very bustable lately). thanks tons!\n. cc @llinder @devinsba @openzipkin/core . >\n\nSo these are the interop models for the out-of-band data?\n\n[zipkin SDK] -> [X-Ray backend] directly\n[zipkin SDK] -> [zipkin collector] -> [X-Ray backend]\n\nTL;DR; correct :)\n\nnit: the component relevant to out-of-band in zipkin lingo is \"reporter\".\nWe don't define the term SDK. http://zipkin.io/pages/architecture.html\nSo, something already recording data in a zipkin format sends directly to\nX-Ray or via a \"collector\". A collector can be any component on a transport\n(streaming or otherwise, zipkin-server or otherwise) that takes standard\nzipkin encoding and pushes it to X-Ray. I'm intentionally being abstract as\npeople have different ways they accept data.\nThe first (direct) technique allows folks for example who are running\nlambdas and want to pay by the trace a way to proceed without any\ninfrastructure. They just align permissions and go. For example, one can\nwrite a lambda and run it inside or outside AWS (reporting to zipkin when\noutside)\n. random note: http://docs.aws.amazon.com/xray/latest/api/API_PutTraceSegments.html The POST format is a list of escaped json, as the doc implies.\nEx.\njson\n{\"TraceSegmentDocuments\": [\n\"{\\\"id\\\": \\\"0b89f1dec76af795\\\", ...\"\n]}. Added an example implementation of trace ID with a time component. Unsurprisingly, it is slower than a fully random ID. However, the scale is still sub microsecond (on my laptop\u2122), and only affects the root span: https://github.com/openzipkin/brave/pull/509\nNext step is to add a converter which proves the concept.. experimental work starting in Brave here https://github.com/openzipkin/brave/pull/510. Thanks to @jcarres-mdsol for making new trace ID provisioning instructions a bit simpler:\n|---- 32 bits for epoc seconds --- | ----- 96 bits for random number --- |\nit can potentially be implemented by:\nHigh 64:\n|---- 32 bits for epoc seconds --- | ----- 32 bits for random number --- |\nLow 64:\n| ----- 64 bits for random number --- |\nOptional cheap sanity check the high 32 bits are epoch seconds\n58000000 = 1476395008 = 2016-10-13 < prior to X-Ray and zipkin supporting 128-bit trace IDs\n60000000 = 1610612736 = 2021-01-14 < of course you can even more future proof. Thanks to @jcarres-mdsol for making new trace ID provisioning instructions a bit simpler:\n|---- 32 bits for epoc seconds --- | ----- 96 bits for random number --- |\nit can potentially be implemented by:\nHigh 64:\n|---- 32 bits for epoc seconds --- | ----- 32 bits for random number --- |\nLow 64:\n| ----- 64 bits for random number --- |\nOptional cheap sanity check the high 32 bits are epoch seconds\n58000000 = 1476395008 = 2016-10-13 < prior to X-Ray and zipkin supporting 128-bit trace IDs\n60000000 = 1610612736 = 2021-01-14 < of course you can even more future proof. thanks, mick!. thanks, mick!. Thanks for the benching. Will take a review sweep today\n. Thanks for the benching. Will take a review sweep today\n.    - changing dependency table to store the individual fields to\n   DependencyLink, rather than the whole list of links as a blob,\n   - refactoring zipkin.storage.cassandra3 to zipkin2.storage.cassandra3\nAh ok will wait on that\n.    - changing dependency table to store the individual fields to\n   DependencyLink, rather than the whole list of links as a blob,\n   - refactoring zipkin.storage.cassandra3 to zipkin2.storage.cassandra3\nAh ok will wait on that\n. Perfect\n. Perfect\n. just added a commit to scrub the v1 compile dep (also preventing re-preparing)\none package change I made was autoconfiguration.. right now, the zipkin-server is still based on the v1 dep (basically it has v2 endpoints bolted on)\nsometime in the near future we'll add a v2 zipkin-server which then removes the whole dep tree on v1 types (and spring boot 1.5). so for now, the autoconfiguration for cassandra3 makes most sense in the v1 package (as its only user is the v1 zipkin server). just added a commit to scrub the v1 compile dep (also preventing re-preparing)\none package change I made was autoconfiguration.. right now, the zipkin-server is still based on the v1 dep (basically it has v2 endpoints bolted on)\nsometime in the near future we'll add a v2 zipkin-server which then removes the whole dep tree on v1 types (and spring boot 1.5). so for now, the autoconfiguration for cassandra3 makes most sense in the v1 package (as its only user is the v1 zipkin server). fyi refactored to reduce commits, but intentionally left the important ones (relevant for performance). ok I have 23 failures locally.. trying to move that number below 20 soon :). >\n\n\n ts_uuid             timeuuid,\n\n\ntrace_id_high       text, // when strictTraceId=false, contains right-most 16 chars if present\n\n\ndid you mean here \"left-most 16 chars\"?\n\n\nyep!\n. 16 failures locally. Down to 10 failures. about 5 failures now... there's still something in the test setup that doesn't fail cleanly as 10 will fail, but if you re-run failures 4 or 5 will fail. According to local docker, there are 4 tests failing. I ran the first in isolation just to verify\nITCassandraStorage$SpanStoreTest#getTraces_duration\nITCassandraStorage$SpanStoreTest#getTraces_endTsAndLookback\nITCassandraStorage$SpanStoreTest#whenSpanTimestampIsMissingClientSendIsPreferred\nITCassandraStorage$SpanStoreTest#getAllServiceNames__allReturned\n./mvnw -Dit.test='ITCassandraStorage$SpanStoreTest#getTraces_duration' -pl zipkin-storage/zipkin2_cassandra clean verify. actually getTraces_duration is the only legit fail. the others fail when getTraces_duration fails, but not when run independently (must be some lack of cleanup somewhere..). I have some local failures after latest fix.. checking to see if they are a test hygiene thing. It appears the test failures are something I can fix without changing the code as they pass when run individually. Thanks @michaelsembwever for getting us this far!\nMore later, hopefully green.. fyi simplified default schema name to \"zipkin2\" as \"zipkin2_cassandra\" was a bit redundant once inside cassandra. ps an earlier change of mine caused a regression. I'll look into it tomorrow morning before working on the spark job. looks green!. ran into guava compatibility issue in spark due to transformAsync. Since the next version of cassandra driver won't have guava anyway, I yanked async composition.  A little polish is needed, but at any rate.. progress. I will try again spark after things prove green or not. ok spark job now works, but I need to fix tests here and there before we can merge either :)\nhelp-wanted on cleaning up the spark job https://github.com/openzipkin/zipkin-dependencies/pull/95. ok spark job now works, but I need to fix tests here and there before we can merge either :)\nhelp-wanted on cleaning up the spark job https://github.com/openzipkin/zipkin-dependencies/pull/95. ok I think tests are passing again, but I have some tech debt to cleanup. also zipkin-dependencies still fails 3 tests.. will swing again later. ok I think tests are passing again, but I have some tech debt to cleanup. also zipkin-dependencies still fails 3 tests.. will swing again later. whoot travis is happy again. whoot travis is happy again. zipkin-dependencies now works.. will cleanup tech debt but on final lap for real. zipkin-dependencies now works.. will cleanup tech debt but on final lap for real. just refactored tests as one failing would kill the rest. each use their own keyspace now. just refactored tests as one failing would kill the rest. each use their own keyspace now. fyi added a commit which pulls out call objects on the read side. The work isn't 100% complete, but I hope the overall flow is easier to read now.. fyi added a commit which pulls out call objects on the read side. The work isn't 100% complete, but I hope the overall flow is easier to read now.. TODO: I think we are missing a spot for all storage providers as our base test classes do not query both by tag and duration.. TODO: I think we are missing a spot for all storage providers as our base test classes do not query both by tag and duration.. did a couple things.. found an interesting bug in a test which had my head scratching. Fixing that made me realize we need to revisit how we handle limit when scanning the spans table.\nAlso, I moved logic around such that async fetching is explicit and only when needed. before, we were fetching in anticipation of needing to do so later. I don't think that's needed anymore as decoding the result set isn't likely to be time consuming. If this proves wrong, happy to re-introduce the early fetch, but let's document and unit test that if we do.. did a couple things.. found an interesting bug in a test which had my head scratching. Fixing that made me realize we need to revisit how we handle limit when scanning the spans table.\nAlso, I moved logic around such that async fetching is explicit and only when needed. before, we were fetching in anticipation of needing to do so later. I don't think that's needed anymore as decoding the result set isn't likely to be time consuming. If this proves wrong, happy to re-introduce the early fetch, but let's document and unit test that if we do.. opened  as https://github.com/openzipkin/zipkin/pull/1780 as I noticed our base tests missed a spot. >\n\nSo FTR: without the pre-emptive asynchronous fetching of the next page,\nthe parsing will stop and block every 5k rows while it fetches the next\npage. Queries that process more than 5k rows are going to be significantly\nslower (the network fetch is the slowest part).\nGood food for thought. since parsing is currently sequential regardless,\nyou are suggesting that \"significantly slower\" is a factor of parsing time\nfor those 5k rows, right? (or specifically however many rows are left\nbefore we made the decision to pipeline)\n\nex. currently, if a request takes 3ms and parsing takes 100us, then if we\nhave 10k rows we take 6200us vs 6100us (or whatever offset). On the other\nhand, we could do pipelining differently which doesn't sequentially parse...\nI'll add a TODO on this topic. I'm not against it, just want to be able to\nmeasure it (this stuff is fun code anyway)\n. added a WIP commit to the end, porting all the cassandra write commands to \"call\" objects. Only raised for the curious as I will replace the commit tomorrow with cleaner code.\nTomorrow, I need to add tests for the call deduper (done internally in InsertServiceSpan). I also need to go back and cleanup (ex make aggregate call do work in parallel, cleanup toStrings etc).. I'd like to merge and release this tomorrow. If that's scary, scream :). @openzipkin/cassandra ok I'm ready to merge this. You might want to check about how pre-computed queries are delimited in the annotation_query column. They are braced with an unlikely character to stop us from accidentally matching on substring in the API. ex bracing like \u2591error\u2591 ensures we don't accidentally match on a tag key named not_error.. ok I've got the dependencies job working.. just waiting for green. Will release 2.3 on master green. Epic help @michaelsembwever and @llinder thanks for all the work on the experimental driver preceding this, including the spark code. One way we could do it is to drop the incoming requests when we notice dispatcher.runningCallsCount()  is at a specific threshold, maybe easy as our in-flight limit (default 64)!\nThis trusts that auto-cancelation based on connect/read/write timeouts work (which defaults to 10s). If there's a possibility of stale in-flight requests otherwise, I guess we'd need something to watchdog in-flight requests to kill them if they are over the various timeouts.\ncc the usual okhttp folks\n@yschimke @swankjesse @jakewharton . One way we could do it is to drop the incoming requests when we notice dispatcher.runningCallsCount()  is at a specific threshold, maybe easy as our in-flight limit (default 64)!\nThis trusts that auto-cancelation based on connect/read/write timeouts work (which defaults to 10s). If there's a possibility of stale in-flight requests otherwise, I guess we'd need something to watchdog in-flight requests to kill them if they are over the various timeouts.\ncc the usual okhttp folks\n@yschimke @swankjesse @jakewharton . There are two main cases in ES anyway, the server fails not nicely (ex out\nof disk) and the server fails nicely (response that says backoff). The\nformal ES client has a backoff strategy for the latter. The former can\nstill happen and did in the issue leading to this one.\nRegardless, one simplification we can make here is that we can choose to\ndrop new requests or drop in-flight ones on overload.\nIn either case, and we don't need to buffer in expectation of a transient\nfailure. For example, first priority is not dying (OOM), though later more\nadvanced things could be done.\n. There are two main cases in ES anyway, the server fails not nicely (ex out\nof disk) and the server fails nicely (response that says backoff). The\nformal ES client has a backoff strategy for the latter. The former can\nstill happen and did in the issue leading to this one.\nRegardless, one simplification we can make here is that we can choose to\ndrop new requests or drop in-flight ones on overload.\nIn either case, and we don't need to buffer in expectation of a transient\nfailure. For example, first priority is not dying (OOM), though later more\nadvanced things could be done.\n. Honestly, I think we should start with dropping on backlog and go from\nthere. The test is easy.. crap out a cluster and throw traffic at zipkin.\nuncrap the cluster and the drop metrics should stop.\n. Honestly, I think we should start with dropping on backlog and go from\nthere. The test is easy.. crap out a cluster and throw traffic at zipkin.\nuncrap the cluster and the drop metrics should stop.\n. downside of dropping on backlog (ex ready queue > 0 or some figure\ndefaulting to zero) is that we will still have a by default 64\nrequests in flight on a crap cluster. however, the change to improve\nthat (ex coordinate on http response) could layer a filter or other\ncode to throttle back. If 64 requests OOM the server anyway, you can\nchange that to a lower value today anyway. At any rate a healthy\nbackoff signal isn't always present, so regardless we need a knob like\ndrop on backlog as otherwise whoever's queue it is will lead to OOM.\n. downside of dropping on backlog (ex ready queue > 0 or some figure\ndefaulting to zero) is that we will still have a by default 64\nrequests in flight on a crap cluster. however, the change to improve\nthat (ex coordinate on http response) could layer a filter or other\ncode to throttle back. If 64 requests OOM the server anyway, you can\nchange that to a lower value today anyway. At any rate a healthy\nbackoff signal isn't always present, so regardless we need a knob like\ndrop on backlog as otherwise whoever's queue it is will lead to OOM.\n. https://github.com/openzipkin/zipkin/pull/1765 I am soak testing this, but the server already survives a lot longer. basically, set ES_MAX_REQUESTS=2 and throw lots traffic at it. Before this change, it would reliably OOM. It is harder to OOM if you have the default max requests of 64 :P. https://github.com/openzipkin/zipkin/pull/1765 I am soak testing this, but the server already survives a lot longer. basically, set ES_MAX_REQUESTS=2 and throw lots traffic at it. Before this change, it would reliably OOM. It is harder to OOM if you have the default max requests of 64 :P. let's try simple first. I've put a semaphore in, and zipkin 2.2 has some nice things for grafana etc. give it a try and let's iterate from there..\nhere's an example of a forced surge and recovery though I'm sure we can kill the server if we try hard enough.\n\n. let's try simple first. I've put a semaphore in, and zipkin 2.2 has some nice things for grafana etc. give it a try and let's iterate from there..\nhere's an example of a forced surge and recovery though I'm sure we can kill the server if we try hard enough.\n\n. >\n\nHardest part with this is that the death is random. So who knows if we'll\nfind the random event again ;)\nwell thanks for dying earlier, as I think the code is a bit safer now, even\nif not bulletproof. cheers!\n. now it is only one line per malformed message. now it is only one line per malformed message. cc @openzipkin/elasticsearch . cc @openzipkin/elasticsearch . actually I was able to crash the server even with this when setting ES_MAX_REQUESTS=2 and 5 concurrent senders of 10 spans. Took about 5m to kill the server.\n\nbash\nzipkin                      | 2017-10-10 12:42:26.056  INFO 5 --- [nio-9411-exec-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization completed in 138 ms\nzipkin                      | OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000f8f2f000, 107704320, 0) failed; error='Cannot allocate memory' (errno=12)\nzipkin                      | #\nzipkin                      | # There is insufficient memory for the Java Runtime Environment to continue.\nzipkin                      | # Native memory allocation (mmap) failed to map 107704320 bytes for committing reserved memory.\nzipkin                      | # An error report file with more information is saved as:\nzipkin                      | # /zipkin/hs_err_pid5.log\nzipkin exited with code 1. actually I was able to crash the server even with this when setting ES_MAX_REQUESTS=2 and 5 concurrent senders of 10 spans. Took about 5m to kill the server.\nbash\nzipkin                      | 2017-10-10 12:42:26.056  INFO 5 --- [nio-9411-exec-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization completed in 138 ms\nzipkin                      | OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000f8f2f000, 107704320, 0) failed; error='Cannot allocate memory' (errno=12)\nzipkin                      | #\nzipkin                      | # There is insufficient memory for the Java Runtime Environment to continue.\nzipkin                      | # Native memory allocation (mmap) failed to map 107704320 bytes for committing reserved memory.\nzipkin                      | # An error report file with more information is saved as:\nzipkin                      | # /zipkin/hs_err_pid5.log\nzipkin exited with code 1. ran with less logging per https://github.com/openzipkin/zipkin/pull/1766\nI simulated a surge, and definitely dropped spans works. \nI watched things recover via prometheus https://github.com/openzipkin/docker-zipkin/pull/135 and a grafana dashboard https://grafana.com/dashboards/1598/\n@Logic-32 I'm going to merge this and cut 2.2.0 (which has the prometheus setup I used). Please setup a dashboard and alerts.. also you probably want to add the elasticsearch queue length to whatever that is. There are probably many places to improve, but I hope this gets things better.. ran with less logging per https://github.com/openzipkin/zipkin/pull/1766\nI simulated a surge, and definitely dropped spans works. \nI watched things recover via prometheus https://github.com/openzipkin/docker-zipkin/pull/135 and a grafana dashboard https://grafana.com/dashboards/1598/\n@Logic-32 I'm going to merge this and cut 2.2.0 (which has the prometheus setup I used). Please setup a dashboard and alerts.. also you probably want to add the elasticsearch queue length to whatever that is. There are probably many places to improve, but I hope this gets things better.. closed via 4c939617a92dada9e25bff5e11f793129c9f4b3a. dupe of https://github.com/openzipkin/openzipkin.github.io/issues/62. servlet, spring boot and dropwizard based applications all have\ninstrumentation available. We don't have instrumentation at the container\nlayer, such as tomcat or any other thing at the moment. What this means is\ntracing is configured at the app layer (ex servlet filter). Definitely\ntomcat is fine and I've heard of folks using WebLogic. It is almost\ncertainly the case folks are using JBoss with brave. I don't know about\nTibco BW. https://github.com/openzipkin/brave-webmvc-example\nOn Wed, Oct 11, 2017 at 7:36 PM prashantnitt07 notifications@github.com\nwrote:\n\nhi,\ncan be deploy zipkin TIBCO BW . please check and reply ,\nWe have four type to JAVA based ..\nTomcat\nWeblogic\nJboss\nTIBCO BW\nplease reply all 4 java type supported\n\nwhat is step for installation of ZIPKIN on linux machine .\nplease send me step or guide document\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1767, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD618zu6ZGeSQILUTHPZ73fjrE9ml7Yks5srKhCgaJpZM4P1VAg\n.\n. hi, can you check the new storage implementation vs ScyllaDB? probably will make more sense to focus on that. The storage type is \"cassandra3\" and its code is under zipkin-storage/zipkin2_cassandra. >\n@adriancole https://github.com/adriancole Scylla only claims to be a\nCassandra 2 implementation. Is there any known protocol difference between\nCassandra 2 and 3?\nIt is a combination of v4 protocol and SASI indexing. Is there a layering\nsolution for SASI with Scylla?\nPlus, do I have to upgrade all client to make them generate v2 spans?\nNope. All recent versions of zipkin will convert depending on the\nunderlying backend (Ex v1 -> v2 or v2 -> v1) no client change is needed,\nthough upgrading to a client that supports v2 model is good long term.\n. >\n@adriancole https://github.com/adriancole Scylla only claims to be a\nCassandra 2 implementation. Is there any known protocol difference between\nCassandra 2 and 3?\nIt is a combination of v4 protocol and SASI indexing. Is there a layering\nsolution for SASI with Scylla?\nPlus, do I have to upgrade all client to make them generate v2 spans?\nNope. All recent versions of zipkin will convert depending on the\nunderlying backend (Ex v1 -> v2 or v2 -> v1) no client change is needed,\nthough upgrading to a client that supports v2 model is good long term.\n. >\nNot that I know of. According to http://www.scylladb.com/2017/\n07/27/materialized-views-preview-scylla-2-0/ and scylladb/scylla#2203\nhttps://github.com/scylladb/scylla/issues/2203, Scylla does not support\nSASI.\n\nopened https://github.com/openzipkin/zipkin/issues/1805\n. >\n\nNot that I know of. According to http://www.scylladb.com/2017/\n07/27/materialized-views-preview-scylla-2-0/ and scylladb/scylla#2203\nhttps://github.com/scylladb/scylla/issues/2203, Scylla does not support\nSASI.\n\nopened https://github.com/openzipkin/zipkin/issues/1805\n. any updates here?. any updates here?. can you attach a screen shot of what you want to see (in osx I use Preview\nannotations to add text)\nex the span detail screen currently shows timestamps, and I think you might\nmean on the trace graph view...\n. the server is spring boot. I believe you can use env SERVER_ADDRESS or any\nother way spring boot accepts config\nhttps://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-external-config.html#boot-features-external-config-multi-profile-yaml\n. the server is spring boot. I believe you can use env SERVER_ADDRESS or any\nother way spring boot accepts config\nhttps://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-external-config.html#boot-features-external-config-multi-profile-yaml\n. Hi, oldest data is dropped when the span count goes over 500K (by default).\nNext time, please join gitter or use stack overflow for questions!\nhttps://github.com/openzipkin/zipkin/blob/4c939617a92dada9e25bff5e11f793129c9f4b3a/zipkin-server/src/main/resources/zipkin-server-shared.yml#L57\n. Hi, oldest data is dropped when the span count goes over 500K (by default).\nNext time, please join gitter or use stack overflow for questions!\nhttps://github.com/openzipkin/zipkin/blob/4c939617a92dada9e25bff5e11f793129c9f4b3a/zipkin-server/src/main/resources/zipkin-server-shared.yml#L57\n. can someone paste a reasonably complex trace? simply removing the span.inFilters > 0 makes the box work again, but it would be ideal to have a trace with several services and multiple levels of nesting to ensure this works cc @igorwwwwwwwwwwwwwwwwwwww . we have a large trace now.. in the zipkin-ui/testdata area. Sounds familiar to me.. are you running latest?\nOn 4 Nov 2017 00:32, \"lhsm\" notifications@github.com wrote:\n\nHi.\nZipkin UI starts with error\nerror_handler.js?934c:54 EXCEPTION: Cannot read property 'timestamp' of\nundefined ErrorHandler.handleError @ error_handler.js?934c:54\nerror_handler.js?934c:59 ORIGINAL STACKTRACE: ErrorHandler.handleError @\nerror_handler.js?934c:59 error_handler.js?934c:60 TypeError: Cannot read\nproperty 'timestamp' of undefined at eval (webpack-internal:///165:117:70)\nat Array.sort (native) at Array.sort (webpack-internal:///875:21:15) at\nTrace.sortTrace (webpack-internal:///165:117:23) at eval\n(webpack-internal:///165:120:23) at Array.forEach () at\nTrace.sortTrace (webpack-internal:///165:119:27) at eval\n(webpack-internal:///165:120:23) at Array.forEach () at\nTrace.sortTrace (webpack-internal:///165:119:27) at Trace.getSortedSpans\n(webpack-internal:///165:80:14) at new Trace (webpack-internal:///165:55:32)\nat eval (webpack-internal:///165:157:65) at Array.map ()\nIf i understand correctly expected structure of traces (response of\napi/v1/traces) is ..{\"traceId\": \"trace\", ..., \"annotations\"} .. -\nannotations is mandatory.\nBut in my case trace looks like [ [ { \"traceId\": \"fca49abacae88255\",\n\"id\": \"fca49abacae88255\", \"name\": \"\", \"timestamp\": 1509704101447000,\n\"duration\": 55875, \"annotations\": [ { \"timestamp\": 1509704101502875,\n\"value\": \"ss\", \"endpoint\": { \"serviceName\": \"\", \"ipv4\": \"\", \"port\": 8080 }\n} ] }, { \"traceId\": \"fca49abacae88255\", \"id\": \"8382a3fc6365f507\", \"name\":\n\"\", \"parentId\": \"fca49abacae88255\", \"timestamp\": 1509704101455000,\n\"duration\": 22310, \"binaryAnnotations\": [ { \"key\": \"OUTPUT_MESSAGE\",\n\"value\": \"\", \"endpoint\": { \"serviceName\": \"\", \"ipv4\": \"\", \"port\": 8080 } },\n{ \"key\": \"lc\", \"value\": \"unknown\", \"endpoint\": { \"serviceName\": \"\", \"ipv4\":\n\"\", \"port\": 8080 } } ] } ] ]\n\"Traditional\" ui works fine with that trace\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1776, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD616WWw622R1AtIeZP9qqGzGAhpn5zks5sy0A0gaJpZM4QRW0q\n.\n. Sounds familiar to me.. are you running latest?\n\nOn 4 Nov 2017 00:32, \"lhsm\" notifications@github.com wrote:\n\nHi.\nZipkin UI starts with error\nerror_handler.js?934c:54 EXCEPTION: Cannot read property 'timestamp' of\nundefined ErrorHandler.handleError @ error_handler.js?934c:54\nerror_handler.js?934c:59 ORIGINAL STACKTRACE: ErrorHandler.handleError @\nerror_handler.js?934c:59 error_handler.js?934c:60 TypeError: Cannot read\nproperty 'timestamp' of undefined at eval (webpack-internal:///165:117:70)\nat Array.sort (native) at Array.sort (webpack-internal:///875:21:15) at\nTrace.sortTrace (webpack-internal:///165:117:23) at eval\n(webpack-internal:///165:120:23) at Array.forEach () at\nTrace.sortTrace (webpack-internal:///165:119:27) at eval\n(webpack-internal:///165:120:23) at Array.forEach () at\nTrace.sortTrace (webpack-internal:///165:119:27) at Trace.getSortedSpans\n(webpack-internal:///165:80:14) at new Trace (webpack-internal:///165:55:32)\nat eval (webpack-internal:///165:157:65) at Array.map ()\nIf i understand correctly expected structure of traces (response of\napi/v1/traces) is ..{\"traceId\": \"trace\", ..., \"annotations\"} .. -\nannotations is mandatory.\nBut in my case trace looks like [ [ { \"traceId\": \"fca49abacae88255\",\n\"id\": \"fca49abacae88255\", \"name\": \"\", \"timestamp\": 1509704101447000,\n\"duration\": 55875, \"annotations\": [ { \"timestamp\": 1509704101502875,\n\"value\": \"ss\", \"endpoint\": { \"serviceName\": \"\", \"ipv4\": \"\", \"port\": 8080 }\n} ] }, { \"traceId\": \"fca49abacae88255\", \"id\": \"8382a3fc6365f507\", \"name\":\n\"\", \"parentId\": \"fca49abacae88255\", \"timestamp\": 1509704101455000,\n\"duration\": 22310, \"binaryAnnotations\": [ { \"key\": \"OUTPUT_MESSAGE\",\n\"value\": \"\", \"endpoint\": { \"serviceName\": \"\", \"ipv4\": \"\", \"port\": 8080 } },\n{ \"key\": \"lc\", \"value\": \"unknown\", \"endpoint\": { \"serviceName\": \"\", \"ipv4\":\n\"\", \"port\": 8080 } } ] } ] ]\n\"Traditional\" ui works fine with that trace\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1776, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD616WWw622R1AtIeZP9qqGzGAhpn5zks5sy0A0gaJpZM4QRW0q\n.\n. This could be a bug, when all labels are empty or unknown. Is it\nintentional that you have a trace with no service or span name?\n\nOn 14 Nov 2017 7:01 am, \"Bruno Bonfils\" notifications@github.com wrote:\n\nSame issue here with the latest version of openzipkin/zipkin-ui docker\nimage\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1776#issuecomment-344088689,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6159pOJVbkTzImtR9JRXUmuXvaxiMks5s2MpJgaJpZM4QRW0q\n.\n. oh you are referring to https://github.com/openzipkin/zipkin-ui\n\nsomeone can fix the bug, but the project is likely to be canceled because no one has been maintaining it.. see also https://github.com/openzipkin/zipkin/issues/1577 and https://github.com/openzipkin/docker-zipkin/issues/160\nAt any rate, this issue should be moved to https://github.com/openzipkin/zipkin-ui. aye. FWIW I don't expect everyone to be able to use an agent, so almost\ncertainly js will continue to have its own library-focused post mechanisms.\nIn zipkin-js this is already implemented albeit not as advanced as java\n(already does http batching and compression is actually quite simple)\n. for example, browsers and native apps won't be able to use an agent,\nneither will android (java) or most clients.. Not suggesting we don't\ndo an agent, just reminding it isn't a magic bullet for all\napplications.\n. Fwiw I think recent py_zipkin allows bundling multiple spans in same\nmessage. Might require some work to do it smartly\nOn 19 Nov 2017 13:22, \"Haochen Tong\" notifications@github.com wrote:\n\n@eirslett https://github.com/eirslett Seems that fluentd cannot combine\nmultiple JSON document to a list and send the list in one message. Reducing\nthe amount of Kafka messages is important to me.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1778#issuecomment-345495219,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xjzFZnkYXpsQjAxIo5hottxSkU6ks5s38kbgaJpZM4QSoCV\n.\n. Fwiw I think recent py_zipkin allows bundling multiple spans in same\nmessage. Might require some work to do it smartly\n\nOn 19 Nov 2017 13:22, \"Haochen Tong\" notifications@github.com wrote:\n\n@eirslett https://github.com/eirslett Seems that fluentd cannot combine\nmultiple JSON document to a list and send the list in one message. Reducing\nthe amount of Kafka messages is important to me.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1778#issuecomment-345495219,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xjzFZnkYXpsQjAxIo5hottxSkU6ks5s38kbgaJpZM4QSoCV\n.\n. I think we should at least consider fluentd as a custom agent is a long\nterm responsibility, and finn (where eirik used to work) is a large site.\nAlso, it can help people migrate off scribe. Right now, we have a problem\nwhich is people are pinned to scribe and this carries long-term weight on\nthe project. Switching to fluentd can help with this, as it could localize\nthe thrift+scribe dep to fluentd, which needs it anyway. Fluentd could emit\nto zipkin on a more supportable protocol.\n\nFinally, it can make it easier to adopt.. custom agents are even less\nfamiliar than a plugin into an existing agent.\nIf fluentd had (re-)bundling capability, are there any other holdbacks?\n@jcchavezs would you be open to porting your work to a fluentd plugin? Does\nanyone have some time for due diligence on this option? Personally, I am\nvery interested in this, for scribe deprecation alone!\nhttps://www.fluentd.org/\n. I think we should at least consider fluentd as a custom agent is a long\nterm responsibility, and finn (where eirik used to work) is a large site.\nAlso, it can help people migrate off scribe. Right now, we have a problem\nwhich is people are pinned to scribe and this carries long-term weight on\nthe project. Switching to fluentd can help with this, as it could localize\nthe thrift+scribe dep to fluentd, which needs it anyway. Fluentd could emit\nto zipkin on a more supportable protocol.\nFinally, it can make it easier to adopt.. custom agents are even less\nfamiliar than a plugin into an existing agent.\nIf fluentd had (re-)bundling capability, are there any other holdbacks?\n@jcchavezs would you be open to porting your work to a fluentd plugin? Does\nanyone have some time for due diligence on this option? Personally, I am\nvery interested in this, for scribe deprecation alone!\nhttps://www.fluentd.org/\n. https://spring.io/blog/2015/12/10/spring-boot-memory-performance is another analysis. https://spring.io/blog/2015/12/10/spring-boot-memory-performance is another analysis. thank you. you might notice we made large cleanup of (v1) code with only\nporting of integration tests left todo. features like this are ok now to\nstart if you want. particularly the collector is now separate jar so we can\nconsider things even past java 8.\n. ps counter-rationale is that I suspect http is the only impl that will fully support reads and writes. udp sounds like something that would only work as a span collector.. I rebased this on latest master and added a commit showing how to lazy initiate the connection, also for example, how to use it in the check() function which is used for health checks. Some of the earlier comments still apply (ex need the META-INF directory and properties could be better based on another), but hopefully this will get you further along. probably next step is to carve out a test that uses influxdb.. start by verifying .check() works (i.e. it works by noticing the server isn't available (by pointing to a wrong url) and also works when it is available. We can use docker(testcontainers) to start influx.. you can look at RabbitMQCollectorRule or similar for an example. lemme know if you need a hand. @gianarb I kindof rejigged things to get the most test things ready to go.\nOne thing I noticed is that the DB driver is synchronous eventhough underneath it uses retrofit which allows asynchronous calls. \nex you can see here that it wraps an async call and invokes it synchronously\nhttps://github.com/influxdata/influxdb-java/blob/47e768930ba6144dfef8abd9c737471c5ee154b7/src/main/java/org/influxdb/impl/InfluxDBImpl.java#L238\nThe problem is that this limits us as for example, there's response mapping code ^^ that we would need, but it would be a limitation to artificially have to invoke things synchronously especially since there are asynchronous mechanisms available. Can you check upstream if it is possible to expose an asynchronous api we can use instead?. ps if it isn't possible to customize the upstream client, it could possibly be less work to just write the http requests directly like we do with okhttp. that's because our zipkin2.Call object has mechanisms such as map and flatMap we'd need to implement the api asynchronously. Thanks for the update on all your hard work! I'll swing the axe on this a little this weekend and report back.. Micros! (But anyway there are two issues.. retrieving the unit sent, and\nquerying. In the case of duration, both units are micros. For span\ntimestamp, the query resolution is millis, but still need to retrieve in\noriginal form (micros)\nOn 17 Dec 2017 8:04 am, \"Chris Goller\" notifications@github.com wrote:\n@goller commented on this pull request.\nIn zipkin-storage/influxdb/src/main/java/zipkin2/storage/\ninfluxdb/InfluxDBSpanConsumer.java\nhttps://github.com/openzipkin/zipkin/pull/1781#discussion_r157354645:\n\n\npublic Call accept(List spans) {\nif (spans.isEmpty()) return Call.create(null);\n+\nBatchPoints batch = BatchPoints\n.database(storage.database())\n.retentionPolicy(storage.retentionPolicy())\n.build();\nfor (Span span : spans) {\nPoint point = Point\n.measurement(storage.measurement())\n.tag(\"trace_id\", span.traceId())\n.tag(\"id\", span.id())\n.tag(\"parent_id\", span.parentId() == null ? span.id() :\nspan.parentId())\n.tag(\"name\", span.name())\n.tag(\"service_name\", serviceName(span))\n.addField(\"duration_ns\", span.duration() * 1000)\n\n\nthat's the go dev in me... I'll change that to milliseconds.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1781#discussion_r157354645, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610mPgvkN0ZJMNufBYHoUVnBBulCaks5tBFqcgaJpZM4QUa6u\n.\n. > Do you think we need to have different tags for both local and remote\nservice names? That'll add more cardinality to the database. Do the queries\nneed to specify a difference between local and remote?\nAt the very least, we need to reconstruct the input that was sent.\nOtherwise other tools such as dependency linking wont work. Usually the raw\nendpoint fields are stored regardless as we need users to see what they\nwrote.\nRemote service is being rediscussed here\nhttps://github.com/openzipkin/zipkin/issues/1794\n. Ps have a look at ES code. One thing we do is a separate field for\nindex/search timestamp, which always has a value in millis vs the actual\ntimestamp. With two fields you can eliminate problems like how to handle\nnull when exists in raw data\n. >\n\nIf we set parent_id to id I can get all the relationships in one query.\nIf parent_id is not set then I'm not able to get back the root span in\nthe count.\nIs there another way to accomplish this? parentid == id has caused a fair\namount of bugs, and when people look at the data they will be tempted to\nassume that a root span is where parentid == id (buggy mindset). What would\nhappen if parentid = 0 (as opposed to unset)?\n. PS the most common problem with people doing parent ID = span ID is them accidentally causing a cycle when they assume that's how zipkin expects the data. While easy enough to resolve, this mistaken assumption has caused troubleshooting issues in dependency graph parsing and clock skew correction. We've so far avoided having to compromise by sealing this practice in a storage layer before, so I really want to make sure there's no other way as almost guaranteed this will cause problems myself or others will have to answer later.. PS if you rebase on latest zipkin, we allow parentID=0L to effectively mean null (root span). Hope this can help. a year has passed since this opened. Feel free to re-open later. we have some ASF stuff to do, so easier to inventory current work. you mean ZIPKIN_UI_ENVIRONMENT=foo doesn't work?\n\nOn Thu, Nov 9, 2017 at 4:35 PM, Simon Dean notifications@github.com wrote:\n\nIt\u2019s great how much Zipkin config can be set via environment variables. At\nthe moment it\u2019s not possible to set the environment name shown in the\nZipkin UI.\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-\nserver/src/main/resources/zipkin-server-shared.yml#L130\nWould it be possible to add an end bar for setting the environment\nname/label?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1783, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61zHzRQHT8WlCpHjcQX8xX12G3OSuks5s0rligaJpZM4QXmgg\n.\n. lemme know if the suggestion didn't work. fixed in latest zipkin/zipkin-reporter/brave. \ncool. Just following up, if you enable server log debug you can see this error\n\nEx.\njava -jar zipkin.jar  --logging.level.zipkin=DEBUG --logging.level.zipkin2=DEBUG\nproduces\n```\n2017-11-12 19:27:49.527 DEBUG 37505 --- [nio-9411-exec-1] zipkin.server.ZipkinHttpCollector        : Cannot decode spans due to IllegalArgumentException(1.1 should be a 1 to 32 character lower-hex string with no prefix reading List from json)\njava.lang.IllegalArgumentException: 1.1 should be a 1 to 32 character lower-hex string with no prefix reading List from json\n```. fyi @michaelsembwever @llinder. pps of course I also verified cassandra still works by manually starting a zipkin server with storage type set to both versions. guessing this is because span.timestamp isn't set (\"sr\", \"ss\" aren't used\nin v2 format. timestamp/duration are)\n. PS this can be done for a small amount of data in the server. The problem\n(and reason why there is a spark job) is because a large amount of data\nusually implies crashing the server to try and load a days worth.\n. Context here is to provide chinese language support\n. PS this is a pretty big bug I caused (in the hot off the press new cassandra impl).\nUntil this goes out a patch release, any first to fire that redid their schema yesterday will have very hot partitions (pun very intended). There's no schema change here, just we aren't slamming it anymore.\nThanks to @michaelsembwever for talking me through this.\ncc @openzipkin/cassandra. out in 2.3.1. right, so the span name is attached to a local service (the services query\nmixes local and remote service when choosing a span name). I wonder if we\nshouldn't have a separate drop-down for remoteService? Or maybe the mix is\nok.. what do you think?\n. >\n\nI think it is better that there is a UI that allows one to \"move up\"\ntowards parent spans or \"down\" to children.\nmind raising a separate issue? maybe with a screen shot with apple preview\nannotation sketch (or your favorite technique) to describe this?\n. This issue came up again in #2337 I think that end users are paying a lot for this, and we should simply stop writing indexes for remote and local service names against the same query conditions.\n\nOne of the more damaging things conceptually is when one conflates local and remote service names, so what we do in the services index actually contributes to this.\nIf we do stop indexing the spans against a remote service name, we will lose the ability to search for remote services.. unless we introduce another query which is remoteService=X. I'm personally still better with this as we reduce expense and in some cases restore usability meanwhile.. re-pinging @openzipkin/core @openzipkin/ui @openzipkin/cassandra as IMHO we can't keep tripping over the same problem year after year.\nIf you are in support of refining the service/span query to local only, please thumbs up this issue. If you don't want this change, thumbs down. If you are ok with this with a caveat such as an additional query option, please add a comment instead of down-thumbing. In any case, if you thumbs down, please make a comment why.. >\n\nIf we refine this to local only then I would vote for introducing another\nquery. Reasoning is that we often call services we don't own and being able\nto search on these remote services could be helpful in some cases.\nThanks @llinder. To dig into this, can you give an example query that you\nwould use? I'm assuming you'd use remoteServiceName the same way as if it\nwere a tag. A realistic query based on your test data would be fantastic\n. so if we did the split thing I think it would help reduce the cardinality of @openzipkin/cassandra data considerably. What would happen is that it would be like this:\n\nNow, we have:\nlocalServiceName + \"\" (for /services endpoint)\nlocalServiceName + spanName (for /spans?serviceName=X endpoint)\nremoteServiceName + \"\" (for new /services endpoint)\nremoteServiceName + spanName (for /spans?serviceName=Y endpoint)\nThe proposal would have less cardinality as we wouldn't index remoteServiceName + spanName:\nlocalServiceName + \"\" (for /services endpoint)\nlocalServiceName + spanName (for /spans?serviceName=X endpoint)\nremoteServiceName + \"\" (for new /remoteServices endpoint)\nThe result would be..\n/services -> only local serviceNames\n/remoteServices -> new: only remote serviceNames for auto-complete\n/spans?remoteServiceName=X -> new: to restore functionality @llinder mentioned\nI think the impl would be somewhat easy and a good idea to do now vs later as we are already adding new autocomplete endpoints for tags. cc @openzipkin/ui . ps thinking about this it might be better to make a /remoteServices?serviceName=X endpoint\nthat way search is dependent on the local endpoint that called the remote one. otherwise it would apply to any span in the trace (any of the perhaps hundreds of services involved). \nit really depends on the intent of the query. PS this particular line of change will not help with instrumentation which accidentally create high cardinality span names. For example, it might not be obvious that client and server documents are sent separately. each document includes a span name and would be indexed regardless of this change. If someone is seeing high cardinality data in span names another solution is needed. Likely deleting the span name that is high cardinality as they should never be. (ex this is why we default client span names to the http method).. PS this particular line of change will not help with instrumentation which accidentally create high cardinality span names. For example, it might not be obvious that client and server documents are sent separately. each document includes a span name and would be indexed regardless of this change. If someone is seeing high cardinality data in span names another solution is needed. Likely deleting the span name that is high cardinality as they should never be. (ex this is why we default client span names to the http method).. Thanks! Can you attempt a unit test to ensure this stays fixed?. ping (aka nag :) ). ping (aka nag :) ). test probably means extracting an exported function for setupSpan like we do for showSpans. then you can test it in trace.test.js\nlemme know if you want more clues. test probably means extracting an exported function for setupSpan like we do for showSpans. then you can test it in trace.test.js\nlemme know if you want more clues. Thanks and good work!. Will look into it. Note latest version is 2.3.1. Is there a reason you\ncan't upgrade?\nOn 15 Nov 2017 12:15 am, \"Scott Rankin\" notifications@github.com wrote:\n\nHello,\nI have been running Zipkin server 1.28 against AWS ElasticSearch (2.3).\nToday I tried to upgrade to 1.29, and I get the below error in stdout\n(domain redacted). I'm running Zipkin via Docker, and passing the following\nconfiguration:\n\"STORAGE_TYPE\": \"elasticsearch\",\n  \"ES_HOSTS\": \"https://my.es.index\",\n  \"AWS_ACCESS_KEY_ID\": \"ACCESSKEYHERE\",\n  \"AWS_SECRET_ACCESS_KEY\": \"SECRETKEYHERE\",\nwhich has been working fine since Zipkin 1.23. I can't seem to find any\nindication of a change to the signing requirements in 1.29 - am I missing\nanything?\nThanks!\nScott\nException in thread \"OkHttp Dispatcher\" java.lang.IllegalStateException: The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'GET\n/_cluster/health/zipkin%3Aspan-%2A\nhost:search-zipkin-sandbox-XXXXXXXXXXXXXX.amazonaws.com\nx-amz-date:20171114T155629Z\nhost;x-amz-date\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20171114T155629Z\n20171114/us-west-2/es/aws4_request\nbf52418dfd335f36303b4b3b7ed80a178ea587c8ff95773ba66b65fbae4c7cf6'\nat zipkin.autoconfigure.storage.elasticsearch.aws.AWSSignatureVersion4.intercept(AWSSignatureVersion4.java:69)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)\n  at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:45)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)\n  at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)\n  at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)\n  at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:120)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)\n  at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:185)\n  at okhttp3.RealCall$AsyncCall.execute(RealCall.java:135)\n  at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n  at java.lang.Thread.run(Thread.java:748)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1796, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD617xSZaIaqJp8ihFGTy8LMr3IXWqAks5s2bypgaJpZM4QdmD2\n.\n. :) understand. I will still smoke check it anyway and give an answer.\nPretty sure latest or at least more recent is in prod cc @devinsba\n\nOn 15 Nov 2017 7:19 am, \"Scott Rankin\" notifications@github.com wrote:\nI was just going in order out of an abundance of caution. I\u2019ll try 2.3\ntomorrow morning.\nSent from my iPhone\n\nOn Nov 14, 2017, at 6:12 PM, Adrian Cole notifications@github.com wrote:\nWill look into it. Note latest version is 2.3.1. Is there a reason you\ncan't upgrade?\nOn 15 Nov 2017 12:15 am, \"Scott Rankin\" notifications@github.com wrote:\n\nHello,\nI have been running Zipkin server 1.28 against AWS ElasticSearch (2.3).\nToday I tried to upgrade to 1.29, and I get the below error in stdout\n(domain redacted). I'm running Zipkin via Docker, and passing the\nfollowing\nconfiguration:\n\"STORAGE_TYPE\": \"elasticsearch\",\n\"ES_HOSTS\": \"https://my.es.index\",\n\"AWS_ACCESS_KEY_ID\": \"ACCESSKEYHERE\",\n\"AWS_SECRET_ACCESS_KEY\": \"SECRETKEYHERE\",\nwhich has been working fine since Zipkin 1.23. I can't seem to find any\nindication of a change to the signing requirements in 1.29 - am I\nmissing\nanything?\nThanks!\nScott\nException in thread \"OkHttp Dispatcher\" java.lang.IllegalStateException:\nThe request signature we calculated does not match the signature you\nprovided. Check your AWS Secret Access Key and signing method. Consult the\nservice documentation for details.\nThe Canonical String for this request should have been\n'GET\n/_cluster/health/zipkin%3Aspan-%2A\nhost:search-zipkin-sandbox-XXXXXXXXXXXXXX.amazonaws.com\nx-amz-date:20171114T155629Z\nhost;x-amz-date\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20171114T155629Z\n20171114/us-west-2/es/aws4_request\nbf52418dfd335f36303b4b3b7ed80a178ea587c8ff95773ba66b65fbae4c7cf6'\nat zipkin.autoconfigure.storage.elasticsearch.aws.AWSSignatureVersion4.\nintercept(AWSSignatureVersion4.java:69)\nat okhttp3.internal.http.RealInterceptorChain.proceed(\nRealInterceptorChain.java:92)\nat okhttp3.internal.connection.ConnectInterceptor.intercept(\nConnectInterceptor.java:45)\nat okhttp3.internal.http.RealInterceptorChain.proceed(\nRealInterceptorChain.java:92)\nat okhttp3.internal.http.RealInterceptorChain.proceed(\nRealInterceptorChain.java:67)\nat okhttp3.internal.cache.CacheInterceptor.intercept(\nCacheInterceptor.java:93)\nat okhttp3.internal.http.RealInterceptorChain.proceed(\nRealInterceptorChain.java:92)\nat okhttp3.internal.http.RealInterceptorChain.proceed(\nRealInterceptorChain.java:67)\nat okhttp3.internal.http.BridgeInterceptor.intercept(\nBridgeInterceptor.java:93)\nat okhttp3.internal.http.RealInterceptorChain.proceed(\nRealInterceptorChain.java:92)\nat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(\nRetryAndFollowUpInterceptor.java:120)\nat okhttp3.internal.http.RealInterceptorChain.proceed(\nRealInterceptorChain.java:92)\nat okhttp3.internal.http.RealInterceptorChain.proceed(\nRealInterceptorChain.java:67)\nat okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:185)\nat okhttp3.RealCall$AsyncCall.execute(RealCall.java:135)\nat okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(\nThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(\nThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:748)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1796, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/\nAAD617xSZaIaqJp8ihFGTy8LMr3IXWqAks5s2bypgaJpZM4QdmD2\n.\n\u2014\nYou are receiving this because you authored the thread.\n\nReply to this email directly, view it on GitHub, or mute the thread.\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1796#issuecomment-344432955,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD614BTBb97c2aILunnwdd85Eqicfzwks5s2h_vgaJpZM4QdmD2\n.\n. fyi the associated bug was fixed in 1.31.0\nhttps://github.com/openzipkin/zipkin/pull/1718\nOn Wed, Nov 15, 2017 at 12:15 AM, Scott Rankin notifications@github.com\nwrote:\n\nHello,\nI have been running Zipkin server 1.28 against AWS ElasticSearch (2.3).\nToday I tried to upgrade to 1.29, and I get the below error in stdout\n(domain redacted). I'm running Zipkin via Docker, and passing the following\nconfiguration:\n\"STORAGE_TYPE\": \"elasticsearch\",\n  \"ES_HOSTS\": \"https://my.es.index\",\n  \"AWS_ACCESS_KEY_ID\": \"ACCESSKEYHERE\",\n  \"AWS_SECRET_ACCESS_KEY\": \"SECRETKEYHERE\",\nwhich has been working fine since Zipkin 1.23. I can't seem to find any\nindication of a change to the signing requirements in 1.29 - am I missing\nanything?\nThanks!\nScott\nException in thread \"OkHttp Dispatcher\" java.lang.IllegalStateException: The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'GET\n/_cluster/health/zipkin%3Aspan-%2A\nhost:search-zipkin-sandbox-XXXXXXXXXXXXXX.amazonaws.com\nx-amz-date:20171114T155629Z\nhost;x-amz-date\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20171114T155629Z\n20171114/us-west-2/es/aws4_request\nbf52418dfd335f36303b4b3b7ed80a178ea587c8ff95773ba66b65fbae4c7cf6'\nat zipkin.autoconfigure.storage.elasticsearch.aws.AWSSignatureVersion4.intercept(AWSSignatureVersion4.java:69)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)\n  at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:45)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)\n  at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)\n  at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)\n  at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:120)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)\n  at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)\n  at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:185)\n  at okhttp3.RealCall$AsyncCall.execute(RealCall.java:135)\n  at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n  at java.lang.Thread.run(Thread.java:748)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1796, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD617xSZaIaqJp8ihFGTy8LMr3IXWqAks5s2bypgaJpZM4QdmD2\n.\n. :) thanks for speaking up. and join https://gitter.im/openzipkin/zipkin anytime. >\nWith the advent of the X-Ray compatible modes for zipkin/brave comes the\nfact that X-Ray spans (segments) are not shared by client and server so the\nUI ends up losing the hierarchy information. It still orders correctly\nbecause of timestamps. I would be great to support this model of spans in\nthe native zipkin UI\nI suspect this is related as well (notably the parent isn't propagated)\nhttps://github.com/openzipkin/zipkin/pull/1745\n. possibly a dupe of #963?. In the current api, I think maybe cheapest win is fusing single-host spans together. Basically move the server span into its client and adjust ids. However, I think possibly there's something more to ask you about.\n\nYou suggest the hierarchy is wrong.. which is a different problem then seeing things in the same detail screen. Can you give an example of this? It would seem a broken hierarchy would also be a problem in X-Ray... please take a look at https://github.com/openzipkin/zipkin/pull/2302 and let me know if this is still desirable!. please take a look at https://github.com/openzipkin/zipkin/pull/2302 and let me know if this is still desirable!. There's no such setting at the moment. We only have an option to disable the UI completely.\nIf we did have an option like this, it would apply to all users as we have no means to identify one. Note that they could still access the api directly, as the api is ultimately what presents the search capability.\nWe could gracefully handle when the api returns unauthorized.. For example, the UI begins with a query against the api services endpoint. If this returns unauthorized, we could possibly guess that someone has disabled exploration, then only present the get trace by ID screen.\nBefore we go there, though, probably maybe elaborate on your end goal and how you plan to enforce it?\ncc @jonathan-lo @sirtyro. >\n\nI want to disable search because our backend Cassandra is unable to handle\nthe load. But one useful feature is going to tracepage directly with\ntraceid which developers have it handy. Blocking search page for all users\nis okay in my case.\n\ndo you know the source of the load? Is it a particular query? Which\ncassandra implementation are you using?\nmaybe we can help?\n. yep!. yep!. Thanks for getting this started!. there are some linting problems here https://travis-ci.org/openzipkin/zipkin/builds/303415206?utm_source=github_status&utm_medium=notification\nyou can change to the zipkin-ui directory and run npm run lint to see them locally. Thanks for the help!. @gianarb noticed this crashes when browser language is italian. There's probably an easy fix for that but.. wonder if we shouldn't crowd-source some more languages?\n@shakuzen japanese?\n@gianarb italian?\n@marcingrzejszczak polish?\n@dsyer english? (hehe). hehe you are both right :)\nthe /zipkin path is only for the UI, and yes it isn't ready for v2 yet\nbecause of this issue. I'd love it to be v2 ready, so pull requests welcome.\nI was mentioning to tommy bc I thought the context was about routine api\nuse. IOTW, routine api users shouldn't depend on the /zipkin path as some\ndeployments disable the UI and would then disable this mapping\n. #1818 forwards the api root. added #2217 to actually use v2 data internally now that we are consuming it. added #2217 to actually use v2 data internally now that we are consuming it. This is in place until https://github.com/openzipkin/zipkin/issues/1230. bravo!. I just upvoted the associated maven issue\nhttps://issues.sonatype.org/browse/MVNCENTRAL-2870\nplease also upvote this. let's see if it can become sorted as this affects\na lot of READMEs. Meanwhile I suppose alternative is to manually download\nor do some fancy sed to correct it... not sure\n. so it has been a while and there's no activity in the upstream issue. wondering if we should revisit this problem.. I recall the original reason we went with maven central was that bintray treated the expression search as a paid feature. I wonder if this is still the case... >\n\nUnfortunately dynamic content download from JFrog is still a premium-only\nfeature (https://bintray.com/docs/api/#_dynamic_download). OTOH, I spent\nsome time researching how we could utilize standard tools to do what we\nwant instead of messing around with APIs like we do. How about this:\n$ mvn dependency:copy -Dartifact=io.zipkin.java:zipkin-server:LATEST:jar:exec -DoutputDirectory=./ -Dmdep.stripVersion=true -Dmdep.stripClassifier=true\n$ java -jar zipkin-server.jar\nThanks.. this is an option, and less janky. One con of it is that it\nrequires install of maven, and maven will download a lot to perform that\ncommand :P Nice to ponder though..\n. >\nI mean... the other thing we could do is create a curl | bash\n\"installer\". That could either do the ugly sed thing, or query some API\nfor the latest version and construct the file path based on that, then\ndownload. Host it under zipkin.io, so getting started would look\nsomething like:\n$ \\curl -sSL http://zipkin.io/quickstart.sh | bash -sFetching version number of latest Zipkin release...Downloading Zipkin stand-alone $VERSION ($REMOTEURL -> $LOCALPATH)...Run the following command to start Zipkin Server. Note that this is intended as a quick-start installation only for evaluating Zipkin, not as a production deployment guide. See $URL for guidance on running Zipkin in production.\n    java -jar ./zipkin-server.jar\nyeah this is probably best for those who can run bash. Suppose we'll want\na windows variant.. @fedj do you know what the windows equiv is?\n. https://github.com/openzipkin/zipkin/pull/1876 will fix this (still needs a bit of work to make the schema install not add the indexes vs just not use them). https://github.com/openzipkin/zipkin/pull/1876 will fix this (still needs a bit of work to make the schema install not add the indexes vs just not use them). cc @anuraaga @shakuzen @denyska FYI a 3x bump in http ingest throughput. cc @anuraaga @shakuzen @denyska FYI a 3x bump in http ingest throughput. PS there's clearly more possible here, just looking for a cheap win. Firstly, use 2.3.1? there was a bug in 2.3.0\n\nSecondly, if you aren't using the server directly, please chat\nhttps://gitter.im/openzipkin/zipkin about why you are creating a custom\nbuild\nOn Tue, Nov 21, 2017 at 8:24 PM, Ladd notifications@github.com wrote:\n\nwhen i use \"zipkin-autoconfigure-storage-elasticsearch-http-2.3.0\",\n\"elasticseach 6.0.0\" it give me an Exception,\nwhy? how to do\njava.lang.NoClassDefFoundError: zipkin/internal/V2StorageComponent$\nLegacySpanStoreProvider\nat java.lang.ClassLoader.defineClass1(Native Method)\nat java.lang.ClassLoader.defineClass(ClassLoader.java:760)\nat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\nat java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\nat java.net.URLClassLoader.access$100(URLClassLoader.java:73)\nat java.net.URLClassLoader$1.run(URLClassLoader.java:368)\nat java.net.URLClassLoader$1.run(URLClassLoader.java:362)\nat java.security.AccessController.doPrivileged(Native Method)\nat java.net.URLClassLoader.findClass(URLClassLoader.java:361)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\nat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\nat zipkin.autoconfigure.storage.elasticsearch.http.\nZipkinElasticsearchHttpStorageAutoConfiguration$HttpLoggingSet.matches(\nZipkinElasticsearchHttpStorageAutoConfiguration.java:80)\nat org.springframework.context.annotation.ConditionEvaluator.\nshouldSkip(ConditionEvaluator.java:102)\nat org.springframework.context.annotation.ConfigurationClassBeanDefiniti\nonReader.loadBeanDefinitionsForBeanMethod(ConfigurationClassBeanDefiniti\nonReader.java:178)\nat org.springframework.context.annotation.ConfigurationClassBeanDefiniti\nonReader.loadBeanDefinitionsForConfigurationClass(\nConfigurationClassBeanDefinitionReader.java:140)\nat org.springframework.context.annotation.ConfigurationClassBeanDefiniti\nonReader.loadBeanDefinitions(ConfigurationClassBeanDefiniti\nonReader.java:116)\nat org.springframework.context.annotation.ConfigurationClassPostProcessor.\nprocessConfigBeanDefinitions(ConfigurationClassPostProcessor.java:320)\nat org.springframework.context.annotation.ConfigurationClassPostProcessor.\npostProcessBeanDefinitionRegistry(ConfigurationClassPostProcesso\nr.java:228)\nat org.springframework.context.support.PostProcessorRegistrationDelegate.\ninvokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDeleg\nate.java:270)\nat org.springframework.context.support.PostProcessorRegistrationDelegate.\ninvokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:93)\nat org.springframework.context.support.AbstractApplicationContext.\ninvokeBeanFactoryPostProcessors(AbstractApplicationContext.java:687)\nat org.springframework.context.support.AbstractApplicationContext.refresh(\nAbstractApplicationContext.java:525)\nat org.springframework.boot.context.embedded.\nEmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.\njava:122)\nat org.springframework.boot.SpringApplication.refresh(\nSpringApplication.java:693)\nat org.springframework.boot.SpringApplication.refreshContext(\nSpringApplication.java:360)\nat org.springframework.boot.SpringApplication.run(\nSpringApplication.java:303)\nat org.springframework.boot.SpringApplication.run(\nSpringApplication.java:1118)\nat org.springframework.boot.SpringApplication.run(\nSpringApplication.java:1107)\nat com.dbl.zipkin.server.BootStrap.main(BootStrap.java:19)\nCaused by: java.lang.ClassNotFoundException: zipkin.internal.\nV2StorageComponent$LegacySpanStoreProvider\nat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\nat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n... 30 common frames omitted\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1807, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610m6AA5kR9_orlFqDqoLuYuZshV3ks5s4s74gaJpZM4Ql1LC\n.\n. thx mike!. cc @abesto fyi. one way is to try \"best of\" for example, the prometheus spring boot setup is not written for dependency injection, hence this bug and also needing to call a magic static DefaultExports.initialize()\n\nAnother option is to use micrometer, which can handle the spring boot layer and still use the prometheus exporter. This is used by folks close to home such as @trustin iiuc as well, so while new, not first to fire. (Disclaimer: author @jkschneider is a colleague!)\nhttps://github.com/micrometer-metrics/micrometer/blob/736fd95ca5a9fb9846699489877eb4021882b9df/micrometer-spring-legacy/src/main/java/io/micrometer/spring/autoconfigure/export/prometheus/PrometheusExportConfiguration.java. >\n\nI guess if not, we could still use this lib and something utterly\nhorrible on our end like exposing the collector with reflection and call\ncleanup ourselves at the right point in the Spring life-cycle. Deciding\nwhether that's better, or replacing the library altogether, I'll leave to\nyou.\nyeah I started subclassing their filter and exactly that (via reflection),\nthen got depressed :P\n. You need to use the same metric object across reloads, otherwise the metric\nwill cause problems for users as it'll keep on resetting.\n\nA metric that even before being used causes an inability to reload the\napplication scope under which it is defined is a problem you dont seem to\nbe able to hear. I wish you could\n. I'm going to stop commenting on this issue for the day as the fact that this library makes a little bomb in our test suite is something I guess we'll have to deal with until @brian-brazil allows the library to be usable. here's a workaround pull request, named for brian in hopes one day it will catch enough attention to reverse the madness https://github.com/openzipkin/zipkin/pull/1812. Agreed! Thanks.\nLets wait for a couple more to give feedback before merge.\n. I guess this is awaiting small feedback fixies\nFYI: I know folks have been used to me merging and releasing things, which is awesome because I have time to do it, and I prefer volunteers having time for code, docs and helping others.\nHeads up: I will be out from 24 Dec back 3 Jan, so someone else will need to merge things in in that span (hehe) or wait till I'm back.\ncc @openzipkin/core. thanks for all the help on this. We can take further changes as different PRs. trying to make a test break around this https://github.com/openzipkin/zipkin/pull/1852. OK rebased over the tests that underpin this. Thanks again!. ok fixed mysql from under this change, so hopefully will build green. yes: https://github.com/openzipkin/brave-webmvc-example is an example\n. It is possible, yes, but unsupported.\nOn 19 Dec 2017 7:08 pm, \"jidianxiake\" notifications@github.com wrote:\n\nZipkin server nested into my own monitoring application\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1853#issuecomment-352716599,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61zwBs7EpEFssBDActRoGFnrEQJJ1ks5tB5lAgaJpZM4RGVx-\n.\n. Zipkin is typically used for distributed applications, meaning more than\none process across one or more hosts. It uses a single listen port, so can\nshare the same host as other processes. Co-locating in one JVM is only\npossible with spring boot, unless you write a even more custom app.\n\nPeople usually encounter issues when they try to make custom servers which\nincreases the burden on this project which is why it isnt supported. If you\ncreate a custom server you will need to bear the burden of supporting it.\nThat support burden may cost you more in salary than the resources to start\na couple hundred megabyte JVM.\nHope this info helps guide your decision and good luck.\nOn 19 Dec 2017 9:53 pm, \"jidianxiake\" notifications@github.com wrote:\n\nIf I do that, I need to change my project into a springboot project\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1853#issuecomment-352757720,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD613EwM9aBIwHj-BDxJhqSKDEXMzH-ks5tB7_2gaJpZM4RGVx-\n.\n. if you want to ask more questions, please use gitter\nhttps://gitter.im/openzipkin/zipkin\n\nNote that the type of questions you are about to ask are the reason why we\ndon't support this. It costs a lot of time for each person that tries to\nmake a custom version because of choices and how to get the UI routes\ncorrect etc.\n. only way to diagnose this type of issue is to see the raw trace sent.\nIf your storage is v2 (elasticsearch or cassandra3), use\n/api/v2/trace/trace_id. Otherwise, use /api/v1/trace/trace_id?raw\nplease use a paste link or wrap in ``` to format it\n. closing from inactivity please try us here if you need https://gitter.im/openzipkin/zipkin. PS rod mentioned this for content https://vimeo.com/247562710\nbeyond selfish aims of maintaining things, seems atomist could also be a way to help instrument libraries. I didn't think of this angle.. sure. here are some examples. some have inherent workflow so not sure if appropriate or not\nautomatically update to latest finagle version\n\nFinagle updates once a month, but the day might change. Update and immediately cut a release\nnag issue placeholder https://github.com/openzipkin/zipkin-finagle/issues/38\nversion bump before release tag https://github.com/openzipkin/zipkin-finagle/commit/221a74bc7f7e8f5f045e09afcddd65df571f1824\n\nupdate spring-cloud-sleuth\n\nonce a \"notable\" release is done to our core library, put it in spring-cloud-sleuth. deployment decoupled as out of our org, usually 2 branches to do\nhttps://github.com/spring-cloud/spring-cloud-sleuth/commit/92e434e26129bef00215cd20253a58b32c8ee12d\n\nupdate extension to our docker image\n\nupdate extension library with latest zipkin, on green build make a release-0.8.5 tag to kick automation\nthing to do to update versions: https://github.com/openzipkin/zipkin-aws/commit/107d9df6cadcd68c20547e4bba9b3543522c1f62\nonce that's done, it is synced to bintray, so we can update and cut a docker image via 0.8.5 tag\nthing to do to update versions: https://github.com/openzipkin/docker-zipkin-aws/commit/91198770444766a848e7a8900de1702ab1b658e3\n. Another angle is we have a number of example projects to show how to configure zipkin. While some are starting anew, many are copy-pasting things out of these into existing apps. Some sort of means to inject zipkin could be great for support.\n\nhttps://github.com/openzipkin/brave-webmvc-example\nhttps://github.com/openzipkin/zipkin-php-example\nhttps://github.com/openzipkin/brave-webmvc-example\n(there are many others). First question is if you use zipkin's search by key/value or not? This is only used for that feature\nIt is possible we could allow a config to whitelist keys to index (like http.path)\n@michaelsembwever any thoughts?. by the way, thank you very much for the detailed info!. @zuochangan did you end up doing  a TokenizingAnalyzer? how did it go?. Thank you for following up!\n. This is cool. can you mention any results (ex difference in space used or\nindexing or query latency?\nOn Fri, Jan 12, 2018 at 2:02 PM, zuochangan notifications@github.com\nwrote:\n\nSorry for late reply.\nHere is my solution.\nIn our situation, we won't query with partial annotation , we always query\nwith a complete annotation. so prefix index and proper TokenAnalyzer is\nenough. Here are steps you can follow.\n\ncompile following code and package it into a jar then put it in\n   cassandra library path and restart cassandra.\n\npackage org.apache.cassandra.index.sasi.analyzer;\nimport java.nio.ByteBuffer;\nimport java.util.ArrayList;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;\nimport com.google.common.base.Strings;\nimport org.apache.cassandra.db.marshal.AbstractType;\npublic class ZipkinStandardAnalyzer extends AbstractAnalyzer\n{\n    private static final Locale DEFAULT_LOCALE = Locale.getDefault();\n    private static final String ZIPKIN_DELIMITER = \"\u2591\";\n    private AbstractType validator;\n    private List tokens = new ArrayList<>();\n    private Iterator iter;\npublic ZipkinStandardAnalyzer()\n{\n}\n\npublic ByteBuffer next()\n{\n    return this.validator.fromString(iter.next());\n}\n\npublic void init(Map<String, String> options, AbstractType validator)\n{\n    this.validator = validator;\n}\n\npublic boolean hasNext()\n{\n    return iter.hasNext();\n}\n\npublic void reset(ByteBuffer input)\n{\n    this.next = null;\n    this.tokens.clear();\n    String ann = validator.getString(input);\n    ann = ann.toLowerCase(DEFAULT_LOCALE);\n    String[] parts = ann.split(ZIPKIN_DELIMITER);\n    for (String part : parts)\n    {\n        if (!Strings.isNullOrEmpty(part))\n        {\n            tokens.add(part);\n        }\n    }\n    this.iter = tokens.iterator();\n}\n\n\npublic boolean isTokenizing()\n{\n    return true;\n}\n\n}\n\nreplace old annotation index with following statement\n\nCREATE CUSTOM INDEX span_annotation_query_idx ON zipkin2.span (annotation_query) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.ZipkinStandardAnalyzer', 'case_sensitive': 'false', 'mode': 'prefix', 'analyzed': 'true'};\n\nmodify class zipkin2.storage.cassandra.SelectTraceIdsFromSpan as\n   following and deploy your zipkin with new code again.\n\nCall>> newCall(\n      @Nullable String serviceName,\n      String annotationKey,\n      TimestampRange timestampRange,\n      int limit\n    ) {\n      Input input = new AutoValue_SelectTraceIdsFromSpan_Input(\n        serviceName,\n        // Notice here,we just need to remove all \u2591\n         annotationKey,\n        timestampRange.startUUID,\n        timestampRange.endUUID,\n        limit\n      );\n      return new SelectTraceIdsFromSpan(\n        this,\n        serviceName != null ? withServiceAndAnnotationQuery : withAnnotationQuery,\n        input\n      ).flatMap(new AccumulateTraceIdTsLong());\n    }\nThat's all. Hope I've made myself clear.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1861#issuecomment-357150841,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610SXmPsqbOBl3lz9iQYp4OsdgxK_ks5tJvVmgaJpZM4RKBi7\n.\n. great idea.. you mean good patch upstream in cassandra? If so, I'm happy to\nhelp review.\n. missed this note, but quick clarification which is indexing by (tag, value,\nservice) -> traceid should be ok even without spanid. query is still valid\nas it is based on service. this lowers writes significantly with deduping\nfor apps that make many local spans.\n\nthis isnt to counter any other points made.\nOn Fri, 16 Mar 2018, 19:03 mck, notifications@github.com wrote:\n\n(service_name, tag_key, tag_value)\nYou're going to get some very hot partition keys there. And there's no way\nof de-duping it because you want each span_id in the results.\nThis is only going to work if you have reverse clustering on start_time,\nTWCS, TTL, and queries are time-bound.\nThat said it's the correct denormalisation approach over what i suggested\nabove in #1861 (comment)\nhttps://github.com/openzipkin/zipkin/issues/1861#issuecomment-353510778,\nwhich in hindsight was a bit of a brain-fart.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1861#issuecomment-373680613,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wXIYpjmtrEPwKqwat-hiyrgmxHpks5te5wagaJpZM4RKBi7\n.\n. missed this note, but quick clarification which is indexing by (tag, value,\nservice) -> traceid should be ok even without spanid. query is still valid\nas it is based on service. this lowers writes significantly with deduping\nfor apps that make many local spans.\n\nthis isnt to counter any other points made.\nOn Fri, 16 Mar 2018, 19:03 mck, notifications@github.com wrote:\n\n(service_name, tag_key, tag_value)\nYou're going to get some very hot partition keys there. And there's no way\nof de-duping it because you want each span_id in the results.\nThis is only going to work if you have reverse clustering on start_time,\nTWCS, TTL, and queries are time-bound.\nThat said it's the correct denormalisation approach over what i suggested\nabove in #1861 (comment)\nhttps://github.com/openzipkin/zipkin/issues/1861#issuecomment-353510778,\nwhich in hindsight was a bit of a brain-fart.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1861#issuecomment-373680613,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wXIYpjmtrEPwKqwat-hiyrgmxHpks5te5wagaJpZM4RKBi7\n.\n. ignore me.. I also had a brain fart! yeah span id is needed when combining\nconditions.\n\nOn Thu, 21 Jun 2018, 07:37 Adrian Cole, adrian.f.cole@gmail.com wrote:\n\nmissed this note, but quick clarification which is indexing by (tag,\nvalue, service) -> traceid should be ok even without spanid. query is still\nvalid as it is based on service. this lowers writes significantly with\ndeduping for apps that make many local spans.\nthis isnt to counter any other points made.\nOn Fri, 16 Mar 2018, 19:03 mck, notifications@github.com wrote:\n\n(service_name, tag_key, tag_value)\nYou're going to get some very hot partition keys there. And there's no\nway of de-duping it because you want each span_id in the results.\nThis is only going to work if you have reverse clustering on start_time,\nTWCS, TTL, and queries are time-bound.\nThat said it's the correct denormalisation approach over what i suggested\nabove in #1861 (comment)\nhttps://github.com/openzipkin/zipkin/issues/1861#issuecomment-353510778,\nwhich in hindsight was a bit of a brain-fart.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1861#issuecomment-373680613,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wXIYpjmtrEPwKqwat-hiyrgmxHpks5te5wagaJpZM4RKBi7\n.\n\n\n. ignore me.. I also had a brain fart! yeah span id is needed when combining\nconditions.\n\nOn Thu, 21 Jun 2018, 07:37 Adrian Cole, adrian.f.cole@gmail.com wrote:\n\nmissed this note, but quick clarification which is indexing by (tag,\nvalue, service) -> traceid should be ok even without spanid. query is still\nvalid as it is based on service. this lowers writes significantly with\ndeduping for apps that make many local spans.\nthis isnt to counter any other points made.\nOn Fri, 16 Mar 2018, 19:03 mck, notifications@github.com wrote:\n\n(service_name, tag_key, tag_value)\nYou're going to get some very hot partition keys there. And there's no\nway of de-duping it because you want each span_id in the results.\nThis is only going to work if you have reverse clustering on start_time,\nTWCS, TTL, and queries are time-bound.\nThat said it's the correct denormalisation approach over what i suggested\nabove in #1861 (comment)\nhttps://github.com/openzipkin/zipkin/issues/1861#issuecomment-353510778,\nwhich in hindsight was a bit of a brain-fart.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1861#issuecomment-373680613,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wXIYpjmtrEPwKqwat-hiyrgmxHpks5te5wagaJpZM4RKBi7\n.\n\n\n. fixed in zipkin 2.11. probably a data problem https://gitter.im/openzipkin/zipkin for more help. looks like the right fix. thanks tons!\n\nmight be a mild pain to extract the code such that it can be unit tested..\nbut mind timeboxing an attempt?\n. here's a start https://github.com/openzipkin/zipkin/pull/1876 cc @drolando . here's a start https://github.com/openzipkin/zipkin/pull/1876 cc @drolando . Current status:\nAs of latest version, SEARCH_ENABLED=false works for elasticsearch, cassandra3 and in-memory storage, disabling all indexing. A future change #1893 should hide the Find UI as it no longer would work when search is disabled. . >\n\nMy client is using Spring Cloud discovery to register Zipkin instances in\nZookeeper.\nIs this via a custom build of zipkin-server?\n. >\nMy client is using Spring Cloud discovery to register Zipkin instances in\nZookeeper.\nIs this via a custom build of zipkin-server?\n. We repeatedly get requests about @EnableDiscoveryClient or @EnableEurekaClient I'd like to try and see if there is an alternative way to register with eureka that doesn't imply literally modifying the main class. If we can do so with auto-configuration instead, it might be worthwhile posting how.. interesting.. https://github.com/spring-cloud/spring-cloud-commons/pull/245/files. https://github.com/spring-cloud/spring-cloud-netflix/blob/master/docs/src/main/asciidoc/spring-cloud-netflix.adoc#registering-with-eureka seems we can make an example integration... here's a way to bolt-on eureka to an existing zipkin server without modifying it. \n\nThis requires that you have maven installed. You don't need to do any custom code as it is 100% packaging.\n```bash\nget normal zipkin server\n$ curl -sSL https://zipkin.io/quickstart.sh | bash -s\nbuild the eureka module\n$ mvn clean install\nrename the jar so it is easier\n$ mv target/eureka-1.0-SNAPSHOT-module.jar eureka.jar\nstart zipkin which now has eureka support\n$ java -Dloader.path='eureka.jar,eureka.jar!/lib' -cp zipkin.jar org.springframework.boot.loader.PropertiesLauncher\n```\nHere's the only file you need:\n```xml\n\n4.0.0\nio.zipkin.custom\neureka\n1.0-SNAPSHOT\nExample module that adds Eureka to an existing Zipkin\n\n make sure this matches zipkin-server's spring boot version \n2.1.1.RELEASE\n\n\n\n\n This makes sure versions are aligned properly \norg.springframework.boot\nspring-boot-dependencies\n${spring-boot.version}\npom\nimport\n\n\n\n\n this is the thing that adds Eureka \n\norg.springframework.cloud\nspring-cloud-starter-netflix-eureka-client\n2.0.2.RELEASE\n\n zipkin already has this \n\norg.springframework.boot\nspring-boot-starter\n\n\n\n\n\n\n\norg.springframework.boot\nspring-boot-maven-plugin\n${spring-boot.version}\n\n\n\nrepackage\n\n\n\n\n\ncustom\n\nmodule\n exclude dependencies already packaged in zipkin-server. \n https://github.com/spring-projects/spring-boot/issues/3426 transitive exclude doesn't work \nio.zipkin.zipkin2,io.zipkin.reporter2,org.springframework.boot,org.springframework,com.fasterxml.jackson.core,com.google.auto.value,com.google.gson,com.google.guava,org.slf4j\n          \n\n\n\nio.zipkin.layout\nzipkin-layout-factory\n0.0.4\n\n\n\n\n\n\n```. here's a way to bolt-on eureka to an existing zipkin server without modifying it. \nThis requires that you have maven installed. You don't need to do any custom code as it is 100% packaging.\n```bash\nget normal zipkin server\n$ curl -sSL https://zipkin.io/quickstart.sh | bash -s\nbuild the eureka module\n$ mvn clean install\nrename the jar so it is easier\n$ mv target/eureka-1.0-SNAPSHOT-module.jar eureka.jar\nstart zipkin which now has eureka support\n$ java -Dloader.path='eureka.jar,eureka.jar!/lib' -cp zipkin.jar org.springframework.boot.loader.PropertiesLauncher\n```\nHere's the only file you need:\n```xml\n\n4.0.0\nio.zipkin.custom\neureka\n1.0-SNAPSHOT\nExample module that adds Eureka to an existing Zipkin\n\n make sure this matches zipkin-server's spring boot version \n2.1.1.RELEASE\n\n\n\n\n This makes sure versions are aligned properly \norg.springframework.boot\nspring-boot-dependencies\n${spring-boot.version}\npom\nimport\n\n\n\n\n this is the thing that adds Eureka \n\norg.springframework.cloud\nspring-cloud-starter-netflix-eureka-client\n2.0.2.RELEASE\n\n zipkin already has this \n\norg.springframework.boot\nspring-boot-starter\n\n\n\n\n\n\n\norg.springframework.boot\nspring-boot-maven-plugin\n${spring-boot.version}\n\n\n\nrepackage\n\n\n\n\n\ncustom\n\nmodule\n exclude dependencies already packaged in zipkin-server. \n https://github.com/spring-projects/spring-boot/issues/3426 transitive exclude doesn't work \nio.zipkin.zipkin2,io.zipkin.reporter2,org.springframework.boot,org.springframework,com.fasterxml.jackson.core,com.google.auto.value,com.google.gson,com.google.guava,org.slf4j\n          \n\n\n\nio.zipkin.layout\nzipkin-layout-factory\n0.0.4\n\n\n\n\n\n\n```. https://twitter.com/toongeens pointed me to this as well https://github.com/gliderlabs/registrator. I used the above bolt-on to answer a related BASIC auth question. This is the main way to add things to the server without recompiling it https://github.com/openzipkin/zipkin/issues/782#issuecomment-433306013. thanks tons!\n. thanks again!. thanks again!. I like it. @openzipkin/core does anyone not want this?. I like it. @openzipkin/core does anyone not want this?. thanks a lot (Again!). is the sender v2? if so, the sender url should be v2 not v1\nOn Sun, Jan 7, 2018 at 6:18 PM, Yuri Schimke notifications@github.com\nwrote:\n\nreproduction\n$ cat ~/.zipkinrc\nSENDER=http://localhost:9411/api/v1/spans\nDISPLAY=http://localhost:9411/zipkin/traces/{traceid} http://localhost:9411/api/v1/spansDISPLAY=http://localhost:9411/zipkin/traces/%7Btraceid%7D\nbrew install yschimke/tap/oksocial\noksocial --zipkin https://httpbin.org/get\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1873#issuecomment-355812436,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xzWzWqsks26o8zO11QVCnb7gwtRks5tIJoSgaJpZM4RVmd2\n.\n. is the sender v2? if so, the sender url should be v2 not v1\n\nOn Sun, Jan 7, 2018 at 6:18 PM, Yuri Schimke notifications@github.com\nwrote:\n\nreproduction\n$ cat ~/.zipkinrc\nSENDER=http://localhost:9411/api/v1/spans\nDISPLAY=http://localhost:9411/zipkin/traces/{traceid} http://localhost:9411/api/v1/spansDISPLAY=http://localhost:9411/zipkin/traces/%7Btraceid%7D\nbrew install yschimke/tap/oksocial\noksocial --zipkin https://httpbin.org/get\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1873#issuecomment-355812436,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xzWzWqsks26o8zO11QVCnb7gwtRks5tIJoSgaJpZM4RVmd2\n.\n. sorry I should have thought about this when you asked on gitter earlier.\ntake care\n. sorry I should have thought about this when you asked on gitter earlier.\ntake care\n. (PS as a clarification, this is not a propagated tag, rather one sent out-of-band and possibly used as an aggregating dimension for metrics). (PS as a clarification, this is not a propagated tag, rather one sent out-of-band and possibly used as an aggregating dimension for metrics). >\nThe Correct Way to solve this would be to use any identifier guaranteed to\nuniquely identify the endpoint\nEffectively I think that is what http.template is in this proposal.\nright, probably the fact that a template is present or not could be\ndistracting. For example, \"post /foo\" as long as it has no variables is\nalso fine (ex not a template). Maybe the name template is not great. The\ngoal indeed is to uniquely identify the endpoint.. there is definitely a\nsecondary goal of it being human readable. For example, a hashed identifier\nwouldn't be great as some will optionally choose to use this as a span name\nor a human readable label.\n. >\nThe Correct Way to solve this would be to use any identifier guaranteed to\nuniquely identify the endpoint\nEffectively I think that is what http.template is in this proposal.\nright, probably the fact that a template is present or not could be\ndistracting. For example, \"post /foo\" as long as it has no variables is\nalso fine (ex not a template). Maybe the name template is not great. The\ngoal indeed is to uniquely identify the endpoint.. there is definitely a\nsecondary goal of it being human readable. For example, a hashed identifier\nwouldn't be great as some will optionally choose to use this as a span name\nor a human readable label.\n. >\nDo you expect span's name be the same as http.template in the most cases?\nOr they semantically different?\nWhat's currently known as http.template is a better span name for most\npeople than something like \"get\". However, think of the redirect case, it\nwould be \"REDIRECT\" or something else it would have infinite cardinality.\nThe other issue is that a span can fail before it can know its endpoint.\nThis is behind the heuristic of a tentative name even if you are planning\nto use template at response time (if you get a response).\n\nReason I hesitated about saying it is semantically the span name is because\nthe name is up to users to decide. This could be a decent choice, but could\nbe added as a tag regardless.\n. >\n\nDo you expect span's name be the same as http.template in the most cases?\nOr they semantically different?\nWhat's currently known as http.template is a better span name for most\npeople than something like \"get\". However, think of the redirect case, it\nwould be \"REDIRECT\" or something else it would have infinite cardinality.\nThe other issue is that a span can fail before it can know its endpoint.\nThis is behind the heuristic of a tentative name even if you are planning\nto use template at response time (if you get a response).\n\nReason I hesitated about saying it is semantically the span name is because\nthe name is up to users to decide. This could be a decent choice, but could\nbe added as a tag regardless.\n. >\n\nI'm not sure about the name, because I don't know if a template (assuming\nwe're talking URI templates https://tools.ietf.org/html/rfc6570 here)\nis sufficient. Here's an example:\nWe have this exact problem with our swagger https://swagger.io/ RPC\nendpoints. Using the petstore example http://petstore.swagger.io/, GET\n/pet/{petId} will show up as /pet/1234 in the http.uri. So 1) it is\ndifficult to know that that corresponds to the /pet/{petId} path\ntemplate, and 2) it is impossible to know that it corresponds to the GET\nversion of that vs the DELETE or POST versions.\n^^ is a tricky problem, and something to deal with in implementation. In\nbrave, we are thinking about using both the request and response to derive\nthe template, which means you can see what the method is. When you can't\nderive a template, you don't produce the tag. Ex. if the framework can't\ntell you, then probably better to not add a tag than add one with infinite\ncardinality.\n\nOn URI templates, for better or worse we already know that deviations are\nused, so probably not best to promise URI template formatting, rather focus\non the limited cardinality aspect.\nWhether or not DELETE or POST is in the template is up to us to decide.\nHttp methods are fixed cardinality, and since a primary consumer of this is\nmetrics, it sort of depends on if the system supports multiple lookup keys\nor not. One thing that could be easier is this.. It is often easier to\nstrip a method than assume it is always there. We could say that the\ntemplate or whatever we call it tag does not include the method. In fact\nthis is probably easier for all frameworks I can think of.. Would that\nhelp?\n. >\n\nI'm not sure about the name, because I don't know if a template (assuming\nwe're talking URI templates https://tools.ietf.org/html/rfc6570 here)\nis sufficient. Here's an example:\nWe have this exact problem with our swagger https://swagger.io/ RPC\nendpoints. Using the petstore example http://petstore.swagger.io/, GET\n/pet/{petId} will show up as /pet/1234 in the http.uri. So 1) it is\ndifficult to know that that corresponds to the /pet/{petId} path\ntemplate, and 2) it is impossible to know that it corresponds to the GET\nversion of that vs the DELETE or POST versions.\n^^ is a tricky problem, and something to deal with in implementation. In\nbrave, we are thinking about using both the request and response to derive\nthe template, which means you can see what the method is. When you can't\nderive a template, you don't produce the tag. Ex. if the framework can't\ntell you, then probably better to not add a tag than add one with infinite\ncardinality.\n\nOn URI templates, for better or worse we already know that deviations are\nused, so probably not best to promise URI template formatting, rather focus\non the limited cardinality aspect.\nWhether or not DELETE or POST is in the template is up to us to decide.\nHttp methods are fixed cardinality, and since a primary consumer of this is\nmetrics, it sort of depends on if the system supports multiple lookup keys\nor not. One thing that could be easier is this.. It is often easier to\nstrip a method than assume it is always there. We could say that the\ntemplate or whatever we call it tag does not include the method. In fact\nthis is probably easier for all frameworks I can think of.. Would that\nhelp?\n. updated with play info. thx @ivantopo cc @jcchavezs . just started on this https://github.com/openzipkin/brave/pull/602. https://github.com/adriancole/play-zipkin-tracing/compare/integration-tests...adriancole:http-template is what I believe would be the way to attack this in play. However, the following always shows up empty..\nscala\n  def apply(nextFilter: (RequestHeader) => Future[Result])(req: RequestHeader): Future[Result] = {\n    var template = req.attrs.get(Router.Attrs.HandlerDef).map(_.path). Had a chat about this in census, and @rakyll suggests the name http.route, which makes sense to me as almost always I'm using the word route to describe what this is. Anyone feel it shouldn't be this?\n\"http.route\"              | Matched request URL route   | \"/users/:userID\". >\n\nHello there! Happy to see this moving \ud83d\ude04\n:)\nIs the scope of this fixed to HTTP-related Spans? I can totally see a\nsimilar tag/mechanism working to gather metrics on JDBC call spans and\nanything else that looks like it (Cassandra driver, etc). This happens to\nbe a pretty common topic when people are starting to instrument their\napps.. they usually end up with either too little granularity or a crazy\nexplosion of metrics unless some better naming is manually applied.\nAnything we can do to provide better default names out of the box is a\ninstant win to all users!\nIndeed, this seems familiar to other things.. usually the search for a good\nname. I wouldn't expect as much handiness in JDBC, for example, as prepared\nstatements etc tend to be too big. Similarly cassandra has this concern.\nThat doesn't mean that a prepared statement or similar tag doesn't end up\nbeing defined in these domains. We'd cross-reference this work when doing\nsomething similar.\n\nbear in mind that http.route isn't enough for a name, as it isn't always\nthere. To make a stable name, we'd need to make constants like redirect or\nnot_found, such as micrometer do. In other words http.route is an input to\na good naming engine, but not necessarily a great name by itself. It is a\nstart towards something better than just http method, and at least several\nframeworks support it, even outside java, so it is actionable!\n\nPS: Play Framework does a really nice job at exposing the route templates through\ntags\nhttps://github.com/playframework/playframework/blob/master/framework/src/play/src/main/scala/play/core/routing/GeneratedRouter.scala#L234-L238,\nthat might be worth adding in the description.\nthx for that!\n. thanks jon, I cleared some of this in latest update hopefully.\nhttps://github.com/openzipkin/brave/pull/602#issuecomment-366398125\n\nOn Thu, Feb 15, 2018 at 11:02 PM, Jon Schneider notifications@github.com\nwrote:\n\nneed to make constants like redirect or not_found\nTo elaborate a bit: not shunting 404s or 302s to a fixed tag name\nessentially becomes a DOS-style attack vector on your monitoring system. I\nguess this is true for any \"user-provided\" tag value. Probably more\nimportantly, it's just easier to dimensionally drill down on auth failures\nand such.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1874#issuecomment-365953326,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xWBd6i3xkMVmtUkonATQwNy1cXXks5tVEb4gaJpZM4RVqQI\n.\n. maybe it is best to go ahead and map \"redirection\" and \"not_found\" (lowercase as span names are lowercase) by default as the http.route value. That way, we know the difference between unsupported and no route found (because if a template is possible, you always get a value, even if it is \"redirection\" or \"not_found\"). Thinking about it, I don't like doing the redirection or not_found mapping as a part of the template tag. I would prefer to have the adapter know if it can parse a route (which is only possible when there is an api for that. For example, there's no way to do this in netty layer). Someone using this can then decide if they want to for example, overwrite the span name with one from a template, and skip overhead trying if there's no chance a route will be known.\n. Chatting about the \"http.route\" tag signal. Some people do span adjustments after the fact, like with sparkstreaming.\n\nRelying on code conventions to tell if http.route was processable or not can be a problem, if doing span renaming downstream. For example, we'd like to consistently apply a naming pattern, even if it is not found or redirected.\nEx zipkin, in a framework that doesn't support routes, it could be more sensible to default to the span name to http method vs have http method or not_found, for consistency's sake. Knowing a route is possible allows us to be consistent: use a template based span namer or another one, depending on what's supported.\nThe easiest path out is to use \"\" as a signal that there is route support, but a route didn't match (for any reason). Use \"http.status_code\" to clarify. This is similar to how we used the marker tag key \"lc\" permitting empty values.\nIndexing impact is limited. For example, optimized backends write one index entry per trace each time \"http.route\" -> \"\" is stored. Of course, it isn't required to be stored ever: it isn't a MUST use empty string on no match, but SHOULD. If frameworks that know about routing write empty string, downstream decorators can use that as input to naming or bucketing policy.\nMake sense?\n. offline feedback is that the whole point of \"http.route\" is both fixed cardinality and grouping. For grouping to work, we want routes that are effectively the same, to in fact be the same. Here are a couple things on that.\n\nRequire a leading slash - prefixing with slash allows you firstly to identify a root path as different than empty string. More importantly it allows you to group \"users/:userId\" with \"/users/:userId\" which some frameworks can be inconsistent about (due to user defined template expressions)\nTry to normalize paths -  For example, if a framework somehow can allow an accident of \"//users/:userId\" to strip out the extra slash. This is to help with grouping, and tests \"should\" be able to smoke this out.\n\nSeems like only one leading slash can be a MUST as it is cheap to enforce. Path normalization is a SHOULD, as you could go forever on this one. The intent MUST :P be clear, which is that for this tag to be effective, things like this should happen.. revised docs etc here. tomorrow will merge after fixing up tests\nhttps://github.com/openzipkin/brave/pull/602/commits/103e5ad8ff1b183ccacb7fd910aec9b63a336603\nin a nutshell, this sets the default span name policy to use the http route where possible (coercing empty to not_found or redirected). Remember in zipkin span names are lowercase that's why not_found. This is taking lessons learned from micrometer. To make this work in practice, the http.method is now tagged by default (otherwise you wouldn't know the http method associated with /users/:userId\nwill also integration test it tomorrrow with sleuth and ratpack. https://github.com/openzipkin/zipkin-api/pull/42 for the thrift constant. @bplotnick digging around on something I dropped that you mentioned.. which is what addresses the request path templating. It seems like pyramid should be able to avail the route pattern somehow (even if it doesn't now). I'm looking at https://docs.pylonsproject.org/projects/pyramid/en/latest/narr/urldispatch.html#route-pattern-syntax. In spark web I've had to request the ability to see this, as they hadn't exposed it prior (eventhough it is reachable).\nOn your other points, I do think that http.route isn't necessary, as in necessary for all, but it is a nice choice that is still http abstraction.. and certainly helps some or else they wouldn't have done similar ad-hoc. As not necessary, I mean a symbolic route name or function name can also be a nice tag and/or span name. Just such are more pinned to the application abstraction vs http, which could be a pro or con depending on POV. For example, regardless of syntax.. /users/{userId} /users/:userId /users/* etc people who don't know the app probably can figure out what the route means in context of the http request. Similarly constants like not_found redirected are fairly intuitive and explain the http layer reasonably well. Importantly, presence of http.route does not mean don't add another identifier like a generated one or otherwise.. for example, in java code there are often other tags for the method the request dispatched to.\nDoes this make sense? Not trying to convince you, just trying to put forth rationale and why some might use http.route (either as a span name or aggregation tag), even if not everyone will choose to.. ps to address the differentiation problem that both @bplotnick and @jcchavezs (offline) mentioned, in brave I'm changing the default span name function to include essentially \"${http.method} ${http.route}\"\nso, for example, the following are all valid. Note that the http.route requires a slash prefix, so naturally differentiates from the constants.\nget /users/:userId\npost redirected\npost /users\npost not_found. to clarify above.. a span naming convention is separate from the http.route value. For example, if you want to drill down by path or template, you should add the tags separately (especially knowing that span names are implicitly downcased). Sweet!\nOn 26 Feb 2018 12:35 pm, \"Ben Plotnick\" notifications@github.com wrote:\n\n@adriancole https://github.com/adriancole Apologies for not responding\nto your comment and thank you for continuing with this regardless. What you\nproposed and implemented is perfect. You're absolutely right that pyramid\nprovides this and I'll add this into pyramid_zipkin asap. I can't wait to\ndelete some hacks \ud83d\ude04\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1874#issuecomment-368387783,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610a3mkch90b16vZ5o91FLRif3SNZks5tYjR2gaJpZM4RVqQI\n.\n. Sweet!\n\nOn 26 Feb 2018 12:35 pm, \"Ben Plotnick\" notifications@github.com wrote:\n\n@adriancole https://github.com/adriancole Apologies for not responding\nto your comment and thank you for continuing with this regardless. What you\nproposed and implemented is perfect. You're absolutely right that pyramid\nprovides this and I'll add this into pyramid_zipkin asap. I can't wait to\ndelete some hacks \ud83d\ude04\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1874#issuecomment-368387783,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610a3mkch90b16vZ5o91FLRif3SNZks5tYjR2gaJpZM4RVqQI\n.\n. @michaelsembwever yeah incomplete on disabling things for CQL. I'm not sure we will check at runtime, just install without SASI on a fresh spot. We could check though. @michaelsembwever yeah incomplete on disabling things for CQL. I'm not sure we will check at runtime, just install without SASI on a fresh spot. We could check though. renamed to \"search-enabled\" as seems to imply more what's being disabled. feedback welcome. renamed to \"search-enabled\" as seems to imply more what's being disabled. feedback welcome. @igorwwwwwwwwwwwwwwwwwwww if /config.json has searchEnabled=false, you think you could hide the find screen?. @igorwwwwwwwwwwwwwwwwwwww if /config.json has searchEnabled=false, you think you could hide the find screen?. added a commit to not install the indexes unless search is enabled. Since a lot of prod sites will have their own copies of the schema files, they'll have to do the same on their own I guess. That or we further templatize the schema files with things like replication factor and ttl cc @openzipkin/cassandra . @drolando thx for the hint.. will not be ambitious then. added elasticsearch implementation. there are a few areas we need some unit tests, and generally some docs. We also need to have the UI conditional on the searchEnabled property. current status: revised server README and rebased\n\nstill a few areas we need some unit tests, and generally some docs. We also need to have the UI conditional on the searchEnabled property. @igorwwwwwwwwwwwwwwwwwwww @Logic-32 can I ask one of you to figure out how to hide the \"find a trace\" and also maybe change the default content to empty or otherwise when the config.json has searchEnabled=false? I could possibly figure this out, but it will likely take orders of magnitude more time. I'll offer a trade to implement a favorite issue, like https://github.com/openzipkin/brave/issues/564 for example :D. pretend you don't know I'll implement the above issue anyway. >\n\n@adriancole https://github.com/adriancole do you want to update the UI\nas part of this CR? It feels long enough already :)\nwell I like to keep master shippable. So, this question means.. is hiding\nthe UI an optional part of this change? Since you are the customer of this\nchange, if you say so, works for me!\n. So, plan is to \"soft launch\" this so that @drolando and others can try it and give feedback eventhough the UI won't be disabled. I'll do that as a patch release. We'll make a more broadcast announcement once feedback comes in (and any changes along with it), and ideally UI support is also in.. ok tests are backfilled. I only need to write relevant README sections in cassandra and elasticsearch. ok all done with tests and README will merge on green. opened a follow-up issue for the UI change https://github.com/openzipkin/zipkin/issues/1893. I getcha, but there's only one (deprecated in Nov 17) storage type with\nthis limitation.\n\nShould we add something like this, knowing this? the configuration would be\na bit spaghetti and the ask while valid is a little late considering where\nwe are today.\nThoughts?\n. I getcha, but there's only one (deprecated in Nov 17) storage type with\nthis limitation.\nShould we add something like this, knowing this? the configuration would be\na bit spaghetti and the ask while valid is a little late considering where\nwe are today.\nThoughts?\n. brilliant idea\n. tx @drolando for the idea. PS the original design for cassandra duration query proved unsupportable, which was why it was pulled. It was redone in cassandra3.. one of the goals was to fix that. ironic that com.twitter.chill is in that stack.. they need to show zipkin\nsome love!. PS here's the related issue in kryo about auto-value https://github.com/magro/kryo-serializers/issues/48. you sure this is kryo 2.24? that version is almost 4 years old. https://github.com/openzipkin/zipkin/pull/1880. chatting with Nara, it seems fair to strip autovalue from the primary model types, not just span. This will if nothing else prevent distractions about whether or not autovalue is interfering with things.\nNote: I'm totally unkeen on the awful default constructor jazz of normal java serialization which is why I re-implemented serialization hooks like we had in zipkin v1.\nNEXT ACTION: update this PR removing autovalue from Endpoint Annotation and DependencyLink. @narayaruna this should be considered done I think.. actually a couple toString polish but that's it. so you are not able to use the in-memory, mysql, elasticsearch or the\npending influxdb? which storage backend are you looking for?\n. It seems like you could do this work as post-processing like via a flink or\nspark job? This could result in less code. For example, use one of the\nnormal zipkin backends, then hang a job off the storage to do\npost-processing into the categories you mention.\nOn Mon, Jan 15, 2018 at 8:16 PM, pickmonster notifications@github.com\nwrote:\n\n@adriancole https://github.com/adriancole mysql, First I need to\nanalyze the address and parameters based on a different URL. And then\nrecord the number of calls to different interfaces based on different\naddresses.Finally accumulate these quantities into the database.\nSo I should start with the storage path in the Zipkin project, then go to\nthe MySQL path of zipkin-storage project to process all processes?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1883#issuecomment-357666863,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD614xZb5i0YYsz9i0rssKtQfDKPUmCks5tK0GrgaJpZM4ReQPm\n.\n. you can look at this for how to change zipkin into something that exports\ntraces to another backend. You don't need to modify anything but you do\nneed to implement a StorageComponent\nhttps://github.com/GoogleCloudPlatform/stackdriver-zipkin/pull/48\n\nOn Mon, Jan 15, 2018 at 8:50 PM, pickmonster notifications@github.com\nwrote:\n\n@adriancole https://github.com/adriancole Its mean I neednt use\nzipkin-storage-mysql. I just write one class which is implement\nStorageAdapters.SpanConsumer, then storage my object bean to database. All\ncodes put in normal zipkin project. Is it right?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1883#issuecomment-357674201,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD619ygBDkQqDYTxMVhvH4eZ7ckku2Lks5tK0mngaJpZM4ReQPm\n.\n. interesting! for a impl summary, is the trace in browser local storage or is it sortof a one-shot deal? Can you attach screen shots? Is there anything sharable between traceViewer.mustache and anything else?. https://groups.google.com/a/lists.datastax.com/forum/#!topic/java-driver-user/rg80-97gJ-k. fingers crossed on the travis build! Love the solution. \u2764\ufe0f . Heh wow.. did you get a clash with mysql? Wonder if your trace id generator\nis working ok as usually clashing on 128bit would require more load than\nour schema would seem possible to support querying of\n. ahh I understand. So right now, our schema is biased towards 128-bit joins...\n\nIs this the join you are looking to optimize? If so, wouldn't the index change need to occur with a query that uses it? Or is it fine to just do it like this?\nreturn ZIPKIN_SPANS.TRACE_ID_HIGH.eq(annotationTable.TRACE_ID_HIGH)\n          .and(ZIPKIN_SPANS.TRACE_ID.eq(annotationTable.TRACE_ID))\n          .and(ZIPKIN_SPANS.ID.eq(annotationTable.SPAN_ID));. the index right now is not parameterized. This means it is setup for when\nthe trace_id_high field is present. The joinCondition primarily serves old\nschemas who haven't upgraded to include the more recent trace_id_high\nfield. So, in this case we don't need to worry about the index part I think.\nOn Tue, Jan 23, 2018 at 11:30 AM, C_G notifications@github.com wrote:\n\n@cgfork commented on this pull request.\nIn zipkin-storage/mysql/src/main/java/zipkin/storage/mysql/\nMySQLSpanStore.java\nhttps://github.com/openzipkin/zipkin/pull/1892#discussion_r163135884:\n\n@@ -271,7 +271,8 @@ private Endpoint endpoint(Record a) {\n           .selectDistinct(ZIPKIN_SPANS.NAME)\n           .from(ZIPKIN_SPANS)\n           .join(ZIPKIN_ANNOTATIONS)\n-          .on(ZIPKIN_SPANS.TRACE_ID.eq(ZIPKIN_ANNOTATIONS.TRACE_ID))\n+          .on(ZIPKIN_SPANS.TRACE_ID_HIGH.eq(ZIPKIN_ANNOTATIONS.TRACE_ID_HIGH))\n\nI think the index is not available without trace_id_high when seach or\njoin because the trace_id_high is the first key of the INDEX(trace_id_high,\ntrace_id, id) and INDEX(trace_id_high, trace_id). So the trace_id_high\nadded can optimize the join or the query. I think it is necessary to\nrefactor the Schema.joinCondition(). Or change the index?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1892#discussion_r163135884,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD618oXUHoUmrSCiXWQAsumfdO9gu55ks5tNVJWgaJpZM4Ro9_s\n.\n. this should do what you want https://github.com/openzipkin/zipkin/pull/1909. please do! thanks for your continued help\n\nOn Thu, Feb 1, 2018 at 11:33 AM, Logic-32 notifications@github.com wrote:\n\nMe and git really don't git along. My apologies for the noisy previous\ncommits. Hopefully a correct one will follow shortly. Do you want a pull\nrequest for it when it does?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1893#issuecomment-362376751,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61_oxzRYw2seQoCJzj9fqI7b1JQW8ks5tQhGGgaJpZM4RpPEJ\n.\n. ps no it doesn't sync. we have to copy/paste it. docker doesn't always listen on localhost, for example, if docker machine it might be 192.168.99.100. @abitrolly if docker is running native, wouldn't docker run -d -p 127.0.0.1:9411:9411 openzipkin/zipkin fail because it is already localhost?. @abesto @jcchavezs can either of you weigh-in on the docker advice? using docker-machine the instructions don't seem to work as just results in socket disconnect. merging to eliminate having to re-answer this again. any work on the links and/or most reliable docker syntax can happen later. ex in my machine (docker-machine) the port forwarding resulted in connection reset. The salient part here is the request to replace your_host with 127.0.0.1\nand how to ensure such a suggestion won't result in more questions than it\nclarifies.\n\nFor example, in the docker project we use the docker ip command vs assuming\nwe can map port 9411.\nOn 7 Feb 2018 3:02 pm, \"Zolt\u00e1n Nagy\" notifications@github.com wrote:\n\nFeels like I'm missing some context here, but would love to help out. What\nare you trying to accomplish? Something like this? \u201cStart up the latest\nZipkin release with Docker and send in some traces so that I can click\naround\"? And if so, what's the problem, where's the connection reset /\ndisconnect? Between the trace generator and Zipkin, or the browser and\nZipkin?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1894#issuecomment-363777680,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD619jR8eWifahncohg5TZnrxwBGsB-ks5tSazggaJpZM4RpmE5\n.\n. Overlay of two spans with same ID could use bubbles too and name events\nsame as before (at least initially)\n\nOn 25 Jan 2018 12:05 am, \"Logic-32\" notifications@github.com wrote:\n\nQuestion: how will this work with the UI gets updated to fully support V2,\nwhich technically doesn't have SR/SS annotations? Currently, V2 spans get\nconverted back to V1 format, so it works. But will they still do that going\nforward?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1895#issuecomment-360183230,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6136nLr0vhDBAt9-ZtULkm8SFdfXEks5tN1SygaJpZM4RqfnM\n.\n. circle ci hates us. fails so often in different projects, tempted to find an alternative second CI cc @openzipkin/devops-tooling . thanks @jcchavezs!. @cgfork  what do you think? our JOOQ code is mysql dialect anyway so as long as we know the trace_id_high column exists, this (forcing index) might be ok I guess... do you have a custom build or are you just downloading the all jar?\n\nOn Fri, Jan 26, 2018 at 8:31 AM, Philipp Krenn notifications@github.com\nwrote:\n\nIn 2.4.5 I'm running into the following error in V2SpanConverter.java\nhttps://github.com/openzipkin/zipkin/blob/5b6efdc70a83fca6e9b09bf8df5a3a861caebf9e/zipkin/src/main/java/zipkin/internal/V2SpanConverter.java#L200\nwhen trying to receive a V2 span in Zipkin:\njava.lang.NoSuchMethodError: zipkin2.Span$Builder.timestamp(J)Lzipkin2/Span$Builder;\n  at zipkin.internal.V2SpanConverter$Builders.maybeTimestampDuration(V2SpanConverter.java:200)\n  at zipkin.internal.V2SpanConverter$Builders.processAnnotations(V2SpanConverter.java:130)\n  at zipkin.internal.V2SpanConverter.fromSpan(V2SpanConverter.java:49)\n  at zipkin.collector.Collector.accept(Collector.java:143)\n  at zipkin.internal.Collector.acceptSpans(Collector.java:62)\n  at zipkin.collector.Collector.acceptSpans(Collector.java:114)\n  at zipkin.server.ZipkinHttpCollector$HttpCollector.handle(ZipkinHttpCollector.java:126)\n  at io.undertow.io.AsyncReceiverImpl.receiveFullBytes(AsyncReceiverImpl.java:399)\n  at zipkin.server.ZipkinHttpCollector.handleRequest(ZipkinHttpCollector.java:98)\n  at zipkin.server.CorsHandler.handleRequest(CorsHandler.java:77)\n  at io.undertow.server.handlers.HttpContinueReadHandler.handleRequest(HttpContinueReadHandler.java:65)\n  at io.undertow.server.Connectors.executeRootHandler(Connectors.java:332)\n  at io.undertow.server.protocol.http.HttpReadListener.handleEventWithNoRunningRequest(HttpReadListener.java:254)\n  at io.undertow.server.protocol.http.HttpReadListener.handleEvent(HttpReadListener.java:136)\n  at io.undertow.server.protocol.http.HttpOpenListener.handleEvent(HttpOpenListener.java:151)\n  at io.undertow.server.protocol.http.HttpOpenListener.handleEvent(HttpOpenListener.java:92)\n  at io.undertow.server.protocol.http.HttpOpenListener.handleEvent(HttpOpenListener.java:51)\n  at org.xnio.ChannelListeners.invokeChannelListener(ChannelListeners.java:92)\n  at org.xnio.ChannelListeners$10.handleEvent(ChannelListeners.java:291)\n  at org.xnio.ChannelListeners$10.handleEvent(ChannelListeners.java:286)\n  at org.xnio.ChannelListeners.invokeChannelListener(ChannelListeners.java:92)\n  at org.xnio.nio.QueuedNioTcpServer$1.run(QueuedNioTcpServer.java:129)\n  at org.xnio.nio.WorkerThread.safeRun(WorkerThread.java:582)\n  at org.xnio.nio.WorkerThread.run(WorkerThread.java:466)\nExactly the same code works correctly with 2.4.3 (didn't try 2.4.4 because\nof the stack overflow bug). From a quick glance at the commit history I\nguess this could have slipped in with the recent Kyro\nhttps://github.com/openzipkin/zipkin/commit/7e6cfdda11fd6990aadcff542141efa080eae340#diff-da4d176c2fbbcd423c140692a19b8dbfL275\nchange?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1898, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6166K7-VQ9pLmZ40N0zj2fznXiq8hks5tORz4gaJpZM4Rtsr0\n.\n. wild guess is that this is a gradle namespacing issue here https://github.com/spring-projects/spring-boot/issues/10778\n\nIf making a custom server with the spring boot gradle plugin, misalign io.zipkin.java:zipkin one patch level less that io.zipkin.zipkin2:zipkin as otherwise an artifact name clash can manifest non-obvious problems.\nex\ngradle\ncompile 'io.zipkin.zipkin2:zipkin:2.4.5'\n// misaligned intentionally https://github.com/spring-projects/spring-boot/issues/10778\ncompile 'io.zipkin.java:zipkin:2.4.4'. Thanks for the follow-up too, Philipp!\n. what storage backend are you using?\n. you can set\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-server#elasticsearch-storage\nES_HTTP_LOGGING to emit the http queries that occur from zipkin to\nelasticsearch. It might explain why there are no results\nOn Fri, Jan 26, 2018 at 6:25 PM, CyberDick notifications@github.com wrote:\n\nI use Elasticsearch 5.4.1.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1899#issuecomment-360742010,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61yR4v47fqfYteiFO629sQBHuRcIQks5tOagxgaJpZM4Rt8LQ\n.\n. Great work! We can probably add fallback functionality for read-only apis\n. By the way.. your server is dropping connections due to load. While we can\nhide the error might want to consider reducing traffic, adding servers or\nincreasing your max connections on this one. Ex you could make a separate\nserver for serving UI and not send write traffic to it.\n. Thanks very much. we had a question about this just a week ago. agree. we can add support for this and document like you said, ignoring the other properties.\n\nwanna have a go?. thx!. sweet!. I will merge this and if someone complains re-nag about i18n. from latest it actually appears the property is set but not having the\ndesired effect...\nZIPKIN_UI_SEARCH_ENABLED=false java -jar zipkin.jar\ncurl -s localhost:9411/configprops|jq\n'.\"zipkin.ui-zipkin.autoconfigure.ui.ZipkinUiProperties\"'\n{\n\"prefix\": \"zipkin.ui\",\n\"properties\": {\n*\"defaultLookback\"**: *3600000*,*\n\n*\"environment\"**: *\"\"*,*\n\n*\"instrumented\"**: *\".*\"*,*\n\n*\"dependency\"**: {*\n\n  *\"lowErrorRate\"**: *0.5*,*\n\n  *\"highErrorRate\"**: *0.75\n\n\n\n},*\n\"basepath\": \"/zipkin\",\n\"queryLimit\": 10,\n\"logsUrl\": null,\n\"searchEnabled\": false\n\n\n}*\n\n\n}\n. zipkin data is always UTC time. no timezones are stored. In fact, timestamps are epoch microseconds https://zipkin.io/zipkin-api/#/default/post_spans. zipkin data is always UTC time. no timezones are stored. In fact, timestamps are epoch microseconds https://zipkin.io/zipkin-api/#/default/post_spans. A lot of services set the root span to have the same span ID as the trace\nID. This helps from generating another random number.\nIf there are a lot of spans, it could be because many operations are in the\nsame trace.\nPlease prefer stack overflow or https://gitter.im/openzipkin/zipkin for\nquestions as github issues are for changes to the codebase.\nThis type of concern is a question, not a change, so please contact on\nhttps://gitter.im/openzipkin/zipkin for questions or troubleshooting\nOn Mon, Feb 5, 2018 at 5:38 AM, CyberDick notifications@github.com wrote:\n\n[image: image]\nhttps://user-images.githubusercontent.com/15179117/35788627-6d8259ba-0a71-11e8-945a-0cd708cb9dc8.png\n[image: image]\nhttps://user-images.githubusercontent.com/15179117/35788625-6925dfa4-0a71-11e8-82c9-2270aaecc405.png\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1905, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD611bQjwTFIp_YI7kPCPoM58tojG7hks5tRoW3gaJpZM4R46Md\n.\n. Yeah these names are unfortunate. It would be better to use a shorter name\nand add a tag with the huge ones.\n\nThere is no plan per se as this is all volunteer. This will sit until\nsomeone who knows how to do something better comes up with an idea.\nOn 5 Feb 2018 9:52 pm, \"Zden\u011bk Sojma\" notifications@github.com wrote:\n\nHello, we just started using zipkin together with linkerd. Unfortunatelly,\nbecause of long names of linkerd services, the trace GUI is not very\nclear... Here is a screen:\n[image: image]\nhttps://user-images.githubusercontent.com/28751135/35827845-cf4d01c6-0abd-11e8-96c9-6507ef84399f.png\nPlease, is there a possibility / some setting / configuration how to\nimprove it so texts do not overlap each other? Or do you plan to improve\nthe appereance?\nThank you\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1906, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610Ux9uB-lQRoa49HWaiUC5XLgGCEks5tR2oPgaJpZM4R6FL_\n.\n. @zeagord I don't know any better option for this situation. I'd go with the service name clipping until there's another way cc @klingerf. fyi https://github.com/openzipkin/zipkin/pull/1919. thanks @tonyd3!. The i18n library is just looking for refined data right? (Ex american\ndialect). 404 is not an error per se, just not finding something better\n. maybe look at this related commit? 54f66a72389b854ae6ba734cf9ddd6b9cc1a8cc1 It is possible that what needs to change is upstream in https://github.com/jquery-i18n-properties but maybe it is configurable cc @igorwwwwwwwwwwwwwwwwwwww . leaving this open a bit longer as I don't know the strategies available to avoid 404's. Maybe we can help upstream.. possibly by keeping a config file of known properties files? (ex the server can compute this so there's no maintenance). leaving this open a bit longer as I don't know the strategies available to avoid 404's. Maybe we can help upstream.. possibly by keeping a config file of known properties files? (ex the server can compute this so there's no maintenance). thanks for reporting.. will look into it. In the mean time, please disable\nself tracing\n. 2.4.7 will fix this. it is going out now. I'll let @jcchavezs do the honors :). cc @igorwwwwwwwwwwwwwwwwwwww can you please weigh-in on this?. @ScienJus go for it. agreed we should look at somehow optimizing elasticsearch for all query if someone knows how (raise a separate issue for that). fair enough. thanks!. thanks for the help!. thanks for the polish.. shines better now. added more logging details in the READMEs. >\nJust out of curiosity, when ensureSchema is enabled, aren't you creating 2\nkeyspaces here?\nyes. one is a template for the UDTs (due to them being defined on\nannotations I think) I tried to remove it by\nhttps://github.com/openzipkin/zipkin/pull/1891, but seems like a different\ntopic.\n\nMaybe @michaelsembwever @llinder can help us write a better comment or note\nin the readme. Even if replying here, I don't mind copy/pasting\n. Fyi originally schema install was optional because the defaults were not\ngood for production only tests (ex rf 1 is for test readback). I think in\nsome other issue there were some people discussing setup better for prod.\nIf we do start implying the default schema is production grade, then yeah\nwe can do some poor person's templating like we do in elasticsearch.\nHowever, partial changes get a bit complex to apply so people may still end\nup in a position of having their own private copy of schema.. just\nmentioning config impact.\nAnother option is to employ another tool if exists in cassandra ecosystem\nfor managing partial schema updates etc.\nAnyway I am also interested in what are good things, what is low hanging\nfruit etc.\n. Ahh.. is this the only change you need? It would be easier to punch a hole\nfor this enum than map a configuration object for all c* options or add a\ndependency on spring config that likely would break once v4 of the driver\nis out.\nAlso we can better explain why (we have limited config but at least\nrecently try to explain why something matters.\nOn 16 Feb 2018 1:52 pm, \"Daniele\" notifications@github.com wrote:\n\nI'm not using the default schema. I changed the keyspace to use the\nNetworkTopologyStrategy, but I still want RF=1.\nI think I haven't explained myself well, what I want to make configurable\nis https://github.com/openzipkin/zipkin/blob/\nf309c19345069eb09604f09858d427bdb1d3e18a/zipkin-storage/\nzipkin2_cassandra/src/main/java/zipkin2/storage/cassandra/\nDefaultSessionFactory.java#L126\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1920#issuecomment-366150439,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD614pfXoUhe8obDnTIiQy18kFbzZV8ks5tVReKgaJpZM4SH3zp\n.\n. @drolando because things land in support there probably is at least a little cause to have concern about setting traps. Usually we document settings with advice on how to use them. If the advice is to please don't use this, it certainly would come at odds with this. It is our concern because we don't have paid support crew to unwind things and all knobs are a way to shoot one's self. We do get a lot of questions we aren't directly responsible for.\n\nTo your point though.. have you found anything mick said to be helpful? In hindsight do you really want to set this, and under what circumstance would you. Can you help write advice?\nIf purely experimental in nature (like what if we click XX?), that's fine but maybe not something to bake in a flag for. We support each thing and it does impact the future, limits how we can change drivers etc. Appreciate your insight regardless and happy to assist if folks stepping back can recommend in good faith exposing this.. thanks for your help @huydx!. very nice!. haha nice branch name!. haha nice branch name!. I think the original solution to this was the zoom box.. (click and drag outline over the bunched up things) is there another way you can think of. Verified this fixes name priority using brave-webmvc-example (with route-based span names)\n\n. @shakuzen wdyt about  'ES_DATE_SEPARATOR=none' ? is there a better way to indicate empty in boot that isn't easily conflated with unset?. @shakuzen wdyt about  'ES_DATE_SEPARATOR=none' ? is there a better way to indicate empty in boot that isn't easily conflated with unset?. empty works, it was something else. https://github.com/openzipkin/zipkin/pull/1941. [A]. a week and no screams for the other order and no alternatives mentioned. Switching and if someone yells, we'll know A wasn't unanimous. heh seems I smoked out a [B] from @basvanbeek \n\nTypically the amount of spans happening in production make it so that newest first has little value. Inspecting slow paths is more likely. In dev newest first is preferable\n\nI certainly agree that newest first is nice when you have very little data... We can use config property I think. There are existing config properties for UI. @Logic-32 want to give a try?. @Logic-32 hopefully I can take some heavy lifting off. if you want to just attack this one, would be appreciated (the stickiness)\nWondering if we shouldn't change the default in the mean time.. it is an optimization question until sticky.. do we make it more difficult for existing users or new ones. Currently it is more difficult for new users and that not only annoys them but likely adds support pressure one way or another.. thanks for raising this. you aren't the first. I think the impl will be the key in annotationQuery from the http api. This can either be a tag key or a timestamp annotation. I think a list of these will be fine (for implementations that do manual indexing like ES and Cassandra)\ncc @llinder @michaelsembwever @anuraaga @devinsba. great. thanks again @igorwwwwwwwwwwwwwwwwwwww !. great. thanks again @igorwwwwwwwwwwwwwwwwwwww !. Works for me. If we do, we should update the webpack files to use the same\nnomenclature or even have a comment with the variable chosen. That way grep\nbased troubleshooting is easier\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-ui/webpack.config.js\nOn 25 Feb 2018 3:39 pm, \"Zolt\u00e1n Nagy\" notifications@github.com wrote:\n\nAs discussed in #1731 https://github.com/openzipkin/zipkin/issues/1731.\nIn situations where we run behind a reverse proxy with limited\nconfigurability (like the nginx ingress of Kubernetes), we can't rely on\nthe reverse proxy injecting the required  tag to make the UI work\non whatever URL path prefix it's assigned. The cleanest solution would be\nadding support to the Spring application serving the UI for adding the tag\nbased on the presence and value of an ENV var.\nI propose we call that ENV var ZIPKIN_UI_BASEPATH. Via @wdittmer\nhttps://github.com/wdittmer, other applications all have custom names\nfor it, there doesn't appear to be a standard:\nFor example, for Prometheus we need to configure:\n\n'--web.external-url=http://management.test.cbo:13370/prometheus'\n'--web.route-prefix=/'\n\nFor Grafana:\n\nname: GF_SERVER_ROOT_URL\n   value: /grafana\n\nFor Kibana:\n\nname: SERVER_BASEPATH\n   value: \"/kibana\"\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1930, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wZcciuaWUB49auCLKIpWNqpKRlRks5tYQ4-gaJpZM4SSM9S\n.\n. Works for me. If we do, we should update the webpack files to use the same\nnomenclature or even have a comment with the variable chosen. That way grep\nbased troubleshooting is easier\n\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-ui/webpack.config.js\nOn 25 Feb 2018 3:39 pm, \"Zolt\u00e1n Nagy\" notifications@github.com wrote:\n\nAs discussed in #1731 https://github.com/openzipkin/zipkin/issues/1731.\nIn situations where we run behind a reverse proxy with limited\nconfigurability (like the nginx ingress of Kubernetes), we can't rely on\nthe reverse proxy injecting the required  tag to make the UI work\non whatever URL path prefix it's assigned. The cleanest solution would be\nadding support to the Spring application serving the UI for adding the tag\nbased on the presence and value of an ENV var.\nI propose we call that ENV var ZIPKIN_UI_BASEPATH. Via @wdittmer\nhttps://github.com/wdittmer, other applications all have custom names\nfor it, there doesn't appear to be a standard:\nFor example, for Prometheus we need to configure:\n\n'--web.external-url=http://management.test.cbo:13370/prometheus'\n'--web.route-prefix=/'\n\nFor Grafana:\n\nname: GF_SERVER_ROOT_URL\n   value: /grafana\n\nFor Kibana:\n\nname: SERVER_BASEPATH\n   value: \"/kibana\"\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1930, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61wZcciuaWUB49auCLKIpWNqpKRlRks5tYQ4-gaJpZM4SSM9S\n.\n. if all that needs to change is index.html I'd be inclined to solve it that way. regardless, we can document the more literal interpretation for now (which is that a message processor missing consumer instrumentation should probably make a faux consumer span so that there's parity). regardless, we can document the more literal interpretation for now (which is that a message processor missing consumer instrumentation should probably make a faux consumer span so that there's parity). closing this for now. we can revisit it if we feel like it with this context kept. >\nSo maybe it isn't a big issue that things like jar -xf won't work?\nHmm does jar -xf actually not work? Feels like I would have tried and\nnoticed this in the past. Maybe the docs in boot are overly cautious? At\nany rate, this won't be a war file, just a standalone. If jar -xf doesn't\nwork on JRE 8+ I'd be surprised, but still we could consider anyway\n. >\nSo maybe it isn't a big issue that things like jar -xf won't work?\nHmm does jar -xf actually not work? Feels like I would have tried and\nnoticed this in the past. Maybe the docs in boot are overly cautious? At\nany rate, this won't be a war file, just a standalone. If jar -xf doesn't\nwork on JRE 8+ I'd be surprised, but still we could consider anyway\n. interestingly I tried. jar -tf works in JDK 1.8+, jar -xf doesn't work\nin 1.8, but it does in version 9\n\nspring boot's header is also quite large.. trying with a smaller one\nto see if that was why 1.8 doesn't work (for completeness, not because\nI feel using jar instead of unzip is a critical feature)\n. interestingly I tried. jar -tf works in JDK 1.8+, jar -xf doesn't work\nin 1.8, but it does in version 9\nspring boot's header is also quite large.. trying with a smaller one\nto see if that was why 1.8 doesn't work (for completeness, not because\nI feel using jar instead of unzip is a critical feature)\n. nope. making the shell header smaller doesn't make jar from 1.8 work.\nPersonally, I'm very unconcerned by this limitation. We don't use jar\nto extract files anyway, and due to support implications 1.8 will\nphase out as a concern anyway\nhttps://github.com/openzipkin/docker-zipkin/blob/master/zipkin/Dockerfile#L29\nITOW, unless others feel strongly, I'm in favor of making this executable\n. nope. making the shell header smaller doesn't make jar from 1.8 work.\nPersonally, I'm very unconcerned by this limitation. We don't use jar\nto extract files anyway, and due to support implications 1.8 will\nphase out as a concern anyway\nhttps://github.com/openzipkin/docker-zipkin/blob/master/zipkin/Dockerfile#L29\nITOW, unless others feel strongly, I'm in favor of making this executable\n. ps in the advice against doing this if doing \"java -jar\" I currently\ndon't see impact to this in 1.8 or 9, though it might be possible. We\nshould check that using the modules (ex kafka10) there's no weird\nloading problem\n. ps in the advice against doing this if doing \"java -jar\" I currently\ndon't see impact to this in 1.8 or 9, though it might be possible. We\nshould check that using the modules (ex kafka10) there's no weird\nloading problem\n. @zeagord do it!. @zeagord do it!. thx. ps unzip has a problem on alpine (eventhough it doesn't on osx) caused the builds to fail last few releases https://quay.io/repository/openzipkin/zipkin?tab=builds. added a temporary hack until @abesto or someone tells me a better way. respinning releases 2.5.1 forward in docker https://github.com/openzipkin/docker-zipkin/commit/72f5b108ff03ce1a851581f2625382edc9671f0a. This is a better solution. Lets go with it. Nice find!\nOn Tue, 20 Mar 2018 05:01 Zolt\u00e1n Nagy, notifications@github.com wrote:\n\nI mean... wow, this is new to me. So technically zip is correct, that's\nnot a valid zip archive :D It starts with a shell script. I'd note that\nunzip can extract the archive even on Alpine, but then complains about\nthe archive not being valid. Here's one way to both fail on other errors,\nand let the build pass:\nzip --fix zipkin-server.jar --out zipkin-server.zip\nunzip zipkin-server.zip\n``\nWhere zip --fix strips out the shell script from the beginning, restoring the general assumption that a jar is a zip.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1932#issuecomment-374375301,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD618hzUwqlaBRZ__kSldN3tVW5lfYiks5tgByrgaJpZM4SWAAe\n.\n. nice idea\n. nice idea\n. good idea. we can detect malformed traces for many cases\n. should we change the README, too? or leave that out cc @abesto \n\nEx the quick start \"could\" strip the jar name and let people just ./zipkin\nhttps://github.com/openzipkin/zipkin#quick-start. should we change the README, too? or leave that out cc @abesto \nEx the quick start \"could\" strip the jar name and let people just ./zipkin\nhttps://github.com/openzipkin/zipkin#quick-start. if we did something like this, we would have to think also about the extension approach (ex adding SQS via a module). in these scenarios, java -jar is still the right choice. if we did something like this, we would have to think also about the extension approach (ex adding SQS via a module). in these scenarios, java -jar is still the right choice. for the readme we could add text below like:\nIf running as a service you can also leave out the \"java -jar \" part. Ex.\nbash\n$ ./zipkin.jar. for the readme we could add text below like:\nIf running as a service you can also leave out the \"java -jar \" part. Ex.\nbash\n$ ./zipkin.jar. ok maybe add a section to the zipkin-server/README and leave the root one\nalone then? ex a section on running as a service\nOn Thu, Mar 1, 2018 at 1:16 PM, Tommy Ludwig notifications@github.com\nwrote:\n\nMentioning the option in the readme sounds reasonable, but it isn't usable\nfor all Windows users for instance. It also could just generally be\nunfamiliar for Java developers used to working with normal JARs. I think\nthe default advice remaining using java -jar is better. But mentioning\nthat it can be registered as a service and linking to the corresponding\nSpring Boot documentation seems like a good idea to me.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1934#issuecomment-369476515,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD613FTbELwfFj9FZ8LIgzQkZWHvnCdks5tZ4QrgaJpZM4SXvfO\n.\n. ok maybe add a section to the zipkin-server/README and leave the root one\nalone then? ex a section on running as a service\n\nOn Thu, Mar 1, 2018 at 1:16 PM, Tommy Ludwig notifications@github.com\nwrote:\n\nMentioning the option in the readme sounds reasonable, but it isn't usable\nfor all Windows users for instance. It also could just generally be\nunfamiliar for Java developers used to working with normal JARs. I think\nthe default advice remaining using java -jar is better. But mentioning\nthat it can be registered as a service and linking to the corresponding\nSpring Boot documentation seems like a good idea to me.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1934#issuecomment-369476515,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD613FTbELwfFj9FZ8LIgzQkZWHvnCdks5tZ4QrgaJpZM4SXvfO\n.\n. OK I think it is best to not document this :) Not documenting it won't confuse people who already work with this as a jar, yet still allows folks to do \"pretend I'm a binary\" mode if they want.\n\nWe can always add docs later if we get questions about it. I am supportive of this, but bearing in mind many know Markdown but not\nAsciidoctor, we will need support.\nCan we have a couple volunteer asciinurses? Basically to help if folks\nstruggle while transitioning.\n. we also mentioned recently that for data to appear, applications need to be\nsending traces\nhttps://github.com/openzipkin/zipkin#quick-start\nyou can tell also by hitting the /metrics endpoint and look at stats named\ncollector\nOn Fri, Mar 2, 2018 at 4:47 PM, JacobAidenSon notifications@github.com\nwrote:\n\nI run my zipkin server with openzipkin zipkin-server#elasticsearch\nhere is my command\nSTORAGE_TYPE=elasticsearch\nES_HOSTS=http://host1:9200,http://host2:9200\njava -jar zipkin.jar\nES's version is 5.2.2\nthis ES is already using with other server.\nso I think, this ES doesn't have any problem.\nzipkin is running and logging is successful.\nbut I can't find any data of zipkin in ES.\nso It isn't persistent.\nIs there any additional option to run openzipkin.jar with ES?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1939, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD618RiE_Angz0WYTKCkaxwmeLpZKu9ks5taQcugaJpZM4SZkoL\n.\n. verified using @yschimke's script\n\nbash\n$ for i in */target/*-SNAPSHOT.jar; do echo $i; unzip -q -c $i META-INF/MANIFEST.MF | grep Automatic-Module-Name ; done\nbenchmarks/target/benchmarks-2.4.10-SNAPSHOT.jar\nzipkin-guava/target/zipkin-guava-2.4.10-SNAPSHOT.jar\nzipkin-junit/target/zipkin-junit-2.4.10-SNAPSHOT.jar\nzipkin-server/target/zipkin-server-2.4.10-SNAPSHOT.jar\nzipkin-ui/target/zipkin-ui-2.4.10-SNAPSHOT.jar\nzipkin-zookeeper/target/zipkin-zookeeper-2.4.10-SNAPSHOT.jar\nzipkin/target/original-zipkin-2.4.10-SNAPSHOT.jar\nzipkin/target/zipkin-2.4.10-SNAPSHOT.jar\nAutomatic-Module-Name: zipkin\nzipkin2/target/original-zipkin-2.4.10-SNAPSHOT.jar\nzipkin2/target/zipkin-2.4.10-SNAPSHOT.jar\nAutomatic-Module-Name: zipkin2. @drolando was also recently confused by the \"more info\" button. Sounds like we have a choice between deleting it or renaming it to something less confusion like \"show ids\". I think the reason why we hid it was that the span detail page was a bit cluttered. https://github.com/openzipkin/zipkin/pull/2290 is a stab at improving. @drolando fyi cassandra 3.11.3 is coming fast and once this merges should affect indexing a lot. rebased in preparation of the pending release of cassandra 3.11.3. pull request welcome or tell us if help is needed.\nthe server is stable with v2 api at this point although i think the swagger\napi doesnt discuss POST with proto3\n. thx for having a look, tommy. @wdittmer thx good catch. thanks tons. thanks tons. closed via #1974 . closed via #1974 . We reverted this as we didn't have good test coverage. Basically the strict trace ID setting still includes the ability to read data prior to when the switch happened. That is getTrace_retrievesBy128BitTraceId_afterSwitch test.\nThis makes the issue more difficult because on one hand, if data is uniform, we should totally never even try to do mixed stuff. OTOH, we do want people to use strict ID, so we have to facilitate migrations.\nI think we can handle this a little differently by looking at the length of the trace IDs inside the search results.\nOne approach is: look to see if any trace IDs clash on lower 64bits. If there is no clash, don't filter.\n. nice catch. nice catch. PS this didn't do everything, it just made it less things breaking :) PublicMetrics is still gone. PS this didn't do everything, it just made it less things breaking :) PublicMetrics is still gone. https://github.com/openzipkin/zipkin/issues/1977 will continue this. https://github.com/openzipkin/zipkin/issues/1977 will continue this. @zeagord @shakuzen either of you game for this one?. @zeagord @shakuzen either of you game for this one?. https://github.com/openzipkin/docker-zipkin/pull/173 adjusts the docker image. https://github.com/openzipkin/docker-zipkin/pull/173 adjusts the docker image. closed by #1980. closed by #1980. sounds good, but should this note be in the server readme, here or both?. sounds good, but should this note be in the server readme, here or both?. Might be useful to link to our httpd doc somehow as proxying the UI is a\nbit tricky\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-ui#apache-http-as-a-zipkin-reverse-proxy\n. thanks so much!. this is to reduce the indexing cost, as the primary consumer of this index\nis zipkin api which doesn't have fuzzy search\n. this is to reduce the indexing cost, as the primary consumer of this index\nis zipkin api which doesn't have fuzzy search\n. cc @shakuzen . cc @shakuzen . red herring. This was all because we interpreted an empty variable. fixing now. red herring. This was all because we interpreted an empty variable. fixing now. wild guess is that your storage layer cannot keep up, causing a backlog on\nthe server. Could that be the case?\n. wild guess is that your storage layer cannot keep up, causing a backlog on\nthe server. Could that be the case?\n. cool. glad you found a way to shed load\n. cool. glad you found a way to shed load\n. This i18n in question is javascript and runs in the browser. There is\ncurrently no property to control language choices. Ir one existed, the\nbrowser would read the property from config.json\n. This i18n in question is javascript and runs in the browser. There is\ncurrently no property to control language choices. Ir one existed, the\nbrowser would read the property from config.json\n. the language is detected from your browser. We have had contributions for a\ncouple languages. Are you looking for a different language? or is it that\nyour language isn't detected properly from your browser.\nhttps://github.com/jquery-i18n-properties/jquery-i18n-properties\n. the language is detected from your browser. We have had contributions for a\ncouple languages. Are you looking for a different language? or is it that\nyour language isn't detected properly from your browser.\nhttps://github.com/jquery-i18n-properties/jquery-i18n-properties\n. you are using latest zipkin right?\nhttps://github.com/openzipkin/zipkin#quick-start\n. you are using latest zipkin right?\nhttps://github.com/openzipkin/zipkin#quick-start\n. I have some tips here, maybe read them and see if it helps?\nhttps://github.com/openzipkin/sleuth-webmvc-example/pull/8\n. if using kafka 1.0, you can use this to make your server if not using\ndocker (this is already built-in with docker)\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-autoconfigure/collector-kafka10#quick-start\n. cool I added a section in that guide about Kafka 0.10+ as well. good luck.\n. It looks like you are using the spring-cloud-sleuth-stream-zipkin approach,\nwhich isn't supported like this as the data format isn't zipkin compatible\ntry this\nhttp://cloud.spring.io/spring-cloud-sleuth/1.3.x/single/spring-cloud-sleuth.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka\n. also KAFKA_BOOTSTRAP_SERVERS is an alternative to KAFKA_ZOOKEEPER. you\ndon't use both. also don't change the yaml just use normal env or\nproperties as this will make it easier to troubleshoot.\n. No prob\n. Please create a zuul example on github as I suspect many are unfamiliar\nwith it. Include your workaround. Thanks!\nOn Sat, 14 Apr 2018, 02:17 momentum123, notifications@github.com wrote:\n\nhi @adriancole https://github.com/adriancole @stepanv\nhttps://github.com/stepanv @abesto https://github.com/abesto\nissue discussed at #1732 https://github.com/openzipkin/zipkin/pull/1732\nstill exits in latest 2.7.1\non hitting localhost:9411 it auto redirects to /zipkin\nand while accessing thru zuul-gateway\nhttps://localhost:443/zipkin\nit can not resolve all the components at\nhttps://localhost:443/zipkin/(resource names)\nwhich should actually be\nhttps://localhost:443/zipkin/zipkin/(resource names)\nalthough i have done some manaul labour to append the host name\nfor few resource in my filter but still doesnot work completely\n`\nif(!StringUtils.substringAfterLast(previousUri, \"zipkin/\").isEmpty()){\ntry {\nctx.setRouteHost(new URL(host+\"/zipkin\"));\n} catch (MalformedURLException e)\ne.printStackTrace();\n}\n}\n`\nplease suggest any fix its been long since most of the issues related to\nthis are closed but issue still persists\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1993, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610OqMNtILywdZA_vPBCD7sCGVhdSks5toOvLgaJpZM4TT3rb\n.\n. is that trace in storage?\n. if you cannot access the trace via /api/v2/trace/your_trace_id then the server didn't receive it. Either the trace wasn't sampled or it dropped along the way. You can look at metrics to see if things are healthy https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md#collector. feel free to troubleshoot via gitter https://gitter.im/openzipkin/zipkin\n\nissues are better for changes to the code. Here are the benchmarks. This time will happen once for all endpoints built from the same IP.\nEx. endpoint.toBuilder().serviceName(\"foo\").build() does not re-parse\nBenchmark                             Mode  Cnt  Score   Error  Units\nEndpointBenchmarks.parseIpv4_addr     avgt   15  0.055 \u00b1 0.003  us/op\nEndpointBenchmarks.parseIpv4_literal  avgt   15  0.039 \u00b1 0.001  us/op\nEndpointBenchmarks.parseIpv6_addr     avgt   15  0.047 \u00b1 0.002  us/op\nEndpointBenchmarks.parseIpv6_literal  avgt   15  0.264 \u00b1 0.003  us/op. In normal proto3 generated code, this allows conversion like so:\njava\nEndpoint toProto3(zipkin2.Endpoint input) {\n  Endpoint.Builder result = Endpoint.newBuilder();\n  if (input.ipv6Bytes() != null) {\n    result.setIpv6(ByteString.copyFrom(input.ipv6Bytes()));\n  }\n  // continue..\n  return result;\n}\nof course our encoder will be a lot more efficient than this.. might expose this functionality differently.. will let it sit until the rest of proto3 is done. this api proved to be the most convenient. interop tests for this are in #1998 I will backfill more unit tests tomorrow. going to revise the encoding approach as it isn't as quick as it could be due to multiple length calculations implicit in the format. actually moving speed even faster isn't urgent. will just complete the encoder. all encoding features are now in, but there is some tests left to port from the other pull request which is compat testing against protoc.. ok test coverage is now ... covered. tomorrow I'll add decoding as a separate PR. tomorrow I'll add decoding as a separate PR. all of your http connections are in use. Sometimes people have separate\nservers for collector traffic vs UI.\nES_MAX_REQUESTS controls the amount for the current process\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-server#elasticsearch-storage\n. @shakuzen @marcingrzejszczak can you help vet this issue. I'm concerned about adding more core dependencies to the base server. I thought there were people who are already using PCF with zipkin and rabbit. I'm assuming the important part is below.. the other variables like SPRING_PROFILES_ACTIVE are not needed?\nyaml\n  services:\n    - your-rabbitmq\n  env:\n    RABBIT_URI: ${vcap.services.your-rabbitmq.credentials.uri}\nWe should likely prefix any special instructions with how to provision \"your-rabbitmq\" and put them in the autoconfigure/collector-rabbitmq README with that section linked from the rabbitmq section of the normal server README I guess?. thanks for the details!\n. so it is odd that a local span would not have a timestamp.. that's what happened here. I will filter out so that bad data won't crash the ability to see things.. https://github.com/openzipkin/zipkin/pull/2009 should fix it. thanks. needs a unit test (made mention in the corresponding issue a while back, basically we should have some data to drive this). thanks. needs a unit test (made mention in the corresponding issue a while back, basically we should have some data to drive this). this needs analysis @zeagord @tacigar ?. well done sir!. Thanks for the hard work and review folks. Feel free to make any later polishing post merge. Thanks for the hard work and review folks. Feel free to make any later polishing post merge. ps added this to sort the test that failed 9d1c94b9ded7a316140d694866eda25ec98bee97. ps added this to sort the test that failed 9d1c94b9ded7a316140d694866eda25ec98bee97. thanks!. custom servers are unsupported per https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/java/zipkin/server/EnableZipkinServer.java\nhowever, we will be moving to spring boot version 2.0 soon anyway. Please watch pull requests for status on this, as it is work in progress.. cc @openzipkin/core . whatever our default registry is sounds fine. please followup with a small\nPR (run the affected bench class in IDE just to check)\n. ps the old (pre micrometer) registry was inefficient so we special cased it\nwith buffers. probably should double check the before and after results\njust in case.\n. thanks for this. nice to see more consistent performance\nOn Wed, 25 Apr 2018, 14:45 Raja Sundaram, notifications@github.com wrote:\n\nBelow are the results after running the benchmarks.\nMicrometer\nBenchmark                                              Mode  Cnt  Score   Error  Units\nMetricsBenchmarks.incrementBytes_longSpans_Actuate     avgt  240  0.389 \u00b1 0.008  us/op\nMetricsBenchmarks.incrementBytes_longSpans_inMemory    avgt  240  0.363 \u00b1 0.010  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_Actuate   avgt  240  0.339 \u00b1 0.001  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_inMemory  avgt  240  0.344 \u00b1 0.030  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_Actuate    avgt  240  0.386 \u00b1 0.008  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_inMemory   avgt  240  0.335 \u00b1 0.006  us/op\nPre Micrometer\nBenchmark                                              Mode  Cnt  Score   Error  Units\nMetricsBenchmarks.incrementBytes_longSpans_Actuate     avgt  240  1.550 \u00b1 0.028  us/op\nMetricsBenchmarks.incrementBytes_longSpans_inMemory    avgt  240  0.228 \u00b1 0.001  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_Actuate   avgt  240  1.472 \u00b1 0.010  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_inMemory  avgt  240  0.234 \u00b1 0.004  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_Actuate    avgt  240  1.498 \u00b1 0.012  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_inMemory   avgt  240  0.235 \u00b1 0.002  us/op\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2014#issuecomment-384177876,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6160i4Cjy1sYIknHjXu_0mwSbfIpGks5tsBuZgaJpZM4Tfcc_\n.\n. thanks for this. nice to see more consistent performance\n\nOn Wed, 25 Apr 2018, 14:45 Raja Sundaram, notifications@github.com wrote:\n\nBelow are the results after running the benchmarks.\nMicrometer\nBenchmark                                              Mode  Cnt  Score   Error  Units\nMetricsBenchmarks.incrementBytes_longSpans_Actuate     avgt  240  0.389 \u00b1 0.008  us/op\nMetricsBenchmarks.incrementBytes_longSpans_inMemory    avgt  240  0.363 \u00b1 0.010  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_Actuate   avgt  240  0.339 \u00b1 0.001  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_inMemory  avgt  240  0.344 \u00b1 0.030  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_Actuate    avgt  240  0.386 \u00b1 0.008  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_inMemory   avgt  240  0.335 \u00b1 0.006  us/op\nPre Micrometer\nBenchmark                                              Mode  Cnt  Score   Error  Units\nMetricsBenchmarks.incrementBytes_longSpans_Actuate     avgt  240  1.550 \u00b1 0.028  us/op\nMetricsBenchmarks.incrementBytes_longSpans_inMemory    avgt  240  0.228 \u00b1 0.001  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_Actuate   avgt  240  1.472 \u00b1 0.010  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_inMemory  avgt  240  0.234 \u00b1 0.004  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_Actuate    avgt  240  1.498 \u00b1 0.012  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_inMemory   avgt  240  0.235 \u00b1 0.002  us/op\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2014#issuecomment-384177876,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6160i4Cjy1sYIknHjXu_0mwSbfIpGks5tsBuZgaJpZM4Tfcc_\n.\n. you need to run https://github.com/openzipkin/zipkin-dependencies when\nusing elasticsearch or Cassandra\n\nOn Wed, 25 Apr 2018, 10:10 65725738, notifications@github.com wrote:\n\nes data:\n{\n\"took\": 1,\n\"timed_out\": false,\n\"_shards\": {\n\"total\": 5,\n\"successful\": 5,\n\"failed\": 0\n},\n\"hits\": {\n\"total\": 95,\n\"max_score\": 1,\n\"hits\": [\n{\n\"_index\": \"zipkin:span-2018-04-25\",\n\"_type\": \"span\",\n\"_id\": \"AWL6eLuJMHyfRRHS9yEt\",\n\"_score\": 1,\n\"_source\": {\n\"traceId\": \"05e5a2968f194a2a\",\n\"duration\": 6000,\n\"localEndpoint\": {\n\"serviceName\": \"iot-gate\",\n\"ipv4\": \"192.168.0.44\",\n\"port\": 8765\n},\n\"timestamp_millis\": 1524619470002,\n\"kind\": \"CLIENT\",\n\"name\": \"http:/client/myclient.do\",\n\"id\": \"34ea496e336ace1a\",\n\"parentId\": \"05e5a2968f194a2a\",\n\"timestamp\": 1524619470002000,\n\"tags\": {\n\"http.host\": \"wangbin-PC\",\n\"http.method\": \"GET\",\n\"http.path\": \"/client/myClient.do\",\n\"http.url\": \"\nhttp://wangbin-PC:8766/client/myClient.do?serviceId=iot-gate&secret=123456\n\",\n\"spring.instance_id\": \"wangbin-PC:iot-gate:8765\"\n}\n}\n},\n{\n\"_index\": \"zipkin:span-2018-04-25\",\n\"_type\": \"span\",\n\"_id\": \"AWL6eLntMHyfRRHS9yDI\",\n\"_score\": 1,\n\"_source\": {\n\"traceId\": \"0ae106a1b48e9a4d\",\n\"duration\": 9534,\n\"localEndpoint\": {\n\"serviceName\": \"iot-gate\",\n\"ipv4\": \"192.168.0.44\",\n\"port\": 8765\n},\n\"timestamp_millis\": 1524619710002,\n\"name\": \"refresh-allowed-client\",\n\"id\": \"0ae106a1b48e9a4d\",\n\"timestamp\": 1524619710002000,\n\"tags\": {\n\"class\": \"ServiceAuthUtil$$EnhancerBySpringCGLIB$$9c94d298\",\n\"lc\": \"scheduled\",\n\"method\": \"refreshAllowedClient\"\n}\n}\n},\n{\n\"_index\": \"zipkin:span-2018-04-25\",\n\"_type\": \"span\",\n\"_id\": \"AWL6eLnzMHyfRRHS9yDP\",\n\"_score\": 1,\n\"_source\": {\n\"traceId\": \"a63fe1bd0ba894ae\",\n\"duration\": 19332,\n\"localEndpoint\": {\n\"serviceName\": \"iot-gate\",\n\"ipv4\": \"192.168.0.44\",\n\"port\": 8765\n},\n\"timestamp_millis\": 1524620010002,\n\"name\": \"refresh-allowed-client\",\n\"id\": \"a63fe1bd0ba894ae\",\n\"timestamp\": 1524620010002000,\n\"tags\": {\n\"class\": \"ServiceAuthUtil$$EnhancerBySpringCGLIB$$9c94d298\",\n\"lc\": \"scheduled\",\n\"method\": \"refreshAllowedClient\"\n}\n}\n},\n{\n\"_index\": \"zipkin:span-2018-04-25\",\n\"_type\": \"span\",\n\"_id\": \"AWL6eLn2MHyfRRHS9yDT\",\n\"_score\": 1,\n\"_source\": {\n\"traceId\": \"b05c4866005fe237\",\n\"duration\": 2653,\n\"shared\": true,\n\"localEndpoint\": {\n\"serviceName\": \"iot-auth\",\n\"ipv4\": \"192.168.0.44\",\n\"port\": 8766\n},\n\"timestamp_millis\": 1524619590005,\n\"kind\": \"SERVER\",\n\"name\": \"http:/client/myclient.do\",\n\"id\": \"17c7981080e31148\",\n\"parentId\": \"b05c4866005fe237\",\n\"timestamp\": 1524619590005000,\n\"tags\": {\n\"mvc.controller.class\": \"ClientController\",\n\"mvc.controller.method\": \"getAllowedClient\",\n\"spring.instance_id\": \"wangbin-PC:iot-auth:8766\"\n}\n}\n},\n{\n\"_index\": \"zipkin:span-2018-04-25\",\n\"_type\": \"span\",\n\"_id\": \"AWL6eLn4MHyfRRHS9yDX\",\n\"_score\": 1,\n\"_source\": {\n\"traceId\": \"cd33f9058f2f585b\",\n\"duration\": 7000,\n\"localEndpoint\": {\n\"serviceName\": \"iot-iotserver\",\n\"ipv4\": \"192.168.0.44\",\n\"port\": 8767\n},\n\"timestamp_millis\": 1524620250003,\n\"kind\": \"CLIENT\",\n\"name\": \"http:/client/myclient.do\",\n\"id\": \"eff98aaae2fa6280\",\n\"parentId\": \"cd33f9058f2f585b\",\n\"timestamp\": 1524620250003000,\n\"tags\": {\n\"http.host\": \"wangbin-PC\",\n\"http.method\": \"GET\",\n\"http.path\": \"/client/myClient.do\",\n\"http.url\": \"\nhttp://wangbin-PC:8766/client/myClient.do?serviceId=iot-iotserver&secret=123456\n\",\n\"spring.instance_id\": \"wangbin-PC:iot-iotserver:8767\"\n}\n}\n},\n{\n\"_index\": \"zipkin:span-2018-04-25\",\n\"_type\": \"span\",\n\"_id\": \"AWL6eLn4MHyfRRHS9yDY\",\n\"_score\": 1,\n\"_source\": {\n\"traceId\": \"cd33f9058f2f585b\",\n\"duration\": 10477,\n\"localEndpoint\": {\n\"serviceName\": \"iot-iotserver\",\n\"ipv4\": \"192.168.0.44\",\n\"port\": 8767\n},\n\"timestamp_millis\": 1524620250002,\n\"name\": \"refresh-allowed-client\",\n\"id\": \"cd33f9058f2f585b\",\n\"timestamp\": 1524620250002000,\n\"tags\": {\n\"class\": \"ServiceAuthUtil$$EnhancerBySpringCGLIB$$eb7a1d5f\",\n\"lc\": \"scheduled\",\n\"method\": \"refreshAllowedClient\"\n}\n}\n},\n{\n\"_index\": \"zipkin:span-2018-04-25\",\n\"_type\": \"span\",\n\"_id\": \"AWL6eLn4MHyfRRHS9yDb\",\n\"_score\": 1,\n\"_source\": {\n\"traceId\": \"26b1a45d43533558\",\n\"duration\": 6000,\n\"localEndpoint\": {\n\"serviceName\": \"iot-iotserver\",\n\"ipv4\": \"192.168.0.44\",\n\"port\": 8767\n},\n\"timestamp_millis\": 1524619830002,\n\"kind\": \"CLIENT\",\n\"name\": \"http:/client/myclient.do\",\n\"id\": \"7ecb19f7a9752bba\",\n\"parentId\": \"26b1a45d43533558\",\n\"timestamp\": 1524619830002000,\n\"tags\": {\n\"http.host\": \"wangbin-PC\",\n\"http.method\": \"GET\",\n\"http.path\": \"/client/myClient.do\",\n\"http.url\": \"\nhttp://wangbin-PC:8766/client/myClient.do?serviceId=iot-iotserver&secret=123456\n\",\n\"spring.instance_id\": \"wangbin-PC:iot-iotserver:8767\"\n}\n}\n},\n{\n\"_index\": \"zipkin:span-2018-04-25\",\n\"_type\": \"span\",\n\"_id\": \"AWL6eLn5MHyfRRHS9yDe\",\n\"_score\": 1,\n\"_source\": {\n\"traceId\": \"88999e1c5c53860d\",\n\"duration\": 9391,\n\"localEndpoint\": {\n\"serviceName\": \"iot-gate\",\n\"ipv4\": \"192.168.0.44\",\n\"port\": 8765\n},\n\"timestamp_millis\": 1524620040001,\n\"name\": \"refresh-allowed-client\",\n\"id\": \"88999e1c5c53860d\",\n\"timestamp\": 1524620040001000,\n\"tags\": {\n\"class\": \"ServiceAuthUtil$$EnhancerBySpringCGLIB$$9c94d298\",\n\"lc\": \"scheduled\",\n\"method\": \"refreshAllowedClient\"\n}\n}\n},\n{\n\"_index\": \"zipkin:span-2018-04-25\",\n\"_type\": \"span\",\n\"_id\": \"AWL6eLn5MHyfRRHS9yDf\",\n\"_score\": 1,\n\"_source\": {\n\"traceId\": \"061d522c4e624835\",\n\"duration\": 2725,\n\"shared\": true,\n\"localEndpoint\": {\n\"serviceName\": \"iot-auth\",\n\"ipv4\": \"192.168.0.44\",\n\"port\": 8766\n},\n\"timestamp_millis\": 1524619560004,\n\"kind\": \"SERVER\",\n\"name\": \"http:/client/myclient.do\",\n\"id\": \"e0acb5b243bb3369\",\n\"parentId\": \"061d522c4e624835\",\n\"timestamp\": 1524619560004000,\n\"tags\": {\n\"mvc.controller.class\": \"ClientController\",\n\"mvc.controller.method\": \"getAllowedClient\",\n\"spring.instance_id\": \"wangbin-PC:iot-auth:8766\"\n}\n}\n},\n{\n\"_index\": \"zipkin:span-2018-04-25\",\n\"_type\": \"span\",\n\"_id\": \"AWL6eLn5MHyfRRHS9yDi\",\n\"_score\": 1,\n\"_source\": {\n\"traceId\": \"d19d64e2abfb41de\",\n\"duration\": 10528,\n\"localEndpoint\": {\n\"serviceName\": \"iot-gate\",\n\"ipv4\": \"192.168.0.44\",\n\"port\": 8765\n},\n\"timestamp_millis\": 1524619650001,\n\"name\": \"refresh-allowed-client\",\n\"id\": \"d19d64e2abfb41de\",\n\"timestamp\": 1524619650001000,\n\"tags\": {\n\"class\": \"ServiceAuthUtil$$EnhancerBySpringCGLIB$$9c94d298\",\n\"lc\": \"scheduled\",\n\"method\": \"refreshAllowedClient\"\n}\n}\n}\n]\n}\n}\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2016#issuecomment-384138888,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD617o3zeC9pOiFnK85jMyIrX5YGVP3ks5tr9sUgaJpZM4TirU8\n.\n. please join gitter for questions. issues alert a lot of people and are only\nfor changes to code\n\nhttps://gitter.im/openzipkin/zipkin\n. please join gitter for questions. issues alert a lot of people and are only\nfor changes to code\nhttps://gitter.im/openzipkin/zipkin\n. be patient\n. be patient\n. no.. currently there's no way to do this. the dependency job is offline. We\ncan consider changing this as so many people ask (to an approach like mysql\nwhich does on-demand if the dependencies job never ran)\n. thanks. thanks. that's probably the older text in the project :P\nIt is true though that RPC names are usually span names (ex grpc) and we prefer route names (endpoints recently clarified using the http.route) tag. last place is \"get\" \"post\" etc.. that's probably the older text in the project :P\nIt is true though that RPC names are usually span names (ex grpc) and we prefer route names (endpoints recently clarified using the http.route) tag. last place is \"get\" \"post\" etc.. I am working through changes for the collector now, to detect proto3 so things like kafka etc will work. I am working through changes for the collector now, to detect proto3 so things like kafka etc will work. are you running a custom build?. I agree this is an issue with better knowledge about the collector we could handle differently. For example, we can do blocking writes.. ya this would work on Kafka but not http as in http would block application\nreporting thread. the trick will be to have some config so that the storage\ncan know the collector is pull based. in worst case this is a property.\nps I am out for a couple weeks so that's why I won't be personally\nresponding until then. take care\n. PS I think easiest way is to add CollectorComponent.blockOnStorage option. This would allow us to wait until there's another request available, and suitable for most storage implementations. Even if elasticsearch returns unavailable, it will at least wait for that to occur.. fyi pull requests are indeed welcome on this.\nessentially this is about pull-model, and in pull model you should be\nable to block the thread with zero impact. Blocking the thread is\ndefinitely not ok when http listener as it can cause applications to\nuse more resources.. this is only appropriate for pulling from kafka\nor rabbitmq (or another buffer)\nYou just need to somehow inventory what is actually wrong, so you know\nwhen to block or reduce rate. Or maybe use a tool like this that is\nsomehow trying to solve arbitrarily\nhttps://github.com/Netflix/concurrency-limits\n. make a fork of zipkin and raise a pull request on a branch from your fork.\nthanks!\n. reverted the naming convention as too many other things broke. going with the document approach:\nFor your prometheus, moving forward make sure your config has a _total on the end for the counters.\ndiff\n       # Received message count\n       - source_labels: [__name__]\n-        regex: '(?:gauge|counter)_zipkin_collector_(.*)_([^_]*)'\n+        regex: '(?:gauge|counter)_zipkin_collector_(.*)_([^_]*)_total'\n         replacement: '${2}'\n         target_label: transport\n       - source_labels: [__name__]\n-        regex: '(?:gauge|counter)_zipkin_collector_(.*)_([^_]*)'\n+        regex: '(?:gauge|counter)_zipkin_collector_(.*)_([^_]*)_total'\n         replacement: 'zipkin_collector_${1}'\n         target_label: __name__\nAlso, replace http_requests_total with http_request_duration_seconds_count. reverted the naming convention as too many other things broke. going with the document approach:\nFor your prometheus, moving forward make sure your config has a _total on the end for the counters.\ndiff\n       # Received message count\n       - source_labels: [__name__]\n-        regex: '(?:gauge|counter)_zipkin_collector_(.*)_([^_]*)'\n+        regex: '(?:gauge|counter)_zipkin_collector_(.*)_([^_]*)_total'\n         replacement: '${2}'\n         target_label: transport\n       - source_labels: [__name__]\n-        regex: '(?:gauge|counter)_zipkin_collector_(.*)_([^_]*)'\n+        regex: '(?:gauge|counter)_zipkin_collector_(.*)_([^_]*)_total'\n         replacement: 'zipkin_collector_${1}'\n         target_label: __name__\nAlso, replace http_requests_total with http_request_duration_seconds_count. this is the ultimate fix https://github.com/openzipkin/zipkin/pull/2026. this is the ultimate fix https://github.com/openzipkin/zipkin/pull/2026. please ask questions on gitter https://gitter.im/openzipkin/zipkin. please ask questions on gitter https://gitter.im/openzipkin/zipkin. Note: we just upgraded to Spring Boot 2.0.1 I've verified that kafka\nstill works using our docker setup with version 2.8.1\nYou can look at how it works just in case:\nhttps://github.com/openzipkin/docker-zipkin/blob/master/zipkin/Dockerfile#L35\n. Note: we just upgraded to Spring Boot 2.0.1 I've verified that kafka\nstill works using our docker setup with version 2.8.1\nYou can look at how it works just in case:\nhttps://github.com/openzipkin/docker-zipkin/blob/master/zipkin/Dockerfile#L35\n. I just manually tested too https://github.com/openzipkin/zipkin/tree/master/zipkin-autoconfigure/collector-kafka10#quick-start\nfeel free to get support here if you need it https://gitter.im/openzipkin/zipkin. I just manually tested too https://github.com/openzipkin/zipkin/tree/master/zipkin-autoconfigure/collector-kafka10#quick-start\nfeel free to get support here if you need it https://gitter.im/openzipkin/zipkin. hmm this would affect things besides Kafka, right? or is Kafka the only thing filebeat uses?\nCan you post the code to your filebeat based zipkin reporter? Generally speaking we only accept our format..  I wonder if there's a way to have filebeat not envelope message data.\ncc @openzipkin/elasticsearch \nfeel free to chat here https://gitter.im/openzipkin/zipkin. hmm this would affect things besides Kafka, right? or is Kafka the only thing filebeat uses?\nCan you post the code to your filebeat based zipkin reporter? Generally speaking we only accept our format..  I wonder if there's a way to have filebeat not envelope message data.\ncc @openzipkin/elasticsearch \nfeel free to chat here https://gitter.im/openzipkin/zipkin. have you asked someone at elasticseatch to make it possible to send raw\ndata? or otherwise unwrap via config?\nfinally there is no zipkin filebeat setup I am aware of. can you please\ndescribe how this is ending up in filebeats?\n. have you asked someone at elasticseatch to make it possible to send raw\ndata? or otherwise unwrap via config?\nfinally there is no zipkin filebeat setup I am aware of. can you please\ndescribe how this is ending up in filebeats?\n. what I mean to ask is how filebeats is used anyway. for example no library\nI know of uses filebeats.\nso I am asking how filebeats is involved in zipkin data.\n. what I mean to ask is how filebeats is used anyway. for example no library\nI know of uses filebeats.\nso I am asking how filebeats is involved in zipkin data.\n. a change to our main server should be due to a supported transport.\nas far as I can guess, you have a custom zipkin sender or maybe even custom\ntracer which puts json into a log.\ninstead of having us guess can you mention exactly what is generating this\ntrace data and why filebeats is your only choice?\nwe have other types of issues and custom setups especially unknown ones\nshould not affect our build. we have a \"rule of 3\" policy which means at\nleast a few different sites need a common concern before we add a feature\nto the server everyone uses.\nfirst step is showing on github exactly what you are doing.\n. a change to our main server should be due to a supported transport.\nas far as I can guess, you have a custom zipkin sender or maybe even custom\ntracer which puts json into a log.\ninstead of having us guess can you mention exactly what is generating this\ntrace data and why filebeats is your only choice?\nwe have other types of issues and custom setups especially unknown ones\nshould not affect our build. we have a \"rule of 3\" policy which means at\nleast a few different sites need a common concern before we add a feature\nto the server everyone uses.\nfirst step is showing on github exactly what you are doing.\n. had a shower moment..\nmaybe if you implement unwrapping as a Kafka bytes deserializer.. just\nunwrapping part.. we could make this work with no changes except putting\nyour type in the classpath and setting a property.\nKafka deserializer is a property iirc and so this could allow you to use\nstandard server.\n. It isn't obvious to me why we would intentionally not track UI latency... Mind elaborating a bit more? I might just not understand.. It isn't obvious to me why we would intentionally not track UI latency... Mind elaborating a bit more? I might just not understand.. thanks.. I'll pull locally also split the logging thing from the other thing\n. thanks.. I'll pull locally also split the logging thing from the other thing\n. we can't remove the log4j thing to zipkin-autoconfigure as it breaks kafka when run as an exec jar..\n[INFO] 2018-04-29 10:43:46.809  WARN [/] 50622 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'kafka' defined in class path resource [zipkin/autoconfigure/collector/kafka/ZipkinKafkaCollectorAutoConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [zipkin.collector.kafka.KafkaCollector]: Factory method 'kafka' threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/log4j/Logger. we can't remove the log4j thing to zipkin-autoconfigure as it breaks kafka when run as an exec jar..\n[INFO] 2018-04-29 10:43:46.809  WARN [/] 50622 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'kafka' defined in class path resource [zipkin/autoconfigure/collector/kafka/ZipkinKafkaCollectorAutoConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [zipkin.collector.kafka.KafkaCollector]: Factory method 'kafka' threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/log4j/Logger. moved log4j stuff to a separate PR https://github.com/openzipkin/zipkin/pull/2032. reviews before during and after the fact greatly appreciated, especially by\nusers like yourself!\n. ps tried with java 1.8 same problem. ok so we need a solution that doesn't break Kafka. right now the exec jar\ntests (maven invoker) fail if we exclude the log4j jar. the point of the\ntests are to prove things work. so either our tests are wrong (false\nnegative) or things are really broke if we do this and so need a different\nsolution. note the Kafka version that breaks is 0.8 so KAFKA_ZOOKEEPER env\nis needed to manually test this.\n. just pushed a deflake commit and picked your commit and pused as https://travis-ci.org/openzipkin/zipkin/builds/373098445 if passes will rebase and merge yours in. closed via 452692479b8424b01f69cfe5b77704612475bfeb there was a slight test glitch. thanks for helping tommy!. agreed should sort it. we should make the existing properties work like\nthey did before as they are added for things like bastion hosts. in other\nwords if feasible change via code not property value\n. verified manually. when #2040 is in, this would break the build\n```bash\ncurl -s localhost:9411/health -v\n   Trying ::1...\n TCP_NODELAY set\n* Connected to localhost (::1) port 9411 (#0)\n\nGET /health HTTP/1.1\nHost: localhost:9411\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 503 Service Unavailable\n< Connection: keep-alive\n< vary: origin\n< Transfer-Encoding: chunked\n< Content-Type: application/json;charset=UTF-8\n< Date: Tue, 01 May 2018 06:46:54 GMT\n< \n* Connection #0 to host localhost left intact\n{\"zipkin\":{\"status\":\"DOWN\",\"details\":{\"InMemoryStorage\":{\"status\":\"UP\"},\"KafkaCollector\":{\"status\":\"DOWN\",\"details\":{\"error\":\"org.I0Itec.zkclient.exception.ZkException: Unable to connect to foo\"}}}},\"status\":\"DOWN\"}\n```. we actually already have a test for it :) in the integration tests we make\nsure health is down when kafka is.\n\nproblem was that kafka was always down!\n. I looked at the code.. the actual code is fine, it believes what's here is actually the default!. This issue was moved to openzipkin/zipkin-reporter-java#109. why are you importing both artifacts?. why are you importing both artifacts?. see also #2045 #2046 as there are elaborate explanations. (interesting pile-on of issues in such a short time!)\nWhile understood, we won't be changing the group of a deprecated artifact. Rather, we will make it unnecessary. It  already is unnecessary for current versions of Brave, for example.\nFeel free to comment here why you are importing both, or let us know on https://gitter.im/openzipkin/zipkin Perhaps there's a way to make this issue unnecessary for you.\n. see also #2045 #2046 as there are elaborate explanations. (interesting pile-on of issues in such a short time!)\nWhile understood, we won't be changing the group of a deprecated artifact. Rather, we will make it unnecessary. It  already is unnecessary for current versions of Brave, for example.\nFeel free to comment here why you are importing both, or let us know on https://gitter.im/openzipkin/zipkin Perhaps there's a way to make this issue unnecessary for you.\n. see https://github.com/openzipkin/zipkin/issues/2047. see https://github.com/openzipkin/zipkin/issues/2047. \nAs mentioned on #2046, the only reason you are experiencing a glitch is likely eclipse doesn't know what to name this on override. For example, Intellij names it \"zipkin (2)\" or something. There's likely a way to do this neater in eclipse, but also mentioned in that issue that this concern will go away when the old library (io.zipkin.java) is removed.\nIn the future, I wouldn't recommend us dual-publishing libraries for more than a release cycle. Rather, we'd keep a long running io.zipkin.zipkin3 branch until there is no dependencies on io.zipkin.zipkin2. However, we are were we are: there's work to do namely around the collector, UI and also mysql which implies we have at least a person-month of effort to get rid of dependencies on the v1 code.. maybe more than that.\nPersonally, I will be working on this effort when I return.. \nAs mentioned on #2046, the only reason you are experiencing a glitch is likely eclipse doesn't know what to name this on override. For example, Intellij names it \"zipkin (2)\" or something. There's likely a way to do this neater in eclipse, but also mentioned in that issue that this concern will go away when the old library (io.zipkin.java) is removed.\nIn the future, I wouldn't recommend us dual-publishing libraries for more than a release cycle. Rather, we'd keep a long running io.zipkin.zipkin3 branch until there is no dependencies on io.zipkin.zipkin2. However, we are were we are: there's work to do namely around the collector, UI and also mysql which implies we have at least a person-month of effort to get rid of dependencies on the v1 code.. maybe more than that.\nPersonally, I will be working on this effort when I return.. see https://github.com/openzipkin/zipkin/issues/2047. see https://github.com/openzipkin/zipkin/issues/2047. You are the only one to report this eclipse problem, even if we did change something, we'd need verification as I'm very surprised eclipse doesn't work. We have core committers who use eclipse.\nWRT workspace, we will continue to have both versions released until such time as we don't require both. Our collector code currently requires both libraries for interop. When I get back from holiday (21 May), I will start working on removing the last dependencies on v1.\nI'll mention point-by-point why there will be no action apart from removing the v1 code as soon as we can (which is at any rate a goal).\nokhttp is an example of another library which only differs on group ID and package name. It is one of the most popular libraries out there. From a packaging POV we are no different (ex okhttp.jar is for all versions). What's different between us and them is that we still release 2 packages (because we must! not because we are intentionally being annoying)\nMost users never have to import our source in the first place. Often times people are importing code, trying to make custom servers (which is unsupported). The larger audience of Zipkin are not affected by this in any way, as there is no such practice in wider repositories such as brave or zipkin-reporter.\nTL;DR: I'm closing this out as it will be addressed by removing the v1 library... to conserve hands for larger benefit and cause least disruption while accomplishing the same goal.\nMeanwhile, I'd recommend joining gitter to chat with folks about whatever you are importing the code. You might not even need to be doing this.. Not checking out this repo is another way to not be annoyed by it! :D\nhttps://gitter.im/openzipkin/zipkin\nFinally, I understand your frustration and appreciate your telling us about it. Closing this issue doesn't mean we won't address the concern. It is just that we won't do it by reorganizing our group IDs etc. It will be by removing the codebase in question as soon as we can. Ideally, but no promises.. in june.. You are the only one to report this eclipse problem, even if we did change something, we'd need verification as I'm very surprised eclipse doesn't work. We have core committers who use eclipse.\nWRT workspace, we will continue to have both versions released until such time as we don't require both. Our collector code currently requires both libraries for interop. When I get back from holiday (21 May), I will start working on removing the last dependencies on v1.\nI'll mention point-by-point why there will be no action apart from removing the v1 code as soon as we can (which is at any rate a goal).\nokhttp is an example of another library which only differs on group ID and package name. It is one of the most popular libraries out there. From a packaging POV we are no different (ex okhttp.jar is for all versions). What's different between us and them is that we still release 2 packages (because we must! not because we are intentionally being annoying)\nMost users never have to import our source in the first place. Often times people are importing code, trying to make custom servers (which is unsupported). The larger audience of Zipkin are not affected by this in any way, as there is no such practice in wider repositories such as brave or zipkin-reporter.\nTL;DR: I'm closing this out as it will be addressed by removing the v1 library... to conserve hands for larger benefit and cause least disruption while accomplishing the same goal.\nMeanwhile, I'd recommend joining gitter to chat with folks about whatever you are importing the code. You might not even need to be doing this.. Not checking out this repo is another way to not be annoyed by it! :D\nhttps://gitter.im/openzipkin/zipkin\nFinally, I understand your frustration and appreciate your telling us about it. Closing this issue doesn't mean we won't address the concern. It is just that we won't do it by reorganizing our group IDs etc. It will be by removing the codebase in question as soon as we can. Ideally, but no promises.. in june.. see https://github.com/openzipkin/zipkin/issues/2047. see https://github.com/openzipkin/zipkin/issues/2047. cc @openzipkin/core @shakuzen @marcingrzejszczak @devinsba I will start on this when I return from holiday 21 May. cc @openzipkin/core @shakuzen @marcingrzejszczak @devinsba I will start on this when I return from holiday 21 May. here's a corresponding issue in brave https://github.com/openzipkin/brave/issues/699. here's a corresponding issue in brave https://github.com/openzipkin/brave/issues/699. @tbwork thanks tons! enjoying the last of it now :)\nPS thought while away:\nio.zipkin.zipkin2:zipkin-collector could be a new java 8+ only jar. The only consumer of this code is the server, which is java 8 anyway. This will make things easier as formerly there was tension about the codebase which is only used by the server/collector.\nNote: A future version of this collector could do reactive streams (1.0 or via java 9) or other fanciness, but I don't think we should do that now. There is so much work just to migrate off the v1 code we should make sure this completes before working on candy.. @tbwork thanks tons! enjoying the last of it now :)\nPS thought while away:\nio.zipkin.zipkin2:zipkin-collector could be a new java 8+ only jar. The only consumer of this code is the server, which is java 8 anyway. This will make things easier as formerly there was tension about the codebase which is only used by the server/collector.\nNote: A future version of this collector could do reactive streams (1.0 or via java 9) or other fanciness, but I don't think we should do that now. There is so much work just to migrate off the v1 code we should make sure this completes before working on candy.. https://github.com/openzipkin/zipkin/pull/2066 is the conversion library.. need to vet this through storage and the dependency linker code. storage are converted. next step is to make the server not require zipkin v1 storage component, which simplifies autoconfig. This also implies redoing autoconfig to not export it either.. storage are converted. next step is to make the server not require zipkin v1 storage component, which simplifies autoconfig. This also implies redoing autoconfig to not export it either.. managed a huge chunk while on a flight. this refactors the server to only use v2 storage beans (note that v1 bridging is still here until collector v2 is done) https://github.com/openzipkin/zipkin/pull/2077. managed a huge chunk while on a flight. this refactors the server to only use v2 storage beans (note that v1 bridging is still here until collector v2 is done) https://github.com/openzipkin/zipkin/pull/2077. Next big chunk is a migration of the colletor code #2078 once this is merged, no remote repositories need to use io.zipkin.java dependency indirectly or directly. Before release, I'll sweep through the server and remove the last things that use old code. Will probably do one last release with io.zipkin.java:zipkin and after that stop publishing it.. Next big chunk is a migration of the colletor code #2078 once this is merged, no remote repositories need to use io.zipkin.java dependency indirectly or directly. Before release, I'll sweep through the server and remove the last things that use old code. Will probably do one last release with io.zipkin.java:zipkin and after that stop publishing it.. For those following along, the zipkin v1 library is now only a test dependency, except in the server, as of v2.9 The server still uses it to present v1 read endpoints. As these are only used by the UI, the path of least resistance is to move the clock skew etc code into the UI, so that it can start reading the v2 endpoints only. This would allow us to decommission the read endpoint of the v1 api, even if we continue to support the v1 POST endpoint on the server.\nSupporting the v1 read endpoint on the server is maintenance better spent elsewhere. Once we've migrated the UI, the only consumer of it would be alternative UIs, which themselves could copy/paste or otherwise the conversion code. This is a lot better than supporting 2 read endpoints in Java imho.. camel no longer uses v1 libraries: https://github.com/apache/camel/pull/2383. try version 2.9.4 with default (java -jar) as kafka 0.10+ is now default.\nex. KAFKA_BOOTSTRAP_SERVERS=10.88.0.81:9093,10.88.0.81:9092,10.88.0.81:9094 java -jar zipkin.jar\nIf the problem is the same, please mention so!. yep. I think this should be elaborated in istio as a part of sampling policy. for example, most tools that originate traces have a means to describe what paths should be traced or not.. Thanks for the help.\nThis looks like a straight port of the mysql one, right?\nWe do have demand, but if we added it, we should use the v2 codebase (ex\nyou can look at the cassandra or elasticsearch ones for an example). Also\nwe'd want to consider the schema, if it is best or not.\n. if doing postgresql, let's make the schema a bit quicker. Everything is v2 type now, so it should be possible!\n #1232. if doing postgresql, let's make the schema a bit quicker. Everything is v2 type now, so it should be possible!\n #1232. thanks, will double-check this tomorrow. Here's the closest hit on the\nissue I can find\nhttps://stackoverflow.com/questions/47947293/maven-using-wrong-version-of-javax-validation\n. thanks, will double-check this tomorrow. Here's the closest hit on the\nissue I can find\nhttps://stackoverflow.com/questions/47947293/maven-using-wrong-version-of-javax-validation\n. please ask questions and make comments with gitter as mentioned in the\nissue template. https://gitter.im/openzipkin/zipkin\n. please ask questions and make comments with gitter as mentioned in the\nissue template. https://gitter.im/openzipkin/zipkin\n. :) in general please don't ask questions directly to a person even if that person will answer you, they might go on holiday etc.\nYes, you can disable the SASI indexing (via SEARCH_ENABLED=false). However I can't say if it works in DSE or not. We don't test with DSE cc @openzipkin/cassandra \nfeel free to ask more on gitter https://gitter.im/openzipkin/zipkin. nice start and thanks for looking at this. cc @ImFlog @bplotnick @drolando\nOne thing I'm wondering is if we can somehow reuse the code from the async reporter which can bundle messages at an appropriate size for the topic. It looked like your design doc abandoned that idea... maybe because it was awkward..  At any rate it would be nice to find a way to get the sizing code into the right place. Maybe we can address this concern in a replacement collector module perhaps.. https://github.com/openzipkin/zipkin/issues/2047#issuecomment-390358196. Thanks for the notes, Andrey. One little thing is that we don't have\nbundling at the storage side at the moment... what this means is the other\nside would make a new storage request for each span. Maybe in some\nenvironments that's ok, notably cassandra would be OK. However, bundling\ncould help those who can take advantage of batching commands.. What are you\nusing for ultimate storage?\n. @jeqo I think we can close this in favor of your experimental repo.. is that the case?. @rmichela proceed with parenting :P\nI'm going to close this as much has changed since.\n For those looking for an http/kafka forwarder, there's https://github.com/HotelsDotCom/pitchfork by @worldtiki and folks\n For those looking for near-realtime processing of spans (ex dependency graph linking) there's  https://github.com/jeqo/zipkin-storage-kafka\nI'm glad we had this to start the fire, but I don't think we need to re-own this PR. Thanks @afalko for being the early bird!. please join gitter for help\nhttps://gitter.im/openzipkin/zipkin\n. ok this can be redone over the new zipkin-collector library, which is an abstract class so no api break :P\nI'd recommend just using the Adjuster type as we did some work on this for the sparkstreaming api.\nhttps://github.com/openzipkin-attic/zipkin-sparkstreaming/blob/master/sparkstreaming/src/main/java/zipkin/sparkstreaming/Adjuster.java\nThe only difference is that it is unlikely the spans will be in the same trace..\nAdjustment should be a pure function, meaning it shouldn't throw exceptions. I think we should start with this assumption and move forward from there.. basically in hindsight I think we can get most mileage by taking some of the work @naoman helped with and make it into the core collector library along with your input too.. two for one that way :P. Note: this requires a little more work, notably to use the provided executorService. https://github.com/openzipkin/zipkin/pull/2085. sounds nice but sorry what is the difference? is there a link or something\nto describe?\nOn Thu, 7 Jun 2018, 01:35 Jos\u00e9 Carlos Ch\u00e1vez, notifications@github.com\nwrote:\n\nNew issue functionality from github allows us to have better templates\nbased on every issue type.\nI propose that we use:\nQuestion\nThis issue tracker is not the place for questions. If you want to ask how\nto do something, or to understand why something isn't working the way you\nexpect it to, use Gitter https://gitter.im/openzipkin/zipkin or Stack\nOverflow, there are lots of\nkind and helpful community members that will help you.\nBug report Describe the bug\nA clear and concise description of what the bug is. If you have a solution\nin mind, skip raising an issue and open a pull request instead.\nSteps to Reproduce\nSteps to reproduce the behavior:\nExpected behaviour\nThe best is to spend some time to write a failing test. Bugs with tests\nget fixed and stay fixed.\nFeature Request\nPlease first, look at existing issues\nhttps://github.com/openzipkin/zipkin/issues to see if the feature has\nbeen requested before. If you don't find anything, tell us what problem\nyou\u2019re trying to solve. Often a solution already exists! Don\u2019t send pull\nrequests to implement new features without first getting our support.\nSometimes we leave features out on purpose to keep the project small.\nPing @openzipkin/core https://github.com/orgs/openzipkin/teams/core\n\u2014\nYou are receiving this because you are on a team that was mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2081, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6196okOfOOXevH4ej1ujrHQ9n7dLbks5t6OW1gaJpZM4Ud99q\n.\n. oh this is cool\n. mysql\n\n\n. cassandra (v1)\n\n. cassandra3\n\n. elasticsearch\n\n. There's more work to complete this as mockito isn't happy\n```\norg.mockito.exceptions.base.MockitoException: \nMockito cannot mock this class: interface zipkin.storage.StorageAdapters$SpanConsumer.\nMockito can only mock non-private & non-final classes.\nIf you're not sure why you're getting this error, please report to the mailing list.\nJava               : 10\nJVM vendor name    : Azul Systems, Inc.\nJVM vendor version : 10.0.1+9\nJVM name           : OpenJDK 64-Bit Server VM\nJVM version        : 10.0.1+9\nJVM info           : mixed mode\nOS name            : Linux\nOS version         : 3.13.0-149-generic\nUnderlying exception : java.lang.UnsupportedOperationException: Cannot define class using reflection\nCaused by: java.lang.UnsupportedOperationException: Cannot define class using reflection\nCaused by: java.lang.IllegalArgumentException: Unknown Java version: 10\n```. internal classes aren't to be used directly.\nIf you have a classpath problem with an internal class, probably there's a\nversion mismatch somewhere. the artifact that would have had that class\nwould be io.zipkin.java:zipkin\nMeanwhile, custom builds aren't supported. However if you are curious\ntroubleshooting questions should be here not on github issues\nhttps://gitter.im/openzipkin/zipkin\n. was there any follow-up on this? we can re-open if so. let us know on gitter https://gitter.im/openzipkin/zipkin. i am not sure what you are eluding to, but I will try to answer.\ncustom servers are unsupported. if you are trying to do a custom zipkin server, I would expect some maintenance moving to 2.0\nif you are talking about applications, you might have the wrong support channel as code that instruments spring boot is not here rather in the spring-cloud-sleuth project. before a traceid of \"\" would end up coercing to a string of zeros, which is invalid. thanks for opening this!\n. sorry but what you are trying to do is unsupported.\n. think of this like mysql.. do you expect mysql to run in a j2ee container?\nHow about cassandra or kafka? Why must zipkin run in a j2ee container?\n. anyway technically there are a few issues with J2EE. the collectors often\nspawn threads which wouldnt be managed. the http POST endpoint is\nimplemented directly with undertow so it doesnt use servlet. at least these\nthings are technical hurdles if you want to create and support a J2EE\nvariant.\nMeanwhile be careful as MySQL is also not a target production environment.\nThe schema is not designed for larger scale data so your configuration may\nhit another problem later. Cassandra and Elasticsearch are the more capable\nones.\nFinally sometimes people mistakenly think they need or should cohost zipkin\nserver with their application. If that is why you are looking for J2EE\nmight be worth looking at our brave examples which show that trace data\nrecording is decoupled from the zipkin server.\nhttps://github.com/openzipkin/brave-webmvc-example\n. I've actually ported all the code, but there are a bunch of tests I need to port, yet. done. PS we need to vet carefully make sure we don't rebreak things in the process. i.e. verify that if a tag \"error\" is sent, followed by \"error.code\" it doesn't crash like it did before.\nElse, we get straight back into playing remapping/escaping things which hurts our ability to see what exactly was sent, resulting in both the problems we had before and also the problem of supporting multiple indexing types!. definitely there's value exploring what seems to be the more ES way which\nis not exactly storing raw data (as dots need escaping etc), but still\nmaintenance causing.\nif we do decide to do the \"dedotting\" approach, basically we'd need to have\nour query side compensate/detect the data written to see if it needs to be\nunescaped or not. There's code in that, not terrible stuff, but not lovely.\nWe also have a slightly different code path on write based on the version\nwhich yeah.. is more code. More code is fine if worth it or with hands.. we\njust need to figure out if that's the case.\nTL;DR; I'd like to see a show of hands of who wants this enough to help :D\nno pressure.. just a question as we have to be careful entering areas of\nnew \"if statements\". Would anyone be able to participate in code, advice,\ntesting, documentation or morale support? :D\n. with this change, the javascript doesn't crash anymore.. even if the actual data from #1744 is quite wrong due to 3 spans having constant ids\n\n. what you are asking for is unsupported, which is why we don't document it. If you look at the javadoc on the deprecated @EnableZipkinServer annotation, it mentions custom servers are not supported.\nWhat is supported is documented here https://github.com/openzipkin/zipkin#quick-start. developing a custom server is not supported by the community. you can at\nyour own risk but please don't raise issues if you try against our advice.\nHere's the reference I mentioned to you which mentions this:\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/java/zipkin/server/EnableZipkinServer.java#L25\n. no prob. If there are features you'd like to see.. please raise issues or\nadd to existing ones about them.\n. i have this nearly complete but wont push the missing tests today. this will complete somewhere around 9am bangkok time tomorrow (monday). done. >\n\nThank you @shakuzen https://github.com/shakuzen My data model is\nmissing []\nis there a problem where we aren't leniently reading something? Or are you\nok now?\n. you can try using our normal kafka reporter, or putting your code in our test to make the effects easier to see \n\nhttps://github.com/openzipkin/zipkin-reporter-java/blob/master/kafka11/src/test/java/zipkin2/reporter/kafka11/KafkaSenderTest.java#L61\nMeanwhile this issues list is for changes to the server, not troubleshooting. Please use gitter for more help https://gitter.im/openzipkin/zipkin. probably not worth doing twice (we have an apache group ID change looming). In docker, best time on normal startup before this change:\nStarted ZipkinServer in 5.258 seconds (JVM running for 6.475)\nIn docker, best time on normal startup after this change:\nStarted ZipkinServer in 4.995 seconds (JVM running for 6.16). I think this is unrelated. This repository is about zipkin server. The\nstacktrace you paste is not from Zipkin server as zipkin doesn't use sleuth\ninternally.\n\"java.io.IOException: Broken pipe\" can be found on trouble shooting sites,\nbut yes such an exception should end up in zipkin data. First verify your\ntracing setup is working normally. After that, join a troubleshooting\nchannel such as https://gitter.im/spring-cloud/spring-cloud-sleuth for more\nI will close this issue as it won't be solved in this repository\n. duplicate of #2136. red is when there is a tag named error. yellow is when there is an\nannotation containing word error\n. do you have variables in your span names? Span names should be fixed\ncardinality. If you have something like an http route, make sure this is\nused instead of paths with variables in it.\nhttps://github.com/openzipkin/brave/tree/master/instrumentation/http#http-route-cardinality\nhttps://github.com/openzipkin/zipkin-api/blob/master/zipkin.proto#L108\nhttps://github.com/openzipkin/zipkin-api/blob/master/zipkin2-api.yaml#L312\n. kafka 1.0 is in the default jar now. Notes here, but 2.10.2 is the latest\nversion https://github.com/openzipkin/zipkin/releases/tag/2.9.4\n. the document pointed to is not the current version of zipkin. If you are\nhaving trouble connecting to kafka, this might also help:\nhttps://github.com/openzipkin/sleuth-webmvc-example/pull/8\nregardless, this seems a more troubleshooting question. please use gitter\nfor more follow-up https://gitter.im/openzipkin/zipkin\n. no problem!. This needs to be tread very carefully, as it doesn't just impact us but\nalso anything downstream or anything aggregation in nature that works off\nassumptions that span is low or relatively fixed cardinality. Internally,\nthis affects how search works, and relaxing this also limits our ability to\nimprove aggregations.\n. example issue which is knock-on effect: https://github.com/openzipkin/zipkin/issues/2139\nalso the work we are trying to do to relate metrics name to span name would be thwarted as metrics dimensions are similarly fixed cardinality. thanks for opening the issue though. context around this topic changes\nevery so often\n. regardless of whether this is merged, please let us know what instrumentation caused this as annotations are not intended to be used as logs or include newlines. When we add workarounds, it is a reasonable request to ask why, so in case whatever we are working around is fixed, the workaround can be considered for removal. cc @nollbit @drolando @michaelsembwever \nbug could happen to anyone.. hard to look at comparisons like this. Anyway tests backfilled now and 2.11.1 should be cut immediately cc @openzipkin/cassandra . FYI credit for this goes to @drolando who somehow was able to think through the unit tests and notice the problem :P Now, we have unit tests so less thinking needed later. cc @jcchavezs . will look at the failure later. you are fantastic. ps please update this issue while you progress!\n. but actually this issue should be in brave, not zipkin\n. I think most who are in a high load scenario dont try to run only one\ninstance of zipkin. if you split your collector deployment from your\nui/read, the problem becomes simpler doesn't it?\n. The thing I'm trying to focus here on is solving at the right layer of abstraction. Usually folks never touch this code and don't want it to become complex.  I do understand there is some finesses here.\nwhy isolate read traffic? it makes concurrency easier and without multiple settings\nSo, my question about the query vs collector was this comment:\nhttps://github.com/openzipkin/zipkin/pull/2166#discussion_r209559144\n\nOur problem now is that the HttpCall is shared for the mixed use of query api operations and collector api insertions. There is no chance for reservation of the same amount of HttpCalls as Kafka worker threads: KAFKA_STREAMS parameter.\n\nSo, what I was getting at was that if there was a semaphore at the collector level, or some bounding by stream count, or a combination.. if only collector traffic went to that instance (via QUERY_ENABLED=false), wouldn't we be able to simplify because we don't need to concern with read traffic to the HttpCall api?\nShould we continue this PR?\nYou asked about continuing this PR. As mentioned many times, I will not merge tight coupling between http call and the collector tier. Please stop proposing this. If that's your goal, maybe you should put this change down and wait for someone else to pick it up as reviewing is taking longer than actually doing the change. There are plenty of options and tight coupling between storage impl and collector impl is the least favorable for maintenance.\nLet's review prior art: SQS\nFor example, in SQS exceptions except IllegalArgumentException are implicitly retried (due to lack of a follow-up delete message) https://github.com/openzipkin/zipkin-aws/blob/master/collector-sqs/src/main/java/zipkin2/collector/sqs/SQSSpanProcessor.java#L130 They also aren't affecting storage layer, rather assuming IllegalArgumentException is not retryable. Of course, this retry policy might not be awesome, but it is easier to reason about and a function to select exceptions would be straight forward to discuss (especially if separate). SQS also has a backoff mechanism which is simple in nature. What I like very much is that SQS did not ask to change the storage layer or borrow a storage specific semaphore to do its backoff etc. That made the SQS code less scary to me: that the retry semantics were isolated.\nLet's review options!\nThis like concurrency gates have options, and zipkin-server is made with spring. You can use AOP and all sorts of things to weave in advanced things as a means to experiment. Let's not forget this as it is better than committing experiments or code that only one site needs. For example, I don't think that most would find it optimal to spend a lot of time on a site who wants to handle a lot of load with only one collector node and also limited storage capacity. Focusing on how that site can use other tools can also be possible.. ex answering the spring question of how to weave in other libraries like https://github.com/Netflix/concurrency-limits\nYou can also make a custom collector! For example, if you are convinced of one approach, you can package your own collector the same way that kafka08 does (module). It is actually a good idea to try and test things, and if you need back/forth style advice on this, gitter is best to do that: https://gitter.im/openzipkin/zipkin\nOK back to work.. what are we trying to do?\nThe most discussed part of this change is that a polling collector is in a loop, and there's no value to pulling off spans you can't store! block on storage will definitely slow the amount of requests going to the backend with minimal impact when polling.  For example, the polling loop will then pause until it knows there is an error or not. \nOur goal was to make some cheap change, like cheap to make cheap to review, isolated.\nHere are some thoughts, but overall use tests to drive your ideas. without tests it is really hard to reason with the full set of options especially when one of the reviewers is asleep!\noption 1:  polling collector ~= limiting the concurrent with synchronous api\nFor example, in elasticsearch this isn't waiting until indexing is done iirc, this is only waiting to know if it was accepted. IOTW, we are blocking until we know there is an error then proceeding. Blocking on storage can be implemented with semaphores and things (possible change 2), or it can be simply using the synchronous api. IMHO, the latter is easier to reason with especially in a loop. @llinder @devinsba or others have feedback on this.\noption 2: polling collector ~= limiting the concurrent requests with semaphores\nif you do block on storage (change 1), yes there is a limit to how many concurrent requests can be in-flight as no more than the concurrency limit for queue processors. In Kafka, yes it would be stream count, in Rabbit the channel count. If the limit should be higher than queue count, this means you can't use the synchronous api (execute), rather a collector tier semaphore. If proposed, I don't personally even want to review this without tests.\nseparate concern: on exception re-process spans\nIn Kafka, we could already implement a means to resubmit work similar to SQS. We can do this with or without a collector-tier change. For example, and especially if in a blocking loop, we can hold a reference to the initial spans and retry them on a condition, whether or not that condition is pluggable. If we did this, ideally there's a way to stop trying (avoiding poison messages from stopping everything). This thing should be heavily tested.\nseparate concern: on exception sleep\nCollector or storage tier can sleep as opposed to immediately redoing the same command. SQS has a naive but probably fine approach for at least one site. If there needs to be more finesse on how to decide which way to sleep, that could be done in a relatively decoupled way, as simply as an okhttp elasticsearch interceptor for example! It could also be possible to make a generic exception handler, but I personally think it is a bit early for this. At any rate this is a separate change!\nWrapping up\nPlease choose an option which makes sense to you and decide which you'd like to do or not. Please consider simplest first and do this in separate steps to conserve time for others who also need review or coding help.. Thanks for the loopback.\nPersonally, I link to https://github.com/Netflix/concurrency-limits as\nonce we get into semaphores and such, the code becomes higher\nmaintenance. I'd rather shade a dependency on that vs start building\nthe code (incidentally concurrency-limits core has no dep except\nslf4j)\nOne approach we can take external to this PR which could help in\nguiding this is to use RejectedExecutionException when there's a\nretryable exception. This is the same runtime exception that\nconcurrency-limits uses.\nhttps://github.com/Netflix/concurrency-limits/blob/744f8ae10523b0963b5d9453ce5d0d5c90d32139/concurrency-limits-core/src/main/java/com/netflix/concurrency/limits/executors/BlockingAdaptiveExecutor.java#L16\nWe can add javadoc to the SpanConsumer saying that implementations\nshould callback with RejectedExecutionException when they know an\nexternal limit such as a BusyPoolException has happened. This should\nbreak nothing, for example it would not break the code in SQS.\nHowever, it could prepare us in a way where we can automatically\naddress resource constraints. It is also nicely decoupled IMHO. what\ndo you think?\nIf you like it, maybe go ahead and make elasticsearch and cassandra\nuse RejectedExecutionException (even if by wrapping) as a separate\nchange?\n. looking forward to it!\n. hi JC so this is http specific, so in addition to the endpoint details.\nWhen talking about a tag usually we dont have interdependencies such as\nthis flag means interpret a certain way.\nmy advice would be to stick with http host or authority as they are defined\nin http semantics.. then decide whether or not it should be standard or not\n(ex recorded by default). this would simplify the issue I think.\n. >\n\n@adriancole https://github.com/adriancole I should have been more\nclear. I meant that having a flag in the options of the middleware that\nsays recordHost: true|false so user could decide whether it is meaningful\nor not to record such information\n\n\nah ok. well I see your point now. hmmm I guess if we decided to not make it\nstandard then people will add it anyway like they do today or we can do\nsomething like you mention to make it easier. otoh if we were going the\nroute of not recording by default I might be more inclined to list the\n\"standard tags\" desired vs a flag for each. this could be programmatic or\ndeclarative like http.client.tags=http.method,http.path,http.host I think\nsome people maybe python does this already.\nat any rate suspect we should get a sense of who wants the default to\ninclude host or more importantly who doesn't. I think we can sort a nice\nway our regardless of the decision.\n\n. lgtm!. can you do the same in zipkin-reporter?. yeap on this being where I was hoping it would be (will apply to all collectors, not just http but good start)\n\nonce you have your results in, we can chat further. thanks so far.. two ways.. look at server logs for \"o.a.k.c.c.ConsumerConfig\" which\nprints out the config used\nyou can also look at the config values accepted by Spring (which\nshould influence ^^)\ncurl -s localhost:9411/actuator/configprops|jq\n'.contexts.application.beans.\"zipkin.collector.kafka-zipkin2.autoconfigure.collector.kafka.ZipkinKafkaCollectorProperties\"'\n{\n\"prefix\": \"zipkin.collector.kafka\",\n\"properties\": {\n\"topic\": \"zipkin\",\n\n\"overrides\": {},\n\n\"bootstrapServers\": \"kafka-zookeeper:9092\",\n\n\"groupId\": \"zipkin\",\n\n\"streams\": 1\n\n}\n}\nOn Tue, Aug 21, 2018 at 10:54 PM Daniele notifications@github.com wrote:\n\nI see a lot of these warnings in the logs:\n2018-08-21 02:33:47.357  WARN 15 --- [pool-2-thread-1] o.a.k.c.c.i.ConsumerCoordinator:\n[Consumer clientId=consumer-10, groupId=zipkin_collector] Synchronous auto-commit of\noffsets  {zipkin.traces-1=OffsetAndMetadata{offset=54995069, metadata=''}} failed:\nCommit cannot be completed since the group has already rebalanced and assigned the\npartitions to another member.\nThis means that the time between subsequent calls to poll() was longer than the configured\nmax.poll.interval.ms, which typically implies that the poll loop is spending too much time\nmessage processing. You can address this either by increasing the session timeout or by\nreducing the maximum size of batches returned in poll() with max.poll.records.\nThey seem to trigger a consumer rebalance which slows down everything and make offset distance grow.\nI want to try setting those options to see whether they help, but I can't figure out how I pass them to the kafka consumer zipkin uses. Should I use -Dzipkin.collector.kafka.overrides.auto.max.poll.records=20? I can't tell if it's being applied or not\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. ah right. this is subtle.. if after the jar you can use -- syntax ex\n--zipkin.collector.kafka.overrides which is a subtely of \"system\nproperties\" vs arguments. you can see example of many ways to do\nsomething with the SPRING_APPLICATION_JSON example here\nhttps://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-external-config.html\n\nOn Wed, Sep 5, 2018 at 8:23 PM Daniele notifications@github.com wrote:\n\nThanks, looking at those properties I was able to figure out that I simply needed to specify the -D zipkin.collector.kafka.overrides values before the -jar zipkin-server.jar otherwise they're ignored.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. happened again today:\n\njquery.js:9600 GET http://***:31399/zipkin/trace_zh.properties 404 (Not Found)\nUncaught TypeError: Cannot read property 'length' of undefined jquery.i18n.properties.js:271 \nfrom @GuangmingLuo. https://github.com/jquery-i18n-properties/jquery-i18n-properties/issues/103. @JoveYu is same for you? on 'trace_zh.properties'?. I will try to workaround the problem in the build but making a copy of XXX_zh_CN.properties as XXX_zh.properties. attempted fix waiting on feedback https://github.com/openzipkin/zipkin/pull/2238. #2239 should fix this. can you rephrase? we do have mysql but it is not considered production\nlevel (also indeed it has a v1 style schema)\nwhat database are you looking for?\n. We don't have plans to change mysql at the moment. It is test only\nanyway, so effort isn't important. It is natively implemented with v2\nlibraries, so you should be fine using it with any recent version of\nzipkin and any data format we accept\n. thanks!. Hi, jeff.\nWhich storage backend are you referring to? We've definitely had something\nsimilar asking for a tag whitelist to index for cassandra.\nOn Tue, Aug 28, 2018 at 3:17 AM Jeff Beck notifications@github.com wrote:\n\nFeature:\nAllow adding tags that will not be indexed OR are somehow optional to\nindex.\nRational\nSome tags that are useful in a span for debugging, are high cardinality so\nwe may not want to search by them. Having a way to add unindexed tags\nallows users to not blow out the storage layer for the team operating\nzipkin.\nExample Scenario\nWhile adding tags to spans about our API calls, we wanted to tag oauth\nclient ids, while we also keep things like user uuid but ideally we don't\nneed/want to index all the uuids.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2178, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD618IwgZrmCusM5nID8-64tcAE_dsRks5uVFPOgaJpZM4WObqo\n.\n. Here's the former thing about whitelist: https://github.com/openzipkin/zipkin/issues/1928\n\nI'm really not sure we will be able to control all indexing (ex mysql), so this would be a request to not index vs a requirement to not index.\neither way, there's work for the operator to do, unless you are hinting at a data model change, which would be vastly more work.. if this is a request about a data model change, I don't think realistically we can do it this year. It is several months to a year of effort to move a data model through the ecosystem and most aren't even completely v2 yet. That doesn't mean we shouldn't track it.. sounds good jeff. FWIW amazon has this feature \"metadata\", so there is\nprior art. https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html#api-segmentdocuments-metadata\nLet's leave it open independently, as I personally would also like\nthis as it makes data management in general easier. For example, we\nshould never attempt to index queries!\n. >\n\nMetadata is a way better way to put it mind if I edit the issue to reflect\nthat terminology?\ngo for it!\n. thanks for the checks. see also https://github.com/openzipkin/zipkin-api-example\n. so the use case you mention isn't typical, so it is hard to say how much\nvalue there is in publishing this. For example, we don't create features\nwithout 3 folks asking and creating a new library before that happens would\nbe too early as well, especially under the assumption it would be backed\ninto a sleuth app :P\n\nI would recommend that you try to use something like feign which if you are\nonly getting a trace by ID will be easy to do on your own. You make sure\nthe decoder uses SpanBytesDecoder.decodeList. Can you try this please?\n\n. you can also look at some older code here, as well which while written against the v1 api (io.zipkin.java:zipkin code in package zipkin), can be a hint for v2 api (io.zipkin.zipkin2:zipkin code in package zipkin2)\n\nhttps://github.com/openzipkin/zipkin/commit/8f99f3cf44cbcdc678b9ae845d501ba2fc2422a4#diff-428d6e11ec1d9804af04afc7ecc13c91L33. I wonder if somehow we are overthinking this..\nThere is a library with a decoder, why do you need to do any entity mapping?\nList decoded =\nSpanBytesDecoder.JSON_V2.decodeList(your_byte_from_http_response);\nThere's no concept of Trace in zipkin it is a list of spans.. if only\ngetting by id using the above makes the code far less text than the\nconversation so far.\nOn Thu, Aug 30, 2018 at 2:50 PM Guangming Luo notifications@github.com wrote:\n\n@shakuzen You are right, Feign or directly using RestTemplate are the alternatives. I know how to do it. The problem is which is the best solution or the most effective way. User will still need model Span or Trace as entity in this case.\nMaybe we can wait for few days if there are other users want the same feature before we close it.\nI still believe its would be valuable to have such features. Zipkin GUI is not enough as an output for the whole distributed tracing solution.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. here's a working example, where the only dependencies are io.github.openfeign:feign-core and io.zipkin.zipkin2:zipkin\n\nHopefully this can buy you time as we don't typically enter work like creating a new library etc within days anyway.\n```java\ninterface Zipkin {\n@RequestLine(\"GET /api/v2/trace/{traceId}\")\nList<Span> getTrace(@Param(\"traceId\") String traceId);\n\nstatic Zipkin connect() {\n  Decoder decoder = new Decoder.Default() {\n    @Override\n    public Object decode(Response response, Type type) throws IOException {\n      if (response.request().url().contains(\"/api/v2/trace/\")) {\n        byte[] bytes = (byte[]) super.decode(response, byte[].class);\n        if (bytes == null) return null;\n        return SpanBytesDecoder.JSON_V2.decodeList(bytes);\n      }\n      return super.decode(response, type);\n    }\n  };\n\n  return Feign.builder()\n      .decoder(decoder)\n      .logger(new Logger.ErrorLogger())\n      .logLevel(Logger.Level.BASIC)\n      .target(Zipkin.class, \"http://localhost:9411\");\n}\n\n}\npublic static void main(String... args) {\n    Zipkin zipkin = Zipkin.connect();\nSystem.out.println(zipkin.getTrace(\"96e7c98dd2d01b3c\"));\n\n}\n```. no problem. my goal is to make people not blocked.. we are doing a lot\nof stuff, like JMS so hard to address everything completely as it\narrives. https://github.com/openzipkin/brave/pull/764\nfeel free to join https://gitter.im/openzipkin/zipkin to chat more if you like\n. sorry this type is intentionally immutable. Span.Builder is mutable (you can call span.toBuilder() to get one). https://github.com/openzipkin/brave/blob/master/brave/src/main/java/brave/internal/recorder/MutableSpan.java is an example of a more efficient mutable impl which will be public api soon.. feel free to talk more about this here https://gitter.im/openzipkin/zipkin. no problem!. @klingerf I think this should help with linkerd names\nThanks so much Raja!. heh yeah I hit the npm versions to squash that vulnerability report and it had some nice side effects :P. this storage implementation requires what it mentions. there is no plan on\nreversing that. Is there a reason you can't use 3.11.3?\n. sorry. this project is run by volunteers. we actively decided to drop support for cassandra 3.0 due to the amount of problems. We have no new information which would reverse that decision. You can try to run a separate instance with an updated version. If not, you'll have to try a different storage type.. true though discouraged. it is best for all if we eventually can stop\nmaintaining two separate codebases for Cassandra!\nOn Wed, 12 Sep 2018, 10:13 Daniele, notifications@github.com wrote:\n\nIf you don't need any feature of the new schema, I'm pretty sure you can\nuse the old \"cassandra\" schema on C* 3.0.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2189#issuecomment-420488404,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD619B2UoZv2cdOiJFgZ_jINHXcsyM7ks5uaG3MgaJpZM4WkSmk\n.\n. zipkin-dependencies is a separate job and needed when you run ES or\ncassandra\n\nOn Thu, 13 Sep 2018, 01:09 Anupma97, notifications@github.com wrote:\n\nIf you don't need any feature of the new schema, I'm pretty sure you can\nuse the old \"cassandra\" schema on C* 3.0.\nYes. I tried after setting property STORAGE_TYPE=cassandra.\nTracing data is storing in Cassandra and I am able to get tracing on\nZipkin Ui but there is no dependencies graph coming. Earlier when I was\nusing in memory , I was able to get the dependencies graph.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2189#issuecomment-420725483,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD619DLt3g_4y0DmnOK1fhRDQPqgYEiks5uaT_GgaJpZM4WkSmk\n.\n. please open an issue on zipkin-dependencies repo about this. We must be\nmissing some windows instructions.\n\nOn Fri, Sep 14, 2018 at 12:20 AM Anupma97 notifications@github.com wrote:\n\nzipkin-dependencies is a separate job and needed when you run ES or\ncassandra\n\u2026 <#m_-597314219786891342_>\nOn Thu, 13 Sep 2018, 01:09 Anupma97, @.**> wrote: If you don't\nneed any feature of the new schema, I'm pretty sure you can use the old\n\"cassandra\" schema on C 3.0. Yes. I tried after setting property\nSTORAGE_TYPE=cassandra. Tracing data is storing in Cassandra and I am able\nto get tracing on Zipkin Ui but there is no dependencies graph coming.\nEarlier when I was using in memory , I was able to get the dependencies\ngraph. \u2014 You are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub <#2189 (comment)\nhttps://github.com/openzipkin/zipkin/issues/2189#issuecomment-420725483>,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD619DLt3g_4y0DmnOK1fhRDQPqgYEiks5uaT_GgaJpZM4WkSmk\n.\nI tried running zipkin-dependencies-2.0.1.jar but I am getting below\nerror.\n18/09/13 21:46:02 ERROR Shell: Failed to locate the winutils binary in the\nhadoop binary path\njava.io.IOException: Could not locate executable null\\bin\\winutils.exe in\nthe Hadoop binaries.\nat org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)\nat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)\nat org.apache.hadoop.util.Shell.(Shell.java:386)\nat org.apache.hadoop.util.StringUtils.(StringUtils.java:79)\nat org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)\nat org.apache.hadoop.security.Groups.(Groups.java:93)\nat org.apache.hadoop.security.Groups.(Groups.java:73)\nat\norg.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)\nat\norg.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)\nat\norg.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)\nat\norg.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)\nat\norg.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)\nat\norg.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)\nat\norg.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)\nat\norg.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)\nat scala.Option.getOrElse(Option.scala:121)\nat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2467)\nat org.apache.spark.SparkContext.(SparkContext.scala:292)\nat\nzipkin2.dependencies.cassandra.CassandraDependenciesJob.run(CassandraDependenciesJob.java:163)\nat\nzipkin2.dependencies.ZipkinDependenciesJob.main(ZipkinDependenciesJob.java:49)\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2189#issuecomment-421065805,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD616U7RlojIWbj8oj3pgwNs2DGR3foks5uaoW0gaJpZM4WkSmk\n.\n. bear in mind this configuration is noted as not for production. even if we\nadd an env option for a full url you may still run into performance\nproblems due to the schema design which is intended for learning not prod.\n\nOn Fri, 14 Sep 2018, 03:57 Auke van Leeuwen, notifications@github.com\nwrote:\n\nCurrently the MySQL data storage is configured roughly here:\nhttps://github.com/openzipkin/zipkin/blob/d47e80433cdf97a78e92712f313ae607e6f61dec/zipkin-autoconfigure/storage-mysql/src/main/java/zipkin2/autoconfigure/storage/mysql/ZipkinMySQLStorageProperties.java#L89-L102\nThis doesn't give a whole lot of flexibility in the datasource setup. In\nmy case I have a Galera cluster for which I can specify a JDBC url like\nthis:\njdbc:mariadb:failover://host1:3306,host2:3306,host3:3306/database?prop1=true\nThis allows for failover in case one of the nodes is down.\nEven if you use 'plain' MySQL there is still a host of failover\nconfiguration options:\nhttps://dev.mysql.com/doc/connector-j/8.0/en/connector-j-config-failover.html\n.\nCan we perhaps add an optional property to ZipkinMySQLStorageProperties\nto specify the JDBC url directly and if it doesn't exist use the current\nimplementation as a fallback?\nGiven a property jdbcUrl, kind of like this?\nStringBuilder url = new StringBuilder();\nif (getJdbcUrl() != null) {\n  url.append(getJdbcUrl());\n} else {\n  url.append(\"jdbc:mysql://\");\n  url.append(getHost()).append(\":\").append(getPort());\n  url.append(\"/\").append(getDb());\n  url.append(\"?autoReconnect=true\");\n  url.append(\"&useSSL=\").append(isUseSsl());\n  url.append(\"&useUnicode=yes&characterEncoding=UTF-8\");\n}\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2190, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610YTOTSYRo5rxq8S_czK0L3DU0uVks5uarikgaJpZM4WoGu5\n.\n. I would classify this as use at your own risk as it is double-discouraged,\none for performance and the other for the schema. This means once you are\nin production and you hit load problems you'll have to do a migration off\nthis to elasticsearch or cassandra. If that's ok with you we can look about\nadding the url option.\n\nOn Fri, Sep 14, 2018 at 4:57 PM Auke van Leeuwen notifications@github.com\nwrote:\n\nHmm I didn't really read** it like that. It sounded like if we really put\na lot (but what is a lot ??) of data into it, the querying of certain\ntraces and or dependencies would be slow. That is okay for now I think.\nI would like to know if putting the data in, so storing the spans,\nwould be going slower as well, thus possibly hampering the clients that are\nsending the data? (Brave, Sleuth)\nAny ball park figure on what 'a lot of data' is?\n https://github.com/openzipkin/zipkin/tree/master/zipkin-storage talks\nabout 'discouraged' for newer systems\n https://github.com/openzipkin/zipkin talks about \"The following\ncomponents are no longer encouraged, but exist to help aid transition to\nsupported ones. \"\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2190#issuecomment-421281391,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD613wPGxUcKx1h9nd28UP65oIpngj2ks5ua2-UgaJpZM4WoGu5\n.\n. thanks!. troubleshooting is best done on gitter as it usually requires back and\nforth. if we do it here people get alerted too much\n\ngitter.im/openzipkin/zipkin\nOn Thu, 20 Sep 2018, 04:54 cosmonaut13, notifications@github.com wrote:\n\nI use zipkin to track traces in my application. I have a unit test. Inside\na unit test a client is created from which I send a request to the first\nserver, then I send the request from the first server to the second, then\nfrom the second service I send a request to the third service, and then I\nreturn the response to the client. All this I do with the help of commands\nClientSend - ServerReciev-ServerSend-ClientReciev. If the request is one,\nthen all is well. If I send asynchronously 100 requests, then some of the\ntraces are lost. But if I debug the test in debug mode, or when I end the\ntest, I set a delay, then all the traces are displayed. It seems that all\nthe traces simply do not have time to send to the collector. Can you help\nme?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2192, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD617WbO7XjWXsE6YVG8dIre3WLPd6Mks5uc4IJgaJpZM4Wx-CF\n.\n. I think most people set their timezone on servers to UTC often the\noperating system, but also possible at JVM launch I think. we don't plan to\nsupport misalignment with UTC as it is quite common server setup and\neverything in the code assumes this.. closing for now, basically the areas are around the following:\n\nIndex creation and span name lookup is based on zipkin2.elasticsearch.internal.IndexNameFormatter. There may be other places, but this is likely where it is. In worst case you can make a copy of this file and change the timezone.. then place it in the front of your classpath.\nIf we get demand to have this as a normal option we'd need to have integration tests etc to show that it works, but above might work for you right now... sorry I think you opened this against the wrong repository https://github.com/CloudNativeJS/appmetrics-zipkin. Thanks.. good info tested with trace viewer. we dont support arbitrary string span IDs. a span ID is 64bit encoded as\nlower hex\n. the root client span is stand-alone-service.. if you click on the first\nspan you should see it in the details. the name chosen on the left is based\non servers. if you go to the dependency graph you should see\nstand-alone-service.\nhttps://gitter.im/openzipkin/zipkin for more!\nOn Fri, Sep 28, 2018 at 4:07 AM wxmimperio notifications@github.com wrote:\n\nTracing my own brave span like this:\nSender sender = OkHttpSender.create(\"http://127.0.0.1:9411/api/v2/spans\");AsyncReporter spanReporter = AsyncReporter.create(sender);Tracing tracing = Tracing.newBuilder()\n      .localServiceName(\"stand-alone-service\")\n      .spanReporter(spanReporter)\n      .sampler(Sampler.ALWAYS_SAMPLE)\n      .propagationFactory(ExtraFieldPropagation.newFactory(B3Propagation.FACTORY, \"user-name\"))\n      .currentTraceContext(ThreadContextCurrentTraceContext.create())\n      .supportsJoin(true)\n      .build();HttpTracing httpTracing = HttpTracing.create(tracing);brave.Span span = tracing.tracer().newTrace().name(\"stand-alone-service\");\nspan.start();\nspan.tag(\"test\", \"stand-alone-service\");\nspan.annotate(\"Method In\");\nokHttpTest(httpTracing); // call springboot with okHttpClient\nspan.annotate(\"Method Out\");\nspan.finish();\ntracing.close();\nspanReporter.close();\nsender.close();\nprivate static void okHttpTest(HttpTracing httpTracing) {\n  String url = \"http://localhost:8082\";\n  OkHttpClient okHttpClient = new OkHttpClient.Builder().addNetworkInterceptor(TracingInterceptor.create(httpTracing)).build();\n  Request.Builder builder = new Request.Builder();\n  Request request = builder.url(url).build();\n  Call call = okHttpClient.newCall(request);\n  try {\n      Response response = call.execute();\n      System.out.println(response.body().string());\n  } catch (IOException e) {\n      e.printStackTrace();\n  }\n}\n[image: image]\nhttps://user-images.githubusercontent.com/8111349/46194901-4f8b1c80-c335-11e8-8076-5ca65638f7d0.png\nhttp://localhost:8082 in okHttpTest is a url for springboot, and this\nproject has two traces like this:\n[image: image]\nhttps://user-images.githubusercontent.com/8111349/46196046-91699200-c338-11e8-8d4c-1d1fa6fc55a7.png\nNow,I want to connect my own span tracing with springboot tracing like\nthis:\nnormal jar (stand-alone-service) \u2014\u2014>\nspringboot2(zipkin-service2)\u2014\u2014>springboot1(zipkin-service1)\nHowever stand-alone-service can not tracing in springboot's call chain.\nHow can I make it works?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2200, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD616AElUWOq_EsUUICz43tx8Fq7PUjks5ufdjUgaJpZM4W-Cmz\n.\n. probably we need to see the json for the trace. can you click the JSON\nbutton and paste it?\n\nOn Fri, 28 Sep 2018, 23:27 wxmimperio, notifications@github.com wrote:\n\n@adriancole https://github.com/adriancole\nThanks.\nHowever there is no relation between stand-alone-service and\nzipkin-service2\u3001zipkin-service1 on stand-alone-service\u2018s graph.\nI want to connect them, but on graph is not.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2200#issuecomment-425611966,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61zG3pmrUX5lk49Pv0FEhGMaPgeiMks5ufuijgaJpZM4W-Cmz\n.\n. sorry issues are not great for support. can you please try https://gitter.im/openzipkin/zipkin. no problem. anything else hop on https://gitter.im/openzipkin/zipkin. This is a good issue description, perhaps on the wrong project. I think you might be asking about zipkin-js.\n\nCan you re-issue this there, and also include the example code and/or example json associated with the trace. Context issues are subtle so the setup is something likely to be helpful, hence the request for example code or failing unit test.. do your metrics show spans dropped? I assume the server is recent version\nOn Wed, 17 Oct 2018, 11:43 Knifeck, notifications@github.com wrote:\n\nkafka can be consumed by zipkin, but no trace data can be storaged.\nI set log lever\n--logging.level.zipkin2=DEBUG\nNo error info .\n2018-10-17 00:11:53.845 DEBUG 5249 --- [pool-2-thread-1]\nz.c.k.KafkaCollectorWorker : Kafka polling returned batch of 1 messages.\n2018-10-17 00:11:54.846 DEBUG 5249 --- [pool-2-thread-1]\nz.c.k.KafkaCollectorWorker : Kafka polling returned batch of 0 messages.\n2018-10-17 00:11:55.847 DEBUG 5249 --- [pool-2-thread-1]\nz.c.k.KafkaCollectorWorker : Kafka polling returned batch of 0 messages.\n2018-10-17 00:11:56.849 DEBUG 5249 --- [pool-2-thread-1]\nz.c.k.KafkaCollectorWorker : Kafka polling returned batch of 0 messages.\n2018-10-17 00:11:57.851 DEBUG 5249 --- [pool-2-thread-1]\nz.c.k.KafkaCollectorWorker : Kafka polling returned batch of 0 messages.\n2018-10-17 00:11:58.851 DEBUG 5249 --- [pool-2-thread-1]\nz.c.k.KafkaCollectorWorker : Kafka polling returned batch of 0 messages.\n2018-10-17 00:11:59.852 DEBUG 5249 --- [pool-2-thread-1]\nz.c.k.KafkaCollectorWorker : Kafka polling returned batch of 0 messages.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2215, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD618lgW0vzP6S7fnO395zqjjsQXdNvks5ulqdzgaJpZM4XjEtq\n.\n. can you join https://gitter.im/openzipkin/zipkin\n\nissues not the best for troubleshooting\nOn Wed, Oct 17, 2018 at 2:19 PM Knifeck notifications@github.com wrote:\n\nthe server is ubantu 16.04, you mean the version can't be support to\nstorage data\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2215#issuecomment-430503594,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD618KBr_kFbZyjI0y89icqwYB-4LfOks5ulsv9gaJpZM4XjEtq\n.\n. we have tests that run on every build, against ES 5.6.12 (currently)\n\nPlease go here for more support https://gitter.im/openzipkin/zipkin\n. I will try start this by changing the UI logic step-by-step to eventually use v2 data:\n\nbreak shared v1 spans into single host spans, but still v1 format\nEx. {\"cs\" \"sr\" \"sa\" \"ca\" \"ss\" \"cr\"} -> {\"cs\" \"cr\", \"sa\"},  {\"sr\" \"ss\", \"ca\"}\nadd span.kind, remoteEndpoint annotation to the UI's model temporarily and use it\n {\"cs\" \"cr\", \"sa\"},  {\"sr\" \"ss\", \"ca\"} -> {kind = CLIENT, \"cs\" \"cr\", remoteEndpoint},  {kind = SERVER, \"sr\" \"ss\", remoteEndpoint}\nadd localEndpoint annotation to the UI's model temporarily and use it\n{kind = CLIENT, \"cs\" \"cr\", remoteEndpoint},  {kind = SERVER, \"sr\" \"ss\", remoteEndpoint} -> {kind = CLIENT, \"cs\" \"cr\", localEndpoint, remoteEndpoint},  {kind = SERVER, \"sr\" \"ss\",  localEndpoint, remoteEndpoint}\ncomplete switch to v2 model\n{kind = CLIENT, \"cs\" \"cr\", localEndpoint, remoteEndpoint},  {kind = SERVER, \"sr\" \"ss\",  localEndpoint, remoteEndpoint} -> {kind = CLIENT, timestamp, duration, localEndpoint, remoteEndpoint},  {kind = SERVER, timestamp, duration,  localEndpoint, remoteEndpoint, shared=true}\n. one interesting design luck is that in the current UI we are a little inefficient. We make one traces api query for the index, then redundantly make a trace api query to show the trace view. A better UI will reuse the same data. However when we are porting logic, this is a bit lucky. Since each screen uses a different api, I can break one screen and not break the other one. For example, I can port the search screen to use v2 without touching the trace view screen.. Another thing to consider. We should have stub/mock data that includes some interesting traces:\ntrace with clock skew\nregular RPC trace\nmessaging trace\n\nThis way we can validate the UI behavior without being data dependent, even if that is visually comparing.. sample data is now in. I will do the trace summary screen conversion to v2 first as it is simplest and decoupled from the trace view one. If it were me, I would probably have tried to hide the details of the cross-cluster search in such as way that it would be less impactful, for example through escaping or otherwise vs expecting everyone to change their clusters. Not everyone grants us ability to change the index templates, so this will shift folks. I've again asked to not do this, so it can be formally accepted for reconsideration or more formally not reconsidered.. Honestly, that issue if the only issue about colons had very little publicly visible consideration. We already have tons more thought in how to work around it, this seems a bad call https://github.com/elastic/elasticsearch/issues/23892#issuecomment-434492664. mighty heads up @openzipkin/elasticsearch 7.x will drop the ability to create indexes with colons and 8.x will drop the ability to read them. This forces us into a migration concern including likely some logic change to do dual-reads (performance affecting), like we had to last time. Aliasing is another choice, but that would require a script of some sort which we don't have a concrete example of yet. I'm still hoping ES can be creative and stop this train, but just in case, your thoughts are welcome.. so @xeraa had some tips.. if things keep going this way off head I think the impact is..\n we have to choose a different delimiter than ':', even if parameterizing this, we still need a default. We can't predict if ES will blacklist a new one so I really want someone at ES to actively confirm the replacement.\n We have to change our delimiter when we detect ES 7+ I suggest we do it like this as that defers maintenance for others vs forcing a change to unaffected versions. For example, this makes the decision site specific and aligns it with the root cause (elastic/elasticsearch#23892) as opposed to what might feel like arbitrary busy work.\n  * this implies a change to the index template in our side, and also on their side. This will almost certainly break people's sites who use secondary templates.\n  * they will likely still have data so we'll need to form dual index patterns which in some cases will overflow the max query length (again)\n  * beyond this, all their maintenance scripts, and any other ES tooling need to be updated. I really want ES to own this problem, ideally they will put together a list of things to do vs expecting us and everyone else in the world to do this individually.. I have a work testing now, which will use single-character wildcard on search _ and explicit characters on insert. So, for <7 index-type delimiter : and >=7 -. It occurred to me that since we use the hyphen delimiter elsewhere, if this becomes banned by ES in the future it will break everything else. Instead of waiting for a special character to be cleared and not working with ES7 at all, better to assume they won't break hyphen until that's not the case.. hmm maybe \"_\" isn't allowed in the POST urls.... PS we can't really test all of this independently as there is more than just the one issue here before Elasticsearch 7 works. thanks for the fast turnaround!. LGTM thanks again!. another yay. To test this, I used \"skew.json\" before and after the change and noticed the service percentage was exactly the same. This is how I know the percentage function doesn't consider offset even without looking at the code (I also looked at the code though!)\n\n. This may indeed be a problem. I will look into it now as we don't have data yet to break this.. What we did is add an async span that runs past the critical path. \nWhen we don't do skew correction, the result is this:\n\nWhen we do skew correction, the result is this:\n\nWhile we don't know which result is correct, we should keep the historical behavior until we know what the correct result should be. So, this reverts the change.. also related https://engineering.salesforce.com/anomaly-detection-in-zipkin-trace-data-87c8a2ded8a1. because we currently need to understand if this is visualizing for the purpose of detecting anomalies or traffic flow or shape of call graphs, we'll want to see more input. Especially in netflix we discussed that people may want something like a service graph with more facts (such as cluster and zone), but we don't know exactly what's in mind without hearing more from Jon or others.. travis is angry https://travis-ci.org/openzipkin/zipkin/builds/449208466?utm_source=github_status&utm_medium=notification. lemme know if you think this is ready to go. pushing this up here @tacigar and @zeagord. Main thing I'm trying to get to is an understanding of what data is being used by what and why. I'm not there yet, but it looks like there will be some opportunities for simplification.. small bug here.. have a fix which I will add tomorrow. Fixed and verified percentages show exact same after refactoring:\n\n. PS I reconsidered having the storage api return the human description for reasons including i18n.\nFollowing is better. We can leave english (or japanese) description to UI assets and/or config.json\nGET /api/v2/tagKeys\n[\"environment\", \"threat.level\"]\n. PS I reconsidered having the storage api return the human description for reasons including i18n.\nFollowing is better. We can leave english (or japanese) description to UI assets and/or config.json\nGET /api/v2/tagKeys\n[\"environment\", \"threat.level\"]\n. https://cwiki.apache.org/confluence/display/ZIPKIN/Sites\n@drolando @kristofa @narayaruna @anuraaga @jcarres-mdsol @zeagord @huydx @jonathan-lo @jcchavezs Can you all add a section to your site doc like below:\n** Service Name\nAt XXX, we use the zipkin service name to represent XX. It is sourced from Y (environment, discovery, etc). We use it (only in zipkin, in correlation to a different system etc)\n** Site-specific tags\nThe following are span tags we frequently use in indexing or aggregation\n| Tag | Description | Usage |. https://cwiki.apache.org/confluence/display/ZIPKIN/Sites\n@drolando @kristofa @narayaruna @anuraaga @jcarres-mdsol @zeagord @huydx @jonathan-lo @jcchavezs Can you all add a section to your site doc like below:\n** Service Name\nAt XXX, we use the zipkin service name to represent XX. It is sourced from Y (environment, discovery, etc). We use it (only in zipkin, in correlation to a different system etc)\n** Site-specific tags\nThe following are span tags we frequently use in indexing or aggregation\n| Tag | Description | Usage |. updated site-specific tag request to also explain how you use service name. cc also @narayaruna as originally I spelled your github user wrong :(. updated site-specific tag request to also explain how you use service name. cc also @narayaruna as originally I spelled your github user wrong :(. update: I think we know a few things cc @openzipkin/core @openzipkin/ui \nHere are some insights from looking at @anuraaga's site https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=95655004\nSite specific tags include both fixed and high cardinality tags\n Fixed would be indexed similar to service/span #2309 these will allow auto-complete. Ex plan-id is likely fixed\n High cardinality is only for things like auto-completing or reserving a search field names. For example, antennae-id is not likely fixed\n* In any case, the UI would need overlay configuration to name the tags. Ex associating a friendly name and/or description with antennae-id. This could be done in /config.json which we can populate from spring configuration or even an environment variable.\n. update: I think we know a few things cc @openzipkin/core @openzipkin/ui \nHere are some insights from looking at @anuraaga's site https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=95655004\nSite specific tags include both fixed and high cardinality tags\n Fixed would be indexed similar to service/span #2309 these will allow auto-complete. Ex plan-id is likely fixed\n High cardinality is only for things like auto-completing or reserving a search field names. For example, antennae-id is not likely fixed\n* In any case, the UI would need overlay configuration to name the tags. Ex associating a friendly name and/or description with antennae-id. This could be done in /config.json which we can populate from spring configuration or even an environment variable.\n. @MrGlaucus @gsta-cloudins @gzchenyong @ChinaTelecomDragon @sheng-wu @v35715009 @JoveYu @GuangmingLuo @jaydensun can one of you confirm this is ok by running a build off this branch? If not, could you confirm if we built a snapshot?. while waiting on feedback here, we can also consider an option which is to replace this library we use https://github.com/openzipkin/zipkin/issues/2239. I heard from @v35715009 that this doesn't help. We probably need to change the library. mind doing pull request? I know we have another UI in progress but nice to\nfix this as so many complain.\nEspecially if you want to add japanese text :D\nOn Wed, Nov 14, 2018 at 5:09 PM tacigar notifications@github.com wrote:\n\n[image: 2018-11-14 18 09 33]\nhttps://user-images.githubusercontent.com/19551419/48471964-785d6780-e838-11e8-86d9-252ce2800a31.png\nIt works well now\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2239#issuecomment-438590041,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61zwMOXPuJYgPVFHTOcTCDl9tVB3Bks5uu93mgaJpZM4YPs7s\n.\n. this would imply many things notably an api change for pagination tokens\n. I think for normal trace browsing, best to use the UI. If you need to scroll through many pages and hold your place between them, it might hint another tool right now. For example, we have a spark job to run through all data to make the dependency graph.\n\nAre you interested in this for normal browsing? How many pages of traces do you think you'd need to see. I think some look at 100 traces (as opposed to 10 pages of 10), as long is the data is small you can consider just loading more.. yes, this is not going to work in the API. I think a user will be unlikely\nto scroll through millions of records, but anyway will hold the issue open\nfor you.\nMeanwhile, it is like better to focus on using Elasticsearch tools as if\nyou need to sift through millions and you cannot through the query we\nprovide, maybe you can with their tools.. only suggestion is to name the files also based on site. this helps folks know they are real, maybe answer questions later or link to site docs.\n. he opted for no attribution, which is totally ok. oops the first is in v1 format :) we should wash this through the server and re-add it.\n(ex POST through the v1 endpoint, then GET via the v2 one). please use http://gitter.im/openzipkin/zipkin for support\notherwise we spam all watchers of the repo with question reply such as what\nis producing the spans, what format etc.\nOn Tue, 6 Nov 2018, 19:37 iwanskit, notifications@github.com wrote:\n\nI also tried to deploy kafka and zipkin locally (without kubernetes) but\nwith the same error\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2242#issuecomment-436223375,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD613awoIjRB16z7X0dL0R6lOwaC5e4ks5usXSGgaJpZM4YQQdN\n.\n. Eureka isn't formally supported, but there is a workaround available here: https://github.com/openzipkin/zipkin/issues/1870#issuecomment-432659584\n\nplease prefer https://gitter.im/openzipkin/zipkin for questions as issues alert everyone. Take care!. ported more logic to javascript after testing in java. need some test backporting.\nthe hard part next will be to rewrite the clock skew to be single side. this code was deleted from java after porting to javascript. I might temporarily put it back until I figure out the logic as I am very slow with javascript due to nuance of the language and not as familiar with how to debug similar to java breakpoints.. this work can be a bit overwhelming so I am going to first port the v2 merging to java (ironically tested in javascript first!), and in a separate PR. Then, I'll change the tree building stuff (Node.java) to work off an assumption that spans with the same (id, kind) are merged. This might be somewhat tricky as late completed server spans might not report kind (but do report localEndpoint!).\nAnyway that strangles the nastiest stuff into one place. After that, it makes the rest of the work easier.. java is easier for me to figure out problems in java before porting to\njavascript. but also dependency linking is still java now also.\nit might be that some pipelines will want code like this anyway.. it would\nhelp them too once done.\nOn Fri, 16 Nov 2018, 00:44 Daniele, notifications@github.com wrote:\n\nDoes this move back the client/server span merging to Java? So the UI\nexpects to receive pre-merged spans?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2245#issuecomment-439107066,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61_s383Xv6Xf6hgAqw3eytzOW5hNrks5uvZnQgaJpZM4YWEuj\n.\n. java is easier for me to figure out problems in java before porting to\njavascript. but also dependency linking is still java now also.\n\nit might be that some pipelines will want code like this anyway.. it would\nhelp them too once done.\nOn Fri, 16 Nov 2018, 00:44 Daniele, notifications@github.com wrote:\n\nDoes this move back the client/server span merging to Java? So the UI\nexpects to receive pre-merged spans?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2245#issuecomment-439107066,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61_s383Xv6Xf6hgAqw3eytzOW5hNrks5uvZnQgaJpZM4YWEuj\n.\n. @drolando sorry to answer you directly. don't worry the UI will have all logic it needs. There's no difference in the query response from the server. @drolando sorry to answer you directly. don't worry the UI will have all logic it needs. There's no difference in the query response from the server. ok rebased over a bunch of internal work. The current code is not correct in any way, but I think I can make it correct now :) IOTW don't pay attention to current code on this branch.. it will be redone with the new Trace.merge stuff. ok rebased over a bunch of internal work. The current code is not correct in any way, but I think I can make it correct now :) IOTW don't pay attention to current code on this branch.. it will be redone with the new Trace.merge stuff. especially now with special casing due to shared IDs, it is getting strange to keep this generic. we only use it for spans. I'm finding myself having to explain awkardly and it is probably more time to make it concrete. I think I originally did this as we had 2 versions of Span (v1.Span v2.Span)\n\nLater I will change the \"Node\" to SpanNode or TraceNode as it makes documenting a lot easier and less confusing to read or write. especially now with special casing due to shared IDs, it is getting strange to keep this generic. we only use it for spans. I'm finding myself having to explain awkardly and it is probably more time to make it concrete. I think I originally did this as we had 2 versions of Span (v1.Span v2.Span)\nLater I will change the \"Node\" to SpanNode or TraceNode as it makes documenting a lot easier and less confusing to read or write. going to restore https://github.com/openzipkin/zipkin/blob/610b8bf796c5f91d2b73d8914a48308b2a3786a4/zipkin/src/main/java/zipkin/internal/CorrectForClockSkew.java and https://github.com/openzipkin/zipkin/blob/610b8bf796c5f91d2b73d8914a48308b2a3786a4/zipkin/src/test/java/zipkin/internal/CorrectForClockSkewTest.java to rework the old clock skew correcter to v2 types before porting back to javascript (and deleting the temporary java code version). finally clock skew is now v2 in java. last step is porting to javascript. ok! I think we are ready for a look. I think we can't always do span.shared because difference between undefined\nand false.\nfunny but I think you can see some tests fail travis not circleci I suppose\ndue to different browser.\nanyway I prefer simplest so if idea that keeps tests passing good by me!\nOn Fri, 23 Nov 2018, 09:02 tacigar <notifications@github.com wrote:\n\n@tacigar commented on this pull request.\nIn zipkin-ui/js/skew.js\nhttps://github.com/openzipkin/zipkin/pull/2245#discussion_r235836330:\n\n\n/**\n\n\nProcessing is taking a span and placing it at the most appropriate place in the trace tree.\n\n\n\n\nFor example, if this is a server span, it would be a different node, and a child of its client\n\n\n\n\neven if they share the same span ID.\n\n\n*\n\n\nProcessing is defensive of typical problems in span reporting, such as depth-first. For\n\n\n\n\nexample, depth-first reporting implies you can see spans missing their parent. Hence, the\n\n\n\n\nresult of processing all spans can be a virtual root node.\n\n\n*/\n_process(span) {\nconst endpoint = span.localEndpoint;\nconst key = keyString(span.id, span.shared, span.localEndpoint);\nconst noEndpointKey = endpoint ? keyString(span.id, span.shared) : key;\n+\nlet parent;\nif (span.shared === true) {\n\n\nnit: How about if (span.shared) { ... } ?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2245#pullrequestreview-177795706,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD611Pq2MlFRWP3PmzlEJtd76LbRYRKks5ux1cZgaJpZM4YWEuj\n.\n. easy enough to try and simplify. test will say so!\n\nOn Fri, 23 Nov 2018, 10:42 tacigar <notifications@github.com wrote:\n\nhmmmm...\nSorry, I thought again, but I don't see problems caused by the difference\nbetween undefined and false in this case...\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2245#issuecomment-441151057,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61y5jwnIRgRqovA7qzYY05o1cUxhiks5ux27BgaJpZM4YWEuj\n.\n. totally right on span.shared @tacigar . heh travis version of chrome still hates me. it is an ordering problem I cannot reproduce on my laptop... it is related to the shared flag\n\n[INFO] HeadlessChrome 0.0.0 (Linux 0.0.0) mergeV2ById should order spans by shared, timestamp then name FAILED\n[INFO]  AssertionError: expected [ '0000000000000004-true-1',\n[INFO]    '0000000000000004-false-2',\n[INFO]    '0000000000000003-false-2',\n[INFO]    '0000000000000002-false-3' ] to deeply equal [ '0000000000000004-false-2',\n[INFO]    '0000000000000004-true-1',\n[INFO]    '0000000000000003-false-2',\n[INFO]    '0000000000000002-false-3' ]. used FireFox locally to figure out the problem (used browser  Firefox with karma-firefox-launcher in zipkin-ui/karma.conf.js). rebased etc and now \ud83d\ude4f . Thank you so much @tacigar for the help with this!. this repo not Apache yet. I think it will be ok especially if you are in\nprogress.\n. thanks for checking!. considering the UI is to be rewritten in less than a month, not sure how much effort will try on this one. will this work for non-humans?. @jcchavezs then by your definition I think most issues are not raised by humans :D. @jcchavezs I am messing with you :D. @jcchavezs I am messing with you :D. I'd highly resist the idea of arbitrary plugins unless there's a well\nknown framework that can accomplish this. I've not seen such a thing\nbut it might exist. If it did, we'd get questions on how to install\nplugins as well.\nEven in haystack which has pluggable functionality, it is still\nrequired to design for the features involved (there is a convention\nbut it isn't automatic afaict).\nOTOH, it is good to consider modularity regardless. We just need to\nbalance requirements with reality which is what people have time to do\nand what exists.\n. so basically I'd leave this open.. probably more a maintainers burden than anything else. to get to the point of \"rule of 3\" we'd need things that aren't features yet that folks want to install.. then we can figure out if we have resources to build a mechanism framework etc.\nmaintainers like @tacigar may end up using a pluggable framework anyway :D\n@jcchavezs what I'd recommend for you is to top-level your request about log correlation. We have the LOGS_URL thing, but you are probably asking something else. It might be easier after the internals change to v2 format (for example we can access tags reliable after this and this might be needed depending on what you want).\nother opinions and challenges welcome :). one related concern to this point is the \"favorite trace\" thing that came up many times, most recently by @drolando. I agree hacking the UI in its current form isn't straightforward.. one related concern to this point is the \"favorite trace\" thing that came up many times, most recently by @drolando. I agree hacking the UI in its current form isn't straightforward.. PS not to overanswer, but I'm always worried about busfactor.\nThings requested and no-op in the past were web components (people like but no traction yet), and grafana (abstract idea unclear exactly what it would be).\nexploration is helpful, getting concrete or actionable is usually the where it stops. I suspect after the redo will be a better time to consider this deeply as folks will be less heads down.. PS not to overanswer, but I'm always worried about busfactor.\nThings requested and no-op in the past were web components (people like but no traction yet), and grafana (abstract idea unclear exactly what it would be).\nexploration is helpful, getting concrete or actionable is usually the where it stops. I suspect after the redo will be a better time to consider this deeply as folks will be less heads down.. does anyone know how folks currently customize via overlay webpacked javascript? One of the main things here is that the assets are intentionally remapped to different names allow the browser to update them. Regardless of jar or not jar (that part is easy to change in the server.. we can supply a file path override), the point will remain that webpack mapped files are not the easiest to change. cc @openzipkin/devops-tooling . does anyone know how folks currently customize via overlay webpacked javascript? One of the main things here is that the assets are intentionally remapped to different names allow the browser to update them. Regardless of jar or not jar (that part is easy to change in the server.. we can supply a file path override), the point will remain that webpack mapped files are not the easiest to change. cc @openzipkin/devops-tooling . added a commit for plain links. added a commit for plain links. maybe we just give gitter link then..\nOn Fri, 16 Nov 2018, 08:48 Daniele, notifications@github.com wrote:\n\nThat string is still more than 200 chars...\n\n\n\nlen(\"This issue tracker is not the place for questions. If you want to ask how to do something, or to understand why something isn't working the way you expect it to, please use https://gitter.im/openzipkin/zipkin or https://stackoverflow.com/questions/tagged/zipkin.\")262\n\n\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2254#issuecomment-439242878,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61-gi_W94C8LJ__GNG88036YA838Qks5uvgtVgaJpZM4Yi4Kp\n.\n. maybe we just give gitter link then..\n\nOn Fri, 16 Nov 2018, 08:48 Daniele, notifications@github.com wrote:\n\nThat string is still more than 200 chars...\n\n\n\nlen(\"This issue tracker is not the place for questions. If you want to ask how to do something, or to understand why something isn't working the way you expect it to, please use https://gitter.im/openzipkin/zipkin or https://stackoverflow.com/questions/tagged/zipkin.\")262\n\n\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2254#issuecomment-439242878,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61-gi_W94C8LJ__GNG88036YA838Qks5uvgtVgaJpZM4Yi4Kp\n.\n. thanks for the watchdog service @beckje01 @drolando!. thanks for the watchdog service @beckje01 @drolando!. please join https://gitter.im/openzipkin/zipkin as it isn't clear how this relates to the server project. possibly this is for the sleuth library?. please join https://gitter.im/openzipkin/zipkin as it isn't clear how this relates to the server project. possibly this is for the sleuth library?. seems related to https://github.com/openzipkin/zipkin/pull/2247 recently\nimproved by @tacigar\n\ncan you try snapshot? not sure the above fixes all of it. that i18n library\nwe use is unfortunately abandoned. we have an open issue to replace it.\nOn Fri, 16 Nov 2018, 13:58 Nara, notifications@github.com wrote:\n\nSeeing the attached errors when loading the homepage. Also only the label\n\"Zipkin Investigate System Behavior\" appears and rest of the UI then after\nfew milliseconds. This issue is appearing only after upgrading to the\nlatest UI, Zipkin version 2.11.8.\n[image: image]\nhttps://user-images.githubusercontent.com/1772168/48600652-9d79d500-e921-11e8-8edb-cd6bb3979c06.png\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2257, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61_7GrXdCnfHFs7xVP-Y4LrvKtRbSks5uvlQigaJpZM4Yle61\n.\n. seems related to https://github.com/openzipkin/zipkin/pull/2247 recently\nimproved by @tacigar\n\ncan you try snapshot? not sure the above fixes all of it. that i18n library\nwe use is unfortunately abandoned. we have an open issue to replace it.\nOn Fri, 16 Nov 2018, 13:58 Nara, notifications@github.com wrote:\n\nSeeing the attached errors when loading the homepage. Also only the label\n\"Zipkin Investigate System Behavior\" appears and rest of the UI then after\nfew milliseconds. This issue is appearing only after upgrading to the\nlatest UI, Zipkin version 2.11.8.\n[image: image]\nhttps://user-images.githubusercontent.com/1772168/48600652-9d79d500-e921-11e8-8edb-cd6bb3979c06.png\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2257, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61_7GrXdCnfHFs7xVP-Y4LrvKtRbSks5uvlQigaJpZM4Yle61\n.\n. snapshot should sort this out, so would appreciate if you can try it\n\nOn Sat, Dec 1, 2018 at 8:17 AM Melloware notifications@github.com wrote:\n\nI have this JS error as well on 2.11.8 Haven't tried the SNAPSHOT release\nyet to see if its fixed.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2257#issuecomment-443342293,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610hY2vp6liva_aWyB53nzN44OZ2Jks5u0aB_gaJpZM4Yle61\n.\n. snapshot should sort this out, so would appreciate if you can try it\n\nOn Sat, Dec 1, 2018 at 8:17 AM Melloware notifications@github.com wrote:\n\nI have this JS error as well on 2.11.8 Haven't tried the SNAPSHOT release\nyet to see if its fixed.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2257#issuecomment-443342293,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610hY2vp6liva_aWyB53nzN44OZ2Jks5u0aB_gaJpZM4Yle61\n.\n. thanks again. thanks again. LGTM thanks!\n. LGTM thanks!\n. cool! looks good\n\nnit maybe the text above no longer needs to say us? maybe it does..\n. cool! looks good\nnit maybe the text above no longer needs to say us? maybe it does..\n. \n@drolando how about this? there's not much room... just added a commit. @drolando if you want change after no problem. hopefully we don't have to describe traces that took minutes or hours too much :D. hmm cassandra docker test might be legit broke :P will look more carefully. [ERROR] getTraces_minDuration(zipkin2.storage.cassandra.ITCassandraStorage$ITSpanStore)  Time elapsed: 1.751 s  <<< FAILURE!\nfails locally as well.... I have to redo how this works because cassandra impl always needs to filter due to how trace IDs are indexed. I was wrong.. this has nothing to do with cassandra. I had a bug in my impl. ergh.. there might be a behaviour needed still... gonna wait until travis passes or fails.. it is too time consuming to keep running the cassandra tests and I have other things to do today. formatting nit on Expand All\n. looks good.. if you have time for before after otherwise I dont mind\nchecking my installing local\n@tacigar curious what you use to make animated gif for before and after..\n. there is always debate about whether collapse all, expand all etc, is good or not\nthe original logic was to only expand spans matching the service from the previous screen. However, we have to consider now we can select all services in the query. Also, one can navigate to a trace directly. It could make sense to expand the service if one was selected in the prior screen, still, though.. PS regardless of last commentary, expand and collapse being buggy is far more important than the behaviour :) I think people will like a not buggy screen more than they care about default. @drolando  on this:\n\nthe original logic was to only expand spans matching the service from the previous screen.\nI'm -1 on this. That a button called \"expand ALL\" only expands some subtrees is very >unintuitive. If people want this I'd add a third button \"expand service\" or something like that.\n\nthat wasn't the suggestion at all. What I meant was when navigating to the screen, and service \"edge\" was what was queried for, the edge services were expanded IIRC. I agree it would be senseless to have an explicit press of a button called \"expand ALL\" result in not expanding all. I was referring to the act of navigation.. > I'm pretty sure that defaulting to everything collapsed I'd get a ton of questions from developers asking why there's only one span and if zipkin is broken. Given that the UI has defaulted to everything expanded for a long time (and that most people don't even know those 2 button exists) changing it to collapsed would be an unexpected change.\nconfiguration is a burden, it needs to be carried over to the new UI etc. I don't think many UIs, if any, default to collapse all. We just default to what we had originally (expand service highlighted or all if no service), or default to expand all.\nThis was intending to fix a bug, not change the feature in other words.\n. Thanks for taking a stab @zeagord all the feedback folks! Great to hear people championing for good :). so clarifying default is expand all looks good.\nthere is one dangling format concern I mentioned which makes me nervous\nabout the i18n plugin. might want to look at that.\n. I'll test locally and fix the lint in a bit. ok last thing this needs to to make expand all back to default. then good to go!. fails like this:\n[INFO]  TypeError: $selected.$expander.html is not a function\n[INFO]      at Object.<anonymous> (webpack:///js/component_ui/trace.js:15:24 <- test/component_ui/trace.test.js:49605:25)\n[INFO]      at Function.each (webpack:///node_modules/jquery/dist/jquery.js:360 <- test/component_ui/trace.test.js:483:19)\n[INFO]      at showSpans (webpack:///js/component_ui/trace.js:11:4 <- test/component_ui/trace.test.js:49601:20)\n[INFO]      at Context.<anonymous> (webpack:///test/component_ui/trace.test.js:181:14 <- test/component_ui/trace.test.js:49495:26). thanks so much!. we could consider supporting the javax.net.ssl.keyStore properties in our ES autoconfiguration. I don't really want to create any new variables and this. I understand okhttp's reply on the topic, but when we are running a server, we are controlling the binary and its only use is for zipkin.\n@arap would you be up to trying this? Goal will be to only affect the autoconfiguration types in our ES code. as the server is most often used as a black box, and there's high value in\nre-using existing docs, I'd prefer usage of \"javax.net.ssl.keyStore\"\nproperties and friends.\nOn Wed, Nov 28, 2018 at 11:43 PM arap notifications@github.com wrote:\n\n@adriancole https://github.com/adriancole I can contribute, but I don't\nfully understand Your point.\nShould autoconfiguration rely on javax.net.ssl.keyStore JVM variable,\napplication property, or something else?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2266#issuecomment-442433954,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xWpOLu6jtcKgkZyz8Q5Om1r0TWJks5uzoUKgaJpZM4YrdjN\n.\n. as the server is most often used as a black box, and there's high value in\nre-using existing docs, I'd prefer usage of \"javax.net.ssl.keyStore\"\nproperties and friends.\n\nOn Wed, Nov 28, 2018 at 11:43 PM arap notifications@github.com wrote:\n\n@adriancole https://github.com/adriancole I can contribute, but I don't\nfully understand Your point.\nShould autoconfiguration rely on javax.net.ssl.keyStore JVM variable,\napplication property, or something else?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2266#issuecomment-442433954,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xWpOLu6jtcKgkZyz8Q5Om1r0TWJks5uzoUKgaJpZM4YrdjN\n.\n. spiked https://github.com/openzipkin/zipkin/pull/2270 in case folks agree. indeed. we can convert using local server or post to the actual server\n. rule of 3. if later things get hectic, it will force the issue of v1 -> v2 client side and we'll take it from there.. LGTM merge when you think it is good.. oh heh @tacigar isn't committer yet :P. > I'll make PR.\nyou are a PR addict\n. you are a detective!. thank you again for the close eyes and able hands @tacigar!. cc @openzipkin/ui . relates to #1794 \n\nso I think what we can do is float the topic of prefer local vs remote until the rest of v2 is done, preferring local for now. \nI think best case is that the mustache template has a remoteServiceName iff it is a root or leaf\nthen the UI logic can do whatever it wants. This sort of decoration will be easier when v2 is done.. 3 is best. it reflects the same logic as dependency graph. we still have an\nissue to help identify remote uninstrumented callers or receivers but I\nthink we should do that without removing the ability to label the server\ncorrectly.. fyi @fyi-coursera opened #2345 just now choosing to display only the remote service name when a client-only trace (which would be very weird in practice unless a bug).\nI think we had discussions in the past about potentially showing two labels similar to I think how haystack works if I recall. I'm not sure if we want to special-case single-span traces in any way. \nHowever, I do think that the bottom of the tree (leaves which are clients) should have handling.. fyi @fyi-coursera opened #2345 just now choosing to display only the remote service name when a client-only trace (which would be very weird in practice unless a bug).\nI think we had discussions in the past about potentially showing two labels similar to I think how haystack works if I recall. I'm not sure if we want to special-case single-span traces in any way. \nHowever, I do think that the bottom of the tree (leaves which are clients) should have handling.. copying discussion from  #2345 as it is very hard to follow decisions and rationale when discussion occurs with partial context in multiple issues.\nfrom @fyi-coursera\n\nThis PR makes the UI use the client remote service name instead of the client local service name if no server local service name exists.\n\nfrom @drolando \n\nI'm +1 on showing the remoteEndpoint name if the server span is missing. For exactly the same reason @fyi-coursera mentioned.\nMy mysql spans now show up as \"serviceX\" rather than \"mysql\" which is very annoying. Same for calls to outside services i.e. AWS or google.\nI remember we discussed this a month or so ago and I thought this was already what we agreed upon. And was about to open an issue as well.\n\nfrom me\n\nI think this should only apply to leaf spans. otherwise it will definitely mess up association with the dependency graph.. copying discussion from  #2345 as it is very hard to follow decisions and rationale when discussion occurs with partial context in multiple issues.\n\nfrom @fyi-coursera\n\nThis PR makes the UI use the client remote service name instead of the client local service name if no server local service name exists.\n\nfrom @drolando \n\nI'm +1 on showing the remoteEndpoint name if the server span is missing. For exactly the same reason @fyi-coursera mentioned.\nMy mysql spans now show up as \"serviceX\" rather than \"mysql\" which is very annoying. Same for calls to outside services i.e. AWS or google.\nI remember we discussed this a month or so ago and I thought this was already what we agreed upon. And was about to open an issue as well.\n\nfrom me\n\nI think this should only apply to leaf spans. otherwise it will definitely mess up association with the dependency graph.. As few maintain the UI code I insist people consider the context of this when making suggestions or raising change.\n\n\nAs noted in this issue, the context for this is either a root span or a leaf client span. In-between will totally mess things up to switch prioritization rules.\nAccordingly, any suggestions must apply IFF the above is the case and not when it is not the case. In other words, tests must prove that code doesn't mess up the intermediate data.\nNot everyone has busted instrumentation. For example, I notice at py_zipkin examples, there is routine mistaken service labels where client and server names are flipped. We should be very careful to get diverse feedback especially if going to change high maintenance code before deferring towards something that may be a side effect of bad instrumentation.\n\nFinally, the prioritization is only needed when there is only one label. We have no need to constrain the lens code to single label.\nSo, my plea is that when folks suggest things, please consider the whole context above so that we can make a good change and not add new tech debt. Happy to help change, just we need to do it with rigor that means tests and with feedback on both good and busted instrumentation.\ncc also @openzipkin/ui . As few maintain the UI code I insist people consider the context of this when making suggestions or raising change.\n\nAs noted in this issue, the context for this is either a root span or a leaf client span. In-between will totally mess things up to switch prioritization rules.\nAccordingly, any suggestions must apply IFF the above is the case and not when it is not the case. In other words, tests must prove that code doesn't mess up the intermediate data.\nNot everyone has busted instrumentation. For example, I notice at py_zipkin examples, there is routine mistaken service labels where client and server names are flipped. We should be very careful to get diverse feedback especially if going to change high maintenance code before deferring towards something that may be a side effect of bad instrumentation.\n\nFinally, the prioritization is only needed when there is only one label. We have no need to constrain the lens code to single label.\nSo, my plea is that when folks suggest things, please consider the whole context above so that we can make a good change and not add new tech debt. Happy to help change, just we need to do it with rigor that means tests and with feedback on both good and busted instrumentation.\ncc also @openzipkin/ui . PS technically checking to see if a span is a leaf implies looking at \"children\" to see if it is empty\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-ui/js/component_data/spanNode.js#L31\nthere is a similar type in zipkin-lens we haven't merged this code to same library yet. PS technically checking to see if a span is a leaf implies looking at \"children\" to see if it is empty\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-ui/js/component_data/spanNode.js#L31\nthere is a similar type in zipkin-lens we haven't merged this code to same library yet. this would need to become more popular and also with clear ownership as last time it became abandoned and we had to remove it #510. #2274 has some follow-up about the service label ranking, which I think will be far easier once all code is in v2, particularly in a tree format. I'll raise alert to @openzipkin/core and @openzipkin/ui as this change decides to be explicit about lifecycle vs annotation. Some may prefer we continue to map the \"sr\" etc style names from v1, but anyway please consider and raise your POV if you have any strong feelings.. Here's a great reason why start/finish is better.. you might recall we had to do some hacks to backport messaging to v1 format. We no longer have to play this game cc @ImFlog @jeqo @jonathan-lo\nBefore:\n\nNow: we don't need to hijack wire send, which is very helpful as the instrumentation might have added it! cc also @anuraaga who is recently using this.\nAfter:\n\n. For the sake of getting work complete before we need it next week, I'll merge this. If after the fact, folks want me to revert the names I'll be ok doing that as it will be somewhat mechanical.. you'll notice I fixed the examples which were not good :P. gracie!\nOn Tue, Nov 27, 2018 at 2:47 AM Daniele notifications@github.com wrote:\n\n@drolando approved this pull request.\nFix & ship\nIn zipkin-ui/static/traces_it_IT.properties\nhttps://github.com/openzipkin/zipkin/pull/2278#discussion_r236379186:\n\n@@ -22,7 +22,7 @@ modal.title3 = 1. Ricerca per nome dello span\n modal.p4 = Span sono generalmente metodi thrift o endpoints Rails. Questo ti permette di recuperare tracce che accedono ad una particolare parte del servizio.\n modal.title4 = 2. Ricerca per annotazione\n modal.p5 = Il suo valore e' di tipo stringa ed e' quello che puoi utilizzare come parametro di ricerca. Possono essere cose come.\n-modal.title5 = 3. Ricerca per chiave/valore\n-modal.p6 = Annotazioni di tipo chiave / valore sono un metadato extra attaccato ad un traccia. Esempio: l'url chiamato, il codice di risposta o un'eccezione. Guarda sotto per alcuni esempio.\n+modal.title5 = 3. Ricerca per etichetta\n+modal.p6 = Etichette sono un metadato extra attaccato ad un traccia. Esempio: l'url chiamato, il codice di risposta o un'eccezione. Guarda sotto per alcuni esempio.\n\n\u2b07\ufe0f Suggested change\n-modal.p6 = Etichette sono un metadato extra attaccato ad un traccia. Esempio: l'url chiamato, il codice di risposta o un'eccezione. Guarda sotto per alcuni esempio.\n+modal.p6 = Le etichette sono un metadato extra attaccato ad un traccia. Esempio: l'url chiamato, il codice di risposta o un'eccezione. Guarda sotto per alcuni esempio.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2278#pullrequestreview-178442229,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD619l7NJA4tu4bQoUwGdyFSC5Fj_Lrks5uzDa6gaJpZM4YzJrr\n.\n. tested by viewing the netflix trace with the missing shared flag and it visualizes properly. tested by viewing the netflix trace with the missing shared flag and it visualizes properly. CI glitch is likely due to difference between travis and circleci chrome. Will use FireFox locally to investigate. CI glitch is likely due to difference between travis and circleci chrome. Will use FireFox locally to investigate. using FireFox locally to figure out the problem (browser Firefox with karma-firefox-launcher in zipkin-ui/karma.conf.js). using FireFox locally to figure out the problem (browser Firefox with karma-firefox-launcher in zipkin-ui/karma.conf.js). At atlassian, they like a \"mini-map\" like in video games. They like the preview version of zipkin lens.. At atlassian, they like a \"mini-map\" like in video games. They like the preview version of zipkin lens.. Others like mini-map as well. We can look at Chrome and Firefox developer tools for examples (besides Zelda). Others like mini-map as well. We can look at Chrome and Firefox developer tools for examples (besides Zelda). Atlassian said one reason they don't like jaeger was exactly this feature! What they said was that the plotting of traces in Jaeger seemed distracting and random. In other words, we should try not to emulate them.\n\nWhat they suggest instead is make sure services are color-coded. They like that you can preview a trace by clicking on it. If doing histogram, do it in a different screen, and only do it based on service+span name (ex maybe same data as trace query but different screen). Atlassian said one reason they don't like jaeger was exactly this feature! What they said was that the plotting of traces in Jaeger seemed distracting and random. In other words, we should try not to emulate them.\nWhat they suggest instead is make sure services are color-coded. They like that you can preview a trace by clicking on it. If doing histogram, do it in a different screen, and only do it based on service+span name (ex maybe same data as trace query but different screen). I see nothing! ps when you submit can you change to v2 format? easy way is to post local to v1 endpoint then GET on the v2. I see nothing! ps when you submit can you change to v2 format? easy way is to post local to v1 endpoint then GET on the v2. thanks tons lance!. thanks tons lance!. context was @kellabyte was asking about a sample. Here's the chat description from gitter:\n\nTo describe it in a few sentences\u2026 we add trace propegation across web redirects to tie several systems together, especially when they involve round trips to other web sites we don\u2019t own. Further we add user IDs to these traces to investigate specific customer issues or complaints. Lastly we also embed the trace context in things like oauth authorization codes so we can tie a full user actions together even when part of that action is performed on backend systems we don\u2019t own. . context was @kellabyte was asking about a sample. Here's the chat description from gitter:\nTo describe it in a few sentences\u2026 we add trace propegation across web redirects to tie several systems together, especially when they involve round trips to other web sites we don\u2019t own. Further we add user IDs to these traces to investigate specific customer issues or complaints. Lastly we also embed the trace context in things like oauth authorization codes so we can tie a full user actions together even when part of that action is performed on backend systems we don\u2019t own. . we dont actively support those methods. I suspect we could just turn them\noff somehow.\n. we dont actively support those methods. I suspect we could just turn them\noff somehow.\n. @tacigar mind trying this?. @tacigar mind trying this?. before:\n\n\nafter:\n\n. before:\n\nafter:\n\n. thanks for the context. thanks for the context. it is possible we could allow a setting to disable the \"all\" search control as indeed it is the most expensive query. meanwhile there could be a way to improve the performance (like by redoing the query in some way that isn't N+1 in nature) cc @openzipkin/cassandra \n. it is possible we could allow a setting to disable the \"all\" search control as indeed it is the most expensive query. meanwhile there could be a way to improve the performance (like by redoing the query in some way that isn't N+1 in nature) cc @openzipkin/cassandra \n. before:\n\nafter:\n\n. before:\n\nafter:\n\n. @tacigar @zeagord either of you able to knock this one out?. @tacigar @zeagord either of you able to knock this one out?. >\n\nWhere ever this gets put it would be nice if consideration for a lot of\nannotations were made. It doesn't happen often, but we sometimes get a lot\nof annotations on a span so a scrollbar/etc... for making it easy to\nnavigate the details would be nice.\ndo you have an example trace? I agree we sometimes get large amounts of\ntags or annotations, sometimes json.\n. >\nWhere ever this gets put it would be nice if consideration for a lot of\nannotations were made. It doesn't happen often, but we sometimes get a lot\nof annotations on a span so a scrollbar/etc... for making it easy to\nnavigate the details would be nice.\ndo you have an example trace? I agree we sometimes get large amounts of\ntags or annotations, sometimes json.\n. >\nWhere ever this gets put it would be nice if consideration for a lot of\nannotations were made. It doesn't happen often, but we sometimes get a lot\nof annotations on a span so a scrollbar/etc... for making it easy to\nnavigate the details would be nice.\ndo you have an example trace? I agree we sometimes get large amounts of\ntags or annotations, sometimes json.\n. one way to visualize what I meant to suggest is comparing to other\nthings.. lacking a mock-up tool :)\n\nso in the trace search screen, an accordion (down arrow or otherwise)\nto show a trace preview seems ok because you aren't blocking a part of\nthe trace when viewing, rather the next result (a different trace).\nThis could be a problem in viewing one trace.. ex if you had an\naccordion that pushed details down, you have the same block the trace\nproblem as pop-out, except that you can see above it.\nNot UX person, but trying to relate the sentiment.. let's say instead\nwhen you select a span you see a preview on a full length vertical\npane on the right side. This has the same position regardless of which\nspan you select, which means you can easily tell difference between\ntags etc. It doesn't block the trace, or make you have to move too\nmuch (you can consider we could use arrow keys to select a span, not\njust mouse). It would re-render on each seletion similar to a checkout\nscreen where the right side has a total which updates when you change\nshipping on the left.\nOn Wed, Nov 28, 2018 at 11:45 PM Tommy Ludwig notifications@github.com wrote:\n\nWhat about a separate pane that instructs to click on a span to see details? This would provide a separate area that doesn't mess with viewing the trace and work guide people to knowing they can click on spans to view more details.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. one way to visualize what I meant to suggest is comparing to other\nthings.. lacking a mock-up tool :)\n\nso in the trace search screen, an accordion (down arrow or otherwise)\nto show a trace preview seems ok because you aren't blocking a part of\nthe trace when viewing, rather the next result (a different trace).\nThis could be a problem in viewing one trace.. ex if you had an\naccordion that pushed details down, you have the same block the trace\nproblem as pop-out, except that you can see above it.\nNot UX person, but trying to relate the sentiment.. let's say instead\nwhen you select a span you see a preview on a full length vertical\npane on the right side. This has the same position regardless of which\nspan you select, which means you can easily tell difference between\ntags etc. It doesn't block the trace, or make you have to move too\nmuch (you can consider we could use arrow keys to select a span, not\njust mouse). It would re-render on each seletion similar to a checkout\nscreen where the right side has a total which updates when you change\nshipping on the left.\nOn Wed, Nov 28, 2018 at 11:45 PM Tommy Ludwig notifications@github.com wrote:\n\nWhat about a separate pane that instructs to click on a span to see details? This would provide a separate area that doesn't mess with viewing the trace and work guide people to knowing they can click on spans to view more details.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. one way to visualize what I meant to suggest is comparing to other\nthings.. lacking a mock-up tool :)\n\nso in the trace search screen, an accordion (down arrow or otherwise)\nto show a trace preview seems ok because you aren't blocking a part of\nthe trace when viewing, rather the next result (a different trace).\nThis could be a problem in viewing one trace.. ex if you had an\naccordion that pushed details down, you have the same block the trace\nproblem as pop-out, except that you can see above it.\nNot UX person, but trying to relate the sentiment.. let's say instead\nwhen you select a span you see a preview on a full length vertical\npane on the right side. This has the same position regardless of which\nspan you select, which means you can easily tell difference between\ntags etc. It doesn't block the trace, or make you have to move too\nmuch (you can consider we could use arrow keys to select a span, not\njust mouse). It would re-render on each seletion similar to a checkout\nscreen where the right side has a total which updates when you change\nshipping on the left.\nOn Wed, Nov 28, 2018 at 11:45 PM Tommy Ludwig notifications@github.com wrote:\n\nWhat about a separate pane that instructs to click on a span to see details? This would provide a separate area that doesn't mess with viewing the trace and work guide people to knowing they can click on spans to view more details.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. > @adriancole https://github.com/msindwan/zipkin-view has that same accordion that you're describing I think. And there's a screenshot there in the README\n\nThanks, Daniele. Yeah so the idea is to move it to a pane on the\nright, which fixes the position of things. As tags are less volatile\nthan timeline, might be best to actually order tags above. Anyway good\ncall that is the comparison I meant.\n. > @adriancole https://github.com/msindwan/zipkin-view has that same accordion that you're describing I think. And there's a screenshot there in the README\nThanks, Daniele. Yeah so the idea is to move it to a pane on the\nright, which fixes the position of things. As tags are less volatile\nthan timeline, might be best to actually order tags above. Anyway good\ncall that is the comparison I meant.\n. Here's an image of what I meant. I used osx preview app with annotate function as I am not more fancy :)\n. Here's an image of what I meant. I used osx preview app with annotate function as I am not more fancy :)\n. saving power by merging now verifying the zipkin-ui part passed :P. saving power by merging now verifying the zipkin-ui part passed :P. circleci passes:\n[INFO] HeadlessChrome 0.0.0 (Linux 0.0.0): Executed 206 of 206 SUCCESS (0.131 secs / 0.087 secs)\n[INFO] Firefox 63.0.0 (Linux 0.0.0): Executed 206 of 206 SUCCESS (0.362 secs / 0.167 secs)\n[INFO] TOTAL: 412 SUCCESS. circleci passes:\n[INFO] HeadlessChrome 0.0.0 (Linux 0.0.0): Executed 206 of 206 SUCCESS (0.131 secs / 0.087 secs)\n[INFO] Firefox 63.0.0 (Linux 0.0.0): Executed 206 of 206 SUCCESS (0.362 secs / 0.167 secs)\n[INFO] TOTAL: 412 SUCCESS. travis also:\n[INFO] HeadlessChrome 0.0.0 (Linux 0.0.0): Executed 206 of 206 SUCCESS (1.081 secs / 0.711 secs)\n[INFO] Firefox 56.0.0 (Linux 0.0.0): Executed 206 of 206 SUCCESS (1.163 secs / 0.474 secs)\n[INFO] TOTAL: 412 SUCCESS. travis also:\n[INFO] HeadlessChrome 0.0.0 (Linux 0.0.0): Executed 206 of 206 SUCCESS (1.081 secs / 0.711 secs)\n[INFO] Firefox 56.0.0 (Linux 0.0.0): Executed 206 of 206 SUCCESS (1.163 secs / 0.474 secs)\n[INFO] TOTAL: 412 SUCCESS. if anyone does a spike, do link back for the fans. if anyone does a spike, do link back for the fans. I see there was a spike of interest, but unsure any outcome. Did anyone do anything?. I see there was a spike of interest, but unsure any outcome. Did anyone do anything?. will fix add tests etc after a few votes come in. will fix add tests etc after a few votes come in. this is only internal to UI because javascript needs string key. the ids\nhere are used for collapse feature and we need a primary ID string per row.\ncould make that a concatenation as it wont matter.\n@drolando asked offline to have the \"show ids\" pane show both client and\nserver ID. that is fine and will do.\n. this is only internal to UI because javascript needs string key. the ids\nhere are used for collapse feature and we need a primary ID string per row.\ncould make that a concatenation as it wont matter.\n@drolando asked offline to have the \"show ids\" pane show both client and\nserver ID. that is fine and will do.\n. @tacigar probably better to do this in lens and not in the old UI. sound good?. @tacigar probably better to do this in lens and not in the old UI. sound good?. @openzipkin/ui so happy this is done.. so much work. I tested by viewing a netflix trace before and after as that's our most complex one.. @openzipkin/ui so happy this is done.. so much work. I tested by viewing a netflix trace before and after as that's our most complex one.. thanks much!. thanks much!. Thank you SOOO much it is great to have a big trace like this. We've been waiting a while to have data like this!\nonce \"lens\" is out, we can try to add screenshots of trace and/or dependency graph to the README cc @openzipkin/ui . Thank you SOOO much it is great to have a big trace like this. We've been waiting a while to have data like this!\nonce \"lens\" is out, we can try to add screenshots of trace and/or dependency graph to the README cc @openzipkin/ui . made a comment about fixed cardinality.. we definitely need to document this as it is indeed inappropriate for unbounded. https://github.com/openzipkin/zipkin/issues/2236#issuecomment-444302547\nOne thing @zeagord and I discussed is initially inheriting the config for the other names (service/span). This is for simplicity. In the future we could add a timestamp/lookback parameter to only fetch the values for a range. However, same problem would apply to service/span so thinking of that later. > reuse _q field in elasticsearch\nwe can think about it but the performance might be bad.\nfor example, getting the key names would require an expression I am not\nsure how to express unless we hard code the possible key names.\nif we hard code the possible key names yes it could work, but we have to\ncheck the performance of scanning all span documents to get the values\n. > reuse _q field in elasticsearch\nwe can think about it but the performance might be bad.\nfor example, getting the key names would require an expression I am not\nsure how to express unless we hard code the possible key names.\nif we hard code the possible key names yes it could work, but we have to\ncheck the performance of scanning all span documents to get the values\n. good news is we can try it. actually I think we need to hard code key names\nanyway especially in Cassandra.\nOn Mon, 10 Dec 2018, 08:27 Adrian Cole <adrian.f.cole@gmail.com wrote:\n\n\nreuse _q field in elasticsearch\n\nwe can think about it but the performance might be bad.\nfor example, getting the key names would require an expression I am not\nsure how to express unless we hard code the possible key names.\nif we hard code the possible key names yes it could work, but we have to\ncheck the performance of scanning all span documents to get the values\n. good news is we can try it. actually I think we need to hard code key names\nanyway especially in Cassandra.\n\nOn Mon, 10 Dec 2018, 08:27 Adrian Cole <adrian.f.cole@gmail.com wrote:\n\n\nreuse _q field in elasticsearch\n\nwe can think about it but the performance might be bad.\nfor example, getting the key names would require an expression I am not\nsure how to express unless we hard code the possible key names.\nif we hard code the possible key names yes it could work, but we have to\ncheck the performance of scanning all span documents to get the values\n. so on cassandra (and elasticsearch) we'll need to ensure the \"deduper\" is in use to avoid thrashing writes.\n\nIn both cases, it might be helpful to reverse-engineer the service-span mapping to re-use the same table. ex PRIMARY KEY ((type, key), value) This could make data management in general easier long term.\nSince there is a time bomb on elasticsearch #2219, we might want to solve that first before merging this (or at least before cutting a release with it).\nMeanwhile, we can allow UI testing to work with static managed list of tags. (ex there may be only several values associated with phase, for example.. so one way is to allow the UI to configure predefined where it is small). so on cassandra (and elasticsearch) we'll need to ensure the \"deduper\" is in use to avoid thrashing writes.\nIn both cases, it might be helpful to reverse-engineer the service-span mapping to re-use the same table. ex PRIMARY KEY ((type, key), value) This could make data management in general easier long term.\nSince there is a time bomb on elasticsearch #2219, we might want to solve that first before merging this (or at least before cutting a release with it).\nMeanwhile, we can allow UI testing to work with static managed list of tags. (ex there may be only several values associated with phase, for example.. so one way is to allow the UI to configure predefined where it is small). this chops off the basic functionality and will allow the  UI work to start immediately when merged. I can help rework the other commits similarly https://github.com/openzipkin/zipkin/pull/2332. this chops off the basic functionality and will allow the  UI work to start immediately when merged. I can help rework the other commits similarly https://github.com/openzipkin/zipkin/pull/2332. other storage impls pulled out in #2333 and #2334. other storage impls pulled out in #2333 and #2334. I think this is nearly ready. we need to test the auto-upgrade logic and also update the README files to talk about how autocomplete works. @michaelsembwever sorry about the formatting thing. I was in a rush to get something stable before I turned off internet for the vacation, but that amplified efforts to others.. not sure the better call but I apologize nevertheless. thanks for reviewing despite this.. FYI travis is failing still on the same tests as last push this needs to be looked into prior to merge. >\n\nCan you add a similar annotation to one of the test traces? That way we\nmight notice of we break this again in the future.\nOr add a unit test for this with a comment\n\n\nread my mind.. I am putting together some high-level test for this now\n\n. >\nCan you add a similar annotation to one of the test traces? That way we\nmight notice of we break this again in the future.\nOr add a unit test for this with a comment\n\n\nread my mind.. I am putting together some high-level test for this now\n\n. so here is the before and after. I used a hacked version of sleuth example to make the data easy to see.\n\nBefore:\n\nAfter:\n\njson:\njson\n[\n  {\n    \"traceId\": \"7928c4337cd28f25\",\n    \"parentId\": \"4819555aa4064ebd\",\n    \"id\": \"61167635a5f59253\",\n    \"name\": \"a\",\n    \"timestamp\": 1543996836036030,\n    \"duration\": 9,\n    \"localEndpoint\": {\n      \"serviceName\": \"backend\",\n      \"ipv4\": \"10.129.173.22\"\n    }\n  },\n  {\n    \"traceId\": \"7928c4337cd28f25\",\n    \"parentId\": \"6415f1af8ceb82d9\",\n    \"id\": \"b125c341ea061052\",\n    \"name\": \"a\",\n    \"timestamp\": 1543996836036034,\n    \"duration\": 12,\n    \"localEndpoint\": {\n      \"serviceName\": \"backend\",\n      \"ipv4\": \"10.129.173.22\"\n    }\n  },\n  {\n    \"traceId\": \"7928c4337cd28f25\",\n    \"parentId\": \"4819555aa4064ebd\",\n    \"id\": \"c667a2ebb9bef8c7\",\n    \"name\": \"b\",\n    \"timestamp\": 1543996836036117,\n    \"duration\": 11,\n    \"localEndpoint\": {\n      \"serviceName\": \"backend\",\n      \"ipv4\": \"10.129.173.22\"\n    }\n  },\n  {\n    \"traceId\": \"7928c4337cd28f25\",\n    \"parentId\": \"6415f1af8ceb82d9\",\n    \"id\": \"c82918163d51af85\",\n    \"name\": \"b\",\n    \"timestamp\": 1543996836036130,\n    \"duration\": 12,\n    \"localEndpoint\": {\n      \"serviceName\": \"backend\",\n      \"ipv4\": \"10.129.173.22\"\n    }\n  },\n  {\n    \"traceId\": \"7928c4337cd28f25\",\n    \"parentId\": \"4819555aa4064ebd\",\n    \"id\": \"7ac1f5beebf7756a\",\n    \"name\": \"c\",\n    \"timestamp\": 1543996836036187,\n    \"duration\": 12,\n    \"localEndpoint\": {\n      \"serviceName\": \"backend\",\n      \"ipv4\": \"10.129.173.22\"\n    }\n  },\n  {\n    \"traceId\": \"7928c4337cd28f25\",\n    \"parentId\": \"6415f1af8ceb82d9\",\n    \"id\": \"89ed8260bd54cfca\",\n    \"name\": \"c\",\n    \"timestamp\": 1543996836036201,\n    \"duration\": 11,\n    \"localEndpoint\": {\n      \"serviceName\": \"backend\",\n      \"ipv4\": \"10.129.173.22\"\n    }\n  },\n  {\n    \"traceId\": \"7928c4337cd28f25\",\n    \"parentId\": \"d19e851964e0c8f8\",\n    \"id\": \"4819555aa4064ebd\",\n    \"name\": \"child-of-backend\",\n    \"timestamp\": 1543996836036000,\n    \"duration\": 241,\n    \"localEndpoint\": {\n      \"serviceName\": \"backend\",\n      \"ipv4\": \"10.129.173.22\"\n    }\n  },\n  {\n    \"traceId\": \"7928c4337cd28f25\",\n    \"parentId\": \"fe9ae3ca755aea02\",\n    \"id\": \"6415f1af8ceb82d9\",\n    \"name\": \"child-of-backend\",\n    \"timestamp\": 1543996836036000,\n    \"duration\": 249,\n    \"localEndpoint\": {\n      \"serviceName\": \"backend\",\n      \"ipv4\": \"10.129.173.22\"\n    }\n  },\n  {\n    \"traceId\": \"7928c4337cd28f25\",\n    \"parentId\": \"7928c4337cd28f25\",\n    \"id\": \"d19e851964e0c8f8\",\n    \"kind\": \"SERVER\",\n    \"name\": \"get /api\",\n    \"timestamp\": 1543996836035120,\n    \"duration\": 1884,\n    \"localEndpoint\": {\n      \"serviceName\": \"backend\",\n      \"ipv4\": \"10.129.173.22\"\n    },\n    \"remoteEndpoint\": {\n      \"ipv4\": \"127.0.0.1\",\n      \"port\": 49570\n    },\n    \"tags\": {\n      \"http.method\": \"GET\",\n      \"http.path\": \"/api\",\n      \"mvc.controller.class\": \"Backend\",\n      \"mvc.controller.method\": \"printDate\"\n    },\n    \"shared\": true\n  },\n  {\n    \"traceId\": \"7928c4337cd28f25\",\n    \"parentId\": \"7928c4337cd28f25\",\n    \"id\": \"fe9ae3ca755aea02\",\n    \"kind\": \"SERVER\",\n    \"name\": \"get /api\",\n    \"timestamp\": 1543996836036050,\n    \"duration\": 1833,\n    \"localEndpoint\": {\n      \"serviceName\": \"backend\",\n      \"ipv4\": \"10.129.173.22\"\n    },\n    \"remoteEndpoint\": {\n      \"ipv4\": \"127.0.0.1\",\n      \"port\": 49569\n    },\n    \"tags\": {\n      \"http.method\": \"GET\",\n      \"http.path\": \"/api\",\n      \"mvc.controller.class\": \"Backend\",\n      \"mvc.controller.method\": \"printDate\"\n    },\n    \"shared\": true\n  },\n  {\n    \"traceId\": \"7928c4337cd28f25\",\n    \"parentId\": \"7928c4337cd28f25\",\n    \"id\": \"fe9ae3ca755aea02\",\n    \"kind\": \"CLIENT\",\n    \"name\": \"get\",\n    \"timestamp\": 1543996836034467,\n    \"duration\": 3058,\n    \"localEndpoint\": {\n      \"serviceName\": \"frontend\",\n      \"ipv4\": \"10.129.173.22\"\n    },\n    \"tags\": {\n      \"http.method\": \"GET\",\n      \"http.path\": \"/api\"\n    }\n  },\n  {\n    \"traceId\": \"7928c4337cd28f25\",\n    \"parentId\": \"7928c4337cd28f25\",\n    \"id\": \"d19e851964e0c8f8\",\n    \"kind\": \"CLIENT\",\n    \"name\": \"get\",\n    \"timestamp\": 1543996836033787,\n    \"duration\": 3729,\n    \"localEndpoint\": {\n      \"serviceName\": \"frontend\",\n      \"ipv4\": \"10.129.173.22\"\n    },\n    \"tags\": {\n      \"http.method\": \"GET\",\n      \"http.path\": \"/api\"\n    }\n  },\n  {\n    \"traceId\": \"7928c4337cd28f25\",\n    \"id\": \"7928c4337cd28f25\",\n    \"kind\": \"SERVER\",\n    \"name\": \"get /\",\n    \"timestamp\": 1543996836033060,\n    \"duration\": 6898,\n    \"localEndpoint\": {\n      \"serviceName\": \"frontend\",\n      \"ipv4\": \"10.129.173.22\"\n    },\n    \"remoteEndpoint\": {\n      \"ipv4\": \"127.0.0.1\",\n      \"port\": 49568\n    },\n    \"tags\": {\n      \"http.method\": \"GET\",\n      \"http.path\": \"/\",\n      \"mvc.controller.class\": \"Frontend\",\n      \"mvc.controller.method\": \"callBackend\"\n    }\n  }\n]. thanks @llinder @huydx and @tacigar for patience and help with this!. thanks @llinder @huydx and @tacigar for patience and help with this!. @redsunsoft so mainly were hoping for a code review in case there are some ways to improve perf. There is a lag loading that trace, it doesn't snap, but larger traces can be slower. It is all subjective I guess. I know larger traces are problematic (approaching 10k spans), but we don't have one here.\nAt the moment, the new UI is under zipkin-lens, so probably feedback there will end up with highest value.\nAt any rate an example setup for benchmarking added to help track down perf concerns would be good if you have integration ideas and/or instructions.. thanks @redsunsoft and happy holidays!. thanks much!. Thank you very much @JoveYu and @wu-sheng \nI assume right now the page is broken for chinese, so I will cut a release. Thank you very much @JoveYu and @wu-sheng \nI assume right now the page is broken for chinese, so I will cut a release. all good. will merge on green which means people can try snapshot build. all good. will merge on green which means people can try snapshot build. cc @ScienJus. cc @ScienJus. wow you dont waste time ;). wow you dont waste time ;). I will close in a sec.. just waiting for latest docker build and will\nverify manually :D\n. oops I think I released docker too fast :) few more minutes. \n. sorry this is a java 6+ library so we cant use list.sort in the main source\ntree.\nnice thought though!\nOn Wed, Dec 12, 2018, 2:57 PM Igor Suhorukov <notifications@github.com\nwrote:\n\n\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/openzipkin/zipkin/pull/2322\nCommit Summary\n\nCollections.sort replace with more object oriented .sort() approach\n\nFile Changes\n\nM zipkin/src/main/java/zipkin2/internal/Trace.java\n   https://github.com/openzipkin/zipkin/pull/2322/files#diff-0 (3)\nM zipkin/src/main/java/zipkin2/storage/InMemoryStorage.java\n   https://github.com/openzipkin/zipkin/pull/2322/files#diff-1 (2)\nM zipkin/src/test/java/zipkin2/internal/TraceTest.java\n   https://github.com/openzipkin/zipkin/pull/2322/files#diff-2 (3)\nM zipkin/src/test/java/zipkin2/storage/ITSpanStore.java\n   https://github.com/openzipkin/zipkin/pull/2322/files#diff-3 (4)\n\nPatch Links:\n\nhttps://github.com/openzipkin/zipkin/pull/2322.patch\nhttps://github.com/openzipkin/zipkin/pull/2322.diff\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2322, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61zyMS-9gMwy5MVjFTcIiiGbvt0dtks5u4KjhgaJpZM4ZO10x\n.\n. sorry this is a java 6+ library so we cant use list.sort in the main source\ntree.\n\nnice thought though!\nOn Wed, Dec 12, 2018, 2:57 PM Igor Suhorukov <notifications@github.com\nwrote:\n\n\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/openzipkin/zipkin/pull/2322\nCommit Summary\n\nCollections.sort replace with more object oriented .sort() approach\n\nFile Changes\n\nM zipkin/src/main/java/zipkin2/internal/Trace.java\n   https://github.com/openzipkin/zipkin/pull/2322/files#diff-0 (3)\nM zipkin/src/main/java/zipkin2/storage/InMemoryStorage.java\n   https://github.com/openzipkin/zipkin/pull/2322/files#diff-1 (2)\nM zipkin/src/test/java/zipkin2/internal/TraceTest.java\n   https://github.com/openzipkin/zipkin/pull/2322/files#diff-2 (3)\nM zipkin/src/test/java/zipkin2/storage/ITSpanStore.java\n   https://github.com/openzipkin/zipkin/pull/2322/files#diff-3 (4)\n\nPatch Links:\n\nhttps://github.com/openzipkin/zipkin/pull/2322.patch\nhttps://github.com/openzipkin/zipkin/pull/2322.diff\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2322, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61zyMS-9gMwy5MVjFTcIiiGbvt0dtks5u4KjhgaJpZM4ZO10x\n.\n. Not sure you were thinking about this.. we don't document our rationale for all things, but this will actually cause our core library to be less performant. This library is used in the main path of applications, so overhead in terms of cpu and allocation is important. For this reason, we prefer to use non-allocating form of iteration across a list. List.get(index) for does not require allocation of an iterator object.\n\nIn other words: for (int...) vs forEach, the forEach part implicitly new's up an Iterator object even if you don't see it happening in the code.\nWhile not all the code you affected here has a benchmark on it that includes data iterated, running one would show up. Let me know if you'd like more info!. Not sure you were thinking about this.. we don't document our rationale for all things, but this will actually cause our core library to be less performant. This library is used in the main path of applications, so overhead in terms of cpu and allocation is important. For this reason, we prefer to use non-allocating form of iteration across a list. List.get(index) for does not require allocation of an iterator object.\nIn other words: for (int...) vs forEach, the forEach part implicitly new's up an Iterator object even if you don't see it happening in the code.\nWhile not all the code you affected here has a benchmark on it that includes data iterated, running one would show up. Let me know if you'd like more info!. If I weren't already married, I would marry your search component. If I weren't already married, I would marry your search component. I think he meant to only allow this feature if you clicked star on zipkin :D\nOn Fri, Dec 14, 2018 at 4:43 AM tacigar notifications@github.com wrote:\n\n@drolando https://github.com/drolando\nThank you for feedback!\nYou mean such behavior, right?\n[image: default-open]\nhttps://user-images.githubusercontent.com/19551419/49966241-cd3cec80-ff62-11e8-9d5c-a49db9618dfd.gif\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2325#issuecomment-447113957,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xlNK4wPI33ivwEXokqtsQPTRPdoks5u4rvigaJpZM4ZRCWB\n.\n. I like this, but I am tempted to type. a big whitespace makes me think this is a search form. If I click into this space, I accidentally create a new search control. it is easy to accidentally make many, and backspace/delete doesn't get rid of them. This might frustrate people who like to use keyboard.\n\n\n. also the result conditions might be better to stay on the right.. for example, max results. not sure if others will feel this way, but it seems odd to mix data conditions with filtering ones. one solution could be to not have the background white! white looks like a text box, so makes me want to type in it. I really like how much space we have for results, now. It is my favorite part. hehe \"Improve lookback design because I don't like it.\". hehe \"Improve lookback design because I don't like it.\". noticed this.. is there a missing file? or is this just me?\n[INFO] /Users/acole/oss/zipkin/zipkin-lens/src/components/Browser/Traces/index.test.js\n[INFO]   5:28  error  Unable to resolve path to module './InitialMessage'  import/no-unresolved\n[INFO] \n[INFO] \u2716 1 problem (1 error, 0 warnings)\n[INFO] \n[ERROR] (node:32732) [ESLINT_LEGACY_OBJECT_REST_SPREAD] DeprecationWarning: The 'parserOptions.ecmaFeatures.experimentalObjectRestSpread' option is deprecated. Use 'parserOptions.ecmaVersion' instead. (found in \"node_modules/eslint-config-airbnb-base/index.js\"). noticed this.. is there a missing file? or is this just me?\n[INFO] /Users/acole/oss/zipkin/zipkin-lens/src/components/Browser/Traces/index.test.js\n[INFO]   5:28  error  Unable to resolve path to module './InitialMessage'  import/no-unresolved\n[INFO] \n[INFO] \u2716 1 problem (1 error, 0 warnings)\n[INFO] \n[ERROR] (node:32732) [ESLINT_LEGACY_OBJECT_REST_SPREAD] DeprecationWarning: The 'parserOptions.ecmaFeatures.experimentalObjectRestSpread' option is deprecated. Use 'parserOptions.ecmaVersion' instead. (found in \"node_modules/eslint-config-airbnb-base/index.js\"). I like no border best. @tacigar can you rebase and force push the conflicts away :)\n. actually no problem I will help. I think I like this right now. One thing maybe not user friendly is the name \"annotationQuery\" and similarly the raw labels like \"serviceName\". I think eventually these should be satisified by i18n\nex \"annotationQuery\" in english label could be \"tag or annotation query\"\nOther thing is now it seems we have to click the double-down arrow to expand the trace inline. This might be ok as it hints where to go to go out or not. However, I found myself double-clicking the bar eventhough the jump down icon is intuitive..\n\n. IMHO minimum change probably is getting rid of the light grey framing of the condition buttons until we figure out something better.. I'm happy with this. I think there will be some further changes in style maybe about the colors used in top bar, but meanwhile we have a lot of code held hostage here. I think it is progress and we can let more feedback occur separately. That said up to @basvanbeek if he's good merge!. I'm happy with this. I think there will be some further changes in style maybe about the colors used in top bar, but meanwhile we have a lot of code held hostage here. I think it is progress and we can let more feedback occur separately. That said up to @basvanbeek if he's good merge!. the thing that is most visually noticeable to me about @basvanbeek suggestions (not to deflate the others) is removal of the shadow boxes. I have to say I really do prefer them removed clicking back and forth looking at both ways.. hah ps the expand collapse being reversed... I never noticed, but now I do :D. discussed here we think it is better to just merge this.. discussed here we think it is better to just merge this.. curios... maybe @trustin or @anuraaga would know.. is there a way to run both normal http and grpc's http on the same port? Currently, we use undertow underneath, but we could consider changing that.. >\n\nIf you use armeria :) Upstream gRPC doesn't support it yet I think, or\neven if it did it would only be HTTP/2, not HTTP/1.\n\n\nfrankly I think we should switch to armeria then. we have several\ncontributors who are also contributors to armeria. zipkin always tries to\nbe simplest possible setup and adding ports betrays this. if we are talking\nabout grpc being shaded anyway we can just make stackdriver use the shaded\none (or potentially convert its storage module also to armeria)\n\n. we dont promise any api compat for our server, so not worries. if we did\narmeria we would just add grpc endpoint directly to the zipkin-server imho\nand not mess with extra modules.\n\nOn Sun, Dec 16, 2018, 11:52 AM Anuraag Agrawal <notifications@github.com\nwrote:\n\nI guess the talking points would be that armeria still isn't GA and has\nmore dependencies. But agree that more ports is annoying and a big reason I\nlove armeria :)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2328#issuecomment-447615813,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD615g9RvQRBh3NmUZu5W5YK1v4W-b8ks5u5cORgaJpZM4ZU_jb\n.\n. also good point on the deps of armeria. similar audit would apply to ensure\nthings still work ;)\n\nFWIW current Cassandra driver supports a range of guava starting at 16\niirc. later it doesnt depend on guava. almost everything else has no\nconflicts due to okhttp directly. we would need to watch that zipkin-aws\nand zipkin-gcp extensions dont conflict.\nOn Sun, Dec 16, 2018, 11:59 AM Adrian Cole <adrian.f.cole@gmail.com wrote:\n\nwe dont promise any api compat for our server, so not worries. if we did\narmeria we would just add grpc endpoint directly to the zipkin-server imho\nand not mess with extra modules.\nOn Sun, Dec 16, 2018, 11:52 AM Anuraag Agrawal <notifications@github.com\nwrote:\n\nI guess the talking points would be that armeria still isn't GA and has\nmore dependencies. But agree that more ports is annoying and a big reason I\nlove armeria :)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2328#issuecomment-447615813,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD615g9RvQRBh3NmUZu5W5YK1v4W-b8ks5u5cORgaJpZM4ZU_jb\n.\n\n\n. to see differences more directly probably add it as a dep on zipkin-server\nthen build the exec jar. do a jar -tvf|sort before and after and diff\nthose. I suspect Jackson is already in the main exec jar even if we don't\nuse it (I think zipkin-aws does)\n\nOn Sun, Dec 16, 2018, 2:49 PM Eric Hauser <notifications@github.com wrote:\n\nThere are a few more dependencies with armeria:\n[INFO] +- com.linecorp.armeria:armeria-grpc-shaded:jar:0.75.0:compile[INFO] |  +- com.linecorp.armeria:armeria-shaded:jar:0.75.0:compile[INFO] |  |  +- com.fasterxml.jackson.core:jackson-annotations:jar:2.9.7:compile[INFO] |  |  +- com.fasterxml.jackson.core:jackson-core:jar:2.9.7:compile[INFO] |  |  +- com.fasterxml.jackson.core:jackson-databind:jar:2.9.7:compile[INFO] |  |  +- io.micrometer:micrometer-core:jar:1.1.0:compile[INFO] |  |  |  +- org.hdrhistogram:HdrHistogram:jar:2.1.9:compile[INFO] |  |  |  - org.latencyutils:LatencyUtils:jar:2.0.3:compile[INFO] |  |  +- io.netty:netty-codec-haproxy:jar:4.1.31.Final:compile[INFO] |  |  |  - io.netty:netty-codec:jar:4.1.31.Final:compile[INFO] |  |  +- io.netty:netty-codec-http2:jar:4.1.31.Final:compile[INFO] |  |  |  +- io.netty:netty-codec-http:jar:4.1.31.Final:compile[INFO] |  |  |  - io.netty:netty-handler:jar:4.1.31.Final:compile[INFO] |  |  +- io.netty:netty-resolver-dns:jar:4.1.31.Final:compile[INFO] |  |  |  +- io.netty:netty-resolver:jar:4.1.31.Final:compile[INFO] |  |  |  - io.netty:netty-codec-dns:jar:4.1.31.Final:compile[INFO] |  |  +- io.netty:netty-tcnative-boringssl-static:jar:2.0.19.Final:compile[INFO] |  |  +- io.netty:netty-transport:jar:4.1.31.Final:compile[INFO] |  |  |  - io.netty:netty-buffer:jar:4.1.31.Final:compile[INFO] |  |  +- io.netty:netty-transport-native-epoll:jar:linux-x86_64:4.1.31.Final:compile[INFO] |  |  |  +- io.netty:netty-common:jar:4.1.31.Final:compile[INFO] |  |  |  - io.netty:netty-transport-native-unix-common:jar:4.1.31.Final:compile[INFO] |  |  +- io.netty:netty-transport-native-unix-common:jar:linux-x86_64:4.1.31.Final:compile[INFO] |  |  - org.reactivestreams:reactive-streams:jar:1.0.2:compile[INFO] |  +- com.google.code.findbugs:jsr305:jar:3.0.2:compile[INFO] |  +- org.curioswitch.curiostack:protobuf-jackson:jar:0.3.0:compile[INFO] |  |  +- javax.annotation:javax.annotation-api:jar:1.3.1:runtime[INFO] |  |  +- com.google.protobuf:protobuf-java-util:jar:3.6.1:runtime\nNeither netty or jackson being shaded is less than ideal. Appears that\narmeria is choosing not to shade netty in the future - line/armeria#705\nhttps://github.com/line/armeria/issues/705.\nOn the plus side, I believe you can run HTTP/1, gRPC, and gRPC-Web all\nfrom the same server instance. GRPC-Web only supports unary RPC at present,\nso would need to provide a unary method for sending spans as well (or\nalternatively drop streaming support).\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2328#issuecomment-447621700,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD619eocVJfmM50scArKKnoLrx0600Zks5u5ez9gaJpZM4ZU_jb\n.\n. to see differences more directly probably add it as a dep on zipkin-server\nthen build the exec jar. do a jar -tvf|sort before and after and diff\nthose. I suspect Jackson is already in the main exec jar even if we don't\nuse it (I think zipkin-aws does)\n\nOn Sun, Dec 16, 2018, 2:49 PM Eric Hauser <notifications@github.com wrote:\n\nThere are a few more dependencies with armeria:\n[INFO] +- com.linecorp.armeria:armeria-grpc-shaded:jar:0.75.0:compile[INFO] |  +- com.linecorp.armeria:armeria-shaded:jar:0.75.0:compile[INFO] |  |  +- com.fasterxml.jackson.core:jackson-annotations:jar:2.9.7:compile[INFO] |  |  +- com.fasterxml.jackson.core:jackson-core:jar:2.9.7:compile[INFO] |  |  +- com.fasterxml.jackson.core:jackson-databind:jar:2.9.7:compile[INFO] |  |  +- io.micrometer:micrometer-core:jar:1.1.0:compile[INFO] |  |  |  +- org.hdrhistogram:HdrHistogram:jar:2.1.9:compile[INFO] |  |  |  - org.latencyutils:LatencyUtils:jar:2.0.3:compile[INFO] |  |  +- io.netty:netty-codec-haproxy:jar:4.1.31.Final:compile[INFO] |  |  |  - io.netty:netty-codec:jar:4.1.31.Final:compile[INFO] |  |  +- io.netty:netty-codec-http2:jar:4.1.31.Final:compile[INFO] |  |  |  +- io.netty:netty-codec-http:jar:4.1.31.Final:compile[INFO] |  |  |  - io.netty:netty-handler:jar:4.1.31.Final:compile[INFO] |  |  +- io.netty:netty-resolver-dns:jar:4.1.31.Final:compile[INFO] |  |  |  +- io.netty:netty-resolver:jar:4.1.31.Final:compile[INFO] |  |  |  - io.netty:netty-codec-dns:jar:4.1.31.Final:compile[INFO] |  |  +- io.netty:netty-tcnative-boringssl-static:jar:2.0.19.Final:compile[INFO] |  |  +- io.netty:netty-transport:jar:4.1.31.Final:compile[INFO] |  |  |  - io.netty:netty-buffer:jar:4.1.31.Final:compile[INFO] |  |  +- io.netty:netty-transport-native-epoll:jar:linux-x86_64:4.1.31.Final:compile[INFO] |  |  |  +- io.netty:netty-common:jar:4.1.31.Final:compile[INFO] |  |  |  - io.netty:netty-transport-native-unix-common:jar:4.1.31.Final:compile[INFO] |  |  +- io.netty:netty-transport-native-unix-common:jar:linux-x86_64:4.1.31.Final:compile[INFO] |  |  - org.reactivestreams:reactive-streams:jar:1.0.2:compile[INFO] |  +- com.google.code.findbugs:jsr305:jar:3.0.2:compile[INFO] |  +- org.curioswitch.curiostack:protobuf-jackson:jar:0.3.0:compile[INFO] |  |  +- javax.annotation:javax.annotation-api:jar:1.3.1:runtime[INFO] |  |  +- com.google.protobuf:protobuf-java-util:jar:3.6.1:runtime\nNeither netty or jackson being shaded is less than ideal. Appears that\narmeria is choosing not to shade netty in the future - line/armeria#705\nhttps://github.com/line/armeria/issues/705.\nOn the plus side, I believe you can run HTTP/1, gRPC, and gRPC-Web all\nfrom the same server instance. GRPC-Web only supports unary RPC at present,\nso would need to provide a unary method for sending spans as well (or\nalternatively drop streaming support).\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2328#issuecomment-447621700,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD619eocVJfmM50scArKKnoLrx0600Zks5u5ez9gaJpZM4ZU_jb\n.\n. yes armeria would imply geting rid of undertow and its deps. undertow was\nonly added for better perf.\n\nplan sounds fine, but I'm really not keen merging another port to expose.\nmeaning if helpful for development can separate, but please don't release\nus needing to support a second http port perpetually unless the change to\nundo that immediately follows.\nOn Mon, Dec 17, 2018 at 6:28 AM Eric Hauser notifications@github.com\nwrote:\n\nSo, @anuraaga https://github.com/anuraaga's trick doesn't work - at\nleast with gRPC Java. The issue is that the marshaller will have already\ncopied the input stream into a ByteString prior to getting access to it.\nSo I took the approach of implementing a custom BindableService. This has\nthe added benefit of removing all dependencies on Protocol Buffers\nentirely. Here are the dependencies introduced now:\n[INFO] +- io.grpc:grpc-netty-shaded:jar:1.17.1:runtime\n[INFO] |  - io.grpc:grpc-core:jar:1.17.1:compile (version selected from constraint [1.17.1,1.17.1])\n[INFO] |     +- io.grpc:grpc-context:jar:1.17.1:compile\n[INFO] |     +- com.google.code.gson:gson:jar:2.7:compile\n[INFO] |     +- com.google.errorprone:error_prone_annotations:jar:2.2.0:compile\n[INFO] |     +- com.google.code.findbugs:jsr305:jar:3.0.2:compile\n[INFO] |     +- org.codehaus.mojo:animal-sniffer-annotations:jar:1.17:compile\n[INFO] |     +- com.google.guava:guava:jar:26.0-android:compile\n[INFO] |     |  +- org.checkerframework:checker-compat-qual:jar:2.5.2:compile\n[INFO] |     |  - com.google.j2objc:j2objc-annotations:jar:1.1:compile\n[INFO] |     +- io.opencensus:opencensus-api:jar:0.17.0:compile\n[INFO] |     - io.opencensus:opencensus-contrib-grpc-metrics:jar:0.17.0:compile\n[INFO] +- io.grpc:grpc-stub:jar:1.17.1:compile\nSo this ends up being much less of a dependency burden than armeria - no\nnetty, protobufs -- with the downside of having the collector run on a\ndifferent port. We can keep this version of gRPC in sync with the version\nthat is being used by zipkin-gcp to avoid any issues.\nHowever, armeria does have the advantage of everything being on the same\nport and it would support gRPC-Web out of the box without a proxy. Would\nswitching to armeria mean that it would eliminate undertow and just use\nit for everything? If so, we shoud probably find a path to finalizing the\ngRPC API in this PR and moving forward with this experimental version on a\ndifferent port. A separate issue could be opened for moving to armeria -\ngiven the size of that change, I would want to make sure there was plenty\nof support for it.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2328#issuecomment-447682189,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xBtWXssSyhRFKTCIDIxNHiQjwLtks5u5skggaJpZM4ZU_jb\n.\n. yes armeria would imply geting rid of undertow and its deps. undertow was\nonly added for better perf.\n\nplan sounds fine, but I'm really not keen merging another port to expose.\nmeaning if helpful for development can separate, but please don't release\nus needing to support a second http port perpetually unless the change to\nundo that immediately follows.\nOn Mon, Dec 17, 2018 at 6:28 AM Eric Hauser notifications@github.com\nwrote:\n\nSo, @anuraaga https://github.com/anuraaga's trick doesn't work - at\nleast with gRPC Java. The issue is that the marshaller will have already\ncopied the input stream into a ByteString prior to getting access to it.\nSo I took the approach of implementing a custom BindableService. This has\nthe added benefit of removing all dependencies on Protocol Buffers\nentirely. Here are the dependencies introduced now:\n[INFO] +- io.grpc:grpc-netty-shaded:jar:1.17.1:runtime\n[INFO] |  - io.grpc:grpc-core:jar:1.17.1:compile (version selected from constraint [1.17.1,1.17.1])\n[INFO] |     +- io.grpc:grpc-context:jar:1.17.1:compile\n[INFO] |     +- com.google.code.gson:gson:jar:2.7:compile\n[INFO] |     +- com.google.errorprone:error_prone_annotations:jar:2.2.0:compile\n[INFO] |     +- com.google.code.findbugs:jsr305:jar:3.0.2:compile\n[INFO] |     +- org.codehaus.mojo:animal-sniffer-annotations:jar:1.17:compile\n[INFO] |     +- com.google.guava:guava:jar:26.0-android:compile\n[INFO] |     |  +- org.checkerframework:checker-compat-qual:jar:2.5.2:compile\n[INFO] |     |  - com.google.j2objc:j2objc-annotations:jar:1.1:compile\n[INFO] |     +- io.opencensus:opencensus-api:jar:0.17.0:compile\n[INFO] |     - io.opencensus:opencensus-contrib-grpc-metrics:jar:0.17.0:compile\n[INFO] +- io.grpc:grpc-stub:jar:1.17.1:compile\nSo this ends up being much less of a dependency burden than armeria - no\nnetty, protobufs -- with the downside of having the collector run on a\ndifferent port. We can keep this version of gRPC in sync with the version\nthat is being used by zipkin-gcp to avoid any issues.\nHowever, armeria does have the advantage of everything being on the same\nport and it would support gRPC-Web out of the box without a proxy. Would\nswitching to armeria mean that it would eliminate undertow and just use\nit for everything? If so, we shoud probably find a path to finalizing the\ngRPC API in this PR and moving forward with this experimental version on a\ndifferent port. A separate issue could be opened for moving to armeria -\ngiven the size of that change, I would want to make sure there was plenty\nof support for it.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2328#issuecomment-447682189,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xBtWXssSyhRFKTCIDIxNHiQjwLtks5u5skggaJpZM4ZU_jb\n.\n. PS it isn't fair to put the burden of switching all to armeria on Eric :P I\nwill spike a PR so in case this happens while I'm away (I'm out tomorrow)\nthe grunt work is ready.\n. PS it isn't fair to put the burden of switching all to armeria on Eric :P I\nwill spike a PR so in case this happens while I'm away (I'm out tomorrow)\nthe grunt work is ready.\n. > I started this whole mess so I'm fine rolling up my sleeves on the switch. Just want to make sure there is support behind it.\n\nI think there's some sort of \"jury's still out\" in general.\nThe trick is that adding an optional module will likely become\neffectively required as there is so much marketing buzz around gRPC.\nSo, we have to be quite careful how to introduce this. scribe being on\na different port caused a bunch of pain back in the day with people\ninvariably choosing the wrong port, having to go through work to\nfigure it out (matters not if docs say use the right port), then going\nonto support and moving the concern there, or triple posted issues\netc.\nBasically, if we're going to do this, in the case of gRPC, I think we\nought to consider this becoming effectively required, and how to stop\nthe most type of problems we can't control. We do have a lot of\ncontrol over the server and extensions of it as we explicitly give\nzero warranty on custom builds. We have little control over config\nmistakes even with docs.\n. > I started this whole mess so I'm fine rolling up my sleeves on the switch. Just want to make sure there is support behind it.\nI think there's some sort of \"jury's still out\" in general.\nThe trick is that adding an optional module will likely become\neffectively required as there is so much marketing buzz around gRPC.\nSo, we have to be quite careful how to introduce this. scribe being on\na different port caused a bunch of pain back in the day with people\ninvariably choosing the wrong port, having to go through work to\nfigure it out (matters not if docs say use the right port), then going\nonto support and moving the concern there, or triple posted issues\netc.\nBasically, if we're going to do this, in the case of gRPC, I think we\nought to consider this becoming effectively required, and how to stop\nthe most type of problems we can't control. We do have a lot of\ncontrol over the server and extensions of it as we explicitly give\nzero warranty on custom builds. We have little control over config\nmistakes even with docs.\n. ps I'll graciously accept your offer to do the armeria spike if comes\nto that. I'll raise an issue on it.\n. ps I'll graciously accept your offer to do the armeria spike if comes\nto that. I'll raise an issue on it.\n. here's the issue for armeria https://github.com/openzipkin/zipkin/issues/2331. here's the issue for armeria https://github.com/openzipkin/zipkin/issues/2331. I was just looking at the proto. It seems probably unary ListOfSpans style request would make more sense as the initial impl as all infrastructure we have is based on ListOfSpans (Collector, Storage etc). It is nice that all of our impls accept literally the same messages. For example, if our initial method was putSpans(ListOfSpans) then the exact codec we use for normal proto over http/2 could be used to process that message, right?\nStarting with just a unary likely also means the largest amount of client support in general. Does that sound right?. I was just looking at the proto. It seems probably unary ListOfSpans style request would make more sense as the initial impl as all infrastructure we have is based on ListOfSpans (Collector, Storage etc). It is nice that all of our impls accept literally the same messages. For example, if our initial method was putSpans(ListOfSpans) then the exact codec we use for normal proto over http/2 could be used to process that message, right?\nStarting with just a unary likely also means the largest amount of client support in general. Does that sound right?. @anuraaga I don't expect many clients to not do bundling at all as that implies a different internal reporting arch only used for grpc, wouldn't it?\nFor example, even kafka is a bundle-at-a-time transport. This was done for reasons including that some spans are so small that sending in a single message is wasteful. Also, often people mistakenly assume that sending one span onto a transport is protecting the app from overhead when usually even kafka clients can (and have) crashed an app enqueueing.\nI wonder if we can decouple this completely as having two supported ways to do grpc seems tech debt creating not just here but the inevitable \"it is streaming on server, so why not also in X (where X is every client library)\". I really hesitate strongly about adding more complexity especially in the initial impl as there's already quite a lot of work to do just for unary.\nAssuming unary is in, it would seem possible for someone who felt strongly about streaming to add a module to their setup that is a streaming alternate as well.. meaning we can completely defer this whole thing as well the impact discussions which now only stall the work entirely. What I mean is bundling the option for streaming means supporting it and has almost the same impact as supporting it enabled by default. I don't quite feel we should add that burden today considering we still have a lot of other things to do to get this in.. @anuraaga PS I like the idea of re-using http/2 flow-control (and also ping for that matter) in theory, and will also help with whatever infra is needed when we experiment with streaming.. fyi I didn't knock out all the kinks here but here's a work in progress for direct armeria integration https://github.com/openzipkin/zipkin/pull/2348. this can be now be rebased in master, which is armeria, avoiding a second http engine. I will start on this next. here is a start https://github.com/openzipkin/zipkin/pull/2348. @tacigar @zeagord @shakuzen PTAL I suspect we can merge this on green. @tacigar @zeagord @shakuzen PTAL I suspect we can merge this on green. this still needs an auto-complete tag write deduplicator similar to what's in cassandra. made some naming adjustments mostly internal code, but also \"tagkey\" json field name to \"tagKey\" @zeagord PTAL! I plan to merge this independent of deduplication code.. tested with\nbash\n$ STORAGE_TYPE=mysql MYSQL_USER=root AUTOCOMPLETE_KEYS=environment  java -jar ./zipkin-server/target/zipkin-server-*exec.jar\nthen, used curl\nbash\n$ curl -s localhost:9411/api/v2/autocompleteKeys\n[\"environment\"]. personally I would not put arbitrary heading like Tag Key. Instead make a drop-down per key, or auto-complete in the annotation query section (with tabs such as haystack do). We aren't expecting more than several site specific tags.. personally I would not put arbitrary heading like Tag Key. Instead make a drop-down per key, or auto-complete in the annotation query section (with tabs such as haystack do). We aren't expecting more than several site specific tags.. also we should mention that redesign in the zipkin-ui code is optional. we'd expect to land lens in a month.. given that, a conservative approach like auto-complete inside the annotationQuery section can make sense (if not hard to impl). also we should mention that redesign in the zipkin-ui code is optional. we'd expect to land lens in a month.. given that, a conservative approach like auto-complete inside the annotationQuery section can make sense (if not hard to impl). closing this as we should let this happen in lens. closing this as we should let this happen in lens. I think this is mostly a duplicate of https://github.com/openzipkin/zipkin/issues/1794 as it applies to everything.. ps I was on holiday when you replied. one thing about your specific question is basically you want something different. you want to conditionally index span names depending on the type of the span. putting a path in any type of span is incorrect. the default choice should be low cardinality worst case http method. you can always set a tag for the path. setting high cardinality for a name will bust other things and I don't expect us to conditionally try to control span name indexing.\nin other words you are asking to conditionally control span name, not service name indexing. correct me if I am wrong.. ps I was on holiday when you replied. one thing about your specific question is basically you want something different. you want to conditionally index span names depending on the type of the span. putting a path in any type of span is incorrect. the default choice should be low cardinality worst case http method. you can always set a tag for the path. setting high cardinality for a name will bust other things and I don't expect us to conditionally try to control span name indexing.\nin other words you are asking to conditionally control span name, not service name indexing. correct me if I am wrong.. closest to what I think we need here is actually to clean up data in general. For example, if the client span name is high cardinality delete it, as instrumentation should not add high cardinality names. There are a few projects with data cleaning aims. otherwise what this ends up as is quirky hooks that might work on cassandra as we manually index, but not on elasticsearch where we don't manually index the span names.. closest to what I think we need here is actually to clean up data in general. For example, if the client span name is high cardinality delete it, as instrumentation should not add high cardinality names. There are a few projects with data cleaning aims. otherwise what this ends up as is quirky hooks that might work on cassandra as we manually index, but not on elasticsearch where we don't manually index the span names.. we should look into the formatting here.. currently, we use okhttp DNS trick to round robin across the multiple\nhosts. if you know an alternative way that works with mixed ports feel feel\nfree to propose!\n. I can take a second look to see if there's a way. https://github.com/square/okhttp/issues/4530 will solve this the right way. meanwhile I will take a quick look to see if any way now (hack). https://github.com/square/okhttp/issues/4530 will solve this the right way. meanwhile I will take a quick look to see if any way now (hack). we are currently using okhttp3.Dns which is InetAddress, not InetSocketAddress. Sorry we cannot support mixed ports until some other feature like loadbalancer happens or someone else is more clever.\ncc @swankjesse. we are currently using okhttp3.Dns which is InetAddress, not InetSocketAddress. Sorry we cannot support mixed ports until some other feature like loadbalancer happens or someone else is more clever.\ncc @swankjesse.    - I don't know why choose okhttp3 which's more suitable for mobile\n   development?\nWe chose okhttp because it is a clean, well designed library, that keeps\napi portability in mind. It is not exclusively targeted to mobile dev even\nif they care about android.\n.    - I don't know why choose okhttp3 which's more suitable for mobile\n   development?\nWe chose okhttp because it is a clean, well designed library, that keeps\napi portability in mind. It is not exclusively targeted to mobile dev even\nif they care about android.\n. @anuraaga @trustin @huydx this code looks up your alley if you are interested :P. cc also @llinder @michaelsembwever as I plan to use this to replace the existing stuff in cassandra from muting redundant requests. pending integration test of elasticsearch limiter. later we should choose a config property for this stuff.\nEx.\n* @param suppressionTtl milliseconds to obviate suppress calls to write the same service name\n     * @param suppressionMaxSize maximum service names to suppress at a time\nideally we have the same property for all autocomplete names, regardless of whether they are service, span or tag autocompletion. I think this would be ideal as it would be less config to present to people.. great feedback @anuraaga I like the code better now and also the parallel tests run much much faster when there's no powermock instrumentation. @zeagord @huydx ok I think this is ready to test. Only thing I have left is to write the README info on elasticsearch. ps I'll let the following idea sink in for a day or two. We need a property for this although only cassandra and elasticsearch will currently use it (mysql doesn't)\nzipkin.storage.autocomplete-ttl or STORAGE_AUTOCOMPLETE_TTL\n* How long in milliseconds to suppress calls to write the same autocomplete key/value pair. Default 3600000 (1 day)\nzipkin.storage.autocomplete-max or STORAGE_AUTOCOMPLETE_MAX\n* How many autocomplete key/value pairs to suppress at a time. Default 5000. note STORAGE_AUTOCOMPLETE_MAX (not thrilled with the name) is more about not crashing the server. we can have a number much higher than 5000. This will be the total cardinality of autocomplete tag values, bounded only to stop things from crashing if someone goofed cardinality.. I'm inclined to rename zipkin.storage.autocomplete-max to zipkin.storage.autocomplete-cardinality set it to a higher number like 20k and not expose an env variable for it.\nwe can add internal notes but I wouldn't expect users to modify this. we then draw attention to TTL which is the more important config.\nwdyt?. will merge on green. thanks all!. thanks. can you click json button for this trace and post it here?\nOn Fri, Jan 4, 2019 at 7:06 AM Aaron Stainback notifications@github.com\nwrote:\n\nThe 2.10 docker image works fine, 2..11 or newer breaks nesting and also\nthe service graph.\nI'm running the following\n2.10 - docker run --rm -it -p 9411:9411 openzipkin/zipkin:2.10\n2.11 - docker run --rm -it -p 9411:9411 openzipkin/zipkin:2.11\n2.10 - Working\n[image: 2 10]\nhttps://user-images.githubusercontent.com/578953/50666042-088a7400-0f81-11e9-83ef-ee838219a24d.jpg\n2.11 - Broken Nesting\n[image: 2 11]\nhttps://user-images.githubusercontent.com/578953/50666045-0a543780-0f81-11e9-9e41-7c25a124ac2f.jpg\nThe following code is using .NET core 2.1\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.Extensions.Logging;\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing zipkin4net;\nusing zipkin4net.Middleware;\nusing zipkin4net.Tracers.Zipkin;\nusing zipkin4net.Transport.Http;\nnamespace TracingTest\n{\n    class Program\n    {\n        static readonly DateTime _startDateTime;\n        static readonly long _startTicks;\n        static Program()\n        {\n            _startDateTime = DateTime.UtcNow;\n            _startTicks = System.Diagnostics.Stopwatch.GetTimestamp();\n        }\n    static async Task Main(string[] args)\n    {\n        using (var services = new ServiceCollection()\n            .AddLogging(builder =>\n            {\n                builder\n                .AddConsole()\n                .AddDebug();\n            })\n            .BuildServiceProvider()\n        )\n        {\n            TraceManager.SamplingRate = 1;\n            var loggerFactory = services.GetRequiredService<ILoggerFactory>();\n            var logger = new TracingLogger(loggerFactory, \"Tracing\");\n            var sender = new HttpZipkinSender(\"http://localhost:9411\", \"application/json\");\n            var serializer = new JSONSpanSerializer();\n            var tracer = new ZipkinTracer(sender, serializer);\n            TraceManager.RegisterTracer(tracer);\n            TraceManager.Start(logger);\n\n            // Timings\n            var uiStart = UtcNow;\n            var gatewayStart = uiStart.AddMilliseconds(10);\n            var gatewayCallAuthStart = gatewayStart.AddMilliseconds(10);\n            var gatewayCallProfileStart = gatewayStart.AddMilliseconds(20);\n            var authStart = gatewayCallAuthStart.AddMilliseconds(10);\n            var authEnd = authStart.AddMilliseconds(500);\n            var gatewayCallAuthEnd = authEnd.AddMilliseconds(10);\n            var profileStart = gatewayCallProfileStart.AddMilliseconds(10);\n            var profileEnd = profileStart.AddMilliseconds(100);\n            var gatewayCallProfileEnd = profileEnd.AddMilliseconds(10);\n            var gatewayEnd = new DateTime(Math.Max(gatewayCallAuthEnd.Ticks, gatewayCallProfileEnd.Ticks), DateTimeKind.Utc).AddMilliseconds(10);\n            var uiEnd = gatewayEnd.AddMilliseconds(10);\n\n            // UI Call Gateway\n            var uiTrace = Trace.Create();\n            uiTrace.Record(Annotations.ServiceName(\"client\"));\n            uiTrace.Record(Annotations.Rpc(\"get-user\"));\n            uiTrace.Record(Annotations.ClientSend(), uiStart);\n            uiTrace.Record(Annotations.ClientRecv(), uiEnd);\n\n            // Gateway\n            var gatewayTrace = uiTrace.Child();\n            gatewayTrace.Record(Annotations.ServiceName(\"api-gateway\"));\n            gatewayTrace.Record(Annotations.Rpc(\"get-user\"));\n            gatewayTrace.Record(Annotations.ServerRecv(), gatewayStart);\n            gatewayTrace.Record(Annotations.ServerSend(), gatewayEnd);\n\n            // Gateway Call Auth\n            var gatewayCallAuthTrace = gatewayTrace.Child();\n            gatewayCallAuthTrace.Record(Annotations.ServiceName(\"auth-service\"));\n            gatewayCallAuthTrace.Record(Annotations.Rpc(\"get-permissions\"));\n            gatewayCallAuthTrace.Record(Annotations.ClientSend(), gatewayCallAuthStart);\n            gatewayCallAuthTrace.Record(Annotations.ClientRecv(), gatewayCallAuthEnd);\n\n            // Gateway Call Profile\n            var gatewayCallProfileTrace = gatewayTrace.Child();\n            gatewayCallProfileTrace.Record(Annotations.ServiceName(\"api-gateway\"));\n            gatewayCallProfileTrace.Record(Annotations.Rpc(\"get-profile\"));\n            gatewayCallProfileTrace.Record(Annotations.ClientSend(), gatewayCallProfileStart);\n            gatewayCallProfileTrace.Record(Annotations.ClientRecv(), gatewayCallProfileEnd);\n\n            // Auth Service\n            var authTrace = gatewayCallAuthTrace.Child();\n            authTrace.Record(Annotations.ServiceName(\"auth-service\"));\n            authTrace.Record(Annotations.Rpc(\"get-permissions\"));\n            authTrace.Record(Annotations.ServerRecv(), authStart);\n            authTrace.Record(Annotations.ServerSend(), authEnd);\n\n            // Profile Service\n            var profileTrace = gatewayCallProfileTrace.Child();\n            profileTrace.Record(Annotations.ServiceName(\"profile-service\"));\n            profileTrace.Record(Annotations.Rpc(\"get-profile\"));\n            profileTrace.Record(Annotations.ServerRecv(), profileStart);\n            profileTrace.Record(Annotations.ServerSend(), profileEnd);\n\n\n            // Wait for key to close program\n            Console.ReadKey(true);\n            TraceManager.Stop();\n            TraceManager.ClearTracers();\n        }\n    }\n\n    static DateTime UtcNow => _startDateTime.AddTicks(System.Diagnostics.Stopwatch.GetTimestamp() - _startTicks);\n}\n\n}\nSee issue on client for more info\nopenzipkin/zipkin4net#224\nhttps://github.com/openzipkin/zipkin4net/issues/224\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2342, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6134byyF9qFxpPDFKtAEAGgZccSK7ks5u_oztgaJpZM4Zo6Ty\n.\n. thanks for all the details\n. thanks for all the details\n. it looks like there is a data problem we didn't account for in the latest server code.\n\nthe shared flag means the server has the same span id as the client. here you can see the shared flag is incorrectly set as the client and server do not share a span id.\nthe library should be fixed to not set shared flag when not sharing an ID parsed from headers.\nmeanwhile can you test against latest snapshot in case we already accounted for this data problem server side since last release?\nhttps://oss.jfrog.org/artifactory/oss-snapshot-local/io/zipkin/java/zipkin-server/2.11.2-SNAPSHOT/zipkin-server-2.11.2-20180819.050659-6-exec.jar\n. it looks like there is a data problem we didn't account for in the latest server code.\nthe shared flag means the server has the same span id as the client. here you can see the shared flag is incorrectly set as the client and server do not share a span id.\nthe library should be fixed to not set shared flag when not sharing an ID parsed from headers.\nmeanwhile can you test against latest snapshot in case we already accounted for this data problem server side since last release?\nhttps://oss.jfrog.org/artifactory/oss-snapshot-local/io/zipkin/java/zipkin-server/2.11.2-SNAPSHOT/zipkin-server-2.11.2-20180819.050659-6-exec.jar\n. oops this is actually the correct latest snapshot\nlink zipkin-server-2.11.13-20190107.062025-14-exec.jar\n. oops this is actually the correct latest snapshot\nlink zipkin-server-2.11.13-20190107.062025-14-exec.jar\n. we had a number of onsite workshops in january which was both good and bad (bad as many occupied and distracted by travel) https://cwiki.apache.org/confluence/display/ZIPKIN/Workshops\non this now, tho! thanks for the ping. root (client) span 6f5f on service \"client\" lands on \"api-gateway\" as server span 76d5\n    client span c3f8 lands on \"profile-service\" as server span 3e19\n    client span f956 lands on \"auth-service\" as server span 5be7\nThe configuration bug is that client span f956 (\"get-permissions\") has the wrong local endpoint. It has accidentally put the remote service name as the local service name.\nIt should be the same as the server it was invoked on, not the server it is invoking. I suspect if you correct the config for \"get-permissions\" to set the correct local endpoint, to \"api-gateway\", not \"auth-service\" which is the remote service name, it will be fine.\nI checked this and it presents properly when the data is correct.. sorry this is won't fix. the instrumentation is sending incorrect data\nwhich is better fixed where the problem exists. The fact that 2.10\naccidentally worked is of no consequence here.\n. the data is and was incorrect. what was always broke was the data. please\nstop arguing as I have already taken time to debug this for you gratis\n. do what you need to do. if jaeger can present incorrect data better and\nthat is a priority for you over fixing the problem.  probably you should do\nthat.\n. the following type definitely should not be from classes:\nat zipkin2.elasticsearch.internal.client.HttpCall.execute(HttpCall.java:79) ~[classes/:?]\nex if it was the normal build it would come from the a jar like this:\nat zipkin2.elasticsearch.internal.client.HttpCall.execute(HttpCall.java:79) ~[zipkin-storage-elasticsearch-2.11.12.jar!/:?]\nPlease use the default server from https://github.com/openzipkin/zipkin#quick-start or if for some reason that doesn't work, normal maven central like https://search.maven.org/remotecontent?filepath=io/zipkin/java/zipkin-server/2.11.12/zipkin-server-2.11.12-exec.jar\nif you still have problems, please continue support on gitter. good luck!. As mentioned on gitter, only guess is there is an invalid index template\nwhich is stopping the code from automatically making the new one. Here's\nnotes from there, but let's keep this troubleshooting in gitter not github\nissue, as github issues spam a lot of people.\nthe server at startup does a \"_template\" GET request to see if there's a\ntemplate available and failing that installs one\nyou could increase logging to see if it is creating the wrong one..\nES_HTTP_LOGGING=BODY\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-server#elasticsearch-storage\n. hi there. FYI your span data is incorrect as it redundantly has \"cr\" \"cs\" and also span.kind. what is generating this?\nwill consider the change in separate comments. I think we have some issue about remote name policy. hi there. FYI your span data is incorrect as it redundantly has \"cr\" \"cs\" and also span.kind. what is generating this?\nwill consider the change in separate comments. I think we have some issue about remote name policy. I don't agree that it is nonsense to display the name of the service, btw. that's the primary thing used for everything including query and aggregation.\nI think it is safe to say that your concern is that you don't see mysql correct? for example, if you saw both names in some visualization you would be fine correct?. I don't agree that it is nonsense to display the name of the service, btw. that's the primary thing used for everything including query and aggregation.\nI think it is safe to say that your concern is that you don't see mysql correct? for example, if you saw both names in some visualization you would be fine correct?. I think this should only apply to leaf spans. otherwise it will definitely mess up association with the dependency graph. please keep rationale discussion on issue #2274 as it is split enough times already. I think this should only apply to leaf spans. otherwise it will definitely mess up association with the dependency graph. please keep rationale discussion on issue #2274 as it is split enough times already. sorry, it's generated by some WIP code in the middle of moving from the v1\n\nmodel to the v2 one. Can I just drop cs and cr from that span data and be\nok?\nWill move the rest of the discussion to #2274\nhttps://github.com/openzipkin/zipkin/issues/2274\n\n\nthanks! incidentally, we are closer than might seem. basically the code\nmust use SpanNode check for children in order to know if it is leaf or not.\nif there are no children it is a leaf. In this case, it can prioritize the\nclient name.. of course the lens UI could show both names\n. sorry, it's generated by some WIP code in the middle of moving from the v1\n\nmodel to the v2 one. Can I just drop cs and cr from that span data and be\nok?\nWill move the rest of the discussion to #2274\nhttps://github.com/openzipkin/zipkin/issues/2274\n\n\nthanks! incidentally, we are closer than might seem. basically the code\nmust use SpanNode check for children in order to know if it is leaf or not.\nif there are no children it is a leaf. In this case, it can prioritize the\nclient name.. of course the lens UI could show both names\n. thanks we can sort lens independently. can you please use gitter for troubleshooting? https://gitter.im/openzipkin/zipkin\nissues spam people with lots of messages.. fyi if someone is up to help advise or otherwise add a commit here it will progress quicker. due to travel etc I will have limited focus for a while. nope didn't try that\nOn Sat, Jan 12, 2019, 1:23 PM Anuraag Agrawal <notifications@github.com\nwrote:\n\n@anuraaga commented on this pull request.\nIn\nzipkin-server/src/test/java/zipkin2/server/internal/ITZipkinMetricsHealth.java\nhttps://github.com/openzipkin/zipkin/pull/2348#discussion_r247303154:\n\n@@ -45,7 +45,10 @@\n @SpringBootTest(\n   classes = ZipkinServer.class,\n   webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT,\n-  properties = \"spring.config.name=zipkin-server\"\n+  properties = {\n+    \"spring.config.name=zipkin-server\",\n+    \"spring.main.web-application-type=none\"\n\nJust a quick note, did you try setting webEnvironment above to NONE?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2348#discussion_r247303154,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xhBjOsuEJgBB8YMn_cqJDtKp29qks5vCXFDgaJpZM4Z8be3\n.\n. fyi battery holding I plan to put a couple more hours into this today while\non a flight\n. I made significant progress. will update later today.\n. current status:\n\na lot works except for some reason..\n POST endpoints that have content-type headers aren't getting invoked\n actuator endpoints don't work\nsee also: probably we want to write the \"zipkin-ui\" forwarding rules into some equivalent in armeria though I don't know what that is (possibly just normal java method dispatch). Also zipkin-ui loads assets from a jar file, so there's probably some armeria equivalent of that (right now MVC stuff doesn't work at all). ok I have a commit not yet pushed from flight. you can delay until then. on\nflight got far except getting actuator and webmvc working\nOn Mon, Jan 14, 2019, 4:39 PM Trustin Lee <notifications@github.com wrote:\n\nWill take a look soon.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2348#issuecomment-453969475,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6102VdOJBDHpGBUFzd6f424b1HbdHks5vDGVVgaJpZM4Z8be3\n.\n. @trustin pushed.. so I think if someone can get the zipkin-autoconfigure/ui module working I can probably get the rest to fall in line. I think it is only the spring related things not working at this point (actuator, mvc forwarding rules for the UI which imho can be redone in pure armeria if helps just I don't know how to get the resource tricks spring does to work in armeria). @anuraaga a pull against this branch works for me. I didn't realize you can use tomcat as a translation layer. Your rationale makes sense.. actually I just gave you karma to push directly. that is mutually easier as\nthen you can get it to run travis etc.\n\nOn Mon, Jan 14, 2019, 7:19 PM Anuraag Agrawal <notifications@github.com\nwrote:\n\nI want to mess with this branch. Is the best way to create a PR to the\narmeria-server branch from a fork? Just wondering what's easiest.\nAlso concept-wise, armeria can serve servlet-api using Tomcat as a\ntranslation layer - not a separate port / tcp handler, just uses the\nbusiness logic. What do you think of serving Zipkin API using armeria\ndirectly for optimal performance while also letting low QPS endpoints like\nactuator to be served using the translation layer? It adds a dependency on\nTomcat, but has the upside of guaranteeing full spring compatibility on\nendpoints we delegate to that layer.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2348#issuecomment-454010565,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610rKWBOO4kEe1BH3LL7gKWCJHzt-ks5vDIsAgaJpZM4Z8be3\n.\n. actually I just gave you karma to push directly. that is mutually easier as\nthen you can get it to run travis etc.\n\nOn Mon, Jan 14, 2019, 7:19 PM Anuraag Agrawal <notifications@github.com\nwrote:\n\nI want to mess with this branch. Is the best way to create a PR to the\narmeria-server branch from a fork? Just wondering what's easiest.\nAlso concept-wise, armeria can serve servlet-api using Tomcat as a\ntranslation layer - not a separate port / tcp handler, just uses the\nbusiness logic. What do you think of serving Zipkin API using armeria\ndirectly for optimal performance while also letting low QPS endpoints like\nactuator to be served using the translation layer? It adds a dependency on\nTomcat, but has the upside of guaranteeing full spring compatibility on\nendpoints we delegate to that layer.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2348#issuecomment-454010565,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610rKWBOO4kEe1BH3LL7gKWCJHzt-ks5vDIsAgaJpZM4Z8be3\n.\n. thanks for the unblocking. there is definitely progress here. the jar is much larger now, which is a bit of a problem as it is a lot larger. A good bit of it is tomcat, though some of the space is due to things like boringssl which is possible for us to punt or just accept.... thanks for the unblocking. there is definitely progress here. the jar is much larger now, which is a bit of a problem as it is a lot larger. A good bit of it is tomcat, though some of the space is due to things like boringssl which is possible for us to punt or just accept.... ```\n\nthis version\n$ du -k  ./zipkin-server/target/zipkin-server-*exec.jar\n51572   ./zipkin-server/target/zipkin-server-2.11.13-SNAPSHOT-exec.jar\nmaster before this change\n$ du -k zipkin.jar\n38760   zipkin.jar\n.\nthis version\n$ du -k  ./zipkin-server/target/zipkin-server-*exec.jar\n51572   ./zipkin-server/target/zipkin-server-2.11.13-SNAPSHOT-exec.jar\nmaster before this change\n$ du -k zipkin.jar\n38760   zipkin.jar\n```. I'll try to figure out which jars are responsible.. a few mins... I'll try to figure out which jars are responsible.. a few mins... here are the differences... besides the huge tomcat dep validation is also suspect. I'm not sure we'd strictly want this as we have so few things to validate...\nleft is armeria right is before\n```diff\n1,3d0\n< 5817106 Mon Dec 24 15:44:12 IST 2018 BOOT-INF/lib/armeria-0.78.1.jar\n< 3235448 Fri Nov 02 14:29:18 IST 2018 BOOT-INF/lib/tomcat-embed-core-9.0.13.jar\n< 3016315 Thu Nov 15 13:36:22 IST 2018 BOOT-INF/lib/netty-tcnative-boringssl-static-2.0.20.Final.jar\n6a4\n\n2256120 Tue Nov 27 11:39:30 IST 2018 BOOT-INF/lib/undertow-core-2.0.16.Final.jar\n14d11\n< 1155681 Wed Aug 22 22:16:56 IST 2018 BOOT-INF/lib/hibernate-validator-6.0.13.Final.jar\n22d18\n< 586456 Mon Oct 29 15:49:58 IST 2018 BOOT-INF/lib/netty-common-4.1.31.Final.jar\n24c20,21\n< 563216 Mon Oct 29 16:03:30 IST 2018 BOOT-INF/lib/netty-codec-http-4.1.31.Final.jar\n\n\n\n582362 Mon Aug 13 11:09:14 IST 2018 BOOT-INF/lib/xnio-api-3.6.5.Final.jar\n531113 Tue Nov 27 11:42:22 IST 2018 BOOT-INF/lib/undertow-servlet-2.0.16.Final.jar\n26d22\n< 463017 Mon Oct 29 15:54:22 IST 2018 BOOT-INF/lib/netty-transport-4.1.31.Final.jar\n30,31d25\n< 419511 Mon Oct 29 16:00:18 IST 2018 BOOT-INF/lib/netty-handler-4.1.31.Final.jar\n< 404008 Mon Oct 29 16:05:56 IST 2018 BOOT-INF/lib/netty-codec-http2-4.1.31.Final.jar\n37d30\n< 316560 Mon Oct 29 15:55:48 IST 2018 BOOT-INF/lib/netty-codec-4.1.31.Final.jar\n40d32\n< 273373 Mon Oct 29 15:51:52 IST 2018 BOOT-INF/lib/netty-buffer-4.1.31.Final.jar\n42d33\n< 249951 Fri Nov 02 14:29:20 IST 2018 BOOT-INF/lib/tomcat-embed-el-9.0.13.jar\n44a36,37\n224014 Thu Apr 25 16:03:30 IST 2013 BOOT-INF/lib/javax.el-3.0.0.jar\n221719 Thu Jan 11 10:35:18 IST 2018 BOOT-INF/lib/wildfly-common-1.3.0.Final.jar\n45a39\n166279 Fri Mar 16 08:35:42 IST 2018 BOOT-INF/lib/jboss-threads-2.3.2.Final.jar\n48d41\n< 141019 Mon Oct 29 16:42:10 IST 2018 BOOT-INF/lib/netty-transport-native-epoll-4.1.31.Final-linux-x86_64.jar\n51d43\n< 128535 Mon Oct 29 16:17:42 IST 2018 BOOT-INF/lib/netty-resolver-dns-4.1.31.Final.jar\n52a45\n121323 Mon Aug 13 11:10:02 IST 2018 BOOT-INF/lib/xnio-nio-3.6.5.Final.jar\n58d50\n<  93107 Tue Dec 19 16:23:28 IST 2017 BOOT-INF/lib/validation-api-2.0.1.Final.jar\n61d52\n<  66540 Tue Mar 27 18:35:34 IST 2018 BOOT-INF/lib/classmate-1.4.0.jar\n66,67d56\n<  55421 Mon Oct 29 16:30:18 IST 2018 BOOT-INF/lib/netty-transport-native-unix-common-4.1.31.Final-linux-x86_64.jar\n<  54859 Mon Oct 29 15:57:40 IST 2018 BOOT-INF/lib/netty-codec-dns-4.1.31.Final.jar\n68a58\n 47189 Thu Aug 10 11:34:20 IST 2017 BOOT-INF/lib/wildfly-client-config-1.0.0.Final.jar\n70d59\n<  42659 Mon Dec 24 15:43:38 IST 2018 BOOT-INF/lib/armeria-tomcat-0.78.1.jar\n73d61\n<  33471 Mon Oct 29 18:10:02 IST 2018 BOOT-INF/lib/netty-transport-native-unix-common-4.1.31.Final.jar\n75d62\n<  32800 Mon Oct 29 15:52:28 IST 2018 BOOT-INF/lib/netty-resolver-4.1.31.Final.jar\n80,81d66\n<  25499 Mon Dec 24 15:43:42 IST 2018 BOOT-INF/lib/armeria-spring-boot-autoconfigure-0.78.1.jar\n<  25305 Mon Oct 29 15:58:58 IST 2018 BOOT-INF/lib/netty-codec-haproxy-4.1.31.Final.jar\n87,89c72\n<  19936 Fri Mar 31 10:55:30 IST 2017 BOOT-INF/lib/jsr305-3.0.2.jar\n<  16515 Thu Mar 16 17:37:30 IST 2017 BOOT-INF/lib/jcl-over-slf4j-1.7.25.jar\n<  16465 Wed Jul 18 22:35:12 IST 2018 BOOT-INF/lib/metrics-json-4.0.3.jar\n\n\n\n19834 Wed Jan 17 13:26:46 IST 2018 BOOT-INF/lib/jboss-annotations-api_1.2_spec-1.0.2.Final.jar\n91d73\n<  12966 Fri Nov 02 14:28:20 IST 2018 BOOT-INF/lib/tomcat-annotations-api-9.0.13.jar\n99,100c81\n<   2497 Tue Oct 13 16:07:04 IST 2009 BOOT-INF/lib/javax.inject-1.jar\n<   2097 Mon Dec 18 21:06:08 IST 2017 BOOT-INF/lib/reactive-streams-1.0.2.jar\n\n\n\n425 Fri Nov 30 08:48:34 IST 2018 BOOT-INF/lib/spring-boot-starter-undertow-2.1.1.RELEASE.jar\n```. here are the differences... besides the huge tomcat dep validation is also suspect. I'm not sure we'd strictly want this as we have so few things to validate...\n\nleft is armeria right is before\n```diff\n1,3d0\n< 5817106 Mon Dec 24 15:44:12 IST 2018 BOOT-INF/lib/armeria-0.78.1.jar\n< 3235448 Fri Nov 02 14:29:18 IST 2018 BOOT-INF/lib/tomcat-embed-core-9.0.13.jar\n< 3016315 Thu Nov 15 13:36:22 IST 2018 BOOT-INF/lib/netty-tcnative-boringssl-static-2.0.20.Final.jar\n6a4\n\n2256120 Tue Nov 27 11:39:30 IST 2018 BOOT-INF/lib/undertow-core-2.0.16.Final.jar\n14d11\n< 1155681 Wed Aug 22 22:16:56 IST 2018 BOOT-INF/lib/hibernate-validator-6.0.13.Final.jar\n22d18\n< 586456 Mon Oct 29 15:49:58 IST 2018 BOOT-INF/lib/netty-common-4.1.31.Final.jar\n24c20,21\n< 563216 Mon Oct 29 16:03:30 IST 2018 BOOT-INF/lib/netty-codec-http-4.1.31.Final.jar\n\n\n\n582362 Mon Aug 13 11:09:14 IST 2018 BOOT-INF/lib/xnio-api-3.6.5.Final.jar\n531113 Tue Nov 27 11:42:22 IST 2018 BOOT-INF/lib/undertow-servlet-2.0.16.Final.jar\n26d22\n< 463017 Mon Oct 29 15:54:22 IST 2018 BOOT-INF/lib/netty-transport-4.1.31.Final.jar\n30,31d25\n< 419511 Mon Oct 29 16:00:18 IST 2018 BOOT-INF/lib/netty-handler-4.1.31.Final.jar\n< 404008 Mon Oct 29 16:05:56 IST 2018 BOOT-INF/lib/netty-codec-http2-4.1.31.Final.jar\n37d30\n< 316560 Mon Oct 29 15:55:48 IST 2018 BOOT-INF/lib/netty-codec-4.1.31.Final.jar\n40d32\n< 273373 Mon Oct 29 15:51:52 IST 2018 BOOT-INF/lib/netty-buffer-4.1.31.Final.jar\n42d33\n< 249951 Fri Nov 02 14:29:20 IST 2018 BOOT-INF/lib/tomcat-embed-el-9.0.13.jar\n44a36,37\n224014 Thu Apr 25 16:03:30 IST 2013 BOOT-INF/lib/javax.el-3.0.0.jar\n221719 Thu Jan 11 10:35:18 IST 2018 BOOT-INF/lib/wildfly-common-1.3.0.Final.jar\n45a39\n166279 Fri Mar 16 08:35:42 IST 2018 BOOT-INF/lib/jboss-threads-2.3.2.Final.jar\n48d41\n< 141019 Mon Oct 29 16:42:10 IST 2018 BOOT-INF/lib/netty-transport-native-epoll-4.1.31.Final-linux-x86_64.jar\n51d43\n< 128535 Mon Oct 29 16:17:42 IST 2018 BOOT-INF/lib/netty-resolver-dns-4.1.31.Final.jar\n52a45\n121323 Mon Aug 13 11:10:02 IST 2018 BOOT-INF/lib/xnio-nio-3.6.5.Final.jar\n58d50\n<  93107 Tue Dec 19 16:23:28 IST 2017 BOOT-INF/lib/validation-api-2.0.1.Final.jar\n61d52\n<  66540 Tue Mar 27 18:35:34 IST 2018 BOOT-INF/lib/classmate-1.4.0.jar\n66,67d56\n<  55421 Mon Oct 29 16:30:18 IST 2018 BOOT-INF/lib/netty-transport-native-unix-common-4.1.31.Final-linux-x86_64.jar\n<  54859 Mon Oct 29 15:57:40 IST 2018 BOOT-INF/lib/netty-codec-dns-4.1.31.Final.jar\n68a58\n 47189 Thu Aug 10 11:34:20 IST 2017 BOOT-INF/lib/wildfly-client-config-1.0.0.Final.jar\n70d59\n<  42659 Mon Dec 24 15:43:38 IST 2018 BOOT-INF/lib/armeria-tomcat-0.78.1.jar\n73d61\n<  33471 Mon Oct 29 18:10:02 IST 2018 BOOT-INF/lib/netty-transport-native-unix-common-4.1.31.Final.jar\n75d62\n<  32800 Mon Oct 29 15:52:28 IST 2018 BOOT-INF/lib/netty-resolver-4.1.31.Final.jar\n80,81d66\n<  25499 Mon Dec 24 15:43:42 IST 2018 BOOT-INF/lib/armeria-spring-boot-autoconfigure-0.78.1.jar\n<  25305 Mon Oct 29 15:58:58 IST 2018 BOOT-INF/lib/netty-codec-haproxy-4.1.31.Final.jar\n87,89c72\n<  19936 Fri Mar 31 10:55:30 IST 2017 BOOT-INF/lib/jsr305-3.0.2.jar\n<  16515 Thu Mar 16 17:37:30 IST 2017 BOOT-INF/lib/jcl-over-slf4j-1.7.25.jar\n<  16465 Wed Jul 18 22:35:12 IST 2018 BOOT-INF/lib/metrics-json-4.0.3.jar\n\n\n\n19834 Wed Jan 17 13:26:46 IST 2018 BOOT-INF/lib/jboss-annotations-api_1.2_spec-1.0.2.Final.jar\n91d73\n<  12966 Fri Nov 02 14:28:20 IST 2018 BOOT-INF/lib/tomcat-annotations-api-9.0.13.jar\n99,100c81\n<   2497 Tue Oct 13 16:07:04 IST 2009 BOOT-INF/lib/javax.inject-1.jar\n<   2097 Mon Dec 18 21:06:08 IST 2017 BOOT-INF/lib/reactive-streams-1.0.2.jar\n\n\n\n425 Fri Nov 30 08:48:34 IST 2018 BOOT-INF/lib/spring-boot-starter-undertow-2.1.1.RELEASE.jar\n```. thanks @anuraaga! I will try to help with polishing on this. thanks @anuraaga! I will try to help with polishing on this. fyi I am stepping off this branch again as I have to pack and move tomorrow. I didn't look into deeply but there is a test failing in zipkin-ui autoconfig. fyi I am stepping off this branch again as I have to pack and move tomorrow. I didn't look into deeply but there is a test failing in zipkin-ui autoconfig. fyi I will work on this next 3hrs offline . if someone's up to progress this farther today, I think I have unblocked the config-related problems.\n\nquick glance at test failures suggest actuator isn't working, then there is some slight mismatch in cors and redirect tests. PS I have another flight or two today so will stab at this a bit more. makes sense. yeah it is possible jetty could be smaller.. good thinking.\nI wouldnt guess it viable to try to implement actuator manually as there\nare a bunch of things in there.. maybe we can raise an issue so spring boot\n3 admin endpoints are not strictly requiring servlet.. possibly that or\nlooking at what an armeria servlet context would look like.\nOn Sat, Jan 26, 2019, 1:00 PM Anuraag Agrawal <notifications@github.com\nwrote:\n\nRealized that even though we ported UI, actuator will still use\nservlet-api and would need it included. Could either manually implement\nactuator endpoints as armeria services or try servlet-api again, maybe this\ntime with jetty to see if it's smaller.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2348#issuecomment-457802538,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD610G8He5Y4xRu7PKotu9CARqVU6wHks5vG-DggaJpZM4Z8be3\n.\n. @anuraaga I realized on a flight that you probably meant implementing support for generic actuator endpoints. I spiked this on the flight and seems to begin progress though the last commit is just proof-concept in nature. It doesn't appear actuator has a hard-dependency on servlet context iotw. So, we need to think about whether to continue this work here or in armeria.. yeah this is for the zipkin http collector endpoints POST /api/v2/spans for\nexample\n\nOn Sun, Feb 10, 2019, 6:23 PM Anuraag Agrawal <notifications@github.com\nwrote:\n\n@anuraaga commented on this pull request.\nIn\nzipkin-server/src/main/java/zipkin2/server/internal/actuate/TolerantPathMapping.java\nhttps://github.com/openzipkin/zipkin/pull/2348#discussion_r255331516:\n\n\n\n\nthe License.\n\n\n*/\n+package zipkin2.server.internal.actuate;\n+\n+import com.linecorp.armeria.common.HttpMethod;\n+import com.linecorp.armeria.common.MediaType;\n+import com.linecorp.armeria.server.PathMapping;\n+import com.linecorp.armeria.server.PathMappingContext;\n+import com.linecorp.armeria.server.PathMappingResult;\n+import com.linecorp.armeria.server.VirtualHost;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+\n+final class TolerantPathMapping implements PathMapping {\n\n\nThanks, will check it. Wouldn't have figured that out since from what I\ncan tell, /loggers is the only default endpoint that sets consumes!\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2348#discussion_r255331516,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD611e36mPSDys7XmjJs7AF-zSJwUOMks5vL_MLgaJpZM4Z8be3\n.\n. dunno probably something must have failed else I wouldnt have written this.\nthe version of me that knew was force pushed\n\nOn Sun, Feb 10, 2019, 6:35 PM Anuraag Agrawal <notifications@github.com\nwrote:\n\nAh - in that case I suspect it's not needed. That API is using annotated\nservice, not actuator. From what I can tell from these docs, actuator\nrequires content type header so the default path mapping probably works\nhttps://docs.spring.io/spring-boot/docs/current/actuator-api/html/#loggers-single\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2348#issuecomment-462121112,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD613sEt_RNRBpPwEEXqH9em9goOOpJks5vL_XpgaJpZM4Z8be3\n.\n. @anuraaga I played with building from your branch of armeria, but I get a mysterious NPE in the exploratory commit\n\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-invoker-plugin:3.2.0:install (integration-test) on project zipkin-server: Failed to install project dependencies: MavenProject: io.zipkin.java:zipkin-server:2.12.2-SNAPSHOT @ /Users/acole/oss/zipkin/zipkin-server/pom.xml: Failed to install project artifacts: null: Failed to install POM: null: NullPointerException -> [Help 1]. will take a look. thanks!\nOn Sun, Mar 3, 2019 at 5:43 PM Anuraag Agrawal notifications@github.com\nwrote:\n\nUpdated this branch and CircleCI passes now. I made one behavior change in\nthat when collector is disabled, it returns 404 instead of 405 - the latter\nseemed complicated and also surprised me, if the API isn't on shouldn't it\nbe 404? Anyways, hopefully it's a small enough change.\nI think there are some more code cleanups possible but at least things are\nmerged and probably working - Travis seems to get stuck on not being able\nto connect to Cassandra but I'm hoping that's not related to armeria.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2348#issuecomment-469005757,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xahZ3pTx2SdsBoxxDX4ZlhOXhLvks5vS5lHgaJpZM4Z8be3\n.\n. quick look noticed we drifted you. if you dont have time to adjust for the\nnew dual mounted ui stuff, I can take a look later maybe Tuesday.\n\nhave a presentation to write then a family visit meanwhile\nthanks for the continued swing on this. I also want this merged asap\nOn Sun, Mar 3, 2019, 7:47 AM Adrian Cole adrian.f.cole@gmail.com wrote:\n\nwill take a look. thanks!\nOn Sun, Mar 3, 2019 at 5:43 PM Anuraag Agrawal notifications@github.com\nwrote:\n\nUpdated this branch and CircleCI passes now. I made one behavior change\nin that when collector is disabled, it returns 404 instead of 405 - the\nlatter seemed complicated and also surprised me, if the API isn't on\nshouldn't it be 404? Anyways, hopefully it's a small enough change.\nI think there are some more code cleanups possible but at least things\nare merged and probably working - Travis seems to get stuck on not being\nable to connect to Cassandra but I'm hoping that's not related to armeria.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2348#issuecomment-469005757,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61xahZ3pTx2SdsBoxxDX4ZlhOXhLvks5vS5lHgaJpZM4Z8be3\n.\n\n\n. I'll try to figure out what this is tonight.\n\n[ERROR]   ITZipkinMetricsHealth.writeSpans_updatesPrometheusMetrics:165 NullPointer. actually ITZipkinMetricsHealth is an unrelated flake. there's no reason it should pass or not pass based on this code change.. using in mem and the script here one request bombed like this. not critical but after merge possibly interesting to folks maybe @trustin \nhttps://github.com/openzipkin/zipkin/pull/1806#issue-153649052\n```\n2019-03-06 17:15:58.938  WARN 69546 --- [worker-nio-2-15] c.l.a.s.HttpServerHandler                : [id: 0xba9745a4, L:0.0.0.0/0.0.0.0:9411 ! R:/0:0:0:0:0:0:0:1:62432][h1c] Unexpected exception:\njava.io.IOException: Protocol wrong type for socket\n    at sun.nio.ch.FileDispatcherImpl.write0(Native Method) ~[?:?]\n    at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) ~[?:?]\n    at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:113) ~[?:?]\n    at sun.nio.ch.IOUtil.write(IOUtil.java:58) ~[?:?]\n    at sun.nio.ch.IOUtil.write(IOUtil.java:50) ~[?:?]\n    at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466) ~[?:?]\n    at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:405) ~[netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:938) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:360) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:905) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1370) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:776) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:768) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:749) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.ChannelDuplexHandler.flush(ChannelDuplexHandler.java:117) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:776) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:768) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:749) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.handler.flush.FlushConsolidationHandler.flushNow(FlushConsolidationHandler.java:206) [netty-handler-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.handler.flush.FlushConsolidationHandler.flushIfNeeded(FlushConsolidationHandler.java:199) [netty-handler-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.handler.flush.FlushConsolidationHandler.resetReadAndFlushIfNeeded(FlushConsolidationHandler.java:194) [netty-handler-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.handler.flush.FlushConsolidationHandler.channelReadComplete(FlushConsolidationHandler.java:147) [netty-handler-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelReadComplete(AbstractChannelHandlerContext.java:398) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelReadComplete(AbstractChannelHandlerContext.java:380) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelReadComplete(AbstractChannelHandlerContext.java:373) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.handler.timeout.IdleStateHandler.channelReadComplete(IdleStateHandler.java:295) [netty-handler-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelReadComplete(AbstractChannelHandlerContext.java:398) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelReadComplete(AbstractChannelHandlerContext.java:380) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelReadComplete(AbstractChannelHandlerContext.java:373) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelReadComplete(DefaultChannelPipeline.java:1413) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelReadComplete(AbstractChannelHandlerContext.java:398) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelReadComplete(AbstractChannelHandlerContext.java:380) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.DefaultChannelPipeline.fireChannelReadComplete(DefaultChannelPipeline.java:936) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:168) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:677) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:612) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:529) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:491) [netty-transport-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:905) [netty-common-4.1.33.Final.jar!/:4.1.33.Final]\n    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.33.Final.jar!/:4.1.33.Final]\n    at java.lang.Thread.run(Thread.java:834) [?:?]\n```. ps above is not a problem as it was one of hundreds of thousands of requests. I'm very happy with this change. I ran wrk and found that this results in far more consistent latency, which is great for apps\nBEFORE\nThread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     9.03ms   27.60ms 275.16ms   92.96%\n    Req/Sec    16.09k     4.18k   23.37k    82.48%\n  Latency Distribution\n     50%    1.78ms\n     75%    2.01ms\n     90%    3.92ms\n     99%  148.75ms\n  1874690 requests in 30.03s, 212.75MB read\nRequests/sec:  62421.95\nTransfer/sec:      7.08MB\nAFTER\nThread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     4.92ms    5.08ms  63.50ms   95.37%\n    Req/Sec     7.62k     1.29k   10.95k    70.99%\n  Latency Distribution\n     50%    3.46ms\n     75%    4.07ms\n     90%    6.93ms\n     99%   32.97ms\n  909271 requests in 30.00s, 84.98MB read\nRequests/sec:  30304.72\nTransfer/sec:      2.83MB. @anuraaga you are the hero of this change. thank you so much. @trustin \nso here's the thing.. build the and start the server per the bottom of zipkin-server/README.md\nMake zipkin-json.lua \nlua\nwrk.method = \"POST\"\nwrk.headers[\"Content-Type\"] = \"application/json\"\nwrk.body   = '[{\"traceId\":\"4d1e00c0db9010db86154a4ba6e91385\",\"parentId\":\"86154a4ba6e91385\",\"id\":\"4d1e00c0db9010db\",\"kind\":\"CLIENT\",\"name\":\"get\",\"timestamp\":1472470996199000,\"duration\":207000,\"localEndpoint\":{\"serviceName\":\"frontend\",\"ipv6\":\"7::0.128.128.127\"},\"remoteEndpoint\":{\"serviceName\":\"backend\",\"ipv4\":\"192.168.99.101\",\"port\":9000},\"annotations\":[{\"timestamp\":1472470996238000,\"value\":\"foo\"},{\"timestamp\":1472470996403000,\"value\":\"bar\"}],\"tags\":{\"http.path\":\"/api\",\"clnt/finagle.version\":\"6.45.0\"}}]'\nrun it 5 times picking best\nbash\nwrk -t4 -c128 -d30s http://localhost:9411/api/v2/spans -s zipkin-json.lua --latency\nyou can compare this vs the previous commit. @trustin ps I think the previous had better throughput as the P<=90 was faster. @jcarres-mdsol yes though the >90% latency is much better which is indicative of what zipkin would be under load. Also, this is a dummy request.. I doubt most real servers were servicing 60k requests/second in real life with the resources of my laptop.. reason I'm less concerned about the raw throughput difference iotw, is that usually storage falls over faster than this.. for those watching from home, a more realistic test would be to load up a megabyte of bundled span data per POST message as opposed to a single tiny span. Then see about 240MB/s (peak throughput site like netflix)/ however many servers that site uses. Also consider non-peak. Also consider that most high throughput sites don't even use http (for example most use kafka). That's how I end up not terribly concerned about the amount of requests per second even if this \"hello span\" might seem damning if looking mostly at the raw throughput.\nThat plus there's likely some optimizations to follow. make sense?. here's another datapoint.. my laptop before this change with docker and mysql attached as opposed to in-memory\nLatency    27.29ms   17.77ms 164.65ms   80.97%\n    Req/Sec     1.24k   430.38     2.42k    75.96%\n  Latency Distribution\n     50%   23.40ms\n     75%   33.27ms\n     90%   47.81ms\n     99%   94.82ms\n  12846 requests in 2.60s, 1.46MB read\n  Socket errors: connect 0, read 75, write 0, timeout 0\nRequests/sec:   4931.43\nTransfer/sec:    573.09KB\nbasically we can isolate some things about http, but all-in this \"wrk\" test is not accurate for measuring integrated actual usage unless you are hitting your own install.. a cooler test is also to hit one of the -example apps, like https://github.com/openzipkin/zipkin-ruby-example. This will make more realistic span data although definitely unrealistic to most sites as only 2 services.\nex wrk -t4 -c128 -d30s http://localhost:8081  --latency\nthen look at prometheus stats on the zipkin server instead of wrk stats (which will only tell you about the example app). anyway this is fun.. Now that we support http/2, I was able to make a few runs to show some differences, using the same test json, against the same server. While this is laptop, it is an interesting thing we can now use.\nhttp 1 (text framing) best run in 5, because toes..\n```\n$ h2load -n100000 -c100 -m10 http://localhost:9411/api/v2/spans -d fooooo.json --h1\nstarting benchmark...\nspawning thread #0: 100 total client(s). 100000 total requests\nApplication protocol: http/1.1\nprogress: 10% done\nprogress: 20% done\nprogress: 30% done\nprogress: 40% done\nprogress: 50% done\nprogress: 60% done\nprogress: 70% done\nprogress: 80% done\nprogress: 90% done\nprogress: 100% done\nfinished in 3.07s, 32558.26 req/s, 3.04MB/s\nrequests: 100000 total, 100000 started, 100000 done, 100000 succeeded, 0 failed, 0 errored, 0 timeout\nstatus codes: 100000 2xx, 0 3xx, 0 4xx, 0 5xx\ntraffic: 9.35MB (9800000) total, 5.05MB (5300000) headers (space savings 0.00%), 1.14MB (1200000) data\n                     min         max         mean         sd        +/- sd\ntime for request:      107us    341.96ms      2.96ms     12.15ms    99.32%\ntime for connect:      215us      6.37ms      2.58ms      1.61ms    64.00%\ntime to 1st byte:     7.42ms     17.79ms     12.71ms      2.53ms    63.00%\nreq/s           :     325.86      363.59      337.85        8.32    70.00%\n```\nhttp 2 (binary framing) best run in 5, because toes..\n```\n$ h2load -n100000 -c100 -m10 http://localhost:9411/api/v2/spans -d fooooo.json \nstarting benchmark...\nspawning thread #0: 100 total client(s). 100000 total requests\nApplication protocol: h2c\nprogress: 10% done\nprogress: 20% done\nprogress: 30% done\nprogress: 40% done\nprogress: 50% done\nprogress: 60% done\nprogress: 70% done\nprogress: 80% done\nprogress: 90% done\nprogress: 100% done\nfinished in 2.01s, 49772.07 req/s, 2.24MB/s\nrequests: 100000 total, 100000 started, 100000 done, 100000 succeeded, 0 failed, 0 errored, 0 timeout\nstatus codes: 100000 2xx, 0 3xx, 0 4xx, 0 5xx\ntraffic: 4.50MB (4713900) total, 295.41KB (302500) headers (space savings 95.20%), 1.14MB (1200000) data\n                     min         max         mean         sd        +/- sd\ntime for request:      550us    231.94ms     19.42ms     26.36ms    95.22%\ntime for connect:      457us      7.64ms      4.09ms      2.11ms    58.00%\ntime to 1st byte:    16.73ms    105.63ms     31.54ms     14.78ms    89.00%\nreq/s           :     498.31      537.56      511.34        9.70    70.00%\n```\nFor those using okhttp sender, you can use http/2 like below, but make sure you are using the latest zipkin server as prior to that, it didn't accept http/2 requests!\njava\n  @Bean Sender sender() {\n    OkHttpSender.Builder senderBuilder = OkHttpSender.newBuilder();\n    senderBuilder.clientBuilder().protocols(Collections.singletonList(Protocol.H2_PRIOR_KNOWLEDGE));\n    return senderBuilder.endpoint(\"http://127.0.0.1:9411/api/v2/spans\").build();\n  }. Now that we support http/2, I was able to make a few runs to show some differences, using the same test json, against the same server. While this is laptop, it is an interesting thing we can now use.\nhttp 1 (text framing) best run in 5, because toes..\n```\n$ h2load -n100000 -c100 -m10 http://localhost:9411/api/v2/spans -d fooooo.json --h1\nstarting benchmark...\nspawning thread #0: 100 total client(s). 100000 total requests\nApplication protocol: http/1.1\nprogress: 10% done\nprogress: 20% done\nprogress: 30% done\nprogress: 40% done\nprogress: 50% done\nprogress: 60% done\nprogress: 70% done\nprogress: 80% done\nprogress: 90% done\nprogress: 100% done\nfinished in 3.07s, 32558.26 req/s, 3.04MB/s\nrequests: 100000 total, 100000 started, 100000 done, 100000 succeeded, 0 failed, 0 errored, 0 timeout\nstatus codes: 100000 2xx, 0 3xx, 0 4xx, 0 5xx\ntraffic: 9.35MB (9800000) total, 5.05MB (5300000) headers (space savings 0.00%), 1.14MB (1200000) data\n                     min         max         mean         sd        +/- sd\ntime for request:      107us    341.96ms      2.96ms     12.15ms    99.32%\ntime for connect:      215us      6.37ms      2.58ms      1.61ms    64.00%\ntime to 1st byte:     7.42ms     17.79ms     12.71ms      2.53ms    63.00%\nreq/s           :     325.86      363.59      337.85        8.32    70.00%\n```\nhttp 2 (binary framing) best run in 5, because toes..\n```\n$ h2load -n100000 -c100 -m10 http://localhost:9411/api/v2/spans -d fooooo.json \nstarting benchmark...\nspawning thread #0: 100 total client(s). 100000 total requests\nApplication protocol: h2c\nprogress: 10% done\nprogress: 20% done\nprogress: 30% done\nprogress: 40% done\nprogress: 50% done\nprogress: 60% done\nprogress: 70% done\nprogress: 80% done\nprogress: 90% done\nprogress: 100% done\nfinished in 2.01s, 49772.07 req/s, 2.24MB/s\nrequests: 100000 total, 100000 started, 100000 done, 100000 succeeded, 0 failed, 0 errored, 0 timeout\nstatus codes: 100000 2xx, 0 3xx, 0 4xx, 0 5xx\ntraffic: 4.50MB (4713900) total, 295.41KB (302500) headers (space savings 95.20%), 1.14MB (1200000) data\n                     min         max         mean         sd        +/- sd\ntime for request:      550us    231.94ms     19.42ms     26.36ms    95.22%\ntime for connect:      457us      7.64ms      4.09ms      2.11ms    58.00%\ntime to 1st byte:    16.73ms    105.63ms     31.54ms     14.78ms    89.00%\nreq/s           :     498.31      537.56      511.34        9.70    70.00%\n```\nFor those using okhttp sender, you can use http/2 like below, but make sure you are using the latest zipkin server as prior to that, it didn't accept http/2 requests!\njava\n  @Bean Sender sender() {\n    OkHttpSender.Builder senderBuilder = OkHttpSender.newBuilder();\n    senderBuilder.clientBuilder().protocols(Collections.singletonList(Protocol.H2_PRIOR_KNOWLEDGE));\n    return senderBuilder.endpoint(\"http://127.0.0.1:9411/api/v2/spans\").build();\n  }. I concur with tommy's summary. @drolando I missed this list. thank you so much!. @drolando I missed this list. thank you so much!. we dont support custom servers. are you suggesting our default build has a\nbean conflict or are you noticing something when you build a custom server?\n. I will make this fail same way as /api/v2/traces when zeros are passed.. @zeagord you in a position to carry this forward? will likely cut a release by monday but this isn't a blocker.. > Can we please target this next release? This one needs some improvement and\n\ncss needs to be fixed.\n\nsgtm\n. @xeraa do you know which version rollover index was added? I agree the core issue here is size. . https://github.com/openzipkin/zipkin/pull/2395 should fix this for real. obviated by #2399. looks good. not late :) I figured there would be no problem\nOn Wed, Feb 13, 2019, 11:25 AM tacigar <notifications@github.com wrote:\n\none other thing to test.. is to make a really really short client/server\nspan.. like one that completes in less than a millisecond, whereas other\nspans have a different scale like more than a millisecond. This might show\nsome rendering issues\nI tried.\n[image: 2019-02-13 12 22 50]\nhttps://user-images.githubusercontent.com/19551419/52684622-36a0a200-2f8a-11e9-94d5-3b4b4af9e315.png\nZoom up\n[image: 2019-02-13 12 23 03]\nhttps://user-images.githubusercontent.com/19551419/52684636-402a0a00-2f8a-11e9-9830-bc0f418a6658.png\n@adriancole https://github.com/adriancole\nSorry for late \ud83d\ude47\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2386#issuecomment-463041949,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD611g_lPtkVm2eTp2ciyxbrdNZX4-Iks5vM4WQgaJpZM4a1kcB\n.\n. looks good. not late :) I figured there would be no problem\n\nOn Wed, Feb 13, 2019, 11:25 AM tacigar <notifications@github.com wrote:\n\none other thing to test.. is to make a really really short client/server\nspan.. like one that completes in less than a millisecond, whereas other\nspans have a different scale like more than a millisecond. This might show\nsome rendering issues\nI tried.\n[image: 2019-02-13 12 22 50]\nhttps://user-images.githubusercontent.com/19551419/52684622-36a0a200-2f8a-11e9-94d5-3b4b4af9e315.png\nZoom up\n[image: 2019-02-13 12 23 03]\nhttps://user-images.githubusercontent.com/19551419/52684636-402a0a00-2f8a-11e9-9830-bc0f418a6658.png\n@adriancole https://github.com/adriancole\nSorry for late \ud83d\ude47\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2386#issuecomment-463041949,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD611g_lPtkVm2eTp2ciyxbrdNZX4-Iks5vM4WQgaJpZM4a1kcB\n.\n. thanks again!. I think the problem we have is that QUERY_ENABLED shouldn't imply a change to SEARCH_ENABLED. SEARCH_ENABLED should be a site-wide setting, whereas QUERY_ENABLED should be something each node can change.\nhttps://github.com/openzipkin/zipkin/tree/master/zipkin-server#environment-variables. I've looked and QUERY_ENABLED has no bearing on SEARCH_ENABLED (what controls the index template). These are controlled independently.\n\nIt might be the case that some race condition is going on in your environment. I will try what you've done locally anyway. I've looked and QUERY_ENABLED has no bearing on SEARCH_ENABLED (what controls the index template). These are controlled independently.\nIt might be the case that some race condition is going on in your environment. I will try what you've done locally anyway. Sorry I asked to open this issue.. when it was being discussed I was on my mobile and it was hard to remember what controlled what. QUERY_ENABLED has no bearing on SEARCH_ENABLED, and only SEARCH_ENABLED would cause the template to be created incorrectly. What may have happened is you had some race lost but it is impossible to guess.\nWhen running with QUERY_ENABLED=true, I verified the index template is still created. Note: our storage images are test-only.. they are not intended for production.\njava\ncurl -s localhost:9200/zipkin:span-2019-02-18/_mapping|jq .\n{\n  \"zipkin:span-2019-02-18\": {\n    \"mappings\": {\n      \"_default_\": {\n        \"dynamic_templates\": [\n          {\n            \"strings\": {\n              \"match\": \"*\",\n              \"match_mapping_type\": \"string\",\n              \"mapping\": {\n                \"ignore_above\": 256,\n                \"norms\": false,\n                \"type\": \"keyword\"\n              }\n            }\n          }\n        ]\n      },\n      \"span\": {\n        \"_source\": {\n          \"excludes\": [\n            \"_q\"\n          ]\n        },\n        \"dynamic_templates\": [\n          {\n            \"strings\": {\n              \"match\": \"*\",\n              \"match_mapping_type\": \"string\",\n              \"mapping\": {\n                \"ignore_above\": 256,\n                \"norms\": false,\n                \"type\": \"keyword\"\n              }\n            }\n          }\n        ],\n        \"properties\": {\n          \"_q\": {\n            \"type\": \"keyword\"\n          },\n          \"annotations\": {\n            \"type\": \"object\",\n            \"enabled\": false\n          },\n          \"duration\": {\n            \"type\": \"long\"\n          },\n          \"id\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          },\n          \"kind\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          },\n          \"localEndpoint\": {\n            \"dynamic\": \"false\",\n            \"properties\": {\n              \"serviceName\": {\n                \"type\": \"keyword\"\n              }\n            }\n          },\n          \"name\": {\n            \"type\": \"keyword\"\n          },\n          \"parentId\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          },\n          \"remoteEndpoint\": {\n            \"dynamic\": \"false\",\n            \"properties\": {\n              \"serviceName\": {\n                \"type\": \"keyword\"\n              }\n            }\n          },\n          \"shared\": {\n            \"type\": \"boolean\"\n          },\n          \"tags\": {\n            \"type\": \"object\",\n            \"enabled\": false\n          },\n          \"timestamp\": {\n            \"type\": \"long\"\n          },\n          \"timestamp_millis\": {\n            \"type\": \"date\",\n            \"format\": \"epoch_millis\"\n          },\n          \"traceId\": {\n            \"type\": \"keyword\"\n          }\n        }\n      }\n    }\n  }\n}\nIf you want to troubleshoot further, you can use a similar POST request if you get the error on a real image. For now, I'm closing this issue as we don't use issues for troubleshooting unless there will be code change.. Sorry I asked to open this issue.. when it was being discussed I was on my mobile and it was hard to remember what controlled what. QUERY_ENABLED has no bearing on SEARCH_ENABLED, and only SEARCH_ENABLED would cause the template to be created incorrectly. What may have happened is you had some race lost but it is impossible to guess.\nWhen running with QUERY_ENABLED=true, I verified the index template is still created. Note: our storage images are test-only.. they are not intended for production.\njava\ncurl -s localhost:9200/zipkin:span-2019-02-18/_mapping|jq .\n{\n  \"zipkin:span-2019-02-18\": {\n    \"mappings\": {\n      \"_default_\": {\n        \"dynamic_templates\": [\n          {\n            \"strings\": {\n              \"match\": \"*\",\n              \"match_mapping_type\": \"string\",\n              \"mapping\": {\n                \"ignore_above\": 256,\n                \"norms\": false,\n                \"type\": \"keyword\"\n              }\n            }\n          }\n        ]\n      },\n      \"span\": {\n        \"_source\": {\n          \"excludes\": [\n            \"_q\"\n          ]\n        },\n        \"dynamic_templates\": [\n          {\n            \"strings\": {\n              \"match\": \"*\",\n              \"match_mapping_type\": \"string\",\n              \"mapping\": {\n                \"ignore_above\": 256,\n                \"norms\": false,\n                \"type\": \"keyword\"\n              }\n            }\n          }\n        ],\n        \"properties\": {\n          \"_q\": {\n            \"type\": \"keyword\"\n          },\n          \"annotations\": {\n            \"type\": \"object\",\n            \"enabled\": false\n          },\n          \"duration\": {\n            \"type\": \"long\"\n          },\n          \"id\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          },\n          \"kind\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          },\n          \"localEndpoint\": {\n            \"dynamic\": \"false\",\n            \"properties\": {\n              \"serviceName\": {\n                \"type\": \"keyword\"\n              }\n            }\n          },\n          \"name\": {\n            \"type\": \"keyword\"\n          },\n          \"parentId\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          },\n          \"remoteEndpoint\": {\n            \"dynamic\": \"false\",\n            \"properties\": {\n              \"serviceName\": {\n                \"type\": \"keyword\"\n              }\n            }\n          },\n          \"shared\": {\n            \"type\": \"boolean\"\n          },\n          \"tags\": {\n            \"type\": \"object\",\n            \"enabled\": false\n          },\n          \"timestamp\": {\n            \"type\": \"long\"\n          },\n          \"timestamp_millis\": {\n            \"type\": \"date\",\n            \"format\": \"epoch_millis\"\n          },\n          \"traceId\": {\n            \"type\": \"keyword\"\n          }\n        }\n      }\n    }\n  }\n}\nIf you want to troubleshoot further, you can use a similar POST request if you get the error on a real image. For now, I'm closing this issue as we don't use issues for troubleshooting unless there will be code change.. agree if they are different both values should show\nOn Wed, Feb 13, 2019, 12:55 PM Bruce Liang <notifications@github.com wrote:\n\nClient side (hapi-demo) and server side (soa-demo-server) has same tag names '__serviceId & __serviceName', but tag values is not same. tag value is '0010100001/hapi-demo' and '0010100003/soa-demo-service', and now display client tag values, shoulde it display server side tag value ?\nTrace JSON: trace.zip\n[image: image]\nhttps://user-images.githubusercontent.com/5990337/52687721-225ea400-2f8e-11e9-81c4-aa9a09ffea33.png\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2390, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61x8jW7zvH2rxvKIVNfKa55hVtRzVks5vM5q5gaJpZM4a4m_o\n.\n. agree if they are different both values should show\n\nOn Wed, Feb 13, 2019, 12:55 PM Bruce Liang <notifications@github.com wrote:\n\nClient side (hapi-demo) and server side (soa-demo-server) has same tag names '__serviceId & __serviceName', but tag values is not same. tag value is '0010100001/hapi-demo' and '0010100003/soa-demo-service', and now display client tag values, shoulde it display server side tag value ?\nTrace JSON: trace.zip\n[image: image]\nhttps://user-images.githubusercontent.com/5990337/52687721-225ea400-2f8e-11e9-81c4-aa9a09ffea33.png\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2390, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61x8jW7zvH2rxvKIVNfKa55hVtRzVks5vM5q5gaJpZM4a4m_o\n.\n. sorry there are a lot of distractions this week. I want to fix this, if\nthat is your question, but if you have time to try you should first.\n\nOn Fri, Feb 15, 2019, 1:09 AM Bruce Liang <notifications@github.com wrote:\n\nDo you have any plans to amend?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2390#issuecomment-463709266,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD618Qa9hEFm_z-etUwtACuGUZ7KN36ks5vNZgygaJpZM4a4m_o\n.\n. sorry there are a lot of distractions this week. I want to fix this, if\nthat is your question, but if you have time to try you should first.\n\nOn Fri, Feb 15, 2019, 1:09 AM Bruce Liang <notifications@github.com wrote:\n\nDo you have any plans to amend?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2390#issuecomment-463709266,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD618Qa9hEFm_z-etUwtACuGUZ7KN36ks5vNZgygaJpZM4a4m_o\n.\n. > And another question: When will version 2.12.2 be released ?\n\nHow about this.. ask on monday if it is not :)\n. > And another question: When will version 2.12.2 be released ?\nHow about this.. ask on monday if it is not :)\n. this is next on my list. this is next on my list. https://github.com/openzipkin/zipkin/pull/2397. https://github.com/openzipkin/zipkin/pull/2397. This is a work in progress to investigate what is actually interfering with the sort order. I didn't actually change the javascript code for this test to pass, which means I got something wrong.. This is a work in progress to investigate what is actually interfering with the sort order. I didn't actually change the javascript code for this test to pass, which means I got something wrong.. hi. thanks. I presume you mean docker image openzipkin/zipkin\ncan you mention the environment variables you are using please?\nOn Thu, Feb 14, 2019, 6:09 PM Finn Poppinga <notifications@github.com wrote:\n\nDescribe the Bug\nUsing a clean installation of Zipkin, the /health endpoint throws a NPE\nwhen being queried.\n2019-02-14 10:02:00.364 ERROR 11 --- [  XNIO-1 task-2] i.u.request                              : UT005023: Exception handling request to /health\norg.springframework.web.util.NestedServletException: Request processing failed; nested exception is java.lang.NullPointerException\n  at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1013) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:897) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at javax.servlet.http.HttpServlet.service(HttpServlet.java:645) ~[javax.servlet-api-4.0.1.jar!/:4.0.1]\n  at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:882) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at javax.servlet.http.HttpServlet.service(HttpServlet.java:750) ~[javax.servlet-api-4.0.1.jar!/:4.0.1]\n  at io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:74) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:129) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at org.springframework.boot.actuate.web.trace.servlet.HttpTraceFilter.doFilterInternal(HttpTraceFilter.java:90) ~[spring-boot-actuator-2.1.2.RELEASE.jar!/:2.1.2.RELEASE]\n  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:92) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.filterAndRecordMetrics(WebMvcMetricsFilter.java:117) ~[spring-boot-actuator-2.1.2.RELEASE.jar!/:2.1.2.RELEASE]\n  at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:106) ~[spring-boot-actuator-2.1.2.RELEASE.jar!/:2.1.2.RELEASE]\n  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler.handleRequest(FilterHandler.java:84) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletChain$1.handleRequest(ServletChain.java:68) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:132) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) ~[undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46) ~[undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60) ~[undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43) ~[undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) ~[undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) ~[undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:292) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletInitialHandler.access$100(ServletInitialHandler.java:81) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:138) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:135) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:48) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:272) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:81) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:104) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.server.Connectors.executeRootHandler(Connectors.java:360) [undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:830) [undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at org.jboss.threads.ContextClassLoaderSavingRunnable.run(ContextClassLoaderSavingRunnable.java:35) [jboss-threads-2.3.2.Final.jar!/:2.3.2.Final]\n  at org.jboss.threads.EnhancedQueueExecutor.safeRun(EnhancedQueueExecutor.java:1985) [jboss-threads-2.3.2.Final.jar!/:2.3.2.Final]\n  at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.doRunTask(EnhancedQueueExecutor.java:1487) [jboss-threads-2.3.2.Final.jar!/:2.3.2.Final]\n  at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1378) [jboss-threads-2.3.2.Final.jar!/:2.3.2.Final]\n  at java.lang.Thread.run(Thread.java:745) [?:1.8.0_92-internal]\nCaused by: java.lang.NullPointerException\n  at zipkin2.storage.cassandra.v1.CassandraSpanConsumer.clear(CassandraSpanConsumer.java:117) ~[zipkin-storage-cassandra-v1-2.12.1.jar!/:?]\n  at zipkin2.storage.cassandra.v1.CassandraStorage.check(CassandraStorage.java:369) ~[zipkin-storage-cassandra-v1-2.12.1.jar!/:?]\n  at zipkin2.server.internal.ZipkinHealthIndicator$ComponentHealthIndicator.health(ZipkinHealthIndicator.java:45) ~[classes/:?]\n  at org.springframework.boot.actuate.health.CompositeHealthIndicator.health(CompositeHealthIndicator.java:98) ~[spring-boot-actuator-2.1.2.RELEASE.jar!/:2.1.2.RELEASE]\n  at org.springframework.boot.actuate.health.CompositeHealthIndicator.health(CompositeHealthIndicator.java:98) ~[spring-boot-actuator-2.1.2.RELEASE.jar!/:2.1.2.RELEASE]\n  at org.springframework.boot.actuate.health.HealthEndpoint.health(HealthEndpoint.java:50) ~[spring-boot-actuator-2.1.2.RELEASE.jar!/:2.1.2.RELEASE]\n  at zipkin2.server.internal.MetricsHealthController.getHealth(MetricsHealthController.java:79) ~[classes/:?]\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_92-internal]\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_92-internal]\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_92-internal]\n  at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_92-internal]\n  at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:189) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:138) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:102) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:800) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1038) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:942) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1005) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  ... 61 more\nSteps to Reproduce\n\nlaunch the openzipkin/zipkin2 image\nObserve above mentioned exception.\n\nExpected Behaviour\nHealth endpoint does not throw.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2392, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61_ahIVOCFCTh7KxIgvz3jioHVs03ks5vNTXpgaJpZM4a7P9t\n.\n. hi. thanks. I presume you mean docker image openzipkin/zipkin\n\ncan you mention the environment variables you are using please?\nOn Thu, Feb 14, 2019, 6:09 PM Finn Poppinga <notifications@github.com wrote:\n\nDescribe the Bug\nUsing a clean installation of Zipkin, the /health endpoint throws a NPE\nwhen being queried.\n2019-02-14 10:02:00.364 ERROR 11 --- [  XNIO-1 task-2] i.u.request                              : UT005023: Exception handling request to /health\norg.springframework.web.util.NestedServletException: Request processing failed; nested exception is java.lang.NullPointerException\n  at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1013) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:897) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at javax.servlet.http.HttpServlet.service(HttpServlet.java:645) ~[javax.servlet-api-4.0.1.jar!/:4.0.1]\n  at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:882) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at javax.servlet.http.HttpServlet.service(HttpServlet.java:750) ~[javax.servlet-api-4.0.1.jar!/:4.0.1]\n  at io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:74) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:129) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at org.springframework.boot.actuate.web.trace.servlet.HttpTraceFilter.doFilterInternal(HttpTraceFilter.java:90) ~[spring-boot-actuator-2.1.2.RELEASE.jar!/:2.1.2.RELEASE]\n  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:92) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.filterAndRecordMetrics(WebMvcMetricsFilter.java:117) ~[spring-boot-actuator-2.1.2.RELEASE.jar!/:2.1.2.RELEASE]\n  at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:106) ~[spring-boot-actuator-2.1.2.RELEASE.jar!/:2.1.2.RELEASE]\n  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.FilterHandler.handleRequest(FilterHandler.java:84) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletChain$1.handleRequest(ServletChain.java:68) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:132) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) ~[undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46) ~[undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60) ~[undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77) ~[undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43) ~[undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) ~[undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) ~[undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:292) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletInitialHandler.access$100(ServletInitialHandler.java:81) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:138) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:135) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:48) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:272) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:81) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:104) [undertow-servlet-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.server.Connectors.executeRootHandler(Connectors.java:360) [undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:830) [undertow-core-2.0.16.Final.jar!/:2.0.16.Final]\n  at org.jboss.threads.ContextClassLoaderSavingRunnable.run(ContextClassLoaderSavingRunnable.java:35) [jboss-threads-2.3.2.Final.jar!/:2.3.2.Final]\n  at org.jboss.threads.EnhancedQueueExecutor.safeRun(EnhancedQueueExecutor.java:1985) [jboss-threads-2.3.2.Final.jar!/:2.3.2.Final]\n  at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.doRunTask(EnhancedQueueExecutor.java:1487) [jboss-threads-2.3.2.Final.jar!/:2.3.2.Final]\n  at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1378) [jboss-threads-2.3.2.Final.jar!/:2.3.2.Final]\n  at java.lang.Thread.run(Thread.java:745) [?:1.8.0_92-internal]\nCaused by: java.lang.NullPointerException\n  at zipkin2.storage.cassandra.v1.CassandraSpanConsumer.clear(CassandraSpanConsumer.java:117) ~[zipkin-storage-cassandra-v1-2.12.1.jar!/:?]\n  at zipkin2.storage.cassandra.v1.CassandraStorage.check(CassandraStorage.java:369) ~[zipkin-storage-cassandra-v1-2.12.1.jar!/:?]\n  at zipkin2.server.internal.ZipkinHealthIndicator$ComponentHealthIndicator.health(ZipkinHealthIndicator.java:45) ~[classes/:?]\n  at org.springframework.boot.actuate.health.CompositeHealthIndicator.health(CompositeHealthIndicator.java:98) ~[spring-boot-actuator-2.1.2.RELEASE.jar!/:2.1.2.RELEASE]\n  at org.springframework.boot.actuate.health.CompositeHealthIndicator.health(CompositeHealthIndicator.java:98) ~[spring-boot-actuator-2.1.2.RELEASE.jar!/:2.1.2.RELEASE]\n  at org.springframework.boot.actuate.health.HealthEndpoint.health(HealthEndpoint.java:50) ~[spring-boot-actuator-2.1.2.RELEASE.jar!/:2.1.2.RELEASE]\n  at zipkin2.server.internal.MetricsHealthController.getHealth(MetricsHealthController.java:79) ~[classes/:?]\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_92-internal]\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_92-internal]\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_92-internal]\n  at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_92-internal]\n  at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:189) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:138) ~[spring-web-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:102) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:800) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1038) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:942) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1005) ~[spring-webmvc-5.1.4.RELEASE.jar!/:5.1.4.RELEASE]\n  ... 61 more\nSteps to Reproduce\n\nlaunch the openzipkin/zipkin2 image\nObserve above mentioned exception.\n\nExpected Behaviour\nHealth endpoint does not throw.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2392, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61_ahIVOCFCTh7KxIgvz3jioHVs03ks5vNTXpgaJpZM4a7P9t\n.\n. ack. this is a bug. fixing now. ack. this is a bug. fixing now. https://github.com/openzipkin/zipkin/pull/2396. https://github.com/openzipkin/zipkin/pull/2396. yeah will cut once https://github.com/openzipkin/zipkin/pull/2404 is merged\n. yeah will cut once https://github.com/openzipkin/zipkin/pull/2404 is merged\n. releasing anyway as #2404 scope just grew. weird.. fails on lint. weird.. fails on lint. no prob!. no prob!. thanks for taking a look!. thanks for taking a look!. @ldcsaa how to design UX for this is a separate issue as there are many ways to do that. want to open one? It will take some time to get feedback on it, which is why I solved the underlying problem seperately. @ldcsaa how to design UX for this is a separate issue as there are many ways to do that. want to open one? It will take some time to get feedback on it, which is why I solved the underlying problem seperately. the thing breaks on lack of default mapping. We have some things there, so we have to be careful how to address this. I'm putting this change down because I didn't anticipate this being more than just the colon problem.. the thing breaks on lack of default mapping. We have some things there, so we have to be careful how to address this. I'm putting this change down because I didn't anticipate this being more than just the colon problem.. the javascript libraries have been updated recently. Please try most recent and see if still an issue? we don't have custom json processing code. the javascript libraries have been updated recently. Please try most recent and see if still an issue? we don't have custom json processing code. one nice thing about the Client vs Server idea is that it doesn't require us to make another Address column for tags.. one nice thing about the Client vs Server idea is that it doesn't require us to make another Address column for tags.. cc @openzipkin/ui . cc @openzipkin/ui . > One piece of feedback is that it doesn't seem obvious to a user how to go\nback to the classic UI right now. What do you think about also adding a\nbutton to Lens that's the complement of the current button? \"Go back to\nclassic UI\" or something.\nI think lens would be cluttered as the sidebar has already a lot of things\nin it. I would prefer to just let someone close their browser as this is\nreally a short term thing..\n. > One piece of feedback is that it doesn't seem obvious to a user how to go\nback to the classic UI right now. What do you think about also adding a\nbutton to Lens that's the complement of the current button? \"Go back to\nclassic UI\" or something.\nI think lens would be cluttered as the sidebar has already a lot of things\nin it. I would prefer to just let someone close their browser as this is\nreally a short term thing..\n. to set expectations.. lens has been in progress for months. the only reason\nwe are doing this at all is to remove FUD about pulling the trigger and\nactually releasing lens as default.\n\nMeanwhile, people are still looking at zipkin as if lens doesnt exist.\nThe primary motive of this wasnt to create a flip flop for an extended\nbeta, but rather the minimum infra to reduce the burden of maintaining 2\nuis simultaneously.. and dispelling myth that lens doesnt exist.\nif we add an icon on lens to go back, people may have impression that we\nwill be keeping the old ui which we certainly wont. it will be deleted\nideally within a month\nmake sense?\n. to set expectations.. lens has been in progress for months. the only reason\nwe are doing this at all is to remove FUD about pulling the trigger and\nactually releasing lens as default.\nMeanwhile, people are still looking at zipkin as if lens doesnt exist.\nThe primary motive of this wasnt to create a flip flop for an extended\nbeta, but rather the minimum infra to reduce the burden of maintaining 2\nuis simultaneously.. and dispelling myth that lens doesnt exist.\nif we add an icon on lens to go back, people may have impression that we\nwill be keeping the old ui which we certainly wont. it will be deleted\nideally within a month\nmake sense?\n. ok so let me ask another way then.\nof people offering for us to do more work.. who is offering to do it? and\nrevert it back later?\n. @drolando yes, but you will still need a control for that somewhere. I didn't see an obvious place that wouldn't clutter the new UI which without this revision request would be lacking said clutter.. @drolando thanks for stepping up. I'll merge this, then as that will let folks building from master or using snapshots give it a try.. thanks again. @shakuzen I agree that there could be some confusion about the query aspect of this. Though it does seem to be in some means related to traceId field... that's how it made sense to me. Also, what we are optimizing for.\nFor example, by putting the concept of locally sourced (which is indeed a bit icky), into the background.. we benefit by retaining the universal search capability which is what I think we want in the foreground.\nMaybe we can down-rank the \"from json\" criteria to the bottom, or possibly quasi hide it as most users won't be using this? For example, if you \"expert mode\" or click something, then the from json option appears, but remains in a place where it is universally considered.\nwdyt?. added for 9am tokyo time monday as that's the only time reasonable and in the schedule. We can collect local feedback even if it is too early. https://cwiki.apache.org/confluence/display/ZIPKIN/2019-03-11+Tapping+into+Zipkin+data . sounds good to me. this would move trace id out if the universal search\nthen right? and the action of this form will be like old ui where it takes\nyou to expanded trace screen?\nOn Mon, Mar 11, 2019, 1:07 AM tacigar notifications@github.com wrote:\n\nPersonally I agree with @shakuzen https://github.com/shakuzen...\nI think it would be better to add another dropdown menu for traceId and\nupload JSON like the following.\n[image: dropdown]\nhttps://user-images.githubusercontent.com/19551419/54108718-1268a180-4420-11e9-961e-c6bff23bf6b6.png\n\u2014\nYou are receiving this because your review was requested.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2424#issuecomment-471440856,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61yB9xx-pEBKBMbIirnQi24XAjgeXks5vVg6ygaJpZM4bbrHk\n.\n. @zeagord wanna try this one?. thanks again for digging in! here's some baseline stuff\nhttps://github.com/openzipkin/zipkin/tree/master/benchmarks/src/main/java/zipkin2/codec\n. PS on the java 8 thing, as long as the signature isn't required, it might be ok. At any rate decoding is usually only on the read side and zipkin-collector is a java 8 library. Moreover, there's no need to use java 8 things to use bytebuffer.\n\nThe more important part would be that we may be implying a change to an interface (BytesDecoder) which would be a breaking change. If we instead do it on literally SpanBytesDecoder, we are ok. Not sure if this works for you.. I guess the other thing we can consider is .. even if it is technically a breaking change to add a method to BytesDecoder, how much surface area of break are we talking.. It would be an issue for 3rd party and also anything that wraps.\nWe could make a helper type SpanByteBufferDecoder for avoiding this issue.. it also depends on the call sites you are interested in.. if they use all of the features of BytesDecoder or only are decoding lists... another thing we can do, if the motive of this is to understand performance, is a lesser change. Add to SpanBytesDecoder an offset option ( I think @narayaruna asked for this)\nIf decoding with an offset, you can use a thread-local byte array that's sized large enough to be bigger than most, but not too large to become a problem. This isn't to eliminate the byte buffer option, but it would be a lot less work. Then, if the goal is to see how much contribution there is to latency.. you could use this rendesvous array to figure that out. If it was a lot, then you can justify more work like switching to byte buffer.\nI mean you can do more work like add byte buffer anyway, just mentioning options that are cheaper in terms of code.. another thing we can do, if the motive of this is to understand performance, is a lesser change. Add to SpanBytesDecoder an offset option ( I think @narayaruna asked for this)\nIf decoding with an offset, you can use a thread-local byte array that's sized large enough to be bigger than most, but not too large to become a problem. This isn't to eliminate the byte buffer option, but it would be a lot less work. Then, if the goal is to see how much contribution there is to latency.. you could use this rendesvous array to figure that out. If it was a lot, then you can justify more work like switching to byte buffer.\nI mean you can do more work like add byte buffer anyway, just mentioning options that are cheaper in terms of code.. thanks for raising the issue. next steps would be more interest in this feature and elaboration on what the technical impact would be. we should note that even supporting normal elasticsearch can be difficult due to the speed of changing ES vs people upgrading it.. thanks for raising the issue. next steps would be more interest in this feature and elaboration on what the technical impact would be. we should note that even supporting normal elasticsearch can be difficult due to the speed of changing ES vs people upgrading it.. going out immediately as 2.12.5. The reason for this is that some backends primary indexes of service/span\nare binary (aka no concept of case sensitivity). So this helps to ensure\nthe user knows up-front that the data will eventually become downcased.\nOtherwise, it would happen later so more surprises. Since we do aggregation\non service and span name, it is important to not miss on silly things like\ncase.\nAgreed that this could be reconsidered, as now people are using zipkin\nformat out to other systems.. We can think about relaxing this, but can't\nforget why we did it in the first place.\nAn implementation likely would not be user selection though as that is\nawkward to do inside a struct library (configuration management of a\nbuilder is weird and hard to reach). Likely, it would just be removing the\nconstraint.\nOn Sat, Mar 9, 2019 at 4:42 AM zhufeizzz notifications@github.com wrote:\n\nzipkin2.Span.Builder#name\nAll span name has forced to lowercase, and it's hard to identify.\nI think it's better to add a switch to let the user choose.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2438, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD615zCH8wWaLTcNuy4LrFtxB0zqaKBks5vU4IWgaJpZM4bmlfE\n.\n. it is probably better to use gitter for questions like this\n. it is probably better to use gitter for questions like this\n. my guess is that the driver is not putting the line in batch that had\nvalidation problem up top. the span name is 255 char I believe. you can\nchange your ddl but anyway span names that big hint of a cardinality problem\n\nOn Tue, Mar 12, 2019, 2:43 PM Daniel Andreyev notifications@github.com\nwrote:\n\nI have error:\n2019-03-12 21:32:38.727 DEBUG 1 --- [         task-5] z.s.i.ZipkinHttpCollector                : Cannot store spans [1a2e2be0635a4ac7/1a2e2be0635a4ac7] due to DataAccessException(SQL [insert ignore into zipkin_annotations (trace_id, span_id, a_key, a_type, a_timestamp, endpoint_service_name) values (1886493536654346951, 1886493536654346951, 'ss', -1, 1552426358574625, 'complaints.lmprod.media5.com')]; (conn=22) Data too long for column 'name' at row 1)\nzipkin                      |\nzipkin                      | org.jooq.exception.DataAccessException: SQL [insert ignore into zipkin_annotations (trace_id, span_id, a_key, a_type, a_timestamp, endpoint_service_name) values (1886493536654346951, 1886493536654346951, 'ss', -1, 1552426358574625, 'complaints.lmprod.media5.com')]; (conn=22) Data too long for column 'name' at row 1\nzipkin                      |   at org.jooq_3.11.9.MYSQL.debug(Unknown Source) ~[?:?]\nzipkin                      |   at org.jooq.impl.Tools.translate(Tools.java:2384) ~[jooq-3.11.9.jar!/:?]\nzipkin                      |   at org.jooq.impl.DefaultExecuteContext.sqlException(DefaultExecuteContext.java:832) ~[jooq-3.11.9.jar!/:?]\nzipkin                      |   at org.jooq.impl.BatchMultiple.execute(BatchMultiple.java:123) ~[jooq-3.11.9.jar!/:?]\nzipkin                      |   at org.jooq.impl.BatchMultiple.execute(BatchMultiple.java:75) ~[jooq-3.11.9.jar!/:?]\nzipkin                      |   at zipkin2.storage.mysql.v1.MySQLSpanConsumer$BatchInsertSpans.apply(MySQLSpanConsumer.java:167) ~[zipkin-storage-mysql-v1-2.12.5.jar!/:?]\nzipkin                      |   at zipkin2.storage.mysql.v1.MySQLSpanConsumer$BatchInsertSpans.apply(MySQLSpanConsumer.java:58) ~[zipkin-storage-mysql-v1-2.12.5.jar!/:?]\nzipkin                      |   at zipkin2.storage.mysql.v1.DataSourceCall.doExecute(DataSourceCall.java:57) ~[zipkin-storage-mysql-v1-2.12.5.jar!/:?]\nzipkin                      |   at zipkin2.storage.mysql.v1.DataSourceCall$1CallbackRunnable.run(DataSourceCall.java:69) [zipkin-storage-mysql-v1-2.12.5.jar!/:?]\nzipkin                      |   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\nzipkin                      |   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\nzipkin                      |   at java.lang.Thread.run(Thread.java:834) [?:?]\nzipkin                      | Caused by: java.sql.BatchUpdateException: (conn=22) Data too long for column 'name' at row 1\nzipkin                      |   at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:288) ~[mariadb-java-client-2.4.0.jar!/:?]\nzipkin                      |   at org.mariadb.jdbc.MariaDbStatement.executeBatch(MariaDbStatement.java:1296) ~[mariadb-java-client-2.4.0.jar!/:?]\nzipkin                      |   at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-3.3.1.jar!/:?]\nzipkin                      |   at com.zaxxer.hikari.pool.HikariProxyStatement.executeBatch(HikariProxyStatement.java) ~[HikariCP-3.3.1.jar!/:?]\nzipkin                      |   at org.jooq.tools.jdbc.DefaultStatement.executeBatch(DefaultStatement.java:100) ~[jooq-3.11.9.jar!/:?]\nzipkin                      |   at org.jooq.tools.jdbc.DefaultStatement.executeBatch(DefaultStatement.java:100) ~[jooq-3.11.9.jar!/:?]\nzipkin                      |   at org.jooq.impl.BatchMultiple.execute(BatchMultiple.java:104) ~[jooq-3.11.9.jar!/:?]\nzipkin                      |   ... 8 more\nBut zipkin_annotations there is no field name.\nWhy i see this error? How fix it?\nAfter that Zipkin UI becomes empty:\n[image: image]\nhttps://user-images.githubusercontent.com/7625387/54238215-eab13080-4528-11e9-881b-193ae5653036.png\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2440, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61whLD9TaokStm9gA5JicMR1fG51kks5vWB-bgaJpZM4br8de\n.\n. my guess is that the driver is not putting the line in batch that had\nvalidation problem up top. the span name is 255 char I believe. you can\nchange your ddl but anyway span names that big hint of a cardinality problem\n\nOn Tue, Mar 12, 2019, 2:43 PM Daniel Andreyev notifications@github.com\nwrote:\n\nI have error:\n2019-03-12 21:32:38.727 DEBUG 1 --- [         task-5] z.s.i.ZipkinHttpCollector                : Cannot store spans [1a2e2be0635a4ac7/1a2e2be0635a4ac7] due to DataAccessException(SQL [insert ignore into zipkin_annotations (trace_id, span_id, a_key, a_type, a_timestamp, endpoint_service_name) values (1886493536654346951, 1886493536654346951, 'ss', -1, 1552426358574625, 'complaints.lmprod.media5.com')]; (conn=22) Data too long for column 'name' at row 1)\nzipkin                      |\nzipkin                      | org.jooq.exception.DataAccessException: SQL [insert ignore into zipkin_annotations (trace_id, span_id, a_key, a_type, a_timestamp, endpoint_service_name) values (1886493536654346951, 1886493536654346951, 'ss', -1, 1552426358574625, 'complaints.lmprod.media5.com')]; (conn=22) Data too long for column 'name' at row 1\nzipkin                      |   at org.jooq_3.11.9.MYSQL.debug(Unknown Source) ~[?:?]\nzipkin                      |   at org.jooq.impl.Tools.translate(Tools.java:2384) ~[jooq-3.11.9.jar!/:?]\nzipkin                      |   at org.jooq.impl.DefaultExecuteContext.sqlException(DefaultExecuteContext.java:832) ~[jooq-3.11.9.jar!/:?]\nzipkin                      |   at org.jooq.impl.BatchMultiple.execute(BatchMultiple.java:123) ~[jooq-3.11.9.jar!/:?]\nzipkin                      |   at org.jooq.impl.BatchMultiple.execute(BatchMultiple.java:75) ~[jooq-3.11.9.jar!/:?]\nzipkin                      |   at zipkin2.storage.mysql.v1.MySQLSpanConsumer$BatchInsertSpans.apply(MySQLSpanConsumer.java:167) ~[zipkin-storage-mysql-v1-2.12.5.jar!/:?]\nzipkin                      |   at zipkin2.storage.mysql.v1.MySQLSpanConsumer$BatchInsertSpans.apply(MySQLSpanConsumer.java:58) ~[zipkin-storage-mysql-v1-2.12.5.jar!/:?]\nzipkin                      |   at zipkin2.storage.mysql.v1.DataSourceCall.doExecute(DataSourceCall.java:57) ~[zipkin-storage-mysql-v1-2.12.5.jar!/:?]\nzipkin                      |   at zipkin2.storage.mysql.v1.DataSourceCall$1CallbackRunnable.run(DataSourceCall.java:69) [zipkin-storage-mysql-v1-2.12.5.jar!/:?]\nzipkin                      |   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\nzipkin                      |   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\nzipkin                      |   at java.lang.Thread.run(Thread.java:834) [?:?]\nzipkin                      | Caused by: java.sql.BatchUpdateException: (conn=22) Data too long for column 'name' at row 1\nzipkin                      |   at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:288) ~[mariadb-java-client-2.4.0.jar!/:?]\nzipkin                      |   at org.mariadb.jdbc.MariaDbStatement.executeBatch(MariaDbStatement.java:1296) ~[mariadb-java-client-2.4.0.jar!/:?]\nzipkin                      |   at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-3.3.1.jar!/:?]\nzipkin                      |   at com.zaxxer.hikari.pool.HikariProxyStatement.executeBatch(HikariProxyStatement.java) ~[HikariCP-3.3.1.jar!/:?]\nzipkin                      |   at org.jooq.tools.jdbc.DefaultStatement.executeBatch(DefaultStatement.java:100) ~[jooq-3.11.9.jar!/:?]\nzipkin                      |   at org.jooq.tools.jdbc.DefaultStatement.executeBatch(DefaultStatement.java:100) ~[jooq-3.11.9.jar!/:?]\nzipkin                      |   at org.jooq.impl.BatchMultiple.execute(BatchMultiple.java:104) ~[jooq-3.11.9.jar!/:?]\nzipkin                      |   ... 8 more\nBut zipkin_annotations there is no field name.\nWhy i see this error? How fix it?\nAfter that Zipkin UI becomes empty:\n[image: image]\nhttps://user-images.githubusercontent.com/7625387/54238215-eab13080-4528-11e9-881b-193ae5653036.png\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2440, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61whLD9TaokStm9gA5JicMR1fG51kks5vWB-bgaJpZM4br8de\n.\n. @anuraaga this should make future work a bit easier. Since these are internal classes we can reshape them a bit with each pull request, taking more care with the non-internal packages.. @anuraaga this should make future work a bit easier. Since these are internal classes we can reshape them a bit with each pull request, taking more care with the non-internal packages.. running CodecBenchmarks before and after to ensure I didn't mess anything up. running CodecBenchmarks before and after to ensure I didn't mess anything up. This doesn't improve performance, and the runs are not reliable enough to say how much it slows things down. It seems to a little, but if we make a bytebuffer version and that saves allocating arrays, it would likely be worth it.\n\nbefore\nBenchmark                                        Mode  Cnt   Score   Error  Units\nCodecBenchmarks.readChineseSpan_json             avgt   15   4.738 \u00b1 0.618  us/op\nCodecBenchmarks.readChineseSpan_proto3           avgt   15   1.293 \u00b1 0.022  us/op\nCodecBenchmarks.readChineseSpan_proto3_protobuf  avgt   15   0.075 \u00b1 0.002  us/op\nCodecBenchmarks.readClientSpan_java              avgt   15   5.362 \u00b1 0.297  us/op\nCodecBenchmarks.readClientSpan_json              avgt   15   4.535 \u00b1 0.154  us/op\nCodecBenchmarks.readClientSpan_proto3            avgt   15   1.191 \u00b1 0.075  us/op\nCodecBenchmarks.readClientSpan_proto3_protobuf   avgt   15   0.070 \u00b1 0.004  us/op\nCodecBenchmarks.readTenClientSpans_json          avgt   15  34.860 \u00b1 0.463  us/op\nCodecBenchmarks.writeChineseSpan_json            avgt   15   1.053 \u00b1 0.062  us/op\nCodecBenchmarks.writeChineseSpan_proto3          avgt   15   0.849 \u00b1 0.021  us/op\nCodecBenchmarks.writeClientSpan_java             avgt   15   2.056 \u00b1 0.133  us/op\nCodecBenchmarks.writeClientSpan_json             avgt   15   0.999 \u00b1 0.024  us/op\nCodecBenchmarks.writeClientSpan_json_legacy      avgt   15   2.109 \u00b1 0.113  us/op\nCodecBenchmarks.writeClientSpan_proto3           avgt   15   0.670 \u00b1 0.015  us/op\nCodecBenchmarks.writeTenClientSpans_json         avgt   15   9.523 \u00b1 0.190  us/op\nCodecBenchmarks.writeTenClientSpans_json_legacy  avgt   15  20.119 \u00b1 0.468  us/op\nafter\nBenchmark                                        Mode  Cnt   Score   Error  Units\nCodecBenchmarks.readChineseSpan_json             avgt   15   4.395 \u00b1 0.186  us/op\nCodecBenchmarks.readChineseSpan_proto3           avgt   15   1.488 \u00b1 0.053  us/op\nCodecBenchmarks.readChineseSpan_proto3_protobuf  avgt   15   0.083 \u00b1 0.010  us/op\nCodecBenchmarks.readClientSpan_java              avgt   15   5.674 \u00b1 0.197  us/op\nCodecBenchmarks.readClientSpan_json              avgt   15   4.614 \u00b1 0.241  us/op\nCodecBenchmarks.readClientSpan_proto3            avgt   15   1.406 \u00b1 0.167  us/op\nCodecBenchmarks.readClientSpan_proto3_protobuf   avgt   15   0.076 \u00b1 0.006  us/op\nCodecBenchmarks.readTenClientSpans_json          avgt   15  35.355 \u00b1 2.559  us/op\nCodecBenchmarks.writeChineseSpan_json            avgt   15   1.039 \u00b1 0.007  us/op\nCodecBenchmarks.writeChineseSpan_proto3          avgt   15   0.846 \u00b1 0.045  us/op\nCodecBenchmarks.writeClientSpan_java             avgt   15   2.187 \u00b1 0.184  us/op\nCodecBenchmarks.writeClientSpan_json             avgt   15   1.118 \u00b1 0.125  us/op\nCodecBenchmarks.writeClientSpan_json_legacy      avgt   15   2.040 \u00b1 0.144  us/op\nCodecBenchmarks.writeClientSpan_proto3           avgt   15   0.688 \u00b1 0.034  us/op\nCodecBenchmarks.writeTenClientSpans_json         avgt   15  10.691 \u00b1 0.410  us/op\nCodecBenchmarks.writeTenClientSpans_json_legacy  avgt   15  20.426 \u00b1 3.441  us/op. This doesn't improve performance, and the runs are not reliable enough to say how much it slows things down. It seems to a little, but if we make a bytebuffer version and that saves allocating arrays, it would likely be worth it.\nbefore\nBenchmark                                        Mode  Cnt   Score   Error  Units\nCodecBenchmarks.readChineseSpan_json             avgt   15   4.738 \u00b1 0.618  us/op\nCodecBenchmarks.readChineseSpan_proto3           avgt   15   1.293 \u00b1 0.022  us/op\nCodecBenchmarks.readChineseSpan_proto3_protobuf  avgt   15   0.075 \u00b1 0.002  us/op\nCodecBenchmarks.readClientSpan_java              avgt   15   5.362 \u00b1 0.297  us/op\nCodecBenchmarks.readClientSpan_json              avgt   15   4.535 \u00b1 0.154  us/op\nCodecBenchmarks.readClientSpan_proto3            avgt   15   1.191 \u00b1 0.075  us/op\nCodecBenchmarks.readClientSpan_proto3_protobuf   avgt   15   0.070 \u00b1 0.004  us/op\nCodecBenchmarks.readTenClientSpans_json          avgt   15  34.860 \u00b1 0.463  us/op\nCodecBenchmarks.writeChineseSpan_json            avgt   15   1.053 \u00b1 0.062  us/op\nCodecBenchmarks.writeChineseSpan_proto3          avgt   15   0.849 \u00b1 0.021  us/op\nCodecBenchmarks.writeClientSpan_java             avgt   15   2.056 \u00b1 0.133  us/op\nCodecBenchmarks.writeClientSpan_json             avgt   15   0.999 \u00b1 0.024  us/op\nCodecBenchmarks.writeClientSpan_json_legacy      avgt   15   2.109 \u00b1 0.113  us/op\nCodecBenchmarks.writeClientSpan_proto3           avgt   15   0.670 \u00b1 0.015  us/op\nCodecBenchmarks.writeTenClientSpans_json         avgt   15   9.523 \u00b1 0.190  us/op\nCodecBenchmarks.writeTenClientSpans_json_legacy  avgt   15  20.119 \u00b1 0.468  us/op\nafter\nBenchmark                                        Mode  Cnt   Score   Error  Units\nCodecBenchmarks.readChineseSpan_json             avgt   15   4.395 \u00b1 0.186  us/op\nCodecBenchmarks.readChineseSpan_proto3           avgt   15   1.488 \u00b1 0.053  us/op\nCodecBenchmarks.readChineseSpan_proto3_protobuf  avgt   15   0.083 \u00b1 0.010  us/op\nCodecBenchmarks.readClientSpan_java              avgt   15   5.674 \u00b1 0.197  us/op\nCodecBenchmarks.readClientSpan_json              avgt   15   4.614 \u00b1 0.241  us/op\nCodecBenchmarks.readClientSpan_proto3            avgt   15   1.406 \u00b1 0.167  us/op\nCodecBenchmarks.readClientSpan_proto3_protobuf   avgt   15   0.076 \u00b1 0.006  us/op\nCodecBenchmarks.readTenClientSpans_json          avgt   15  35.355 \u00b1 2.559  us/op\nCodecBenchmarks.writeChineseSpan_json            avgt   15   1.039 \u00b1 0.007  us/op\nCodecBenchmarks.writeChineseSpan_proto3          avgt   15   0.846 \u00b1 0.045  us/op\nCodecBenchmarks.writeClientSpan_java             avgt   15   2.187 \u00b1 0.184  us/op\nCodecBenchmarks.writeClientSpan_json             avgt   15   1.118 \u00b1 0.125  us/op\nCodecBenchmarks.writeClientSpan_json_legacy      avgt   15   2.040 \u00b1 0.144  us/op\nCodecBenchmarks.writeClientSpan_proto3           avgt   15   0.688 \u00b1 0.034  us/op\nCodecBenchmarks.writeTenClientSpans_json         avgt   15  10.691 \u00b1 0.410  us/op\nCodecBenchmarks.writeTenClientSpans_json_legacy  avgt   15  20.426 \u00b1 3.441  us/op. cc @anuraaga @trustin the http 200 with zero data sounds dubious but not sure if that is directly from the server or from k8s.\nI will look into this tonight. hopefully can be reproducable outside k8s if only docker.\nIf anyone has armeria troubleshooting tips for dubious http responses.. like log categories etc please pass them.\n. cc @anuraaga @trustin the http 200 with zero data sounds dubious but not sure if that is directly from the server or from k8s.\nI will look into this tonight. hopefully can be reproducable outside k8s if only docker.\nIf anyone has armeria troubleshooting tips for dubious http responses.. like log categories etc please pass them.\n. the main change was switch to armeria library for http routing. I maybe\ncreated a bug here.\nI will investigate tonight and thanks for checking. it is good that it is\nreproducible outside k8s\n. the main change was switch to armeria library for http routing. I maybe\ncreated a bug here.\nI will investigate tonight and thanks for checking. it is good that it is\nreproducible outside k8s\n. verified I get the white screen using latest zipkin (built from master) just by clicking refresh cc @tacigar . verified I get the white screen using latest zipkin (built from master) just by clicking refresh cc @tacigar . it doesn't matter whether it is classic or lens btw. it doesn't matter whether it is classic or lens btw. the workaround right now is to do force reload. I'm looking into this.. the workaround right now is to do force reload. I'm looking into this.. There is a bug in cache control handling. The server should be returning 304, but it is returning 200\nEx.\n```bash\n$ curl -H'If-None-Match: \"7mC/RgQ/AWmBfiYQ\"' localhost:9411/zipkin/ -v\n   Trying ::1...\n TCP_NODELAY set\n* Connected to localhost (::1) port 9411 (#0)\n\nGET /zipkin/ HTTP/1.1\nHost: localhost:9411\nUser-Agent: curl/7.54.0\nAccept: /\nIf-None-Match: \"7mC/RgQ/AWmBfiYQ\"\n< HTTP/1.1 200 OK\n< date: Fri, 15 Mar 2019 13:21:35 GMT\n< last-modified: Fri, 15 Mar 2019 13:15:22 GMT\n< etag: \"7mC/RgQ/AWmBfiYQ\"\n< content-type: text/html\n< cache-control: max-age=60\n< content-length: 0\n< \n* Connection #0 to host localhost left intact\n```. There is a bug in cache control handling. The server should be returning 304, but it is returning 200\n\nEx.\n```bash\n$ curl -H'If-None-Match: \"7mC/RgQ/AWmBfiYQ\"' localhost:9411/zipkin/ -v\n   Trying ::1...\n TCP_NODELAY set\n* Connected to localhost (::1) port 9411 (#0)\n\nGET /zipkin/ HTTP/1.1\nHost: localhost:9411\nUser-Agent: curl/7.54.0\nAccept: /\nIf-None-Match: \"7mC/RgQ/AWmBfiYQ\"\n< HTTP/1.1 200 OK\n< date: Fri, 15 Mar 2019 13:21:35 GMT\n< last-modified: Fri, 15 Mar 2019 13:15:22 GMT\n< etag: \"7mC/RgQ/AWmBfiYQ\"\n< content-type: text/html\n< cache-control: max-age=60\n< content-length: 0\n< \n* Connection #0 to host localhost left intact\n```. I think I found the bug. I think I found the bug. https://github.com/openzipkin/zipkin/pull/2445 will fix this. https://github.com/openzipkin/zipkin/pull/2445 will fix this. 2.12.6 on the way. thanks for the bug reports and sorry I made this bug. 2.12.6 on the way. thanks for the bug reports and sorry I made this bug. ok I verified it is good with docker now. ok I verified it is good with docker now. good to hear @msmsimondean !. good to hear @msmsimondean !. seems duplicate of #2443\n\nwill investigate tonight thank you for reporting. seems duplicate of #2443\nwill investigate tonight thank you for reporting. wrt testing yep. I can do it on the plane tomorrow as it will take more than a few minutes . wrt testing yep. I can do it on the plane tomorrow as it will take more than a few minutes . will put in the integration tests during my flight. see ya. will put in the integration tests during my flight. see ya. 2.12.6 on the way. 2.12.6 on the way. thanks! please add the \"how it works\" image to the README at the bottom under a markdown section named design or a separate DESIGN.md. thanks! please add the \"how it works\" image to the README at the bottom under a markdown section named design or a separate DESIGN.md. note: the view trace screen blows up (white screen) if using v1 json\n```\nTypeError: \"e is undefined\"\ngetServiceNameColor http://localhost:9411/zipkin/app-d005bdc780c99e8fec7a.min.js:12 value http://localhost:9411/zipkin/app-d005bdc780c99e8fec7a.min.js:88 value http://localhost:9411/zipkin/app-d005bdc780c99e8fec7a.min.js:88\n\n```\nIn the old uploadTrace.js we had something like this. If you import it, please also import the unit tests.\njavascript\nexport function ensureV2(trace) {\n  if (!Array.isArray(trace) || trace.length === 0) {\n    throw new Error('input is not a list');\n  }\n  const first = trace[0];\n  if (!first.traceId || !first.id) {\n    throw new Error('List<Span> implies at least traceId and id fields');\n  }\n  if (first.binaryAnnotations || (!first.localEndpoint && !first.remoteEndpoint && !first.tags)) {\n    throw new Error(\n      'v1 format is not supported. For help, contact https://gitter.im/openzipkin/zipkin');\n  }\n}. note: the view trace screen blows up (white screen) if using v1 json\n```\nTypeError: \"e is undefined\"\ngetServiceNameColor http://localhost:9411/zipkin/app-d005bdc780c99e8fec7a.min.js:12 value http://localhost:9411/zipkin/app-d005bdc780c99e8fec7a.min.js:88 value http://localhost:9411/zipkin/app-d005bdc780c99e8fec7a.min.js:88\n\n```\nIn the old uploadTrace.js we had something like this. If you import it, please also import the unit tests.\njavascript\nexport function ensureV2(trace) {\n  if (!Array.isArray(trace) || trace.length === 0) {\n    throw new Error('input is not a list');\n  }\n  const first = trace[0];\n  if (!first.traceId || !first.id) {\n    throw new Error('List<Span> implies at least traceId and id fields');\n  }\n  if (first.binaryAnnotations || (!first.localEndpoint && !first.remoteEndpoint && !first.tags)) {\n    throw new Error(\n      'v1 format is not supported. For help, contact https://gitter.im/openzipkin/zipkin');\n  }\n}. Looks good besides the blowup. I tested uploading a v2 json and also get trace by ID. Looks good besides the blowup. I tested uploading a v2 json and also get trace by ID. goodie!. KafkaConsumer is in a try-finally.. what more correct way of closing are you suggesting? cc @jeqo \njava\n  @Override\n  public void run() {\n    try (KafkaConsumer kafkaConsumer = new KafkaConsumer<>(properties)) {. KafkaConsumer is in a try-finally.. what more correct way of closing are you suggesting? cc @jeqo \njava\n  @Override\n  public void run() {\n    try (KafkaConsumer kafkaConsumer = new KafkaConsumer<>(properties)) {. I'm guessing you mean to share some signal (like atomic boolean isRunning) such that when KafkaCollector.close() is called it trips some conditional the poll loop would read and then exit vs using interrupt. Is that right?. I'm guessing you mean to share some signal (like atomic boolean isRunning) such that when KafkaCollector.close() is called it trips some conditional the poll loop would read and then exit vs using interrupt. Is that right?. PS I agree that the error message is distracting and it would be a better idea to close differently than to try to change that logging category. PS I agree that the error message is distracting and it would be a better idea to close differently than to try to change that logging category. good points all around. Are you up to raising a change for the things found on this issue?. good points all around. Are you up to raising a change for the things found on this issue?. ps I checked Gerald already has an ASF iCLA http://people.apache.org/unlistedclas.html\nWhile we haven't yet moved this repo, it will soon.. ps I checked Gerald already has an ASF iCLA http://people.apache.org/unlistedclas.html\nWhile we haven't yet moved this repo, it will soon.. It is better to send in v2 format, and duration explicitly vs derived. duration can be written again, but be careful to use correct instrumentation. For example, a shared field needs to be set if there is a server span re-using the same ID as the client. Otherwise duration precedence on merge will get messed up.. it is best to not report duration unless you know what it is. why is it that you are sending a tentative duration?. so not send duration meaning leave absent (zero is often coersed to absent as we define minimal duration of 1 due to nuance on this point). so the problem is that certain tools will not expect varying duration as the general span contract is you start it, you finish it (once). Things like duration query won't work right when the value changes. Most stores are document oriented as opposed to SQL upsert. Readback might be ok. At any rate, problems like this are why we added the \"shared\" flag to the server span, so that it could be ignored when computing the client duration.. my guess is that what you are experiencing is tension on span model as what you are modeling doesn't seem to have a defined start and finish. It might be helpful to be more concrete about what's going on as perhaps there are ways to model it differently.. not sure. there are some things that don't work well with span model, like chat sessions.. yes you can send any fields late except trace id/span id. We did this for the use case of in-flight data that isn't complete actually :) before we had to play games with fields initialized to empty string.. I should clarify it is more efficient to send as much of it at the same time (ideally complete). anyway hope this answers the question.. @AceHack no backend requires duration, though display won't look great lacking one.\ntimestamp is important though.. please send that if you can. It is also in some aggregation cases helpful. meaning if you have to send duration late, it can help some tooling to send timestamp in the same doc.. PS I will add a unit test referencing this issue to ensure late duration is ok. @AceHack usually I would have punted to gitter by now but I thought we were at the end of the duration dillemma :) can we chase up other questions on gitter as this is starting to spam the repo https://gitter.im/openzipkin/zipkin. cc @openzipkin/armeria in case y'all have insight into this. This started after the switcheroo and maybe there's an easy explanation/setting somewhere. generally, I'd scrub comments like this. leave it to git log\n. nit: I don't think your change should impact copyrights. Typically projects only affect these when adding new files.\n. @mosesn regardless of the value of the constant, do you agree with this change?\n. dependency not configured.. fixing before merge\n. I think I get what you are doing here (guard on no last annotation), but it would be more clear if there was a test.\n. added via 31a8ef65c4153795d07f39944e191e1756aa70c9\n. > maybe just say queryLimit() instead of queryLimit.apply()?\n\nthx!\n. done\n. @abesto everything above the kafka-junit line should be done to the build regardless of my change\n. let's keep this for now, as it is progress.. thanks!\n. this dance is to ensure the rule is a singleton.\n. roger that!\n. done\n. thank @charithe!\n. this was needed for my mac's JDK 8\n. can try that now.. I basically just ported the prior config.\n. that syntax doesn't work, but it also doesn't work for unrelated projects. I'm hesitant to jump down more build areas, but if you think this is important let's at least log an issue.\n\nDoesn't seem related, or a feature we use in travis or the release process. That's why I think we can separate out \"sbt project X\" from this change, and from the general work on getting the build green.\n. running assembly now.\n+1 to having some sort of automated test for this!\n. Let me verify, but I think they are default. If not I will update. Thx!\n. Typesafe isn't needed. Jcenter isn't either except for \"performance\", but I\nthink it would come last so wouldn't help there anyway. Resolver order is\nhard to control unless you set a JVM arg to override. I recommend leaving\nthis as is until gradle.. SG?\n. Scratch that... Something is falling through to the Twitter repo.. I will\nlook into this!\n```\n[error] SERVER ERROR: Service Temporarily Unavailable url=\nhttp://maven.twttr.com/org/slf4j/slf4j-parent/1.6.4/slf4j-parent-1.6.4.jar\n``\n. supernit: can you get rid of the fuzz here? looks like the only needed one is the last few lines.\n.publish-localdoes this right?\n. rebase against master, as we have this, now\n. remove this as it is redundant.. I think all you need is bintray-sbt..\n. great comment!\n. This should stay snapshot. The release number should be taken from the tag name.\n. maybe use something more like the if statement here, as it emits what's going on? https://github.com/Netflix/feign/blob/master/installViaTravis.sh\n. or here https://github.com/Netflix/feign/blob/master/buildViaTravis.sh\n. I think this is where you want to parse the tag to get the version number, or use a plugin that does that for you.\n. adding a common dep here is not so hot imho, as we already have a lot of coupling. Can you just add RunWith to all the tests? Then, in the main build file, put a TODO to replace boilerplate@RunWith(classOf[JUnitRunner])` with getting scalatest plugin working?\n. PS I would help with pulling this out.. just don't want anything else depending on common than absolutely must :P\n. Oh right.. This is a shared branch! OK I will help today but will be away\nuntil Tuesday am Pacific. (FYI)\n. keyword retry! so whenever maven.twttr.com fails (and yes.. it does often).. we might actually be able to proceed.\n. not easy to find, but yeah 3 tries\nhttp://docs.travis-ci.com/user/build-timeouts/\n. TL;DR; I really prefer these being declared where used, as it highlights tech debt without pushing it to all users. Our build files should be documented well enough to help someone understand why repositories are there.\nThis isn't special to gradle.. I've noticed that the normal finagle thing requires you to define repositories here or there (ex. like zipkin itself needing to track thrift 0.5.0-1). I've been troubled by this topic. It is a usual concern to have to hunt down repos to get twittery things working, particularly maven.twttr.biz. However, at least inside twitter, there's a move towards all things are sonatype -> maven central. In other words, the extra repos, at least some of them, should become less burdensome. Also, we should consider that in some cases, the extraneous repo definition is at the edge of the graph (ex zipkin-cassandra). Those who need to redefine edges are power users already, or a symptom of a config problem. Power users should be able to read our well-commented build files to get what they need. Config problems should be addressed. Plopping a large list or repositories seems a short-cut, but I'm generally uncomfortable with making things that make tech debt creation or retention easier.\n. I agree with tyler that we should go with conventional first. Looking back at SBT problems we've had, I don't want to do anything that would cause confusion for future maintainers.\nIn other words, if the guard (if-statement) is functional, then comment when it can be removed, hopefully via a github issue being resolved.\n. In cases that we need to switch on scala version, we should be able to do if statements.\nhttps://github.com/apache/kafka/blob/trunk/build.gradle#L237\nI'd avoid making new words in general. Ex. the following won't invite other gradle folks to maintain this even if we are saving characters. We should just declare inline, and avoid special build language like mkDepGen as zipkin is hopefully not special enough to need its own build vocab. I'd rather optimize for immediate understanding vs saving character count. DRY is something to watch out for, but build files and unit tests have special concerns which IMHO override that in many cases.\nOn technical notes.. if the goal is to force a specific version for twitter things, I would do that in a way that is more idiomatic to gradle. ex.\nconfigurations.all {\n    resolutionStrategy {\n        eachDependency { DependencyResolveDetails details ->\n            if (details.requested.group == 'com.twitter:finagle-foo') {\n                details.useVersion \"6.26.0\"\n            }\n        }\n    }\n}\n. I have projects that put nothing into gradle.properties, so not sure if\nthat's best. Rewriting is only important when you need to force a version,\nso I agree it isn't natural for many things.\n. maybe explain this a bit more in the comment. Ex. what's expecting a property of this naming convention? when is it loaded (during tests?) how is it correlated to the anormDriverDependencies below (if at all)?\n. sorry about the import change.. the amount of unused ones was driving me nuts. I don't feel like making a pull request just to do that.\n. Somewhere else in this file, this content is basically the result of an InputStream read into a byte array. Is there a shortcut to create a shareable Buf from an InputStream directly?\n. have to admit that this is more complex than the existing code.. I think I'll leave it.\n. done. thanks!\n. yep syntax is bin/collector cassandra\n. Good point. Will use gradle.properties to default and -P to override.\nNullification concern from bash is still there, but at least the change is\nmore idiomatic. Thanks!\n. we don't support on JRE6 anyway..\n. ps. this is the 'ah-hah!' which was screwing up collector vs query. Basically each was creating their own DB files.\n. I took this out, as the simplest start is to use SQLite (as it doesn't need an external process running). Ack that until tonight, that didn't work :)\n. bin/collector-redis has the same character count as ./bin/collector redis :p\n. this isn't something we should host\n. SGTM\n. so the root build directory isn't created by default.. I'll have to make sure it is.\n. best way out was to point query to collector's build dir when configKey == \"dev\". The fundamental relationship is the coupling of these two processes, not to the root project. Ends up less work, too.\n. I bet @dsyer loves the fact we have to do this :)\n. this project doesn't exist\n. unrelated, just didn't feel like opening a PR just to do this.\n. square is a rectangle, but not every rectangle is a square\n. forgot this last time\n. closer is baked into App, and therefore TwitterServer\n. I like the formatting etc, but not sure this is compiled.. maybe move to notes in ZipkinBlockingQueuedCollectorFactory's scaladoc? It might also be a decent idea to make this default behavior on collector...\n. dead code\n. little things like this and a bunch of others means there are no more compiler warnings on this project\n. not sure why all of these were lazy. might un-lazy them in a later change.\n. didn't need to rewrite this class.. I just formatted it.\n. once this file is dropped.. ready to merge (on my side anyway)\n. for JDK 7 to work, replace this with new ArrayList<ByteBuffer>()\n. rename to CassandraSpanStoreFactory\n. Travis builds w/JDK7 and 8. JDK7 is particularly important to retain due to\ncurrent scala version 2.10\n. currently trying to work around our famous org.apache.thrift:libthrift:0.5.0-1 vs 0.9.2 runtime compat story.\n. noticed this bug lying around..\n. This is the last type in redis that references Storage or Index!\n. I'd inline the lambdas, as the val declarations above make reading this complicated.\nsomething like\nscala\nassertEventually(() => stats.counter(\"queueFull\")() >= 1)\n. val expectedItemCount = 12\n. remove as this is redundant\n. The combination of the above 2 asserts are confusing... the queue has a size zero and it is full?\n. does this need to use futures? why not just loop?\n. yeah was thinking like for (1 to count) if (foo.apply()) return true; else sleep 1000 in scala. The thread stuff is distracting.\n. Yeah, it is shoe-horning. Migrating zipkin-collector-server and zipkin-query-server off ostrich is yet another todo :) Where to, is yet another question! finatra, finch etc.\n. never used\n. only used once\n. not necessary to await as the spanstore doesn't have an initialization routine.\n. this one was easy.\n. we shouldn't take on random responsibilities such as registering arbitrary services.\n. we shouldn't take on random responsibilities such as registering arbitrary services.\n. was implicit in serverset's dep tree\n. this file was incorrectly named AdaptiveSamplerTest before. It is just a rename, but git didn't understand.\n. this is just folding in the ZK trait. I removed the trait as we don't want to encourage its use.\n. this sounds lovely\n. I've found most project avoid declaring constants just to snake case words in the same file. Seems the only below that isn't just a lower-snake is ts, but we use ts often in the codebase. By removing these, the file is less arduous to navigate.\n. whoot\n. Easiest way our would be to add doc to base span store, say it is\neventually consistent, and change the base test. I think many ops will end\nup eventually consistent anyways. Regardless, I'm fine with this.\n. nit add link https://travis-ci.org/openzipkin/zipkin\nnit s/version you're tagging/commit you're tagging/\n. I like how you've extracted this. In case someone knows a better way, they can replace it easily. FWIW this looks good to me.\n. this is a little complex.. why not just split on comma, then pass that directly to .addContactPoints(contactPoints:_*)\n. guessing this is to support port overrides... nm\n. beautiful!\n. do we need this flag? could we instead use existence of user to accomplish the same?\n. rename to getField? also no need to declare vals.. getField(authProvider, \"username\") should be(empty)\np.s. I'd be surprised if there was no reflection-based matcher, but I'm often surprised.\n. I dug through the code a bit.. \nshould be able to do this instead. TestFactory.createClusterBuilder().getConfiguration(). getAuthProvider()\ncast as PlainTextAuthenticator then getCredentials() should match.\nThat's more refactor save and doesn't use reflection. If doesn't work, lemme know!\n. it isn't typical, so I'd remove it. saying I want to set a username but not authenticate doesn't make sense.\n. yep and your base case test can simply set no flags and check the type of this.\n. I wouldn't do the getOrElse things unless \"cassandra/cassandra\" is a builtin-user/password? Even then, I'd have people set it.\niotw use sys.env.get(\"CASSANDRA_USER\").isSet or whatever to determine if the flags are set as we aren't using default values for this.\n. nit.. use the character form of 'u', 's', ... that way you don't have to explain what 117 is\n. we should make this today :) @jamescway mind?\n. I still think this is an awkward use of optional. instead of getOrElse(\"\"), you could leave that off and check isPresent or whatever below.\n. I'd just set flags the same we do in the unit tests. Forgive the quasi-scala.\n```\nargs = mutableList()\nval user: String = sys.env.get(\"CASSANDRA_USER\")\nval pass: String = sys.env.get(\"CASSANDRA_PASS\")\nif (user.isSet && pass.isSet) {\nargs.add(\"-zipkin.store.cassandra.user\").add(user.get)\nargs.add(\"-zipkin.store.cassandra.password\").add(password.get)\n}\n// similar for other args.\nTestFactory.nonExitingMain(args.toArray)\nval cluster = TestFactory.createClusterBuilder().build()\n```\n. I think you want to take out the default of \"\"? iirc this param should just be left out to make optional.\n. I think this would be clearer to first initalize, then guard entry to withCredentials.\nex. \nval builder = addContactPoint(Cluster.builder())\nif(cassandraUser.isDefined && cassandraPassword.isDefined) {\n  builder.withCredentials(cassandraUser(), cassandraPassword())\n}\n. reworded logic to read better, too.\n. I like where you're going, but I think it would be clearer to just check it is AuthProvider.NONE?\nTestFactory.createClusterBuilder()\n        .getConfiguration()\n        .getProtocolOptions()\n        .getAuthProvider() must be (AuthProvider.NONE)\n. well cool thing is that we can now toy with it! Ex. we can put in another row for actually running the gradle daemon, or only skipping it on a jdk it actually breaks on. I'm soooo excited :P\n. this is also very non-core :)\n. there was some weird thing once upon a time that forced us to use commas instead of multiple lines of include. Maybe it was intellij?\nProbably what we want to do is make a list, and then condtionally add to the end of it. After that, \"include\" that list.\n. Fwiw I think most of the span stores accept a TTL, which not sure if the\ngranularity is right. That said, James raises a good point. We don't have a\ndynamic config story. At least having it statically configured (similar to\ndefault ttl in SpanStore) would seem a good start.\n. spelling\n. @michaelsembwever note.. this is no longer needed\n. @michaelsembwever fyi this no longer has a caller, so it and its supporting things are nuked.\n. only user was obsolete query methods.\n. seems it wasn't convenient enough for the UI to use it!\n. wasn't actually used by spanstore\n. well, the only caller of this was web. and thrift will ignore fields it doesn't know about (ex. field 7 below).\nAn experiment to prove this would be to run bin/query and bin/collector from master, and then run bin/web from tag 1.2.2.\n. After I rebase, I'll double-check again.\n. will change this to a comment so that we don't re-use ordinal 7\n. this and below it silently failed based on trying to have a timestamp comparison with zero!\n. this is the big one.\n. Actually we can't because the method scrooge uses is removed in 0.9.1\n(unless it was too late last night)\n. this will move to the web package when we remove the obsolete thrift methods.\n. mostly indent from extracting from getTraceIds\n. this is the only actual change\n. removed receiver-kafka as it wasn't used. can be added back in the future.\n. was this change intentional?\n. nice catch!\n. yay less deps!\n. :eyes: \n. sure thing\n. done\n. not sure we should be doing this in a tight loop. when you are finished testing, mind removing the stats?\n. these did nothing.. we can re-introduce them when that's not the case\n. Short answer is yes! I have a different impl which I will push when I get\nconnected later.\n. this was just wrong\n. fancy syntax compliments of @kristofa!\n. This is the best way to make sure \"all tracing\" gets affected. Otherwise, it is whack-a-mole where some use builders and others Thrift/Httpx.foo\n. missing .getHostAddress, but also awkard way to set the value.. below is less awkward\n. I thought about that, but I think I'm out of scala.. is there a non-awkward way to look peek if the cause is CancelledRequestException using case?\n. Thanks for the help!\nhttps://github.com/openzipkin/zipkin/commit/6702ab104c7edc40636408243bc6c06f0ac0fd55\n. @yurishkuro here's what to reproduce in custom code\n. It is optional, so the reader will skip it.. One good thing about thrift\nand protobuf ;) that said, I should have left it commented out so we don't\nre-use that number for a different purpose. Will add that.\n. https://github.com/openzipkin/zipkin/commit/daebe8eb843d942bd49eed6f80889cd178e8384d\n. bumped as this is a schema affecting change\n. this thing was the reason it took me so long to finish this. eventually, I figured out why everything was flooring to a day!\n. because the thrift service definition change. eventhough we soon will move this out of thrift.\n. total lie\n. linked hashmap retains order\n. this broke javac 7\n. orphaned file\n. orphaned file\n. orphaned file\n. The duties of this class are too simple to depend on algebird. Algebird shouldn't be a core dep, and now isn't.\n. @yurishkuro so api/v1 is basically moving the existing functionality over to a more supportable framework. I don't think we are ready to do multiple protocols as there's still lots of tech debt left (ex there's still ostrich in the all of the services, and the config story is awful). My take is that get the project into good shape before going towards flexibility. Ex. let's prove we can cleanly swap storage or the known listener concerns (ex kafka vs scribe) before we start optimizing for multiple protocols for a query server.\n. All that said.. I agree the code isn't well organized, so feel free to send a refactor idea/PR to help with this.. I'd love to have help on the tech debt side.\n. yeah this is simpler than depending on the bijection library\n. This was making explicit what the previous value was. Neat, huh?\n. we can try :)\n. lets call this servicesMaxAge - much shorter and the max-age concept in http caching is already seconds. sg?\n. actually I think this should move to the query server. You already have a response object, so it is less code. Then all you need to do is forward the request, like we do with /api/spans.\nget(\"/api/v1/services\") { request: Request =>\n    response.ok(spanStore.getAllServiceNames()).cacheControl(servicesMaxAge)\n  }\n. plus.. you can actually test it there. ex. in ZipkinQueryServerFeatureTest you can look for the default cache control header.\n. this wasn't a bug, just wanted to be consistent\n. good point.\n. the size this was a carry-over from an old impl.. the size is always 1.\n. @yurishkuro here's some clarity about the span merging\n. bug\n. the vals weren't used, as these were producers for dependency injection. Not assigning vals means they don't conflict with other vals when mixed in.\n. goofy and completely unnecessary. Many indexes cannot be configured to exclude certain words, so this is essentially busy-work to enforce.\n. look. now we have actual error messages!\n. the confusingly named minStartTime and maxStartTime were never used\n. @jfeltesse-mdsol bumping the version as there's a schema change. Now's a good time to get docker working, since the schema can now support all functions in zipkin\n. quietly failing to zero at this level of abstraction is a timebomb\n. it is easier to completely replace the codec system than properly work around this in the version of jackson we are using.\n. something magical happened where after I added the scrooge dep to the query project, this started to fail compile unless I added a type hint\n. this type of bug was easy as Span lied about it not being optional\n. good riddance crazy code\n. pet peeve\n. Well a new model could choose to do that, but this form has been in place\nfor a couple years. Main thing is documenting what it is already.\n. Sounds good.\n. PS if in a new model we made endpoint a value type, we might get away with\nnot logging it as \"host\" all the time. I suspect the original decision of\nCA/SA as a marker annotation was because the value are thrift primitive\ntypes, not structs.\nAlso, by doing a marker type the normal\nannotation.host/binaryannotation.host fields and related logic could be\nreused.\nRegardless, this is a great idea to bear in mind for model 2.0. I like the\nidea of logging the protocol and also revisiting to not constrain to ipv4.\n. done\n. done\n. moved the long discussion here: https://docs.google.com/document/d/1ixxEs9TvhiGjJObGbRSPhSna3zHdadoUTQIZ5JKgLzU/edit#\nAdded more clarification in the thrift doc about why this is important, hopefully not too much explaining..\n. not used\n. PS I don't see this as complicating, particularly as I've touched all the\ncode in zipkin. The current codebase is a mess as it has implicit logic\naround things like startTs and duration. This formalizes it, making it\npossible to create an un-hairy implementation.\n. fair enough.\n. done\n. this changes from mutable to immutable list\n. @dsyer @spencergibb fyi, this is good because it will break the zipkin-java project. I've a partial java impl of clock skew correction somewhere, and will finish that or ignore the test when bumping.\n. well, you do in order to query. Ex. the dependencies query is strictly bound by time, right?\n. this logic was moved. The change for this to become duration based is in another PR.\n. span.duration I mean.. anyway I agree it is weird and will move the logic from that branch here.\n. Well, it is bucketed by day in the old job, but anyways I agree\ndependencies is something that is easier w/ storage api's directly.\n. clarified the comment, which explains why I think a flag would be instant tech-debt and not actually solve anything.\n. moved the code.. it looks terrible in scala (bc these are options)\n. @yurishkuro this is the best I could do with scala.\nFeel free to offer another solution post-merge\nI'm suggesting post-merge because my experience with people's suggestions on scala is that they don't work. After this is merged, we have tests that will pass or fail based on an optimized solution as opposed to what I'm usually subjected to, which is \"try this!\" where \"this\" doesn't work for some reason.\n. sure.. can bump!\n. https://github.com/openzipkin/zipkin/pull/822\n. Thanks for looking deep enough to have questions like this!\n\nQ1 - if I am reading sql query correctly, the duration limit applies to individual span duration, not the overall trace duration, is that correct?\n\nSince the query only applies to root spans, it is essentially the same as\ntrace. There's no trace table in SQL as a trace is just where id = traceid\n\nQ2 - what are the semantics of serviceName here? Any service involved in a span satisfying the duration limit? What about services involved in a long trace, but not in the long span in that trace?\n\nSo this doesn't constrain the search to only service names in the root\nspan. It applies to any service in the trace, at the moment (I think).\nProbably we should make a test about this nuance. What's your goal? Limit\nto service names only in the root span?\nKeep in mind that other queries act similarly, ex annotation query isn't\nbound to only annotations from that service iirc. In other words, this may\nbe a bigger issue.\n\nQ3 - shouldn't we include span name in the search?\nYou are only seeing a partial query. The scala lib does this thing called\nquery slice. I don't like that but don't have time to rewrite all the\nbackends. This is part of a deprecated class.\n\nEx in the java port, you'd see this implemented differently (still passing\ntests). My hope is that we delete anorm once the java port has a release.\nCassandra should write directly to SpanStore as opposed to via the\ndeprecated SliceQuery approach. Would love your help with that.\n. PS I may be totally wrong on interpreting how the queries work in relation\nto service name. If SpanStoreSpec tests don't answer these questions, we\nneed to make them.\n. I will add tests tonight or tomorrow, basically load 2 traces. One has root\nspan with service A calling child span with service B. The other is visa\nversa.\nI suspect the intended behavior is that one returns, not both!\n. OK so that basically means remove the root span condition. Makes sense.\nPull request on the way!\n. this doc was a combination of obvious and obsolete\n. @yurishkuro this was a bug!! maybe related to your inability to see spans. pushing a fix now.\n. I'd not expect anyone to recognize this. Please just throw IllegalStateException vs declaring single-use exceptions.\n. rename this flag to be same depth as others.. zipkin.store.cassandra.ensureSchema\n. literally same SQL as the name filter earlier in the file\n. you'll need to insert this add this at line 4 to resolve 10.minutes\nscala\nimport com.twitter.conversions.time._\n. @danchia you mean \"require\" instead of this? mind giving it a go? Note, we do this a few places, so you could do a reference hunt for Util.checkArgument\n. cool! first use of java 8!\n. is there a space or performance advantage to using single-character fields in cassandra?\n. supernit, but usually I inline doc like this: /** Returns a map of trace id -> timestamp */\n. this can be addressed later, but sometimes spans come in as \"\" or \"unknown\" and later merge with a valid span name. Does this impact anything?\n. neat\n. :)\n. all of these keys are different than the prior ones in the same file, which could lead to confusion. I'd suggest the following, if this is ideal.\n1. add a line noting why single-character preference is used, as it obfuscates the schema. In other words, make it obvious why we are being cryptic (ex for some performance reason?)\n2. add a TODO: to change the other schema to the same naming convention.\n. OK mind just noting this as a schema declaration comment? Would help answer\nfuture questions.\n. thanks!\n. typo\n. missing\n. missing from prior commit!\n. If you want to change this, we should log an issue because all core instrumentation currently set  traceid = spanid for root spans (ex finagle, ruby tracer, brave, as well zipkin has logic around this)\nBasically, I'm not opposed to changing this, but it is in fact a change, and likely has impact. I don't want to document something that isn't status quo and also breaks things.\n. @yurishkuro ps I finally hunted down what that endian stuff was about!! little-endian wasn't added until thrift 0.9.3!\n. nit: I'd call this apache as it is a bolder highlight when not 2.0\n. is there a way to apply this only to compile deps? I'm concerned w/ GPL here\n. minor bump\n. dead code\n. it is possible for someone to make their own query-postgresql.sql which has these contents. However, I hope that we have a release of the java project soon, which is more configurable, and obviates anorm.\n. dead link\n. I agree that if we are making awkward decisions based on size, these tests\nshould be transparent because there are a bunch of ways we can screw that\nup without considering character length.\nPassing thought, but right now Travis tests don't pass on current impl\nanyway. Should we have another pass, addressing cryptic names and improving\nperformance?\n. remove this now :)\n. remove\n. remove\n. remove\n. guess we need to do something to fix this? suppose changing the jar plugin to add a license file?\n. some jars like this are very surprising...\n. wat?! we have an alpha zk dep?! lovely\n. Thx for logging this\n. nice comment!\n. nit: fuzz\n. I prefer we leave the serialization hack for now, as there's a comment\nwhere this goes away in datastax 3 (as you can use long directly)\n. ps we want this because the current datastax client over-eagerly wants a j.u.Date type. So it complains if you serialize as long until you can control this in v3.\n. ps issue resolved via https://github.com/openzipkin/zipkin/pull/861 thx @yurishkuro \n. neat trick!\n. don't need to manually update the JDK, since the dist is quite new\n. this found a bug!\n. https://github.com/openzipkin/zipkin/pull/879 thanks for reporting!\n. no change except indent\n. classpath isn't mutable, so caching is effectively always on, now, as that's the mustache default.\n. it does now :P\n. I'd say use this or remove it. This was used to coerce empty string to unknown.\n. higher in the file, on the serviceName field, I'd add a note that \"\" implies the span won't be indexed. (or similar succinct thing)\n. interesting use case.. basically this is when you know the IP/port, but not its name (pretty common, esp w/ X-Forwarded-For)\n. good revision.. I agree that when something isn't reliable, empty is better than pasting \"unknown\". For example, \"\" means.. wait a bit longer.. this will clear up! vs \"unknown\" saying \"this may be as good as it gets\"\n. by contract, this shouldn't return null (as we shouldn't return spans w/o a service name) Any reason for this change?\n. guess what I'm wondering is ...\n- if we known we won't return spans who have no service name..\n- and we fix prioritization to deprioritize empty string..\nwouldn't it be a bug for this to be null (or empty)?\n. excellent point. Trace.serviceName shouldn't ever be empty, but this could.\n. it is converted before this, so if not an int, would bomb before here.\n. hah extremely good point! will change all these call-sites\n. This test is subtly different than before? The former test took the inputs derived from the trace, and now they are constants, and for example, not using the lookback arg anymore. That means the test might be passing for the wrong reasons. (maybe it is fine, but it changed a lot, not only the trace timestamps, but the test inputs too)\nCan we use the same semantic inputs, even if they are extracted to locals for readability\n. overall question. Are we saying that the spark job cannot aggregate data earlier than today? One possibly un-obvious aspect of this test is that it is explicitly older than today. Ex. it is easy to accidentally create an impl that cannot re-process historical spans.\nIf these stamps were redone to be \"last week\" or \"3 days ago\" I think it could call out the intent more explicitly.\n. I'm sure that the spark job can be written in a way that it can retain the\ntimestamps and that we can also find a way to allow it to run more flexibly.\nFor example, to test the spark job at all required refactoring. I know\nbecause I did that work. The current form of the spark job is in no way\ngood enough to dictate changes here.\nI'm quite sure you can open issues and find other defences that would\nsupport dumbing down this test to allow the current spark job to pass.\nI think it is a better choice to allow the spark job to become better and\npass tests on its own virtue rather than changing the tests so they pass as\nis.\nFor example, this test we are talking about directly supports the current\npull request to support a different endTs.\nThe back and forth with you is more effort than fixing the code\n. I will raise an PR to make the date range concern a separate one and more\nexplicit. This might result in another test to skip, but at least it will\nisolate the topic to a specific test. Even if that test also needs to be\nskipped, I hope this is progress.\n. Hopefully this addresses the concerns you had. As long as this passes,\nor is explicitly skipped, I don't mind if the other test input\nparameters are changed.\nhttps://github.com/openzipkin/zipkin/pull/922\n. I am happy about the red lines!\n. Add a comment might explain your intent. Ex. // Intermediate local spans are not service calls and so shouldn't increase a dependency links call count\n. Which store specific feature does this change imply?\n. I think I understand, though maybe not share that capabilities api is\nalways the answer to this problem. At the moment, this repo is the master,\nand implementations follow that or skip test.\nDivergence within zipkin has so far been transient, ex people not making\ntime to adjust leaf implementations.\nCapabilities api is a better solution for things that are relatively\npermanent, and notable enough to justify the complexity of such an api, and\nmoreover actionable outside tests (ex users actually invoke the api).\nIt isn't even needed to share these scala interfaces, it is just a\nconvenience. Ex java doesn't use these at all except to run tests.\nSimilarly if you wrote a python dependency store you wouldn't be calling a\ncapabilities api written in scala.\nMy 2p is I wouldn't write a capabilities api only to drive tests.. Ignoring\ntests that don't pass and keeping issues or README is more appropriate and\nbetter use of the limited time.\nIOTW, there's nothing about this change that leads directly to a\ncapabilities api. We may just feel differently about that.\n. > Another demand for capability API is constructing switches in UI such as\n\ncounting failed traces, advanced query condition, tenary call counts and\netc.\nSo a HTTP API is potentially required but not yet\nThis is the golden point.. The actionable api is the http api, as this is\nthe only way trace or dependency elements receive data. Maybe you can\ncontribute thoughts here, yuki\nhttps://docs.google.com/document/d/1ixxEs9TvhiGjJObGbRSPhSna3zHdadoUTQIZ5JKgLzU/edit\n. hah! didn't know that one\n. this looks backwards, but it is actually correct as the derived duration isn't present when there are no annotations\n. removed via https://github.com/openzipkin/zipkin/commit/08bd0ee6d95ced8c9f6f11c24269b64e887bc62a\n\nthx for thinking this through!\n. removed the copy/pasting here as it is subject to drift (and unlike other READMEs)\n. we don't have a great way of testing env variables at the moment, so I tested this via a debugger\n. Hmm the original complaint was about the remote pool. Do we want two\nvariables, then?\n. great question @drax68 care to chime in?\n. I'll make an attempt at documenting ^^ in a followup. thanks for the explanation!\n. This is much nicer\n. nice\n. cool! so basically this endpoint exports the defaults (and environment flag) that previously were done server-side.\n. > > +Import the the result via File, Open, (path you invoked gradlew). Dismiss any pop-ups about unlinked Gradle projects, as clicking those will likely break your project.\n\nnot sure if other people get that, but when I import the project on a\nclean repo, IntelliJ cannot build it because of configuration files in\nzipkin-collector-service/config and zipkin-query-service/config. These\ndirs are somehow added as both test resource and some other type, and I\nhave to open project settings and unselect them to make sure they are only\nincluded as test resource (so that IntelliJ doesn't try to compile them).\nYou get this when you do import? or file, open? If using the offline\ngeneration and open (plus never clicking a popup), I've not had to correct\nanything to get build or test working.\n. looks like there's an existing fix for this issue.. maybe it needs to be revised? let's punt to #978 for now.\n. mistake!\n. fixed. thanks for the catch\n. done https://github.com/openzipkin/zipkin/pull/980\n. will change this to affect local, not remote.\n. Ps happy to dig deeper into the cache-control header thing if you want to\nopen an issue on it (or reopen the old one).\n. This thread is about #718, so let's keep it contained there. There's been overwhelming support to undo the server-side mustache templates and I don't want to halt this PR on account of an old issue\n. client annotations should be labeled with the name of the instrumented client. In this case, I mistakenly used the name of the destination. If you looked at the span data before this change, you'd see \"cs\" labeled with the endpoint zipkin-query. This is incorrect as core annotations should be labeled with the local service name.\n. test bug\n. I like how the defaults are explicit\n. was this obviated prior to this change?\n. cool.. I should be able to see this in self-tracing!\n. So before Yuki made a change, we had no way to specify the interval of\ndependencies query in the UI. If we want to poke a hole and default it from\nenv on the web or query server, I suppose that would be in the other config\nendpoint?\n. In general I think now is a good time to think of things like this as\nconfig servers vs abstraction leaks (like JavaScript reading env values of\nthings). That's why I prefer the config endpoint approach as at least it\ncontains it until there's a better answer.\n. I'll look it up and ping you. This change made a historical one obvious and shouldn't be held back on account of moving it two spaces to the right.\n. ps this TODO can be satisfied later, as even this check is somewhat decent. Ex guice wiring must have worked.\n. \\n is the same convention as twitter server.. also helps when using curl.\n. great. I know we had an outstanding issue wrt moment.. particularly the duration thing, which defaults to user hard-entering micros, should be easier now! #864 cc @danchia @yurishkuro \n. maybe convert these two comments to code comments, as they are less likely to get lost.\n. ack\n. wat? a test class?! sorry I must be in the wrong project...\n. agreed\n. I was curious about that, too\n. oh ok nevermind then\n. @abesto PS this is the exact encoding of the POST /api/v1/spans endpoint (when content-type is application/x-thrift). Maybe we can move this to the new website and link out to there at some point.\n. ps there isn't a likely scenario when we will remove support for single-span-per-message prior without a new model (like zipkin 2). In other words, this is just a request to please not write new code that does this.\n. reason is that instrumentation is hard to change. ex htrace is out in the wild and uses this encoding.\n. this broke my build, btw\n. TraceSummary and TraceSummaryTest can be deleted now\n. actually I'll delete these things.\n. Agreed.. Fwiw if we accept gzip (in the javascript client) and the limit is\n10, probably have some time to sort out what's best.\n. removed this from docker as twitter server is very sensitive about being passed more properties than needed https://github.com/openzipkin/docker-zipkin/commit/d97ac3402e20bd4aca295fdead11b48d40416345\n. this.. wasn't all that fun!\n. whoah you can get money that easy?\n. unused and was bothering me :)\n. thx Set(\"/\") was confusing\n. maybe add a comment what's going on here (ex why we are looking for a dot)\n. guessing // if the path includes a file name (ex index.html), serve it relatively from static\n\nand the otherwise case is.. // if there was no file name in the path, boot them to the home page\n. cool. this was also weird, even if understandable\n. this is an unfortunate naming train wreck. thriftSpanToSpan returns a\nwrapper.\n. this change prevented self-tracing from logging the proxied requests under the service 127.0.0.1:9411\n. basically it was a small regression, as the annotations should be logged under the local service name (in this case zipkin-web)\n. the only scenario I can think of for this is that a span with only binary-annotations was returned by a query. I'll update this commit to add the comment.\n. this should actually help a lot of sites not ask for so many traces. cc @jcarres-mdsol \n. great idae\n. done\n. needed this because lint hates expect syntax\n. well it hates things that have no return value. Ex. expect(undefined).to.be.undefined; results in Expected an assignment or function call and instead saw an expression\n. done\n. done\n. done!\n. copy/paste (had no idea what this did)\n. should.not.exist(parsed.startTs); worked and didn't make lint angry!\n. no idea, but happy to change this and the dependency mustache post-merge, if you don't mind explaining on gitter what you are thinking\n. fixed\n. should.not.exist worked (even with lint) thanks!\n. made it convertToApiQuery. We can change this later, if you have a better name. I can't think of one.\n. done\n. done!\n. old reference.. this code doesn't exist\n. This class was extracted from AdaptiveSampler. Notably, it coordinates with Atomic* and addresses the observer stuff (Var etc) internally. This allows it to be easily used with java.\n. no custom imports needed\n. the smoking gun was local references ending up in the global storage rate calculator. It makes no sense to check if the local store rate is set, then later blindly access targetStoreRate. In chasing this through, I found the more interesting bug.\n. fwiw I don't think this needs to be an Option unless you need to get a list of all span names for some reason (ex the http api wouldn't call this)\n. needed because the top-level build has this in allProjects, and we can't remove that without breaking the idea goal...\n. > I don't have a personal use case for span names across services, so I\n\ndon't mind backing out of this change\ncool. I'd suggest backing it out to lower the scope of things to do :)\n. test deps were never used (as there were no tests)\n. this will likely cause some havoc, as there's sometimes thousands of service names. Maybe throw UnsupportedException for now? Then we can open up another issue to add an index for annotation w/o service name\n. spigo demos always use docker, so I've moved this to a new issue https://github.com/openzipkin/docker-zipkin-java/issues/6 I think there are some ways we can help with spigo to reduce steps (possibly making it report directly as opposed to generating files similar to https://github.com/adrianco/spigo/issues/67)\n. how is this controlled?\n. sorry I meant how do you control this via the UI. Ex if the user wants to\nview the raw json (api/v1/trace/id?raw), what do they do? Or does raw mean\nsomething else?\n. \"raw\" in the code here confused me as it is a parameter GET /api/v1/trace\napi\n. ex not sure why the website doesn't render it, but here's the definition\n- name: raw\n  in: query\n  required: false\n  description: |\n  Note this flag has no value. Ex. /trace/{traceId}?raw\n  Normally, the trace endpoint cleans trace data. For example, it merges\n  spans by id, adds missing timestamp or duration, corrects clock skew..\n  Specifying this flag is a debug case, when you are debugging zipkin\n  logic or zipkin instrumentation, and want to see the input to these\n  adjusters. For example, this might explain or rule out clock skew.\n  type: boolean\n\nhttps://github.com/openzipkin/zipkin-api/blob/master/zipkin-api.yaml#L146\n. good point.\nAlso, principle of least surprise wins for not defaulting to \"raw\" flag:\nwithout disclaimers, it would confuse people when timestamps don't match\nwhat the UI says.\nI'm game for as-is.. I'd prefer that variable to be named something besides\nraw (or clarified that it doesn't imply the raw parameter)\n. this was the chicken-egg. if the keyspace wasn't installed, getKeyspaceMetadata would throw\n. heh didn't even think that --data implies POST. nice shortcut!\n. in case you want some more characters chopped.. -s lets you skip the http://. Though I can see value in not doing this since it is intuitive to see the whole url\n. sadly can't unless we refactor further due to the later if\n(ElasticsearchStorage.FLUSH_ON_WRITES)\n. oh.. actually that looks not too bad.. will do\n. nope was too bad.. leaving un-final :P\n. possible add a unit test for this?\n. not sure, but I think this function is unit tested, if accessible, mind adding a test for 'error' -> isError?\n. nit: if you're looking for final, I'd move it to the field\n. interesting.. so this isn't just zipkin, this is all spring metrics!\n. no big deal, but we don't put final on variables even if it is explicit and there are good reasons why some folks make conventions like that.\nleaving them out keeps line shorter and focuses more attention on the test code (at risk of someone doing the shell-game with references)\n. should probably make a link from zipkin-server's README to this, under the metrics section. Saying something like: Zipkin also exports metrics in prometheus format (link)\n. thanks!\n. noticed this validation bug while writing tests\n. removed some redundant preconditions which are established elsewhere\n. this is the important change\n. cache is now cleared along with the keyspace during tests\n. dead code removal as partial down payment on new code\n. not documenting this one, yet, as it has a whole lot of analysis pending per https://github.com/openzipkin/zipkin-java/issues/200\n. sure\n. First, I'll own any bug on this addition.\nTo answer your question, there aren't compiler (libthrift, scrooge, etc)\ninterop tests for thrift (or openapi interop tests for json) in this repo.\nThe thrifts live in a separate repo\nhttps://github.com/openzipkin/zipkin-api/tree/master/thrift, and making\nsuch tests would add maintenance due to factors including the race\ncondition of adding new fields like this. Such a thing would also be a\nheavier build as we'd need to choose which generator (assuming apache maybe\n0.93?), hook it up for the test source (or make a module ignored from\npublishing etc), then design some tests that pass data through. That's\nguaranteed to be a lot more to maintain in this repo than exists now.\nSince we add fields very rarely, I'm not sure I want to add all of that\nright now. Regardless, it would be a large enough change to warrant a\nseparate pull request.\n. moved this to https://github.com/openzipkin/zipkin/issues/1179\n. ah I think I understand why you asked! TType.STRING is used (because there's no TType.BINARY). the binary keyword in IDL is a hint, but the actual thrift type is string.\nEx. thrift makes this out of 4: optional binary ipv6\njava\n  private static final org.apache.thrift.protocol.TField IPV6_FIELD_DESC = new org.apache.thrift.protocol.TField(\"ipv6\", org.apache.thrift.protocol.TType.STRING, (short)4);\n. found this bug, too.\n. found a way to cheaply help with this. added a commit\n. For example, we could count this table, and if empty aggregate on-demand...\n. I know I suggested this.. :) but wondering if we shouldn't say \"sql.query\" instead of \"jdbc.query\"\nThat way, the annotation would be valid for non-java\n. @jcarres-mdsol @basvanbeek @eirslett what say you? (asking for your non-java opinion)\n. did you mean to drop IF NOT EXISTS?\n. trace id isn't here bc now we're timeuuid (just saying out loud)\n. if possible, I'd tighten this up, as 20 (in the later assertion) is also greaterThanOrEqualTo(4)\nor put a TODO and I can look into it post merge\n. Indexer is still used for the annotation rows, right? If so, I'd leave the test in for now.\n. since this makes the indexer thing really only used for a single table, now, you can put in a TODO for me to cleanup and simplify this\n. lie.. we actually crept up to 272K! I wrote in 190, not 188, to give a little room :)\n. heavy on the ish :D\n. ah ok so this is for publishing against snapshots..\n. agreed. we should re-use zipkin.query as I don't expect us to apply this to other components such as the /health endpoint\n. Provided we document cross origin in the zipkin-server/README I'm cool with\ndefaulting to *\nEx. a section similar to Self-Tracing\nOn Tue, Aug 16, 2016 at 9:58 PM, Tommy Ludwig notifications@github.com\nwrote:\n\nIn zipkin-server/src/main/java/zipkin/server/ZipkinQueryApiV1.java\nhttps://github.com/openzipkin/zipkin/pull/1234#discussion_r74939322:\n\n@@ -43,6 +44,7 @@\n  */\n @RestController\n @RequestMapping(\"/api/v1\")\n+@CrossOrigin(\"${zipkin.server.allowed-origins:''}\")\n\nMaking the default value here also * would be a good idea for those\nmaking custom Zipkin servers with @EnableZipkinServer or those using\ntheir own YAML file of properties that might not contain the new property.\nThat is, if we want the default value to be * in these cases also. Do we?\nOr do we want people in those situations to make a conscious decision about\nthe allowed origins for the query API?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1234/files/607ccc3512ff70c8d6dd946bca7903e0730f96d3#r74939322,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD618AuBfDqSbM-nHOKkOWf-bQVRfqpks5qgcH6gaJpZM4JhLqg\n.\n. yeah src/it would be ideal!\n\nOn Wed, Aug 17, 2016 at 9:03 AM, Ho Yan Leung notifications@github.com\nwrote:\n\nIn zipkin-server/src/main/java/zipkin/server/ZipkinQueryApiV1.java\nhttps://github.com/openzipkin/zipkin/pull/1234#discussion_r75048464:\n\n@@ -43,6 +44,7 @@\n  /\n @RestController\n @RequestMapping(\"/api/v1\")\n+@CrossOrigin(\"${zipkin.query.allowed-origins:''}\")\n\nYeah the single quotes aren't needed - removed them.\nHappy to try and get some ITs in there - are the ones in\nzipkin-server/src/it a good example to start with?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1234/files/87ecb3a27cd70ad4ec42cc432532cee40f561a82#r75048464,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD619LSQdcAAAtv2WfFKbQccJGabT2xks5qgl37gaJpZM4JhLqg\n.\n. nit: is the quote needed?\n. ah ok. I just saw .* used later in the file, so figured it might be valid to use *\n. we've been adding these... I think it is helpful \"documentation\"\n. this approach looks great. only thing I'd ask is to move it to the normal src/test tree as it doesn't have anything that requires it to be tested as an integration test. Moving this to normal src/test will make it easier to troubleshoot if it fails.\n. last nits.. we are using \"endpoint\" not \"end point\". Then put an example, so people know what the value should be. something like...\n\nFor example, to only allow requests from mycompany.com, you can set the environment variable ZIPKIN_QUERY_ALLOWED_ORIGINS=mycompany.com.\n(pretty sure that's the syntax when set invoked as an env variable)\n. removed toStrings on leave nodes while leaving json toString on entrypoint ones (Span and DependencyLink). This reduces the amount of internal class leak\n. Indexer's still alive (as far as I can tell), so we shouldn't delete this until we delete that.\n. oops.. didn't see this! I'll change both places\n. fixed on master.. \"the Zipkin server\"\n. note: I'm not sure elasticsearch 5 has a guava dep anymore..\n. this will lead to drift as this default has changed in the past. let's leave it out of this change, pls (maybe just put it in the cassandra tests if you need it)\n. very cool stuff SASI looks like an amazing exact fit!\n. numeric should be fine here, as it is an enum\n. I'd probably keep this separate from timestamp, since timestamp is nullable when a raw span. ex. ts_id = epoch millis and \"ts\" = normal timestamp. PS why \"ts\" not timestamp here an other places for the timestamp field of the span or annotation?\n. I can try and help with this part. thx for the heads-up\nOn Mon, Sep 5, 2016 at 4:53 AM, mck notifications@github.com wrote:\n\nIn zipkin/src/main/java/zipkin/storage/QueryRequest.java\nhttps://github.com/openzipkin/zipkin/pull/1252#discussion_r77458464:\n\n@@ -178,7 +178,7 @@ public static Builder builder() {\n     private Long minDuration;\n     private Long maxDuration;\n     private Long endTs;\n-    private Long lookback;\n-    private Long lookback = 86400000L; // see ZipkinQueryApiV1.defaultLookback\n\nI've removed it, and added it to the tests in zipkin-storage/cassandra-3\nBut it'll be a performance problem for tests, as many of the tests are in\nSpanStoreTest and SpanCassandraTest.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1252/files/248a42064098a4aee21934d09739def2cc2331ad#r77458464,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61885Yefz0giH5aMhMLd8liUnTWBFks5qmy_fgaJpZM4Jrhum\n.\n. I'm assuming you don't care about this.. I'm going to rename this to \"cassandra3\" as opposed to \"cassandra-3\" since it makes the jar look less weird and keeps the field number consistent vs other things\n. :) thx\n. gonna solve this with package names as otherwise we can't get a bridge to work due to classpath conflicts\n. this was the bugger. the raised issue had special characters in a binary annotation! for json encoding, we need to peek at it.\n. this was the only field that wasn't guarded in the 1.8.3\n. @abesto not sure if this died because of the out-of-memory warning we are getting in circleci, or if I somehow made a syntax error I can't see.. can you give second eyes? https://circleci.com/gh/openzipkin/zipkin/138\n. ps the -R parameter doesn't exist on this version iirc\n. Mick, besides indexing there is literally no difference between what is in\nthe 3.0.8 change here and what you proposed. Why are you hard-lined for 3.9\nknowing that DSE users and others cant use it? Are you willing to support\ncassandra fixes more than you have the past year?\n. Since you arent always on gitter or watching issues, you might not realize\nthat not everyone rolls their own or supports their own cassandra. I often\nam in the middle of this. It was very hard to get people to even move to\ncassandra 2.1. We cant whistle by this reality.\n\nIf the sasi aspect is strictly important to you, id suggest a\nself-supported separate repo for 3.9 (even in zipkin org). Since I put in a\nsignificant amount of effort I would like to see a 3.0 version of our work\nhere. Having that here will also easy your future work and also mean the\ndependency graph will be compatible when completed\n. by the way.. it seems from docs SASI requires 3.4, not 3.9\nhttps://docs.datastax.com/en/cql/3.3/cql/cql_reference/refCreateSASIIndex.html\ncan you explain what about the design requires cassandra 3.9? is there\na bug in 3.4 fixed later?\nin the mean time, I'll test with 3.4 to see if it works or not\n. here's the fallback to not use SASI on duration (when we can't due to version). Since indexes are in a separate file, we should be able to more easily maintain them than the manually created indexes of the old implementation.\n. Thanks for finding something that breaks the model. I agree that this implies the annotations bundle field would need to go somewhere else.\n. formatting is off. zipkin is 2 spaces, etc.  Easiest way (if using intellij) is to use this: https://github.com/square/java-code-styles\n2 space tab and indent, with 4 space continuation indent... https://github.com/openzipkin/zipkin/blob/master/CONTRIBUTING.md\n. albeit unlikely the middle bits will throw, I'd add a try-finally. Also note server-modules are language level 8, so you can do a lambda\n. I think the reason I did this (created a wrapped future vs chaining) was I a heck of a time getting async to actually work properly. I presume you've verified this patch actually works as expected (versions have changed, since so maybe a driver bug I hit was been squashed)\n. please carry-forward the comments here and elsewhere unless they are invalid now. Ex. this one had..\n// via an internal class z.s.cassandra3.NamedBoundStatement, toString() is a nice name\nWe don't want default BoundStatement toString as it breaks most advice about span naming\n. I noticed statements like this.. how is this possible?\n. I mean how is it now possible to have a cycle in self-tracing?\n. before, I added this for a couple reasons.. one is to address perhaps your concern about the server amplifying itself, and another is to special-case it so that we don't have as much of a test burden. What's worrying me about trying to generify this is that it is getting past the point where we should kite without tests..\nEx the below was a heavily guarded thing that ensured we don't have to worry about things like local collectors amplifying, nor any other traces we don't expect to make. A generic cassandra tracer would have to be more... generic, but then it would have a higher burden of testing etc. If that's the goal, I might copy/paste the work and then make a cassandra client tracer out of it\n-    // Only join traces, don't start them. This prevents LocalCollector's thread from amplifying.\n -    if (brave.serverSpanThreadBinder().getCurrentServerSpan() != null &&\n -        brave.serverSpanThreadBinder().getCurrentServerSpan().getSpan() != null\n -        && method.getName().equals(\"executeAsync\") && args[0] instanceof BoundStatement) {\n. we don't have to do this, and probably shouldn't. We don't support non-string binary annotations on search\n. please guard on BinaryAnnotation.Type.STRING before adding to the things to index. I don't want to accidentally increase scope to include or promote problematic annotation types\n. set dedupes. maybe I should have put in a comment, but I think we need this (probably also a unit test to show it works)\n. basically it is an edge case, where request.annotations has a key that's in request.binaryAnnotations.. perhaps we can just dedupe in the request object instead.. or not bother and switch to list with a risk of accidentally making the same query twice.. wdyt\n. ok if we are doing this, then set seems simpler. and less likely for someone to accidentally change one line and not the other.. is there a bigger reason why you aren't using set for this?\n. yeah really not a fan of defensive list add vs just using a set.\n. interesting.. surprised this works, but maybe I shouldn't be\n. > feasibility of it aside, has it value?\n\nNo, this is value-reducing. For example, we would have to increase scope\nof other things to make sure special-case boolean annotations like \"sa\"\naren't accidentally now indexed.\n\nMoreover, there are numerous problems in the subtleties of thrift types.\nWhen we start indexing them by default, we are reversing our position (that\ndiscouraged them), and would have to take responsibility to support them\ndespite their problems. For example, this makes transitioning to a new\nmodel that doesn't have an I16 type more difficult. It raises questions on\nlossiness of I64 when rendered in json (happened last week!) Right now, the\napi is pretty clear about text annotations only, and this is what we should\nbe doing.\n. LinkedHashSet retains order.. how about using that?\nOn Mon, Sep 12, 2016 at 11:16 AM, mck notifications@github.com wrote:\n\nIn zipkin-storage/cassandra3/src/main/java/zipkin/storage/\ncassandra3/CassandraUtil.java\nhttps://github.com/openzipkin/zipkin/pull/1289#discussion_r78311519:\n\n@@ -96,14 +97,17 @@ public static int durationIndexBucket(long ts_micro) {\n       return Collections.emptyList();\n     }\n     checkArgument(request.serviceName != null, \"serviceName needed with annotation query\");\n-    Set annotationKeys = new LinkedHashSet<>();\n\nwent to list with manual dedupe to keep order. (i think the reason was to\nhave some consistency when reading the data in cqlsh. ie you're always\nreading it as service, annotations, binaryAnnotations)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1289/files/ded65ab27c2839701ce7ac523bcf8f4df79a3526#r78311519,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6175ia4jL2tNbs_IWjREzwvwvkTLbks5qpMQKgaJpZM4J5_ci\n.\n. I think the \"sortedList\" thing at the bottom was the real problem\nunderneath the behavior your raised. We could copy-out a linkedHashSet to a\nnon-sorted list.\n\nOn Mon, Sep 12, 2016 at 11:17 AM, Adrian Cole adrian.f.cole@gmail.com\nwrote:\n\nLinkedHashSet retains order.. how about using that?\nOn Mon, Sep 12, 2016 at 11:16 AM, mck notifications@github.com wrote:\n\nIn zipkin-storage/cassandra3/src/main/java/zipkin/storage/cassa\nndra3/CassandraUtil.java\nhttps://github.com/openzipkin/zipkin/pull/1289#discussion_r78311519:\n\n@@ -96,14 +97,17 @@ public static int durationIndexBucket(long ts_micro) {\n       return Collections.emptyList();\n     }\n     checkArgument(request.serviceName != null, \"serviceName needed with annotation query\");\n-    Set annotationKeys = new LinkedHashSet<>();\n\nwent to list with manual dedupe to keep order. (i think the reason was to\nhave some consistency when reading the data in cqlsh. ie you're always\nreading it as service, annotations, binaryAnnotations)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1289/files/ded65ab27c2839701ce7ac523bcf8f4df79a3526#r78311519,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6175ia4jL2tNbs_IWjREzwvwvkTLbks5qpMQKgaJpZM4J5_ci\n.\n. this one should fail I think\n. ps I only now noticed that for a year we were indexing binaryAnnotation.key in the same space as annotation.value. This goes against all of our docs, and probably people just didn't notice.\n\n\nTook it out in https://github.com/openzipkin/zipkin/pull/1292\n. fyi @devinsba @dan-tr @anuraaga I had to adjust the index a bit to fix this\n. cc @michaelsembwever on \"cassandra3\" I had to change delimiters of binary annotations to ';' to avoid a LIKE clash with annotations (which are delimited with ':')\n. not terribly thrilled at what the library seems to be giving us. Looks like a combination of lock-into apache http client types plus all the work you usually need when manually writing http apis!\nIt might be more straightforward (less duplication etc) to hack a http adapter for the transport client. At least then we can choose any http library.\n. cc @devinsba @anuraaga @shakuzen \n. temporarily forwards to a transport-client so we can test things.\n. copy pasted test which actually passes\n. basically I'm not thinking about implementing a standalone library for TransportClient, rather intercept the places where we use Client and make a zipkin-specific interface which either forwards to TransportClient or invokes http. That reduces the scope a lot I think, as it only requires us to support things we actually use.\n. luckily you are talking to the original author of jclouds.. I think I've\nwritten that signature code several times :P\nOn Wed, Sep 14, 2016 at 12:24 AM, Brian Devins notifications@github.com\nwrote:\n\nIn zipkin-storage/elasticsearch-rest/src/main/java/zipkin/\nstorage/elasticsearch/ElasticsearchRestSpanConsumer.java\nhttps://github.com/openzipkin/zipkin/pull/1294#discussion_r78594416:\n\n\nfinal RestClient client;\nfinal IndexNameFormatter indexNameFormatter;\n  +\nElasticsearchRestSpanConsumer(RestClient client, IndexNameFormatter indexNameFormatter) {\nthis.client = client;\nthis.indexNameFormatter = indexNameFormatter;\n}\n  +\n@Override public void accept(List spans, final Callback callback) {\nStringBuilder request = new StringBuilder();\nfor (Span s : spans) {\ncreateSpanIndexRequest(s, request);\n}\n  +\nHttpEntity entity = new NStringEntity(request.toString(), ContentType.APPLICATION_JSON);\nclient.performRequest(\"POST\", \"/_bulk\", Collections.emptyMap(), entity,\n\n\nIn order to authenticate requests to amazon when locking down the security\non our domain, we will need to be able to add on an authentication header\nwhich is a signature based on the request body.\nhttp://docs.aws.amazon.com/elasticsearch-service/latest/\ndeveloperguide/what-is-amazon-elasticsearch-service.html#signing-requests\nhttp://docs.aws.amazon.com/general/latest/gr/signature-\nv4-examples.html#signature-v4-examples-java\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1294/files/f6fcf14595940b112d303c7105a0ca126de963a8#r78594416,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD61-Wpzl0RYsZkSIsNuZ0PPrMxjndTks5qps4zgaJpZM4J7dnf\n.\n. wonder if we shouldn't use jest.. it seems to be more than just an apache http configuration project https://github.com/searchbox-io/Jest\n. jest seems to also have a AWS thing in limbo https://github.com/searchbox-io/Jest/pull/368\n. jest depends on guava 19, which will also throw things out of whack.. reverting to either using the apache substrate from elastic or similar\n. @rogeralsing fyi, so we are custom making a bulk request, here, but here are notes in general:\n\nWe insert an extra \"timestamp_millis\" field into each span, which allows kibana to work. When we index, we index as type \"span\" to the index \"span-yyyy-MM-dd\"\n. this unbreaks the api, which is helpful :)\n. this doesn't need to know about NativeClient\n. resurrects these so no api break\n. better than sharing a static across multiple implementations\n. copy/paste error!\n. here's a fix (for all providers) https://github.com/openzipkin/zipkin/pull/1305\nNote I didn't presume a summary message as the cause for skip could be anything. Nice catch\n. I'd prefer this moved to be inside the class of its only caller\n. check.exception.getMessage() instead of \"cluster was not healthy\" as the failure could be unrelated to the cluster\n. not required to change, but just so you know why I didn't mark these private. This is a package protected final internal class anyway, so we have no contract to uphold wrt changing thing. each private thing has potential to bloat class sizes and mixed visibility is distracting.\nIf you really want these things to be invisible to the package, a quicker and less bloaty way to do it is just to make the type private. At that point, you can't reference the fields and constructors anyway, so default visibility is fine. You'll notice this practice used in grpc I think..\n. imports here are likely a mistake as I don't see any other changes\n. are all these imports used?\n. does this work on elasticsearch 5?\n. probably don't need throws here and below\n. nit: please use LinkedHashSet/Map always :) order bugs in tests aren't worth tracking down, and such things often happen well after the first commit.\n. heh, so jest not only binds to apache hc, but also gson?\n. pretty heavy imho. why not 10 * 1000? this is fairly obvious milliseconds and the compiler will fold the constant\n. method scoped classes creep me out. they can easily accidentally leak references to the enclosing object. mind making this a static inner class?\n. static utility methods unless they require access to the enclosing object. makes it easier to see impact.\n. please revert this. I don't want my ES polluted\n. \"A List of base http urls to connect to, e.g. http://localhost:9200\" right?\n. http://localhost:9200, right?\n. :) I used to over-use guava like this.. it is hard to realize when it is happening.\n. ehh.. still here :P\n. Sorry, but this isn't a good enough reason. This will break api as it will make change the bytecode level for components that don't define it. Also, it changes the default, which was specifically there.\nI do have a workaround, which I paste into the terminal any time dependencies change. That isn't terribly often, though certainly has been in the work you are doing.\nbash\nfind . -name '*.iml' -print0 | xargs -0 sed -i '' 's/JDK_1_6/JDK_1_8/g'\nfind . -name '*.iml' -print0 | xargs -0 sed -i '' 's/JDK_1_7/JDK_1_8/g'\nsed -i '' 's/target=\"1.7\"/target=\"1.8\"/g' .idea/compiler.xml\nsed -i '' 's/target=\"1.6\"/target=\"1.8\"/g' .idea/compiler.xml\nps please upvote https://youtrack.jetbrains.com/issue/IDEA-161060\nhttps://github.com/openzipkin/zipkin/issues/1256#issuecomment-246564227\n. unless this needs to be public, I wouldn't make it public (rather final and package private). That way we don't have to worry about changing ctor etc.\n. thx yeah opposite order of the maven artifact\n. I'll look into it. I think we need to improve this\n. yeah I switched as GenericResultAbstractAction as the Health constructor had a race condition on setting its url. That was scary to me and very dodgy code. I preferred using the base type that wasn't dodgy\n. obviously I will revert this. we can't willy nilly change zipkin's core library to java 8! It will break a few libraries\n. I'm having doubts about maintaining code like this. this sort of stuff has breeds bugs. I'd much rather point people to the existing docs and code like use \"cloud.aws.region.auto\" or \"cloud.aws.region.static\", which of course we can map to env variables like AWS_REGION.\n. ah I see what you mean. this isn't a class we defined.\n. noticed this which was bad! leaking clients\n. changing the base here is fine as you can override a method reducing its throws (especially as we're talking about test code)\n. @sethp-jive hopefully this will help next time when troubleshooting (well hopefully you won't need to) :)\n. before, we weren't returning the nested annotations agg.. now we are, so it properly merges\n. before, a binary annotation had the same service name as a regular annotation, masking the bug where we weren't querying the latter\n. /me whistles\nnothing to see here\n/me whistles\n. tests seem to initialize port to -1, so I'm guarding on that here\n. done\n. done\n. good catch\n. this was a copy/pasted test of a static method (so it is redundant)\n. I actually started googling how to get a generic type from a bean factory.. probably best is to make a holder class that has an auto-wired list?\n. https://github.com/openzipkin/zipkin/pull/1342 thanks for noticing!\n. @michaelsembwever will need your help post-merge on this I think. I started playing around with a TraceIdUDT (with hi/lo fields).. that worked, but then I didn't get how to query by half of the traceId UDT considering it is a part of the partition key. Maybe it is a SASI concern..\n. @openzipkin/elasticsearch My assumption is that this is not a costly step, and it also doesn't seem to increase the size of index needed (since this doesn't create multiple tokens). If these assumptions are invalid, report back!\n. As much as I don't love creating things like this, it allows us to work even when the field isn't present, yet. This is how we addressed other minor schema changes such as pre-aggregated deps and ipv6.\n. nit: fuzz\n. you could probably remove keyValue and just compare idx vs ann.length (ex I think the goal here is to ensure it works with a value like foo=\n. formatting nit s/idx+1/idx + 1/\n. the problem was in the inheritance\n. this might need to be reconsidered if we want to support elasticsearch 5 https://github.com/openzipkin/zipkin/issues/1379\n. needs a rebase on master, as this shifted\n. basically this is in CassandraUtil now\n. there's still some drift here, but I'll take care of it post merge\n. maybe remind people the file that blocked imports go is bnd.bnd? I think many don't know this\n. I think you need to affect another file ZipkinCassandraStorageProperties for this to take.\n. this was dead code!\n. running against mixed versions increases the sorts of exception we can get, so tightened this and backfilled test cases.\n. I'll change that comment, since we've a different reason for using curl (to get v5)\n. ideally we'd want to test both versions without changing configuration, so that the module fails if 2.x or 5.x fail. That way people don't have to remember to run the module's tests twice\n. probably :)\n. ah interesting idea!\n. just copy/paste the classes and rename them to like ElasticsearchHttp2xSpanStoreTest? (for now.. we can do more fancy refactoring later, but having a class name like this is very simple to deal with, and we won't have more than 2 versions to check)\n. neat. once we port-in changes like this, we could probably do something to stop as necessary. For example, the zookeeper tests kill a ZK node to test resilience. we'll eventually need things like this I think.\n. so I think the real real test is removing the manual installation of elasticsearch further down this file and seeing if the tests still run\n. does caching of docker image downloads happen incidentally, or would we need separate setup for that?\n. yeah I was hoping to not publish another jar in support of this..\n. We could consider removing bookkeeping at the slight chance of error by using the \"latest\" tag here instead of a specific version. This works fine on master, since it is always ahead of docker and shouldn't break. If/when we start doing a long-term release branch (like 1.x) we might consider changing that. Food for thought.. ok maybe something like a DockerZipkin class in zipkin's core test jar? That way people can run the tests in an IDE with no config, and can also override manually (as needed sometimes).\nex.\njava\n  new LazyElasticsearchHttpStorage(\"openzipkin/zipkin-elasticsearch5\", DockerZipkin.VERSION);\nwhich would allow someone to easily temporarily adjust:\nex.\njava\n  new LazyElasticsearchHttpStorage(\"openzipkin/zipkin-elasticsearch5\", \"1.6.2\");\nMain thing I'm looking for is to keep the layers consistent (to reduce downloading for folks like me), but yeah without making it hard to start the tests in the IDE.\n. right now we use square code style (noted in the CONTRIBUTING section), and there's a PR for eclipse as yet unmerged https://github.com/square/java-code-styles/pull/32. I'd initialize this to null, so that you don't end up with an empty string in the json. set if not null or empty. I'd test this in ZipkinUiAutoConfigurationTest basically that when zipkin.ui.logs-url: (empty string), getLogsUrl() is null, and when it is set to a url, that's readable as getLogsUrl(). This helps make sure there's no typos hard to see.. I think you can shorten this to if (logsUrl) ?. if you export this function, you should be able to test it by making a class test/component_data/trace.test.js \nsomething like... (please use the exact format of kibana search url you use as that helps folks understand how it is used)\n```javascript\nimport {toContextualLogsUrl} from '../../js/component_data/trace';\ndescribe('toContextualLogsUrl', () => {\n  it('replaces token in logsUrl when set', () => {\n    toContextualLogsUrl('http://some-realistic-logs-url/{traceId}', 'abcd').should.equal(''http://some-realistic-logs-url/abcd'');\n  });\n});\n``. I would make this method scope, and set it to undefined as I think that's what it would be in practice.\nthen, add a test for what happens when it isn't undefined. For example, traceToMustache result includes the logsUrl. not sure there's an existing class to cover this, so one would need to be made.. I can help think of one, or maybe the notes I've made so far can get you there?. actually, looking at this here, it seems we probably shouldn't have given this a special env variable (since none of the other UI ones do). Can you remove the mapping from zipkin-server-shared.yml? Instead, it would be set normally withZIPKIN_UI_LOGS_URL(which eliminates the need to duplicate some of the docs). add a link below to an example screenshot <img width=\"1280\" alt=\"logs button\" src=\"https://cloud.githubusercontent.com/assets/9842366/20482538/6e35ca66-afed-11e6-90e9-1e28f66d985e.png\">. this can be activated by setting the propertyzipkin.ui.logs-url` or its corresponding environment variable:\nthen use the environment syntax. @marcingrzejszczak do you have an example kibana link that looks up from the id pattern sleuth logs with slf4j?. fixed formatting here as it was annoying me... Maybe add your comment here that this reduces the resulting size to ~NNNKiB. I'd move this test to SpanTest as it doesn't require live servers to verify. keep this fix though!. we actually do. the client is authoritative in a shared span. Let's set it to the client one (span with \"cr\" annotation), or null if cr is absent. so a suggested fix (which considers client span) would be to move this block below where we iterate through annotations. When we iterate through annotations decide which side is the client span and prefer that somehow... we can't do this as we only include authoritative data reported to us (as opposed to deriving span's timestamp from annotations). The prior data incorrectly tried to merge authoritatively set timestamps (which was the bug). The better way is to prefer the client as mentioned before.. nit: instead of isServerRecvSpan as a parameter, I'd just invoke the method here and save as a local. nit move this below storeSpan if you choose to not pass as a param. works for me :P. ps this part is for annotations (not binary annotion/tags). This change would make the UI timeline look weird, as single-line is ideal. I would revert this part.. is this only affecting crossroads? I don't expect us to mount the api under the subpath /zipkin at the server level by default.. well at least not without significantly broadening the discussion beyond those interested in this UI (as it has deployment and other doc implications). wow simple fix turns out. bet it wasn't easy to find.. spelling Equalds. since all units are microseconds. I'd take out things that make us have to think back and forth, especially in this test as it doesn't actually use a clock.\nFor example, the skew doesn't need to be in a unit different than microseconds. If everything is the same unit, we don't need to multiply by 1000 or increase the navigation needed in this test.. just an fyi, but this could result in further maintenance as local spans are not required to have the tag \"lc\". The local component tag is primarily a tool to use when you don't have any other annotation or binary annotations (as it allows you to have some endpoint attached that you can search with.\nSo, while this will work for many single-host spans, it won't for all of them. It might be best to change the heuristic or note in a TODO that this won't work for single-host spans that don't contain \"lc\". nit formatting here and below. :heart:. I think this is used in a subquery of spans, too, so it wouldn't be accurate to say max raw items (I can verify). Would be nice if there was a way to say \"give me all\". maybe MAX_RESULT_WINDOW instead, as that matches index.max_result_window which is what this is about?. might need to add java heap size args.. not sure why the build died from out-of-memory (haven't looked to see if it was doing this formerly). in tests, most of the time, we use incrementing numbers to make obvious the order in the tree. This confused me a little as 3 is the parent of 1 and 2.\nMaybe rework a little?\nlong parentId = 1L; // by declaring a local, it is obvious that the parent is missing. It is more obvious because the traceId low is the same as the parent id.\n// then use (parentId, 2L) and (parentId, 3L). maybe also add a test here for headless trace that has obvious links.\nFor example, \n(rootId, 2L, \"service1\") and (rootId, 2L, 4L, \"service2\")\n(rootId, 3L, \"service1\") and (rootId, 3L, 5L, \"service3\")\nThis will make sure this didn't accidentally break the headless function (without resorting to integration tests). rename this variable to currentSpan. If I'm not mistaken, the only case where value is null is the case where we've added a synthetic (sentinel).\nI think the code will be more clear if we show this a different way. ex. a package-private static constant Node.MISSING_ROOT\nThen, you can compare against this reference, vs checking null.\nex. if (current == Node.MISSING_ROOT) continue (ps do this at the very top of this block before the processing statement). similarly here, if we use a sentinel instead of null, I think the code is more self-describing..\nif (ancestor == Node.MISSING_ROOT) {\n  // explain\n} else ... existing code. rootNode = Node.MISSING_ROOT\n^^ something like this makes the code more obvious, plus you can search references for Node.MISSING_ROOT, where you can't search references for null. ps I'm ok to add this as a elasticsearch property instead. ex zipkin.storage.elasticsearch.names-lookback (we have similar things in cassandra). mentioned offline, this is perfectly fine as our server is java 8 since the beginning. I'd probably make a test case for this class, particularly for those like me who had no idea you could inject an optional :P. this is new logic, with two conditionals and a lot of commentary. we try not to rely exclusively on integration tests for things that can be unit tested because it makes bugs harder to find later. We have been trying to be better at tests, lately, adding unit tests for the classes we change, especially auto-configuration as bugs and drift are easy there.\nIf it isn't clear how to setup a test like this (ex this link doesn't help). That's cool, I can try and help. Just asking if you can.. thx!. this can be package private and final. this isn't dynamic, so best to just initialize the credentials here vs on each request. revert unnecessary reformatting pls. please simplify to \"basic-auth-user-name\" to just \"username\" and same for password (the property name is overly specific as we won't have any other approach). pls undo import formatting change. might want to consider setting up the same code style as we use since it will eliminate this sort of stuff https://github.com/openzipkin/zipkin/blob/master/.github/CONTRIBUTING.md#contributing-to-zipkin. simplify to username and password and javadoc the properties individually as it helps with tooling.\nEx. username used for basic auth. Needed when Shield or X-Pack security is enabled\n. nit revert formatting. revert all the formatting changes in this file. this part down should be the only things in the diff. this needs a unit test, especially since we are special-casing 403. You can use mockwebserver to do that.. please avoid misleading terminology.. this is not a signature :) Simply add the header and proceed!. nice :). I understand why you chose ES_USER/PASS, seeing below that mysql does this. just that mysql is the anomaly (maybe a mistake, but too late to fix as in use).\nUSERNAME and PASSWORD are the more frequently used, and are nice as they match the property naming conventions.. how many is this usually. I wonder if we can change our tests to make sure this is invoked?. by reformatting into a date, it will be obvious when someone accidentally stores span.timestamp as seconds or millis (instead of micros). It will be obvious as the date will come out as 1970 (and in this query not return at all). while many ppl use 64bit trace ids, this part of the query will match what's in the json or B3 headers. if static, should be initialized outside the ctor.. maybe make it final static.\n. you may end up needing to tune this later.. maybe for now put in some unit tests, then later some contended benchmarks. can make a comment that usually spans will be in the same day, except at midnight. below, that entries correspond to unique service/span combinations. it is probably find to say 1000, but even 10000 should be reasonable as there's not much ram involved (ex if you have 1000 services, you will need a count greater than that for the diversity of span names w/in a service).. I wouldn't do synchronized map here, because you have a good bit of places which need atomic ops (put if absent). more straightforward to just synchronize when you need to imho. here's how I think I'd write this..\nno outer synchronized ..\n```java\nfor (Map.Entry>> entry : indexToServiceSpans.entrySet()) {\n  Set> serviceSpansEntryCached = null;\n  synchronized (indexToServiceSpansCache) { // lock the whole cache \n    serviceSpansEntryCached = indexToServiceSpansCache.get(entry.getKey());\n  }\n  if (serviceSpansEntryCached == null || serviceSpansEntryCached.isEmpty()) continue;\nsynchronized (serviceSpansEntryCached) { // lock the cache for the index\n    serviceSpansEntryCached.removeAll(entry.getValue());\n  }\n}\n```. also note.. this code probably needs to be extracted to a method, because you will need to do this in the catch block, but more importantly in response to Callback.onError. You'll need to decorate the input callback with one like this...\n```java\nnew Callback() {\n  void onSuccess(){\n    realCallback.onSuccess();\n  }\n  void onError(Throwable t){\n    clearCache(indexToServiceSpans);\n    realCallback.onError(t);\n  }\n}\n``. looking at this, the logic is in a tight loop (for each span). I'd move the cache stuff to a higher scope, where you seeif (!indexToServiceSpans.isEmpty()) {`. note an identical copy of this patched class was in Brave for quite a while now.. cc @llinder fyi this was just for sanity as indent level was blinding. no logic change. here's the new test. is JRE8 a Kafka 0.10 constraint? If not, I might drop it back to JRE 7 as some use the components directly, and we usually don't use a JRE level higher than the underlying driver.. remember some use this code w/o autoconfigure (as a library). Notes about autoconfigure and modules etc ideally go into the auto-configure module README, since it is produced there. You can look at this for an example:\nhttps://github.com/openzipkin/zipkin-aws/tree/master/collector-sqs\nhttps://github.com/openzipkin/zipkin-aws/tree/master/autoconfigure/collector-sqs. note: rebase and bump versions as we are now at 1.25.1-SNAPSHOT. I've started inlining little classes like this as static inner classes on the autoconfigure type. It keeps the header-weight down, and they are pretty insignificant.\nAlso, for single-property conditions, it is unambiguous what failed, so you can use the slightly less complex normal condition as @Conditional(KafkaBootstrapServersSet.class):\nex.\njava\n  static final class HttpLoggingSet implements Condition {\n    @Override public boolean matches(ConditionContext condition, AnnotatedTypeMetadata ignored) {\n      return !isEmpty(condition.getEnvironment()\n          .getProperty(\"zipkin.storage.elasticsearch.http-logging\"));\n    }\n  }. ex this is a tiny file, you can add condition type here (noting you'll need to qualify it ). Latest fashion is to provide one-line docs here /** password used for basic auth. Needed when Shield or X-Pack security is enabled */ and leave all values at null (imply change streams to Integer).\nThe one-lines shouldn't include javadoc tags like @see as this is looked at by spring boot utilities that add a tool tip.\n. this doesn't need to be public as nothing else uses it.. here, you can use this form to avoid accidentally overriding defaults (which avoids coordinating defaults in two places)\nKafkaCollector.Builder result = KafkaCollector.builder();\n    if (topic != null) result.topic(topic);\n    if (groupId != null) result.groupId(groupId);\n    if (bootstrapServers != null) result.bootstrapServers(bootstrapServers);\n   .... note: you'll want to coerce empty to null as sometimes people can't set null with properties. Usually I add a static method for this called emptyToNull. here's a neat way to test properties set properly. you'll still need a test to ensure the object only loads when its signaling property is set https://github.com/openzipkin/zipkin-sparkstreaming/blob/master/autoconfigure/stream-kafka/src/test/java/zipkin/sparkstreaming/autoconfigure/stream/kafka/ZipkinKafkaStreamFactoryPropertiesTest.java. while not required, you could consider using autovalue to make your builder https://github.com/openzipkin/zipkin-sparkstreaming/blob/master/stream/kafka/src/main/java/zipkin/sparkstreaming/stream/kafka/KafkaStreamFactory.java. I think we usually wait 1 second by default now.. though it is arbitrary anyway. maybe pull this into a method named warmUp with this as its javadoc?. nit: probably looks nicer to just accept the Builder and take values from it (using package protected). doesn't need to be public. you can use the package protected field for testing. that lets you move this javadoc to the field itself.. hmm ok :). move this to a note under \"This collector remains a Kafka 0.8.x consumer, while Zipkin systems update to 0.9+.\"? I think more people will notice it this way.. please revert formatting changes to all files in this PR. please move this into a local variable in its only caller: DependencyLinkerTest.linksSpansShouldHandleNullSpans() also please don't concatenate words like this as it is not readable. Once a local variable use lower-camel. rename this method. Obviously the data is very incorrect, also remember adjusting to not crash will hide the problem, punting it to other places.\na better method name would be linksSpans_skipsOnInvalidParentId (as opposed to crashing). when making test data, don't conflate concerns.. for example, it is unclear if linking a span with a bad parent (set to itself) is the problem, or if it is only a problem if somehow duplicate spans with a bad parent are not merged? The annotations added are distracting I think, unless they are contribute to the problem.\nstart with simplest to break. when you redefine this as a local in the test class, simply mess up the parent id and see if it breaks with a list of only one span.\n. seems ok to have an unstable dep here because the server-side dependency is optional (which makes it a part of the all-jar, but not a strict dep for customizers). unstable I meant the patch-only version number :)\nnit: maybe use a property to coordinate these? ex.\n```\nin the properties section\n0.0.23\nthen in places like here:\n${simpleclient.version}\n``. sorry there's still a lot of format-only changes in this PR. adding QUERY_ENABLED to this section was probably a bit of a mistake as that env never existed before. How about adding a new section based onScribe Collector? In the new section, I'd explain this accepts spans viaPOST /api/v1/spans`\nI feel awkward about HTTP_COLLECTOR_ENABLED as much as HTTP_ENABLED almost to the point of punting re-mapping of the spring property. That said, it isn't a strong feeling.. I suppose trying to be aware, but not limited by what spring-boot-dependencies says. For example, we'll soon get to a point where people may be using us for multiple versions of boot, so not sure I'd want to implicitly take what's in one version.. I like the idea of what you are doing, but I don't want us to add a top-level type that only supports in-memory storage. It adds a significant cognitive and maintenance impact in support of a non-production configuration.\nIf this is needed, I would probably flatten the hierarchy and expose as an explicit hook for InMemoryStorage (without exposing a new public type). If not needed beyond tests, I would remove it entirely. \nKeep in mind we don't even have counters like this for production code such as cassandra cache counters!. this is good.. always linkedhashset :) helps to avoid breaking tests or other things that look at order. less distractions. for each page break, prefix with <p> so the javadoc looks like what you mean. wrap with\n```\n{@code\nyour stuff here\n}\n``. mentioned earlier, squash by exposing one or two key counters you need in tests. Avoid new public types.. this feels like a fair amount of bookkeeping which may make this class even less attractive to maintain. See how much of this you can get rid of and have it still work. nix this. since the description is written like this, maybe@return` is better that a newline and the same.. ps noticing the names of these types are getting a bit terrible. might clean them up later if this goes anywhere. nit: this should be copy/paste of the other one... the fix to the aforementioned bug is in master now, so guessing 0.0.24 will sort it out. hehe. hehe well it would if there was a zipkin-dependencies-spark for cassandra3 :P. @joel-airspring FYI: I renamed this type to more generic as it keeps the comparators in one place.. @joel-airspring added this note, as clearly I forgot why when you asked earlier. ZipkinRuleTest reminded me :). added a test closer to the source for the redundant write issue. If you are looking to create multiple annotations, I might do so in a different file as this one is used for all storage tests (and also some non-storage ones). Particularly, this creates an invalid \"party-line\" span, which has multiple servers for the same RPC.. please don't change the behavior of existing tests (if that's what's going on here.. can't tell if this is changing the test or if the file has been re-ordered). Main thing we need to know is if this is changing behavior or not.. usually we add tests to show new things happening and keeping the old tests help us know we didn't break anything.. nice. next step is to make some concurrency tests... something like https://github.com/openzipkin/brave/blob/7fd9bae7ee3ba8abaa536ad1d29634099a337d83/brave/src/test/java/brave/CurrentTracingTest.java#L53\nyou'll want to make sure that they break when you remove the synchronized block. hopefully, this estimate is high :). I will remove this note on the way in, as -1 won't work :P. I'll fix formatting on the way in.. big change here is reduced cardinality on the values to service -> traceId vs service -> (traceId, timestamp)\nThe impact is easier cleanup and a bit less scanning during writes (due to less cleanup there). Should be measurable, but we don't have benchmarks in for this at the moment.. note with this change, we no longer need to derive data from spans when cleaning up. Instead we use the values pinned at insert time (because we are deleting by trace ID, this all works). opened https://github.com/openzipkin/zipkin/issues/1647 to track. thanks!. It is invalid to put a CLIENT_ADDR on a client span. Scrubbed this as it confuses other things. This test was accidentally testing defaults related to ignoring traceIdHigh.. this data had incorrectly aligned timestamps (which wasn't the point of the test). this test is also testing \"raw spans\". To make sure this passes, we should make a span which has no authoritative timestamp (client spans do have an authoritative timestamp and that was the prior test data). nit final. I think there's still a race condition here. you should double-check lost race I think. (ex do indexToServiceSpansCache.get here in the synchronized block before put). eventhough this makes a line break please rename \"cache\" to \"indexToServiceSpansCache\" or visa versa, so it is easier to tell what's going on from a lock pov.. I suspect the serviceSpansEntryCached.isEmpty() needs to be in the synchronized block for visibility concerns.. I think you want to hide this private to ensure it is read and modified only via synchronized methods. this should be synchronized on indexToServiceSpansCache just to make it obvious. I'd probably make a package-private type that encapsulates access to this map, with the map itself private. All things that affect it would be package protected synchronized methods to make it easier to reason with.. nit: no wildcards. I like the idea here, but I think you are testing map, not your code. If you encapsulate access to this indexToServiceSpansCache, you can test high-level methods you need (ex similar to how we have Multimap in InMemorySpanStore). es.clear should call this. ok will do in a bit.. I'll change this code to throw, and the code that calls it to skip and log (since that code has the logger people turn on). ps dependencyWarnThreshold and dependencyCriticalThreshold might also work?. 0.0.26 is out, so probably time to see if we're sorted?. This, Encoding and MessageEncoder were taken from zipkin-reporter-java, which afterwards can be a place to host just the implementations. removed requirement to use json as serialization as it isn't important and adds code. people can use whatever they want, like kryo. converted this to mock because a near future change will make this type abstract. I added this to the first test that invokes a map or flatMap (as doing for all tests would be distracting). IllegalStateException?. sure.. we might make a test image at some point to reduce layers needed to be downloaded. I might suggest following conventions in the file and/or restructuring them generally. I'd accept a list, so that we can later use things like AutoValue (which don't allow arrays), and just convert to array at the site you need.. how about a v2 one!\n[{\n  \"traceId\": \"9032b04972e475c5\",\n  \"id\": \"9032b04972e475c5\",\n  \"kind\": \"SERVER\",\n  \"name\": \"get\",\n  \"timestamp\": 1505990621526000,\n  \"duration\": 612898,\n  \"localEndpoint\": {\n    \"serviceName\": \"brave-webmvc-example\",\n    \"ipv4\": \"192.168.1.113\"\n  },\n  \"remoteEndpoint\": {\n    \"serviceName\": \"\",\n    \"ipv4\": \"127.0.0.1\",\n    \"port\": 60149\n  },\n  \"tags\": {\n    \"error\": \"500 Internal Server Error\",\n    \"http.path\": \"/a\"\n  }\n}]. I think the topic-like things have always defaulted to \"zipkin\" If that becomes ambiguous, we can append for whatever else we do. I'd prefer consistency and something obvious to guess.\n(you might suggest this consistency is violated in ES, but that's a bit different as we have no dependency link transport, even if we have dependency link storage. if we did, we'd have to append to all the others, too.. I'd prefer plain old zipkin here, as otherwise folks might accidentally suggest it in support and have that be a lie!. is this an indirect dep of rabbit? if not, let's not use it (as it is unnecessary). use JUL instead.. volatile and start and close are different threads. careful about saving builder here as it is mutable and start is different thread than .build(). one way is to make your builder auto-value and make a copy of the builder (or just save off the params). otherwise add a TODO mentioning the edge-case mutability concern. kill all this for collector.acceptSpans as single-span encoding was a legacy of kafka specifically (we don't need to carry that here). there is insulation and also the ability to test locally in helpers like LazyCassandra3Storage might want to do that here. nit doc drift here perhaps elsewhere (default). oops this isn't used anymore (or needed). this should check the connection, if there's a way to. nit should be renamed to ITRabbitMQCollector (as it uses docker). this could possibly be moved to a normal unit test or left.. no matter. hmm why would this pass locally!. k. thx for the ref. I guess this is implicitly closed via the connection, ya?. since this is a class rule, it shouldn't run for each test, right?\nThe error looks like it is failing even before tests are run...\nRunning zipkin.collector.rabbitmq.ITRabbitMQCollector\n        \u2139\ufe0e Checking the system...\n        \u2714 Docker version is newer than 1.6.0\n        \u2714 Docker environment has more than 2GB free\n        \u2714 File should be mountable\n        \u2714 Exposed port is accessible\nStarting docker image rabbitmq:3.6-alpine\n[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.925 s <<< FAILURE! - in zipkin.collector.rabbitmq.ITRabbitMQCollector\n[ERROR] zipkin.collector.rabbitmq.ITRabbitMQCollector  Time elapsed: 5.925 s  <<< ERROR!\njava.lang.IllegalStateException: Unable to establish connection to RabbitMQ server\nCaused by: java.io.IOException\nCaused by: com.rabbitmq.client.ShutdownSignalException: connection error\nCaused by: java.io.EOFException. so basically I'd suggest having an alternative that reads from config.json and if a key like \"permanentZipkinUrl\" is present, show the link, and on clicking the link copy the result of /api/v2/trace/ID to a POST ${permanentZipkinEndpoint} (ex http://foo/bar/api/v2/spans) \nyou don't need to process the body in any way or even know about the v2 model. The server hosting the permanent repo doesn't need to be using v2 native storage, though obviously in the case of elasticsearch it should as v2 is a better investment for a concept like permanent.\nFor the handling, just copy the bytes from the first response to the request body of the second. the code will be easy and the results easy to reason with. The url associated with this task could be defined in crossroads.\nIf this isn't clear etc, lemme know!. one other consideration  is to add a tag to the first span (easy in v2 model), just insert one in the tags dict.\nex tags.outage='disaster happened in JIRA FOO-1234' then in your permanent store, you can search for \"outage\" and get this trace. Such a feature could be worth a little javascript to make it work.\nThis will allow you to label the trace such that in your long-term storage you can look it up somehow. Otherwise, eventually, these favorite traces will become annoying to browse.. this is the only one I don't know just by reading it. Can you put a comment line of why we do this, or tell me and I'll add it? Thx!. great info. ty. not needed right?. cc @openzipkin/cassandra . this part needs buffing. I will probably un-autovalue some or most of these as I need to override toString anyway as preparedStatement's toString is crap. @llinder if you know why we can drop futures in the places I made TODO, or why 2000 is special, please note and I'll backfill some comments. . I presume this means we are out of pages... thanks. will refactor slightly to ensure call modeling is coherent with this. is this to dodge something? If not, use \"zipkin\" as when there's no existing storage we don't have to dodge.. probably better to throw UnsupportedOperationException() here and elsewhere. long term, you'll probably need to decouple these (ex not implement SpanStore or SpanConsumer rather provide instances of them). I think the client makes a network connection, so it should be lazy initialized and closed. you can look for similar code where @Memoized is used. Make a builder even if it only calls this.. it will help you later. we don't do public constructors as defensive programming. you are missing a META-INF directory in src/main/resources as well a yaml file. you can look at others for this. The META-INF contents will let the server automagically find this. hmm maybe, but all the more reason to lazy initialize (so that the batch processor can be closed). Can you link to the code behind this? In general it is not great to have hidden buffers, as it makes what's in flight more difficult to see.. fwiw a collector2 module would likely duplicate this batching, for example. It matters a bit less now (provided the batching is smart enough to not write empty data). some other thoughts.. in tests, you'll need to disable batching or else they will fail on readback. also, probably best to hide the influxdb type (ex not accept it as a builder). this will give us flexibility later, for example if we end up with an okhttp or moshi conflict in the future, we can write native influxdb driver. for that matter, maybe best to not start with batch mode :). @michaelsembwever @llinder I noticed we have no SASI on this, which afaict means that we can't do a search for span name when service isn't present.. what should we do about this?. nix the -http. nit: InfluxDB Storage. set defaults here instead of in in the properties type. for the properties type https://github.com/openzipkin/zipkin/blob/master/zipkin-autoconfigure/collector-rabbitmq/src/main/java/zipkin/autoconfigure/collector/rabbitmq/ZipkinRabbitMQCollectorProperties.java is a better example as it ensures you only have to change properties once (here). ps this part only works if you can match on a substring of the trace ID. For example, in cassandra, we store the trace_id as either whatever was passed or right-16 characters when strictId=false. when strictId = false, we store the other part in a field trace_id_high. This field is ignored and never set unless strictId=false\nIn elasticsearch, we can change indexing to actually store traceID twice when strictId=false (via tokenizing). How to handle this in influxdb will depend on features.. @llinder @michaelsembwever ok I've rewritten everything at this point. everything has pretty toStrings, ex if you put a breakpoint, you can tell exactly what it is doing.. This type is pretty much done now, except docs. This works in parallel and when there are errors in a fan-out the call that caused the async failure is logged along with the problem.. ex. the \"error from \" + call already saved my butt today.. Some reads and writes have a fairly significant fan-out. Now you know which exact command resulted in a particular cassandra response error. I think you want INFLUXDB_PASSWORD: (trailing colon which makes it default to empty string). thanks captured. I couldn't quickly find out how to do a range query in a stress test, so left it out for now with a TODO. ps to proceed, and get practice, you can continue this blocking transform. Ex get the service names from the query result into a list. then return Call.create(thatList). This helps get most of the logic where it needs to be, and allows you to start implementing tests. A future move to async invocation can build on progress like this independently. add another bean like below as some components specifically look for v2 variants. V2StorageComponent is an adapter that is actually an instance of the v1 api.\njava\n  @Bean InfluxDBStorage v2Storage(V2StorageComponent component) {\n    return (InfluxDBStorage) component.delegate();\n  }. so is it normal to have the english repeated here and in the resource file? Just asking as I don't know. indeed see https://github.com/openzipkin/zipkin/issues/1804 and specifically https://issues.sonatype.org/browse/MVNCENTRAL-2870 which covers the root problem (those watching from home). @gianarb this is how to run a storage integration test from the CLI, though you can via the IDE as well. ex you can make sure the test setup works by running this... then take off the ignore tag to see what works and what doesnt. https://github.com/openzipkin/zipkin/issues/1836. nit when documenting and testing properties, we use lower-hyphen (although seems not the case in this README)\nEx from zipkin-server-shared.yml\n# Using ssl for connection, rely on Keystore\n      use-ssl: ${CASSANDRA_USE_SSL:false}. \"finagle.timeout\" follows this, so yeah abnormal.. maybe comment deserving? or perhaps better to scrap the strong and inline the annotation it refers to. considering this is for zipkin, why are we changing the unit to nanoseconds?. this is invalid.. why put the span id as parent id? If there's some special reason why, it should be in a comment.. why the word host? (this represents a ip:port string it looks like). please comment when doing something not zipkin conventional. this is confusing.. why are we using the word annotation to describe tags? If this is about special-casing the annotationQuery API, it should be mentioned.. however, this seems like it is not for such a reason... this looks not good :) let's not conflate the local and remote service name!. this is incorrect because of the mapping of the table to ns which should be us. The minDuration is in us. rename to seriesTags as confusing otherwise as zipkin also has a term tags. also comment on magic numbers, in this case 1. self-note: scrub this as the call is obviated higher up. I think here, we should assume zipkin v2 format unless we know heuristically it is not. Worst case we can copy/paste a modified version of https://github.com/openzipkin/zipkin-js/blob/master/packages/zipkin/src/jsonEncoder.js#L25 to map v2 json to v1 format.  Ex here are the tests for that: https://github.com/openzipkin/zipkin-js/blob/master/packages/zipkin/test/jsonEncoder.JSON_V1.test.js\nReason is that it is only time resource which is why the UI isn't internally rewritten to v2 yet. I don't want to encourage people to keep using v1. The best heuristic to detect if incoming json is v2 or not is presence of the \"localEndpoint\" field, which is only absent on instrumentation error.. ps I recall a conversation with @swankjesse that in java an int is better than a byte even if you only need a few flags. That the usage is easier and that it behaves like an int anyway in memory, iirc. correct me if wrong. PTAL cc also @bplotnick . guess I'm wondering if this needs to be refactored to use Schema.joinCondition() or similar?. ps still valid. once tests are running here, we've the framework in place to objectively squash issues with or without unit tests (some things should be unit tested, though). maybe add a comment here with the same rationale as the PR desc (which this allows dropping the SASI fields). @MrGlaucus @gianarb @drolando can you contribute chinese and italian text for this?. @MrGlaucus @gianarb @drolando can you contribute chinese and italian text for this?. shouldn't this be using the contextRoot variable? (if not, those proxy-mounting zipkin will be broken I think.. look at the README for more). ignore this.. just didn't want to litter with a commit update to satisfy license plugin. Spring endpoints already exist for these things.. noted here https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md#endpoints\nfor example, if you hit /configprops you'd see if ensureSchema was set to false or not. If I weren't distracted while you were troubleshooting, I would probably have mentioned to use that. there's also /autoconfig, too\nhttps://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html\nI use both of these a lot.. while we actively decided to not replicate spring docs, it might make sense to note both of these in the server README.\nIt is true that there aren't many debug statements in the storage code. Will add a couple.. This is incorrect change. the new library still supports encoding to v1 format, and it is used today https://github.com/openzipkin/zipkin/blob/master/zipkin2/src/main/java/zipkin2/codec/SpanBytesEncoder.java#L27. interesting that it throws so many things. It doesn't really matter though as we don't have api compatibility warranty on autoconfigure or zipkin-server types (though we should mention in a readme this hidden trivia). maybe just for good measure, add a test that when you set URI it is readable and hosts is ignored?. makes sense. yeap!. license is MIT (ok) and afaict no deps (good!). clever. was this there before?. here's the deprecation notice. is repackage redundant? or is there another reason why this isn't needed anymore?. nope.. just asking!. I'm pretty sure javac does this for us :P. oic yeah suppose it wouldn't pre-define the length of the builder, though the length here is only correct if ascii (utf 8 will overrun this). though maybe ES length is chars anyway? regardless, that's a different issue.. I'm fine with this.. the added tests 100% cover this new code. heh :P. we don't need this one. did this path exact match beforehand?. prometheus was formerly optional dep. I'd be careful on this and also make sure if this is to replace zipkin-autoconfigure-metrics-prometheus that the format especially of collector metrics is exactly the same. this will probably trip up the license formatter.. move it above the package?. does this need to be public?. is this the same as @Autowired on a field? (I mean does spring happily inject this?). HttpRequestDurationHandler is still referencing it.. maybe later, we will move that one to the zipkin-server, but I failed to get it working :P. ahh this must be for crossroads. ps people will set this with properties.. if there's a behavior change we might want to attack that in the UiConfiguration class. Appreciate the thought, but this change would have broken the entire ecosystem if published to maven central. Basically, people working now would not be able to simply update their versions.\nio.zipkin.zipkin2:zipkin is a completely different artifact if changed to io.zipkin.zipkin2:zipkin2. our dodging versions actually misaligned!. here is the actual change. cc @drolando this will be the replacement for that internal class you were looking at earlier\ncc also @wu-sheng. TODO: delete and reuse same code from v2 as it is the same. any impact to this? especially for existing modules? cc @zeagord cc @shakuzen. we don't have to inject optional right? can't we just get an empty list?. remove this.. as we don't log builder invocations. probably can accomplish the same at higher abstraction ex /autoconfig endpoint. this seems leaky abstraction as it isnt necessarily http but has http code. also extending throwable is a bit weird. maybe use a built-in exception or document how this is used as a signal. we don't currently define custom exceptions anywhere else in the codebase . hmm are you hinting there is a partial success here? ex you can return without exception?. we won't be filtering on two types. you can assume later it will be only zipkin2.span. heh no way jose ;) gave can't be a core dep as it conflicts. please remove forward refs to http filters which I am not sure what they will be. makes the change easier to pin down.. wow this javadoc is a bit out of date :P. fyi as of latest, this dance is no longer needed as we don't need to support v1 storage anymore from a bean POV. will defer to @abesto for any post-merge fixies. question.. I guess this means that zipkin2.span (annotation_query) results in an index named zipkin2.span_annotation_query_idx? so essentially we are re-creating that?. yeah this one is the exact match thing which our api uses. second question: does dropping this cleanup the data associated? and third: if we create an index below, is there a command we can use to rebuild the index based on old data?. so this means when you ask to automatically install the schema, we crash if an older version. Sounds fine as long as ENSURE_SCHEMA=false doesn't kill others. we could do something like detect if the indexing type is updated. Ex if the mode=CONTAINS then box annotationKey like before?. This is the ported test suite. sorry this was intellij cleanup. https://github.com/openzipkin/zipkin/issues/2132 thanks for the reminder!. note: all the autoconfigure types were package protected, meaning no-one could make a hard code dep on them without craziness. this one is public, but I have no problem creating a change in zipkin-aws to accomodate. here's the actual important side effect. is this required? I mean to preformat and code the word null.. seems like an accidental change.. unit test both with and without newline to ensure you've done what you want (and it stays like that). Or, remove the special casing of newlines, as annotations should certainly not have newlines. we cannot assume this.. there are spans that are messaging in nature (PRODUCER/CONSUMER). also, RPC spans themselves are optional... not terribly intuitive, but this should have been >=. the last patch worked, but wouldn't allow newer versions :P. does this or anything else mean we can drop any deps?. I would prefer the logic be handled here without changing the Call api. What if you just did spanConsumer.execute() instead in a try/finally where after or on exception you call the callback. what if we move the semaphore here?. actually we don't need the semaphore right? because in the case of kafka and rabbit (the only valid pull consumers) concurrency is controlled at that abstraction, and blocking will limit concurrency to stream count..\nI suppose you are asking how to expose how to block more ex on a certain exception?. probably easier to just catch exception. if the goal is to initialize (then later close) the admin client once, I think you will want to double check initialize with volatile ref like some other code in this repo\nreason is close isnt guaranteed to be same thread as check. is this really a dev only dep?. is this the key change on the trace going off screen?. nit: there's no value to using stringbuilder here. just exit early then retain existing logic if not present. now... is there a place for a unit test? :D. our ordering code orders root spans first. good comment. this was the bug. all the data are in microseconds, yet somehow this line thought it should be millis :P. most important change was removing this TODO :P. second most important change (to me) was removing dead data as it helps us understand what is needed by which screen. 3rd change is the actual UI nicies, like this horrible rendering of fractions of a us :D. implicit todo here is to figure out how best to re-jig the data, and not use magic numbers in tests. this test now works.. although a lot of others fail now.. whoot!. notable (id, shared) is not enough. This has to be kindof different. will think on a plane trip tonight about it. good point.. I think lack of suffix should be \u03bcs. Is it not already the case? good suggestion on us as well.. self-note make this illegal. self-note \"send w/o a shared endpoint\" rewrite. formatting.. not sure if it will hurt or not, but guessing the newline might. nothing ever read this!. this page is still v1 format. that will change in a later pull request. great. maybe.. doesn't seem obvious for a hashCode replacement. do you know one or should I manually write one like we have in java?. thanks. chevron? \nok by me :D. just curious what is i? I don't know. cc @ui interesting nuance here.. remember zipkin was originally written to only have remote spans. local (intermediate) spans were added later. This accidentally degraded the usefulness of what originally was equivelant to remote span depth to .. well a number :) cc also @drolando as I don't remember if you are on ui list or not.. cc also @vsen as you may have same concern in haystack. basically I'm not sure the value of tree depth when the tree is any kind of span. Seems more important to have depth of remote links.. fyi the logic below I will rewrite in a different PR after I figure out what it is doing.. this is new.. the rest of the file is extracted from another with no change. look below! see how we used to have serviceNameAndSpanCounts summing to a different count than spanCount!. these two tests are new. the rest of the file is extracted. no change in tests... just moved from the other file. I need to check this out.. I suspect it will not look right at all :P. it is fine. before we were getting NaN. @tacigar this might be useful for the new UI. the current one is still one row per ID not per span. So can't use this logic really... ps in a later change we can even pull up servers to fix the single host span issue. nit typo in the file enclosing this explination/explanation. I presume this is exact same as service-span. possible refactor possibility, as I recognise this class :P. suggestion\n   * Retrieves the list of tag values. oops thanks!. agree. check null. tags wont be null. I would also expect this to filter key names that actually should be indexed. no need if keys are immutable (collections.unmodifiable or util.sortedlist). since api is named TagStore redundant to say TagXxx in methods. throw exception on null or empty string instead . just return static list from config? less code. unpublic. be more specific in names. tags is too generic maybe siteTags. always linkedhashmap to prevent test flakes on order. ooops. unpublic. unpublic. Make this derived from a constant list from config I think.  maybe abstract base class instead of interface . fine to reuse other constant I think. if you make an abstract base type for the TagStore, it will be less tempting to make a specific api for this. Then, your base type has a builder which assigns the list tagKeys and then returns that (wrapped in a call to fit the api, but used directly elsewhere.. for example, if asking for values of a tag not in the list, you just return empty). That would be my advice.. note: the builder could be StorageComponent.Builder as it would be unexpected for TagStore to be built independently. I would make this GrpcCollector extends CollectorComponent, then make a static inner class and/or package private that extends SpanServiceGrpc.SpanServiceImplBase. Then the start on collector would call this, and close, could call stop. you can look at the kafka one for an example. Notably, this allows you to add a health check so that the normal health status can return (though in this case it probably isn't of much consequence unless it fails to start for some reason).. ticker is a guava type.. if it is really important to not use powermock we can..\n make DelayLimiter not final and expose a nanoTime method overridden in tests\n add a custom Ticker type only used in tests\nIn either case it would only be used in tests, as this is an internal type anyway. Probably the least messy and class bloat way would be to make an overridable method. otherwise we can leave it.. I'm not sure I follow how threads could spin here. we guard max size can't be zero. how can we turn this concern into a test?. I thought so too at first, but then I put in a parallel test and we leaked 12%. The problem this is trying to solve is to make the data structure bounded, which maybe isn't completely clear in the docs... going with overridable method for now as I also don't like the powermock cruft. ergh it bloats other things to do this as each suppression would also need a ref to the ticker. I think the cure is worse than the disease on this one. We use powermock elsewhere and the same pattern in RateLimiter in brave too.. I'll take it back.. there's only likely 5000 of these suppression references per instance, and max 4 instances (in the cassandra impl: ES has only 1), so who cares :P. now the tests run much much faster :). yeah we can't change this to an if statement as it leaks bad. For example, in 10k invocations under 2 contended threads it leaks between 100 and 400 with bound set to 1000.. will try a different way before giving up :P. yeah I think you are right. the main goal is not to go over size, vs worrying about going under. I will remove the check here and just let it go to 999 in a race condition which is better than the other way!. I think I got what you mean now. check me!. I changed to not loop on max size and instead just loop on suppressions.peek. rewrote so maybe it is ok now check me!. thanks for the tip on atomicinteger. on awaitility I'm deferring this by just removing the redundant latch thing as awaitTermination does the same thing anyway :P. so we use log4j2 and this dep makes boot up angry. also I haven't used the special currenttracecontext armeria provider here yet. this might be redundant but this test currently dies starting. making that not the case would be progress!. indent changes aren't intentional.. just I've not cleaned them out. oooo nesting.... :D. took me a few minutes to remember!\nyeah so we want to handle when no content type header is sent at all.. whoops this test does nothing. will rewrite. is this different than traceSummary.timestamp?. heh ok!. interestingly this passes which means I need to keep looking at that test trace! Maybe I missed the real culprit. nevermind this.. it is just a backport of another test. Eventhough there is some labor, I wanted to ensure tests are the same across zipkin-ui and zipkin-lens until we complete. this is the part that fixes it. maybe add a TODO as while I think your assessment is correct based on span row conversion, the underlying data only requires trace and span ID. We need to revisit things like this once UX is working as desired https://github.com/openzipkin/zipkin-api/blob/master/zipkin2-api.yaml. Add above this line\n// TODO: Verify which fields we should enforce here, as some are optional per https://github.com/openzipkin/zipkin-api/blob/master/zipkin2-api.yaml. haha whoops. the cookie is removed, but the page isn't reloaded.... lint made me put this there.. and yeah maybe that's what broke it :P. pinned due to react-scripts. follow-up PRs welcome. I mainly merged this to get @anuraaga out of the work pit of rebasing massive shifts. looks good enough to move forward with, but yeah any follow-ups very welcome!. an unrelated question is: why is a timestamp field named duration?. being a geek, and ignore me. Technically it isn't required to return them out of order :) it is just not required to return them in order either. iotw order is undefined.. it won't be fixed for everything because we never defined the order in the api and we know there are clones etc. iotw, it would only be \"fixed\" for that version of the impl. I think since the api doesn't define order, what you have is correct.. PS I do like the idea of us sorting even if it won't obviate doing it here. When you like I think all you'd need to do is a sort here https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/java/zipkin2/server/internal/ZipkinQueryApiV2.java#L159 and then add a test in ITZipkinServer.  That line should cover service, span names, and tag values in one go. did a quick check and I don't think any backends return immutable lists. so this should be fine. incidentally this is only used for v1 json, but yeah still used.. this is special cased as I can't think a better way to write a long as ascii.. and don't really want to think I know how a given buffer type will write a value backwards. For example, if using okio.buffer we don't need to implement this at all as it already has a native function to write a decimal as ascii... bug was this HttpHeaders.of(HttpStatus.OK) which was a hack no longer needed as cacheControl can be done directly now.. just added a comment that should have been there :). maybe alert the json is malformed? I assume this catch will result in someone uploading an incorrect file (ex just one span) and getting no feedback about it. correct?. old UI did something like this..\n\n. this change is fine as our source level is 1.8 now (for collectors). nit this needn't be public, but I'm not concerned about it enough to ask for a revision.. ",
    "dreid": "I ran into this problem in production, this is a trace id generated by our node-tryfer library.\n```\n\n\n\n5373169613188104192 < (math.pow(2, 63) - 1)\nTrue\n(5373169613188104192).bit_length()\n63\n```\n\n\n\nThe last thing seen in the zipkin collector logs for this traceid is:\n2012-11-09_04:01:45.70135 DEB [20121109-04:01:45.700] processor: Processing span: Span(5373169613188104192,GET,6149484339907264512,None,ArrayBuffer(Annotation(1352433670373000,sr,Some(Endpoint(2130706433,9000,rsr:node-rproxy)),None), Annotation(1352433671543000,ss,Some(Endpoint(2130706433,9000,rsr:node-rproxy)),None)),ArrayBuffer(BinaryAnnotation(request_headers,java.nio.HeapByteBuffer[pos=0 lim=198 cap=198],String,Some(Endpoint(2130706433,9000,rsr:node-rproxy))), BinaryAnnotation(user_id,java.nio.HeapByteBuffer[pos=0 lim=4 cap=4],String,Some(Endpoint(2130706433,9000,rsr:node-rproxy)))),false)\nFor successful traces this is immediately followed by a series of cassie operations.  In this case however there are no further log messages that appear related to this trace.\n. ",
    "elliottneilclark": "Pretty sure this one is already completed. Other than one location.\n. Pretty sure this one is already completed. Other than one location.\n. > this is pretty neat! have you found that hbase is fast enough to keep a fairly snappy web ui?\nYep HBase is happily running as a backend server for lots of different web ui's.  I haven't finished this part yet, but it should be plenty fast enough and scalable.\n. > this is pretty neat! have you found that hbase is fast enough to keep a fairly snappy web ui?\nYep HBase is happily running as a backend server for lots of different web ui's.  I haven't finished this part yet, but it should be plenty fast enough and scalable.\n. I changed the hbase version to a released version so it should be good to go on travis-ci now.\n. > For another project (hRaven), we had to modify the .travis.yml file to ensure umask was set to \"022\" prior to the test run\nAwesome thanks.  That just saved me a whole lot of debugging time. I'll add that in the next push.\n. So I pushed out more commits.  Mostly this is just so that everyone can see the idea behind how the indexes will look.\nI also added some scaladoc comments.  There are still a lot of places that need more (They're coming I promise).\nThanks everyone for all of the reviews already.\n. The latest push runs the web ui and is pretty well complete.\nI think there's still a timestamp bug and indexing annotation values.\n. > It'd be nice to get an aggregates store in there too.\nI'll give that a shot once I have some more tests and a lot more polish on the code.\n\nI'll give you some more review feedback next week.\n\nAwesome,  thanks for your help.\n. Pushed a new commit that squashed the old ones.  It contains:\n- rebase to master.\n- Moved to HTablePool so that the usage inside of HBaseTable is a little clearer on why we're doing that.\n- Changed annotation index to use annotation values\n- Cleaned up most of the source code nits that @franklinhu was talking about.  Thought the test code still could have some.  I'll address that soon.\n. Pushed aggregates.\n. I pushed a new version.  The mapping code still has some blocking; keeping some blocking just makes the code much cleaner and allows a fully tail recursive version of the retry loop.  In addition it allows me to completely stay off of all thread executors on cache hits.\n. Hmmm not sure what's going on I'll try and work out how to fix the tests.  They seem like a test only issue on travis.\n. Sure.  Here's a rebased version.  It has:\n- New dependencies\n- New version of HBase\n- Better threading\n- Un constrained thread creation is a bad idea with the # of qps that hbase can generate :-)\n. Sure.  Here's a rebased version.  It has:\n- New dependencies\n- New version of HBase\n- Better threading\n- Un constrained thread creation is a bad idea with the # of qps that hbase can generate :-)\n. Ok, I think that addresses all of your thoughts.\nI added more thread pools so they can spin up faster when load is applied.\n. Ok, I think that addresses all of your thoughts.\nI added more thread pools so they can spin up faster when load is applied.\n. Whoops yeah some debugging stuff got left in there.  Lets run this past ci again.\n. Whoops yeah some debugging stuff got left in there.  Lets run this past ci again.\n. Yep ready to go in.\n. Yep ready to go in.\n. +1\n. +1\n. Failure seems un-related.  Seems like someone's maven repo was down.\n. Failure seems un-related.  Seems like someone's maven repo was down.\n. I've got a newer version in a few mins.\n. I've got a newer version in a few mins.\n. Yep that should only be called as the thread pool starts expanding (Probably only on the first few calls to hbase).\n. HBase can't set a ttl on a row.  It has a ttl automatically associated with the column family.  So when the data is initially put it the ttl is calculated from the initial timestamp.\nhttp://hbase.apache.org/book.html#ttl\nI'll add some better comments here.\n. Will do\n. will do\n. K, I'll clean it up.\n. Will do\n. Asynchbase doesn't support the newer versions of hbase.  And the HBase team is hoping to use this to test the newest version of HBase.\n@tsuna hasn't finished his newest version of asynchbase. When he does it will probably be pretty easy (and desirable ? ) to move to that.\n. I'll clean it up anyway.  No harm in removing locks where possible.\n. Thanks Benoit.\n. SBT compile fails without it.  I haven't had time to track down why yet.\nYay for the wonderful world of Hadoop java dependencies (aka everything ever written in java ever).\n. So I actually am un-doing that change in the next push.\nI put that in there because there is a disconnect between serviceNames and serviceName.  However it looks like serviceName is used for the ui, and It's probably better to just take note of the weirdness and keep the user experience better.\n. The thought was that this class has nothing specific to zipkin in it.  It's really just an async facade on the hbase client.\n. Will do.\n. Will Do.\n. Will Do.\n. I don't think that Local is right here.\nLocal seems like it should be used to make sure that a single value moves along with an executor's tree.  Local proviceds update/set.  None of these are really what I need.  \nHere I'm really using ThreadLocal as a way to ensure that no two threads are accessing the same HTable (HTable has buffers that aren't protected from concurrent access).  \nI should probably change this to use HTablePool, but that's a lot more code.\n. Yea they are returned in time sorted order.  So if we need to limit it I would like to take the newest spans.\n. Done.\n. Done\n. Done.  Thanks\n. Done.\n. Sure.\n. Ok.\n. Seems like making this blocking is actually better here.  It means there are a lot fewer calls to an ExecutorService on the most used path.\nSince most mappings will be in the cache keeping it blocking means that only one future will ever be created on hits.\n. I could put an assert here.\n. Yeah I think I optimized too much last night and I optimized away the retry.... Oooops.\n. The cache is a concurrent hash map.  If putting into a map fails there are bigger issues.  I could add an assert to make sure that the value returned is what was expected (eg if we're replacing make sure that we're replacing a mapping that is exactly equal to what we just created).\n. I actually really liked the idea of adding an assert just to make 100% sure that the mapping table stays consistent.\n. I've got another version that has much better comments.  I'll post that in a second after I do some more local testing.\n. I would have thought so too.  However when passing a Mapping of type T to Retry, the type falls back to java.lang.Object if Null isn't in the parameterized type here\n. Sure I'll use that now.\n. Yep that was left over from debugging so I could put breakpoints.  I'll clean that up.\n. Will do.\n. I broke the executors up a little bit more.  I'll post a version.\n. Nice, I like the for comprehension.  Makes it more explicit that it's iterative.\n. ",
    "olix0r": "https://github.com/twitter/zipkin/tree/resolve-localhost\n. => java -version\njava version \"1.7.0_04\"\nJava(TM) SE Runtime Environment (build 1.7.0_04-b21)\nJava HotSpot(TM) 64-Bit Server VM (build 23.0-b21, mixed mode)\n. https://github.com/twitter/zipkin/pull/204\n. Are you suggesting that this be implemented as a Finagle Announcer?  e.g.\nscala\nval scribe = Thrift[Scribe].serveAndAnnounce(\"zk:scribe!//path/dir\", ...)\nThis does sound quite a bit cleaner.\nNote that scribe has its own service registry scheme that is not compatible with finagle-serverset.\n. @adriancole @mosesn's proposal suggests removing trait-based approach entirely to push this problem into finagle, so I think this fits in line with what you want to achieve and isn't predicated on a larger refactor.\n. ",
    "fspillner": "+1 \nThis makes the configuration API more easier to understand / use. And you decouple the internal of the configuration classes, so you can feel free to change it without change the interface of this new configuration API. These changes make the configuration more fluent to read and will have positive affect on the documentation of the configuration API. I like the idea, that the whole Zipkin project can be configured on one place with multiple builders. \n. ",
    "PinkyJie": "Finally, I got a solution using proxy...\n. I found an instruction file centos6-quickstart.txt in the 'doc' directory, with the help of this file I finally launched the project. \nNew Question!!! It seems that zipkin can not find the services. I open the localhost:8080 and see the zipkin UI, the Service droplist has no items. I open the localhost localhost:9900 and localhost:9901, I can see the hello text.\nI think it is the problem of zookeeper, it can not find the servers, right? When I launch the bin/collector and bin/query, I find some WARNs and ERRORs.\nWAR [20121231-15:19:57.754] zookeeper: server set empty!\nINF [20121231-16:14:32.895] cassie: Mapping cluster...\nFAT [20121231-16:14:32.903] cassie: error mapping ring\nFAT [20121231-16:14:32.903] cassie: com.twitter.finagle.FailedFastException\nFAT [20121231-16:14:32.903] cassie:     at com.twitter.finagle.NoStacktrace(Unknown Source)\n. Yes, I follow the install page step by step. \nI wanna know that, do I need to rewrite the page localhost:9900 and localhost:9901 to provide the source services for monitor? \n. ",
    "jamescway": "I had this error error mapping ring\nI had a network aware instance of cassanda, with replication settings that were different than my laptop.  Changed it with the statment below.\n```\ncqlsh> describe schema;\nCREATE KEYSPACE \"Zipkin\" WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1': '1'}  AND durable_writes = true;\ncqlsh> ALTER KEYSPACE \"Zipkin\" WITH REPLICATION =\n  { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n``\n. I had this errorerror mapping ring`\nI had a network aware instance of cassanda, with replication settings that were different than my laptop.  Changed it with the statment below.\n```\ncqlsh> describe schema;\nCREATE KEYSPACE \"Zipkin\" WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1': '1'}  AND durable_writes = true;\ncqlsh> ALTER KEYSPACE \"Zipkin\" WITH REPLICATION =\n  { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n``\n. Do you guys think we could merge this branch?  I have some code based on this which allows the zipkin-tracer to use kafka.\n. Do you guys think we could merge this branch?  I have some code based on this which allows the zipkin-tracer to use kafka.\n. @mosesn yea LGTM :+1: \n. @mosesn yea LGTM :+1: \n. @sprsquish comments addressed\n. @sprsquish comments addressed\n. Now that the big switch to use datastax is is committed, we can revisit this issue... :grin:\n. Now that the big switch to use datastax is is committed, we can revisit this issue... :grin:\n. :+1: not really expecting the built to succeed\n. :+1:  can't hurt!  based on what @eirslett was saying u won't be able to observe this behavior until u merge to master rite?\n. fixed by https://github.com/openzipkin/zipkin/pull/476\n. :+1: :+1:  (u get 2)  super awesome!\n. +1 to this.  This would solve a lot of the monolithic problems that we've been facing with the build and simplify things.  \n. I took another look at zipkin-kafka and I realized that we don't actually use it.  It seems like this is an \"experimental\" library as the readme says, where zipkin-receiver-kafka is the module that we use.  I wonder if anybody really uses zipkin-kafka.\n. +1 looks like this was a lot of work!\n. :+1: \n. +1 cut the fat! :cow2: \n. @abesto, not related at all, :+1: to killing it\n. having some problems after rebase...\ntraced it down to this lineaddSbtPlugin(\"com.twitter\" %% \"scrooge-sbt-plugin\" % \"3.19.0\")previously it wasaddSbtPlugin(\"com.twitter\" %% \"scrooge-sbt-plugin\" % \"3.16.3\")`\nWith the newer lib sbt publish now causes: \n[error] (zipkin-collector-core/*:publish) java.lang.NoSuchMethodError: org.jboss.netty.handler.codec.http.HttpRequest.setHeader(Ljava/lang/String;Ljava/lang/Object;)V\nhere's the full stack trace\n```\n\nlast zipkin-collector-core/*:publish\njava.lang.NoSuchMethodError: org.jboss.netty.handler.codec.http.HttpRequest.setHeader(Ljava/lang/String;Ljava/lang/Object;)V\n    at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.construct(NettyAsyncHttpProvider.java:693)\n    at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.buildRequest(NettyAsyncHttpProvider.java:650)\n    at com.ning.http.client.providers.netty.NettyConnectListener$Builder.build(NettyConnectListener.java:144)\n    at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.doConnect(NettyAsyncHttpProvider.java:1070)\n    at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.execute(NettyAsyncHttpProvider.java:935)\n    at com.ning.http.client.AsyncHttpClient.executeRequest(AsyncHttpClient.java:499)\n    at dispatch.HttpExecutor$class.apply(execution.scala:47)\n    at dispatch.Http.apply(execution.scala:12)\n    at dispatch.HttpExecutor$class.apply(execution.scala:42)\n    at dispatch.Http.apply(execution.scala:12)\n    at bintry.Requests.request(Client.scala:42)\n    at bintry.Methods$Repo$Package$Version$Upload.apply(Methods.scala:177)\n    at bintray.BintrayIvyRepository.put(Resolver.scala:54)\n    at org.apache.ivy.plugins.resolver.RepositoryResolver.put(RepositoryResolver.java:234)\n    at org.apache.ivy.plugins.resolver.RepositoryResolver.publish(RepositoryResolver.java:216)\n    at sbt.IvyActions$$anonfun$publish$3.apply(IvyActions.scala:336)\n    at sbt.IvyActions$$anonfun$publish$3.apply(IvyActions.scala:335)\n    at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)\n    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n    at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)\n    at sbt.IvyActions$.publish(IvyActions.scala:335)\n    at sbt.IvyActions$$anonfun$publish$1$$anonfun$apply$1.apply$mcV$sp(IvyActions.scala:121)\n    at sbt.IvyActions$$anonfun$publish$1$$anonfun$apply$1.apply(IvyActions.scala:121)\n    at sbt.IvyActions$$anonfun$publish$1$$anonfun$apply$1.apply(IvyActions.scala:121)\n    at sbt.IvyActions$.withChecksums(IvyActions.scala:130)\n    at sbt.IvyActions$.sbt$IvyActions$$withChecksums(IvyActions.scala:125)\n    at sbt.IvyActions$$anonfun$publish$1.apply(IvyActions.scala:121)\n    at sbt.IvyActions$$anonfun$publish$1.apply(IvyActions.scala:114)\n    at sbt.IvySbt$Module$$anonfun$withModule$1.apply(Ivy.scala:155)\n    at sbt.IvySbt$Module$$anonfun$withModule$1.apply(Ivy.scala:155)\n    at sbt.IvySbt$$anonfun$withIvy$1.apply(Ivy.scala:132)\n    at sbt.IvySbt.sbt$IvySbt$$action$1(Ivy.scala:57)\n    at sbt.IvySbt$$anon$4.call(Ivy.scala:65)\n    at xsbt.boot.Locks$GlobalLock.withChannel$1(Locks.scala:93)\n    at xsbt.boot.Locks$GlobalLock.xsbt$boot$Locks$GlobalLock$$withChannelRetries$1(Locks.scala:78)\n    at xsbt.boot.Locks$GlobalLock$$anonfun$withFileLock$1.apply(Locks.scala:97)\n    at xsbt.boot.Using$.withResource(Using.scala:10)\n    at xsbt.boot.Using$.apply(Using.scala:9)\n    at xsbt.boot.Locks$GlobalLock.ignoringDeadlockAvoided(Locks.scala:58)\n    at xsbt.boot.Locks$GlobalLock.withLock(Locks.scala:48)\n    at xsbt.boot.Locks$.apply0(Locks.scala:31)\n    at xsbt.boot.Locks$.apply(Locks.scala:28)\n    at sbt.IvySbt.withDefaultLogger(Ivy.scala:65)\n    at sbt.IvySbt.withIvy(Ivy.scala:127)\n    at sbt.IvySbt.withIvy(Ivy.scala:124)\n    at sbt.IvySbt$Module.withModule(Ivy.scala:155)\n    at sbt.IvyActions$.publish(IvyActions.scala:114)\n    at sbt.Classpaths$$anonfun$publishTask$1.apply(Defaults.scala:1283)\n    at sbt.Classpaths$$anonfun$publishTask$1.apply(Defaults.scala:1282)\n    at scala.Function3$$anonfun$tupled$1.apply(Function3.scala:35)\n    at scala.Function3$$anonfun$tupled$1.apply(Function3.scala:34)\n    at scala.Function1$$anonfun$compose$1.apply(Function1.scala:47)\n    at sbt.$tilde$greater$$anonfun$$u2219$1.apply(TypeFunctions.scala:40)\n    at sbt.std.Transform$$anon$4.work(System.scala:63)\n    at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226)\n    at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226)\n    at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17)\n    at sbt.Execute.work(Execute.scala:235)\n    at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226)\n    at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226)\n    at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159)\n    at sbt.CompletionService$$anon$2.call(CompletionService.scala:28)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n```\n\nSeems like maybe the lib bintry is using an old version of dispatch maybe?\n. looks like we have netty-3.10.1.Final.jar and async-http-client-1.8.10.jar both loading netty\n. @eirslett yeah, dispatch uses it, I have a PR merged into bintry going to find out if he will release it.  Actually scrooge has the newer version of netty, bintry (dep of bintray-sbt has an older version.\n@adriancole rolling back and using scrooge-sbt-plugin\" % \"3.16.3\" would solve the problem in the meantime\nWhat do u guys think?  Going to roll it back to 3.16.3 feel free to thumbs down :p\n. After update to use gradle https://github.com/openzipkin/zipkin/pull/503, much of this stuff is not relevant, going to close and start another PR\n. @abesto just noticed that tool, I'll give it a shot\n. :+1: :+1: :+1:  looks like u got everything\n. :+1: looks great\n. :+1:  I guess I'll have to rebase 0.3.20 :p\n. :+1:  but it doesn't seem like its used anywhere?\n. don't have strong feelings about mustache versions, updating lgtm :100: \n. :star: :+1: :star: \n. :+1: \n. :+1: \n. :+1: :+1: \n. :+1: \n. @adriancole I'll rebase and test it out\n. +1 flags.  We've had success using and adding flags which have 1) default values, 2) prints usage if misused, 3) in the compiled code, 4) already being used in many places.  Only con was the documentation was hard to find.\n. :+1: \n. :+1: \n. :+1:  might be good in the mac quick start too :p\n. :+1:  very interesting\n. :+1:  nice pattern to follow\n. Regarding \"Building from oraclejdk7\", is this the final version we're going with?\n. @adriancole @abesto @rtyler any outstanding issues?  I ready to merge it.\n. @vprithvi publishing to local repo's wasn't something I tested.  If its a concern, lets create an issue for it and we can address in another PR.\n. :+1: \n. it wasn't sleeping, I figured rather than make a new method, i modified this existing one.  Then added the retry code to the kafka code, because it didn't backoff and retry, it just died\n. :star: \n. :+1:\n. @gneokleo if you have time you could copy your code into the main method of collector. \nAlternatively, we could merge this and file a ticket to migrate collector away from builders with this example.  @adriancole ?\n. nice I can update the kafka code based on this commit\n. :+1: \n. :+1: very cool so basically is has all the dependencies\n. :+1: look at that its :green_apple: \n. and also BlockingItemQueue Test.\n. answering @adriancole's points\n- the name of the pure-java impl\n  - not sure if java needs to be in the name since its self evident?  +1 to core or something based on current name scheme.\n- whether it is ok to ditch cassandra-aggregates temporarily\n  - :+1: , but don't know if there's people out there that might be in an awkward spot, hopefully they come on gitter and ask for help.\n. Interesting that you uncovered the testing framework.  I restarted the travis job, but there was a problem with the redis span store spec.  Not sure if this is an intermittent failure or not.\n. :+1: \n. nice alternate workaround :smile:  :+1: \n. :+1:  cool new yield obj\n. :+1: lgtm\n. :+1: \n. added longer timeouts for all Awaits in ItemQueueTest, due to failures in travis\n. Done have a ton of context, but everything lgtm except that one thing, which I'm assuming is necessary.\n:+1:  nice deletes :100: \n. Yeah funny -b didn't fail before.\n:+1:  looks like the red build is from publishing not building.\n. - Don't see the new travis.yml behavior?\n- also I think the branch name set was not master (it was the tag name).  So in order to push the release notification commits to master, had to change the branch.\n. :broken_heart: \n. :+1: \n. LGTM :)\n. LGTM :)\n. :+1: \n. :+1: \n. @michaelsembwever @adriancole tested locally, updated to factory, fixed up getorelse stmts.  Thx for the review!\n. @adriancole gtg\n. :+1:  lol\n. :+1:  lol\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. @abesto I remember you removing all the include stmts, in favor of include a,b,c... \nHopefully this doesn't cause any problems\n. :+1: \n. :+1: \n. :+1: :cake: \n. got to go :100: \n. :+1: \n. :+1:, maybe could use some simple docs?\n. looks great :)\n. Definitely some room for additional work like you mentioned, but sounds like we could revisit. \n:+1: \n. Definitely some room for additional work like you mentioned, but sounds like we could revisit. \n:+1: \n. :shipit: merge it!\n. :shipit: merge it!\n. :+1: \n. :+1: \n. :+1: just had a non-merge-blocking question\n. :+1: just had a non-merge-blocking question\n. :+1:  :+1: \n. :+1:  :+1: \n. nice :ramen: \n. nice :ramen: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: thanks for doing this, I left a NIO comment\n. :+1: thanks for doing this, I left a NIO comment\n. :+1: \n. I guess all travis wants is some output..?\n. thanks!\n. thanks!\n. :+1:  to :green_heart: build\n. :+1:  to :green_heart: build\n. :+1: \n. :+1: \n. :+1: \n. We're still on jdk7 but that doesn't need to stop us.  What would be some alternatives for those that are stuck for the time being?\n. We're still on jdk7 but that doesn't need to stop us.  What would be some alternatives for those that are stuck for the time being?\n. The schema functions better as an example to follow at the moment.  For individual cassandra clusters there are enough differences (i.e. datacenter, keyspace, users, etc) that make a generalized setup not a good idea.  In the future we could consider creating database migrations at which point we could think about using config variables.  Not sure when the road map for that would be, but would welcome any assistance :smile:  Another alternative could be a better readme outlining what variables could be changed and what some example fields might be.\n. Sorry my tech-ese is not too good here.  What do u mean by collector/query doing DDL?  I would have guessed that creating a config would be more of a DDL\n. @pbetkier ah sorry I didn't realize it was hard coded, that does sound like a problem.  Disabling sounds like a good idea!\n. :+1: \n. Pretty cool, kafka-unit allows you to delete all that test code\n. yeah, now that you mention that, this seems totally unnecessary and seems to override expected behavior, removing altogether.\n. how many times does it retry?  Kind of looks like 3 by default? Looking for reference...\n. yeah its 3 times  \"Beyond Bundler, you can wrap commands using the function travis_retry which checks the return code of a command, retrying it three times if the return code is non-zero.\"\n. is it still possible to run https://github.com/openzipkin/zipkin/blob/master/zipkin-collector-service/config/collector-cassandra.scala ?\n. yeah thats correct, I realized that only after failing inside of the subprojects block many times.\n. @abesto so this was a question, does the build happen here already?  Are you saying that bintrayUpload should cause the build to happen?\n. also it seems like -x test has stopped testing of the entire build O_o\n. ^^^ is not correct, it was the code in travis under script tag.  Moved that into a before_script section.\n. What was the reason for defaulting to 30s?  I was seeing the queue having room after about 1s.\n. Maybe moving this into here (main.scala)? \nAnd another change to the command line args in build.gradle\n. hopefully its not a poorly documented hack O_o\n. For the timeout \n. It could also be done without future and pass the end time\n. I don't have a lot of experience with starting ostrich services, so I hope this is the accept way of doing it.  Perhaps a bit of shoehorning legacy?\n. need to rebase this out\n. weird thing that travis encrypt put into here\n. I started with that at first, but I figure explicitness over inference, eitherway WFM\n. I guess the method name wasn't too funny womp womp, thanks for finding that :smile: \n. @adriancole actually I couldn't find it, am I doing something different?\n\n. nevermind getConfiguration.getProtocolOptions().getAuthProvider() \n. justed updated the spacing\n. Not that I can see, I think you implemented the SpanStoreBuilder to be backwards compatible.  Will roll it forward.\n. It looks like if we use the factory we would use it here?  Maybe I could do that in another PR.\nor were you thinking something like:\nscala\nobject TestFactory extends App with CassandraSpanStoreFactory\ncluster = TestFactory.createClusterBuilder()\nNot sure if this is valid ^^^ or if there's a more direct way to do it.\n. @adriancole yeah sorry forgot to mention, already done when he said it :smile: \n. i'd be curious what happens at this size\n. hard to find documentation indeed, ended up grepping finagle repo\n. yeah ok, thats what I was asking Zoltan\n. makes sense :)\n. In a heavy write scenario would it be possible for the memory usage to grow to an unwieldy size?  In this case maybe it would be worth using an LRUMap (threadsafe), which evicts based on size and might reduce the need to nuke it periodically.  I also heard that the Guava cache is super cool and full featured.  Another possible scenario would be getting cache misses if the data is in the Set in another thread.\n. Limiting the size sounds good, I think evicting would be important to prevent spill over and cache would always be warm (maybe thread local lrumap O_o).  Sounds like we could just revisit these issues should they surface.\n. Yeah, I think it would definitely help.  We would have something we could tune, should a problem arise.\nHave fun with the flags :stuck_out_tongue_winking_eye:\n. reminds me of ruby's method_missing LOL\n. out of curiosity, does this break anything for people who used the old definition?\n. cool.  maybe chat with you later about integration testing offline.\n. I don't suppose theres something we could do to simplify this?  Do you think it would be worth abstracting this to some kind of type type SpanReceiver = A => B => C.\n. [non-blocking] is there any significance to 0.0.0.0 in this case? is the intention the same as localhost?\n. cool!\n. ",
    "capotej": "woot!\nYeah, I'm looking to replace my hacky config system with something cleaner in the future, perhaps https://github.com/typesafehub/config\n. ",
    "beerium": "[info] Loading project definition from /home/qinyuchun/source/zipkin-setup/zipkin/project\n[info] Updating {file:/home/qinyuchun/source/zipkin-setup/zipkin/project/}default-64d38a...\n[info] Resolving com.twitter#sbt-package-dist;1.0.5 ...\n[error] Server access Error: Connection reset url=http://freemarker.sourceforge.net/maven2/com/twitter/sbt-package-dist/1.0.5/sbt-package-dist-1.0.5.jar\n[info] Resolving ivysvn#ivysvn;2.1.0 ...\n[info] Resolving org.markdownj#markdownj;0.3.0-1.0.2b4 ...\n[info] Resolving org.freemarker#freemarker;2.3.16 ...\n[info] Resolving com.twitter#sbt11-scrooge;3.0.0 ...\n[error] Server access Error: Connection reset url=http://freemarker.sourceforge.net/maven2/com/twitter/sbt11-scrooge/3.0.0/sbt11-scrooge-3.0.0.jar\n[info] Resolving com.twitter#sbt-thrift2;0.0.1 ...\nqinyuchun@ssmc-ProLiant-BL460c-G1:~/source/zipkin-setup/zipkin$ \nqinyuchun@ssmc-ProLiant-BL460c-G1:~/source/zipkin-setup/zipkin$ bin/sbt update dist-packages\n[info] Loading project definition from /home/qinyuchun/source/zipkin-setup/zipkin/project\n[info] Updating {file:/home/qinyuchun/source/zipkin-setup/zipkin/project/}default-64d38a...\n[info] Resolving com.twitter#sbt-package-dist;1.0.5 ...\n[error] Server access Error: Connection reset url=http://freemarker.sourceforge.net/maven2/com/twitter/sbt-package-dist/1.0.5/sbt-package-dist-1.0.5.jar\n[info] Resolving ivysvn#ivysvn;2.1.0 ...\n[info] Resolving org.markdownj#markdownj;0.3.0-1.0.2b4 ...\n[info] Resolving org.freemarker#freemarker;2.3.16 ...\n[info] Resolving com.twitter#sbt11-scrooge;3.0.0 ...\n[error] Server access Error: Connection reset url=http://freemarker.sourceforge.net/maven2/com/twitter/sbt11-scrooge/3.0.0/sbt11-scrooge-3.0.0.jar\n[info] Resolving com.twitter#sbt-thrift2;0.0.1 ...\n[error] Server access Error: Connection reset url=http://freemarker.sourceforge.net/maven2/com/twitter/sbt-thrift2/0.0.1/sbt-thrift2-0.0.1.jar\n[info] Resolving com.eed3si9n#sbt-assembly;0.8.2 ...\n[info] Resolving org.scala-tools.sbt#sbt_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#main_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#actions_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#classfile_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#io_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#control_2.9.1;0.11.2 ...\n[info] Resolving org.scala-lang#scala-library;2.9.1 ...\n[info] Resolving org.scala-tools.sbt#interface;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#logging_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#process_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#classpath_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#launcher-interface_2.9.1;0.11.2 ...\n[info] Resolving org.scala-lang#scala-compiler;2.9.1 ...\n[info] Resolving org.scala-tools.sbt#incremental-compiler_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#collections_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#api_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#persist_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbinary#sbinary_2.9.0;0.4.0 ...\n[info] Resolving org.scala-tools.sbt#compile_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#ivy_2.9.1;0.11.2 ...\n[info] Resolving org.apache.ivy#ivy;2.2.0 ...\n[info] Resolving com.jcraft#jsch;0.1.31 ...\n[info] Resolving commons-httpclient#commons-httpclient;3.1 ...\n[info] Resolving commons-logging#commons-logging;1.0.4 ...\n[info] Resolving commons-codec#commons-codec;1.2 ...\n[info] Resolving org.scala-tools.sbt#completion_2.9.1;0.11.2 ...\n[info] Resolving jline#jline;0.9.94 ...\n[info] Resolving org.scala-tools.sbt#run_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#task-system_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#tasks_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#tracking_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#cache_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#testing_2.9.1;0.11.2 ...\n[info] Resolving org.scala-tools.testing#test-interface;0.5 ...\n[info] Resolving org.scala-tools.sbt#compiler-interface;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#precompiled-2_8_1;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#precompiled-2_8_0;0.11.2 ...\n[info] Resolving org.scala-tools.sbt#precompiled-2_9_0;0.11.2 ...\n[info] downloading http://maven.twttr.com/com/twitter/sbt-thrift2_2.9.1_0.11.2/0.0.1/sbt-thrift2-0.0.1.jar ...\n[warn]  [FAILED     ] com.twitter#sbt-thrift2;0.0.1!sbt-thrift2.jar: Downloaded file size doesn't match expected Content Length for http://maven.twttr.com/com/twitter/sbt-thrift2_2.9.1_0.11.2/0.0.1/sbt-thrift2-0.0.1.jar. Please retry. (30591ms)\n[warn]  [FAILED     ] com.twitter#sbt-thrift2;0.0.1!sbt-thrift2.jar: Downloaded file size doesn't match expected Content Length for http://maven.twttr.com/com/twitter/sbt-thrift2_2.9.1_0.11.2/0.0.1/sbt-thrift2-0.0.1.jar. Please retry. (30591ms)\n[warn] ==== twitter.com: tried\n[warn]   http://maven.twttr.com/com/twitter/sbt-thrift2_2.9.1_0.11.2/0.0.1/sbt-thrift2-0.0.1.jar\n[info] downloading http://maven.twttr.com/ivysvn/ivysvn/2.1.0/ivysvn-2.1.0.jar ...\n[warn]  [FAILED     ] ivysvn#ivysvn;2.1.0!ivysvn.jar: Downloaded file size doesn't match expected Content Length for http://maven.twttr.com/ivysvn/ivysvn/2.1.0/ivysvn-2.1.0.jar. Please retry. (57953ms)\n[warn]  [FAILED     ] ivysvn#ivysvn;2.1.0!ivysvn.jar: Downloaded file size doesn't match expected Content Length for http://maven.twttr.com/ivysvn/ivysvn/2.1.0/ivysvn-2.1.0.jar. Please retry. (57953ms)\n[warn] ==== twitter.com: tried\n[warn]   http://maven.twttr.com/ivysvn/ivysvn/2.1.0/ivysvn-2.1.0.jar\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  ::              FAILED DOWNLOADS            ::\n[warn]  :: ^ see resolution messages for details  ^ ::\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  :: ivysvn#ivysvn;2.1.0!ivysvn.jar\n[warn]  :: com.twitter#sbt-thrift2;0.0.1!sbt-thrift2.jar\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[error] {file:/home/qinyuchun/source/zipkin-setup/zipkin/project/}default-64d38a/*:update: sbt.ResolveException: download failed: ivysvn#ivysvn;2.1.0!ivysvn.jar\n[error] download failed: com.twitter#sbt-thrift2;0.0.1!sbt-thrift2.jar\nProject loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? \n. wget is ok , but where to put this file ?\n. Sure, I am in China. but I guess it is not a VPN problem.\nI rebuild  through VPN , but problem still.\nand let's focus on the log  below:\n[warn] [FAILED ] com.twitter#sbt-thrift2;0.0.1!sbt-thrift2.jar: Downloaded file size doesn't match expected Content Length for http://maven.twttr.com/com/twitter/sbt-thrift2_2.9.1_0.11.2/0.0.1/sbt-thrift2-0.0.1.jar. Please retry. (52886ms)\nDownloaded file size doesn't match expected Content Length \n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nby the way,  http://maven.twttr.com/com/twitter/sbt-thrift2_2.9.1_0.11.2/0.0.1/sbt-thrift2-0.0.1.jar  can access without VPN\n. ",
    "eodgooch": "You can delete it. There are a few inconsistencies and could be updated but\nthe ubuntu guide should cover centos install as well..\nAaron\nOn Tue, Mar 12, 2013 at 5:16 PM, Brian Degenhardt\nnotifications@github.comwrote:\n\nCan you comment on how relevant the existing centos guide is?\nShould we delete it or is it still useful?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/pull/234#issuecomment-14803944\n.\n. \n",
    "netease-datastream": "I run on Linux\nversion is \nDistributor ID:Debian\nDescription:Debian GNU/Linux 7.0 (wheezy)\nRelease:7.0\nCodename:wheezy\nAt 2013-04-03 01:19:54,\"Brian Degenhardt\" notifications@github.com wrote:\nI've never seen that before. What OS is this on?\nOn Mon, Apr 1, 2013 at 10:36 PM, Flume-patch-netease \nnotifications@github.com wrote:\n\nWhen I start web UI and query the trace log, it throws a Unknown method\nexception\nUnsafe is a class in rt.jar. But I try all JDK such as JDk\n7u17,openJDK7,JDK6u22,openJDK6 , I still cannot resolve this problem.\nI wonder this\nsun.misc.Unsafe.copyMemory(Ljava/lang/Object;JLjava/lang/Object;JJ)V In\nwhich JDK version .\nFAT [20130402-13:28:03.663] ZipkinWeb:\norg.apache.thrift.TApplicationException: Internal error processing\ngetTraceSummariesByIds: 'java.lang.NoSuchMethodError:\nsun.misc.Unsafe.copyMemory(Ljava/lang/Object;JLjava/lang/Object;JJ)V'\nFAT [20130402-13:28:03.663] ZipkinWeb: at\norg.apache.thrift.TApplicationException.read(TApplicationException.java:108)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.zipkin.gen.ZipkinQuery$FinagledClient.decodeResponse(ZipkinQuery.scala:5263)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.zipkin.gen.ZipkinQuery$FinagledClient$$anonfun$getTraceSummariesByIds$1.apply(ZipkinQuery.scala:5480)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.zipkin.gen.ZipkinQuery$FinagledClient$$anonfun$getTraceSummariesByIds$1.apply(ZipkinQuery.scala:5479)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.util.Future$$anonfun$flatMap$1.apply(Future.scala:477)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.util.Future$$anonfun$flatMap$1.apply(Future.scala:476)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.util.Promise$$anonfun$transform$1.apply(Future.scala:883)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.util.Promise$$anonfun$transform$1.apply(Future.scala:879)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.util.Promise$$anonfun$respondWithoutChaining$1.apply(Future.scala:868)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.util.Promise$$anonfun$respondWithoutChaining$1.apply(Future.scala:863)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.concurrent.IVar$$anonfun$runqs$1.apply$mcV$sp(IVar.scala:174)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.concurrent.IVar$LocalScheduler.run(IVar.scala:125)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.concurrent.IVar$LocalScheduler.apply(IVar.scala:105)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.concurrent.IVar.runqs(IVar.scala:169)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.concurrent.IVar.set(IVar.scala:311)\n\u00a1\u00aa\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/237\n.\n\n\u00a1\u00aa\nReply to this email directly or view it on GitHub.\n. I`m compiling and running on the same version \nmysql scala version is \nScala code runner version 2.10.0 -- Copyright 2002-2012, LAMP/EPFL\nI only run the command bin/web . Does it do compiling in this script or I should run sbt compile alone.\nAt 2013-04-03 10:48:53,\"Brian Degenhardt\" notifications@github.com wrote:\nI'm guessing there's a mismatch between the scala+jvm you're compiling with\nversus the scala+jvm you're running with.\nOn Tue, Apr 2, 2013 at 7:05 PM, Flume-patch-netease \nnotifications@github.com wrote:\n\nI run on Linux\nversion is\nDistributor ID:Debian\nDescription:Debian GNU/Linux 7.0 (wheezy)\nRelease:7.0\nCodename:wheezy\nAt 2013-04-03 01:19:54,\"Brian Degenhardt\" notifications@github.com\nwrote:\nI've never seen that before. What OS is this on?\nOn Mon, Apr 1, 2013 at 10:36 PM, Flume-patch-netease \nnotifications@github.com wrote:\n\nWhen I start web UI and query the trace log, it throws a Unknown method\nexception\nUnsafe is a class in rt.jar. But I try all JDK such as JDk\n7u17,openJDK7,JDK6u22,openJDK6 , I still cannot resolve this problem.\nI wonder this\nsun.misc.Unsafe.copyMemory(Ljava/lang/Object;JLjava/lang/Object;JJ)V In\nwhich JDK version .\nFAT [20130402-13:28:03.663] ZipkinWeb:\norg.apache.thrift.TApplicationException: Internal error processing\ngetTraceSummariesByIds: 'java.lang.NoSuchMethodError:\nsun.misc.Unsafe.copyMemory(Ljava/lang/Object;JLjava/lang/Object;JJ)V'\nFAT [20130402-13:28:03.663] ZipkinWeb: at\norg.apache.thrift.TApplicationException.read(TApplicationException.java:108)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.zipkin.gen.ZipkinQuery$FinagledClient.decodeResponse(ZipkinQuery.scala:5263)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.zipkin.gen.ZipkinQuery$FinagledClient$$anonfun$getTraceSummariesByIds$1.apply(ZipkinQuery.scala:5480)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.zipkin.gen.ZipkinQuery$FinagledClient$$anonfun$getTraceSummariesByIds$1.apply(ZipkinQuery.scala:5479)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.util.Future$$anonfun$flatMap$1.apply(Future.scala:477)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.util.Future$$anonfun$flatMap$1.apply(Future.scala:476)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.util.Promise$$anonfun$transform$1.apply(Future.scala:883)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.util.Promise$$anonfun$transform$1.apply(Future.scala:879)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.util.Promise$$anonfun$respondWithoutChaining$1.apply(Future.scala:868)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.util.Promise$$anonfun$respondWithoutChaining$1.apply(Future.scala:863)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.concurrent.IVar$$anonfun$runqs$1.apply$mcV$sp(IVar.scala:174)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.concurrent.IVar$LocalScheduler.run(IVar.scala:125)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.concurrent.IVar$LocalScheduler.apply(IVar.scala:105)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.concurrent.IVar.runqs(IVar.scala:169)\nFAT [20130402-13:28:03.663] ZipkinWeb: at\ncom.twitter.concurrent.IVar.set(IVar.scala:311)\n\u00a1\u00aa\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/issues/237>\n.\n\n\u00a1\u00aa\nReply to this email directly or view it on GitHub.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/237#issuecomment-15813809\n.\n\n\u2014\nReply to this email directly or view it on GitHub.\n. Thanks you very much \nit works now.\nMaybe it is my faults of set JDK version wrongly\nAt 2013-04-03 11:21:02,\"Trustin Lee\" notifications@github.com wrote:\n@netease-datastream Are you using OpenJDK shipped by your distribution? In that case, using the latest OpenJDK or the Oracle JVM should fix the problem. A certain revision of OpenJDK was shipped with mistakenly removed unsafe operations.\n\u00a1\u00aa\nReply to this email directly or view it on GitHub.\n. I wonder if I can add a message annotation in the RPC method by finagle-zipkin?\nI cannot get the tracer referrence in the RPC method because it is hidden by the  Server of finagle.\nI found the source code that  tracer only exists in ThriftServerFramedCodec. \nAt 2013-04-03 11:21:02,\"Trustin Lee\" notifications@github.com wrote:\n@netease-datastream Are you using OpenJDK shipped by your distribution? In that case, using the latest OpenJDK or the Oracle JVM should fix the problem. A certain revision of OpenJDK was shipped with mistakenly removed unsafe operations.\n\u00a1\u00aa\nReply to this email directly or view it on GitHub.\n. ",
    "trustin": "@netease-datastream Are you using OpenJDK shipped by your distribution?  In that case, using the latest OpenJDK or the Oracle JVM should fix the problem.  A certain revision of OpenJDK was shipped with mistakenly removed unsafe operations.\n. +1 for avoiding redundant information, i.e. no port in http.host. If a user insists to have a port, we could define an optional property like http.authority which is clearer.. @ewhauser wrote:\n\nI would want to make sure there was plenty of support for it.\n\nThe Armeria project maintainers including LINE employees like me (@hyangtack and @minwoox as well) would be happy to support Zipkin community for adoption of Armeria and its sustainable development. I believe you don't need to worry too much about the future of Armeria given that it's being used in various critical parts of LINE services.. @ewhauser wrote:\n\nI would want to make sure there was plenty of support for it.\n\nThe Armeria project maintainers including LINE employees like me (@hyangtack and @minwoox as well) would be happy to support Zipkin community for adoption of Armeria and its sustainable development. I believe you don't need to worry too much about the future of Armeria given that it's being used in various critical parts of LINE services.. Thanks a lot for your interest in Armeria and we'd be very happy to work with Zipkin community to make this real. :+1: . Thanks a lot for your interest in Armeria and we'd be very happy to work with Zipkin community to make this real. :+1: . Will take a look soon.. Armeria 0.79.0 has been released. :-). Didn't think much about making DocService as an optional dependency but we may have to eventually once we add more bells and whistles to it, like metric dashboard and on demand profiler.. See https://line.github.io/armeria/server-http-file.html for HttpFile API.. > we need to think about whether to continue this work here or in armeria.\nSounds like fun. Would be a nice addition to our Spring integration.. > @trustin remember those old APIs? :D\nOhh, that's a very long time ago. :rofl: Never knew you tried to replace Spring Web at that time! I hope we are getting better and better in terms of API usability and user happiness. :crossed_fingers: . I'm curious why we have half the original throughput. Is this expected? I'd like to do some profiling later. Would you mind letting me know how to reproduce the benchmark, e.g. how to set up and what wrk command to run.. @adriancole Thanks. Will take a look to find out what we can do to make it perform better. \n@anuraaga Great job!. Yep. A user can use HttpResponse.of(...). AggregatedHttpMessage will be translated into HttpResponse anyway.. ",
    "derekchiang": "I would certainly like to fix the bugs if this isn't a local issue.\n. Yeah I'm talking about both.  Glad that it works for you.  Not working for me :(\n. Yeah I'm talking about both.  Glad that it works for you.  Not working for me :(\n. I think that's a front end issue.  I will look into it and reopen this if necessary.  Thanks for the support.\n. I think that's a front end issue.  I will look into it and reopen this if necessary.  Thanks for the support.\n. That's what I thought but it didn't appear so.  The trace id and the id in\nthe URL appeared different on my machine... What could be the reasons?\nBest Regards,\nDerek Chiang\n(Sent from Android phone)\nOn Jun 7, 2013 3:27 PM, \"Brian Degenhardt\" notifications@github.com wrote:\n\nhttp://your.zipkin.hostname.example.com/traces/\nOn Fri, Jun 7, 2013 at 3:22 PM, Derek Chiang (Enchi Jiang) \nnotifications@github.com wrote:\n\nIs there a way to get the URL to a specific trace? So, having a trace\nid,\nhow do I programmatically get the URL to the web page of that trace?\nThanks,\nDerek\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/issues/254>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/254#issuecomment-19136305\n.\n. That's what I thought but it didn't appear so.  The trace id and the id in\nthe URL appeared different on my machine... What could be the reasons?\n\nBest Regards,\nDerek Chiang\n(Sent from Android phone)\nOn Jun 7, 2013 3:27 PM, \"Brian Degenhardt\" notifications@github.com wrote:\n\nhttp://your.zipkin.hostname.example.com/traces/\nOn Fri, Jun 7, 2013 at 3:22 PM, Derek Chiang (Enchi Jiang) \nnotifications@github.com wrote:\n\nIs there a way to get the URL to a specific trace? So, having a trace\nid,\nhow do I programmatically get the URL to the web page of that trace?\nThanks,\nDerek\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/issues/254>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/254#issuecomment-19136305\n.\n. @bmdhacks  aha here we go!  thanks!\n. @bmdhacks  aha here we go!  thanks!\n. \n",
    "jaredlwong": "The build failed. I think the tests require that there is a Redis instance running listening to port 6379. Does Travis run a redis instance? The errors just pertain to the Redis code (https://travis-ci.org/twitter/zipkin/builds/7741148#L1684). The tests pass for me on my machine (when I'm running redis). \n. You guys have any idea about the redis errors?\n. I'm fairly sure that it's rebased to master. Here are the last 5 commits\nfrom my fork\n77db949 Added better support for Redis.\n1c0cab5 Zookeeper Fixes Our recent changes to bind to 0.0.0.0 was causing\nthe scribe collector to publish an incorrect address in zookeeper.\n4052873 Merge branch 'master' of github.com:twitter/zipkin\n40db92f Fixing Build Properties Generation For whatever reason sbt does not\nwant to change the resourceGenerators inside of the BuildProperties plugin,\nso I just do it in each project I need it in.\n3eeb8fc Added an indicator of duration to the Firefox extension\nvisualizations\nSee\nhttps://github.com/jaredlwong/zipkin/commits/redis\nhttps://github.com/twitter/zipkin/commits/master\nOn Thu, Jun 27, 2013 at 4:26 PM, Brian Degenhardt\nnotifications@github.comwrote:\n\nYes, see franklin's request right above this. Rebase to master and the\ntests should pass.\nOn Thu, Jun 27, 2013 at 4:25 PM, Jared Wong notifications@github.comwrote:\n\nYou guys have any idea about the redis errors?\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/pull/253#issuecomment-20161815>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/pull/253#issuecomment-20161854\n.\n\n\nJared Wong :: jaredlwong@gmail.com\n. @franklinhu Have you had the chance to look at this?\n. I don't know Zipkin in depth, but it seems as if you might not have the\ndatabase set up. Or, you are somehow putting bad data into your database.\nCan you ensure that everything is running? The collector, querier and web\napplications. As well as your database, usually either cassandra or redis.\nAdditionally, are you using the collector to store data into the database?\nOr rather, how are you storing data into your database?\nBest,\nJared\nOn Wed, Jul 10, 2013 at 3:28 AM, \u65f6\u91d1\u9b41 notifications@github.com wrote:\n\nhttp://127.0.0.1:8080/\nhelp me\ncom.twitter.finagle.FailedFastException\ncom.twitter.zipkin.gen.QueryException$Immutable$.decode(QueryException.scala:67)\ncom.twitter.zipkin.gen.QueryException$.decode(QueryException.scala:25)\ncom.twitter.zipkin.gen.ZipkinQuery$getServiceNames$result$Immutable$.decode(ZipkinQuery.scala:3486)\ncom.twitter.zipkin.gen.ZipkinQuery$getServiceNames$result$.decode(ZipkinQuery.scala:3432)\ncom.twitter.zipkin.gen.ZipkinQuery$getServiceNames$result$.decode(ZipkinQuery.scala:3420)\ncom.twitter.zipkin.gen.ZipkinQuery$FinagledClient.decodeResponse(ZipkinQuery.scala:5512)\ncom.twitter.zipkin.gen.ZipkinQuery$FinagledClient$$anonfun$getServiceNames$1.apply(ZipkinQuery.scala:5832)\ncom.twitter.zipkin.gen.ZipkinQuery$FinagledClient$$anonfun$getServiceNames$1.apply(ZipkinQuery.scala:5831)\ncom.twitter.util.Future$$anonfun$flatMap$1.apply(Future.scala:607)\ncom.twitter.util.Future$$anonfun$flatMap$1.apply(Future.scala:606)\ncom.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:67)\ncom.twitter.util.Promise$Transformer.k(Promise.scala:67)\ncom.twitter.util.Promise$Transformer.apply(Promise.scala:76)\ncom.twitter.util.Promise$Transformer.apply(Promise.scala:58)\ncom.twitter.util.Promise$$anon$1.run(Promise.scala:271)\ncom.twitter.concurrent.Scheduler$LocalScheduler.run(Scheduler.scala:60)\ncom.twitter.concurrent.Scheduler$LocalScheduler.submit(Scheduler.scala:40)\ncom.twitter.concurrent.Scheduler$.submit(Scheduler.scala:26)\ncom.twitter.util.Promise.runq(Promise.scala:248)\ncom.twitter.util.Promise.updateIfEmpty(Promise.scala:502)\ncom.twitter.util.Promise.update(Promise.scala:475)\ncom.twitter.util.Promise.setValue(Promise.scala:458)\ncom.twitter.concurrent.AsyncQueue.offer(AsyncQueue.scala:71)\ncom.twitter.finagle.transport.ClientChannelTransport.handleUpstream(ChannelTransport.scala:127)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786)\norg.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\norg.jboss.netty.handler.codec.oneone.OneToOneDecoder.handleUpstream(OneToOneDecoder.java:70)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786)\norg.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\norg.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:458)\norg.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:439)\norg.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\norg.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\ncom.twitter.finagle.thrift.ThriftFrameCodec.handleUpstream(ThriftFrameCodec.scala:17)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786)\norg.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142)\ncom.twitter.finagle.channel.ChannelStatsHandler.messageReceived(ChannelStatsHandler.scala:74)\norg.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558)\norg.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786)\norg.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142)\ncom.twitter.finagle.channel.ChannelRequestStatsHandler.messageReceived(ChannelRequestStatsHandler.scala:35)\norg.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558)\norg.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:553)\norg.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\norg.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\norg.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:84)\norg.jboss.netty.channel.socket.nio.AbstractNioWorker.processSelectedKeys(AbstractNioWorker.java:471)\norg.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:332)\norg.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:35)\norg.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:102)\norg.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\njava.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\njava.lang.Thread.run(Thread.java:662)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/273\n.\n\n\nJared Wong :: jwong@pinterest.com\n. How would you go about doing that? Just create another Project file for Redis? I'm not too familiar with how sbt determines which Project file to read.\n. Fixed!\n. ",
    "daandi": "I'd like to use some parts of zipkin to get them to run with brave (https://github.com/kristofa/brave-zipkin-spancollector. ).\nThus just running zipkin would not be enough.\nThe implementation uses:\n- zipkin-common\n- zipkin-scrooge\n  for which I didn't find an artifact compatible with Scala 2.10. \nI tried checking out the project and changing the Scala Version in the build.sbt to generate the aritifacts locally. But the Scala Version seems to be defined/overridden somewhere else. Also checked the build properties found in the project directory but no trace of a version definition except for the one in the build.sbt .\n. Thanks a lot. :) That really helps.\n. ",
    "harryw": "As an outsider already interested in using Zipkin, I'd just like to say that this looks like an awesome feature!\n. As an outsider already interested in using Zipkin, I'd just like to say that this looks like an awesome feature!\n. ",
    "shijinkui": "@mosesn , compiling passed.  thx\n. @mosesn , compiling passed.  thx\n. @bmdhacks @mosesn\n. @bmdhacks @mosesn\n. find it. thanks\n. find it. thanks\n. ",
    "theatrus": "+1\n. +1\n. Concur on lowercasing. \n. These Anorm operations should probably ne mapped into a Finagle FuturePool\n. ",
    "jdb8": "Seconding this request, needing to update the sort dropdown every time after querying for new traces is a bit inconvenient.\n. Any reason these kind of settings can't just be query params? Then they'd also be linkable to others/bookmarkable.\n. ",
    "t-yuki": "I want to implement this.\nWell, Which storage we should use? Cookie? Local/Session Storage?\nCurrently the cookie has last-serviceName param.\n. I think AnormDB implementation also have same problem and I'm working on to fix it with the other problem.\n. Is adding new interface acceptable?\nPreview: \n\n. I changed the label to \"Analyze Dependencies\".\nAlso I added a missing file.\nPTAL.\n. It queries to /dependency?endTs=1234&startTs but no effect for server side.\nInstead, dependency.js converts startTs to lookback and sends a query with that parameter.\nHowever, error cases like startTs > endTs is not handled well. (it ignores startTs simply)\nI have no idea for wording of the button. Find traces page has Find Traces so I write Analyze Dependencies in Dependencies page. Also Query is nice.\nScreens here:\n\n\n. Thanks, I read thrift file firstly but I'm not sure the server should start new span or use provided span id by Request Header, for SERVER_RECV annotation as the spec.\nAnyway implementations are varied...\n. Oh I see. You are right, I didn't checked tracer.with_new_span method carefully.\nSo as far as I know. the remaining problem is that default zipkin-web, zipkin-query and tracegen create obscurity annotations.\nOK, tracegen maybe misused CLIENT_SEND and CLIENT_RECV because they annotates it with nextEp, not current ep: https://github.com/openzipkin/zipkin/blob/master/zipkin-tracegen/src/main/scala/com/twitter/zipkin/tracegen/TraceGen.scala#L127\nShould I open a pull request to fix?\nAlso zipkin-web, zipkin-query may have another problem as explained above but it is not examined yet.\n. Thanks, the workshop is interesting. Maybe I have no time to join actively but I'll watch gitter.\n. Ok, I add description with  refactoring.\nAlso I try SQL with our dataset (n GB per day for complete tracing)\n. Tests are passed.\nPTAL with fixing tests including a fix for #917\nShould I squash it?\nNote that, maybe, default lookback time 7 days is too long for this kind of SQL in usually loaded hosts.\n. I've squashed and split tests of intermediate spans and not-instrumented clients and servers.\n. Both instrumented spans are good.\nzipkin-web and zipkin-query are still bad.\n\n. Both instrumented spans are good.\nzipkin-web and zipkin-query are still bad.\n\n. Oops, nice catch. get spans have Annotation with timestamp but call spans are not.\n. Does it mean to find by element ID in timestamp.js?\nI think it is too specific and no other element like start time field can use timestamp class.\n. I found that e.date contains date only, no time.\nSo before this change, if you changed date, time will be cleared to 00:00\n. Another demand for capability API is constructing   switches in UI such as counting failed traces, advanced query condition, tenary call counts and etc.\nSo a HTTP API is potentially required but not yet.\n. This change is linked to https://github.com/openzipkin/zipkin-dependencies-spark/pull/18\n. This change is linked to https://github.com/openzipkin/zipkin-dependencies-spark/pull/18\n. ",
    "yurishkuro": "@adriancole the way I understood it, the sorting is still done on UI side, but the sorting order is determined by\n1. if in query param, use that\n2. otherwise, if in cookie, use that\n3. otherwise use the default order \"longest first\"\n. @sprsquish have you seen http://opentracing.io ?  The OpenTracing Spec does not have examples specifically for Java, but the API is nearly identical across all languages and there are already contributed projects that show instrumentations of various server frameworks, e.g.\n- https://github.com/opentracing-contrib/\n- https://github.com/uber/jaeger-client-java (JAXRS2 / Dropwizard)\nThe Jaeger lib above implements OpenTracing API, and is Zipkin-compatible even on the wire, thanks to recent PRs from @adriancole (I owe him a published version, will release one today, but meanwhile there are snapshots on sonatype )\n. @adriancole \n\nalways buffer and use rules based on child spans\n\nBuffer where? Given that spans of a single trace can be reported by different hosts to different collectors, it seems like this would need:\n1. consistent hashing to assemble spans of the same trace in a single collector\n2. short term storage of all traces, until sampling decision can be made and the trace is either saved or dropped\nIs that what you had in mind?\n. Given that existing implementation is based on Finagle, there are a lot of stats already being reported. However, they are not being emitted to any backend in the default configuration, and in fact there seems to be no external configuration mechanism aside from putting a text file in META-INF specifying the class name of the metrics backend.\n. it's pretty much in all tables, e.g. annotations_index for standard annotations like cs/cr/ss/sr will be always hitting row key \"${serviceName}:${annotation}\". The old code was using BucketedColumnFamily for most of the tables.\n. also, if we go for a dedicated bucket column included in the composite row key, I would suggest changing the schema for annotations_index. Right now it is stored as a composite string ${serviceName}:${annotation} [:$annotationValue]. I would suggest the following schema instead:\nCREATE TABLE IF NOT EXISTS zipkin.annotations_index (\n    service_name      string,\n    annotation_name   string,\n    annotation_value  blob, -- optional for non-binary annotations.\n    bucket_no      tinyint,\n    ts             timestamp,\n    trace_id       bigint,\n    PRIMARY KEY ((service_name, annotation_name, annotation_value, bucket_no), ts)\n)\n. at least keeping the bucket number as a separate column would be useful to start with.\n. at least keeping the bucket number as a separate column would be useful to start with.\n. Looks good, just missing the schema change for service_name table.\n. Looks good, just missing the schema change for service_name table.\n. I agree, bucketing doesn't seem particularly helpful for service names. But the pull request does contain code to use buckets for this table, so I was pointing out to mismatch with the schema. Maybe undo the code change for this table?\n. I agree, bucketing doesn't seem particularly helpful for service names. But the pull request does contain code to use buckets for this table, so I was pointing out to mismatch with the schema. Maybe undo the code change for this table?\n. sorry. my bad, got tripped by the column name. Good to go!\n. sorry. my bad, got tripped by the column name. Good to go!\n. @michaelsembwever when do you plan to merge this? \nI've tested with my collector (written in Go), the new schema works well.\n. @michaelsembwever when do you plan to merge this? \nI've tested with my collector (written in Go), the new schema works well.\n. @adriancole added test\n. @adriancole added test\n. @adriancole squashed to 1\n. @adriancole squashed to 1\n. @adriancole is the Aggregate page supposed to show something like this:\nhttps://g.twimg.com/blog/blog/image/observability_1.png ?\nCould you please share a link to \"@eirslett's demo\"?\n. @adriancole is the Aggregate page supposed to show something like this:\nhttps://g.twimg.com/blog/blog/image/observability_1.png ?\nCould you please share a link to \"@eirslett's demo\"?\n. > a decorating SpanStore which caches in memory would be something those doing custom builds could add, if the middleware itself doesn't have a caching feature\nWe already did that a few months ago, and I am still occasionally seeing the dropdown staying blank for a second or more after page loading. I feel it's a combination of transferring large JSON (via two hops due to query service running separately) and DOM creation altogether result in this visible latency. It just seems like the wrong design that the UI needs to keep re-loading this large list. Soon our list of services will be approaching a 1000.\n. +1\n. LGTM\n. LGTM\n. lgtm\n. lgtm\n. +1 to move to 8\n. +1 to move to 8\n. @adriancole what does pinpoint do for sequences?  From their description https://github.com/naver/pinpoint/wiki/Technical-Overview-Of-Pinpoint the data model sounds very similar to Zipkin.\n. @adriancole what does pinpoint do for sequences?  From their description https://github.com/naver/pinpoint/wiki/Technical-Overview-Of-Pinpoint the data model sounds very similar to Zipkin.\n. nextSpanId in pinpoint is at the Span Event level - aren't those just like zipkin annotations? That model is so denormalized it's hard to judge. But they do have something else that's interesting\n30: optional i32 asyncId;\n  31: optional i32 nextAsyncId;\n  32: optional i16 asyncSequence;\n. what I am not clear on is why sequencing is important. It was brought up in the context of async spans in the document, where sequencing of async siblings (or, more precisely, of their 'beginSpan' events) doesn't strike me as particularly useful. On the other hand, in the sync context, the call/return pairs can be easily serialized by the timestamps because there is no clock skew in the parent span.\n. Fair enough. span.beginChildSpan() can internally assign a sequence number, which can be emitted as a predefine span attribute. \nAdrian, could you please create a \"Zipkin v.2 Data Model\" doc in the Breakouts folder? There are too many open questions here to discuss in a linear chat.\n. @michaelsembwever https://drive.google.com/folderview?id=0B0tSnQT3uGdAfmZaVzZfOXJRRzZPUmRDRFVGRUFHVm1fX2ZXenM1VGN3RElTQzc3azI3X1k&usp=sharing&tid=0B0tSnQT3uGdAfndwS3RKVnMtM0ZKMWU4R1dicTNEXy1NSlBJaTlJQ05abGVneTlucW5HdFk#list\n. @prat0318  Which storage backend were you running?\n. @prat0318  Which storage backend were you running?\n. when A->B and B is not instrumented, would you not want to use the name from \"cs\" emitted by A?\n. does it mean that query service now writes to storage?\n. does it mean that query service now writes to storage?\n. serviceName.js has function which looks like it just keeps appending to the list:\n```\n      this.updateServiceNameDropdown = function(ev, data) {\n        $.each(data.serviceNames, function(i, item) {\n            $('').val(item).text(item).appendTo('#serviceName');\n        });\n    this.$node.find('[value=\"' + data.lastServiceName + '\"]').attr('selected', 'selected');\n\n    this.trigger('chosen:updated');\n  };\n\n``\n.serviceName.js` has function which looks like it just keeps appending to the list:\n```\n      this.updateServiceNameDropdown = function(ev, data) {\n        $.each(data.serviceNames, function(i, item) {\n            $('').val(item).text(item).appendTo('#serviceName');\n        });\n    this.$node.find('[value=\"' + data.lastServiceName + '\"]').attr('selected', 'selected');\n\n    this.trigger('chosen:updated');\n  };\n\n```\n. works for me from cmd line, so lg.\n. works for me from cmd line, so lg.\n. IntelliJ is happy now as well\n. IntelliJ is happy now as well\n. > In order to support a query by duration, we need to have an index of (service name, duration) -> traceId limited by startTs. We've always had implicit Span.duration and Span.startTs fields. To ensure we can support duration queries, we need to make these explicit.\nThe last sentence does not follow from the requirements. Yes, we do need an index, but we don't need to capture start/duration in the collected spans, we just need a way to calculate it in the collector.  We could  use cs/cr for that, provided they reach the collector in a single submission from instrumentation.\n. > In order to support a query by duration, we need to have an index of (service name, duration) -> traceId limited by startTs. We've always had implicit Span.duration and Span.startTs fields. To ensure we can support duration queries, we need to make these explicit.\nThe last sentence does not follow from the requirements. Yes, we do need an index, but we don't need to capture start/duration in the collected spans, we just need a way to calculate it in the collector.  We could  use cs/cr for that, provided they reach the collector in a single submission from instrumentation.\n. These are my concerns with adding the fields to thrift: \n- it suggests to the instrumentation libs to send redundant data, since they are already sending the start/end annotations from which startTs/duration can be derived. The Zipkin story about what and how the instrumentation libs have to send is already super convoluted.\n- In RPC case the start/duration can be derived either from server annotations or from client annotations, depending on where the instrumentation exists. There isn't really such a thing as a \"span\" in the ingestion pipeline, more like multiple slices of span. A slice + annotations is not ambiguous (although  \"span name\" is), but slice + start/duration definitely are. So someone need to resolve that somewhere, and I think it's better resolved using the raw data of annotations than using potentially incorrectly calculated start/duration.\n- In cases when instrumentation cannot send the complete span, some other mechanism, like a Spark pipeline, can do a post-processing and calculate start/duration having seen the complete span in the storage (multiple slices stored independently). It would certainly be storage implementation dependent how such post-caculated result would be stored. In Cassandra we'd add a record to the index by duration. If other stores have a record representing a complete span, they would have to make an update to that record. Forcing Cassandra to store an update of post-calculation would be pretty bad for performance.\n. These are my concerns with adding the fields to thrift: \n- it suggests to the instrumentation libs to send redundant data, since they are already sending the start/end annotations from which startTs/duration can be derived. The Zipkin story about what and how the instrumentation libs have to send is already super convoluted.\n- In RPC case the start/duration can be derived either from server annotations or from client annotations, depending on where the instrumentation exists. There isn't really such a thing as a \"span\" in the ingestion pipeline, more like multiple slices of span. A slice + annotations is not ambiguous (although  \"span name\" is), but slice + start/duration definitely are. So someone need to resolve that somewhere, and I think it's better resolved using the raw data of annotations than using potentially incorrectly calculated start/duration.\n- In cases when instrumentation cannot send the complete span, some other mechanism, like a Spark pipeline, can do a post-processing and calculate start/duration having seen the complete span in the storage (multiple slices stored independently). It would certainly be storage implementation dependent how such post-caculated result would be stored. In Cassandra we'd add a record to the index by duration. If other stores have a record representing a complete span, they would have to make an update to that record. Forcing Cassandra to store an update of post-calculation would be pretty bad for performance.\n. btw, what is your concern with having these two fields be implementation-specific? The span API that query service deals with can have them as def's, and storage implementations fill it. In many cases the actual calculation should be the same, e.g. if it's from annotations, so that logic can be refactored to a base class or composition. I thought right now it it's already in the shared class.\n. btw, what is your concern with having these two fields be implementation-specific? The span API that query service deals with can have them as def's, and storage implementations fill it. In many cases the actual calculation should be the same, e.g. if it's from annotations, so that logic can be refactored to a base class or composition. I thought right now it it's already in the shared class.\n. ok, I'll shut up, go for it. I believe in my system we may be able to calculate those fields in the collector.\n. ok, I'll shut up, go for it. I believe in my system we may be able to calculate those fields in the collector.\n. the usual way to get custom constructors for case classes is to implement conversion logic in the companion's apply method, and make the constructor private. I think the only thing you loose by doing that is the copy method (but not sure).\n. the usual way to get custom constructors for case classes is to implement conversion logic in the companion's apply method, and make the constructor private. I think the only thing you loose by doing that is the copy method (but not sure).\n. lg\n. lg\n. has something changed with the styling of UI elements recently?  I just upgraded, and the dropdowns/textboxes look taller / bigger.\nCirca 1.20.x:\n\nNow:\n\n. has something changed with the styling of UI elements recently?  I just upgraded, and the dropdowns/textboxes look taller / bigger.\nCirca 1.20.x:\n\nNow:\n\n. Looking @ Cassandra. One difficulty there is that the query becomes a composite query, ts <= X and duration >= Y, which cannot be satisfied directly by Cassandra. \ncqlsh:zipkin> select * from duration_index where service_name='mezzanine' and time_bucket=1 and ts > '2015-11-07T11:00:00' and duration > 100000;\nInvalidRequest: code=2200 [Invalid query] message=\"PRIMARY KEY column \"duration\" cannot be restricted (preceding column \"ts\" is restricted by a non-EQ relation)\"\nThe only efficient way I can think of doing it is by bucketing the timestamps, say hourly, so that the schema will be \nCREATE TABLE IF NOT EXISTS zipkin.duration_index (\n    service_name      text,\n    time_bucket       int,\n    duration          bigint,\n    ts                timestamp,\n    trace_id          bigint,\n    PRIMARY KEY ((service_name, time_bucket), duration, ts)\n) WITH CLUSTERING ORDER BY (duration DESC, ts DESC)\nThe drawback of this is that if the time buckets are small, the distribution of returned traces will be skewed. If the buckets are large, like 1d, the partition/row might become too large.\nIs there another solution I am missing?\n. Looking @ Cassandra. One difficulty there is that the query becomes a composite query, ts <= X and duration >= Y, which cannot be satisfied directly by Cassandra. \ncqlsh:zipkin> select * from duration_index where service_name='mezzanine' and time_bucket=1 and ts > '2015-11-07T11:00:00' and duration > 100000;\nInvalidRequest: code=2200 [Invalid query] message=\"PRIMARY KEY column \"duration\" cannot be restricted (preceding column \"ts\" is restricted by a non-EQ relation)\"\nThe only efficient way I can think of doing it is by bucketing the timestamps, say hourly, so that the schema will be \nCREATE TABLE IF NOT EXISTS zipkin.duration_index (\n    service_name      text,\n    time_bucket       int,\n    duration          bigint,\n    ts                timestamp,\n    trace_id          bigint,\n    PRIMARY KEY ((service_name, time_bucket), duration, ts)\n) WITH CLUSTERING ORDER BY (duration DESC, ts DESC)\nThe drawback of this is that if the time buckets are small, the distribution of returned traces will be skewed. If the buckets are large, like 1d, the partition/row might become too large.\nIs there another solution I am missing?\n. small good news, Cassandra allows multi-column restrictions on clustering columns, it just requires special syntax like (duration, ts) >= (20, '2015-11-07T11:00:00'). With the table above, and the following data\nservice_name | time_bucket | duration | ts                       | trace_id\n--------------+-------------+----------+--------------------------+----------\n            x |           1 |       20 | 2015-11-07 12:00:00-0500 |        1\n            x |           1 |       20 | 2015-11-07 11:00:00-0500 |        1\n            x |           1 |       10 | 2015-11-07 12:00:00-0500 |        1\n            x |           1 |       10 | 2015-11-07 11:00:00-0500 |        1\nthe following query finds both D=20 records at different timestamps:\n```\ncqlsh:zipkin> select * from duration_index where service_name='x' and time_bucket=1 and (duration, ts) >= (20, '2015-11-07T11:00:00');\nservice_name | time_bucket | duration | ts                       | trace_id\n--------------+-------------+----------+--------------------------+----------\n            x |           1 |       20 | 2015-11-07 12:00:00-0500 |        1\n            x |           1 |       20 | 2015-11-07 11:00:00-0500 |        1\n```\nThe question of buckets skewing the distribution still remains, but it could be addressed by asking the user to specify both ends of the timeframe, not just endTs, and searching all relevant buckets. Then the bucket size can be tuned depending on the traffic volume in a given deployment.\n. small good news, Cassandra allows multi-column restrictions on clustering columns, it just requires special syntax like (duration, ts) >= (20, '2015-11-07T11:00:00'). With the table above, and the following data\nservice_name | time_bucket | duration | ts                       | trace_id\n--------------+-------------+----------+--------------------------+----------\n            x |           1 |       20 | 2015-11-07 12:00:00-0500 |        1\n            x |           1 |       20 | 2015-11-07 11:00:00-0500 |        1\n            x |           1 |       10 | 2015-11-07 12:00:00-0500 |        1\n            x |           1 |       10 | 2015-11-07 11:00:00-0500 |        1\nthe following query finds both D=20 records at different timestamps:\n```\ncqlsh:zipkin> select * from duration_index where service_name='x' and time_bucket=1 and (duration, ts) >= (20, '2015-11-07T11:00:00');\nservice_name | time_bucket | duration | ts                       | trace_id\n--------------+-------------+----------+--------------------------+----------\n            x |           1 |       20 | 2015-11-07 12:00:00-0500 |        1\n            x |           1 |       20 | 2015-11-07 11:00:00-0500 |        1\n```\nThe question of buckets skewing the distribution still remains, but it could be addressed by asking the user to specify both ends of the timeframe, not just endTs, and searching all relevant buckets. Then the bucket size can be tuned depending on the traffic volume in a given deployment.\n. Q1 - I see that it only does root spans, due to |WHERE trace_id = id. Which is probably a bad assumption anyway, nowhere do we say that root span ID must be equal to trace_id (a more accurate check would've been parent_id == nil or 0).\nQ2 - the sql duration query checks for service_name == query.sn. In combination with the above, I conclude it's only limited to the services mentioned in the root span.\nQ3 - I remember the slice query, iirc it's one of the reasons the limit param doesn't work correctly in combination queries. This is very hard to support in Cassandra, actually. I would go as far as forcing the UI to make the use choose between annotations query and duration queries. If we want to support both, then at least in Cassandra implementation it requires in-memory join of trace IDs from different index queries, and it's fairly hard to make that join work correctly with LIMIT.\nIt's fairly easy to implement correct LIMIT for serviceName {+span name} + duration combination in Cassandra, as it can all be stored in the duration index I showed above. As for the semantics of such query, I would go for span-level query, i.e. where both duration and service-name (and span name) must apply to the same span in order for the trace ID to qualify. It seems like it would satisfy most of the queries. The one that it would not satisfy is finding long traces by service name whose span was not long.\n. Q1 - I see that it only does root spans, due to |WHERE trace_id = id. Which is probably a bad assumption anyway, nowhere do we say that root span ID must be equal to trace_id (a more accurate check would've been parent_id == nil or 0).\nQ2 - the sql duration query checks for service_name == query.sn. In combination with the above, I conclude it's only limited to the services mentioned in the root span.\nQ3 - I remember the slice query, iirc it's one of the reasons the limit param doesn't work correctly in combination queries. This is very hard to support in Cassandra, actually. I would go as far as forcing the UI to make the use choose between annotations query and duration queries. If we want to support both, then at least in Cassandra implementation it requires in-memory join of trace IDs from different index queries, and it's fairly hard to make that join work correctly with LIMIT.\nIt's fairly easy to implement correct LIMIT for serviceName {+span name} + duration combination in Cassandra, as it can all be stored in the duration index I showed above. As for the semantics of such query, I would go for span-level query, i.e. where both duration and service-name (and span name) must apply to the same span in order for the trace ID to qualify. It seems like it would satisfy most of the queries. The one that it would not satisfy is finding long traces by service name whose span was not long.\n. Does this change require UI/JS changes? When I emit a span with LC only, without RPC-bounding annotations, the UI doesn't show it.\n. Does this change require UI/JS changes? When I emit a span with LC only, without RPC-bounding annotations, the UI doesn't show it.\n. looks good\n. @adriancole I pulled this, but still not able to see local spans, even though I do send ts and duration. Your screenshots above show \"cr\" for both examples of local spans. Did you try it without having any RPC annotations on the local span?\n. the json result from query service does not return that local span either, so it might be deeper than web layer\n. Look good. The only comment is why not make lookback argument mandatory in the Dependency Store API, same as in SpanStore? Since the upstream query service already has defaulting mechanism for this field, it would make this API simpler. Otherwise it needs a sensible default, and endTs as default means we're looking at dependencies at a single point in time, which is weird.\n. Look good. The only comment is why not make lookback argument mandatory in the Dependency Store API, same as in SpanStore? Since the upstream query service already has defaulting mechanism for this field, it would make this API simpler. Otherwise it needs a sensible default, and endTs as default means we're looking at dependencies at a single point in time, which is weird.\n. @jcarres-mdsol This sounds like a bug somewhere in the specific storage implementation. With Cassandra storage the LIMIT applies to the most recent traces, i.e. out of the box you get the most recent trace at the top of search results (unless UI re-sorts by duration).\n. I think keeping UI restricted to endTs + duration is fine. Having those controls accept free-form time and interval is a great bonus.\n. +1\n. We had an issue once that was manifesting in very long query latencies. It turned out to be due to the driver directing some queries to nodes in another data center. Solved by DCAwareRoundRobinPolicy:\n```\n  // on Mac the default data center name in Cassandra is 'datacenter1'\n  val dataCenter  = flag[String] (\"datacenter\", \"data center name used by DC-aware load balancing policy, required.\")\noverride def createClusterBuilder(): Cluster.Builder = {\n    val dc = dataCenter.apply()\n    val lbPolicy = new TokenAwarePolicy(new LatencyAwarePolicy.Builder(new DCAwareRoundRobinPolicy(dc)).build())\n    super.createClusterBuilder()\n      .withLoadBalancingPolicy(lbPolicy)\n      .withQueryOptions(new QueryOptions().setConsistencyLevel(ConsistencyLevel.LOCAL_ONE))\n  }\n```\nI think it would make sense to make this a default setting in zipkin.\nSpecifically on this issue, I don't think 12s is a reasonable timeout if Cassandra is operating normally, it either indicates misconfigured cluster, or bad query pattern (too much data requested?)\n. @drax68 do you know how many span names per service name you have on average?  It could be the case that the cardinality of your span names is too high. For example, if you have a REST end point /user/{used_id}, then the span name should be just get_user. If you include user_id in the span name, you will get an explosion of span names that neither UI nor Cassandra would be happy with.\n. > 300000 span_names for single service.\nlol... there you go. A typical observability misuse problem: \"Why don't I include UUID in my metric name...\"\nThere is already a write cache writtenNames in the Repository for both service and span names. It's currently flat, but if we convert it to be a map by service name, than you can cap the number of span names per service, at least within the TTL interval of that cache (1hr by default, we're using 12hrs).  When span names exceed the cap, you can replace them with some fixed name like \"too-many-names\". Maybe add the original name as b-annotation. This will protect the service from misbehaving span senders.\n. No objection to logging errors and/or emitting metrics that bad things are happening, but the system should not let garbage data into the storage. We had to fight quite a few fires when bad players were bringing the system to its knees, affecting good citizens just the same as bad ones. It's a basic quota system. \nBesides, the way I suggest handling it does not make the system \"not work\", it just collapses garbage span names into one. The data is still there and can be searched.\n. > 1000 could be wrong for one installation and 100000 could be correct for another.\nI am not sure I see a realistic scenario where 100k span names per service is correct. The only reason we collect span names is to show them in the second dropdown in the UI. I'd argue any number over 100 is an overkill, who's gonna want to scroll through that? If a service has over 100 legitimately unique operations, yeah perhaps this 100 const can be made tunable, but perhaps it's time to consider refactoring the service, as something is probably wrong with its abstractions, maybe a certain portion of the span name should really be in the service name.\nBesides these semantic considerations, however, there's a separate need to protect Zipkin storage from runaway emitters, by implementing a simple circuit-breaker.\n\nWhat were the fires you were experiencing? \n\nThe fires weren't with the tracing systems, but with metrics. We use Cassandra as tsdb, and Elasticsearch for indexing metric names, e.g. to serve Graphite queries like stats.counts.service.*.xxx.*.yyy.*.zzz.*.*. From time to time some service would come up that emits metrics with UUID or timestamp or some other high cardinality value as part of the metric name, which explodes the writes to ES, and it's very difficult to build a pattern match that would separate good names from bad. But by capping the number of metrics per service we were able to get a handle on it and build alerts & circuit-breakers.\n. no, sorry, I am not, unlikely to get to it in the next couple of weeks, have a marathon project at work. \n. +1\n. +1\n. wow!  +1\n. umm, not sure it helps, the query service returns the client/server parts of the span already merged into one span:\n[\n  {\n    \"traceId\": \"597ed34198086fe1\",\n    \"name\": \"ping-pong\",\n    \"id\": \"597ed34198086fe1\",\n    \"timestamp\": 1447300441535689,\n    \"duration\": 100000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1447300441535689,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      },\n      {\n        \"timestamp\": 1447300441535690,\n        \"value\": \"lightning\",\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      },\n      {\n        \"timestamp\": 1447300441635689,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"weather\",\n        \"value\": \"rainy\",\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"597ed34198086fe1\",\n    \"name\": \"encrypt\",\n    \"id\": \"2dac845b71a0931c\",\n    \"parentId\": \"597ed34198086fe1\",\n    \"timestamp\": 1447300441535691,\n    \"duration\": 5626,\n    \"annotations\": [],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"lc\",\n        \"value\": \"cypher\",\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"597ed34198086fe1\",\n    \"name\": \"pong\",\n    \"id\": \"22e22b06aa965ae8\",\n    \"parentId\": \"597ed34198086fe1\",\n    \"timestamp\": 1447300441541347,\n    \"duration\": 8421,\n    \"annotations\": [\n      {\n        \"timestamp\": 1447300441541347,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      },\n      {\n        \"timestamp\": 1447300441542673,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"example2\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      },\n      {\n        \"timestamp\": 1447300441548638,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"example2\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      },\n      {\n        \"timestamp\": 1447300441549768,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"ca\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      },\n      {\n        \"key\": \"sa\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"example2\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      }\n    ]\n  }\n]\n. umm, not sure it helps, the query service returns the client/server parts of the span already merged into one span:\n[\n  {\n    \"traceId\": \"597ed34198086fe1\",\n    \"name\": \"ping-pong\",\n    \"id\": \"597ed34198086fe1\",\n    \"timestamp\": 1447300441535689,\n    \"duration\": 100000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1447300441535689,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      },\n      {\n        \"timestamp\": 1447300441535690,\n        \"value\": \"lightning\",\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      },\n      {\n        \"timestamp\": 1447300441635689,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"weather\",\n        \"value\": \"rainy\",\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"597ed34198086fe1\",\n    \"name\": \"encrypt\",\n    \"id\": \"2dac845b71a0931c\",\n    \"parentId\": \"597ed34198086fe1\",\n    \"timestamp\": 1447300441535691,\n    \"duration\": 5626,\n    \"annotations\": [],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"lc\",\n        \"value\": \"cypher\",\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"597ed34198086fe1\",\n    \"name\": \"pong\",\n    \"id\": \"22e22b06aa965ae8\",\n    \"parentId\": \"597ed34198086fe1\",\n    \"timestamp\": 1447300441541347,\n    \"duration\": 8421,\n    \"annotations\": [\n      {\n        \"timestamp\": 1447300441541347,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      },\n      {\n        \"timestamp\": 1447300441542673,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"example2\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      },\n      {\n        \"timestamp\": 1447300441548638,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"example2\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      },\n      {\n        \"timestamp\": 1447300441549768,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"ca\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"example\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      },\n      {\n        \"key\": \"sa\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"example2\",\n          \"ipv4\": \"0.0.0.0\"\n        }\n      }\n    ]\n  }\n]\n. But even with that span, the info on the screen is all over the place, top says Total Spans 3, but under Expand buttons is says example - x3 and example2 - x1\n\n. But even with that span, the info on the screen is all over the place, top says Total Spans 3, but under Expand buttons is says example - x3 and example2 - x1\n\n. Actually, maybe the counts are correct if interpreted as \"how many spans mention service X\". However, the bug wasn't about that, it was about the search results, where it said the trace has 4 spans. It has 4 records in the db, that's true, but only 3 spans, with 3 unique span IDs.\n\nAs for buttons on details page, I think they are useful, as they control highlighting of the spans. If I searched for a service and that service turns out to be right in the middle of a span 20-levels deep, then being able to highlight it is a nice touch.\n. Actually, maybe the counts are correct if interpreted as \"how many spans mention service X\". However, the bug wasn't about that, it was about the search results, where it said the trace has 4 spans. It has 4 records in the db, that's true, but only 3 spans, with 3 unique span IDs.\n\nAs for buttons on details page, I think they are useful, as they control highlighting of the spans. If I searched for a service and that service turns out to be right in the middle of a span 20-levels deep, then being able to highlight it is a nice touch.\n. I have built it on Mac with\n$ java -version\njava version \"1.8.0_45\"\nJava(TM) SE Runtime Environment (build 1.8.0_45-b14)\nJava HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)\nThe CI job runs with \"1.8.0_31\", maybe that's the reason for compiler failure. Can we bump java version in CI?\n. I have built it on Mac with\n$ java -version\njava version \"1.8.0_45\"\nJava(TM) SE Runtime Environment (build 1.8.0_45-b14)\nJava HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)\nThe CI job runs with \"1.8.0_31\", maybe that's the reason for compiler failure. Can we bump java version in CI?\n. @abesto I just finished python instrumentation for urllib2, sqlalchemy, and tornado async client, if there's interest. We're planning to open source that soon, although I'd prefer to adjust the API to match opentracing.  django-zipkin seems to only have the middleware, not the client call site instrumentation?\n. LGTM, but I would really like to understand and ideally remove the restriction traceId == spanId for root spans.\n. LGTM, but I would really like to understand and ideally remove the restriction traceId == spanId for root spans.\n. +1\nSo despite the old docs/comments, doesn't seem like there was any actual code that required traceId==spanId for root span?\n. +1\n. sgtm\n. Should we close this as fixed by my PR?\n. Fixed #857 \n. field names are consistent with the rest of the tables.\n. Considering that the DependencyLink is used almost in the final stage of the Spark job:\n```\n    val dependencies: Dependencies = aggregates\n      .map { case ((parent: String, child: String), callCount: Long) =>\n        Dependencies(0, 0, Seq(DependencyLink(parent = parent, child = child, callCount = callCount)))\n      }\n      .reduce( + ) // merge DLs under one Dependencies object, which overrides +\nsaveToCassandra(sc, dependencies)\n\n```\nA simple work around could be to define a local version of it as a normal, serializable case class. This should allow the  final .reduce(_ + _) step to succeed and produce a single in-memory object. Then in saveToCassandra method we can re-map it to the thrift-compatible class.\n. I suggest closing this issue.\n. I just ran into the same thing. \nFutureUtil.toFuture(repository.getDependencies(startEpochDayMillis, endEpochDayMillis))\n      .map { dependencies => dependencies.asScala\n          .map(codec.decode(_))\n          .map(thriftToDependencies(_).toDependencies)\n          .flatMap(_.links)\n      }\nThe last flatMap there joins all edges from runs on different days. I don't think we want to do that, we just want to pick the entry with the latest timestamp, and only use its links.\ncc @adriancole \n. This fixes it:\nFutureUtil.toFuture(repository.getDependencies(startEpochDayMillis, endEpochDayMillis))\n      .map { dependencies =>\n        dependencies.asScala\n          .map(codec.decode(_))\n          .map(thriftToDependencies(_).toDependencies)\n          .maxBy(_.endTs).links\n      }\n. I would prefer if the UI could simply display the time interval for which the aggregated results are shown. Adding multiple records together doesn't sound right to me - what if their startTs-endTs intervals overlap or are disjoint? The aggregate numbers wouldn't really make sense. At least if we're showing a single aggregate record we can guarantee that the counts are consistent.\nAlso, summing may work for counts, but once we start adding more measures, like p99, we won't be able to aggregate across multiple time intervals (maybe we'll need to go back to 'moments').\nIncidentally, the current Spark job impl leaves startTs/endTs==0, so the maxBY above won't quite work.\nval dependencies: Dependencies = aggregates\n      .map { case ((parent: String, child: String), callCount: Long) =>\n        Dependencies(0, 0, Seq(DependencyLink(parent = parent, child = child, callCount = callCount)))\n      }\n      .reduce(_ + _) // merge DLs under one Dependencies object, which overrides +\nThe job also doesn't filter traces by timestamps, it just reads everything from the table, so a single record already presents fully aggregated result. If we want that job to generate data for adjacent time intervals, and considering that it clamps he timestamp to 24hrs, maybe it should be doing a filter on the traces. \nstartTs = (ts floor 24hrs) - 24hr\nendTs = ts floor 24hrs\n. If it's already established elsewhere that summing up multiple records is the expected behavior, then this would fix CassandraDependencyStore: https://github.com/openzipkin/zipkin/pull/869\nscala\n    FutureUtil.toFuture(repository.getDependencies(startEpochDayMillis, endEpochDayMillis))\n      .map { dependencies =>\n        dependencies.asScala\n          .map(codec.decode(_))\n          .map(thriftToDependencies(_).toDependencies)\n          .flatMap(_.links)\n          .groupBy(link => (link.parent, link.child))\n          .map {\n            case ((parent, child), links) => DependencyLink(parent, child, links.map(_.callCount).sum)\n          }.toSeq\n      }\n. But the Spark job would produce duplicate results until this is fixed: https://github.com/openzipkin/zipkin-dependencies-spark/issues/7\n. beats me. The primary key has both a timestamp and a trace ID\nPRIMARY KEY ((service_name, span_name, bucket), duration, ts, trace_id)\nSince we index each span individually, perhaps we can add a span ID as well? \n. @michaelsembwever not sure I follow. The table's TTL should simply expire the records after N days and drop them during compactions. But they shouldn't create tombstones any more than with any other table in zipkin schema. Where I do see tombstones being possible is if the spans are sub-millisecond, i.e. true microseconds timestamps are different, but they are reported from a millisecond-precision language like JS, thus producing non-unique PKs.\nIs there a way in C* to read tombstoned data and compare it with live keys, to understand what exactly is different between the two and what causes the PKs to be overwritten?\n. > In this situation we're talking about expired cells, rather than tombstones from explicit deletes.\nNot sure how C* does this, does it treat cells past TTL as tombstones as well?\nWe already filter by startTs:\nreturn StreamSupport\n                                    .stream(it.spliterator(), false)\n                                    .map(DurationRow::new)\n                                    .filter((row) -> row.timestamp >= startTs && row.timestamp <= endTs)\n                                    .limit(limit)\n                                    .collect(Collectors.toList());\nHowever, I am not sure it helps in this case since the ts column comes after the duration.\nIt could also be that @liyichao has his installation misconfigured where the lookback period is longer than TTL.\nIf it's the case that we're running into expired records, maybe @liyichao needs to tweak the settings to make sure expiration compactions are done reliably? What's gc_grace_seconds set to?  We have it set to 0, since zipkin schema doesn't do any updates.\n. We have a dual filter in this query, d1 <= duration <= d2 and t1 <= timestamp <= t2, something that Cassandra cannot handle without client-side joins. The partition key (service_name, span_name, bucket) includes the time bucket, which is a timestamp floored to a certain precision (1hr by default). If the installation is configured with TTL > default lookback, the query should never look at expired cells. The client-side filter on the timestamp is actually no-op in most cases, except for the edge buckets. Sure, we can put a min(ttl, lookback) when calculating the startTs, which of course only helps if your ttl in the query service matches the actual TTL in the DB.\n. lgtm, let's see if it makes a difference for @liyichao\n. Thanks!\n. Does this approach only work in deployments with a single collector process, i.e. it's not scalable to multiple collectors?\n. I replaced the references to \"unknown\" with \"\" in zipkinCore.thrift.\nFWIW, we've been sending empty strings as service name without any ill effects, the only exception being the UI issue #896. And I recall seeing somewhere, maybe in the old Hadoop job, a code that was checking for empty strings.\n. Why use slash as a separator instead of dot?\n- for those accustomed to urls and xpaths, having slash in the middle but not at the front looks weird\n- in informal written English slash is often used to mean \"or\", as in \"If/when Mary ever shows up\"\n- slash needs to be encoded when passed in http query string http%2Fpath, dot doesn't\n. Why use slash as a separator instead of dot?\n- for those accustomed to urls and xpaths, having slash in the middle but not at the front looks weird\n- in informal written English slash is often used to mean \"or\", as in \"If/when Mary ever shows up\"\n- slash needs to be encoded when passed in http query string http%2Fpath, dot doesn't\n. I like it.\n. I like it.\n. sorry, replied on PR before seeing this. If the primary concern was \"test that shows we can process data older than a day\", then the fix is simple - I can probably leave the trace timestamps the same, but only change the query time to \"today\", because it's how Cassandra job always stores the result.\n. Actually, it might be better to use an enum {ParentAndChild, ParentOnly, ChildOnly} instead of a boolean flag.\n. Just to clarify what I want to do. I think of Dependency Graph as representing the architecture of the system, showing which services talk to each other. That function should be independent of how different tracers report data on RPC spans - one span or two spans. But at least in the Cassandra Spark job it is implemented to only build a link between two services if their names come from two distinct spans. I want to change that, and allow the job to consider, for example, standalone spans where the other service is still known (from SA or CA), as that still represents a valid architectural link.\nAs an illustration, look at the issue https://github.com/openzipkin/zipkin/issues/917. For a human it is obvious that there are 3 services involved in that \"headless\" trace, but the test expects only one link in the output.\nSo that's the primary objective of this ticket. The enum field is just a bit of extra data to record in each link, since the job now knows which of the two services was actually sending spans that were responsible for the link.\n@jcarres-mdsol I am not quite following your suggestion. There is no way to query Cassandra in bulk, a M/R job is the only way to process all the data.  Per above, all the information is already present during the main Spark job, so the recording that information in the link seems like the best approach.\n. @adriancole @jcarres-mdsol I've updated the description with the actual two new tests I am using to test the behavior. So far they only account for the shape of the graph, not for the extra enum describing the edge origin.\n. @kristofa it does work if the client is instrumented and emits ServerAddress. It does not work if only the server is instrumented, see https://github.com/openzipkin/zipkin/issues/917\n. @michaelsembwever the graph only says a->b, it doesn't say if a and b have been instrumented, and that info is critical in driving the adoption efforts in the company.\n. @adriancole I've restored the odd timestamps, as they don't affect how the test runs in the Spark job. The main fix is to make the query as-of today, else it doesn't find the dependencies data stored in Cassandra.\n. @adriancole ok I am going to take a look at https://github.com/openzipkin/zipkin-dependencies-spark/issues/7, and then perhaps this might just work out of the box. \n. Replaced by https://github.com/openzipkin/zipkin-dependencies-spark/pull/16\n. There's another option. From the description of this test, it's main goal is to check if the store can deal with traces where root span has traceID != spanID. To make this test forward-compatible with #914, we can change the assertion to expect at least List(zipkin-query->zipkin-jdbc(1)), but not break if there are other edges. My PR https://github.com/openzipkin/zipkin-dependencies-spark/pull/18 contains two more tests that specifically test the addition of edges for single-sided spans, so when they make it to core they will test the extra conditions. And once other stores implement support single-sided spans, this \"headless\" test can be changed back to \"should equal\". \n. An even better option - Capabilities API, #931\n. @adriancole by far-right do you mean those that will not log their own annotations? In that case the span will have Client Send/Recv and a ServerAddress, and the latter will win. I am not proposing to change that, only to let Server Send/Recv win over ServerAddress.\n. ok, LGTM\n. @t-yuki enhancing the interface is great! Only it's not clear to me what \"take back\" button means. Should it be just called \"Run\" or \"Query\" or \"Execute\"?\n. lgtm\n. Oh, I wasn't thinking of some \"magical\" services here (it's a different issue), but of a normal service that creates normal spans as any other service. But from the system architecture view this service X looks like a proxy of sort, e.g. it can have 50 services on the left and 200 services on the right, and the only path from left to right is through X. So the diagram looks like {50}->X->{200}, but gives no indication of the actual dependencies between left and right side. Maybe one of 50 depends only on one of 200, we can't tell from the diagram. But the trace data does tell that, so I'd like to have a way to reflect it in the diagram.\nNote that this problem has a second level of depth - showing dependencies by span/operation name, which is also something my users are very interested in. It would be nice to come up with a richer aggregation data model than just (parent, child, count), so that it would allow answering these questions.\n. The discussion is etching away from original topic, but afaik there is no restriction on the architecture diagram to be a DAG, e.g. below there are bidirectional dependencies. \n\n. We recently had a couple discussions about this internally, and decided that the best approach for us is to treat background tasks as a separate trace, which contains the original trace Id as a tag/foreign key.\n. Agreed that a new child span in the parent trace is always a good idea.  Also agree that it's up to the application to decide is the background work should be a part of the same trace or not - for us it's better as a separate trace, because the job can be queued and executed minutes after the main trace is finished, so merging them together creates various unwanted side effects, like the duration of the trace becomes all messed up, and the UI rendering is poor due to different scale.\nw.r.t. capturing the parent, one idea that was discussed in OpenTracing was that capturing multiple parents of a span (e.g. via something like span.add_parents(other_spans)) can work not only for joins within the same trace, but also for the above use case of linking multiple traces. The root span of the background job trace will have the trace/span ID of the \"separate span\" that you mentioned from the parent trace.\n. @sidneyshek we're very interested in ES backend!!! Would love to see a working impl. Cassandra is good at throughput, but the querying capabilities are exceptionally poor in the current Zipkin implementation.\n. not much interest in this, so closing\n. > here's an issue where binary annotations were a bit confusing (as they allow repeats on keys)\nis it fair to say that ^^^ is only confusing because the UI does not associate binary annotation with the \"span kind\" that contains them? For example, I made a fix that adds service name to SA/CA annotations\n\nAnother common scenario I've seen with same keys is the http.url, which is sent by both client and server, and the value may or may not be the same.\nIf we adopt the suggestion that (spanID, spanKind, emittingService) combination is unique (which I think implies ditching core annotations and moving the Endpoint on the Span itself), then seemingly identical binary annotations may still be distinguishable by the service that emits them.\n. @prat0318 just to make sure, did you click on Expand All?\n. @prat0318 just to make sure, did you click on Expand All?\n. Incidentally, there may be another issue (maybe with our instrumentation), because all spans in this trace are actually from the same host, so the local span should have skew=0, which would've caused the adjustment logic to be skipped, yet that didn't happen. I haven't investigated yet why it finds a non-zero skew (most likely because of the parent spans).\n. Incidentally, there may be another issue (maybe with our instrumentation), because all spans in this trace are actually from the same host, so the local span should have skew=0, which would've caused the adjustment logic to be skipped, yet that didn't happen. I haven't investigated yet why it finds a non-zero skew (most likely because of the parent spans).\n. @adriancole currently the change is limited to LC spans. It breaks one test in the span store spec that expects span-reported duration to be overwritten by the adjuster.\nShould this actually apply to ALL spans? I.e. I can't think of a good reason for the adjuster to ever override the span's timestamp / duration based on the annotations, it should only be allowed to shift the timestamp by the skew.\n. @adriancole currently the change is limited to LC spans. It breaks one test in the span store spec that expects span-reported duration to be overwritten by the adjuster.\nShould this actually apply to ALL spans? I.e. I can't think of a good reason for the adjuster to ever override the span's timestamp / duration based on the annotations, it should only be allowed to shift the timestamp by the skew.\n. @adriancole yes, sounds like the right approach.\n. lgtm\n. @adriancole do you mean they were unqueryable because the storage did not index them by the timestamp since it was missing?\n. ingenious solution\nlgtm\n. I don't have a strong opinion about the new driver, if anything I prefer not to upgrade because it might be incompatible with our Cassandra version (I started getting non-fatal exceptions in the Spark job after the last upgrade). But don't let me stop you if there are perf improvements - we are not exposed to perf on the writing side since our collectors are in Go.\n. I don't have a strong opinion about the new driver, if anything I prefer not to upgrade because it might be incompatible with our Cassandra version (I started getting non-fatal exceptions in the Spark job after the last upgrade). But don't let me stop you if there are perf improvements - we are not exposed to perf on the writing side since our collectors are in Go.\n. tbh, I don't even know when it's valid for a child to affect parent's duration, which is what causes updates to duration in the adjuster. If a client emitted cs/cr, their difference defines the duration and no clock skew should be able to affect it. If the server logged a pair of sr/ss, then their difference should never be larger than the client side duration, independent of clock skew, as otherwise it breaks causality. The only scenario where server span can be longer than client span is in fire and forget cases, which arguably should not be modeled with client and server sharing the span ID.\nSo unless I am missing a use case, it seems perfectly valid for collectors to precompute duration based on annotations (ideally from the same host), and clock skew adjuster should never need to change that duration.\n. tbh, I don't even know when it's valid for a child to affect parent's duration, which is what causes updates to duration in the adjuster. If a client emitted cs/cr, their difference defines the duration and no clock skew should be able to affect it. If the server logged a pair of sr/ss, then their difference should never be larger than the client side duration, independent of clock skew, as otherwise it breaks causality. The only scenario where server span can be longer than client span is in fire and forget cases, which arguably should not be modeled with client and server sharing the span ID.\nSo unless I am missing a use case, it seems perfectly valid for collectors to precompute duration based on annotations (ideally from the same host), and clock skew adjuster should never need to change that duration.\n. > batching writes to Cassandra into small, unlogged batches, which I think would improvement performance.\nI've been told quite the opposite by the team with more experience with Cassandra. A batch can span multiple nodes, so batching puts more load on the coordinating node, rather than that work to be done by a client with token-aware connection.\nSo you may want to profile it first, before changing the storage implementation to support batches.\n. > batching writes to Cassandra into small, unlogged batches, which I think would improvement performance.\nI've been told quite the opposite by the team with more experience with Cassandra. A batch can span multiple nodes, so batching puts more load on the coordinating node, rather than that work to be done by a client with token-aware connection.\nSo you may want to profile it first, before changing the storage implementation to support batches.\n. Thanks for the link, Daniel. It seems the articles agrees with what I said, that fan-out from the client is more efficient then batching and sending to a coordination node. But if collector reads KAFKA and writes to C one span at a time, then yes it would be bad and batching might help. It's just you'd be optimizing the wrong end. We don't use collectors over KAFKA yet (actually working on it for another use case), but our collectors (in Go) have an internal queue which is feeding N go-routines, over a token-aware connection. The same thing can be easily done in Scala/Java collectors.\nAlso worth noting that Cassandra connections are multiplexing, I think up to 128 or 256 streams, but we haven't gone that high in the # of threads.\n. Thanks for the link, Daniel. It seems the articles agrees with what I said, that fan-out from the client is more efficient then batching and sending to a coordination node. But if collector reads KAFKA and writes to C one span at a time, then yes it would be bad and batching might help. It's just you'd be optimizing the wrong end. We don't use collectors over KAFKA yet (actually working on it for another use case), but our collectors (in Go) have an internal queue which is feeding N go-routines, over a token-aware connection. The same thing can be easily done in Scala/Java collectors.\nAlso worth noting that Cassandra connections are multiplexing, I think up to 128 or 256 streams, but we haven't gone that high in the # of threads.\n. seems reasonable\n. seems reasonable\n. ok, that worked, I can submit a pr, but first need to fix the build issue I posted in gitter\n. I haven't looked deep into how kafka consumers work and whether running >1 threads will increase read throughput. But what I was suggesting is putting an intermediate in-process queue \n{kafka reader(s)} => {queue} => {executor pool of Cassandra writers}\nIf reader(s) can read fast enough, you can tune this to saturate Cassandra write throughput just by increasing the # of threads on the right. Queue can be blocking, to cause back pressure.\nThe other advantage of this approach is in allowing the collectors to receive data via different input sources.\n. I haven't looked deep into how kafka consumers work and whether running >1 threads will increase read throughput. But what I was suggesting is putting an intermediate in-process queue \n{kafka reader(s)} => {queue} => {executor pool of Cassandra writers}\nIf reader(s) can read fast enough, you can tune this to saturate Cassandra write throughput just by increasing the # of threads on the right. Queue can be blocking, to cause back pressure.\nThe other advantage of this approach is in allowing the collectors to receive data via different input sources.\n. @adriancole the examples you asked for. Looks fairly normal, the one weird thing is the query service adds server annotations to the middle span client.post. Also, the server span is centered, even though the IP is the same (though I haven't used the latest code).\n3b47c02255029f91.json.txt\n\n\n\n\n. lgtm\n. thank you!\n. The default Cassandra installation uses dc=datacenter1. The real one should come from env, as you said.\n. As for impact, it depends how the prod cluster is configured. We had one cluster name spanning 2 DCs, with distinct keyspaces per DC, yet the plain round robin policy would periodically pick a host from the other DC as coordinating node, causing latency up to 60sec.\n. @eirslett how is the webpack being installed now?  I don't see it mentioned in the npm install logs.\n. @eirslett how is the webpack being installed now?  I don't see it mentioned in the npm install logs.\n. Setting NODE_ENV=development fixed the build. Is that how it's supposed to be during the build?\n. Setting NODE_ENV=development fixed the build. Is that how it's supposed to be during the build?\n. https://github.com/openzipkin/zipkin/pull/975\n. https://github.com/openzipkin/zipkin/pull/975\n. @eirslett since you're actively working on renovation of the UI, do you mind creating an issue with a high level roadmap, like\n- [ ] do X\n- [ ] do Y\n  ?\nThis will provide better visibility into how far the work is / going to be and when it's best to upgrade the forks.  I had a couple of minor UI fixes in our fork that I planned to push back to openzipkin, but with these and upcoming changes they may not work.\n. @eirslett since you're actively working on renovation of the UI, do you mind creating an issue with a high level roadmap, like\n- [ ] do X\n- [ ] do Y\n  ?\nThis will provide better visibility into how far the work is / going to be and when it's best to upgrade the forks.  I had a couple of minor UI fixes in our fork that I planned to push back to openzipkin, but with these and upcoming changes they may not work.\n. @eirslett in zipkin-web/src/main/resources/templates/v2/layout.mustache I had this diff below to fix nav bar highlighting. The variables isIndexPage/isDependenciesPage were set somewhere in Scala. Where is the best place to set them now?\ndiff\n -            <li class=\"active\"><a href=\"/\">Find a trace</a></li>\n -            <li><a href=\"/dependency\">Dependencies</a></li>\n +            <li {{#isIndexPage}}class=\"active\"{{/isIndexPage}}><a href=\"/\">Find a trace</a></li>\n +            <li {{#isDependenciesPage}}class=\"active\"{{/isDependenciesPage}}><a href=\"/dependency\">Dependencies</a></li>\n +            <li><a href=\"http://t.uber.com/jaeger\">Wiki / Help</a></li>\n. @eirslett another question, in zipkin-web/src/main/resources/templates/v2/index.mustache we used to have a section that was displayed on the home page (before query was executed), for example:\nhtml\n{{^queryResults}}\n  <div class=\"alert alert-info\" id=\"help-msg\">\n    <p>Please select the criteria for your trace lookup.</p>\n  </div>\n  <div id=\"logo\"></div>\n{{/queryResults}}\nHow this section never shows. Is it because the syntax {{^queryResults}} is no longer recognized?\n. I get the same:\n$ grep 'file://\\$MODULE_DIR\\$/config' */*.iml\nzipkin-collector-service/zipkin-collector-service.iml:      <sourceFolder url=\"file://$MODULE_DIR$/config\" isTestSource=\"false\" type=\"java-resource\"/>\nzipkin-query-service/zipkin-query-service.iml:      <sourceFolder url=\"file://$MODULE_DIR$/config\" isTestSource=\"false\" type=\"java-resource\"/>\nActually just tried clean import from clean zipkin repo and didn't get compile errors. Maybe it's something about our fork, even though #544 is present. \n. lgtm\n. I'm also hitting cache, except for the first time\n\nIt's probably that first time that I've been noticing, since the cache is only for 10min. If the list was permanently stored in the browser in local storage, the first load after 10min would still be unnoticeable. Is it possible to refresh the list in the background after first displaying from local storage? Or would it completely disrupt the experience if the user has already started interacting with the dropdown?\n. I think it's ok to filter dups, provided that high fidelity data is used for comparisons.\nspan id=1\n   anno:\n     sr, host=1.2.3.4\n   bin_anno:\n     x = y\nspan id=1\n   anno:\n     cs, host=5.6.7.8\n   bin_anno:\n     x = y\nhere the two identical binary annotations are actually sent by different servers, and may have different meaning. This correlation gets lost once the spans are merged in the UI into one.\n. no\n. @jcarres-mdsol do you have suggestion on what the health endpoint should be checking?  I often see those implemented as return OK, which only tells the server is running, not that it's healthy. If we decide, for example, to include some db query in the health check, then it may require extending span storage api.\n. Did anyone do any benchmarks to show that writing/reading Kafka messages with a single span is slowed than messages with batch of spans?\nI do agree with @eirslett that APIs that accept just a single span instead of a collector are suboptimal. Although logging a warning would just spam the collector's log, it can't do much about what clients submit.\n. is there indeed a lot of setup overhead to use a different topic for different message format?\n. do we also supported gzipped GETs?\n. sounds like a cool feature, although not clear who the intended audience is. I do have one team who wants to read /dependencies endpoint for some other SRE service, but not really the query API.\n. The other fields of this struct are also used. Spark job does this:\ndef saveToCassandra(sc: SparkContext, keyspace: String, dependencies: DependenciesInfo): Unit = {\n    val thrift = dependencies.toDependencies(startTs = startTs, endTs = endTs).toThrift\nand Cassandra store looks at these timestamps when filtering iirc. That's not to say that it must stay this way, a cleaner approach would've been to extend the table schema and store those values as proper fields in the table.\nIs anything else dependent on the Link struct?  If not, this could be a candidate for de-thrifting as well, e.g. by using  native vector types that Cassandra supports (not sure how easy it is to write those from Spark).\nAs far as transition from old schema to new, I would instead go with a simple migration script that would copy from thrift-based dependencies table into a new table with clean schema, without thrift. Then people can upgrade both the server and Spark job and still have all the historical data (e.g. we have no TTL on the dependencies table, we keep it forever).\n. lgtm\n. LGTM\nThanks for putting this in!  It will be very useful during our instrumentation work.\nHint: the new trace image on zipkin.io is actually partially broken, if you pay attention - the root (local) span is not shown, so the first span does not encompass the whole trace.  I think with ^^^ change I can dig down easier why that happens (I'm pretty sure adjusters drop it for some reason).\n. LGTM\nThanks for putting this in!  It will be very useful during our instrumentation work.\nHint: the new trace image on zipkin.io is actually partially broken, if you pay attention - the root (local) span is not shown, so the first span does not encompass the whole trace.  I think with ^^^ change I can dig down easier why that happens (I'm pretty sure adjusters drop it for some reason).\n. from my perspective, I wanted the \"raw\" as in \"no adjustments\", which includes not only clock skew, but all other adjustments that can potentially drop or alter the spans recorded in the storage.\n. +1\n. +1\n. Yes, definitely should be a config option, like maxSpanCountForExpandAll\nI haven't seen the new config mechanism yet, do we have docs on how one can change config options at deployment?\n. Personally, I've always disliked it when websites open tabs without being asked to. It breaks the default behavior of the browser and user expectations. Here's a good summary with many references: http://libux.co/links-should-open-in-the-same-window/\nA user can always choose to open a new tab via middle-click or Cmd-click.\n. Personally, I've always disliked it when websites open tabs without being asked to. It breaks the default behavior of the browser and user expectations. Here's a good summary with many references: http://libux.co/links-should-open-in-the-same-window/\nA user can always choose to open a new tab via middle-click or Cmd-click.\n. I believe the query REST API accepts a lookback parameter, it's just not exposed in the UI.\n. I believe the query REST API accepts a lookback parameter, it's just not exposed in the UI.\n. a good UI widget for lookback would be the way Grafana2 does it. I wonder if it's available somewhere in reusable form.\n\n. a good UI widget for lookback would be the way Grafana2 does it. I wonder if it's available somewhere in reusable form.\n\n. +1\n. +1\n. In our instrumentation libraries we adopted a convention that parentID==0 means no parent (a more natural \"empty\" value for int64 than null ptr). The collector adjusts that into null Thrift value when inserting into Cassandra. Might be useful convention to adopt.\n. Since the UI has been decoupled from the server side, another possible solution is to have a Save As, so that the user can save the trace json as a file on local disk and later load it into the UI. It's especially useful considering that tracing ultimately is used to troubleshoot perf issues, so one could save a trace, attach it to a ticket, and someone else can later load the trace and see it in the UI.\nWe already have the JSON button, so Save As is there, but we don't have a Load function in the UI.\n. huh, I didn't realize, thought it was in already.\n. lgtm\nany particular new feature you are after?\n. One can always drag a box around 5 spans on the right to zoom in.\n. @adriancole are there unit tests for what gets displayed in the UI?\n. this PR is unrelated to that discussion. It is here to display service name normally encoded in SA/CA/LC, which is otherwise hidden (LC had a special handling, which with this PR becomes unnecessary). The service name in the SA/CA annotations is respected in other parts of the code, specifically in the logic that derives a single service name for a span: https://github.com/openzipkin/zipkin/blob/master/zipkin-ui/js/component_ui/traceSummary.js#L53\n. lgtm. Thanks for improvements & tests. I was porting this change from our pre-JS fork where it was only affecting the bottom part.\n. lgtm\n. what about moving scala code to a branch in zipkin repo?  If people have forks which are reasonably up to date, can they just rebase from the scala branch?\n. I meant as a flavor of option 2, i.e. consolidate both java and scala impls in zipkin repo, in master and scala branches respectively. It may address the first con (with rebase), but not the second. \nThe second con is a bit questionable, as there may be many things in closed issues that shed light as to why certain things are done one way or another, especially if java impl just copied the original logic. But I hardly ever used search on the old issues/PRs, so I'm not the right person to judge that.\n. is json raw or merged?\n. @adriancole  we added two buttons, raw and regular json, as both come in handy. But we just have them as plain links to json documents, not popups, and we leave pretty rendering to browser plugin. That way it's easy to search inside the json, and do a Save As. \n. in OpenTracing a span in error is indicated by span.SetTag(\"error\", true). Would be neat if Zipkin UI recognized it out of the box, without translation.\nAnother option, not yet standardized in OpenTracing, is span.log_event(\"error\"), which could be displayed as a red dot on a timeline, rather than coloring the whole span. \nThe former is a stronger statement (made by end user) about the state of the span than the latter.\n. fwiw, I know exactly why LIMIT is not working correctly with Cassandra. In MySQL all the data is in one place, so however complex the query is, it is first satisfied against all AND clauses and then a limit is applied. With Cassandra, each AND condition may need to be resolved against a different index table, by doing direct shard key lookup. So instead of (x AND y AND z) % LIMIT, the Cassandra SpanStore implementation does (x % LIMIT) AND (y % LIMIT) AND (z % LIMIT). The resulting intersection can easily produce < LIMIT results, quite often 0 if you have many AND clauses and LIMIT is small.\n. Yes, making the LIMIT issue reproducable would be nice.\nOn a simple query, I wonder if this is because the same trace ID gets returned multiple times. Most index tables in Cassandra allow dups of (search_key -> trace_id) because timestamps are used to differentiate the records. Doing otherwise would've resulted in lots of tombstones, degrading performance. The LIMIT clause does not know that the same trace_id is being returned.\n. > This means it can miss traces that happen against the same service in the same millisecond.\nYep, sounds right, given PRIMARY KEY ((service_name, bucket), ts). Assuming they also hit the same bucket.\nWould've been better with this key:\nPRIMARY KEY ((service_name, bucket), ts, trace_id)\n. lgtm\n. I think this is certainly an improvement. I would be careful with two things, though:\n1. an RPC span may contain more than 2 service names, e.g. from annotations added by the routing layer like haproxy. It would be good to restrict the display to only respect service names from rpc client & server\n2. The current display of service name sometimes can't even fit one service name, so perhaps some clever shortening of the names with a tooltip is warranted\n. > Actual query constraints against a duration range would no longer be possible\nThat was the reason duration index was introduced in the first place. My users often run queries where duration > XXXms, and the timeframe of the results it much less important than finding the actual long traces.\n. >  I don't think this will be a problem though, because queries are still implicitly constrained by a time range.\nWell, that's the problem, if my primary goal is finding long traces, applying a time range constraint first may produce no results. The duration index was duration constraint first, time range second.\n. Duration query usually would be D > Nms. The 3days time range has little impact on the results because of the LIMIT, which defaults to 100 in our installation. If a long span happens say every 10min, then at our traffic volume you'd need to run a limit of about 1,000,000 to find a matching span with desired duration.\n. @michaelsembwever don't hold on my account, we haven't even upgraded to the Java version of Zipkin, we're still running Scala build.\nHowever, since you mentioned SASI, would it make sense to reimplement all indexing of Zipkin spans via secondary indices? The existing manual indexing doesn't work well anyway if a query contains clauses that must be resolved by different manual index tables today.\n. @adriancole does this work in either of the Cassandra implementations? Looks like it might in v2 because of zipkin2.span (annotation_query) and zipkin2.trace_by_service_span (duration) SASI indices, but v1 uses service name as a PK field iirc.. ah, fan-out, nice trick. Not an option for us, unfortunately (3k services).\nthanks.. > actually, I don't like traceIdHigh as it would leave identifiers in a transitional state (and make querying awkward). I think it would be best to obviate the old traceId field with a wider one rather than append to it.\n@adriancole do you have an alternative proposal for thrift?  binary?\n. Yes, I did mean extension to current thrift. String sounds rather wasteful, binary will have the minimal required length. Problem with both is that in-memory they will have to be pointers, whereas something like traceIdHigh could be a primitive value (then again, if it's optional it's still going to be a pointer).\n. > there is no encoded string type in thrift. string (TType 11) fields are binary\nWhat about these?\nthrift\nstruct BinaryAnnotation {\n  1: string key,\n  2: binary value,\nIn the generated classes those will be string vs. []byte so to store 128bit value in string it would have to be encoded as hex or base64, while in binary it will be stored as is.\n. Do Amazon docs explain why recording the time is useful?. That sounds like an odd reason. The collection pipeline already knows when it receives the spans and can establish a TTL from that point, passing the timestamp in the headers doesn't add much there. There must be some other reason.. I see. Makes sense. Thanks.. @adriancole so what kind of interop are you targeting, a Zipkin-instrumented system talking to AWS SaaS?. I am, of course, biased, but just want to mention that using the OpenTracing API opens up a lot of possibilities to reuse already existing instrumentations like these:\n- https://github.com/uber-common/opentracing-python-instrumentation\n- https://github.com/opentracing-contrib/python-django\n- https://github.com/opentracing-contrib/python-flask\nI do not have free cycles to work on supporting B3 headers in jaeger-client-python, but am happy to provide guidance and prompt code reviews if someone wants to add the B3-based injectors/extractors and a Sender to ship already Thrift-formatted spans to one of Zipkin supported transports.\n. Yes, all 3 instrumentation libs above can be used with any OpenTracing tracer.\n. @mjbryant Adrian recently sent a PR to our jaeger-client-java that added a sender and codecs to support B3 headers and report to Zipkin directly. A similar change can be made to jaeger-client-python\n. @mjbryant Adrian recently sent a PR to our jaeger-client-java that added a sender and codecs to support B3 headers and report to Zipkin directly. A similar change can be made to jaeger-client-python\n. In similar situations (ingesting from a queue-like pipeline) we simply emit a metric as timer(\"ingestion.latency\", time.Now() - span.startTime). Wouldn't this be sufficient for this use case, rather than storing it in ES?\n. agreed. As I replied on jaeger ticket, the incompatibility is indeed intentional, as the endpoints are all identical and redundant, so we don't send them nor store them, but we do build the index for all tags/logs.\n. agreed. As I replied on jaeger ticket, the incompatibility is indeed intentional, as the endpoints are all identical and redundant, so we don't send them nor store them, but we do build the index for all tags/logs.\n. @adriancole in our UI we make it collapsed by default, so as not to distract casual users.\n\n. At the moment just the span ID ('cause trace ID is in the URL anyway). We also have the capability to show the JSON (pretty-printed) of that particular span, but didn't feel we need it yet (we have a separate JSON view for the full trace).. \n. Tip: we were able to scale Jaeger UI to 50k spans by using a viewport technique.. Tip: we were able to scale Jaeger UI to 50k spans by using a viewport technique.. @adriancole what about the old format that was blocking openapi ?. Curious as to the reasoning behind different TTLs. So these are the interop models for the out-of-band data?\n  * [zipkin SDK] -> [X-Ray backend] directly\n  * [zipkin SDK] -> [zipkin collector] -> [X-Ray backend]\n. looks like the schema change for service_name table is missing\n. this removes bucketing functionality for span names - why?\n. a couple of problems with this cache. First, why one hour? Different installations might use different TTL in Cassandra, in most cases at least 1 day, so cache TTL of 1hr is too fine. I have similar logic in Go collector, we use 12hr TTL for cache. In this OSS implementation I'd suggest making this TTL externally configurable.\nSecond problem - the TTL is on the whole cache, not on individual entries. It means once an hour all threads in all collectors will drop the cache and re-insert unnecessary data. \nGranted, both are not huge issues given the ratio of writes, but the implementation would not be much more difficult if both were addressed.\nAlso, thread locals are unnecessary here, a concurrent map shared between threads would do.\n. Is there a way to completely disable scribe reporter in this case? I am not sure if setting sampling to 0 would prevent it from trying to make connections to scribe. I was able to disable ALL trace reporting with below, but not selectively disable only scribe reporter.\nval role = Stack.Role(\"TraceInitializerFilter\")\n    val stack = Httpx.Server.stack.remove(role) // hack!!!\n    val server = Httpx.Server(stack)\n    val web = server.serve(webServerPort(), ServiceFactory.const(webService))\n. @adriancole note that even though we can override the tracer this way, the DefaultTracer still instantiates some default ones via\nprivate[this] val tracers = LoadService[Tracer]()\n. can we keep it here and just document as deprecated?  I'm concerned this would make historical data in Cassandra incompatible.\n. Would be nice to separate HTTP API concerns from the business logic into different classes, to allow different implementations of the end-point, possibly with alternative (non-http) protocols / discovery mechanisms.\n. lest I forget - I found it useful to also measure latency, not just counts\n. given that it's also OR within the collection, a Map would be limiting struct, a Set[(String, String)] would be better\n. not clear on this last part - meaning that annotations will be sorted within each span?  Also, should this do a merge of spans with the same ID?  Otherwise it returns redundant data.\n. Perhaps the recommendation may be to leave endpoint logging to CS and SR events (\"begin\" events), not do it in the \"end\" events.\n. Perhaps the recommendation may be to leave endpoint logging to CS and SR events (\"begin\" events), not do it in the \"end\" events.\n. Shouldn't the value be the actual address, in whichever form user code chose to send it? A URI format could be  recommended: tcp://hostname:port.\nAlso, if this attribute is sent, why have the endpoints in the events?\n. I don't think that's a reasonable limitation. Just the other day I had a discussion with the data team where they sometimes have spans measured in hours. I would go for i64.\n. What is the benefit of capturing this explicitly instead of the usual cs/cr? It feels like it's going to complicate other code paths as they need to handle dual representation.\n. maybe make it a flag, since e.g. for Dependencies job you don't need timestamps, just the relationships.\n. not sure I follow this one. Shouldn't it be like this:\nstart := min(*.timestamp)\nend := max(*.timestamp + *.duration)\nduration := end - start\n. I think the dependencies job itself just reads all data from traces table. There's no index in C* implementation to only query for traces within a given timeframe. But anyway, dep job does its own merging now, so ok.\n. I suggest Duration (&#956;s) &#8805;\n. @adriancole \nQ1 - if I am reading sql query correctly, the duration limit applies to individual span duration, not the overall trace duration, is that correct?\nQ2 - what are the semantics of serviceName here? Any service involved in a span satisfying the duration limit? What about services involved in a long trace, but not in the long span in that trace?\nQ3 - shouldn't we include span name in the search?\n. If you have A->C->B, and any other trace involving B, I think the query by service B should always return all those traces if the B span was long. The owner of service B has no control over the shape of traces above B, so when they want to diagnose performance of B, they should see all traces containing B.\n. shockingly, yes.  https://issues.apache.org/jira/browse/CASSANDRA-4175\nIt may be solved in v. 2.2+, so maybe I should put it back to long names.\n. Can't do much about that, if the correct span name is not available/incorrect at the time of the insert. It will only get indexed by the service name.\n. Just checked, according to my teammates who use Cassandra for metrics, it's not solved in 2.2 either.\n. I am going by the experience of our metrics team. They have tables of similar complexity, if not simpler, and they observed a significant impact of shortening column names on the overall performance of the nodes.  The issue has been addressed in Cassandra 3.0, according to the history in the JIRA above.\n. here's some anecdotal evidence (since it's hard to measure exactly)\n\n... as an example, for data file sizes > 24h (and thus at the max level of compaction)\n168829777409 with long column names\n31931936664 with short column names\n. Yes, this was with compression. The schema is like below. Only the n(name) field would have longer strings, others are usually a single number, which become comparable in space usage to column names if those are fully spelled out.\n\nThe x5 size difference may not be directly attributed to column names, but that's the best measure they had. I don't know how much of intellectual burden short names really are, there are like 10 lines of code that depend on them. Taking extra disk space affects performance in various ways, in streaming, further compactions, etc.\nBut I am not strongly against changing this to long names.\nCREATE TABLE events (\n  n text,\n  rt bigint,\n  t bigint,\n  v blob,\n  PRIMARY KEY ((n, rt), t)\n) WITH compression={'sstable_compression': 'LZ4Compressor'};\n. What specifically depends on traceId == id condition for the root span?  It seems very artificial, why is it not enough to have empty parent ID?\n. Added https://github.com/openzipkin/zipkin-dependencies-spark/issues/3\n. I am not sure we do have a contract to never return spans without service name. We said we can't find such spans by service name (obviously), but if they are retrieved as part of a larger trace, I think we need to return them. They may contain lots of interesting information useful even without a service name.\n. There is one known issue in the spark job that it always aggregates everything in the DB, i.e. it does not filter out any spans by timestamps. I am looking to fix it in https://github.com/openzipkin/zipkin-dependencies-spark/issues/7. But that's not the problem here.\nIt is that the job takes no inputs, and always saves the computed result as-of today. That's why the tests with hardcoded query time are going to fail.\n. What was the objective of the test? I thought it was testing the shape of the resulting graph, sounds like you think it was doing more than that. There are certain caveats with testing the specific timestamps & lookback values I described in the previous comment. E.g. the as-of query, with/out lookback, is unlikely to work without making the Spark job taking the startTs/endTs parameters.\n. if we want to test query param, I would rather do different tests (after passing timestamps to the Spark job), where we have traces A->B, C->D, etc. placed in or out of the time range.  For traces that are on both sides of the time range, the behavior is not even clearly defined today, e.g. Spark job might read the trace partially (after  openzipkin/zipkin-dependencies-spark#7 is done)\n. you could've used the existing endTs element ID\n. I see, you want this function to apply to all timestamps, not just this one endTs.\n. why do we need to add duration?\n. how is the timeStamp function scoped? When it does this.$node.find(\".timestamp-value\");, is it guaranteed to only search children of some element, e.g. the div? If it does a global search it could find other timestamp fields with the same class.\n. I guess it does the same for date-input and time-input, so nvm.\n. iirc this is the same as the const at the top of the class\n. ironically, Cassandra Spark job might succeed on this, by accident. But in general we need #931 to allow adding per-store features, so that tests can be checking capabilities before setting expectations.\n. Well, this test is artificially constructed to allow stores to \"cheat\" (i.e. they pass it by coincidence). So technically it does not introduce a new feature, but if we strictly follow its intent , i.e. building A->B link when there are local spans between A and B, then I can easily construct traces that will fail in most stores. And if we want Anorm to pass after those harder traces, it would be a store-specific feature.\n. I agree that capabilities API is not needed if you can simply ignore the test. Unfortunately, that's not always the case, e.g. the headless_trace test actually needs a different comparison target if the dependencies job can detect uninstrumented clients.\nI also think introducing capabilities that require the actual business code doing if-then checks is generally a very bad idea, so I would carefully watch for that. The preferred way is when the store without the capability degrades gracefully into accepted default, e.g. in case of #930, the default null value (or ParentAndChild at the domain model) does not break anything and allows the rest of the code work as is.\n. the default load balancing is not ideal, we ran into issue with it and changed to \nval lbPolicy = new TokenAwarePolicy(new LatencyAwarePolicy.Builder(new DCAwareRoundRobinPolicy(dc)).build())\n. from L289 it looks like svc->spans has already been resolved, but 290 is doing it again via getSpansByService(svc)?\n. not sure if other people get that, but when I import the project on a clean repo, IntelliJ cannot build it because of configuration files in zipkin-collector-service/config and zipkin-query-service/config. These dirs are somehow added as both test resource and some other type, and I have to open project settings and unselect them to make sure they are only included as test resource (so that IntelliJ doesn't try to compile them).\n. I open the project file generated by gradle. It then offers to convert the project file to a project dir, that's the only popup. And then it always fails to build unless I remove config files from being considered \"source\"\n. why skip latency-aware policy in this case?\n. in our installation, populating service names takes from one to several seconds.  Adrian tried to add caching control some time ago, but it doesn't look like it helped in our case. Does it makes sense to cache the whole HTML of the dropdown in the UI?\n. mine concern is that retrieving 100s of service names even from cache and rebuilding the DOM is expensive.\n. perhaps is would be better to define a flag based on modelView.traces.length?  What happens if I GET the home page with ?serviceName=x? \n. is this the same behavior as before?  I thought the lookback interval was configurable via env variable.\n. can it be read from config.js?\n. s/defaultStartTs/defaultLookback/ ?\n. looks like travis's linter wants single quotes around \"object\"\n. java.util.concurrent.ThreadLocalRandom ?\n. @adriancole I do not see a change in the thrift file. Are there unit tests verifying that this manual serialization is compatible with the native Thrift serialization done by classes generated from .thrift IDL file?\n. ",
    "ghelmling": "Pretty cool, thanks for the pointer!\nOn Mon, Jul 15, 2013 at 6:12 PM, dvryaboy notifications@github.com wrote:\n\n@ghelmling https://github.com/ghelmling you and Chris might be\ninterested :)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/pull/274#issuecomment-21015278\n.\n. I think the Travis CI failure is due to build environment (MiniDFSCluster expects a umask of 022).  Funny enough, I just answered another question on this earlier today: \nhttp://stackoverflow.com/questions/17625938/hbase-minidfscluster-java-fails-in-certain-environments/17665439#17665439\n\nFor another project (hRaven), we had to modify the .travis.yml file to ensure umask was set to \"022\" prior to the test run by adding the line:\nscript: umask 0022 && mvn test\n. ",
    "IceCreamYou": "Thanks! Glad you're enjoying the project.\n. @franklinhu this is against master after the anorm commit.\nHowever, I'm getting confused tracking two concurrent but dependent sets of changes with the async PR and that one is more current so I'm going to close this one.\n. Yes\n. I think writes are async with Cassandra and Redis so this wouldn't change behavior.\n. I think writes are async with Cassandra and Redis so this wouldn't change behavior.\n. Anyone know if there is some kind of issue with Await.result() on FuturePools? Not sure why the AnormIndexSpec tests are timing out.\n. Anyone know if there is some kind of issue with Await.result() on FuturePools? Not sure why the AnormIndexSpec tests are timing out.\n. Per my comment here I am getting confused tracking two concurrent but dependent sets of changes to the Anorm module (adding async + transactions and adding aggregates support) so I am closing the other PR and using this one for both since this branch is more current.\n. Per my comment here I am getting confused tracking two concurrent but dependent sets of changes to the Anorm module (adding async + transactions and adding aggregates support) so I am closing the other PR and using this one for both since this branch is more current.\n. Heads up, I force-pushed so I could rebase to current master since there were merge conflicts.\n. Heads up, I force-pushed so I could rebase to current master since there were merge conflicts.\n. Committed fix to master :)\n. Committed fix to master :)\n. Thanks! One of those \"can't believe I missed that\" moments... :)\nOn the \"multiple exit points\" question -- guard clauses are the only recommended use of returns, but I have no objection to doing it the more idiomatic way.\n. Thanks! One of those \"can't believe I missed that\" moments... :)\nOn the \"multiple exit points\" question -- guard clauses are the only recommended use of returns, but I have no objection to doing it the more idiomatic way.\n. Note that this is a breaking change -- that is, people who have installed Zipkin and are using the Anorm module already (e.g. with SQLite) will get errors after applying this change because their schema will not match the schema expected after this change. We should probably have a plan for how to approach schema migration.\n. Note that this is a breaking change -- that is, people who have installed Zipkin and are using the Anorm module already (e.g. with SQLite) will get errors after applying this change because their schema will not match the schema expected after this change. We should probably have a plan for how to approach schema migration.\n. I like this. I think changing the hash works better here than window.history.pushState and friends. LGTM.\n. I like this. I think changing the hash works better here than window.history.pushState and friends. LGTM.\n. Per discussion with Brian, we don't actually care about testing this case since we want to fix it, so we might as well go ahead and remove this test.\n. Nope, that's my mistake. Zipkin should work on JDK 1.6 too. And the point about parens is not too nitpicky. Will fix shortly.\n. For example, I have Zipkin running locally on my macbook, but it's using a production query daemon. The result is that I can develop Zipkin locally but have real data to work with in order to see the effects of my front-end changes. This is mainly because it's kind of hard to generate a bunch of useful test data.\nIs that clearer? Do you have a suggestion for a better way to phrase this?\n. Well presumably we would like to change this in the future when we deprecate JDK 1.6 support, so it would be good to note that somewhere. And we'd also like to avoid someone coming along later and saying \"oh, that shouldn't be hardcoded, I should change it to getLocalHost.\" Besides, I think it's a fair bet that someone capable of installing and maintaining Zipkin would know what the JDK is. Thoughts?\n. In principal yes, but in practice because this renders in the browser chrome, there are a bunch of limitations on what JS can do and I believe Hogan would have to be significantly altered to work in that environment. Firefox has its own Chrome DOM templating engine called DOMplate which I found significantly more obtuse and wordy than writing the HTML by hand. That said, it would be nice to find a better solution here.\n. I don't know if there's a clear convention. In writing this extension I found some that used HTML and some that used Domplate. Domplate isn't the same kind of templating library as Hogan because you write the template as code rather than as HTML with substitutions.\nLet's address this in a separate issue: https://github.com/twitter/zipkin/issues/264\n. It's the SQLite driver for JDBC. We decided that the MVP would be to get Zipkin running on SQLite, so until we can load config dynamically I just put this in directly. But Anorm supports any SQL database in theory, and the DB class already has settings for MySQL, PostgreSQL, and H2 -- so one of the TODOs is to figure out the easiest way for people who want to use something other than SQLite to get the right driver dependencies loaded.\n. What I meant above is that ideally we figure out an easy way for people to choose their database type and then have Zipkin pull in the required dependency for that database. So you would say you wanted to use SQLite or MySQL instead of Cassandra or whatever, and then you'd load the SQLite or MySQL dependency instead of the Cassandra dependencies. But I haven't looked into how to do that yet or even if there's a straightforward way to do it easily. In the mean time, I'm just including SQLite directly so there is something to test with.\n. Not a commit-blocker, but for style points, the typical JS pattern is like this:\nfunction Node(name) {\n    this.name = name;\n    /* ... */\n}\n// Instantiate a new node with new Node(name);\n. Seems like you could do this in the map() above instead of iterating through the nodes again\n. If style properties have to be defined literally in code, I like to make them properties of an object, e.g. var STYLE = { topMargin: 20, /* ... */ }; just to make it clearer when they're used that they are set, not computed.\n. Nitpick: camelCase for consistency\n. Nitpick: camelCase for consistency\n. Is there some place magic names like \"blockTarget\" are defined, and can we set a variable for them so they can be easily identified and changed?\n. This looks like the same code that gets executed for parents, except for some alignment variables -- can we abstract it to a function?\n. Is there nowhere else a node would ever need to be drawn? If you make node into a class as I suggested above, I would consider making drawNode() into Node#draw().\n. If you're creating a bunch of Moments, all the extra objects that are created briefly just for the purpose of creating a new Moment and then thrown away afterwards can create significant GC pause. Two alternatives are to pass in the moments as parameters directly (e.g. new Moment(m0, m1, m2, m3, m4)) or to assign the object to this (this = m).\n. Can different RadialDependencies instances have different diameters? If not, I would make this a static property (i.e. RadialDependencies.diameter = 1024;)\n. span_name is in the annotation to make getTraceIdsByName() and getSpanNames() in AnormIndex easy.\nRight now we use Annotation#serviceName() to get the service name that is stored in the database. That returns \"Unknown service name\" if there is no service name.\n. Separate FuturePools keep the caller from having to know anything about the implementation; we wouldn't have to put scary thread-related code in the configuration. On the other hand, reusing a single FuturePool is a little more efficient with cached threadpools because threads can be reused for both indexing and storage. (The efficiency difference is ambiguous with fixed thread pools because fewer threads get created total at the expense of less parallel computation.) In principle, we could get the benefit of both approaches by using an optional parameter that spawns a new FuturePool if no existing pool was passed, and not using it by default (so that people who know what they're doing could use the more efficient approach and people who don't wouldn't be intimidated by seeing threads in their config). Thoughts?\nI also just realized that cached thread pools are self-closing (or at least, the threads are) and fixed thread pools don't seem to be. The code right now doesn't close the thread pools. I think it should be okay to close them when the database connection is closed since after that the threads wouldn't be able to do anything interesting anyway, but I'm not really sure what is supposed to happen if e.g. AnormStorage is closed synchronously while writes are occurring in a separate thread...\n. ",
    "xbeta": "I guess the maven repo is down and move to somewhere else according to MK's comment here, anyone knows where the new location is?\nhttps://groups.google.com/forum/#!topic/travis-ci/zqneGcTEv4g\n\nIt has been shut down. Since travis-ci.org infrastructure is now U.S.-based, a separate\n mirror is no longer necessary.\n -- \n MK\n. @bmdhacks  Thanks! It does fail because it couldn't access to download any pom files, thus can't get anything build.\n. @bmdhacks \nThanks!\n. \n",
    "yuanke": "fixed?\n. ",
    "Jeffliu": "I met the problem too.  But my Zipkin project worked a few days ago. \nUpdate: I finally make it work by removing the jars downloaded in the latest 2 days:\nfind ~/.ivy2/cache/com.twitter -type f -mtime -2 -exec rm -rf {} \\;\n. ",
    "dirkweinhardt": "Substituting -D env=production for -D local_docroot=zipkin-web/src/main/resources did the trick for me:\njava -cp libs -jar zipkin-web-1.1.0.jar -f config/web-dev.scala -D env=production\nhttps://groups.google.com/forum/#!msg/zipkin-user/oqgXXa_HA0k/oszvXx_EtgYJ\n. ",
    "wadey": "I ran into the same issue while testing zipkin (1.1.0 and master) with the sqlite backend. Changing to redis fixed it.\n. I still think this is a good idea, but I need to re-do it in a way that makes sense for the new UI. I'll close this pull request and open a new one once I have done that (we are still running a very old version of Zipkin internally and we need to update to latest).\n. ",
    "sprsquish": "Any update on this? :)\n. Oop. No. That's a left over until Finatra is dual published.\n. Pulled Finatra entirely as a dependency. Zipkin Web thus takes control of its own destiny regarding Scala, Finagle, Mustache, and Jackson versions.\nFinatra is still basically a side project, it doesn't have dedicated support. We (temporarily) lose some flexibility at development time due to resource caching, but that can be fixed. Endpoints are now simple Finagle Services with composed with Filters. Multiplexing is done with HttpMuxer. There might be a better way to organize the endpoints. I'll keep experimenting.\n. Last call for comments before I merge this.\n. Redis and Cassie are behind the times, by quite a bit. They don't implement Closable, yet. And updating them isn't a yak I want to shave just yet.\n. took this a different direction\n. I played with this a little. I like the idea, but think it needs a bit more. Having the extra line only gives a sense of what's happening but no real data is provided. \nWhat about @bmdhacks' suggestion of a tooltip? Add a little popover that provides the timing breakdowns?\n. As long as we paint it blue\n. Final iteration of this.\n. Unit tests and I spun up the example on my system to check that the ZK announcer was working properly. More is coming, but I wanted to get this in front of eyes.\n. It's not in here just yet. It's a complicated enough piece of code that I think it deserves a separate review anyway.\n. lgtm. use the script in bin to pull submit\n. Hrm. Can you merge master? You'll probably have to fix the Project file.\n. It's still possible. ZK is only required for certain uses.\n. Regarding the rate calculate, I'm trying to re-write this in a way that's pretty much a drop in replacement for what already exists. I plan on revisiting later. When we move to the new storage backend, it's likely we wont need to sample anyway.\n. Last call for comments. I'd like to get this in then get the query service in. That will complete the core components.\n. Thanks! Pulled this internally. We're changing our patch/release process. So the fix will show up the next time we do a push.\n. pulled internally. thanks!\n. fixed: e1a0b5b\n. pulled internally. will show up here soon.\nThanks!\n. I believe this is now fixed in the latest version of the UI.\n. I've pulled this internally. Should show up here soon.\nThanks!\n. This looks good. I got backed up with other stuff today, but will pull it internally tomorrow. Thanks!\n. @fr0stbyte please merge master.\nThe build file changed recently and hadn't been pushed to GitHub in a while. When I tried to pull this PR in it failed due to a conflict.\n. Thanks! Pulled internally. I have a few tweaks to make then it'll get pushed back here.\n. Closed by: 6107d9b\nThank you!\n. Thanks for this @fr0stbyte. Can you make the change against master though? I made some edits on the original PR mostly to match the style of the rest of the project.\n. 1. It's never used as a trait. The process doesn't take a KafkaDecoder it takes a Decoder[Option[List[Span]]] (I think that's the right approach, btw). Having it as a trait doesn't really mean anything. It's used as a type alias, so I made that explicit. \n2. Flag already provides a map type: https://github.com/twitter/util/blob/master/util-app/src/main/scala/com/twitter/app/Flag.scala#L86\n. Types have to be defined inside something. We could put it in a package object, but there's little different between doing that and leaving it where it is.\n. @fr0stbyte hey. sorry. I've been pretty buried. I don't think I'm going to have the change to get to this until next week.\n. pulled internally will show up here soon.\n. Awesome! Thanks for this. Pulled internally. Will show up here soon.\n. When I try to run the tests for this bin/sbt zipkin-hbase/test I get the following:\n[error] /Users/jeff/workspace/zipkin/oss/zipkin-hbase/src/test/scala/com/twitter/zipkin/storage/hbase/utils/HBaseSpecification.scala:4: HBaseTestingUtility is not a member of org.apache.hadoop.hbase\n[error] import org.apache.hadoop.hbase.HBaseTestingUtility\n[error]        ^\n[error] /Users/jeff/workspace/zipkin/oss/zipkin-hbase/src/test/scala/com/twitter/zipkin/storage/hbase/utils/HBaseSpecification.scala:34: not found: type HBaseTestingUtility\n[error]   lazy val sharedUtil = new HBaseTestingUtility()\n[error]                             ^\n[error] /Users/jeff/workspace/zipkin/oss/zipkin-hbase/src/test/scala/com/twitter/zipkin/storage/hbase/utils/HBaseSpecification.scala:11: not found: type HBaseTestingUtility\n[error]   lazy val _util: HBaseTestingUtility = HBaseSpecification.sharedUtil\n. pulled internally. tests pass. thanks!\n. @Oscil8 TravisCI hasn't been working reliably for some time and the gem tests don't get run anyway.\nI've been completely swamped the last couple weeks and will continue to be for the next couple. I'll take a closer look and pull this in when I can catch my breath. Thanks for the contribution! \n. @Oscil8 Please rebase this against master. I tried pulling it in after committing your other change and it failed on me.\n. Thanks! Merged.\n. @synk why'd you close this?\nI like the idea and wanted to pull it in eventually and make some tweaks to it.\n. Realtime is an alpha feature and requires some extra setup. It's not quite ready for use externally.\n. @synk is it possible to switch to AnormSpanStore? I'm starting to deprecate the split store/index classes.\n. Pulled internally. Thanks!\n. Just to check this off the list, have you done a clean compile?\n. Just to check this off the list, have you done a clean compile?\n. Thanks for this. I took one pass at it and will take another pass tomorrow. \nThere isn't a written style guide, but the style here doesn't match the rest of the project very well.\n. How it's structured. It's not a show stopper. I'll probably end up cleaning it up. It's on me to have a style guide in place and not simply expect people to know what I'm looking for :)\n. @hunglin I'll restart it. Travis is notoriously flaky for Zipkin so I don't put too much stock in it. It may be a few days before I'll have a chance to re-review.\n. @hunglin good catch. I'll see if I can get Travis to use our script instead.\n. Might be better altogether to switch to sbt-onejar: https://github.com/sbt/sbt-onejar\nIt wont create an uber jar of all classes, rather package the project into a single jar along with its libraries thus eliminating the conflicts.\n. @eirslett I just pushed a bunch of changes that bring the OSS version of Zipkin in line with our internal version. On top of which, I've updated our sync tools so we'll start to schedule our syncs with the other Twitter libraries (once a week). \nThe TravisCI integration has also been fixed and should now be trustworthy.\nThe biggest change is probably that we've moved to scala 2.10.4 and we're more closely tracking the core Twitter libraries.\nPlease merge master and we'll take a look and get this merged in.\nThanks again for all your work.\n. @eirslett I just pushed a bunch of changes that bring the OSS version of Zipkin in line with our internal version. On top of which, I've updated our sync tools so we'll start to schedule our syncs with the other Twitter libraries (once a week). \nThe TravisCI integration has also been fixed and should now be trustworthy.\nThe biggest change is probably that we've moved to scala 2.10.4 and we're more closely tracking the core Twitter libraries.\nPlease merge master and we'll take a look and get this merged in.\nThanks again for all your work.\n. @eirslett looks like you got it fixed! I'll pull this internally and we'll have a look.\n. @eirslett looks like you got it fixed! I'll pull this internally and we'll have a look.\n. Closed via e217c056\nThank you!\n. Closed via e217c056\nThank you!\n. Pulled internally. Will show up in the next sync.\n. Pulled internally. Will show up in the next sync.\n. Thanks for catching! Fixed.\nThe docs themselves will be part of the repo with the next sync. That should happen soon.\n. Thanks for catching! Fixed.\nThe docs themselves will be part of the repo with the next sync. That should happen soon.\n. Thanks @jamescway. Pulled internally. Will show up here soon.\n. Thanks @jamescway. Pulled internally. Will show up here soon.\n. @caniszczyk Any idea why this is failing for openjdk 6 and 7? \n. @caniszczyk Any idea why this is failing for openjdk 6 and 7? \n. Sweet!\nStyle issue: we use two spaces for indentation.\nIs there a way to populate some example data so someone could see an example of this in action?\n. Sweet!\nStyle issue: we use two spaces for indentation.\nIs there a way to populate some example data so someone could see an example of this in action?\n. Great. Anyway to get this into the zipkin-example project? It's already possible to boot it up with the right flags set to populate it with example data. This could piggyback on that.\n. Great. Anyway to get this into the zipkin-example project? It's already possible to boot it up with the right flags set to populate it with example data. This could piggyback on that.\n. Hrm. Eventually we'll need some sort of API for this. For now, maybe just put the data in zipkin-tracegen along with some documentation on what to do with it.\n. Hrm. Eventually we'll need some sort of API for this. For now, maybe just put the data in zipkin-tracegen along with some documentation on what to do with it.\n. Okay. Pulled internally.\n. Okay. Pulled internally.\n. @eirslett SpanStore was me trying to update the core interfaces. I knew many OSS users would still be on the old ones, so I was going to deprecate and remove them over time.\n. Probably. I upgraded mustache.java.\n. I could have render return Response, but in all but one case it's going to side-effect what it gets and pass it back. Filling in the response is inherently a side-effecting operation. \nIn fact, I think I want to take this in a slightly different direction and add an apply method to Renderers. The filter would provide the response object to the renderer which would fill it in.\n. done\n. removed\n. removed.\nupgraded Ostrich and Cassie\n. ah.. didn't see that.\n. They're private to the Mapper trait. \n. updated to CassieSpanStoreFactory vs CassieSpanStore\n. Done\n. done\n. cool. done\n. noted\n. I'm going to push back on this. I think it makes it prettier, but not more readable. \n. We are. That's DefaultTracer\n. Are stats for this actually useful?\n. This should be implemented in ScalaTest\n. Yea. After I get all these things setup I'm going to write an example service that will allow you to exec say \"./bin/run\" and have a single instance started up with the receiver/collector and web listening for scribe and/or http and storing to sqlite.\n. fixed\n. Look to line 68. :)\n. You're right. It's possible, but very unlikely.\n. com.twitter.common.zookeeper has a mechanism for running leadership elections with groups. If this particular instance has been elected leader it will allow Some to passthrough, otherwise it'll transform it to a None.\n. it's the incoming option.\n. from the current implementation\n. Yea, it's thin enough that I don't care here.\n. Unfortunately SimpleFilter is an abstract class and not a type alias. Thus any filter would have to be a SimpleFilter. I don't want to make that restriction.\n. Yep. I'll clarify in the comments\n. fixed\n. Hold over from old code. s/URI/path/\n. I don't think so?\n. added\n. I don't think so. What would you suggest?\n. the watch isn't awaitable\n. the watch isn't awaitable\n. This is pretty common for our services. A total count at the root then a count per X (in this case service name) under it.\n. perhaps we could use c.t.u.RingBuffer but they don't have the same semantics. this one is per ordered the way we need it to be and provides a thread safe interface\n. pre-existing code\n. This is a re-implementation of existing code. I don't yet know why the original authors chose this method.\n. also taken from the current implementation. \n. eh. I just really hate that style. \n. That'll happen in a separate commit\n. nope, that's going away too\n. Yea.. Initially I thought about just asking for a base path. I'm going to switch to a base path flag and pass the rest of the paths in to newAdaptiveSamplerFilter while providing defaults based on basePath. If someone really needs to use different paths they can do it when they create the filter.\n. I'll re-implement it in terms of the c.t.u.RingBuffer\n. observe has been deprecated in favor of changes. Though, if it weren't, I still wouldn't be able to use it. Outside of c.t.u.Var.scala it's pretty much impossible to create my own kind of Var and I need to be able to hook the close method to unset the watches.\n. done\n. No. It was brought in because a version of it existed in the old code. Seems reasonable that someone might want to provide a sampler that can be updated via the admin interface.\n. Iterators are Seqs too and I wanted to force evaluation.\n. oop. oversite.\n. how do you mean?\n. You mean so you could update the value via the HTTP interface and that would update the value in ZooKeeper?\n. I am. This wraps util-zk.\n. toSeq on an Iterator is lazy by default.\n. I think it's intuitive. You can write the same thing you'd write in the code: 1.hour, 30.seconds, 100.milliseconds, etc.\n. sorted\n. case Return(s) =>\nthat way you wont need to apply it every time.\nIn fact, sorry to bump you back and forth, but this is basically a try/catch block. There's no need for c.t.u.Try\nAlso, no need for the braces here or at the throw\n. Import java.util.Arrays\nArrays.asList\n. You shouldn't need to cast here, were you getting compile errors?\n. type param isn't necessary\n. For consistency please use FunSuite: https://github.com/twitter/zipkin/commit/34998bf2376da8a64747819cd083530b39670618#diff-cc6e36fb0d77281c22b57e2acc535f0bR26\n. Are stats exposed anywhere?\n. indentation nit\n. It's wrong in the current implementation. The current implementation uses it as the start of the slice [1][2]. I intend to fix it after moving to the new CassieSpanStore.\n1) https://github.com/twitter/zipkin/blob/master/zipkin-cassandra/src/main/scala/com/twitter/zipkin/storage/cassandra/CassandraIndex.scala#L147\n2) https://github.com/twitter/cassie/blob/master/cassie-core/src/main/scala/com/twitter/cassie/ColumnFamily.scala#L148\n. @franklinhu am I missing something somewhere though?\n. It's possible for a span to not have any annotations in which case, this would blow up. FlatMap over traceIds and have the body return an Option[TraceIdDuration] based on the existence of annotations.\n. why not roll this into the other test?\n. This could be put in the if body (with an else clause)\n. prefix the string with two spaces (like the \"  pass\" println on line 73).\n. space between \"if\" and \"(\"\n. fixed\n. copy/paste holdover. removed\n. copy/paste holdover. removed\n. Is it not possible to build the as a 2.9.2 project with a few 2.9.3 dependencies? I ask because it seems like most of our deps are 2.9.2 while only the storm deps are 2.9.3.\n. style: spaces before and after each brace.\n. that only works if the value is nil or false\n. As discussed offline, we're working on a better version.\n. yes! :)\n. I think it's clear this way. Not everything needs to be a for comprehension.\n. fixed\n. It's not released as a library yet. So we don't follow a versioning system very closely. However, it will.\n. style: space between the trait and the brace\n. extends com.twitter.util.Closable\n. Package level scoping of visibility is the default. There's no need to put \"[mongodb]\" after \"private\"\n. buffer.duplicate() is more idiomatic\n. You can use the already existing list: https://github.com/twitter/zipkin/blob/82594bb081a829ad43c55962b73fdab977d70437/zipkin-common/src/main/scala/com/twitter/zipkin/Constants.scala#L32\n. Revert this. zipkin-example should continue to use AnormDB as its default.\n. Use the one from SpanDecoder \n. Indent by two spaces, not four\n. ",
    "nekto0n": "@bmdhacks yes, I understand that :)\nAny suggestions/ideas/anything how can this be patched or where to look for help/advice?\n. @bmdhacks yes, I understand that :)\nAny suggestions/ideas/anything how can this be patched or where to look for help/advice?\n. ",
    "fedor57": "Hi, I have some idea, can you judge it?\n- I assume that IPv4 addresses are just being stored in Zipkin DB without any interpretation, indexing or using as keys, right? \n- make sure instrumentation does not break while working on pure IPv6 hosts. Just to store 0.0.0.0 as IPv4 in this case\n- add extra default annotation like this \"ipv6\"=\"1a02:3b8::402:7a31:c2ff:fef2:b61d\" for span at server side\n- - tweak Web UI to do not show 0.0.0.0 as IP\nAnother useful step:\n- * add two annotations like \"remotehost\" = \"myservice.mydomain.com\" and \"localhost\" = \"myservice-345-23-node.mydomain.com\" to see the names outside and inside SLB.\nWhat do you think? Will it significantly increase the amount of data? Do you have any proposal of how to do it more naturally?\n. Hi, I have some idea, can you judge it?\n- I assume that IPv4 addresses are just being stored in Zipkin DB without any interpretation, indexing or using as keys, right? \n- make sure instrumentation does not break while working on pure IPv6 hosts. Just to store 0.0.0.0 as IPv4 in this case\n- add extra default annotation like this \"ipv6\"=\"1a02:3b8::402:7a31:c2ff:fef2:b61d\" for span at server side\n- - tweak Web UI to do not show 0.0.0.0 as IP\nAnother useful step:\n- * add two annotations like \"remotehost\" = \"myservice.mydomain.com\" and \"localhost\" = \"myservice-345-23-node.mydomain.com\" to see the names outside and inside SLB.\nWhat do you think? Will it significantly increase the amount of data? Do you have any proposal of how to do it more naturally?\n. ",
    "mattbenjamin": "Is anyone working on this?  My preference would be either:\na) not constrain endpoint except to be appropriately unique\nb) make endpoint self-describing--just supporting ipv6 just kicks the can down the road, and not that far, for me\nMatt\n. Is anyone working on this?  My preference would be either:\na) not constrain endpoint except to be appropriately unique\nb) make endpoint self-describing--just supporting ipv6 just kicks the can down the road, and not that far, for me\nMatt\n. ",
    "abesto": "Agree we need to support ipv6 somehow. How about the na\u00efve approach, just adding a new field \"ipv6\" to the annotations? The only problem I see is enforcing that at least one of ipv[4,6] is set. Or we could just say that we don't really care whether it's an ipv4 or an ipv6 address, anyone who cares can decide about an address trivially which one it is.\nSo, idea: let's introduce a new field \"ip\" or \"ipAddress\" in the relevant annotations, and deprecate \"ipv4\". The new field can be either an ipv4 or an ipv6 address.\n. How about this: since Zipkin itself doesn't semantically use the IP, just displays it (is this true?), we can create a permissive data structure - separate ipv[4,6] fields, and let users decide what they want to put there. Let them come up with their own convention, and make this explicit, maybe providing best practices.\n. You convinced me Endpoint.ipv6 is the way to go (with binary in thrift, string in json).\n. After a lengthy debugging session I realized this doesn't work for zipkin-web as the application fails to load the required resources. The package-dist method seems to work.\n. Here's the output of ./bin/sbt --debug assembly 2>/dev/null | ag Merging | ag META-INF:\n[warn] Merging 'META-INF/MANIFEST.MF' with strategy 'discard'\n[warn] Merging 'META-INF/DEPENDENCIES' with strategy 'discard'\n[warn] Merging 'META-INF/MANIFEST.MF' with strategy 'discard'\n[warn] Merging 'META-INF/DEPENDENCIES' with strategy 'discard'\n[warn] Merging 'META-INF/services/com.twitter.finagle.Announcer' with strategy 'filterDistinctLines'\n[warn] Merging 'META-INF/services/com.twitter.finagle.Resolver' with strategy 'filterDistinctLines'\n[warn] Merging 'META-INF/services/javax.ws.rs.ext.MessageBodyWriter' with strategy 'first'\n[warn] Merging 'META-INF/MANIFEST.MF' with strategy 'discard'\n[warn] Merging 'META-INF/DEPENDENCIES' with strategy 'discard'\n[warn] Merging 'META-INF/INDEX.LIST' with strategy 'discard'\n[warn] Merging 'META-INF/services/javax.ws.rs.ext.MessageBodyReader' with strategy 'first'\n[warn] Merging 'META-INF/MANIFEST.MF' with strategy 'discard'\n[warn] Merging 'META-INF/DEPENDENCIES' with strategy 'discard'\n[warn] Merging 'META-INF/services/com.twitter.finagle.Announcer' with strategy 'filterDistinctLines'\n[warn] Merging 'META-INF/services/com.twitter.finagle.Resolver' with strategy 'filterDistinctLines'\n[warn] Merging 'META-INF/services/javax.ws.rs.ext.MessageBodyWriter' with strategy 'first'\n[warn] Merging 'META-INF/MANIFEST.MF' with strategy 'discard'\n[warn] Merging 'META-INF/DEPENDENCIES' with strategy 'discard'\n[warn] Merging 'META-INF/services/com.twitter.finagle.Announcer' with strategy 'first'\n[warn] Merging 'META-INF/INDEX.LIST' with strategy 'discard'\n[warn] Merging 'META-INF/services/com.twitter.finagle.Resolver' with strategy 'first'\n[warn] Merging 'META-INF/services/javax.ws.rs.ext.MessageBodyReader' with strategy 'first'\nI don't see anything that may be related to resources, but\n- I may have messed up and didn't find all merges, I don't use sbt daily\n- an interesting line may be in there, I just don't recognize it\nDoes anything stand out to you?\nMy gut feeling is that https://github.com/twitter/zipkin/blob/master/zipkin-web/src/main/scala/com/twitter/zipkin/web/Main.scala#L54 and https://github.com/twitter/zipkin/blob/master/zipkin-web/src/main/scala/com/twitter/zipkin/web/Handlers.scala#L191-L200 don't agree with assembly on where the resources should be. I tried passing in -zipkin.web.resourcesRoot='/' (since the resources are in the root of the fat-jar), which did nothing, so either I'm on the wrong track, or I messed up the fix.\n. sbt-assembly already has special handling for stuff under META-INF: https://github.com/sbt/sbt-assembly/blob/0.9.2/src/main/scala/sbtassembly/Plugin.scala#L347-L360\nI tried to change from discard to concat on manifest.mf, but then the build fails with java.util.zip.ZipException: duplicate entry: META-INF/MANIFEST.MF (which is pretty strange btw).\nI feel this is starting to take more effort than what a fat jar is worth; the package-dist method is safe and stable. I'm completely open to trying out anything you guys come up with, but I don't plan to investigate this further.\n. sbt-assembly already has special handling for stuff under META-INF: https://github.com/sbt/sbt-assembly/blob/0.9.2/src/main/scala/sbtassembly/Plugin.scala#L347-L360\nI tried to change from discard to concat on manifest.mf, but then the build fails with java.util.zip.ZipException: duplicate entry: META-INF/MANIFEST.MF (which is pretty strange btw).\nI feel this is starting to take more effort than what a fat jar is worth; the package-dist method is safe and stable. I'm completely open to trying out anything you guys come up with, but I don't plan to investigate this further.\n. I messed up and created the PR from master. Closing to avoid confusion.\n. I messed up and created the PR from master. Closing to avoid confusion.\n. I don't know, and have no easy way of reproducing. I'm +1 for closing (too much effort to even reproduce, didn't seem like anyone else has hit this)\n. Can this be closed?\n. Disregarded :)\nOn a more serious note: I have very limited actual experience with the long-term maintenance of multi-project builds. Do you foresee more problems with the many-build.sbts approach, or is it just that it's extra effort to initially get it to work?\n. @adriancole nope, #478 continues the work in this, but does very different things and has very different scope, so I thought a separate PR was in order.\n. Thanks! How about having these three commits?\n. @eirslett Sure. Using a small snippet in the top of the top-level build.gradle (HAH!), this is what I get when processed through dot (from Graphviz):\n\nSnippet used to generate the dotfile: https://gist.github.com/abesto/cdcdd38263eacf1cbb51\nThe dotfile, if you want to play around with the visualization: https://gist.github.com/abesto/949730fb42a7b974a630\n. :+1: \nRemoved code, cleaner resolvers, jcenter, me likes. I don't see any issues.\n. Tests run OK locally, didn't find even nitpicks in the diff. I'm +1 for merge.\nStill, I'd prefer getting another set of eyes on this (the brains behind which would ideally have a closer familiarity with the tests).\n. LGTM :rainbow: +1\n. How is zipkin-kafka related to zipkin-receiver-kafka (if at all)?\n. Cool, cool. I'm all for removing dead code.\n. OTOH Bintray publishes an official Gradle plugin: https://bintray.com/jfrog/jfrog-jars/gradle-bintray-plugin\n. Tried maiflai/gradle-scalatest before going with the JUnit integration thing, couldn't get it to work :(\n. Tests are now run using maiflai/gradle-scalatest, and some manual tests suggest the whole thing working (see details in description).\n. Thanks a lot, good points! I'll address them in detail tomorrow.\n. I suggest we break out the anormdb thing to a separate issue. Users shouldn't have to recompile to switch the storage they use, but I'd like to keep the scope of this PR minimal.\nOn tabs: oh my, I just let IntelliJ do whatever it wanted. Spaces incoming.\n. @adriancole plaese check the comments added in https://github.com/openzipkin/zipkin/commit/e3619a765069c29b15bc2e3f75c906f4f12345fb. I hope that explains what's going on.\n. Yep, I'll take care of just this last TODO, then squash.\n. Thanks a lot for writing this up, and also the awesome graph :) Definitely +1\n. Builds fine locally, looks good to me. :shipit: \n. The implementation looks fine to me. Maybe add some context on why this is happening?\n. :shipit: :rainbow: \n. So green! :shipit:\n. Other than the typo, :+1: \n. Cool! :+1: \n. :shipit: \n. Tried the whole collector-query-web-tracegen setup locally, works like a charm. Great stuff! :shipit: \n. I'll have a look tonight (~9 hours from now)\n. #536 fixes it for me, please have a look.\n. Closing as fixed, feel free to reopen / open another issue if things are still borken.\n. ZOMG\n. Could you elaborate on what the issue \"building from oraclejdk7\" means?\n. Don't see any blocking issues. I say :shipit:, refinements will be simpler anyway once the whole pipeline is set up.\n. This is because of https://github.com/openzipkin/zipkin/blob/master/zipkin-query-service/build.gradle#L26-L28 and friends. It's just a word-for-word translation of the relevant part of the old SBT build. I don't see the way forward at the moment, but I feel it shouldn't be too hard to fix.\n. Two notes, neither one blocking. Other than those, LGTM :+1: \nVery excited, once this is merged, we might have a release on Maven Central :)\n. He said merge. Did you see him say merge? :)\n. These overrides feel like they'd be better as configurations options inside the release plugin. I may (or may not) get around to creating a PR for that to the plugin itself. As it is, we are on the border of abusing internals of the plugin instead of using the stable API.\n. That's right, thanks. Updated the description.\n. Short answer: yes I think, we still build all the zips and tarballs (though I'm not 100% confident, we'll need to see). Added some context to the description.\n. :ship: :it:\n. And I guess the non-dirty way would be to have a separate bower config that installs stuff to a separate directory.\n. Oh, didn't know there's deep bower integration. In that case it's probably not worth playing around with multiple bower configs, especially as this is not a situation we want to maintain in the long term (I guess).\n. Totally. Licensing is something I'm recently very interested in; if no-one picks it up, I'll have a look this weekend.\n. Sure thing. I'll have time on Saturday in the worst case.\n. The issue was introduced in 2412c4b8, Groovy doesn't interpolate on ', only on \". PR coming up.\n. This looks awesome, I'm trying it out now. First note: collector and query fail to build due to An exception has occurred in the compiler (1.8.0_31) in :zipkin-cassandra-core:compileJava. I suspect it's unrelated, gonna look at just the web without data for now and debug that later.\n. Static assets are served correctly for me locally both with gradle run and from the shadowjar. @adriancole which assets fail to load for you?\n. Huh. Nice find.\n. Will take a look tonight (~4 hours from now)\n. Learning nr. 1: zipkin-web/src/main/resources/dist is not removed by zipkin-web:clean. It's also ignored by git clean, because .gitignore. Now that I've removed it manually, I can reproduce the issue locally.\n. Learning nr. 1: zipkin-web/src/main/resources/dist is not removed by zipkin-web:clean. It's also ignored by git clean, because .gitignore. Now that I've removed it manually, I can reproduce the issue locally.\n. Slightly confused here. Both Annotation and BinaryAnnotation have an optional Endpoint field (going by https://github.com/openzipkin/zipkin/blob/master/zipkin-thrift/src/main/thrift/com/twitter/zipkin/zipkinCore.thrift). Why not add that as a column to the UI? \n. Got it. Thanks for the explanation; with those in mind, it does sound like the effort outweighs the gain for now.\n. I love the idea of making the UI customizable.\nAnother way would be bringing in features like this in a configurable way, linking to the discussion of the trade-offs involved. This approach could nicely support bringing in features from forks: keep the default experience consistent and always correct, but let users choose their trade-offs. Of course that adds overhead in maintenance, or rather instability; we'll never ever test all 600 combinations of feature flags. Initially I'd solve that by adding cautionary comments and documentation. Over time we could start supplying well-tested configuration presets.\nAll in all, I'm not sure it's a good trade-off, but it's a nice idea to play with.\n. Sure, sounds fun. The next few days look kinda chaotic, no promises on the timeline.\n. I'm +1 for the idea, I love self-describing APIs. Not sure about how / what other, existing apps would want to import from this? Would those other apps also serve the same API description? Or is that about distributing the machine-readable API spec?\n. @eirslett @adriancole \n. Yes, but consider: if package.json didn't change, but i manually removed node_modules, then you'd expect gradle to run npm install. Same if I just removed one package manually. AFAICT the plugin handles this by considering the whole node_modules dir for diffs, as evidenced by https://github.com/srs/gradle-node-plugin/issues/94; I don't have a good idea for improving things.\n. Or even @openzipkin/cassandra \n. Be advised, after this is merged, I'll also delete the gh-pages branch (it was built from content deleted in this PR)\n. Ouch. I should've noticed that, sorry. Could hot-link, but that'll break in the far future and be even harder to notice. I'll pull that image back.\n. Ouch. I should've noticed that, sorry. Could hot-link, but that'll break in the far future and be even harder to notice. I'll pull that image back.\n. There are a lot of unknowns in this for me (that I'd be OK exploring). It looks like https://www.npmjs.com/package/bintray can be used to publish programmatically from NPM / Grunt / whatever to Bintray. The question then is what structure the uploaded package should have. I have practically zero experience on that front, so I'll be asking for help on that front.\n. There are a lot of unknowns in this for me (that I'd be OK exploring). It looks like https://www.npmjs.com/package/bintray can be used to publish programmatically from NPM / Grunt / whatever to Bintray. The question then is what structure the uploaded package should have. I have practically zero experience on that front, so I'll be asking for help on that front.\n. Looks reasonable at a glance, I'll do a proper review tonight. Thanks!\n. Looks reasonable at a glance, I'll do a proper review tonight. Thanks!\n. Other than the note on the dependency chain, this looks pretty good to me. I'm taking a quick look to see if there's an easy way to make web load assets from ui. Will have more time tomorrow, and then there's a long weekend coming, you know what that means :)\nOn the package format, I don't know what would be the relevant best practice in this case, or even which stack to draw it from. Maybe a war file, or maybe just a zip to keep it simple, but then a jar is just a glorified zip so... Yeah.\n. Other than the note on the dependency chain, this looks pretty good to me. I'm taking a quick look to see if there's an easy way to make web load assets from ui. Will have more time tomorrow, and then there's a long weekend coming, you know what that means :)\nOn the package format, I don't know what would be the relevant best practice in this case, or even which stack to draw it from. Maybe a war file, or maybe just a zip to keep it simple, but then a jar is just a glorified zip so... Yeah.\n. Result of the short experiment: just adding a compile project(':zipkin-ui') dependency to zipkin-web gives zipkin-web access to the resources of zipkin-ui, but there's much confusion around paths, especially since in the current setup index.html is in the same directory in the jar as all the static assets. My current best idea is to organize the jar in a way that there's clear separation between html files (especially index.html which needs special treatment in the handlers), and static assets. For example something like\nzipkin-ui.jar\n  |\n  +-- html\n  |    |\n  |    +-- *.html\n  +-- assets\n       |\n       +-- *.css\n       +-- *.js\nAnother option is to put just index.html in a special place, and let the static handler serve all other HTML files from the same source as all other static assets, adding html => text/html to the type map.\nAnother option is redesigning the way the handlers are structured, such that there's no need for special-casing index.html.\nAm I making sense? It's getting late, and this is the first time I'm messing around with Finagle.\nPS. Don't forget to remove the node, webpack, and karma stuff from zipkin-web/build.gradle\n. Result of the short experiment: just adding a compile project(':zipkin-ui') dependency to zipkin-web gives zipkin-web access to the resources of zipkin-ui, but there's much confusion around paths, especially since in the current setup index.html is in the same directory in the jar as all the static assets. My current best idea is to organize the jar in a way that there's clear separation between html files (especially index.html which needs special treatment in the handlers), and static assets. For example something like\nzipkin-ui.jar\n  |\n  +-- html\n  |    |\n  |    +-- *.html\n  +-- assets\n       |\n       +-- *.css\n       +-- *.js\nAnother option is to put just index.html in a special place, and let the static handler serve all other HTML files from the same source as all other static assets, adding html => text/html to the type map.\nAnother option is redesigning the way the handlers are structured, such that there's no need for special-casing index.html.\nAm I making sense? It's getting late, and this is the first time I'm messing around with Finagle.\nPS. Don't forget to remove the node, webpack, and karma stuff from zipkin-web/build.gradle\n. Implemented the \"redesign handler structure\" option, with extra trickery to always serve index.html when in doubt.\n. Implemented the \"redesign handler structure\" option, with extra trickery to always serve index.html when in doubt.\n. I consistently fail to check how things work with shadowJar (and am\nconsistently saddened that they don't work the same as when run from\nGradle). Will take a look tomorrow.\nOn Thu, Mar 10, 2016 at 4:06 PM Adrian Cole notifications@github.com\nwrote:\n\ngreat. I like this mission\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1024#issuecomment-194890978.\n\nZolt\u00e1n Nagy\nhttps://abesto.net\n. I consistently fail to check how things work with shadowJar (and am\nconsistently saddened that they don't work the same as when run from\nGradle). Will take a look tomorrow.\n\nOn Thu, Mar 10, 2016 at 4:06 PM Adrian Cole notifications@github.com\nwrote:\n\ngreat. I like this mission\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1024#issuecomment-194890978.\n\nZolt\u00e1n Nagy\nhttps://abesto.net\n. Awesome, thank you!\n\nOn Thu, Mar 10, 2016 at 4:06 PM Adrian Cole notifications@github.com\nwrote:\n\ngreat. I like this mission\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1024#issuecomment-194890978.\n\nZolt\u00e1n Nagy\nhttps://abesto.net\n. Awesome, thank you!\n\nOn Thu, Mar 10, 2016 at 4:06 PM Adrian Cole notifications@github.com\nwrote:\n\ngreat. I like this mission\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1024#issuecomment-194890978.\n\nZolt\u00e1n Nagy\nhttps://abesto.net\n. :shipit: \n. :shipit: \n. Can do. Would also simplify the handler logic a bit. Reasoning for the current setup is in https://github.com/openzipkin/zipkin/pull/1024#issuecomment-195077081\n\nI have minimal experience with WebJars, was pretty smooth when I used it. But if we go that way, wouldn't we want to use that for dependency management (instead of npm)? How would that work together with webpack?\n. Can do. Would also simplify the handler logic a bit. Reasoning for the current setup is in https://github.com/openzipkin/zipkin/pull/1024#issuecomment-195077081\nI have minimal experience with WebJars, was pretty smooth when I used it. But if we go that way, wouldn't we want to use that for dependency management (instead of npm)? How would that work together with webpack?\n. SGTM. @eirslett, you mentioned this is easy to do with webpack?\n. SGTM. @eirslett, you mentioned this is easy to do with webpack?\n. There seems to be a missed dependency on eslint-plugin-react.\nHere's what I'd do:\ngradle\ntask npmLint(type: NpmTask) {\n    args = ['run-script', 'lint']\n    dependsOn npmInstall\n}\ncheck.dependsOn npmLint\nEdit: this was implicit, but lemme state it explicitly: yes, I think making code quality checks part of the build process is great. I'm all for it.\n. There seems to be a missed dependency on eslint-plugin-react.\nHere's what I'd do:\ngradle\ntask npmLint(type: NpmTask) {\n    args = ['run-script', 'lint']\n    dependsOn npmInstall\n}\ncheck.dependsOn npmLint\nEdit: this was implicit, but lemme state it explicitly: yes, I think making code quality checks part of the build process is great. I'm all for it.\n. :heart: :shipit: \n. :heart: :shipit: \n. This assumes the \"vision\" is to stay with NPM for the duration. If instead we want to replace NPM and keep webpack + karma, the same argument says we need to NOT include npm in the name of the tasks, and this should not be merged.\n@eirslett?\n. This assumes the \"vision\" is to stay with NPM for the duration. If instead we want to replace NPM and keep webpack + karma, the same argument says we need to NOT include npm in the name of the tasks, and this should not be merged.\n@eirslett?\n. @adriancole @eirslett The Scala is slowly coming back to me, please check to see if I made a mess or not.\n. Side-note: I kind of feel like changing index.html to refer to /static/*.js instead of /*.js. That would make the handlers considerably simpler, and just feels nice. How would that play together with Spring etc.?\n. SGTM. Would we want to set up the same folder structure inside the jar? I imagine that'd make routing easier on users (see https://github.com/openzipkin/zipkin-java/pull/98#issuecomment-197276575)\n. Also. @adriancole @eirslett I propose we merge this to un-break master and re-think the folder structure if need be, in another PR. Go?\n. :O Why would that change? Checking...\nEdit: oh I see, not sure that filename changes are picked up by live reload. Still checking.\n. With npm run dev the site doesn't load, I get a bad gateway for config.json. Assuming it wants to proxy to zipkin-web (makes sense), so I did ./gradlew zipkin-web:run. But zipkin-web gives me a 500 with this unhelpful message in the console:\nE 0322 16:31:25.350 THREAD21: Exception propagated to the root monitor!\ncom.twitter.finagle.FailedFastException: Endpoint zipkin-web is marked down. For more details see: https://twitter.github.io/finagle/guide/FAQ.html#why-do-clients-see-com-twitter-finagle-failedfastexception-s\n    at com.twitter.finagle.NoStacktrace(Unknown Source)\nThis is on master on a clean build. Unfortunately I'm out of time to spend on this ATM; if you can quickly verify that this PR works, that's much appreciated. Otherwise I'll have more time tonight.\n. Awesome, thanks! Hope that'll be a nice wake-up present for @adriancole :)\n. Nice!\n. @klingerf Great stuff. I tried to capture short versions in the description, feel free to edit it to clarify / correct whatever I messed up.\n. Oops. I was wondering why I didn't see any new entries. Checking.\n. So! The problem is that the script finds and extracts all the javadoc jars of the right version, but it looks like we have two of some of them, for instance:\n- ./zipkin-server/target/local-repo/io/zipkin/java/zipkin-autoconfigure-ui/1.7.0/zipkin-autoconfigure-ui-1.7.0-javadoc.jar\n- ./zipkin-autoconfigure/ui/target/zipkin-autoconfigure-ui-1.7.0-javadoc.jar -d javadoc-builddir/1.7.0/zipkin-autoconfigure-ui\nMore importantly: is it safe to just ignore all but the first jar per module?\n. CC @adriancole @shakuzen @marcingrzejszczak @eirslett \n. I like this approach. Will separate out publishing into a separate PR, and squash this into two commits (one the CircleCI setup, the other changes to Zipkin code for the Circle env).\nSide question: the flakes that we had on Circle, are those things our users would experience in their (non-test) environments?\n. Disclaimer: I've never used Cassandra, won't use Cassandra in the near future, so take this with appropriate grain of salt.\nAs someone who identifies as a sysop-when-needed, using an unreleased version of a database makes me very uncomfortable. I definitely wouldn't roll that out into my infrastructure. Putting on my Zipkin hat, I'm +1 on imagining it'd introduce even more maintenance overhead. Not to mention, sooner or later that version is going to be released, and we can get all the extra nicenes then, in a cleaner, safer, dryer way.\n. <3 Thanks!\n. I'm wondering if any @openzipkin/devops-tooling members have Windows experience? I have only the basics - could probably set something up with https://www.appveyor.com/, but it won't be pretty. . @wdittmer Thanks for the detailed response. So translating this to my non-k8s-trained mind: annotations apply globally to the ingress definition, but this specific rule would need to apply to just one backend, so like you've already explained before, this is not possible to implement from the k8s side.\nFWIW, to me it'd be weird anyway to depend on a reverse proxy to rewrite HTML content. Since the HTML is served up in this setup by something \u201cwe\u201d (as in OpenZipkin) control, it shouldn't be hard to implement logic similar to Kibana and Prometheus in Zipkin. I believe the backend for the UI in this setup is the Zipkin backend, so we should be able to conditionally inject the HTML <base> tag based on an ENV var. @adriancole am I making any wrong assumptions here / messing up the conclusion?\n(Meta: should we move this discussion to a separate issue?). Well \ud83d\udca9 that's unexpected. Here's hoping the upstream issue gets a constructive resolution in the end.\nI guess if not, we could still use this lib and something utterly horrible on our end like exposing the collector with reflection and call cleanup ourselves at the right point in the Spring life-cycle. Deciding whether that's better, or replacing the library altogether, I'll leave to you.. Umm... Moderately well :D It's on my list of things to do, definitely on top of my Zipkin things to do. I'm hoping to get a (first?) implementation done this weekend.. Oh wow. I thought this'd be a quick Google search, adopting things a bit for the specifics of our case, open PR, all good. Nope, changing HTML is apparently not simple.\nMy first approach was extending the config.json we generate, and injecting the <base> tag from JS. The server side is clean, adding a new config value is trivial. BUT: of course the JS that'd load config.json won't know where to load the file from, because the <base> tag is not set. \ud83d\udc14 and \ud83e\udd5a , you see.\nMy second approach was to do exactly what was originally described: rewrite the HTML content. Apparently \"the way\" to do that is with ServletFilters, which... I tried to do, got something that might work, but I don't see how I'd inject the new filter from the context of the autoconfigure module. Sounds like a horrible idea anyway.\nIs the \"right\" way to just ... change https://github.com/openzipkin/zipkin/blob/master/zipkin-autoconfigure/ui/src/main/java/zipkin/autoconfigure/ui/ZipkinUiAutoConfiguration.java#L118 into a method returning ResponseEntity<String>, read the HTML file like a peasant, and do the HTML parsing / mutation, then pass a string to the ResponseEntity? (And of course cache the contents of the HTML file in memory, no need to hit the disk on each request)\n... or pull in a full-fledged template system with proper Spring integration? That sounds overkill.. I mean... wow, this is new to me. So technically zip is correct, that's not a valid zip archive :D It starts with a shell script. I'd note that unzip can extract the archive even on Alpine, but then complains about the archive not being valid. Here's one way to both fail on other errors, and let the build pass:\nzip --fix zipkin-server.jar --out zipkin-server.zip\nunzip zipkin-server.zip\nWhere zip --fix strips out the shell script from the beginning, restoring the general assumption that a jar is a zip.. My two cents: I like how explicit java -jar is. If the target audience is both Java experts and folks who want Zipkin to \"just work\", then I'd mention this as an alternative for people who know what this change does (personally, I have no idea :D ). Sounds reasonable. I don't have any counterarguments.\nOn Mar 14, 2018 09:30, \"Adrian Cole\" notifications@github.com wrote:\n\nBASEPATH will cause less config glitches I think, vs BASE_PATH, so taking\na stand to match on that. Reason is that words between the dots when\nmapping env variables can be a bit crap (and since BASEPATH is used in\nKibana... hey their fault)\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1946#issuecomment-372941712,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AADqTitNUMAQqiDNlN9gPOS2dT9qR5PAks5teNUugaJpZM4Sl2ni\n.\n. Oh fuck. Because I'm a stupid h00man, and put the link not in the href attribute, but in the html content of the tag.. And even wrote nice unit tests that verify this totally borken behavior.. Please go ahead and take care of it then. I've since switched context to work stuff.. Also much cleaner. Nice job.. Also much cleaner. Nice job.. It uhh... seems like we're done? I'll be optimistic and close this.. Read the proposal, looks pretty solid to me (added a link to the server javadocs, just for completeness' sake). Exciting!. Gotcha. Ping me once this is merged and I'll take care of the conflict accordingly.\n. Correct. I went with this because in quite a few places there's only a single package included, which would require another round of redundant concatenation / flatten.\n. Ah, that's right, I didn't think of that. Yeah, it's a little bit of magic we'd sprinkle all over the place. I'm fine with that too, don't have a strong opinion at this point.\n\nFor now I think let's leave it like this, if anyone feels strongly about changing this we can always go back and do it.\n. \"That's how I found it\" :) Good point, defaultSettings includes the other one. I'll remove Project.defaultSettings in a sec.\n. Nice, I like the consistency\n. Does this drop jcenter and typesafe/releases, or are those included by default?\n. That works too. Feel free to get started if you want, help is greatly appreciated. I'll probably get around to it this weekend otherwise.\n. Cool, thanks! Also for the heads up.\n. I agree in principle, but can't point to any specific line / part of line I'd move. Do you have specifics in mind?\n. Nice catch. Here: https://github.com/openzipkin/zipkin/commit/8d2dd007db6c9ca826a90b216d96fd7ac9ce7db8\n. I found that if project A depends on project B, and B has a dependency from resolver R, then A must also have R declared, otherwise compiling A will fail, even if it's marked as being dependent on B. I see two solutions:\n1. Current setup\n2. Put the appropriate line into the build.gradle of each project where it's needed, of course DRY-ing it by declaring the URL once in a common place.\nDid I miss something in the \"problem statement\"? Do you see another approach? Would you prefer the second solution?\nI'll definitely document whatever the end result is in a comment, this is cause for confusion.\n. Same issue as https://github.com/openzipkin/zipkin/pull/503#discussion_r35494029, let's keep the discussion there.\n. Done in https://github.com/openzipkin/zipkin/commit/5df2d8b306e1d79ee0ac21ec26a76893ff0a4fc3, thanks\n. First saw this in https://raw.githubusercontent.com/akhikhl/gradle-onejar/master/pluginScripts/gradle-onejar.plugin, looked like good practice. Is there a down-side to this being here?\n. It's just DRY; same reason for all the other helpers using mkDepGen. The map is needed because we need different versions for different artifacts from the same group.\nWhat would an alternative implementation look like?\n. That's correct. Would you prefer having the common parts factored out?\n. Maybe this should be docker-compose up?\n. Thanks for the insights. I'm convinced, removed in https://github.com/openzipkin/zipkin/commit/b4e7113e437f0c47731f28f25f3f89dcce6194f7\n. Alright, I'll switch over to inline dependency declarations. I think introducing version rewriting like you suggest will cause confusion, I'd rather keep the version numbers in gradle.properties. Would that work for you?\n. I think I addressed this in https://github.com/openzipkin/zipkin/commit/22424ac363a3cc797e2c970237b14e7d2fd8de41.\n. How about this? https://github.com/openzipkin/zipkin/commit/e3619a765069c29b15bc2e3f75c906f4f12345fb\n. Typo :)\n. <3\n. For the record (wasn't obvious for me): the README for net.researchgate.release says it'll only work in a multi-project build if the plugin is only applied to the root project, so we need to do this here instead of inside subprojects.\n. Why do we need a build here? By this time that should be done, and if not, bintrayUpload should pull it in, non?\n. TRAVIS_REPO_SLUG also contains the repo name, so this will evaluate to github.com/openzipkin/zipkin/zipkin.git\n. Is all the white-space change intentional? It's kind of hard to find the real changes :(\n. What does dd do? Had a hard time Google-ing for it :D\n. This just cleans up some mess left behind by the previous release.\n. The x is candidate for removal of course, but experience shows it'll be useful to have it there for the first few releases.\n. But we know whether the Cassandra store uses TTLs, no? So we could just have the specialized code here?\n(also in the assert at the bottom)\n. That sounds pretty bad; I'd expect it can lead to all kinds of inconsistencies because the Cassandra store is not telling the truth (\"truth\" as defined by the API of SpanStore). I don't know how to formulate this properly, but maybe it's worth opening an issue to track this?\n. Nits addressed (on different line, so the comment stays here for eternity)\n. The application plugin applies the distribution plugin, but the build scripts for subprojects run after the build scripts for the root project, so we need to resort to this callback structure.\n. That's correct, having different include statements caused problems for gradle idea. I think if we move just some modules to the bottom that are not dependencies of other projects, it should be fine.\n. Added URL to make sure we don't alias earlier Apache licenses to Apache 2\n. URL is the same, but I want the C capitalized :)\n. Addressed with dependencyConfiguration = 'compile' in downloadLicenses. Unfortunately we still have logback with LGPL and mysql-connector-java with GPL (latter one used for anorm).\n. I think the plugin works from the POM file, I'll verify that soon.\n. Hm? This dependency is not gone.\n. Same here, we still depend on logback.\n. Since ~/.npm is cached, I don't think it'll have a big impact; I expect the majority of the build time to come from downloading the packages from NPM. The only thing I can think of that can slow us down is if any compilation takes place, say if there are native extensions to build.\n. Sounds good, I've made a note of it. Thanks for the mention.\n. Here's how I'd set up the task dependencies. check is the Gradle standard \"run all the tests and whatevers\" task; build depends on it. I'm assuming the karma tests need the webpack build to have completed.\nnpmInstall ---> webpack ---> karma ---> check\n                               |\n                               + -----> processResources\nThis includes (either explicitly or transitively) all the dependencies you've specified, except for one: karma -> processResources. I'm guessing there's no actual dependency there, can you verify that? If there is, we can just move processResources to depend on karma instead of webpack (which depends on webpack, so we still get it transitively)\n. My browser says import \u20ac from 'jquery' is a syntax error :(\n. Yes, but everything else will still go into static, not static/assets:\nzipkin-ui/build/resources/main/\n\u2514\u2500\u2500 static\n    \u251c\u2500\u2500 448c34a56d699c29117adc64c43affeb.woff2\n    \u251c\u2500\u2500 614fad616d014daf5367e068505cad35.png\n    \u251c\u2500\u2500 89889688147bd7575d6327160d64e760.svg\n    \u251c\u2500\u2500 8b55a822e72b8fd5e2ee069236f2d797.png\n    \u251c\u2500\u2500 app.min.css\n    \u251c\u2500\u2500 app.min.css.map\n    \u251c\u2500\u2500 assets\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 app.min.js\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 app.min.js.map\n    \u251c\u2500\u2500 e18bbf611f2a2e43afc071aa2f4e1512.ttf\n    \u251c\u2500\u2500 f4769f9bdb7466be65088239c12046d1.eot\n    \u251c\u2500\u2500 fa2772327f55d8198301fdb8bcfc8158.woff\n    \u2514\u2500\u2500 index.html\n. Sure thing. I'll be putting down Zipkin soon for the day; I'd say let's wait for more input on https://github.com/openzipkin/zipkin-java/pull/98#issuecomment-197361662, that'll probably affect this as well.\n. Needed because otherwise we get an \"ambiguous symbol\" error for getClass\n. I meant: Why not create the package in an existing package format? Good feedback, that this is not clear.\n. Watch out, wall-of-text incoming ;)\nServing index.html is possible with any server; that's exactly what's documented in the second option. The question is more whether we want to do that. In the end, all users of zipkin-ui are free to do with the package whatever they want, what we need to decide is the use-cases we support / document.\nTo me \"serve index.html always\" feels icky; if we know something's not there, I have a slight preference to let the server already tell me so. Not doing so breaks strong conventions. I've already seen several times that it makes debugging harder (I look at the network tab of my browser, see 200s, but nothing works; I guess that one of the JS files is wrong, click through, and then see that the contents of app.min.js are the contents of index.html, and then from that infer that this is in fact a 404, and to verify that, I'll need to open the URL in a new browser tab)\nThose are my arguments for \"support this use-case\".\nArguments I see so far against \"support this use-case\": I see that maintaining index-routes.json is extra work. Based on what I've seen so far, I expect that to mean adding an extra line every few months at most. I understand that defaulting to index.html is the way many modern single-page apps are built. Dropping this use-case also does away with the extra complexity in users that'd use index-routes.json, for the price of not allowing them to implement that extra complexity, even if they want to. (I'd want to, but today I'm playing maintainer of Zipkin as opposed to user, so that's mostly off-topic)\nDid I miss any arguments on either side? If no: based on these, I have a weak preference to provide index-routes.json. I understand your evaluation is more firmly on the other side. I'd love to get more opinions.\nPS. I'm totally OK getting rid of the relevant code if we decide to not support index-routes.json, that \"wasted work\" should not be a factor here.\n. +1 for turning zipkin-web into a dumb proxy. You can consider this code a proof-of-concept to see how index-routes.json works in practice.\n. All good points. After these, and more discussion with @adriancole, I'm convinced we'll be better off dropping index-routes.json.\nI'll also simplify the README to describe only one hosting method (default to index.html) to simplify things, and keep the options in the description of this issue for history.\n. I'll just copy-paste this into the README, if you don't mind :)\n. (Will do the proxy in a separate PR)\n. Oops.\nDo we care enough to figure out how to fix it?\n. Oh, I see you did that already. Thanks!\n. Not blocker: I'd love to see a one (two?) liner to generate traces into Zipkin with Spigo here to replace tracegen.\n. AFAICT on Travis, this runs on the commit with the non-SNAPSHOT version. But here by the time we get to this point, we're on version (release + 0.0.1)-SNAPSHOT. I suspect we'll need to expose a sync.version property so we can pass in the version to sync. @adriancole WDYT?\n. Sure thing, will check tonight\n. Nothing stands out. More, if there was a syntax  / bash problem, I'd expect to see some relevant error message. Methinks this is the OOM.\n. Sorry, I don't speak fluent Maven. By unstable dep, do you mean we shouldn't pin the version of io.prometheus:simpleclient? Or that I shouldn't remove org.springframework:spring-test?. Ah, I see.\nYep, will set up a property for the version number.. \ud83d\ude28 Sad response time is sad. Yep, I'll check it out today.. Confirmed, Prometheus can successfully scrape data with 0.0.26. Pushed a commit; I'll start updating the Grafana dashboard for the new metric names, upload once this is merged, then we can hopefully also merge https://github.com/openzipkin/docker-zipkin/pull/135. In terms of Java best practices, yes, for sure. In terms of failing gracefully, I'm not sure what the expected behavior is for Spring applications: if we fail to parse the HTML, that most likely means the HTML is invalid. Should we then:\n\nBlow up, fail early and fail hard?\nServe the HTML anyway, without modification (which we know will fail behind a reverse proxy with the base-path config set?)\nServe the HTML anyway, but only if base-path is not set. If it's set, blow up?\nServe a custom error page that describes what happened, maybe even includes the exception?. \n",
    "fulinlin924": "The root cause is, in cassandra database, AnnotationsIndex stores RowKey with uppercase letters, but ServiceNames, ServiceNameIndex, SpanNames, ServiceSpanNameIndex use only lowercase letters.\nThere are two solution, one is AnnotationsIndex uses lowercase letters, the other solution is ServiceNames, ServiceNameIndex, SpanNames, ServiceSpanNameIndex NOT convert to lowercase. Which solution is preferred from your perspective? I can make the change and send a pull request.\n. ",
    "zztztt": "I can over the wall, I can access the http://maven.twttr.com/,but I can't find the http://maven.twttr.com/com/github/spullara/mustache/java/mustache.java/0.8.12... and more.\nin the http://maven.twttr.com/com/github/.I can only see the \"igor-petruk\".\n. > about\n\n[info] This is sbt 0.12.3\n[info] The current project is {file:/opt/zipkin/}zipkin\n[info] The current project is built against Scala 2.9.2\n[info] Available Plugins: com.twitter.scrooge.ScroogeSBT, ZipkinResolver, com.twitter.sbt.GitProject, com.twitter.sbt.PackageDist, com.twitter.sbt.BuildProperties\n[info] sbt, sbt plugins, and build definitions are using Scala 2.9.2\n. \n",
    "tdrisdelle": "yes please!\n. Agreed.  I had to do this:\n```\nval conf = HBaseConfiguration.create()\nconf.set(HConstants.ZOOKEEPER_QUORUM, storageHost)\nconf.set(HConstants.ZOOKEEPER_CLIENT_PORT, storagePort)\nval storeBuilder = Store.Builder(\n  hbase.StorageBuilder(confOption = Some(conf)),\n  hbase.IndexBuilder(confOption = Some(conf))\n)\n```\n. Looking forward to this one!\n. Looking forward to this one!\n. I agree with eirslett, I'd think this is a valuable change that should be merged.\n. I agree with eirslett, I'd think this is a valuable change that should be merged.\n. I'm sorry for the confusion.... what I meant was \"I've checked out this patch and tested it on my local build with my own HBase back end and now it works fine, why isn't it being merged?\"\nThanks for providing the patch.\n. I'm sorry for the confusion.... what I meant was \"I've checked out this patch and tested it on my local build with my own HBase back end and now it works fine, why isn't it being merged?\"\nThanks for providing the patch.\n. Duplicate of one that I raised:\nhttps://github.com/twitter/zipkin/issues/403\n. Duplicate of one that I raised:\nhttps://github.com/twitter/zipkin/issues/403\n. ",
    "suchang": "lgtm - shipit\n. thanks - shipit\n. The scalding job failed due to following error:\n2014-02-07 14:55:40,209 ERROR cascading.flow.stream.TrapHandler: caught Throwable, no trap available, rethrowing\ncascading.pipe.OperatorException: [_pipe_0*_pipe_1][com.twitter.scalding.GroupBuilder$$anonfun$mapStream$1.apply(GroupBuilder.scala:229)] operator Every failed executing operation: BufferOp[decl:'value']\nat cascading.flow.stream.BufferEveryWindow.receive(BufferEveryWindow.java:151)\nat cascading.flow.stream.BufferEveryWindow.receive(BufferEveryWindow.java:41)\nat cascading.flow.hadoop.stream.HadoopGroupGate.run(HadoopGroupGate.java:90)\nat cascading.flow.hadoop.FlowReducer.reduce(FlowReducer.java:133)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:469)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)\nat org.apache.hadoop.mapred.Child$4.run(Child.java:266)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1310)\nat org.apache.hadoop.mapred.Child.main(Child.java:260)\nCaused by: java.lang.AbstractMethodError: com.twitter.zipkin.common.Dependencies$$anon$1.sumOption(Lscala/collection/TraversableOnce;)Lscala/Option;\nat com.twitter.algebird.Semigroup$.sumOption(Semigroup.scala:78)\nat com.twitter.scalding.typed.KeyedList$$anonfun$sumLeft$1.apply(KeyedList.scala:119)\nat com.twitter.scalding.typed.KeyedList$$anonfun$sumLeft$1.apply(KeyedList.scala:119)\nat scala.Function1$$anonfun$andThen$1.apply(Function1.scala:57)\nat com.twitter.scalding.GroupBuilder$$anonfun$4.apply(GroupBuilder.scala:228)\nat com.twitter.scalding.GroupBuilder$$anonfun$4.apply(GroupBuilder.scala:228)\nat com.twitter.scalding.BufferOp.operate(Operations.scala:399)\nat cascading.flow.stream.BufferEveryWindow.receive(BufferEveryWindow.java:143)\n. You are correct. Monoid is not the root cause of the AbstractMethodError. After I updated the algebird version to 0.4.0, the error went away. Gonna just pump the algebird version and leave Monoid as it was.\n. LGTM - could you also please update the scalatest dependency in zipkin-storm for consistency?\n. shipit - thanks\n. Yes, you are right. And I'm actually running the job using a monoid to compute the avg. But so far I've not been able to come up with the proper monoid for percentile :(\n. aha, that's what I'm looking for. will take a look. thanks!\n. lgtm - shipit!\n. Hmm, I can use gen.Span#toSpan to convert the scrooge generated one to common.Span.  Seems both approaches do the job here, but bijection will need extra dependencies and make the jar bigger for job submission. Did I missed any benefits bijection provided?\n. Hm, seems Try is only available since Scala 2.10 while we are still on 2.9.2 now..\n. updated.\n. done\n. done\n. dropped this stats\n. changed to ScalaTest\n. Yes, it actually outputs the fields of comom.Span - Fields(\"traceId\", \"spanId\", \"name\", \"serviceName\", \"isClientSide\"). We could expand the field list if needed by the jobs. \n. Changed to FunSuite.\n. yes, I'm getting compile errors for Long and Boolean. String seems fine.\ntype mismatch;\n[error]  found   : Long\n[error]  required: java.lang.Object\n[error] Note: an implicit exists from scala.Long => java.lang.Long, but\n[error] methods inherited from Object are rendered ambiguous.  This is to avoid\n[error] a blanket implicit which would convert any scala.Long to any AnyRef.\n[error] You may wish to use a type ascription: x: java.lang.Long.\n. done\n. Changed back to try/catch block.\n. turns out storm provides build-in metrics. removing this.\n. fixed\n. I tried to build it against 2.9.2 but there's compile errors in the storm code complaining missing classes. \n. Fixed the Monoid for span. The plus will keep merging span as long as it's not invalid.\n. fixed, thanks.\n. ",
    "johnynek": "Monoid is a subclass of Semigroup. That should not cause an error. What did you see?\n. Sounds good.\nOn Sat, Feb 8, 2014 at 10:45 PM, Chang Su notifications@github.com wrote:\n\nYou are correct. Monoid is not the root cause of the AbstractMethodError.\nAfter I updated the algebird version to 0.4.0, the error went away. Gonna\njust pump the algebird version and leave Monoid as it was.\n\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/pull/340#issuecomment-34567040\n.\n\n\nOscar Boykin :: @posco :: http://twitter.com/posco\n. in fact, zero is a method that is sometimes overridden with val.\n. don't think that's the safest approach. Can you find a way to a make a factory method tha generates new instances with the value you want, but once generated they are constant?\n. if you have a Monoid, you should leave it. Semigroup is a superclass, and it seems you have a zero.\n. should not be needed if you leave the Monoid on depList.\n. if depList.isEmpty, this will throw.  Monoid.sum( ) is better here if you have a zero.\n. this should be .sum which exactly means what you wrote.\n. not usually needed and clutters things a bit (in my view).\n. actually, if you are on a recent version of scalding, .group.sum == .sumByKey\n. ",
    "nshkrob": "Fixed the issues above.\n. Ship it!\n. Ship it!\n. Tests pass, so Ship it!\n. Ship it!\n. Neat! LGTM other than the extra imports.\n. Can you send a pull request please?\nOn Tue, Apr 15, 2014 at 10:15 AM, Mathias Bogaert\nnotifications@github.comwrote:\n\n+1\n\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/368#issuecomment-40508324\n.\n. shipit!\n. 2014? Also add the header to other files.\n. This is not used anywhere.\n. Is this needed? Same for 'create index' statements below.\n. ZipkinCollectorFactory, WriteSpanStore are unused.\n. I think you can do this:\nuser_name = user_obj['name'] || user_login\n. I see. Shipit!\n. \n",
    "pteichman": "Works for me\n. ",
    "elbiczel": "Hi, @sprsquish. I tried doing so and I've run into a quite nasty problem. Running bin/web causes following StackOverflowError:\n[info] Running com.twitter.zipkin.web.Main \n[error] java.lang.StackOverflowError\n[error]     at scala.runtime.AbstractFunction0.<init>(AbstractFunction0.scala:12)\n[error]     at scala.runtime.AbstractFunction0$mcV$sp.<init>(AbstractFunction0.scala:12)\n[error]     at com.twitter.server.Closer$$anonfun$closeOnExit$1.<init>(Closer.scala:29)\n[error]     at com.twitter.server.Closer$class.closeOnExit(Closer.scala:29)\n[error]     at com.twitter.zipkin.web.Main$.closeOnExit(Main.scala:101)\n[error]     at com.twitter.app.App$class.onExit(App.scala:87)\n[error]     at com.twitter.zipkin.web.Main$.onExit(Main.scala:101)\n[error]     at com.twitter.server.Closer$class.closeOnExit(Closer.scala:29)\n[error]     at com.twitter.zipkin.web.Main$.closeOnExit(Main.scala:101)\n[error]     at com.twitter.app.App$class.onExit(App.scala:87)\n[error]     at com.twitter.zipkin.web.Main$.onExit(Main.scala:101)\nBumping plugin works fine, bumping scrooge library causes the error. I had a quick look at the source code and I think it is due to a bug in the new scrooge version.\n. That makes perfect sense. I've updated the pull request.\n. ",
    "bbgasj": "but I used wget, it can download this jar, I don't know why? And could you\ntell me where the\njna-3.2.3.jarhttp://www.google.com/url?q=http%3A%2F%2Frepo1.maven.org%2Fmaven2%2Fnet%2Fjava%2Fdev%2Fjna%2Fjna%2F3.2.3%2Fjna-3.2.3.jar&sa=D&sntz=1&usg=AFQjCNHY8wq_StPyxPv5IWFA5fJ2kOgvuw\nput\nto?\nthank you!\nOn Sat, Mar 29, 2014 at 1:23 AM, Brian Degenhardt\nnotifications@github.comwrote:\n\nClosed #366 https://github.com/twitter/zipkin/issues/366.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/366\n.\n\n\n@\u51af\u6b23\u4f1f | \u827a\u9f99\u65c5\u884c\u7f51 | \u7814\u53d1\u5de5\u7a0b\u5e08\n\u5fae\u4fe1 bbgasj | QQ 11538018\n. ",
    "analytically": "+1\n. ",
    "mjwall": "Sure, I'll do a pull request right now.  Thanks\nOn Tue, Apr 15, 2014 at 3:25 PM, Nik Shkrob notifications@github.comwrote:\n\nCan you send a pull request please?\nOn Tue, Apr 15, 2014 at 10:15 AM, Mathias Bogaert\nnotifications@github.comwrote:\n\n+1\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/twitter/zipkin/issues/368#issuecomment-40508324>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/twitter/zipkin/issues/368#issuecomment-40523293\n.\n. Pull request submitted, #373.  Seems the builds launched by the pull request have been failing.  I don't think this was my pull request.  How does this get resolved?\n\nThanks \n. Closed in #373.  Thanks\n. Thanks for the reply Jeff.  Happy to move this to the google group if that is better.\nAny thoughts on the sqlite errors?  My thought was that too many spans where coming in asynchronously, and sqlite couldn't handle it.  But I wasn't sure.\nI think I am using the \"newer code\".  My testing was on master, commit 20f148b35ed18fa543cbedbc8ad977de54990b7a from 2 Apr.\nWhen you say flag, do you mean \"sqlite-persistent\"  on https://github.com/twitter/zipkin/blob/master/zipkin-anormdb/src/main/scala/com/twitter/zipkin/util/DBConfig.scala#L54 and https://github.com/twitter/zipkin/blob/master/project/Project.scala#L272?  If so, I don't see how to change backends without changing those values and recompiling.\nAt the very least, I'd like to try to get unit tests for sqlite (memory and persistent) and h2 (memory and persistent) against the SQL changes I made.  I can test manually against MySQL and Postgres.\n. I pushed in about 50 spans/second for 2 seconds.  Got about 8 of those exceptions, but it was hard to tell exactly which spans failed.\nI'll take a look at the AnormSpanStore, thanks.  I did notice the SQL was duplicated there and in AnormIndex.  Thanks.\n. So with the AnormDBSpanStoreFactory, it looks like only the AnormSpanStore and SpanStoreDB are needed from the rest of zipkin-anormdb subproject.  Is that right and what is the plan for the rest of the subproject?  Is there a ticket to refactor out old and unneeded code out of anormdb I can watch?\nThanks\n. ",
    "sethp-jive": "It looks like only one bug was still valid in tip, so I've backed out the conflicting change. It looks like it merges cleanly locally to me now, but I'm not sure how to confirm that it'll work via the PR? \n. Whoops, looks like a test failure \u2013\u00a0I'm on it.\n. Sorry it took so long \u2013 I opened #374 to fix the tests I broke.\n. Oh, cool, I had totally missed that this was already in progress, I'm sorry for dropping in unannounced. Thanks for the feedback! \nI'll get started teasing this stuff out into modules as you described, but I have a few questions for you:\n1. Jest's dependency on Elasticsearch core (and its drag) is optional. I've just kept it around it for the query builders really \u2013 if you're comfortable with embedding those in the source as JSON instead of objects, we could have a zipkin-storage/elasticsearch, zipkin-storage/elasticsearch-transport, zipkin-storage/elasticsearch-http (or Jest) tree?\n2. As far as I know (which is not very far, admittedly), the difference between the Transport client and Jest is largely of Java API (+ discovery entrypoint, naturally) \u2013 do we want to consider simplifying things by just replacing the elasticsearch implementation with one over HTTP instead of the binary transport? So just zipkin-storage/elasticsearch and zipkin-storage/elasticsearch-http? I didn't go that route initially because it would be a breaking change for existing configurations, but I thought I should mention it.\n3. I'm not very familiar with spring-boot or the autoconfiguration stuff, but I think I can figure it out if you can help me understand the final vision for how this stuff gets driven. Is the current (user-facing) magic regarding URL parsing and host-pattern-matching acceptable? e.g. one STORAGE_TYPE=elasticsearch and switching on the value in ES_HOSTS, or are we aiming for a suite of storage options like STORAGE_TYPE=elasticsearch, STORAGE_TYPE=elasticsearch-http, STORAGE_TYPE=elasticsearch-aws?\n4. Regarding testing the AWS integration \u2013 I don't know if it was clear, but the AWS code makes identical requests to the normal HTTP client with the addition of a pair of magic AWS headers. Regrettably, the contents of those headers are tough to validate except against a running AWS ES instance (I have a gnarly story about just how many URL encoding rounds it accepts for commas, if you want to hear it). I think that means just detecting on the presence of credentials is not quite enough \u2013 we also need a \"domain\" (what AWS calls an ES cluster) to connect to. How would you imagine this working in an environment that's not just my machine (with access to my AWS account)? i.e. should I be looking in an environment variable for that domain? I could code the tests to try creating a domain with the credentials at hand, but regrettably that takes quite a few minutes.\nI think if you're alright with it, I'd like to keep this PR as a kind of \"tracking\" issue and for the final zipkin-storage/elasticsearch-aws modules when we get there, and I can send you other PRs for the steps along the way.\n. Oh, cool, I had totally missed that this was already in progress, I'm sorry for dropping in unannounced. Thanks for the feedback! \nI'll get started teasing this stuff out into modules as you described, but I have a few questions for you:\n1. Jest's dependency on Elasticsearch core (and its drag) is optional. I've just kept it around it for the query builders really \u2013 if you're comfortable with embedding those in the source as JSON instead of objects, we could have a zipkin-storage/elasticsearch, zipkin-storage/elasticsearch-transport, zipkin-storage/elasticsearch-http (or Jest) tree?\n2. As far as I know (which is not very far, admittedly), the difference between the Transport client and Jest is largely of Java API (+ discovery entrypoint, naturally) \u2013 do we want to consider simplifying things by just replacing the elasticsearch implementation with one over HTTP instead of the binary transport? So just zipkin-storage/elasticsearch and zipkin-storage/elasticsearch-http? I didn't go that route initially because it would be a breaking change for existing configurations, but I thought I should mention it.\n3. I'm not very familiar with spring-boot or the autoconfiguration stuff, but I think I can figure it out if you can help me understand the final vision for how this stuff gets driven. Is the current (user-facing) magic regarding URL parsing and host-pattern-matching acceptable? e.g. one STORAGE_TYPE=elasticsearch and switching on the value in ES_HOSTS, or are we aiming for a suite of storage options like STORAGE_TYPE=elasticsearch, STORAGE_TYPE=elasticsearch-http, STORAGE_TYPE=elasticsearch-aws?\n4. Regarding testing the AWS integration \u2013 I don't know if it was clear, but the AWS code makes identical requests to the normal HTTP client with the addition of a pair of magic AWS headers. Regrettably, the contents of those headers are tough to validate except against a running AWS ES instance (I have a gnarly story about just how many URL encoding rounds it accepts for commas, if you want to hear it). I think that means just detecting on the presence of credentials is not quite enough \u2013 we also need a \"domain\" (what AWS calls an ES cluster) to connect to. How would you imagine this working in an environment that's not just my machine (with access to my AWS account)? i.e. should I be looking in an environment variable for that domain? I could code the tests to try creating a domain with the credentials at hand, but regrettably that takes quite a few minutes.\nI think if you're alright with it, I'd like to keep this PR as a kind of \"tracking\" issue and for the final zipkin-storage/elasticsearch-aws modules when we get there, and I can send you other PRs for the steps along the way.\n. I put up the alternative implementation as you described (I think) in #1306 \u2013 I wasn't quite sure what to do with some parts (especially the autoconfigure conditional stuff), but it seems to approximate the right behavior for me.\nIf you've got anything you want me to take care of, let me know. Otherwise, I'll focus on getting the AWS request signing stuff wedged in via a new autoconfigure package.\n. I put up the alternative implementation as you described (I think) in #1306 \u2013 I wasn't quite sure what to do with some parts (especially the autoconfigure conditional stuff), but it seems to approximate the right behavior for me.\nIf you've got anything you want me to take care of, let me know. Otherwise, I'll focus on getting the AWS request signing stuff wedged in via a new autoconfigure package.\n. Yup \u2013 I'm in the final stages of testing here on my end before wrapping up the last change and sending it along for review.\n. Yup \u2013 I'm in the final stages of testing here on my end before wrapping up the last change and sending it along for review.\n. Hmm, my only concern about that spring-aws module is that it looks like it might establish a different set of conventions from the pre-existing AWS client stuff (a rich set of system properties, environment variables, and/or config files that parameterize a bunch of different entrypoints into things like AWS credentials).\nFor example, in local development it's really convenient to use AWS_PROFILE=zipkin and have a role defined that I can switch into that emulates a restricted permission user comparable to what'd be running in production. Do you think that module will match pre-existing expectations?\n. I have good news and bad news about the test failures \u2013 the good news is I can reproduce them locally now that I figured out Circle CI is running elasticsearch 1.7.2! Huzzah!\nThe bad news is that there seems to be a fundamental incompatibility between Jest for the 2.x elasticsearches and for the 1.x stuff \u2013 we might be able to paper over the difference with enough effort, but what do you think about just declaring Zipkin as only supporting elasticsearch >= 2.x?\n. Would you help me understand the logic behind this line? I found it helpful to have the full exception + stacktrace when my problem was an NPE (e.g. accessing the wrong field in a JSON response) \u2013 I'm happy to keep it just the message, but if this change shows up again it means I hit another NPE and forgot to back it out before committing :)\n. We totally can \u2013 the varargs is just there to implement one-or-many semantics. Either this is a first-level aggregation, or it's nested \"arbitrarily\" deep (in practice, we only have two levels). This seemed clearer from both the implementation and user side to me than two methods (e.g. getKeys and getNestedKeys) .\n. The java elasticsearch client API uses varargs for its index lists; this allows a direct pass-through on that stuff and a zero-copy single-allocation wrapper when we need a list (i.e. Arrays#asList).\nAlternatively, we can take a list here, but then we'd have to do the indices.toArray(new String[indices.size()]) dance before handing them off to the elasticsearch client.\n. I had to bring this back to support the index-query-rewriting thing I'm doing in the RestClient (see the javadoc attached to MAX_INDICES). It's definitely ugly, but IMO less so than making the client check the # of indices and do the query rewriting themselves.\n. nope, good catch!\n. Fairly well \u2013 they got rid of the string data type (I think we want keyword, since we're treating ES as a big String.equals engine and not doing \"full text\" search), but with a few changes I can get many of the tests to pass. The rest fail because ES goes into overdrive and pegs my CPU without returning any results within 10 seconds. The errors it does log suggest I've missed some important fields (i.e. \"traceId\" is still \"text\" and not \"keyword\"), but I stopped there for now. \n. Removed!\n. Yup, good catch \u2013 I've fixed it up to look like the others in #1305 \n. I changed it back \u2013 as you say, not a big deal, IntelliJ just happened to mention that it could be a weaker visibility & so I made the warning go away :)\n. Yup, cleaned up now!\n. I'll change it, but as to the \"why\" \u2013 I would've preferred the java 8 Duration.ofSeconds(10).toMillis() (and the checked cast is there because apparently Jest uses ints for time instead of longs), but this is the closest I could come in Java 7. \n. Yeah, but due to repackaging it's a different gson than zipkin core uses.\n. I think this is more conventional this way? I changed it to mirror your change to the HTTP autoconfiguration.\n. Since the endpoint URLs that get dumped out of the AWS ES service aren't particularly friendly (e.g. https://search-name-wvkjy7wgk6j7iihjqcqemhhfju.us-west-2.es.amazonaws.com ) I found it useful to have Zipkin do the lookup for me to map from the name of the domain to the endpoint for a few different testing purposes. I also thought it might be nice to have a second more explicit path in just in case the AWS \"magic\" parsing of the URLs fails.\n. I'm sorry about this, but it was the only way I could find to prevent Spring from (eagerly) spinning up an unused JestClient in the application context (with pursuant confusing log messages about its initialization). I suppose we could have found a way to use that configuration, but it doesn't quite meet our needs \u2013 there's no hook for adding the signing interceptor, for example. I'd also suggest that makes more sense to have the HttpClient own its own connectivity rather than having it injected, similarly to how the NativeClient handles things.\n. I had this change locally because, as I'm sure you noted, my previous attempt to move the method-local class out didn't quite work (the constructor needed the additional setURI call).\nThe worst part about this is that I think it means the ES tests didn't run for my last PR \u2013 all the tests are passing consistently for me locally, but maybe having some trouble in CI. I might need your help to dig into that if it persists.\n. ah, sorry! I thought I was careful to get that out of the way before committing...\n. yeah, I'm sorry, I'd been good about keeping this out of the commit (I know it's not going in), but an errant git commit -a seems to have undone that effort.\nThanks for the JIRA and the workaround!\n. Was there a race condition? I only saw a janky ordering thing (with the constructor calling setURI out of order with its descendant) but it was limited to the constructing single thread (no leaking this outside the class that I saw, at least).\nI'll swap the base class out and fix up LenientSearch (which has sorta the same problem, except it has no constructor parameters for now). In that case, though, it seems I just misunderstood the Jest API and that class can go away entirely.\n. I don't disagree, but this seemed the only way to semi-reliably make a really easy path to get started \u2013 setting ES_HOSTS=https://my-aws-domain.us-west-2.amazonaws.com was the first thing I tried, and I think there's value in making that sort of thing work. And since the region is encoded in that URL (and is unfortunately required for the AWS v4 signature stuff), it seemed much more straightforward to peel it out of the semi-standard URL format.\nThat said, it's definitely the kind of magic that breeds complications. Wether it's worth the tradeoff for ease of getting-started, I suppose, is up to you.\n. ",
    "fr0stbyte": "Also wanted to say that I'm working on adding stats and also allowing the collector to be configured via zookeeper. Don't have timeline for these yet.\n. @sprsquish just rebased off upstream/master can you please try again ?\n. I noticed that some code was looking strangely unfamiliar. \nTwo questions : \n1. here https://github.com/twitter/zipkin/commit/6107d9b#diff-f70d68ae7ea4c895a87b3be96ca84af6R15 \n   why is the trait moved down as an internal type of KafkaProcessor ? Decoder[Option[List[Span]]]] is used somewhere else in the code, plus the implementing collectors would use it. Can we keep it top level and use it throughout the project ? \n2. https://github.com/twitter/zipkin/pull/390/files#diff-edc7ec48aa7cab8b9004553848474e92R22 the flag is a , separated sequence of type a=b. Without the definition of the flag, how will the system understand -zipkin.kafka.topics=a=1,b=2,c=3 ?\n. Sure, not as a trait but as a type then ? \nDidn't know about the map type, thanks for that.\n. Got it. Please check the updated diff. \n. @sprsquish bump\n. Have you tried recently ?\n. Have you tried recently ?\n. ",
    "maz": "That's another dependency problem. I think it should be fixed by\ndiff --git a/project/Project.scala b/project/Project.scala\nindex 61c2e9e..c5b2695 100644\n--- a/project/Project.scala\n+++ b/project/Project.scala\n@@ -480,8 +480,10 @@ object Zipkin extends Build {\n       \"org.apache.hadoop\"     % \"hadoop-common\"         % \"2.4.0\",\n       \"org.apache.hbase\"      % \"hbase\"                 % hbaseVersion,\n       \"org.apache.hbase\"      % \"hbase-common\"          % hbaseVersion,\n+      \"org.apache.hbase\"      % \"hbase-common\"          % hbaseVersion % \"test\" classifier(\"tests\") classifier(\"\"),\n       \"org.apache.hbase\"      % \"hbase-client\"          % hbaseVersion,\n       \"org.apache.hbase\"      % \"hbase-client\"          % hbaseVersion % \"test\" classifier(\"tests\") classifier(\"\"),\n+      \"org.apache.hbase\"      % \"hbase-server\"          % hbaseVersion % \"test\" classifier(\"tests\") classifier(\"\"),\n       \"com.google.guava\"      % \"guava-io\"              % \"r03\" % \"test\",\n       \"com.google.protobuf\"   % \"protobuf-java\"         % \"2.4.1\",\n       \"org.apache.hadoop\"     % \"hadoop-core\"           % \"1.2.1\" notTransitive(),\nI'm having trouble adding an additional resolver to get hbase-common-0.98.3-hadoop2-tests.jar since Typesafe doesn't have it in their repo. Once I've got the resolver working (and assuming that the dependency changes in the diff above solve the problem), I'll push a new commit with the changes.\n. It appears that com.twitter.common requires Google guava version 16.  Hadoop's org.apache.hadoop.util.JvmPauseMonitor requires a lower version of guava: one which implements the elapsedMillis() method. I'm looking into a solution.\n. For some reason, sbt is only trying the typesafe resolver, and is not trying the DefaultMavenRepository resolver as declared in Project.scala. Once the artifacts are resolved, the tests pass (at least on my machine). I'll continue trying to get the resolver working.\n. Do you mean style in terms of whitespace formatting or in terms of how the code itself is structured?\n. ",
    "Oscil8": "It appears that there are other tests that are failing occasionally in Travis? There aren't actually any tests for the gems.\n. @franklinhu Let me know if you've got any additional comments.\n. Ah, I see you probably wanted handling of the X-B3-Flags header described in https://github.com/twitter/zipkin/blob/master/doc/collector-api.md ; have an update about to come through.\n. Done, thanks.\n. @franklinhu Let me know if you've got any additional comments.\n. Sounds good, I'll update my patch\n. Sure, we could take it in as a parameter to get_or_create_trace_id, e.g.\nget_or_create_trace_id(env, flags = ::Trace::Flags::EMPTY)\n      ...\n      trace_parameters << flags\nAlthough this still requires work to allow configuring the default value of that flag in the plugin.\n. ",
    "synk": "Sorry to be confused...\nI will be glad if you pull this!\n. I see.\nthanks!\n. @tdrisdelle Are you using HBase? We are using HBase as span storage in our production. And annotation search didn't work until this patch.  It seems that a wrong query is thrown into HBase. The reason is that  bb is array backed ByteBuffer, then bb.array() returns whole of backed array. \n. @tdrisdelle Are you using HBase? We are using HBase as span storage in our production. And annotation search didn't work until this patch.  It seems that a wrong query is thrown into HBase. The reason is that  bb is array backed ByteBuffer, then bb.array() returns whole of backed array. \n. Thank you for your test!\n. Thank you for your test!\n. ",
    "liyichao": "I encounter this error too.\n. I encounter this error too.\n. I reclone the project and build with\n./bin/collector zk-adaptive\nthe problem remains. Do you mean deleting all files in ~/.ivy2\n. I reclone the project and build with\n./bin/collector zk-adaptive\nthe problem remains. Do you mean deleting all files in ~/.ivy2\n. good!\n. good!\n. May related to this issue: https://datastax-oss.atlassian.net/browse/JAVA-852?jql=text%20~%20%22tokens%20null%22 .\nI notice that zipkin has used the official client instead of twitter/cassie when moved to zipkin.\n. Yeah, recently, we mark the dead cassandra node as seed. The host down error  dissappear, but the Unable to find compaction strategy... error remains.\n. Yeah, recently, we mark the dead cassandra node as seed. The host down error  dissappear, but the Unable to find compaction strategy... error remains.\n. Can I select  traces by specifying a time interval ?\n. Can I select  traces by specifying a time interval ?\n. We also encounter this in 1.25.1.\nAnother problem, when I select span name, and send the request, the page returns, with span name reset to all, which is unexpected. I expect the span name remains to be what I select.\n. Any update? another problem, why the graph initially collapsed ?  It makes me click the expand all all the time.\n. Have this released to https://repo1.maven.org/maven2/io/zipkin/ ?\n. Then I misunderstand the meaning, which is not that straightforward. Close the issue if you think it is not a problem.. When I click expand all, it expands as expected.\n. trace.txt\n. ScribeClient:\n```\n    socket, err := thrift.NewTSocket(zipkinAddr)\n    if err != nil {\n        log.Printf(\"Failed to create socket: %v\\n\", err)\n        return\n    }\n    btrans := thrift.NewTFramedTransport(socket)\n    proto := thrift.NewTBinaryProtocolTransport(btrans)\n    if err = btrans.Open(); err != nil {\n        log.Printf(\"Failed to connect to zipkin: %v\\n\", err)\n        return\n    }\n    defer btrans.Close()\nscribeClient := scribe.NewScribeClientProtocol(btrans, proto, proto)\n\n```\nEvery time it stucks, there is a zipkin Collector log which I do not understand:\nMar 05, 2016 9:20:25 PM com.twitter.finagle.mux.ServerDispatcher com$twitter$finagle$mux$ServerDispatcher$$process\nWARNING: Received duplicate tag 2 from client /127.0.0.1:50796\nI confirm that 50796 is the port my collector uses to connect to zipkin collector.\n. Hi, It seems it is a bug in finagle 6.33.0, any plan to update fingale to 6.34.0 ?\n. Our reporter sends message less than 1MB (I have stats to ensure that). But the kafka server's configuration is 200MB (other message needs this)\uff0cand the config seems to be global and affects our topic.\n. Yeah.\n. Hi, any update ?\n. ",
    "yancl": "because the project depends on two different version com.twitter.common.zookeeper.group(0.0.44&0.0.67).\nbut they have different signature:\n0.0.44:\npublic final void watch(final GroupChangeListener groupChangeListener)\n      throws WatchException, InterruptedException {\n0.0.67:\npublic final Command watch(final GroupChangeListener groupChangeListener)\n      throws WatchException, InterruptedException {\nand your compile error shows that it need 0.0.44(void return value),so failed.\n[error] java.lang.NoSuchMethodError: com.twitter.common.zookeeper.Group.watch(Lcom/twitter/common/zookeeper/Group$GroupChangeListener;)V\nlook at the (V) of end of the line, it needs void return version watch method.\nso when you remove dependency of 0.0.67, it works.\nas a way to test the problem,just change the followwing ivy files to group 0.0.44 from 0.0.67:\n~/.ivy2/cache/com.twitter/util-zk-common_2.9.2/ivy-6.16.0.xml\n~/.ivy2/cache/com.twitter.common.zookeeper/server-set/ivy-1.0.72.xml\nhopes this will be useful and you admin can fix it:)\n. because the project depends on two different version com.twitter.common.zookeeper.group(0.0.44&0.0.67).\nbut they have different signature:\n0.0.44:\npublic final void watch(final GroupChangeListener groupChangeListener)\n      throws WatchException, InterruptedException {\n0.0.67:\npublic final Command watch(final GroupChangeListener groupChangeListener)\n      throws WatchException, InterruptedException {\nand your compile error shows that it need 0.0.44(void return value),so failed.\n[error] java.lang.NoSuchMethodError: com.twitter.common.zookeeper.Group.watch(Lcom/twitter/common/zookeeper/Group$GroupChangeListener;)V\nlook at the (V) of end of the line, it needs void return version watch method.\nso when you remove dependency of 0.0.67, it works.\nas a way to test the problem,just change the followwing ivy files to group 0.0.44 from 0.0.67:\n~/.ivy2/cache/com.twitter/util-zk-common_2.9.2/ivy-6.16.0.xml\n~/.ivy2/cache/com.twitter.common.zookeeper/server-set/ivy-1.0.72.xml\nhopes this will be useful and you admin can fix it:)\n. ",
    "ggsonigg": "does anyone really have pointers on this? I wanted to try out brave APIs with my java code, as an example, but there is nothing I could found which could help me use zipkin.\n. does anyone really have pointers on this? I wanted to try out brave APIs with my java code, as an example, but there is nothing I could found which could help me use zipkin.\n. ",
    "hunglin": "@sprsquish , it got OutOfMemoryError from Travis CI build, is this a real issue or just a hiccup of Travis?  How to do it if I want to relaunch the CI build?\n. Thanks a lot!\nOn Aug 8, 2014 9:36 AM, \"Jeff Smick\" notifications@github.com wrote:\n\n@hunglin https://github.com/hunglin I'll restart it. Travis is\nnotoriously flaky for Zipkin so I don't put too much stock in it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/zipkin/pull/402#issuecomment-51600917.\n. openjdk6 passed this time, maybe the old saying is true: third time's the charm \n. one question, @sprsquish , it seems zipkin/sbt set -XX:MaxPermSize=1024m but Travis doesn't use it so the the MaxPermSize is 512M for ci build, that's probably the reason.\n. \n",
    "zeagord": "This must be fixed in the latest versions after https://github.com/openzipkin/zipkin/pull/2224. This must be fixed in the latest versions after https://github.com/openzipkin/zipkin/pull/2224. This must be fixed in the latest versions after #2224\n. This must be fixed in the latest versions after #2224\n. Long services names will be truncated and available on mouseover in the latest UI versions. https://github.com/openzipkin/zipkin/pull/2184. Long services names will be truncated and available on mouseover in the latest UI versions. https://github.com/openzipkin/zipkin/pull/2184. Clipping the text by setting the text-overflow in the css may help and the full name can be displayed on mouseover . I can try do that If it is an agreeable behaviour.. This is fixed by #2184 . I made the jar executable and tested in PCF Dev. It works fine. \nname:              zipkin\nrequested state:   started\ninstances:         1/1\nusage:             256M x 1 instances\nroutes:            zipkin.local.pcfdev.io\nlast uploaded:     Thu 01 Mar 09:41:41 +07 2018\nIf we are good, i can submit a PR.. I made the jar executable and tested in PCF Dev. It works fine. \nname:              zipkin\nrequested state:   started\ninstances:         1/1\nusage:             256M x 1 instances\nroutes:            zipkin.local.pcfdev.io\nlast uploaded:     Thu 01 Mar 09:41:41 +07 2018\nIf we are good, i can submit a PR.. #1934 Submitted the PR.. #1934 Submitted the PR.. I think we should change the read me too. It will be easier for readers.. I think we should change the read me too. It will be easier for readers.. Running as a system service gives better control in production environments. If it is an executable jar or service it is straight forward to create to symlink and manage it. Secondly our Zipkin server is a springboot app  and springboot supports executable jar. . https://docs.spring.io/spring-boot/docs/current/reference/html/deployment-install.html. AFAIK generating asciidoc from MD is little tricky everytime. I think we have to convert all the docs to asciidoc and then add new documentation in the asciidoc format. . Tested and the changes are not required until the release of Layout Factory. I don't think it is a Zipkin issue. Please try again.. I don't think it is a Zipkin issue. Please try again.. Of Course. If you can help outline what needs to be done. Most of the metrics that we need JVM, Http_Requests are available out of the box. I think we need to implement custom meters for spans, bytes collected, etc. . Of Course. If you can help outline what needs to be done. Most of the metrics that we need JVM, Http_Requests are available out of the box. I think we need to implement custom meters for spans, bytes collected, etc. . Migrated to springboot2 and verified health endpoints. \nTODO: Implement the collector metrics via Micrometer. Added the Collector Counter using Micrometer and delegated the HealthEnpoint to the root  path/health. Polished the metrics collection and health enpoint. \nSample output from /metrics\n{\n  \"counter.zipkin_collector.messages.http\": 1.0,\n  \"counter.zipkin_collector.spans\": 0.0,\n  \"counter.zipkin_collector.spans_dropped\": 0.0,\n  \"counter.zipkin_collector.spans.http\": 0.0,\n  \"counter.zipkin_collector.spans_dropped.http\": 0.0,\n  \"counter.zipkin_collector.bytes\": 0.0,\n  \"gauge.zipkin_collector.message_bytes\": 0.0,\n  \"gauge.zipkin_collector.message_spans.http\": 0.0,\n  \"counter.zipkin_collector.messages_dropped\": 0.0,\n  \"counter.zipkin_collector.messages\": 0.0,\n  \"gauge.zipkin_collector.message_bytes.http\": 0.0,\n  \"counter.zipkin_collector.bytes.http\": 0.0,\n  \"gauge.zipkin_collector.message_spans\": 0.0,\n  \"counter.zipkin_collector.messages_dropped.http\": 0.0\n}\nAnd from the /health\n{\n  \"status\": \"UP\",\n  \"details\": {\n    \"zipkin\": {\n      \"status\": \"UP\",\n      \"details\": {\n        \"InMemoryStorage\": {\n          \"status\": \"UP\"\n        }\n      }\n    },\n    \"diskSpace\": {\n      \"status\": \"UP\",\n      \"details\": {\n        \"total\": 121123069952,\n        \"free\": 10457477120,\n        \"threshold\": 10485760\n      }\n    }\n  }\n}. Implemented all the meters, polished tests. . Closing this PR to breakdown the tests and the Micrometer/Boot2 upgrade in to separate ones.. \n\nthis needs analysis @zeagord @tacigar ?\n\nYes, It doesn't work. Let me look in to it. Below are the results after running the benchmarks. \n```\nMicrometer\nBenchmark                                              Mode  Cnt  Score   Error  Units\nMetricsBenchmarks.incrementBytes_longSpans_Actuate     avgt  240  0.389 \u00b1 0.008  us/op\nMetricsBenchmarks.incrementBytes_longSpans_inMemory    avgt  240  0.363 \u00b1 0.010  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_Actuate   avgt  240  0.339 \u00b1 0.001  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_inMemory  avgt  240  0.344 \u00b1 0.030  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_Actuate    avgt  240  0.386 \u00b1 0.008  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_inMemory   avgt  240  0.335 \u00b1 0.006  us/op\nPre Micrometer\nBenchmark                                              Mode  Cnt  Score   Error  Units\nMetricsBenchmarks.incrementBytes_longSpans_Actuate     avgt  240  1.550 \u00b1 0.028  us/op\nMetricsBenchmarks.incrementBytes_longSpans_inMemory    avgt  240  0.228 \u00b1 0.001  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_Actuate   avgt  240  1.472 \u00b1 0.010  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_inMemory  avgt  240  0.234 \u00b1 0.004  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_Actuate    avgt  240  1.498 \u00b1 0.012  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_inMemory   avgt  240  0.235 \u00b1 0.002  us/op\n. Below are the results after running the benchmarks.\nMicrometer\nBenchmark                                              Mode  Cnt  Score   Error  Units\nMetricsBenchmarks.incrementBytes_longSpans_Actuate     avgt  240  0.389 \u00b1 0.008  us/op\nMetricsBenchmarks.incrementBytes_longSpans_inMemory    avgt  240  0.363 \u00b1 0.010  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_Actuate   avgt  240  0.339 \u00b1 0.001  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_inMemory  avgt  240  0.344 \u00b1 0.030  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_Actuate    avgt  240  0.386 \u00b1 0.008  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_inMemory   avgt  240  0.335 \u00b1 0.006  us/op\nPre Micrometer\nBenchmark                                              Mode  Cnt  Score   Error  Units\nMetricsBenchmarks.incrementBytes_longSpans_Actuate     avgt  240  1.550 \u00b1 0.028  us/op\nMetricsBenchmarks.incrementBytes_longSpans_inMemory    avgt  240  0.228 \u00b1 0.001  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_Actuate   avgt  240  1.472 \u00b1 0.010  us/op\nMetricsBenchmarks.incrementBytes_mediumSpans_inMemory  avgt  240  0.234 \u00b1 0.004  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_Actuate    avgt  240  1.498 \u00b1 0.012  us/op\nMetricsBenchmarks.incrementBytes_shortSpans_inMemory   avgt  240  0.235 \u00b1 0.002  us/op\n``. This is fixed by #2198. Custom builds are not supported. Use the released jar or docker version.. @chuguoren Well. Firstly, There is nothing stopping you from using custom builds. You can still use it at your own risk. For most of the users, the released versions are sufficient enough for running it production. If you're learning/exploring Zipkin, I would recommend using the released version and if you feel any limitations you can request that feature to be included in the core library or explore the option of running a custom build. \n@nolan4954 - FYI - The latest verision of Zipkin 2.8.4 is using Springboot 2.0.1 under the hood. . @chuguoren Well. Firstly, There is nothing stopping you from using custom builds. You can still use it at your own risk. For most of the users, the released versions are sufficient enough for running it production. If you're learning/exploring Zipkin, I would recommend using the released version and if you feel any limitations you can request that feature to be included in the core library or explore the option of running a custom build. \n@nolan4954 - FYI - The latest verision of Zipkin 2.8.4 is using Springboot 2.0.1 under the hood. . AFAIK Spring logging doesn't support multiple libs. \nhttps://github.com/spring-projects/spring-boot/issues/11148\nhttps://github.com/spring-projects/spring-boot/issues/10847. Did not work. :/. Closing this as this is not a valid fix.. JSYK. Custom servers are not supported since 2.7.x.@EnableZipkinServer` is deprecated. Please use one of the supported binary release directly or use the Docker version.. Can you try this with the latest Zipkin and check?. Could be related to this https://github.com/openzipkin/zipkin/issues/1906. I don't think we can turn off the 2.x either because I suspect many of them are still using it in production. It will be nice to reintroduce the standard indexing.. I don't think we can turn off the 2.x either because I suspect many of them are still using it in production. It will be nice to reintroduce the standard indexing.. This is related to #1206. Here's the screen capture of the feature.\n\n. Here's the screen capture of the feature.\n\n. Looks like this fixed in the spring-boot 2.0.3.RELEASE. I just validated it.\nhttps://github.com/spring-projects/spring-boot/issues/12659\nhttps://github.com/spring-projects/spring-boot/commit/1a0dfa06abdf5ca81b7396c8d882c914c11e7235. +1 from me. I think this is a progressive step.\nOn Wed 1 Aug, 2018, 17:56 Bas van Beek, notifications@github.com wrote:\n\nif we put the libraries in the openzipkin namespace in here too, I'd\nsuggest doing a triage of the ones in there now. Maybe not all of them\nbenefit at this time and might need to be demoted to contrib.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2152#issuecomment-409535948,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AEgSsLdkG-Qb9K9Km4ff2LcTQqjF41b5ks5uMYl6gaJpZM4Vpy4t\n.\n. Can you try set the base path to/? I'm using like the below one in the latest zipkin version and found no problems.\n\n{\n  \"path\": \"/\",\n  \"backend\": {\n  \"serviceName\": \"zipkin-service\",\n  \"servicePort\": 9411\n}. Small tip. Wrap the deployment script with ```. So that it will be readable.\nTry the below deployment script. I assume you're using Minkube. If that's true. Enable the ingress by minikube addons enable ingress. If not you have choose a ingress controller.\n```\napiVersion: v1\nkind: Service\nmetadata:\n  namespace: default\n  name: zipkin-service\nspec:\n  ports:\n    - port: 9411\n  selector:\n    app: zipkin\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: zipkin\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: zipkin\n  template:\n    metadata:\n      labels:\n        app: zipkin\n    spec:\n      containers:\n      - name: zipkin\n        image: openzipkin/zipkin\n      readinessProbe:\n        httpGet:\n        path: \"/actuator/health\"\n        port: 9411\n        initialDelaySeconds: 5\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: zipkin-ingress\n  annotations:\n    kubernetes.io/ingress.allow-http: \"true\"\nspec:\n  backend:\n    serviceName: zipkin-service\n    servicePort: 9411\n``. I honestly think, this is not a Zipkin issue since it has nothing to do it. It is more specific on how you design your ingress or routes. It should be a question that might be suit well for stackoverflow or kubernetes.. It's completed now. . Screenshot of the fix\n![screen shot 2561-08-29 at 15 16 55](https://user-images.githubusercontent.com/4723376/44774942-9eab2980-ab9e-11e8-87a0-fc50e0826f10.png)\n. Relates to https://github.com/openzipkin/zipkin/issues/1906. RemovedCharacterEncodingFilteras it is no longer required since Springboot doesn't enforce the response encoding\nRemovedWebMvcConfigurerAdapteras it is deprecated and we canWebMvcConfigurer` interface to add the resource handlers. Nice catch. Thanks.. I have a feeling like this will become like the custom server. If there is a strong use case and request from the community we can actually add it to our UI app itself. Anyhow people who wants extra stuffs can always fork and develop something that makes sense for them. Otherwise this will lead to a lot of confusion and support from the community. . Sure. I will fix this or can we wait for the new UI? Because I was under the impression that we don't want to spend effort on the current UI unless it is a show stopper. . I have added the in-memory api. I will revisit the cassandra tomorrow and address the above concerns.. Need to add tests and revisit the Elastic search design. . > is this tagkey because \"key\" is reserved? I suspect if not easier to just do key/value\nBodyconverters for ES looks for \"key\" from the result of aggregations which clashes with the key in the tag {k,v}. It could be the name of the aggregation. I will change the name of the aggs alone and see if it works. . cc @wu-sheng. Sure. I think so too.. Looks like the index templates are getting created. It will be easier to troubleshoot and help in gitter. \nhttps://gitter.im/openzipkin/zipkin. It's not a bug. what you're seeing is a remote endpoint service name which is not instrumented. Join the gitter https://gitter.im/openzipkin/zipkin if you have questions.. What is your backend? Are you trying out Zipkin? Please join gitter it will be easier to help and debug.. Can we please target this next release? This one needs some improvement and\ncss needs to be fixed.\nOn Sat, 16 Feb, 2019, 19:14 Adrian Cole, notifications@github.com wrote:\n\n@zeagord https://github.com/zeagord you in a position to carry this\nforward? will likely cut a release by monday but this isn't a blocker.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2366#issuecomment-464342100,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AEgSsGfVsZ3h24V9AbXiQ8xiV-gPHQcXks5vN_Y7gaJpZM4aUIJg\n.\n. TODO: Need to fix the CI and test. Looking at it, Possibly since I moved the css under the src directory. > I will try clean build again just in case\n\nI am also seeing the same. Checking the issue.. Sure. I am on it.. Sure. I am on it.. Yeah it is redundant and IMHO repackage is not needed. If we need to we have to provide the launcher in the layout factory as optional. Is there a situation where we need to run this as a standalone jar?. Thanks @shakuzen for the review. I think Gauge can observe values that are settable so I was using AtomicDouble. \nhttp://micrometer.io/docs/concepts#_gauges\nPlease correct If I am wrong.. Okay.. I need to implement the http_request_duration_seconds_count. Yeah. I have just completed that as well. Doing the clean up and updating the tests now.. Yeah spring inject this bean.. Oh now. I will change the access modifier.. My bad. I should have looked closer. Can we include Micrometer and do the benchmarks with a SimpleRegistry or remove this one? . I have removed it in the new PR. Makes sense. I will change it. But i makes the tests fail :) writeSpans_updatesPrometheusMetrics. We may not need this since we use the PrometheusMeterRegistry. Changed it to logo only. Removed it. I would change this to package specific access to reduce the size of the bytecode. . Added them :). Yep. Same as service-span. Fixed. I wasn't aware of the reason why this one exists. I will remove it.. Remvoed . Added the whitelist implementation. Let me know your thoughts.. Unused import. I am returning the static list in the TagStore here https://github.com/zeagord/zipkin/blob/b64e7950527d83f49598247ccf0f2385220cff4b/zipkin-storage/cassandra/src/main/java/zipkin2/storage/cassandra/CassandraTagStore.java#L32. Can you move this outside the component. Just for better readability.. Did you happen to try the loader: 'svg-inline-loader' from webpack itself?. Yeah, to refer import styles, etc.. KeyCode is deprecated. We must use the e.key instead. More better to follow the instruction here everywhere we handle keyboard events. \nhttps://developer.mozilla.org/en-US/docs/Web/API/KeyboardEvent/keyCode#Example. LGTM other than that. Cheers.. proxy is used for development mode. Since we package the UI in the server we don't have to specify the endpoint. We can create a separate config as an environment variable to point to the UI if needed.. Added. It's autogenerated from the build process. But can check if there is a way. ",
    "hzhaofb": "Found following log showing duplicate traceIds in the ArrayBuffer.\nerror DEB [20140827-22:47:00.036] cassie: multiget_slice(Zipkin, [java.nio.HeapByteBuffer[pos=0 lim=8 cap=8], java.nio.HeapByteBuffer[pos=0 lim=8 cap=8], java.nio.HeapByteBuffer[pos=0 lim=8 cap=8], java.nio.HeapByteBuffer[pos=0 lim=8 cap=8], java.nio.HeapByteBuffer[pos=0 lim=8 cap=8], java.nio.HeapByteBuffer[pos=0 lim=8 cap=8], java.nio.HeapByteBuffer[pos=0 lim=8 cap=8], java.nio.HeapByteBuffer[pos=0 lim=8 cap=8], java.nio.HeapByteBuffer[pos=0 lim=8 cap=8], java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]], ColumnParent(column_family:SpanNames), SlicePredicate(slice_range:SliceRange(start:, finish:, reversed:false, count:2147483647)), ONE)\u001b[0m\nerror DEB [20140827-22:47:00.051] query: getTraceSummariesByIds. traceIds: ArrayBuffer(-418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004, -418964871134254004) adjust ArrayBuffer(TimeSkew)\u001b[0m\nerror\u001b[0m] \u001b[0mDEB [20140827-22:47:00.052] cassie: multiget_slice(Zipkin, [-418964871134254004], ColumnParent(column_family:Traces), SlicePredicate(slice_range:SliceRange(start:, finish:, reversed:false, count:100000)), ONE)\u001b[0m\nerror\u001b[0m] \u001b[0mDEB [20140827-22:47:01.455] query: getServiceNames\u001b[0m\n. ",
    "boernd": "Did you find a solution to this problem?\n. Did you find a solution to this problem?\n. ",
    "postwait": "I also have this problem... I had a trace repeat listed for each span it contains.\n. This works for me:\n```\ndiff --git a/zipkin-web/src/main/scala/com/twitter/zipkin/web/Handlers.scala b/zipkin-web/src/main/scala/com/twitter/zipkin/web/Handlers.scala\nindex dbb14f9..0d5f7e7 100644\n--- a/zipkin-web/src/main/scala/com/twitter/zipkin/web/Handlers.scala\n+++ b/zipkin-web/src/main/scala/com/twitter/zipkin/web/Handlers.scala\n@@ -76,7 +76,7 @@ class Handlers(jsonGenerator: ZipkinJson, mustacheGenerator: ZipkinMustache) {\n     case ids =>\n       val adjusters = getAdjusters(request)\n\n\nclient.getTraceSummariesByIds(ids, adjusters) map { .map { .toTraceSummary } }\nclient.getTraceSummariesByIds(ids distinct, adjusters) map { .map { .toTraceSummary } }\n       }\n     }\n   }\n```\n. \n",
    "srapp": "@sprsquish Is there any intention to pull this into the open source repo?  Seems like a really cool feature that many users could benefit from!\n. ",
    "jollychang": "any update?\n. any update?\n. ",
    "baloo": "@adriancole fixed\n. @adriancole fixed\n. ",
    "kristofa": "You can also use Flume. It supports a ScribeSource (I guess like fluentd).  It did not have a Scribe Sink out of the box but you can use flume-zipkin-collector-sink and I also found flume-ng-scribe just now.\nI used Flume with flume-zipkin-collector-sink as a drop-in replacement for Scribe and it worked well.\n. @eirslett Yes, agree. When I initially started integrating Zipkin we were actually using logstash but logstash had no codec support at that time. This is supported as of version 1.2.0.  So I guess it should be possible now to use logstash with Thrift.\n. Having the versioned and published fat jars will be great!\n:+1: \n. Besides the build that is not happy (not sure if related to change or flaky test) :+1: \n. :+1: \n. ",
    "schrepfler": "I think this can be closed now as per https://github.com/openzipkin/zipkin-java/pull/30 ?\n. Finatra being based on Finagle, I imagine it would be possible to make some sort of a Filter to perform authentication and resource protection scheme but UI work would be needed and probably some way to model the security model.\n. Is the opposite possible (using 2.2 drivers with 3.x Cassandra)?\n. \ud83d\udc4d \n. I would go with external repo and some sort of contrib model and zipkin-storage-dse5 sounds fine (ideally Datastax themselves would be maintainers as they know their features best and it would advocate optimal use of their product.)\nIs the DSE Solr interface so much different when starting Cassandra with the parameters to turn on string indexing using ES or Solr (I was under the impression Cassandra ships with such an example config but I can't find it in my instance, perhaps I've seen it in use on some modified Cassandra ex. Titan+Cassandra+ES)?\n. I would go with external repo and some sort of contrib model and zipkin-storage-dse5 sounds fine (ideally Datastax themselves would be maintainers as they know their features best and it would advocate optimal use of their product.)\nIs the DSE Solr interface so much different when starting Cassandra with the parameters to turn on string indexing using ES or Solr (I was under the impression Cassandra ships with such an example config but I can't find it in my instance, perhaps I've seen it in use on some modified Cassandra ex. Titan+Cassandra+ES)?\n. I think that's the way to go too, haven't dealt with it directly though.\n. I think that's the way to go too, haven't dealt with it directly though.\n. Is 2) a problem? I think in this case we care mostly about the publisher API (agent/middleware -> pulsar) which can also be batched/delayed locally (at risk of loss). . No problem. I think Kafka did mitigate the blocking client a bit in the last releases (just for fairness/completeness of the argument)  https://cwiki.apache.org/confluence/display/KAFKA/KIP-266%3A+Fix+consumer+indefinite+blocking+behavior. ",
    "drpacman": "Thanks for the link, a re-energised community would be a good thing.\n. ",
    "michaelsembwever": "work is started in https://github.com/openzipkin/zipkin/commit/cb9f063f5e9b\nit's implemented i believe but just needs a unit test for it.\n. work is started in https://github.com/openzipkin/zipkin/commit/cb9f063f5e9b\nit's implemented i believe but just needs a unit test for it.\n. Yeah, i just wanted to throw a simple implementation out there to begin with.\nYou also have the AuthProvider class in the datastax cql driver, along with the PlainTextAuthProvider implementation of it.\n. Yeah, i just wanted to throw a simple implementation out there to begin with.\nYou also have the AuthProvider class in the datastax cql driver, along with the PlainTextAuthProvider implementation of it.\n. I'm in favour of replacing all (possible)  traits with Java interfaces.  Would have saved me time already. Not even worrying about what's the APIs needing exposing are. Unless I'm mistaken and Java interfaces cause problems? \nI'm really not in favour of trying to wrap traits.\n(traits can extend those Java interfaces if we need to maintain that compatibility). \n. rewriting just the traits to interfaces is a small incremental step in a\npositive direction.\ni'm a fan of small atomic tickets. can we open up a new ticket, an epic,\nfor \"rewrite (core parts) in java\"?\nOn 16 July 2015 at 16:16, Adrian Cole notifications@github.com wrote:\n\nActually, maybe it is simpler just to rename this issue to rewrite in java?\nWe can always raise incremental pull requests.. Wdyt?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/zipkin/issues/451#issuecomment-121969227.\n\n\nMick Semb Wever\nScandinavia\nThe Last Pickle\nApache Cassandra Consulting\nhttp://www.thelastpickle.com\n. Taking a look at this it doesn't look like the first step is replacing traits, since most of them reference other scala classes. \nI suspect a right \"first step\" is to look into rewriting zipkin-common. This then gets us closer to replacing traits with interfaces, and allowing new subprojects in the zipkin server-side stack to be written with just java (which will help attract more developers to the community).\n. Yes, we want the stack to be friendly to both java and scala implementations.\nJava implementations of the server-side stack should be free to be entirely java.\nToday zipkin-common is the \"interface\" that plays a large part in gluing together the different stack implementations. Whether it is rewritten completely to java, or made java compatible, i have no strong opinion to.\nI agree that we don't need to spend time right now on rewriting existing scala component implementations to the server's stack, where they are isolated, to java. If we have a true polyglot stack then if people want a particular component in java rather than scala they are free to do so, and this will be a addition rather than a rewrite. The question of what developers and the community prefers delegates simply to what is popular and maintained best.\n. +1\nOn 16 July 2015 at 18:25, R. Tyler Croy notifications@github.com wrote:\n\nThe subtree history has been moved here:\nhttps://github.com/openzipkin/browser-extension\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/openzipkin/zipkin/pull/456\nCommit Summary\n- Prune the zipkin-browser-extension code from the core tree\nFile Changes\n- D zipkin-browser-extension/firefox/CONTRIBUTING.md\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-0 (68)\n- D zipkin-browser-extension/firefox/LICENSE.txt\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-1 (202)\n- D zipkin-browser-extension/firefox/README.md\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-2 (37)\n- D zipkin-browser-extension/firefox/ant.properties\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-3 (1)\n- D zipkin-browser-extension/firefox/bootstrap.js\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-4 (141)\n- D zipkin-browser-extension/firefox/build.xml\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-5 (60)\n- D zipkin-browser-extension/firefox/chrome.manifest\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-6 (3)\n- D zipkin-browser-extension/firefox/chrome/content/generateOutput.js\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-7 (158)\n- D zipkin-browser-extension/firefox/chrome/content/getTraceId.js\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-8 (26)\n- D zipkin-browser-extension/firefox/chrome/content/http.js\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-9 (40)\n- D zipkin-browser-extension/firefox/chrome/content/log.js\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-10 (16)\n- D zipkin-browser-extension/firefox/chrome/content/main.js\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-11 (116)\n- D zipkin-browser-extension/firefox/chrome/content/mainOverlay.js\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-12 (9)\n- D zipkin-browser-extension/firefox/chrome/content/mainOverlay.xul\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-13 (4)\n- D zipkin-browser-extension/firefox/chrome/content/options.js\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-14 (88)\n- D zipkin-browser-extension/firefox/chrome/content/options.xul\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-15 (6)\n- D zipkin-browser-extension/firefox/chrome/content/timer.js\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-16 (41)\n- D zipkin-browser-extension/firefox/chrome/content/traces.js\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-17 (78)\n- D zipkin-browser-extension/firefox/chrome/content/translate.js\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-18 (9)\n- D zipkin-browser-extension/firefox/chrome/content/zipkinModule.js\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-19 (30)\n- D zipkin-browser-extension/firefox/chrome/content/zipkinPanel.js\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-20 (88)\n- D\n  zipkin-browser-extension/firefox/chrome/locale/en-US/zipkin.properties\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-21 (11)\n- D zipkin-browser-extension/firefox/chrome/skin/classic/icon.png\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-22 (0)\n- D zipkin-browser-extension/firefox/chrome/skin/classic/icon64.png\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-23 (0)\n- D zipkin-browser-extension/firefox/chrome/skin/classic/loader.gif\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-24 (0)\n- D zipkin-browser-extension/firefox/chrome/skin/classic/zipkin.css\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-25 (150)\n- D zipkin-browser-extension/firefox/defaults/preferences/prefs.js\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-26 (6)\n- D zipkin-browser-extension/firefox/install.rdf\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-27 (33)\n- D zipkin-browser-extension/firefox/zipkin.xpi\n  https://github.com/openzipkin/zipkin/pull/456/files#diff-28 (0)\nPatch Links:\n- https://github.com/openzipkin/zipkin/pull/456.patch\n- https://github.com/openzipkin/zipkin/pull/456.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/456.\n\n\nMick Semb Wever\nScandinavia\nThe Last Pickle\nApache Cassandra Consulting\nhttp://www.thelastpickle.com\n. This can be done with the Repository.java class now.\nSchema creation is automatic and tested.\nI'm thinking with migration it's just about versioning the keyspace name and creating a new keyspace whenever there's an incompatible change.\nSimple migrations for compatible changes can also be achieved (eg \"ALTER TABLE...\")\n. yes. thanks for spotting this. i'll have a fix out in the next hour or so.\n. that would be a separate ticket @yurishkuro \ni looked into that while doing the related pull request, but because there's a blob there i left it for now. in a later pull request i'd like to remove all the blobs so we have a completely transparent schema.\n. @yurishkuro you want to take a look at that pull request for us?\n. i really don't think bucketing will help for service_name.\nand while we get hot partitions there, beyond \"tombstones\" (old writes), each partition only ever has one column. it might be smarter to change the compaction to LeveledCompactionStrategy rather than bucketing. i also added a comment in the code regarding caching writes, since this table is going to see basically just duplicated writes going to it.\n. noooooo, where? :-) \n(might be easier to write those comments against the code lines in the PR)\n. it's merged\n. the service_span_name_index table doesn't need buckets i believe (and didn't previously).\nthe combination of service and span is enough to avoid hot partitions.\nyou are right about splitting service and span columns. i will tackle that in a subsequent PR.\n. > Otherwise, as @abesto said, people will be\n\nvery surprised to find they can't read their writes.\n\nHow would people know?\nArn't the writes we're talking about here coming from an event system, and the system at large is already asynchronous?\nI think trying to make the store strictly consistent is a bit of a pointless exercise here.\n\nI'd suggest changing this code to subclass CassandraSpanStore upon\ninitialization. Override setttl with a method that calls super then has the\nblocking loop before returning. This allows the test to stay in tact (ex\nnot subject to copy/paste drift).\n\nMy scala remains too young to accomplish that.\nCould you provide an example?\n. >  people will be very surprised to find they can't read their writes.\nmaybe you mean by \"people\" the zipkin developers rather the zipkin users?\nif so, is there any reason to not presume that all stores are eventually consistent, since that would be a) the design of the system we're working with at large, and b) beneficial to performance as it will prevent read-before-write anti-patterns creeping into the code.\ni don't know\u2026 it's a more involved discussion than that in regards to the user.\nand yes it would have been nice if the Repository.storeSpan(..) returned a java future that was wrapped into a scala/twitter future.\n. thanks @yurishkuro !\n. LGTM   +1\n. I'll add the flag then... :)\n. flag (just a simple system variable to begin with) added with https://github.com/thelastpickle/zipkin/commit/2f5ac5753e30ac319a3b5f2de2972a89ea4b2839\n(will squash to this PR)\n. two small fixes coming for the hour field and https://github.com/thelastpickle/zipkin/commit/2f5ac5753e30ac319a3b5f2de2972a89ea4b2839#commitcomment-12938413\n. https://github.com/thelastpickle/zipkin/commit/efbed5b6684dc54d37197432789401aa55cc714d\naddresses the field and property naming.\nit's been squashed into this PR, with the commit msg given a title.\n. we're tripping up on https://issues.apache.org/jira/browse/CASSANDRA-6722 here\n. the stacktrace from the UI isn't particularly helpful. getting the matching stacktrace from the query service logs would be more useful.\n. :+1: \n. > the query to get all service names in Cassandra needs to hit every node in the cluster. \nbest to increase RF of the zipkin keyspace.\n(answers more to the availability than performance)\n\nI think we should solve this at the HTTP abstraction, ex. sending cache headers back, so that the client doesn't call so much.\n\n:+1: \n. :+1: \n. :+1: \n. very nice.\n. nice work @yurishkuro \n. :+1: \n. Superseded by https://github.com/openzipkin/zipkin/pull/1252\n. I've pushed again. Still WIP.\nHave collapsed commits into one for brevity sake.\nCode changes are still on top of existing classes. Will move everything out to a separate cassandra-3 storage component once everything starts to work and test.\nCassandraSpanConsumer and CassandraSpanStore have been rewritten (but not tested).\nCompositeIndexer, DeduplicatingExecutor, Indexer, InsertTraceIdByServiceName, InsertTraceIdBySpanName, TimestampCodec, and corresponding tests, have been removed.\n. > This change reminded me of the 128bit trace id concern which pops up quite often. If there's support to switch towards that, I'd suggest changing the trace id lookup fields to be something big enough to hold 128bits.\nThe schema, and the consumer and store classes have been updated to  use variable length integers (java's BigInteger).\n. > just made the java packages \"zipkin.storage.cassandra3\" which will allow us to define STORAGE_TYPE=cassandra3 and also have both jars in the classpath without overlapping\nthanks!\nchecking out what you've done so far\u2026\n. i returned the code to use the MappingManager approach. i had tried to do the custom codecs like you'd done without luck. i think that should be added after the PR once we have it working.\ni did move the UDT annotated classes into a separate \"zipkin3_udts\" keyspace though, to keep things clean in the interim.\n. I agree regarding udt, but don't think it should block the pr. Cassandra3\nshould be marked experimental as well to begin with.\nOnce we're fixed it then I'll raise an issue /PR with driver.\nOn 5 Sep 2016 6:27 pm, \"Adrian Cole\" notifications@github.com wrote:\n\ncircleci fail is because its cassandra version isn't recent enough.. will\nfix\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1252#issuecomment-244829518,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAiJVHzWC88L6VxrAHGQtiyWuWeGg8VXks5qnMGbgaJpZM4Jrhum\n.\n. >  if we go with 3.9, there's no stable download, so docker needs to point to a snapshot. some might complain, but it does work.\n\nI would like to see a data model off 3.9\nThat was the purpose of this PR. Something off 3.0.8 is an interim model that doesn't interest me, and I'm not keen in introducing a third C* model.\nThe C* cluster used should be separated from production, and the datamodel will be initially released as Beta, with users free to use the older model. We are not forcing the users to use vanilla Cassandra or a newer version. But are giving them that option while focusing our support against only two models.\nI can chase up a proper download binary. I'm not sure why 3.9 is not on apache.org\n\nregardless of version, the dependencies job needs to be rewritten. I can help with that.. looks like 4-8hrs of work.\n\nI'll be keen to look into this. For other reasons I've keen to look into and understand this code.\n\nI'd like to not install a fake keyspace just to satisfy the @UDT annotation processor. I didn't have time to look into it, but if we merge without this, I'll create a separate issue to track getting rid of that.\n\nWe agree on this. \n. >  if we go with 3.9, there's no stable download, so docker needs to point to a snapshot. some might complain, but it does work.\nI would like to see a data model off 3.9\nThat was the purpose of this PR. Something off 3.0.8 is an interim model that doesn't interest me, and I'm not keen in introducing a third C* model.\nThe C* cluster used should be separated from production, and the datamodel will be initially released as Beta, with users free to use the older model. We are not forcing the users to use vanilla Cassandra or a newer version. But are giving them that option while focusing our support against only two models.\nI can chase up a proper download binary. I'm not sure why 3.9 is not on apache.org\n\nregardless of version, the dependencies job needs to be rewritten. I can help with that.. looks like 4-8hrs of work.\n\nI'll be keen to look into this. For other reasons I've keen to look into and understand this code.\n\nI'd like to not install a fake keyspace just to satisfy the @UDT annotation processor. I didn't have time to look into it, but if we merge without this, I'll create a separate issue to track getting rid of that.\n\nWe agree on this. \n. > As someone who identifies as a sysop-when-needed, using an unreleased version of a database makes me very uncomfortable.\n@abesto Cassandra-3.9 is a released version. It is not an unreleased version.\n@adriancole 3.8 and 3.9 are the same thing, 3.9 just has some extra bug fixes. Both should work with this PR, both are released.\n. > As someone who identifies as a sysop-when-needed, using an unreleased version of a database makes me very uncomfortable.\n@abesto Cassandra-3.9 is a released version. It is not an unreleased version.\n@adriancole 3.8 and 3.9 are the same thing, 3.9 just has some extra bug fixes. Both should work with this PR, both are released.\n. > I can chase up a proper download binary. I'm not sure why 3.9 is not on apache.org\n\n@abesto Cassandra-3.9 is a released version. It is not an unreleased version.\n\nOk, my bad. 3.8 was cut and not released. 3.9 will be released any day now.\nMy point remains the same, while we have a working safe version to support in Cassandra-2.2, I would like to focus on only one additional data model that is optimal.\n. > I can chase up a proper download binary. I'm not sure why 3.9 is not on apache.org\n\n@abesto Cassandra-3.9 is a released version. It is not an unreleased version.\n\nOk, my bad. 3.8 was cut and not released. 3.9 will be released any day now.\nMy point remains the same, while we have a working safe version to support in Cassandra-2.2, I would like to focus on only one additional data model that is optimal.\n. I remain in favour for sticking with only 3.9+ support for this new data model.\nLet me explain :-)\nWhile I have no objection to adding support so that it works on both 3.0.8 and 3.9, by optionally applying the SASI. I don't get why you want to. We already support an older versions (ie Cassandra-2.2), so there simply isn't any need to provide interim support for Cassandra-3.0.x. This is not a PR that's discussing or proposing that we drop the Cassandra-2.2 datamodel.\nSo we're both looking at this as \"how do we minimize support and complexity\" but from very different angles.\nIf this PR only targets 3.9+ \n- then the code is simpler\n- there's less data-models out there that we have to support,\n- the adoption of the new 3.9 data-model (or migration from the 2.2 datamodel) will be more gradual,  and from only vanilla Cassandra installations,\n- includes Cassandra that can do self-tracing (just a bonus thrown in really).\nThese things make it easier to support it. Significantly easier if you ask me. So we're arguing for the same goal (i believe) in completely opposite directions. Honestly I had thought about the optional SASI approach so that we could target 3.0.8, but it just adds complexity, and seems a premature thing to do at this stage since Cassandra-2.2 remains fully supported. I really think that it's advantageous to avoid pushing all those Cassandra-2.2 users as quickly as possible onto this. If it ain't broke don't fix it, which gives us time to mature such a big change appropriately and at the same time provides a lot of clean horizon for the new data model.\n\nrewrite the dependency graph\n   redo docker to support this\n   support users when they come in asking about how this works\n\nI am happy to take responsibility for these things. I will ask for some help on the docker i'm sure. And you'll need to grab me when users need help, because I don't constantly read gitter. But grab me and I will help those in need.\n. I remain in favour for sticking with only 3.9+ support for this new data model.\nLet me explain :-)\nWhile I have no objection to adding support so that it works on both 3.0.8 and 3.9, by optionally applying the SASI. I don't get why you want to. We already support an older versions (ie Cassandra-2.2), so there simply isn't any need to provide interim support for Cassandra-3.0.x. This is not a PR that's discussing or proposing that we drop the Cassandra-2.2 datamodel.\nSo we're both looking at this as \"how do we minimize support and complexity\" but from very different angles.\nIf this PR only targets 3.9+ \n- then the code is simpler\n- there's less data-models out there that we have to support,\n- the adoption of the new 3.9 data-model (or migration from the 2.2 datamodel) will be more gradual,  and from only vanilla Cassandra installations,\n- includes Cassandra that can do self-tracing (just a bonus thrown in really).\nThese things make it easier to support it. Significantly easier if you ask me. So we're arguing for the same goal (i believe) in completely opposite directions. Honestly I had thought about the optional SASI approach so that we could target 3.0.8, but it just adds complexity, and seems a premature thing to do at this stage since Cassandra-2.2 remains fully supported. I really think that it's advantageous to avoid pushing all those Cassandra-2.2 users as quickly as possible onto this. If it ain't broke don't fix it, which gives us time to mature such a big change appropriately and at the same time provides a lot of clean horizon for the new data model.\n\nrewrite the dependency graph\n   redo docker to support this\n   support users when they come in asking about how this works\n\nI am happy to take responsibility for these things. I will ask for some help on the docker i'm sure. And you'll need to grab me when users need help, because I don't constantly read gitter. But grab me and I will help those in need.\n. > @michaelsembwever pointed out a place that implies a full table scan unless SASI is used \n\n(obviously something I'd have preferred pointed out well before this point in the conversation!).\n\nThanks for the merge Adrian. My apologies for not formalising the technical objections/limitations clearer/better. It was unfortunately a gut feeling that we'd have a lot of trouble implementing a good model on anything pre-3.9. And my intentions for this PR were to aim for a really strong data model that carries the future of Zipkin. Existing problems and headaches of support with the current supported data model understandably gave this PR a different dimension. Many do consider the supported data model in fact to be broken.\nWith this PR merged next steps are:\n- get rid of custom keyspace for UDTs  \u2013 https://github.com/openzipkin/zipkin/issues/1276\n:checkered_flag:  docker - https://github.com/openzipkin/docker-zipkin/pull/111\n- dependencies \u2013 https://github.com/openzipkin/zipkin-dependencies/issues/44\n- email dev with further information\n:checkered_flag:  update links when 3.9 is released (waiting on https://issues.apache.org/jira/browse/CASSANDRA-11195 )\n- keep an open-eye on if/how the new data model can be used against Cassandra 3.0.x, and if a strategy opens for Zipkin to move users onto a less broken data model sooner rather than later.\n. The idea of light grey area on each side of a span indicating the network time to and fro that call would be very cool. (The white circles are best reserved for non core annotations imho).\n. This looks like a good idea. But I don't have much experience on how annotation search works (and is typically used) in the UI layer.\n. > Either our SpanStoreTest isn't good enough to show this doesn't work, or it works and it is a far more elegant index. Regardless, I'm inclined to go with it until someone screams.\nI've tested it through the UI, and it does work well (making search more usable imho), and significantly reduces the index size on disk.\ni'll restore the set.\n. This is as much an experiment and alternative approach as an improvement.\nA point for discussion, if you like.\nWhether it is merged or not I have no care about.\n. Basic thoughts and a skeleton for using Zipkin with the new style of Cassandra datamodel, but replacing the SASI with Solr Cores. This allows the data model to run against Cassandra-3.0.8+ but does require a DSE installation.\nA fair bit of work still has to happen here, wiring it together, etc.\nBut the basic idea is in place.\n. Basic thoughts and a skeleton for using Zipkin with the new style of Cassandra datamodel, but replacing the SASI with Solr Cores. This allows the data model to run against Cassandra-3.0.8+ but does require a DSE installation.\nA fair bit of work still has to happen here, wiring it together, etc.\nBut the basic idea is in place.\n. Personally I would rather not include the code into master, for the same reasons you describe @adriancole \nBut I did want to at least throw the code out here, so that others could take it (if only the idea) and run if they wanted to.\n. Personally I would rather not include the code into master, for the same reasons you describe @adriancole \nBut I did want to at least throw the code out here, so that others could take it (if only the idea) and run if they wanted to.\n. Yes it is the same schema as far as writes go. Just that the SASI are not used. DSE Solr Cores replace them.\n. Yes it is the same schema as far as writes go. Just that the SASI are not used. DSE Solr Cores replace them.\n. > Is the DSE Solr interface so much different when starting Cassandra with the parameters to turn on string indexing using ES or Solr \u2026\n@schrepfler i'm only familiar with the rather basic approach of using dsetool create_core \u2026 from the command line to create the Solr Core off a C* table.\n. > Is the DSE Solr interface so much different when starting Cassandra with the parameters to turn on string indexing using ES or Solr \u2026\n@schrepfler i'm only familiar with the rather basic approach of using dsetool create_core \u2026 from the command line to create the Solr Core off a C* table.\n. :+1: \n. :+1: \n(notifications from openzipkin/zipkin are still missing from my inbox. this has been an ongoing headache. second attempt let's hope i've fixed it. my plans were to always be available for cassandra related issues. this is tricky as i'm not paid to do zipkin work, and some weeks with clients can get crazy, but i aim to at worse be in the loop, and at best be assignee.)\n. :+1: \n. :+1: \n. @longinhit it's supported in the cassandra3 storage, if you're able to upgrade.\n. @longinhit it's supported in the cassandra3 storage, if you're able to upgrade.\n. Problem with ulimit ?\n. Problem with ulimit ?\n. Span names in the Cassandra3 storage are stored within the trace_by_service_span table. The manual denormalisation of the span name list to a separate table no longer exists, instead it is implicitly denormalised via a materialized view to the trace_by_service table.\nThe UI calls store.getSpanNames(serviceName) for a given serviceName and this does the cql query: SELECT span_name FROM trace_by_service where service_name = '<service_name>' LIMIT 100;\nThe materialized view doesn't actually need to store blank span_names (the origin table trace_by_service_span does though). Unfortunately materialized views can not yet filter out non-blank strings yet, they can only filter out the non-nulls.\nThe only approach valid that I can see is to filter out blank span_names at runtime time in the cassandraSpanStore.getSpanNames(serviceName) method. This is a shitty solution, but I suspect it'll be better solved in future C* versions.\n(( It might also be worth investigating performance differences when the bucket cluster key in the materialized view is defined after the span_name clustering key. ))\n. Span names in the Cassandra3 storage are stored within the trace_by_service_span table. The manual denormalisation of the span name list to a separate table no longer exists, instead it is implicitly denormalised via a materialized view to the trace_by_service table.\nThe UI calls store.getSpanNames(serviceName) for a given serviceName and this does the cql query: SELECT span_name FROM trace_by_service where service_name = '<service_name>' LIMIT 100;\nThe materialized view doesn't actually need to store blank span_names (the origin table trace_by_service_span does though). Unfortunately materialized views can not yet filter out non-blank strings yet, they can only filter out the non-nulls.\nThe only approach valid that I can see is to filter out blank span_names at runtime time in the cassandraSpanStore.getSpanNames(serviceName) method. This is a shitty solution, but I suspect it'll be better solved in future C* versions.\n(( It might also be worth investigating performance differences when the bucket cluster key in the materialized view is defined after the span_name clustering key. ))\n. > Another problem with the materialized view is that it doesn't return a distinct list of span names. In other words it would have the same problem with non blank span names in that it will only return the first N which might not include all of the non blank names either.\nThat's correct. The suggestion was to ensure you included enough non-blank names.\nThe manual denormalisation over a materialized view works around the limitation nicely though, will check it out.\n. Possible approaches are \n1) SASI based\n```\nCREATE TABLE IF NOT EXISTS zipkin3.traces (\n    trace_id            varint,\n    trace_id_low        bigint    STATIC,\n    ts_uuid             timeuuid,\n    id                  bigint,\n    ts                  bigint,\n    span_name           text,\n    parent_id           bigint,\n    duration            bigint,\n    annotations         list>,\n    binary_annotations  list>,\n    all_annotations     text, //-- can't do SASI on set: comma-joined until CASSANDRA-11182\n    PRIMARY KEY (trace_id, ts_uuid, id)\n)\n    WITH CLUSTERING ORDER BY (ts_uuid DESC)\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.TimeWindowCompactionStrategy'}\n    AND default_time_to_live =  604800;\nCREATE CUSTOM INDEX ON zipkin3.traces (trace_id_low) USING 'org.apache.cassandra.index.sasi.SASIIndex'\n   WITH OPTIONS = {'mode': 'SPARSE'};\n```\nor\n2) clustering key based\nCREATE TABLE IF NOT EXISTS zipkin3.traces (\n    trace_id_low        bigint,\n    trace_id            varint,\n    ts_uuid             timeuuid,\n    id                  bigint,\n    ts                  bigint,\n    span_name           text,\n    parent_id           bigint,\n    duration            bigint,\n    annotations         list<frozen<annotation>>,\n    binary_annotations  list<frozen<binary_annotation>>,\n    all_annotations     text, //-- can't do SASI on set<text>: comma-joined until CASSANDRA-11182\n    PRIMARY KEY (trace_id_low, ts_uuid, trace_id, id)\n)\n    WITH CLUSTERING ORDER BY (ts_uuid DESC)\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.TimeWindowCompactionStrategy'}\n    AND default_time_to_live =  604800;\n. Possible approaches are \n1) SASI based\n```\nCREATE TABLE IF NOT EXISTS zipkin3.traces (\n    trace_id            varint,\n    trace_id_low        bigint    STATIC,\n    ts_uuid             timeuuid,\n    id                  bigint,\n    ts                  bigint,\n    span_name           text,\n    parent_id           bigint,\n    duration            bigint,\n    annotations         list>,\n    binary_annotations  list>,\n    all_annotations     text, //-- can't do SASI on set: comma-joined until CASSANDRA-11182\n    PRIMARY KEY (trace_id, ts_uuid, id)\n)\n    WITH CLUSTERING ORDER BY (ts_uuid DESC)\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.TimeWindowCompactionStrategy'}\n    AND default_time_to_live =  604800;\nCREATE CUSTOM INDEX ON zipkin3.traces (trace_id_low) USING 'org.apache.cassandra.index.sasi.SASIIndex'\n   WITH OPTIONS = {'mode': 'SPARSE'};\n```\nor\n2) clustering key based\nCREATE TABLE IF NOT EXISTS zipkin3.traces (\n    trace_id_low        bigint,\n    trace_id            varint,\n    ts_uuid             timeuuid,\n    id                  bigint,\n    ts                  bigint,\n    span_name           text,\n    parent_id           bigint,\n    duration            bigint,\n    annotations         list<frozen<annotation>>,\n    binary_annotations  list<frozen<binary_annotation>>,\n    all_annotations     text, //-- can't do SASI on set<text>: comma-joined until CASSANDRA-11182\n    PRIMARY KEY (trace_id_low, ts_uuid, trace_id, id)\n)\n    WITH CLUSTERING ORDER BY (ts_uuid DESC)\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.TimeWindowCompactionStrategy'}\n    AND default_time_to_live =  604800;\n. Approach (2) would mean always querying by the low-bits (trace_id_low), and filtering out rows that have a non-matching (64 or 128-bit) trace_id. This is an effective approach during the transitional mixed stage.\nApproach (1) relies on the sparse b-tree index. Using sparse mode should help it be effective during the transitional mixed stage but depends on low cardinality of 128-bit id 64-bit id. When the transitional mixed stage is over then the trace_id_low column and the index can be dropped without loosing any data. There's going to be some overhead in the number of queries having to execute during the transitional mixed stage (the trade-off compared to having to filter out the non-matching trace_ids in (1)).\n. Approach (2) would mean always querying by the low-bits (trace_id_low), and filtering out rows that have a non-matching (64 or 128-bit) trace_id. This is an effective approach during the transitional mixed stage.\nApproach (1) relies on the sparse b-tree index. Using sparse mode should help it be effective during the transitional mixed stage but depends on low cardinality of 128-bit id 64-bit id. When the transitional mixed stage is over then the trace_id_low column and the index can be dropped without loosing any data. There's going to be some overhead in the number of queries having to execute during the transitional mixed stage (the trade-off compared to having to filter out the non-matching trace_ids in (1)).\n. A third approach, 3) is to store the trace under two IDs without changing the schema.\nThese two IDs are the original 128-bit trace_id along with the low-64-bit trace_id.\n. A third approach, 3) is to store the trace under two IDs without changing the schema.\nThese two IDs are the original 128-bit trace_id along with the low-64-bit trace_id.\n. > we'd assume 3) doubles the storage right?\nKinda. Zero-high-bit 128-bit trace ids are the same as their 64-bit counterparts and therefore would not be stored twice.\n. > we'd assume 3) doubles the storage right?\nKinda. Zero-high-bit 128-bit trace ids are the same as their 64-bit counterparts and therefore would not be stored twice.\n. What's the verdict here? Happy to code if direction is agreed\u2026\n. What's the verdict here? Happy to code if direction is agreed\u2026\n. @adriancole \nThinking more about this, with the code open in my editor, I'm in favour of (3).\nThe simplicity of (3) is definitely there, and the extra rows (the extra storage required) only happen when true 128-bit IDs are consumed.\nIf also leaves us in the position where the schema doesn't need to change from migration to final phase.\nCheckout the following\u2026 https://github.com/openzipkin/zipkin/tree/mck/cassandra-128-bit\nBut it does not provide the forward-compatibility of query-api/UI to storage, as dictated by the CassandraSpanStoreTest#getTrace_retrieves128bitTraceIdByLower64Bits and  CassandraSpanStoreTest#getTrace_retrieves128bitTraceIdByLower64Bits_mixed` tests. \n. @adriancole \nThinking more about this, with the code open in my editor, I'm in favour of (3).\nThe simplicity of (3) is definitely there, and the extra rows (the extra storage required) only happen when true 128-bit IDs are consumed.\nIf also leaves us in the position where the schema doesn't need to change from migration to final phase.\nCheckout the following\u2026 https://github.com/openzipkin/zipkin/tree/mck/cassandra-128-bit\nBut it does not provide the forward-compatibility of query-api/UI to storage, as dictated by the CassandraSpanStoreTest#getTrace_retrieves128bitTraceIdByLower64Bits and  CassandraSpanStoreTest#getTrace_retrieves128bitTraceIdByLower64Bits_mixed` tests. \n. please hold @adriancole \nwould like to check this.\n. This looks good. It's a shame that MV didn't work out, but i can't see a way around it compared to the simplicity of a manually denormalised table.\nAm updating the stress yaml files appropriately for the schema change.\nLooking into compatibility too. I see no incompatibility, although the MV should be dropped, and make sure the table gets created within an existing keyspace.\n. ## Stress results\nVery rough runs on a dual-core laptop.\n15k partitions entered into each table over 5 minutes, in parallel.\nStress yaml profiles were used from the project according to their documentation.\nWrite Only\nExisting\ntraces\nLatency mean              :    2.9 ms [insert: 2.9 ms]\nLatency median            :    2.1 ms [insert: 2.1 ms]\nLatency 95th percentile   :    5.9 ms [insert: 5.9 ms]\nLatency 99th percentile   :   28.5 ms [insert: 28.5 ms]\n```\ntrace_by_service_span\nLatency mean              :    3.6 ms [insert: 3.6 ms]\nLatency median            :    0.8 ms [insert: 0.8 ms]\nLatency 95th percentile   :   12.2 ms [insert: 12.2 ms]\nLatency 99th percentile   :   43.4 ms [insert: 43.4 ms]\n```\nChanges here\ntraces\nLatency mean              :    4.4 ms [insert: 4.4 ms]\nLatency median            :    3.0 ms [insert: 3.0 ms]\nLatency 95th percentile   :    8.6 ms [insert: 8.6 ms]\nLatency 99th percentile   :   35.1 ms [insert: 35.1 ms]\nspan_name_by_service\nLatency mean              :    2.1 ms [insert: 2.1 ms]\nLatency median            :    0.6 ms [insert: 0.6 ms]\nLatency 95th percentile   :    3.5 ms [insert: 3.5 ms]\nLatency 99th percentile   :   30.7 ms [insert: 30.7 ms]\ntrace_by_service_span\nLatency mean              :    1.9 ms [insert: 1.9 ms]\nLatency median            :    0.8 ms [insert: 0.8 ms]\nLatency 95th percentile   :    2.8 ms [insert: 2.8 ms]\nLatency 99th percentile   :   29.8 ms [insert: 29.8 ms]\nWrite&Read\nExisting\ntraces\nLatency mean              :    1.8 ms [by_annotation: 2.0 ms, by_trace: 1.1 ms, by_trace_ts_id: 1.2 ms, insert: 2.9 ms]\nLatency median            :    1.0 ms [by_annotation: 1.3 ms, by_trace: 0.5 ms, by_trace_ts_id: 0.5 ms, insert: 2.2 ms]\nLatency 95th percentile   :    4.8 ms [by_annotation: 4.5 ms, by_trace: 3.5 ms, by_trace_ts_id: 3.5 ms, insert: 7.4 ms]\nLatency 99th percentile   :   14.5 ms [by_annotation: 15.4 ms, by_trace: 10.6 ms, by_trace_ts_id: 12.0 ms, insert: 17.7 ms]\ntrace_by_service_span\nLatency mean              :    1.6 ms [by_duration: 3.0 ms, insert: 0.9 ms, select: 0.8 ms]\nLatency median            :    0.7 ms [by_duration: 2.4 ms, insert: 0.6 ms, select: 0.5 ms]\nLatency 95th percentile   :    4.4 ms [by_duration: 6.0 ms, insert: 1.4 ms, select: 1.2 ms]\nLatency 99th percentile   :    9.2 ms [by_duration: 15.5 ms, insert: 5.9 ms, select: 4.2 ms]\nChanges here\ntraces\nLatency mean              :    3.1 ms [by_annotation: 3.9 ms, by_trace: 1.7 ms, by_trace_ts_id: 1.7 ms, insert: 5.0 ms]\nLatency median            :    1.5 ms [by_annotation: 3.1 ms, by_trace: 1.1 ms, by_trace_ts_id: 1.1 ms, insert: 4.6 ms]\nLatency 95th percentile   :    7.9 ms [by_annotation: 6.6 ms, by_trace: 3.2 ms, by_trace_ts_id: 3.1 ms, insert: 9.9 ms]\nLatency 99th percentile   :   18.9 ms [by_annotation: 18.0 ms, by_trace: 14.2 ms, by_trace_ts_id: 15.2 ms, insert: 23.6 ms]\nspan_name_by_service\nLatency mean              :    2.0 ms [insert: 1.4 ms, select: 3.1 ms, select_span_names: 1.5 ms]\nLatency median            :    1.0 ms [insert: 0.8 ms, select: 2.5 ms, select_span_names: 0.9 ms]\nLatency 95th percentile   :    3.7 ms [insert: 2.4 ms, select: 4.6 ms, select_span_names: 2.2 ms]\nLatency 99th percentile   :   11.7 ms [insert: 8.7 ms, select: 14.1 ms, select_span_names: 10.1 ms]\n```\ntrace_by_service_span\nLatency mean              :    2.6 ms [by_duration: 5.0 ms, insert: 1.3 ms, select: 1.5 ms]\nLatency median            :    1.0 ms [by_duration: 3.8 ms, insert: 0.8 ms, select: 0.9 ms]\nLatency 95th percentile   :    8.0 ms [by_duration: 10.1 ms, insert: 1.5 ms, select: 1.7 ms]\nLatency 99th percentile   :   15.3 ms [by_duration: 20.1 ms, insert: 7.4 ms, select: 10.2 ms]\n``\n. A concern i've had with the cassandra3 schema is the partition keys ofservice_name` in either trace_by_service or span_name_by_service are a bit broad and susceptible to hot spots and wideness.\nThis problem can be addressed, if need be, on the schema proposed here by re-introducing the DeduplicatingExecutor (see the previous cassandra storage code).\n. The stress tests are interesting in that they (potentially) show that the schema proposed here is slower than the existing materialised view. I did not expect this. I wonder if some of the complexity and its latency to the materialised view is hidden from these results. I have no objections to accepting the new schema and any (potentially) small lost in performance.\n. I pushed my commit to this branch containing the updates to the stress yaml profiles.\nI didn't actually mean to push it to your fork. I had no idea I could do that\u2026 @llinder \n. > I will review the DeduplicatingExecutor code\u2026\nLet's leave the DeduplicatingExecutor out for now. \nFeel free to squash my additional commit into your commit, I would appreciate having just one commit in the branch before merging.\n\nRegarding the schema change. Is the preference to create a new CQL that contains the drop/add statements?\n\nCheck out how the Schema class in the old cassandra storage dealt with upgrades.\n. > One of the reasons we merged earlier was under the assumption that this is experimental. \n+1\n. I'm not too sure what happened with this merge, but it did not include my commit.\n. Re-created in https://github.com/openzipkin/zipkin/pull/1392\n. @llinder the commit is still authored by you\u2026 so please check it and say if you're ok with it.\n. Can we close this in favour of https://github.com/openzipkin/zipkin/pull/1401 ?\n. Can we close this in favour of https://github.com/openzipkin/zipkin/pull/1401 ?\n. LGTM, including \n- @adriancole 's comments on README\n- need to add this also to cassandra3\n- squash the commits before merge\n. LGTM, including \n- @adriancole 's comments on README\n- need to add this also to cassandra3\n- squash the commits before merge\n. I'm pretty sure in all versions that TTL is only stored in the cell.\nThe table setting is a default value that gets added to the write. Each column or cell of data contains a timestamp of when it was written and a TTL value.\nTherefore changing the table's TTL setting will have no impact on data already on in C*.\nIt will have an impact on subsequent writes though and @maryoush example is correct. \nYou can reduce gc_grace_seconds though and trigger a user defined compaction on those sstables that then have high tombstone ratios in them. This will help alleviate disk space issues.\nGiven the nature of zipkin's data you may also want to reduce the gc_grace_seconds. There are some consequences to too low a gc_grace_second value, see http://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlCreateTable.html#tabProp__cqlTableGc_grace_seconds ,but for the case of Zipkin where the need for durability isn't critical it probably isn't an issue.. > Each column or cell of data contains a timestamp of when it was written and a TTL value.\n\nTherefore changing the table's TTL setting will have no impact on data already on in C*.\n\nTo elaborate a bit on this\u2026\nYou can't \"change\" TTL of existing data. The data on disk is immutable, so updating data with a new TTL effectively means duplicating it with only the newer version having the new TTL. Further updating old data is a bad idea when using the TimeWindowCompactionStrategy, as it's expecting data written in chronological order (at least not across days).. > note annotation_query would be shorter per span as it only needs to concat annotation and tag pairs (as opposed to also putting in the service name, because the local_endpoint covers this part)\ni don't get this. when we search against that index we still are searching for local_serviceName:annotation or local_serviceName;tag_key;tag_value.\n. > Make sense?\nYup. I've put the extra column in and the extra SASI on it, so to achieve this.\nIt could well be that two SASI indexes working together like this is not faster than having the bigger single SASI. Benchmarking later in the PR will prove that.\nThe branch has been updated, the main code looks roughly in shape, next to do is get the tests compiling and working.. Some experimental benchmarking, aiming only to give rough latency spread using a safe throughput rate.\n\nmacbook pro (macOS 10.13 , 3.5GHz dual-core Intel Core i7 processor, 16GB 2133MHz, SSD)\nCCM (4 nodes, Cassandra-3.11.0)\nccm stress run for each of the three profiles in write mode beforehand\nstress testing ran for 6 hours (overnight), but throttled to very low throughput.\n\nResults:\nccm stress  user profile=trace-stress.yaml ops\\(insert=1,by_trace=1,by_trace_ts_id=1,by_annotation=1\\) no-warmup  duration=6h  -rate threads=4 throttle=50/s\nResults:\nOp rate                   :       50 op/s  [by_annotation: 12 op/s, by_trace: 13 op/s, by_trace_ts_id: 13 op/s, insert: 13 op/s]\nPartition rate            :       50 pk/s  [by_annotation: 12 pk/s, by_trace: 13 pk/s, by_trace_ts_id: 13 pk/s, insert: 13 pk/s]\nRow rate                  :      174 row/s [by_annotation: 137 row/s, by_trace: 13 row/s, by_trace_ts_id: 13 row/s, insert: 13 row/s]\nLatency mean              :   10.1 ms [by_annotation: 31.9 ms, by_trace: 1.8 ms, by_trace_ts_id: 1.8 ms, insert: 5.1 ms]\nLatency median            :    2.5 ms [by_annotation: 23.7 ms, by_trace: 0.6 ms, by_trace_ts_id: 0.7 ms, insert: 3.1 ms]\nLatency 95th percentile   :   43.6 ms [by_annotation: 77.7 ms, by_trace: 6.3 ms, by_trace_ts_id: 6.3 ms, insert: 15.4 ms]\nLatency 99th percentile   :   84.7 ms [by_annotation: 125.1 ms, by_trace: 20.6 ms, by_trace_ts_id: 20.5 ms, insert: 37.5 ms]\nLatency 99.9th percentile :  159.6 ms [by_annotation: 221.4 ms, by_trace: 65.2 ms, by_trace_ts_id: 65.3 ms, insert: 88.3 ms]\nccm stress  user profile=trace_by_service_span-stress.yaml ops\\(insert=1,select=1,by_duration=1\\) no-warmup  duration=6h  -rate threads=4 throttle=50/s\nResults:\nOp rate                   :       50 op/s  [by_duration: 17 op/s, insert: 17 op/s, select: 17 op/s]\nPartition rate            :       33 pk/s  [by_duration: 0 pk/s, insert: 17 pk/s, select: 17 pk/s]\nRow rate                  :       33 row/s [by_duration: 0 row/s, insert: 17 row/s, select: 17 row/s]\nLatency mean              :    2.6 ms [by_duration: 4.0 ms, insert: 1.8 ms, select: 1.9 ms]\nLatency median            :    0.8 ms [by_duration: 1.8 ms, insert: 0.6 ms, select: 0.6 ms]\nLatency 95th percentile   :   11.1 ms [by_duration: 15.9 ms, insert: 7.5 ms, select: 8.1 ms]\nLatency 99th percentile   :   27.9 ms [by_duration: 35.0 ms, insert: 22.6 ms, select: 22.8 ms]\nLatency 99.9th percentile :   73.3 ms [by_duration: 82.4 ms, insert: 64.2 ms, select: 69.9 ms]\nccm stress  user profile=span_by_service-stress.yaml ops\\(insert=1,select=1,select_spans=1\\) no-warmup  duration=6h  -rate threads=4 throttle=50/s\nResults:\nOp rate                   :       50 op/s  [insert: 17 op/s, select: 17 op/s, select_spans: 17 op/s]\nPartition rate            :       50 pk/s  [insert: 17 pk/s, select: 17 pk/s, select_spans: 17 pk/s]\nRow rate                  :    1,707 row/s [insert: 17 row/s, select: 1,673 row/s, select_spans: 17 row/s]\nLatency mean              :    7.5 ms [insert: 1.7 ms, select: 18.9 ms, select_spans: 1.8 ms]\nLatency median            :    0.8 ms [insert: 0.5 ms, select: 11.2 ms, select_spans: 0.5 ms]\nLatency 95th percentile   :   34.3 ms [insert: 7.4 ms, select: 57.1 ms, select_spans: 8.1 ms]\nLatency 99th percentile   :   69.5 ms [insert: 21.5 ms, select: 93.3 ms, select_spans: 22.5 ms]\nLatency 99.9th percentile :  136.4 ms [insert: 63.6 ms, select: 176.8 ms, select_spans: 68.5 ms]\nAs is to be expected the writes are reasonably on-par for the different tables (bc bound by the one piece of hardware).\nAgain as expected reads are fast when selecting traces by their IDs, when finding trace ids by the service+span, and when selecting span names by service name.\nThe three SASI indexes here all perform well, roughly 5x times the latency compared to the fast (select by partition key) reads.\nOf the three SASI indexes reads, searching \"by_annotation\" in the traces table is the slowest, as expected as this actually uses two SASI indexes, one of which is the only SASI index that uses the CONTAINS mode permitting full text searching.. Some experimental benchmarking, aiming only to give rough latency spread using a safe throughput rate.\n\nmacbook pro (macOS 10.13 , 3.5GHz dual-core Intel Core i7 processor, 16GB 2133MHz, SSD)\nCCM (4 nodes, Cassandra-3.11.0)\nccm stress run for each of the three profiles in write mode beforehand\nstress testing ran for 6 hours (overnight), but throttled to very low throughput.\n\nResults:\nccm stress  user profile=trace-stress.yaml ops\\(insert=1,by_trace=1,by_trace_ts_id=1,by_annotation=1\\) no-warmup  duration=6h  -rate threads=4 throttle=50/s\nResults:\nOp rate                   :       50 op/s  [by_annotation: 12 op/s, by_trace: 13 op/s, by_trace_ts_id: 13 op/s, insert: 13 op/s]\nPartition rate            :       50 pk/s  [by_annotation: 12 pk/s, by_trace: 13 pk/s, by_trace_ts_id: 13 pk/s, insert: 13 pk/s]\nRow rate                  :      174 row/s [by_annotation: 137 row/s, by_trace: 13 row/s, by_trace_ts_id: 13 row/s, insert: 13 row/s]\nLatency mean              :   10.1 ms [by_annotation: 31.9 ms, by_trace: 1.8 ms, by_trace_ts_id: 1.8 ms, insert: 5.1 ms]\nLatency median            :    2.5 ms [by_annotation: 23.7 ms, by_trace: 0.6 ms, by_trace_ts_id: 0.7 ms, insert: 3.1 ms]\nLatency 95th percentile   :   43.6 ms [by_annotation: 77.7 ms, by_trace: 6.3 ms, by_trace_ts_id: 6.3 ms, insert: 15.4 ms]\nLatency 99th percentile   :   84.7 ms [by_annotation: 125.1 ms, by_trace: 20.6 ms, by_trace_ts_id: 20.5 ms, insert: 37.5 ms]\nLatency 99.9th percentile :  159.6 ms [by_annotation: 221.4 ms, by_trace: 65.2 ms, by_trace_ts_id: 65.3 ms, insert: 88.3 ms]\nccm stress  user profile=trace_by_service_span-stress.yaml ops\\(insert=1,select=1,by_duration=1\\) no-warmup  duration=6h  -rate threads=4 throttle=50/s\nResults:\nOp rate                   :       50 op/s  [by_duration: 17 op/s, insert: 17 op/s, select: 17 op/s]\nPartition rate            :       33 pk/s  [by_duration: 0 pk/s, insert: 17 pk/s, select: 17 pk/s]\nRow rate                  :       33 row/s [by_duration: 0 row/s, insert: 17 row/s, select: 17 row/s]\nLatency mean              :    2.6 ms [by_duration: 4.0 ms, insert: 1.8 ms, select: 1.9 ms]\nLatency median            :    0.8 ms [by_duration: 1.8 ms, insert: 0.6 ms, select: 0.6 ms]\nLatency 95th percentile   :   11.1 ms [by_duration: 15.9 ms, insert: 7.5 ms, select: 8.1 ms]\nLatency 99th percentile   :   27.9 ms [by_duration: 35.0 ms, insert: 22.6 ms, select: 22.8 ms]\nLatency 99.9th percentile :   73.3 ms [by_duration: 82.4 ms, insert: 64.2 ms, select: 69.9 ms]\nccm stress  user profile=span_by_service-stress.yaml ops\\(insert=1,select=1,select_spans=1\\) no-warmup  duration=6h  -rate threads=4 throttle=50/s\nResults:\nOp rate                   :       50 op/s  [insert: 17 op/s, select: 17 op/s, select_spans: 17 op/s]\nPartition rate            :       50 pk/s  [insert: 17 pk/s, select: 17 pk/s, select_spans: 17 pk/s]\nRow rate                  :    1,707 row/s [insert: 17 row/s, select: 1,673 row/s, select_spans: 17 row/s]\nLatency mean              :    7.5 ms [insert: 1.7 ms, select: 18.9 ms, select_spans: 1.8 ms]\nLatency median            :    0.8 ms [insert: 0.5 ms, select: 11.2 ms, select_spans: 0.5 ms]\nLatency 95th percentile   :   34.3 ms [insert: 7.4 ms, select: 57.1 ms, select_spans: 8.1 ms]\nLatency 99th percentile   :   69.5 ms [insert: 21.5 ms, select: 93.3 ms, select_spans: 22.5 ms]\nLatency 99.9th percentile :  136.4 ms [insert: 63.6 ms, select: 176.8 ms, select_spans: 68.5 ms]\nAs is to be expected the writes are reasonably on-par for the different tables (bc bound by the one piece of hardware).\nAgain as expected reads are fast when selecting traces by their IDs, when finding trace ids by the service+span, and when selecting span names by service name.\nThe three SASI indexes here all perform well, roughly 5x times the latency compared to the fast (select by partition key) reads.\nOf the three SASI indexes reads, searching \"by_annotation\" in the traces table is the slowest, as expected as this actually uses two SASI indexes, one of which is the only SASI index that uses the CONTAINS mode permitting full text searching.. @adriancole ta.\nTwo commits added:\n - changing dependency table to store the individual fields to DependencyLink, rather than the whole list of links as a blob,\n - refactoring zipkin.storage.cassandra3 to zipkin2.storage.cassandra3. @adriancole ta.\nTwo commits added:\n - changing dependency table to store the individual fields to DependencyLink, rather than the whole list of links as a blob,\n - refactoring zipkin.storage.cassandra3 to zipkin2.storage.cassandra3. @adriancole oh, i edited my comment above. both items have been pushed already (and exist as separate commits).. @adriancole oh, i edited my comment above. both items have been pushed already (and exist as separate commits).. LGTM. thanks for a lot of the clean up in the last commit: a few silly bugs and typos there you caught!. LGTM. thanks for a lot of the clean up in the last commit: a few silly bugs and typos there you caught!. >  if a request takes 3ms and parsing takes 100us, then if we have 10k rows we take 6200us vs 6100us (or whatever offset)\nthat's correct :-). all the \"java cql driver\" metrics should be available, since the Zipkin server uses dropwizard metrics (afaik).. thanks @adriancole \nmea culpa for not catching this.. One idea is here is that one could simply and elegantly execute in cqlsh the following:\nDROP INDEX zipkin2.traces_annotation_query_idx ; \nDROP INDEX zipkin2.traces_l_service_idx ;\nand annotation searches in the UI would silently fail (simply returning no result or a simple explanation that such searches have been disabled).\nDropping these indexes can improve write throughput ten or twenty fold. So it's not just about supporting ScyllaDB but also providing an expert mode for those that really want to push how many spans per second they are collecting (eg hundreds of thousands per second ++).. One idea is here is that one could simply and elegantly execute in cqlsh the following:\nDROP INDEX zipkin2.traces_annotation_query_idx ; \nDROP INDEX zipkin2.traces_l_service_idx ;\nand annotation searches in the UI would silently fail (simply returning no result or a simple explanation that such searches have been disabled).\nDropping these indexes can improve write throughput ten or twenty fold. So it's not just about supporting ScyllaDB but also providing an expert mode for those that really want to push how many spans per second they are collecting (eg hundreds of thousands per second ++).. > It is possible we could allow a config to whitelist keys to index (like http.path)\n@adriancole, a whitelist sounds like a good idea to start with.\nbeyond that: the only reason we're using CONTAINS here is because SASI on collections types are not supported. see https://issues.apache.org/jira/browse/CASSANDRA-11182\ni'm thinking the right way to move forward would be to denormalise the annotations like\u2026\n```\nCREATE TABLE IF NOT EXISTS zipkin2.span_by_annotation_query (\n    trace_id            text, // when strictTraceId=false, only contains right-most 16 chars\n    ts_uuid             timeuuid,\n    id                  text,\n    trace_id_high       text, // when strictTraceId=false, contains left-most 16 chars if present\n    annotation_query    text, \n    PRIMARY KEY         (trace_id, ts_uuid, id, annotation_query)\n);\nCREATE CUSTOM INDEX IF NOT EXISTS ON zipkin2.span_by_annotation_query (annotation_query) USING 'org.apache.cassandra.index.sasi.SASIIndex'\n   WITH OPTIONS = {'mode': 'PREFIX'};\n``\nEach annotation would be written toannotation_query, instead of the\u2591` joined string.\nAnd the look-up would then mean in SelectTraceIdsFromSpan\u2026\n\nthe statement would instead be prepared with QueryBuilder.like(\"annotation_query\", bindMarker(\"annotation_query\"))\nand the statement called with  just annotationKey instead of \"%\u2591\" + annotationKey + \"\u2591%\".\n\nThis would allow removing the span.annotation_query column and its CONTAINS SASI.\nA custom TokenizingAnalyzer is another approach, and would be very cool to see.. @llinder ,\n you can now just drop the CONTAINS SASI, a la the annotation query index.\nIf you don't need search by annotation in the UI this is by far the simplest solution.\nthis was done in https://github.com/openzipkin/zipkin/pull/1902. @zuochangan, your token analyzer would be a good patch for Cassandra.\nIf the delimiter character was parameterised, then as a DelimiterTokenAnalyzer i think it would be a patch that would get excepted. And we could then make the use of it permanent in zipkin.. > If the delimiter character was parameterised, then as a DelimiterTokenAnalyzer i think it would be a patch that would get excepted. And we could then make the use of it permanent in zipkin.\nhttps://issues.apache.org/jira/browse/CASSANDRA-14247\nFeel free to offer a review @adriancole and @zuochangan \nAlso the nomenclature, which i'm kinda struggling with.\nAny testing on this would do wonders, if you found time, @zuochangan .. @zuochangan, your work has been committed to Cassandra. It will become available in the next release: Cassandra-3.11.3\nAnd an added win: an interesting suggestion by @mkjellman saw a ~60% performance improvement at write time.. @tramchamploo,\n\nPRIMARY KEY ((service_name, tag_key, tag_value) \u2026\n\nYou're going to get some very hot partition keys there. And there's no way of de-duping it because you want each span_id in the results.\nThis is only going to work if you have reverse clustering on start_time, TWCS, TTL, and queries are time-bound.\nThat said it's a better denormalisation approach over what i suggested above in https://github.com/openzipkin/zipkin/issues/1861#issuecomment-353510778, which in hindsight was a bit of a brain-fart.\n. I notice there's no check that the SASIs have actually been dropped.\nWith indexing=false do you still want the writes to be getting amplified (which will happen unless the SASI are dropped via cql)?. I notice there's no check that the SASIs have actually been dropped.\nWith indexing=false do you still want the writes to be getting amplified (which will happen unless the SASI are dropped via cql)?. Forced push a new version of the branch.\nSolution turned out to be too remarkably simple.. > Maybe we can ship the NPE as a Server 500 error with a substatus that indicates it's due to SASI being dropped? That could eventually be used in the UI to show a sane error message.\nGood idea. A IllegalStateException with an appropriate message is now thrown.. Hi @drolando,\n i'm hesitant about the request here for a number of reasons.\n\n'ConsistencyLevel.ANY' is strongly frowned upon. And I'm unaware of anyone actually using it, so can't speak to it's popularity and stability in the C* codebase.\nRF=1 is not recommended. There's a reason the code logs a warning if RF remains at 1.\n\nFor what you're trying to achieve it would be smarter (imo) to use the Zipkin collector, or the kafka transport, as the buffering mechanism as you perform a rolling restart. ('ConsistencyLevel.ANY' really can't be recommended for such buffering.)\nFor example you could turn off the collector while performing the rolling restart (if insisting on RF=1). Maybe the collector fails gracefully (it does have a retry mechanism in place) and handles the rolling restart already?\nAnyway, because this is a improvement request to accept a situation that breaks two recommendations i'd be uneasy about agreeing to it.. > Will be nice to see the storage impact before and after, and some throughput difference.\nHere you go @adriancole \u2026\nThese are the numbers for three separate benchmarks, each running for an hour with, as much throughput as possible,\n- Cassandra-3.11.2 without the annotations index\n- Cassandra-3.11.3 with the new delimiter SASI annotations index\n- Cassandra-3.11.2 with the previous CONTAINS SASI annotations index\nIt's visible from these that the new delimiter SASI annotations index is almost as fast as with no annotations index, both of which are ~20x as fast as the previous CONTAINS SASI annotations index.\nBoth Cassandra and the stress client ran on the same Lenovo X1 Carbon gen5 (Intel Core i7-5600U Processor, 8Gb RAM) with Ubuntu 17.10.\n```\nCassandra-3.11.2 without the annotations index\n\nop rate                    : 8,997 op/s [by_trace: 692 op/s, by_trace_ts_id: 1,387 op/s, insert: 6,918 op/s]\npartition rate             : 8,997 pk/s [by_trace: 692 pk/s, by_trace_ts_id: 1,387 pk/s, insert: 6,918 pk/s]\nrow rate                   : 8,997 row/s [by_trace: 692 row/s, by_trace_ts_id: 1,387 row/s, insert: 6,918 row/s]\nlatency mean               : 1.4 ms [by_trace: 1.5 ms, by_trace_ts_id: 1.5 ms, insert: 1.4 ms]\nlatency median             : 1.0 ms [by_trace: 1.1 ms, by_trace_ts_id: 1.1 ms, insert: 1.0 ms]\nlatency 95th percentile    : 3.7 ms [by_trace: 3.8 ms, by_trace_ts_id: 3.8 ms, insert: 3.6 ms]\nlatency 99th percentile    : 6.1 ms [by_trace: 6.2 ms, by_trace_ts_id: 6.2 ms, insert: 6.0 ms]\nlatency 99.9th percentile  : 16.3 ms [by_trace: 17.7 ms, by_trace_ts_id: 18.6 ms, insert: 15.9 ms]\nlatency max                : 140.4 ms [by_trace: 140.4 ms, by_trace_ts_id: 121.6 ms, insert: 136.8 ms]\ntotal gc count             : 1,532\ntotal gc memory            : 471.324 GiB\ntotal gc time              : 32.5 seconds\navg gc time                : 21.2 ms\nstddev gc time             : 9.2 ms\nTotal operation time       : 01:00:00\nCassandra-3.11.3 with the new delimiter SASI annotations index\nop rate                    : 7,536 op/s [by_annotation: 580 op/s, by_trace: 577 op/s, by_trace_ts_id: 579 op/s, insert: 5,799 op/s]\npartition rate             : 7,531 pk/s [by_annotation: 575 pk/s, by_trace: 577 pk/s, by_trace_ts_id: 579 pk/s, insert: 5,799 pk/s]\nrow rate                   : 7,586 row/s [by_annotation: 630 row/s, by_trace: 577 row/s, by_trace_ts_id: 579 row/s, insert: 5,799 row/s]\nlatency mean               : 2.1 ms [by_annotation: 3.4 ms, by_trace: 2.1 ms, by_trace_ts_id: 2.2 ms, insert: 2.0 ms]\nlatency median             : 1.4 ms [by_annotation: 2.6 ms, by_trace: 1.4 ms, by_trace_ts_id: 1.5 ms, insert: 1.3 ms]\nlatency 95th percentile    : 5.9 ms [by_annotation: 8.1 ms, by_trace: 5.9 ms, by_trace_ts_id: 5.9 ms, insert: 5.7 ms]\nlatency 99th percentile    : 10.3 ms [by_annotation: 14.6 ms, by_trace: 10.3 ms, by_trace_ts_id: 10.3 ms, insert: 9.8 ms]\nlatency 99.9th percentile  : 24.3 ms [by_annotation: 29.1 ms, by_trace: 24.1 ms, by_trace_ts_id: 24.4 ms, insert: 23.6 ms]\nlatency max                : 368.8 ms [by_annotation: 368.8 ms, by_trace: 157.0 ms, by_trace_ts_id: 116.3 ms, insert: 148.9 ms]\ntotal gc count             : 3,760\ntotal gc memory            : 1165.746 GiB\ntotal gc time              : 70.8 seconds\navg gc time                : 18.8 ms\nstddev gc time             : 7.6 ms\nTotal operation time       : 01:00:00\nCassandra-3.11.2 with the previous CONTAINS SASI annotations index\nop rate                    : 442 op/s [by_annotation: 34 op/s, by_trace: 34 op/s, by_trace_ts_id: 34 op/s, insert: 340 op/s]\npartition rate             : 442 pk/s [by_annotation: 34 pk/s, by_trace: 34 pk/s, by_trace_ts_id: 34 pk/s, insert: 340 pk/s]\nrow rate                   : 445 row/s [by_annotation: 37 row/s, by_trace: 34 row/s, by_trace_ts_id: 34 row/s, insert: 340 row/s]\nlatency mean               : 36.2 ms [by_annotation: 6.8 ms, by_trace: 4.2 ms, by_trace_ts_id: 4.1 ms, insert: 45.5 ms]\nlatency median             : 10.6 ms [by_annotation: 2.4 ms, by_trace: 1.1 ms, by_trace_ts_id: 1.1 ms, insert: 34.9 ms]\nlatency 95th percentile    : 118.0 ms [by_annotation: 31.1 ms, by_trace: 20.6 ms, by_trace_ts_id: 20.2 ms, insert: 125.1 ms]\nlatency 99th percentile    : 167.5 ms [by_annotation: 61.8 ms, by_trace: 47.3 ms, by_trace_ts_id: 47.1 ms, insert: 177.5 ms]\nlatency 99.9th percentile  : 423.6 ms [by_annotation: 114.8 ms, by_trace: 98.3 ms, by_trace_ts_id: 90.4 ms, insert: 470.5 ms]\nlatency max                : 1117.8 ms [by_annotation: 242.6 ms, by_trace: 192.5 ms, by_trace_ts_id: 243.4 ms, insert: 1,117.8 ms]\ntotal gc count             : 23,728\ntotal gc memory            : 7321.556 GiB\ntotal gc time              : 517.7 seconds\navg gc time                : 21.8 ms\nstddev gc time             : 8.5 ms\nTotal operation time       : 01:00:00\n```. And screenshots of the benchmarking graphs.\nOperations per second.\n\nMean latencies.\n\n99th latencies.\n\n. The original benchmarking graph is available here: http://michaelsembwever.github.io/zipkin_1948.html. rebased.. @adriancole Cassandra-3.11.3 is out. . hot dog!. > So instructions for folks is to basically recreate their keyspace right? (or use a different name), correct?\nThe recommended approach would be to drop and create the keyspace.\nAdvanced users could just run zipkin-storage/cassandra/src/main/resources/zipkin2-schema-indexes.cql\nas it drops the index and re-creates it (which will re-index existing data).\nFor example:\ncqlsh -f zipkin-storage/cassandra/src/main/resources/zipkin2-schema-indexes.cql. +1 (and will be humbled to help on the journey). Under the \"Source and Intellectual Property Submission Plan\" section I would add a sentence along the lines of:\n\nAll source code is copyrighted to 'The OpenZipkin Authors', to which the existing core community has the rights to re-assign to the ASF.. It can be narrowed to DESCRIBE TABLE zipkin2.span;\nMostly likely related to the unicode character here used in the SASI. (Confirmed: if the delimiter is changed to any ascii character then cqlsh is happy to describe the span table.)\n@drolando and @amitskatti I'm presuming that Zipkin otherwise is working fine for you?. It's a bug in https://github.com/datastax/python-driver\n in that in metadata.py is trying to unpack the options (to the custom index) without ever first explicitly setting it to unicode, eg .encode('utf-8')\n(for example the different between calling encoder.cql_encode_object(..) and encoder.cql_encode_unicode(..)). have filed an upstream ticket: https://issues.apache.org/jira/browse/CASSANDRA-14632. > In both cases, it might be helpful to reverse-engineer the service-span mapping to re-use the same table. ex PRIMARY KEY ((type, key), value) This could make data management in general easier long term.\nif service-span and tags are collapsed to one table it does make full table scans (eg QueryBuilder.select(\"key\").distinct().from(TABLE__)) more painful\u2026\ni suspect keeping two separate tables is in fact wiser, thanks to @drolando for raising this.. > In both cases, it might be helpful to reverse-engineer the service-span mapping to re-use the same table. ex PRIMARY KEY ((type, key), value) This could make data management in general easier long term.\nif service-span and tags are collapsed to one table it does make full table scans (eg QueryBuilder.select(\"key\").distinct().from(TABLE__)) more painful\u2026\ni suspect keeping two separate tables is in fact wiser, thanks to @drolando for raising this.. Code changes LGTM.\nOne comment though: most of the diff is whitespace/code-style changes. For the reviewer there's a huge waste of time reading diffs that have nothing to do with the actual PR. It would be great if those changes where separated out to a separate follow-up commit (still within the PR). That way by reviewing just the first commit of the PR a lot of time could be saved. . +1\n (was intentionally leaving it until later in the PR lifecycle so the diff on the file remains clear)\n. thanks for spotting that Adrian. \njava7 isn't specified in the build.gradle yet? or is it just my ide not picking it up maybe.\n. done\n. i've left this. i can't think of an intuitive variable name here, hence the comment.\nthe closest variable name i can think of is timestampByTraceId, but i find that not intuitive enough for two lines of code.\n. i played around with .mapValues(_.values) and .flatten and found nothing that worked :-(\ni'm open to working suggestions otherwise.\n. done\n. follow up on this at https://github.com/openzipkin/zipkin/pull/622\n. yeah in this test we know that the test is writing a TTL and that cassandra honours that.\nit's a good catch and i will fix.\n. It's not as bad as it sounds because it only is a problem in the context of tests.\nThe cassandra backend is intentionally eventually consistent here. Writes are written with only a consistency level of one, and so are reads. So if you had increased the replication of the cluster for the zipkin keyspace (something you definitely should do in production) then it's quite likely that there's a few microseconds after a write that if you were to read from a different node you wouldn't get the latest value.\nAnd I can't see any reason in the zipkin datamodel that such truth, or strict consistency is ever required. Therefore the fire-and-forget of the writes is a triviality.\nExcept for the tests. But it's solved here in this pull request.\n. fix with this commit https://github.com/thelastpickle/zipkin/commit/26991109926b2c070b111277e173ecd4a7d7b235\nwhich has been squashed into the pull request.\n. obvious bug there, given we've just slept for 50ms. let me fix.\n. fixed with https://github.com/thelastpickle/zipkin/commit/b518cbbf69dfb97f68328b9e05d5388b66f7f057\nwhich has been squashed into this PR\n. service_name intentionally does not use buckets.\n. any reason we're not using the factory here instead?\n. yeah. i read the diff wrong. sorry about that.\nthese variables should get renamed from cassieXX to cassandraXX someday.\n. yes like that, and override createClusterBuilder() and inside the overridden method you can add credentials from the system properties.\nthat way you don't loose the customisations we make to the default Cluster setup (which can have important behavioural differences).\nultimately though it would make more sense IMO to pass the system variables on to the flags. but i don't know how the flags really work so well\u2026\n. nice!\n. yeah good catch. (i had the field outside to begin with\u2026)\n. fixed with https://github.com/thelastpickle/zipkin/commit/dd3ba49169298eabc9a90ed342de0080ba1de114\n(i'll squash that commit into this PR's one commit)\n. it won't be (afaik) needed.\nno service_span combination will be written more than a handful of times each hour (for each collector jvm running).\nfrom a write per span stored to just a few per hour i do think removes the need for bucketing.\n. > Different installations might use different TTL in Cassandra, in most cases at least 1 day, so cache TTL of 1hr is too fine.\nWe're not needing to match the index ttl value here.\nFor example if the cache ttl matched the index ttl, ie 3 days, then after 3 days all service and service-span names would disappear, despite there having been plenty of spans stored over the past 3 days with these values.\nDescribing it the other way around, with a cache ttl of one hour it means that the service and service-names will disappear after 2 days and 23 hours rather than the index ttl of 3 days.\nSo\u2026 the cache ttl needs to be some small proportion of the index ttl, and i'm thinking that a cache ttl of one hour is certainly big enough in regards to both performance (the reduction of writes) and to the UI (ensuring the service and spans names are available to choose from), and i don't think it's too small (is anyone setting the index ttl so low as just a few hours?)\n\nSecond problem - the TTL is on the whole cache, not on individual entries.\n\nThis isn't a problem. This improvement is not trying to remove duplicate writes, simply improve performance (and remove the need for bucketing).\n\nAlso, thread locals are unnecessary here, a concurrent map shared between threads would do.\n\nconcurrency has overhead, here it is a choice between letting the zipkin jvm handle it or letting cassandra handle it (ie duplicates writes). performance wise, which is the context for this improvement, cassandra is superior at handling the concurrency. hence the thread local approach.\n. the size of the set is a potential concern, and i had considered it, but i can't imagine the number of service and span names within any one hour having such high cardinality.\nif it is a problem one simple way to solve it is just limit the size of the set and let writes spill over.\n (if this were to happen i would re-introduce the bucketing, one reason i left it in the schema)\nbut the easiest approach would be just to bring the cache ttl from one hour down to for example one minute.\n\nI also heard that the Guava cache is super cool and full featured. \n\na guava cache (or a LRUMap) introduces concurrency which as i tried to explain above is just overhead, which i'm explicitly avoiding here.\n\nAnother possible scenario would be getting cache misses if the data is in the Set in another thread.\n\nnot a problem really. a cache miss only means that the write happens. today we have a 100% cache miss and it's \"not a problem\" :-)\n. > Limiting the size sounds good, \u2026\nWhat about solving it with Yuri suggestion?\n\nIn this OSS implementation I'd suggest making this TTL externally configurable.\n\nThen in any setup where the size of the thread-local maps become an issue it is just to set the cache ttl to a smaller value.\n. the field name \"hour\" is misleading\n. +1\n. why did this get moved into the try-catch ?\n. not related to this PR, but is this trailing comment still applicable after https://github.com/openzipkin/zipkin/pull/742 got merged?\n. do you know this to be an actual problem in this use-case?\nyou are talking about very short lived objects\n (it's not like there's lots of different long column names\u2026)\n. I'm aware of the problem and that it affects both heap and sstable size.\nIn the heap it's not so much about space but objection creation/deletion, IIRC.\nAnd in the sstable it is kinda crazy that every column value is labeled with its column name in every single row through the file. (It's a hangup from when Cassandra used thrift and was truly schema-free).\nAnd I've been aware that it's getting addressed between CASSANDRA-8099 and CASSANDRA-7447\nBut those numbers above look surprising for what you say is off a simple schema.\nJust how long were the long column names? And was compression enabled (which usually makes up for the difference)?\nOtherwise it's evidence on sstable size, not performance.\nAnd if we don't have a problem with that (we want to encourage people to scale out rather than scale up) it can well be a better optimisation of developer time to use meaningful column names.\nTL;DR i'd love to hear more around the details of from what your metrics team discovered.\n. table comments can also go into the table's schema, to help developers understand it when using cqlsh.\nFor example, \nCREATE TABLE \u2026 ()\n    WITH comment =' This table \u2026';\nIf using the short column names (which I suspect hasn't any real benefit but rather just overhead on developer productivity) then i would add the column name comments in as well. Transparency of data is gold after all.\n. But what were the original length of the columns names?\nWhat are we actually comparing here, single-char column names vs what?\n\nOnly the n(name) field would have longer strings, others are usually a single number, which become comparable in space usage to column names if those are fully spelled out.\n\nThose columns are bigints, their size are fixed.\nA 5x difference doesn't sound right, given that compression will do a good job with repeated column names in such a small table.\n\nThe x5 size difference may not be directly attributed to column names\u2026\n\nok, this kinda worries me. How many sstables were there in each case? Was there variations in data generated too?\n\n\u2026there are like 10 lines of code that depend on them.\n\nThat's fine if you're coming to it from within the code, but that's not always the case.\n\nBut I am not strongly against changing this to long names.\n\nWhat actually is a long name though? is 5 characters long? there's nothing empirical so far to go off here.\nI would like to get more empirical data down, because otherwise it smells like premature optimisation.\nI would not bother in changing this, but it would be nice to orientate ourselves so we know what practice we should be pushing.\n.  there's something odd there with the tests and how they sometimes invoke with with empty timestamps. i didn't get my head properly around that, rather just accepted zero as a value that won't get adjusted. i think that's correct but am not entirely sure.\n. Indexer is removed.\n. I've removed it, and added it to the tests in zipkin-storage/cassandra-3\nBut it'll be a performance problem for tests, as many of the tests are in SpanStoreTest and SpanCassandraTest.\n. like i'm said in the PR comment, i'm opposed to backporting this PR to 3.0.8\n. don't install the datastax download.\nplease continue with the latest 3.9-SNAPSHOT until it is released (very soon).\nthe correct link is actually http://cassci.datastax.com/job/cassandra-3.9/lastSuccessfulBuild/artifact/build/apache-cassandra-3.9-SNAPSHOT-bin.tar.gz\nI have no objection to this PR waiting until 3.9 is on the official apache.org downloads page before being merged to master. Although I see no reason for it not to go to master and the link get updated in a few days.\nAs I've stated elsewhere I do object to reverting back to 3.0.8.\n. this won't work in any real installation because it is doing a full table scan every request.\nthis approach (using set for annotations in the trace table) would have to use a separate new materialized view. which then becomes a different approach to SASI for annotations as I had originally proposed. my original proposal was the fast approach, and simpler in the long run.\n. but we can\u2026\ni've tested this and it means that searching on non-string binary annotations is possible.\nfeasibility of it aside, has it value?\n. went to list with manual dedupe to keep order. (i think the reason was to have some consistency when reading the data in cqlsh. ie you're always reading it as service, annotations, binaryAnnotations)\n. i have no objection to changing this back to a set.\nam more interested in the higher level ramifications of this PR.\nit does bring us closer to full-text searching.\n. ok\n. note that jvm options are now found in conf/jvm.options\nbut this will still work fine. \n. thanks for opening https://github.com/openzipkin/zipkin/issues/1364\nwill let the discussion flow from there\u2026\n. I'm a bit late to the party on this one.\nWould it not be simpler to call rs.fetchMoreResults() ?\nfetchMoreResults() is an async call, and improves the chances that the next page is ready to go by the time this page is exhausted.\nTaking this approach is simpler as it removes the concurrency/futures from this code, allowing it to be just the single iteration loop. if local_dc isn't defined then LOCAL_ONE sticks requests to the dc of the first seed host that the driver connects to.\nONE will at least in this situation connect to any dc, and then any latency awareness will bring it back to (eventually) using the local dc.\nfurthermore LOCAL_ONE can fail easily if you're not using 'NetworkTopologyStrategybut that's a fix for another day.. added comment. (can you please squash the 4 commits!). in this file i still have to add some comments, and i want to duplicate all comments into the schema's table-level WITH COMMENT = '\u2026';` so that operators get that information too\u2026\n. done.. @llinder @fedj \ni want to change duration to be stored as tens of milliseconds (or in hundredths of seconds).\nthis will make the SASI on this column contains fewer distinct values, improving write performance.\nand i can't see users being so discerning about searches they perform in the UI.\nduration would be rounded up, so the result list would not skip any results. and duration in the span table would remain accurate to microseconds.\nany objections?. added as an extra commit. can be undone if any objections/concerns.. did you mean here \"left-most 16 chars\"?. rs.fetchMoreResult() is both a hint as well as a way to wait on the subsequent page.\nhere it is only being used as a hint: that is to initiate in the background the fetch of the subsequent page while we process the current page. otherwise the fetch for the next page is not initiated until this page becomes exhausted. during the processing of any specific page, multiple calls to rs.fetchMoreResult() will always return the same future. even if we do nothing with the returned future, when the current page is exhausted it will seamlessly move onto the next page as it has been fetched for us in the background (asynchronously).\n2000 is just an appropriate place (through the default of 5000 rows per page) to pre-emptively kick-off the asynchronous fetch of the next page.. the current page is still valid. but there are no subsequent pages.. So FTR: without the pre-emptive asynchronous fetching of the next page, the parsing will stop and block every 5k rows while it fetches the next page. Queries that process more than 5k rows are going to be significantly slower (the network fetch is the slowest part).. line 229 needs to be removed. rounding to centiseconds no longer happens.\n(after further benchmarking it was shown to be a bad idea). if you need to search by span name without service name, you'd do this via the trace_by_service_span table.\nyou'd do this via writing ((\"\", span_name, bucket), timestamp_uuid) rows\u2026\nbut\u2026 take care of hot partitions: if there's not a high enough cardinality on span_name then there's going to be hot partitions on those (\"\",span_name) combinations. hot partitions affect write throughput and latency. it can be addressed by putting in a write cache like the DeduplicatingExecutor.. tests that CassandraSpanStore still works smoothly after dropping the SASI index would be a fantastic addition.\nthis SASI index is the limiting factor in being able to truly scale this backend to Zipkin.\nthis, and to a lesser degree the hot partitions, are the hurdles to making this a million+ writes per second backend (even though i can't see anymore provisioning hundreds of virtual server c* nodes just for the sake of zipkin visibility\u2026)\nin benchmarking i was able to write comfortably 150 spans per second per cpu, over many hours, keeping latency generally in double-digit milliseconds. (cpu isn't really the best scaling base, but for typical cpu-ram-diskIO, it's good enough. this benchmarking was on a laptop). \nwhen i dropped just the one SASI on annotation_query, I could then write comfortably 6k spans per second per cpu, over many hours, keeping latency in single-digit milliseconds.. Retouching on the null binding resulting in tombstone issue.\nHere there's 8 columns here that can be null. (maybe more?)\nIf there's consistently 8 tombstones (nulls) per row, then we'll only need 125 spans in a trace (rows in a partition) to trigger the `tombstone_warn_threshold warnings being logged in the C* nodes. And if we go to 12500 spans in a trace then that whole trace partition would become unreadable. Cassandra warns at a 1000 tombstones in any query, and fails on 100000 tombstones.\nThere's also a small question about disk usage efficiency. Each tombstone is a cell name and basically empty cell value entry stored on disk. Given that the cells are, apart from tags and annotations, generally very small then this could be proportionally an unnecessary waste of disk.\nTo avoid this relying upon a number of variant prepared statements for inserting a span is the normal practice.\nAnother popular practice is to insert those potentially null columns as separate statements (and optionally put them together into UNLOGGED batches). This works as multiple writes to the same partition has little overhead, and here we're not worried about lack of isolation between those writes, as the write is asynchronous anyway. An example of this approach is in the cassandra-reaper project here: https://github.com/thelastpickle/cassandra-reaper/blob/master/src/server/src/main/java/io/cassandrareaper/storage/CassandraStorage.java#L622-L642\nUPDATE: a few lines down, under protected ResultSetFuture newFuture() the nulls are in fact not written (not bound). . the code above avoids writing the nulls (tombstones) :-). it looks like we're missing two queries here:\n - search by timestamp range,\n - search by timestamp range and duration. done.. Only if the user has already used zipkin-storage/zipkin2_cassandra and created the schema.\n(This is the default name of an index created, ie from CREATE \u2026\u00a0INDEX \u2026\u00a0ON zipkin2.span (annotation_query) \u2026)\nAnd if that is the case then the code won't actually be calling this cql file.\nSo this line only has any purpose if the cql file is going to be manually run.\nThis comes back to this question\u2026\n\nupgrade schema approach, or require users to drop keyspace if already using this storage backend\n\nIf we're happy to say anyone already using the new schema is expected to either:\n - manually run cqlsh -f zipkin2-schema-indexes.cql, or\n - drop keyspace and let it be recreated from scratch.\nThen this approach is ok.\nIf not then we need something more along the lines of zipkin-storage/cassandra/src/main/resources/cassandra-schema-cql3-upgrade-1.txt. I was a bit confused when writing this\u2026\nIsn't this type of check done somewhere else in the codebase?. If any of the nodes in the Cassandra cluster are not 3.11.3 then the Zipkin server will crash.\nFor example someone accidently starts Zipkin up when the Cassandra cluster is mid-way through a rolling upgrade to 3.11.3. first question: yes. re-creating it with different mode and analyzer_class\nsecond question: yes it's drops the data. creating the new index will go over the base data and rebuild it into the new index.. I don't know what the cardinality of tags will be? But this won't scale.\nBut, for example, if there's 1 million tags in a Zipkin storage this full-table scan is going to be painful (if not timeout).. ",
    "kmckenney-PayPal": "I realize that I didn't call it out in the original issue description, so maybe it should be addressed in a follow-on, but I believe the support for providing credentials should support the possibility of securely providing the credentials.\nDifferent organizations will certainly have different mechanisms for providing these credentials, so perhaps the implementation should provide an interface for providing credentials. The default implementation may be as simple as the command line option approach used here (and/or securely prompting for credentials).\n. ",
    "noslowerdna": "Rebased.\n. ",
    "brettwooldridge": "@eirslett @noslowerdna @adriancole Thanks for the Cc.\n7pm here in Tokyo.  I'm on the way home, and formulating a more detailed comment.  I'll update in about 4 hours (after sending daughter to dreamland).\n\nThanks for taking a look at HikariCP.  While HikariCP is probably best known for being fast, we have actually spent more effort on achieving that speed within the constraints of providing the highest reliability possible.  We are confident that HikariCP is not slightly more reliable, but substantially more reliable than currently available pools.\nMany (most?) available pools (including DBCP) default to a mode of operation where performance is prioritized over reliability.  In contrast, HikariCP has no \"unsafe\" operational modes -- no way to disable \"correct\" behavior.\nThe benchmark cited on our page is actually extremely generous to other pools.  They are run against a JDBC stub-driver  in which every operation is an empty method.  When a real driver is put into the loop instead, the difference in results begs believability -- but believe them.  We should probably beat this drum more loudly, but...\nSo, what is unfair about the typical comparison?  I'm going to talk about HikariCP, Apache DBCP2, and Tomcat DBCP here; talking some about speed, and then bringing in the reliability pieces.  Running HikariCP-benchmark against the three pools against a real database (MySQL) instead of a stub.\nFirst, all three pools in default configuration (+ autocommit=false):\nBenchmark                                             (jdbcUrl)  (pool)   Mode   Score\nConnectionBench.cycleCnnection  jdbc:mysql://192.168.20.15/test  hikari  thrpt   45289.116  ops/ms\nConnectionBench.cycleCnnection  jdbc:mysql://192.168.20.15/test  tomcat  thrpt    2329.692  ops/ms\nConnectionBench.cycleCnnection  jdbc:mysql://192.168.20.15/test  dbcp2   thrpt      21.750  ops/ms\nDBCP2\nWhat is not visible here is that DBCP2 is generating ~3MB/sec of traffic to the DB, because rollbackOnReturn defaults to true and it is unconditionally rolling back.  It gets bonus points for defaulting on the side of safety.  Unfortunately, it is not validating connections on borrow.\nHikariCP\nHikariCP is generating zero traffic to the DB.  HikariCP also defaults to \"rollback on return\" (it can't be turned off because that is the correct behavior for a pool), but it additionally tracks transaction state and does not rollback if the SQL has already been committed (or no SQL was run).  HikariCP also defaults to \"test on borrow\" (it can't be turned off...), but employs an optimization that says, \"If a connection had activity within the past 1000ms, bypass connection validation.\"\nTomcat\nTomcat DBCP is also generating zero traffic to the DB, but for a different reason.  It simply is not validating connections at all, nor is it rolling back on return.\n\nNow, let's try to level the playing field as a little.  For Tomcat and DBCP, we need to enable connection validation.\nBenchmark                                             (jdbcUrl)  (pool)   Mode   Score\nConnectionBench.cycleCnnection  jdbc:mysql://192.168.20.15/test  hikari  thrpt   45289.116  ops/ms\nConnectionBench.cycleCnnection  jdbc:mysql://192.168.20.15/test  tomcat  thrpt    2133.992  ops/ms\nConnectionBench.cycleCnnection  jdbc:mysql://192.168.20.15/test  dbcp2   thrpt       5.296  ops/ms\nDBCP2\nDBCP2 took a hit here, because it does not have a validation optization like HikariCP.  It is still generating ~3MB/sec of traffic to the DB.\nTomcat\nTomcat DBCP does support a similar optimzation to HikariCP, the config goes something like this:\nsetTestOnBorrow(true)\nsetValidationInterval(1000)\nsetValidator( validator )\nBut we forgot \"rollback on return\" for Tomcat:\nConnectionBench.cycleCnnection  jdbc:mysql://192.168.20.15/test  tomcat  thrpt      20.706  ops/ms\nAnd there goes the performance.  Tomcat is now generating ~3MB/sec of traffic to the DB.  It does not track transaction state and therefore must unconditionally rollback.  I thought maybe enabling \"ConnectionState tracking\" might help, but it does not.\nThere is a lot more that HikariCP is doing, not covered here ... guarding against network partitions, checking SQLExceptions for vendor disconnect codes, resetting auto-commit, transaction isolation, catalog, network timeout, tracking open Statements (and closing them), etc.  All while keeping the performance levels you see above.\n. @eirslett @noslowerdna @adriancole Thanks for the Cc.\n7pm here in Tokyo.  I'm on the way home, and formulating a more detailed comment.  I'll update in about 4 hours (after sending daughter to dreamland).\n\nThanks for taking a look at HikariCP.  While HikariCP is probably best known for being fast, we have actually spent more effort on achieving that speed within the constraints of providing the highest reliability possible.  We are confident that HikariCP is not slightly more reliable, but substantially more reliable than currently available pools.\nMany (most?) available pools (including DBCP) default to a mode of operation where performance is prioritized over reliability.  In contrast, HikariCP has no \"unsafe\" operational modes -- no way to disable \"correct\" behavior.\nThe benchmark cited on our page is actually extremely generous to other pools.  They are run against a JDBC stub-driver  in which every operation is an empty method.  When a real driver is put into the loop instead, the difference in results begs believability -- but believe them.  We should probably beat this drum more loudly, but...\nSo, what is unfair about the typical comparison?  I'm going to talk about HikariCP, Apache DBCP2, and Tomcat DBCP here; talking some about speed, and then bringing in the reliability pieces.  Running HikariCP-benchmark against the three pools against a real database (MySQL) instead of a stub.\nFirst, all three pools in default configuration (+ autocommit=false):\nBenchmark                                             (jdbcUrl)  (pool)   Mode   Score\nConnectionBench.cycleCnnection  jdbc:mysql://192.168.20.15/test  hikari  thrpt   45289.116  ops/ms\nConnectionBench.cycleCnnection  jdbc:mysql://192.168.20.15/test  tomcat  thrpt    2329.692  ops/ms\nConnectionBench.cycleCnnection  jdbc:mysql://192.168.20.15/test  dbcp2   thrpt      21.750  ops/ms\nDBCP2\nWhat is not visible here is that DBCP2 is generating ~3MB/sec of traffic to the DB, because rollbackOnReturn defaults to true and it is unconditionally rolling back.  It gets bonus points for defaulting on the side of safety.  Unfortunately, it is not validating connections on borrow.\nHikariCP\nHikariCP is generating zero traffic to the DB.  HikariCP also defaults to \"rollback on return\" (it can't be turned off because that is the correct behavior for a pool), but it additionally tracks transaction state and does not rollback if the SQL has already been committed (or no SQL was run).  HikariCP also defaults to \"test on borrow\" (it can't be turned off...), but employs an optimization that says, \"If a connection had activity within the past 1000ms, bypass connection validation.\"\nTomcat\nTomcat DBCP is also generating zero traffic to the DB, but for a different reason.  It simply is not validating connections at all, nor is it rolling back on return.\n\nNow, let's try to level the playing field as a little.  For Tomcat and DBCP, we need to enable connection validation.\nBenchmark                                             (jdbcUrl)  (pool)   Mode   Score\nConnectionBench.cycleCnnection  jdbc:mysql://192.168.20.15/test  hikari  thrpt   45289.116  ops/ms\nConnectionBench.cycleCnnection  jdbc:mysql://192.168.20.15/test  tomcat  thrpt    2133.992  ops/ms\nConnectionBench.cycleCnnection  jdbc:mysql://192.168.20.15/test  dbcp2   thrpt       5.296  ops/ms\nDBCP2\nDBCP2 took a hit here, because it does not have a validation optization like HikariCP.  It is still generating ~3MB/sec of traffic to the DB.\nTomcat\nTomcat DBCP does support a similar optimzation to HikariCP, the config goes something like this:\nsetTestOnBorrow(true)\nsetValidationInterval(1000)\nsetValidator( validator )\nBut we forgot \"rollback on return\" for Tomcat:\nConnectionBench.cycleCnnection  jdbc:mysql://192.168.20.15/test  tomcat  thrpt      20.706  ops/ms\nAnd there goes the performance.  Tomcat is now generating ~3MB/sec of traffic to the DB.  It does not track transaction state and therefore must unconditionally rollback.  I thought maybe enabling \"ConnectionState tracking\" might help, but it does not.\nThere is a lot more that HikariCP is doing, not covered here ... guarding against network partitions, checking SQLExceptions for vendor disconnect codes, resetting auto-commit, transaction isolation, catalog, network timeout, tracking open Statements (and closing them), etc.  All while keeping the performance levels you see above.\n. ",
    "NiteshKant": "If this was the case, we would not have written our own custom dapper implementation inside Netflix, so I certainly support the intent here :)\n. I think the try-with-resources is a good case for using java.io.Closeable which makes the code terse so +1 for that! In RxNetty, the primary usage of close is async, so, I couldn't use Closeable.\n. :) I think they both have their uses. I would use async close() for something like a channel, request, response, etc which will be called inline with request processing. For infrequent tasks, a synchronous close is appropriate/sufficient. I do not have insight into zipkin code to have an opinion about specific close() uses.\n. ",
    "b": "This is helpful, but doesn't address the related problem of wanting to avoid having to build and deploy scala components at all. Is there already an issue tracking that? It is something we would like to help with, regardless.\n. I agree, a separate issue is best.\nhttps://github.com/openzipkin/zipkin/issues/463\n. Look like keeping this independent on the Scala traits question was a bit optimistic! My view on that is that it is insufficient and doesn't address what this issue is for.\nAs @jpinner just described, there is a lot of value in a Java backend, as well. I think if we were to prioritize, elimination of client side Scala needs to come first. I don't think that is sufficient, however. The process of producing Java backend components will be helpful in componentizing things such that the ecosystem can support both Java and Scala backends without making it mutually exclusive.\n. ",
    "3thinkthendoit": "hi rtyler, sampleRate on http trace is a bug?\n. here is my java Code.\nTracer tracer = new SamplingTracer(new RawZipkinTracer(flumeIp, port, sr), Float.valueOf(sampleRate));\nWhen i record the data with http filter:\nHttpServletRequest request = (HttpServletRequest) req;      \n    HttpServletResponseWrapper response = new HttpServletResponseWrapper((HttpServletResponse) res);\n    //logger.info(\"ContentType:\"+request.getContentType());\n    long parent = System.currentTimeMillis();\n    SpanId spanId = SpanId.apply(System.nanoTime());\n    TraceId traceId = TraceId.apply(Option.<SpanId> apply(spanId), Option.<SpanId> empty(), SpanId.apply(parent), Option.<Object>apply(Boolean.TRUE));\n    Trace.pushTracerAndSetNextId(tracer, false);\n    //Trace.pushTracerAndSetNextId(tracer,true);\n    Trace.setId(traceId, false);\n    Trace.record(new Annotation.ServerRecv());\n    Trace.recordBinary(\"http.uri\", request.getRequestURI());\n    Trace.recordBinary(\"http.port\", \"\"+request.getLocalPort());\n    //\u8bb0\u5f55\u53c2\u6570\n    for (String param : params)\n    {\n        if (request.getParameter(param) != null)\n            Trace.recordBinary(\"param.\"+param, request.getParameter(param));\n    }\n    //Trace.recordBinaries(arg0);\n    Trace.recordServiceName((\"http-\"+module).toLowerCase());\n    Trace.recordRpc(request.getMethod());       \n    Trace.recordClientAddr(new InetSocketAddress(request.getRemoteAddr(), request.getRemotePort()));\n    Trace.recordServerAddr(new InetSocketAddress(NetUtil.getLocalHost(), request.getLocalPort()));\n    try {\n        chain.doFilter(request, response);\n    } catch (Throwable t) {\n        Trace.recordBinary(\"http.hasError\", \"true\");\n        Trace.recordBinary(\"http.error\", t.getStackTrace());\n        try\n        {\n            throw t;\n        }\n        catch (Throwable e)\n        {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }               \n    }\n    //chain.doFilter(request, response);\n    //Trace.setId(traceId, false);\n    Trace.recordBinary(\"http.status\",\"\"+response.getStatus());\n    Trace.record(new Annotation.ServerSend());\nset the samplerate = 0.0f  the Treace should not record the span . But this recorded .why?\nin the RPC finagle trace  set samplerate = 0.0f ,the record  is closed.\n. oh  use \" package-dist\"\n. ",
    "petemounce": "+1, would find it very useful if Zipkin emitted metrics to allow it to be monitored and alerted on.\n. ",
    "nikhil2406": "\n1. Above screenshot shows user selecting a time range between 23-34 ms.\n\n1. This one shows the span view redrawn/zoomed in according to selected time range. All spans are rearranged to show their respective positions and widths in new time range. The new time range and spans which started in that time range are shown with different color. Also the zoomout button appears with same color scheme.\n\n3. After zoomout button is clicked original span view appears back.\n. \nDone. Please check now.\n. I've kept red color as a common theme to highlight all changes that happened in span view when user selected the zoom-in time range. Red text specifies that the span actually 'started' in selected time range. I think it's going to help users to easily locate the spans they are interested in analyzing.\nI've changed border color of selection-rectangle also to red.\n. Squashed and rebased. Please check now.\n. ",
    "mzagar": "Thanks!\n. I believe I can take a look at this one.\n. I believe I can take a look at this one.\n. Isn't 30 seconds a bit low - are new services created that often? I was thinking about adding a property allowing to set this timeout to a custom value at web startup and setting the default to 5 minutes, is that too long?\nOptimization when not to cache sounds good.\nI'll make a PR fpr the js side later today. I noticed /api/services was removed from available endpoints... should I put that back in?\n. @adriancole please review the CachedRenderer 'thingy'... I was not really sure how to handle that one better (fighting my way through scala)\nOptimization regarding service name count is not implemented yet.\nSuggestions welcome!\n. TODO: write zipkin-query test -- figure out how to setup controller with custom parameters so I can test Cache-Control behaviour on 3 test services...\n. @adriancole I'm stuck writing a test for /api/v1/services that sets Cache-Control header depending on number of services available (for < 5, no Cache-Control set, else cache it); please see my comments here and here.\nCan you help me and point me in the right direction please?\n. ",
    "htaunay": "No problem\n. ",
    "blaines": "Sure no problem I'll have a look!\n. I wasn't able to repro this, but adding the idea plugin is a good thing.\n. Hey there I was looking at this pull request with James and I had some comments. Everything looks to be working here, I just noticed the \"dev\" default set in multiple places. What can be done to improve this a bit is to use the gradle.properties file to set the serviceType default. That will eliminate the need to grab system environment variables. Also another thing that can be done is to use -P which sets a project property that overrides the default. So you could end up with something more like:\n./gradlew :zipkin-collector-service:run -PserviceType=${1}\n(in this case it may be needed to have a default defined here as well, -PserviceType=${1:-dev} so as to not accidentally nullify the value, but I didn't test myself)\ngradle --help\n-P, --project-prop      Set project property for the build script (e.g. -Pmyprop=myvalue).\n. ",
    "vprithvi": "I also have the same problem. I worked around it by adding the gradle idea plugin #532 \n2015-07-30 14:16:33,348 [94548379]   WARN - ge.ModuleDependencyDataService - Can't import module dependencies [ModuleDependencyData: dependency=module 'io.zipkin:zipkin-common:1.2.0-SNAPSHOT'|scope=Compile|exported=false|owner=module 'io.zipkin:zipkin-web:1.2.0-SNAPSHOT', ModuleDependencyData: dependency=module 'io.zipkin:zipkin-scrooge:1.2.0-SNAPSHOT'|scope=Compile|exported=false|owner=module 'io.zipkin:zipkin-web:1.2.0-SNAPSHOT']. Reason: target module (ModuleData: module 'io.zipkin:zipkin-web:1.2.0-SNAPSHOT') is not found at the ide and can't be imported\n2015-07-30 14:16:33,359 [94548390]  ERROR - llij.ide.plugins.PluginManager - null\njava.lang.NullPointerException\n    at org.jetbrains.plugins.scala.project.package$ModuleExt.compilerConfiguration(package.scala:116)\n    at org.jetbrains.plugins.scala.project.package$ModuleExt.configureScalaCompilerSettingsFrom(package.scala:113)\n    at org.jetbrains.plugins.scala.project.gradle.ScalaGradleDataService.org$jetbrains$plugins$scala$project$gradle$ScalaGradleDataService$$doImport(ScalaGradleDataService.scala:33)\n    at org.jetbrains.plugins.scala.project.gradle.ScalaGradleDataService$$anonfun$doImportData$1.apply(ScalaGradleDataService.scala:22)\n    at org.jetbrains.plugins.scala.project.gradle.ScalaGradleDataService$$anonfun$doImportData$1.apply(ScalaGradleDataService.scala:22)\n    at scala.collection.Iterator$class.foreach(Iterator.scala:743)\n    at scala.collection.AbstractIterator.foreach(Iterator.scala:1177)\n    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n    at org.jetbrains.plugins.scala.project.gradle.ScalaGradleDataService.doImportData(ScalaGradleDataService.scala:22)\n    at org.jetbrains.plugins.scala.project.gradle.AbstractDataService$$anonfun$importData$1.apply$mcV$sp(AbstractDataService.scala:20)\n    at org.jetbrains.plugins.scala.project.gradle.AbstractDataService$$anon$1.execute(AbstractDataService.scala:39)\n    at com.intellij.openapi.externalSystem.util.DisposeAwareProjectChange.run(DisposeAwareProjectChange.java:36)\n    at com.intellij.openapi.externalSystem.util.ExternalSystemApiUtil$7$1.run(ExternalSystemApiUtil.java:462)\n    at com.intellij.openapi.application.impl.ApplicationImpl.runWriteAction(ApplicationImpl.java:931)\n    at com.intellij.openapi.externalSystem.util.ExternalSystemApiUtil$7.run(ExternalSystemApiUtil.java:459)\n    at com.intellij.openapi.externalSystem.util.ExternalSystemApiUtil.executeOnEdt(ExternalSystemApiUtil.java:472)\n    at com.intellij.openapi.externalSystem.util.ExternalSystemApiUtil.executeProjectChangeAction(ExternalSystemApiUtil.java:457)\n    at org.jetbrains.plugins.scala.project.gradle.AbstractDataService$.invoke(AbstractDataService.scala:37)\n    at org.jetbrains.plugins.scala.project.gradle.AbstractDataService.importData(AbstractDataService.scala:19)\n    at com.intellij.openapi.externalSystem.service.project.manage.ProjectDataManager.importData(ProjectDataManager.java:132)\n    at com.intellij.openapi.externalSystem.service.project.manage.ProjectDataManager.importData(ProjectDataManager.java:102)\n    at com.intellij.openapi.externalSystem.service.project.manage.ProjectDataManager.importData(ProjectDataManager.java:141)\n    at com.intellij.openapi.externalSystem.service.project.manage.ProjectDataManager.importData(ProjectDataManager.java:102)\n    at com.intellij.openapi.externalSystem.service.project.manage.ProjectDataManager.importData(ProjectDataManager.java:141)\n    at com.intellij.openapi.externalSystem.service.project.wizard.AbstractExternalProjectImportBuilder$2$1.run(AbstractExternalProjectImportBuilder.java:163)\n    at com.intellij.openapi.roots.impl.ProjectRootManagerImpl.mergeRootsChangesDuring(ProjectRootManagerImpl.java:328)\n    at com.intellij.openapi.externalSystem.service.project.wizard.AbstractExternalProjectImportBuilder$2.execute(AbstractExternalProjectImportBuilder.java:160)\n    at com.intellij.openapi.externalSystem.util.DisposeAwareProjectChange.run(DisposeAwareProjectChange.java:36)\n    at com.intellij.openapi.externalSystem.util.ExternalSystemApiUtil$7$1.run(ExternalSystemApiUtil.java:462)\n    at com.intellij.openapi.application.impl.ApplicationImpl.runWriteAction(ApplicationImpl.java:931)\n    at com.intellij.openapi.externalSystem.util.ExternalSystemApiUtil$7.run(ExternalSystemApiUtil.java:459)\n    at com.intellij.util.ui.UIUtil.invokeLaterIfNeeded(UIUtil.java:2350)\n    at com.intellij.openapi.externalSystem.util.ExternalSystemApiUtil.executeOnEdt(ExternalSystemApiUtil.java:479)\n    at com.intellij.openapi.externalSystem.util.ExternalSystemApiUtil.executeProjectChangeAction(ExternalSystemApiUtil.java:457)\n    at com.intellij.openapi.externalSystem.util.ExternalSystemApiUtil.executeProjectChangeAction(ExternalSystemApiUtil.java:453)\n    at com.intellij.openapi.externalSystem.service.project.wizard.AbstractExternalProjectImportBuilder.commit(AbstractExternalProjectImportBuilder.java:157)\n    at com.intellij.projectImport.ProjectImportBuilder.commit(ProjectImportBuilder.java:60)\n    at com.intellij.projectImport.ProjectOpenProcessorBase.doOpenProject(ProjectOpenProcessorBase.java:222)\n    at com.intellij.ide.impl.ProjectUtil.openOrImport(ProjectUtil.java:124)\n    at com.intellij.ide.MacOSApplicationProvider$Worker$1.handleOpenFile(MacOSApplicationProvider.java:114)\n    at com.apple.eawt._AppEventLegacyHandler$5.dispatchEvent(_AppEventLegacyHandler.java:122)\n    at com.apple.eawt._AppEventLegacyHandler.sendEventToEachListenerUntilHandled(_AppEventLegacyHandler.java:168)\n    at com.apple.eawt._AppEventLegacyHandler.openFiles(_AppEventLegacyHandler.java:120)\n    at com.apple.eawt._AppEventHandler$_OpenFileDispatcher.performUsing(_AppEventHandler.java:353)\n    at com.apple.eawt._AppEventHandler$_OpenFileDispatcher.performUsing(_AppEventHandler.java:344)\n    at com.apple.eawt._AppEventHandler$_AppEventDispatcher$1.run(_AppEventHandler.java:489)\n    at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:209)\n    at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:715)\n    at java.awt.EventQueue.access$400(EventQueue.java:82)\n    at java.awt.EventQueue$2.run(EventQueue.java:676)\n    at java.awt.EventQueue$2.run(EventQueue.java:674)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.security.AccessControlContext$1.doIntersectionPrivilege(AccessControlContext.java:86)\n    at java.awt.EventQueue.dispatchEvent(EventQueue.java:685)\n    at com.intellij.ide.IdeEventQueue.e(IdeEventQueue.java:734)\n    at com.intellij.ide.IdeEventQueue._dispatchEvent(IdeEventQueue.java:569)\n    at com.intellij.ide.IdeEventQueue.dispatchEvent(IdeEventQueue.java:382)\n    at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:296)\n    at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:211)\n    at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:201)\n    at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:196)\n    at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:188)\n    at java.awt.EventDispatchThread.run(EventDispatchThread.java:122)\n. The maven plugins add the  publishToMavenLocal and  publish tasks which seem to not publish anything to the local maven repository in the ~/.m2 folder\n. @adriancole I entirely missed reading that task as  ./gradlew tasks  placed publishToMavenLocal before the install task. We might just need to hide the publishToMavenLocal task\n. ",
    "gneokleo": "@jamescway I'll do another commit to move the code into the main method of the collector today.\n. Sure. Doing that now.\n. cc @adriancole @eirslett\nI will post a screenshot in the zipkin chat room\n. \n\n. \n\n. Thanks for the review guys. I moved the code to where the rest of the javascript code is\n. Thanks for the review guys. I moved the code to where the rest of the javascript code is\n. This is a breaking change for some people. Some spans they might already have stored will not be readable. I would bump the version in the gradle file too\nFor the record this is what redis is doing. \nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-redis/src/main/scala/com/twitter/zipkin/storage/redis/RedisStorage.scala#L23\n. A working commit that addresses this\nhttps://github.com/openzipkin/zipkin/pull/709\n. Snappy compression with the compact protocol was merged to master\n. Yes, it does. Same as the cassandra change a few commits ago which removed snappy compression on the client side\n. This really depends on the application. I chose 30 because our application was seriously falling behind. I'll change it to 1s though. It sounds closer to what a lot of people will face.\n. @jamescway @adriancole What was the verdict on builder vs factory? We use the factory method so if that's the case I can move it to the file you suggested.\n. done\n. ",
    "spencergibb": ":+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. belated :+1: \n. belated :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Nice, that removes a ton of complexity. :+1: \n. Nice, that removes a ton of complexity. :+1: \n. :+1: \n. :+1: \n. :+1: \n. ",
    "dhutchis": "Aggregates are blank for me as well.  I don't know what should appear on the aggregates page, i.e. if I am missing some configuration or if the page is not implemented or something else.  \nI'm running Zipkin using the bin/collector, bin/query and bin/web tools.  I can see traces fine on the main query page.\n. ",
    "adarshaj": "I am stuck because of this! Where should I start?\n. :+1: \n. ",
    "jfeltesse-mdsol": "All deployment options are on the table at this stage, docker could be it as well as building & deploying on AWS instances manually.\nYour reply leads me to ask another question, if Cassandra is being phased out why is the docker-zipkin project relying on it? What is the recommended storage backend these days? (I guess the answer is \"it depends\", I'm just trying to find out which option is going to be the best supported).\n. All deployment options are on the table at this stage, docker could be it as well as building & deploying on AWS instances manually.\nYour reply leads me to ask another question, if Cassandra is being phased out why is the docker-zipkin project relying on it? What is the recommended storage backend these days? (I guess the answer is \"it depends\", I'm just trying to find out which option is going to be the best supported).\n. awesome! :100: \n. :+1: \n. > Docs for docker should be in docker-zipkin\n:+1: \n. cool stuff! :100: \n. :ok_woman: \n. @adriancole updated, sorry for missing this!\n. Never used Mustache for real but could it be the front end side expects all requests to stem from the root page and that all page transitions to be handled by itself, confusing it when someone bypasses that?\nSounds like it's not going to be implemented so easily if from the start the web component has not been designed to handle those cases (which is wrong but what's done is done).\n. Never used Mustache for real but could it be the front end side expects all requests to stem from the root page and that all page transitions to be handled by itself, confusing it when someone bypasses that?\nSounds like it's not going to be implemented so easily if from the start the web component has not been designed to handle those cases (which is wrong but what's done is done).\n. Strange, I'll try to investigate more this week and report back here.\nThanks for having a look!\n. Strange, I'll try to investigate more this week and report back here.\nThanks for having a look!\n. Sorry for the delay. After some more testing it seems to work alright now.\nThanks for having a look nevertheless! :+1: \n. :+1: \n. nice! :confetti_ball: \n. you might want to add \"on Mac OS X\" because parts are mac os only (eg, brew install stuff)\n. yes, that's the broken link. \n.../openzipkin/... \u2192 404\n. nitpicking here but there should be an empty line above and, having said that, if we're going to have the schema and index defs done at once maybe having the index defs close to the table they change would be easier to understand?\n. ",
    "danchia": "@adriancole is the DTCS error flaky, or does it always happen?\n. @adriancole is the DTCS error flaky, or does it always happen?\n. Ah, do we not use cassandra-unit which brings in it's own cassandra?\n. Ah, do we not use cassandra-unit which brings in it's own cassandra?\n. If we're using travis, it seems like the latest version is 2.0.9 which doesn't have DTCS, which was introduced in 2.0.11.\nIt seems like there's two options here:\n1) In test, don't create the tables with DTCS. Compaction strategies shouldn't affect test behavior. However, it would require maintaining two separate cql files (one for test and one for end-users)..\n2) Self-install a newer version of Cassandra manually in the travis script.\n. If we're using travis, it seems like the latest version is 2.0.9 which doesn't have DTCS, which was introduced in 2.0.11.\nIt seems like there's two options here:\n1) In test, don't create the tables with DTCS. Compaction strategies shouldn't affect test behavior. However, it would require maintaining two separate cql files (one for test and one for end-users)..\n2) Self-install a newer version of Cassandra manually in the travis script.\n. That's possible too =)\nWe need at least 2.0.11\nOn Sun, Sep 27, 2015, 9:28 AM Adrian Cole notifications@github.com wrote:\n\noption 3.. ask travis to update?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/706#issuecomment-143574399.\n. That's possible too =)\n\nWe need at least 2.0.11\nOn Sun, Sep 27, 2015, 9:28 AM Adrian Cole notifications@github.com wrote:\n\noption 3.. ask travis to update?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/706#issuecomment-143574399.\n. The fix would look something like https://github.com/coursera/zipkin/commit/807e48d2c627c14438acc5e53dd2795bc6378639\n\nhowever because of the breaking nature, not sure how we should proceed with this.\n. The fix would look something like https://github.com/coursera/zipkin/commit/807e48d2c627c14438acc5e53dd2795bc6378639\nhowever because of the breaking nature, not sure how we should proceed with this.\n. Happy to do so :)\n. Happy to do so :)\n. @adriancole added a test as requested\n. @adriancole added a test as requested\n. Whoops. @adriancole I've updated the diff, but forgot to ping you. I don't know why it's failing though. Can you trigger a retest? It works locally for me.\n. Ah good catch.. clean surfaced the failure. Silly incremental.\nFixing now.\n. @adriancole I lowercased the spanName in the Span constructor to normalize is, as discussed in gitter.\n. @adriancole I lowercased the spanName in the Span constructor to normalize is, as discussed in gitter.\n. LGTM, thanks for digging deeper.\n. private constructor with apply starts going down the manual case class route, so may not be the best choice.\nOne thing I might suggest however, is adding a require into the case class body to make sure all the things that should be lowercase are indeed lowercase, so people who skip the conversions are not caught off guard.\n. private constructor with apply starts going down the manual case class route, so may not be the best choice.\nOne thing I might suggest however, is adding a require into the case class body to make sure all the things that should be lowercase are indeed lowercase, so people who skip the conversions are not caught off guard.\n. @adriancole let me give this a stab.\n. So I took a quick stab at this, and it turns out it's not that easy.\nmomentjs supports ASP style durations (H:m:s), but not human readable ones like \"1 second\".\nEven worse is that their human readable forms of durations, e.g. moment.duration(\"0:00::00:1\").humanize() gives you a few seconds which is not parsable back into a duration.\nAny ideas?\n. Hm.. interesting. Still have to solve the bijection problem, but I have an idea..\n. Nah, really busy at work :(\nZipkin is definitely one of my favorite projects to watch :)\n. Even within the same collector, because a trace composed of many different spans from different machines I don't think we can avoid multiple messages.\nThat said, if I'm not wrong the per message overhead for Kafka is pretty low, and there are tunables to help with higher message volumes (batching on the producer size, and fetch size on the consumer side).\n. @prat0318 Do you have your kafka consumer settings handy? A long time ago the defaults in zipkin were not very good, causing offset updates to ZooKeeper very often (which was super expensive).\n. https://docs.datastax.com/en/developer/java-driver/3.0/java-driver/reference/customCodecs.html\nCustom codecs seems like it would solve the ugly hack we had to do for the us timestamps?\n. https://docs.datastax.com/en/developer/java-driver/3.0/java-driver/reference/customCodecs.html\nCustom codecs seems like it would solve the ugly hack we had to do for the us timestamps?\n. Actually, driver 3.0 supports all versions of Cassandra with CQL.\nhttp://datastax.github.io/java-driver/manual/native_protocol/\n. I can't think of any reason why you wouldn't want to, it's quite a reasonable tuning option.\nSpeaking of which, one improvement I had planned to test was batching writes to Cassandra into small, unlogged batches, which I think would improvement performance.\n. I can't think of any reason why you wouldn't want to, it's quite a reasonable tuning option.\nSpeaking of which, one improvement I had planned to test was batching writes to Cassandra into small, unlogged batches, which I think would improvement performance.\n. I was basing it off: https://dzone.com/articles/efficient-cassandra-write, which could mean there is some benefit to be had if we have locality.\nI think the other thing that was motivating me to batch was that AFAIK we wait for the each trace to be stored before processing the next one, at least for Kafka. This greatly reduces potential throughput than say, if we allowed for up to X number of store operations in flight in the collector.\n. I was basing it off: https://dzone.com/articles/efficient-cassandra-write, which could mean there is some benefit to be had if we have locality.\nI think the other thing that was motivating me to batch was that AFAIK we wait for the each trace to be stored before processing the next one, at least for Kafka. This greatly reduces potential throughput than say, if we allowed for up to X number of store operations in flight in the collector.\n. I'll have to look at the article again, but I think if we have a lot of partition key locality the article suggests a (very modest) improvement. But you're probably right in that there will be no improvement or even an performance decrease.\nWith regarding to batching kafka writes, each StreamProcessor does indeed wait for C* to finish writing, which would limit throughput a lot if you don't have enough partitions / latency isn't particularly good.\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-receiver-kafka/src/main/scala/com/twitter/zipkin/receiver/kafka/KafkaStreamProcessor.scala#L21\nWe could fix this by allows for more than 1 process invocation in flight to increase throughput.\n. I'll have to look at the article again, but I think if we have a lot of partition key locality the article suggests a (very modest) improvement. But you're probably right in that there will be no improvement or even an performance decrease.\nWith regarding to batching kafka writes, each StreamProcessor does indeed wait for C* to finish writing, which would limit throughput a lot if you don't have enough partitions / latency isn't particularly good.\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-receiver-kafka/src/main/scala/com/twitter/zipkin/receiver/kafka/KafkaStreamProcessor.scala#L21\nWe could fix this by allows for more than 1 process invocation in flight to increase throughput.\n. Agree that timeuuid is the typical pattern in cassandra for this. However, since the timestamps and trace_ids here are application assigned and cannot be changed, in my opinion it would be better to promote the trace_id field to the PRIMARY KEY as suggested by @yurishkuro \n. That seems reasonable to me.\nMaybe if someone really wants this in future, maybe we could index the hash of the really long value instead?\n. That seems reasonable to me.\nMaybe if someone really wants this in future, maybe we could index the hash of the really long value instead?\n. +1 from me\n. AFAIK there's the command line client, and a deprecrated? HTTP endpoint. Sadly can't create the indexes from CQL directly since you need the solr config.\n. AFAIK there's the command line client, and a deprecrated? HTTP endpoint. Sadly can't create the indexes from CQL directly since you need the solr config.\n. woops, no real reason for this one\n. do we still need to use this workaround now that we're writing millis \u2013 we should be able to just do .setDate? Unless we're just trying to avoid allocating the Date object.\n. I think the default is actually 8 in the Datastax driver.\n. I think we should be setting HostDistance.LOCAL. REMOTE is only for nodes not in the same DC (as determined by the first node you contact), and the driver should prefer LOCAL nodes.\n. If we change to LOCAL, then the default is 8.\n. Other C* people can chime in too, but I'd like to understand better why the OP ended up using what the driver thought were REMOTE nodes. In C* these typically are the nodes that are in a whole different datacenter (potentially across the US, for example).\n. there's a typo here. translates to the\n. Oops sorry, hit finish review too early. Meant to finish my pass later..\n. Minor readability suggestion (@adriancole probably can comment more) but I prefer to not have abbreviations that are not generally well known in public APIs. e.g ep probably don't mean much to most people\n. I don't recall if in Cassandra 3.x whether long column names inflates storage use, but I would encourage not using abbreviations.\n. ",
    "mansu": "We have hundreds of services here at pinterest and we see the lag in the UI dropdown being populated. So my experience is same as @yurishkuro's. Can we add an environment variable to set the zipkin.queryService.servicesMaxAge?. @adriancole ah, that works also. Looked around for setting values via command line but couldn't find anything right away. Thanks!. Does it make sense to merge annotations and tags into a single structure? I feel tags should also be a list. . @adriancole making favoriting/saving a tracing part of the client and then the client copy the traces between the 2 stores is my preferred approach for the following reasons:\n(a) the spans will remain immutable\n(b) In addition to the UI, other applications can also write to this end point to save traces for ever.\n(c) with an additional annotation on the root span we can create save or favorite feature.\nHowever, I prefer making this a feature of the current backend instead of having separate clusters though. I think that way the backend would be easier to operate. Multiple clusters increase operational overhead in large organizations.\n. @adriancole making favoriting/saving a tracing part of the client and then the client copy the traces between the 2 stores is my preferred approach for the following reasons:\n(a) the spans will remain immutable\n(b) In addition to the UI, other applications can also write to this end point to save traces for ever.\n(c) with an additional annotation on the root span we can create save or favorite feature.\nHowever, I prefer making this a feature of the current backend instead of having separate clusters though. I think that way the backend would be easier to operate. Multiple clusters increase operational overhead in large organizations.\n. Are we doing this on the read path or the write path?\n. Are we doing this on the read path or the write path?\n. strictTraceId makes more sense than all the other options.\n. strictTraceId makes more sense than all the other options.\n. +1 for this feature. I was looking through my spans today and was wondering what time they were ingested. I used a crude '/zipkin-/_stats' end point to see when the index was last written to but having a timestamp field on the ingested span would make this process a lot easier.\nI think the best way approach here is to add an @timestamp field to every span that is ingested. The value of the @timestamp field would be the time the span is written to ES. This is a standard practice for other logs ingested by logstash, so it is not odd thing to do in ES land. Since this change is localized to the ES module, it shouldn't impact other storage engines or the interface.\n. @adriancole I looked at the code earlier and had the same feeling. I was looking for a better idea but can't think of one. I like your suggestion. However, I would prefer the name of the field to be collector_timestamp_millis\n. +1 for fixing this fast! This is confusing people who are new to the UI. \n. Let me clarify my intent here: \nFor example say Trace1 has span1, span2 and span3. Let's say the entire trace took 10ms and span1 took 1ms, span2 took 2ms and span3 took 3ms. In the trace view, it would be nice to show an additional line of text like: \"Total span duration is 60%(6ms) of trace duration (10ms)\"  to summarize the data better.\nThis kind of message is useful because the rest 40% is either a missing span or overhead not captured by the network requests and needs further investigation. We typically have ~120 spans in a request and we assume that 75% of the time is spent on the network and the rest is processing overhead. Currently, our users manually calculate these latency numbers every time. \nIf the community feels that this is a valuable feature for everyone, I will add it to the UI.\n. Let me clarify my intent here: \nFor example say Trace1 has span1, span2 and span3. Let's say the entire trace took 10ms and span1 took 1ms, span2 took 2ms and span3 took 3ms. In the trace view, it would be nice to show an additional line of text like: \"Total span duration is 60%(6ms) of trace duration (10ms)\"  to summarize the data better.\nThis kind of message is useful because the rest 40% is either a missing span or overhead not captured by the network requests and needs further investigation. We typically have ~120 spans in a request and we assume that 75% of the time is spent on the network and the rest is processing overhead. Currently, our users manually calculate these latency numbers every time. \nIf the community feels that this is a valuable feature for everyone, I will add it to the UI.\n. @adriancole I think @dragontree101 is asking for example ES queries on how to search for spans from the kibana UI. I think this is a valid use case that we should document since most people who use ES will also use Kibana. In practice, I found that Kibana is a better UI to search for traces esp when they are broken and don't show up the UI. We are currently doing the same, happy to share what we come up with shortly.. @adriancole We used the open source spark job to get a dependency graph and visualized it with vizceral. It was a very good visualization. But the data quality can be improved. We will try to open source this also. . /cc @naoman . @adriancole While cutting off after a certain date is a good idea I think a better alternative is to add a mechanism to cache the service and span names in the persistent layer. That way we can run an expensive query periodically and cache accurate data instead of returning incomplete data every time.  The current model of relying on HTTP caching is too expensive when a lot of users use the zipkin UI and is not scalable(for us) in the long run. Loading span and service names is the slowest part in our UI and currently it takes 10-15 seconds before the UI is usable. We are currently sampling 1/3 the expected rate but once we sample at the expected rate, the UI will take even longer to load.. @adriancole While cutting off after a certain date is a good idea I think a better alternative is to add a mechanism to cache the service and span names in the persistent layer. That way we can run an expensive query periodically and cache accurate data instead of returning incomplete data every time.  The current model of relying on HTTP caching is too expensive when a lot of users use the zipkin UI and is not scalable(for us) in the long run. Loading span and service names is the slowest part in our UI and currently it takes 10-15 seconds before the UI is usable. We are currently sampling 1/3 the expected rate but once we sample at the expected rate, the UI will take even longer to load.. Thanks. Will look into this and let you know.. Thanks. Will look into this and let you know.. @sammypbaird I didn't get a chance to look into this yet. Just curious, how much perf improvement did you see when you just scanned 2 days worth of indices?. @sammypbaird This is great to hear. We will push the fix to our prod and report back as well. While this is a great improvement, ~8 seconds is still way above sub-second interactive speeds. Also, please let us know if you can come up with any more improvements.. I think disallowing using dots in keys is a non-starter since it breaks backward compatibility. A lot of our instrumentation already uses that notation and breaking it is not desirable.\nTurning dots to underscores is also a problem because people expect to search for an item by the key they used. Traning them to replace dots with underscores is a huge overhead. . The change sounds good. . +1 for dropping the native driver. . HTTP auth into ES would should work. However, we need to make sure that this approach doesn't interfere with ES hosted on AWS. . This sounds great! A couple of questions:\n\nWhen is this new index populated? Is it when a span is written to storage?\nHaving the ability to set the granularity of the index name would be awesome. So, it would be nice to set the granularity to weekly or monthly also. That way, we may not need to clean up this index if we so desire.. Thanks for the explanation. +1 in this case. \n\nI was asking a longer term index since it would mean cleaning up one more index when we purge the data. But, if this is added an additional index type on the daily zipkin index, daily granularity is an awesome solution.. ",
    "prat0318": "I don't think that is the problem. When we are splitting, it should split only by the first = and the rest of them should stay as in, in the value. The limitation of this is = should not be in the key, but that is not a deal breaker.\n. <3 @srijs \n. cassandra\n. cassandra\n. it got fixed in one of the recent ui versions. good to close.\nOn Wednesday, March 16, 2016, Eirik Sletteberg notifications@github.com\nwrote:\n\nIs this still a problem?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/783#issuecomment-197418806\n\n\nPrateek Agarwal\n. @adriancole i checked even with no query params in the URL and i select a previous date from the dropdown, the timestamp added in the query is still the current one (ignoring the selected endTs).\n. I think api-docs tool should not be coupled with e2e testing tool. api-docs tool should be judged on its expressionability. A decent enough e2e testing framework should be easily plugged in with any of the api-docs tools or can be also tested directly with endpoints (not sure how api-docs help there).\nI haven't have an experience with blueprint so can't comment much. But i would say, a feature to explore APIs and test them directly (like here) comes handy a lot of times.\n. @karlunho++ i will be happy to help/review the swagger apis.\n. @karlunho++ i will be happy to help/review the swagger apis.\n. :+1: \n. :+1: \n. :+1: travis is failing though. fix-n-ship\n. :+1: travis is failing though. fix-n-ship\n. i don't have any strong opinions on the name. Though i am curious if these binary annotations (http/path, http/staus_code) will have special behaviour in the zipkin system? in other words, if my tracer still pushes http.uri, will it lose something? (other than being different from java/scala tracers).\n. i don't have any strong opinions on the name. Though i am curious if these binary annotations (http/path, http/staus_code) will have special behaviour in the zipkin system? in other words, if my tracer still pushes http.uri, will it lose something? (other than being different from java/scala tracers).\n. right. i missed the point about different tracers contributing to the same system.\n. right. i missed the point about different tracers contributing to the same system.\n. ++\nOn Saturday, January 23, 2016, Yuri Shkuro notifications@github.com wrote:\n\nI like it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/909#issuecomment-174200253.\n\n\nPrateek Agarwal\n. <3 @t-yuki \n. that will happen only if the first collector updates the lowmark. I am fine even if the second group also starts reading from the first message\n. i dont think there is such an api. i think the way kafka works, if multiple consumer groups are reading from a topic both will receive the same message independant of each other. I think that is fine. so we can work out the example even with a single span.\n. I am down for benchmarking any tweaks that we have in mind like batching cql queries or optimizing span stores for bulk.\nI think even adding the functionality of collector receiving a trace-at-a-time can improve the performace by decreasing the total kafa messages. What do you think about that?\n. @eirslett i meant collector should be able to handle bundle of different spans of the same trace coming again.\nLike currently, my tracer sends spans 1-3 and 4-6 one at a time:\n```\n                                 +------+\n                                 |      |\n                  TraceId:1      |      |\n                  Spans: 1-3     |      |               +------------+\n  +------------------------------>      +--------------->            |\n  |            |----------------->      |---------------> Collector  |\n  |  Tracer 1  +----------------->      +--------------->            |\n  |            |                 |      |               +-^^^--------+\n  +------------+                 |Kafka |                 |-|\n                                 |      |                 |-|\n                                 |      |                 |-|\n                   TraceId:1     |      |                 |-|\n  +------------+   Spans: 4-6    |      |                 |-|\n  |            +----------------->      +-------------------|\n  |  Tracer 2  |----------------->      |-------------------|\n  |            +----------------->      +-------------------+\n  +------------+                 +------+\n\n```\nInstead, facility to bundle up spans 1-3 and 4-6 together, and let collector separate those out.\n+------+\n                               |      |\n                TraceId:1      |      |\n                Spans: 1-3     |      |               +------------+\n+------------+----------------->      +--------------->            |\n|            |                 |      |               | Collector  |\n|  Tracer 1  |                 |      |               |            |\n|            |                 |      |               +-^----------+\n+------------+                 |Kafka |                 |\n                               |      |                 |\n                               |      |                 |\n                 TraceId:1     |      |                 |\n+------------+   Spans: 4-6    |      |                 |\n|            +-----------------+      +------------------\n|  Tracer 2  |                 |      |\n|            |                 |      |\n+------------+                 +------+\nI say this, because we see each span whether with large data or small takes around same ~90ms. if @adriancole is correct and is limited by kafka receiver, this should lead to good perf win for us, as our tracer can bundle up the spans together and send.\n. @danchia i am using all the defaults of zipkin. I think offset update happens every 10s. by default (if i am not wrong). please let me know if there is a way from outside to see the consumer config.\n. @eirslett but the bottleneck is on the collector end when it is reading the spans. It is currently constrained to receive one span message at a time. So, i was thinking making that span message blob more like a list of span messages blob.\n. @eirslett but the bottleneck is on the collector end when it is reading the spans. It is currently constrained to receive one span message at a time. So, i was thinking making that span message blob more like a list of span messages blob.\n. > SpanStore doesn't require the bundle of spans it receives to be in the same trace\nThat means, it should not be hard to make changes from collector end if we want to receive a bundle of spans. Right?\n\nA few tracers do send bundles at a time, subject to either span count or bundle size.\n\nconfused how they can do that if they use the same collector? Does collector support this yet?\n\nNot sure if we try to read a list and only one is present..\n\nI think that is important point to keep it Backwards Compat. Collector should be able to accept both a list of spans or a single span correctly.\noverall, i am very excited to try out the idea of span bundle.\n. > SpanStore doesn't require the bundle of spans it receives to be in the same trace\nThat means, it should not be hard to make changes from collector end if we want to receive a bundle of spans. Right?\n\nA few tracers do send bundles at a time, subject to either span count or bundle size.\n\nconfused how they can do that if they use the same collector? Does collector support this yet?\n\nNot sure if we try to read a list and only one is present..\n\nI think that is important point to keep it Backwards Compat. Collector should be able to accept both a list of spans or a single span correctly.\noverall, i am very excited to try out the idea of span bundle.\n. @yurishkuro yeah that didn't work. In fact, adding random suffices after query for each span, indeed showed up different spans.\n. @yurishkuro yeah that didn't work. In fact, adding random suffices after query for each span, indeed showed up different spans.\n. My bad. It was a subtle bug in pyramid_zipkin. Closing it up\n. My bad. It was a subtle bug in pyramid_zipkin. Closing it up\n. You were right. It was actually happening from before. This happens when a value for binary annotation is None instead of str if type expected is str. It was a error from our end. Although zipkin can return some better error than NPE. I am ok to close the issue though. Thanks!\n. You were right. It was actually happening from before. This happens when a value for binary annotation is None instead of str if type expected is str. It was a error from our end. Although zipkin can return some better error than NPE. I am ok to close the issue though. Thanks!\n. We currently run Cassandra 2.1.4 and i think driver 3.0 supports 2.2+ versions. But yeah, if the driver is more performant we can look into upgrading our cluster to 2.2+.\n. Just that it was working fine. But if we have plans to upgrade the driver, i can work with ops to upgrade the cluster.\n. ok. i will prioritise the upgrade from my end and will keep posted here for the updates.\n. Ah. I checked the What's new section and thought it supports just 2.2 and 3.0+. Thanks for checking again.\n. ++ would be nice\n. ++ would be nice\n. I will try running the collector with this patch to check the perf\nOn Wednesday, February 24, 2016, Eirik Sletteberg notifications@github.com\nwrote:\n\nDon't get me wrong, I'm +1 for this change. I don't like the solution, but\nI still think it's the best solution - there's no smooth migration path.\nOne possible alternative would be to consume span collections on a separate\nkafka topic, but then we get lots of additional complexity from handling\ntwo topics.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/995#issuecomment-188353240.\n\n\nPrateek Agarwal\n. From usability perspective, maintaining multiple topics would be hard imo. For testing perf and maintaining backwards compat, it will be helpful though\n. In my experiment, each Trace has 9 spans, which means without patch 9 kafka messages per trace.\n| - | Traces/s/partition | Spans/s/partition | Messages/s/partition |\n| --- | --- | --- | --- |\n| Production | 174.5 | 1570 | 1570 |\n| Consumption | 40.5 | 364 | 364 |\nWith bundling, all 9 spans are batched up in 2 kafka messages.\n| - | Traces/s/partition | Spans/s/partition | Messages/s/partition |\n| --- | --- | --- | --- |\n| Production | 200 | 1800 | 400 |\n| Consumption | 175 | 1580 | 351 |\nAs we can see, we straight away get a perf improvement of ~4.5 times. This should\nbe ~9 times if i batch up all spans in a single kafka message (it just needed some\nmore code changes at my end, so i just went ahead with 2 messages).\nSo, what i see is Message consumption rate is around constant to ~350 but we can increase\nour Trace consumption rate by bundling together. So, i am super excited to have this\npatch merged to master. :+1: \ncc @adriancole @yurishkuro \n. :+1: \n. sg2m\n. lgtm. we didn't have any pinned local dc till now, so half of our inserts would be going to remote dc. this change should improve the query perf.\n. Actually i am not sure, if bumping version from 1.32.3 to 1.35.0 is the main reason. This bump was made on Mar 2, but in the graph below (which shows memory being consumed) the slope didn't change much on that date. (Our service restarts when it reaches the limit of 4G, which shows the sudden drop in memory at 4G)\nBut we did increase the number of spans being ingested drastically from Mar 3 onwards and that bumped up the memory consumption by collector significantly.\n\nAlso we turned the span volume down on Mar 9 afternoon and that reflects the decrease in slope. So, my hypothesis says memory consumption gets directly propotional to the no. of spans ingested\n. Actually i am not sure, if bumping version from 1.32.3 to 1.35.0 is the main reason. This bump was made on Mar 2, but in the graph below (which shows memory being consumed) the slope didn't change much on that date. (Our service restarts when it reaches the limit of 4G, which shows the sudden drop in memory at 4G)\nBut we did increase the number of spans being ingested drastically from Mar 3 onwards and that bumped up the memory consumption by collector significantly.\n\nAlso we turned the span volume down on Mar 9 afternoon and that reflects the decrease in slope. So, my hypothesis says memory consumption gets directly propotional to the no. of spans ingested\n. I haven't yet tried to attach a profiler but if there is nothing apparent which looks like the cause, will try attaching a profiler sometime.\n. I haven't yet tried to attach a profiler but if there is nothing apparent which looks like the cause, will try attaching a profiler sometime.\n. thats so cool! will try it out some time. \n. thats so cool! will try it out some time. \n. trace pb\njson\n[\n    {\n        \"traceId\": \"a83668f5b355bd70\",\n        \"name\": \"post\",\n        \"id\": \"0000000000000001\",\n        \"parentId\": \"0000000000000000\",\n        \"timestamp\": \u200b1459723138170202,\n        \"duration\": \u200b604053,\n        \"annotations\": [\n            {\n                \"timestamp\": \u200b1459723138170202,\n                \"value\": \"sr\",\n                \"endpoint\": {\n                    \"serviceName\": \"yelp\",\n                    \"ipv4\": \"10.56.5.4\",\n                    \"port\": \u200b80\n                }\n            },\n            {\n                \"timestamp\": \u200b1459723138774255,\n                \"value\": \"ss\",\n                \"endpoint\": {\n                    \"serviceName\": \"yelp\",\n                    \"ipv4\": \"10.56.5.4\",\n                    \"port\": \u200b80\n                }\n            }\n        ],\n        \"binaryAnnotations\": [\n            {\n                \"key\": \"servlet\",\n                \"value\": \"connect\",\n                \"endpoint\": {\n                    \"serviceName\": \"yelp\",\n                    \"ipv4\": \"10.56.5.4\",\n                    \"port\": \u200b80\n                }\n            },\n            {\n                \"key\": \"http.uri\",\n                \"value\": \"register\",\n                \"endpoint\": {\n                    \"serviceName\": \"yelp\",\n                    \"ipv4\": \"10.56.5.4\",\n                    \"port\": \u200b80\n                }\n            },\n            {\n                \"key\": \"http.uri.qs\",\n                \"value\": \"register\",\n                \"endpoint\": {\n                    \"serviceName\": \"yelp\",\n                    \"ipv4\": \"10.56.5.4\",\n                    \"port\": \u200b80\n                }\n            }\n        ]\n    }\n]\n. It shows up fine with the older UI.\nAlso, as per the docs, span_id can be the same trace_id but is not mentioned it must or should. So, i don't think it is a bug.\n. @adriancole i am a bit confused. How come the way messages from kafka are read can affect something specific to ElasticSearch. If i am not wrong, the flow will be kafka -> collector -> ES/C. From what i read, message loss is only seen for ES and not C. But the transport (kafka/http) work is done once the data is fetched via collector. So how is the issue related to kafka batching plus ES storage?\n. I second @adriancole, changing backend store like cassandra/mysql shouldn't affect the # of returned spans. Try changing limit to 100 or so.\nI would say, start with a small number like # of partitions = # of brokers. If you see latency constraints, try the KAFKA_STREAMS option with the value same as # of partitions (collector now tries to read from all partitions parallely). As the number of partitions can't be reduced and it is just a one way road, increase should only be done if latency becomes an issue. Increase in partitions generally leads to overhead in maintainence etc.\n. Just curious, we are thinking modifying just kafka transport because it can handle throttling and none of http/scribe can, correct?\nOverall, looks a good idea.\n. We added off-heap memtable allocation of 20G which reduced the # of flushes and resulted in lesser compaction.\n. We added off-heap memtable allocation of 20G which reduced the # of flushes and resulted in lesser compaction.\n. i will be highly interested to follow on this. Do we have a doc or similar tracer in another language (i think we have one in ruby?) to get the APIs sketched out?\n. I will dig more on the weekend if jaeger-client can be integrated with B3 headers (at the cost of depping on opentracing, tornado & others) or will it make sense to create a simple new one.\n. just a doubt : so streams would be passed in string. won't that be an issue in casting. (scala noob)\n. ",
    "teedilo": "I know this discussion was closed, but the referenced elasticsearch page, https://github.com/openzipkin/zipkin-java/pull/107, does not exist.  Are more sophisticated search capabilities still being pursued via the Zipkin API?  We are using an Elasticsearch index for Zipkin, but even directly searching this index via Elasticsearch or Kibana is limited.  For example, there's no way to search for fields in annotations due to Elasticsearch's limited capabilities of searching complex objects.  I'd like to be able to say, \"show me all spans containing an HTTP status code with a value other than 200\".  No can do -- neither with the current Zipkin API nor using Elasticsearch to search the Zipkin Elasticsearch index.. Thanks for the tip, Adrian.  I'll see whether I can get a nested query to work.. ",
    "srijs": "CI failure does not like it's related. A flakey test, perhaps? I don't have the permissions to rerun it, maybe you could?\n. CI failure does not like it's related. A flakey test, perhaps? I don't have the permissions to rerun it, maybe you could?\n. Yeah, so it was both inefficient and broken ;) Anyways, thanks for the quick fix! :+1: \n. Closing since PR is merged.\n. @adriancole fixed!\n. I'm not sure case insensitivity is a good idea with regards to being able to offer different storage backends, which might or might not offer the ability to query the span name that way, and I don't think  schema-on-write is a thing that should be done for user-provided strings like this.\n. ",
    "rjbhewei": "i use proxy solved this problem\n. ",
    "zfy0701": "both solutions sounds file, from user point of view, it's nice to have them display with cases such as the service name (but you still query using lower case\"\n. ",
    "jcarres-mdsol": "a_timestamp is never null.\nOne more thing we have just realized.\nWhen we run it locally with 3 test apps, we create 16 annotations. When executing this against RDS, we create 8 annotations.\nChecking the differences, we have seen that the \"sr\", \"ss\" and related binary annotations were all in RDS. And the \"cs\" \"cr\" annotations and related binary annotations were none of them in RDS.\n. a_timestamp is never null.\nOne more thing we have just realized.\nWhen we run it locally with 3 test apps, we create 16 annotations. When executing this against RDS, we create 8 annotations.\nChecking the differences, we have seen that the \"sr\", \"ss\" and related binary annotations were all in RDS. And the \"cs\" \"cr\" annotations and related binary annotations were none of them in RDS.\n. Sure, there you go:\njson\n[\n  {\n    \"traceId\": \"4cc794c53b921301\",\n    \"name\": \"GET\",\n    \"id\": \"4cc794c53b921301\",\n    \"annotations\": [\n      {\n        \"timestamp\": 1445136539256150,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"TraceProducerOne\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 9410\n        }\n      },\n      {\n        \"timestamp\": 1445136540408729,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"TraceProducerOne\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 9410\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"http.uri\",\n        \"value\": \"/\",\n        \"endpoint\": {\n          \"serviceName\": \"TraceProducerOne\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 9410\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"4cc794c53b921301\",\n    \"name\": \"GET\",\n    \"id\": \"be536b964472dc09\",\n    \"parentId\": \"4cc794c53b921301\",\n    \"annotations\": [\n      {\n        \"timestamp\": 1445136539764798,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"TraceProducerOne\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 3001\n        }\n      },\n      {\n        \"timestamp\": 1445136539816432,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"TraceProducerTwo\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 9410\n        }\n      },\n      {\n        \"timestamp\": 1445136540401414,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"TraceProducerTwo\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 9410\n        }\n      },\n      {\n        \"timestamp\": 1445136540404135,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"TraceProducerOne\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 3001\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"http.status\",\n        \"value\": \"200\",\n        \"endpoint\": {\n          \"serviceName\": \"TraceProducerOne\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 3001\n        }\n      },\n      {\n        \"key\": \"http.uri\",\n        \"value\": \"/\",\n        \"endpoint\": {\n          \"serviceName\": \"TraceProducerOne\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 3001\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"4cc794c53b921301\",\n    \"name\": \"GET\",\n    \"id\": \"5b2bb3ba2f5c364e\",\n    \"parentId\": \"be536b964472dc09\",\n    \"annotations\": [\n      {\n        \"timestamp\": 1445136540025751,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"TraceProducerTwo\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 3002\n        }\n      },\n      {\n        \"timestamp\": 1445136540072846,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"TraceProducerThree\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 9410\n        }\n      },\n      {\n        \"timestamp\": 1445136540394644,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"TraceProducerThree\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 9410\n        }\n      },\n      {\n        \"timestamp\": 1445136540397049,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"TraceProducerTwo\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 3002\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"http.status\",\n        \"value\": \"200\",\n        \"endpoint\": {\n          \"serviceName\": \"TraceProducerTwo\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 3002\n        }\n      },\n      {\n        \"key\": \"http.uri\",\n        \"value\": \"/\",\n        \"endpoint\": {\n          \"serviceName\": \"TraceProducerTwo\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 3002\n        }\n      }\n    ]\n  }\n]\n. The traces UI for this same trace looks perfect:\n\n. Go perfect or go home!\n. Refreshing the UI once gives me one connection to the DB\nRefreshing it once again gives me 34 connection\nOnce again and I get 89 connections.\nThey are all executing the command \"sleep\" and they do not finish.\n. Strange enough, it kind of hovered over 100 and stayed stable.  Still not conformtable having 100 unused connections\n. I see most of the connections are just sleeping doing nothing and they stay there forever.\n. I do not know what's 'sa' \nIn ruby and C# we have the endpoint for cs cr set to the service we are calling. Not the service calling\n. I think QUERY_LOOKBACK will be confusing. We will have a default start_ts in the search that is invisible to the user. I do not think it will be obvious why most of the data is invisible in searches (i.e. can't search for anything before the QUERY_LOOKBACK which users do not know how was setup as is invisible to them.\nAlso is inflexible, if I deployed 3 days ago and would like to see data from the last 4 days to compare before and after, I can not if QUERY_LOOKBACK is set to 2 days. I can set it to 5days but then high-volume services will still have the original problem, etc.\n. I do not think it is necessary.\nYou can get all unique services by sr/ss from the store compare them with unique by ca and there you have your stats.\n. The method in ruby is named terribly, it does create a new Span object in Ruby but it does not find a new ID for this span, so if your trace_id had the ids from the http request, the SR will send to zipkin a span with the same IDs than the CS which I think will work as per the spec\n. how do you know if you should sample this trace?\nWould make sense for tags to have timestamp?. Is V2 API dependent on solving single host spans?. I think I'm still confused on this one, is single host spans a better representation than shared spans among services?. I think I'm still confused on this one, is single host spans a better representation than shared spans among services?. I am all for this to happen. I think I was confused by all this shared spans vs single-host, etc.\nWe are talking about simplifying the json sent to the http API. Then the earlier the better.. I am all for this to happen. I think I was confused by all this shared spans vs single-host, etc.\nWe are talking about simplifying the json sent to the http API. Then the earlier the better.. Yes, please, current discussion will be confusing to anyone.\nI'm very looking forward a new format. Yes, please, current discussion will be confusing to anyone.\nI'm very looking forward a new format. This is a great step forward.\nI see we will compile the js into one file now. Looking forward the faster UI. The datepicker.css was not needed? The datepicker UI still working?\n. This is a great step forward.\nI see we will compile the js into one file now. Looking forward the faster UI. The datepicker.css was not needed? The datepicker UI still working?\n. if we have:\nA calls B calls C\nand in the same request\nA calls B calls D\nYou would disambiguate the tree by looking at timestamps and Endpoint's IPs?\n. I see. \nBecause ServiceA will already create a spanID before calling ServiceB. \nThen ServiceB just uses that spanID on SR/SS , creates a new spanID on CS/CR to next service and that one has a parentID which is sent to zipkin but it is not sent to the next service. The only thing serviceC needs to know is the current spanID and then it can create children from that.\nIt will also work for localspans on serviceB as they will be all children from the spanID created by serviceA\nSeems that should work\n. \n. \n. Even after refreshing a couple of times, it does not seem to work for me :no_good: \nI've tried both chrome and firefox, just in case\n. 0_0! you are right, it's kind of embarrasing not to realize till now :hear_no_evil: \n. we at least are careful to not send some stuff we know will be duplicated when the upstream system is also zipkin\nhttps://github.com/openzipkin/zipkin-tracer/blob/master/lib/zipkin-tracer/rack/zipkin-tracer.rb#L55\n. Yes, usually is either \"OK\"  or a DB check. \nWe can add the route now with a simple \"OK\" and later as the check in the spanstores are implemented upgrade it to check these.\n. It will be useful, else you need to open firewall ports just because of that\n. Sounds good!\nOn Tue, Feb 23, 2016 at 10:39 AM, Adrian Cole notifications@github.com\nwrote:\n\nok, so localhost:9411/health?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/994#issuecomment-187624982.\n\n\nJordi Polo Carres | Software Architect  | Medidata Solutions Worldwide\nhttp://www.mdsol.com/\n. LGTM\n. I got this output from the API, mysql backend\n. Yes, clicked that, clicked each individually, clicked all clickable links \ud83d\udc83 \n. Yes! The new update solves my issue, thanks!\n. List of instrumentations:\nhttp://zipkin.io/pages/existing_instrumentations.html\nhttps://github.com/mdsol/Medidata.ZipkinTracerModule Should be compatible with current server and other implementations \n. There should be some documentation so client implementors know that this functionality is available\n. There should be some documentation so client implementors know that this functionality is available\n. Good idea, I do see much value in using time-ranges smaller than one day\n. Good idea, I do see much value in using time-ranges smaller than one day\n. Sounds good. I think most serious users will aggregate the data as it gets unusable quite quickly to calculate dependencies realtime.\nMaybe as you said still aggregate realtime is good for demos, if maintenance is real high, killing that code and displayed an empty graph if the spark job has not created the graph is not terrible either methinks.\n. Sounds good. I think most serious users will aggregate the data as it gets unusable quite quickly to calculate dependencies realtime.\nMaybe as you said still aggregate realtime is good for demos, if maintenance is real high, killing that code and displayed an empty graph if the spark job has not created the graph is not terrible either methinks.\n. So this will not mix with the current system?\nFor instance:\nUser -> System 1(trace1, sr cs,cr, ss)  -> System2(trace1 sr ms, ss) -----> System3(trace1 mr cs,cr) -> System3(sr ss)\nInstead you are saying that ms involves creating a trace2?\n. Why the last CS/CR to itself?. If serviceC finishes processing the message but does not generate any other zipkin information after mr (for instance we never trace calls to DBs) how will we know the trace has in fact ended?. I thought mr was emitted when the worker got the message from the queue.\nPerformance wise I would want to know two things:\n- How long the message was in the messaging bus\n- How long it took the worker to process the message\nTo do both we need at least one more annotation, no?\n. Ah, ok, then ServiceC would also use lc to specify how long its processing took. I do not know how realistic are the chances of clashing in a 2**64 space. But most other systems are moving to 128-bit uuids so for the sake of uniformity sounds like a good change to me\n. Intriging. \nIn my company we are fighting the fact that there is no control over AWS stuff. \nAny kind of compatibility would be interesting. Sounds awesome to me. I see no problem following their format, for better or worse AWS is a big part of internet traffic, making your system work well with theirs will be a win-win for everyone. +1 \nFor newcomers, this has always been confusing\n. I think this PR lacked documentation on how to use this new feature. . Are you planning more info there in the future? If not I would just show the ids there , no collapsable thing, else the naming will depend on what is coming next IMHO. Awesome! This has always bothered me!. So it seems zipkin server would benefit from having a buffer functionality which can be used for all inputs?. So it seems zipkin server would benefit from having a buffer functionality which can be used for all inputs?. Nice. We use nginx but same situation, it would be great if we could use trace_ids. Maybe this issue should be moved somewhere else thought as it is not related to the ruby client.\n. BTW @kmalecki maybe you would be interested on this https://github.com/JordiPolo/lorekeeper\nWe have a middleware which takes the trace Ids and put it in here so we have nice json formatted log messages with trace_ids. It may be too much depending on how much are you building around querying your logs but it can be a life-saver.\n. I've talked to @bvillanueva-mdsol  @lschreck-mdsol  and the conclusions are:\n- Having one good tracer which most of the people like is ideal. We would like that also.\n- Do not stress too much about other tracers, if they are not good eventually they will die. It's a pity they do not use the official one but not much can be done.\n- We think our tracer is the most complete but it has big problems. The name sucks, depends on Owin and it is not .NET core compatible. We are aware of these problems and we want to improve them all. Unfortunately our developer team switches projects often so it is difficult to tell how much effort we can put on this.\n- We are not very familiar with @aliostad  work, the idea of a core component and then specific projects for Owin, winAPI or whatever sound good, we have just not checked the project , same with the work by @sbebrys which seems like a cleanup over ours (most of the code is the same, a pity he did not keep the git history, it woudl be easier to see the changes)\nMy recommendation would be to pickup one and add it to openzipkin org. Then refuse to support any other attempt other than that one.\n. That would be tough as it would depend on size of the servers, network, etc.. That would be tough as it would depend on size of the servers, network, etc.. What's the timeline? First changes to internal storage then API or all together?. What's the timeline? First changes to internal storage then API or all together?. The flag is a good idea, would allow for incremental adoption. I would imagine that we just wipe out old data and store new in the new format. I think most users do not store the data for long anyways\n. The flag is a good idea, would allow for incremental adoption. I would imagine that we just wipe out old data and store new in the new format. I think most users do not store the data for long anyways\n. So then new spans will be stored in the span2 format? How are you planning to roll this out?. So then new spans will be stored in the span2 format? How are you planning to roll this out?. I like that the second item, maybe a configuration item so you can opt in this new format at some point when you are ready? Or things are getting too complicated?. I like that the second item, maybe a configuration item so you can opt in this new format at some point when you are ready? Or things are getting too complicated?. Woah, that's great, thanks!. Seems a hacky solution, most people will not have this setup. Would the button appear in the UI by default? It may set expectations it would work when in most cases would not. Seems a hacky solution, most people will not have this setup. Would the button appear in the UI by default? It may set expectations it would work when in most cases would not. @naoman  I think it is hacky because to add this new functionality, I need to double my infrastructure.. Just curious, in your performance numbers above there are less than half the req/sec in the after compared with the before. Ruby will definitely be the bottleneck.\nThanks for the details!. I'm ambivalent. \nCompanies (as ourselves) will deploy this in their internal networks as part of the monitoring infra. \nIn our particular case we do have in some homegrown software links to slack rooms and I'm concerned people will confuse those.  If you start seeing people in gitter asking why the User service is slow, you know why \ud83d\udc83 \n. CC @johnduhart. We are hardcoding sa to 1 somehow we thought that was the way. We'll change that to true (boolean or the string \"true\"?) if you can verify.\nMore weird still, I've looked at the code and I see where we set the sa, it is in the client span, which in fact it is in the trace also. At that point in the code there are other annotations, and they do show up:\n{\n    \"traceId\": \"7309633b16097a5c\",\n    \"parentId\": \"4f829b6d0eb7bfb5\",\n    \"id\": \"d1887feaeb2591de\",\n    \"kind\": \"CLIENT\",\n    \"name\": \"post\",\n    \"timestamp\": 1551123015543929,\n    \"duration\": 993347,\n    \"localEndpoint\": {\n      \"serviceName\": \"hermes-sandbox\",\n      \"ipv4\": \"172.17.0.20\",\n      \"port\": 3000\n    },\n    \"tags\": {\n      \"http.method\": \"POST\",\n      \"http.path\": \"/v1/protocol_versions/94365fe0-d84d-4ed7-af48-3991b12bde4a/publish\",\n      \"http.status\": \"200\",\n      \"sa\": \"1\"\n    }\n  },\nand nowhere else the sa is setup, so really no idea where this particular sa with no extra info comes from.\nSo to summarize, we have 3 pieces here , a SERVER, a CLIENT, both I can find in the client code. And a SA which has no extra info which I can't find.\nAlso we have the SA == 1 thing which if you tell me so I'll change to the boolean true.\n. The only place where I see SA  set is here\nspan.record_tag(Trace::BinaryAnnotation::METHOD, method.upcase, Trace::BinaryAnnotation::Type::STRING, local_endpoint)\n        span.record_tag(Trace::BinaryAnnotation::PATH, url.path, Trace::BinaryAnnotation::Type::STRING, local_endpoint)\n        span.record_tag(Trace::BinaryAnnotation::SERVER_ADDRESS, SERVER_ADDRESS_SPECIAL_VALUE, Trace::BinaryAnnotation::Type::BOOL, remote_endpoint)\n        span.record(Trace::Annotation::CLIENT_SEND, local_endpoint)\n        response = @app.call(env).on_complete do |renv|\n          record_response_tags(span, renv[:status].to_s, local_endpoint)\n        end\n        span.record(Trace::Annotation::CLIENT_RECV, local_endpoint)\nWhich is a client span. I never see it in a server span.\nYeah I want to do V2 also, not enough hands. It is not missing information, the client span is there, the server span is there, and then there is this extra sa thing which I can't see where is coming from. Fixed by https://github.com/openzipkin/zipkin-ruby/pull/139. Does the trace lookup header have any purpose in this screen?. If it has no use, then should not be there. \nAnother option would be to, have a \"search traceid\" component instead, which is also a functionality in the classic UI.\nThat would make sense, this would be the place where you look at one single trace, either by searching by its traceid in the storage or uploading it from outside storage\n. It is an intriguing idea, to me it is less discoverable as UI (I did not even see the traceid search option there) but I can see the appeal of a single search bar, something like the browser. . Cool, thanks!. Cool, thanks!. Agreed. What's the meaning of having endpoint sent all the time if there is no use for it and the canonical source is ca sa ?\nIn fact makes it confusing.\nThe original ruby code was assuming cr cs should have the servicename and addresses of the server which was called. Kind of makes sense. And the UI looks good following that assumption also.\n. Shouldn't you be using the constants you have added?\n. maybe \ns\"window.config =  ${ZipkinJson.writeValueAsString(config)};\"\n. I usually see only OK but does nor really matter, I do not think any tool would check anything other than the 200 response\n. So is it possible to deploy the newest Zipkin with the old data format on the backend?. ",
    "marcingrzejszczak": "\n@marcingrzejszczak (some folks do custom auth in sleuth, presumably via custom servers?)\n\nThat's more related to securing connection between Sleuth and Zipkin. What I think would need to be done here is more related to wrapping the Tracer. In the Tracer you have the addTag method which should be used to add tags. You could write an aspect or just provide your own implementation that extends the DefaultTracer that would check some security details when somebody calls addTag.\nAnother approach would be to provide your custom implementation of the SpanReporter that would first e.g. remove the tags that shouldn't be there prior to sending them to Zipkin.. This is what was returned from the query server\n. The missing service name is a \"btw\". This one is about missing dependencies. Maybe it's related - if that's the case then I can close this one.\n. Closing until more data is gathered\n. Twice the graph. As you can see the last couple of spans don't have a\nduration set. For some spans the duration of negative whereas in the json\nit's not.\n. So it should look more or less like this (I've done the edits in Gimp ;) ):\n\nIn the initial picture you can see 16 spans. Actually those are 2 x 8 spans. However the second 8 spans have their durations completely ignored. Actually they have everything ignored in the UI. Also when I click in the span I get some negative times - I'll try to upload the image in a second.\n. And when I click at one of the spans (I've clicked it on another computer so it won't have much to do with the attached JSON) I get such entries:\n\n. It seems that the issue was with the initial sent span. Let's close it for now\n. Oh this would fix us needing to do the \"fake span\" on the server side in Sleuth (in case the communication would end in backend), wouldn't it?\n. That's a good idea. In Spring Cloud we've basically only done simple builds in Travis so our transition could have been extreme (create a CircleCI build and disable Travis ASAP). Here the process can be more gradual definitely. \n. @hvandenb @adriancole there's some work in progress on the topic. Until now it's possible to do it without having a separate tile for that. Check out the brewery which is also deployed to CF and tested there - https://github.com/spring-cloud-samples/brewery\n. ",
    "devinsba": "So it seems there are 2 slightly different concerns here for people:\n\nauthentication: securing the webapp, including the span reporting endpoint\nauthorization: securing the data from specific services to only certain users\n\nNeither of these are straightforward as many orgs will have different ways of authenticating applications to each other and authorizing users\nSecuring the data seems to be more involved change though from securing the endpoints. As @dsyer noted, there are UI/UX implication to hiding data from certain users and showing it to others, and there is the question of what kinds of data can be hidden. @srijs maybe you can give more clarity to which types of things (tags? logs? certain tags/logs? entire spans?) you would like to have require authorization?\nIt seems to me that any authorization strategy will require authentication so these two components can be designed separately from each other as long as any design for authorization lays out its assumptions about what the authentication component will provide it. Ok, looking up that error and looking over the database schema I have some ideas for what on our end is causing the issue.\nI do still have the concern though about the fact that a single bad trace completely stalls the receiver out so that I have to wipe kafka, zookeeper, and restart the receiver in order for messages to flow again\n. On the question about elasticsearch 2.3-, we just switched to 5.3 in AWS so we are cool with this. Based on the amount of discussion lately on messaging, my gut feeling is that if we go the zipkin2.Span route that we make messaging concerns a first class citizen of the data model if needed. Specifically around the single producer multiple consumer case. Maybe there should be a Span.Kind that makes this relationship explicit and a defined practice around it.\nMaybe I'm not following the current messaging story since I don't use it myself but I think if we go V2 we have the chance to make it first class is my point. EDIT: turns out the tests wanted me to not have the security.user.* properties even if they are empty, so I ended up switching to a profile anyway. For the question of it affecting both the API and UI, yes it does affect both.\nOn the question of the reporters working with this: Sleuth does support this out of the box with a single extra bean being required to add the Authorization header.\nHonestly I'd prefer a pluggable interface for this that would allow us to create a custom authentication module that we could drop into the docker image and would just work. I just wanted to put something out there quick in order to get a conversation started. I can try to find time to prototype an authentication provider component and create a PR with that? Or I can put together a proposal issue and try to get feedback from those in the gitter room.. Closing as my requirements have changed and a solution that works for everyone is unlikely to come from this work. We no longer need to use the http collector so an auth proxy just works\nwithout having to worry about the reporters\nOn Thu, Jul 27, 2017 at 8:37 PM, Adrian Cole notifications@github.com\nwrote:\n\nAre you in a position to share your new requirements? In back of mind was\nthinking we can re-eval some things about modularity when we go spring boot\n2\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1537#issuecomment-318524290,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAIiooBQFJd38qnbuxwjYn2ChNZUHVZ3ks5sSS1FgaJpZM4MZlwN\n.\n. Fixed :+1:. I feel like having the details slide down like a drawer out of the span might be more natural way to show that information, then it is even tied to the context. In order to authenticate requests to amazon when locking down the security on our domain, we will need to be able to add on an authentication header which is a signature based on the request body.\n\nhttp://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/what-is-amazon-elasticsearch-service.html#signing-requests\nhttp://docs.aws.amazon.com/general/latest/gr/signature-v4-examples.html#signature-v4-examples-java\n. By default there will be no change for users other than seeing the randomly generated UUID password that spring-security-starter creates no matter what. An issue should probably be created for this before merge. What changed to cause this?. Our JS/CSS bundles get a hash appended right?. ",
    "anuraaga": "Authentication during collection is probably pretty easy as long as the storage is only accessible from zipkin (a precondition for any authorization I think) - SSL certificates work well for providing an identity in internal RPCs and seem to be the standard in linkerd, kubernetes, etc (I use it with armeria too for RPCs, though not with zipkin which I'd rather be free as in beer ;) ). After that, it's just up to the data model to store this restriction some way - a somewhat complicated way that doesn't require changing the model would probably be some binary annotation like secure_annotations = \"adriancole;key1,key2,key3\" (not really binary at this point...). The collector would validate this annotation against the provided identity (SSL certificate). The web UI could then trust this annotation to authorize when returning spans.\nAuthenticating and authorizing requests to a web UI tends to be more complicated - if I was a Google Apps user I would expect authenticating to be Google Login Oauth and authorizing a lookup of Google groups. Other organizations might have different methods for employee authentication like LDAP. \nI guess the three components needed would be \n1) What are the authorized parts of a span (specific annotations, entire span, etc)\n2) Given a span-writing request, what's it's identity (pluggable, default could be SSL certs)\n3) Given a request to the web ui, what identities can it access (pluggable, not sure of a good default)\nEdit: I recalled vanilla zipkin might not support SSL - I guess that would be a prerequisite to any work on security.. Authentication during collection is probably pretty easy as long as the storage is only accessible from zipkin (a precondition for any authorization I think) - SSL certificates work well for providing an identity in internal RPCs and seem to be the standard in linkerd, kubernetes, etc (I use it with armeria too for RPCs, though not with zipkin which I'd rather be free as in beer ;) ). After that, it's just up to the data model to store this restriction some way - a somewhat complicated way that doesn't require changing the model would probably be some binary annotation like secure_annotations = \"adriancole;key1,key2,key3\" (not really binary at this point...). The collector would validate this annotation against the provided identity (SSL certificate). The web UI could then trust this annotation to authorize when returning spans.\nAuthenticating and authorizing requests to a web UI tends to be more complicated - if I was a Google Apps user I would expect authenticating to be Google Login Oauth and authorizing a lookup of Google groups. Other organizations might have different methods for employee authentication like LDAP. \nI guess the three components needed would be \n1) What are the authorized parts of a span (specific annotations, entire span, etc)\n2) Given a span-writing request, what's it's identity (pluggable, default could be SSL certs)\n3) Given a request to the web ui, what identities can it access (pluggable, not sure of a good default)\nEdit: I recalled vanilla zipkin might not support SSL - I guess that would be a prerequisite to any work on security.. @adriancole Do you mind taking a look at this?\n. FWIW, it is possible to adjust the default queue size through elasticsearch.yml or cluster settings if you need\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html\n...though Adrian beat me to it ;)\nBut as Adrian said, it seems like you are having issues with a highly-synthetic workload, not a real-world one. If this synthetic workload is important to you, then I'd recommend trying to modify the cluster settings, but otherwise would recommend trying to send spans in batch instead (as the Java brave would). And keep in mind, Elasticsearch is IMO a medium-scale datastore. It's not going to get reasonable performance on a single node (this config is only for testing / development) and it's not unlikely you'd run into perf issues with it.\n. LGTM - thanks for fixing (and feel the pain on the difficulty of unit testing ES client...).\n. Surprised by the double counting but anyways LGTM\n. LGTM Could have sworn I kept UTC in mind when writing it but maybe it was indeed all in the mind :)\n. LGTM\n. LGTM\n. I think having a path that can save to an index named something like '.zipkin-infinite', which is not a daily bucketed index, would be pretty trivial to implement and allow saving traces. I don't think elasticsearch's TTL functionality should be used at all as per the caveats in their documentation\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-ttl-field.html\n1) It's deprecated with no successor\n2) Performance hit is not worth it compared to pruning daily indexes\n. Just want to clarify one point, that strictly speaking dropping daily indexes isn't handled \"in ES\". ES team maintains the curator tool that makes it relatively easy, but still requires installing + running a cron-type job.\nI can see an argument for the ease of set-up of zipkin-server including a background job that handles cleaning up daily indexes. It also seems weird for a frontend server, which there may be N replicas of, running cleanup tasks against a single backend - the worst case though would probably be N-1 replicas logging an error and humming along.\nSo my recommendation would be that it'd be ok to have a background cleanup thread, with clearly documented caveats that it's mainly for simple deployments and in general is preferred to use a cleanup cron external to zipkin-server.\n. For a deployment with a single zipkin-server, I think curator and DeleteIndex are essentially equivalent. For a deployment with multiple zipkin-servers (IMO any production deployment should have at least 2 for redundancy), I'd be nervous having multiple machines essentially competing to have their DeleteIndex succeed, where all but one will fail - normal failures tend to make systems fragile. \n. For a deployment with a single zipkin-server, I think curator and DeleteIndex are essentially equivalent. For a deployment with multiple zipkin-servers (IMO any production deployment should have at least 2 for redundancy), I'd be nervous having multiple machines essentially competing to have their DeleteIndex succeed, where all but one will fail - normal failures tend to make systems fragile. \n. There would never be a feature difference between the Transport client and REST since non-Java clients have to use the REST client ;)\nThe Transport client is a lot more efficient thanks to somewhat more compact serialization and multiplexing over the wire. ES unfortunately doesn't support HTTP2 on the rest port so the lack of multiplexing can matter at high loads.\nI think it's possible to convert a request object that we construct already directly into a json object that can be POSTed, so supporting both may not be so bad. Though I generally don't recommend AWS managed ES since going by history, it'll be a year behind the times again when 5.0 comes out...\n. There would never be a feature difference between the Transport client and REST since non-Java clients have to use the REST client ;)\nThe Transport client is a lot more efficient thanks to somewhat more compact serialization and multiplexing over the wire. ES unfortunately doesn't support HTTP2 on the rest port so the lack of multiplexing can matter at high loads.\nI think it's possible to convert a request object that we construct already directly into a json object that can be POSTed, so supporting both may not be so bad. Though I generally don't recommend AWS managed ES since going by history, it'll be a year behind the times again when 5.0 comes out...\n. Just for reference, Stackdriver Trace uses a 32-byte string in their protobuf API. 16 bytes wasted, but compared to the rest of the span, maybe not a big deal.\nhttps://cloud.google.com/trace/api/reference/rpc/google.devtools.cloudtrace.v1\nI tend to have a preference for readable IDs since they're used very often in debugging, so find the string type appropriate for it but can also see arguments for wanting to optimize the data over the wire.\n. LGTM thanks!\n. Agree that duration queries across documents without a preprocessing step would be very difficult. Though now I need to catch up on what an async span is to understand the implication of going back to start/duration ;). SGTM!. SGTM. SGTM. I believe this is about adding HTTP host (e.g. zipkin.io) which is generally just one string, not machine hostname which will be one of many load balanced machines and thus high cardinality.\nFor reference, in Kubernetes, you usually refer to other services as something like  my-cool-service.svc.less-cool-namespace.cluster.local. Now that I think about it, if I could convert this into the service name (my-cool-service), I wouldn't need to set localServiceName to Tracing all the time which would be nice.. I think there are many systems where the host is the only way, or the only practical way, for a request to reach a server. In such a case I guess the host has meaning in a server-side context too.\nReally though, it means that host doesn't provide additional value over the service name where it's generally semantically equivalent - I don't know if I'd recommend tagging http.host but would possibly recommend instrumentation provide an easy way to convert host to service name.. If you use armeria :) Upstream gRPC doesn't support it yet I think, or even if it did it would only be HTTP/2, not HTTP/1.. I guess the talking points would be that armeria still isn't GA and has more dependencies. But agree that more ports is annoying and a big reason I love armeria :). For the zero copy aspect, it might be simplest to define the client API in zipkin-api as is but use an internal proto here that replaces Span with bytes. Embedded messages and strings are encoded exactly the same in proto.\nhttps://developers.google.com/protocol-buffers/docs/encoding. Also I guess you would want to use the new support for plugging in as a spring webflux server - I don't know what that means since I haven't used spring recently but probably relevant.\nhttps://github.com/line/armeria/commit/70785f8438e25385c5a30947ffc26a1ad3ab148a\nAnd if you use this unsafe option you'd be able to have real zero-copy with no heap arrays involved until the zipkin2.Span\nhttps://github.com/line/armeria/blob/master/grpc/src/main/java/com/linecorp/armeria/server/grpc/GrpcServiceBuilder.java#L181. I was thinking this would be simplest by having\nPublic API proto, used by clients\n```\nmessage WriteSpansRequest {\n  repeated zipkin.proto3.Span span = 1;\n}\nservice ZipkinService {\n  rpc WriteSpans (stream WriteSpansRequest) returns (whatever);\n}\n```\nwhile the server actually implements a different, non-exposed proto \n```\nmessage WriteSpansRequest {\n  repeated bytes span = 1;\n}\nservice ZipkinService {\n  rpc WriteSpans (stream WriteSpansRequest) returns (whatever);\n}\n```\nThe protos are compatible so the clients get the advantage of the generated code and easy-to-populate structs while the server would only see the ByteString and decode straight into a Java span.\nIt has the downside of duplicate protos but it's probably a lot less complex than a custom BindableService. I was thinking this would be simplest by having\nPublic API proto, used by clients\n```\nmessage WriteSpansRequest {\n  repeated zipkin.proto3.Span span = 1;\n}\nservice ZipkinService {\n  rpc WriteSpans (stream WriteSpansRequest) returns (whatever);\n}\n```\nwhile the server actually implements a different, non-exposed proto \n```\nmessage WriteSpansRequest {\n  repeated bytes span = 1;\n}\nservice ZipkinService {\n  rpc WriteSpans (stream WriteSpansRequest) returns (whatever);\n}\n```\nThe protos are compatible so the clients get the advantage of the generated code and easy-to-populate structs while the server would only see the ByteString and decode straight into a Java span.\nIt has the downside of duplicate protos but it's probably a lot less complex than a custom BindableService. For the copying issue, just to clarify, using the proto approach makes\nByteBuf -> zipkin.proto3.Span -> byte[] -> zipkin2.Span become\nByteBuf -> byte[] -> zipkin2.Span\nand then using Armeria's unsafe option can make it become\nByteBuf -> zipkin2.Span \nProto3Codec needs to be updated to accept ByteBuffer, which is pretty simple change I think. On the flip side, I think it's not practical for it to parse the InputStream directly since doing that efficiently would probably require using the KnownLength class inside of Proto3Codec which shouldn't have gRPC dependencies.\nOf course it depends on the decision of moving to armeria, but if we do, I'd still recommend using the proto trick + unsafe option.\n. For the copying issue, just to clarify, using the proto approach makes\nByteBuf -> zipkin.proto3.Span -> byte[] -> zipkin2.Span become\nByteBuf -> byte[] -> zipkin2.Span\nand then using Armeria's unsafe option can make it become\nByteBuf -> zipkin2.Span \nProto3Codec needs to be updated to accept ByteBuffer, which is pretty simple change I think. On the flip side, I think it's not practical for it to parse the InputStream directly since doing that efficiently would probably require using the KnownLength class inside of Proto3Codec which shouldn't have gRPC dependencies.\nOf course it depends on the decision of moving to armeria, but if we do, I'd still recommend using the proto trick + unsafe option.\n. I think streaming is interesting if the client doesn't do any batching so if you try comparing the perf, I'd recommend doing it without batching in the streaming version at least as an option. Yeah just going unary seems fine. I think streaming without batching is interesting since HTTP/2 flow control is basically a type of batching - having two layers of batching feels like it should be unnecessary, so it would be interesting to see if it makes things simpler to just use the flow control and stream a Span at a time.. /cc @hyangtack @trustin. /cc @hyangtack @trustin. By the way I use armeria to push traces to Stackdriver in production using this\nhttps://github.com/curioswitch/curiostack/blob/master/common/google-cloud/trace/src/main/java/org/curioswitch/gcloud/trace/GcloudTraceModule.java\nIt's self traced so I keep on pushing spans forever ;). Added a commit that forwards unhandled requests through the tomcat layer. Interestingly, we're back to the standard armeria-spring example instead of the minimal one. It seems to make sense for zipkin-server to include a servlet context for things like the UI unless we instead ported the latter off of webmvc.. Thank for listing up the diffs. I remembered that we should be able to use jetty similar to tomcat here, as a servlet-api translator. It might be smaller. Though actually I didn't look too much into the webmvc code yet should we just port it to annotated services?\nSince the default config for zipkin uses http instead of https, excluding boringssl could make sense. JDK9+ users wouldn't really need it anyways. Another option is to include the linux-only jar, armeria includes the uber jar for ease of getting started on various OS.\nhibernate-validator I had always assumed is a requirement of spring boot - if not I suspect there's sometime that could change on armeria side to make it optional.\nAnd finally a large part of the armeria jar is the doc service webapp. I had originally thought of making it a separate jar in case users want to exclude it but @trustin suggested keeping it in core for simplicity. @trustin any thoughts? Though the doc service's swagger support might be useful in zipkin server anyways.. Realized ui is just a simple controller mostly returning static files so it was easy to port to armeria. Actually @trustin just added some much easier APIs to add cache headers + redirect to index.html so we should wait until next armeria release before finishing those up. But managed to remove tomcat and validator.. Yeah sorry should have mentioned there's some known failing tests. After the next armeria release, which I think is soon, it'll be much easier to fix them :). Recommend final since it's a branched initialization \n. While in some sense it \"limits\", I think it's a bit too semantic.\nThe number of shards to split the index into. Each shard and its replicas are assigned to a machine in the cluster. Increasing the number of shards and machines in the cluster will improve read and write performance. Number of shards cannot be changed for existing indices, but new daily indices will pick up changes to the setting. Defaults to 5.\n. The number of replica copies of each shard in the index. Each shard and its replicas are assigned to a machine in the cluster. Increasing the number of replicas and machines in the cluster will improve read performance and redundancy, but not write performance. Number of replicas can be changed for existing indices. Defaults to 1. It is highly discouraged to set this to 0 as it would mean a machine failure results in data loss.\n. Heh - modifying the JSON was supposed to be the annoying part of this, but I didn't think of this reasonable(-ish) solution :)\n. Since you're using assertj, just wondering any reason not to use \nassertThatThrownBy(() -> call.enqueue(callback)).isInstanceOf(IllegalStateException.class);. Assert foo is \"foo\" (or could do something like foo.equals(\"foo\") ? barCall : badCall). I believe it would be the same exact logic if you use Call.create(Collections.emptyList()) here instead of clone, but would let you avoid having the extra static constant.. Can we add a Ticker here and inject it so no need for the powermocks?. Would put this comment on the if statement instead of this, was confused how a var declaration can have that much power. This is a bit scary, it seems like in a bad case, which maximumSize wants to protect against, threads may just be spinning on this.. This seems like overkill if the impact is an extra lost suppression or so. It seems like it's safe to remove the while loop above if we loosen the preciseness here which might be safer.. Dunno if it's kosher for this codebase but awaitility makes these sort of tests simpler without having to worry about latches. Think assertj has an override for atomicinteger, something like assertThat(trueCount).hasValue(Foo). Just a quick note, did you try setting webEnvironment above to NONE?. I think HttpReaponse will be on the event loop, not blocking executor. There's some blocking here (though easily made async which we should at some point). I'm looking at making an armeria module for actuator, mostly by just following this code. Just wondering, why did you need this class? I'm not really following what it does, but suspect a change in core could be made to avoid it.. Thanks, will check it. Wouldn't have figured that out since from what I can tell, /loggers is the only default endpoint that sets consumes!. Hi @hyangtack - as that issue has been fixed, I tried deleting the below preflight hack but it still didn't work. Is there something else that needs to be done too?. @hyangtack Thanks! By the way I should have mentioned that by \"didn't work\", I meant the integration test didn't succeed. I didn't dig at all so there is also the chance the test itself needs an update and not Armeria (I don't know cors very well and it's not something I want to add to my life ;) ). Does this need to be run after the other catch all bean? If the two have an ordering dependencies, we should specify values to @Order to guarantee the order I think.. Ah no don't think we need to always specify it when there is no order dependency for the default value. But just wanted to confirm there is no ordering issue with the bean here which has the same order.\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-autoconfigure/metrics-prometheus/src/main/java/zipkin2/autoconfigure/prometheus/ZipkinPrometheusMetricsAutoConfiguration.java. ",
    "pbadenski": "For all who just need a simple oauth2 in front of zipkin web ui we used nginx + oauth2_proxy: https://gist.github.com/pbadenski/10069aaab5d8f0a2b2f8e943c2502e0e. ",
    "ajessup": "SPIFFE (https://github.com/spiffe/spiffe) could be a good fit here. If Zipkin supported the SVID X.509 certificate (or even better, the SPIFFE Workload API) then it would (a) be able to automatically extract the service name from the certificate's SAN, and (b) would be able to use the associated PK to establish an mTLS connection to the Zipkin server. The SPIFFE infrastructure (eg. https://github.com/spiffe/spire) would automatically issue PKs to each client, and certificate bundles to the server, rotating them as necessary.. ",
    "jbedalov": "@adriancole, we tested the auth configurations you recommended above and they work nicely but for only the GET APIs and WebUi however there are a issues with securing the POST APIs.\n\n\nThe zipkin server's POST APIs use undertow, so they are missed in the Spring Security configuration https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/java/zipkin2/server/internal/ZipkinHttpCollector.java\n\n\nClients cannot inject any auth headers, they only add content type https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin2/sender/RestTemplateSender.java#L125. \n\n\nZipkinRestTemplate is package private so it would be a bad code smell to inject auth headers there.\n\n\nJust a heads up for those interested in pursuing an auth approach. For the most part, there's no way to lock down POST without some sort of private network approach. Maybe the other transports Kafka or Scribe may have a way to auth writing.. > You could deploy the POST servers separately from the GET servers, behind a network firewall, if that's sufficient?\n@eirslett, yeah basically, that's what we will have to do. Thanks for the replies.\n. > You could deploy the POST servers separately from the GET servers, behind a network firewall, if that's sufficient?\n@eirslett, yeah basically, that's what we will have to do. Thanks for the replies.\n. ",
    "MatMoore": "Ah... I hadn't checked zipkin.io... I followed the link from README.md to http://twitter.github.com/zipkin instead. Can we just change that?\nI should clarify that I'm not using docker, so it looks like I should just grab a jar from bintray for each of the individual services I plan to use?\nI know there has been a lot of work to separate out non-essential components but I hadn't thought to look in the subdirectories for readmes. It would be great if we could integrate that information into the zipkin.io docs as well.\n. I think just what goes where and how to start is enough for me, but I'm sure recipes would be helpful. I'm personally using ansible and could maybe contribute a sample playbook. If I have time this weekend I'll try running the latest artifacts.\n. Just to update - I did eventually manage to get it working using the fat jars from bintray.\nThis is just a small development deployment and last time I checked the mysql storage wasn't able to expire old data, so I went with redis instead. I also used scribe rather than kafka to keep things simple.\nI am running everything like this:\njava  -jar zipkin-collector-service-1.25.1-all.jar  -f /collector-redis.scala\njava  -jar zipkin-query-service-1.25.1-all.jar  -f /query-redis.scala\njava -cp 'libs/' -jar zipkin-web-1.25.1-all.jar -zipkin.web.cacheResources=true\n. ",
    "pbetkier": "@jamescway I'm referring to zipkin-cassandra-core/src/main/resources/cassandra-schema-cql3.txt DDL script that is executed during startup of cassandra-based collector and query service. It's called in the constructor of org.twitter.zipkin.storage.cassandra.Repository and as far as I can see there is currently no way of disabling it. I need control over what's being executed :) Ideally make zipkin not execute anything during startup and let me handle the database schema by myself.\n. ",
    "pszymczyk": "I'm not sure about suggested commit message: 'To run with less privileges, set the\nenvironment variable CASSANDRA_ENSURE_SCHEMA=true, when you have less privileges you probably don't want to ensure schema and execute script on your DB so CASSANDRA_ENSURE_SCHEMA should be set to false?\n. Wow great to hear that! :D thanks for quick review :)\n. ",
    "drax68": "Anyone can help with this? @adriancole?\n. Anyone can help with this? @adriancole?\n. Here is a stacktrace, it times out strictly after 12 seconds:\n06:53:38.952 [cluster1-nio-worker-0] ERROR c.t.f.h.i.e.FinatraDefaultExceptionMapper - internal server error\ncom.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: cassandra.example.com/1.2.3.4:9042 (com.datastax.driver.core.OperationTimedOutException: [cassandra.example.\ncom/1.2.3.4:9042] Operation timed out))\n        at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:84) ~[zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37) ~[zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.ArrayBackedResultSet$MultiPage.prepareNextRow(ArrayBackedResultSet.java:285) ~[zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.ArrayBackedResultSet$MultiPage.isExhausted(ArrayBackedResultSet.java:245) ~[zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.ArrayBackedResultSet$1.hasNext(ArrayBackedResultSet.java:126) ~[zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at org.twitter.zipkin.storage.cassandra.Repository$6.apply(Repository.java:439) ~[zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at org.twitter.zipkin.storage.cassandra.Repository$6.apply(Repository.java:435) ~[zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.google.common.util.concurrent.Futures$1.apply(Futures.java:713) ~[zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.google.common.util.concurrent.Futures$ChainingListenableFuture.run(Futures.java:861) ~[zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.google.common.util.concurrent.AbstractFuture.set(AbstractFuture.java:185) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:144) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:183) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler.access$2300(RequestHandler.java:44) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler$SpeculativeExecution.setFinalResult(RequestHandler.java:748) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:429) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1013) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:936) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60]\nCaused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: cassandra.example.com/1.2.3.4:9042 (com.datastax.driver.core.OperationTimedOutException: [cassandra.example.com/1.2.3.4:9042] Operation timed out))\n        at com.datastax.driver.core.RequestHandler.reportNoMoreHosts(RequestHandler.java:217) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler.access$1000(RequestHandler.java:44) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler$SpeculativeExecution.sendRequest(RequestHandler.java:276) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler$SpeculativeExecution$1.run(RequestHandler.java:374) ~[zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_60]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_60]\n        ... 1 common frames omitted\n06:53:38.953 [cluster1-nio-worker-0] INFO  c.t.f.h.filters.AccessLoggingFilter - 127.0.0.1 - - [14/Nov/2015:06:53:38 +0000] \"GET /api/v1/spans?serviceName=servicename HTTP/1.1\" 500 36 12085 \"-\"\n06:53:40.351 [cluster1-worker-18] ERROR c.t.f.h.i.e.FinatraDefaultExceptionMapper - internal server error\ncom.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: cassandra.example.com/1.2.3.4:9042 (com.datastax.driver.core.OperationTimedOutException: [cassandra.example.com/1.2.3.4:9042] Operation timed out))\n        at com.datastax.driver.core.RequestHandler.reportNoMoreHosts(RequestHandler.java:217) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler.access$1000(RequestHandler.java:44) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler$SpeculativeExecution.sendRequest(RequestHandler.java:276) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler$SpeculativeExecution$1.run(RequestHandler.java:374) [zipkin-query-service-1.23.2-SNAPSHOT-all.jar:na]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_60]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_60]\n        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60]\n06:53:40.351 [cluster1-worker-18] INFO  c.t.f.h.filters.AccessLoggingFilter - 127.0.0.1 - - [14/Nov/2015:06:53:40 +0000] \"GET /api/v1/traces?limit=10&serviceName=servicename&minDuration=&spanName=all&annotationQuery=&endTs=1447483984514000 HTTP/1.1\" 500 36 12043 \"-\"\n. Hi, any update on this?\n. It's actually not connectivity issue, all servers hosted inside vpc and networking is not a problem. \nThis error arises only if there is large amount of data to fetch from cassandra (large traces with thousands of spans), and cassandra requires more than 12 seconds to provide it. It matter of cassandra performance (though, it hosted on r3.xlarge with ssd disks).\nNow we have ~15Gb of data daily, and zipkin becomes unusable after 1-2 days.\nI guess more people will require that timeout to be configurable in future.\nDCAwareRoundRobinPolicy shouldn't be the issue, since there is single cassandra instance.\n. Currently running 1.25.1-SNAPSHOT with cassandra 2.0.17.\nWe are trying to keep 7d of data (afair, same default for lookback).\nRoughly, it's 16 000 000 traces per day.\n. Had to add \"import com.datastax.driver.core.Cluster\"\nHere is a fresh stacktrace with suggested changes in, timeout override not works (let's move to gitter):\n22:15:19.425 [cluster1-nio-worker-3] INFO  c.t.f.h.filters.AccessLoggingFilter - 127.0.0.1 - - [18/Nov/2015:22:15:19 +0000] \"GET /api/v1/traces?limit=10&serviceName=serviceName&minDuration=&endTs=1447871160169&spanName=all&annotationQuery= HTTP/1.1\" 200 101450 1461 \"-\"\n22:15:31.058 [cluster1-nio-worker-2] ERROR c.t.f.h.i.e.FinatraDefaultExceptionMapper - internal server error\ncom.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: cassandra.example.com/1.2.3.4:9042 (com.datastax.driver.core.OperationTimedOutException: [cassandra.example.com/1.2.3.4:9042] Operation timed out))\n        at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:84) ~[zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37) ~[zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.ArrayBackedResultSet$MultiPage.prepareNextRow(ArrayBackedResultSet.java:285) ~[zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.ArrayBackedResultSet$MultiPage.isExhausted(ArrayBackedResultSet.java:245) ~[zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.ArrayBackedResultSet$1.hasNext(ArrayBackedResultSet.java:126) ~[zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at org.twitter.zipkin.storage.cassandra.Repository.lambda$getSpanNames$3(Repository.java:463) ~[zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.google.common.util.concurrent.Futures$1.apply(Futures.java:713) ~[zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.google.common.util.concurrent.Futures$ChainingListenableFuture.run(Futures.java:861) ~[zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.google.common.util.concurrent.AbstractFuture.set(AbstractFuture.java:185) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:144) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:183) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler.access$2300(RequestHandler.java:44) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler$SpeculativeExecution.setFinalResult(RequestHandler.java:748) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:429) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1013) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:936) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60]\nCaused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: cassandra.example.com/1.2.3.4:9042 (com.datastax.driver.core.OperationTimedOutException: [cassandra.example.com/1.2.3.4:9042] Operation timed out))\n        at com.datastax.driver.core.RequestHandler.reportNoMoreHosts(RequestHandler.java:217) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler.access$1000(RequestHandler.java:44) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler$SpeculativeExecution.sendRequest(RequestHandler.java:276) [zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at com.datastax.driver.core.RequestHandler$SpeculativeExecution$1.run(RequestHandler.java:374) ~[zipkin-query-service-1.25.1-SNAPSHOT-all.jar:na]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_60]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_60]\n        ... 1 common frames omitted\n22:15:31.069 [cluster1-nio-worker-2] INFO  c.t.f.h.filters.AccessLoggingFilter - 127.0.0.1 - - [18/Nov/2015:22:15:31 +0000] \"GET /api/v1/spans?serviceName=serviceName HTTP/1.1\" 500 36 13109 \"-\"\n. Sure, attaching report here 832.report.txt\n. Yes, no reads. Uptime should be near 40 hours.\n. I mean that value didn't applied. You can see that from stacktrace I posted earlier.\n. Here is a trace \nzipkin-cassie.trace.txt\n. Replication factor is default and unchanged due to single-node cassandra setup.\nIssue repeatable with cassandra 2.2.3, after few hours of running. There is also very low read io rate (kilobytes), I assume that all data cached in-memory by cassandra.\nFrom traces \"Response received on stream 0 but no handler set anymore\" ~50ms after zipkin closed connection.\nAnything else to test?\n. @michaelsembwever \n1. Confirmed, exception always contains org.twitter.zipkin.storage.cassandra.Repository.lambda$getSpanNames\n1. New version, just after fresh exception \n   832.report2.txt\n2. When caching enabled got same exception.\n3. 300000 span_names for single service. Need to sync up with service developers.\nGuys, you are extremly helpfull, thank you.\n. #835 helped, thank you. Will hope that 5mb will be enough for gzipped data. Now dealing with #832 \n. #835 helped, thank you. Will hope that 5mb will be enough for gzipped data. Now dealing with #832 \n. Expected that new tables addition automated, since zipkin can initially create tables.\n. No, auto-addition not disabled in any way.\n. I'm running custom-built jars using https://github.com/openzipkin/zipkin/blob/master/zipkin-query-service/config/query-cassandra.scala \nIn that case what value will be assigned to zipkin.store.cassandra.ensureSchema?\n. After checking scala config, noticed that it lacks line you pointed. That's the case, thank you.\n. I guess driver threats cassandra installation as non-local, due to non-default dc name we use. Hope this will be changed after #967\n. ",
    "netmilk": "@adriancole I think it's a great idea! The possibility to let non native programmers edit the documentation and be sure it's still valid is the win of using Dredd. \nThe idea behind Dredd is be able abstract the discussion about the HTTP API to the lingua franca contract - the API Blueprint, but keep it tightly bound to the implementation with hooks and tested in CI (!), so it won't get out of sync.\nI think the only catch in your case can be Dredd doesn't support Scala or Java at this moment. But it has precedent support for compiled language (Go) and tutorial support for adding new language including the language agnostic testing harness so it shouldn't be a problem to add your's if needed. \nI'm excited to help with anything. Just ping me here, on twitter (@ntmlk) or we can schedule a little call if needed.\n. @adriancole  I just had a look at the API Blueprint document in #888. I don't know how does the entire API looks like but it API seems to be quite simple to eme. If it has only GET methods you could probably seed it with some script before Dredd execution and any sophisticated isolation of each action with hooks shouldn't be necessary.\n. ",
    "karlunho": "Will look at this over the weekend\n. Will look at this over the weekend\n. I'd love to see an ES backend, because then it would in theory give the ability to do faceted searches.\nBtw - I'm self promoting here, but if you might be interested in Apache usergrid (http://usergrid.apache.org ). It's a Backend as a Service that uses ES for indexing but C* for the the storage. It was originally designed for mobile app back ends , but we've built in graph query capabilities on top of C_, and added all sorts of neat features that might be applicable such as C_ counter abstractions, which might be useful for doing aggregations. \nThis is similar to the idea of how AWS recommends using ES in conjunction with DynamoDB (except that Usergrid can run in a docker container ;-)\nOk - this is the 2nd self promo of usergrid today since Parse shut down today too ;-) \nLet me know if anyone is interested ....\nRegards,\nAlan Ho\n\nOn Jan 28, 2016, at 10:43 PM, Adrian Cole notifications@github.com wrote:\nHi!\nThanks for thinking about this! Our new data stores are in separate github\nrepositories. If it becomes very popular and well maintained, we could\nconsider promoting it into the main repo, but certainly best to at least\nincubate it outside.\nIf you get to a point where you are closer.. Maybe suggest a name!\nPs Are you planning to write this in scala or java?\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "chemicL": "I'll code a work around for now, but I suppose it would make sense to make the DependencyLink a serializable class, as it forms an interface for various uses.\n. Thanks for the clarification, I suppose for Spark it makes sense to use the thrift wrappers. I noticed @yurishkuro faced the same problem with other classes and defined his own case classes, which seems more readable. I'll leave the implementation with his approach, but this can be revisited in the future.\n. Not sure if that's a desired behaviour - assuming we select dependencies for two days, we should get an aggregation of all the spans in that period. That's why summing the counts would make more sense than selecting just the latest aggregation. Otherwise, if on day_1 there were 5 spans from A to B, and 3 on day_2, the above solution would show A -> B with count of 3, while there were total 8 spans.\nUnless I'm mistaken the sum approach would also be in line with the AnormDependencyStore implementation.\n. ",
    "sreeram-boyapati": "I faced this issue too. Can you check for it before?\n. ",
    "skstronghold": "Yes, that will do it\n. All parameters required for MySQL SSL can be passed as system properties to the JVM trying to use Mysql over SSL.\nWe can accept JAVA_OPTS for collector/query via environment variable, in which user can configure these parameters as:\n-Djavax.net.ssl.keyStore=path_to_keystore_file\n-Djavax.net.ssl.keyStorePassword=password\n-Djavax.net.ssl.trustStore=path_to_truststore_file\n-Djavax.net.ssl.trustStorePassword=password\nPath to keystores can be mounted as a volume in container.\n. ",
    "gamefundas": "I followed the example at DZone (https://dzone.com/articles/knowing-how-all-your) and ended up with a UI thats very different than the screenshots posted on the blog. \nFor me, no Timeline tab, Dependencies appearing on the top menu, no service summary, stats box, favorites etc. Sort of limited UI functionality overall. Not sure if I am running a different version of zipkin-web.\nTried the tracegen to generate some data but still nothing to the effect. Does it mean that DB backend is required for the UI to be fully featured?\n. Thanks for the clarification. We plan to use MySQL. Will try it out. Appreciate your help on this. \n. ",
    "gauravrmazra": "Thanks for clarifying @adriancole \n. ",
    "virtuald": "As a new user, I'm adding zipkin to my system because I don't know what my system is doing underneath the hood. Chances are, because the system architecture isn't managed at all (complex political problem...), it's not a DAG. \n. I've ran into this too -- and the common thread for mine seems to be that they are missing the core annotations (cs, sr, et al).\n. Sorry, that was a bad example, I missed the detection for endpoint. However, there's somewhere that it's not checking because it's crashing on me... \nAh, CORE_ADDRESS: https://github.com/openzipkin/zipkin/blob/master/zipkin-ui/js/component_ui/traceToMustache.js#L114\n. Yep, confirmed that it was a CLIENT_ADDR annotation. It appears that the value doesn't matter for CLIENT_ADDR, as it will always be overridden by the endpoint.\n. There is, but I'll need to revisit to remember what it was... \n. There is, but I'll need to revisit to remember what it was... \n. Heh. Ran into this today when I tried to modify the UI, if only if I had seen it before my flight...\n. Thanks for the comments, I'll address these tonight or tomorrow.\n. Comments addressed.\n. I've got another set of changes coming too dealing with better error messages, just testing it now.\n. Yep. Different PR.\n. The JSON should be whatever was retrieved from the call to /api/v1/traces/XXX, as it's the same object.\n. I'll try to get to the adjustments tomorrow, no time tonight.\n. Updated code to include concept of transient (has annotation) vs critical (has binary annotation) errors, added unit tests, updated screenshots, updated JSON.\n. It comes through the trigger in js/component_data/trace.js\n. This PR does not provide a mechanism to do that. I suspect that is best saved for future work -- if there isn't a way to show an uploaded trace, no sense downloading the trace? \nI could be convinced otherwise, it would be easy to add an actual raw download link somewhere.\n. Oh, ha. Didn't realize that.\n. Interesting. I suppose we could add a mechanism that could display either. I chose the default since it was already retrieved by the server -- so it would show the JSON that's currently being displayed without another call to the server. Theoretically, if you made another call down, the JSON could show something different which wasn't currently being displayed.\n. Renamed to traceJson\n. It's used in trace.mustache line 7. \n. Hm, can you use the value thing on an <a> element? Using the same event mechanism as everything else is convenient.\n. Ok, done.\n. @eirslett you're right, this ends up being cleaner anyways, it never really belonged in trace.js\n. Yeah, that seems a bit much.\n. I'll look into it tomorrow.\n. @eirslett but a.key is inside of a closure, I don't think you will be able to reassign the const each time.\n. ",
    "sidneyshek": "I feel embarrassed we haven't had the time to push up our ES implementation yet. If you guys are close then it might be worth closing off the scala repo.\n. ",
    "nicmunroe": "I like this model. Clear, concise, easy to understand what everything is and what it's for.. FWIW I'm personally in favor of the simplified single-host spans. Anything that makes it more difficult for instrumentors to accidentally introduce bugs due to confusion/complexity/etc is a good thing. And being simpler improves barrier-to-entry and learning curve issues.. FWIW I'm personally in favor of the simplified single-host spans. Anything that makes it more difficult for instrumentors to accidentally introduce bugs due to confusion/complexity/etc is a good thing. And being simpler improves barrier-to-entry and learning curve issues.. Interop with wingtips should work out of the box since trace IDs are modeled internally as strings, and we can add some config to wingtips to auto-generate trace IDs as 128 bit if desired by the user. So no concerns from me. :+1: \n. @jpiechowka Merged, built, and deployed @adriancole's wingtips PR as wingtips version 0.11.1. Thanks guys!\n. @guettli I'm late to the discussion but wanted to share with you what we do at Nike, since we loosely coupled logging and distributed tracing concerns in a way that keeps them separate but provides significant value.\nWe make sure the distributed tracing's trace ID ends up in the SLF4J MDC for each request, configure our logging system's pattern to output the trace ID from the MDC for each log message, and then log normally. All the log messages from all the services end up in a log aggregation system like Splunk which we can then use to do searches for all log messages associated with a given trace ID, or perform useful reports, etc.\nThis combination has proven to be quite powerful and I would highly recommend it. You get to leverage your distributed tracing and logging capabilities to their fullest without compromising anything or locking yourself into awkward ways of logging info, and the combination gives you benefits you wouldn't otherwise be able to take advantage of - e.g. all your log messages get tagged with the trace ID, including log messages from third party libraries, which may have the critical bit of info you need to understand why a given request failed (for example).. @guettli at the moment we track every request. The performance penalty for individual services is usually negligible (YMMV depending on instrumentation and/or throughput), and since we don't have to funnel all spans to a collector but rather do all our distributed tracing analysis via searches and reports from the log aggregator there's no performance concern on the collector. Of course there's a monetary cost in our case for having the log aggregator pick up every span from the logs, but so far the tradeoff has been worth it for us. You'll have to weigh all the performance and monetary tradeoffs yourself to see what will work best in your situation, but before assuming tracing every request will be too much of a performance burden I'd recommend doing some performance testing. You might be pleasantly surprised.\nWe have our services return the trace ID in the response headers so that if we run into a request that took too long or had an error or otherwise needs investigation we can dig into it immediately. When investigating these issues the question is almost always \"what log messages are associated with this request (trace ID)?\", not \"what log messages are associated with this small portion of the request (span ID)?\". That's why we've only worried about spitting out trace ID for every log message, not span ID. That said, it might be a good idea to spit out both trace ID and span ID for every log message. I just wouldn't recommend outputting only span ID - it's likely to be much less useful in general.. @guettli yes it is easy to get the trace ID from span ID, but it's an extra hop that is not needed the vast majority of the time (at least in our experience - YMMV). And like I said, I think it's a good idea to log both.\nAvoiding redundancy (\"don't repeat yourself\", or DRY) is good for mainline code, but it's not a mantra that should be blindly followed in all cases. For example, IMO in unit tests it's more important to have the tests be readable and understandable (and therefore maintainable) than it is to wipe out all redundancy. It's a balancing act. If DRY-ing up your unit tests makes them difficult to understand for other people (or yourself in 6 months) then you've probably gone too far. See this stackoverflow question/answer for a good overview of this idea.\nIn regards to logging trace ID and span ID - it might be sort of redundant to log both since you can technically get to trace ID from span ID but the drawback is nil (a few extra bytes per log message) and it doesn't hurt maintainability since it's just logs (the usual reason for DRY-ing mainline code), so might as well log both.\nOf course that's all just my opinion. If you're unsure, try both ways and see which makes more sense for your use cases.\n. @guettli yes it is easy to get the trace ID from span ID, but it's an extra hop that is not needed the vast majority of the time (at least in our experience - YMMV). And like I said, I think it's a good idea to log both.\nAvoiding redundancy (\"don't repeat yourself\", or DRY) is good for mainline code, but it's not a mantra that should be blindly followed in all cases. For example, IMO in unit tests it's more important to have the tests be readable and understandable (and therefore maintainable) than it is to wipe out all redundancy. It's a balancing act. If DRY-ing up your unit tests makes them difficult to understand for other people (or yourself in 6 months) then you've probably gone too far. See this stackoverflow question/answer for a good overview of this idea.\nIn regards to logging trace ID and span ID - it might be sort of redundant to log both since you can technically get to trace ID from span ID but the drawback is nil (a few extra bytes per log message) and it doesn't hurt maintainability since it's just logs (the usual reason for DRY-ing mainline code), so might as well log both.\nOf course that's all just my opinion. If you're unsure, try both ways and see which makes more sense for your use cases.\n. The Riposte thing I did was a quick proof-of-concept, and Riposte currently does not support http/2. Armeria looks like a great choice!. ",
    "RobbyTassy": "Running npm installed resolved my issue of missing webpack merge - I guess I must've deleted something while I was playing around..\n. ",
    "mohsen1": "The swagger-docs project has a configuration capability to load different api specs. Maybe you can take advantage of that and host the UI itself outside of query server. \n. ",
    "zxy12": "$ ./bin/query --info --stacktrace\n:zipkin-common:compileJava UP-TO-DATE\n:zipkin-common:compileScala UP-TO-DATE\n:zipkin-common:processResources UP-TO-DATE\n:zipkin-common:classes UP-TO-DATE\n:zipkin-common:jar UP-TO-DATE\n:zipkin-scrooge:idlJar UP-TO-DATE\n:zipkin-scrooge:copyDependencyIdl UP-TO-DATE\n:zipkin-scrooge:copyIncludedIdl UP-TO-DATE\n:zipkin-scrooge:generateInterfaces UP-TO-DATE\n:zipkin-scrooge:compileJava UP-TO-DATE\n:zipkin-scrooge:compileScala UP-TO-DATE\n:zipkin-scrooge:processResources UP-TO-DATE\n:zipkin-scrooge:classes UP-TO-DATE\n:zipkin-scrooge:jar UP-TO-DATE\n:zipkin-anormdb:compileJava UP-TO-DATE\n:zipkin-anormdb:compileScala UP-TO-DATE\n:zipkin-anormdb:processResources UP-TO-DATE\n:zipkin-anormdb:classes UP-TO-DATE\n:zipkin-anormdb:jar UP-TO-DATE\n:zipkin-cassandra-core:compileJava UP-TO-DATE\n:zipkin-cassandra-core:compileScala UP-TO-DATE\n:zipkin-cassandra-core:processResources UP-TO-DATE\n:zipkin-cassandra-core:classes UP-TO-DATE\n:zipkin-cassandra-core:jar UP-TO-DATE\n:zipkin-cassandra:compileJava UP-TO-DATE\n:zipkin-cassandra:compileScala UP-TO-DATE\n:zipkin-cassandra:processResources UP-TO-DATE\n:zipkin-cassandra:classes UP-TO-DATE\n:zipkin-cassandra:jar UP-TO-DATE\n:zipkin-query:compileJava UP-TO-DATE\n:zipkin-query:compileScala UP-TO-DATE\n:zipkin-query:processResources UP-TO-DATE\n:zipkin-query:classes UP-TO-DATE\n:zipkin-query:jar UP-TO-DATE\n:zipkin-query-service:compileJava UP-TO-DATE\n:zipkin-query-service:compileScala UP-TO-DATE\n:zipkin-query-service:processResources UP-TO-DATE\n:zipkin-query-service:classes UP-TO-DATE\n:zipkin-query-service:run\nException in thread \"main\" java.lang.NullPointerException\n    at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:213)\n    at com.google.common.io.Resources$UrlByteSource.(Resources.java:82)\n    at com.google.common.io.Resources$UrlByteSource.(Resources.java:77)\n    at com.google.common.io.Resources.asByteSource(Resources.java:71)\n    at com.google.common.io.Resources.asCharSource(Resources.java:120)\n    at com.google.common.io.Resources.toString(Resources.java:145)\n    at com.twitter.zipkin.query.Main$.main(Main.scala:36)\n    at com.twitter.zipkin.query.Main.main(Main.scala)\n:zipkin-query-service:run FAILED\nFAILURE: Build failed with an exception.\n- What went wrong:\n  Execution failed for task ':zipkin-query-service:run'.\n\nProcess 'command '/Library/Java/JavaVirtualMachines/jdk1.8.0_73.jdk/Contents/Home/bin/java'' finished with non-zero exit value 1\n- Try:\n  Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.\n\nBUILD FAILED\nTotal time: 3.712 secs\n. my jdk is 1.8, and my os is OSX, i use the step like this:\n$git clone https://github.com/openzipkin/zipkin /var/www/zipkin\n$cd /var/www/zipkin\n$./bin/query\n. @adriancole thanks a lot,now it looks good when  i remove the arg..\n. ",
    "gena01": "Thank you for fixing this. It's loading now and seems a bit faster. I guess there's no duration if both timestamps match?\n. Thank you for fixing this. It's loading now and seems a bit faster. I guess there's no duration if both timestamps match?\n. Should i open a separate bug on Last-Modified/If-Modified-Since being missing as well?\n. Should i open a separate bug on Last-Modified/If-Modified-Since being missing as well?\n. Actually i thought it was even simpler than that. Maybe I should have rephrased things a little better.\nWhen loading zipkin ui homepage (/ url) it shouldn't be loading any spans since nothing has been selected. This is also where the undefined comes from for the serviceName. Is it possible to make this a conditional of sorts just for the homepage case?\n. I had this confused with https://jcenter.bintray.com/io/zipkin/zipkin-query/1.39.0/ today. :(\n. I had this confused with https://jcenter.bintray.com/io/zipkin/zipkin-query/1.39.0/ today. :(\n. ",
    "fluidjeel": "[root@ixxxx ]# java -version\nopenjdk version \"1.8.0_77\"\nOpenJDK Runtime Environment (build 1.8.0_77-b03)\nOpenJDK 64-Bit Server VM (build 25.77-b03, mixed mode)\n. [root@ixxxx ]# java -version\nopenjdk version \"1.8.0_77\"\nOpenJDK Runtime Environment (build 1.8.0_77-b03)\nOpenJDK 64-Bit Server VM (build 25.77-b03, mixed mode)\n. JAVA_HOME was pointed to /usr/java/latest, i changes it to jdk. Apologies for the trouble\n. JAVA_HOME was pointed to /usr/java/latest, i changes it to jdk. Apologies for the trouble\n. @eirslett Thanks for you inputs . Looks like a great starting point for me. I would just leave this request open for couple of days incase I have any follow-up questions.\n. Thanks\n. ",
    "klingerf": "Ok, I will take the pro-scribe-anti-java side of this debate :stuck_out_tongue_winking_eye:\nOur primary use case for Zipkin is exporting tracing data from Finagle-based services.  Finagle includes a finagle-zipkin library that implements a Zipkin tracer. Unfortunately, that tracer is only configured to work with scribe. I'm also not a big fan of scribe, and at present we bypass scribe altogether by configuring Finagle's ZipkinTracer to send data directly to a zipkin-collector process on port 9410.  All of that said, we still require scribe protocol support for this setup to work.\nThere's an open issue with the Finagle folks tracking implementation of an http-based tracer instead: https://github.com/twitter/finagle/issues/465.  That seems promising, and given the widespread use of Zipkin+Finagle, I think that upgrading Finagle is a logical precursor to removing scribe support. It's true that folks could patch their finagle-zipkin tracers to speak http, but that feels like more of a workaround than an actual fix.\nSwapping out Finagle's scribe implementation with an http one brings up a larger issue that I think is worth pointing out.  How do we know that an http-backed Zipkin tracer is as performant as Finagle's scribe-backed Zipkin tracer?  The existing scribe tracer is widely deployed within Twitter's own infrastructure. It has been tuned and tested in thousands of super-high-traffic applications over many years. Do we have any side-by-side comparisons of the proposed http tracer versus the existing tracer?\nI have a similar concern with the zipkin-java project, although I have to admit that I'm not nearly as familiar with that project, so my concerns may be unfounded. The scala-based zipkin-collector and zipkin-query processes have been heavily tuned to withstand a high volume of requests. They operate independently, such that a degradation in one process does not affect the ability of the other to serve requests.  Do we have the same guarantees with an all-in-one zipkin-java process?  \"Lower complexity\" may not actually be an improvement in this case if it has a negative impact on resiliency.\nThanks for opening this issue as a place to track discussion. Those are just my initial thoughts / questions, and I'm certainly happy to discuss in more detail.\n. @abesto Looks great -- thanks for summing it up.\n. Just wanted to chime in and say that 3.0.8/3.8+ works for me.  We're not using full text search on annotations for anything we're doing internally here at Buoyant.  The use of materialized views seems like a huge improvement as well.  Thanks for working on this!\n. ",
    "jtrobec": "I'd like to +1 Kevin's concerns...from Twitter's perspective, moving the fundamental infra away from scribe and scala isn't inherently problematic as much as it is labor intensive. We would really need to make sure that it scales enough to support our infra before we could make a switch.\n. ",
    "zywei": "thanks very much;\n. ",
    "dangets": "Moving discussion from #1222 - Briefly I'm trying to design a way for generic saving and eviction of threads and I proposed adding some methods - SpanStore.setTraceExpiration(long traceId, Date date)  & SpanConsumer.setDefaultTraceExpiration(int amount, TimeUnit unit).  It would be up to the implementations on if they could actually handle ttl eviction and how granular the units could be.  It would be valid for these to be noops if the store couldn't support it - though this might be confusing on the UI.\nI could see MySql having a ttl column in the Spans table that could be used, Elasticsearch could just drop daily indexes, etc...\nI not a huge fan of this as it is backwards incompatible with existing implementations, but throwing it out there.\n. I'm currently prototyping Zipkin with ES backend and one concern is the eviction strategy.\nI think it'd be useful to have a configurable default ttl - days, week, etc..  But I think it'd also be pretty neat to be able to mark specific traces to be retained for much longer - so if we're working on investigation the trace and link can stay around for much longer.\nI'd be willing to help design / implement this, but would like to get a discussion going.\n. That sounds good to me - or you could always kick it out to an arbitrary date index instead of infinite.  Might need a way to delete specific traces to help cleanup the 'infinite' but this can probably be a separate issue.\n. I'd like to start prototyping some things out for this, but would like to get opinions on best places to put the functionality.\nI'd like to add a couple of methods to the interfaces, but I know this would break existing implementations if we can't use Java8 default method implementations.\nI was thinking something like SpanStore.setTraceExpiration(long traceId, Date date) and SpanConsumer.setDefaultTraceExpiration(int numDays).  Of course it we could change units to allow finer granularity - or use (int duration, TimeUnit unit)\nEDIT:  It would be up to the implementations on the granularity of eviction that they would support - so e.g. Elasticsearch could use the daily indexes to drop batches.\n. As far as kicking off the eviction process - I was thinking just a background scheduled thread.\n. I was trying to consolidate the ideas of individual trace retention along with default trace eviction - though I don't like it on the grounds of backwards-incompatibility.\n. Is curator \"better\" than doing Delete Index ?  I can see arguments for having daily cleanup external to zipkin-server, but just curious.\n. Is curator \"better\" than doing Delete Index ?  I can see arguments for having daily cleanup external to zipkin-server, but just curious.\n. Is this the same issue with the Elasticsearch backend?  Results that shouldn't be are getting filtered out.\n\n\n. We are using brave v3.15.1.  We were previously using v3.9.1 until a few weeks ago and that is around when we started seeing the problem (upgraded brave and zipkin-server jar).\nWe do use a custom jmeter setup to run automated tests and we generate a traceId before sending the initial request so that we can log traceIds alongside request logs.  Using the raw endpoint I am seeing sr and ss on the top-level span but it looks like the duration field is missing from that span.\nAgain, this was working previously - is this something that has changed during the deprecation of the SpanCollector classes?  I can try switching to the Reporter classes to see if that fixes it.. I verified that when I hit an endpoint using curl and specifying a traceId in the headers the duration does not show up.  Without the headers it does.\nAnd we found this comment that says a timestamp/duration won't be kept if a traceId is sent - perhaps the cause? https://github.com/openzipkin/brave/blob/181a2f438e5142510cb72eefc723839bc525fe4d/brave-core/src/main/java/com/github/kristofa/brave/ServerRequestInterceptor.java#L50\nDoes a backfilling as mentioned in the comment ever happen?. Done, thanks!. ",
    "basvanbeek": "Sounds good to me if we take the plugin approach for zipkin-ui, that way people have the biggest flexibility to tie this together with their usage concerns and make their own infrastructure choices with the greatest ease of use. \n. zipkin-go-opentracing does not force you to do binary annotations only. \nIf you want to annotate a span as a RPC client node calling some service, you should use: ext.SpanKindRPCClient.Set(span) on the span.\nIf you want to annotate a span as a RPC server node receiving an incoming request, you should extract the B3 headers propagating the client side span with tracer.Extract() after which you create the server side span feeding it the received wirecontext tracer.StartSpan(\"operationName\", ext.RPCServerOption(wireContext)) which can be seen in the example middleware:\nhttps://github.com/openzipkin/zipkin-go-opentracing/blob/master/examples/middleware/http.go#L74\nIf you are doing internal local component spans you can use the logic as seen here: https://github.com/openzipkin/zipkin-go-opentracing/blob/master/examples/cli_with_2_services/svc2/implementation.go#L53 where doing a timestamped LC annotation by invoking ext.SpanKind.Set(span, \"resource\") or do a span.LogEvent(\"lc\") or span.LogEvent(zipkincore.LOCAL_COMPONENT)  which is more native to Zipkin.\n. the approach I took in zipkin-go-opentracing wrt sampling has been to just keep the sampler on the low 64 bit. There is plenty of resolution in the lower 64 bit to make the sampling decision and keeps it consistent in its behavior between 64bit and 128bit traces. I don't see the need or benefit of sampling on 128bit.\n. Also want to note that as stated above the trace_id is more an internal thing. If needing to do some sort of correlation with external id's you have, it's best to use binary annotations for this. In OT speak, this would mean adding a Tag with your (uuid) string payload. I would be interested in being one of those parties moving this forward using Go, optionally taking pieces from the Zipkin-Go pakage.\nRegardless with which language / ecosystem we end up with I think we could already start specifying our requirements, wishlists, scope, etc.. Still have an issue with the background cluttering the design. How about instead of a solid background color you do a thin border to box the tags. This also allows the delete icon to be grouped in.. I'd remove the shading on the border for a cleaner more modern look. You can then choose to stick with the previously shown darker background or the lighter one as shown here. The border box color should not have too much contrast so depending on background you'd offset the color just a bit so it's visible but not distracting.\n. I will test locally and if I see no blockers I will merge in a few hours when back in my office. . I agree with you that it should be sql.query as the data payload itself has more to do with sql than jdbc. With the added bonus that regardless of your database driver / engine you would annotate as sql.query payload for consistency. If needing to let know that JDBC is used i'd rather have a separate annotation which then could provide the used jdbc driver / version.\n. ",
    "esbie": "Hi @adriancole! Thanks for looking at the PR :)\nI know it can be difficult to complete a migration when folks are still adding features to legacy code.  Since I am more familiar with Scala than Java and I already have a system that uses this scala version, I figured this was a good place for me to add & test the feature.\nI'd be happy to work on this feature in the zipkin-java version! Is that really a blocker for addressing this in scala?\n. Ok I added another test and fixed the currently failing tests. My cassandra queries still seem pretty bad. For getTraceIdsByAnnotation I don't think it's even possible to do in a single query since the binary annotations include serviceName\n. I don't have a personal use case for span names across services, so I don't mind backing out of this change\n. OK will do\n. ",
    "anandshah05": "Same issue for normal annotation values, can you please fix it in logs as well. Yes its annotations. We are actually storing the full json request and response message in annotation so that it become easier to co-relate.\nWe could use tags, but tag has it own overhead of indexing at database layer (like cassandra, elastic search)\nI have added same fix in annotations, can you please verify and merge. I understand, annotation is meant for one line only, but appending full request/response in log helps in tracing the message. Changes are minor and has no impact on any other functionality.\nThis is to fix the UI Display when there is json message in log, attaching UI screenshots\nbefore fix\n\nafter fix\n\n. wrapped null with  to fix unit tests. I actually copied from the formatBinaryAnnotationValue func, which works fine. I agree with your point, let me delete these unnecessary code. ",
    "roson9527": "Thanks Reply\nThis is unusual Trace picture.\n\n@adriancole \n\nps to get the json, use http://your_host/api/v1/trace/hex_trace_id_here\n\n============= Acquired through this link is json==================\n[{\"traceId\":\"493e8314428aa606\",\"name\":\"todo\",\"id\":\"493e8314428aa606\",\"timestamp\":1463732526416075,\"duration\":5998,\"annotations\":[{\"timestamp\":1463732526416075,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526422073,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526416075,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526422073,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}],\"binaryAnnotations\":[{\"key\":\"TraceId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"ParentId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"CurId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.416075    UnixTime:1463732526416075\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.422073    tUnixTime:1463732526422074\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}]},{\"traceId\":\"493e8314428aa606\",\"name\":\"func2\",\"id\":\"419606bc1013db96\",\"parentId\":\"493e8314428aa606\",\"timestamp\":1463732526417074,\"duration\":4000,\"annotations\":[{\"timestamp\":1463732526417074,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526421074,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526417074,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526421074,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}],\"binaryAnnotations\":[{\"key\":\"lc\",\"value\":\"\u6267\u884cFunc2\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"TraceId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"ParentId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"CurId\",\"value\":\"419606bc1013db96\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.417074    UnixTime:1463732526417075\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.421074    tUnixTime:1463732526421074\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}]},{\"traceId\":\"493e8314428aa606\",\"name\":\"func22\",\"id\":\"4bc2e66823900abc\",\"parentId\":\"419606bc1013db96\",\"timestamp\":1463732526418074,\"duration\":2000,\"annotations\":[{\"timestamp\":1463732526418074,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526420074,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526418074,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526420074,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}],\"binaryAnnotations\":[{\"key\":\"lc\",\"value\":\"\u6267\u884cFunc22\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"TraceId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"ParentId\",\"value\":\"419606bc1013db96\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"CurId\",\"value\":\"4bc2e66823900abc\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.419074    UnixTime:1463732526419074\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.421074    tUnixTime:1463732526421074\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}]},{\"traceId\":\"493e8314428aa606\",\"name\":\"func21\",\"id\":\"412a17134fba0d19\",\"parentId\":\"419606bc1013db96\",\"timestamp\":1463732526418074,\"duration\":1000,\"annotations\":[{\"timestamp\":1463732526418074,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526418074,\"value\":\"\u6267\u884cFunc21_timerecord\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526419074,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}],\"binaryAnnotations\":[{\"key\":\"lc\",\"value\":\"e2d1bd62-bbe0-4ce0-a8bc-0fc569120b7c\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"GUID\",\"value\":\"5a801afa-3ae3-48ae-a0ff-c0b3f32e3542\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"TraceId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"ParentId\",\"value\":\"419606bc1013db96\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"CurId\",\"value\":\"412a17134fba0d19\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.418074    UnixTime:1463732526418075\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.419074    tUnixTime:1463732526419074\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}]},{\"traceId\":\"493e8314428aa606\",\"name\":\"func221\",\"id\":\"4c6c93f2ddf1fbb1\",\"parentId\":\"4bc2e66823900abc\",\"timestamp\":1463732526418574,\"duration\":1000,\"annotations\":[{\"timestamp\":1463732526418574,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526419574,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}],\"binaryAnnotations\":[{\"key\":\"lc\",\"value\":\"\u6267\u884cFunc221\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"TraceId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"ParentId\",\"value\":\"4bc2e66823900abc\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"CurId\",\"value\":\"4c6c93f2ddf1fbb1\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.420074    UnixTime:1463732526420074\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.421074    tUnixTime:1463732526421074\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}]},{\"traceId\":\"493e8314428aa606\",\"name\":\"func3\",\"id\":\"45266bd2e8b1097d\",\"parentId\":\"493e8314428aa606\",\"timestamp\":1463732526418574,\"duration\":999,\"annotations\":[{\"timestamp\":1463732526418574,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526419573,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}],\"binaryAnnotations\":[{\"key\":\"lc\",\"value\":\"\u6267\u884cFunc3\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"TraceId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"ParentId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"CurId\",\"value\":\"45266bd2e8b1097d\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.421074    UnixTime:1463732526421074\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.422073    tUnixTime:1463732526422074\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}]},{\"traceId\":\"493e8314428aa606\",\"name\":\"func1\",\"id\":\"4ad3dcac1c362d53\",\"parentId\":\"493e8314428aa606\",\"timestamp\":1463732526418574,\"duration\":999,\"annotations\":[{\"timestamp\":1463732526418574,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526419573,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}],\"binaryAnnotations\":[{\"key\":\"lc\",\"value\":\"\u5f00\u59cb\u6267\u884cFunc1\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"lc\",\"value\":\"\u7ed3\u675f\u6267\u884cFunc1\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"TraceId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"ParentId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"CurId\",\"value\":\"4ad3dcac1c362d53\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.416075    UnixTime:1463732526416075\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.417074    tUnixTime:1463732526417075\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}]}]\n============= Via HTTP POST to queryserver original json:==================\n1\uff09\n{\"traceId\":\"493e8314428aa606\",\"name\":\"Func1\",\"id\":\"4ad3dcac1c362d53\",\"parentId\":\"493e8314428aa606\",\"annotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cs\",\"timestamp\":1463732526416075},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cr\",\"timestamp\":1463732526417074}],\"binaryAnnotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"lc\",\"value\":\"\u5f00\u59cb\u6267\u884cFunc1\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"lc\",\"value\":\"\u7ed3\u675f\u6267\u884cFunc1\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"TraceId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"ParentId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"CurId\",\"value\":\"4ad3dcac1c362d53\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.416075    UnixTime:1463732526416075\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.417074    tUnixTime:1463732526417075\"}]}\n2\uff09\n{\"traceId\":\"493e8314428aa606\",\"name\":\"Func22\",\"id\":\"4bc2e66823900abc\",\"parentId\":\"419606bc1013db96\",\"annotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cs\",\"timestamp\":1463732526419074},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cr\",\"timestamp\":1463732526421074}],\"binaryAnnotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"lc\",\"value\":\"\u6267\u884cFunc22\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"TraceId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"ParentId\",\"value\":\"419606bc1013db96\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"CurId\",\"value\":\"4bc2e66823900abc\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.419074    UnixTime:1463732526419074\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.421074    tUnixTime:1463732526421074\"}]}\n3\uff09\n{\"traceId\":\"493e8314428aa606\",\"name\":\"Func221\",\"id\":\"4c6c93f2ddf1fbb1\",\"parentId\":\"4bc2e66823900abc\",\"annotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cs\",\"timestamp\":1463732526420074},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cr\",\"timestamp\":1463732526421074}],\"binaryAnnotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"lc\",\"value\":\"\u6267\u884cFunc221\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"TraceId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"ParentId\",\"value\":\"4bc2e66823900abc\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"CurId\",\"value\":\"4c6c93f2ddf1fbb1\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.420074    UnixTime:1463732526420074\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.421074    tUnixTime:1463732526421074\"}]}\n4\uff09\n{\"traceId\":\"493e8314428aa606\",\"name\":\"Func21\",\"id\":\"412a17134fba0d19\",\"parentId\":\"419606bc1013db96\",\"annotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cs\",\"timestamp\":1463732526418074},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cr\",\"timestamp\":1463732526419074},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"\u6267\u884cFunc21_timerecord\",\"timestamp\":1463732526418074}],\"binaryAnnotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"lc\",\"value\":\"e2d1bd62-bbe0-4ce0-a8bc-0fc569120b7c\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"GUID\",\"value\":\"5a801afa-3ae3-48ae-a0ff-c0b3f32e3542\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"TraceId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"ParentId\",\"value\":\"419606bc1013db96\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"CurId\",\"value\":\"412a17134fba0d19\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.418074    UnixTime:1463732526418075\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.419074    tUnixTime:1463732526419074\"}]}\n5\uff09\n{\"traceId\":\"493e8314428aa606\",\"name\":\"ToDo\",\"id\":\"493e8314428aa606\",\"annotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cs\",\"timestamp\":1463732526416075},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cr\",\"timestamp\":1463732526422073}],\"binaryAnnotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"TraceId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"ParentId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"CurId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.416075    UnixTime:1463732526416075\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.422073    tUnixTime:1463732526422074\"}]}\n6\uff09\n{\"traceId\":\"493e8314428aa606\",\"name\":\"Func3\",\"id\":\"45266bd2e8b1097d\",\"parentId\":\"493e8314428aa606\",\"annotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cs\",\"timestamp\":1463732526421074},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cr\",\"timestamp\":1463732526422073}],\"binaryAnnotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"lc\",\"value\":\"\u6267\u884cFunc3\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"TraceId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"ParentId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"CurId\",\"value\":\"45266bd2e8b1097d\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.421074    UnixTime:1463732526421074\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.422073    tUnixTime:1463732526422074\"}]}\n7\uff09\n{\"traceId\":\"493e8314428aa606\",\"name\":\"Func2\",\"id\":\"419606bc1013db96\",\"parentId\":\"493e8314428aa606\",\"annotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cs\",\"timestamp\":1463732526417074},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cr\",\"timestamp\":1463732526421074}],\"binaryAnnotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"lc\",\"value\":\"\u6267\u884cFunc2\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"TraceId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"ParentId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"CurId\",\"value\":\"419606bc1013db96\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.417074    UnixTime:1463732526417075\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.421074    tUnixTime:1463732526421074\"}]}\nI hope I can get your help, thank you.\n. Checking the data flow process, we found the results will be different if the root Span of ParentId to \"\" (empty string) or \"00000000000000.\"\nWhen the root Span parentId value is \"0000000000000000\", the display may produce strange errors, such as when the display can not find the root Span, but sometimes is normal.\nThank u.\n. 5.json is root span data, parent ID is the correct way to set (unset) :)\nWhen I post 1.json data, trace is correct, but if then post 5.json data, found trace changes, the original Func1 had changed the timestamp:(\nHope to get your help, thank you\uff1a\uff09\n\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1donly 1.json \uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\n[{\"traceId\":\"493e8314428aa606\",\"name\":\"func1\",\"id\":\"4ad3dcac1c362d53\",\"parentId\":\"493e8314428aa606\",\"timestamp\":1463732526416075,\"duration\":999,\"annotations\":[{\"timestamp\":1463732526416075,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526417074,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}],\"binaryAnnotations\":[{\"key\":\"lc\",\"value\":\"\u5f00\u59cb\u6267\u884cFunc1\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"lc\",\"value\":\"\u7ed3\u675f\u6267\u884cFunc1\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"TraceId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"ParentId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"CurId\",\"value\":\"4ad3dcac1c362d53\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.416075 UnixTime:1463732526416075\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.417074 tUnixTime:1463732526417075\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}]}]\n\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d 1.json + 5.json \uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\n[{\"traceId\":\"493e8314428aa606\",\"name\":\"todo\",\"id\":\"493e8314428aa606\",\"timestamp\":1463732526416075,\"duration\":5998,\"annotations\":[{\"timestamp\":1463732526416075,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526422073,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526416075,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526422073,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}],\"binaryAnnotations\":[{\"key\":\"TraceId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"ParentId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"CurId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.416075 UnixTime:1463732526416075\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.422073 tUnixTime:1463732526422074\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}]},{\"traceId\":\"493e8314428aa606\",\"name\":\"func1\",\"id\":\"4ad3dcac1c362d53\",\"parentId\":\"493e8314428aa606\",\"timestamp\":1463732526418574,\"duration\":999,\"annotations\":[{\"timestamp\":1463732526418574,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"timestamp\":1463732526419573,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}],\"binaryAnnotations\":[{\"key\":\"lc\",\"value\":\"\u5f00\u59cb\u6267\u884cFunc1\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"lc\",\"value\":\"\u7ed3\u675f\u6267\u884cFunc1\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"TraceId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"ParentId\",\"value\":\"493e8314428aa606\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"CurId\",\"value\":\"4ad3dcac1c362d53\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.416075 UnixTime:1463732526416075\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}},{\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.417074 tUnixTime:1463732526417075\",\"endpoint\":{\"serviceName\":\"test001\",\"ipv4\":\"127.0.0.1\",\"port\":80}}]}]\n\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1djson src \uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\n1\uff09\n{\"traceId\":\"493e8314428aa606\",\"name\":\"Func1\",\"id\":\"4ad3dcac1c362d53\",\"parentId\":\"493e8314428aa606\",\"annotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cs\",\"timestamp\":1463732526416075},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cr\",\"timestamp\":1463732526417074}],\"binaryAnnotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"lc\",\"value\":\"\u5f00\u59cb\u6267\u884cFunc1\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"lc\",\"value\":\"\u7ed3\u675f\u6267\u884cFunc1\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"TraceId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"ParentId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"CurId\",\"value\":\"4ad3dcac1c362d53\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.416075 UnixTime:1463732526416075\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.417074 tUnixTime:1463732526417075\"}]}\n5\uff09\n{\"traceId\":\"493e8314428aa606\",\"name\":\"ToDo\",\"id\":\"493e8314428aa606\",\"annotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cs\",\"timestamp\":1463732526416075},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"value\":\"cr\",\"timestamp\":1463732526422073}],\"binaryAnnotations\":[{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"TraceId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"ParentId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"CurId\",\"value\":\"493e8314428aa606\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"StartTime\",\"value\":\"2016-05-20 16:22:06.416075 UnixTime:1463732526416075\"},{\"endpoint\":{\"ipv4\":\"127.0.0.1\",\"port\":80,\"serviceName\":\"Test001\"},\"key\":\"EndTime\",\"value\":\"2016-05-20 16:22:06.422073 tUnixTime:1463732526422074\"}]}\n. I have found the reason, the problem is the data we generate, it has been corrected\n*Performs clock-skew method when the root span only have cr and cs timestamp.\n. ",
    "NegatioN": "I'm not sure if this is exactly what you were looking for. Please give me a shout if it wasn't. :)\n. Nice. :+1:  This is almost identical to the way zipkin-reporter does it, am I right? Double-plus for consistency.. ",
    "rogeralsing": "How do you do this when running Zipkin using the Docker image?\n. Sure\n[\n  {\n    \"traceId\": \"48c1b3160196c098\",\n    \"id\": \"48c1b3160196c098\",\n    \"name\": \"main\",\n    \"timestamp\": 1473400356307624,\n    \"duration\": 199117,\n    \"annotations\": [\n      {\n        \"timestamp\": 1473400356307624,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"client\",\n          \"ipv4\": \"12.123.12.169\",\n          \"port\": 80\n        }\n      },\n      {\n        \"timestamp\": 1473400356506741,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"client\",\n          \"ipv4\": \"12.123.12.169\",\n          \"port\": 80\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"48c1b3160196c098\",\n    \"id\": \"48c898fd24ab75a4\",\n    \"name\": \"get /customers\",\n    \"parentId\": \"48c1b3160196c098\",\n    \"timestamp\": 1473400356318643,\n    \"duration\": 199117,\n    \"annotations\": [\n      {\n        \"timestamp\": 1473400356307624,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"client\",\n          \"ipv4\": \"12.123.12.169\",\n          \"port\": 80\n        }\n      },\n      {\n        \"timestamp\": 1473400356318643,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473400356493766,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473400356506741,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"client\",\n          \"ipv4\": \"12.123.12.169\",\n          \"port\": 80\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"Request\",\n        \"value\": \"some payload\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"key\": \"Result\",\n        \"value\": \"some result\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"48c1b3160196c098\",\n    \"id\": \"406faffcd864013d\",\n    \"name\": \"select orders\",\n    \"parentId\": \"48c898fd24ab75a4\",\n    \"timestamp\": 1473400356452666,\n    \"duration\": 52099,\n    \"annotations\": [\n      {\n        \"timestamp\": 1473400356441667,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473400356452666,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"orderdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473400356483168,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"orderdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473400356493766,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"sql.query\",\n        \"value\": \"select * from Orders where OrderId = 123\",\n        \"endpoint\": {\n          \"serviceName\": \"orderdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"48c1b3160196c098\",\n    \"id\": \"46320d346edc5a84\",\n    \"name\": \"select customers\",\n    \"parentId\": \"48c898fd24ab75a4\",\n    \"timestamp\": 1473400356329640,\n    \"duration\": 123024,\n    \"annotations\": [\n      {\n        \"timestamp\": 1473400356318643,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      },\n      {\n        \"timestamp\": 1473400356329640,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"customerdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473400356430668,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"customerdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      },\n      {\n        \"timestamp\": 1473400356441667,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"fakewebapi\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 9000\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"sql.query\",\n        \"value\": \"select * from Customers where CustomerId = 123\",\n        \"endpoint\": {\n          \"serviceName\": \"customerdb\",\n          \"ipv4\": \"1.0.0.127\",\n          \"port\": 1234\n        }\n      }\n    ]\n  }\n]\n. The lack of overhead is intentional just to see if there is any overhead to the C# provider, just to see that it doesnt introduce milliseconds latency on each request.\n(Its all just fake data)\n. cc @Horusiath \nWhere does the timespan and duration come from?\nFrom what I can see, its not set at all in the thrift payload:\n``` csharp\n        public static Span ToThrift(this Zipkin.Span span)\n        {\n            var s = new Span\n            {\n                TraceId = (long)span.TraceHeader.TraceId,\n                Id = (long)span.TraceHeader.SpanId,\n                Name = span.Name,\n                Debug = span.TraceHeader.IsDebug,\n               //No values provided for duration or timestamp\n            };\n        if (span.TraceHeader.ParentId.HasValue) s.ParentId = (long)span.TraceHeader.ParentId.Value;\n\n        s.__isset.annotations = span.Annotations.Count != 0;\n        foreach (var annotation in span.Annotations)\n        {\n            var thrifted = annotation.ToThrift();\n            var ep = thrifted.__isset.host ? thrifted.Host : span.Endpoint.ToThrift();\n            ep.ServiceName = span.ServiceName;\n            thrifted.Host = ep;\n            s.Annotations.Add(thrifted);\n        }\n\n        s.__isset.binary_annotations = span.BinaryAnnotations.Count != 0;\n        foreach (var binaryAnnotation in span.BinaryAnnotations)\n        {\n            var thrifted = binaryAnnotation.ToThrift();\n            var ep = thrifted.__isset.host ? thrifted.Host : span.Endpoint.ToThrift();\n            ep.ServiceName = span.ServiceName;\n            thrifted.Host = ep;\n            s.BinaryAnnotations.Add(thrifted);\n        }\n\n        return s;\n    }\n\n```\nSo is it the Zipkin server that calculates and sets the timespan and duration or where does it come from?\n[Edit] debugging the C# library I can verify that the Timestamp and duration for the span itself is not being set, nor written to the thrift stream.\nSo the values must come from the Zipkin server, which then incorrectly calculates them from the annotations? or?\n. It looks correct now, thanks!\n. > If there is a \"cs\", use that (because client always is authoritative)\nDoes this mean that if client sends a request at t0, and the server starts receiving at t100, then the span will be rendered from t0?\n. After inspecting the DOM, I noticed that this is sort of already in place. there are DIV's for each annotation, but they are hidden..\nChanging them to display:block makes them appear.\n(The white circles on the 2nd span)\nIs there any way in the UI to enable them or are those work in progress or why are they hidden?\n\n. ``` css\ntrace-container .span .duration .annotation.core {\ndisplay: none; //<- setting this to block shows annotations\n\n}\n```\n. ",
    "singram": "There is a open pull request to resolve this issue here  https://github.com/openzipkin/zipkin-tracer/pull/67\n. ",
    "wjc133": "OK, Thx\n. OK, Thx\n. ",
    "xiangqiao123": "how to resolve this problem ?\nthanks!\n. how to resolve this problem ?\nthanks!\n. thank you\uff01\n. thank you\uff01\n. ",
    "liangman": "./mvnw com.mycila:license-maven-plugin:format\nin CONTRIBUTING.md.\n. ./mvnw com.mycila:license-maven-plugin:format\nin CONTRIBUTING.md.\n. I set id from 1-500000 in hex. I think that the index have a conflict.\n. The lost data is different for storing in the elasticsearch  every, when I wrote data in the kafka.\n. Ok, It is normal for using \"POST\", but why is the data  lost for using \"kafka + ES\"?\n. Of course, it was the \"POST + ES\";\nAccording to what you said, I find that it has the same result.\nkafka:\n\"counter.zipkin_collector.messages.kafka\":100,\"gauge.zipkin_collector.message_spans.kafka\":1.0,\"gauge.zipkin_collector.message_bytes.kafka\":4068.0,\"counter.zipkin_collector.spans.kafka\":100,\"counter.zipkin_collector.bytes.kafka\":406770,\"httpsessions.max\":-1,\"httpsessions.active\":0}\nhttp:\n\"gauge.zipkin_collector.message_bytes\":1569.0,\"counter.zipkin_collector.spans\":100,\"gauge.zipkin_collector.message_spans\":1.0,\"counter.zipkin_collector.messages.http\":100,\"counter.zipkin_collector.bytes\":156870,\"gauge.response.api.v1.spans\":2.0,\"counter.status.202.api.v1.spans\":100,\"httpsessions.max\":-1,\"httpsessions.active\":0}\nAnd I run the zipkin with the argument --logging.level.zipkin=DEBUG, but it is normal.\n. I had sent the same 100 data with kafka and http.\nkafka + es:\npaas@PaasAPMBootstrap:/var/paas$ curl -XGET '129.188.37.108:9200/_cat/indices?v'\nhealth status index             pri rep docs.count docs.deleted store.size pri.store.size \nyellow open   zipkin-2016-06-20   5   1        114            0      8.8kb          8.8kb\nhttp+es:\npaas@PaasAPMBootstrap:/var/paas$ curl -XGET '129.188.37.108:9200/_cat/indices?v'\nhealth status index             pri rep docs.count docs.deleted store.size pri.store.size \nyellow open   zipkin-2016-06-20   5   1        200            0     20.4kb         20.4kb \n. senddata.sh:\n``` bash\n!/bin/bash\nTIMESTAMP=$(node -e 'console.log(new Date().getTime())')000\ncurl -s localhost:9411/api/v1/spans -X POST -H \"Content-Type: application/json\" --data '[{\n    \"traceId\": \"'${1}'\",\n    \"name\": \"fermentum\",\n    \"id\": \"'${1}'\",\n    \"annotations\": [\n      {\n        \"timestamp\": '${TIMESTAMP}',\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"semper\",\n          \"ipv4\": \"113.29.89.129\",\n          \"port\": 2131\n        }\n      },\n      {\n        \"timestamp\": '${TIMESTAMP}',\n        \"value\": \"sagittis\",\n        \"endpoint\": {\n          \"serviceName\": \"semper\",\n          \"ipv4\": \"113.29.89.129\",\n          \"port\": 2131\n        }\n      },\n      {\n        \"timestamp\": '${TIMESTAMP}',\n        \"value\": \"montes\",\n        \"endpoint\": {\n          \"serviceName\": \"semper\",\n          \"ipv4\": \"113.29.89.129\",\n          \"port\": 2131\n        }\n      },\n      {\n        \"timestamp\": '${TIMESTAMP}',\n        \"value\": \"augue\",\n        \"endpoint\": {\n          \"serviceName\": \"semper\",\n          \"ipv4\": \"113.29.89.129\",\n          \"port\": 2131\n        }\n      },\n      {\n        \"timestamp\": '${TIMESTAMP}',\n        \"value\": \"malesuada\",\n        \"endpoint\": {\n          \"serviceName\": \"semper\",\n          \"ipv4\": \"113.29.89.129\",\n          \"port\": 2131\n        }\n      },\n      {\n        \"timestamp\": '${TIMESTAMP}',\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"semper\",\n          \"ipv4\": \"113.29.89.129\",\n          \"port\": 2131\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"mollis\",\n        \"value\": \"hendrerit\",\n        \"endpoint\": {\n          \"serviceName\": \"semper\",\n          \"ipv4\": \"113.29.89.129\",\n          \"port\": 2131\n        }\n      }\n    ]\n  }]'\n```\n. start_send.sh:\n``` bash\n!/bin/bash\ni=1\nwhile [ $i -lt $1 ]\n        do\n        echo $i\n        ./senddata.sh printf \"%x\" $i\n        let \"i=${i}+1\"\ndone\n```\n. ./start_send.sh 101\nThis is the script of \"post + kafka\".\n. I use the java for writing the data in kafka, so I don't know how i post it. But I can write the script with the python. Please wait a moment.\n. senddatatokafka.py:\n``` bash\n!/bin/python\nimport time\nimport os\ndata=data=\"\"\"[{\n\"traceId\": \"%x\",\n\"name\": \"fermentum\",\n\"id\": \"%x\",\n\"annotations\": [\n{\n\"timestamp\": %i,\n\"value\": \"sr\",\n\"endpoint\": {\n\"serviceName\": \"semper\",\n\"ipv4\": \"113.29.89.129\",\n\"port\": 2131\n}\n},\n{\n\"timestamp\": %i,\n\"value\": \"sagittis\",\n\"endpoint\": {\n\"serviceName\": \"semper\",\n\"ipv4\": \"113.29.89.129\",\n\"port\": 2131\n}\n},\n{\n\"timestamp\": %i,\n\"value\": \"montes\",\n\"endpoint\": {\n\"serviceName\": \"semper\",\n\"ipv4\": \"113.29.89.129\",\n\"port\": 2131\n}\n},\n{\n\"timestamp\": %i,\n\"value\": \"augue\",\n\"endpoint\": {\n\"serviceName\": \"semper\",\n\"ipv4\": \"113.29.89.129\",\n\"port\": 2131\n}\n},\n{\n\"timestamp\": %i,\n\"value\": \"malesuada\",\n\"endpoint\": {\n\"serviceName\": \"semper\",\n\"ipv4\": \"113.29.89.129\",\n\"port\": 2131\n}\n},\n{\n\"timestamp\": %i,\n\"value\": \"ss\",\n\"endpoint\": {\n\"serviceName\": \"semper\",\n\"ipv4\": \"113.29.89.129\",\n\"port\": 2131\n}\n}\n],\n\"binaryAnnotations\": [\n{\n\"key\": \"mollis\",\n\"value\": \"hendrerit\",\n\"endpoint\": {\n\"serviceName\": \"semper\",\n\"ipv4\": \"113.29.89.129\",\n\"port\": 2131\n}\n}\n]\n}]\"\"\"\ndef main():\n  count = 100\n  data1=data.replace(' ', '').replace('\\n', '')\n  cmdp = r'./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic zipkin'\n  pipp = os.popen(cmdp, 'w')\n  i = 0\n  while i < count:\n    i += 1\n    timestamp = time.time() * 10 ** 6\n    pipp.write(data1%(i, i, timestamp, timestamp, timestamp, timestamp, timestamp, timestamp) + \"\\r\\n\")\n    #print data1%(i, i, timestamp, timestamp, timestamp, timestamp, timestamp, timestamp)\n  print 'finsh!'\n  pipp.close()\nif name == 'main':\n  main()\n```\nbash\npython senddatatokafka.py\n. Beause I remove the \" \" and \"\\n\" for sending to kafka. I think that you can try it with the script.\n. en, I will try it again following you step.\n. No, when I send 100 data to the kafka, all data is written in cassandra. But I view 10 data from the page of zipkin. Maybe there is a bug in the code...\n. I have edited the script again. It may meet your requirements.\n. Ok, I have updated the file ElasticsearchSpanConsumer.java. \ncount = 250\nBut when I set the count of log-data for sending the kafka, there is an Wranning here:\n```\n2016-06-27 11:33:08.106  WARN 59971 --- [[listener][T#3]] zipkin.collector.kafka.KafkaCollector    : Cannot store spans [00000000000000e3.00000000000000e3<:00000000000000e3] due to EsRejectedExecutionException(rejected execution of org.elasticsearch.transport.TransportService$4@53793b3c on EsThreadPoolExecutor[index, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@33aec4eb[Running, pool size = 4, active threads = 4, queued tasks = 200, completed tasks = 671]])\norg.elasticsearch.common.util.concurrent.EsRejectedExecutionException: rejected execution of org.elasticsearch.transport.TransportService$4@53793b3c on EsThreadPoolExecutor[index, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@33aec4eb[Running, pool size = 4, active threads = 4, queued tasks = 200, completed tasks = 671]]\n    at org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(EsAbortPolicy.java:50) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) [na:1.8.0_73]\n    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369) [na:1.8.0_73]\n    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.execute(EsThreadPoolExecutor.java:85) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.elasticsearch.transport.TransportService.sendLocalRequest(TransportService.java:346) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:310) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.performAction(TransportReplicationAction.java:463) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.doRun(TransportReplicationAction.java:444) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.elasticsearch.action.support.replication.TransportReplicationAction.doExecute(TransportReplicationAction.java:125) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.elasticsearch.action.index.TransportIndexAction.innerExecute(TransportIndexAction.java:134) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:118) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:65) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.elasticsearch.action.support.replication.TransportReplicationAction$OperationTransportHandler.messageReceived(TransportReplicationAction.java:238) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.elasticsearch.action.support.replication.TransportReplicationAction$OperationTransportHandler.messageReceived(TransportReplicationAction.java:235) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:244) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:114) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[netty-3.10.5.Final.jar!/:na]\n    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75) ~[elasticsearch-2.3.2.jar!/:2.3.2]\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[netty-3.10.5.Final.jar!/:na]\n    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[netty-3.10.5.Final.jar!/:na]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_73]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_73]\n    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_73]\n```\ncurl -s 'localhost:9411/api/v1/traces?lookback=500000000&limit=100'|jq '. | length'\n100\n. The result in the ES:\nhealth status index             pri rep docs.count docs.deleted store.size pri.store.size \nyellow open   zipkin-2016-06-27   5   1        414            0    147.1kb        147.1kb\n. Maybe I have to consider changing the database from ES to cassandra. \nBecause we forecast that there are at least 100 thousand log-datas to be sent to kafka per 1s in our micro service. \nSo I will need a distributed Zipkin for consuming the data.\n. When I use the cassandra for storing, it spends about 100s for zipkin consuming 500000 data (per about 1kb).\n. ok.\n. ",
    "alphatan": "thank you!. Looking for this feature eagerly....\nOr could someone leave us a hint how to integrate with rabbitmq (RabbitTemplate) ?. Thanks for the hint. \nWe're trying to add tracing ability to a traditional project, neither spring-cloud nor \n spring-boot.\nI'll digest the one-way propogration. It makes me feel strange that only few information can be found for the integration of zipkin(brave) and mq.. Thanks very much.. ",
    "jahnkey": "thanks  very much\uff01\nwelcome to guangzhou china @adriancole  :-)\n. ",
    "hakanson": "Thanks. I guess I should have started by looking at the docs instead of the code.\n. ",
    "xqliang": "I use the latest Zipkin(1.26) and with a single node elasticsearch(2.3.4) for storage(the default docker-zipkin configuration), and still encounter data lost.\nHere is the test script(sendtozipkin.sh):\n```bash\n!/bin/bash\nfunction send_to_zipkin() {\n    id=$1\n    id2=$2\n    millis=$(python -c 'import time; print \"%d\" % (time.time() * 1000 * 1000)')\n    curl localhost:9411/api/v1/spans -X POST -H \"Content-Type: application/json\" --data '[{\n        \"traceId\": \"'${id}'\",\n        \"name\": \"fermentum\",\n        \"id\": \"'${id}'\",\n        \"annotations\": [\n          {\n            \"timestamp\": '${millis}',\n            \"value\": \"sr\",\n            \"endpoint\": {\n              \"serviceName\": \"semper\",\n              \"ipv4\": \"113.29.89.129\",\n              \"port\": 2131\n            }\n          }\n        ]\n    }, {\n        \"traceId\": \"'${id}'\",\n        \"name\": \"fermentum1\",\n        \"id\": \"'${id2}'\",\n        \"annotations\": [\n         {\n            \"timestamp\": '${millis}',\n            \"value\": \"sr\",\n            \"endpoint\": {\n              \"serviceName\": \"semper\",\n              \"ipv4\": \"113.29.89.129\",\n              \"port\": 2131\n            }\n          }\n        ]\n    }]'\n}\ni=0\nwhile [ $i -lt $1 ]; do\n    let \"i=${i}+1\"\n    let \"j=${i}+16777216\"\n    echo $i\n    send_to_zipkin printf \"%016x\" $i printf \"%016x\" $j\ndone\n```\nThen send 100(50*2) messages to Zipkin:\nbash\n$ ./sendtozipkin.sh 50\nAnd some random messages are lost, I addES_HTTP_LOGGING=BODY (introduced in Zipkin 1.25 ) environment to docker-compose-elasticsearch.yaml,  and saw these errors:\n```bash\ngrep 'errors\\\":true' $(docker inspect --format='{{.LogPath}}' zipkin) | head -1\n{\"log\":\"2017-06-12 03:04:29.733  INFO 6 --- [41.133:9200/...] z.s.e.http.ElasticsearchHttpStorage      : \n{\\\"took\\\":1,\\\"errors\\\":true,\\\"items\\\":[{\\\"create\\\":{\\\"_index\\\":\\\"zipkin-2017-06-03\\\",\\\"_type\\\":\\\"span\\\",\n\\\"_id\\\":\\\"AVyaQoUkPGYSeXOvPsRT\\\",\\\"status\\\":429,\\\"error\\\":{\\\"type\\\":\\\"es_rejected_execution_exception\\\",\n\\\"reason\\\":\\\"rejected execution of org.elasticsearch.transport.TransportService$4@5742b77e on EsThreadPoolExecutor[bulk, queue capacity = 50, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@2fcaec02[Running, pool size = 8, active threads = 8, queued tasks = 50, completed tasks = 0]]\\\"}}},\n{\\\"index\\\":{\\\"_index\\\":\\\"zipkin-2017-06-03\\\",\\\"_type\\\":\\\"servicespan\\\",\n\\\"_id\\\":\\\"gamerebategift|timeraddpoolamounttask\\\",\\\"status\\\":429,\\\"error\\\":{\\\"type\\\":\\\"es_rejected_execution_exception\\\",\n\\\"reason\\\":\\\"rejected execution of org.elasticsearch.transport.TransportService$4@70fc3bfe on EsThreadPoolExecutor[bulk, queue capacity = 50, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@2fcaec02[Running, pool size = 8, active threads = 8, queued tasks = 50, completed tasks = 0]]\\\"}}}]}\\n\",\n\"stream\":\"stdout\",\"time\":\"2017-06-12T03:04:29.733933728Z\"}\n```\nCheck the default configuration:\nbash\n$ curl -XGUT localhost:9200/_cluster/settings \n{\"persistent\":{},\"transient\":{}}\nChange threadpool.bulk.queue_size to 500:\nbash\n$ curl -XPUT localhost:9200/_cluster/settings -d '{\"transient\": {\"threadpool.bulk.queue_size\": 500}}'\n{\"acknowledged\":true,\"persistent\":{},\"transient\":{\"threadpool\":{\"bulk\":{\"queue_size\":\"500\"}}}}\nThen rerun the sendtozipkin.sh script, there were no data lost any more.. Yes, at least Zipkin should log a WARNING/ERROR log if writing ES with errors returned while not setting ES_HTTP_LOGGING=BODY, so we can monitor the log for alerting.. Sorry, I'm not very familiar with the UI code :(\n. Sorry, I'm not very familiar with the UI code :(\n. You should move delete query.serviceName after the serviceName if condition?\n~~~diff\n// zipkin-ui/js/component_data/default.js#L23\nexport default component(function DefaultData() {\n  this.after('initialize', function() {\n    const query = convertToApiQuery(window.location.search);\n    const serviceName = query.serviceName;\n    if (serviceName) {\n+     if (query.serviceName === 'all') {\n+       delete query.serviceName;\n+     }\n      const apiURL = api/v1/traces?${queryString.stringify(query)};\n~~~. ",
    "luoyongjiee": "I found it that if no request comes,\nwhen i query ,\nthe number of the json will returns in descending timestamp order,Why?\nIt looks strange,but if  storage in mysql ,it will always return the correct number.\n. ",
    "klette": "Alright-y,\nmoved the code into a auto configure module, and added some docs. Couldn't find any Java based parser, and building and running go code as a part of the test suite seemed a bit overkill :)\n. Alright-y,\nmoved the code into a auto configure module, and added some docs. Couldn't find any Java based parser, and building and running go code as a part of the test suite seemed a bit overkill :)\n. Fixed :)\n. Done!\n. Ah, cool. Just my brain that does that automatically after x years with that as a coding standard :)\n. ",
    "dragontree101": "oh,  your mean is zipkin run on 3.7 will is ok,  but you have not test?\ni will test zipkin in cassandra 3.7\nthanks.\n. i have done some simplest test\uff0c it seems ok.  thanks!\n. now does zipkin 1.14 version support es 5 ?  i want to upgrade cassandra to es 5.\n. sorry i am not describe clearly\none of a trace is\n\nand chrome debug, i found get data from zipkin-server is fast\n\nand from chrome timeline i found scripting is slow, a lot of time use to execute this script\n\ni guess exec jquery js is slow, i don't know this could be optimize? \n. could you give me an es query, i want to compare have sort query with no sort query use time. thanks. now in zipkin-ui one-way is NaN?\n\n. ",
    "songxin1990": "Is there something 3rd lib can support Kafka  with Zipkin?. ",
    "tennenbaum": "Thanks, I will look at the v2 discussion. I would like to include the asynchronous span in the same trace. An alternative approach I can see (for v1 code) is to submit non-core (custom) annotations when consuming and producing (using the same spanId). It appears that Zipkin would not attempt to adjust for clock skew where the core annotations are not present. It also seems that it would derive the duration of the span from the difference between the newest and oldest timestamps of the custom annotations. I am not sure whether this will play nice with the rest of the tooling though (very new to this code base). Any thoughts on that?\nNote: Implied in the above is that we are sending the in-band tracing information (e.g. spanId) as metadata in the queued message.\n. Thanks, I will look at the v2 discussion. I would like to include the asynchronous span in the same trace. An alternative approach I can see (for v1 code) is to submit non-core (custom) annotations when consuming and producing (using the same spanId). It appears that Zipkin would not attempt to adjust for clock skew where the core annotations are not present. It also seems that it would derive the duration of the span from the difference between the newest and oldest timestamps of the custom annotations. I am not sure whether this will play nice with the rest of the tooling though (very new to this code base). Any thoughts on that?\nNote: Implied in the above is that we are sending the in-band tracing information (e.g. spanId) as metadata in the queued message.\n. ",
    "AndrewWang996": "Measuring the latency between producing to and consuming from Kafka queues is precisely what I'm trying to do, although unlike @tennenbaum, I only considered adding the \"cs\" annotation upon produce and \"sr\" upon consume without worrying about how long it takes the message to be produced and consumed. \nMy problem is that I was attempting to use the ClientRequestInterceptor + ServerRequestInterceptor mentioned in Brave's 3.0.0 api, but it seems that this only submits the span if either [\"cs\", \"cr\"] or [\"sr\", \"ss\"] are handled in the Interceptor.handle(Adapter) methods. I only needed to handle \"cs\" and \"sr\" with the ClientRequestInterceptor and ServerRequestInterceptor, but in order for Brave to submit the spans, I needed to make a dummy adapter and submit \"ss\" as well, not to close the span, but just so that Brave knew to submit it. This is suboptimal. I'm not even worrying about dependencies or duration.\n. ",
    "tylerhjones": "\ud83d\udc4d , just ran into this issue. \ud83d\udc4d , just ran into this issue. ",
    "mikewrighton": "I guess I was thinking that since there is some useful code around the schema loading, like in zipkin.storage.cassandra.Schema, it might be good if it were somehow extensible e.g. if you could provide your own schema or 'upgrade schema' file, and/or modify some of the parameters in the default schema like replication factor.\n. Good point, changed.. ",
    "ewhauser": "@adriancole Agreed that's a better choice. Updated the PR.\n. Got it. Yes, that is a definitely easier and a nice trick. Will give it a shot. Thanks!. So, @anuraaga's trick doesn't work - at least with gRPC Java. The issue is that the marshaller will have already copied the input stream into a ByteString prior to getting access to it.\nSo I took the approach of implementing a custom BindableService. This has the added benefit of removing all dependencies on Protocol Buffers entirely. Here are the dependencies introduced now:\nconsole\n[INFO] +- io.grpc:grpc-netty-shaded:jar:1.17.1:runtime\n[INFO] |  \\- io.grpc:grpc-core:jar:1.17.1:compile (version selected from constraint [1.17.1,1.17.1])\n[INFO] |     +- io.grpc:grpc-context:jar:1.17.1:compile\n[INFO] |     +- com.google.code.gson:gson:jar:2.7:compile\n[INFO] |     +- com.google.errorprone:error_prone_annotations:jar:2.2.0:compile\n[INFO] |     +- com.google.code.findbugs:jsr305:jar:3.0.2:compile\n[INFO] |     +- org.codehaus.mojo:animal-sniffer-annotations:jar:1.17:compile\n[INFO] |     +- com.google.guava:guava:jar:26.0-android:compile\n[INFO] |     |  +- org.checkerframework:checker-compat-qual:jar:2.5.2:compile\n[INFO] |     |  \\- com.google.j2objc:j2objc-annotations:jar:1.1:compile\n[INFO] |     +- io.opencensus:opencensus-api:jar:0.17.0:compile\n[INFO] |     \\- io.opencensus:opencensus-contrib-grpc-metrics:jar:0.17.0:compile\n[INFO] +- io.grpc:grpc-stub:jar:1.17.1:compile\nSo this ends up being much less of a dependency burden than armeria - no netty, protobufs - with the downside of having the collector run on a different port. We can keep this version of gRPC in sync with the version that is being used by zipkin-gcp to avoid any issues.\nHowever, armeria does have the advantage of everything being on the same port and it would support gRPC-Web out of the box without a proxy. Would switching to armeria mean that it would eliminate undertow and just use it for everything? If so, we shoud probably find a path to finalizing the gRPC API in this PR and moving forward with this experimental version on a different port. A separate issue could be opened for moving to armeria - given the size of that change, I would want to make sure there was plenty of support for it.. I started this whole mess so I'm fine rolling up my sleeves on the switch. Just want to make sure there is support behind it.. Agreed. If armeria is the way forward, then this is the better option.\nOn Sun, Dec 16, 2018 at 9:27 PM Anuraag Agrawal notifications@github.com\nwrote:\n\nFor the copying issue, just to clarify, using the proto approach makes\nByteBuf -> zipkin.proto3.Span -> byte[] -> zipkin2.Span become\nByteBuf -> byte[] -> zipkin2.Span\nand then using Armeria's unsafe option can make it become\nByteBuf -> zipkin2.Span\nProto3Codec needs to be updated to accept ByteBuffer, which is pretty\nsimple change I think. On the flip side, I think it's not practical for it\nto parse the InputStream directly since doing that efficiently would\nprobably require using the KnownLength class inside of Proto3Codec which\nshouldn't have gRPC dependencies.\nOf course it depends on the decision of moving to armeria, but if we do,\nI'd still recommend using the proto trick + unsafe option.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2328#issuecomment-447726620,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAIBPYlevMan0sbk2a4G6hQKSHDqLdb8ks5u5ys9gaJpZM4ZU_jb\n.\n. Trustin - armeria seems like a great project. More so wanted to make sure\nthere was support for using it within zipkin-server. There are a lot of\nconsumers that I don\u2019t want to break; thank you for the offer to help\nsupport with the adoption!\n\nOn Sun, Dec 16, 2018 at 9:04 PM Trustin Lee notifications@github.com\nwrote:\n\n@ewhauser https://github.com/ewhauser wrote:\nI would want to make sure there was plenty of support for it.\nThe Armeria project maintainers including LINE employees like me (\n@hyangtack https://github.com/hyangtack and @minwoox\nhttps://github.com/minwoox as well) would be happy to support Zipkin\ncommunity for adoption of Armeria and its sustainable development. I\nbelieve you don't need to worry too much about the future of Armeria given\nthat it's being used in various critical parts of LINE services.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2328#issuecomment-447723669,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAIBPcM8jeHwTR8aNnQ7R5wclDKRwkanks5u5yXNgaJpZM4ZU_jb\n.\n. Yes. I\u2019m was planning on swapping out in the next rev as streaming is not\nyet supported by GRPC-Web.\n\nRe: putSpans and ListOfSpans - I think that\u2019s the right approach. armeria\nsays it supports gRPC over HTTP/1 so I need to explore how that works. If\nit\u2019s similar to GRPC Gateway where you use annotations to define the HTTP/1\nmappings, then you could potentially share the code for both\nimplementations.\nOn Mon, Dec 17, 2018 at 3:24 AM Adrian Cole notifications@github.com\nwrote:\n\nI was just looking at the proto. It seems probably unary ListOfSpans style\nrequest would make more sense as the initial impl as all infrastructure we\nhave is based on ListOfSpans (Collector, Storage etc). It is nice that all\nof our impls accept literally the same messages. For example, if our\ninitial method was putSpans(ListOfSpans) then the exact codec we use for\nnormal proto over http/2 could be used to process that message, right?\nStarting with just a unary likely also means the largest amount of client\nsupport in general. Does that sound right?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2328#issuecomment-447812182,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAIBPbYLSknVTC9k07_TrkYE1zMzu2Ulks5u537lgaJpZM4ZU_jb\n.\n. For inspiration, I took a look at the API provided by OpenCensus trace collector.\n\nThey actually use bidirectional streaming in their implementation although there are some nuances in their protocol which do not exist for Zipkin.\nI still think implementing unary makes sense since gRPC-Web does not support streaming, but I'll probably perf test both solutions before finishing.. ",
    "calm2016": "In microservice applications, searching among all services is really useful and required. Please consider to at least allow \"all\" keywords to indicate searching in all services.\nBtw, it would be great if simple regular expression is supported, such as xx** in the annotation/key-value search function. Thanks\n. ",
    "lukeotterblad": "You're basically stuck making another field as you can either use their defined formats (of which epoch_millis is one) or define a custom format, which you can do via ...\nhttp://www.joda.org/joda-time/apidocs/org/joda/time/format/DateTimeFormat.html\nBut this effectively uses code similar to below (this example is in groovy):\n```\n@Grab( 'joda-time:joda-time:2.1' )\nimport org.joda.time.\nimport org.joda.time.format.\nString a = \"1455242412402125\"\nDateTimeFormatter dtf = DateTimeFormat.forPattern( \"\" ); //<-- There's no pattern you can define to parse the microsecond representation as a string\ndef timestamp = dtf.parseDateTime( a )\nprintln timestamp \n```\nYou also cant add any kind of processing layer on the timestamp field (similar to an analyzer for text) such that you could have elasticsearch process your times (prior to indexing) to millis as analyzers are only for string fields. So, it seems like it's either make another field timestamp_millis as originally suggested and index that as a date or write a very nice feature request for adding epoch_micros to the definable date mapping formats. \nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html\n. You can add a value in your index mapping where elasticsearch will record the timestamp of the documents last ingestion time (even if you provide it nothing). This might be a better solution for you, though it appears they are also deprecating the feature. \nSee the following for examples:\nhttp://stackoverflow.com/questions/27255596/elasticsearch-index-last-update-time\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-timestamp-field.html\n. ",
    "gboucher90": "Indeed, thank you for your help!\n. I tried with a modified json with correct service names, though the relative time is still invalid.\n```\n[\n   {\n      \"traceId\":\"1e223ff1f80f1c69\",\n      \"name\":\"post\",\n      \"id\":\"74280ae0c10d8062\",\n      \"parentId\":\"bf396325699c84bf\",\n      \"duration\":93576,\n      \"annotations\":[\n         {\n            \"endpoint\":{\n               \"serviceName\":\"serviceB\",\n               \"ipv4\":\"192.0.0.0\"\n            },\n            \"timestamp\":1470150004008761,\n            \"value\":\"sr\"\n         },\n         {\n            \"endpoint\":{\n               \"serviceName\":\"serviceB\",\n               \"ipv4\":\"192.0.0.0\"\n            },\n            \"timestamp\":1470150004102338,\n            \"value\":\"ss\"\n         }\n      ],\n      \"binaryAnnotations\":[\n      ],\n      \"debug\":false\n   }\n,\n   {\n      \"traceId\":\"1e223ff1f80f1c69\",\n      \"name\":\"get\",\n      \"id\":\"bf396325699c84bf\",\n      \"duration\":99410,\n      \"annotations\":[\n         {\n            \"endpoint\":{\n               \"serviceName\":\"serviceA\",\n               \"ipv4\":\"127.0.0.0\"\n            },\n            \"timestamp\":1470150004071068,\n            \"value\":\"sr\"\n         },\n         {\n            \"endpoint\":{\n               \"serviceName\":\"serviceA\",\n               \"ipv4\":\"127.0.0.0\"\n            },\n            \"timestamp\":1470150004170479,\n            \"value\":\"ss\"\n         }\n      ],\n      \"binaryAnnotations\":[\n      ],\n      \"debug\":false\n   },\n   {\n      \"traceId\":\"1e223ff1f80f1c69\",\n      \"name\":\"post\",\n      \"id\":\"74280ae0c10d8062\",\n      \"parentId\":\"bf396325699c84bf\",\n      \"duration\":94539,\n      \"annotations\":[\n         {\n            \"endpoint\":{\n               \"serviceName\":\"serviceA\",\n               \"ipv4\":\"127.0.0.0\"\n            },\n            \"timestamp\":1470150004074202,\n            \"value\":\"cs\"\n         },\n         {\n            \"endpoint\":{\n               \"serviceName\":\"serviceA\",\n               \"ipv4\":\"127.0.0.0\"\n            },\n            \"timestamp\":1470150004168741,\n            \"value\":\"cr\"\n         }\n      ],\n      \"binaryAnnotations\":[\n      ],\n      \"debug\":false\n   }\n]\n```\n. The problem is that if the root span doesn't have any skew which is likely since it starts with only a local span SR/SS then the children don't get adjusted.\nClockSkew skew = getClockSkew(node.value());\nreturns null for the root span and the processing stops\n. Performing the clock skew adjustment even if the current skew is null works for relative time though the duration displayed in the UI is now incorrect.\n\n\n. Yes I did use a different set of IP, \"127.0.0.0\" for serviceA and \"192.0.0.0\" for serviceB (see json or screenshots)\nAnything else I might be missing?\n. Fixed in #1465 . Fixed in #1465 . ",
    "mathuo": "We have cases where children are missing, think they may be our fault though. Also get odd cases where the UI displays the spans all as the lightest blue color rather than progressively darker colors as we nest into spans. Do you know the case that causes this UI behavior since the structure still appears, all be it a few spans are missing but the coloring is odd.\n. ",
    "Jaware": "so i have to modify zipkin source codes  to add back end sorting and remove front sorting . i have modified the es part,i think es is the most efficient and powerful backend,cassandra is a bit hard to use like hbase,you have to add index to to the search field,really bad. i have modified the es part,i think es is the most efficient and powerful backend,cassandra is a bit hard to use like hbase,you have to add index to to the search field,really bad. client side sorting is really incorrect when the result list size is more than a few hundred,you mean we have to modify the source code ourselves?@adriancole. wrong on  mac,linux ok. ",
    "aliostad": "I am happy to follow the lead from you guys. As I have said, I am new to this.\nInitially it seemed a stand-alone collector is cleaner but frankly since it has to have access to storage, it seems that needs zipkin-server (core bits) anyway. UI obviously is not necessary.. I am happy to follow the lead from you guys. As I have said, I am new to this.\nInitially it seemed a stand-alone collector is cleaner but frankly since it has to have access to storage, it seems that needs zipkin-server (core bits) anyway. UI obviously is not necessary.. @adriancole Awesome mate! That is excellent. \nThanks I believe I have all I need. As you can see, it is only the skeleton with no embellishment, etc. As you might have guessed, Java is not my primary language but I will make the effort to ensure this is of the quality rest of zipkin is akin to.\nI will close the issue but I might get back to you soon.\n. @adriancole Awesome mate! That is excellent. \nThanks I believe I have all I need. As you can see, it is only the skeleton with no embellishment, etc. As you might have guessed, Java is not my primary language but I will make the effort to ensure this is of the quality rest of zipkin is akin to.\nI will close the issue but I might get back to you soon.\n. OK, I have made progress and trying to get it run. I know this could be a silly question but I am new to all this: how is zipkin-aws supposed to run for example? \nMy expectation for zipkin-collector-eventhub is that I drop zipkin-collector-eventhub.jar along with the zipkin-server.jar and run the zipkin-server.jar and specify configurations in application.yaml or application.properties and the Spring Boot autoconfig will load it and start running my host in addition to zipkin-server. But I have not been able to get this to work. Do I need to make my jar a Spring Boot application, like Stackdriver Trace Zipkin Collector?\nCan you please provide some pointers?\nThanks a lot.. OK, I have made progress and trying to get it run. I know this could be a silly question but I am new to all this: how is zipkin-aws supposed to run for example? \nMy expectation for zipkin-collector-eventhub is that I drop zipkin-collector-eventhub.jar along with the zipkin-server.jar and run the zipkin-server.jar and specify configurations in application.yaml or application.properties and the Spring Boot autoconfig will load it and start running my host in addition to zipkin-server. But I have not been able to get this to work. Do I need to make my jar a Spring Boot application, like Stackdriver Trace Zipkin Collector?\nCan you please provide some pointers?\nThanks a lot.. @llinder Hi, thanks for the comments. So this fat jar, what script gets used to build this? I would like to see what the process is.\nThis weekend I will spend some time getting my head around thin launcher.. @llinder Hi, thanks for the comments. So this fat jar, what script gets used to build this? I would like to see what the process is.\nThis weekend I will spend some time getting my head around thin launcher.. OK I have been looking into samples of thin launcher. I cannot see any ZIP layout so properties can be changed, am I missing something or we can configure thin launcher in other means?\n[UPDATE]\nI finally got zipkin-server to read application.properties using --spring.config.location. So I guess I don't need any magic for supplying config as I can use the same trick.. OK I have been looking into samples of thin launcher. I cannot see any ZIP layout so properties can be changed, am I missing something or we can configure thin launcher in other means?\n[UPDATE]\nI finally got zipkin-server to read application.properties using --spring.config.location. So I guess I don't need any magic for supplying config as I can use the same trick.. Wow! @adriancole that is so nice! Thanks a lot. I will hopefully get to play with this today or tomorrow.. Wow! @adriancole that is so nice! Thanks a lot. I will hopefully get to play with this today or tomorrow.. So unless I am missing something, this is not launcher, no? \nSo do you think it is worth to try that out (I was trying to get my head around it and made some progress) or this is the canonical way to go (using MODULE to create a fat jar)?. Thank you @adriancole .\nFrankly, in recent days I have been looking deep into the details of this project I can see that this could have never been properly working:\n\nThere was no Duration defined in the Span\nTraceHeader did not have IsSampled\nIDs defined as unsigned long and then being converted to long\nI changed all of this without a single test breaking - obviously not related to functionality but it seems there were more that could have been done.\n\nMy view was to fix the primitives and this project could serve as the basic building block to build upon. But of course, the primitives are pretty small and implementing them all is not an issue warranting a dedicated project mainly because everyone has moved on now and built their own. So Criteo project is one that shines since it has all that and more.\nBut there are problems, not of taste and implementation strategy but actual practicality:\nAs the name implies, the package is Criteo.Profiling.Tracing - and not even mention of zipkin. It is very specific to how Criteo does things. They are on the new world of .NET (core and .net standards) which include only a very small percentage of .NET ecosystem - my guess is less than 2%. The builds are all purely bash, no bat or powershell, which I guess are run on mac or linux unless you want to yse Windows 10's bash for windows (which comes with no warranty). This probably covers 5% of people's work machines - frankly many devs will panic seeing a file having .sh extension.\nI work for a company which is one of the top (if not toppest) Azure consumers on this side of Atlantic, with 40 development teams. And it is ahead in many respect to the rest of the \"Dark Matter\" Enterprise. But Criteo is just at the bleeding edge, and they do not seem to make an effort to cover anyone else (and nothing wrong with that as the name of the project implies). I personally cannot use any of the stuff, at work I am running Windows 7 along with the rest of the company (and I use my personal Mac for ssh, etc) and we are only trialing .NET Core in one of our 40 dev teams.\nThere is not even a .NET 4.5-4.6 package of the library on nuget - it is all new world. Yes, you can run using the new world's backward compatibility but that is just not something we can do at this point. So if for me this is not usable, you can guess what it is for all those \"Dark Matter\" developers out there.\nAs such, my guess is we still need another project:\n\nBuilds the primitives in all supported .NET frameworks (4.5.2 and plus)\nHas multi-target build which targets .NET Standard\nZipkin is the first class citizen. Logging/Monitoring/Alerting is a big domain and it needs to be just about the Zipkin side of it.\n\nSo here are my 2 cents. Frankly I wish I could just use Criteo packages but I cannot, so I have to build it from scratch.\n. @fedj thank you for your response.\nIn fact, it addresses many of my concerns. I am happy to help if it is needed to make it useable for 4.5 users as well. I am really pleased that one project is selected, this means the efforts in the community gets more focused and consolidated.\nSo please let me know how and if I can help. I surely need a Zipkin addon for my Perfit project and need it quickly.\n. ",
    "llinder": "The thin launcher thing sounds interesting.  Reading the docs it seems like it would be about the same amount of work as making a new fat jar.  To the thin launchers advantage it sounds like it should avoid the extra bloat in the docker container since the thin wrapper could make informed decisions to only layer in new jars.  Also it would hopefully maintain easy local testing/debugging like the fat jar provides.. The thin launcher thing sounds interesting.  Reading the docs it seems like it would be about the same amount of work as making a new fat jar.  To the thin launchers advantage it sounds like it should avoid the extra bloat in the docker container since the thin wrapper could make informed decisions to only layer in new jars.  Also it would hopefully maintain easy local testing/debugging like the fat jar provides.. Makes sense.  I will move this to the zipkin-aws project. We already make our own Zipkin builds for things like this so not a big deal for us.\nI'm not sure there is a one size fits all approach to solve this but instructions for adding it to the classpath is probably the right approach for now.  The trickiest part will probably be managing the zipkin-server.yml file.  Maybe Spring Boot has the ability to overlay files from external sources?  Once thats figured out then maybe we can address the Docker build.\n. Makes sense.  I will move this to the zipkin-aws project. We already make our own Zipkin builds for things like this so not a big deal for us.\nI'm not sure there is a one size fits all approach to solve this but instructions for adding it to the classpath is probably the right approach for now.  The trickiest part will probably be managing the zipkin-server.yml file.  Maybe Spring Boot has the ability to overlay files from external sources?  Once thats figured out then maybe we can address the Docker build.\n. Moving to zipkin-aws project\n. Moving to zipkin-aws project\n. Another problem with the materialized view is that it doesn't return a distinct list of span names.  In other words it would have the same problem with non blank span names in that it will only return the first N which might not include all of the non blank names either.\nMy quick solution was to just create another table to hold service_name and span_name like the Cassandra2 storage component.  This kind of defeats the purpose of this experimental storage implementation but at least it still enables duration and text search on annotations.\n. Another problem with the materialized view is that it doesn't return a distinct list of span names.  In other words it would have the same problem with non blank span names in that it will only return the first N which might not include all of the non blank names either.\nMy quick solution was to just create another table to hold service_name and span_name like the Cassandra2 storage component.  This kind of defeats the purpose of this experimental storage implementation but at least it still enables duration and text search on annotations.\n. @michaelsembwever thanks for updating the stress yaml profiles and running a comparison test.  It is indeed interesting to see the results of the stress test.  Really wish the MV worked as well, especially given it appears to perform better as well.  I will review the DeduplicatingExecutor code and get this rebased today or tomorrow at the latest.\nRegarding the schema change.  Is the preference to create a new CQL that contains the drop/add statements?\nThanks for the detailed review and comments!\n. Rebased the latest upstream changes on master.  Also added the deduper on writes writes on trace_by_service_span and span_name_by_service.\nI think the only outstanding thing to address is the schema changes.  Given that this is still marked experimental do we want to start maintaining schema change sets?  If thats what is preferred I will pull the table/view changes out to a separate file.\n. @michaelsembwever thanks for jumping on this so quickly!\n. @michaelsembwever thanks for jumping on this so quickly!\n. I like this better than the approach in #1396.  Further configuration of the driver SSL context beyond the defaults feels like it should supplied as Spring Boot auto configuration extension.  That is obviously a bigger change and doesn't make sense if the default driver SSL context works.\n. I like this better than the approach in #1396.  Further configuration of the driver SSL context beyond the defaults feels like it should supplied as Spring Boot auto configuration extension.  That is obviously a bigger change and doesn't make sense if the default driver SSL context works.\n. For SQS it buffers on the sender before writing to SQS.  This helps to make use of the 256KB message cap that SQS imposes and reduces API calls.  The SQS collector only reads as fast as the storage layer accepts writes.  For our use case SQS is effectively an off memory buffer just as Kafka would be.\nLooking at the MySQLSpanConsumer I don't see any fixed 6KB limiting logic.  It might be possible to introduce some logic to dynamically adjust the batch size to some tunable value though.\nIf your stuck with MySQL for storage and your using HTTP as the transport layer I would probably consider augmenting writes with SQS or Kafka just to avoid spikes in traffic from overwhelming your storage layer.\nBeyond tuning batch inserts for a specific storage component, I don't think there is much benefit in Zipkin server buffering anything since there are much better solutions such as SQS, Kafka or a more scalable storage layer.. For SQS it buffers on the sender before writing to SQS.  This helps to make use of the 256KB message cap that SQS imposes and reduces API calls.  The SQS collector only reads as fast as the storage layer accepts writes.  For our use case SQS is effectively an off memory buffer just as Kafka would be.\nLooking at the MySQLSpanConsumer I don't see any fixed 6KB limiting logic.  It might be possible to introduce some logic to dynamically adjust the batch size to some tunable value though.\nIf your stuck with MySQL for storage and your using HTTP as the transport layer I would probably consider augmenting writes with SQS or Kafka just to avoid spikes in traffic from overwhelming your storage layer.\nBeyond tuning batch inserts for a specific storage component, I don't think there is much benefit in Zipkin server buffering anything since there are much better solutions such as SQS, Kafka or a more scalable storage layer.. good catch! \ud83d\udc4d . good catch! \ud83d\udc4d . @aliostad to answer the question about how I do this with zipkin-aws.  Right now we make a fat jar by depending on zipkin and our extra auto configuration libs.  From there we create a new layer on the zipkin docker container by exploding the fat jar in the same way the upstream build does.  This is  fairly easy though it does make the container larger than needed since the old layer still remains.\nIdeally we need to figure out an elegant way to only layer in new jars.  Both the property launcher and thin launcher sound promising though I don't have experience with either so I'm anxious to see what the developer experience would be like.. @aliostad to answer the question about how I do this with zipkin-aws.  Right now we make a fat jar by depending on zipkin and our extra auto configuration libs.  From there we create a new layer on the zipkin docker container by exploding the fat jar in the same way the upstream build does.  This is  fairly easy though it does make the container larger than needed since the old layer still remains.\nIdeally we need to figure out an elegant way to only layer in new jars.  Both the property launcher and thin launcher sound promising though I don't have experience with either so I'm anxious to see what the developer experience would be like.. It would be great to reproduce in a test.   Right now its pretty hit or miss in production though so I'm not sure the best way to reproduce it consistently yet.. It would be great to reproduce in a test.   Right now its pretty hit or miss in production though so I'm not sure the best way to reproduce it consistently yet.. The case where we hit this the most was on getSpansByTraceIds.  If we put a high number, sometimes even 1000, in the UI trace limit field it can trigger the error.  For other read paths such as getSpanNames we didn't hit this error.  Even though we didn't encounter this in other read paths, the pattern seemed easy enough to generalize that it could be applied to all areas and not just getSpansByTraceIds. . Testing this locally I was able to trigger it by increasing the trace count here: https://github.com/openzipkin/zipkin/blob/master/zipkin-storage/cassandra3/src/test/java/zipkin/storage/cassandra3/CassandraSpanStoreTest.java#L71\nI was hesitant to break 5000 traces just for the test.  Ideally we could set the default fetch size on the driver to a lower value and it will trigger this error as well.  Setting the fetch size for the tests will take some work though and possibly some changes to the Cassandra3Storage builder.. Sure i will try increasing that tomorrow.. Was hoping that this would have triggered it.  Setting the fetch size to something lower immediately triggers it so I'm a bit puzzled why this didn't.\nI'm going to close this for now until I can trigger this in a local test.  Then I will tackle the accept async join bit.. Locally I've been able to trigger this error with as few as 2000 traces.  After some investigation I see that the Cassandra tests are skipped in the Travis PR builds which is why this PR didn't fail.\nAlso doubling the trace count didn't have a noticeable impact on test time so I'm wondering if its worth the effort to fan out the accept calls.  I can still add that if it seems valuable though.\nRight now I'm not sure how to trigger the failure in a pull request without merging it which will surely break the build :). Looks like CircleCI is skipping the C tests as well.  My suspicion is that C is failing to fully start.  When I get a chance I will SSH into the build machine and look at the logs.\n```\nRunning zipkin.storage.cassandra3.CassandraSpanStoreTest\n21:10:37.900 [main] INFO  c.datastax.driver.core.ClockFactory - Using native clock to generate timestamps.\n21:10:37.952 [main] INFO  com.datastax.driver.core.NettyUtil - Did not find Netty's native epoll transport in the classpath, defaulting to NIO.\nTests run: 41, Failures: 0, Errors: 0, Skipped: 41, Time elapsed: 3.571 sec\nResults :\nTests run: 60, Failures: 0, Errors: 0, Skipped: 41\n```. Looks like the tests are still being skipped on the CI server.  This has been working locally for me so my best guess is that the CI server is a bit slower and the container isn't completely ready so the health check fails and subsequently the tests are skipped.  Might need to work on a custom CassandraContainer implementation that executes a command as the container wait condition.. Success!  We now have 5 C* test failures in the CI build :-). zipkin-layout-factory sounds fine to me. zipkin-layout-factory sounds fine to me. Looks great so far!  Thanks a ton for working on this @michaelsembwever . Looks great so far!  Thanks a ton for working on this @michaelsembwever . If we refine this to local only then I would vote for introducing another query.  Reasoning is that we often call services we don't own and being able to search on these remote services could be helpful in some cases.. Storage has been a pain point for me as well and finally to the point that I need to consider options.  Both of the suggestions here would work for me.  Using a different TokenAnalyzer is a really nice idea and I'm open to adopting that approach.  My only concern is that it might not be something new users can easily adopt and might be a source of confusion long term.\nShort term the TokenAnalyzer would be an easier \"quick fix\" and at the same time still allow for adopting a longer term solution later without data loss.\nIf I was to invest some resource to assist on an improvement, which of these two would others like to see as a long term solution?. @michaelsembwever search in the UI is super important to us and one of the drivers for our early adoption of the Cassandra3 span store vs the original Cassandra span store.\nSounds like things might be leaning towards a TokenAnalyzer approach.  For now I will try and adopt that approach as well and contribute, review or test where I can.. I didn't realize we had negative tests for this.  I was looking forward to the ability to do prefixed based searches :/  It makes sense not to change API contracts for this right now though.\nP.S.  sorry for closing this.  Comment and Close is way to close to the Comment button :/. Moving this to a separate branch https://github.com/openzipkin/zipkin/tree/multifetch. Moving this to a separate branch https://github.com/openzipkin/zipkin/tree/multifetch. Ha guess it would help if I saved the file before making the pull request!. Ha guess it would help if I saved the file before making the pull request!. The default fetch size is 5000.  I think there are 2 ways to try and test, either set fetch size lower or add more data.  Setting the fetch size lower is a bit harder but we could try doing that when the driver is instantiated.. :D https://github.com/SmartThingsOSS/zipkin-dependencies-cassandra3\nTried to make that work in the Java project but couldn't figure out how to write TypeConverter classes for the UDT objects.  To be honest I didn't try hard to figure it out since I knew I could get it working in Scala fairly quickly.. @michaelsembwever thanks for taking a look at this.  I'm not sure I completely follow your suggestion though.  Is there a reference example that you can point me at?\nThe approach I've taken here was borrowed from Datastax documentation on async paging https://docs.datastax.com/en/developer/java-driver/3.1/manual/async/#async-paging. Sounds like a good optimization to me. . Is there a reason we wouldn't want the trailing %?   I'm guessing that without the trailing % it will just do a strict match vs a partial prefix right?. From what I recall we decided to use a separate query request for this called DependencyQueryRequest.. ",
    "dan-tr": "We are a fairly mature ES shop and already have a cleanup process for our ES clusters. So I would definitely like the option to turn off zipkin-server cleanup (when/if it becomes available) and instead use our existing external cleanup process.\n. We are a fairly mature ES shop and already have a cleanup process for our ES clusters. So I would definitely like the option to turn off zipkin-server cleanup (when/if it becomes available) and instead use our existing external cleanup process.\n. Actually, I think all I am really looking for is when did the record get inserted into Elasticsearch. If Zipkin does only inserts, then I am looking for the record creation timestamp. If it does inserts and updates, then I am looking for the last update timestamp.\nSo I don't think any calculation needs to be done. Zipkin just needs to store the current time on each insert/update.\nIn production this timestamp can be valuable when something goes wrong and you want to know the last time Zipkin wrote anything to ES.\nDoes that make sense?\n. I agree this is pretty opsy. I think logstash does this by adding an @ sign to the front of the field. Maybe Zipkin could use the same convention. \n. Where is the metric emitted to? And how does it correlate that to the record written to Elasticsearch? Seems like we would need to emit the ingest latency along with the span id. \n. Maybe the insert timestamp could be an optional field turned on either through an environment variable or maybe the Elasticsearch template (not sure about that).\nBecause we are using async messaging (via Kafka) there is always the possibility of a message backlog. Knowing the insert date would be very help in deducing how far behind the server is in processing the backlog.\n. Hum... To me that feels like we would be overloading a data field for two different purposes (for timestamping and identification).\nI think for now I would prefer to just let this issue \"percolate\" for a while and see anyone else asks for it.\nNo point making a change like this just for me :smiley: \n. I like the field name collector_timestamp_millis as well.\n. @adriancole Is there a reason Zipkin can't use a wildcard in the index name to just query across all zipkin indexes: \nzipkin-*\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/multi-index.html\n. I would really like this feature. As comprise though, I wouldn't mind if this was a configurable options specifically for the Elasticsearch storage option.\n. @adriancole Yes, we are using Docker. I like the idea of implementing a plugin for this. Maybe it could be even more generic where by I could build a jar (or something similar) that could intercept the data as it written to ES and that would allow me to add the collection timestamp or do anything else at the time of ingestion.\nJust a thought.\n. @adriancole Yes, we are using Docker. I like the idea of implementing a plugin for this. Maybe it could be even more generic where by I could build a jar (or something similar) that could intercept the data as it written to ES and that would allow me to add the collection timestamp or do anything else at the time of ingestion.\nJust a thought.\n. I was just made aware of the new feature in ES 5:\nhttps://www.elastic.co/blog/new-way-to-ingest-part-1\nWith the new processors we may be able to have ES add an ingest date for us.\n. I was just made aware of the new feature in ES 5:\nhttps://www.elastic.co/blog/new-way-to-ingest-part-1\nWith the new processors we may be able to have ES add an ingest date for us.\n. We're not running ES 5 yet, but hope to be soon (Next 6 months :smile: ), but I am willing to table this issue until after we get a chance to see if ES 5 will solve the problem.\n. We're not running ES 5 yet, but hope to be soon (Next 6 months :smile: ), but I am willing to table this issue until after we get a chance to see if ES 5 will solve the problem.\n. We are deploying Zipkin Server with an Elasticsearch backend, and would actually prefer to use the HTTP rather than the transport protocol. . ",
    "jcchavezs": "Is this still a valid issue? does Zipkin still persisting data in MySQL with V1 model? If not, does V2 have a decent performance?. @adriancole shall we close this?. @adriancole shall we close this?. I also have interest on this. Golang is a good option IMO as a portability advantage and also performance wise. I guess as outcome of this issue we might come up with a list of requirements for the agent.. I also have interest on this. Golang is a good option IMO as a portability advantage and also performance wise. I guess as outcome of this issue we might come up with a list of requirements for the agent.. I just started working on an agent. Have a look at https://github.com/jcchavezs/zipkin-agent. @hexchain regarding the combination of json and Kafka @stakhiv has an idea on how to make it work in a overheadless way.. :+1:. +1 to http.route. You can probably get a faster answer from the chat:\nhttps://gitter.im/openzipkin/zipkin\n\n--\nJos\u00e9 Carlos Ch\u00e1vez\nSpain: (+34) 672921339\nPer\u00fa: (+51) 708-5422 Anexo 918\n\nhttps://github.com/jcchavezs\n. Yeah, I realized I made a mistake right after I open the PR so I just closed it. Sorry for he buzz.. I think this is more an itsio feature question rather than zipkin. Zipkin\ndisplays what you send to the backend.. Yeah, have a look at this: https://blog.github.com/2018-05-02-issue-template-improvements/.\nEvery time someone attempts to open an issue, that person have to select the issue type and then fill the stuff.\n\n. Yeah, have a look at this: https://blog.github.com/2018-05-02-issue-template-improvements/.\nEvery time someone attempts to open an issue, that person have to select the issue type and then fill the stuff.\n\n. +1 from my side. I would like us to consider also moving the libraries (at\nleast the ones in the openzipkin namespace.\n. Mixed feelings here. In server side you do not always know the host whereas\nin client side host can be a domain, an IP or just a name (if in a docker\ncompose network). What I would prefer as a user is to have an boolean\noption which is true when I want to record the host and false when I don't\nboth in server and client side instrumentation.\nIn any case, being explicit about host not including the port is important.\nDoes that make sense?. @adriancole I should have been more clear. I meant that having a flag in the options of the middleware  that says recordHost: true|false so user could decide whether it is meaningful or not to record such information.\nIn all setup I have been dealing with (20 services) http.host is not so relevant because in client side it is usually serviceName what you check. In server side it can be anything (IP, AWS instance id or localhost). I see this tag in both cases as optional.. Yeah that sounds reasonable. For example, in Go http middleware the\nhttp.response_size is not recorded by default (because it the response\nbody is a buffer and if you want to measure it you gotta read it and that\ndrains the buffer) so I see this per-tag option comming. recordedTags:...\nsounds reasonable to me.\nDen l\u00f8r. 11. aug. 2018, 11:53 skrev Adrian Cole notifications@github.com:\n\n\n@adriancole https://github.com/adriancole I should have been more\nclear. I meant that having a flag in the options of the middleware that\nsays recordHost: true|false so user could decide whether it is meaningful\nor not to record such information\n\n\nah ok. well I see your point now. hmmm I guess if we decided to not make it\nstandard then people will add it anyway like they do today or we can do\nsomething like you mention to make it easier. otoh if we were going the\nroute of not recording by default I might be more inclined to list the\n\"standard tags\" desired vs a flag for each. this could be programmatic or\ndeclarative like http.client.tags=http.method,http.path,http.host I think\nsome people maybe python does this already.\nat any rate suspect we should get a sense of who wants the default to\ninclude host or more importantly who doesn't. I think we can sort a nice\nway our regardless of the decision.\n\n\u2014\nYou are receiving this because you are on a team that was mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2167#issuecomment-412264607,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AC7sAjbbaChpxE_HOY3VBce3zk4SX4Phks5uPqmsgaJpZM4V49h7\n.\n-- \n\nJos\u00e9 Carlos\n. Could you elaborate more on this? Since the dropdown allows you to search I\nam curious about the value being added by sorting the service names. This\ncould be easy to do if we sort in frontend tho.\nJos\u00e9 Carlos Ch\u00e1vez\nDen tor. 4. okt. 2018 kl. 15:46 skrev Jorg Heymans <notifications@github.com\n\n:\nIt would make sense to alphabetically order the list of services in the\n'Service Name' dropdown.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2204, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AC7sArpFdGS6EJzZbcK2DnyUeYrL9CvWks5uhhEdgaJpZM4XIJJ0\n.\n. @adriancole I don't think not humans read the text before clicking the button.. Uhmm, that is post hoc ergo propter hoc, I am saying robots don't read the text and click, I am not saying someone is a robot if and only if they click in the button without reading.. Spoiler alert, he is always doing benchmarks.\n\nDen ons. 5. des. 2018, 13:02 skrev Adrian Cole <notifications@github.com:\n\nI think we are probably not doing things in the most efficient way. I'm\nhoping maybe someone like @ghermeto https://github.com/ghermeto could\nreview some of our code for opportunities for improvement or even better\nimprove it. Goal would be to ensure faster rendering of traces with 1k\nspans (look at zipkin-ui/testdata for examples)\nNotably the following files would be helpful to get some performance\nadvice on, and especially some thoughts on tooling to ensure we know how\nwell they are doing:\nzipkin-ui/js/component_data/skew.js\nzipkin-ui/js/component_data/spanCleaner.js\nzipkin-ui/js/component_data/spanNode.js\nzipkin-ui/js/component_ui//traceSummary.js\nzipkin-ui/js/component_ui//traceToMustache.js\ncc @openzipkin/ui https://github.com/orgs/openzipkin/teams/ui\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2314, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AC7sAuBO4ISzzyMpF5ONmwjTiixjLUtdks5u17XLgaJpZM4ZClnp\n.\n. Spoiler alert, he is always doing benchmarks.\n\nDen ons. 5. des. 2018, 13:02 skrev Adrian Cole <notifications@github.com:\n\nI think we are probably not doing things in the most efficient way. I'm\nhoping maybe someone like @ghermeto https://github.com/ghermeto could\nreview some of our code for opportunities for improvement or even better\nimprove it. Goal would be to ensure faster rendering of traces with 1k\nspans (look at zipkin-ui/testdata for examples)\nNotably the following files would be helpful to get some performance\nadvice on, and especially some thoughts on tooling to ensure we know how\nwell they are doing:\nzipkin-ui/js/component_data/skew.js\nzipkin-ui/js/component_data/spanCleaner.js\nzipkin-ui/js/component_data/spanNode.js\nzipkin-ui/js/component_ui//traceSummary.js\nzipkin-ui/js/component_ui//traceToMustache.js\ncc @openzipkin/ui https://github.com/orgs/openzipkin/teams/ui\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2314, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AC7sAuBO4ISzzyMpF5ONmwjTiixjLUtdks5u17XLgaJpZM4ZClnp\n.\n. What language are you instrumenting?\n\nDen tir. 19. mar. 2019, 07:17 skrev Aaron Stainback \nnotifications@github.com:\n\nIf I have already sent a {cs,cr} or {sr,cs} with a duration is it possible\nto \"overwrite\" what's currently there? This seems to work fine if the first\nduration I sent was 0 then I can still update it in the future but if the\nfirst duration I sent was anything else I can't seem to overwrite it with a\nnew duration. Any help is appreciated. Thanks.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2452, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AC7sAsefWgGXXhyCIhDx6bx1rDcV0mZaks5vYIEBgaJpZM4b7XRc\n.\n. Right. I would do a progressive query (by pagination but only simple one, no ordering and deifnirively not with offset but with after.. \n",
    "ahus1": "I've been able to relocate single-page web apps, but only when they used hash based client side routing. Once the single-page web app uses hash based routing, all API calls can use paths relative to location of the single page app. I don't know if this is an option here?\n. I've been able to relocate single-page web apps, but only when they used hash based client side routing. Once the single-page web app uses hash based routing, all API calls can use paths relative to location of the single page app. I don't know if this is an option here?\n. ",
    "bpatelcs": "@adriancole  I tried the second approach by changing the js source code as mentioned in your comment, then I built the zipkin-ui.jar and put it under the /zipkin/BOOT-INF/lib/ in the docker image , but it does not seem to work, it return 404 for following path,\n/zipkin\nam I missing something?. @adriancole  I tried the second approach by changing the js source code as mentioned in your comment, then I built the zipkin-ui.jar and put it under the /zipkin/BOOT-INF/lib/ in the docker image , but it does not seem to work, it return 404 for following path,\n/zipkin\nam I missing something?. ",
    "ilutz": "This feature would be a nice to have.\nWe have a standard-ish way of addressing apps and services in our architecture, which basically has the service name as the first element on the path. Various  services , our service monitoring, health, and service discovery rely on this. We will be running zipkin-servers as a standard service (standard by our architecture) and hence we want to follow suite and follow our standard practices. \nHaving said that, we can quite easily work around that, so for the sake of keeping zipkin as standard as possible, that's our approach for now.\n. ",
    "doo-gl": "I am running into this exact problem right now.\nHaving the root be /zipkin would at least make it possible to put behind a proxy, like zuul, provided the person implementing is happy to change their proxy configuration to fit with zipkin's root (or is able to change their proxy configuration, for some people this will not be an option).\nBeing able to configure the root would make putting the zipkin server and ui behind a proxy a lot easier and allow people to change zipkin to suit their network infrastructure needs, rather than getting their network infrastructure to suit zipkin's needs.\nMy current project would be fine with a root of /zipkin but this fix won't be a solution in all cases.. I am running into this exact problem right now.\nHaving the root be /zipkin would at least make it possible to put behind a proxy, like zuul, provided the person implementing is happy to change their proxy configuration to fit with zipkin's root (or is able to change their proxy configuration, for some people this will not be an option).\nBeing able to configure the root would make putting the zipkin server and ui behind a proxy a lot easier and allow people to change zipkin to suit their network infrastructure needs, rather than getting their network infrastructure to suit zipkin's needs.\nMy current project would be fine with a root of /zipkin but this fix won't be a solution in all cases.. ",
    "camerondavison": "I like the idea of at least having it on /zipkin if nothing else. . I like the idea of at least having it on /zipkin if nothing else. . I just tried this out, not sure what I am doing wrong, but the api request is going to /api/v1 instead of /zipkin/api/v1 and since zipkin is setup behind a proxy /api/ will not get proxied to zipkin. Did I miss some setting?. I will see what I can do. I was just about to say that https://github.com/openzipkin/zipkin/blob/b9cfb3ecca506c9c013d0e2451bdd8fcdc162f05/zipkin-ui/js/component_data/trace.js#L17 is not correct. When I try to go to /zipkin/api/v1/services on a local running server, and on the proxied server both are returning 404\nI do not know why https://github.com/openzipkin/zipkin/commit/b9cfb3ecca506c9c013d0e2451bdd8fcdc162f05#diff-fc3d346937cd3a0d3094ab2716386dddR260 test is passing.\nit seems like maybe there is something wrong with https://github.com/openzipkin/zipkin/commit/b9cfb3ecca506c9c013d0e2451bdd8fcdc162f05#diff-3f99bf822098bb694547a878f8e7b166R134 that specification but I have no idea how to fix that in spring.\nI added\njava\n  @RequestMapping(value = \"/zipkin/api/v1/services\", method = GET)\n  public ModelAndView forwardApiServices() {\n    return new ModelAndView(\"forward:/api/v1/services\");\n  }\nto that file and it worked as expected so just need to figure out how to do a route prefix correctly. ",
    "ggn06awu": "Any progress on this, would be nice to serve zipkin off a path under the main application's context, rather than off a different endpoint altogether. Seems like quite a big glaring omission for such a great tool!. Any progress on this, would be nice to serve zipkin off a path under the main application's context, rather than off a different endpoint altogether. Seems like quite a big glaring omission for such a great tool!. ",
    "gurinderu": "@adriancole \nCould you tell me how to run a Zipkin under a custom path (like /services/zipkin)?. @adriancole \nCould you tell me how to run a Zipkin under a custom path (like /services/zipkin)?. @eirslett is it impossible to change a base href tag?. @eirslett is it impossible to change a base href tag?. @eirslett Are you sure? we can modify index.html with a transformer in Spring. @eirslett Are you sure? we can modify index.html with a transformer in Spring. @adriancole yep, I know. \nBut I think your last commit for this issue will help me). @adriancole yep, I know. \nBut I think your last commit for this issue will help me). ",
    "shakuzen": "Note that the maven-jar-plugin is up to version 3.0.2 now which should support JDK 9.\n. First of all, thank you for the PR @hyleung!\nAs of Spring 4.3, the @CrossOrigin's origins parameter supports placeholders (SPR-14010). Zipkin is already on Spring 4.3.x, and thus, we could just put the annotation on the ZipkinQueryApiV1 controller with some property like zipkin.query.cors-origins to configure the origins, with a default of * in zipkin-server.yml. This way people can customize it.\nIt feels a little weird to me to be able to configure the allowed CORS origins for any path on the Zipkin server via properties, as the implementation at the time of this comment allows. I don't think we want to allow that, actually.\n. I guess I could update all the links to relative while I'm at it :)\n. +1 for metrics over adding a meta field to the data model.\n. > My suggestion is the following:\n\n\nHard limit the begin timestamp to no later than when zipkin's elasticsearch was written, March 2016.\nLookup the indexes dynamically and hard-limit queries to not attempt to read before them\n\n\nThese both sound reasonable to me.\n. http://docs.spring.io/spring-boot/docs/current/reference/html/howto-embedded-servlet-containers.html#howto-configure-accesslogs\n. http://docs.spring.io/spring-boot/docs/current/reference/html/howto-embedded-servlet-containers.html#howto-configure-accesslogs\n. > Would replacing \"timestamp\" with \"micros\" or adding a suffix clear up enough ambiguity to be worth its characters?\nI personally think unambiguous field names are best, so startTimestampMicros and finishTimestampMicros sound good to me. This way, the field names largely document themselves.\nI don't see much value in optimizing for size here, especially considering compression. The marginal performance that I presume would be gained would not be worth the loss in clarity. And if such performance were a significant factor, JSON is already the wrong choice.. Drop it like it's hot. @Akhilesh-Anb feel free to come chat with us on Gitter if you still have questions. As Adrian mentioned, you can follow the Quick Start instructions if you just want to run Zipkin Server: https://github.com/openzipkin/zipkin#quick-start. Should be able to do this easily with @Profile or some Conditional on the controller. I forget if it is ill-advised to use Spring Boot's @ConditionalOnProperty outside of an auto-configuration class...\nEDIT: Confirmed using a Conditional including @ConditionalOnProperty should be fine. See this Gitter conversation. @ronperei the recommended and supported way of using the Zipkin Server is running the pre-built Zipkin Server as described in the quick start instructions. Most users should not need to build anything themselves to run the Zipkin Server. If you have more questions, please feel free to join us on Gitter.\nEdit: sorry @ezraroi, I mentioned you by mistake originally. @ronperei the recommended and supported way of using the Zipkin Server is running the pre-built Zipkin Server as described in the quick start instructions. Most users should not need to build anything themselves to run the Zipkin Server. If you have more questions, please feel free to join us on Gitter.\nEdit: sorry @ezraroi, I mentioned you by mistake originally. You can also reference what load and number of instances some users handle, as documented in sub pages here: https://cwiki.apache.org/confluence/display/ZIPKIN/Sites\nAs @jcarres-mdsol mentioned, your results will vary depending on sizing and different bottlenecks.. I like the brevity of zipkin-layout-factory, but that (for better or worse) restricts the repo to a very narrow scope. zipkin-spring-boot-loader-tools on the other hand feels like it could have a couple related concerns in it (though so far I don't know what they would be other than the one mentioned in this issue)\nIf I had to pick one of the suggested names now, I would go with zipkin-layout-factory. I can\u2019t think of a better one now, either.. Closing as this is done thanks to the great work of @zeagord https://github.com/openzipkin/zipkin-layout-factory. It is also worth noting that Spring Cloud Sleuth is one library that offers an option to use RabbitMQ via Spring Cloud Stream and a custom Zipkin server (Zipkin Stream Server), as per the documentation. That implementation could be a good reference for standardizing this.. There is an official RabbitMQ Java client: https://github.com/rabbitmq/rabbitmq-java-client (only depends on SLF4J, I believe)\nSince version 4.0, its releases are decoupled from the RabbitMQ server releases while compatibility is still maintained, which is nice.\nRegarding the version of RabbitMQ to test against, I think we are probably fine to use the latest stable (currently 3.6.11). I took a quick look at some SaaS offerings for RabbitMQ (AmqpHosting.com, CloudAMQP, and Compose) and cloud providers' RabbitMQ offerings (Azure, Google Cloud Platform, and Pivotal Cloud Foundry). They all seem to keep up-to-date with the latest stable, and most of the SaaS offerings provide one-click RabbitMQ cluster version upgrades.\nThat said, companies managing their own RabbitMQ servers are quite likely not on the latest version. Even so, our use case should be straight forward enough. That is, I wouldn't expect our use case to have had backwards incompatibilities introduced in the past few years (famous last words).\nSince there is a RabbitMQ server Docker image, we could use TestContainers to test against any version of the server we want fairly easily.\n\nAlso, let me reduce the scope on this issue to just collection since that is all that would be implemented in this repo. We can still list tracers that support reporting to RabbitMQ here after collection is implemented in case people find this issue when searching. I will open and link an issue in zipkin-reporter-java for reporting to RabbitMQ, for now.. I think Hazelcast as a storage option is quite interesting, but I will also echo what @adriancole stated. I think this would do better, at least initially, outside of the main Zipkin repository. Once it is off the ground there with some users and support, it might be a good time to consider merging it in as an official storage option. I'd be willing to help review an implementation as well, if useful.. @jprateekvmware is there still an issue with the latest versions?. Looks like this might be related: https://stackoverflow.com/questions/3401113/spring-mvc-redirect-prefix-always-redirects-to-http-how-do-i-make-it-stay. Looks like this might be related: https://stackoverflow.com/questions/3401113/spring-mvc-redirect-prefix-always-redirects-to-http-how-do-i-make-it-stay. @sri420 Also note that the in-memory storage is only really meant to be used when doing local testing or proof of concept demos. For production, you should use one of the persistent storage options.\nhttps://github.com/openzipkin/zipkin#storage-component. @sri420 Also note that the in-memory storage is only really meant to be used when doing local testing or proof of concept demos. For production, you should use one of the persistent storage options.\nhttps://github.com/openzipkin/zipkin#storage-component. As mentioned in previous comments, we have a property to do this for the in-memory storage which is only meant for demos / local testing. For persistent storage, how to expire data depends on the storage implementation used and is not done by Zipkin itself.. @Mobel123 is this still an issue with the latest version? Note we have a new cassandra storage since this issue was opened. https://github.com/openzipkin/zipkin/tree/master/zipkin-storage/cassandra. Could we make the red and yellow thresholds configurable?\nI guess maybe something like highErrorRate (red) and lowErrorRate (yellow) instead of the hard-coded values?. Could we make the red and yellow thresholds configurable?\nI guess maybe something like highErrorRate (red) and lowErrorRate (yellow) instead of the hard-coded values?. @adriancole Wouldn't a rate of zero (or less) make all the lines yellow or red? A value above 1 I would expect to effectively disable the coloring (since you can't have more than a 100% error rate).. @adriancole Wouldn't a rate of zero (or less) make all the lines yellow or red? A value above 1 I would expect to effectively disable the coloring (since you can't have more than a 100% error rate).. Related to #1833?. @pvprsd please join us on Gitter (https://gitter.im/openzipkin/zipkin) if you have questions.. Resolved via #1758. I believe this should be resolved with latest versions now. If not, we can reopen.. I have at times wanted this information without having to do the subtraction in my head.. I also think this would be useful. What does everyone else think?. Fixed by https://github.com/openzipkin/openzipkin.github.io/pull/90. We no longer publish the io.zipkin.java:zipkin artifact, so this should not be an issue with recent versions.. In addition to Adrian's comment, you do not need to build Zipkin Server yourself. You can use the pre-built package we provide as a JAR or Docker image, mentioned in the Quick Start: https://github.com/openzipkin/zipkin/tree/master/zipkin-server#quick-start\nYou can configure it to connect to Elasticsearch as mentioned in the documentation: https://github.com/openzipkin/zipkin/tree/master/zipkin-server#elasticsearch-storage\nCome chat with us on Gitter if you have additional questions: https://gitter.im/openzipkin/zipkin. It seems the information provided has resolved the issue. For additional questions, come chat with us on Gitter: https://gitter.im/openzipkin/zipkin. As mentioned in the issue template in the description of the issue you opened, questions should be asked on Stack Overflow or Gitter. Feel free to come chat with us on Gitter and share some more details https://gitter.im/openzipkin/zipkin. As mentioned on Gitter, the part of doing this for the Zipkin JAR everyone uses that is a concern is the warning in the linked documentation, copied below:\n\nFully executable jars work by embedding an extra script at the front of the file. Currently, some tools do not accept this format so you may not always be able to use this technique. For example, jar -xf may silently fail to extract a jar or war that has been made fully-executable. It is recommended that you only make your jar or war fully executable if you intend to execute it directly, rather than running it with java -jar or deploying it to a servlet container.\n\nFor instance, I thought the Cloud Foundry Java buildpack did not support these because it tries to unpack the JAR before running it, but it appears that with the latest version of the Java buildpack, it has no problem running these. My memory may have been before support was added.\nSo maybe it isn't a big issue that things like jar -xf won't work?. Mentioning the option in the readme sounds reasonable, but it isn't usable for all Windows users for instance. It also could just generally be unfamiliar for Java developers used to working with normal JARs. I think the default advice remaining using java -jar is better. But mentioning that it can be registered as a service and linking to the corresponding Spring Boot documentation seems like a good idea to me.. @WiFeng Is this still an issue with the latest version of Zipkin? What browser are you using? If you can provide details so we can reproduce this, we will reopen this and look into it.. > Why not support custom builds?\n@chuguoren it's possible we can support more features in the prebuilt Zipkin Server, but we need to gauge user interest and adoption/support for each. Supporting any possible custom build requires an unbounded amount of support resources, while we have limited resources (volunteers and paid staff) to help. As @zeagord explained, using a custom build is still an option, as long as you are willing to support yourself in doing so.\n\nI want to register zipkin to eureka server\n\nsee #1870 for the feature request to support service discovery. You can comment there explaining your situation.\n\nand also get configuration from spring config server.\n\nI don't think we've had a request for this, so you can be the first to make a ticket for it if you're interested in it and want to see if others are as well.. Since #1802, the UI now uses the v2 API endpoints. Therefore, I think it's no longer an issue for you, but if it is, please try with the latest version and come chat with us on Gitter: https://gitter.im/openzipkin/zipkin. Fixed in openzipkin/docker-jre-full#7 and docker images since zipkin 2.7.4. I have reproduce the issue. Indeed, we should check for an existing queue of the configured name and do nothing in that case.. @ldcsaa sorry for the long wait on this, but the bug should be fixed in the latest release (2.12.1). Let us know if you still have any issue with this.. @alseddnm are you able to try with more recent versions to see if things work better?. @wuyoushan Please use the latest released version of the server. You can find the quick start instructions here: https://github.com/openzipkin/zipkin/#quick-start\nIf you have further questions, please ask us on Gitter (https://gitter.im/openzipkin/zipkin) because we prefer to use GitHub issues for feature requests, bug reports, and enhancement requests.. As part of the 2.10 release, we dropped the v1 read HTTP endpoints. If you use the v2 endpoint (as the UI itself does now), it should work. I'm curious: what is the use case for accessing the HTTP endpoint directly?. @595979101 Thank you for the additional information. This looks like a bug, then. We should update the UI to use the v2 endpoint.. @595979101 the fix for this is released now as version 2.10.3.. @WywTed have you tried with the latest versions? if it is still happening, can you provide the JSON (remove any sensitive data) for us to troubleshoot?. @lukylent can you please open an issue in Brave instead? https://github.com/openzipkin/brave/issues. @lukylent can you please open an issue in Brave instead? https://github.com/openzipkin/brave/issues. @malonso1976 were you able to try this out in a real environment?. @malonso1976 No problem at all. Let us know when you have a chance to test it out.. A similar request has been made before (I can search for it later when not on mobile). Could you explain your use case first, though? Why do you want to access the Zipkin HTTP API programmatically? We are under the assumption that in most cases, only the UI and reporters interact with the HTTP API directly.. Resolved by #2260. I suppose we could merge, but we wouldn't be able to release with this as currently logging is broken. As mentioned in chat, we think this is the same issue as https://github.com/spring-projects/spring-boot/issues/15074.. What that duration even meant was confusing to users and a ticket was opened long ago: https://github.com/openzipkin/zipkin/issues/998. I had never really paid much attention to it before, but I agree it isn't clear what it means. This change seems to be an improvement for people who understand what it means in the first place.. Resolved by #2259. Sounds good to me. If it causes too much trouble for folks we can reconsider then.. According to my testing, TRACK is already disallowed. TRACE is indeed allowed currently.\nI couldn't find a Spring Boot exposed property to control this kind of thing with external configuration, so we will probably have to add some configuration internally for it.. According to my testing, TRACK is already disallowed. TRACE is indeed allowed currently.\nI couldn't find a Spring Boot exposed property to control this kind of thing with external configuration, so we will probably have to add some configuration internally for it.. What about a separate pane that instructs to click on a span to see details? This would provide a separate area that doesn't mess with viewing the trace and work guide people to knowing they can click on spans to view more details.. What about a separate pane that instructs to click on a span to see details? This would provide a separate area that doesn't mess with viewing the trace and work guide people to knowing they can click on spans to view more details.. Do we have tests for Lens that run as part of the build and would catch this kind of thing in the future?. Thanks for the info, @tacigar! I think those can be done as a separate pull request. Could you make an issue to track it just so we don't forget? I don't want to block resolving this pull request that fixes an issue that a user is facing trying out Lens.. @tacigar is this resolved by https://github.com/openzipkin/zipkin/pull/2409?. Could you also make sure it is pulling the latest 2.11 tag? Or tell us which version of the server this is happening on by the response from GET /info?. Thank you for noticing and fixing this!. Thank you for noticing and fixing this!. Here is the original pull request that added it: https://github.com/openzipkin/zipkin/pull/1884\nI definitely used it at my previous job and encouraged others to use it. It is one way of getting around the storage period limitation. That is, for a particular interesting trace, you can save it to your local machine or attach it to an issue (e.g. JIRA) so that others can check that trace at any time easily, without having to worry about the original data expiring from the data store. It's a nicer experience than everyone wanting to do this learning how to POST the data to a locally running instance or something.. As @adriancole mentioned, we don't support custom servers so we won't debug this issue. If you do figure out how to work around whatever the issue is, feel free to post it here so it might help others that face this problem (despite our advice to avoid making a custom Zipkin Server in the first place). The bean definitions mentioned in the log do not appear to be Zipkin-made beans, anyways, so I doubt the issue is in Zipkin code itself.. As @adriancole mentioned, we don't support custom servers so we won't debug this issue. If you do figure out how to work around whatever the issue is, feel free to post it here so it might help others that face this problem (despite our advice to avoid making a custom Zipkin Server in the first place). The bean definitions mentioned in the log do not appear to be Zipkin-made beans, anyways, so I doubt the issue is in Zipkin code itself.. @yyyogev To expand on Raja's answer, even if you build this outside of the Zipkin repo, it should be possible to run it with the standard Zipkin Server as an add-on module like some others we have (for example, see zipkin-gcp's stackdriver storage component).. @Incarnation-p-lee No, 500 response code is generally not expected, but that should be unrelated to the mapping not being logged for the reason Adrian mentioned above.\nIt will be easier to help troubleshoot this issue on Gitter, I think. Come chat with us there: https://gitter.im/openzipkin/zipkin. I'm not sure this change should be any more confusing than Grafana or Prometheus having links to their stuff in their UIs (they do). Arguably this is a little more noticeable than theirs, but not much if you ask me.. Awesome work, Adrian! I think this will really help the transition to Lens.\nOne piece of feedback is that it doesn't seem obvious to a user how to go back to the classic UI right now. What do you think about also adding a button to Lens that's the complement of the current button? \"Go back to classic UI\" or something.. Awesome work, Adrian! I think this will really help the transition to Lens.\nOne piece of feedback is that it doesn't seem obvious to a user how to go back to the classic UI right now. What do you think about also adding a button to Lens that's the complement of the current button? \"Go back to classic UI\" or something.. Duplicate of #2275. I opted to do a find-and-replace on http: in the package-lock.json files rather than regenerate them because regenerating them changes a number of versions and I don't know if that is okay or not.. > Does the trace lookup header have any purpose in this screen?\n@jcarres-mdsol do you mean the search bar at the top where you can select criteria to search? If so, I agree it doesn't have a use on this page and could be confusing, but I think it is there for consistency as a global search context. I'm not a designer so not sure what's best to do about it (hide it for this view only? show it as disabled in this view?), but thank you for bringing it up. What do others think? Maybe something to spin off into a separate issue so we don't block adding this functionality that helps bring Lens up to feature parity with the classic UI.. Personally, I worry that if we put a json option in the universal search dropdown, we're overloading the meaning of options there too much. Currently you select a property of a span (in some sense). Putting \"fromJSON\" or something similar there feels a bit confusing. If we want the JSON upload to be part of the universal search instead of a dedicated view/tab, maybe we should make a dedicated upload button in the universal search? Though that might give the impression that the uploaded JSON is saved.. > whatever we do, we should alert the user they are responsible to edit the file manually before posting anywhere. for example, our masking can take some heavy lifting out of the process but it is still their responsibility to not leak data not the responsibility of our UI code.\nI fully agree. We want to make this clearly stated however the feature surfaces.. We don't support custom Zipkin Servers - it is meant to be run as the executable JAR we publish or the Docker container we publish. That said, there are several ways to add-on features depending on what you are trying to accomplish. It will be easier to discuss this on Gitter, so feel free to join us there: https://gitter.im/openzipkin/zipkin\nIf you want to process the span data somehow before saving, there are folks doing this already and we would be happy to guide you to a solution.. Is your Elasticsearch server run on Windows and does it need to be? If it is just to test locally, maybe you can use Docker to run Elasticsearch?. Anyone finding this later and curious, here is the start of the linked Gitter conversation: https://gitter.im/openzipkin/zipkin?at=5c85f33f3162ec7bc751cd16. Anyone finding this later and curious, here is the start of the linked Gitter conversation: https://gitter.im/openzipkin/zipkin?at=5c85f33f3162ec7bc751cd16. We do not offer support for custom-built servers anymore (using @EnableZipkinServer), but it is still possible for you to make them, technically. Join us on Gitter (https://gitter.im/openzipkin/zipkin) and share what is your use case for wanting to make a custom build. Perhaps there are ways to achieve your goals without making a custom server.. I thought reusing the zipkin.query namespace would be appropriate for this property. @adriancole WDYT?\n. Making the default value here also * would be a good idea for those making custom Zipkin servers with @EnableZipkinServer or those using their own YAML file of properties that might not contain the new property.\nThat is, if we want the default value to be * in these cases also. Do we? Or do we want people in those situations to make a conscious decision about the allowed origins for the query API?\n. I don't know if these single quotes around the * are needed or make a difference.\nA few UT/IT to verify the situations we are covering would, of course, be best. Manual tests in the meantime might suffice.\n. \"the Zipkin\"?\n. \ud83d\udca9\n. Per convention, JavaDoc comments on @ConfigurationProperties fields are plain text (don't use JavaDoc keywords like @see) because they are used by the configuration processor and then by IDEs as descriptions of the fields during auto-completion, for instance.\n. Not a comma-separated list but an actual List\n. This is a broken link in master.\n. This is a broken link in master.\n. This redirects to docker-zipkin-dependencies, which seems wrong.\n. Minor: Could use the new SpringRunner as advised in the tip here.\n. This is not needed anymore, I believe. Reference\n. Is this the right import? I question it because of the Java6Assertions part...\n. Good, thorough explanation \ud83d\udcaf \n. Can you get a List<Interceptor> from the context? Otherwise this looks like an assertion that may break in the future when there is more than one Interceptor bean, which I thought was the point of switching to the List.\n. Should this be annotated with @Configuration?\n. don't need the \"is\"\n. eventhough \u2192 even though\n. I was curious and looked at that issue. It looks like ES 2.x is whitelisted now.\n. Small nit-pick: other variables are kebab-case in this file, so it might match the style better if this were too. Thanks to Spring Boot's relaxed property binding, it shouldn't effectively matter either way, though.. Looks like main.java.version is 1.7.. Nits: name might be more clear than value (which is an alias for name). Maybe it is just me but value = x, havingValue = y reads kind of funny. Also, the default of havingValue should be sufficient for our purposes. Specifying \"true\" might mean something like \"TRUE\" doesn't work.. If we do remove the havingValue parameter on the @ConditionalOnProperty sites, perhaps writing false (formatted) makes it clearer we're looking for that specifically to disable querying. With the havingValue anything but true will disable, I believe.. Is there a reason we don't use the spring-boot-dependencies BOM (or Spring IO Platform BOM) for dependency management if we want to keep versions in sync?. I like those too. Just one thing: it is nice that rate (hopefully) conveys that the value should be between 0 and 1. Threshold could be (mis)taken to mean an absolute number of errors, perhaps.. Looking forward to our eventual Spring Boot 2 upgrade, this would become the hard-to-read ZIPKIN_UI_DEPENDENCYLOWERRORRATE per the new relaxed binding for environment variables. If we wanted to make it a bit easier to read, we could nest dependency properties (even though the two introduced in this PR are the only properties currently...) and it would become ZIPKIN_UI_DEPENDENCY_LOWERRORRATE.. Along those lines, should we have some validation that values are within bounds?. This is ignored because it can't actually connect to a RabbitMQ server at localhost and the application context fails to refresh. This test was taken from the Kafka collector tests, but Kafka doesn't fail to start if it cannot connect to Kafka, I believe. It would still be nice to test that the conditional and rest of the auto-configuration is working. Let me know what you want to do about this.. This is a bit wonky due to the mismatch of types: the properties uses a List<String>, the builder Address[], and of course we are passing just a String via the environment/YAML but Spring converts to a list for us. We could add a converter for Address but this felt like more work than it is worth right now.. This tests against the latest patch version of RabbitMQ 3.6. We could probably fairly easily parameterize this if we wanted to test against multiple versions. A 3.7 release candidate was published recently, for instance.. I chose the alpine varient just because it is a smaller image. I'm not sure I understand. I copied this convention from the corresponding table under \"HTTP Collector\". There seems to be a lack of consistency throughout the README. The \"Self-Tracing\" section reverses two of the columns, for the order of: \"Variable\", \"Property\", \"Description\". All the Storage sections and the \"Scribe Collector\" only give the environment variable and a description. Is this perhaps the convention you meant?. slf4j-api is the RabbitMQ client library's only required dependency but we don't need to use it ourselves. I'm not sure why I did... probably just habit.. Done in latest force pushed commit.. The Channel created here will remain open until the connection is closed, which is not a big deal but it isn't necessary and it may be asked about by operators looking at open channels on RabbitMQ.. Generally speaking, yes. Channels should not be shared among threads. From the Channel JavaDoc:\n\nChannel instances must not be shared between threads. Applications should prefer using a Channel per thread instead of sharing the same Channel across multiple threads. While some operations on channels are safe to invoke concurrently, some are not and will result in incorrect frame interleaving on the wire. Sharing channels between threads will also interfere with Publisher Confirms. As such, applications need to use a Channel per thread.. Yes, closing the connection will close any Channels that it opened. Ref. https://www.rabbitmq.com/releases/rabbitmq-java-client/current-javadoc/com/rabbitmq/client/Connection.html#close--. @bsideup \nFor our other tests run with test containers, I see the following output before the Checking the system... sequence:\n\n[INFO] Running zipkin.storage.mysql.ITMySQLStorage\n07:07:47.617 [main] INFO  o.t.d.EnvironmentAndSystemPropertyClientProviderStrategy - Found docker client settings from environment\n07:07:47.633 [main] INFO  o.t.d.DockerClientProviderStrategy - Found Docker environment with Environment variables, system properties and defaults. Resolved: \n    dockerHost=unix:///var/run/docker.sock\n    apiVersion='{UNKNOWN_VERSION}'\n    registryUrl='https://index.docker.io/v1/'\n    registryUsername='travis'\n    registryPassword='null'\n    registryEmail='null'\n    dockerConfig='DefaultDockerClientConfig[dockerHost=unix:///var/run/docker.sock,registryUsername=travis,registryPassword=<null>,registryEmail=<null>,registryUrl=https://index.docker.io/v1/,dockerConfig=/home/travis/.docker,sslConfig=<null>,apiVersion={UNKNOWN_VERSION}]'\n07:07:47.638 [main] INFO  o.testcontainers.DockerClientFactory - Docker host IP address is localhost\n07:07:47.780 [main] INFO  o.testcontainers.DockerClientFactory - Connected to docker: \n  Server Version: 17.03.1-ce\n  API Version: 1.27\n  Operating System: Ubuntu 14.04.5 LTS\n  Total Memory: 7479 MB\n        \u2139\ufe0e Checking the system...\n        \u2714 Docker version is newer than 1.6.0\n        \u2714 Docker environment has more than 2GB free\n        \u2714 File should be mountable\n        \u2714 Exposed port is accessible\nNote it is entirely missing for the RabbitMQ tests case (as you can see from the output Adrian posted above), which leads me to believe that a different Docker discovery strategy is being used, but looking at the code for the tests I can't figure out why this would be different.\n. I believe this is so if you opened this file, it could (mostly?) render as normal HTML and it would have values in it. Those values will be replaced when the whole application is running.. Unfortunately this link does not work due to the ongoing Maven central certificate problem. I'm not sure what's best for us to do about it, though.. Fixed. This sentence applies to the Kibana 4+ query parameters (and not the Kibana 3 URL). Perhaps this could be made more clear.. Sorry, I should have checked how it rendered. This looks good. Thank you!. Added https://github.com/openzipkin/zipkin/pull/1922/files#diff-e3e00988ebb1a8c1c756f3a67a6d533f. I think it should just be @Value(\"${local.server.port}\") without the @Autowired. OMG CORBA \ud83d\ude31. @ReadOperation is for an Actuator endpoint, but this class is just a regular RestController. I think this annotation isn't having any effect.. In Spring Boot 2, the format of the /health endpoint changed slightly. That is, a $.details element was added, under which all of the health indicators' details are given. Before Spring Boot 2, each detail was at the root level.\nWe'll need to account for this difference if we want to remain backwards compatible with all parts of the /health response body.\nSample Spring Boot 1 GET /health\njson\n{\n    \"diskSpace\": {\n        \"free\": 804290560,\n        \"status\": \"UP\",\n        \"threshold\": 10485760,\n        \"total\": 1056858112\n    },\n    \"status\": \"UP\",\n    \"zipkin\": {\n        \"ElasticsearchHttpStorage\": {\n            \"status\": \"UP\"\n        },\n        \"status\": \"UP\"\n    }\n}. Do we need to use AtomicDouble? Avoiding Guava would seem preferable.\nDo we need strict atomicity for these gauges? Maybe a Double is sufficient.. You are correct. We need a strong reference to some object that can produce the double value when the registry is queried. It doesn't need to be an AtomicDouble, though, I believe. Zipkin Server doesn't have a direct dependency on Guava and there are version considerations due to transitive dependencies on Guava (which are hopefully going away in the future). My comment was mostly to point out I think we would like to avoid a direct dependency on Guava.\nYou are correct it needs to be settable per the warning in that section of the documentation, so my suggestion of Double (which is immutable) is indeed no good.. Regarding http_request_duration_seconds_count as Adrian said on Gitter, I believe, if it isn't documented then I think we can punt that to later. Since we are using Undertow directly outside of Spring MVC, I think we'll need to make some kind of Undertow filter to time (and count) the server requests.. I don't think you can autowire here because this is a JMH test that knows nothing about Spring.. Rather than pass the MeterRegistry, why don't we pass the Timer so we don't have to instantiate various objects and reregister on each request handled?. sysout call intentionally left?. Nevermind, I guess my comment doesn't make much sense because we need to set the tags based on information in the exchange.. If this endpoint does the same thing as the Actuator version, we could just redirect or forward that route. This is what the Spring Initializr did to maintain backwards compatibility with its /info endpoint: https://github.com/spring-io/initializr/blob/540918567b60da18341883750a5a327f4456fd84/initializr-web/src/main/java/io/spring/initializr/web/autoconfigure/InitializrWebConfig.java#L47. I inlined what it appeared the test.sh script was effectively doing. Let me know if I've missed something or you would prefer to call the test.sh file.. I got rid of these as they did not seem needed given the note at the top about MySQL being tested on Travis.. I'm not sure how this is passed or what it does. . Can we get rid of this?. Does this change the Zipkin in the upper left of the UI to uppercase?. Not that I imagine I'll be using microsecond units, as not having to use that was the point of the change, but I think many people don't know how to easily type \u03bc on their keyboard. What if we also accept us for microseconds?. Why do we no longer want to normalize (lowercase) the input? Does the change to keyword make it a case insensitive search? Or if we want to be case sensitive, we should probably have a test for that.. I'm not sure the right verbosity here, but I think we should guide users more. The classic UI has a \"Browse\" button at least, which I think gives more an indication that clicking it will allow one to select a file from their computer to upload. An improvement on that might be to reword with something like \"Upload a file by drag/drop or click to browse\".\nAs an aside, we haven't gotten any reports of confusion, but maybe even more explanation somehow about what kind of file/contents are expected would be good. Anyways, that could be a separate discussion from this pull request.. Why sort them in the server and in the UI? I think just sorting in the UI is fine until we have a reason to sort them in the API response.. ",
    "dwelch2344": "Any word on this? We dev against PG and would love to not have another database dep just for zipkin (especially on dev machines) \n. Any word on this? We dev against PG and would love to not have another database dep just for zipkin (especially on dev machines) \n. Yes! Definitely. Our company is about to launch and one of our goals is to contribute to active open source projects, so we have buy-off from an organizational level to contribute to FOSS. \nWe're on 9.5+, but we could easily support earlier versions. I agree to your previous statements that 9.1 would be a good goal to shoot for. \n. ",
    "hwasungmars": "Is there any update on this front?  I would like to see PostgreSQL support for Zipkin as well.\n. I think most companies don't really need Cassandra on the backend.  I suspect a high-performance alternative for medium-size data will something that will appeal to this audience.  What do you think?\n. I would happily volunteer, but I don't have enough knowledge of Zipkin nor PostgreSQL to take on the task...\n. ",
    "ys": "I sadly have not much to give but would love to see a postgres backend:P\n. I sadly have not much to give but would love to see a postgres backend:P\n. I would be happy to try to review schema ideas/implementations. Do not hesitate to ping me if that happens.\n. I would be happy to try to review schema ideas/implementations. Do not hesitate to ping me if that happens.\n. I have the feeling that looking at the driver, the withSSL part is inevitable for cassandra. \nWith that part, I think that using the default keystore by be enough... but would be rad to have a basic support with just the withSSL merged already so that we can try it.\nNobody ever thought about using encrypted communication between server and Cassandra?\n. I have the feeling that looking at the driver, the withSSL part is inevitable for cassandra. \nWith that part, I think that using the default keystore by be enough... but would be rad to have a basic support with just the withSSL merged already so that we can try it.\nNobody ever thought about using encrypted communication between server and Cassandra?\n. According to the docs https://datastax.github.io/java-driver/manual/ssl/#jsse-property-based it should work\n. According to the docs https://datastax.github.io/java-driver/manual/ssl/#jsse-property-based it should work\n. It works. I confirmed locally.\nCould we get this landed with a version bump?\n. It works. I confirmed locally.\nCould we get this landed with a version bump?\n. @michaelsembwever @adriancole  I did the port to CASSANDRA3, Added the info to the readme and the squash can be done here via the merge button.\n. @michaelsembwever @adriancole  I did the port to CASSANDRA3, Added the info to the readme and the squash can be done here via the merge button.\n. Forgot to push it -- @adriancole Nagging is fine:D\n. Forgot to push it -- @adriancole Nagging is fine:D\n. @adriancole I can try to take a look. Am I correct seeing this broken or am I doing something wrong?. It is a sidekiq background job. \nSo yeah async tasks on same app.. This enqueue the job into redis.\nSide question is how would you render that ?\nIt is async jobs with a queue that is redis.\nDifferent instances. +-------------+       +-------------------------+       +-----------------------------------+\n| web request |  --> | Enqueuing job in sidekiq |  --> | worker pick the job and process it  |\n+-------------+       +-------------------------+       +-----------------------------------+\nhttps://gist.github.com/ys/a7190081d466747e1d42376cd65928fa here are the two classes responsible for serializing the trace and deserialize it.. Did it in https://github.com/openzipkin/zipkin/pull/1401/commits/75d19a6b95cae017207d8a767418e78bef1c34c7\n. ",
    "varyumin": "I too would like see a PostgreSQL backend. ",
    "tajdars": "Is there any work going on for postgres persistence support ? Any way I can help ?. Came across this :\nhttps://github.com/IAMTJW/zipkin/tree/master/zipkin-storage/postgresql\nAny chance of this getting merged(accepted) ?\n. +1 for bounded InMemoryStorage.\n. ",
    "hyleung": "Hmmm... not sure why there's a test failure. I ran ./mvnw  clean install locally and it seems to build ok.\n. Might be able to do it in a ConfigurerAdapter. I'll have a look into it!\nOn Wed, Aug 10, 2016 at 6:57 PM Adrian Cole notifications@github.com\nwrote:\n\nIs @CrossOrigin(*) the way to do this, or is there a parameterized way to\ndo this in config that would allow it to be tightened? cc @shakuzen\nhttps://github.com/shakuzen @dsyer https://github.com/dsyer\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1234#issuecomment-239055450,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAPH-v4J9ZLCfsfTPrv2-AeUA7noqXB8ks5qeoGGgaJpZM4JhLqg\n.\n. Looking at the docs, think you can do it like this:\n\n@Bean\npublic WebMvcConfigurer corsConfigurer() {\n    return new WebMvcConfigurerAdapter() {\n        @Override\n        public void addCorsMappings(CorsRegistry registry) {\n            registry.addMapping(\"/greeting-javaconfig\").allowedOrigins(\"http://localhost:9000\");\n        }\n    };\n}\nMaybe we could get the list of allowed origins out of zipkin-server.yml? Something like:\nserver:\n  port: ${QUERY_PORT:9411}\n  compression:\n    enabled: true\n    # compresses any response over min-response-size (default is 2KiB)\n    # Includes dynamic json content and large static assets from zipkin-ui\n    mime-types: application/json,application/javascript,text/css,image/svg\n  cors:\n    -\n      path: \"/api/v1/*\"\n      allowed-origins:\n        - \"*\"\n        - http://myserver.com\n. @shakuzen, awesome, I had no idea you could do that. That's way easier. I'll make some changes and push the branch again at some point today. Thanks!\n. @adriancole I updated the PR based on @shakuzen's feedback - but still can't get the PR build to pass. \nTests in error: \n  multithreaded(zipkin.storage.cassandra.DeduplicatingExecutorTest): test timed out after 2000 milliseconds\nAs far as I can tell, the build is ok locally - maybe there's something else I should check? (I'm just running ./mvnw clean install)\n. @adriancole hmm...I rebased, pretty sure at the moment I'm up to date vs. upstream getting:\nTests in error: \n  zipkin.collector.kafka.KafkaCollectorTest: test timed out after 10 seconds\nIn any case, I've made the changes discussed above and added a little blurb to the README\n. Cool. Should be able to get those last couple of things done at some point tonight \n. I suppose of it's only the query API that this applies to (which is the case at the moment) then this would make sense (reusing zipkin.query. \n. Yeah the single quotes aren't needed - removed them. \nHappy to try and get some ITs in there - are the ones in zipkin-server/src/it a good example to start with? \n. I don't think that's considered valid yaml(?) IIRC Spring Boot doesn't like it. Yamllint doesn't like it either...\n. \u00af(\u30c4)/\u00af if it's possible for somebody to \"suck at yaml\", that would be me. Thinking about it a little more, this entry in the config file is a little redundant now since the default value is * now - wasn't sure if it was perhaps better \"documentation\" to leave it in.\n. Originally I wanted to add these tests as part of the fixture that's already there - I thought I'd be able to alter the configuration using EnvironmentTestUtils.addEnvironment(...) on a per test basis, but it didn't seem to pick up the updated config. Could very well be my lack of experience with Spring, so if there's a better way, I'm happy to revisit this.\n. ",
    "jpiechowka": "Thank you Adrian for your reply. Unfortunately I cannot access gitter from work.\nSo on the server side, at the beginning I build new Brave object like this:\nBrave brave = new Brave.Builder(\"Server\")\n                .spanCollector(HttpSpanCollector.create(\"http://localhost:9411/\",new EmptySpanCollectorMetricsHandler()))\n                .build();\nThen I played around and I was able to send spans to zipkin like this:\n```\nSpanId span = brave.localTracer().startNewSpan(\"TEST SPAN\", request.url());\nbrave.localTracer().startNewSpan(\"SPAN\", request.url());\nbrave.localTracer().submitAnnotation(\"sr\");\nThread.sleep(2000);\nbrave.localTracer().submitAnnotation(\"ss\");\nbrave.localTracer().finishSpan();\n```\nI don't really understand how to use clientTracer(), serverTracer(), or the interceptors. And the example provided is not very helpful. Can I use localTracer for tracing like this?\nHow to extract all the necessary ids that I receive in the HTTP request for use with brave?\n. @adriancole You were so fast I was not even able to respond :D I will stick with 1.7.0 for now.\n. @adriancole @nicmunroe I am using Wingtips\n. Thank you @adriancole! I will test the new version when it is out.\n. @adriancole Downloaded from https://search.maven.org/remote_content?g=io.zipkin.java&a=zipkin-server&v=LATEST&c=exec and I still get this error with my JSON. Version 1.8.3\n. Hahaha, thanks @adriancole for checking this out again.\n. Tested the new Zipkin version right now and it works well. Thanks @adriancole \n. Tested the new Zipkin version right now and it works well. Thanks @adriancole \n. @adriancole @rogeralsing That would be a nice improvement to the UI if the sorting method would persist.\n. ",
    "lijunyong": "@adriancole when publish zipkin 2 release,it support MQ and asyn call. @adriancole I interested trace MQ and asyn call\n. @adriancole The path of least resistance for async (half-duplex, or messaging), is to follow this model. This prevents us from needing to wait for a big-bang zipkin 2 release of the entire ecosystem.. @adriancole What\u2018s the plans,probably when released,Thanks a lot. @adriancole I don't understand\uff0cCould you provide the example about MQ in github?Thanks a lot. @adriancole should I add span(traceid, parent, spanid) in message header,then send the json data[add \"ms\" and \"mr\" in annotations] to zipkin by spanCollector(HttpSpanCollector),but the duration how to calculate like rest by interceptor?\n. @adriancole I run ./mvnw -DskipTests --also-make -pl zipkin-server clean install  show exception:\n[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.0:npm (npm install) on project zipkin-ui: Failed to run task: 'npm install' failed. (error code -1073741795) -> [Help 1]\n[ERROR]\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR]\n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn  -rf :zipkin-ui\nand which you use IDE?\n. @adriancole I run ./mvnw -DskipTests --also-make -pl zipkin-server clean install  show exception:\n[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.0:npm (npm install) on project zipkin-ui: Failed to run task: 'npm install' failed. (error code -1073741795) -> [Help 1]\n[ERROR]\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR]\n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn  -rf :zipkin-ui\nand which you use IDE?\n. @adriancole thank you very much\n. @adriancole thank you very much\n. @eirslett How to do?\n. @eirslett How to do?\n. @adriancole Thank you,I will change the annotations,but why is black the serviceName in zipkin UI. Thank you,what's the time to support 'ms','ma','mr' in zipkin UI. @adriancole thank you very much, very nice. ",
    "sslavic": "Would be nice if both push (notification/topic) and pull (queue) communication was covered.\nOne tricky thing, which would also be nice to be covered as well, is batching support in both produce (publish), and consume side (push, pull). E.g. one can have multiple publish requests which end up consumed in one pull request. Also, single publish request could publish multiple messages, which get delivered with multiple independent push requests.. ",
    "mbogoevici": "@marcingrzejszczak @adriancole the initial discussion was in the context of Stream apps - we like to have a continuous trace that covers the entire pipeline - but I guess most of the reasoning applies in the general case of apps talking to each other over a broker - including linked spans for pub-sub with multiple consumers. In terms of an update, I have a working POC for Sleuth sans Baggage which I will happy to share once I change it to use the Baggage support that @marcingrzejszczak has added in 1.2. @adriancole re https://github.com/openzipkin/zipkin/issues/1243#issuecomment-266359841 \nThis is largely consistent with what we envisioned for Spring Cloud Stream - with the addition of propagating the root id of the sequence as baggage and sending additional spans towards it as messages enter input channels and exit output channels, so that we can have a span that covers the entire duration of a logical stream. \nI like the differentiation between ws/wr ms/mr as we might use that to capture the time spent with pre-send/post-receive activities (e.g. unmarshalling, etc) as opposed to raw time spent in the broker. In my POC the root span remained parent, but I guess I'll have to update that as the lineage is more correct by setting parents sequentially. \nThe other thing I can think of is identifying the message system as a separate endpoint - i.e. using the broker as the endpoint (e.g. 'kafka' or something else - for Stream we can adopt the logical binder name, in other cases it could be the broker name) and the logical destination as the span name, e.g.\n{traceid=1, parent=, spanid=1 [sr=serviceA, ss=serviceA]} // server span\n{traceid=1, parent=1, spanid=2 [\"make message\", lc=serviceA]} // local span\n{traceid=1, parent=2, spanid=3 [ms=kafka]} // messaging span sent_\nService B consumes the message and puts a message on a second topic\n{traceid=1, parent=2, spanid=3 [mr=kafka]} // messaging span received (same span id)\n{traceid=1, parent=3, spanid=4 [\"process and make another message\", lc=serviceB]} // local span\n{traceid=1, parent=4, spanid=5 [ms=kafka]} // messaging span sent\nService C consumes the message and saves to a database\n{traceid=1, parent=4, spanid=5 [mr=kafka]} // messaging span received (same span id)\n{traceid=1, parent=5, spanid=6 [\"process message\", lc=serviceC]} // local span\n{traceid=1, parent=6, spanid=7 [cs=serviceC, cr=serviceC, sa=database]} // client span\nThen for instance you could search for spans involving 'kafka'. WDYT?. >we still need to set mr's endpoint to the local\nservice receiving from kafka.\nOK, I see the weak point of my proposal :). I suppose 'ms' then gets set to the sender, then ?\n\nOne way is to add \"ma\" which is similar to client address and server\naddress, except it represents the broker (using m for congruence with the\ntimestamp annotations).\n\nGreat - I think it would be nice to emphasize that those spans are traversing the broker (e.g. Kafka) as opposed to being either a sending or a receiving service span. . ",
    "wu-sheng": "Do you mean, if a traced operation need a queue, it should be considered as two traces? and link to each other by trace-id? \nAnother option is considering a queue is something like rpc, generate a client-span and a server-span.\nWhat's you prefer?\n. Do you mean, if a traced operation need a queue, it should be considered as two traces? and link to each other by trace-id? \nAnother option is considering a queue is something like rpc, generate a client-span and a server-span.\nWhat's you prefer?\n. Glad to help. :). +1 for sure.\nI am so excited and glad to see a friendly and wide use project to join Apache community. Even I am not a member of openzipkin/core. I want to say, I want to offer my help as an initial committer and PPMC member during your Apache incubating or further ways in Apache. \nAt least, I can help on License and release processes. After SkyWalking graduated, I think I even can do more.. > @wu-sheng honestly you have acted like a part of Zipkin for a long time. We'd love your help.\n@adriancole When you begin to write the proposal, let me know.  WIKI of this repo? Or somewhere else?. Good to know. . > if we put the libraries in the openzipkin namespace in here too, I'd suggest doing a triage of the ones in there now. Maybe not all of them benefit at this time and might need to be demoted to contrib.\nOpenZipkin org could take responsibilities of hosting the related repos, which are willing to donate those to Apache in the future. . Zipkin proposal page has been added. https://wiki.apache.org/incubator/ZipkinProposal When we are ready, I can move the proposal to Apache WIKI. If I remember right, that WIKI has a different format, sadly, have to do several manual works.. > Note: The ASF requires full trademark rights to the project brand before it can graduate. The community would need to figure out during incubation (you have time to do this) if the Apache project would take on a new name or not.  The issue is that you couldn't have a podling releasing 'Zipkin' software while others in the ecosystem are also releasing other parts of 'Zipkin' software. I can help answer some questions, or otherwise the privately-archived trademarks@ mailing list can help figure out what that means\nThis may cause some side effect to brave project, I think?  Zipkin server and zipkin brave are both under io.zipkin, right?. The proposal has been moved from etherpad.net to Apache WIKI. Anyone has Apache WIKI edit access right could change, or ping me to change.\nHere is the address: https://wiki.apache.org/incubator/ZipkinProposal. > Under the \"Source and Intellectual Property Submission Plan\" section I would add a sentence along the lines of:\nAll source code is copyrighted to 'The OpenZipkin Authors', to which the existing core community has the rights to re-assign to the ASF.\n@michaelsembwever Done. Since GitHub org team member can't be seen from outside, @adriancole known that too, I add this sentence:\nAll source code is copyrighted to 'The OpenZipkin Authors', to which the existing core community(members list in Initial Committers) has the rights to re-assign to the ASF.. IMO, I prefer the http.host on the client side, which has more meanings. The client-side host is real and makes sense for developer or ops people. \nOn the other hand, I am not sure the meaning of server-side host.. :) I will review this.. :) I will review this.. Sure. Travel around in the whole weekend. :). Span\u540d\u79f0. \u6839\u636eAnnotation\u67e5\u8be2. \u6309\u767e\u5206\u6bd4\u964d\u5e8f\u6392\u5217. \u6309\u767e\u5206\u6bd4\u5347\u5e8f\u6392\u5217. \u901a\u8fc7Span\u540d\u79f0\u67e5\u627e. Span\u540d\u79f0\u901a\u5e38\u662fRPC\u65b9\u6cd5\u540d\u79f0\u6216Rails\u7aef\u70b9\u3002\n@adriancole What is the original word? Rails? . Span \u540d\u79f0\u3002Span\u662f\u4e0d\u9700\u8981\u7ffb\u8bd1\u7684\u3002. ",
    "ndrwrbgs": "@adriancole there are a lot of threads about these async span concepts. Is there one you\u2019re tracking in particular or where we can read the lastest on the plans in the space?. Apologies, my question was very poorly structured :-) My interest is mostly on attribution of work concerns involving fork/join and other such constructs like very delayed messages (do we just use 1 trace for that or relate another) that aren't well supported by the common tree based modeling of traces today. I think I am in the right issue, but used the wrong words.. ",
    "hvandenb": "The bosh release looks interesting. I've been able to deploy it as a normal java app without any issue. Using Sleuth with my apps. However, I like the simplicity of being able to doing a cf bind or adding Zipin as a service dependency in the manifest. That way apps don't have to know about all the HTTPS and/or streaming to Zipkin server. \n. The bosh release looks interesting. I've been able to deploy it as a normal java app without any issue. Using Sleuth with my apps. However, I like the simplicity of being able to doing a cf bind or adding Zipin as a service dependency in the manifest. That way apps don't have to know about all the HTTPS and/or streaming to Zipkin server. \n. Like the Hazelcast guide with two options\n- deploy normal and define user service\n- bosh managed with marketplace\nI remember seeing a tool at Springone that simplifies the marketplace / service broker part will have to look back at the conf materials.\n. Like the Hazelcast guide with two options\n- deploy normal and define user service\n- bosh managed with marketplace\nI remember seeing a tool at Springone that simplifies the marketplace / service broker part will have to look back at the conf materials.\n. On @eirslett comment I like what Spring Cloud Dataflow has done where they use a DPI (think Java spi) for deployments that makes it agnostic of runtime platform. So you can deploy on cloud foundry, kubernetics, docker etc. in consistent manner.\n. On @eirslett comment I like what Spring Cloud Dataflow has done where they use a DPI (think Java spi) for deployments that makes it agnostic of runtime platform. So you can deploy on cloud foundry, kubernetics, docker etc. in consistent manner.\n. There is also a nice project to do the title - marketplace generation http://cf-platform-eng.github.io/isv-portal/ Time to start pulling something together.\n. There is also a nice project to do the title - marketplace generation http://cf-platform-eng.github.io/isv-portal/ Time to start pulling something together.\n. @shalako great work for pushing this I really look forward to seeing this.\n. @shalako great work for pushing this I really look forward to seeing this.\n. Anyone work on this integration and made it available? There is an example project that shows how this was done with Hystrix. https://github.com/OskarKjellin/vizceral-hystrix\n. ",
    "shalako": "Release notes for cf-release v243 noted a bump to routing-release v138\nhttps://github.com/cloudfoundry/cf-release/releases/tag/v243\nIn release notes for routing-release 138 we note initial support for the\nnew zipkin config property\nhttps://github.com/cloudfoundry-incubator/routing-release/releases/tag/0.138.0\nRelease notes for v245 noted a bump to routing-release v140 (so includes\nchanges in routing-release 139 also)\nIn release notes for routing-release 139 we note added enhancements to\nZipkin support\nhttps://github.com/cloudfoundry-incubator/routing-release/releases/tag/0.139.0\nShannon Coen\nProduct Manager, Cloud Foundry\nPivotal, Inc.\nOn Thu, Oct 13, 2016 at 6:12 PM, Adrian Cole notifications@github.com\nwrote:\n\n@shalako https://github.com/shalako I noticed that 245 is out, but\nthere's no mention of this feature in the main or router release notes\nhttps://github.com/cloudfoundry/cf-release/releases/tag/v245 can you ping\nus when the release notes are updated? cc @tushar-dadlani\nhttps://github.com/tushar-dadlani\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1251#issuecomment-253683522,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAJjmM1gFCXtux2kMaERD6aVHlVfY-y1ks5qztcFgaJpZM4JqfUV\n.\n. Release notes for cf-release v243 noted a bump to routing-release v138\nhttps://github.com/cloudfoundry/cf-release/releases/tag/v243\n\nIn release notes for routing-release 138 we note initial support for the\nnew zipkin config property\nhttps://github.com/cloudfoundry-incubator/routing-release/releases/tag/0.138.0\nRelease notes for v245 noted a bump to routing-release v140 (so includes\nchanges in routing-release 139 also)\nIn release notes for routing-release 139 we note added enhancements to\nZipkin support\nhttps://github.com/cloudfoundry-incubator/routing-release/releases/tag/0.139.0\nShannon Coen\nProduct Manager, Cloud Foundry\nPivotal, Inc.\nOn Thu, Oct 13, 2016 at 6:12 PM, Adrian Cole notifications@github.com\nwrote:\n\n@shalako https://github.com/shalako I noticed that 245 is out, but\nthere's no mention of this feature in the main or router release notes\nhttps://github.com/cloudfoundry/cf-release/releases/tag/v245 can you ping\nus when the release notes are updated? cc @tushar-dadlani\nhttps://github.com/tushar-dadlani\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1251#issuecomment-253683522,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAJjmM1gFCXtux2kMaERD6aVHlVfY-y1ks5qztcFgaJpZM4JqfUV\n.\n. In Cloud Foundry, we already use a 128-bit trace id, in uuid format. To add support for Zipkin, we're having to add a second trace id because zipkin only supports a 64-bit id. It would be preferable to only have one trace id.\n. \n",
    "poblin-orange": "@alwaysastudent correct. What type of other messages are you thinking off ?. ",
    "phoebetse": "thanks @adriancole ! wow i think that's it! didn't know about the byteserializer. i'm having some other issues with my spark job writing to elasticsearch now, but it looks like that's not related to this. thank you so much!!!\n. thanks @adriancole ! wow i think that's it! didn't know about the byteserializer. i'm having some other issues with my spark job writing to elasticsearch now, but it looks like that's not related to this. thank you so much!!!\n. ",
    "fedj": "Glad to see I'm not the only one interested in this one !\nDefining it as a separate application would go in the way of the second solution with notifications.\nIt could be either embedded in the dependency job or a completely new app triggered by the end of the dependency job.\n. I didn't have time to finish it. Can you please wait until I polish it to merge it ?. > CS-CR (non-instrumented server)\n\nCS-SR-SS-CR with same end point (loopback call)\nSR-SS (root span)\nCS (async span without response with non-instrumented server)\n\nNo skew since we are on the same machine\n\nCS-SR (async span without response ?)\nCS-SR-SS-CR with different end points (RPC-span, with possibility of skew)\nCS-CR with SR-SS child (the case we're trying to solve here)\n\nPossible clock skew since clocks will most probably be desynchronized\nWe now have two choices:\n\nBuild the tree with span correction (CS-CR with SR-SS child would be flatten into a classic RPC span)\n    Pros: Corrected for UI too\n    Cons: possibility to introduce bugs while trying to correct things\nAdapt skew algorithm to handle this case\n    Pros: Less implicit\n    Cons: Could introduce bugs + UI would have to handle it too\n\nI tend to prefer the first solution.\n. After a discussion with @adriancole, it seems that flattening the tree can loose datas (e.g. which span id should we use when flattening data ?).\nThe solution would be to adapt the clock skew algorithm (or at least how we look at the tree).\nWe also need to adapt it in the dependency linker.. Hi @aliostad,\nThanks for the context and the history behind this project. I figured I\u2019d answer a few things mentioned.\nAs a part of moving to openzipkin, we'd expect to change the package names to Zipkin. No worries about that.\nFrom an environment POV, we run both on OSX and Windows 7. We did setup a bash script for travis, and would be happy to accept a powershell script PR if you find it helpful.\nWe compile both .Net standard and v4.5 but haven't published nuget v4.5 yet. As a part of publishing openzipkin packages, we'll start with .Net standard and if users ask for it, certainly can find a solution for 4.5\nWe hope that in moving to openzipkin, folks can share what they need in the issues list or even as a pull request. Either way is far better than guessing.. LGTM. Done for all but for the mustache test. I'm not sure to see how to do it properly here.. It would need to pass the span though and from what I see in the code, this is not what was intended (we extract everything needed from the span before). Done. Good catch, done. OPT: we could update this comment. Since we're dropping datas, shouldn't we log that there's something wrong ? IMO, dropping data is a good way to deal with this problem but people need to know that their datas have a problem. OPT: By extracting the traceId in a constant, it would emphasize that the trace is supposed to be considered the same. ",
    "nklmish": "@adriancole We can also add(zipkin-autoconfigure, zipkin-collector, zipkin-guava, zipkin-storage) and update(only for zipkin module not the parent) java.version in pom like this:\n<main.java.version>1.6</main.java.version> -> <main.java.version>1.8</main.java.version>\nI tried and intellij was happy but not sure whether zipkin will be happy ?\n. @adriancole We can also add(zipkin-autoconfigure, zipkin-collector, zipkin-guava, zipkin-storage) and update(only for zipkin module not the parent) java.version in pom like this:\n<main.java.version>1.6</main.java.version> -> <main.java.version>1.8</main.java.version>\nI tried and intellij was happy but not sure whether zipkin will be happy ?\n. ",
    "schlosna": "+1 from me as we have had to go from 128-bit to 64-bit trace IDs when pushing internally traced things into Zipkin\n. This might be a separate request, but one item I haven't had a chance to look into would be exposing something along the lines of a span predicate filter that would allow dropping spans based on specific logic -- say drop spans with duration less than 1 millisecond, or in noop case drop everything.\n. ",
    "mjbryant": "@adriancole we're in the process of doing this right now, and are probably a few days away from open-sourcing it. We essentially just pulled all the non-pyramid stuff out of pyramid-zipkin. \n. I am also interested in an Apache module to properly start a Zipkin trace. We currently generate Apache spans quite hackily by having Apache log certain Zipkin headers and generating the spans entirely out-of-band. It is very difficult to maintain, though probably has lower in-band overhead.. ",
    "lookfwd": "Hello folks, there's been some further progress on the openzipkin/zipkin-python-opentracing#1 . It's able to talk to ot-zipkin-js and zipkin and do some basic tracing. It's multi-threaded-happy and it has some tests. Some problems it has is that it logs events wrongly as binary annotations instead of real events and general alpha-ness. Will come back with more, but if any of you wants to have a play - please feel free... comments and screenshots of bugs greatly appreciated.. ",
    "tristanpenman": "You might have set a record for fastest issue response there. Thanks for looking into it!\nSo it sounds like the right thing for me to do here would be to raise issues (or submit PRs) for the projects that I'm using, since string values are clearly the direction forward.\n. It might help if I paste the code block and path.\n/zipkin/zipkin/src/main/java/zipkin/internal/Buffer.java:\nBuffer writeAscii(long v) {\n    if (v == 0) return writeByte('0');\n    checkArgument(v > 0, \"Expected %s to be unsigned\", v);\n    int width = asciiSizeInBytes(v);\n    int pos = this.pos += width; // We write backwards from right to left.\n    while (v != 0) {\n      int digit = (int) (v % 10);\n      buf[--pos] = HEX_DIGITS[digit];\n      v /= 10;\n    }\n    return this;\n  }\n. No problem at all. This response has already exceeded all of my expectations! \ud83d\udc4d \n. I'm pleased to say that your fix in commit 3b8d56ac1b817729517e32c7a3d311b7ceaa2fb7 has resolved the issue.\nThanks again for your help!\n. This is definitely an improvement. I can now access the traces via the UI.\nNow there is one issue that I'm observing, but I'm not yet sure if it something about the demo code (which I modified slightly) but the annotated values that I see in the UI seem to have different trailing digits to those reported by the demo.\ne.g.\n179750381162269401 (generated long) -> 179750381162269400 (UI)\n-7437073205494488636 (generated long) -> -7437073205494489000 (UI)\nSo it looks like the values might be getting truncated, and then filled in with random digits, somewhere.\n. Thanks for the tip about the JSON button. That view shows the same number that is reported by the UI. So assuming it is the raw trace data received from the demo, your explanation makes perfect sense.\n. ",
    "bamb00": "Hi,\nHow is the zipkin3_udts keyspace gets created? from java/python code.  I want to automate creating a keyspace in cassandra for zipkin opentracing when the zipkin is deployed.  \nThanks.. Is there another way I can create a keyspace for zipkin when the the openkin/zipkin pod is deployed?\nThanks for responding.. ",
    "bkleef": "@adriancole this is still the case with version 1.23.3. As you can see we only set binaryAnnotations possibly because we use https://github.com/openzipkin/zipkin-go-opentracing.\nOn browser console we get a JS error:\nEXCEPTION: this.root.annotations is undefined\nResponse:\njson\n[\n   [\n      {\n         \"traceId\":\"414e44fb181df47e6ec3273cd5e68c13\",\n         \"id\":\"48f69e22bceb3828\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"timestamp\":1493817271284756,\n         \"duration\":10181,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.api.v1.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.api.v1.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      },\n      {\n         \"traceId\":\"414e44fb181df47e6ec3273cd5e68c13\",\n         \"id\":\"7b6b5f79a727456e\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"parentId\":\"48f69e22bceb3828\",\n         \"timestamp\":1493817271292233,\n         \"duration\":1703,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.srv.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.srv.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      }\n   ],\n   [\n      {\n         \"traceId\":\"6022cde94bff790803ccfec040a020d6\",\n         \"id\":\"62f37b8ddfb2c9d0\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"timestamp\":1493817222041547,\n         \"duration\":12139,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.api.v1.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.api.v1.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      },\n      {\n         \"traceId\":\"6022cde94bff790803ccfec040a020d6\",\n         \"id\":\"735f5fdd1cf20628\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"parentId\":\"62f37b8ddfb2c9d0\",\n         \"timestamp\":1493817222050620,\n         \"duration\":2124,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.srv.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.srv.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      }\n   ],\n   [\n      {\n         \"traceId\":\"70cdb97b817d2d5069da6015f1b58242\",\n         \"id\":\"4824ed25b2ce7b68\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"timestamp\":1493817220184318,\n         \"duration\":10554,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.api.v1.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.api.v1.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      },\n      {\n         \"traceId\":\"70cdb97b817d2d5069da6015f1b58242\",\n         \"id\":\"427c20a173159005\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"parentId\":\"4824ed25b2ce7b68\",\n         \"timestamp\":1493817220191750,\n         \"duration\":2071,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.srv.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.srv.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      }\n   ],\n   [\n      {\n         \"traceId\":\"4633b79f27358e0a4a668a87dd6db8f6\",\n         \"id\":\"5d0e401ceb6199b1\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"timestamp\":1493816987763840,\n         \"duration\":17246,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.api.v1.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.api.v1.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      },\n      {\n         \"traceId\":\"4633b79f27358e0a4a668a87dd6db8f6\",\n         \"id\":\"099b6f99a3d302fc\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"parentId\":\"5d0e401ceb6199b1\",\n         \"timestamp\":1493816987773581,\n         \"duration\":6534,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.srv.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.srv.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      }\n   ],\n   [\n      {\n         \"traceId\":\"77bd870f0f9937eb1921da490b61a621\",\n         \"id\":\"42c74cfe6ff12b92\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"timestamp\":1493816987572696,\n         \"duration\":11595,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.api.v1.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.api.v1.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      },\n      {\n         \"traceId\":\"77bd870f0f9937eb1921da490b61a621\",\n         \"id\":\"34de313496a58702\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"parentId\":\"42c74cfe6ff12b92\",\n         \"timestamp\":1493816987580616,\n         \"duration\":2552,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.srv.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.srv.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      }\n   ],\n   [\n      {\n         \"traceId\":\"12c36c4ed14fd71520afcc13d5d4afc3\",\n         \"id\":\"5351db552b8ef348\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"timestamp\":1493816984668362,\n         \"duration\":17917,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.api.v1.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.api.v1.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      },\n      {\n         \"traceId\":\"12c36c4ed14fd71520afcc13d5d4afc3\",\n         \"id\":\"4fa53a43d7320099\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"parentId\":\"5351db552b8ef348\",\n         \"timestamp\":1493816984681105,\n         \"duration\":3786,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.srv.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.srv.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      }\n   ]\n]. @adriancole this is still the case with version 1.23.3. As you can see we only set binaryAnnotations possibly because we use https://github.com/openzipkin/zipkin-go-opentracing.\nOn browser console we get a JS error:\nEXCEPTION: this.root.annotations is undefined\nResponse:\njson\n[\n   [\n      {\n         \"traceId\":\"414e44fb181df47e6ec3273cd5e68c13\",\n         \"id\":\"48f69e22bceb3828\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"timestamp\":1493817271284756,\n         \"duration\":10181,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.api.v1.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.api.v1.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      },\n      {\n         \"traceId\":\"414e44fb181df47e6ec3273cd5e68c13\",\n         \"id\":\"7b6b5f79a727456e\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"parentId\":\"48f69e22bceb3828\",\n         \"timestamp\":1493817271292233,\n         \"duration\":1703,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.srv.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.srv.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      }\n   ],\n   [\n      {\n         \"traceId\":\"6022cde94bff790803ccfec040a020d6\",\n         \"id\":\"62f37b8ddfb2c9d0\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"timestamp\":1493817222041547,\n         \"duration\":12139,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.api.v1.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.api.v1.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      },\n      {\n         \"traceId\":\"6022cde94bff790803ccfec040a020d6\",\n         \"id\":\"735f5fdd1cf20628\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"parentId\":\"62f37b8ddfb2c9d0\",\n         \"timestamp\":1493817222050620,\n         \"duration\":2124,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.srv.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.srv.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      }\n   ],\n   [\n      {\n         \"traceId\":\"70cdb97b817d2d5069da6015f1b58242\",\n         \"id\":\"4824ed25b2ce7b68\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"timestamp\":1493817220184318,\n         \"duration\":10554,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.api.v1.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.api.v1.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      },\n      {\n         \"traceId\":\"70cdb97b817d2d5069da6015f1b58242\",\n         \"id\":\"427c20a173159005\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"parentId\":\"4824ed25b2ce7b68\",\n         \"timestamp\":1493817220191750,\n         \"duration\":2071,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.srv.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.srv.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      }\n   ],\n   [\n      {\n         \"traceId\":\"4633b79f27358e0a4a668a87dd6db8f6\",\n         \"id\":\"5d0e401ceb6199b1\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"timestamp\":1493816987763840,\n         \"duration\":17246,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.api.v1.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.api.v1.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      },\n      {\n         \"traceId\":\"4633b79f27358e0a4a668a87dd6db8f6\",\n         \"id\":\"099b6f99a3d302fc\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"parentId\":\"5d0e401ceb6199b1\",\n         \"timestamp\":1493816987773581,\n         \"duration\":6534,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.srv.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.srv.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      }\n   ],\n   [\n      {\n         \"traceId\":\"77bd870f0f9937eb1921da490b61a621\",\n         \"id\":\"42c74cfe6ff12b92\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"timestamp\":1493816987572696,\n         \"duration\":11595,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.api.v1.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.api.v1.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      },\n      {\n         \"traceId\":\"77bd870f0f9937eb1921da490b61a621\",\n         \"id\":\"34de313496a58702\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"parentId\":\"42c74cfe6ff12b92\",\n         \"timestamp\":1493816987580616,\n         \"duration\":2552,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.srv.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.srv.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      }\n   ],\n   [\n      {\n         \"traceId\":\"12c36c4ed14fd71520afcc13d5d4afc3\",\n         \"id\":\"5351db552b8ef348\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"timestamp\":1493816984668362,\n         \"duration\":17917,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.api.v1.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.api.v1.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      },\n      {\n         \"traceId\":\"12c36c4ed14fd71520afcc13d5d4afc3\",\n         \"id\":\"4fa53a43d7320099\",\n         \"name\":\"com.example.srv.time.transport.getlist\",\n         \"parentId\":\"5351db552b8ef348\",\n         \"timestamp\":1493816984681105,\n         \"duration\":3786,\n         \"binaryAnnotations\":[\n            {\n               \"key\":\"lc\",\n               \"value\":\"com.example.srv.time\",\n               \"endpoint\":{\n                  \"serviceName\":\"com.example.srv.time\",\n                  \"ipv4\":\"127.0.0.1\"\n               }\n            }\n         ],\n         \"debug\":true\n      }\n   ]\n]. @basvanbeek thanks a lot for your feedback. We will look into it.\nIMHO it's still a bad thing that the new Zipkin UI crashes (only services are displayed in \"Service\" dropdown but no traces are shown while they are visualized in old UI) when annotations is missing in JSON response. Especially because old UI can handle this.\n. @basvanbeek thanks a lot for your feedback. We will look into it.\nIMHO it's still a bad thing that the new Zipkin UI crashes (only services are displayed in \"Service\" dropdown but no traces are shown while they are visualized in old UI) when annotations is missing in JSON response. Especially because old UI can handle this.\n. ",
    "joel-airspring": "I've created a first pass on this feature.  Any feedback on what I did?\nI did it in two commits since there were a few plumbing changes needed to make the feature possible.  The two commits can be combined, though, if that is preferable.. The latter.\nIf we hit it big, we're going to buy the mattress company though.  Ha ha!. Just made a change to speed up the purge.  With commit e0b6af5, a purge of old spans with 1 million spans in-memory goes from ~150 ms to 5 ms.. I fixed the issue with Long.max() not being available in Java 1.7, and fixed a warning about a missing \"@Override\".. Adrian, thanks for all the detailed feedback!  I'll go through it today and check out #1636 too.\nJoel. @adriancole Here's a second pass.  I think I've incorporated all the feedback.\n\nIt's based on your latest refactor.\nAll metrics/bookkeeping stuff is gone.\nI used your evictToRecoverSpans() and generally changed all terminology from 'purge' to 'evict'.  Much better nomenclature!\nIt does have the same performance profile as my original checkin.  Before it starts evicting, adding new spans takes 2-5 ms, but after it starts evicting, it takes 200 - 400 ms (@ 1M spans).  I'd like to improve that.\nI added a size() method to SortedMultimap, but it doesn't handle properly the case of using an iterator to remove items.  The current code doesn't depend on size() in these cases, but it's kind of ugly, and I'd like to fix that.. Rebased.. One other comment:  I made the documentation describing the in-memory data layout a separate commit so it can be axed if you want.  It's helpful to to me to see things visually like that, but you may not love it since it's not super-precise. . > it might be easiest to reason with synchronization as coarse grained. ex\nsynchronize every method, then as performance requires, tidy the locks. I\nknow I started the suggestion towards finer policy, but realized I missed a\nfew spots :)\n\nDoes that mean to add 'synchronized' to every mutating method?  Those would be put() and remove().  (And, of course, take out any fine-grained synchronize blocks within the methods.)\n\nFor ex copy-out in the methods which returns collections, which ensures\nread ops don't get hit when eviction is in progress. Then, just need to\nmore carefully check null on gets resulting from an iteration of those\ncollections (ex maybe there's a race condition when eviction is in\nprogress, where you iterate a key that was there when you read the list,\nbut not when you went to get the spans associated with it).\n\nI'm not quite sure what you mean by 'copy-out'.  Do you mean that every time we do a get(), we should make a copy of the data and return that?  I guess that means we either couldn't mutate that copy or if we did, it would have to be merged back into the real data, which sounds painful.\nPerhaps it would simplify things if we only made changes directly via the API of SortedMultimap (and not by getting a linked list, for example, and deleting its contents directly.  Maybe add a synchronized removeIf(key, ) method to SortedMultimap?  And use it like:\nserviceToTraceIdTimeStamp.removeIf(\"foo\", \"id:aaaaa\");  --> Go through the list for service name \"foo\" and remove any entries that have an traceId of aaaaa.\nAm I headed in the right direction here?. Third pass:\n\nAll methods on InMemorySpanStore synchronized.  This is the simplest possible way to ensure that every read sees every write.\nWith 4 clients writing simultaneously, it's 2-10ms for each span submission (~30 at a time).  Once the evicting starts @1M spans, it drops to 250-2000 ms.\nI added a subclass of SortedMultimap for the serviceToTraceIdTimeStamp type with a removeServiceIfTraceId() method.\nWith 4 clients writing simultaneously, it's 2-10ms for each span submission (~30 at a time).  Once the evicting starts @1M spans, it drops to 400-4000 ms.  (I don't know why this is slower than case 1, above)\nIn another commit, I enhanced removeServiceIfTraceId() to terminate early based on latest time stamp.  This cut the time when evicting to double-digit ms when evicting starts @1M spans.\n\nIs there anything else you'd like for this PR?  It seems to be fast enough with the coarse locking with 4 clients sending data simultaneously while evicting @ 1 million spans.. Thanks.  I only noticed it because I hit a problem with that data structure when trying to evict spans!. Bummer!  I had run the zipkin/zipkin tests, but not those zipkin/zipkin-junit tests that failed.  Thanks for checking.  I'll keep rolling with the existing linked list implementation.. I tested with commit eabaebe, ran it up to 1 M datapoints.  Looks good to me!\nI like this cleanup.  It alway feels good to get rid of a loop!\nJoel. ",
    "tramchamploo": "After setting the 'ca' annotation, client entrypoints are not listed as well.\n. Working on integration between vizceral and zipkin. I've added a new api like /vizceral which simply query all traces in the last few seconds and use DependencyLinker to link them. Tried to set the limit to Integer.MAX_VALUE in order to retrieve traces as much as possible to get a full dependency graph. But ES won't allow that, throwing \norg.elasticsearch.common.io.stream.NotSerializableExceptionWrapper: too_many_clauses: maxClauseCount is set to 1024\n    at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137) ~[lucene-core-5.5.2.jar!/:5.5.2 8e5d40b22a3968df065dfc078ef81cbb031f0e4a - sarowe - 2016-06-21 11:38:23]\n    at org.elasticsearch.index.query.TermsQueryParser.parse(TermsQueryParser.java:200) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at org.elasticsearch.search.SearchService.createContext(SearchService.java:667) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:293) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-2.4.1.jar!/:2.4.1]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.8.0_91]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.8.0_91]\n    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]\nAnyone have any ideas?. I still think we should persist service and span names in storage layer. I doubt if there is really much we can do to optimize ES.. I'm trying to implement this, and defaultLookback goes a long way through ZipkinElasticsearchHttpStorageProperties -> ZipkinElasticsearchHttpStorageAutoConfiguration -> ElasticsearchHttpStorage -> ElasticsearchHttpSpanStore.\nAlso in ZipkinElasticsearchHttpStorageProperties  there is \n@Autowired\n  @Value(\"${zipkin.query.lookback:86400000}\")\n  private int defaultLookback = 86400000; // 1 day in millis\nany idea a better way to implement this?. I'm trying to implement this, and defaultLookback goes a long way through ZipkinElasticsearchHttpStorageProperties -> ZipkinElasticsearchHttpStorageAutoConfiguration -> ElasticsearchHttpStorage -> ElasticsearchHttpSpanStore.\nAlso in ZipkinElasticsearchHttpStorageProperties  there is \n@Autowired\n  @Value(\"${zipkin.query.lookback:86400000}\")\n  private int defaultLookback = 86400000; // 1 day in millis\nany idea a better way to implement this?. Just merge limit-es-names, seems the speed does not much improve. 100GB data every day with 12 hosts cluster, 64GB memory per host.. Just merge limit-es-names, seems the speed does not much improve. 100GB data every day with 12 hosts cluster, 64GB memory per host.. I save data for just one day, a service name query takes about more than 5 secs. After limitation I see a about 1s improvement or just nothing. Timeout sometimes happens.. I save data for just one day, a service name query takes about more than 5 secs. After limitation I see a about 1s improvement or just nothing. Timeout sometimes happens.. +1 for a new index, but is there any payload that is from existence check when persisting service/span names?\nAlso would change shape of spans be a long-term consideration that will be helpful for overall performance?. Great, this improvement on service/span name query will greatly improve user experience.. Seems the DelimiterAnalyzer.java works out fine.\nAnd I find this jaeger's solution here. Schema\nUsing another table for tag index: \nCREATE TABLE IF NOT EXISTS ${keyspace}.tag_index (\n    service_name    text,\n    tag_key         text,\n    tag_value       text,\n    start_time      bigint,\n    trace_id        blob,\n    span_id         bigint,\n    PRIMARY KEY ((service_name, tag_key, tag_value), start_time, trace_id, span_id)\n)\nCan anyone tell the  difference of performance between these two?. ",
    "pavolloffay": "It is from app instrumented with https://github.com/smoketurner/dropwizard-zipkin\n. So the problem is in https://github.com/openzipkin/zipkin-tracer not in brave library. It happens when the app is deployed in docker container.\n. I have posted a possible solution here: https://github.com/travis-ci/travis-ci/issues/7254#issuecomment-387689631. copying here as this issue is the first in google search. ",
    "sachinpkale": "I ran Zipkin with --logging.level.com.datastax.driver.core.QueryLogger=trace and insert queries for service_name_index and service_span_name_index did not appear. \nThe instrumentation library I am using is old one and does not send span.timestamp and duration. Could that be the reason?\n. I have thrift serialized data that clj-zipkin is sending to scribe.\n#ns525700959.Span{:trace_id 10339031, :name Renderer, :id 9887479, :parent_id 3778525, :annotations (#ns_1186963580.Annotation{:timestamp 1475082174174000, :value end:Renderer, :host #ns1048748842.Endpoint{:ipv4 3232243969, :port 0, :service_name RendererService}, :duration 0} #ns_1186963580.Annotation{:timestamp 1475082174071000, :value start:Renderer, :host #ns1048748842.Endpoint{:ipv4 3232243969, :port 0, :service_name RendererService}, :duration 0}), :binary_annotations [], :debug 0}\nI tried to get JSON by running Zipkin jar in DEBUG mode but could not collect it. Any way to capture it?\n. @adriancole Got the json by following your comment in Gitter channel.\n[\n  {\n    \"traceId\": \"0000000000043ece\",\n    \"id\": \"00000000005245a2\",\n    \"name\": \":get:/\",\n    \"timestamp\": 1475082087593000,\n    \"duration\": 919000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1475082087593000,\n        \"value\": \"start::get:/\",\n        \"endpoint\": {\n          \"serviceName\": \"testservice\"\n        }\n      },\n      {\n        \"timestamp\": 1475082088512000,\n        \"value\": \"end::get:/\",\n        \"endpoint\": {\n          \"serviceName\": \"testservice\"\n        }\n      }\n    ],\n    \"debug\": true\n  },\n  {\n    \"traceId\": \"0000000000043ece\",\n    \"id\": \"0000000000db87ad\",\n    \"name\": \"renderer\",\n    \"parentId\": \"00000000005245a2\",\n    \"timestamp\": 1475082088097000,\n    \"duration\": 105000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1475082088097000,\n        \"value\": \"start:Renderer\",\n        \"endpoint\": {\n          \"serviceName\": \"rendererservice\"\n        }\n      },\n      {\n        \"timestamp\": 1475082088202000,\n        \"value\": \"end:Renderer\",\n        \"endpoint\": {\n          \"serviceName\": \"rendererservice\"\n        }\n      }\n    ],\n    \"debug\": true\n  }\n] \n. ip32 overflows beyond 127.255.255.255. I was testing on my machine with intranet IP address 192.168.XXX.XXX. That's why I changed it to ip64.\n. Thanks for these changes. Much useful. I made few corrections to it and added it to separate branch. Spans are being stored in Cassandra successfully.\n. ",
    "MHarris021": "+1, change the paths to be relative to the generated index.html please\nFound out that specifying the output.publicPath = \"/\" in webpack.config.js creates the absolute href links in the generated index.html. If this were made to be \"\" instead then it should be relative, the only other change I can see that would then need to be made is the ajax request in config.js file. Making the request be \"./config.json\" instead of \"/config.json\" or possibly configurable should resolve this Feature Request.\n. ",
    "knikitin": "I added the prefix for urls in UI, for using zipkin UI, when it located after the proxy. fork. Sorry. I made a mistake with the creation of the pull request. I made a mistake with the choice of destination.\nIf you do not mind, I will close this pull request.. ",
    "dgafka": "Hello,\nI was about to take a challenge of implementing OpenTracing in PHP and later add adapter for zipkin.\nBut currently, I've really limited amount of free time. So as much as I would love to have such solution in PHP, I am not able to develop it in the nearest time. \n. ",
    "ralphiech": "Hey guys, I am currently checking if I will be able to spend some more time to extend (add a scribe receiver) and probably mainly document (write a how to use / example readme) to the https://github.com/getyourguide/zipper library. We might also be able to register it with https://packagist.org/ for easy integration.\n@adriancole what are the main features you need a 'champion' to support? or in other words what are the above libraries missing, what would you like to see happening here?\n. ",
    "sroze": "I'm also really interested by a Zipkin tracer in PHP and already wrote one, private at the moment. An interesting amount of work have already been done on Tolrance and will add a Zipkin MessageProfile storage in the next couple of days!\n. I'm also really interested by a Zipkin tracer in PHP and already wrote one, private at the moment. An interesting amount of work have already been done on Tolrance and will add a Zipkin MessageProfile storage in the next couple of days!\n. @drefined as we can already do that with Tolerance (everything in the Tracer is framework-agnostic), would it be better to just improve the documentation and add extension points if needed?\n. @drefined as we can already do that with Tolerance (everything in the Tracer is framework-agnostic), would it be better to just improve the documentation and add extension points if needed?\n. @drefined I would like to split the components into read-only repositories, the same way Symfony is doing for instance. What would you like to see in this \"Zipkin-only\" library then? Only the models and the clients? If last question's answer is yes, what about splitting Tracer's component?\n. @drefined I would like to split the components into read-only repositories, the same way Symfony is doing for instance. What would you like to see in this \"Zipkin-only\" library then? Only the models and the clients? If last question's answer is yes, what about splitting Tracer's component?\n. @drefined https://github.com/Tolerance/Tolerance/tree/master/src/Tolerance/Tracer\n. @drefined https://github.com/Tolerance/Tolerance/tree/master/src/Tolerance/Tracer\n. I used to advocate the many-repository architecture but I'm not anymore, as the dependencies are mostly dev dependencies and the users can pick what they want without installing 10 different packages. Tho, the HttpFondation factory can probably be moved to the Symfony bridge and the Psr7 factory to a Psr7 bridge, I'll do that when adding the AMQP factory later today.\n. I used to advocate the many-repository architecture but I'm not anymore, as the dependencies are mostly dev dependencies and the users can pick what they want without installing 10 different packages. Tho, the HttpFondation factory can probably be moved to the Symfony bridge and the Psr7 factory to a Psr7 bridge, I'll do that when adding the AMQP factory later today.\n. ",
    "drefined": "@adriancole I'm trying to start a simple php library for zipkin, inspired from Tolerance and Hoopak. It doesn't have much now, but it can be used to send spans to Zipkin. \nhttps://github.com/drefined/php-zipkin\n. @adriancole I'm trying to start a simple php library for zipkin, inspired from Tolerance and Hoopak. It doesn't have much now, but it can be used to send spans to Zipkin. \nhttps://github.com/drefined/php-zipkin\n. @sroze, I like the idea of Tolerance, but ideally I would like to have a \"pure\" zipkin library. If you want to break out the zipkin piece from Tolerance and have that in a separate repo, I'd be interested in helping out with documentation and feature implementation. \n. @sroze, I like the idea of Tolerance, but ideally I would like to have a \"pure\" zipkin library. If you want to break out the zipkin piece from Tolerance and have that in a separate repo, I'd be interested in helping out with documentation and feature implementation. \n. @sroze, I currently have a single repo with everything related to zipkin for php, but I think it should be broken out into several different repos: model/transport/client as one, framework / library specific instrumentation (laravel, symfony, redis, pdo, http) as separate repos, etc. Also, what do you mean by splitting Tracer's component? \n. @sroze, I currently have a single repo with everything related to zipkin for php, but I think it should be broken out into several different repos: model/transport/client as one, framework / library specific instrumentation (laravel, symfony, redis, pdo, http) as separate repos, etc. Also, what do you mean by splitting Tracer's component? \n. @sroze, Ah, yes. I think the factory stuff could be broken out into its own repos. I don't think everyone would need that with the models/clients unless they are integrating it with Symfony or Laravel. I see the models/clients as the \"base\" then people could use that to build wrappers or integrations into other frameworks or libraries.\n. @sroze, Ah, yes. I think the factory stuff could be broken out into its own repos. I don't think everyone would need that with the models/clients unless they are integrating it with Symfony or Laravel. I see the models/clients as the \"base\" then people could use that to build wrappers or integrations into other frameworks or libraries.\n. ",
    "Logic-32": "@adriancole, it looks like I need to revive this case.  We are currently running zipkin 1.28.0 and using Elasticsearch as our storage component using HTTP all the way from Java/Brave down through zipkin to Elasticsearch (so no Kafka) and are having this issue.\nWe have a couple of heap dumps which seem to implicate OkHttp3 as the thing causing the leaks.  There are, specifically, a bunch of okhttp3.RealCall$AsyncCall's dangling about.\nPlease let me know what other information I can provide to help diagnose this because we have frequent OOM issues as a result of whatever is causing this; the two most recent ones correlating with issues on Elasticsearch's side.\nAlso, can you provide a script for what you did to attempt and reproduce the OOM issue?  I have an idea of how I'd like to try and get something reproducible over here and it'd be nice to see if my idea is the same as yours before wasting time duplicating the effort ;). Good news is, we're not using Kafka, so I suppose this is a bit off topic.  I'm happy to create a new case if so, or just comment on #1741.  However, that even seems a bit inappropriate since I'm not in a position to \"talk about what is possible, especially what is easy to support.\"\nAs it stands, I'm also not sure \"backpressure\" is entirely the issue we're having.  The second instance we had an OOM, for example, was because Elasticsearch ran out of disk space.  Something obviously easy to remedy, but in that case I wouldn't expect zipkin-server to lock up and die like it did.  Instead, imo, it should try to push what data it gets and drop whatever Elasticsearch says it can't store; assuming Elasticsearch has the ability to say it can't store it.  To some extent, I'm curious why the ES_HTTP_READ_TIMEOUT property doesn't resolve this issue.  I'm guessing it's because OkHttp3 may not be able to establish the connection to Elasticsearch and therefore isn't getting to a point where it's waiting on a read?\nWith all that said, aside from the obvious reasons, would you still recommend updating to v2 to test this issue or do you suppose we'll still have the same problem but with a bit more buffer until we blow the heap?  Note, we have locally made some changes to zipkin-server that would likely have the same effect on GC/heap usage.  We were planning on doing a pull request but v2 came out before we could get some solid tests in ;)\np.s. For what it's worth, we have instrumented zipkin-server with metrics we report to Graphite and our heap remains sub-1.5GB, averaging about 500MB, until ES dies.  At which point there is a rapid, borderline immediate spike to 3GB where it then churns on GC indefinitely.  The heap dumps I mentioned earlier were taken at this point.. I can definitely say we occasionally throw some large requests at this.  We actually had to up the 10k span limit in ThriftCodec to 20k because we were semi-regularly getting 13k spans per message (rouge service we've identified and quieted down a bit).  On average I'd say we're in the 10s per message but with the number of services we have we get hundreds of thousands of spans a second (IIRC).  So, in short, the messages probably aren't that big on average but there are a lot of them.\nI'd definitely be pro dropping early if you know the backend is down though I'm not in a position to try and implement something like that myself.  We can definitely try setting 'maxRequests=something' but, out of curiosity, where would we set that?  Spring-boot, OkHttp3, StorageComponent, environment variable, other...?\nNo worries about moving the ground out from beneath us!  From what I can tell the changes we have will move nicely to Zipkin 2 :)  As for us actually moving to it: it'll be awhile.  But I'll try to get some momentum in that direction and report back if progress is ever made and anything positive happens.. I can definitely say we occasionally throw some large requests at this.  We actually had to up the 10k span limit in ThriftCodec to 20k because we were semi-regularly getting 13k spans per message (rouge service we've identified and quieted down a bit).  On average I'd say we're in the 10s per message but with the number of services we have we get hundreds of thousands of spans a second (IIRC).  So, in short, the messages probably aren't that big on average but there are a lot of them.\nI'd definitely be pro dropping early if you know the backend is down though I'm not in a position to try and implement something like that myself.  We can definitely try setting 'maxRequests=something' but, out of curiosity, where would we set that?  Spring-boot, OkHttp3, StorageComponent, environment variable, other...?\nNo worries about moving the ground out from beneath us!  From what I can tell the changes we have will move nicely to Zipkin 2 :)  As for us actually moving to it: it'll be awhile.  But I'll try to get some momentum in that direction and report back if progress is ever made and anything positive happens.. We are using Brave with a URLConnectionSender to send the data.  I'm surprised that no tool could handle more than 10k spans in your workshop; we handle it with minimal problems on only one zipkin-server, no cluster.  The only problem is that the ES_READ_TIMEOUT is set to 60s and it sometimes takes more than 60s to write the data out.  So, the data does get written but zipkin erroneously reports them as dropped.  But our specific situation that got us into that state was having a 10s messageTimeout and a tight loop that hit the database; both of which have been remedied.\nQuick review of the documentation indicates that the max connections only affects the size of the pool they use; which probably means everything else would be queued and backlogged.\nAgain, not knowing OkHttp that well, I wonder if you can inspect the Dispatcher queue and cancel anything in the queue if you exceed a certain threshold?\nRef: https://gist.github.com/RyanRamchandar/64c5863838940ec67f03. We are using Brave with a URLConnectionSender to send the data.  I'm surprised that no tool could handle more than 10k spans in your workshop; we handle it with minimal problems on only one zipkin-server, no cluster.  The only problem is that the ES_READ_TIMEOUT is set to 60s and it sometimes takes more than 60s to write the data out.  So, the data does get written but zipkin erroneously reports them as dropped.  But our specific situation that got us into that state was having a 10s messageTimeout and a tight loop that hit the database; both of which have been remedied.\nQuick review of the documentation indicates that the max connections only affects the size of the pool they use; which probably means everything else would be queued and backlogged.\nAgain, not knowing OkHttp that well, I wonder if you can inspect the Dispatcher queue and cancel anything in the queue if you exceed a certain threshold?\nRef: https://gist.github.com/RyanRamchandar/64c5863838940ec67f03. Have you considered a slightly more optimized rendering technique?  I haven't profiled things myself but the places I'd start poking around are:\n1. Mustache templates; are they slow to compile/render at that scale?\n2. Lazy render; only add to the DOM what is visible and potentially start with things collapsed after a certain threshold.\nOur use case, btw, is that we have a fairly involved data gathering/generation system that sometimes has to do a deep dive through various web services to get all of it's data.  We try to identify repeated requests and consolidate them into bulk requests (in code, not zipkin) to alleviate this issue but we still have it on occasion.\nOne of our biggest improvements in this area was to break up traces so that the typical \"large traces\" now link to \"child traces\" so that you can see at the high level how long things took or navigate off to another trace to see detail for a particular section.. Have you considered a slightly more optimized rendering technique?  I haven't profiled things myself but the places I'd start poking around are:\n1. Mustache templates; are they slow to compile/render at that scale?\n2. Lazy render; only add to the DOM what is visible and potentially start with things collapsed after a certain threshold.\nOur use case, btw, is that we have a fairly involved data gathering/generation system that sometimes has to do a deep dive through various web services to get all of it's data.  We try to identify repeated requests and consolidate them into bulk requests (in code, not zipkin) to alleviate this issue but we still have it on occasion.\nOne of our biggest improvements in this area was to break up traces so that the typical \"large traces\" now link to \"child traces\" so that you can see at the high level how long things took or navigate off to another trace to see detail for a particular section.. @igorwwwwwwwwwwwwwwwwwwww, I have a trace with 9k spans in it that can probably be used for your purposes (with the aid of #1884).  Is there a \"private\" way I can send it to you?  Nothing super-confidential in it but enough I don't want it to be public.  Or did @adriancole's lua script fit your need?\nEdit: sent the trace :). @yschimke, looking at the EventListener interface, I'm not quite sure it'd be able to accomplish what we'd need by it self.  Specifically, this line from the documentation makes me a bit uneasy:\n\nAll event methods must execute fast, without external locking, cannot throw exceptions, attempt to mutate the event parameters, or be reentrant back into the client.\n\nThe inability for a listener to modify the client being the phrase to note there.  Maybe an EventListener in conjunction with a queue handling mechanism in front of OkHttp would work but that sounds like a lot of hoops to jump through and equally as many places for something to go wrong.\n@swankjesse, what would the benefit of putting a queue in front of OkHttp be (aside from what I mentioned above)?  Would querying the Dispatcher directly be inappropriate in some way or are you just suggesting that an extra abstraction might lead to a cleaner implementation?\n@adriancole, the StorageComponent code is a bit deep and I haven't managed to get through it all yet, particularly to the ES client, but for reference, there was another situation where we hit an OOM: when one of our ES nodes went down.  I wasn't able to identify and exact order of events that caused the issue but for some reason after the node (1 out of 3 of them) went offline we had a backup of requests and spiked to an OOM.  That indicates to me that I'm guessing the server was not failing nicely and triggering the backoff strategy you mentioned.  Again, not sure what other factors were at play but it's another scenario to consider at the very least.\nAs for the simplification: that sounds ideal to me as long as proper metrics are maintained with respect to number of spans dropped ;). @yschimke, looking at the EventListener interface, I'm not quite sure it'd be able to accomplish what we'd need by it self.  Specifically, this line from the documentation makes me a bit uneasy:\n\nAll event methods must execute fast, without external locking, cannot throw exceptions, attempt to mutate the event parameters, or be reentrant back into the client.\n\nThe inability for a listener to modify the client being the phrase to note there.  Maybe an EventListener in conjunction with a queue handling mechanism in front of OkHttp would work but that sounds like a lot of hoops to jump through and equally as many places for something to go wrong.\n@swankjesse, what would the benefit of putting a queue in front of OkHttp be (aside from what I mentioned above)?  Would querying the Dispatcher directly be inappropriate in some way or are you just suggesting that an extra abstraction might lead to a cleaner implementation?\n@adriancole, the StorageComponent code is a bit deep and I haven't managed to get through it all yet, particularly to the ES client, but for reference, there was another situation where we hit an OOM: when one of our ES nodes went down.  I wasn't able to identify and exact order of events that caused the issue but for some reason after the node (1 out of 3 of them) went offline we had a backup of requests and spiked to an OOM.  That indicates to me that I'm guessing the server was not failing nicely and triggering the backoff strategy you mentioned.  Again, not sure what other factors were at play but it's another scenario to consider at the very least.\nAs for the simplification: that sounds ideal to me as long as proper metrics are maintained with respect to number of spans dropped ;). Thanks! As previously mentioned, transition to zipkin 2 will be a little involved for us but I will report back when I can!\nHardest part with this is that the death is random.  So who knows if we'll find the random event again ;). My favorite would involve a change to Zipkin more than the server it's running on.  But it's: #1960.  Basically, if you don't buffer data then you don't need a big heap ;)\nI think we've also found that small, potentially rapid requests help (easier to flush small bits from the buffer than large blobs).  But we didn't have fantastic metrics when we made that change :. 1. It's a one shot deal.  You refresh the page and you have to browse for the file again.\n2. Attaching.\n3. traceViewer.mustache is trace.mustache with just a couple of small changes.  I did a horrible job committing it (it should've been a copy).  If you have a good reference for how I can share more between the two, accounting for the small changes, I'd be happy to refactor a bit.\n\n\nEdit: updated images to reflect new link name.. You da boss!  Say the words and I will make the change :). So, I just realized I messed up the commit a bit (new to and hating git, sorry).  So bear with me for a bit while I try to correct things.\nEdit: Corrected but in 2 commits this time :\\\nAlso, I do agree this should work with v2 data.  But at the moment I'm not sure where zipkin-js is or how to update it to work with v2 data.  I'd be happy to help with what I can given the right direction but for now I'll wait for the feedback from others.. > Haha if you have a trace with 9k spans you will have other problems.\nLol.  Perhaps.  But it is an unfortunate fact of life in the system I work on.  It's not really all that common but I deal with the worst-of-the-worst so I have the unfortunate pleasure of seeing it daily :'(\n\nLooping through 10k elements in JS is still quicker than touching the DOM, I don't think it will be a problem. (But don't take my word for it, might be a good idea to profile it.) I would be concerned if it took more than 30ms.\n\nTouche!  My head must be in the clouds right now :)\n\nCould copy/paste with a comment or add a dependency on zipkin-js and use\nthe functions directly.\n\nWill start with copy/paste and update the pull request when I can.  No promise on ETA given work priorities and such :. Back to working on this.\n\nIs there anything sharable between traceViewer.mustache and anything else?\n\nJust found out that with partials I can.\nRef:\n- https://stackoverflow.com/questions/24267810/hogan-js-partials-in-pre-compiled-template?rq=1\n- https://www.npmjs.com/package/mustache-loader <-- I have to add the 'noShortcut' option to do so.  Which isn't a hard refactor to do but before I go through that effort I wanted to run it by you to see how important it was before I did?\nAlso, I renamed \"Import A Trace\" to \"View Saved Trace\".  I added some missing i18n properties as well.  No translations, just the properties and default values.  Hopefully that doesn't mess with the foreign languages too much :\\\nReference pictures in prior post updated to reflect these changes.\n(commit to follow...). @drolando, in addition to what Adrian said, where I work we have a habit of attaching trace JSON to bug cases as output via the JSON button in the UI.  Without this feature, or some other tool to import the JSON, the trace JSON is pretty useless.  We have no desire to actually re-persist the JSON since our ES cluster is already pretty full of Zipkin data.  So this makes for a nice on-the-fly viewing of data with no storage overhead.\n@adriancole, any updates on feedback from the core team?. > the UI still says \"view a saved trace\" which is confusing as data in zipkin is saved.\nThat's because I changed it from \"Import a Trace\" ;)\n\nI wonder if the upload functionality should be a widget next to \"go to trace\"\n\nMaybe?  It could certainly clear up the verbiage issue we're having.  I attached a rough mockup below of what it would look like (in IE).  Having it below the \"go to trace\" box isn't really practical as I don't think Bootstrap navs support multi-row very well (at least, I've never seen one).  Either way, I can't say it looks very clean.  It's not immediately obvious which textbox should be used for what purpose without some descriptive text around the file input.  Additionally, there are some workflow complications with doing it that way.  After selecting the file we'd have to change the URL to remove any reference to current state.  Otherwise somebody may try to copy/paste it and expect the same results.  Which won't happen.  We can use JS to fix the URL but we have to be careful with history management so the back/forward buttons work as expected.\nSo the short version is that I believe a dedicated page for viewing the traces is the simplest way to do it.  But I'm not sure how to disambiguate the online from offline stuff like you're concerned about.  We can consider a multi-level nav but that means extra clicks to get to a page.  With as simple as the UI is at the moment I don't think that's appropriate either.\n\n\nI tried the functionality and notice I can't collapse any of the spans (even clicking expand-all collapse-all). Might want to check that.\n\nWorks on my box in Zipkin 2.4.6 with this change applied.  Tried IE, FF, and Chrome.  Behaves identically to normal/stored traces.  Did those work for you?  Which version/branch were you using?\np.s. Will try to find a test to add when I can.. Just got tests added for the span conversion stuff.  Unfortunately there is a somewhat higher level bug that may make them moot.\nCurrently, the /zipkin/v1/trace/{traceIdHex} has an optional query parameter to dump raw traces.  From what I can tell, getting that dump avoids some timestamp adjustments and sorting that a non-raw dump would do.  Unfortunately, it seems like /zipkin/v2/trace/{traceIdHex} defaults to the same /v1 raw behavior.  Now, it's unfortunate because the UI doesn't like it when the initial/root span isn't the first in the list of spans.  It actually fails quite miserably when that's the case.  Additionally, the lack of timestamp adjustments can cause spans to render in odd places on the timeline.  Is this something that would hold up this feature?  If so, I have no idea how I'd handle the situation at the moment.\nFor reference:\n- We just deployed Zipkin 2.4.2 to our production servers so I was using it as my test case\n- I dumped a normal v1 trace, a raw one, and a v2 trace for comparison\n- The normal v1 trace rendered properly, neither of the latter did\n- The raw v1 trace and v2 trace rendered identically after moving the root span up to be the first span\np.s.  How does \"View Local Trace\" sound?. Happy to help @adriancole :)  However, I'm not sure I'm up for porting those changes over.  I know Java/JS well enough but it's a lot of testing/etc... and some other priorities have come up for me.  Is it a requirement to have those working before the change can be merged?  If so, what other options do we have?\nAt a minimum, I could probably implement a sort routine to at least make sure the root span is first in the list.  That's about all I'd have time for.  And if I did that, would you want me to do it for raw v1 traces as well?. Sounds good to me!  Will update when I can.. Was able to look at sorting a little bit last week.  Going to try and polish/commit today :)\nEdit: done.. To make sure I understand correctly:\n1. You're trace is in v2 format and there is an issue converting to v1.\n2. You're going to essentially merge this request while simultaneously fixing the bug?\n3. Sorry for this being on my master branch.  Still new to git and didn't think about that.  I'm honestly going to delete the repo once it gets merged though so feel free to do whatever :)\n    - Deleting the repo so I can start fresh/branch properly if I make more changes.\n4.  You're welcome, and thank you!. Awesome!  Thank you for fixing up that merging issue for me :). Me and git really don't git along (pun intended).  My apologies for the noisy previous commits.  Hopefully a correct one will follow shortly.  Do you want a pull request for it when it does?. Question: how will this work with the UI gets updated to fully support V2, which technically doesn't have SR/SS annotations?  Currently, V2 spans get converted back to V1 format, so it works.  But will they still do that going forward?. :). I'm not sure where all @adriancole  merged that change.  According to the tags on the merge it should be in 2.5.2 but 2.5.3 seems the most likely to have it.\nIf there, it would disable the \"Find a Trace\" page and show a big red box instead of the search controls.. This is kinda funny...\n// config.js\nexport default function loadConfig() {\n  return $.ajax('config.json', {\n    type: 'GET',\n    dataType: 'json'\n  }).then(data => function config(key) {\n    return data[key] || defaults[key];\n  });\nSince we set searchEnabled to false, it defaults to the default.  Because that's how OR statements work :. How about a compromise?  Use LocalStorage, or some such mechanism, to persist it.  So once I change it, it stays the way I want it for my use case.  Which is [A] ;). Lol, I actually have been wanting to add LocalStorage/a cookie to the front-end to persist that setting for awhile.  I probably wouldn't do a config property for it personally.  But if I get any free time back I'll certainly take a stab.  God knows that won't be until after the Zipkin2 UI goes out though :(. Where ever this gets put it would be nice if consideration for a lot of annotations were made.  It doesn't happen often, but we sometimes get a lot of annotations on a span so a scrollbar/etc... for making it easy to navigate the details would be nice.. Where ever this gets put it would be nice if consideration for a lot of annotations were made.  It doesn't happen often, but we sometimes get a lot of annotations on a span so a scrollbar/etc... for making it easy to navigate the details would be nice.. Are you suggesting that we iterate through the spans in trace, see if they have a 'localEndpoint', and if they do, then call jsonEncoder.encodeV1() (effectively) on each span so that the UI can display both v1 and v2 traces?\nI'm midly afraid that v2 traces with 10k spans will suffer a bit from that.  But that concern aside, is there an easy way to extract jsonEncoder without having to copy/paste it into zipkin-ui?  Of course, I'm talking about extracting/sharing the conversion more so than the call to JSON.stringify() at the end.\n\nI think here, we should assume zipkin v2 format unless we know heuristically it is not.\n\nGiven that the UI will only show you traces in v1 format I think it is safer to assume v1 until zipkin-ui is updated to support v2.  But that doesn't mean we can't still support v2 pending the above discussion.. Woops!  My bad.  Will fix.. ",
    "Dreampie": "error's json:\n[{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"8353ecbd3f082ee4\",\"name\":\"http:/sessions\",\"parentId\":\"8353ecbd3f082ee4\",\"timestamp\":1476263414880000,\"duration\":5000,\"annotations\":[{\"timestamp\":1476263414880000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"timestamp\":1476263414885000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"a1ebd26feb6368ed\",\"name\":\"http:/principals\",\"parentId\":\"e7afbf720c4c2afe\",\"timestamp\":1476263407033000,\"duration\":14000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/principals\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/principals?username=anonymous\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"b31e7ba035fa7863\",\"name\":\"http:/principals\",\"parentId\":\"9cfc718eaa9ac68a\",\"timestamp\":1476263406993000,\"duration\":19000,\"annotations\":[{\"timestamp\":1476263406993000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476263407012000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"b9243445b0abe1f3\",\"name\":\"http:/sessions\",\"parentId\":\"b31e7ba035fa7863\",\"timestamp\":1476263407694000,\"duration\":6000,\"annotations\":[{\"timestamp\":1476263407694000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476263407700000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"cd79e9a448961760\",\"name\":\"http:/sessions\",\"parentId\":\"8353ecbd3f082ee4\",\"timestamp\":1476263414880000,\"duration\":5000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.20\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"POST\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.20:5201/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"e7afbf720c4c2afe\",\"name\":\"http:/principals\",\"parentId\":\"05c9ccf0a7207624\",\"timestamp\":1476263407033000,\"duration\":14000,\"annotations\":[{\"timestamp\":1476263407033000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476263407047000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476263414343000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}},{\"timestamp\":1476263414361000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}}],\"binaryAnnotations\":[{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"05c9ccf0a7207624\",\"name\":\"http:/usr/users\",\"parentId\":\"b31e7ba035fa7863\",\"timestamp\":1476263414328000,\"duration\":670000,\"annotations\":[{\"timestamp\":1476263414328000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}}],\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.26\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}},{\"key\":\"http.path\",\"value\":\"/usr/users\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.26:5701/usr/users\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"085bed5c66734fbf\",\"name\":\"http:/principals\",\"parentId\":\"b31e7ba035fa7863\",\"timestamp\":1476263406993000,\"duration\":19000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/principals\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/principals?username=anonymous\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"164fb5ad724af35f\",\"name\":\"http:/users\",\"parentId\":\"05c9ccf0a7207624\",\"timestamp\":1476263414383000,\"duration\":610000,\"annotations\":[{\"timestamp\":1476263414383000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}},{\"timestamp\":1476263414974000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"timestamp\":1476263414993000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}},{\"timestamp\":1476263415571000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}],\"binaryAnnotations\":[{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"4ffd423b8b006578\",\"name\":\"http:/users\",\"parentId\":\"164fb5ad724af35f\",\"timestamp\":1476263414974000,\"duration\":597000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.20\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/users\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.20:5201/users?user=%7B%22id%22%3Anull%2C%22username%22%3Anull%2C%22nickname%22%3Anull%2C%22signature%22%3Anull%2C%22email%22%3Anull%2C%22mobileCode%22%3Anull%2C%22mobile%22%3Anull%2C%22avatar%22%3Anull%2C%22passwordMatch%22%3Anull%2C%22passwordSalt%22%3Anull%2C%22password%22%3Anull%2C%22signedIp%22%3Anull%2C%22signedAt%22%3Anull%2C%22deletedAble%22%3Anull%2C%22createdIp%22%3Anull%2C%22createdAt%22%3Anull%2C%22createrId%22%3Anull%2C%22updatedAt%22%3Anull%2C%22updaterId%22%3Anull%2C%22deletedAt%22%3Anull%2C%22deleterId%22%3Anull%2C%22userRoles%22%3Anull%2C%22info%22%3Anull%2C%22userData%22%3Anull%2C%22tagIds%22%3Anull%2C%22tagEntities%22%3Anull%2C%22level%22%3Anull%2C%22userQQ%22%3Anull%2C%22userWeibo%22%3Anull%2C%22userWechat%22%3Anull%2C%22ids%22%3Anull%2C%22roleId%22%3Anull%2C%22changedPassword%22%3Anull%2C%22createdTime%22%3Anull%2C%22roleGroupId%22%3Anull%2C%22needRoleGroupChildren%22%3Afalse%2C%22managerId%22%3A2%2C%22signedTime%22%3Anull%2C%22needInfo%22%3Afalse%2C%22updateType%22%3Anull%2C%22oldPassword%22%3Anull%2C%22captcha%22%3Anull%2C%22credentials%22%3Anull%2C%22oldMobile%22%3Anull%2C%22oldEmail%22%3Anull%2C%22oldCaptcha%22%3Anull%7D&page=%7B%22total%22%3A-1%2C%22content%22%3A%5B%5D%2C%22num%22%3A1%2C%22size%22%3A20%2C%22sort%22%3Anull%7D\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"6eab4d68da610379\",\"name\":\"http:/sessions\",\"parentId\":\"b9243445b0abe1f3\",\"timestamp\":1476263407694000,\"duration\":6000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"POST\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]}]\nright's json:\n[{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"af8f6725eafc87f4\",\"name\":\"http:/sessions\",\"timestamp\":1476262582964000,\"duration\":9000,\"annotations\":[{\"timestamp\":1476262582964000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476262582973000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"9d6f73594c8c6832\",\"name\":\"http:/limits\",\"parentId\":\"888e26be35f19854\",\"timestamp\":1476262590544000,\"duration\":14000,\"annotations\":[{\"timestamp\":1476262583230000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}},{\"timestamp\":1476262583251000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}},{\"timestamp\":1476262590544000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5205}},{\"timestamp\":1476262590558000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5205}}],\"binaryAnnotations\":[{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"fd4320a4b08f357b\",\"name\":\"http:/principals\",\"parentId\":\"af8f6725eafc87f4\",\"timestamp\":1476262590870000,\"duration\":51000,\"annotations\":[{\"timestamp\":1476262590870000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"timestamp\":1476262590921000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"0fd81ac3f7071a6e\",\"name\":\"http:/sessions\",\"parentId\":\"af8f6725eafc87f4\",\"timestamp\":1476262582964000,\"duration\":9000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"POST\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"134d250e11696372\",\"name\":\"http:/principals\",\"parentId\":\"8c6ecfb78627edc9\",\"timestamp\":1476262583205000,\"duration\":22000,\"annotations\":[{\"timestamp\":1476262583205000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476262583227000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"2189002d8116cac9\",\"name\":\"http:/sessions/a49f7a16-bde9-4f75-9244-6868a07167a6\",\"parentId\":\"888e26be35f19854\",\"timestamp\":1476262583279000,\"duration\":5000,\"annotations\":[{\"timestamp\":1476262583279000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476262583284000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"40da64c48ff48b9c\",\"name\":\"http:/limits\",\"parentId\":\"888e26be35f19854\",\"timestamp\":1476262583257000,\"duration\":16000,\"annotations\":[{\"timestamp\":1476262583252000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}},{\"timestamp\":1476262583257000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5205}},{\"timestamp\":1476262583273000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5205}},{\"timestamp\":1476262583276000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}}],\"binaryAnnotations\":[{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"3295121e292ffa69\",\"name\":\"http:/limits\",\"parentId\":\"9d6f73594c8c6832\",\"timestamp\":1476262590544000,\"duration\":14000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.26\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5205}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5205}},{\"key\":\"http.path\",\"value\":\"/limits\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5205}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.26:5205/limits?limit=%7B%22target%22%3A%22101.36.73.157%22%2C%22typeId%22%3A2%2C%22action%22%3A%7B%22id%22%3A1%2C%22value%22%3Anull%2C%22name%22%3Anull%2C%22intro%22%3Anull%2C%22deletedAble%22%3Atrue%2C%22method%22%3Anull%2C%22path%22%3Anull%2C%22pathMatch%22%3Anull%2C%22createrId%22%3Anull%2C%22createdAt%22%3Anull%2C%22updaterId%22%3Anull%2C%22updatedAt%22%3Anull%2C%22deleterId%22%3Anull%2C%22deletedAt%22%3Anull%7D%2C%22needExpired%22%3Afalse%2C%22fromDdcms%22%3Afalse%2C%22expired%22%3Afalse%7D&page=%7B%22total%22%3A-1%2C%22content%22%3A%5B%5D%2C%22num%22%3A1%2C%22size%22%3A1%2C%22sort%22%3Anull%7D\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5205}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5205}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"49871086608b358c\",\"name\":\"http:/principals\",\"parentId\":\"fd4320a4b08f357b\",\"timestamp\":1476262590870000,\"duration\":51000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.20\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/principals\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.20:5201/principals?username=anonymous\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"11d62da5ee0f23c6\",\"name\":\"http:/principals\",\"parentId\":\"134d250e11696372\",\"timestamp\":1476262583205000,\"duration\":22000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/principals\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/principals?username=admin\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"b9d7d59753fa4441\",\"name\":\"http:/users/1\",\"parentId\":\"2189002d8116cac9\",\"timestamp\":1476262591181000,\"duration\":96000,\"annotations\":[{\"timestamp\":1476262591181000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"timestamp\":1476262591277000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"5a31469bf8237656\",\"name\":\"http:/sessions/a49f7a16-bde9-4f75-9244-6868a07167a6\",\"parentId\":\"2189002d8116cac9\",\"timestamp\":1476262583279000,\"duration\":5000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"DELETE\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/sessions/a49f7a16-bde9-4f75-9244-6868a07167a6\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/sessions/a49f7a16-bde9-4f75-9244-6868a07167a6\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"ac410eabdefd7501\",\"name\":\"http:/limits\",\"parentId\":\"40da64c48ff48b9c\",\"timestamp\":1476262583257000,\"duration\":16000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5205}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5205}},{\"key\":\"http.path\",\"value\":\"/limits\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5205}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5205/limits?limit=%7B%22target%22%3A1%2C%22typeId%22%3A1%2C%22action%22%3A%7B%22id%22%3A1%2C%22value%22%3Anull%2C%22name%22%3Anull%2C%22intro%22%3Anull%2C%22deletedAble%22%3Atrue%2C%22method%22%3Anull%2C%22path%22%3Anull%2C%22pathMatch%22%3Anull%2C%22createrId%22%3Anull%2C%22createdAt%22%3Anull%2C%22updaterId%22%3Anull%2C%22updatedAt%22%3Anull%2C%22deleterId%22%3Anull%2C%22deletedAt%22%3Anull%7D%2C%22needExpired%22%3Afalse%2C%22fromDdcms%22%3Afalse%2C%22expired%22%3Afalse%7D&page=%7B%22total%22%3A-1%2C%22content%22%3A%5B%5D%2C%22num%22%3A1%2C%22size%22%3A1%2C%22sort%22%3Anull%7D\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5205}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5205}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"82fc140bcfa8c2b7\",\"name\":\"http:/users/1\",\"parentId\":\"b9d7d59753fa4441\",\"timestamp\":1476262591181000,\"duration\":96000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.20\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"PUT\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/users/1\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.20:5201/users/1\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"884206ceb0dc4478\",\"name\":\"http:/actions\",\"parentId\":\"b9d7d59753fa4441\",\"timestamp\":1476262583422000,\"duration\":23000,\"annotations\":[{\"timestamp\":1476262583422000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}},{\"timestamp\":1476262583445000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}},{\"timestamp\":1476262591320000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"log-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5202}},{\"timestamp\":1476262591337000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"log-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5202}}],\"binaryAnnotations\":[{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"d3c248cf60845196\",\"name\":\"http:/principals\",\"parentId\":\"b9d7d59753fa4441\",\"timestamp\":1476262583388000,\"duration\":27000,\"annotations\":[{\"timestamp\":1476262583386000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}},{\"timestamp\":1476262583388000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476262583415000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476262583417000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}}],\"binaryAnnotations\":[{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"06f7f5d4f03300e8\",\"name\":\"http:/sessions\",\"parentId\":\"b9d7d59753fa4441\",\"timestamp\":1476262583454000,\"duration\":5000,\"annotations\":[{\"timestamp\":1476262583454000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476262583459000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"41c8c5e848a5b0c5\",\"name\":\"async\",\"parentId\":\"82fc140bcfa8c2b7\",\"timestamp\":1476262591265000,\"binaryAnnotations\":[{\"key\":\"lc\",\"value\":\"async\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"thread\",\"value\":\"SimpleAsyncTaskExecutor-6\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"a0623b4169acd037\",\"name\":\"http:/actions\",\"parentId\":\"884206ceb0dc4478\",\"timestamp\":1476262591320000,\"duration\":17000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.20\",\"endpoint\":{\"serviceName\":\"log-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5202}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"log-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5202}},{\"key\":\"http.path\",\"value\":\"/actions\",\"endpoint\":{\"serviceName\":\"log-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5202}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.20:5202/actions?action=%7B%22id%22%3Anull%2C%22name%22%3Anull%2C%22value%22%3A%22%2Fsessions%22%2C%22createrId%22%3Anull%2C%22createdAt%22%3Anull%2C%22updaterId%22%3Anull%2C%22updatedAt%22%3Anull%2C%22deleterId%22%3Anull%2C%22deletedAt%22%3Anull%7D&page=%7B%22total%22%3A-1%2C%22content%22%3A%5B%5D%2C%22num%22%3A1%2C%22size%22%3A1%2C%22sort%22%3Anull%7D\",\"endpoint\":{\"serviceName\":\"log-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5202}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"log-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5202}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"ee34c91e7dece1ff\",\"name\":\"http:/principals\",\"parentId\":\"d3c248cf60845196\",\"timestamp\":1476262583388000,\"duration\":27000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/principals\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/principals?username=admin\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"631caba0b6c03941\",\"name\":\"http:/sessions\",\"parentId\":\"06f7f5d4f03300e8\",\"timestamp\":1476262583454000,\"duration\":5000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"POST\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"d3e21bb6864d2b66\",\"name\":\"senduser\",\"parentId\":\"41c8c5e848a5b0c5\",\"timestamp\":1476262591265000,\"binaryAnnotations\":[{\"key\":\"class\",\"value\":\"DdcmsSender\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"async\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"method\",\"value\":\"sendUser\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]}]\n. error's json:\n[{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"8353ecbd3f082ee4\",\"name\":\"http:/sessions\",\"parentId\":\"8353ecbd3f082ee4\",\"timestamp\":1476263414880000,\"duration\":5000,\"annotations\":[{\"timestamp\":1476263414880000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"timestamp\":1476263414885000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"a1ebd26feb6368ed\",\"name\":\"http:/principals\",\"parentId\":\"e7afbf720c4c2afe\",\"timestamp\":1476263407033000,\"duration\":14000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/principals\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/principals?username=anonymous\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"b31e7ba035fa7863\",\"name\":\"http:/principals\",\"parentId\":\"9cfc718eaa9ac68a\",\"timestamp\":1476263406993000,\"duration\":19000,\"annotations\":[{\"timestamp\":1476263406993000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476263407012000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"b9243445b0abe1f3\",\"name\":\"http:/sessions\",\"parentId\":\"b31e7ba035fa7863\",\"timestamp\":1476263407694000,\"duration\":6000,\"annotations\":[{\"timestamp\":1476263407694000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476263407700000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"cd79e9a448961760\",\"name\":\"http:/sessions\",\"parentId\":\"8353ecbd3f082ee4\",\"timestamp\":1476263414880000,\"duration\":5000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.20\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"POST\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.20:5201/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"e7afbf720c4c2afe\",\"name\":\"http:/principals\",\"parentId\":\"05c9ccf0a7207624\",\"timestamp\":1476263407033000,\"duration\":14000,\"annotations\":[{\"timestamp\":1476263407033000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476263407047000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476263414343000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}},{\"timestamp\":1476263414361000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}}],\"binaryAnnotations\":[{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"05c9ccf0a7207624\",\"name\":\"http:/usr/users\",\"parentId\":\"b31e7ba035fa7863\",\"timestamp\":1476263414328000,\"duration\":670000,\"annotations\":[{\"timestamp\":1476263414328000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}}],\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.26\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}},{\"key\":\"http.path\",\"value\":\"/usr/users\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.26:5701/usr/users\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"085bed5c66734fbf\",\"name\":\"http:/principals\",\"parentId\":\"b31e7ba035fa7863\",\"timestamp\":1476263406993000,\"duration\":19000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/principals\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/principals?username=anonymous\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"164fb5ad724af35f\",\"name\":\"http:/users\",\"parentId\":\"05c9ccf0a7207624\",\"timestamp\":1476263414383000,\"duration\":610000,\"annotations\":[{\"timestamp\":1476263414383000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}},{\"timestamp\":1476263414974000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"timestamp\":1476263414993000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}},{\"timestamp\":1476263415571000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}],\"binaryAnnotations\":[{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5701}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"4ffd423b8b006578\",\"name\":\"http:/users\",\"parentId\":\"164fb5ad724af35f\",\"timestamp\":1476263414974000,\"duration\":597000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.20\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/users\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.20:5201/users?user=%7B%22id%22%3Anull%2C%22username%22%3Anull%2C%22nickname%22%3Anull%2C%22signature%22%3Anull%2C%22email%22%3Anull%2C%22mobileCode%22%3Anull%2C%22mobile%22%3Anull%2C%22avatar%22%3Anull%2C%22passwordMatch%22%3Anull%2C%22passwordSalt%22%3Anull%2C%22password%22%3Anull%2C%22signedIp%22%3Anull%2C%22signedAt%22%3Anull%2C%22deletedAble%22%3Anull%2C%22createdIp%22%3Anull%2C%22createdAt%22%3Anull%2C%22createrId%22%3Anull%2C%22updatedAt%22%3Anull%2C%22updaterId%22%3Anull%2C%22deletedAt%22%3Anull%2C%22deleterId%22%3Anull%2C%22userRoles%22%3Anull%2C%22info%22%3Anull%2C%22userData%22%3Anull%2C%22tagIds%22%3Anull%2C%22tagEntities%22%3Anull%2C%22level%22%3Anull%2C%22userQQ%22%3Anull%2C%22userWeibo%22%3Anull%2C%22userWechat%22%3Anull%2C%22ids%22%3Anull%2C%22roleId%22%3Anull%2C%22changedPassword%22%3Anull%2C%22createdTime%22%3Anull%2C%22roleGroupId%22%3Anull%2C%22needRoleGroupChildren%22%3Afalse%2C%22managerId%22%3A2%2C%22signedTime%22%3Anull%2C%22needInfo%22%3Afalse%2C%22updateType%22%3Anull%2C%22oldPassword%22%3Anull%2C%22captcha%22%3Anull%2C%22credentials%22%3Anull%2C%22oldMobile%22%3Anull%2C%22oldEmail%22%3Anull%2C%22oldCaptcha%22%3Anull%7D&page=%7B%22total%22%3A-1%2C%22content%22%3A%5B%5D%2C%22num%22%3A1%2C%22size%22%3A20%2C%22sort%22%3Anull%7D\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"8353ecbd3f082ee4\",\"id\":\"6eab4d68da610379\",\"name\":\"http:/sessions\",\"parentId\":\"b9243445b0abe1f3\",\"timestamp\":1476263407694000,\"duration\":6000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"POST\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]}]\nright's json:\n[{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"af8f6725eafc87f4\",\"name\":\"http:/sessions\",\"timestamp\":1476262582964000,\"duration\":9000,\"annotations\":[{\"timestamp\":1476262582964000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476262582973000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"9d6f73594c8c6832\",\"name\":\"http:/limits\",\"parentId\":\"888e26be35f19854\",\"timestamp\":1476262590544000,\"duration\":14000,\"annotations\":[{\"timestamp\":1476262583230000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}},{\"timestamp\":1476262583251000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}},{\"timestamp\":1476262590544000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5205}},{\"timestamp\":1476262590558000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5205}}],\"binaryAnnotations\":[{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"fd4320a4b08f357b\",\"name\":\"http:/principals\",\"parentId\":\"af8f6725eafc87f4\",\"timestamp\":1476262590870000,\"duration\":51000,\"annotations\":[{\"timestamp\":1476262590870000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"timestamp\":1476262590921000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"0fd81ac3f7071a6e\",\"name\":\"http:/sessions\",\"parentId\":\"af8f6725eafc87f4\",\"timestamp\":1476262582964000,\"duration\":9000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"POST\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"134d250e11696372\",\"name\":\"http:/principals\",\"parentId\":\"8c6ecfb78627edc9\",\"timestamp\":1476262583205000,\"duration\":22000,\"annotations\":[{\"timestamp\":1476262583205000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476262583227000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"2189002d8116cac9\",\"name\":\"http:/sessions/a49f7a16-bde9-4f75-9244-6868a07167a6\",\"parentId\":\"888e26be35f19854\",\"timestamp\":1476262583279000,\"duration\":5000,\"annotations\":[{\"timestamp\":1476262583279000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476262583284000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"40da64c48ff48b9c\",\"name\":\"http:/limits\",\"parentId\":\"888e26be35f19854\",\"timestamp\":1476262583257000,\"duration\":16000,\"annotations\":[{\"timestamp\":1476262583252000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}},{\"timestamp\":1476262583257000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5205}},{\"timestamp\":1476262583273000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5205}},{\"timestamp\":1476262583276000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}}],\"binaryAnnotations\":[{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"3295121e292ffa69\",\"name\":\"http:/limits\",\"parentId\":\"9d6f73594c8c6832\",\"timestamp\":1476262590544000,\"duration\":14000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.26\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5205}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5205}},{\"key\":\"http.path\",\"value\":\"/limits\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5205}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.26:5205/limits?limit=%7B%22target%22%3A%22101.36.73.157%22%2C%22typeId%22%3A2%2C%22action%22%3A%7B%22id%22%3A1%2C%22value%22%3Anull%2C%22name%22%3Anull%2C%22intro%22%3Anull%2C%22deletedAble%22%3Atrue%2C%22method%22%3Anull%2C%22path%22%3Anull%2C%22pathMatch%22%3Anull%2C%22createrId%22%3Anull%2C%22createdAt%22%3Anull%2C%22updaterId%22%3Anull%2C%22updatedAt%22%3Anull%2C%22deleterId%22%3Anull%2C%22deletedAt%22%3Anull%7D%2C%22needExpired%22%3Afalse%2C%22fromDdcms%22%3Afalse%2C%22expired%22%3Afalse%7D&page=%7B%22total%22%3A-1%2C%22content%22%3A%5B%5D%2C%22num%22%3A1%2C%22size%22%3A1%2C%22sort%22%3Anull%7D\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5205}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.26\",\"port\":5205}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"49871086608b358c\",\"name\":\"http:/principals\",\"parentId\":\"fd4320a4b08f357b\",\"timestamp\":1476262590870000,\"duration\":51000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.20\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/principals\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.20:5201/principals?username=anonymous\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"11d62da5ee0f23c6\",\"name\":\"http:/principals\",\"parentId\":\"134d250e11696372\",\"timestamp\":1476262583205000,\"duration\":22000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/principals\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/principals?username=admin\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"b9d7d59753fa4441\",\"name\":\"http:/users/1\",\"parentId\":\"2189002d8116cac9\",\"timestamp\":1476262591181000,\"duration\":96000,\"annotations\":[{\"timestamp\":1476262591181000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"timestamp\":1476262591277000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"5a31469bf8237656\",\"name\":\"http:/sessions/a49f7a16-bde9-4f75-9244-6868a07167a6\",\"parentId\":\"2189002d8116cac9\",\"timestamp\":1476262583279000,\"duration\":5000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"DELETE\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/sessions/a49f7a16-bde9-4f75-9244-6868a07167a6\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/sessions/a49f7a16-bde9-4f75-9244-6868a07167a6\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"ac410eabdefd7501\",\"name\":\"http:/limits\",\"parentId\":\"40da64c48ff48b9c\",\"timestamp\":1476262583257000,\"duration\":16000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5205}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5205}},{\"key\":\"http.path\",\"value\":\"/limits\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5205}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5205/limits?limit=%7B%22target%22%3A1%2C%22typeId%22%3A1%2C%22action%22%3A%7B%22id%22%3A1%2C%22value%22%3Anull%2C%22name%22%3Anull%2C%22intro%22%3Anull%2C%22deletedAble%22%3Atrue%2C%22method%22%3Anull%2C%22path%22%3Anull%2C%22pathMatch%22%3Anull%2C%22createrId%22%3Anull%2C%22createdAt%22%3Anull%2C%22updaterId%22%3Anull%2C%22updatedAt%22%3Anull%2C%22deleterId%22%3Anull%2C%22deletedAt%22%3Anull%7D%2C%22needExpired%22%3Afalse%2C%22fromDdcms%22%3Afalse%2C%22expired%22%3Afalse%7D&page=%7B%22total%22%3A-1%2C%22content%22%3A%5B%5D%2C%22num%22%3A1%2C%22size%22%3A1%2C%22sort%22%3Anull%7D\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5205}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"limit-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5205}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"82fc140bcfa8c2b7\",\"name\":\"http:/users/1\",\"parentId\":\"b9d7d59753fa4441\",\"timestamp\":1476262591181000,\"duration\":96000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.20\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"PUT\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/users/1\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.20:5201/users/1\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"884206ceb0dc4478\",\"name\":\"http:/actions\",\"parentId\":\"b9d7d59753fa4441\",\"timestamp\":1476262583422000,\"duration\":23000,\"annotations\":[{\"timestamp\":1476262583422000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}},{\"timestamp\":1476262583445000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}},{\"timestamp\":1476262591320000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"log-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5202}},{\"timestamp\":1476262591337000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"log-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5202}}],\"binaryAnnotations\":[{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"d3c248cf60845196\",\"name\":\"http:/principals\",\"parentId\":\"b9d7d59753fa4441\",\"timestamp\":1476262583388000,\"duration\":27000,\"annotations\":[{\"timestamp\":1476262583386000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}},{\"timestamp\":1476262583388000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476262583415000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476262583417000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}}],\"binaryAnnotations\":[{\"key\":\"sa\",\"value\":true,\"endpoint\":{\"serviceName\":\"user-api-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5701}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"06f7f5d4f03300e8\",\"name\":\"http:/sessions\",\"parentId\":\"b9d7d59753fa4441\",\"timestamp\":1476262583454000,\"duration\":5000,\"annotations\":[{\"timestamp\":1476262583454000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"timestamp\":1476262583459000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"41c8c5e848a5b0c5\",\"name\":\"async\",\"parentId\":\"82fc140bcfa8c2b7\",\"timestamp\":1476262591265000,\"binaryAnnotations\":[{\"key\":\"lc\",\"value\":\"async\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"thread\",\"value\":\"SimpleAsyncTaskExecutor-6\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"a0623b4169acd037\",\"name\":\"http:/actions\",\"parentId\":\"884206ceb0dc4478\",\"timestamp\":1476262591320000,\"duration\":17000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.20\",\"endpoint\":{\"serviceName\":\"log-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5202}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"log-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5202}},{\"key\":\"http.path\",\"value\":\"/actions\",\"endpoint\":{\"serviceName\":\"log-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5202}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.20:5202/actions?action=%7B%22id%22%3Anull%2C%22name%22%3Anull%2C%22value%22%3A%22%2Fsessions%22%2C%22createrId%22%3Anull%2C%22createdAt%22%3Anull%2C%22updaterId%22%3Anull%2C%22updatedAt%22%3Anull%2C%22deleterId%22%3Anull%2C%22deletedAt%22%3Anull%7D&page=%7B%22total%22%3A-1%2C%22content%22%3A%5B%5D%2C%22num%22%3A1%2C%22size%22%3A1%2C%22sort%22%3Anull%7D\",\"endpoint\":{\"serviceName\":\"log-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5202}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"log-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5202}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"ee34c91e7dece1ff\",\"name\":\"http:/principals\",\"parentId\":\"d3c248cf60845196\",\"timestamp\":1476262583388000,\"duration\":27000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"GET\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/principals\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/principals?username=admin\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"631caba0b6c03941\",\"name\":\"http:/sessions\",\"parentId\":\"06f7f5d4f03300e8\",\"timestamp\":1476262583454000,\"duration\":5000,\"binaryAnnotations\":[{\"key\":\"http.host\",\"value\":\"10.1.0.21\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.method\",\"value\":\"POST\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.path\",\"value\":\"/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"http.url\",\"value\":\"http://10.1.0.21:5201/sessions\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"unknown\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.21\",\"port\":5201}}]},{\"traceId\":\"af8f6725eafc87f4\",\"id\":\"d3e21bb6864d2b66\",\"name\":\"senduser\",\"parentId\":\"41c8c5e848a5b0c5\",\"timestamp\":1476262591265000,\"binaryAnnotations\":[{\"key\":\"class\",\"value\":\"DdcmsSender\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"lc\",\"value\":\"async\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}},{\"key\":\"method\",\"value\":\"sendUser\",\"endpoint\":{\"serviceName\":\"user-service-provider\",\"ipv4\":\"10.1.0.20\",\"port\":5201}}]}]\n. curl  return was nothing. I was using chrome\n. curl  return was nothing. I was using chrome\n. \n\n. \n\n. \n\n. \n\n. I upgrded\uff0cbut trace 8353ecbd3f082ee4  still not ok\n. I upgrded\uff0cbut trace 8353ecbd3f082ee4  still not ok\n. ok\uff0cthanks\n. ok\uff0cthanks\n. ",
    "StarpTech": "Hi, I have the same issue. I don't know which version but it works before. How can I fetch a fixed version from docker?\nUncaught RangeError: Maximum call stack size exceeded\n    at Array.map (<anonymous>)\n    at u (:9411/zipkin/app-ee5f6a9258bb25ee5325.min.js:89)\n    at :9411/zipkin/app-ee5f6a9258bb25ee5325.min.js:89\n    at Array.map (<anonymous>)\n    at u (:9411/zipkin/app-ee5f6a9258bb25ee5325.min.js:89)\n    at :9411/zipkin/app-ee5f6a9258bb25ee5325.min.js:89\n    at Array.map (<anonymous>)\n    at u (:9411/zipkin/app-ee5f6a9258bb25ee5325.min.js:89)\n    at :9411/zipkin/app-ee5f6a9258bb25ee5325.min.js:89\n    at Array.map (<anonymous>). I don't know why but all traces are merged.\nZipkin Api 1\njson\n[[{\"traceId\":\"277bdc68c19f4256a5d50bdb5c4d3dd0\",\"id\":\"ac74541bc8054ef5\",\"name\":\"cmd:login,topic:auth\",\"parentId\":\"ac74541bc8054ef5\",\"timestamp\":1527933133370000,\"duration\":9000,\"annotations\":[{\"timestamp\":1527933133341000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"auth\"}},{\"timestamp\":1527933133344000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"auth\"}},{\"timestamp\":1527933133346000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"profile\"}},{\"timestamp\":1527933133353000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"profile\"}},{\"timestamp\":1527933133355000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"email\"}},{\"timestamp\":1527933133355000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"profile\"}},{\"timestamp\":1527933133362000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"profile\"}},{\"timestamp\":1527933133363000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"email\"}},{\"timestamp\":1527933133364000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"email\"}},{\"timestamp\":1527933133364000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"search\"}},{\"timestamp\":1527933133366000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"email\"}},{\"timestamp\":1527933133367000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"search\"}},{\"timestamp\":1527933133367000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"search\"}},{\"timestamp\":1527933133369000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"email\"}},{\"timestamp\":1527933133370000,\"value\":\"cs\",\"endpoint\":{\"serviceName\":\"account\"}},{\"timestamp\":1527933133372000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"search\"}},{\"timestamp\":1527933133373000,\"value\":\"sr\",\"endpoint\":{\"serviceName\":\"account\"}},{\"timestamp\":1527933133373000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"account\"}},{\"timestamp\":1527933133376000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"account\"}},{\"timestamp\":1527933133377000,\"value\":\"ss\",\"endpoint\":{\"serviceName\":\"auth\"}},{\"timestamp\":1527933133379000,\"value\":\"cr\",\"endpoint\":{\"serviceName\":\"auth\"}}],\"binaryAnnotations\":[{\"key\":\"query\",\"value\":\"SELECT FROM User;\"},{\"key\":\"rpc.method\",\"value\":\"cmd:login,topic:auth\"},{\"key\":\"rpc.method\",\"value\":\"cmd:get,topic:profile\"},{\"key\":\"rpc.method\",\"value\":\"cmd:send,topic:email\"},{\"key\":\"rpc.method\",\"value\":\"cmd:friends,topic:search\"},{\"key\":\"rpc.method\",\"value\":\"cmd:delete,topic:account\"},{\"key\":\"rpc.timeout\",\"value\":\"2000\"},{\"key\":\"rpc.timeout\",\"value\":\"2000\"},{\"key\":\"rpc.timeout\",\"value\":\"2000\"},{\"key\":\"rpc.timeout\",\"value\":\"2000\"},{\"key\":\"rpc.timeout\",\"value\":\"2000\"},{\"key\":\"rpc.topic\",\"value\":\"auth\"},{\"key\":\"rpc.topic\",\"value\":\"profile\"},{\"key\":\"rpc.topic\",\"value\":\"email\"},{\"key\":\"rpc.topic\",\"value\":\"search\"},{\"key\":\"rpc.topic\",\"value\":\"account\"},{\"key\":\"server.topic\",\"value\":\"profile\"},{\"key\":\"server.topic\",\"value\":\"email\"},{\"key\":\"server.topic\",\"value\":\"search\"},{\"key\":\"server.topic\",\"value\":\"email\"},{\"key\":\"server.topic\",\"value\":\"account\"},{\"key\":\"server.topic\",\"value\":\"auth\"},{\"key\":\"server.topic\",\"value\":\"auth\"},{\"key\":\"server.topic\",\"value\":\"profile\"},{\"key\":\"server.topic\",\"value\":\"email\"},{\"key\":\"server.topic\",\"value\":\"email\"},{\"key\":\"server.topic\",\"value\":\"search\"},{\"key\":\"server.topic\",\"value\":\"account\"}]}]]\nThat's the send data (in order):\n```json\n[\n  {\n    \"traceId\": \"67e8f8006e8b422fac70afb8f2cac9fb\",\n    \"name\": \"cmd:login,topic:auth\",\n    \"id\": \"f258fb0fb3534f30aaeb30ccff30311c\",\n    \"annotations\": [\n      {\n        \"endpoint\": {\n          \"serviceName\": \"auth\",\n          \"ipv4\": 0,\n          \"port\": 0\n        },\n        \"timestamp\": 1527933340260000,\n        \"value\": \"cs\"\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"rpc.topic\",\n        \"value\": \"auth\"\n      },\n      {\n        \"key\": \"rpc.method\",\n        \"value\": \"cmd:login,topic:auth\"\n      },\n      {\n        \"key\": \"rpc.timeout\",\n        \"value\": \"2000\"\n      }\n    ],\n    \"timestamp\": 1527933340260000\n  },\n  {\n    \"traceId\": \"8406eabacaa04c04bd0abdc661a9f073\",\n    \"name\": \"cmd:friends,topic:search\",\n    \"id\": \"b364b5e75c7d462e8dcbf7cddf9fbd4f\",\n    \"annotations\": [\n      {\n        \"endpoint\": {\n          \"serviceName\": \"search\",\n          \"ipv4\": 0,\n          \"port\": 0\n        },\n        \"timestamp\": 1527933340261000,\n        \"value\": \"cs\"\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"rpc.topic\",\n        \"value\": \"search\"\n      },\n      {\n        \"key\": \"rpc.method\",\n        \"value\": \"cmd:friends,topic:search\"\n      },\n      {\n        \"key\": \"rpc.timeout\",\n        \"value\": \"2000\"\n      }\n    ],\n    \"timestamp\": 1527933340261000\n  },\n  {\n    \"traceId\": \"67e8f8006e8b422fac70afb8f2cac9fb\",\n    \"name\": \"cmd:send,topic:email\",\n    \"id\": \"f258fb0fb3534f30aaeb30ccff30311c\",\n    \"annotations\": [\n      {\n        \"endpoint\": {\n          \"serviceName\": \"email\",\n          \"ipv4\": 0,\n          \"port\": 0\n        },\n        \"timestamp\": 1527933340264000,\n        \"value\": \"sr\"\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"server.topic\",\n        \"value\": \"email\"\n      }\n    ],\n    \"parentId\": \"f258fb0fb3534f30aaeb30ccff30311c\"\n  },\n  {\n    \"traceId\": \"67e8f8006e8b422fac70afb8f2cac9fb\",\n    \"name\": \"cmd:send,topic:email\",\n    \"id\": \"f258fb0fb3534f30aaeb30ccff30311c\",\n    \"annotations\": [\n      {\n        \"endpoint\": {\n          \"serviceName\": \"email\",\n          \"ipv4\": 0,\n          \"port\": 0\n        },\n        \"timestamp\": 1527933340265000,\n        \"value\": \"ss\"\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"server.topic\",\n        \"value\": \"email\"\n      }\n    ],\n    \"parentId\": \"f258fb0fb3534f30aaeb30ccff30311c\"\n  },\n  {\n    \"traceId\": \"8406eabacaa04c04bd0abdc661a9f073\",\n    \"name\": \"cmd:friends,topic:search\",\n    \"id\": \"b364b5e75c7d462e8dcbf7cddf9fbd4f\",\n    \"annotations\": [\n      {\n        \"endpoint\": {\n          \"serviceName\": \"search\",\n          \"ipv4\": 0,\n          \"port\": 0\n        },\n        \"timestamp\": 1527933340266000,\n        \"value\": \"cr\"\n      }\n    ],\n    \"binaryAnnotations\": [],\n    \"duration\": null\n  }\n]\n[\n  {\n    \"traceId\": \"67e8f8006e8b422fac70afb8f2cac9fb\",\n    \"name\": \"cmd:login,topic:auth\",\n    \"id\": \"f258fb0fb3534f30aaeb30ccff30311c\",\n    \"annotations\": [\n      {\n        \"endpoint\": {\n          \"serviceName\": \"auth\",\n          \"ipv4\": 0,\n          \"port\": 0\n        },\n        \"timestamp\": 1527933340271000,\n        \"value\": \"cr\"\n      }\n    ],\n    \"binaryAnnotations\": [],\n    \"duration\": null\n  }\n]\n```. Thanks for the fast response. That issue sounds really bad.. ",
    "gcnote": "got it , thanks\n. got it , thanks\n. ",
    "nollbit": "Thank you.\nHowever; spans reported for an endpoint where the port is larger than what a signed short can hold leads to query errors? There's an exception when the endpoint is unpacked from JSON.\nI can't see that being fixed, so I guess this bug should perhaps still be open?\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-storage/elasticsearch-http/src/main/java/zipkin/storage/elasticsearch/http/ZipkinAdapters.java#L154 \nport() asserts port being > 0 https://github.com/openzipkin/zipkin/blob/master/zipkin/src/main/java/zipkin/Endpoint.java#L159-L163\nwhich leads to the exception which actually also breaks the UI completely, no error message.\n. Thank you.\nHowever; spans reported for an endpoint where the port is larger than what a signed short can hold leads to query errors? There's an exception when the endpoint is unpacked from JSON.\nI can't see that being fixed, so I guess this bug should perhaps still be open?\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-storage/elasticsearch-http/src/main/java/zipkin/storage/elasticsearch/http/ZipkinAdapters.java#L154 \nport() asserts port being > 0 https://github.com/openzipkin/zipkin/blob/master/zipkin/src/main/java/zipkin/Endpoint.java#L159-L163\nwhich leads to the exception which actually also breaks the UI completely, no error message.\n. No worries at all, thanks for the speedy fix!\n. No worries at all, thanks for the speedy fix!\n. Numbers.\nThe JSON above is copied from the response from /api/v1/trace. \n. ",
    "alseddnm": "Hey @adriancole we see a bunch of errors in our service log as such ERROR: cannot load service names: Request processing failed; nested exception is com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /xxxxx:9042 (com.datastax.driver.core.exceptions.TransportException: [/1xxxx:9042] Connection has been closed),/(com.datastax.driver.core.exceptions.TransportException: [xyz/10.124.8.97:9042] Connection has been closed))\ncom.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried:  (com.datastax.driver.core.exceptions.TransportException: [test-cassandra-cloud-mot.service.consul/10.124.4.155:9042] Connection has been closed), /10.124.17.24:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [/10.124.17.24:9042] Timed out waiting for server response), /10.124.8.97:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [/10.124.8.97:9042] Timed out waiting for server response))\n    at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:84) ~[cassandra-driver-core-3.5.0-shaded.jar!/:?]\n    at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:37) ~[cassandra-driver-core-3.5.0-shaded.jar!/:?]\n    at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37) ~[cassandra-driver-core-3.5.0-shaded.jar!/:?]\n    at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245) ~[cassandra-driver-core-3.5.0-shaded.jar!/:?]\n    at zipkin2.storage.cassandra.internal.call.ResultSetFutureCall.getUninterruptibly(ResultSetFutureCall.java:74) ~[zipkin-storage-cassandra-2.9.1.jar!/:?]\n    at at zipkin2.storage.cassandra.internal.call.ResultSetFutureCall.getUninterruptibly(ResultSetFutureCall.java:74) ~[zipkin-storage-cassandra-2.9.1.jar!/:?]\n    at zipkin2.storage.cassandra.internal.call.ResultSetFutureCall$1CallbackListener.run(ResultSetFutureCall.java:50) [zipkin-storage-cassandra-2.9.1.jar!/:?]\n    at zipkin2.storage.cassandra.internal.call.DirectExecutor.execute(DirectExecutor.java:23) [zipkin-storage-cassandra-2.9.1.jar!/:?]````\nis this related to this issue?\n. Hey @adriancole we see a bunch of errors in our service log as suchERROR: cannot load service names: Request processing failed; nested exception is com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /xxxxx:9042 (com.datastax.driver.core.exceptions.TransportException: [/1xxxx:9042] Connection has been closed),/(com.datastax.driver.core.exceptions.TransportException: [xyz/10.124.8.97:9042] Connection has been closed))```\n```com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried:  (com.datastax.driver.core.exceptions.TransportException: [test-cassandra-cloud-mot.service.consul/10.124.4.155:9042] Connection has been closed), /10.124.17.24:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [/10.124.17.24:9042] Timed out waiting for server response), /10.124.8.97:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [/10.124.8.97:9042] Timed out waiting for server response))\n    at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:84) ~[cassandra-driver-core-3.5.0-shaded.jar!/:?]\n    at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:37) ~[cassandra-driver-core-3.5.0-shaded.jar!/:?]\n    at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37) ~[cassandra-driver-core-3.5.0-shaded.jar!/:?]\n    at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245) ~[cassandra-driver-core-3.5.0-shaded.jar!/:?]\n    at zipkin2.storage.cassandra.internal.call.ResultSetFutureCall.getUninterruptibly(ResultSetFutureCall.java:74) ~[zipkin-storage-cassandra-2.9.1.jar!/:?]\n    at at zipkin2.storage.cassandra.internal.call.ResultSetFutureCall.getUninterruptibly(ResultSetFutureCall.java:74) ~[zipkin-storage-cassandra-2.9.1.jar!/:?]\n    at zipkin2.storage.cassandra.internal.call.ResultSetFutureCall$1CallbackListener.run(ResultSetFutureCall.java:50) [zipkin-storage-cassandra-2.9.1.jar!/:?]\n    at zipkin2.storage.cassandra.internal.call.DirectExecutor.execute(DirectExecutor.java:23) [zipkin-storage-cassandra-2.9.1.jar!/:?]````\nis this related to this issue?\n. We are using mesos/marathon to manage our docker containers, zipkin ran fine for 15 mins or so -> then heath check starts failing. Some how the instance dies and mesos spin a new one (during this small time window  C* connections closed then those errors start happening)..\nI noticed this issue is about BusyPoolException so ours not related.. will investigate (my guess we may need to consider tuning JVM). Thank you....!. @adriancole I believe this issue is zipkin/Cassandra related, going to open a new issue. . @adriancole @michaelsembwever I believe this what we need for #2094 .. similar problem I think.. we are getting lots write timeout\n\nwe did disable the search feature because of this,  we need to visualize our data in the UI.\nI already started looking into Elastic or Kafka datastore , I will try this fix 1st.. @adriancole @michaelsembwever I believe this what we need for #2094 .. similar problem I think.. we are getting lots write timeout\n\nwe did disable the search feature because of this,  we need to visualize our data in the UI.\nI already started looking into Elastic or Kafka datastore , I will try this fix 1st.. my bad ):- we did create zipkin2-schema-indexes closing... my bad ):- we did create zipkin2-schema-indexes closing... @adriancole Just saw your message, Not even able to access our c nodes this morning.. I see a bunch of errors in c logs \n Can't open index file at /cassandra/data/zipkin2/span-15bb5b006e7111e8a8d2af46ca93ec1b/mc-2673-big-SI_span_l_service_idx.db, skipping. org.apache.cassandra.io.FSReadError: java.io.EOFException at org.apache.cassandra.index.sasi.disk.OnDiskIndex.<init>(OnDiskIndex.java:164) ~[apache-cassandra-3.9.0.jar:3.9.0] at org.apache.cassandra.index.sasi.SSTableIndex.<init>(SSTableIndex.java:68) ~[apache-cassandra-\nERROR [Reference-Reaper:1] 2018-06-14 14:46:11,044 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@74bce35c) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@2054886331:/cassandra/data/zipkin2/span-15bb5b006e7111e8a8d2af46ca93ec1b/mc-2673-big was not released before the reference was garbage collected. as of now we don't have much service names I see only 292 distinct service in the table\ntotal records are 404118\nselect count(*) from span_by_service;\ncount\n404118\n. ",
    "altfatterz": "understand. thanks Adrian\n. understand. thanks Adrian\n. ",
    "cschneider": "I am now at the stage where I can install zipkin and zipkin reporter bundles. I will now try to first get everything installed and started in OSGi and then write a first actual example that uses it.\n. Makes sense .. I also found another issue\n. ",
    "jaredstehler": "sorry, only meant this for my fork\n. sorry, only meant this for my fork\n. ",
    "cburroughs": "I'm having trouble understanding the build infrastructure enough to know what failed with circleci.  When I click on the details tab I see \"Your build ran 256 tests in junit with 0 failures\" :-/\nIf I click on container 1 (does circleci run two instances of the test script?) I see a failure in postSpans_disconnectDuringBody but in container 2  there is a failure in usesInterceptorsQualifiedWith_zipkinElasticsearchHttp.\n. Here is a shot with right aligned 'More Info'.\n\nIf I understand it right, with v2/simplified formats, having data from multiple addresses on a single span is going to become even less common, and I'm wary of dynamically mucking with the number of table columns.. My understanding of #1046 was that showing partial spans was consistently confusing to users, so just show them the whole span was hoped to be a better default.  For reference this is how it all looks back in 122bb0193ccae30353fd9683d33031836b025871\nI can search for spans by service name but not with any name (that I can tell, I wish I could!)  \n\nIf I click on a span I get taken to traces/{{traceId}}?serviceName=foo which highlights spans with that service name, but hides all other spans\n\nThis is a different view from \"Collapse All\"\n\n\"Expand All\"\n\nAnd from navigating to the trace URL directly (no query parameter) and clicking on the service name\n\nI think the best default view would be with up to 100 (or whatever limit) spans expanded. The service you searched for would be highlighted but other services dimmed, not hidden nor collapsed.  But that's not an option that exists today and I agree with #1046 (or at least what I thought the sentiment there was) that \"Expand All\" is the least confusing of the current options.  Maybe this is a subtle different in use cases between \"I am trying to understand the emergent properties of my whole system\" and \"as service owner X, I wan to know how my thing is used and can be made faster?. Sure, so if you click on a span you can now hit esc to close the dialog with the span details:\n\nAnd the same is true of the help message...\n\nThe two dependency related dialogs boxes can also be closed with esc or an x button.  (Before you had to click somewhere else to close them which was called out in #1277.)\n\n\n. On first load:\n\n. With a script filled test span I was able to search with the UI normally and no longer got alerts.\njson\n[\n  {\n    \"traceId\": \"fb0c8b7d5a4a4632a8462d044d2fffff\",\n    \"id\": \"81d0a94237ae54de\",\n    \"name\": \"<script>alert(document.domain)</script>\",\n    \"timestamp\": 1481324510534000,\n    \"duration\": 21272000,\n      \"annotations\": [\n      {\n        \"timestamp\": 1481324510709000,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"<script>alert(document.domain)</script>\"\n        }\n      },\n      {\n        \"timestamp\": 1481324510760000,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"<script>alert(document.domain)</script>\"\n        }\n      },\n      {\n        \"timestamp\": 1481324510760000,\n        \"value\": \"<script>alert(document.domain)</script>\",\n        \"endpoint\": {\n          \"serviceName\": \"<script>alert(document.domain)</script>\"\n        }\n      }\n    ],\n      \"binaryAnnotations\": [\n          {\n              \"key\": \"<script>alert(document.domain)</script>\",\n              \"value\": \"<script>alert(document.domain)</script>\"\n          }\n      ]\n  }\n]\n\n. \n\n. > oh interesting.. this is the raw json for all the results.\nWasn't that your proposal in #1143?. Could you give some details about the particular trace that is slow as a point of reference?  How many spans does it have?  Or how many KiB is the json?. Could you give some details about the particular trace that is slow as a point of reference?  How many spans does it have?  Or how many KiB is the json?. Would the actual \"badness\" detection happen on the server, or in javascript?  My initial inclination leans towards server side, but I\"m not sure if adding more fields to the API response has complications or not.. ",
    "StephenWithPH": "Thoughts on revisiting this? Using the 0.8 consumer brings with it a Zookeeper dependency; newer versions of the consumer library use Kafka itself for offset tracking.\nhttps://github.com/openzipkin/zipkin/pull/904 sticks with 0.8.x clients until we either make a standalone 0.9 kafka collector or re-evaluate when 0.9 adoption is higher... it's hard to gauge the pace of Kafka broker upgrades in the world at large. Anecdotally, half of the room at the last Kafka meetup was still on 0.8.\nThe standalone zipkin-kafka-connector approach sounds viable; it's similar to https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/connectors/kafka.html.\nWe've just started digging deep into this part of Zipkin to brainstorm how to carve out the pinned 0.8 consumer. If the above sounds viable, we'll come back with a more detailed sketch.. I'll answer the questions before diving into the details:\n\n\nwe are using Docker in prod\n\n\nwe use the factory openzipkin Docker image :heart:\n\n\nnewer versions of the Kafka consumer library write to a consumer offsets topic in Kafka itself. That's automatic... I don't believe there's any way not to do that, and no user code is needed. This Confluent blog post gives lots of details; note that the post is somewhat dated... this behavior is no longer in beta.\n\n\nnewer consumers do indeed discover all the brokers; see bootstrap.servers here.\n\n\nFrom a high level, we strongly prefer to protect our Zookeeper from all of our various Kafka consumers. That's why we want to avoid indirectly using the old style consumer via Zipkin. Quoting some docs: ZooKeeper does not scale extremely well (especially for writes) when there are a large number of offsets (i.e., consumer-count * partition-count).\nI agree that maintaining support for 0.8.x is necessary so we don't break existing users whose brokers are 0.8.x. We'd like to add support for the 0.10.x consumer (since our brokers are 0.10.x). The consumer-to-broker version limitation is still a thing, but sounds marginally less painful with 0.10.x.\nIt's likely a matter of time until 0.9.x support is a feature request from other Zipkin users. This is what lead us to link to the Apache Flink manner of handling \"pluggable\" Kafka consumer (or producer) client versions.. We do have cycles. We'll be back on this early next week.. @sammypbaird and/or @adriancole... have either of you built a Docker image from this branch? I was hoping to do that yesterday, but ran into some problems following along Zipkin & Zipkin-Docker's container build process in order to get something I could test against.\nOur live Zipkin is running in Docker (in Kubernetes), so I was hoping to do a side-by-side test atop the production Elasticsearch store so I could report results here.\nIf you have a Docker image, I'm happy to test. Either way, I'm glad to see that this looks like it will be merged and released soon.. @dgrabows let me know if I can do anything to help with the Docker work.. @dgrabows looking forward to your PR in docker-zipkin. In the interest of keeping the discussion contained, I'll add comments there. I think everything is very close. Thank you for taking this on!. Can't make package private due to the test class ZipkinElasticsearchHttpStorageAutoConfigurationTest.java... looks like there are some echoes of prior project structure. I didn't want to start refactor/rename/moving things around without checking in. Thoughts?. still need to do this. ",
    "NithinMadhavanpillai": "Hi  @adriancole ,  \n@StephenWithPH and I   were looking at building from scratch a new Kafka 10 collector implementation based on your inputs. Today found that @dgrabows has covered a lot more on this and we would like to help you two in moving this forward. Could you please let us know if we can help you folks getting this to the master. \n. done. ",
    "bsideup": "@adriancole if you think that it's fine to change your testing infrastructure right now, I can make sure that Travis will run your tests with TestContainers already :) \n. @adriancole added Travis, I see debug message Will use TestContainers Elasticsearch instance running at http://localhost:32769 and storage/elasticsearch-http tests are green:\nResults :\nTests run: 100, Failures: 0, Errors: 0, Skipped: 0\nBut I got one failure:\nusesInterceptorsQualifiedWith_zipkinElasticsearchHttp(zipkin.storage.elasticsearch.http.ZipkinElasticsearchHttpStorageAutoConfigurationTest)  Time elapsed: 0.147 sec  <<< FAILURE!\njava.lang.AssertionError: \nActual and expected have the same elements but not in the same order, at index 0 actual element was:\n  <zipkin.storage.elasticsearch.http.ZipkinElasticsearchHttpStorageAutoConfigurationTest$InterceptorConfiguration$$Lambda$3/257608605@21282ed8>\nwhereas expected element was:\n  <zipkin.storage.elasticsearch.http.ZipkinElasticsearchHttpStorageAutoConfigurationTest$InterceptorConfiguration$$Lambda$2/1636050357@36916eb0>\nWill investigate and do 2.x.x/5.x.x a bit later\n. Hey @adriancole,\nI did some major refactorings to ES-related tests, also switched Travis to run Docker+TestContainers-based mode. \nCould you please take a look? You might not agree with some of them, so let me know if you want me to change something or duplicate some classes to remove the dependency :) \n. @adriancole ok, I will remove the dependency by duplicating the code\n\nWondering if this is mostly downloading the Elasticsearch image or not\n\nI spotted this as well, but... \nTestContainers does a good logging, so it's pretty easy to verify:\n10:07:19.260 [main] INFO  o.t.d.DockerClientProviderStrategy - Found docker client settings from environment\n10:07:19.263 [main] INFO  o.t.d.DockerClientProviderStrategy - Docker host IP address is localhost\n10:07:19.287 [main] INFO  o.t.d.DockerClientProviderStrategy - Looking for Docker environment. <...>\n10:07:19.610 [main] INFO  o.testcontainers.DockerClientFactory - Connected to docker: <...>\n10:07:21.414 [main] INFO  o.testcontainers.DockerClientFactory - Disk utilization in Docker environment is 64% (10261 MB available )\nDocker environment detection took 2 seconds in total.\nImages:\n10:07:21.442 [main] INFO  \ud83d\udc33 [elasticsearch:5.0.1] - Pulling docker image: elasticsearch:5.0.1. Please be patient; this may take some time but only needs to be done once.\n10:07:35.774 [main] INFO  \ud83d\udc33 [elasticsearch:5.0.1] - Creating container for image: elasticsearch:5.0.1\n10:07:35.864 [main] INFO  \ud83d\udc33 [elasticsearch:5.0.1] - Starting container with ID: 6ec98192cd60c9540d4a8c1838d53d2c1a19339c75e4804adb89f03ea00fb315\n10:07:36.071 [main] INFO  \ud83d\udc33 [elasticsearch:5.0.1] - Container elasticsearch:5.0.1 is starting: 6ec98192cd60c9540d4a8c1838d53d2c1a19339c75e4804adb89f03ea00fb315\n10:07:36.320 [main] INFO  \ud83d\udc33 [elasticsearch:5.0.1] - Waiting for 60 seconds for URL: http://localhost:32769/\n10:07:46.462 [main] INFO  \ud83d\udc33 [elasticsearch:5.0.1] - Container elasticsearch:5.0.1 started\n5.x took 14 seconds to download. Once we migrate to Zipkin's image, it should be lower.\n10:07:48.114 [main] INFO  ?.16.1] - Pulling docker image: openzipkin/zipkin-elasticsearch:1.16.1. Please be patient; this may take some time but only needs to be done once.\n10:07:54.639 [main] INFO  ?.16.1] - Creating container for image: openzipkin/zipkin-elasticsearch:1.16.1\n10:07:54.718 [main] INFO  ?.16.1] - Starting container with ID: 6afcf6613742fd05883ec82b27017473d42d8882bcb0a03c90fcc7a52f2bd46c\n10:07:54.898 [main] INFO  ?.16.1] - Container openzipkin/zipkin-elasticsearch:1.16.1 is starting: 6afcf6613742fd05883ec82b27017473d42d8882bcb0a03c90fcc7a52f2bd46c\n10:07:54.909 [main] INFO  ?.16.1] - Waiting for 60 seconds for URL: http://localhost:32771/\n10:08:01.968 [main] INFO  ?.16.1] - Container openzipkin/zipkin-elasticsearch:1.16.1 started\nOnly 6 second for Zipkin's 2.x. Startup took 7 seconds (I think we can tune Elasticsearch a bit to make it start faster)\nSpanConsumerTest is slow itself. Right now we test both 2.x and 5.x, it means that the time was increased by test executing mostly. \nHere is a screenshot from IntelliJ IDEA with test execution times:\n\nSo I would assume that most of the time is not an overhead from TestContainers, but the second test suite for ES 5.x\nWe can configure both 2.x and 5.x on Travis, start them, and then run the tests again.Numbers will be pretty the same :)\n. @adriancole well, you can always migrate to Gradle and use their build scans ;) Gradle can also share some code without having to publish an artifact. Just saying =) \nFYI I pushed the changes. Now there is no dependency between es and es-http. \n. > Weird that testcontainers can't use mariadb\nIt can :) There is mariadb container as well. Just if we use MySQLContainer it will attempt to use MySQL driver to check if it's started. I can remove that, for sure\n\nOne docker image version... one base layer.\n\nOk, I'll attempt to use Zipkin's image one more time, shouldn't be that hard after all :). @adriancole \n\nchange description of the PR to reflect today :)\n\ndone\n\nunhook circleci as I think that's still running the old mysql manually\n\nI didn't touch CircleCI config in both Elasticsearch and MySQL PRs because I was thinking that we want to keep (for now) one of the CI services to use old testing approach without Docker, no?. @adriancole well, lack of - docker service record is a good reminder I guess :D But ok, I can make it explicit :). @adriancole added :). :+1:. @adriancole great change! IDEA setup is painful indeed. @adriancole Travis is really nice when it comes to Docker testing; I wouldn't recommend to drop it entirely. \nI can't find any reported issue, so Travis team might not be aware of it. . Hi Adrian! I'm happy to help, but can take a look only tomorrow, I hope\nit's fine :)\nOn Sat, 23 Sep 2017 at 10:19, Adrian Cole notifications@github.com wrote:\n\n@bsideup https://github.com/bsideup we are stumped as to why this fails\nin travis.. do you have some time to help?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1742#issuecomment-331619352,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABAIip9jVuCLMyhO9nN7r7Vx0yCIkmjJks5slL73gaJpZM4PeAF2\n.\n. @adriancole does it work locally if you stop your local RabbitMQ and run tests with Docker? I have a feeling that some tests use local RabbitMQ (it explains why it passes locally). A few non-func points from me:\n\n\nPulsar provides truly async client, while Kafka is coming from the blocking world\nPulsar consumer API is simpler than Kafka's (where if you don't poll you might end up de-assigned from the partition (sic!))\nRunning Kafka behind NAT is problematic, while Pulsar can be used with a proxy\nPulsar SQL is a driver for Presto, not a brand new tech from scratch by a commercial owner of the tech\nBacking up Pulsar clusters is much easier thanks to the tiered storage. No2 is an advantage of Pulsar, not a problem :) Sorry for confusing :)\n\nAlso, Kafka's publisher can block on publish, even if async API is used, while afaik there is no such issue in Pulsar. Only in the consumer part, but there are still blocking calls in Kafka's async producer :(. wouldn't be started until .start(). \nWill be automatically destroyed on JVM shutdown. \nCan be manually destroyed with container.stop().\n. read as \"map random port on my host to 9200 inside a container\". No port conflicts.\n. \"elasticsearch:5\" works as well, wasn't sure which one to use. Also, can be set dynamically, of course. Maybe two separate test suites to test different ES versions (even locally)\n. will throw if Docker is not available (but will try to find it with different strategies like Docker Machine, Docker for Mac or native Docker on linux\n. wasn't sure how to fix it, decided to use endpoint's getter. Is it fine?\n. Ok, it's just a string after all. Do you have any good ideas how to make it use both quickly? I Mean, in standard JUnit I would use Parameterized runner, for instance, but Zipkin's testing infrastructure is a bit more advanced\n. done :+1:\n. :D changed :D\n. Docker image caching is a hot topic in Travis world :) It's not supported by Travis, however you can cache it manually with \"docker export\", \"docker import\". But! Travis devs were saying that it's actually slower than pulling from Docker Hub :D \n. Problems with latest:\n1) inconsistency - test might work locally (cache), but fail on CI\n2) Minor ES release (i.e. 5.1) might break your app. Happened to us with 2.x branch :). Sorry :)\nWould be really helpful to integrate something like Eclipse Code Formatter, so it will fail if the code style is broken (the same as eslint fails on the frontend)\nI use IntelliJ IDEA and their vision of imports is a bit different :) . If the version is consistent across all images under openzipkin then we can extract some base class for all TestContainers-based storages and just pass an image name to it. Base class will append version stored as a private constant inside it.\nWDYT?. no need for it, it's a default value. compute runs once, lazily, but you call close many times (after every JUnit statement). This is why the tests are failing. @adriancole . ",
    "sammypbaird": "@adriancole , thanks for the sample elasticsearch query. I was having trouble querying the serviceName too, until I realized from your code snippet that the annotations are stored as a nested object in elasticsearch, which requires a different type of query. One question I had though was why the serviceName is stored for every annotation. I would assume serviceName is applicable to the whole span, and should be stored as a field on the span. Under what situations can you have different serviceNames for the same span?\n@dragontree101 , here's a simpler query that you can use on elasticsearch to find spans that match a specific serviceName (in this example, I'm searching for spans with a serviceName of jdbc):\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"nested\": {\n            \"query\": {\n              \"term\": {\n                \"annotations.endpoint.serviceName\": \"jdbc\"\n              }\n            },\n            \"path\": \"annotations\"\n          }\n        }\n      ]\n    }\n  }\n}\nThat is what worked for me.. Sorry, I meant the raw spans (i.e. the spans you get when you query a trace from the zipkin api and pass in the parameter \"raw\"). Each raw span looks like they always have the same serviceName (in the raw trace, the client and server belong to different spans). I would imagine that the serviceName would exist at the same level as the span name, but instead it's populated for each annotation in the raw span, which makes querying from elasticsearch more difficult. I'm sure there is a reason for this, just wondering why.. @mansu, we are experiencing the same behavior, taking 30 seconds to load the service names on Zipkin UI. We implemented the suggestion @adriancole  had to only load the last 2 days of indexes (which does improve performance considerably), but we'd prefer a solution similar to yours with caching. Did you get a chance to implement something like this? . I haven't deployed it on production yet, but on our development cluster (6 servers in a cluster), loading span names went from taking 2 seconds down to 600 ms. I'll tell you once we get the changes out to our production cluster (which has a lot more traffic) what the impact is.. I think that would be a useful option to set. I would definitely use it (all our services get hit at least once a day, so there's no need to go back further). Thanks!\nI have already changed the code locally to insert the service name at the root level, allowing us to write much simpler queries in Kibana and visualizations, and seems to also help speed up the Zipkin queries a bit. . @mansu, I finally got my change to limit the indices it searches to the last two days out to production. Loading the serviceNames went from ~30 seconds down to ~8 seconds. I'm still looking for improvements based off assumptions we can make on our data, but that helped considerably.\nWe also hope to make some hardware changes to have our elasticsearch cluster writing to SSDs instead of HDDs, which is what we currently have.. And for context, we are saving about 80 GB of data to elasticsearch a day. I'm not sure what is typical for these type of systems.. That sounds great. The only reason I chose 2 days for the indices was to not have a small sample early in the morning - 24 hours should address this. I'm not sure however how overloading this variable will impact the current system. What does it currently control and how is it currently used? As long as it doesn't restrict some other part of the system, I'm happy with that idea. I would also want it  to limit the indices for the spanName lookup too (which is also unbounded right now to all indices).. That sounds great. The only reason I chose 2 days for the indices was to not have a small sample early in the morning - 24 hours should address this. I'm not sure however how overloading this variable will impact the current system. What does it currently control and how is it currently used? As long as it doesn't restrict some other part of the system, I'm happy with that idea. I would also want it  to limit the indices for the spanName lookup too (which is also unbounded right now to all indices).. @adriancole , I just pulled the code and tested. I verified that changing the query lookback does speed up the loading of serviceNames and spanNames. Thanks!. @adriancole , I just pulled the code and tested. I verified that changing the query lookback does speed up the loading of serviceNames and spanNames. Thanks!. ",
    "haf": "@adriancole I think it would be excellent. We have a Kibana dashboard at qvitoo and frequently need to correlate logs. Logary does all of the above collecting too. I'm working through the https://github.com/logary/logary/issues and have been for a while now; the next two parts is creating a complete C# facade like the existing Logary.Facade, improving how derived metrics are configured and then create examples including ones with Zipkin in them. This would enable e.g. sending a snapshot 1 minute rolling average together with the span, together with the annotations, that are sent to Zipkin.\nHowever, Logary's API is now well geared towards the logging structure needed for Zipkin; hierarchical passing-down of loggers, applying middleware on the fly (with TimeScope), or alternatively correlating span annotations in the Logary Zipkin target. So the extra work needed to fully support Zipkin now, is very small.. So what about fsharp? Does the project have to be named by the language? Why not name it dotnet?. @fedj So you'd maintain a package I create named zipkin-fsharp separate from zipkin-csharp?. ",
    "naoman": "Sorry for the delay guys. We were busy with open sourcing our (Pinterest) Spark collector for Zipkin and couldn't share much here.\nAs part of a hackathon, we were able to modify this spark collector to build service dependency graph on the streaming data and push the graph to Vizceral, in real-time. I'll clean up the code and share it here, hopefully in next couple of weeks.\n. Thanks for reviewing the code @adriancole. I've made following changes:\n1. Moved the test code from SpanStoreTest to SpanTest\n2. Updated Span.Builder.merge() code to return timestamp and duration of client after merge. However, instead of computing it by going through annotations, I'm calling ApplyTimestampAndDuration.apply(). Since the code is so similar, I didn't want to copy paste. And later if we want to change the logic of computing ApplyTimestampAndDuration, there will be just one place to make the change. Let me know what you think.. It would be great if you can do it as part of the merge. I couldn't think of a cleaner way of doing this, since we were only looping through \"that\" annotations before the change.\nThanks!. We have added checks in our pipeline, since we ran into this issue, to make sure we keep the annotation size in control. The data that triggered this was a value from a key value store, that was exceptionally large.. This change breaks the UI if we run it behind ELB with port mapping. Our ELB listens on port 80 and backend server listens on port 9411. http://zipkin_domain.com/ now redirects to http://zipkin_domain.com:9411/zipkin/ . This change breaks the UI if we run it behind ELB with port mapping. Our ELB listens on port 80 and backend server listens on port 9411. http://zipkin_domain.com/ now redirects to http://zipkin_domain.com:9411/zipkin/ . @adriancole thanks for making the change. Also, --server.use-forward-headers=true fixed the problem. \nAdding a bit more information about our issue, in case someone else runs into same. We had two layers of proxy, one for SSL termination (NGINX) and one for load balancing (ELB). Both layers were doing port mapping, and adding X-Forwarded headers. On top of using --server.use-forward-headers=true, we had to update the ELB to stop updating X-Forwarded headers (by converting it to TCP from HTTP).. @adriancole similar to @devinsba, we don't modify the path and are not planning to any time soon. . \n\n\n\nuse v2 api as this is a new feature? Easier than having to do a migration of this to v2 later\n\nWe have not migrated to V2 yet. Our datastore is in V1 format, so we have to read via V1 api. For writing to permanent store, either V1 or V2 is fine. Looks like zipkin server writes in V2 format even if we call V1 api, which seems a bit strange.\nIdeally we should be able to run the server in V1 mode or V2 mode, and all APIs can then work accordingly. Do we have a config property that can do this?\n\ninteresting.. I was thinking this was going to happen in javascript. maybe it should? that way it doesn't need to expose a new server-side endpoint.\n\nI wanted to keep the client light. Also, if its on the server side, other clients/scripts can also use it. We can move it to javascript if needed.\n. >I don't want to make an api like this as a part of the server api, especially not under v1 namespace. it doesn't impact the UI to do the blocking here or there.\nSo you want to go for javascript solution, where client (UI) reads form one zipkin-server and saves to permanent zipkin-server? I'll make the change.\n\nIn order to deploy this UI change, you will inherit the v2 api anyway, unless you are somehow hosting static assets independently.. is that the case?\n\nNot sure what you mean. What I'm thinking is that UI/Javascript will read raw spans for a trace using /api/v1/trace/<trace_id> and post them to http://<permanent_zipkin_address>/api/v1/spans . >Ex we have a saved trace, but this UI wont be able to find saved traces.. Is that ok? If so, why? If not, why not?\nIdeally same UI should provide access to saved traces, but I think its okay to have a separate UI, if it helps keeping the system simple and maintainable. It could be considered an archiving feature, and in most systems, retrieving archived data has a separate process than active data.\nAlso, if a user is trying to access a trace thats not available in the regular datastore, the UI can share a link to archive/permanent store, or even pull trace from there. Search will be a bit tricker, but we can handle that too by picking the correct datastore based on the TTL for the regular store.. >... and so far I think archive is it. do you agree?\nYes.\n\nError message\n\nSuccess message, when zipkin.ui.archive-read-endpoint is not specified.\n\nSuccess message, when zipkin.ui.archive-read-endpoint is specified.\n\n. hum.. I messed up rebase. Added unit test and removed unnecessary dependency. The exception I was seeing earlier must be because of some setup issue.. @adriancole can you help with this ci/circleci failure? Looks like its a setup issue. \n\n. @adriancole can you help with this ci/circleci failure? Looks like its a setup issue. \n\n. @adriancole Yes, Archive button shows only when zipkin.ui.archive-endpoint is present. Similar to Logs button.. @adriancole Yes, Archive button shows only when zipkin.ui.archive-endpoint is present. Similar to Logs button.. @jorgheymans when a trace is archived, the UI will show a link to archived trace in the confirmation dialog. Screenshot attached above.\n\n@naoman just curious how many traces do you save to permanent store ? Is it a regularly occuring thing ?\n\nNot many, maybe just a few traces per day. Because of our high trace volume, regular datastore has TTL of 10 days. This feature will let users save \"interesting\" traces, specially something related to a bug, for longer duration. . @jorgheymans when a trace is archived, the UI will show a link to archived trace in the confirmation dialog. Screenshot attached above.\n\n@naoman just curious how many traces do you save to permanent store ? Is it a regularly occuring thing ?\n\nNot many, maybe just a few traces per day. Because of our high trace volume, regular datastore has TTL of 10 days. This feature will let users save \"interesting\" traces, specially something related to a bug, for longer duration. . @jorgheymans Yes, this can be useful. But at this point I want to keep the code simple and clean, since this feature will not be used by many users. In my personal experience, when archiving a trace, users will save the link to the trace itself, since they are archiving the trace to save or share it.\nI would say we should go ahead with the current code, and revisit based on user feedback.. @jorgheymans Yes, this can be useful. But at this point I want to keep the code simple and clean, since this feature will not be used by many users. In my personal experience, when archiving a trace, users will save the link to the trace itself, since they are archiving the trace to save or share it.\nI would say we should go ahead with the current code, and revisit based on user feedback.. Like I mentioned in my last message, this change is ready and fully functional. Adding a link would be an improvement on top of this change that can be made in a separate pull request. Lets merge this change, get the feature out there so that people can start using it, get user feedback, and then improve iteratively. . Like I mentioned in my last message, this change is ready and fully functional. Adding a link would be an improvement on top of this change that can be made in a separate pull request. Lets merge this change, get the feature out there so that people can start using it, get user feedback, and then improve iteratively. . >We don't merge single user stuff, so consider it lucky someone else is interested!\nNot sure what you mean. Do you think I'm trying to push something thats Pinterest specific, and not needed by other users? \nLet me try to explain one more time. While the request to add an additional url link is valid, its an add-on feature for this change, and does not block it. \nCan you please explain why these two changes can not be made in two separate pull requests?\n. >We don't merge single user stuff, so consider it lucky someone else is interested!\nNot sure what you mean. Do you think I'm trying to push something thats Pinterest specific, and not needed by other users? \nLet me try to explain one more time. While the request to add an additional url link is valid, its an add-on feature for this change, and does not block it. \nCan you please explain why these two changes can not be made in two separate pull requests?\n. @jorgheymans I've added a link to archive server on the menu bar. Let me know what you think. Below is the screenshot.\n\n\nSeems a hacky solution, most people will not have this setup. \n\n@jcarres-mdsol can you please elaborate on why you feel this solution is hacky and how it can be improved. \n\nWould the button appear in the UI by default? It may set expectations it would work when in most cases would not\n\n@jcarres-mdsol No it would not. I've added this information in Readme as well. Let me know if you think it needs more explanation there.\n. @jorgheymans I've added a link to archive server on the menu bar. Let me know what you think. Below is the screenshot.\n\n\nSeems a hacky solution, most people will not have this setup. \n\n@jcarres-mdsol can you please elaborate on why you feel this solution is hacky and how it can be improved. \n\nWould the button appear in the UI by default? It may set expectations it would work when in most cases would not\n\n@jcarres-mdsol No it would not. I've added this information in Readme as well. Let me know if you think it needs more explanation there.\n. @eirslett Yes, thats do able, however it wont be very scaleable and convenient. Consider the use case where we want to save a trace related to a bug. Without this feature, we'll first have to download the trace, attach it to the bug ticket, and upload if back to zipkin server for looking at the trace. This feature provides a convenient way to archive traces with a single click.  . @eirslett Yes, thats do able, however it wont be very scaleable and convenient. Consider the use case where we want to save a trace related to a bug. Without this feature, we'll first have to download the trace, attach it to the bug ticket, and upload if back to zipkin server for looking at the trace. This feature provides a convenient way to archive traces with a single click.  . @eirslett varnish proxy based cache will have its own limitations. Below are few:\n- what if we want to keep data for longer than cache TTL? \n- how do we enable data replication?\n- What if other tools (service dependency graph, trace analyzer scripts, etc) want to pull this data? . @eirslett varnish proxy based cache will have its own limitations. Below are few:\n- what if we want to keep data for longer than cache TTL? \n- how do we enable data replication?\n- What if other tools (service dependency graph, trace analyzer scripts, etc) want to pull this data? . > I think it is hacky because to add this new functionality, I need to double my infrastructure.\n@jcarres-mdsol Can you please elaborate on this? This solution does require some additional infrastructure setup, but by no means it would double the infrastructure. For example, below is how Elasticsearch based solution look like:\n\nA new index for archived traces in the same Elasticsearch cluster. \nA new Zipkin Server instance for archived traces. This instance can run on the same hosts running regular Zipkin Server\nA load-balancer in front of Zipkin archive server. This is the only additional infrastructure component needed for this solution.\n\nIf you still feel that this is too much hassle for archiving traces, can you propose a better solution?. I was getting this exception while posting spans to /api/v2/spans\n```\n2017-10-03 11:08:28.605 ERROR 46979 --- [nio-9412-exec-2] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Handler dispatch failed; nested exception is java.lang.NoClassDefFoundError: org/springframework/util/concurrent/SettableListenableFuture] with root cause\njava.lang.NoClassDefFoundError: org/springframework/util/concurrent/SettableListenableFuture\n    at zipkin.server.ZipkinHttpCollector.validateAndStoreSpans(ZipkinHttpCollector.java:88) ~[classes!/:na]\n    at zipkin.server.ZipkinHttpCollector.uploadSpansJson2(ZipkinHttpCollector.java:67) ~[classes!/:na]\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_101]\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_101]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_101]\n```\nThis stack overflow suggested this solution. I couldn't get it working without adding this dependency.. ",
    "xiaoshuang-lu": "Hi @adriancole, thanks your comments.\nLet's forget HTTP, Apache Thrift, and such existing things supported by zipkin.\nAssuming that there is a third-party framework. This framework has its own ID system and wanna reuse tracing functionalities of zikpin. If some traceID looks like \"a very very ... long trace id\" (ID may be more than 32 hex characters), how does zipkin handle this scenario with the prerequisite that all trace id shall be encoded to a fixed length integer?. Hi @eirslett, there is no need to convert a string ID to integer. I don't understand why zipkin does not use string directly. It makes sense and is simple.. A design defect. Someone doesn't want to use integer as \"primary key\" indeed.. ",
    "martinambrus": "hi guys, could anyone point me to the right direction as to how to convert string into a 128 bit integer, please? I've been searching for a day and still have no clue how to convert uuids to usable traceIds. thanks in advance :) we use opentracing in NodeJS, so the ideal thing would be some JS code :). the scenario I'm faced with is:\n\nfor each request, we generate a certain string which serves us as UUID in our system (so, it's not a real UUID, but our own unique ID string)\nthen, we want to convert it to a traceId\nthen, we want to use this for all spans we generate across multiple microservices as the request travels, so we can assign all those spans to the same trace. \n",
    "licheng-xd": "@ezraroi  @adriancole \nI have the sample problem where run zipkin with exec jar and elasticsearchr5.6 with docker. \nrg.springframework.web.util.NestedServletException: \nRequest processing failed; nested exception is java.lang.IllegalStateException: \nresponse for aggregation failed: \n{\"error\":{\"root_cause\":[{\"type\":\"illegal_argument_exception\",\"reason\":\"Fielddata is disabled on text fields by default. Set fielddata=true on [name] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\"}],\"type\":\"search_phase_execution_exception\",\"reason\":\"all shards failed\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[{\"shard\":0,\"index\":\"zipkin:span-2018-07-03\",\"node\":\"IqUVIo7GQJiTXx5ts4fWpg\",\"reason\":{\"type\":\"illegal_argument_exception\",\"reason\":\"Fielddata is disabled on text fields by default. Set fielddata=true on [name] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\"}}]},\"status\":400}. ",
    "rush24": "so is there anything i could do to solve it ? before changing the storage to ES, we have been using the cassandra, and the speed of putting data is more faster, but had not that problem.. thank you very much!\nso if i understand it, you want to change that two values to solve it?. i can't find that two param, would you please tell me about it. then i can have a test see if that could work . premade thanks~. ",
    "raiRaiyan": "@adriancole The ui itself is not accessible when proxied.  The index.html requests for a file /app.js non relatively. . ",
    "saturnism": "@adriancole i ran into a similar issue recently. If I hit Zipkin host from a proxied host, e.g. http://myzipkin.tld/ it'll redirect me to http://127.0.0/1/zipkin. But if I hit the context root, http://myzipkin.tld/zipkin then it's loads fine.\nIs there any chance to redirect to context w/ relative URL?. ",
    "brunouska": "I was able to make it happen with Zipkin latest version, 2.5.3, using \u201czipkin.ui.basepath\u201d. I have Zipkin behind Zuul, with service discovery working like a charm. In my Zuul routes I have mapped Zipkin to \u201c/api/trace/\u201d, pointing to \u201czipkin-service\u201d as registered in Eureka and at Zipkin YML config, my \u201czipkin.ui.basepath\u201d points to \u201c/api/trace/zipkin\u201d. With this settings I can run for \u201chttp://localhost:8080/api/trace/\u201c and all redirects works. Hope it help another people. Passed two days figuring out what to do.... I was able to make it happen with Zipkin latest version, 2.5.3, using \u201czipkin.ui.basepath\u201d. I have Zipkin behind Zuul, with service discovery working like a charm. In my Zuul routes I have mapped Zipkin to \u201c/api/trace/\u201d, pointing to \u201czipkin-service\u201d as registered in Eureka and at Zipkin YML config, my \u201czipkin.ui.basepath\u201d points to \u201c/api/trace/zipkin\u201d. With this settings I can run for \u201chttp://localhost:8080/api/trace/\u201c and all redirects works. Hope it help another people. Passed two days figuring out what to do.... @momentum123, I\u2019m accessing zipkin by Zuul exposed url. Zuul uses Service id at route mapping.. Let's go @momentum123,\nMy Spring Boot Version for my implementation is 1.5.9.RELEASE\nMy Spring Cloud Version for my implementation is Edgware.RELEASE\nHere's my Zuul routes config, at my YML:\nzuul:\n  ignored-services: \"\"\n  prefix: /api\n  routes:\n    trace:\n      path: /tracing/*\n      serviceId: bahauser-trace-server\nHere's my Zipkin Config, at my YML:\neureka:\n  instance:\n    lease-renewal-interval-in-seconds: 5\n    instance-id: ${spring.application.name}:${vcap.application.instance_id:${spring.application.instance_id:${random.value}}}\n    prefer-ip-address: true\n  client:\n    registryFetchIntervalSeconds: 5\n    region: default\n    service-url:\n      defaultZone: http://${bahauser-discovery-server.username}:${bahauser-discovery-server.password}@${bahauser-discovery-server.host}:${bahauser-discovery-server.port}/eureka/\nzipkin:\n  ui:\n    basepath: /api/tracing/zipkin\nWhen accessing Zipkin thru Zuul, i use \"http://localhost:8080/api/tracing/zipkin/\", note the \"/\" at the end, you have to put it in order to access the UI. You can handle this with a redirection if you want it, at my needing this not a issue. Try with this settings.\nYou can not get it working we could make a git repository only with the modules involved. I have so many other things, like rabbitmq, auth, so on... If you can not get working I put something at Git Hub for you.  \n. This was a little tricky for me to get working, pay attention to the minimum details at your configuration.. ",
    "momentum123": "hi@brunouska  how are u accessing zipkin thru serviceid or url.. @brunouska  i have tried the same but doesnt work for me tried on earlier version and 2.5.3 and on 2.7.1\ndoesnt work on any of them please tell me if im doing something wrong and given that im setting the property on jar invocation : java -Dzipikin.ui.basepath=/api/trace/zipkin -jar zipkin.jar\nsetting property\nzipkin.ui.basepath=/api/trace/zipkin\nzuul.routes.zipkin.path=/api/trace/\nzuul.routes.zipkin.url=http://localhost:9411\naccessing thru gateway\nhttps://localhost:443/api/trace\ngetting same broken ui without any changes \nhttps://localhost:443/zipkin\ni have tried a workaround in  #1993  works partially: ui\n. sorry for the really late reply. thanx the above solution worked like a charm for me. @adriancole  @stepanv @abesto \nzuul properties for accessing  gateway\nzuul.routes.zipkin.path=/zipkin/**\nzuul.routs.zipkin.url=http://localhost:9411/\niam adding /zipkin in host name for all the requests having context /zipkin thru following filter\n@Override       public Object run() {\n        RequestContext ctx = RequestContext.getCurrentContext();\n        HttpServletRequest request = ctx.getRequest();\n        Object serviceId=ctx.get(\"serviceId\");\n        String previousUri = request.getRequestURI();\n        Object host = ctx.getRouteHost();`\nif(!StringUtils.substringAfterLast(previousUri, \"zipkin/\").isEmpty()){\n           try {\n            ctx.setRouteHost(new URL(host+\"/zipkin\"));\n        } catch (MalformedURLException e) {\n        e.printStackTrace();\n    }\n\n   }\n\n   host = ctx.getRouteHost();\n\n      return null;\n}\n\n`\nthe filter works fine for fetching some of the ui resource not all\nattached images of resources thru gateway with above filter\ngateway\ndirect\nwithout filter\ncan u please explain y some of the resources are being accessed successfully while others are not despite \nadding  /zipkin to context or it would be better if u suggest any cleaner solution for reverse proxy\n. ",
    "guettli": "I am in the happy situation: Green fielding (we do not have tracing yet, only logging) and our servers are far from being saturated (we can handle the overhead created by tracing).\nThat's why I think, that in my situation it better to focus on tracing. I hope to get a directed acyclic graph of processes involved handling one request. \nYes, metrics are a different topic. But for me tracing and logging share a lot.. @nicmunroe for me this topic is still hot. Thank you for sharing your way. I have a question: Do all your requests have a trace-id, or do you trace only one of (for example) thousand requests (to reduce performance penalty)?\nAdding the ID in every logging output makes sense. \nSince the span-ID is more detailed than the trace-ID, wouldn't it make sense to log the span-id instead of the trace-id? You see, I am still learning :-)\n. @nicmunroe AFAIK it's easy to get the trace-ID from a span-ID. I am afraid to loose information if I would log only the trace-ID. Up to now I am still learning. At university I was told to avoid redundancy. At my current (naive) viewpoint it would be redundant to log trace-ID and span-ID. What do you think?\n . @nicmunroe AFAIK it's easy to get the trace-ID from a span-ID. I am afraid to loose information if I would log only the trace-ID. Up to now I am still learning. At university I was told to avoid redundancy. At my current (naive) viewpoint it would be redundant to log trace-ID and span-ID. What do you think?\n . @nicmunroe  I like the \"trace id vs span id vs both\" discussion. Do you have a idea where (forum, mailing-list, ..) this could be discussed with other experts?. ",
    "hwinkel": "@guettli we are in a similar situation and I would agree that logging and tracing shares a lot. The was a good talk of @bensigelman at kubecon 2016 https://youtu.be/n8mUiLIXkto?t=876 explaining logging and tracing where spans in opentracing are \"just\" log messages.  but if it comes to graphs extracted and aggregated from Log entries like ELK is doing I could imagine this could be better done with metric collection at the source and not by counting, slicing and dicing log entries. IMHO.\nhttps://twitter.com/copyconstruct/status/809239415558017024. @adriancole thanks for your statement, makes sense to me.\n. ",
    "raam86": "Thanks for the quick reply, I am using scribe, I will check and see if it helps.. After further investigation the problem seems to lie in htrace.. After further investigation the problem seems to lie in htrace.. ",
    "djoyner": "I've run into this exact issue. I'm using a cheap db.t2.micro RDS instance that is dedicated to Zipkin. I saw bursts of up to ~1350 IOPS with average write sizes of 6.1KB, quickly exhausting the IO credit balance.\nSince the Zipkin MySQL client isn't batching and probably won't be any time soon, I \"detuned\" MySQL using the innodb_flush_log_at_trx_commit parameter. RDS's default parameter group doesn't set this and unless you've overridden it in your own parameter group then MySQL will default to flushing the InnoDB log to disk after every transaction.\nSince I don't especially care about the durability of trace data I set this parameter to 2:\nWith a value of 2, the contents of the InnoDB log buffer are written to the log\nfile after each transaction commit and the log file is flushed to disk approximately\nonce per second. Once-per-second flushing is not 100% guaranteed to happen\nevery second, due to process scheduling issues. Because the flush to disk\noperation only occurs approximately once per second, you can lose up to a\nsecond of transactions in an operating system crash or a power outage.\nWith this change I saw that MySQL performs slightly larger writes but many fewer for the same workload. This was enough to make MySQL/RDS a workable solution for me.\nSee also: https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_flush_log_at_trx_commit.. ",
    "jakubhava": "Sure \ud83d\udc4d , I just need to make myself a bit familiar with tests structure. So I've been trying to code the tests, but it seems like I'll pass on it, at least for now because:\nMy time to work on this is limited and I couldn't find any test doing anything similar - ie. simulate receiving JSON at zipkin-ui, look at the created span at html level and the do bunch of asserts. All tests test some basic span attributes, but I couldn't find any tests which are actually testing if spanPanel is showing correct data. It seems more like need for integration test ? This is definitely doable, but it's more time consuming then I initially thought.\nMaybe if you can point me to some basics how to test this scenario that would be great and I could give it a second try!. Awesome, thanks! I applied the change only to binary annotation and squashed all the commits into one for nicer git history.\nThanks for support with the test, I'll be happy to review or help with it.. Cool, thanks a lot @adriancole ! I was trying to find something really complicated in writing these tests, but this is easy and great!. oki, reverting this part. ",
    "ghost": "@adriancole we've also experienced issues where by a trace with over 3,000 spans ended up crashing the chrome instance we were using to view the zipkin UI. The UI was also pretty non perform-ant prior to the crash.. @adriancole we've also experienced issues where by a trace with over 3,000 spans ended up crashing the chrome instance we were using to view the zipkin UI. The UI was also pretty non perform-ant prior to the crash.. CC'ing @garyd203 @ilutz @jonathan-lo. CC'ing @garyd203 @ilutz @jonathan-lo. CC'ing @sigerber. CC'ing @sigerber. Nice guys! That's pretty much how I would have done it :dancer: . Nice guys! That's pretty much how I would have done it :dancer: . Sorry @adriancole , late to the party, but we like this and will def use it!. Sorry @adriancole , late to the party, but we like this and will def use it!. I would imagine you can attack it in two ways. When you receive data, you sanitise it.\nand when you render it, you should be able to ask your framework to sanitise the data. What's the UI framework you're using?. I would imagine you can attack it in two ways. When you receive data, you sanitise it.\nand when you render it, you should be able to ask your framework to sanitise the data. What's the UI framework you're using?. We only found the issue to be on the span detail modal, if that helps narrow it.. I must admit I was surprised as well when our security guy found it.. Sure thing, give me about 20min. Improved but not totally fixed. When I click on a span I get two pop ups. Its less than I was getting before. The detail screen looks better now and you can tell that a few more things are sanitised:\n\n. @adriancole @eirslett , nice work. No more pop ups! :D :+1: . :shipit: , straight to prod ;). @adriancole, sorry for the lag. It certainly looks interesting. If we bump to latest zipkin, do we get it for free?. Much appreciated @adriancole . ",
    "igorwwwwwwwwwwwwwwwwwwww": "In order to reproduce the issue it would be helpful to get a gist with a JSON blob of the trace, that can be POSTed against zipkin.. Animation before:\n\nAnimation after:\n\n. Page load before:\n\nPage load after (also includes #1846):\n\n. i've pushed some changes that I believe address all the feedback given so far. one additional change is that \"showing n of k\" and the \"json\" badge are only shown when the form was submitted.\nbefore submit:\n\nafter submit:\n\n. @adriancole that would be hiding the annotations query input, right? should be quite straight forward to change in zipkin-ui.. @adriancole that would be hiding the annotations query input, right? should be quite straight forward to change in zipkin-ui.. lgtm \ud83d\udc4d . i believe the slowness in rendering may be due to #1914. it may not be related to the i18n stuff at all.\nalso, the 404s on language files were already happening in previous versions of the zipkin-ui, this is inherent in how the i18n plugin operates. so as far as i know, this is pre-existing and \"expected\" behaviour.\nthe length error on the other hand doesn't look good. it looks like it might be due to a missing key in the language file (based on the code here https://github.com/jquery-i18n-properties/jquery-i18n-properties/blob/master/jquery.i18n.properties.js#L150). this is a bit hard to tell without a reproduce case and the ability to dig deeper. though if the error does not cause the UI to be unusable, I don't think it is absolutely critical.. sorry about that, i was unaware that the lack of serviceName results in such a slow query against elasticsearch. i'd be curious to see if that can be optimized, but reverting that part of the patch as a short term fix sounds good to me. \ud83d\udc4d . @saddampojee should be fixed in https://github.com/openzipkin/zipkin/pull/1929.. @saddampojee should be fixed in https://github.com/openzipkin/zipkin/pull/1929.. ",
    "kmalecki": "I think that the only solution is to generate trace id by a module for apache and add header to request. Unfortunately there is no available module, that can generate such thing.\nThank You All for tips.\nI will not close this issue yet, maybe I or someone else will find something else.. ",
    "mpetazzoni": "@adriancole, @fedj, any progress made on dealing with clock skew when spans come from different hosts? Even when using ntpd, the clocks are very close, but evidently not close enough:\n\nWhat's the best way to deal with this when deploying distributed tracing?. ",
    "bvillanueva-mdsol": "@adriancole what does Absent mean in \"kind\"? What kind of use case it stands for? Will it mean that we put 'Absent' as the value or put nothing at all?. @adriancole what does Absent mean in \"kind\"? What kind of use case it stands for? Will it mean that we put 'Absent' as the value or put nothing at all?. @adriancole We were talking about async traces with @jcarres-mdsol . Is it safe to assume that if finishTimestamp is empty with kind as Client, an async process is being started with that span?. @adriancole We were talking about async traces with @jcarres-mdsol . Is it safe to assume that if finishTimestamp is empty with kind as Client, an async process is being started with that span?. Thanks for the clarification @adriancole . Thanks for the clarification @adriancole . ",
    "bplotnick": "IMO, I think it's fine to have restrictions on names for the new format as long as we are explicit and clear about violations. It seems reasonable to disallow dots entirely in the keys for a json format. We can easily change the default annotations in pyramid_zipkin.\nOut of curiosity, would this also fail if you had a keys like \"foo\" and \"foo.bar\"?. Is this solved by https://github.com/openzipkin/zipkin#disabling-search now?. I'm not opposed to this, but 1) i'm not sure about the name and 2) i don't know if it's necessary\nI'm not sure about the name, because I don't know if a template (assuming we're talking URI templates here) is sufficient. Here's an example:\nWe have this exact problem with our swagger RPC endpoints. Using the petstore example, GET /pet/{petId} will show up as /pet/1234 in the http.uri. So 1) it is difficult to know that that corresponds to the /pet/{petId} path template, and 2) it is impossible to know that it corresponds to the GET version of that vs the DELETE or POST versions.\nWe solve the second problem by making the span name equal to the method and the path in pyramid_zipkin. However, this doesn't solve the path parameter problem (the uri template).\nThe Correct Way to solve this would be to use any identifier guaranteed to uniquely identify the endpoint, which in the swagger case is operationId. I think that this should just be set as the span name as I think @SergeyKanzhelev is alluding to. This is why I'm saying that it is probably not necessary.. I'm not opposed to this, but 1) i'm not sure about the name and 2) i don't know if it's necessary\nI'm not sure about the name, because I don't know if a template (assuming we're talking URI templates here) is sufficient. Here's an example:\nWe have this exact problem with our swagger RPC endpoints. Using the petstore example, GET /pet/{petId} will show up as /pet/1234 in the http.uri. So 1) it is difficult to know that that corresponds to the /pet/{petId} path template, and 2) it is impossible to know that it corresponds to the GET version of that vs the DELETE or POST versions.\nWe solve the second problem by making the span name equal to the method and the path in pyramid_zipkin. However, this doesn't solve the path parameter problem (the uri template).\nThe Correct Way to solve this would be to use any identifier guaranteed to uniquely identify the endpoint, which in the swagger case is operationId. I think that this should just be set as the span name as I think @SergeyKanzhelev is alluding to. This is why I'm saying that it is probably not necessary.. @adriancole Apologies for not responding to your comment and thank you for continuing with this regardless. What you proposed and implemented is perfect. You're absolutely right that pyramid provides this and I'll add this into pyramid_zipkin asap. I can't wait to delete some hacks \ud83d\ude04. @adriancole Apologies for not responding to your comment and thank you for continuing with this regardless. What you proposed and implemented is perfect. You're absolutely right that pyramid provides this and I'll add this into pyramid_zipkin asap. I can't wait to delete some hacks \ud83d\ude04. I think the logic behind having a /swagger.json endpoint for your server is that without this, how do you know where to find the swagger spec? You know for sure that the swagger spec that you download from the server is correct for that API if you get it directly from the same location as where you are going to make API calls.\nI can't find any specific language in the swagger docs about this, but it seems to be convention (e.g. https://github.com/swagger-api/swagger-core/wiki/Swagger-Core-JAX-RS-Project-Setup-1.5.X). We use https://github.com/striglia/pyramid_swagger for this.\nYou hinted to one factor, which is that you could have two different versions of the API and then you would want to create a different client depending on which one you are talking to. For example, if you were talking to an old version of the Zipkin API and you used a newer swagger spec, you might think that you could use some newer endpoint, but that was added later. In practice, I haven't seen this crop up, but I can imagine it could\nThe main advantage I have personally found is just for development. It's pretty useful to just point https://github.com/Yelp/bravado to the swagger spec and start talking to the API.. ",
    "ahxm": "I think it is really necessary that detailed steps  to run zipkin in windows environment.\nI am not running  java jar zipkin-server.jar \nI use IntelliJ IDEA to run ZipkinServer.main(). \nlocalhost:9411/config.json is OK\nlocalhost:9411/health is OK\nlocalhost:9411/info is OK\nlocalhost:9411/metrics is OK\nlocalhost:9411/ is error\nthere is the errors from the console:\n2017-01-23 11:05:32.000 ERROR 4712 --- [io-9411-exec-10] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet dispatcherServlet threw exception\njava.io.FileNotFoundException: class path resource [zipkin-ui/index.html] cannot be resolved to URL because it does not exist\n    at org.springframework.core.io.ClassPathResource.getURL(ClassPathResource.java:187) ~[spring-core-4.3.5.RELEASE.jar:4.3.5.RELEASE]\n    at org.springframework.core.io.AbstractFileResolvingResource.contentLength(AbstractFileResolvingResource.java:151) ~[spring-core-4.3.5.RELEASE.jar:4.3.5.RELEASE]\n    at org.springframework.http.converter.ResourceHttpMessageConverter.getContentLength(ResourceHttpMessageConverter.java:99) ~[spring-web-4.3.5.RELEASE.jar:4.3.5.RELEASE]\n    at org.springframework.http.converter.ResourceHttpMessageConverter.getContentLength(ResourceHttpMessageConverter.java:50) ~[spring-web-4.3.5.RELEASE.jar:4.3.5.RELEASE]\n    at org.springframework.http.converter.AbstractHttpMessageConverter.addDefaultHeaders(AbstractHttpMessageConverter.java:260) ~[spring-web-4.3.5.RELEASE.jar:4.3.5.RELEASE]\n    at org.springframework.http.converter.AbstractHttpMessageConverter.write(AbstractHttpMessageConverter.java:205) ~[spring-web-4.3.5.RELEASE.jar:4.3.5.RELEASE]\n    at org.springframework.web.servlet.mvc.method.annotation.AbstractMessageConverterMethodProcessor.writeWithMessageConverters(AbstractMessageConverterMethodProcessor.java:247) ~[spring-webmvc-4.3.5.RELEASE.jar:4.3.5.RELEASE]\n    at org.springframework.web.servlet.mvc.method.annotation.HttpEntityMethodProcessor.handleReturnValue(HttpEntityMethodProcessor.java:203) ~[spring-webmvc-4.3.5.RELEASE.jar:4.3.5.RELEASE]\n    at org.springframework.web.method.support.HandlerMethodReturnValueHandlerComposite.handleReturnValue(HandlerMethodReturnValueHandlerComposite.java:81) ~[spring-web-4.3.5.RELEASE.jar:4.3.5.RELEASE]\n    at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:132) ~[spring-webmvc-4.3.5.RELEASE.jar:4.3.5.RELEASE]\n    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827) ~[spring-webmvc-4.3.5.RELEASE.jar:4.3.5.RELEASE]\n    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738) ~[spring-webmvc-4.3.5.RELEASE.jar:4.3.5.RELEASE]\n    at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.3.5.RELEASE.jar:4.3.5.RELEASE]\n    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:963) [spring-webmvc-4.3.5.RELEASE.jar:4.3.5.RELEASE]\n    at \nthen I  query the readme document in zipkin-ui,there is a step:\nIn another terminal, launch the zipkin UI server:\nDo this in another terminal!\n$ cd zipkin-ui\n$ proxy=http://localhost:9411 ./npm.sh run dev\nbut I don't know how to execute \"proxy=http://localhost:9411 ./npm.sh run dev\"  command at windows. I found a lot of people have run zipkin local success,I am not sure what environment they did. Maybe they all run in a Linux environment. ",
    "zxeoc": "To people who got the same issue, I think the reason was that @ahxm  forgot to run mvn compile in intellij. And don't forgot to make sure npm install (which include in the mvn compile) was successful.. To people who got the same issue, I think the reason was that @ahxm  forgot to run mvn compile in intellij. And don't forgot to make sure npm install (which include in the mvn compile) was successful.. ",
    "ImFlog": "+1. Hello,\nTo create the dependency graph when not in memory you have to use the spark job.\nRead the documentation, you should find your way easily.\nHope it helps.. This seems to be the easiest thing to do.. This seems to be the easiest thing to do.. Could it also be a way to migrate from a Span V1 => V2 in the same storage ?\nOr any breaking change in general (even if it only happens once a in a very while) ?. Could it also be a way to migrate from a Span V1 => V2 in the same storage ?\nOr any breaking change in general (even if it only happens once a in a very while) ?. Good initiative ! Some remarks :\n- The \"JSON\" button is out of the box.\n- I agree with @hexchain about the width\n- I kinda liked the fact that the \"Showing x of x ...\" was under the search part, just above the results.\nMy 2 cents. My client is using Spring Cloud discovery to register Zipkin instances in Zookeeper.. My client is using Spring Cloud discovery to register Zipkin instances in Zookeeper.. Exactly : @EnableDiscoveryClient with the right configuration.. If we can't turn off the 2.X, we could provide a switch via a property for example to use the old workaround ?. If we can't turn off the 2.X, we could provide a switch via a property for example to use the old workaround ?. ",
    "mbarbon": "I added the extra test as suggested (not pushed yet).\nWRT Node.MISSING_ROOT: it can't be a statically-allocated node instance (because childs are added to it); I can make it a sentinel value passed to the TreeBuilder, and then user code can check with node.value() == MISSING_ROOT.\nSpecifically regarding == MISSING_ROOT: I'm getting a warning that maybe I don't mean reference equality. Given I do, how do you prefer for me to suppress the warning?\nOr I can add an isFakeRoot() accessor to the node.\nLet me know which solution you prefer.. Sorry it took a while, I had other stuff to do at work.\nIn the end I did not go with the sentinel (using an Object subclass as the sentinel works, but it would make code like Span foo = node.value() throw a ClassCast exception, unless you special-case the sentinel in value() to be returned as null).. ",
    "tomgithub2016": "ok\u3002. ```\n$ git log C:/zipkin/zipkin-guava/src/main/java/zipkin/storage/guava/\ncommit 7cd7b687aebe52db2923fdd0a7d827df1940ca8d\nAuthor: Adrian Cole adriancole@users.noreply.github.com\nDate:   Sat Nov 12 12:26:26 2016 +0800\nSupports querying traces with mixed-length trace IDs (#1385)\n\nUnless you only issue 128-bit traces when all applications support them,\nthe process of updating applications from 64 to 128-bit trace IDs results\nin a mixed state. This mixed state is mitigated by the setting\n`STRICT_TRACE_ID=false`, explained below. Once a migration is complete,\nremove the setting `STRICT_TRACE_ID=false` or set it to true.\n\nSupporting changes are:\n\n`SpanStore.getTrace(long traceIdHigh, long traceIdLow)`\n`StorageComponent.Builder.strictTraceId(boolean strictTraceId)`\n\ncommit 27686c6ec9bed77e254c2b0d527db25897d0e205\nAuthor: Adrian Cole adriancole@users.noreply.github.com\nDate:   Sun Jul 31 10:39:30 2016 -0700\nUpdates versions, most notably spring boot 1.4 (#1211)\n\ncommit 0635ffbc848e7ed351055205f21918d7e3bcf860\nAuthor: Adrian Cole acole@pivotal.io\nDate:   Fri May 13 09:27:13 2016 +0800\nFiles storage components under zipkin.storage package\n\nThis moves storage components under the zipkin.storage package.\n\nBy doing so, we clear the main `zipkin` package for classes used by all\ncode, including instrumentation. Also, it helps with logging, as storage\nrelated operations share the same category.\n\n.\n$ git log C:/zipkin/zipkin-guava/src/main/java/zipkin/storage/guava/\ncommit 7cd7b687aebe52db2923fdd0a7d827df1940ca8d\nAuthor: Adrian Cole adriancole@users.noreply.github.com\nDate:   Sat Nov 12 12:26:26 2016 +0800\nSupports querying traces with mixed-length trace IDs (#1385)\n\nUnless you only issue 128-bit traces when all applications support them,\nthe process of updating applications from 64 to 128-bit trace IDs results\nin a mixed state. This mixed state is mitigated by the setting\n`STRICT_TRACE_ID=false`, explained below. Once a migration is complete,\nremove the setting `STRICT_TRACE_ID=false` or set it to true.\n\nSupporting changes are:\n\n`SpanStore.getTrace(long traceIdHigh, long traceIdLow)`\n`StorageComponent.Builder.strictTraceId(boolean strictTraceId)`\n\ncommit 27686c6ec9bed77e254c2b0d527db25897d0e205\nAuthor: Adrian Cole adriancole@users.noreply.github.com\nDate:   Sun Jul 31 10:39:30 2016 -0700\nUpdates versions, most notably spring boot 1.4 (#1211)\n\ncommit 0635ffbc848e7ed351055205f21918d7e3bcf860\nAuthor: Adrian Cole acole@pivotal.io\nDate:   Fri May 13 09:27:13 2016 +0800\nFiles storage components under zipkin.storage package\n\nThis moves storage components under the zipkin.storage package.\n\nBy doing so, we clear the main `zipkin` package for classes used by all\ncode, including instrumentation. Also, it helps with logging, as storage\nrelated operations share the same category.\n\n``. I want to download the latest source code  to modify and expand using In out project,\nSo,How to operate can  Compile successfully. I want to download the latest source code  to modify and expand using In out project,\nSo,How to operate can  Compile successfully. You mean if with Linux and modify the local source code,there will be compile successfully?\n. You mean if with Linux and modify the local source code,there will be compile successfully?\n. I change to Linux,andgit clone \"https://github.com/openzipkin/zipkin/git\"If I do not modify any file,compile successful\nIf I modify any files compile failed.\nIf I understand is wrong, should be usegit checkout`?\nI only want to download the source code, modify the source code, and compile successful \nHow to do?\n```\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Guava support library 1.20.2-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ zipkin-guava ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /root/zipkin/zipkin-guava/src/main/resources\n[INFO] \n[INFO] --- maven-compiler-plugin:3.6.0:compile (default-compile) @ zipkin-guava ---\n[INFO] Changes detected - recompiling the module!\n[INFO] Compiling 10 source files to /root/zipkin/zipkin-guava/target/classes\n\u6ce8: /root/zipkin/zipkin-guava/src/main/java/zipkin/storage/guava/InternalGuavaToAsyncSpanStoreAdapter.java\u4f7f\u7528\u6216\u8986\u76d6\u4e86\u5df2\u8fc7\u65f6\u7684 API\u3002\n\u6ce8: \u6709\u5173\u8be6\u7ec6\u4fe1\u606f, \u8bf7\u4f7f\u7528 -Xlint:deprecation \u91cd\u65b0\u7f16\u8bd1\u3002\n[INFO] \n[INFO] --- license-maven-plugin:3.0:check (default) @ zipkin-guava ---\n[INFO] Checking licenses...\n[WARNING] Missing header in: /root/zipkin/zipkin-guava/src/main/java/zipkin/storage/guava/GuavaStorageAdapters.java\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 2.229 s\n[INFO] Finished at: 2017-03-09T21:28:21+08:00\n[INFO] Final Memory: 28M/266M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal com.mycila:license-maven-plugin:3.0:check (default) on project zipkin-guava: Some files do not have the expected license header -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n``. I change to Linux,andgit clone \"https://github.com/openzipkin/zipkin/git\"If I do not modify any file,compile successful\nIf I modify any files compile failed.\nIf I understand is wrong, should be usegit checkout`?\nI only want to download the source code, modify the source code, and compile successful \nHow to do?\n```\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Guava support library 1.20.2-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ zipkin-guava ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /root/zipkin/zipkin-guava/src/main/resources\n[INFO] \n[INFO] --- maven-compiler-plugin:3.6.0:compile (default-compile) @ zipkin-guava ---\n[INFO] Changes detected - recompiling the module!\n[INFO] Compiling 10 source files to /root/zipkin/zipkin-guava/target/classes\n\u6ce8: /root/zipkin/zipkin-guava/src/main/java/zipkin/storage/guava/InternalGuavaToAsyncSpanStoreAdapter.java\u4f7f\u7528\u6216\u8986\u76d6\u4e86\u5df2\u8fc7\u65f6\u7684 API\u3002\n\u6ce8: \u6709\u5173\u8be6\u7ec6\u4fe1\u606f, \u8bf7\u4f7f\u7528 -Xlint:deprecation \u91cd\u65b0\u7f16\u8bd1\u3002\n[INFO] \n[INFO] --- license-maven-plugin:3.0:check (default) @ zipkin-guava ---\n[INFO] Checking licenses...\n[WARNING] Missing header in: /root/zipkin/zipkin-guava/src/main/java/zipkin/storage/guava/GuavaStorageAdapters.java\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 2.229 s\n[INFO] Finished at: 2017-03-09T21:28:21+08:00\n[INFO] Final Memory: 28M/266M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal com.mycila:license-maven-plugin:3.0:check (default) on project zipkin-guava: Some files do not have the expected license header -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n```. @adriancole  thank you.. @adriancole  thank you.. ",
    "fryckbos": "I am also looking for this functionality. I want to look for all traces that went through a certain container. So searching on IPv4 literals using annotation.endpoint.ipv4 makes a lot a sense to me.. ",
    "fan807017570": "Thank you for your reply\uff0cYes, when I visit the zipkin home page query service name, elasticsearch without any response, it will not print the log, and through the ps command I can see the elasticsearch is still alive.Eager to get your opinion\u3002. I have the same problem with you,and I come from china.Do you have solve thi problem?. I have the same problem with you,and I come from china.Do you have solve thi problem?. ",
    "harsharastogi": "+1. ",
    "pauldraper": "This works when I use in-memory store.\nI tried this with the example data from the documentation (http://zipkin.io/pages/data_model.html) and had the same result with the cassandra store: no matter what the min duration, the trace still appeared.. ",
    "Coalla": "When do you suppose to merge this?. ",
    "denyska": "Probably easiest way to test conditional logic is to enhance some of the Cassandra integration tests if any. I will take a look.. Probably easiest way to test conditional logic is to enhance some of the Cassandra integration tests if any. I will take a look.. There is no need for standalone test, it is covered by existing integration E2E test: zipkin.server.ZipkinServerIntegrationTest#writeSpans_updatesMetrics. agree, can be removed.. oh, makes sense. I thought you meant whether ActuateCollectorMetrics gets instantiated at all with default configs. Unit test is probably OK, but might not cover use cases with multiple Collectors present. Alternatively we can make ActuateCollectorMetrics completely independent and always instantiate CounterBuffers/GaugeBuffers in the ActuateCollectorMetrics's constructor.\n. Test case added. ",
    "praveenbarli": "@adriancole  I debugged throught the code and I see that span count on search details page is calculated as \"Sum of  (Unique services (part of annotation and binary annotation endpoints) in each span ) \". As this is happeing at UI end the Span's are merged ones coming from API. However I didn't understand why it is called Span count. I would describe this count as sum of unique events across spans.. Thanks Adrian! I\u2019ll pull latest.\nFrom: Adrian Cole [mailto:notifications@github.com]\nSent: Wednesday, April 19, 2017 3:24 AM\nTo: openzipkin/zipkin zipkin@noreply.github.com\nCc: Praveen Barli prbarl@microsoft.com; Mention mention@noreply.github.com\nSubject: Re: [openzipkin/zipkin] Span count of same trace is showing differently between Search page and trace details page in UI (#1544)\nthis should be fixed now.. try latest (currently 1.23.0)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fopenzipkin%2Fzipkin%2Fissues%2F1544%23issuecomment-295205595&data=02%7C01%7Cprbarl%40microsoft.com%7C3e47750fb3a342ca9b5a08d4870e338e%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636281942450246989&sdata=pgN%2FcJ%2FS%2Ba7z3OplezrSy0wvoU%2Fcvv0FOA%2F4NLSSx8M%3D&reserved=0, or mute the threadhttps://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAVEG7u5swmkX4o_m4MWC5gBj92yXTUqQks5rxeDCgaJpZM4MnyVZ&data=02%7C01%7Cprbarl%40microsoft.com%7C3e47750fb3a342ca9b5a08d4870e338e%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636281942450256992&sdata=rY98XABtmlWtjW7CglQzGLL74C5BP9GoGnPO94uRqr8%3D&reserved=0.\n. ",
    "vitaliydrebotif": "StackTrace:\njava.lang.NoSuchMethodError: zipkin.internal.MergeById.apply(Ljava/lang/Iterable;)Ljava/util/List;\n    at zipkin.storage.cassandra3.CassandraSpanStore$AdjustTrace.apply(CassandraSpanStore.java:257) ~[zipkin-storage-cassandra3-1.21.0.jar:na]\n    at zipkin.storage.cassandra3.CassandraSpanStore$AdjustTrace.apply(CassandraSpanStore.java:253) ~[zipkin-storage-cassandra3-1.21.0.jar:na]\n    at com.google.common.util.concurrent.Futures$2.apply(Futures.java:760) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.Futures$ChainingListenableFuture.run(Futures.java:906) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.AbstractFuture.set(AbstractFuture.java:185) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.Futures$ChainingListenableFuture$1.run(Futures.java:918) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:106) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.Futures$ChainingListenableFuture.run(Futures.java:914) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.AbstractFuture.set(AbstractFuture.java:185) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.Futures$ChainingListenableFuture$1.run(Futures.java:918) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:106) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.Futures$ChainingListenableFuture.run(Futures.java:914) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145) ~[guava-18.0.jar:na]\n    at com.google.common.util.concurrent.AbstractFuture.set(AbstractFuture.java:185) ~[guava-18.0.jar:na]\n    at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:174) ~[cassandra-driver-core-3.1.4.jar:na]\n    at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:177) ~[cassandra-driver-core-3.1.4.jar:na]\n    at com.datastax.driver.core.RequestHandler.access$2500(RequestHandler.java:46) ~[cassandra-driver-core-3.1.4.jar:na]\n    at com.datastax.driver.core.RequestHandler$SpeculativeExecution.setFinalResult(RequestHandler.java:799) ~[cassandra-driver-core-3.1.4.jar:na]\n    at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:502) ~[cassandra-driver-core-3.1.4.jar:na]\n    at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1070) ~[cassandra-driver-core-3.1.4.jar:na]\n    at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:993) ~[cassandra-driver-core-3.1.4.jar:na]\n    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266) ~[netty-handler-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) ~[netty-codec-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293) ~[netty-codec-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:267) ~[netty-codec-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1280) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:890) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:564) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:505) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:419) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:391) ~[netty-transport-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:112) ~[netty-common-4.0.37.Final.jar:4.0.37.Final]\n    at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:145) ~[netty-common-4.0.37.Final.jar:4.0.37.Final]\n    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]\n. Sorry for disturbing you, I simply added whole library 'io.zipkin.java:zipkin:1.21.0' and it fixed my problem. ",
    "maryoush": "The proper cql could look like - AA arbitrary TTL in seconds\nALTER TABLE zipkin.traces\n    WITH compaction = {'class': 'org.apache.cassandra.db.compaction.DateTieredCompactionStrategy', 'max_window_size_seconds': 'AA'}\n    AND gc_grace_seconds = AA\n    AND default_time_to_live =  AA;\n. would that support multiple credentials ?. ",
    "felixbarny": "Nice!. Right. The overflow should be handled just like in the JSON panel.. ",
    "ruinanchen": "@adriancole Thank you very very much, I solve the problem.. ",
    "yzfshuijing": "I can not understand what 60 means?. @adriancole thanks so much for you explanation, i'v got it \nand i'v the last question \nint discountedAverage = discountedAverage(measurements, discountRate)\nWhere there is a specific description of the algorithm called 'discountedAverage' \nit's difficult to understand it \n. @adriancole \noh, i got it , it's a good way to solve the problem you mentioned above \nthanks for your help again!. these days i used the way of discountedAverage(), i found it just like forgetting factor\nwe should make sure the first value be big enough, just you test case you given (1000, 5, 0)\nexcept the first value, the other values have a low influence on the average result\n@adriancole . these days i used the way of discountedAverage(), i found it just like forgetting factor\nwe should make sure the first value be big enough, just you test case you given (1000, 5, 0)\nexcept the first value, the other values have a low influence on the average result\n@adriancole . @jcarres-mdsol \nI means is there any test report about the qps using the httpcollector, so i can conclude the qps in my envirment\nafter all, the datas reported to the collector may be very large per second\nthanks. @adriancole . @MrGlaucus thanks\nif I want to getDependencies \uff0cI should execute STORAGE_TYPE=elasticsearch ES_HOSTS=host1:9201 java -jar zipkin-dependencies.jar\nis there a way to make zikin-server and zikpin-dependencies work together \uff1fnot to execute the command everytime\uff1f. ok\uff0cthank you so much @MrGlaucus . ",
    "strizhov": "thanks for response. \nI have tried SELF_TRACING_ENABLED=true and received internal traces, but UI stopped reporting any new traces after about 20 mins run.\nI also tried sql command from README file (after about 30 min zipkin collector run), but it didn't return anything. \nI am going to try cassandra storage at this point.\n. ",
    "necolas": "Hi, as far as Twitter Lite goes, we didn't transition but started from scratch. @rondevera might know of some internal apps that are transitioning from FlightJS to React. ",
    "dmitry-prokopchenkov": "Thanks!. ",
    "dgrabows": "It took a lot longer than I was hoping, but I've got something working with the docker images.\nSee https://github.com/dgrabows/docker-zipkin/tree/kafka10-collector and the comments on the commit https://github.com/dgrabows/docker-zipkin/commit/02470624be97b2b5108b91c2b138be96c1a7d160.\nI don't expect any of that to be in a final state, but it's a (more or less) working starting point for discussion on how this should work, what to do about upgrading the kafka container image to 0.10, and how to deal with differences between (at least on Macs) Docker Toolbox and Docker for Mac.\nI'll try to write up a cohert PR tonight for those changes in docker-zipkin to allow for more detailed feedback.\n@StephenWithPH Any feedback on the direction those docker changes are going would be greatly appreciated.. Pull request for the initial attempt a docker changes is now created.\nhttps://github.com/openzipkin/docker-zipkin/pull/142. That last commit address my final outstanding todo. Assuming the tests pass (they did for me locally) and there are no concerns from anyone else, I think this is good to go.. Alright. Rebase and version catch-up are complete. Assuming the tests pass like they did locally, I think that's all the outstanding items.. Thank you.. This change make sense to me.\nI don't think the ConsumerRebalanceListener is a concern. That was already there and not introduced with this change. It gets used for the worker to maintain a list of currently assigned partitions. That list is only used in the tests for multiple consumer threads, where the test needs to wait until each consumer has some assignment before proceeding.\nThe listener will be invoked each time partitions are reassigned in the consumer group, which will only happen once per restart if running a single instance of the collector. If running multiple collectors, invocations will occur whenever a collector is started, stopped, or looses connectivity to the kafka brokers. Those should be relatively infrequent events.. This is the autoconfigure readme. :)\nI can change the title of the page to make that clearer.. JRE7 is the minimum requirement for using kafka-clients, including version 0.10. I copied the JRE8 requirement from the zipkin-server readme.\nThere's no reason collector-kafka10 can't be JRE 7 compatible. I'll check the code to see if there's anything Java 8 specific (other than labmdas, which I'm assuming retrolambda will take care of).. I think I'll need to replace the forEach() call here to maintain Java 7 compatibility.. There is one use of forEach() in KafkaCollectorWorker that needs to be replaced (see comment further down).. I went back and forth on this a few times. It seems awkard. But, without it, the Kafka consumers will rarely get closed gracefully, because a thread interrupt is the primary reason the polling loop will be terminated and the kafka consumer will throw an exception if the thread has the interrupted status set.\nClosing the consumer isn't strictly necessary, although it can reduce the delay before the consumer group rebalances to account for the consumer being gone.\nYour reaction is enough to tip me towards taking it out, since it was a 50/50 decision to begin with. I will do that.. Done.. Done.. I ended up leaving this public, because ZipkinKafkaCollectorPropertiesOverrideTest needed access to both this method and the package-private fields on KafkaCollector.Builder. This seemed better than adding public getters for each of those fields.. Done in newly added ZipkinKafkaCollectorPropertiesOverrideTest.. I updated this to clarify that zipkin-server and the kafka10 collector have difference java/JRE requirements.. I did change the page title and make some other edits to this readme and the readme for the collector module to try and clarify this.. Done.. Done.. Done.. I switched this to wait 1 second.. Done.. Done.. Moved the comment and removed this method.. The call to Thread.interrupted() is gone now.. Done.. Done.. I ran out of steam before getting to this. If anybody feels strongly about using autovalue here, let me know and I will circle back around to it.. I've rebased the kafka10-collector branch in my local repo and committed the version updates to these two pom files. It won't push cleanly, because my local branch and the one on github have diverged. I suspect I can force the push to resolve this (`git push -f origin +kafka10-collector'), but I'm wary of doing something that rewrites history and screws up the PR.\nAm I heading in the right direction, or did I not execute the rebase properly?\nCurrent state of affairs on my local clone of the repo:\ndgrabowski:zipkin$ git status\nOn branch kafka10-collector\nYour branch and 'origin/kafka10-collector' have diverged,\nand have 37 and 25 different commits each, respectively.\n  (use \"git pull\" to merge the remote branch into yours)\nnothing to commit, working tree clean\ndgrabowski:zipkin$ git push\nTo github.com:dgrabows/zipkin.git\n ! [rejected]        kafka10-collector -> kafka10-collector (non-fast-forward)\nerror: failed to push some refs to 'git@github.com:dgrabows/zipkin.git'\nhint: Updates were rejected because the tip of your current branch is behind\nhint: its remote counterpart. Integrate the remote changes (e.g.\nhint: 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\ndgrabowski:zipkin$. I made some changes to the readme to create a section for the HTTP collector.. As far as the naming or existence of the environment variable, I'm happy to leave it in or take it out (of the docs and zipkin-server-shared.yml). The one argument I can think of in favor of keeping it is support for disabling the HTTP collector for users of the openzipkin/zipkin docker image. Either way won't affect my particular situation, because I am already configuring other aspects of zipkin-server via Java system properties. . ",
    "a-roberts": "+1, looking for this too, defaults aren't particularly useful if you wanted to do a quick \"zero to hero\" demonstration - fiddling with defaults would be a necessary first step.\nWhile modifying this (I propose just setting it to include anything from the previous hour as well, like with Prometheus) it would be great to order by newest first as well, and just having a limit of 10 is really quite small.... ",
    "zhanglc": "Yup.... the /health will return {'MYSQL':DOWN} if there is no 'zipkin' database . the CheckResult metho should checked with user settings.\nie:\nstart with the line:\nSTORAGE_TYPE=mysql MYSQL_HOST=192.168.1.231 MYSQL_TCP_PORT=3306 MYSQL_DB=zipkin_test MYSQL_USER=zipkin_user  nohup java -jar zipkin.jar &\nthe CheckResult method should check zipkin_test but not zipkin. ",
    "ezraroi": "yes, i was searching for the template to fix it there, but couldn't find it. thanks, just for the record.. the \"zipkin_template\" field is not needed. This solved the issue. I will report the logs form the branch.. ",
    "ronperei": "is there any other way of creating zipkin 2 server other than building zipkin jar\n. is there any other way of creating zipkin 2 server other than building zipkin jar\n. thanks @shakuzen ..\nI have downloaded the source from github.also have build the zipkin jar and I am able to have zipkin server .\nBut i need to have custom requirements specific to  my project like adding discovery client to zipkin.\nSo is it possible to have custom open zipkin server?\n. thanks @shakuzen ..\nI have downloaded the source from github.also have build the zipkin jar and I am able to have zipkin server .\nBut i need to have custom requirements specific to  my project like adding discovery client to zipkin.\nSo is it possible to have custom open zipkin server?\n. ",
    "semyonslepov": "Wow, that should be superhelpful! We had some problems with AWS EC2 soon and were like blind kittens trying to understand what's going on. With this debug option it would be much easier.. How will it work if there is an index with the same date in old and new storage? Scan through both for traceId until the first match is found?\nP.S. In general sounds good for me.. How can we move this stuff to \"try\" block if we do this removing only on fail (maybe I'm missing something)?. Or you are talking about moving filling cache outside of putServiceSpans()? Messed a bit by comments (:\nIn this case I agree, don't see any reason for doing this job for each span too.... This particular one was reordering.\nWe need to make some changes in the tests here, because servicespan cache changes behaviour: some tests expect adding of some servicespan pairs in request to ES, but the purpose of this cache is not to create existing pairs twice and old tests can't work anymore. Some existing tests have been changed for several reasons:\n\nrelated functionality (\"servicespan\" caching) eliminates repetitive writing of the same \"servicespan\" pairs to ES, therefore breaking some tests expectations\ntests order is important in a case of \"servicespan\" caching, so reordered existing tests in the file for better readability. \n",
    "cemo": "@adriancole is this attractive only for me? DynamoDB based on storage means less cost and a better scalability. Am I missing something at here? Or Should I open this issue on zipkin-aws?. ",
    "Crevil": "As requested, here is a ping @adriancole :). That was fast! Good job and thank you for doing this! \ud83d\ude03. ",
    "nickHsiung": "\nlooks like a change here: https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/java/zipkin/server/ZipkinServerConfiguration.java#L84 wanna try (also adding a unit test)?\n\npage not found.. ",
    "swankjesse": "So our ConnectionPool tries it's best to minimize the number of connections. This would create more?. Another strategy: have your own queue that sits in front of the dispatcher and does its own prioritization and enqueueing.\nGetting signaled when dispatcher\u2019s size changes may be tricky.. ",
    "gquintana": "In the mean time, consider raising the log level of this trace from Debug to Error to help people understand what's wrong:\n2019-03-18T10:12:41,155 DEBUG reporter.AsyncReporter$BoundedAsyncReporter - Dropped 0 spans due to IllegalArgumentException(Only http supported with multiple hosts [https://host1:9200, https://host2:9200, https://host3:9200])\njava.lang.IllegalArgumentException: Only http supported with multiple hosts [https://host1:9200, https://host2:9200, https://host3:9200]\n        at zipkin2.elasticsearch.PseudoAddressRecordSet.create(PseudoAddressRecordSet.java:66) ~[zipkin-storage-elasticsearch-2.12.5.jar!/:?]\n        at zipkin2.elasticsearch.ElasticsearchStorage.http(ElasticsearchStorage.java:372) ~[zipkin-storage-elasticsearch-2.12.5.jar!/:?]\n        at zipkin2.elasticsearch.AutoValue_ElasticsearchStorage.http(AutoValue_ElasticsearchStorage.java:46) ~[zipkin-storage-elasticsearch-2.12.5.jar!/:?]\n        at zipkin2.elasticsearch.ElasticsearchStorage.ensureIndexTemplates(ElasticsearchStorage.java:350) ~[zipkin-storage-elasticsearch-2.12.5.jar!/:?]\n        at zipkin2.elasticsearch.AutoValue_ElasticsearchStorage.ensureIndexTemplates(AutoValue_ElasticsearchStorage.java:31) ~[zipkin-storage-elasticsearch-2.12.5.jar!/:?]\n        at zipkin2.elasticsearch.ElasticsearchStorage.spanConsumer(ElasticsearchStorage.java:260) ~[zipkin-storage-elasticsearch-2.12.5.jar!/:?]\n        at zipkin2.server.internal.brave.TracingStorageComponent.spanConsumer(TracingStorageComponent.java:49) ~[classes!/:?]\n        at zipkin2.storage.StorageComponent$$FastClassBySpringCGLIB$$37dbd580.invoke(<generated>) ~[zipkin-2.12.5.jar!/:?]. In the mean time, consider raising the log level of this trace from Debug to Error to help people understand what's wrong:\n2019-03-18T10:12:41,155 DEBUG reporter.AsyncReporter$BoundedAsyncReporter - Dropped 0 spans due to IllegalArgumentException(Only http supported with multiple hosts [https://host1:9200, https://host2:9200, https://host3:9200])\njava.lang.IllegalArgumentException: Only http supported with multiple hosts [https://host1:9200, https://host2:9200, https://host3:9200]\n        at zipkin2.elasticsearch.PseudoAddressRecordSet.create(PseudoAddressRecordSet.java:66) ~[zipkin-storage-elasticsearch-2.12.5.jar!/:?]\n        at zipkin2.elasticsearch.ElasticsearchStorage.http(ElasticsearchStorage.java:372) ~[zipkin-storage-elasticsearch-2.12.5.jar!/:?]\n        at zipkin2.elasticsearch.AutoValue_ElasticsearchStorage.http(AutoValue_ElasticsearchStorage.java:46) ~[zipkin-storage-elasticsearch-2.12.5.jar!/:?]\n        at zipkin2.elasticsearch.ElasticsearchStorage.ensureIndexTemplates(ElasticsearchStorage.java:350) ~[zipkin-storage-elasticsearch-2.12.5.jar!/:?]\n        at zipkin2.elasticsearch.AutoValue_ElasticsearchStorage.ensureIndexTemplates(AutoValue_ElasticsearchStorage.java:31) ~[zipkin-storage-elasticsearch-2.12.5.jar!/:?]\n        at zipkin2.elasticsearch.ElasticsearchStorage.spanConsumer(ElasticsearchStorage.java:260) ~[zipkin-storage-elasticsearch-2.12.5.jar!/:?]\n        at zipkin2.server.internal.brave.TracingStorageComponent.spanConsumer(TracingStorageComponent.java:49) ~[classes!/:?]\n        at zipkin2.storage.StorageComponent$$FastClassBySpringCGLIB$$37dbd580.invoke(<generated>) ~[zipkin-2.12.5.jar!/:?]. About the question \"which HTTP client library?\". I suggest using the official Elasticsearch REST low level client\nhttps://www.elastic.co/guide/en/elasticsearch/client/java-rest/current/java-rest-low.html\nIt supports HTTPS and is able to auto discover new nodes using Elasticsearch cluster API.. About the question \"which HTTP client library?\". I suggest using the official Elasticsearch REST low level client\nhttps://www.elastic.co/guide/en/elasticsearch/client/java-rest/current/java-rest-low.html\nIt supports HTTPS and is able to auto discover new nodes using Elasticsearch cluster API.. Also occurs in UI:\nCaused by: java.lang.IllegalStateException: response for update-template failed: {\"error\":{\"root_cause\":[{\"type\":\"parse_exception\",\"reason\":\"Failed to parse content to map\"}],\"type\":\"parse_exception\",\"reason\":\"Failed to parse content to map\",\"caused_by\":{\"type\":\"json_parse_exception\",\"reason\":\"Unexpected character ('z' (code 122)): was expecting comma to separate Object entries\\n at [Source: org.elasticsearch.transport.netty4.ByteBufStreamInput@6ffd0b34; line: 2, column: 24]\"}},\"status\":400}\n        at zipkin2.elasticsearch.internal.client.HttpCall.parseResponse(HttpCall.java:151) ~[zipkin-storage-elasticsearch-2.12.0.jar!/:?]\n        at zipkin2.elasticsearch.internal.client.HttpCall.execute(HttpCall.java:79) ~[zipkin-storage-elasticsearch-2.12.0.jar!/:?]\n        at zipkin2.elasticsearch.EnsureIndexTemplate.apply(EnsureIndexTemplate.java:42) ~[zipkin-storage-elasticsearch-2.12.0.jar!/:?]\n        at zipkin2.elasticsearch.ElasticsearchStorage.ensureIndexTemplates(ElasticsearchStorage.java:351) ~[zipkin-storage-elasticsearch-2.12.0.jar!/:?]\n        at zipkin2.elasticsearch.AutoValue_ElasticsearchStorage.ensureIndexTemplates(AutoValue_ElasticsearchStorage.java:31) ~[zipkin-storage-elasticsearch-2.12.0.jar!/:?]\n        at zipkin2.elasticsearch.ElasticsearchStorage.spanStore(ElasticsearchStorage.java:248) ~[zipkin-storage-elasticsearch-2.12.0.jar!/:?]\n        at zipkin2.server.internal.ZipkinQueryApiV2.getServiceNames(ZipkinQueryApiV2.java:89) ~[classes!/:?]\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_121]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]\n        at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:189) ~[spring-web-5.1.3.RELEASE.jar!/:5.1.3.RELEASE]\n        at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:138) ~[spring-web-5.1.3.RELEASE.jar!/:5.1.3.RELEASE]\n        at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:102) ~[spring-webmvc-5.1.3.RELEASE.jar!/:5.1.3.RELEASE]\n        at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895) ~[spring-webmvc-5.1.3.RELEASE.jar!/:5.1.3.RELEASE]\n        at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:800) ~[spring-webmvc-5.1.3.RELEASE.jar!/:5.1.3.RELEASE]\n        at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87) ~[spring-webmvc-5.1.3.RELEASE.jar!/:5.1.3.RELEASE]\n        at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1038) ~[spring-webmvc-5.1.3.RELEASE.jar!/:5.1.3.RELEASE]\n        at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:942) ~[spring-webmvc-5.1.3.RELEASE.jar!/:5.1.3.RELEASE]\n        at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1005) ~[spring-webmvc-5.1.3.RELEASE.jar!/:5.1.3.RELEASE]\n        ... 84 more. It was not a bug, I had a double quote in ES_INDEX environment variable, as a result JSON to Elasticsearch was invalid.\n. @adriancole That's it. @adriancole That's it. About logging, I also found this code strange:\nLOG.info(\"Closing Kafka consumer...\");\n      LOG.info(\"Kafka consumer closed.\");\nIt gives the feeling the KafkaConsumer is close between the 2 logs, which is not the case.. About logging, I also found this code strange:\nLOG.info(\"Closing Kafka consumer...\");\n      LOG.info(\"Kafka consumer closed.\");\nIt gives the feeling the KafkaConsumer is close between the 2 logs, which is not the case.. I'll have a look and see if I can fix it.. I'll have a look and see if I can fix it.. ",
    "cforce": "No idea where I gets thumbs here \ud83d\udc4d. ",
    "nheitz": "Definitely interested...this is key channel for us that needs to be properly traced. +1. ",
    "qq494679975": "well\uff0ci use elasticsearch 2.4.4\u00b7\u00b7Is there any other way?. ",
    "xeraa": "That works (now even with 1.26.1). Thanks a lot!. That works (now even with 1.26.1). Thanks a lot!. @adriancole multiple options:\n\nReindex API: As long as you are using Elasticsearch 5.0+ and can use a query to address the right documents that's the easiest solution. You can also use scripting to change the documents:\n\nPOST _reindex\n{\n  \"source\": {\n    \"index\": \"source_index\",\n    \"query\": {\n      \"match\": {\n        \"foo\": \"bar\"\n      }\n    }\n  },\n  \"dest\": {\n    \"index\": \"destination_index\"\n  },\n  \"script\": {\n    \"inline\": \"if (ctx._source.foo == 'bar') {ctx._version++; ctx._source.remove('foo')}\",\n    \"lang\": \"painless\"\n  }\n}\n\n\nIf you need something more complex, we normally rely on Logstash with Elasticsearch as the input and output and a filter with the selection and transformation in the middle.. @adriancole multiple options:\n\n\nReindex API: As long as you are using Elasticsearch 5.0+ and can use a query to address the right documents that's the easiest solution. You can also use scripting to change the documents:\n\n\nPOST _reindex\n{\n  \"source\": {\n    \"index\": \"source_index\",\n    \"query\": {\n      \"match\": {\n        \"foo\": \"bar\"\n      }\n    }\n  },\n  \"dest\": {\n    \"index\": \"destination_index\"\n  },\n  \"script\": {\n    \"inline\": \"if (ctx._source.foo == 'bar') {ctx._version++; ctx._source.remove('foo')}\",\n    \"lang\": \"painless\"\n  }\n}\n\nIf you need something more complex, we normally rely on Logstash with Elasticsearch as the input and output and a filter with the selection and transformation in the middle.. Nice work \ud83d\udc4d . The only minor concern is that you're tripling the number of shards; though as long as the retention is short it shouldn't be a real issue.\n\nWith a retention of 7 days and the default settings you will end up with 7 days * 3 indices * 5 shards * 2 replicas = 210 shards and each one of them has a certain overhead of memory, file handles,...\nIn some scenarios it makes more sense to add a custom type field and to apply filters on that (that's also the migration suggestion). But I'm leaning towards 3 indices here as well.. The only minor concern is that you're tripling the number of shards; though as long as the retention is short it shouldn't be a real issue.\nWith a retention of 7 days and the default settings you will end up with 7 days * 3 indices * 5 shards * 2 replicas = 210 shards and each one of them has a certain overhead of memory, file handles,...\nIn some scenarios it makes more sense to add a custom type field and to apply filters on that (that's also the migration suggestion). But I'm leaning towards 3 indices here as well.. Since you are using your own client this won't be directly usable, but the bulk processor of the Elasticsearch Java client has a configurable BackoffPolicy.\nPS: The new high-level REST Java client has just been released with 5.6.0.. @adriancole there is the Profile API, which I would try first.\nThere is also the graphical profiler based on that API. That one is in X-Pack Basic, so you will need to install X-Pack into Elasticsearch and Kibana and then you can either use the 30 day trial or get the free basic license for extended periods.\nWould be interesting to see the graphical breakdown where you are spending your time and then you could start digging further how / where to improve.. Follow up:\nI'm running this as a Spring Boot application and the following dependencies have been working for nearly a year now:\n\"io.zipkin.java:zipkin:${zipkinVersion}\",\n\"io.zipkin.java:zipkin-server:${zipkinVersion}\",\n\"io.zipkin.java:zipkin-autoconfigure-storage-elasticsearch-http:${zipkinVersion}\",\n\"io.zipkin.java:zipkin-autoconfigure-ui:${zipkinVersion}\"\n\nThe trick with 2 different versions works, but exchanging the first line also fixed the problem:\n\"io.zipkin.zipkin2:zipkin:${zipkinVersion}\",\n\"io.zipkin.java:zipkin-server:${zipkinVersion}\",\n\"io.zipkin.java:zipkin-autoconfigure-storage-elasticsearch-http:${zipkinVersion}\",\n\"io.zipkin.java:zipkin-autoconfigure-ui:${zipkinVersion}\"\n\nAnd thanks again for all the help, @adriancole . Yeah, they asked me that question on Gitter directly as well and that's how I started investigating. And I really like the feature :). > These outputs are all common components, and AFAIK the filebeat only can config one output method. In production environment a various of the log file should be collected, so couldn't specify the output is the zipkin api.\nI'm not sure I fully understand that sentence. Though I get the general appeal of just sending out information once and then using it multiple times \u2014 for example for logs and traces.\nSince you already seem to have Filebeat -> Kafka our general approach would then be Logstash to get the data to Elasticsearch for logging and you might be able to forward the data to Zipkin for tracing as well. There is a community plugin for a Logstash to Zipkin output; while it seems to be unfinished that would be a good approach in general. We see it pretty much as a feature that the Beats only have a single output (at the moment) and for any further distribution and splitting you'd use Logstash \u2014 optionally with a queue in between.. > These outputs are all common components, and AFAIK the filebeat only can config one output method. In production environment a various of the log file should be collected, so couldn't specify the output is the zipkin api.\nI'm not sure I fully understand that sentence. Though I get the general appeal of just sending out information once and then using it multiple times \u2014 for example for logs and traces.\nSince you already seem to have Filebeat -> Kafka our general approach would then be Logstash to get the data to Elasticsearch for logging and you might be able to forward the data to Zipkin for tracing as well. There is a community plugin for a Logstash to Zipkin output; while it seems to be unfinished that would be a good approach in general. We see it pretty much as a feature that the Beats only have a single output (at the moment) and for any further distribution and splitting you'd use Logstash \u2014 optionally with a queue in between.. Background: https://github.com/elastic/elasticsearch/issues/23892. I might be reading https://github.com/elastic/elasticsearch/pull/26247/files wrong, but this should only apply to the creation of new indices in 7.0+. So you can keep using the old pattern for reads / searches, but it will indeed need a migration path for new indices and searching the combination of old and new indices.\nWe might not be aware how common colons are in index names, so this is definitely good feedback. With the collision for cross cluster search I'd assume that we want to make this switch, but maybe we can ease the migration path.\nHackish workaround: Configurable separator and existing indices get an alias to conform to the new pattern (though that was a 30s thought process at 3am, so this might totally not work for various reasons).. Leading _ are not allowed for index names. Unfortunately we don't have whitelisted characters in our docs ATM, but both - and _ are very widely used, so I have high hopes that those will stay around \ud83d\ude05 . Leading _ are not allowed for index names. Unfortunately we don't have whitelisted characters in our docs ATM, but both - and _ are very widely used, so I have high hopes that those will stay around \ud83d\ude05 . 1. I assume this can't be easily fixed with alias trickery, right? 2019-01-01 pointing to 2019-01-01-00 and you switch that every hour. As long as the alias is pointing to a single index you can write to it. Pointing to multiple indices makes it read-only.\n2. Probably the better approach long-term is a rollover index where you can specify a certain age or number of docs or size. I'd generally go for size so you have a very even distribution of data per shard (otherwise weekends might be oversharded and a peak during the week undersharded). Also note that we will very soon have Index Lifecycle Management (ILM) built into Elasticsearch and Kibana, which will make the management of rollover indices and deleting old data much simpler. Though it's under the (free) Basic license and not Apache2 \u2014 not sure if that is acceptable to be used in Zipkin then.. @adriancole 6.6 (the current version): https://www.elastic.co/guide/en/elasticsearch/reference/6.6/index-lifecycle-management.html\nYou can fully managed it through the Elasticsearch API, but Kibana also provides a UI for it. And as I said: Not open source but free to use (Basic license).. @adriancole 6.6 (the current version): https://www.elastic.co/guide/en/elasticsearch/reference/6.6/index-lifecycle-management.html\nYou can fully managed it through the Elasticsearch API, but Kibana also provides a UI for it. And as I said: Not open source but free to use (Basic license).. 2.x is really old and I don't have a Windows machine at hand, so I cannot try it out, but I have a hunch: We used the index name as the folder name in Elasticsearch's data directory. I think with 5.0 this was changed to use a UUID to avoid any issues with file systems and special naming patterns in them (like calling an index .., etc). Is : generally not allowed on Windows and you are trying to create a folder with such a name?\nFrom Elasticsearch's point of view the colon will only be disallowed in 7.0 and you can see the current rules for disallowed characters in https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java#L162-L188.. The visualization actually looks like this:\n\nBut if you think this could be confusing with Kibana 3 I'll try to find another way to visualize it.. ",
    "danielkwinsor": "I was figuring a request for tests would come :)  I definitely will do it when I can fit it between my heavy work queue.  My only concern is that area didn't look easy to mock, so I don't know when I'll be able to get to it.\nManual testing did show this worked out with single or multiple topics, but I didn't test things like whitespace, etc.. All good... in the process of adding docs and tests, but rebased from\nupstream and it wiped out my commit, then auto closed.  Expect by EoD.\nOn Jun 28, 2017 11:15, \"Adrian Cole\" notifications@github.com wrote:\n\nAll good or did I scare you off? :)\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1622#issuecomment-311743424,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACsTUM4tjnDnj6bkqY538y__XJPAoc0Eks5sIphUgaJpZM4N-oBx\n.\n. I spent too long on the unit test and gave up.  It's much too hard to mock something coming from a constructor without powermockito.  Here's the code plus docs.. \n",
    "neilstevenson": "@adriancole zipkin-ui/js/config.js tries to load from the old URL. I've already started, I shouldn't need help. If you get volunteers, get them to pick something else off the wishlist to progress that. PR to follow soon hopefully.. I have an implementation in https://github.com/neilstevenson/zipkin/tree/hazelcast. All tests pass, but I don't think it's quite right, so I'll have to add some more to the unit tests too :-)\n. Pragmatically, how would you like to proceed ?\nA PR to a branch on the main repo, then start addressing, or address in my repo first ?\nEither way, I guess we'd need a list of mandatory and optional suggestions. ",
    "JodeZer": "This is interesting. However, l have no idea how to impl span model on influxdb. What's your design on metrics?. ",
    "goller": "@adriancole Great!  I'll take a look at it to get some ideas for modeling data.  I know it would be an effort to maintain an additional backend to zipkin, but, we at Influx are walking the same open source path and are very willing to stay involved.. @JodeZer I have some ideas about how to implement the span model on influxdb.  Now, this is really rough idea yet.  @adriancole 's link to appdash may very well change my ideas, but here we go:\nThe span's time would be start time. \nAn individual span would be stored with these tags (indexed):\n traceID\n spanID\n spanName\n serviceName\n parentID\n annotation key\nThe span's fields (not indexed) would be:\n duration\n end time\nI'm not sure yet about binary annotations...\nIn reviewing the QueryRequest.java I think this design would cover most queries.\nAs for cardinality, I'm planning on leaning on InfluxDB's new TSI engine that allows us to store and query over a billion unique series.\n. @adriancole we have been working on various schemas for the queries in SpanStore.    \n\ngetTrace and getRawTrace\nsql\n  SELECT * from zipkin where \"trace_id\"='2623801863023620058'\ngetServiceNames\n    ```sql\n    show tag values with key=\"service_name\"\n    ````\ngetSpanNames\n    ```sql\n    show tag values with key=\"name\" where \"service_name\"='myservice'\n    ````\n\ngetDependencies\nsql\n    select count(\"duration\") from zipkin where time > now() - 30m and time < now() group by \"id\",\"parent_id\", time(1d)\nBecause getDependencies aggregates the call counts between parent and child link I have it as a tag.\n\n\ngetTraces \n    This has a bit of logic to turn QueryRequest into InfluxQL.  Here is some pseudo-code:\n```c\nquery = \"select * from zipkin where service_name=%s and name=%s and time > %d\"\n\n\nif (annotations)\n  foreach:\n      query += \"and annotation_key=%s and annotation_value=%s\", key, value\nif (duration) \n      query += \"and duration > %d and duration < %d\"\nquery += \"limit %d order by time DESC\"\n```\n. @adriancole Great ty!\nTags in influx mean data that will be indexed for fast lookup (https://docs.influxdata.com/influxdb/v1.2/concepts/schema_and_data_layout/#encode-meta-data-in-tags) \nTags also have special optimized query functions to look them up (e.g. SHOW TAGS)\nAs for the getDependencies, I'll work on a better query.  Thanks for the tips \ud83d\udc4d . Hey @adriancole I've been hacking around in jaeger here: https://github.com/uber/jaeger/compare/master...influxdata:master#diff-d3419a852db652ac429192d6bd54262a\nin order to support telegraf's zipkin collector plugin\n(I'm far more comfortable in go than java!)\nI'm pretty happy with the queries at this point and will update our Influx branch here : https://github.com/openzipkin/zipkin/compare/master...influxdata:feature/influx-store\nRegarding span2 with my telegraf refactor today (https://github.com/influxdata/telegraf/pull/3150) , it should be straightforward to add another codec.. Hey @adriancole, I think I understand ! \nAre you saying that the storage format should be span2 because the zipkin queries would work against span1 and span2, thus, I should use span2?\n. Hi @adriancole, nope, it's just that I'm a one man show on the tracing front right now.  We've delayed this work as we work towards our 1.4 release of InfluxDB.  1.4 will have much better support for very high cardinality, but, it's taking a while to get it stable.  \nIt is certainly my plan to continue to work integrating influxdb and zipkin.  Regarding the v2 vs v1 model in telegraf, I very much want to support that as well.. Hi @adriancole, nope, it's just that I'm a one man show on the tracing front right now.  We've delayed this work as we work towards our 1.4 release of InfluxDB.  1.4 will have much better support for very high cardinality, but, it's taking a while to get it stable.  \nIt is certainly my plan to continue to work integrating influxdb and zipkin.  Regarding the v2 vs v1 model in telegraf, I very much want to support that as well.. @adriancole awesome ! thanks for the review!  I'll try to fix up on these this weekend.. I like the mix option because from a UI point-of-view one is looking for a specific service regardless if it is local or remote.  \nI think it is better that there is a UI that allows one to \"move up\" towards parent spans or \"down\" to children.. that's the go dev in me... I'll change that to milliseconds.. good point.  I'll change this to tag.  This was from our previous notions of binary annotations.. I'll change this as well.  I was trying to decrease cardinality a small bit by combining the ip and port together into one string.. Do you think we need to have different tags for both local and remote service names?  That'll add more cardinality to the database.  Do the queries need to specify a difference between local and remote?. Having parent set to id allows efficient querying for relationships for dependencies.\nThe query to find all relationships (and aggregate them server-side) is:\nsql\nSELECT count(duration_us) FROM zipkin.autogen.zipkin WHERE annotation = '' GROUP BY id, parent_id, service_name, time(1d) fill(none)\nIf we set parent_id to id I can get all the relationships in one query.  If parent_id is not set then I'm not able to get back the root span in the count.\nI'll add a comment about this for sure.. ",
    "gianarb": "Here some info about the new TSI mentioned by @goller https://www.influxdata.com/path-1-billion-time-series-influxdb-high-cardinality-indexing-ready-testing/. @adriancole InfluxDB is a time series database and the idea is to have it as a backend for both because we think it can be a valuable way to store traces efficiently. As @goller said we are working hard on making it more efficient for this kind of data, plus we are not very java-oriented people. That's why we are using both. It probably creates a bit of confusion and we are sorry about this. But you know how it works :D At some point everything will be ready.\nBtw as you said if there is some java dev happy to help here let us know! . @adriancole InfluxDB is a time series database and the idea is to have it as a backend for both because we think it can be a valuable way to store traces efficiently. As @goller said we are working hard on making it more efficient for this kind of data, plus we are not very java-oriented people. That's why we are using both. It probably creates a bit of confusion and we are sorry about this. But you know how it works :D At some point everything will be ready.\nBtw as you said if there is some java dev happy to help here let us know! . @adriancole Telegraf is modular, we can write a new plugin called zipkin2 at some point. This is not a problem at all. We wrote the plugin because we were looking to build a data flow to test InfluxDB with traces and for us was more comfortable to write a telegraf plugin because we know the code better.\n\nbefore you guys dropped off the face of the earth\n\nWe spoke internally about this and I agree with you, we created a small chaos but only because we are really engaged with tracing. I am sorry about that.\nWhat I am trying to say is that Telegraf is not related to this issue, what I would like to have is influxdb as backend in zipkin. Do you think we should re-start the integration with zipkinv2? People that are using zipkin now will be able to use the influxdb backend in a easy way (just updating zipkin and configuring it properly) ? Or the migration path from zipkin1 to zipkin2 is more complicated?\nThanks!\n. @adriancole Telegraf is modular, we can write a new plugin called zipkin2 at some point. This is not a problem at all. We wrote the plugin because we were looking to build a data flow to test InfluxDB with traces and for us was more comfortable to write a telegraf plugin because we know the code better.\n\nbefore you guys dropped off the face of the earth\n\nWe spoke internally about this and I agree with you, we created a small chaos but only because we are really engaged with tracing. I am sorry about that.\nWhat I am trying to say is that Telegraf is not related to this issue, what I would like to have is influxdb as backend in zipkin. Do you think we should re-start the integration with zipkinv2? People that are using zipkin now will be able to use the influxdb backend in a easy way (just updating zipkin and configuring it properly) ? Or the migration path from zipkin1 to zipkin2 is more complicated?\nThanks!\n. Ok, thank you for your clarification. At this point, we can speak internally about how to proceed in order to open a PR here with the new influxdb backend for zipkin2.. Ok, thank you for your clarification. At this point, we can speak internally about how to proceed in order to open a PR here with the new influxdb backend for zipkin2.. Great, thanks. At the moment my personal idea is to have a influxdb storage up and running in openzipkin. As I said previously the telegraf plugin was for us an easy way to validate our InfluxDB performs with traces and cardinality. We will keep it updated and as best as we can but it's not in the scope of this issue :+1: . Great, thanks. At the moment my personal idea is to have a influxdb storage up and running in openzipkin. As I said previously the telegraf plugin was for us an easy way to validate our InfluxDB performs with traces and cardinality. We will keep it updated and as best as we can but it's not in the scope of this issue :+1: . Thanks @adriancole ! I merged what @goller did around InfluxDB queries. I think now the next step is to hydrate what we get from influxdb with what zipkin requires.. Hello! \nWe are all busy these days I would like to write down what we did and what we need to do on this PR to allow other people to work a bit on it if they have time.\nAll the setup for influxdb-store and influxdb-autoconfigure is done. There are modules and they are taking configurations and so on. If you start the zipkin-server.jar with STORAGE_TYPE=influxdb it will look for an influxdb running on localhost:8086.\nYou can start one using Docker:\ndocker run -p 8086:8086 -i influxdb\nThe storage module has queries to support:\n\nwrites\nget spans/traces\nget span/trace\ndependencies\n\nIt should be almost everything there. We tried to remove the @Ignore from integration tests but they are not working. Probably some queries are not right or something else.\nThat's what it's still missed. Why integration tests are failing. As soon as we know it we should be able to fix what is wrong.. All done! . Thanks @drolando !. I like it @igorwwwwwwwwwwwwwwwwwwww ! Thanks. InfluxDB client supports batching\n// Flush every 2000 Points, at least every 100ms\ninfluxDB.enableBatch(2000, 100, TimeUnit.MILLISECONDS);\nDo you think is it something that we can add to zipkin? . Perfect, let's keep this thinks simple and we can add more complexity if we need it. done. Hello @fmachado thanks for your feedback. Cardinality is something that we are aware of. With the new TSI we will handle a high cardinality and trace_id needs to be indexed because it is used as lookup value during a lot of operation. Tracing from a storage point of view is a complicated and big uses cases :+1: . We discussed a long time ago with @adriancole about this and I think it's good as it is atm. Thanks @fmachado . That test is working \n./mvnw -Dlicense.skip=true -Dit.test='ITInfluxDBStorage$BabyStepsTest' -pl zipkin-storage/influxdb clean verify. ",
    "wdalmut": ":+1: for me. ",
    "eacdy": "@ImFlog @adriancole Thans a lot. . ",
    "flier": "Hi @adriancole,\nThanks for your advice, I'm open to use either zipkin-cpp or zipkin-cpp-opentracing, if it could provide enough features that we are using, such as native C API, kafka collector etc. \nWe maintains zipkin-cpp and rust-zipkin because the exists library doesn't meet our requirements. We glad to fix issues and accept PR for those projects, because our internal projects depend on it. \nIf @tedsuo like to sync up API design, I think we may extract a thin API layer (C and C++), such as a zipkin-cpp-api project, as the official project, and maybe a zipkin-cpp-spec project to define scenes with test cases.. ",
    "jprateekvmware": "So in that case we need to have a fix in zipkin dependencies, as it showing dependencies from M1 -->M3, when some of the M2 traces have got dropped , it should take into account the parent span id of M3 (which is m2 span id) and whether it exist in system , before defining the dependencies, else it gets a wrong picture projected . Done\nWill track as part of it\nhttps://github.com/openzipkin/zipkin-dependencies/pull/75. I will be there, Thanks adrian. Publishing the JSON payload.\nDependencies is captured as api-gateway--->M1-service--> central-service [Which is wrong]\nwhereas when i track the dependency tree based on parent id its perfect \napi-gateway--> central-service--->M1-service\njson\n[\n  {\n    \"traceId\": \"787a41fdfb6d59cb\",\n    \"name\": \"http:/api/v1/testworflow\",\n    \"id\": \"fe70d9705ea17279\",\n    \"parentId\": \"94aa5c0443cc5582\",\n    \"timestamp\": 1499719237112500,\n    \"duration\": 6099000,\n    \"annotations\": [],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"http.host\",\n        \"value\": \"incoming.url.com\",\n        \"endpoint\": {\n          \"serviceName\": \"central-service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 3008\n        }\n      },\n      {\n        \"key\": \"http.method\",\n        \"value\": \"POST\",\n        \"endpoint\": {\n          \"serviceName\": \"central-service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 3008\n        }\n      },\n      {\n        \"key\": \"http.path\",\n        \"value\": \"/api/v1/testworflow\",\n        \"endpoint\": {\n          \"serviceName\": \"central-service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 3008\n        }\n      },\n      {\n        \"key\": \"http.url\",\n        \"value\": \"http://incoming.url.com:9090/api/v1/testworflow\",\n        \"endpoint\": {\n          \"serviceName\": \"central-service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 3008\n        }\n      },\n      {\n        \"key\": \"lc\",\n        \"value\": \"unknown\",\n        \"endpoint\": {\n          \"serviceName\": \"central-service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 3008\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"787a41fdfb6d59cb\",\n    \"name\": \"http:/myapp/api/v1/testworflow\",\n    \"id\": \"787a41fdfb6d59cb\",\n    \"timestamp\": 1499719237125000,\n    \"duration\": 6102000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1499719237125000,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"api-gateway\",\n          \"ipv4\": \"2.2.2.2\",\n          \"port\": 9090\n        }\n      },\n      {\n        \"timestamp\": 1499719243227000,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"api-gateway\",\n          \"ipv4\": \"2.2.2.2\",\n          \"port\": 9090\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"http.host\",\n        \"value\": \"incoming.url.com\",\n        \"endpoint\": {\n          \"serviceName\": \"api-gateway\",\n          \"ipv4\": \"2.2.2.2\",\n          \"port\": 9090\n        }\n      },\n      {\n        \"key\": \"http.method\",\n        \"value\": \"POST\",\n        \"endpoint\": {\n          \"serviceName\": \"api-gateway\",\n          \"ipv4\": \"2.2.2.2\",\n          \"port\": 9090\n        }\n      },\n      {\n        \"key\": \"http.path\",\n        \"value\": \"/myapp/api/v1/testworflow\",\n        \"endpoint\": {\n          \"serviceName\": \"api-gateway\",\n          \"ipv4\": \"2.2.2.2\",\n          \"port\": 9090\n        }\n      },\n      {\n        \"key\": \"http.url\",\n        \"value\": \"https://incoming.url.com:9090/myapp/api/v1/testworflow\",\n        \"endpoint\": {\n          \"serviceName\": \"api-gateway\",\n          \"ipv4\": \"2.2.2.2\",\n          \"port\": 9090\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"787a41fdfb6d59cb\",\n    \"name\": \"http:/myapp/api/v1/testworflow\",\n    \"id\": \"94aa5c0443cc5582\",\n    \"parentId\": \"787a41fdfb6d59cb\",\n    \"timestamp\": 1499719237125000,\n    \"duration\": 6102000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1499719237112500,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"central-service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 3008\n        }\n      },\n      {\n        \"timestamp\": 1499719237125000,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"api-gateway\",\n          \"ipv4\": \"2.2.2.2\",\n          \"port\": 9090\n        }\n      },\n      {\n        \"timestamp\": 1499719237140500,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"M1-Service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 4060\n        }\n      },\n      {\n        \"timestamp\": 1499719243209500,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"M1-Service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 4060\n        }\n      },\n      {\n        \"timestamp\": 1499719243211500,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"central-service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 3008\n        }\n      },\n      {\n        \"timestamp\": 1499719243227000,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"api-gateway\",\n          \"ipv4\": \"2.2.2.2\",\n          \"port\": 9090\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"lc\",\n        \"value\": \"zuul\",\n        \"endpoint\": {\n          \"serviceName\": \"api-gateway\",\n          \"ipv4\": \"2.2.2.2\",\n          \"port\": 9090\n        }\n      },\n      {\n        \"key\": \"sa\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"api-gateway\",\n          \"ipv4\": \"2.2.2.2\",\n          \"port\": 9090\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"787a41fdfb6d59cb\",\n    \"name\": \"http:/api/v1/action/approve\",\n    \"id\": \"f5f9dcc8edb20ae7\",\n    \"parentId\": \"94aa5c0443cc5582\",\n    \"timestamp\": 1499719237140500,\n    \"duration\": 6069000,\n    \"annotations\": [],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"http.host\",\n        \"value\": \"3.3.3.3\",\n        \"endpoint\": {\n          \"serviceName\": \"M1-Service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 4060\n        }\n      },\n      {\n        \"key\": \"http.method\",\n        \"value\": \"POST\",\n        \"endpoint\": {\n          \"serviceName\": \"M1-Service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 4060\n        }\n      },\n      {\n        \"key\": \"http.path\",\n        \"value\": \"/api/v1/action/approve\",\n        \"endpoint\": {\n          \"serviceName\": \"M1-Service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 4060\n        }\n      },\n      {\n        \"key\": \"http.url\",\n        \"value\": \"http://3.3.3.3:4060/api/v1/action/approve\",\n        \"endpoint\": {\n          \"serviceName\": \"M1-Service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 4060\n        }\n      },\n      {\n        \"key\": \"lc\",\n        \"value\": \"unknown\",\n        \"endpoint\": {\n          \"serviceName\": \"M1-Service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 4060\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"787a41fdfb6d59cb\",\n    \"name\": \"http:/api/v1/action/approve\",\n    \"id\": \"b963390959854a97\",\n    \"parentId\": \"fe70d9705ea17279\",\n    \"timestamp\": 1499719237168000,\n    \"duration\": 6071000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1499719237168000,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"central-service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 3008\n        }\n      },\n      {\n        \"timestamp\": 1499719243239000,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"central-service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 3008\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"sa\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"central-service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 3008\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"787a41fdfb6d59cb\",\n    \"name\": \"https:/outside-url/test//approve\",\n    \"id\": \"b49392870c173a9f\",\n    \"parentId\": \"f5f9dcc8edb20ae7\",\n    \"timestamp\": 1499719237171000,\n    \"duration\": 6066000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1499719237171000,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"M1-Service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 4060\n        }\n      },\n      {\n        \"timestamp\": 1499719243237000,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"M1-Service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 4060\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"sa\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"M1-Service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 4060\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"787a41fdfb6d59cb\",\n    \"name\": \"async\",\n    \"id\": \"16efa3c1cc735c3c\",\n    \"parentId\": \"fe70d9705ea17279\",\n    \"timestamp\": 1499719243239000,\n    \"duration\": 27000,\n    \"annotations\": [],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"lc\",\n        \"value\": \"async\",\n        \"endpoint\": {\n          \"serviceName\": \"central-service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 3008\n        }\n      },\n      {\n        \"key\": \"thread\",\n        \"value\": \"SimpleAsyncTaskExecutor-1044\",\n        \"endpoint\": {\n          \"serviceName\": \"central-service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 3008\n        }\n      }\n    ]\n  },\n  {\n    \"traceId\": \"787a41fdfb6d59cb\",\n    \"name\": \"updateauditlog\",\n    \"id\": \"0436028911f276fe\",\n    \"parentId\": \"16efa3c1cc735c3c\",\n    \"timestamp\": 1499719243239000,\n    \"duration\": 27000,\n    \"annotations\": [],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"class\",\n        \"value\": \"AuditLogService\",\n        \"endpoint\": {\n          \"serviceName\": \"central-service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 3008\n        }\n      },\n      {\n        \"key\": \"lc\",\n        \"value\": \"async\",\n        \"endpoint\": {\n          \"serviceName\": \"central-service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 3008\n        }\n      },\n      {\n        \"key\": \"method\",\n        \"value\": \"updateAuditLog\",\n        \"endpoint\": {\n          \"serviceName\": \"central-service\",\n          \"ipv4\": \"1.1.1.1\",\n          \"port\": 3008\n        }\n      }\n    ]\n  }\n]. I got all the traces arranged and it seems the\nApi-gateway is pointing to m1-service.\n94aa5c0443cc5582-->f5f9dcc8edb20ae7, seems problem with the way the data is generated from the Spring Sleuth, I will try having it updated with the latest version. it does not seems zipkin -dependency for the above JSON issue. \n\n. ",
    "dpoddar": "With Spring boot Dalston.SR3 you can achieve this by setting property zipkin.storage.mem.max-spans=xxxx this will limit the number of spans and discard old ones.. ",
    "yschimke": "Feel free to close.  I have a zipkin docker setup on 2 home machines, I'm not against doing it. I'm mainly wondering whether it would be useful to have a server where people could explore zipkin.. Feel free to close.  I have a zipkin docker setup on 2 home machines, I'm not against doing it. I'm mainly wondering whether it would be useful to have a server where people could explore zipkin.. From the Gist but also knowing the EventListener interface, is this something that could be prototyped first as an external library.  Seems like all the hooks are there to come up with something pretty smart.. From the Gist but also knowing the EventListener interface, is this something that could be prototyped first as an external library.  Seems like all the hooks are there to come up with something pretty smart.. This sort of thing is hard to get right without knowing how the client and server(s) are handling things. Will OkHttpClient ever make the optimal decision?  e.g. is it worth submitting a queued call shortly before timeout, and therefore newer calls should be prioritised over older ones.. This sort of thing is hard to get right without knowing how the client and server(s) are handling things. Will OkHttpClient ever make the optimal decision?  e.g. is it worth submitting a queued call shortly before timeout, and therefore newer calls should be prioritised over older ones.. reproduction\n$ cat ~/.zipkinrc\nSENDER=http://localhost:9411/api/v1/spans\nDISPLAY=http://localhost:9411/zipkin/traces/{traceid}\nbrew install yschimke/tap/oksocial\noksocial --zipkin https://httpbin.org/get. Thanks, just found that also\nuri.scheme == \"http\" || uri.scheme == \"https\" -> AsyncReporter.builder(OkHttpSender.create(uri.toString())).build(SpanBytesEncoder.JSON_V2)\n\nand updated to v2 in the SENDER  . Thanks, just found that also\nuri.scheme == \"http\" || uri.scheme == \"https\" -> AsyncReporter.builder(OkHttpSender.create(uri.toString())).build(SpanBytesEncoder.JSON_V2)\n\nand updated to v2 in the SENDER  . Cleaner way of doing this\n$ /Library/Java/JavaVirtualMachines/jdk-9.jdk/Contents/Home/bin/jar -d -f ./build/install/oksocial/lib/okhttp-3.10.0.jar\nNo module descriptor found. Derived automatic module.\nmodule okhttp@3.10.0 (automatic)\n  requires mandated java.base\n  contains okhttp3\n  contains okhttp3.internal\n  contains okhttp3.internal.cache\n  contains okhttp3.internal.cache2\n  contains okhttp3.internal.connection\n  contains okhttp3.internal.http\n  contains okhttp3.internal.http1\n  contains okhttp3.internal.http2\n  contains okhttp3.internal.io\n  contains okhttp3.internal.platform\n  contains okhttp3.internal.publicsuffix\n  contains okhttp3.internal.tls\n  contains okhttp3.internal.ws. ",
    "hfgbarrigas": "@adriancole looks good. Thank you. ",
    "aaronyoungkash": "My 2c is I'd prefer to have the column expand to fit and shove everything beside it over. Some traces already require horizontal scrolling anyway.. ",
    "excavador": "Actual behavior is useless.\nHow to find the longest queries for some condition?...\nSeems like we need to support limit + sort processing pn storage side. ",
    "SimenB": "Probably a better idea, I just noticed that the URLs in the table generated by the plugin are missing /zipkin as well. I can take a whack at it, sure. How should we check if /zipkin path is needed? Make a request to root, and if we're redirected, use that?. Cool, I'll get a PR out soonish. I'll try to fix the issue I reported as well. ",
    "wilkinsona": "Looks like you're missing server.use-forward-headers: true. ",
    "cbcing": "thank you, ^_^  @adriancole \ud83d\udc4d . ",
    "stepanv": "You're right it adds complexity. As you can see from the linked merge request, / (as a root) has to be substituted with the context path where absolute paths are used.\nMaybe it would be worth replacing the __webpack_public_path__ with something else but in general I think the idea itself doesn't result with too complex code. Let me know what you think.. @wdittmer, I was able to make Zipkin run in K8S thanks to the change (which is linked) https://github.com/openzipkin/zipkin/pull/1732 .\nThis issue should have been closed few months ago which I'm going to do now.\nCheck out the doc: https://github.com/openzipkin/zipkin/pull/1732/files#diff-18334c04bd4802385d1ec95912bba36c (I have no idea where this documentation is accessible as a document). As you can see, your reverse proxy has to rewrite the tag in the index.html file and then the rest of the UI (and the backend) works.. My use-case is quite simple, I need to run Zipkin in a different path than just /zipkin/.\nThe best solution would be to have all static links relative and let Javascript calculate the current url and not rely on /zipkin/.\nI was able to implement the latter but the former was not that easy due to limitations of the html-webpack-plugin (see comments in the code).\nWith the changes I'm proposing, the good news is that if the reverse proxy supports rewriting of the static content (base tag and its href attribute) (e.g., Apache Httpd with enabled ProxyHTMLURLMap for the base tag, see https://httpd.apache.org/docs/2.4/mod/mod_proxy_html.html#proxyhtmlurlmap), the Zipkin works in any path such as /foo/bar/zipkin/.\nIt is of course possible to run Zipkin the same as before at the path /zipkin/, so this is not a breaking change.\nRegarding tests and readme, I understand that both are important but I wanted to know how you feel about this change before updating the Docs. There are no tests ensuring that this functionality is preserved in future development. I propose to implement a single sanity integration test that Zipkin loads and that most common links work under a different path.\n. By the way, as for the test error reported by CircleCI:\n[ERROR] Tests run: 14, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 6.503 s <<< FAILURE! - in zipkin.junit.ZipkinRuleTest\n[ERROR] postSpans_disconnectDuringBody(zipkin.junit.ZipkinRuleTest)  Time elapsed: 5.028 s  <<< ERROR!\njava.io.IOException: unexpected end of stream on Connection{localhost:46744, proxy=DIRECT hostAddress=localhost/127.0.0.1:46744 cipherSuite=none protocol=http/1.1}\n    at zipkin.junit.ZipkinRuleTest.postSpans(ZipkinRuleTest.java:276)\n    at zipkin.junit.ZipkinRuleTest.postSpans_disconnectDuringBody(ZipkinRuleTest.java:197)\nCaused by: java.io.EOFException: \\n not found: limit=0 content=\u2026\n    at zipkin.junit.ZipkinRuleTest.postSpans(ZipkinRuleTest.java:276)\n    at zipkin.junit.ZipkinRuleTest.postSpans_disconnectDuringBody(ZipkinRuleTest.java:197)\nI'm unable to reproduce that locally. Isn't it possible it's an intermittently failing test? If so, are you able to instruct CircleCI to re-run the test/build? In the worst case I could add a dummy commit to re-run it.. To answer the question regarding the Apache Httpd configuration for the reverse proxying. \n(For now I'm not adding it to a readme, let me know if you have any suggestions where to put the information regarding reverse proxying.)\nLets assume we want to access Zipkin (running on localhost:9411) through Apache Httpd server (running on localhost:80 as a reverse proxy) in a context root /test/foo/zipkin/. No extra Zipkin configuration is required.\nApache Httpd configuration:\n```\nLoadModule proxy_module libexec/apache2/mod_proxy.so\nLoadModule proxy_html_module libexec/apache2/mod_proxy_html.so\nLoadModule proxy_http_module libexec/apache2/mod_proxy_http.so\nProxyPass /test/foo/ http://localhost:9411/\nSetOutputFilter proxy-html\nProxyHTMLURLMap /zipkin/ /test/foo/zipkin/\nProxyHTMLLinks  base        href\nTo access Zipkin UI behind the reverse proxy,  execute:\n$ curl http://localhost/test/foo/zipkin/\n\n      add 'base' tag to work around the fact that 'html-webpack-plugin' does not work\n      with '__webpack_public_path__' being set as reported at https://github.com/jantimon/html-webpack-plugin/issues/119\n    Webpack App\n\n``\nAs you can see, the attributehrefof thebasetag is rewritten which is the way to get around thehtml-webpack-plugin` limitations.\nUploading the span is easy as\ncurl -H \"Content-Type: application/json\" --data-binary \"[$(cat /path/to/zipkin/benchmarks/src/main/resources/span-local.json)]\" http://localhost/test/foo/api/v1/spans\nAnd then it's of course observable in the UI:\nopen http://localhost/test/foo/zipkin/?serviceName=zipkin-server&startTs=1378193040000&endTs=1505463856013\n. By the way, in the latest commit I got rid of the __webpack_public_path__ everywhere. Instead, a publicPath module is used to import contextRoot variable that is referenced in all the URIs.. @eirslett, to be honest, I wanted to avoid even looking at the html-webpack-plugin codebase. Even if it was easy, it would take weeks to have the feature I need in Zipkin working. \nThe base tag is not an ugly work around, it's a way to go, clean and simple, that happens to work well even for the html-webpack-plugin. It puts a constraint on the reverse proxy, to rewrite the tag, but that's such a common behavior. For me it's a reasonable trade-off.. And the fact that html-webpack-plugin devs don't bother that it doesn't conform with the webpack spec (https://github.com/jantimon/html-webpack-plugin/issues/119) is quite discouraging for me, someone who's not from the JS world... I pushed new changes. \nIf you think it's ok, please let me know what kind of tests and documentation you require in order to accept the pull request.. @adriancole, I pushed another commit with extended README.md and fixed devServer.\nJust note that I had to add /zipkin/ context root to the proxy env var to make it work (I hope it's ok):\nproxy=http://localhost:9411/zipkin/ ./npm.sh run dev\n. You're right, I'll append / if missing.. Seems reasonable. I'll take a look at it.. Done. Done. As a matter of fact, the $.ajax call with relative paths use the <base> tag and as such there was no need to wrap it as you suggested.. ",
    "wdittmer": "Was typing the stuff below and then found it was probably the same as this issue.\nBelow are my findings, which might help someone else to see if there can be done something to make this work.\nThis issue is related to the work already done for #1229 and #1627 .\nWhat we are trying to do is the following:\nWe have build up a Kubernetes cluster where we run the openzipkin/zipkin service and an ingress controller to reach various services like Kibana, Grafana, ElasticSearch and also zipkin.\nThe ingress controller looks a follows:\nyaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: cbo-ingress-mgmt-elb\n  namespace: monitoring\n  labels:\n    app: ingressservice-mgmt-elb\n    dns: route53\n  annotations:\n    # kubernetes.io/tls-acme: \"true\"\n    kubernetes.io/ingress.class: \"nginx\"\n    ingress.kubernetes.io/rewrite-target: /\n    ingress.kubernetes.io/ssl-redirect: \"false\"\n    domainName: \"mgmt.$DOMAIN\"\nspec:\n  rules:\n  - host: management.${CNF_ENV}.cbo\n    http:\n      paths:\n      - backend:\n          serviceName: alertmanager\n          servicePort: 9093\n        path: /alertmanager\n      - backend:\n          serviceName: elasticsearch\n          servicePort: 9200\n        path: /elasticsearch\n      - backend:\n          serviceName: grafana\n          servicePort: 3000\n        path: /grafana\n      - backend:\n          serviceName: kibana\n          servicePort: 5601\n        path: /kibana\n      - backend:\n          serviceName: prometheus-monitoring\n          servicePort: 9090\n        path: /prometheus\n      - backend:\n          serviceName: zipkin-service\n          servicePort: 9411\n        path: /zipkin\nSo for example, the Kibana service is now reachable via: http://management.test.cbo/kibana.\nKibana will then forward the browser to http://management.test.cbo/kibana/app/kibana\nKibana is configured with a SERVER_BASEPATH environment variable with value: \"/kibana\" and that works.\nIf we now go to http://management.test.cbo/zipkin then zipkin will forward the browser to http://management.test.cbo/zipkin/zipkin/ and you get a broken UI with the page only showing two links.\n```html\nhtml\n--\n\u00a0 | \n\u00a0 | \n\u00a0 | \n\u00a0 | add 'base' tag to work around the fact that 'html-webpack-plugin' does not work\n\u00a0 | with '__webpack_public_path__' being set as reported at https://github.com/jantimon/html-webpack-plugin/issues/119\n\u00a0 | \n\u00a0 | \n\u00a0 | \u00a0\n\u00a0 | \n\u00a0 | \u00a0\n\u00a0 | \n\u00a0 | Webpack App\n\u00a0 | \n\u00a0 | \n\u00a0 | \u00a0\n\u00a0 |  we include a reduced header on page load, as this makes the page load feel much smoother \n\u00a0 | \u00a0\n\u00a0 | \n\u00a0 | \n\u00a0 | \n\u00a0 | \n\u00a0 | ZipkinInvestigate system behavior\n\u00a0 | \n\u00a0 | \n\u00a0 | /.nav-collapse \n\u00a0 | \n\u00a0 | \n\u00a0 | \u00a0\n\u00a0 | \n\u00a0 | \n```\n\nWhen we look at the generated nginx.conf from the ingress controller, we find that ingress.kubernetes.io/rewrite-target: / adds the following two redirects:\nrewrite /zipkin/(.*) /$1 break;\nrewrite /zipkin / break;\nproxy_pass http://monitoring-zipkin-service-9411;\nWe created a simple nginx docker container:\n```Docker\nFROM nginx:latest\nCOPY nginx.conf /etc/nginx/nginx.conf\n```\nAnd the nginx.conf:\n```\nworker_processes 1;\nevents { worker_connections 1024; }\nhttp {\nlog_format compression '$remote_addr - $remote_user [$time_local] '\n    '\"$request\" $status $upstream_addr '\n    '\"$http_referer\" \"$http_user_agent\" \"$gzip_ratio\"';\n\nserver {\n    listen 8080;\n    server_name reverse-proxy;\n    access_log /var/log/nginx/access.log compression;\n\n    location /zipkin {\n        proxy_pass          http://docker.for.mac.localhost:32786;\n        proxy_set_header    Host             $host;\n        proxy_set_header    X-Real-IP        $remote_addr;\n        proxy_set_header    X-Forwarded-For  $proxy_add_x_forwarded_for;\n        proxy_read_timeout 1800;\n        proxy_connect_timeout 1800;\n    }\n}\n\n}\nWhere port 32768 is mapped on the openzipkin/zipkin container 9411 port. The /etc/hosts file contains\n127.0.0.1 reverse-proxy\n127.0.0.1 zipkin-server\n```\nIf we go to http://reverse-proxy:8080/zipkin then the UI works.\nIf we now add the two extra rewrite rules, the UI breaks by going to /zipkin/zipkin/\nI tried setting the SERVER_CONTEXT_PATH but that didn't do anything.\nSo this narrows it down to the ingress.kubernetes.io/rewrite-target: / setting, which does not work for zipkin. Since we have many services in the same namespace and domain, we are going to see if we can move zipkin out to its own configuration section where we can set the rewrite-target to /zipkin and see if that works.. The problem is that the proxy configuration is generated by k8s Ingress controller. I.e. I cannot change the generated nginx.conf. Unless there is an annotation that does this, I am still stuck.. Currently most applications made some provision to deal with this situation.\nFor example, for Prometheus we need to configure:\n          - '--web.external-url=http://management.test.cbo:13370/prometheus'\n          - '--web.route-prefix=/'\nFor Grafana:\n            - name: GF_SERVER_ROOT_URL\n              value: /grafana\nFor Kibana:\n          - name: SERVER_BASEPATH\n            value: \"/kibana\"\nHopefully these settings will give a better understanding what the problem is. Maybe there is already a similar setting for Zipkin (i.e. I tried SERVER_CONTEXT_PATH but that didn't work) that does the same thing.. @abesto Thanks for thinking with us. The problem we have is that we need to ingress a lot of services/backends in the same namespace and domain, as you can see in the configuration file above. I can only set the annotations on the top level and they will be applied to all services/backends. If I only had Zipkin, then my rewrite-target would be '/zipkin'. We need to set it to '/' so that we can use it for e.g. Kibana, Grafana and Prometheus at the same time. Those services have settings with which you can make them work with the ingress controller with the settings above. Hopefully there is already something similar in Zipkin and/or it is easy to investigate if it can be made to work in Zipkin.. @abesto Thanks for thinking with us. The problem we have is that we need to ingress a lot of services/backends in the same namespace and domain, as you can see in the configuration file above. I can only set the annotations on the top level and they will be applied to all services/backends. If I only had Zipkin, then my rewrite-target would be '/zipkin'. We need to set it to '/' so that we can use it for e.g. Kibana, Grafana and Prometheus at the same time. Those services have settings with which you can make them work with the ingress controller with the settings above. Hopefully there is already something similar in Zipkin and/or it is easy to investigate if it can be made to work in Zipkin.. [A] for me, though I agree that it depends\nAfter starting the new environment I just want to see my test calls coming in and how long they take.\nOnce it is running, I would like [B] to catch the longest one. Maybe indeed an extra 'keep this setting' is nice.. Hey, how is it going with this?. I upgraded tot openzipkin/zipkin:2.5.3 and added\n         - name: ZIPKIN_UI_BASEPATH\n           value: \"/zipkin/zipkin\"\nto the deployment yaml and I have a working UI via the bastion via reverse proxy \ud83d\udc4d \nThanks all!. A bit more information about the deployment:\nWe have a AWS VPC in which an EC2 instance runs that functions as the bastion service, to which we can setup our SSH tunnel. Then we have EC2 instance(s) with a kubernetes cluster. One of the pods is an nginx-ingress pod that reverse proxies the services in the cluster for multiple namespaces. One of the services is Zipkin in the monitoring namespace.\nThe ingress-nginx configuration:\n```apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: cbo-ingress-mgmt-elb\n  labels:\n    app: ingressservice-mgmt-elb\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    nginx.ingress.kubernetes.io/rewrite-target: \"/\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\nspec:\n  rules:\n  - host: management.test.cbo\n    http:\n      paths:\n      - backend:\n          serviceName: authentication-service\n          servicePort: 8210\n        path: /authentication\n      - backend:\n          serviceName: authorization-service\n          servicePort: 8160\n        path: /authorization\n      - backend:\n          serviceName: configuration-service\n          servicePort: 8110\n        path: /configuration\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: cbo-ingress-mgmt-elb\n  namespace: monitoring\n  labels:\n    app: ingressservice-mgmt-elb\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    nginx.ingress.kubernetes.io/rewrite-target: \"/\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\nspec:\n  rules:\n  - host: management.test.cbo\n    http:\n      paths:\n      - backend:\n          serviceName: alertmanager\n          servicePort: 9093\n        path: /alertmanager\n      - backend:\n          serviceName: elasticsearch-proxy\n          servicePort: 9200\n        path: /elasticsearch\n      - backend:\n          serviceName: grafana\n          servicePort: 3000\n        path: /grafana\n      - backend:\n          serviceName: kibana\n          servicePort: 5601\n        path: /kibana\n      - backend:\n          serviceName: prometheus-monitoring\n          servicePort: 9090\n        path: /prometheus\n      - backend:\n          serviceName: zipkin-service\n          servicePort: 9411\n        path: /zipkin\nAnd the Zipkin configuration:\napiVersion: v1\nkind: Service\nmetadata:\n  namespace: monitoring\n  name: zipkin-service\nspec:\n  ports:\n    - port: 9411\n  selector:\n    app: zipkin\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  namespace: monitoring\n  name: zipkin\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: zipkin\n  template:\n    metadata:\n      labels:\n        app: zipkin\n    spec:\n      containers:\n      - name: zipkin\n        image: openzipkin/zipkin:2.5.3\n        env:\n         - name: STORAGE_TYPE\n           value: elasticsearch\n         - name: ES_HOSTS\n           value: http://elasticsearch-proxy:9200\n         - name: ES_DATE_SEPARATOR\n           value: .\n         - name: ZIPKIN_UI_BASEPATH\n           value: \"/zipkin/zipkin\"\n``\nThe separate configurations result in one concatenated nginx.conf file.\nThe crucial thing is that we cannot changenginx.ingress.kubernetes.io/rewrite-target: \"/\"` which is applied to all backends.\nNow that this issue has been fixed it works.\nIf we set up the SSH tunnel with the bastion now, and curl to /zipkin, we get:\n```\n$ curl -vvv -L http://management.test.cbo:13370/zipkin\n   Trying 127.0.0.1...\n TCP_NODELAY set\n* Connected to management.test.cbo (127.0.0.1) port 13370 (#0)\n\nGET /zipkin HTTP/1.1\nHost: management.test.cbo:13370\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 302 Found\n< Date: Thu, 15 Mar 2018 10:32:57 GMT\n< Location: ./zipkin/\n< Server: nginx/1.13.8\n< vary: origin\n< X-Application-Context: zipkin-server:shared:9411\n< Content-Length: 0\n< Connection: keep-alive\n< \n Connection #0 to host management.test.cbo left intact\n Issue another request to this URL: 'http://management.test.cbo:13370/zipkin/'\n Found bundle for host management.test.cbo: 0x7fdb19c21450 [can pipeline]\n Re-using existing connection! (#0) with host management.test.cbo\n Connected to management.test.cbo (127.0.0.1) port 13370 (#0)\nGET /zipkin/ HTTP/1.1\nHost: management.test.cbo:13370\nUser-Agent: curl/7.54.0\nAccept: /*\n< HTTP/1.1 302 Found\n< Date: Thu, 15 Mar 2018 10:32:57 GMT\n< Location: ./zipkin/\n< Server: nginx/1.13.8\n< vary: origin\n< X-Application-Context: zipkin-server:shared:9411\n< Content-Length: 0\n< Connection: keep-alive\n< \n Connection #0 to host management.test.cbo left intact\n Issue another request to this URL: 'http://management.test.cbo:13370/zipkin/zipkin/'\n Found bundle for host management.test.cbo: 0x7fdb19c21450 [can pipeline]\n Re-using existing connection! (#0) with host management.test.cbo\n Connected to management.test.cbo (127.0.0.1) port 13370 (#0)\nGET /zipkin/zipkin/ HTTP/1.1\nHost: management.test.cbo:13370\nUser-Agent: curl/7.54.0\nAccept: /*\n< HTTP/1.1 200 OK\n< Cache-Control: max-age=60\n< Content-Language: en-\n< Content-Type: text/html; charset=UTF-8\n< Date: Thu, 15 Mar 2018 10:32:57 GMT\n< Server: nginx/1.13.8\n< vary: origin\n< X-Application-Context: zipkin-server:shared:9411\n< Content-Length: 1041\n< Connection: keep-alive\n< \n<!doctype html>\n\n\n\n      add 'base' tag to work around the fact that 'html-webpack-plugin' does not work\n      with '__webpack_public_path__' being set as reported at https://github.com/jantimon/html-webpack-plugin/issues/119\n    \n\n\n\nWebpack App\n\n\n\n we include a reduced header on page load, as this makes the page load feel much smoother \n\n\n\n ZipkinInvestigate system behavior \n\n\n/.nav-collapse \n\n\n\n* Connection #0 to host management.test.cbo left intact\n```. I can only use official containers at the moment (e.g. openzipkin/zipkin:2.4.2) to test in the testing/staging environments. I could try locally with the nginx test project as I described in the other ticket, but have to postpone to after our internal release date (~next week).\nThanks for picking it up so quick though, much appreciated!\n. If there is a new container image I can pull it in the testing environment easily and we know if it works quite quickly and provide feedback. Don't worry about the release, we can roll-out a patch release with the new Zipkin afterwards. For me it is more important that Zipkin doesn't break for others with this change :). If you have zipkin running locally you could try:\nDockerfile:\nFROM nginx:latest\nCOPY nginx.conf /etc/nginx/nginx.conf\n\nnginx.conf:\nworker_processes 1;\nevents { worker_connections 1024; }\nhttp {\nlog_format compression '$remote_addr - $remote_user [$time_local] '\n    '\"$request\" $status $upstream_addr '\n    '\"$http_referer\" \"$http_user_agent\" \"$gzip_ratio\"';\n\nserver {\n    listen 8080;\n    server_name reverse-proxy;\n    access_log /var/log/nginx/access.log compression;\n\n    location /zipkin {\n        proxy_pass          http://docker.for.mac.localhost:9411;\n        proxy_set_header    Host             $host;\n        proxy_set_header    X-Real-IP        $remote_addr;\n        proxy_set_header    X-Forwarded-For  $proxy_add_x_forwarded_for;\n        proxy_read_timeout 1800;\n        proxy_connect_timeout 1800;\n        rewrite /zipkin/(.*) /$1 break;\n        rewrite /zipkin / break;\n    }\n}\n\n}\nAnd in /etc/hosts:\n\n127.0.0.1 reverse-proxy\n127.0.0.1 zipkin-server\n\n. (man I suck at getting it to format straight...)\nI will await 2.5.3 :). ",
    "eshujiushiwo": "@adriancole \nThats very kind of you.\nThanks a lot, ive set MYSQL_MAX_CONNECTIONS to 30.\nBut still shows that problem .\nShould i change the storage to cassandra?\n. ",
    "jorgheymans": "will the UI have an href somewhere then to the archive endpoint ? . will the UI have an href somewhere then to the archive endpoint ? . @naoman what i meant was just a top level link on the ui somewhere that points to the archiving endpoint if it is configured. That way ppl only have to remember (or bookmark) the \"main\" zipkin instance.. @naoman what i meant was just a top level link on the ui somewhere that points to the archiving endpoint if it is configured. That way ppl only have to remember (or bookmark) the \"main\" zipkin instance.. For sure this can be done in a separate PR, after all it was just a\nsuggestion. We don't really have the need to save traces right now but once\nthe feature is in I'll definitely configure an archive endpoint because it\ndoes pop up every now and then. I'll send a PR myself if I still see the\nneed for it then.\nOp za 7 okt. 2017 08:06 schreef Naoman Abbas notifications@github.com:\n\nWe don't merge single user stuff, so consider it lucky someone else is\ninterested!\nNot sure what you mean. Do you think I'm trying to push something thats\nPinterest specific, and not needed by other users?\nLet me try to explain one more time. While the request to add an\nadditional url link is valid, its an add-on feature for this change, and\ndoes not block it.\nCan you please explain why these two changes can not be made in two\nseparate pull requests?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1747#issuecomment-334913230,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAL1AMs4RtHPZeC-7SanUzvZNNXV9H08ks5spxTVgaJpZM4Pmh70\n.\n. For sure this can be done in a separate PR, after all it was just a\nsuggestion. We don't really have the need to save traces right now but once\nthe feature is in I'll definitely configure an archive endpoint because it\ndoes pop up every now and then. I'll send a PR myself if I still see the\nneed for it then.\n\nOp za 7 okt. 2017 08:06 schreef Naoman Abbas notifications@github.com:\n\nWe don't merge single user stuff, so consider it lucky someone else is\ninterested!\nNot sure what you mean. Do you think I'm trying to push something thats\nPinterest specific, and not needed by other users?\nLet me try to explain one more time. While the request to add an\nadditional url link is valid, its an add-on feature for this change, and\ndoes not block it.\nCan you please explain why these two changes can not be made in two\nseparate pull requests?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1747#issuecomment-334913230,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAL1AMs4RtHPZeC-7SanUzvZNNXV9H08ks5spxTVgaJpZM4Pmh70\n.\n. Spot on that's exactly where I had imagined the link to appear :-)\n\nOp za 7 okt. 2017 22:15 schreef Naoman Abbas notifications@github.com:\n\n@jorgheymans https://github.com/jorgheymans I've added a link to\narchive server on the menu bar. Let me know what you think. Below is the\nscreenshot.\n[image: screen shot 2017-10-07 at 1 03 43 pm]\nhttps://user-images.githubusercontent.com/6273158/31311426-d91ed056-ab60-11e7-8e5e-0e9790596fff.png\nSeems a hacky solution, most people will not have this setup.\n@jcarres-mdsol https://github.com/jcarres-mdsol can you please\nelaborate on why you feel this solution is hacky and how it can be improved.\nWould the button appear in the UI by default? It may set expectations it\nwould work when in most cases would not\n@jcarres-mdsol https://github.com/jcarres-mdsol No it would not. I've\nadded this information in Readme as well. Let me know if you think it needs\nmore explanation there.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1747#issuecomment-334962894,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAL1AApvJYx6IVFNpFbQ41OR04fsc6Q9ks5sp9vJgaJpZM4Pmh70\n.\n. Spot on that's exactly where I had imagined the link to appear :-)\n\nOp za 7 okt. 2017 22:15 schreef Naoman Abbas notifications@github.com:\n\n@jorgheymans https://github.com/jorgheymans I've added a link to\narchive server on the menu bar. Let me know what you think. Below is the\nscreenshot.\n[image: screen shot 2017-10-07 at 1 03 43 pm]\nhttps://user-images.githubusercontent.com/6273158/31311426-d91ed056-ab60-11e7-8e5e-0e9790596fff.png\nSeems a hacky solution, most people will not have this setup.\n@jcarres-mdsol https://github.com/jcarres-mdsol can you please\nelaborate on why you feel this solution is hacky and how it can be improved.\nWould the button appear in the UI by default? It may set expectations it\nwould work when in most cases would not\n@jcarres-mdsol https://github.com/jcarres-mdsol No it would not. I've\nadded this information in Readme as well. Let me know if you think it needs\nmore explanation there.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1747#issuecomment-334962894,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAL1AApvJYx6IVFNpFbQ41OR04fsc6Q9ks5sp9vJgaJpZM4Pmh70\n.\n. \ud83d\udc4d . \ud83d\udc4d . fixed via #1825 . this doesn't work on my machine (it was working yesterday though). Looking at the debug output it seems that the URL to download the jar from bintray contains some backslashes maven/io\\/zipkin\\/java and this is why curl downloads nothing - at least on my machine.\n\n```\n+ echo 2.4.0\n+ grep -E '^[0-9]+.[0-9]+.[0-9]+$'\n+ echo 'Downloading io.zipkin.java:zipkin-server:2.4.0:exec to zipkin.jar...'\nDownloading io.zipkin.java:zipkin-server:2.4.0:exec to zipkin.jar...\n+ local 'url=https://dl.bintray.com/openzipkin/maven/io\\/zipkin\\/java/zipkin-server/2.4.0/zipkin-server-2.4.0-exec.jar'\n+ echo 'https://dl.bintray.com/openzipkin/maven/io\\/zipkin\\/java/zipkin-server/2.4.0/zipkin-server-2.4.0-exec.jar -> https://dl.bintray.com/openzipkin/maven/io\\/zipkin\\/java/zipkin-server/2.4.0/zipkin-server-2.4.0-exec.jar'\nhttps://dl.bintray.com/openzipkin/maven/io\\/zipkin\\/java/zipkin-server/2.4.0/zipkin-server-2.4.0-exec.jar -> https://dl.bintray.com/openzipkin/maven/io\\/zipkin\\/java/zipkin-server/2.4.0/zipkin-server-2.4.0-exec.jar\n+ curl -L -o zipkin.jar 'https://dl.bintray.com/openzipkin/maven/io\\/zipkin\\/java/zipkin-server/2.4.0/zipkin-server-2.4.0-exec.jar'\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0    33    0    33    0     0    123      0 --:--:-- --:--:-- --:--:--   146\n```. +1 for offering an aggregated version of the documentation, if this is what the ticket implies. \nWill you auto-generate asciidoctor from markdown or how would this work ? . will this eventually lead to zipkin starting as a grpc server ? or it's really just the protobuf encoding of the traces at this point ?. @ryanrupp tx !. I was looking for this as well, our api-gateway team requires a definition to be configured so that zipkin browser clients can start reporting span data 'safely' through the gateway. \nI guess i could just tell them to fetch it from zipkin.io , but somehow it feels safer if it comes from an actual running server instance on the inside .. but that's probably unjustified angst knowing how stable the api is / has been / will be.. I was looking for this as well, our api-gateway team requires a definition to be configured so that zipkin browser clients can start reporting span data 'safely' through the gateway. \nI guess i could just tell them to fetch it from zipkin.io , but somehow it feels safer if it comes from an actual running server instance on the inside .. but that's probably unjustified angst knowing how stable the api is / has been / will be.. Good point, maybe we should also point out then that there are different\nentry points to consider (browser URL vs rest api). Most people don't\ndistinguish between them initially. But layer they might want to have\ndifferent security rules for these. I'll revise and update. Maybe it will\nbe more clear where to put it as well then.\nOp wo 4 apr. 2018 01:56 schreef Adrian Cole notifications@github.com:\n\nsounds good, but should this note be in the server readme, here or both?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1982#issuecomment-378435913,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAL1AMSZ3ISh0t6t2CX6CQWVr3ImyQb3ks5tlAwZgaJpZM4TFuSK\n.\n. Good point, maybe we should also point out then that there are different\nentry points to consider (browser URL vs rest api). Most people don't\ndistinguish between them initially. But layer they might want to have\ndifferent security rules for these. I'll revise and update. Maybe it will\nbe more clear where to put it as well then.\n\nOp wo 4 apr. 2018 01:56 schreef Adrian Cole notifications@github.com:\n\nsounds good, but should this note be in the server readme, here or both?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/1982#issuecomment-378435913,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAL1AMSZ3ISh0t6t2CX6CQWVr3ImyQb3ks5tlAwZgaJpZM4TFuSK\n.\n. added note on the endpoints.\n\nas to how to secure data ingestion via the api that should be put in zipkin-server and somewhere in brave because it affects them both, should be done in a different PR.. done.. Imagine service names like serviceA_dev, serviceA_test, serviceA_uat,\nhaving them in predictable order instead of all over the place makes for a\nbetter user experience.\nOp do 4 okt. 2018 18:10 schreef Raja Sundaram notifications@github.com:\n\nIt is quite easy to do in the front end. I think it makes sense to sort it\nalphabetically.\nOn Thu 4 Oct, 2018, 21:07 Jos\u00e9 Carlos Ch\u00e1vez, notifications@github.com\nwrote:\n\nCould you elaborate more on this? Since the dropdown allows you to\nsearch I\nam curious about the value being added by sorting the service names. This\ncould be easy to do if we sort in frontend tho.\nJos\u00e9 Carlos Ch\u00e1vez\nDen tor. 4. okt. 2018 kl. 15:46 skrev Jorg Heymans <\nnotifications@github.com\n\n:\nIt would make sense to alphabetically order the list of services in the\n'Service Name' dropdown.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2204, or mute the thread\n<\n\nhttps://github.com/notifications/unsubscribe-auth/AC7sArpFdGS6EJzZbcK2DnyUeYrL9CvWks5uhhEdgaJpZM4XIJJ0\n\n.\n\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\n<https://github.com/openzipkin/zipkin/issues/2204#issuecomment-427032514\n,\nor mute the thread\n<\nhttps://github.com/notifications/unsubscribe-auth/AEgSsE77ao6YlMHoYJJCKVhLkpb-Sefgks5uhhY8gaJpZM4XIJJ0\n.\n\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2204#issuecomment-427077289,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAL1AM_HgTycUwoUWJPnowIrfh1FbgKfks5uhjLjgaJpZM4XIJJ0\n.\n. Confirming with java -jar zipkin.jar --logging.level.root=TRACE that the armeriaServer bean indeed is taking a long time to destroy:\n2019-03-19 09:14:25.531 TRACE 28697 --- [       Thread-5] o.s.b.f.s.DisposableBeanAdapter          : Invoking destroy method 'close' on bean with name 'cassandraCluster'\n2019-03-19 09:14:25.555 TRACE 28697 --- [       Thread-5] o.s.b.f.s.DisposableBeanAdapter          : Invoking destroy method 'close' on bean with name 'armeriaServer'\n2019-03-19 09:14:32.784 TRACE 28697 --- [       Thread-5] o.s.b.f.s.DisposableBeanAdapter          : Invoking destroy() on bean with name 'defaultValidator'\n2019-03-19 09:14:32.784 TRACE 28697 --- [       Thread-5] o.s.b.f.s.DefaultListableBeanFactory     : Retrieved dependent beans for bean 'prometheusMeterRegistry': [metrics, zipkin2.server.internal.MetricsHealthController, metricsEndpoint, zipkin2.autoconfigure.prometheus.ZipkinPrometheusMetricsAutoConfiguration, metricsRestTemplateCustomizer]\n2019-03-19 09:14:32.784 TRACE 28697 --- [       Thread-5] o.s.b.f.s.DefaultListableBeanFactory     : Retrieved dependent beans for bean 'metrics': [zipkin2.server.internal.ZipkinHttpCollector]\n. \n",
    "kahootali": "@adriancole I am using Elastic search cluster on Azure, how can I configure to send my traces from Zipkin there? And I want to persist the data. ",
    "raymrg": "Hi,\nDid you manage to integrate BW to ZipKin? I am investigating a similar solution and your input would be appreciated.\nThanks. ",
    "emmanuel": "@hexchain I am interested in using ScyllaDB as a backend for Zipkin, but I haven't started working on it yet. \nHave you published your fork with the disabled statement anywhere? Was the scope of the change literally just commenting out the session.prepare call? . @hexchain I am interested in using ScyllaDB as a backend for Zipkin, but I haven't started working on it yet. \nHave you published your fork with the disabled statement anywhere? Was the scope of the change literally just commenting out the session.prepare call? . Looking at this a little more, it seems more like you needed to switch from the IN clause to a strict equality query, here on the service_name key: https://github.com/openzipkin/zipkin/blob/eeeeb3c02b0e0c578eb36c08d80cf2bf400067fc/zipkin-storage/cassandra/src/main/java/zipkin/storage/cassandra/CassandraSpanStore.java#L163\nIs that more like it?. ",
    "hexchain": "@emmanuel It was a very dirty fix: just replace the else block with selectTraceIdsByServiceNames = null;.. @adriancole Scylla only claims to be a Cassandra 2 implementation. Is there any known protocol difference between Cassandra 2 and 3?\nPlus, do I have to upgrade all client to make them generate v2 spans?. > It is a combination of v4 protocol and SASI indexing. Is there a layering\n\nsolution for SASI with Scylla?\n\nNot that I know of. According to http://www.scylladb.com/2017/07/27/materialized-views-preview-scylla-2-0/ and https://github.com/scylladb/scylla/issues/2203, Scylla does not support SASI.\n\nNope. All recent versions of zipkin will convert depending on the\nunderlying backend (Ex v1 -> v2 or v2 -> v1) no client change is needed,\nthough upgrading to a client that supports v2 model is good long term.\n\nGood to know!. > It is a combination of v4 protocol and SASI indexing. Is there a layering\n\nsolution for SASI with Scylla?\n\nNot that I know of. According to http://www.scylladb.com/2017/07/27/materialized-views-preview-scylla-2-0/ and https://github.com/scylladb/scylla/issues/2203, Scylla does not support SASI.\n\nNope. All recent versions of zipkin will convert depending on the\nunderlying backend (Ex v1 -> v2 or v2 -> v1) no client change is needed,\nthough upgrading to a client that supports v2 model is good long term.\n\nGood to know!. I wrote an agent like this several months ago, mainly because we have lots of microservices written in Python and running in small containers, which connect to Kafka directly to send spans. And that's just too many connections for Kafka brokers.\nAll it does is to receive spans from a UDP port and then sends them in batch to Kafka.. @eirslett Seems that fluentd cannot combine multiple JSON document to a list and send the list in one message. Reducing the amount of Kafka messages is important to me.. @eirslett Seems that fluentd cannot combine multiple JSON document to a list and send the list in one message. Reducing the amount of Kafka messages is important to me.. We do have such mechanism in our tracing library, but I still prefer to have this kind of batching in agent, simply because the agent receives spans from all containers on one host so batching can be done more efficiently.. We do have such mechanism in our tracing library, but I still prefer to have this kind of batching in agent, simply because the agent receives spans from all containers on one host so batching can be done more efficiently.. Does it feel better if the \"Service name\" and \"Span name\" line has the same width as the \"Annotation Query\" line?. Guess you can just squash my prior commit into your second one, to make Git history a bit more cleaner, since that block of code changed a lot.. ",
    "conorgriffin": "Sure, so I'm talking about showing the calculated latencies between the annotations in the detail view where the red arrows are in the first screenshot.  You could toggle these on/off with a button as shown in the upper-right of the second screenshot where I've done a rough mockup of what I mean.\n\n\n. I'd be happy to work on it if this or some variation of it is deemed to be useful. ",
    "layao": "I have solved this problem. Thanks a lot!. I have solved this problem. Thanks a lot!. ",
    "lhsm": "Sorry, i mixed up the target project. Create one https://github.com/openzipkin/zipkin-ui/issues/42 just in case\nThanks. ",
    "jeqo": "@adriancole is this one a duplicate of #2098?. @adriancole that would be awesome! didn't know this effort before, but it would be great if @afalko @rmichela could check if https://github.com/jeqo/zipkin-storage-kafka covers their use-cases before.. @adriancole that would be awesome! didn't know this effort before, but it would be great if @afalko @rmichela could check if https://github.com/jeqo/zipkin-storage-kafka covers their use-cases before.. We could solve this by adding an AdminClient on the KafkaColletor, and query Cluster description on health-check to validate if connection with Kafka is OK, instead of checking Consumer metrics.\nWDYT?. It will reuse Consumer's properties, so it will fail as Consumer does if no broker available (and try to reconnect and so on).\nWe can use a very basic query like describeCluster to validate that connection is working properly.\nIf it make sense, I can create a PR to evaluate this solution. . Great! I have created #2168 and I have ask for validation on Confluent's Slack and Kafka devs mailing list to confirm in the meantime.. Thanks @gquintana !\nLGTM. Thanks @gquintana !\nLGTM. @zeagord didn't know that, thanks! :) Fixing it.. Agree. I'll fix it.. Thanks for the advise, I'll check it. . ",
    "narayaruna": "Brave is sufficient for services implemented in Java. However for JS and other runtimes it may become tedious to repeat implementing common aspects of the tracing client like compression, batching and posting data to backend. These aspects can be implemented in the agent instead. The sampling decision though should be taken in the library.\nWe should also think about compatibility between tracing library version and the agent because there will scenarios where the agent is not updated but the client library is and so on. This also means that we should avoid tight coupling between them.\nWe should probably start listing out what we want to implement in the library and the agent.. ",
    "liugj": "I find the answer 'id' and parentId length must be equal 16. yes ,thank you very much!. ",
    "MrGlaucus": "see this zipkin-dependencies. @yzfshuijing im working on this now. At present, you can make a it a scheduled job in zipkin-server.. As my experience, singly \"zh\" always means Simplified Chinese, this happens when you just want to support Chinese only. if you want to support Simplified and Traditional Chinese both, we usually use \"zh_CN\" for Simplified Chinese and \"zh_TW\" for Traditional Chinese.(Of course, if you want to be more specific, there are zh_HK for Hong-Kong and zh_SG for Singapore and so on, but \"zh_CN\" and \"zh_TW\" are already enough.). I added Service Name and Span Name labels in index.mustache, so you might have to add two more translations in i18n file, thank you so much.. Thank you for trusting me, i'll keep on this. :). I will work on the translation as the merge is done.. ",
    "dos65": "@adriancole oh, sorry for waiting. I started investigating how to test TraceUI component, but it's not so obvious for me.. @adriancole oh, sorry for waiting. I started investigating how to test TraceUI component, but it's not so obvious for me.. @adriancole it was good idea to test it, fix was incomplete ). ",
    "sdwr98": "I was just going in order out of an abundance of caution. I\u2019ll try 2.3 tomorrow morning. \nSent from my iPhone\n\nOn Nov 14, 2017, at 6:12 PM, Adrian Cole notifications@github.com wrote:\nWill look into it. Note latest version is 2.3.1. Is there a reason you\ncan't upgrade?\nOn 15 Nov 2017 12:15 am, \"Scott Rankin\" notifications@github.com wrote:\n\nHello,\nI have been running Zipkin server 1.28 against AWS ElasticSearch (2.3).\nToday I tried to upgrade to 1.29, and I get the below error in stdout\n(domain redacted). I'm running Zipkin via Docker, and passing the following\nconfiguration:\n\"STORAGE_TYPE\": \"elasticsearch\",\n\"ES_HOSTS\": \"https://my.es.index\",\n\"AWS_ACCESS_KEY_ID\": \"ACCESSKEYHERE\",\n\"AWS_SECRET_ACCESS_KEY\": \"SECRETKEYHERE\",\nwhich has been working fine since Zipkin 1.23. I can't seem to find any\nindication of a change to the signing requirements in 1.29 - am I missing\nanything?\nThanks!\nScott\nException in thread \"OkHttp Dispatcher\" java.lang.IllegalStateException: The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'GET\n/_cluster/health/zipkin%3Aspan-%2A\nhost:search-zipkin-sandbox-XXXXXXXXXXXXXX.amazonaws.com\nx-amz-date:20171114T155629Z\nhost;x-amz-date\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20171114T155629Z\n20171114/us-west-2/es/aws4_request\nbf52418dfd335f36303b4b3b7ed80a178ea587c8ff95773ba66b65fbae4c7cf6'\nat zipkin.autoconfigure.storage.elasticsearch.aws.AWSSignatureVersion4.intercept(AWSSignatureVersion4.java:69)\nat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)\nat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:45)\nat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)\nat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)\nat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)\nat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)\nat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)\nat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)\nat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)\nat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:120)\nat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)\nat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)\nat okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:185)\nat okhttp3.RealCall$AsyncCall.execute(RealCall.java:135)\nat okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:748)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1796, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD617xSZaIaqJp8ihFGTy8LMr3IXWqAks5s2bypgaJpZM4QdmD2\n.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n. Figures, I stopped at 1.30.  \n\nI deployed 2.3.1 in all my environments today and it all looks good.  Thanks for your quick response! \nScott . ",
    "codingzombie": "Thank you!\nI want to disable search because our backend Cassandra is unable to handle\nthe load. But one useful feature is going to tracepage directly with\ntraceid which developers have it handy. Blocking search page for all users\nis okay in my case.\nOn Wed, Nov 15, 2017 at 6:42 PM Adrian Cole notifications@github.com\nwrote:\n\nThere's no such setting at the moment. We only have an option to disable\nthe UI completely.\nIf we did have an option like this, it would apply to all users as we have\nno means to identify one. Note that they could still access the api\ndirectly, as the api is ultimately what presents the search capability.\nWe could gracefully handle when the api returns unauthorized.. For\nexample, the UI begins with a query against the api services endpoint. If\nthis returns unauthorized, we could possibly guess that someone has\ndisabled exploration, then only present the get trace by ID screen.\nBefore we go there, though, probably maybe elaborate on your end goal and\nhow you plan to enforce it?\ncc @jonathan-lo https://github.com/jonathan-lo @sirtyro\nhttps://github.com/sirtyro\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1800#issuecomment-344798007,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGUj-yY1dX_zHitenYxlHJObHex59mMSks5s26ELgaJpZM4Qfuwi\n.\n. \n",
    "gzchenyong": "This pull request is done by our UI team, sorry for the wire account name :)\nPlease refer to the attached screen shot, including English and Chinese UI\n\n\n. ",
    "SecretPigeon": "I got a 403 from the maven web server. In retrospect it could be transient\nbut at the time it appeared the URL wasn't supported by them any more.\nOn Fri, 17 Nov 2017, 15:53 Tommy Ludwig, notifications@github.com wrote:\n\n@mattchurchy https://github.com/mattchurchy Can you elaborate more on\n\"doesn't work\"? Are you getting an error? Is it the same or similar to the\nfollowing?\nERROR: no certificate subject alternative name matches\n  requested host name 'repo2.maven.org'.\nTo connect to repo2.maven.org insecurely, use `--no-check-certificate'.\nI believe this is caused by MVNCENTRAL-2870\nhttps://issues.sonatype.org/browse/MVNCENTRAL-2870.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1804#issuecomment-345281658,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAFZKoZ4elaT3r1O37wscSjx7IkgoN-1ks5s3awFgaJpZM4QiCVj\n.\n. Agreed appears to be a wider issue then your docs.\n\nOn Sat, 18 Nov 2017, 14:34 Zolt\u00e1n Nagy, notifications@github.com wrote:\n\nHwell, the certs still do look like they're messed up - hit this at work\ntoo. OTOH even after accepting the wrong cert, I get a 403 from repo2, but\nthe download works from repo1. Maaaaybe this was done intentionally by\nMaven Central to prevent people from using repo2 with the bad cert, because\nbad certs can lead to all kinds of ... badness. repo1 is properly\nconfigured. repo2 says \"Use https://repo1.maven.org\" after all.\nOverall I'm inclined to not do anything - Maven Central certs being messed\nup is not something individual projects should work around, methinks.\nAt the same time, yeah, this is messing up the first start experience.\nThis ugly-AF one-liner can be used as an alternative, if needed:\nwget -O zipkin.jar \"$(curl -D- -s 'https://search.maven.org/remote_content?g=io.zipkin.java&a=zipkin-server&v=LATEST&c=exec' | grep '^Location: ' | cut -f2 -d' ' | tr -d $'\\r' | sed 's/repo2/repo1/')\"\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/1804#issuecomment-345446010,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAFZKg50x-K4cnxS2ajVLcpruS8MNKvnks5s3urbgaJpZM4QiCVj\n.\n. \n",
    "brian-brazil": "You need to use the same metric object across reloads, otherwise the metric will cause problems for users as it'll keep on resetting.. Registering a metric counts as using it. The rest of your comment got cut off.\nI pointed out this incompatibility in models as far back as when the Spring boot integration was being added, and I've yet to hear of any solutions proposed that make sense.. ",
    "reshmik": "Hello,\nPlease see the format of rabbitmq(amqps) provided in PCF. We tried to parse and hardcode the values such as addresses, vhost, user, password and it keeps saying \"connection refused\".  \n\"p-rabbitmq\": [\n        {\n          \"credentials\": {\n            \"http_api_uris\": [\n              \"https://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@pivotal-rabbitmq-management.run.pivotal.io/api/\"\n            ],\n            \"ssl\": true,\n            \"dashboard_url\": \"https://pivotal-rabbitmq-management.run.pivotal.io/#/login/574fb21f-1f2c-45cc-ba98-79a1a1601d16/rrsvmq4o2vmfo8v1qgkj3576ns\",\n            \"password\": \"rrsvmq4o2vmfo8v1qgkj3576ns\",\n            \"protocols\": {\n              \"management+ssl\": {\n                \"path\": \"/api/\",\n                \"ssl\": false,\n                \"hosts\": [\n                  \"10.10.38.8\",\n                  \"10.10.102.8\"\n                ],\n                \"password\": \"rrsvmq4o2vmfo8v1qgkj3576ns\",\n                \"username\": \"574fb21f-1f2c-45cc-ba98-79a1a1601d16\",\n                \"port\": 15672,\n                \"host\": \"10.10.38.8\",\n                \"uri\": \"http://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8:15672/api/\",\n                \"uris\": [\n                  \"http://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8:15672/api/\",\n                  \"http://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.102.8:15672/api/\"\n                ]\n              },\n              \"amqp+ssl\": {\n                \"vhost\": \"721c2f68-c5da-45a3-9810-4dc80f70ecd7\",\n                \"username\": \"574fb21f-1f2c-45cc-ba98-79a1a1601d16\",\n                \"password\": \"rrsvmq4o2vmfo8v1qgkj3576ns\",\n                \"port\": 5671,\n                \"host\": \"10.10.38.8\",\n                \"hosts\": [\n                  \"10.10.38.8\",\n                  \"10.10.102.8\"\n                ],\n                \"ssl\": true,\n                \"uri\": \"amqps://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8:5671/721c2f68-c5da-45a3-9810-4dc80f70ecd7\",\n                \"uris\": [\n                  \"amqps://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8:5671/721c2f68-c5da-45a3-9810-4dc80f70ecd7\",\n                  \"amqps://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.102.8:5671/721c2f68-c5da-45a3-9810-4dc80f70ecd7\"\n                ]\n              },\n              \"management\": {\n                \"vhost\": \"721c2f68-c5da-45a3-9810-4dc80f70ecd7\",\n                \"username\": \"574fb21f-1f2c-45cc-ba98-79a1a1601d16\",\n                \"password\": \"rrsvmq4o2vmfo8v1qgkj3576ns\",\n                \"port\": 15672,\n                \"host\": \"10.10.38.8\",\n                \"hosts\": [\n                  \"10.10.38.8\",\n                  \"10.10.102.8\"\n                ],\n                \"ssl\": false,\n                \"uri\": \"http://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8:15672/721c2f68-c5da-45a3-9810-4dc80f70ecd7\",\n                \"uris\": [\n                  \"http://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8:15672/721c2f68-c5da-45a3-9810-4dc80f70ecd7\",\n                  \"http://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.102.8:15672/721c2f68-c5da-45a3-9810-4dc80f70ecd7\"\n                ]\n              },\n              \"mqtt\": {\n                \"username\": \"721c2f68-c5da-45a3-9810-4dc80f70ecd7:574fb21f-1f2c-45cc-ba98-79a1a1601d16\",\n                \"password\": \"rrsvmq4o2vmfo8v1qgkj3576ns\",\n                \"port\": 1883,\n                \"host\": \"10.10.38.8\",\n                \"hosts\": [\n                  \"10.10.38.8\",\n                  \"10.10.102.8\"\n                ],\n                \"ssl\": false,\n                \"uri\": \"mqtt://721c2f68-c5da-45a3-9810-4dc80f70ecd7%3A574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8:1883\",\n                \"uris\": [\n                  \"mqtt://721c2f68-c5da-45a3-9810-4dc80f70ecd7%3A574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8:1883\",\n                  \"mqtt://721c2f68-c5da-45a3-9810-4dc80f70ecd7%3A574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.102.8:1883\"\n                ]\n              },\n              \"mqtt+ssl\": {\n                \"username\": \"721c2f68-c5da-45a3-9810-4dc80f70ecd7:574fb21f-1f2c-45cc-ba98-79a1a1601d16\",\n                \"password\": \"rrsvmq4o2vmfo8v1qgkj3576ns\",\n                \"port\": 8883,\n                \"host\": \"10.10.38.8\",\n                \"hosts\": [\n                  \"10.10.38.8\",\n                  \"10.10.102.8\"\n                ],\n                \"ssl\": true,\n                \"uri\": \"mqtt+ssl://721c2f68-c5da-45a3-9810-4dc80f70ecd7%3A574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8:8883\",\n                \"uris\": [\n                  \"mqtt+ssl://721c2f68-c5da-45a3-9810-4dc80f70ecd7%3A574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8:8883\",\n                  \"mqtt+ssl://721c2f68-c5da-45a3-9810-4dc80f70ecd7%3A574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.102.8:8883\"\n                ]\n              },\n              \"stomp\": {\n                \"vhost\": \"721c2f68-c5da-45a3-9810-4dc80f70ecd7\",\n                \"username\": \"574fb21f-1f2c-45cc-ba98-79a1a1601d16\",\n                \"password\": \"rrsvmq4o2vmfo8v1qgkj3576ns\",\n                \"port\": 61613,\n                \"host\": \"10.10.38.8\",\n                \"hosts\": [\n                  \"10.10.38.8\",\n                  \"10.10.102.8\"\n                ],\n                \"ssl\": false,\n                \"uri\": \"stomp://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8:61613\",\n                \"uris\": [\n                  \"stomp://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8:61613\",\n                  \"stomp://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.102.8:61613\"\n                ]\n              },\n              \"stomp+ssl\": {\n                \"vhost\": \"721c2f68-c5da-45a3-9810-4dc80f70ecd7\",\n                \"username\": \"574fb21f-1f2c-45cc-ba98-79a1a1601d16\",\n                \"password\": \"rrsvmq4o2vmfo8v1qgkj3576ns\",\n                \"port\": 61614,\n                \"host\": \"10.10.38.8\",\n                \"hosts\": [\n                  \"10.10.38.8\",\n                  \"10.10.102.8\"\n                ],\n                \"ssl\": true,\n                \"uri\": \"stomp+ssl://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8:61614\",\n                \"uris\": [\n                  \"stomp+ssl://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8:61614\",\n                  \"stomp+ssl://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.102.8:61614\"\n                ]\n              }\n            },\n            \"username\": \"574fb21f-1f2c-45cc-ba98-79a1a1601d16\",\n            \"hostname\": \"10.10.38.8\",\n            \"hostnames\": [\n              \"10.10.38.8\",\n              \"10.10.102.8\"\n            ],\n            \"vhost\": \"721c2f68-c5da-45a3-9810-4dc80f70ecd7\",\n            \"http_api_uri\": \"https://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@pivotal-rabbitmq-management.run.pivotal.io/api/\",\n            \"uri\": \"amqps://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8/721c2f68-c5da-45a3-9810-4dc80f70ecd7\",\n            \"uris\": [\n              \"amqps://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.38.8/721c2f68-c5da-45a3-9810-4dc80f70ecd7\",\n              \"amqps://574fb21f-1f2c-45cc-ba98-79a1a1601d16:rrsvmq4o2vmfo8v1qgkj3576ns@10.10.102.8/721c2f68-c5da-45a3-9810-4dc80f70ecd7\"\n            ]\n          },\n          \"syslog_drain_url\": null,\n          \"volume_mounts\": [],\n          \"label\": \"p-rabbitmq\",\n          \"provider\": null,\n          \"plan\": \"standard\",\n          \"name\": \"rabbitmq-stream\",\n          \"tags\": [\n            \"rabbitmq\",\n            \"messaging\",\n            \"message-queue\",\n            \"amqp\",\n            \"stomp\",\n            \"mqtt\",\n            \"pivotal\"\n          ]\n        }\n      ]\n    }. Yes most definitely, I have created a new space and given you space manager, dev permissions. You can deploy code and add services from marketplace there. I can assist with additional testing time permitting this week(since we will be at Spring One).@shakuzen  : please give me your best email so I can add you as well.. ",
    "sanleshusheng": "java.lang.IllegalStateException: over capacity\n    at zipkin2.elasticsearch.internal.client.HttpCall.enqueue(HttpCall.java:87) [zipkin-storage-elasticsearch-2.3.1.jar!/:na]\n    at zipkin2.Call$FlatMapping.doEnqueue(Call.java:280) [io.zipkin.zipkin2-zipkin-2.3.1.jar!/:na]\n    at zipkin2.Call$Base.enqueue(Call.java:394) [io.zipkin.zipkin2-zipkin-2.3.1.jar!/:na]\n    at zipkin2.Call$Mapping.doEnqueue(Call.java:241) [io.zipkin.zipkin2-zipkin-2.3.1.jar!/:na]\n    at zipkin2.Call$Base.enqueue(Call.java:394) [io.zipkin.zipkin2-zipkin-2.3.1.jar!/:na]\n    at zipkin.internal.V2SpanStoreAdapter.getTraces(V2SpanStoreAdapter.java:53) [io.zipkin.java-zipkin-2.3.1.jar!/:na]\n    at zipkin.internal.LenientDoubleCallbackAsyncSpanStore.getTraces(LenientDoubleCallbackAsyncSpanStore.java:42) [io.zipkin.java-zipkin-2.3.1.jar!/:na]\n    at zipkin.storage.InternalAsyncToBlockingSpanStoreAdapter.getTraces(InternalAsyncToBlockingSpanStoreAdapter.java:31) [io.zipkin.java-zipkin-2.3.1.jar!/:na]\n    at zipkin.server.ZipkinQueryApiV1.getTraces(ZipkinQueryApiV1.java:108) [classes!/:na]\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_111]\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_111]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_111]\n    at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_111]\n    at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:635) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:742) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) [tomcat-embed-websocket-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at io.prometheus.client.filter.MetricsFilter.doFilter(MetricsFilter.java:170) [simpleclient_servlet-0.1.0.jar!/:na]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:101) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:101) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:101) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:101) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:101) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:101) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:101) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationDispatcher.invoke(ApplicationDispatcher.java:728) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationDispatcher.processRequest(ApplicationDispatcher.java:467) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationDispatcher.doForward(ApplicationDispatcher.java:392) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationDispatcher.forward(ApplicationDispatcher.java:311) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.web.servlet.view.InternalResourceView.renderMergedOutputModel(InternalResourceView.java:168) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.servlet.view.AbstractView.render(AbstractView.java:303) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.servlet.DispatcherServlet.render(DispatcherServlet.java:1286) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.servlet.DispatcherServlet.processDispatchResult(DispatcherServlet.java:1041) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:984) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:635) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:742) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) [tomcat-embed-websocket-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at io.prometheus.client.filter.MetricsFilter.doFilter(MetricsFilter.java:170) [simpleclient_servlet-0.1.0.jar!/:na]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55) [spring-boot-1.5.8.RELEASE.jar!/:1.5.8.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:110) [spring-boot-actuator-1.5.8.RELEASE.jar!/:1.5.8.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:108) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:81) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:106) [spring-boot-actuator-1.5.8.RELEASE.jar!/:1.5.8.RELEASE]\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.12.RELEASE.jar!/:4.3.12.RELEASE]\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:199) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:478) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:140) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:81) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:87) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:677) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:803) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:868) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1459) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]\n    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-embed-core-8.5.23.jar!/:8.5.23]\n    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]. There is no this problem when I write data to the storage of In-memory.. the storage type of mysql have the same problem.. [A] according to the time series. [A] according to the time series. ",
    "newguylhl": "Could anyone help me please? I have searched but couldn't find answers. \nmy environment\nfilebeat 5.4.3\nes 2.35\nzipkin 2.4.0\nlog in es\n{\n  \"_index\": \"zipkin-2017-12-13\",\n  \"_type\": \"span\",\n  \"_id\": \"AWBQBtU_LclY2yufRwdG\",\n  \"_score\": null,\n  \"_source\": {\n    \"@timestamp\": \"2017-12-13T13:18:09.546Z\",\n    \"beat\": {\n      \"hostname\": \"my host\",\n      \"name\": \"my name\",\n      \"version\": \"5.4.3\"\n    },\n    \"duration\": 612898,\n    \"id\": \"9032b04972e475c5\",\n    \"input_type\": \"log\",\n    \"kind\": \"SERVER\",\n    \"localEndpoint\": {\n      \"ipv4\": \"192.168.1.113\",\n      \"serviceName\": \"brave-webmvc-example\"\n    },\n    \"name\": \"get\",\n    \"offset\": 123116,\n    \"remoteEndpoint\": {\n      \"ipv4\": \"127.0.0.1\",\n      \"port\": 60149,\n      \"serviceName\": \"\"\n    },\n    \"source\": \"my log path\",\n    \"timestamp\": 1502101460678880,\n    \"traceId\": \"9032b04972e475c5\",\n    \"type\": \"span\"\n  },\n  \"fields\": {\n    \"@timestamp\": [\n      1513171089546\n    ]\n  },\n  \"sort\": [\n    1513171089546\n  ]\n}\njson on zipkin web page\n[\n  {\n    \"traceId\": \"9032b04972e475c5\",\n    \"id\": \"9032b04972e475c5\",\n    \"name\": \"get\",\n    \"timestamp\": 1502101460678880,\n    \"duration\": 612898\n  }\n]\nSome keys are missing and I cannot search use this link http://localhost:9411/zipkin/api/v2/trace/9032b04972e475c5. But request api v1 can get json above.\nThe original log is from here. I just remove tags. \n$ curl -X POST -s localhost:9411/api/v2/spans -H'Content-Type: application/json' -d'[{\n  \"traceId\": \"9032b04972e475c5\",\n  \"id\": \"9032b04972e475c5\",\n  \"kind\": \"SERVER\",\n  \"name\": \"get\",\n  \"timestamp\": 1502101460678880,\n  \"duration\": 612898,\n  \"localEndpoint\": {\n    \"serviceName\": \"brave-webmvc-example\",\n    \"ipv4\": \"192.168.1.113\"\n  },\n  \"remoteEndpoint\": {\n    \"serviceName\": \"\",\n    \"ipv4\": \"127.0.0.1\",\n    \"port\": 60149\n  },\n  \"tags\": {\n    \"error\": \"500 Internal Server Error\",\n    \"http.path\": \"/a\"\n  }\n}]'. Ah, I see. I just test zipkin prefix for api v1 and zipkin:span for api v2. Both works well. Thanks a lot! Close.. I rerun the test and cannot get trace with both methods mentioned above, including using _search?q=tags.error:500. I don't know what;s wrong...\ntrace\n[\n  {\n    \"traceId\": \"f28a3fece3e311e7\",\n    \"id\": \"f28aae3ce3e311e7\",\n    \"name\": \"bid request v2\",\n    \"timestamp\": 1513595382428000,\n    \"duration\": 10000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1513595382428000,\n        \"value\": \"sr\",\n        \"endpoint\": {\n          \"serviceName\": \"pc.info.v2\",\n          \"ipv4\": \"10.20.1.14\",\n          \"port\": 57101\n        }\n      },\n      {\n        \"timestamp\": 1513595382433000,\n        \"value\": \"request 5ms\",\n        \"endpoint\": {\n          \"serviceName\": \"pc.info.v2\",\n          \"ipv4\": \"10.20.1.14\",\n          \"port\": 57101\n        }\n      },\n      {\n        \"timestamp\": 1513595382435000,\n        \"value\": \"request 7ms\",\n        \"endpoint\": {\n          \"serviceName\": \"pc.info.v2\",\n          \"ipv4\": \"10.20.1.14\",\n          \"port\": 57101\n        }\n      },\n      {\n        \"timestamp\": 1513595382438000,\n        \"value\": \"ss\",\n        \"endpoint\": {\n          \"serviceName\": \"pc.info.v2\",\n          \"ipv4\": \"10.20.1.14\",\n          \"port\": 57101\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"http_path\",\n        \"value\": \"/request\",\n        \"endpoint\": {\n          \"serviceName\": \"pc.info.v2\",\n          \"ipv4\": \"10.20.1.14\",\n          \"port\": 57101\n        }\n      },\n      {\n        \"key\": \"info\",\n        \"value\": \"bbb\",\n        \"endpoint\": {\n          \"serviceName\": \"pc.info.v2\",\n          \"ipv4\": \"10.20.1.14\",\n          \"port\": 57101\n        }\n      }\n    ],\n    \"debug\": true\n  },\n  {\n    \"traceId\": \"f28a3fece3e311e7\",\n    \"id\": \"f28b1a02e3e311e7\",\n    \"name\": \"check request v2\",\n    \"parentId\": \"f28aae3ce3e311e7\",\n    \"timestamp\": 1513595382429000,\n    \"duration\": 700,\n    \"annotations\": [\n      {\n        \"timestamp\": 1513595382429000,\n        \"value\": \"wr\",\n        \"endpoint\": {\n          \"serviceName\": \"dspserver.v2\",\n          \"ipv4\": \"10.20.1.14\",\n          \"port\": 57101\n        }\n      },\n      {\n        \"timestamp\": 1513595382429700,\n        \"value\": \"mr\",\n        \"endpoint\": {\n          \"serviceName\": \"dspserver.v2\",\n          \"ipv4\": \"10.20.1.14\",\n          \"port\": 57101\n        }\n      }\n    ],\n    \"debug\": true\n  },\n  {\n    \"traceId\": \"f28a3fece3e311e7\",\n    \"id\": \"f28c0b2ee3e311e7\",\n    \"name\": \"get\",\n    \"timestamp\": 1513595382430000,\n    \"duration\": 5000,\n    \"annotations\": [\n      {\n        \"timestamp\": 1513595382430000,\n        \"value\": \"cs\",\n        \"endpoint\": {\n          \"serviceName\": \"brave-webmvc-example\",\n          \"ipv4\": \"192.168.1.113\"\n        }\n      },\n      {\n        \"timestamp\": 1513595382435000,\n        \"value\": \"cr\",\n        \"endpoint\": {\n          \"serviceName\": \"brave-webmvc-example\",\n          \"ipv4\": \"192.168.1.113\"\n        }\n      }\n    ],\n    \"binaryAnnotations\": [\n      {\n        \"key\": \"error\",\n        \"value\": \"500\",\n        \"endpoint\": {\n          \"serviceName\": \"brave-webmvc-example\",\n          \"ipv4\": \"192.168.1.113\"\n        }\n      },\n      {\n        \"key\": \"http_path\",\n        \"value\": \"/a\",\n        \"endpoint\": {\n          \"serviceName\": \"brave-webmvc-example\",\n          \"ipv4\": \"192.168.1.113\"\n        }\n      },\n      {\n        \"key\": \"sa\",\n        \"value\": true,\n        \"endpoint\": {\n          \"serviceName\": \"\",\n          \"ipv4\": \"127.0.0.1\",\n          \"port\": 60149\n        }\n      }\n    ]\n  }\n]\ntemplate\n{\n    \"zipkin:span-2017-12-18\": {\n        \"aliases\": {},\n        \"mappings\": {\n            \"_default_\": {\n                \"_all\": {\n                    \"enabled\": false\n                },\n                \"dynamic_templates\": [\n                    {\n                        \"strings\": {\n                            \"mapping\": {\n                                \"norms\": {\n                                    \"enabled\": false\n                                },\n                                \"ignore_above\": 256,\n                                \"index\": \"not_analyzed\",\n                                \"type\": \"string\"\n                            },\n                            \"match\": \"*\",\n                            \"match_mapping_type\": \"string\"\n                        }\n                    }\n                ]\n            },\n            \"span\": {\n                \"_all\": {\n                    \"enabled\": false\n                },\n                \"_source\": {\n                    \"excludes\": [\n                        \"_q\"\n                    ]\n                },\n                \"dynamic_templates\": [\n                    {\n                        \"strings\": {\n                            \"mapping\": {\n                                \"norms\": {\n                                    \"enabled\": false\n                                },\n                                \"ignore_above\": 256,\n                                \"index\": \"not_analyzed\",\n                                \"type\": \"string\"\n                            },\n                            \"match\": \"*\",\n                            \"match_mapping_type\": \"string\"\n                        }\n                    }\n                ],\n                \"properties\": {\n                    \"@timestamp\": {\n                        \"type\": \"date\",\n                        \"format\": \"strict_date_optional_time||epoch_millis\"\n                    },\n                    \"_q\": {\n                        \"type\": \"string\",\n                        \"index\": \"not_analyzed\"\n                    },\n                    \"annotations\": {\n                        \"type\": \"object\",\n                        \"enabled\": false\n                    },\n                    \"beat\": {\n                        \"properties\": {\n                            \"hostname\": {\n                                \"type\": \"string\",\n                                \"index\": \"not_analyzed\",\n                                \"ignore_above\": 256\n                            },\n                            \"name\": {\n                                \"type\": \"string\",\n                                \"index\": \"not_analyzed\",\n                                \"ignore_above\": 256\n                            },\n                            \"version\": {\n                                \"type\": \"string\",\n                                \"index\": \"not_analyzed\",\n                                \"ignore_above\": 256\n                            }\n                        }\n                    },\n                    \"debug\": {\n                        \"type\": \"boolean\"\n                    },\n                    \"duration\": {\n                        \"type\": \"long\"\n                    },\n                    \"id\": {\n                        \"type\": \"string\",\n                        \"index\": \"not_analyzed\",\n                        \"ignore_above\": 256\n                    },\n                    \"input_type\": {\n                        \"type\": \"string\",\n                        \"index\": \"not_analyzed\",\n                        \"ignore_above\": 256\n                    },\n                    \"kind\": {\n                        \"type\": \"string\",\n                        \"index\": \"not_analyzed\",\n                        \"ignore_above\": 256\n                    },\n                    \"localEndpoint\": {\n                        \"dynamic\": \"false\",\n                        \"properties\": {\n                            \"serviceName\": {\n                                \"type\": \"string\",\n                                \"index\": \"not_analyzed\"\n                            }\n                        }\n                    },\n                    \"name\": {\n                        \"type\": \"string\",\n                        \"index\": \"not_analyzed\"\n                    },\n                    \"offset\": {\n                        \"type\": \"long\"\n                    },\n                    \"parentId\": {\n                        \"type\": \"string\",\n                        \"index\": \"not_analyzed\",\n                        \"ignore_above\": 256\n                    },\n                    \"remoteEndpoint\": {\n                        \"dynamic\": \"false\",\n                        \"properties\": {\n                            \"serviceName\": {\n                                \"type\": \"string\",\n                                \"index\": \"not_analyzed\"\n                            }\n                        }\n                    },\n                    \"shared\": {\n                        \"type\": \"boolean\"\n                    },\n                    \"source\": {\n                        \"type\": \"string\",\n                        \"index\": \"not_analyzed\",\n                        \"ignore_above\": 256\n                    },\n                    \"tags\": {\n                        \"type\": \"object\",\n                        \"enabled\": false\n                    },\n                    \"timestamp\": {\n                        \"type\": \"long\"\n                    },\n                    \"timestamp_millis\": {\n                        \"type\": \"date\",\n                        \"format\": \"epoch_millis\"\n                    },\n                    \"traceId\": {\n                        \"type\": \"string\",\n                        \"index\": \"not_analyzed\"\n                    },\n                    \"type\": {\n                        \"type\": \"string\",\n                        \"index\": \"not_analyzed\",\n                        \"ignore_above\": 256\n                    }\n                }\n            }\n        },\n        \"settings\": {\n            \"index\": {\n                \"number_of_shards\": \"5\",\n                \"mapper\": {\n                    \"dynamic\": \"false\"\n                },\n                \"creation_date\": \"1513595408874\",\n                \"requests\": {\n                    \"cache\": {\n                        \"enable\": \"true\"\n                    }\n                },\n                \"analysis\": {\n                    \"filter\": {\n                        \"traceId_filter\": {\n                            \"type\": \"pattern_capture\",\n                            \"preserve_original\": \"true\",\n                            \"patterns\": [\n                                \"([0-9a-f]{1,16})$\"\n                            ]\n                        }\n                    },\n                    \"analyzer\": {\n                        \"traceId_analyzer\": {\n                            \"filter\": \"traceId_filter\",\n                            \"type\": \"custom\",\n                            \"tokenizer\": \"keyword\"\n                        }\n                    }\n                },\n                \"number_of_replicas\": \"1\",\n                \"uuid\": \"4dK1GnaPTmC6C0UnqtlISg\",\n                \"version\": {\n                    \"created\": \"2030599\"\n                }\n            }\n        },\n        \"warmers\": {}\n    }\n}\n. ",
    "WangHouLing": "If you want to configure jvm, you can run this command(just an example):\n$ sudo STORAGE_TYPE=mysql MYSQL_USER=root MYSQL_PASS=123456 nohup java -Xms2g -Xmx2g -jar zipkin.jar &. ",
    "dumbdonkey": "@adriancole I have add a whitelist by myself. which still consume lot of space. I plan to develop a customed TokenizingAnalyzer. . @adriancole I have finished it.    But I'm a little busy now. After the completion of the work at hand, I will share with solutions. \n. Sorry for late reply. \nHere is my solution. \nIn our situation, we won't query with partial annotation , we always query with a complete annotation. so prefix index and proper TokenAnalyzer is enough. Here are steps you can follow. \n\ncompile following code and package it into a jar then put it in cassandra library path and restart cassandra.\n\n```\npackage org.apache.cassandra.index.sasi.analyzer;\nimport java.nio.ByteBuffer;\nimport java.util.ArrayList;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;\nimport com.google.common.base.Strings;\nimport org.apache.cassandra.db.marshal.AbstractType;\npublic class ZipkinStandardAnalyzer extends AbstractAnalyzer\n{\n    private static final Locale DEFAULT_LOCALE = Locale.getDefault();\n    private static final String ZIPKIN_DELIMITER = \"\u2591\";\n    private AbstractType validator;\n    private List tokens = new ArrayList<>();\n    private Iterator iter;\npublic ZipkinStandardAnalyzer()\n{\n}\n\npublic ByteBuffer next()\n{\n    return this.validator.fromString(iter.next());\n}\n\npublic void init(Map<String, String> options, AbstractType validator)\n{\n    this.validator = validator;\n}\n\npublic boolean hasNext()\n{\n    return iter.hasNext();\n}\n\npublic void reset(ByteBuffer input)\n{\n    this.next = null;\n    this.tokens.clear();\n    String ann = validator.getString(input);\n    ann = ann.toLowerCase(DEFAULT_LOCALE);\n    String[] parts = ann.split(ZIPKIN_DELIMITER);\n    for (String part : parts)\n    {\n        if (!Strings.isNullOrEmpty(part))\n        {\n            tokens.add(part);\n        }\n    }\n    this.iter = tokens.iterator();\n}\n\n\npublic boolean isTokenizing()\n{\n    return true;\n}\n\n}\n```\n\nreplace old annotation index with following statement\n\n```\nCREATE CUSTOM INDEX span_annotation_query_idx ON zipkin2.span (annotation_query) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.ZipkinStandardAnalyzer', 'case_sensitive': 'false', 'mode': 'prefix', 'analyzed': 'true'};\n```\n\nmodify class zipkin2.storage.cassandra.SelectTraceIdsFromSpan as following and deploy your zipkin with new code again.\n\n```\nCall>> newCall(\n      @Nullable String serviceName,\n      String annotationKey,\n      TimestampRange timestampRange,\n      int limit\n    ) {\n      Input input = new AutoValue_SelectTraceIdsFromSpan_Input(\n        serviceName,\n        // Notice here,we just need to remove all \u2591\n         annotationKey,\n        timestampRange.startUUID,\n        timestampRange.endUUID,\n        limit\n      );\n      return new SelectTraceIdsFromSpan(\n        this,\n        serviceName != null ? withServiceAndAnnotationQuery : withAnnotationQuery,\n        input\n      ).flatMap(new AccumulateTraceIdTsLong());\n    }\n```\nThat's all. Hope I've made myself clear.\n. @adriancole following is current disk space usage\nIndex Size/ Data Size ratio is 0.15. I remeber old ratio is 60, even bigger. \n\nSince we save a lot of spans every day. query latency is very high with annotation_query index.  most of the time, we just got a timeout error. I have move column 'annotation_query' to table 'trace_by_service_span' . that gives us a reasonable response time.\nex:\nan query like \nhttp://*/zipkin/?serviceName=all&spanName=all&startTs=1515834112059&endTs=1515837712059&minDuration=&limit=10&annotationQuery=error&sortOrder=duration-desc\nquery with annotation_index on table span will show timeout error at last.\nquery with annotation_index on table trace_by_service_span has a latency lower than 5 seconds, which is acceptable.\n\n. ",
    "mclarke47": "Kubernetes services helm example. Other services can then just access zipkin at http://zipkin:9411. ",
    "chuguoren": "I register zipkin server to spring cloud eureka server, for other services to discover, and also get configuation from spring cloud config server. We use it by custom maven build, the spring boot version is 1.5.X. When I use spring boot 2.0 build, the UI displays error: \"Cannot load service names: No message available\". I register zipkin server to spring cloud eureka server, for other services to discover, and also get configuation from spring cloud config server. We use it by custom maven build, the spring boot version is 1.5.X. When I use spring boot 2.0 build, the UI displays error: \"Cannot load service names: No message available\". Why not support custom builds?\nI want to register zipkin to eureka server, and also get configuration from spring config server.. Why not support custom builds?\nI want to register zipkin to eureka server, and also get configuration from spring config server.. ",
    "thekalinga": "@adriancole \nThanks for the script\nA minor update to the script would register zipkin under the service id zipkin-server instead of unknown. For this we need to add -Dspring.application.name=zipkin-server when launching zipkin.jar\nThe final line in the script would be \nsh\njava -Dloader.path='eureka.jar,eureka.jar!/lib' -Dspring.application.name=zipkin-server -cp zipkin.jar org.springframework.boot.loader.PropertiesLauncher. I merged everything to a single script including pom.xml\nNow first time, we need to run this (which downloads zipkin & builds eureka.jar)\n```sh\n!/bin/sh\nget normal zipkin server\ncurl -sSL https://zipkin.io/quickstart.sh | bash -s\nWrite pom.xml thats responsible for generating eureka.jar\ncat << EOF > pom.xml\n\n4.0.0\nio.zipkin.custom\neureka\n1.0-SNAPSHOT\nExample module that adds Eureka to an existing Zipkin\n\n make sure this matches zipkin-server's spring boot version \n2.1.3.RELEASE\n\n\n\n\n This makes sure versions are aligned properly \norg.springframework.boot\nspring-boot-dependencies\n${spring-boot.version}\npom\nimport\n\n\n\n\n this is the thing that adds Eureka \n\norg.springframework.cloud\nspring-cloud-starter-netflix-eureka-client\n2.1.0.RELEASE\n\n zipkin already has this \n\norg.springframework.boot\nspring-boot-starter\n\n\n\n\n\n\n\norg.springframework.boot\nspring-boot-maven-plugin\n${spring-boot.version}\n\n\n\nrepackage\n\n\n\n\n\ncustom\n\nmodule\n exclude dependencies already packaged in zipkin-server. \n https://github.com/spring-projects/spring-boot/issues/3426 transitive exclude doesn't work \nio.zipkin.zipkin2,io.zipkin.reporter2,org.springframework.boot,org.springframework,com.fasterxml.jackson.core,com.google.auto.value,com.google.gson,com.google.guava,org.slf4j\n          \n\n\n\nio.zipkin.layout\nzipkin-layout-factory\n0.0.4\n\n\n\n\n\n\nEOF\nBuild eureka module\nmvn clean install\nRename the jar so it is easier\nmv target/eureka-1.0-SNAPSHOT-module.jar eureka.jar\n```\nSubsequently, we need to run just this to start & register server with eureka\n```sh\n!/bin/sh\nStart zipkin which now has eureka support\njava -Dloader.path='eureka.jar,eureka.jar!/lib' -Dspring.application.name=zipkin-server -cp zipkin.jar org.springframework.boot.loader.PropertiesLauncher\n```. ",
    "SergeyKanzhelev": "Do you expect span's name be the same as http.template in the most cases? Or they semantically different?. ",
    "jkschneider": "\nThe Correct Way to solve this would be to use any identifier guaranteed to uniquely identify the endpoint\n\nEffectively I think that is what http.template is in this proposal. Making the parameterized URI the de facto identifier across many frameworks makes your traces/metrics look somewhat uniform across the org (i.e. those services using Swagger and those that are not).\n  . > The Correct Way to solve this would be to use any identifier guaranteed to uniquely identify the endpoint\nEffectively I think that is what http.template is in this proposal. Making the parameterized URI the de facto identifier across many frameworks makes your traces/metrics look somewhat uniform across the org (i.e. those services using Swagger and those that are not).\n  . Just want to make a note that Spring's HandlerMapping.BEST_MATCHING_PATTERN_ATTRIBUTE does not normalize away extraneous slashes.. Just want to make a note that Spring's HandlerMapping.BEST_MATCHING_PATTERN_ATTRIBUTE does not normalize away extraneous slashes.. > need to make constants like redirect or not_found\nTo elaborate a bit: not shunting 404s or 302s to a fixed tag name essentially becomes a DOS-style attack vector on your monitoring system. I guess this is true for any \"user-provided\" tag value. Probably more importantly, it's just easier to dimensionally drill down on auth failures and such.. @shakuzen Would love to see that undertow filter in micrometer-core.. ",
    "ivantopo": "Hello there! Happy to see this moving :smile: \nIs the scope of this fixed to HTTP-related Spans? I can totally see a similar tag/mechanism working to gather metrics on JDBC call spans and anything else that looks like it (Cassandra driver, etc). This happens to be a pretty common topic when people are starting to instrument their apps.. they usually end up with either too little granularity or a crazy explosion of metrics unless some better naming is manually applied. Anything we can do to provide better default names out of the box is a instant win to all users!\nPS: Play Framework does a really nice job at exposing the route templates through tags, that might be worth adding in the description.. ",
    "drolando": "@adriancole This is awesome! It'd have probably taken me a week to find all the places I needed to change :). > well I like to keep master shippable\nYeah, I agree with having a working master. But I usually tend to do multiple smaller PRs that change it gradually to not have a too long testing and reviewing cycle (and huge diffs). :)\nYou're the main maintainer though, so I'm fine with whatever you prefer.. If that's the only storage where it's disabled and given that it seems kinda deprecated I'm fine with leaving this.\nAre you fine if I just send a CR to change the error message to be \"getTraces with duration is unsupported. Upgrade to the new cassandra3 schema.\"?. If that's the only storage where it's disabled and given that it seems kinda deprecated I'm fine with leaving this.\nAre you fine if I just send a CR to change the error message to be \"getTraces with duration is unsupported. Upgrade to the new cassandra3 schema.\"?. Isn't the JSON returned by the UI valid V2 format? Couldn't you simply POST it back to /api/v2/spans and use the UI as usual?. Where is that message supposed to appear? I'm running 2.5.2 with SEARCH_ENABLED=false but I don't see any difference from the normal UI. I'm not using the default schema. I changed the keyspace to use the NetworkTopologyStrategy, but I still want RF=1.\nI think I haven't explained myself well, what I want to make configurable is https://github.com/openzipkin/zipkin/blob/f309c19345069eb09604f09858d427bdb1d3e18a/zipkin-storage/zipkin2_cassandra/src/main/java/zipkin2/storage/cassandra/DefaultSessionFactory.java#L126. @michaelsembwever my idea was to allow people to configure it to whatever they want, so they could simply set CASSANDRA_CONSISTENCY=QUORUM or CASSANDRA_CONSISTENCY=ANY.\nWhat level they set then it's not a zipkin concern, all consistency levels come with different guarantees and tradeoffs.. @michaelsembwever my idea was to allow people to configure it to whatever they want, so they could simply set CASSANDRA_CONSISTENCY=QUORUM or CASSANDRA_CONSISTENCY=ANY.\nWhat level they set then it's not a zipkin concern, all consistency levels come with different guarantees and tradeoffs.. What we experimented doing is showing a small warning sign left to spans without a parent rather than showing a big banner. That's less intrusive and much more intuitive. It also helps you understand if you're just missing a very small portion of the trace or a lot of spans.\nIf you're wondering why there's a blank hole in the trace and notice that the next span is missing its parent that'd explain it. It also helps when spans are rendered in weird places in the UI (usually at the bottom) since the reason there is that without a parent id it's impossible to attach them in the right place. This seems a better approach than a single banner since that one wouldn't give you enough context to understand how broken the trace is and what's actually missing.\n\nJust make sure you put the appropriate message in the warning\n\nIn this case the error is simply something like \"The parent for this span is missing\", which would work whatever the reason for dropping that span.. @adriancole @michaelsembwever I'll wait until C 3.11.3 is out before upgrading to Cassandra 3 then.. @adriancole @michaelsembwever I'll wait until C 3.11.3 is out before upgrading to Cassandra 3 then.. @michaelsembwever Any idea about when they'll release the new version? I saw a blog post from 2 years ago about Cassandra doing monthly releases but that doesn't seem to be the case at all.. @michaelsembwever Any idea about when they'll release the new version? I saw a blog post from 2 years ago about Cassandra doing monthly releases but that doesn't seem to be the case at all.. Is there a related issue I can read to understand what you're trying to do?. Lol there's a PR to allow this in chosen which is open since 2011 and never got merged: https://github.com/harvesthq/chosen/pull/166. Lol there's a PR to allow this in chosen which is open since 2011 and never got merged: https://github.com/harvesthq/chosen/pull/166. The main suggestion seems to be to use select2 :/. The main suggestion seems to be to use select2 :/. Yeah, the best approach is probably for us to implement https://github.com/Yelp/pyramid_zipkin/issues/93.\nThat might solve it for most services (as long as they don't have > 1k endpoints). Yeah, the best approach is probably for us to implement https://github.com/Yelp/pyramid_zipkin/issues/93.\nThat might solve it for most services (as long as they don't have > 1k endpoints). EDIT: Ignore this.\nWhat's the advantage over endpoint.ip? Just that you get a human readable string?\nI'm not sure how kubernetes works, but in our paas the hostname is a combination of host ip and docker container id. Which means it's very high cardinality and so indexing would probably be very expensive. And I'd assume this problem to get more common as more companies move to containerized services.\nThat's the main reason why we don't add the hostname as tag right now.\nSent from my iPhone\n\nOn Aug 11, 2018, at 12:06 PM, Jos\u00e9 Carlos Ch\u00e1vez notifications@github.com wrote:\nYeah that sounds reasonable. For example, in Go http middleware the\nhttp.response_size is not recorded by default (because it the response\nbody is a buffer and if you want to measure it you gotta read it and that\ndrains the buffer) so I see this per-tag option comming. recordedTags:...\nsounds reasonable to me.\nDen l\u00f8r. 11. aug. 2018, 11:53 skrev Adrian Cole notifications@github.com:\n\n\n@adriancole https://github.com/adriancole I should have been more\nclear. I meant that having a flag in the options of the middleware that\nsays recordHost: true|false so user could decide whether it is meaningful\nor not to record such information\n\n\nah ok. well I see your point now. hmmm I guess if we decided to not make it\nstandard then people will add it anyway like they do today or we can do\nsomething like you mention to make it easier. otoh if we were going the\nroute of not recording by default I might be more inclined to list the\n\"standard tags\" desired vs a flag for each. this could be programmatic or\ndeclarative like http.client.tags=http.method,http.path,http.host I think\nsome people maybe python does this already.\nat any rate suspect we should get a sense of who wants the default to\ninclude host or more importantly who doesn't. I think we can sort a nice\nway our regardless of the decision.\n\n\u2014\nYou are receiving this because you are on a team that was mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2167#issuecomment-412264607,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AC7sAjbbaChpxE_HOY3VBce3zk4SX4Phks5uPqmsgaJpZM4V49h7\n.\n-- \n\nJos\u00e9 Carlos\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @anuraaga you're right, I misunderstood the description.. Thanks, looking at those properties I was able to figure out that I simply needed to specify the -D zipkin.collector.kafka.overrides values before the -jar zipkin-server.jar otherwise they're ignored.. If you don't need any feature of the new schema, I'm pretty sure you can use the old \"cassandra\" schema on C* 3.0.. Does this move back the client/server span merging to Java? So the UI expects to receive pre-merged spans?. Does this move back the client/server span merging to Java? So the UI expects to receive pre-merged spans?. There's still a bug. The button now collapses all its immediate children that are \"leaves\", but won't hide children that are \"roots\" of a subtree.. You can reproduce that with the messaging-kafka.json example trace. If you click on servicea on-message (the 2nd span) it doesn't collapse the entire subtree, it only hides a few spans. Having an easy way to add my custom plugin would be amazing.\n\nI wanted to add a button to automatically save a trace without forcing users to download the json and re-upload it manually somewhere. The JS change is pretty simple, adding it to zipkin-ui is not.\nI have to download the jar, unzip it, run sed against index.html to make it import my new js file, add the js file, rebundle everything in a jar and override the one embedded in zipkin-server when I start it. It works now but it's super hacky and ugly.. Having an easy way to add my custom plugin would be amazing.\nI wanted to add a button to automatically save a trace without forcing users to download the json and re-upload it manually somewhere. The JS change is pretty simple, adding it to zipkin-ui is not.\nI have to download the jar, unzip it, run sed against index.html to make it import my new js file, add the js file, rebundle everything in a jar and override the one embedded in zipkin-server when I start it. It works now but it's super hacky and ugly.. cc @zeagord . cc @zeagord . What about just adding the url for gitter? Would that fit?. What about just adding the url for gitter? Would that fit?. That string is still more than 200 chars...\n```python\n\n\n\nlen(\"This issue tracker is not the place for questions. If you want to ask how to do something, or to understand why something isn't working the way you expect it to, please use https://gitter.im/openzipkin/zipkin or https://stackoverflow.com/questions/tagged/zipkin.\")\n262\n```. That string is still more than 200 chars...\n\n\n\n```python\n\n\n\nlen(\"This issue tracker is not the place for questions. If you want to ask how to do something, or to understand why something isn't working the way you expect it to, please use https://gitter.im/openzipkin/zipkin or https://stackoverflow.com/questions/tagged/zipkin.\")\n262\n```. Lgtm!\n\n\n\nCan you also add a tip in the textbox showing the possible options?. > Yes, this makes \"collapse all\" by default\nCan you make this configurable? It shouldn't be too hard.\nI'm pretty sure that defaulting to everything collapsed I'd get a ton of questions from developers asking why there's only one span and if zipkin is broken. Given that the UI has defaulted to everything expanded for a long time (and that most people don't even know those 2 button exists) changing it to collapsed would be an unexpected change.. > the original logic was to only expand spans matching the service from the previous screen.\nI'm -1 on this. That a button called \"expand ALL\" only expands some subtrees is very unintuitive. If people want this I'd add a third button \"expand service\" or something like that.\n\n@zeagord thanks for trying to fix this!. Can you elaborate a bit more on what you mean by \"remote service name\" and \"local service name\"?\nThere are 3 different service names (maybe 4) in a RPC call:\n1) CLIENT local service name\n2) CLIENT remote service name\n3) SERVER local service name\nWhich one would you show in the UI? I think the one that makes the most sense is 3, using 2 in case there's no server span. Which I believe it's the current UI behavior.. Yeah, I'm using Cassandra 3.11.3 with the new schema. ~200 service names in dev.\nI didn't select anything at all. I just opened the search page, left everything to their default and just inserted \"100ms\" as duration filter. (Same issue happen whatever I write in that textbox, so it's not an issue woth @tacigar's PR).\nThe errors are all timeouts while reading from Cassandra.. Yeah, I'm using Cassandra 3.11.3 with the new schema. ~200 service names in dev.\nI didn't select anything at all. I just opened the search page, left everything to their default and just inserted \"100ms\" as duration filter. (Same issue happen whatever I write in that textbox, so it's not an issue woth @tacigar's PR).\nThe errors are all timeouts while reading from Cassandra.. @adriancole https://github.com/msindwan/zipkin-view has that same accordion that you're describing I think. And there's a screenshot there in the README. @adriancole https://github.com/msindwan/zipkin-view has that same accordion that you're describing I think. And there's a screenshot there in the README. Can you add a similar annotation to one of the test traces? That way we might notice of we break this again in the future.\nOr add a unit test for this with a comment\nSent from my iPhone\n\nOn Dec 3, 2018, at 7:02 PM, Adrian Cole notifications@github.com wrote:\nMerged #2310 into master.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @tacigar Can you make it so that when you click on the +1 the scroll down that lets you select the key appears automatically?\n\nNice work anyway!. I closed this issue since I realized as you say that the best option is to\nonly put the http method in the client span name.\nWe always have server spans anyway so that name doesn't really matter. The\nonly car where there's no server is when we call external services like\nGoogle or Amazon but those are custom use cases and it's up to the\ndeveloper to use a reasonable name.\nOn Wed, Jan 9, 2019 at 9:39 PM Adrian Cole notifications@github.com wrote:\n\nclosest to what I think we need here is actually to clean up data in\ngeneral. For example, if the client span name is high cardinality delete\nit, as instrumentation should not add high cardinality names. There are a\nfew projects with data cleaning aims. otherwise what this ends up as is\nquirky hooks that might work on cassandra as we manually index, but not on\nelasticsearch where we don't manually index the span names.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/issues/2337#issuecomment-452977120,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADKfhpBT_qazNGqYDcsKBBdz8gLLf6x5ks5vBtIrgaJpZM4Zc69Z\n.\n. I'm +1 on showing the remoteEndpoint name if the server span is missing. For exactly the same reason @fyi-coursera mentioned.\n\nMy mysql spans now show up as \"serviceX\" rather than \"mysql\" which is very annoying. Same for calls to outside services i.e. AWS or google.\nI remember we discussed this a month or so ago and I thought this was already what we agreed upon. And was about to open an issue as well.. Yeah this commit still doesn't fix my traces.\nOn the other hand it the issue is simply that the ips don't match I can probably fix that, but it'd still be nice if the code didn't rely on ip values.\nWhy does it btw? You already have the parentId and spanId, why do you care about the ip?. Some people don't really want to close their browsers to not lose all their tabs.\nAlso this is not intuitive at all and we'd get a bunch of questions on how to go back and forth\nSent from my iPhone\n\nOn Feb 19, 2019, at 6:10 PM, Adrian Cole notifications@github.com wrote:\n\nOne piece of feedback is that it doesn't seem obvious to a user how to go\nback to the classic UI right now. What do you think about also adding a\nbutton to Lens that's the complement of the current button? \"Go back to\nclassic UI\" or something.\nI think lens would be cluttered as the sidebar has already a lot of things\nin it. I would prefer to just let someone close their browser as this is\nreally a short term thing..\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. duplicated string, delete. - Un'annotazione\n- iterno --> interno\n\n\nI'd say: \"Il suo valore e' di tipo stringa ed e' quello che puoi utilizzare come parametro di ricerca. Possono essere cose come\". - Annotazioni\n- una traccia\n- esempi\n- un'eccezione. translation missing. translation missing. span lowercase. nit: you usually don't add the plural \"s\" when using English words in Italian. I'd say \"Span totali:\". Btw, I know that the English version is the same, but is it normal that the sentence doesn't end?. we also need to say something about why you'd ever want to do this. suggestion\nmodal.p6 = Le etichette sono un metadato extra attaccato ad un traccia. Esempio: l'url chiamato, il codice di risposta o un'eccezione. Guarda sotto per alcuni esempio.. If we want to have a whitelist of tags to index and not just index all tags you might get away without doing this at all. And just return the list of whitelisted tags.. I don't think this is needed since this is a brand new table that already has a default ttl, no?. will this work if zipkin lens is not running on localhost?. :(\nIs the code completely duplicated between the ui and lens? Given that the core modules should be identical, could we symlink them to avoid drift?. Involuntarily you raise a good point: this is the wrong place to fix this. I should fix it in the java API so then it's fixed for everything.. I just tried that and it looks like sorting an array of strings in java is not as easy as I thought: https://circleci.com/gh/openzipkin/zipkin/3097?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link\njava\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:compile (default-compile) on project zipkin-server: Compilation failure\n[ERROR] /home/circleci/project/zipkin-server/src/main/java/zipkin2/server/internal/ZipkinQueryApiV2.java:[80,10] error: no suitable method found for sort(List<String>)\n[ERROR]     method Arrays.sort(int[]) is not applicable\n[ERROR]       (argument mismatch; List<String> cannot be converted to int[])\n[ERROR]     method Arrays.sort(long[]) is not applicable\n[ERROR]       (argument mismatch; List<String> cannot be converted to long[])\n[ERROR]     method Arrays.sort(short[]) is not applicable\n[ERROR]       (argument mismatch; List<String> cannot be converted to short[])\n[ERROR]     method Arrays.sort(char[]) is not applicable\n[ERROR]       (argument mismatch; List<String> cannot be converted to char[])\n[ERROR]     method Arrays.sort(byte[]) is not applicable\n[ERROR]       (argument mismatch; List<String> cannot be converted to byte[])\n[ERROR]     method Arrays.sort(float[]) is not applicable\n[ERROR]       (argument mismatch; List<String> cannot be converted to float[])\n[ERROR]     method Arrays.sort(double[]) is not applicable\n[ERROR]       (argument mismatch; List<String> cannot be converted to double[])\n[ERROR]     method Arrays.sort(Object[]) is not applicable\n[ERROR]       (argument mismatch; List<String> cannot be converted to Object[])\n[ERROR]     method Arrays.<T#1>sort(T#1[],Comparator<? super T#1>) is not applicable\n[ERROR]       (cannot infer type-variable(s) T#1\n[ERROR]         (actual and formal argument lists differ in length))\n[ERROR]     method Arrays.<T#2>sort(T#2[],int,int,Comparator<? super T#2>) is not applicable\n[ERROR]       (cannot infer type-variable(s) T#2\n[ERROR]         (actual and formal argument lists differ in length))\n[ERROR]   where T#1,T#2 are type-variables:\n[ERROR]     T#1 extends Object declared in method <T#1>sort(T#1[],Comparator<? super T#1>)\n[ERROR]     T#2 extends Object declared in method <T#2>sort(T#2[],int,int,Comparator<? super T#2>)\nso I dropped that.. ",
    "pickmonster": "@shakuzen Yes, i want use a Spring Cloud Sleuth Stream Server to synchronize request, and i set application like this:\n@EnableZipkinStreamServer// //\u4f7f\u7528Stream\u65b9\u5f0f\u542f\u52a8ZipkinServer\n@SpringBootApplication\npublic class ZipkinStreamServerApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(ZipkinStreamServerApplication.class,args);\n    }\n}\n============\u2193 this is my application.properties===========\nserver.port=11020\nspring.application.name=microservice-zipkin-stream-server\nspring.sleuth.enabled=false\nzipkin.storage.type=mysql\nspring.datasource.schema[0]=classpath:/zipkin.sql\nwhy give me a exception?. @adriancole mysql, First I need to analyze the address and parameters based on a different URL. And then record the number of calls to different interfaces based on different addresses.Finally accumulate these quantities into the database.\nSo I should start with the storage path in the Zipkin project, then go to the MySQL path of zipkin-storage project to process all processes?. @adriancole Its mean I neednt use zipkin-storage-mysql. I just write one class which is implement StorageAdapters.SpanConsumer, then storage my object bean to database. All codes put in normal zipkin project. Is it right?. @adriancole OK, Thanks very much!!!. ",
    "ouaibsky": "Hi\nThx for the suggestion.\nI was not aware of this elastic feature: getting many templates matching same index with order apply.\nSounds good with elastic 2.4, but I spent many to get something working fine. looks like you have to make sure there is no same mapping property in the both.\nAnyway at the end it works like a charm. Thx for insight.\nCheers\nChristophe. ",
    "cgfork": "I think this pr cloud be closed. can see #1909 . @adriancole I explained the SQLs mentioned by ikatson in MySQL  v5.5.58. The two SQLs are not indexed:\nexplain select * from `zipkin_spans` where (`trace_id_high`, `trace_id`) in ((0, 8469892162089594262), (0, 8799770482667418914));\nand\nexplain select *  from `zipkin_spans` force index(trace_id_high)  where (`trace_id_high`, `trace_id`) in ((0, 8469892162089594262), (1, 8799770482667418914));\nAnd then, I explained the SQLs found in the zipkin logger(DEBUG level).  The result is as follows:\n``\nmysql> explain selectzipkin_spans.trace_id_high,zipkin_spans.trace_id,zipkin_spans.id,zipkin_spans.name,zipkin_spans.parent_id,zipkin_spans.debug,zipkin_spans.start_ts,zipkin_spans.durationfromzipkin_spanswhere (zipkin_spans.trace_id_high,zipkin_spans.trace_id`) in ((0, -6619022611187644456), (0, -71407062960380663), (0, 7031975218516113626), (0, -2742537077799283914), (0, 4118160195282248130), (0, -3938357985047845295), (0, 8527674997628098243), (0, 1663613191369793931), (0, 6089093287099230342), (0, -7576711827521986558));\n+----+-------------+--------------+------+---------------+------+---------+------+--------+-------------+\n| id | select_type | table        | type | possible_keys | key  | key_len | ref  | rows   | Extra       |\n+----+-------------+--------------+------+---------------+------+---------+------+--------+-------------+\n|  1 | SIMPLE      | zipkin_spans | ALL  | NULL          | NULL | NULL    | NULL | 231996 | Using where |\n+----+-------------+--------------+------+---------------+------+---------+------+--------+-------------+\n1 row in set (0.00 sec)\nThis query is not indexed\nmysql> explain select distinct zipkin_spans.name from zipkin_spans join zipkin_annotations on (zipkin_spans.trace_id = zipkin_annotations.trace_id and zipkin_spans.id = zipkin_annotations.span_id) where zipkin_annotations.endpoint_service_name = 'accountbasicservice' order by zipkin_spans.name asc;\n+----+-------------+--------------------+-------+--------------------------------+-----------------------+---------+-------+--------+----------------------------------------------+\n| id | select_type | table              | type  | possible_keys                  | key                   | key_len | ref   | rows   | Extra                                        |\n+----+-------------+--------------------+-------+--------------------------------+-----------------------+---------+-------+--------+----------------------------------------------+\n|  1 | SIMPLE      | zipkin_annotations | ref   | endpoint_service_name,trace_id | endpoint_service_name | 768     | const |      1 | Using where; Using temporary; Using filesort |\n|  1 | SIMPLE      | zipkin_spans       | index | NULL                           | name                  | 767     | NULL  | 232030 | Using where; Using index; Using join buffer  |\n+----+-------------+--------------------+-------+--------------------------------+-----------------------+---------+-------+--------+----------------------------------------------+\n2 rows in set (0.01 sec)\nALTER TABLE zipkin_spans ADD INDEX(name) COMMENT 'for getTraces and getSpanNames';\nALTER TABLE zipkin_annotations ADD INDEX(endpoint_service_name) COMMENT 'for getTraces and getServiceNames';\nmysql> explain select zipkin_annotations.trace_id_high, zipkin_annotations.trace_id, zipkin_annotations.span_id, zipkin_annotations.a_key, zipkin_annotations.a_value, zipkin_annotations.a_type, zipkin_annotations.a_timestamp, zipkin_annotations.endpoint_ipv4, zipkin_annotations.endpoint_ipv6, zipkin_annotations.endpoint_port, zipkin_annotations.endpoint_service_name from zipkin_annotations where zipkin_annotations.trace_id in (-7576711827521986558, -6619022611187644456, -3938357985047845295, -2742537077799283914, -71407062960380663, 1663613191369793931, 4118160195282248130, 6089093287099230342, 7031975218516113626, 8527674997628098243) order by zipkin_annotations.a_timestamp asc, zipkin_annotations.a_key asc\n    -> ;\n+----+-------------+--------------------+-------+---------------+----------+---------+------+------+-----------------------------+\n| id | select_type | table              | type  | possible_keys | key      | key_len | ref  | rows | Extra                       |\n+----+-------------+--------------------+-------+---------------+----------+---------+------+------+-----------------------------+\n|  1 | SIMPLE      | zipkin_annotations | range | trace_id      | trace_id | 8       | NULL |   10 | Using where; Using filesort |\n+----+-------------+--------------------+-------+---------------+----------+---------+------+------+-----------------------------+\n1 row in set (0.00 sec)\nALTER TABLE zipkin_annotations ADD INDEX(trace_id, span_id, a_key) COMMENT 'for dependencies job';\n```\n. y. this happened in v1 format with brave-4.1.1 and zipkin-1.24.0. The span created in the server(contains 'sr' and 'ss') arrived zipkin after the span created in the client(contains 'cs' and 'cr\"), so the 'start_ts' and the 'duration' was replaced by the timestamp of 'sr' and the duration from 'sr' to 'ss' I think.. ok. I think the index is not available without trace_id_high when seach or join because the trace_id_high is the first key of the INDEX(trace_id_high, trace_id, id)  and INDEX(trace_id_high, trace_id). So the trace_id_high added can optimize the join or the query. I think it is necessary to refactor the Schema.joinCondition(). Or change the index?. ",
    "CyberDick": "I use Elasticsearch 5.4.1.. Thanks! I will try that.. I find the problem, but I don't know how to solve it. When tryAcquire() is false, zipkin will return none.\nThere are screenshots of log and code.\n\n\n\n. Great! I can make separate server for that!!! Thx!!!. ",
    "mvallebr": "This LGTM, from user's point of view. Please let me know as soon as you release a version with this, I am looking forward to it. Can I suggest you to add more debug logs on key places? It was really hard for me to find cassandra.ensureSchema was really set to false, I thought it could be an error to set the variable... I think it's worth logging it as well, as debug\nNot on this PR, but I think it would be great as well to have a command line option on zipkin server to dump all spring config. It's not something hard to do, but extremely helpful for the user.. Just out of curiosity, when ensureSchema is enabled, aren't you creating 2 keyspaces here?. ",
    "huydx": "@adriancole thanks for the comment, rollbacked incorrect part.. IMHO I want to preserve the same mapping with span. From user's point of view, not normalize things may lead to less surprise. . Idea from me: a dropdown button with 2 options: Download and Download With Masking \nDownload with Masking will simply mask data with random string (except trace id etc) for display purpose debugging. The logic will be implemented in UI side.. you may want to pass meter registry to armeria too (sb.meterRegistry(registry)). not sure it must be AggregatedHttpMessage or we could just use HttpResponse.of() here ? cc @hyangtack . ",
    "feyman2016": "[A]. ",
    "buerjindeqiu": "[A] . ",
    "saddampojee": "Thanks @igorwwwwwwwwwwwwwwwwwwww . ",
    "jonathan-lo": "Our messaging traces were pre consumer / producer span types, so we them as one-way client interactions.\nThe proposal above makes sense to me, but not sure if it's a little disconcerting having a producer span without a corresponding consumer one. . Our messaging traces were pre consumer / producer span types, so we them as one-way client interactions.\nThe proposal above makes sense to me, but not sure if it's a little disconcerting having a producer span without a corresponding consumer one. . ",
    "joschi": "\nAFAIK generating asciidoc from MD is little tricky everytime.\n\nJust leaving this comment without further weighing in:\nPandoc is able to convert Markdown (in various flavors) into AsciiDoc.\nAs an example I've taken the README.md file from the 2.5.0 tag and converted it from Markdown to AsciiDoc using the following command:\ntext\npandoc -f markdown -t asciidoc -o README.adoc README.md\nThe result can be seen in this gist:\nhttps://gist.github.com/joschi/7d1e18d2e3f78eea9a5ccce38d177da0\nIt might not work 100% perfect every time, but it's a huge timesaver if you intend to migrate to Asciidoctor.\nAs a final note, the authors behind Asciidoctor have started a new project called Antora which might prove useful for consolidating the documentation from multiple repositories into a single static documentation site.. ",
    "JaxXu": "Because we recently used SpringCloud 2.0.0 and found this problem, there is a problem with the startup dependencies by Spring, or can there be other solutions?. Thanks for your reply, let's try it without Spring Boot. There are more bugs, waiting for the official version to be used again @bodiam . ",
    "bodiam": "Hi, just wanted to let you know I'm running into the same problem. All our applications are running on Spring Boot 2.0, which is why are running into this. I'll downgrade to 1.x for now.. Hi @shakuzen , sorry for the confusion. What I was trying to do is the following:\n```\n@SpringBootApplication\n@EnableZipkinStreamServer\nclass InfrazipkinApplication\nfun main(args: Array) {\n    runApplication(*args)\n}\n```\nThis is a Spring Boot 2.0 application.\nThe @EnableZipkinStreamServer doesn't seem to work, because of the missing class, which is also reported here: https://stackoverflow.com/questions/45581389/where-is-undertowembeddedservletcontainerfactory-in-spring-boot-2-0-0. Hi @adriancole , ah, that's a shame to hear. My organisation is quite good at deploying Spring (Boot) services, but less proficient at deploying anything different than that. What kind of deployment for the zipkin server would you recommend in such a case, if the Spring Boot annotation will be deprecated?. @adriancole Technically, it might sound the same, but it's not. We don't deploy jars, wars, we commit source code, and our build pipeline triggers everything. So, having an Annotated Spring Boot app, pushing that to our git repo, will trigger a build, deploy, etc. For a jar, it's very different. We don't put them in source control, there's no source to commit, etc. I'm not saying it's ideal, but I don't think it's unique either, and it's not something I can change overnight. Rolling out new infrastructure, like Eureke, Zipkind, Zuul, etc, is far more complex than rolling out a new Microservice. \nCould you explain me why this dependency on a particular Spring version exists? Afaik, something like @EnableEurekaServer works on Spring Boot 1.x and Spring Boot 2.x. Would it be hard to support both versions?\n. @shakuzen Thanks for the amazing writeup. Very clear! (A small note though: it's not that I want to deploy source code per se this way, but it's a big organisation, and it can't support all possible deployment types, and the current approach, while maybe limited for some usecases, works well for the services we're building.\nThanks for the note on @EnableZipkinStreamServer btw, I'll change that to @EnableZipkinServer, and more than happy to run it on Spring Boot 1.x.. @adriancole Ah, thanks for pointing that out, it was a typo indeed! I've updated my comment!. ",
    "DelayQueue": "Oh, yeah. Thanks for reminding me .  I use hbase to save data ,but hbase cannot keep up. Maybe I should modify AsyncSpanConsumer without thread pool..    I add limitation to thread pool to avoid backlog. \nZipkinHbaseStorageAutoConfiguration\n  @Bean\n  @ConditionalOnMissingBean(Executor.class)\n  Executor executor() {\n    ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();\n    executor.setThreadNamePrefix(\"ZipkinHbaseStorage-\");\n    executor.setCorePoolSize(20); // add limitation\n    executor.setMaxPoolSize(20); // add limitation\n    executor.initialize();\n    return executor.getThreadPoolExecutor();// change return value\n  }\n```\nInternalBlockingToAsyncSpanConsumerAdapter\n@Override public void accept(final List spans, Callback callback) {\n    ThreadPoolExecutor t = (ThreadPoolExecutor) executor;\n    while (t.getQueue().size() >= 20) { // add condition\n      try {\n        Thread.sleep(3000);\n      } catch (InterruptedException e) {\n        e.printStackTrace();\n      }\n    }\n    executor.execute(new InternalCallbackRunnable(callback) {\n      @Override\n      Void complete() {\n        delegate.accept(spans);\n        return null;\n      }\n  @Override\n  public String toString() {\n    return \"Accept(\" + spans + \")\";\n  }\n});\n\n}\n```. ",
    "halower": "it also means that i need to run zipkin ui independently use i18n to change the local language?. it also means that i need to run zipkin ui independently use i18n to change the local language?. thank you very much for your reply, I set in my Google browser is chinese, but the Zipkin UI is still not detected, display english. thank you very much for your reply, I set in my Google browser is chinese, but the Zipkin UI is still not detected, display english. \ngreat, the interface is localized. ",
    "Alexhendar": "what version you use ,@halower ,i try zipkin-server-2.11.6-exec.jar  and zipkin-server-2.11.7-exec.jar  but still display in english. what version you use ,@halower ,i try zipkin-server-2.11.6-exec.jar  and zipkin-server-2.11.7-exec.jar  but still display in english. ",
    "AndyWilks79": "I have just tried it and that works! Thanks for the very quick response and resolution. Would it be a good idea to add this to the RabbitMQ Documentation as another configuration example ?. ",
    "65725738": "ok another question \u3002\ntrace\uff1a\n95.000ms 3 spans\nall 0%\niot-auth x3 95ms\n04-25-2018T14:22:52.566+0800\n25.000ms 3 spans\nall 0%\niot-auth x3 NaNms\n04-25-2018T14:22:52.541+0800\n16.000ms 3 spans \nALL 0%  what is meaning\uff1f\n. I can't open the page -----https://gitter.im/openzipkin/zipkin\nhelp me  .  ALL 0% what is meaning\uff1f\nplease!!!. ",
    "amlyj": "Hello, if I don't run zipkin-dependency.jar, can I use POM dependency to start zipkin-server only to achieve the same result?\n. yes. After all, there is a dependency button on UI, but it can not display data. It will make some people feel very strange. Thank you for your answer.. ",
    "hlpdlpu": "good. good. ",
    "jakerobb": "Also, the date/time fields don't repopulate after performing a search with a custom lookback.. ",
    "nolan4954": "I had resolved it. Just downgrade springboot2.0 to 1.5.x.\nTo refrence\nhttps://stackoverflow.com/questions/50027127/zipkin-ui-displaying-error-message-error-cannot-load-service-names-no-message\nWell, it is great to upgrade faster the version of springboot dependenced by zipkin-server.jar  :) @adriancole. I had resolved it. Just downgrade springboot2.0 to 1.5.x.\nTo refrence\nhttps://stackoverflow.com/questions/50027127/zipkin-ui-displaying-error-message-error-cannot-load-service-names-no-message\nWell, it is great to upgrade faster the version of springboot dependenced by zipkin-server.jar  :) @adriancole. @zeagord \nI tried to upgrade zipkin-server to 2.8.1 , springboot to 2.0.x\nBut @EnableZipkinServer cannot be imported. \nAnd there will be 404 error with zipkin ui home page if doesn't use @EnableZipkinServer\nSo, It's better to use 2.7.1 v and springboot 1.5.x ?. @zeagord \nI tried to upgrade zipkin-server to 2.8.1 , springboot to 2.0.x\nBut @EnableZipkinServer cannot be imported. \nAnd there will be 404 error with zipkin ui home page if doesn't use @EnableZipkinServer\nSo, It's better to use 2.7.1 v and springboot 1.5.x ?. ok,Thank All~\nI have start zipkin-server with zipkin-server.jar only, that's great.. ok,Thank All~\nI have start zipkin-server with zipkin-server.jar only, that's great.. ",
    "ankeway": "zipkin Use the released jar or docker version is enough for running it production, Don't need downgrade springboot to 1.5.x,  Custom build zipkinserver with spring Boot is unnecessary, Starting from the Edgware release, the Zipkin Stream server is deprecated. In the Finchley release, it got removed.\n@EnableZipkinServer and @EnableZipkinStreamServer is deprecated,\nWeb\njava -jar zipkin-server-2.8.3-exec.jar\nRabbitmq\njava -jar zipkin-server-2.8.3-exec.jar --RABBIT_ADDRESSES=localhost:5672 --RABBIT_USER=user --RABBIT_PASSWORD=password --RABBIT_VIRTUAL_HOST=host\nkafak\njava -jar zipkin-server-2.8.3-exec.jar --KAFKA_ZOOKEEPER=localhost:2181. ",
    "malonso1976": "Hi, \nin order to do so:\n@Override public void enqueue(Callback<V> delegate) {\n    try {\n      semaphore.acquire();\n      call.enqueue(new V2CallbackAdapter<>(semaphore, bodyConverter, delegate));\n    } catch (InterruptedException e) {\n      delegate.onError(e);\n    }\n  }\nAs a result, spans are no longer dropped under heavy load. Manuel and I have already tested it.\n. Ok, please let me know if you are interested in a pull-request for this issue with ElasticSearch and feel free to discuss on the details. Sure, I will. I hope I find time in the next days. I have a pull request ready to be submitted. I have no permissions to push it to this repo, how can I send it back to you?. Done. Hi Dan,\n   my first approach was to replace tryAquire by acquire without a timeout to block until done. That works.\nBut Adrian told me to use concurrency-limits because it will make another scenario covered by the solution: direct zipkin span write through rest api. It makes sense... I could not test it under load, but it is on my task list. Have you setup elastic capacity settings according to concurrency queue?\n. Sure, write your email in this thread and will mail you back . Yes, we are running 2 instances of zipkin. One for the querying api/ui and another one for collector processing. \nDo you still find it sensible to work on this pull request?\n. One more comment: Kafka collector can use as many thread workers (stream parameter) as topic's partitions, RabbitMQ can use an unbounded number of workers. In order to scale up Kafka there is a limitation regarding the number of consumers.\n. Hi Adrian,\n   I am really interested in solving this issue and contributing back to zipkin. I really appreciate the time you are investing in reviewing my pull request and I thank you for that.\n   Having said that, I want to be sure that the solution proposed is acceptable for you, so that the pull-request succeeds. All we are doing together is finding a suitable solution that meets your acceptance criteria.\nAlternatives:\n\nSemaphore at collector level: this would mimic the behaviour defined by elastic HttpCall at the collector level. It is easy to implement, requires and additional configuration parameter to setup the semaphore count. This solution works is like an intermediate queue/buffer between collector and storage to overcome capacity problems on the storage side, limiting maximum storage running span insertions.\nRetry policy at collector level: we need to define under which exceptions retry shall be performed. On elastic IllegalStateException(\"over capacity\"). On cassandra it might be a DriverException subclass (BusyPoolException). This solution is similar to SQS: max number of retries and an adaptative wait time, upper bounded by a max wait time. Optionally a concurrency limit algorithm can be used to control congestion.\n\nI'd rather go for Semaphore at collector level, but if I choose a solution on my own without agreeing it with you, makes this a \"trial-error\" processs none of us would like to enter into.\nWhat do you think?\nRegards,\n. Hi Adrian,\n   I have commited a pull request with semaphore implementation.\nI will review your proposal and start a new branch to integrate netflix's concurrency-limits, it looks good.\nRegards. Hi Adrian,\n   I just made a initial unit test (removed from code now) to see if ConcurrencyLimiter worked as expected. I want to test it in one of our environments under load to see if it works for us.\n   For your second comment, I did not take into consideration http collector... my fault. In our setup we do not received spans directly through http endpoint.\nDo you have any design concern how shall concurrency limiter applied on http collector? The http collector is a special case, because http client knows synchronously whether collection succeeded or not, and current ConcurrentLimiter works as a pool of threads, processing requests asynchronously.\n. Hi again,\n  just tried to move up the configuration and shared the concurrency limiter with the http collector. Please see if it is closer to what you suggested and I will try to test it in a real environment as soon as possible.\nRegards,. Not really, but I am on it.\nI stopped working on it because I had a family emergency that took me away\nfrom work a couple of months.\nI am back and want to test it soon.\nEl El mi\u00e9, 24 oct 2018 a las 2:11, Tommy Ludwig notifications@github.com\nescribi\u00f3:\n\n@malonso1976 https://github.com/malonso1976 were you able to try this\nout in a real environment?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/openzipkin/zipkin/pull/2169#issuecomment-432465879,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AFx3OsKlvOke_5BR9OqMV-CEbwJztqLIks5un7A_gaJpZM4WCgvm\n.\n. Ok, I understand. But will not be able to lock on the HttpCall semaphore, because its is not available  for this use.\n\nSome other contention mechanism must be develop here when storage begins to drop spans. Hi Adrian,\n  when using zipking with a Kafka collector and heavy load of spans, we tried with several storage alternatives. Mysql does not scale as well as others so it was a quick discard. Cassandra could not handle so many Future calls and began throwing driver errors, no matter how we tuned it (it was zipkin 2.7.1). Elastic did the job, but began receiving exceptions like this:\n10:13:25.604 [pool-1-thread-1] DEBUG z.collector.kafka10.KafkaCollector - Cannot store spans [1420afe62427f4ce559a0e797bb1b777/13468e1e3b2d9e26] due to IllegalStateException(over capacity)\njava.lang.IllegalStateException: over capacity\n    at zipkin2.elasticsearch.internal.client.HttpCall.enqueue(HttpCall.java:87)\n    at zipkin.internal.V2Collector.record(V2Collector.java:57)\n    at zipkin.internal.Collector.accept(Collector.java:79)\n    at zipkin.collector.Collector.accept(Collector.java:145)\n    at zipkin.collector.kafka10.KafkaCollectorWorker.run(KafkaCollectorWorker.java:84)\n    at zipkin.collector.kafka10.KafkaCollector$LazyKafkaWorkers$1.run(KafkaCollector.java:196)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\nThe max-connections parameter determined the semaphore count:\nhttps://github.com/openzipkin/zipkin/blob/master/zipkin-storage/elasticsearch/src/main/java/zipkin2/elasticsearch/internal/client/HttpCall.java#L77\nThis count must be shared between query calls and collector insertion calls. In order to guarantee that insertion calls wait to be executed to avoid dropping spans, I suggested to change the semaphore.tryAcquire to semaphore.acquire, and that fixed the case. \nOur problem now is that the HttpCall is shared for the mixed use of query api operations and collector api insertions. There is no chance for reservation of the same amount of HttpCalls as Kafka worker threads: KAFKA_STREAMS parameter.\nAlternatives:\n- Split pool of HttpCalls used for collector processing from those used for query prrocessing\n- Implement a retry behaviour when java.lang.IllegalStateException or RuntimeException is caught \nvoid record(List<Span> sampled, Callback<Void> callback) {\n    if(blockOnStorage) {\n      try {\n        storage.spanConsumer().accept(sampled).execute();\n        callback.onSuccess(null);\n      } catch (IOException | RuntimeException | Error e) {\n        // TODO: retry on error\n        callback.onError(e);\n      }\n    } else {\n      storage.spanConsumer().accept(sampled).enqueue(callback);\n    }\n  }\nWhat do you think? Retry behaviour requires additional parameters such as maximum number of retries and wait interval between retries... \n. ",
    "afalko": "We are facing the same problem. @malonso1976 were you still hoping to create a pull request? . @adriancole Yeh, my initial PoC of this used the KafkaSender async reporter, but then I had no way to pull in the server-components from zipkin-server. I built my own undertow server that would go out of date with zipkin-server improvements. \nIn theory, I could include the async reporter kafka module into the storage module here; but that seems like it would be a strange dependency chain. Happy to reconsider that; I might need to copy some of the callback stuff to handle errors. \nAlthough the current code sends a kafka message per span, it'll carry us through for a while in terms of scale. With 3x acks=all kafka configuration, we can push 25k 1k messages/sec, which is 2.2 billion spans per day. If you don't replicate on kafka and set acks=0, then you can do 100k+ messages/sec. . Ahh, I didn't think about what happens on the other end. Do you want me to do the bundling work as part of this PR or a followup PR?\nWe are running ES right now, but it is on my todo list to research whether ES or Cassandra scales better. Has anyone in the community done an evaluation between the two already?. Thinking about it more, bundling seems be to be better done on the storage layer-side; e.g. why should the sender know the optimal bundle size for a storage backend they might not even know about. \nBuilding off your idea for an MQ based sender; what if Zipkin were to be made to run storage-less and instead an admin could configure MQ ingest instead? The current kafka code probably wouldn't change too much; but get shifted around to be behind a more suitable MQ interface. Batching config could be added later, so there is a batch for MQ, then a separate batch for storage.  \nOnce I submit the filter interface PR w/ quota filters; putting that stuff in the senders will be dangerous because there is no guarantee that service owners will depend on the correct sender lib and configure it correctly.\n. After working on https://github.com/openzipkin/zipkin/pull/2072 and talking about it internally. We think it would be easiest to pull our Kafka storage out to a separate repo that we would be happy to maintain and make changes (in needed) to make storage backends more pluggable. Thoughts on this @adriancole? . Good news @adriancole I was able to get this working by pulling the kafka impl out of code base: https://github.com/afalko/zipkin-storage-kafka . The only thing I haven't tested out yet is whether you can plop config values into zipkin.yml. That is low priority for me at the moment. \nHere is how I launch the new storage module:\nSTORAGE_TYPE=kafka java -Dzipkin.storage.kafka.bootstrap-servers=localhost:9092 -Dloader.path=../zipkin-storage-kafka/zipkin-autoconfigure-storage-kafka/target/zipkin-autoconfigure-storage-kafka-2.9.3.jar,../zipkin-storage-kafka/zipkin-storage-kafka/target/zipkin-storage-kafka-2.9.3.jar -cp zipkin-server/target/zipkin-server-*exec.jar org.springframework.boot.loader.PropertiesLauncher\ncc: @rmichela. @jeqo Thanks for doing that work; very cool you got writes working with Streams. I'm no longer on the company and team maintaining this branch/abovementioned write-only Kafka solution. @rmichela will be in a better position to make the call on how to proceed. . @jeqo Thanks for doing that work; very cool you got writes working with Streams. I'm no longer on the company and team maintaining this branch/abovementioned write-only Kafka solution. @rmichela will be in a better position to make the call on how to proceed. . Please accept my appologies @adriancole. I meant to make this a PR to my own fork before blasting it out to the community. I'll close it and create a proper one when I'm ready.. Thanks for the pointer! @rmichela was able to get the add-classpath-and-go part working in the sample plugin: https://github.com/afalko/ExampleSpanFilter/pull/1\nIt works e2e, so stay tuned :). Most of our use cases are outlined in the doc I linked, but I'll paste them in the PR description shortly. Other names I can think of are SpanProcessors, SpanManipulators, SpanManglers. Open to other suggestions or votes; naming is hard :). @adriancole I took a closer look at adjuster and I think is too specific for what we are trying to do. I can imagine a filter implementation having that adjuster class internal to it and it would be invoked within the process implementation. I'm meeting with @naoman next week to find out more about his work and if my soon-to-be-revised PR will work for his use-cases. . @adriancole I took a closer look at adjuster and I think is too specific for what we are trying to do. I can imagine a filter implementation having that adjuster class internal to it and it would be invoked within the process implementation. I'm meeting with @naoman next week to find out more about his work and if my soon-to-be-revised PR will work for his use-cases. . I've rebased on latest master and removed the custom exception. I also pass CollectorMetrics through into the SpanFilter interface. I've added a utest that drops spans too. . @rmichela helped me with all of the Spring stuff, so he might be able to go into more detail on this. . I had this with @Nullable before, but I kind of like Optional from a readability standpoint. Happy to change it back. @rmichela, is it better to use Optional or @Nullable?. I'll look around from suitable built-ins. I'll add better documentation too. The reason I extended Throwable was because the error call takes a throwable: https://github.com/openzipkin/zipkin/pull/2072/files#diff-1b30356bd2b01dbbfd9152c17a1ac4baR148\nIn theory, I could copy what you did in that same function: message.startsWith(\"Cannot store\"), but that isn't strongly typed and requires an implementor to have to be careful about their error strings. I wanted to avoid that drawback and make it easier on filter implementors. Happy to send another PR to clean replace message.startsWith. \n. If the implementor so chooses they could consider ingesting all spans, but throwing an error. I think that would be a rare case, but I can't think of reasons to get in someone's way of doing that; nor can I think of a strong reason to go out of our way to support that very well. Do you want me to have the call of the interface stop if exception is added to callback?. Ahh, that's good news; I won't need the guava token type once that is that case. In terms of \"later\", do you want me to make this only supported for v2 as part of this PR?. I only need it for TokenType to determine v1 vs. v2. If zipkin2.Span is that only thing that is supported in the next week or two, then I can remove this. Otherwise, I need suggestions :).. @adriancole I tried to find a built-in exception class that carries something like http code, but unfortunately there isn't one that I can bring in without big dependencies. \nThese two are the most commonly used: \n1) http://hc.apache.org/httpclient-3.x/apidocs/org/apache/commons/httpclient/HttpException.html\n2) https://docs.oracle.com/javase/7/docs/api/javax/xml/ws/http/HTTPException.html\nI still think the most reasonable option is to have the custom exception. Without this, a filter implementor cannot adjust the http code emitted if there is an error, which would make for a bad user experience. . I take part of this back. The latter exception will suffice for the error code and is bolted in java. The only drawback is that it doesn't provide a way to set a custom message. . ",
    "DanMelman": "Hi,\nWe were having the same exact exception on our production system, having some spans lost and not written to elastic. I have tried the above mentioned fix by forking from @malonso1976 and deploying the code from the new branch, but unfortunately it does not seem to solve the problem (still getting the exception).\nI tried another approach - in the HttpCall class, inside enqueue, I gave the tryAcquire a timeout of a few seconds. That seems to solve the problem for us. Let me know what you think.. By \"concurrency queue\" you refer to the \"limit\"/\"concurrency\" in the configuration?\nLet me know how can I contact you directly, I can do some runs under load.. ",
    "WiFeng": "@adriancole  It wouldn't affect others things. I've also thought about other options, such as modifying the filebeat configuration to achieve the format zipkin requires, but it won't work.\nIf someone use filebeat, perhaps it's useful.\n. @adriancole I haven't asked anyone at elasticsearch. I hope they can help me that make it possible to send raw data.\nI can't understand your last words. Can you explain again ?. Now the filebeat supports these output methods (Configure the output):\nElasticsearch\nLogstash\nKafka\nRedis\nFile\nConsole\nThese outputs are all common components, and AFAIK the filebeat only can config one output method. In production environment a various of the log file should be collected, so couldn't specify the output is the zipkin api.\n@tsg  @ruflin . ",
    "finddog": "solved.\nadd  management.metrics.web.server.auto-time-requests=false  to application.properties ,then it works well.. ",
    "henrik242": "\nWhy are you importing both artifacts?\n\n@adriancole: I don\u2019t.  io.zipkin.java:zipkin adds this dependency itself:\nhttps://github.com/openzipkin/zipkin/blob/328904f476208bc2454626f818798c72e3290bc4/zipkin/pom.xml#L38\n. ",
    "tbwork": "@adriancole Thanks for your response. I understood your original intention. Thanks again for providing such a good app tracing tool. Good night and keep healthy!. @adriancole Thanks for your response. I understood your original intention. Thanks again for providing such a good app tracing tool. Good night and keep healthy!. @adriancole Enjoy your vacation~. @adriancole Enjoy your vacation~. Rogar that!. ",
    "jensraaby": "This should resolve the exception seen when following the Scribe collector README:\n```\no.s.b.SpringApplication                  : Application run failed\norg.springframework.beans.factory.BeanCreationException: Error creating bean with name 'defaultValidator' defined in class path resource [org/springframework/boot/autoconfigure/validation/ValidationAutoConfiguration.class]: Invocation of init method failed; nested exception is java.lang.NoSuchMethodError: javax.validation.BootstrapConfiguration.getClockProviderClassName()Ljava/lang/String;\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1702) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:579) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:501) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:760) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:869) ~[spring-context-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550) ~[spring-context-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.0.1.RELEASE.jar!/:2.0.1.RELEASE]\n    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:759) [spring-boot-2.0.1.RELEASE.jar!/:2.0.1.RELEASE]\n    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:395) [spring-boot-2.0.1.RELEASE.jar!/:2.0.1.RELEASE]\n    at org.springframework.boot.SpringApplication.run(SpringApplication.java:327) [spring-boot-2.0.1.RELEASE.jar!/:2.0.1.RELEASE]\n    at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:137) [spring-boot-2.0.1.RELEASE.jar!/:2.0.1.RELEASE]\n    at zipkin.server.ZipkinServer.main(ZipkinServer.java:27) [classes!/:?]\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]\n    at java.lang.reflect.Method.invoke(Method.java:564) ~[?:?]\n    at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) [zipkin.jar:?]\n    at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) [zipkin.jar:?]\n    at org.springframework.boot.loader.Launcher.launch(Launcher.java:50) [zipkin.jar:?]\n    at org.springframework.boot.loader.PropertiesLauncher.main(PropertiesLauncher.java:592) [zipkin.jar:?]\nCaused by: java.lang.NoSuchMethodError: javax.validation.BootstrapConfiguration.getClockProviderClassName()Ljava/lang/String;\n    at org.hibernate.validator.internal.xml.Validat\n. This should resolve the exception seen when following the Scribe collector README:\no.s.b.SpringApplication                  : Application run failed\norg.springframework.beans.factory.BeanCreationException: Error creating bean with name 'defaultValidator' defined in class path resource [org/springframework/boot/autoconfigure/validation/ValidationAutoConfiguration.class]: Invocation of init method failed; nested exception is java.lang.NoSuchMethodError: javax.validation.BootstrapConfiguration.getClockProviderClassName()Ljava/lang/String;\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1702) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:579) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:501) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:760) ~[spring-beans-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:869) ~[spring-context-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550) ~[spring-context-5.0.5.RELEASE.jar!/:5.0.5.RELEASE]\n    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.0.1.RELEASE.jar!/:2.0.1.RELEASE]\n    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:759) [spring-boot-2.0.1.RELEASE.jar!/:2.0.1.RELEASE]\n    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:395) [spring-boot-2.0.1.RELEASE.jar!/:2.0.1.RELEASE]\n    at org.springframework.boot.SpringApplication.run(SpringApplication.java:327) [spring-boot-2.0.1.RELEASE.jar!/:2.0.1.RELEASE]\n    at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:137) [spring-boot-2.0.1.RELEASE.jar!/:2.0.1.RELEASE]\n    at zipkin.server.ZipkinServer.main(ZipkinServer.java:27) [classes!/:?]\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]\n    at java.lang.reflect.Method.invoke(Method.java:564) ~[?:?]\n    at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) [zipkin.jar:?]\n    at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) [zipkin.jar:?]\n    at org.springframework.boot.loader.Launcher.launch(Launcher.java:50) [zipkin.jar:?]\n    at org.springframework.boot.loader.PropertiesLauncher.main(PropertiesLauncher.java:592) [zipkin.jar:?]\nCaused by: java.lang.NoSuchMethodError: javax.validation.BootstrapConfiguration.getClockProviderClassName()Ljava/lang/String;\n    at org.hibernate.validator.internal.xml.Validat\n```. ",
    "mikebz": "Thanks @adriancole - just wanted to make sure you know I am not compiling it from scratch.  I just got the latest jar.. Thanks @adriancole - just wanted to make sure you know I am not compiling it from scratch.  I just got the latest jar.. ",
    "rmichela": "I like the idea of being able to load custom storage backends from the classpath as plugins.. I\u2019ve been on paternity leave for the past few months and am really out of the loop. . I\u2019ve been on paternity leave for the past few months and am really out of the loop. . The Optional<T> is required so Spring knows the constructor injected filters parameter is not required. Without it, the bean graph will fail to load at startup when no SpanFilter plugin implementations are registered with Spring.\nhttps://stackoverflow.com/a/42135423. ",
    "zhaole425": "\n\nIs there a method to set the sampling config of service A/B???\n\n\nDoes the zipkin-server need to set   the sampling config???. 1.  Is there a method to set the sampling config of service A/B???\n\n\nDoes the zipkin-server need to set   the sampling config???. ok\u3002\u3002\u3002       :). ok\u3002\u3002\u3002       :). I saw an earlier issue describe this:\nhttps://github.com/openzipkin/zipkin/issues/1841\nI will try it firstly\n\n\nmodifing the ES_MAX_REQUESTS do not wors...  :\uff08. I saw an earlier issue describe this:\nhttps://github.com/openzipkin/zipkin/issues/1841\nI will try it firstly\nmodifing the ES_MAX_REQUESTS do not wors...  :\uff08. ",
    "ldcsaa": "PS: while zipkin server start, it will create a queue uses it's defaut args. so, conform the target queue is not exists (if exists delete it) ! and DO NOT start zipkin server. we create the queue use different args manually.\nand now, start zipkin server, it would raise exception.\n(I think, you should check if the queue exists before create it)\nmy args example:\nVirtual host: monitor\nQueue Name: queue.monitor.zipkin\nDurability: true\nAuto delete: false\narg \"x-message-ttl\": 36000000\narg \"x-max-length\": 1000000\narg \"x-max-length-bytes\": 300485760\nmy startup cmd:\njava -jar zipkin-server-2.8.4-exec.jar -server -Xmx128m -Xms128m --management.endpoints.enabled-by-default=true --zipkin.collector.rabbitmq.uri=amqp://user_name:password@hostname:port/vhost --zipkin.collector.rabbitmq.queue=queue.monitor.zipkin --zipkin.collector.rabbitmq.connection-timeout=15000 --zipkin.collector.rabbitmq.concurrency=1 . Thanks very much, I have make it work now ~. This is Examples, please check:\nexamples.zip\n. Do you have any plans to amend?. Do you have any plans to amend?. OK, I will wait.\nAnd another question: When will version 2.12.2 be released ?. OK, I will wait.\nAnd another question: When will version 2.12.2 be released ?. Thanks a lot .... Thanks a lot .... If show like these would be better ?\nhttp.path (C [or Client])    /content/persional\nhttp.path (S [ or Server])    /content/persionals. If show like these would be better ?\nhttp.path (C [or Client])    /content/persional\nhttp.path (S [ or Server])    /content/persionals. OK~ \nI create a new issue #2403. OK~ \nI create a new issue #2403. ",
    "xihw": "Sure, just wanted to clarify my question again in case we are not in the same page. \nCollector getting down does not necessary mean bug in tracer. We deployed collector as a separate remote service, it can be down on its own, and I'm just wondering how instrumented service would handle the connection error.. ",
    "rfyiamcool": "yes, thank u.. ",
    "dgt-amexio": "Thx for your answer. Do you have any plan to get this supported in future release. I cannot figure out why such an application could not run into a J2EE container.\nRegards. Thx for your answer. Do you have any plan to get this supported in future release. I cannot figure out why such an application could not run into a J2EE container.\nRegards. Thx for your feedback, the point is clear to me now.\n. Thx for your feedback, the point is clear to me now.\n. ",
    "h-karaki": "Including Zipkin Server in a spring boot project is unsupported? . Including Zipkin Server in a spring boot project is unsupported? . I'm just saying, its not immediately clear how to start developing a Zipkin server.. I'm just saying, its not immediately clear how to start developing a Zipkin server.. Thank you for the clarification. I appreciate it.. Thank you for the clarification. I appreciate it.. ",
    "JaryZhen": "I have see the doc EnableZipkinServer which is Deprecated . Are there any example about  tracing between play project and  undertow project. ",
    "wuyoushan": "Thank you, I did what you said, but still do not support the v2 data model. My operating system windows7 64bit. Thank you @shakuzen My data model is missing []. 1. Windows7 system,jdk1.8\n2. java -jar zipkin-server-2.7.5-exec.jar\n3 kafka-1.0.1\nI don't know why not add [], can not read the v2 data model\nhttps://github.com/wuyoushan/zipkin-kafka-test/blob/master/src/main/java/cqut/wys/biz/KafkaReporterSpanV2.java\n@Override\n    public void report(Span span) {\n        System.out.println(\"span to String = \" + span.toString());\n        //not add [], can not read the v2 data model\n        String mess = \"[\"+span.toString()+\"]\";\n        kafkaProducer.send(zipkinTopic, mess.getBytes());\n    }. 1. Windows7 system,jdk1.8\n2. java -jar zipkin-server-2.7.5-exec.jar\n3 kafka-1.0.1\nI don't know why not add [], can not read the v2 data model\nhttps://github.com/wuyoushan/zipkin-kafka-test/blob/master/src/main/java/cqut/wys/biz/KafkaReporterSpanV2.java\n@Override\n    public void report(Span span) {\n        System.out.println(\"span to String = \" + span.toString());\n        //not add [], can not read the v2 data model\n        String mess = \"[\"+span.toString()+\"]\";\n        kafkaProducer.send(zipkinTopic, mess.getBytes());\n    }. ",
    "sre01sh": "Thanks for the quick response.   We are using Spring Cloud -Dalston RELEASE with sleuth.  By default, it puts variables in the span name. . Thanks for the quick response.   We are using Spring Cloud -Dalston RELEASE with sleuth.  By default, it puts variables in the span name. . I just tested the lastest Spring Cloud and found the cardinality has been fixed in this Finchley RELEASE,  like span name: \" get /user/{userId}\". I just tested the lastest Spring Cloud and found the cardinality has been fixed in this Finchley RELEASE,  like span name: \" get /user/{userId}\". ",
    "zhongyusimon": "thank you, i use the command to run it success. \nKAFKA_BOOTSTRAP_SERVERS=127.0.0.1:9092  java -jar zipkin.jar. but the document also confused me.. thank you very much, you are so nice.. ",
    "595979101": "version is zipkin-release-2.10.2. \nThe download address of zipkin-server.jar is https://repo1.maven.org/maven2/io/zipkin/java/zipkin-server/2.10.2/zipkin-server-2.10.2-exec.jar . Ok, thank you. You fixed this bug too fast. ",
    "tdanylchuk": "Thank you so much for your investigation and time you've spent, but unfortunately I messed everything up with service naming and led you to wrong assumptions. \ncamel-producer and camel-consumer are not using camel-zipkin lib, that lib is not covering my needs, so I decided to make lib myself (https://github.com/Playtika/sleuth-camel) hopeful after I get proper result we will publish it to maven central. Next step - to contribute to spring-cloud-sleuth project for 2.0 boot and brave support, my lib is targeting 1.5 boot which is my preference so far.\nAnyway issue I posted is related to JSON's which I attached below, they seems to be correct and I see that Zipkin got them and parse them in proper way, but unfortunately not showing the desired result of lag, which I trying yo achieve. I'm not pointing that this is a bug, maybe it's by design, just wanted to have confirmation of that.. ",
    "JarVZhao": "so i will submit a light PR without sorting,since  sorting indeed  needs a lot of modifications. i have tested codes locally, if you have any questions,please @ JarVzhao ~.   sorry,i forgot to run the unit tests. i use a local es,spans are from sleuth, the ui query is ok. . i see,in our company,we use different queries ,for example in messaging circumstance,the kind may be messaging.UI splits them,not combine them all together. aggregation is very slow in es especially in large indices. ",
    "ShaneCurcuru": "[Left two important notes/questions in the proposal -SC].  Overall it looks really good; you've covered all the high points and it seems like the community would be a good fit.\nOnce ready, someone will need to put the proposal in the incubator wiki; you may need to email general@incubator.a.o \"[WIKI] Permissions to add page for proposal\" with your wiki username.\nReminder: chat rooms are fine, but always bring final discussion and decision of project changes back to the mailing lists (at Apache).\nAlso - Apache infra is investigating a new supported chat tool - likely Slack or Rocket.Chat.  Projects can run their own chat instances on other tools (and do their own maintenance work); infra will support rooms and integrations on the main chat tool that is finally chosen (we currently support hipchat, which is going away :sob: ). Replies to above questions:\n\nThe ASF requires full trademark rights to whatever project name and primary software product releases that a podling makes.  (These rights have to be transferred before you can graduate - so there's time to figure out the details, but it is a hard requirement)\n\nSo - it depends.  If you want to have \"Apache Zipkin\" that releases zipkin-client-x.y.zip, then the ZIPKIN name has to be solely given to the ASF; no other projects could release software using the ZIPKIN name.\nI'm not sure exactly how Brave works/is branded, but a group outside could continue to release brave-server-x.y.zip if they wanted, and call it \"BRAVE\" or \"BRAVE, powered by Apache Zipkin\" or things like that.  They could not call the product being released ZIPKIN BRAVE or anything like that.\nNote also that you can choose to change to a new name during incubation if the community wants to (although it doesn't sound like you'd want to).\n\nThe ASF does not want your copyright assignments.  What the ASF does require is IP Clearance of the code drop being donated (verifying that all the IP is licensed such that the ASF can license it without restriction under Apache-2.0), and a signed ICLA license agreement from anyone who wants to be a committer on the new podling at the Incubator.\n\nIf that doesn't make sense, don't worry: the point is the ASF only needs the right license for the work, not an actual assignment of the underlying copyright.  Apache committers keep copyright on the code they write; it's just that the ICLA to the ASF ensures the ASF has enough rights to always outbound license any submissions to the world under our license.  . ",
    "oscerd": "My pleasure, just minor things :-). @adriancole sure, I noticed Mockito version after the first PR. Sorry for the mess.. CircleCI is still failing but it doesn't seem to be related to the PR.. ",
    "amitskatti": "The traceback we see is:\n```\n[cqlsh 5.0.1 | Cassandra 3.11.1 | CQL spec 3.4.4 | Native protocol v4]\nUse HELP for help.\ncqlsh> desc KEYSPACE zipkin2;\nTraceback (most recent call last):\n  File \"/usr/bin/cqlsh.py\", line 919, in onecmd\n    self.handle_statement(st, statementtext)\n  File \"/usr/bin/cqlsh.py\", line 956, in handle_statement\n    return custom_handler(parsed)\n  File \"/usr/bin/cqlsh.py\", line 1539, in do_describe\n    self.describe_keyspace(ksname)\n  File \"/usr/bin/cqlsh.py\", line 1275, in describe_keyspace\n    self.print_recreate_keyspace(self.get_keyspace_meta(ksname), sys.stdout)\n  File \"/usr/bin/cqlsh.py\", line 1225, in print_recreate_keyspace\n    out.write(ksdef.export_as_string())\n  File \"/usr/share/cassandra/lib/cassandra-driver-internal-only-3.10.zip/cassandra-driver-3.10/cassandra/metadata.py\", line 661, in export_as_string\n    + [t.export_as_string() for t in self.tables.values()])\n  File \"/usr/share/cassandra/lib/cassandra-driver-internal-only-3.10.zip/cassandra-driver-3.10/cassandra/metadata.py\", line 1116, in export_as_string\n    ret = self._all_as_cql()\n  File \"/usr/share/cassandra/lib/cassandra-driver-internal-only-3.10.zip/cassandra-driver-3.10/cassandra/metadata.py\", line 1125, in _all_as_cql\n    ret += \"\\n%s;\" % index.as_cql_query()\n  File \"/usr/share/cassandra/lib/cassandra-driver-internal-only-3.10.zip/cassandra-driver-3.10/cassandra/metadata.py\", line 1402, in as_cql_query\n    ret += \" WITH OPTIONS = %s\" % Encoder().cql_encode_all_types(options)\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 111: ordinal not in range(128)\n```. ",
    "james-callahan": "The Host header may contain a port that does not match the port of the request; e.g. if it has gone through a load balancer.\nIf you add a http.host field, I think it should contain the full contents of the host (or :authority) header as sent/received by/from the client.. > hi JC so this is http specific\nI would expect so from the http. prefix. The Host header is also HTTP specific. I don't see the issue/question?. ",
    "golonzovsky": "I would vote for client side http.host, as it would less likely to explode and makes client span complete and easier to reason about. This is especially relevant for calls to external APIs or uninstrumented endpoints. \nNeed to check kubernetes ingress controller to see if it forwards requests to services (which would be stable as well)..  . ",
    "beckje01": "At SmartThings we could use the host for client side. I think it would be best as a default but regardless I will end up adding it soon as something we track a few reasons I feel we need it:\n\nCalls to external systems\nCalls to systems that are untraced \nAbility to search by the host for a particular service\nTells us more info in the case of a failure that dosent result in any trace being collected on the other side ( network issues, service bugged and not tracing etc)\nClears up calls to different systems with the same path, we are using this pattern more and more. . Personally I use Cassandra.\n\nThe idea of making it something I can opt into while adding tags makes sense to me personally. I could express that this tag didn't have to be searchable vs having to have a whitelist maintained by the operator of zipkin.\nBut if there is either solution we probably don't need both.\nI was thinking it would make sense to allow the storage backends decide what to do with the unindex state of tags since the cost of index may be vastly different.. Honestly this request was intended to see if this is a feature people would find useful. I think it makes sense but wanted to see about a more broad appeal. I don't expect any immediate changes. If the feature seems like a good direction instead of the whitelist #1928 I'm happy to try and come up with some possible technical solutions and propose them.\nThe more we have talked I was leaning toward this being something that notes a tag as something that has eligibility to not be indexed.. Metadata is a way better way to put it mind if I edit the issue to reflect that terminology?. I'll check, also not sure the best way to preview this PR I can make a sample repo if you want.. Yes if you are happy with the content, we can merge I'll be online for a bit to make sure everything is good. And I'll research auto labeling for another time.. Yes if you are happy with the content, we can merge I'll be online for a bit to make sure everything is good. And I'll research auto labeling for another time.. Sorry not :100:  fixed it up in https://github.com/openzipkin/zipkin/pull/2223. Sorry not :100:  fixed it up in https://github.com/openzipkin/zipkin/pull/2223. This actually broke the question template if you goto https://github.com/openzipkin/zipkin/blob/master/.github/ISSUE_TEMPLATE/question.md You will see the error with the max length of about. I don't know that you can make links work maybe its always been a length issue. This actually broke the question template if you goto https://github.com/openzipkin/zipkin/blob/master/.github/ISSUE_TEMPLATE/question.md You will see the error with the max length of about. I don't know that you can make links work maybe its always been a length issue. Looks good now. Looks good now. I like it as optional, I still times I want to see local spans. I like it as optional, I still times I want to see local spans. I'll redo this into the readme I was thinking it should go there but wasn't sure.. I'll redo this into the readme I was thinking it should go there but wasn't sure.. We have lens running in parallel to the original up in our dev to find issues. So anything we submit isn't urgent to us just trying to get early testing in.. ",
    "JoveYu": "same problem, anyone help?. same problem, anyone help?. Fix above. Fix above. \u7279\u6b8a\u540d\u8bcd  \u4e0d\u5e0c\u671b\u5e26\u6765\u8bef\u89e3\uff0c\u5982\u679c\u6709\u4e13\u6709\u540d\u8bcd\u7684\u5b98\u65b9\u7ffb\u8bd1\uff0c\u53ef\u4ee5\u8c03\u6574\u4e0b. original world is: Span names are generally RPC method names or Rails endpoints. This allows you to look up traces that accessed a particular part of the service.\n. maybe is Ruby on Rails endpoints. ",
    "IAMTJW": "the zipkin's V1 is support mysql\uff0cbut the zipkin's V2 not support\u3002Do you plan to support mysql in zipkin's V2.. thanks. ",
    "GuangmingLuo": "@shakuzen  Hi, the use cases could be: user may want to get/post a span; user want to get the trace by traceId. Since the Zipkin APIs are exposed to public, and we can even override the reporter and Tracefilter to add extra tags and so on,  it's is possible that users will make use of those APIs for their own Log & tracing solution. I would be very appreciate if this functionalities can be provided and they will be very helpful.. @adriancole Hi, many thanks that you show me an example. But pity is that it make uses of swagger for code generation, which I have explained before is very heavy for user. Besides, \"localhost:9411\" is hard-coded in the example and it's in go. It would be better if a common java library is provided and make use of the user input of spring.zipkin.base-url, and expose those APIs as methods for call.. @shakuzen You are right, Feign or directly using RestTemplate are the alternatives. I know how to do it. The problem is which is the best solution or the most effective way. User will still need model Span or Trace as entity in this case.\nMaybe we can wait for few days if there are other users want the same feature before we close it.\nI still believe its would be valuable to have such features. Zipkin GUI is not enough as an output for the whole distributed tracing solution. . @adriancole  You are right, Trace is a list of Span with the same traceId. \nGreat to know that there is already a library zipkin2.Span for span and there is a decoder.\nIn this case, Feign could  be the most effective solution.\nMany thanks for the working example. . Hi, \nI guess you don't quite understand the concept of Span.\n\"Span: The basic unit of work. For example, sending an RPC is a new span, as is sending a response to an RPC.\"  \nBR. Hi, \nI recently find that I can't search trace by id in this UI [Go to trace] when I use 128 bit traceId for both version 2.10.4 and 2.11, even though it does exist in the DB. \nDoes this relate to this issue, or maybe raise a new issue ?\nBR. Hi, \nI recently find that I can't search trace by id in this UI [Go to trace] when I use 128 bit traceId for both version 2.10.4 and 2.11, even though it does exist in the DB. \nDoes this relate to this issue, or maybe raise a new issue ?\nBR. ",
    "Anupma97": "Thanks a lot for your quick response . Appreciated !!! . My project's cassandra version is 3.0.11.1485. I can not upgrade it to latest one as it will impact whole project. Please advice if there is any work around.. > If you don't need any feature of the new schema, I'm pretty sure you can use the old \"cassandra\" schema on C* 3.0.\nYes. I tried after setting property STORAGE_TYPE=cassandra.\nTracing data is storing in Cassandra and I am able to get tracing on Zipkin Ui but there is no dependencies graph coming. Earlier when I was using in memory , I was able to get the dependencies graph.. > zipkin-dependencies is a separate job and needed when you run ES or cassandra\n\n\u2026\nOn Thu, 13 Sep 2018, 01:09 Anupma97, @.**> wrote: If you don't need any feature of the new schema, I'm pretty sure you can use the old \"cassandra\" schema on C 3.0. Yes. I tried after setting property STORAGE_TYPE=cassandra. Tracing data is storing in Cassandra and I am able to get tracing on Zipkin Ui but there is no dependencies graph coming. Earlier when I was using in memory , I was able to get the dependencies graph. \u2014 You are receiving this because you modified the open/close state. Reply to this email directly, view it on GitHub <#2189 (comment)>, or mute the thread https://github.com/notifications/unsubscribe-auth/AAD619DLt3g_4y0DmnOK1fhRDQPqgYEiks5uaT_GgaJpZM4WkSmk .\n\nI tried running zipkin-dependencies-2.0.1.jar but I am getting below error.\n18/09/13 21:46:02 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path\njava.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)\n        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)\n        at org.apache.hadoop.util.Shell.(Shell.java:386)\n        at org.apache.hadoop.util.StringUtils.(StringUtils.java:79)\n        at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)\n        at org.apache.hadoop.security.Groups.(Groups.java:93)\n        at org.apache.hadoop.security.Groups.(Groups.java:73)\n        at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)\n        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)\n        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)\n        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)\n        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)\n        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)\n        at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)\n        at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)\n        at scala.Option.getOrElse(Option.scala:121)\n        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2467)\n        at org.apache.spark.SparkContext.(SparkContext.scala:292)\n        at zipkin2.dependencies.cassandra.CassandraDependenciesJob.run(CassandraDependenciesJob.java:163)\n        at zipkin2.dependencies.ZipkinDependenciesJob.main(ZipkinDependenciesJob.java:49). ",
    "aukevanleeuwen": "Hmm I didn't really read** it like that. It sounded like if we really put a lot (but what is a lot ??) of data into it, the querying of certain traces and or dependencies would be slow. That is okay for now I think. \nI would like to know if putting the data in, so storing the spans, would be going slower as well, thus possibly hampering the clients that are sending the data? (Brave, Sleuth)\nAny ball park figure on what 'a lot of data' is?\n https://github.com/openzipkin/zipkin/tree/master/zipkin-storage talks about 'discouraged' for newer systems\n https://github.com/openzipkin/zipkin talks about \"The following components are no longer encouraged, but exist to help aid transition to supported ones. \" . Not sure what's up with the checks, should I amend my commit and try a couple of times? Doesn't seem related to my changes?. \ud83d\udc4d done. ",
    "tacigar": "I prefer 2nd idea! :). I think that is good!\nBut I have a question.\nI think that adjustTimestamps in skew.js may change the timestamps of the spans, and traceDuration in traceSummary.js will be used for duration calculation.\nAnd I think that in many case traceDuration function returns span[0].timestamp + span[0].duration - span[0].timestamp (== span[0].duration).\nSo in many case, I think the process of clock-skew is unnecessary.\nIs my opinion correct?\nAnd is this even in all cases?\nSorry for newbie question \ud83d\ude1e . Sorry, I fixed indent :cry: . OK I'll do that. In elasticsearch, how about using _q instead of changing mapping?. OK!. Done by #2325 . Currently...\nGIF1: \n\nGIF2:\n\n. @zeagord Thank you for telling me the typo!. @drolando \nThank you for feedback!\nYou mean such behavior, right?\n\n. OK. \nI think that UX related to operation has improved considerably.. I start to improve design.. @adriancole @basvanbeek \nThank you!\nI'll do so.. Finally I'll add tests for GlobalSearch. Umm...\nI'll revert the design.\nThe reasons are below.\n\nIf there are no borders, it is ugly when the search condition becomes multiple lines.\nI want to add a button for deleting all search conditions. If there are no borders, there is no place to put it. (If I put it by force, it may be bad)\n\nif there are no borders, limit condition and lookback condition appear strangely floating on the gray background.. Umm...\nI'll revert the design.\nThe reasons are below.\n\n\nIf there are no borders, it is ugly when the search condition becomes multiple lines.\n\nI want to add a button for deleting all search conditions. If there are no borders, there is no place to put it. (If I put it by force, it may be bad)\nif there are no borders, limit condition and lookback condition appear strangely floating on the gray background.. I see...\nWhat do you think about this design?\n\n\nor\n\n. I see...\nWhat do you think about this design?\n\nor\n\n. @basvanbeek \nHow about this?\nShould I remove shadow of search condition badges, too?\n\n. @basvanbeek \nHow about this?\nShould I remove shadow of search condition badges, too?\n\n. Current design\n\nThank you @basvanbeek @adriancole for a lot of valuable feedback!!. Current design\n\nThank you @basvanbeek @adriancole for a lot of valuable feedback!!. @basvanbeek \nThank you for your suggestions!!\nLooking at the screenshots, I think your design is better than the current design completely.\nHowever, the active highlight of the sidebar may be better on the right side.... @drolando \nThank you for your feedback!!\nI'll fix them \ud83d\ude47 . Fixed. Thank you very much for bug report!. No... not yet.\nThere are some tests, but these are not enough.\nI have to add tests.\nBut I think what I have to do at first is refactoring codes before adding tests, because I believe that these bugs will never occur again by refactoring and appropriate use prop types.\nThe refactoring I believe is necessary is below.\n\nSplit props properly\n\nCurrently traces related props are large objects such as traceSummary like the following.\nhttps://github.com/openzipkin/zipkin/blob/9b91acbaa3b90ad1c9e4ab62489db05dfd9edf74/zipkin-lens/src/components/Browser/Traces/Trace.js#L15\nThis is too bad... :sob: \nprops should be primitive as much as possible.\nIt helps us find such bugs early.\n\nDefine prop types strictly\n\nCurrently the definitions of the prop types are too rough like the following.\nhttps://github.com/openzipkin/zipkin/blob/9b91acbaa3b90ad1c9e4ab62489db05dfd9edf74/zipkin-lens/src/components/Browser/Traces/Trace.js#L15\nThis is a bad way as prop types tell us a type bug.\nSo I should define prop types strictly like the following\ntraceSummary: PropTypes.shape({\n  serviceSummaries: PropTypes.arrayOf({\n    serviceName: PropTypes.string,\n    ...\n  }),\n  ...\n}).isRequired,\n\nBut these changes are quite large.\nShould I do it now or later ... :thinking: ?. @shakuzen \nOK! I'll make issues\n@beckje01 \nI see! Thank you very much \ud83d\udc4d . Yes!!!. To tell the truth, I am wondering whether this function is necessary...\nAt least I have never used this...\nWDYT? @openzipkin/ui . @shakuzen @adriancole \nThank you for your opinions!!!\nOK, I see.\nI'll implement this feature \ud83d\ude47 . @drolando Thank you!!!. @drolando Thank you!!!. * the services list is not alphabetically ordered (span names are sorted instead)\n*  in the search results there should be a space between the number and \"spans\"\nresolved  by #2434 \n. * the services list is not alphabetically ordered (span names are sorted instead)\n*  in the search results there should be a space between the number and \"spans\"\nresolved  by #2434 \n. \n. \n. Thank you!!!. Thank you!!!. Almost finished.\nBut there are still some problems left...\n. Almost finished.\nBut there are still some problems left...\n. Update\n\n. Update\n\n. And I fixed 2 problems of #2351 in this PR.\n\nsearch page result rows and traces are mismatched: https://www.dropbox.com/s/60s8vmz4ogmbt7g/Screenshot%202019-01-16%2014.13.56.png As you can see in the screenshot the result row says there are a lot of services and spans, but the trace viewer shows very few. I think the result lines and the trace data are simply mismatched.\nthe + and - symbols from collapsing / expanding subtrees are inverted. I'd expect pressing + to expand, not to collapse the tree\n\nThank you @drolando !!. Thank you!!. For example.\njavascript\nclass GlobalSearch extends React.Component {\n  constructor(props) {\n    ...\n    this.state = {\n      isConditionFocused: false, // <- Add state.\n    };\n    ...\n  }\n  ...\n  handleKeyPress(event) {\n    const { isConditionFocused } = this.state;\n    if (event.key === 'Enter' && !isConditionFocused) {\n      this.handleSearchButtonClick();\n    }\n  }\n  ...\n  handleConditionFocus() {\n    this.setState({ isConditionFocused: true });\n  }\n  handleConditionKeyBlur() {\n    this.setState({ isConditionFocused: false });\n  }\n  handleConditionValueBlur() {\n    // Delay for avoiding to fetch\n    setTimeout(() => { this.setState({ isConditionFocused: false }); }, 0);\n  }\n  ...\n  renderSearchCondition(condition, index) {\n    ...\n    return (\n      <SearchCondition\n        {...commonProps}\n        onKeyFocus={this.handleConditionFocus}\n        onValueFocus={this.handleConditionFocus}\n        onKeyBlur={this.handleConditionKeyBlur}\n        onValueBlur={this.handleConditionValueBlur}\n      >\n        ...\n      </SearchCondition>\n    )\n    ...\n  }\n}. Thank you!!!\nLGTM!!. This PR solves one of #2351 \n\nit'd be nice if pressing enter executed the search. Some of the components are broken.\n\n\n\n. After thinking, I'm not negative about using react-scripts now.\nBut I think we should fix css bugs at first.. I used GlobalSearch when implementing this feature.\nThe user can specify traceId in the same way as other search conditions.\nIf traceId is specified when the search button is pressed, jump to tracePage without searching even if other search conditions are specified.. \n. @adriancole \nOK, thank you!!\nI'll change them in this PR now.. Fixed. > haven't installed locally yet. is there a way to have this work with hyperlink if not already? I mean a link with a trace ID embedded shortcut to the trace view screen?\nYes, there is.\nYou mean this link, right?\n\n. > this question was about people who use bookmarks. you already addressed\nthis with the /zipkin/traces change. basically we want existing bookmarks to work at least for a while I think.\nAh, I got it.\nThank you~. The same warning (about webpack-dev-server) occurs in zipkin-ui, so it is easy to fix.\nI'll fix it in this PR.. Oh... webpack-dev-server used by zipkin-ui is too old...\nI think that we have to be a bit cautious about this matter.\nI said that I fix it in this PR, but I will give up...\nI'll make other PR for this.. @shakuzen \nThank you for your valuable opinion and suggestion!!\nI added a link to https://zipkin.apache.org/.\n\n. Related #2351 \n\nleaf spans should use remoteEndpoint service name rather than the localEndpoint. If the SERVER span exists, the server's localEndpoint should be used (same as now).. LGTM!!. These are netflix.json and ascend.json.\n\nMaybe ascend.json is a bit strange.\nIt has following span data.\n...,\n    {\n        ...,\n        \"id\": \"52b1ab4956917c39\",\n        \"kind\": \"CLIENT\",\n        \"timestamp\": 1531282088533834,\n        \"duration\": 14077,\n        ...,\n    },\n    {\n        \"id\": \"52b1ab4956917c39\",\n        \"kind\": \"SERVER\",\n        \"name\": \"http:/parent/auth-service/v1/authentications/\",\n        \"timestamp\": 1531282088553000,\n        \"duration\": 4157,\n        \"localEndpoint\": {\n            \"serviceName\": \"auth-service\",\n            ...,\n        },\n        ...,\n        \"shared\": true\n    },\n    ...\nClient \u2192 ...33834 ~ ...47911\nServer \u2192 ...53000 ~ ...57157\n. > one other thing to test.. is to make a really really short client/server span.. like one that completes in less than a millisecond, whereas other spans have a different scale like more than a millisecond. This might show some rendering issues\nI tried.\nThat's the third span.\n\nZoom up\n\n@adriancole \nSorry for late \ud83d\ude47 . > one other thing to test.. is to make a really really short client/server span.. like one that completes in less than a millisecond, whereas other spans have a different scale like more than a millisecond. This might show some rendering issues\nI tried.\nThat's the third span.\n\nZoom up\n\n@adriancole \nSorry for late \ud83d\ude47 . I'll fix them... \ud83d\ude22 . I'll fix them... \ud83d\ude22 . @adriancole \nThank you!!. @adriancole \nThank you!!. LGTM. LGTM. PTAL \ud83d\ude47 . Sorry, I've forgot to enter button for sending review.... \n. @jcarres-mdsol Thank you for your feedback.\n\nAnother option would be to, have a \"search traceid\" component instead, which is also a functionality in the classic UI.\n\nLens already has that feature.\n\nI tried implementing the feature to upload JSON in the same way as above.\nWhat do you think about this?\n\ncc: @shakuzen. Personally I agree with @shakuzen...\n\nMaybe we can down-rank the \"from json\" criteria to the bottom, or possibly quasi hide it as most users won't be using this? For example, if you \"expert mode\" or click something, then the from json option appears, but remains in a place where it is universally considered.\n\numm...\nI feel a little complicated.\nI think it would be better to add another dropdown menu for traceId and upload JSON like the following.\n. Personally I agree with @shakuzen...\n\nMaybe we can down-rank the \"from json\" criteria to the bottom, or possibly quasi hide it as most users won't be using this? For example, if you \"expert mode\" or click something, then the from json option appears, but remains in a place where it is universally considered.\n\numm...\nI feel a little complicated.\nI think it would be better to add another dropdown menu for traceId and upload JSON like the following.\n. Yes. Yes. I'll close this pr\nand make a new pr. I'll close this pr\nand make a new pr. Thank you for your feedback!! I'll take a look.. Oh, I forgot to remove traceId component from search bar.... Oh, I forgot to remove traceId component from search bar.... Removed TraceId component from search bar...\nPlease refer #2368 \ud83d\ude47. Removed TraceId component from search bar...\nPlease refer #2368 \ud83d\ude47. Search by trace ID\n\n. Search by trace ID\n\n. Upload Trace JSON\n\n. Upload Trace JSON\n\n. Current component architecture is a bit complex.\nI drew the graph.\n\n. Current component architecture is a bit complex.\nI drew the graph.\n\n. > note: the view trace screen blows up (white screen) if using v1 json\n\n```\nTypeError: \"e is undefined\"\ngetServiceNameColor http://localhost:9411/zipkin/app-d005bdc780c99e8fec7a.min.js:12 value http://localhost:9411/zipkin/app-d005bdc780c99e8fec7a.min.js:88 value http://localhost:9411/zipkin/app-d005bdc780c99e8fec7a.min.js:88\n\n```\nIn the old uploadTrace.js we had something like this. If you import it, please also import the unit tests.\njs\nexport function ensureV2(trace) {\n  if (!Array.isArray(trace) || trace.length === 0) {\n    throw new Error('input is not a list');\n  }\n  const first = trace[0];\n  if (!first.traceId || !first.id) {\n    throw new Error('List<Span> implies at least traceId and id fields');\n  }\n  if (first.binaryAnnotations || (!first.localEndpoint && !first.remoteEndpoint && !first.tags)) {\n    throw new Error(\n      'v1 format is not supported. For help, contact https://gitter.im/openzipkin/zipkin');\n  }\n}\n\nI see!\nI'll do so. If uploaded file is not JSON...\n\nIf v1 format file is uploaded...\n\n. yes, lack of suffix will be \u03bcs now.\nbut I think us is also good.\nI think we should support both of them.\nI'll make another PR.. nit: How about !!shared?\n. nit: dito. I think span.localEndpoint is a little large for using as key string...\nI think that it is better to use a hash value if it is possible... \ud83e\udd14 . dito. nit: How about if (span.shared) { ... } ?. nit: dito. ~nit: How about if (next.parentId || next.shared) ?~. nit: I think left ? 1 : -1 is OK.\nAnd sorry, this is not comments for this line, but I noticed wrong codes in L111 & L112, so I add comments.\nI think those code should be the following.\nL111:  if (typeof(left) === 'undefined') return -1;\nL112:  if (typeof(right) === 'undefined') return 1;\n. Great!\nV2 is very simple! \ud83d\udc4d . Oh chevron is an oil company! \nI didn't know that :D. addFilter and removeFilter are removed when filter features are removed.\nSo this process does not finish successfully.\nThe process of this line seems to be called in https://github.com/openzipkin/zipkin/blob/2e32676489acd3c807cfcc57a6e0e456a00fd607/zipkin-ui/js/page/default.js#L89 , so later processes of this line will not be executed.\nSo https://github.com/openzipkin/zipkin/blob/2e32676489acd3c807cfcc57a6e0e456a00fd607/zipkin-ui/js/page/default.js#L92 is not called, and then https://github.com/openzipkin/zipkin/blob/2e32676489acd3c807cfcc57a6e0e456a00fd607/zipkin-ui/js/component_ui/backToTop.js#L11 is not called, too.\nAs a result, this callback https://github.com/openzipkin/zipkin/blob/2e32676489acd3c807cfcc57a6e0e456a00fd607/zipkin-ui/js/component_ui/backToTop.js#L13 is not set... \ud83d\ude22 \nThis is the cause of this bug.. There is not this method now. I think the type of return value should be List<String> or ResponseEntity<List<String>>.. Thank you for your comment!. Ah, sorry for omitting code in example.\nPlease set callback props for 336-342 like the following.\n```\n {\n    fetchAutocompleteValues(condition.key);\n    this.handleConditionFocus();\n  }\n  onValueFocus{() => {\n    fetchAutocompleteValues(condition.key);\n    this.handleConditionFocus();\n  }\n  onKeyBlur={this.handleConditionKeyBlur}\n  onValueBlur={this.handleConditionValueBlur}\n\n...\n\n``. Nit: How about to renamehandleKeyDown?. Probably I think that this file should be git ignore.. Why delete polyfill?. Why do you use es2015?. Sorry, I don't know the purpose of this file.\nPlease tell me :bow:. I didn't use.jsxextension.. Isnode_modulesnecessary?. What isajvused for?. What isnode-fetchused for?. This may be a very basic question, but what arebrowserlistandproxysettings used for?. Ah,browserslistis used by react-scripts, right?. Ah, I see.. I think these should be contained indevDependencies`.. No.\nYou need to change this line like,\n\n\"proxy\": \"http://zipkin-server-example:8080\",. :). Thank you for information \ud83d\ude47 \nI only use event.key for now.. traceSummary does not have timestamp field now.... Add repository field since warning occurred when npm install was executed.. This file has all prop-type definitions of spans and traces.. @adriancole \nSorry, I don't understand what you mean \ud83d\ude2d \nWhat should I do?\n. OK\nThank you!. Maybe this file should be git-ignored?. I want to import SVG as a SVG file.\nIs it impossible?. This file is for requiring modules using absolute path, right?\nIf you add this file, I want you to apply the changes to all existing js files or I want you to split PR to avoid confusion... \ud83d\ude47 \n. The way to pass environment variables(API_BASE) when executing commands is also used in zipkin-ui, and I did not think that was a problem.\nCan not be overridden by environment variables when executing commands?\nI mean I want to use proxy=http://localhost:9411 when I input the following command.\nnpm start\nand I want to use proxy=http://example-zipkin.com when I input the following command.\nproxy=http://example-zipkin.com npm start. this. is not needed.. OK. I'll fix it.. Can you not create main.css?. Please new line. This trailing slash causes redirecting issue in dependency page.. Zipkin-ui requires trailing slash in root page (/zipkin)\nso if location is /zipkin, add trailing slash\nhttps://github.com/openzipkin/zipkin/blob/649aeefdbaf68be7951c44ac0f689acd10b24a03/zipkin-ui/js/main.js#L23\nhttps://github.com/openzipkin/zipkin/blob/649aeefdbaf68be7951c44ac0f689acd10b24a03/zipkin-ui/js/publicPath.js#L9-L11. Thank you!!. Use BEM notation. Rename trace-viewer -> timeline. Sine there is a possibility of writing later, I made a file. I refactored to reuse DetailedTraceSummary.\n\nRemove a process of fetching trace data from DetailedTraceSummary component.\nMake a new component TracePage, and add that process and DetailedTraceSummary component to the new component.. Rename ticks \u2192 timeMarker. I mistook method name...\nThese are overlooked bugs...\nMaybe current lens does not work correctly... \ud83d\ude2d . startTs and endTs must be set when traceSummary is changed.\nIf I don't do so, MiniTimeline's range will be displayed as the previous range.... I forgot to use BEM modifier.... I forgot to rename callback function.. Yes.\nDo you think that it is better to display the alert dialog?. OK, Thank you.\nI'll do so. \n",
    "danielkhan": "I will.. I will.. ",
    "Knifeck": "the server is ubantu 16.04, you mean the version can't be support to storage data. the server is ubantu 16.04, you mean the version can't be support to storage data. ",
    "yanggy11": "yes, it doesn't support es 5.5, I replaced es5.5 to es 2.4.0, and it works.. ",
    "blenolopes": "\nopened an issue on the colon bomb #2219\n\nTks adriancole, I'll go there!. ",
    "sunyaox": "Thank you for your quick response.I want to try again.\n. We now have large amounts of data, millions, and we need a normal paging. That's why I consulted you. Just like using ES query, the'from'and'size' parameter is set.. OK, thanks again for your reply. Thank you very much.\nWe need to customize some functions. But now using the Zipkin you developed is very good.. ",
    "iwanskit": "I also tried to deploy kafka and zipkin locally (without kubernetes) but with the same error. ",
    "duanhairuo": "the follow is spring document\nEven though the MessageHeaders implements Map, it is effectively a read-only implementation. Any attempt to put a value in the Map will result in an UnsupportedOperationException. The same applies for remove and clear. Since Messages may be passed to multiple consumers, the structure of the Map cannot be modified. Likewise, the Message's payload Object can not be set after the initial creation. However, the mutability of the header values themselves (or the payload Object) is intentionally left as a decision for the framework user.. ",
    "melloware": "I have this JS error as well on 2.11.8  Haven't tried the SNAPSHOT release yet to see if its fixed.. Tested with 2.11.9 this morning and still happening.\n\n. ",
    "arap": "OKHttp configures only trustmanager verifying server certificates, but it does not configure keystore. \nThats related with https://github.com/square/okhttp/issues/1711#issuecomment-377132486. \n. @adriancole I can contribute, but I don't fully understand Your point. \nShould autoconfiguration rely on javax.net.ssl.keyStore JVM variable, application property, or something else?. @adriancole I can contribute, but I don't fully understand Your point. \nShould autoconfiguration rely on javax.net.ssl.keyStore JVM variable, application property, or something else?. ",
    "ikv163": "+1. +1. ",
    "igorbljahhin": "+1. +1. ",
    "lambcode": "Some things I'd like to see:\n A way to reset zoom\n Zooming into a certain time frame will filter the spans below to only those that were open during the window. (and all of it's parent spans back to the root for contextual info). This would help manage \"tall\" traces. ",
    "bogdandrutu": "@adriancole why not modify the client span-id to be server span-id?. ",
    "ghermeto": "I can take look this weekend... For Node.js I usually use Benchmark.js which powers JsPerf or Autocannon for HTTP/1.1.\nAlthough Benchmark.js works for javascript in general, it doesn't help with rendering, so I ask around here...\n. I can take look this weekend... For Node.js I usually use Benchmark.js which powers JsPerf or Autocannon for HTTP/1.1.\nAlthough Benchmark.js works for javascript in general, it doesn't help with rendering, so I ask around here...\n. @redsunsoft this is the ticket. @redsunsoft this is the ticket. ",
    "redsunsoft": "We gave this a try with \"smartthings-oauth-authorization.json\" data and it rendered quite quickly.  Are we reproing correctly?  . Got it.  I'll take a read through the code.. Took a good look at the code and React scripts but nothing jumped out to me.  Your composition and lifecycle hooks all seem to be fine.  So unless there's a specific issue that needs attention, you're looking good with the new codebase.. ",
    "igor-suhorukov": "@adriancole thank you for feedback. ",
    "maoling": "Let me take a look. @adriancole \n- yes,if okhttp3.Dns use InetSocketAddress,the problem will be solved.\n- the problem seems to be not very urgent,if really need,we can implement a simple round robin using the InetSocketAddress inside\uff08\uff1aD\uff09.\n- I don't know why chooseokhttp3 which's more suitable for mobile development?. @adriancole \n- yes,if okhttp3.Dns use InetSocketAddress,the problem will be solved.\n- the problem seems to be not very urgent,if really need,we can implement a simple round robin using the InetSocketAddress inside\uff08\uff1aD\uff09.\n- I don't know why chooseokhttp3 which's more suitable for mobile development?. ",
    "AceHack": "So I just ran the jar file from https://oss.jfrog.org/artifactory/oss-snapshot-local/io/zipkin/java/zipkin-server/2.11.13-SNAPSHOT/zipkin-server-2.11.13-20190107.062025-14-exec.jar\nAnd it did not fix the issue.\n\n[\n  {\n    \"traceId\": \"53df269d1222145a\",\n    \"parentId\": \"f39c2f9f496ec3f8\",\n    \"id\": \"737482d98c973e19\",\n    \"kind\": \"SERVER\",\n    \"name\": \"get-profile\",\n    \"timestamp\": 1546959945748631,\n    \"duration\": 100000,\n    \"localEndpoint\": {\n      \"serviceName\": \"profile-service\",\n      \"ipv4\": \"10.0.75.1\"\n    },\n    \"shared\": true\n  },\n  {\n    \"traceId\": \"53df269d1222145a\",\n    \"parentId\": \"a052e711897376d5\",\n    \"id\": \"19fad211d53cf956\",\n    \"kind\": \"CLIENT\",\n    \"name\": \"get-permissions\",\n    \"timestamp\": 1546959945698631,\n    \"duration\": 520000,\n    \"localEndpoint\": {\n      \"serviceName\": \"auth-service\",\n      \"ipv4\": \"10.0.75.1\"\n    }\n  },\n  {\n    \"traceId\": \"53df269d1222145a\",\n    \"parentId\": \"a052e711897376d5\",\n    \"id\": \"f39c2f9f496ec3f8\",\n    \"kind\": \"CLIENT\",\n    \"name\": \"get-profile\",\n    \"timestamp\": 1546959945738631,\n    \"duration\": 120000,\n    \"localEndpoint\": {\n      \"serviceName\": \"api-gateway\",\n      \"ipv4\": \"10.0.75.1\"\n    }\n  },\n  {\n    \"traceId\": \"53df269d1222145a\",\n    \"id\": \"46c96eaf94956f5f\",\n    \"kind\": \"CLIENT\",\n    \"name\": \"get-user\",\n    \"timestamp\": 1546959945678631,\n    \"duration\": 560000,\n    \"localEndpoint\": {\n      \"serviceName\": \"client\",\n      \"ipv4\": \"10.0.75.1\"\n    }\n  },\n  {\n    \"traceId\": \"53df269d1222145a\",\n    \"parentId\": \"19fad211d53cf956\",\n    \"id\": \"e5064baf50225be7\",\n    \"kind\": \"SERVER\",\n    \"name\": \"get-permissions\",\n    \"timestamp\": 1546959945708631,\n    \"duration\": 500000,\n    \"localEndpoint\": {\n      \"serviceName\": \"auth-service\",\n      \"ipv4\": \"10.0.75.1\"\n    },\n    \"shared\": true\n  },\n  {\n    \"traceId\": \"53df269d1222145a\",\n    \"parentId\": \"46c96eaf94956f5f\",\n    \"id\": \"a052e711897376d5\",\n    \"kind\": \"SERVER\",\n    \"name\": \"get-user\",\n    \"timestamp\": 1546959945688631,\n    \"duration\": 540000,\n    \"localEndpoint\": {\n      \"serviceName\": \"api-gateway\",\n      \"ipv4\": \"10.0.75.1\"\n    },\n    \"shared\": true\n  }\n]. ",
    "fyi-coursera": "\nhi there. FYI your span data is incorrect as it redundantly has \"cr\" \"cs\" and also span.kind. what is generating this?\nwill consider the change in separate comments. I think we have some issue about remote name policy\n\nsorry, it's generated by some WIP code in the middle of moving from the v1 model to the v2 one. Can I just drop cs and cr from that span data and be ok?\nWill move the rest of the discussion to #2274 . ",
    "liuzigenius": "\nI see console has a error like that, it seems like js has some exception.... ",
    "WywTed": "Use zipkin-server 2.11.12 , storage_type is es(5.x), and use rabbitmq to transport data.\n{\"zipkin\":{\"version\":\"2.11.12\"}}. @zeagord  but one day ago, i can find two service from /services api\uff0c and today it's just one.  This is the second time i found this situation. . ",
    "joelroyer": "Yes, our Zipkin is embedded in a custom server, based on Spring Boot 2.1.2.RELEASE and Spring Cloud Greenwich RC2.\nPrevious version used Spring Boot 2.0.7.RELEASE and Spring Cloud Finchley.SR2 and worked well (no conflict).\n. Yes, our Zipkin is embedded in a custom server, based on Spring Boot 2.1.2.RELEASE and Spring Cloud Greenwich RC2.\nPrevious version used Spring Boot 2.0.7.RELEASE and Spring Cloud Finchley.SR2 and worked well (no conflict).\n. ",
    "yyyogev": "OK, Thanks.. I see.. If we do that, does that mean we'll need to maintain it with every update of your repo?. Ok then, so how does it work? you would open a zipkin-logzio repo that we could work on?. Ok, sorry for that. ",
    "yinjihuan": "There is no data on my visit.\nhttp://localhost:9411/zipkin/dependency/\nData is available when searching\nWhat is the reason for this?\nVersion 2.12.0 used\n. \n. \nData is displayed in the Spring Cloud document example\n. ",
    "snsingh": "I believe the issue is similar to the one addressed in issue# 1623  hence the similar classification.\nI might be wrong though, so please feel free to correct the classifications and any other discrepancies that might be there in my post.\nThanks.\n\nfor context, the ui uses the sub path /zipkin which does not permit POST requests. I am wondering why in this case this is classified as a cross site attack as the attacker cannot control the data displayed (or maybe I am missing something?)\n\u2026\n. That would be great @adriancole . I will also look into the zipkin-lens directory.\nok to answer question directly I can help find the code you are asking about but bear in mind the ui is being replaced. there is a directory zipkin-lens with the new code. you might want to have a look there if concerned\n\u2026\n\n. ",
    "Incarnation-p-lee": "so the POST result 500 is expected ?\n2019-01-23 08:32:30.950 DEBUG [time-web-service,,,] 1 --- [ender@17eee177}] o.s.c.s.z.s.ZipkinRestTemplateWrapper    : Created POST request for \"http://137.135.113.148:9999/cloud-zipkin-server/api/v2/spans\"\n2019-01-23 08:32:30.950 DEBUG [time-web-service,,,] 1 --- [ender@17eee177}] o.s.c.s.z.s.ZipkinRestTemplateWrapper    : Setting request Accept header to [text/plain, application/json, application/*+json, */*]\n2019-01-23 08:32:30.951 DEBUG [time-web-service,,,] 1 --- [ender@17eee177}] o.s.c.s.z.s.ZipkinRestTemplateWrapper    : Writing [[B@63f72a1b] as \"application/json\" using [org.springframework.http.converter.ByteArrayHttpMessageConverter@7cdf7f38]\n2019-01-23 08:32:30.972 DEBUG [time-web-service,,,] 1 --- [ender@17eee177}] o.s.c.s.z.s.ZipkinRestTemplateWrapper    : POST request for \"http://137.135.113.148:9999/cloud-zipkin-server/api/v2/spans\" resulted in 500 (Internal Server Error); invoking error handler. ",
    "libeilin": "OK, thanks for your reply, because we have encountered some problems when compiling by ourselves. Therefore, we are looking for your help here.\nIf this requirement is made, I hope you can release it as soon as possible. At present, the data volume of one day is too large, and the ES query speed cannot keep up with it.. ",
    "fpoppinga": "Hi @adriancole,\nyes I'm using openzipkin/zipkin, sorry for omitting the Environment. \nWe are using the following env:\nJAVA_OPTS=\"-Xmx512m -XX:MaxMetaspaceSize=128m\"\n            SCRIBE_ENABLED=true\n            STORAGE_TYPE=cassandra\n            CASSANDRA_KEYSPACE=<our-keyspace>\n            CASSANDRA_CONTACT_POINTS=<our-contact-points>\n            CASSANDRA_USERNAME=<our-username>\n            CASSANDRA_PASSWORD=<our-password>\nFunnily the service itself is working fine, it is throwing on the /health endpoint. Adding and requesting traces, etc. just works. \n. Hi @adriancole,\nyes I'm using openzipkin/zipkin, sorry for omitting the Environment. \nWe are using the following env:\nJAVA_OPTS=\"-Xmx512m -XX:MaxMetaspaceSize=128m\"\n            SCRIBE_ENABLED=true\n            STORAGE_TYPE=cassandra\n            CASSANDRA_KEYSPACE=<our-keyspace>\n            CASSANDRA_CONTACT_POINTS=<our-contact-points>\n            CASSANDRA_USERNAME=<our-username>\n            CASSANDRA_PASSWORD=<our-password>\nFunnily the service itself is working fine, it is throwing on the /health endpoint. Adding and requesting traces, etc. just works. \n. @adriancole thanks for fixing it this quick. Is there a chance that this will land in an official release soon? . @adriancole thanks for fixing it this quick. Is there a chance that this will land in an official release soon? . ",
    "gougouzcm": "\nthe javascript libraries have been updated recently. Please try most recent and see if still an issue? we don't have custom json processing code\n\nI use the newest zipkin-servre , It still not work.\n\n. > to save a little time. can you say specifically what is wrong ? ex what field is incorrect display and how should it look?\n\nis the top image zipkin 2.6?\n\nno.The first and second picture boss  are 2.12\u3002\nthis is zipkin-server 2.6\n\nthis is zipkin-server 2.12\n\n2.6 will auto newline if the data is too long,  2.12 not.\n. ",
    "musabqamri123": "Running it on AWS fargate... i guess request is sent by AWS ELB for health check or so..... Looks like its part of undertow 2.0.19 \nundertow-io/undertow@d63b767\nand the undertow version which ships with spring-boot 2.1.3 is 2.0.17\n\n. io.undertow.util.BadRequestException: java.lang.IllegalArgumentException: Invalid string contents \u0016\u0001\uffa3\u0001\uff9fT\uffac\uffb6\uffcc/b\b\ufffd\uff97\uffd8\uffd4N\u0010r\u0001\uff9d\uffedUV\uffd3\u0013^\uffc8z%\uff92>l9\uffb6. Looks like undertow issue for unicode.\nSorry its not related to version.\nLet me know if you have any solution for this. I am running it as a docker container on aws.. locally its running fine..\ni am running with following command .. which enable TRACE for undertow but the request content is not getting printed in logs \n\uf141\n14:07:40\n2019-03-01 14:07:40.639 TRACE 1 --- [ XNIO-2 I/O-1] i.u.request : Opened connection with /10.202.20.152:18256\n\uf141\n14:07:40\n2019-03-01 14:07:40.640 DEBUG 1 --- [ XNIO-2 I/O-1] i.u.r.io : UT005014: Failed to parse request\n\uf141\n14:07:40\nio.undertow.util.BadRequestException: null\n\uf141\n14:07:40\nat io.undertow.server.protocol.http.HttpRequestParser$$generated.handleHttpVerb(Unknown Source) ~[undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n14:07:40\nat io.undertow.server.protocol.http.HttpRequestParser.handle(HttpRequestParser.java:243) ~[undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n14:07:40\nat io.undertow.server.protocol.http.HttpReadListener.handleEventWithNoRunningRequest(HttpReadListener.java:187) [undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n14:07:40\nat io.undertow.server.protocol.http.HttpReadListener.handleEvent(HttpReadListener.java:136) [undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n14:07:40\nat io.undertow.server.protocol.http.HttpOpenListener.handleEvent(HttpOpenListener.java:162) [undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n14:07:40\nat io.undertow.server.protocol.http.HttpOpenListener.handleEvent(HttpOpenListener.java:100) [undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n14:07:40\nat io.undertow.server.protocol.http.HttpOpenListener.handleEvent(HttpOpenListener.java:57) [undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n14:07:40\nat org.xnio.ChannelListeners.invokeChannelListener(ChannelListeners.java:92) [xnio-api-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n14:07:40\nat org.xnio.ChannelListeners$10.handleEvent(ChannelListeners.java:291) [xnio-api-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n14:07:40\nat org.xnio.ChannelListeners$10.handleEvent(ChannelListeners.java:286) [xnio-api-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n14:07:40\nat org.xnio.ChannelListeners.invokeChannelListener(ChannelListeners.java:92) [xnio-api-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n14:07:40\nat org.xnio.nio.QueuedNioTcpServer$1.run(QueuedNioTcpServer.java:131) [xnio-nio-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n14:07:40\nat org.xnio.nio.WorkerThread.safeRun(WorkerThread.java:612) [xnio-nio-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n14:07:40\nat org.xnio.nio.WorkerThread.run(WorkerThread.java:479) [xnio-nio-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n14:08:10\n2019-03-01 14:08:10.582 TRACE 1 --- [ XNIO-2 I/O-2] i.u.request : Opened connection with /10.202.10.145:41022\n\uf141\n14:08:10\n2019-03-01 14:08:10.582 DEBUG 1 --- [ XNIO-2 I/O-2] i.u.r.io : UT005014: Failed to parse request\nENTRYPOINT [\"java\",\"-Dserver.servlet.context-path=/zipkinserver\",\"-Djava.security.egd=file:/dev/./urandom\",\"-Dlogging.level.io.undertow=trace\",\"-Dserver.undertow.accesslog.enabled=true\",\"-Dlogging.level.root=debug\",\"-jar\", \"/usr/src/zipkin-server.jar\"]\n. Its same with latest zipkin version 2.12.3\n06:05:48\nio.undertow.util.BadRequestException: java.lang.IllegalArgumentException: Invalid string contents \u0016\u0001\uffa3\u0001\uff9fkro\uffc1z\uffa88e\uffa4\nio.undertow.util.BadRequestException: java.lang.IllegalArgumentException: Invalid string contents \u0016\u0001\uffa3\u0001\uff9fkro\uffc1z\uffa88e\uffa4\n\uf141\n06:05:48\nat io.undertow.server.protocol.http.HttpRequestParser.handle(HttpRequestParser.java:245) ~[undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:05:48\nat io.undertow.server.protocol.http.HttpReadListener.handleEventWithNoRunningRequest(HttpReadListener.java:187) [undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:05:48\nat io.undertow.server.protocol.http.HttpReadListener.handleEvent(HttpReadListener.java:136) [undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:05:48\nat io.undertow.server.protocol.http.HttpOpenListener.handleEvent(HttpOpenListener.java:162) [undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:05:48\nat io.undertow.server.protocol.http.HttpOpenListener.handleEvent(HttpOpenListener.java:100) [undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:05:48\nat io.undertow.server.protocol.http.HttpOpenListener.handleEvent(HttpOpenListener.java:57) [undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:05:48\nat org.xnio.ChannelListeners.invokeChannelListener(ChannelListeners.java:92) [xnio-api-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n06:05:48\nat org.xnio.ChannelListeners$10.handleEvent(ChannelListeners.java:291) [xnio-api-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n06:05:48\nat org.xnio.ChannelListeners$10.handleEvent(ChannelListeners.java:286) [xnio-api-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n06:05:48\nat org.xnio.ChannelListeners.invokeChannelListener(ChannelListeners.java:92) [xnio-api-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n06:05:48\nat org.xnio.nio.QueuedNioTcpServer$1.run(QueuedNioTcpServer.java:131) [xnio-nio-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n06:05:48\nat org.xnio.nio.WorkerThread.safeRun(WorkerThread.java:612) [xnio-nio-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n06:05:48\nat org.xnio.nio.WorkerThread.run(WorkerThread.java:479) [xnio-nio-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n06:05:48\nCaused by: java.lang.IllegalArgumentException: Invalid string contents \u0016\u0001\uffa3\u0001\uff9fkro\uffc1z\uffa88e\uffa4\n\uf141\n06:05:48\nat io.undertow.util.HttpString.(HttpString.java:116) ~[undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:05:48\nat io.undertow.util.HttpString.(HttpString.java:106) ~[undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:05:48\nat io.undertow.server.protocol.http.HttpRequestParser$$generated.handleHttpVerb(Unknown Source) ~[undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:05:48\nat io.undertow.server.protocol.http.HttpRequestParser.handle(HttpRequestParser.java:243) ~[undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:05:48\n... 12 more\n\uf141\n06:06:18\n2019-03-04 06:06:18.037 TRACE 1 --- [ XNIO-2 I/O-1] i.u.request : Opened connection with /10.202.20.143:11302\n\uf141\n06:06:18\n2019-03-04 06:06:18.037 DEBUG 1 --- [ XNIO-2 I/O-1] i.u.r.io : UT005014: Failed to parse request\n\uf141\n06:06:18\nio.undertow.util.BadRequestException: null\n\uf141\n06:06:18\nat io.undertow.server.protocol.http.HttpRequestParser$$generated.handleHttpVerb(Unknown Source) ~[undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:06:18\nat io.undertow.server.protocol.http.HttpRequestParser.handle(HttpRequestParser.java:243) ~[undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:06:18\nat io.undertow.server.protocol.http.HttpReadListener.handleEventWithNoRunningRequest(HttpReadListener.java:187) [undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:06:18\nat io.undertow.server.protocol.http.HttpReadListener.handleEvent(HttpReadListener.java:136) [undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:06:18\nat io.undertow.server.protocol.http.HttpOpenListener.handleEvent(HttpOpenListener.java:162) [undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:06:18\nat io.undertow.server.protocol.http.HttpOpenListener.handleEvent(HttpOpenListener.java:100) [undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:06:18\nat io.undertow.server.protocol.http.HttpOpenListener.handleEvent(HttpOpenListener.java:57) [undertow-core-2.0.17.Final.jar!/:2.0.17.Final]\n\uf141\n06:06:18\nat org.xnio.ChannelListeners.invokeChannelListener(ChannelListeners.java:92) [xnio-api-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n06:06:18\nat org.xnio.ChannelListeners$10.handleEvent(ChannelListeners.java:291) [xnio-api-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n06:06:18\nat org.xnio.ChannelListeners$10.handleEvent(ChannelListeners.java:286) [xnio-api-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n06:06:18\nat org.xnio.ChannelListeners.invokeChannelListener(ChannelListeners.java:92) [xnio-api-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n06:06:18\nat org.xnio.nio.QueuedNioTcpServer$1.run(QueuedNioTcpServer.java:131) [xnio-nio-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n06:06:18\nat org.xnio.nio.WorkerThread.safeRun(WorkerThread.java:612) [xnio-nio-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n06:06:18\nat org.xnio.nio.WorkerThread.run(WorkerThread.java:479) [xnio-nio-3.7.0.Final.jar!/:3.7.0.Final]\n\uf141\n06:06:18\n2019-03-04 06:06:18.046 TRACE 1 --- [ XNIO-2 I/O-2] i.u.request : Opened connection with /10.202.10.64:64718\n\uf141\n06:06:18\n2019-03-04 06:06:18.046 DEBUG 1 --- [ XNIO-2 I/O-2] i.u.r.io : UT005014: Failed to parse request. ",
    "brightvip": "@adriancole \nelasticsearch.log\nHello\n\u00a0 \nZipkin + elasticsearch-2.3.1 +WIN7 error\nZipkin + elasticsearch-2.3.1 + centos6.5 success\nI don't think it is an elasticsearch error because win7 can't use a colon as a folder name.\n. I hope this colon can be specified with a different symbol such as ES_DATE_SEPARATOR so I can specify @\n. ",
    "thegryfon": "Oh okay @adriancole , i'll try gitter .. i'm not aware of that\nthanks for the suggestions. ",
    "msmsimondean": "I'm seeing this issue too.  I see it both with Zipkin running locally on Docker and also with Spring Sleuth's demo instance https://docssleuth-zipkin-server.cfapps.io/zipkin/ . I'm seeing this issue too.  I see it both with Zipkin running locally on Docker and also with Spring Sleuth's demo instance https://docssleuth-zipkin-server.cfapps.io/zipkin/ . The issue seems to start in version 2.12.4.  It affects both 2.12.4 and 2.12.5.  Rolling back to version 2.12.3 of the Docker image fixes the issue for me.  . The issue seems to start in version 2.12.4.  It affects both 2.12.4 and 2.12.5.  Rolling back to version 2.12.3 of the Docker image fixes the issue for me.  . Thanks @adriancole.  The fix in 2.12.6 is working for me.  . Thanks @adriancole.  The fix in 2.12.6 is working for me.  . ",
    "krishnanraman": "The parameter should say gen.Span, otherwise the apply looks quite odd.\n. I think you want\nif (l == invalid || r == invalid ) invalid else {...other stuff }\n. If the conditional is empty vs non-empty, perhaps we can check sizes of the incoming annotations instead of iterating through all of them in a forcomp, making a long sequence & then checking if its empty or not. Should probably define Seq(\"cs\", \"cr\", \"ss\", \"sr\") as a private val inside object Span.\n. I would check for invalid span in the trace ( I presume a trace is a sequence of spans, and if any one of those spans is invalid, then the trace addition is invalid as well ?) before proceeding with the addition.\n. ",
    "tsuna": "Yup, it's still work in progress.  Should be available for testing within a week or two.\n. ",
    "dsyer": "Good catch. Fixed thanks.\n. ",
    "taylorleese": "@adriancole remove ^^\n. @dschobel @luciferous Can you help with Adrian's question?\n. ",
    "luciferous": "Sorry if I'm narrow on the context. I'll look closer and follow up tomorrow, but here's a quick answer for now.\nTo get a Buf from an InputStream we have to use a Reader.\nimport com.twitter.io.Reader\nval reader: Reader = Reader.fromStream(stream)\nval buf: Future[Buf] = Reader.readAll(reader)\nThis gives a Future[Buf], which you could flatMap to get at the Buf.\n. ",
    "clehene": "could someone have KAFKA_ZOOKEEPER_CONNECT set (for something else) and activate this by mistake? \n. left this duplication on purpose?\n. Add a note on the default topic and perhaps how to override that\n. ",
    "rnorth": "1.1.7 is out now \ud83d\ude01 \n. ",
    "C-Otto": "There are two } instead of one. I don't know JavaScript enough to be sure, but I assume that ${traceId} and {traceId} are different when replacing?. ",
    "leegggg": "Add this as a elasticsearch property may be a better idea. zipkin.query.lookback is a 32bit int variable which has a maximum of 2_147_483_647(ms) that is about 59.6621 hours. It is really not enough. \nA zipkin.storage.elasticsearch.names-lookback property in secs or even days can be great. Or may be you can use a 64bit variable for zipkin.query.lookback.\nps. Should this be a new issue?. ",
    "fmachado": "Just a heads up:\n\n\"Tags containing highly variable information like UUIDs, hashes, and random strings will lead to a large number of series in the database, known colloquially as high series cardinality. High series cardinality is a primary driver of high memory usage for many database workloads.\"\n\nhttps://docs.influxdata.com/influxdb/v1.3/concepts/schema_and_data_layout/#don-t-have-too-many-series. This will synchronously write to InfluxDB (https://github.com/influxdata/influxdb-java/issues/386).\nIf you want to do it async, enable the batch mode when you create the client:\nJava\n// Flush every 2000 Points, at least every 1000ms\ninfluxDBClient.enableBatch(2000, 1000, TimeUnit.MILLISECONDS);\nand use the write method for Point and not BatchPoint.. This may cause a NPE if there is a null value returned by InfluxDB. Would be possible to use String.valueOf(...)?. This may cause a NPE if there is a null value returned by InfluxDB. Would be possible to use String.valueOf(...)?. ",
    "jplock": "Should this be in a try block to ensure the stream is always closed even if there\u2019s a parsing exception?. ",
    "hyangtack": "MediaType.ANY_TEXT_TYPE is for text/*, so I think you need to use some other media type such as MediaType.PLAIN_TEXT_UTF_8.. You may know but you can omit the value of @Param annotation if you specify -parameters javac option.\nhttps://line.github.io/armeria/server-annotated-service.html#parameter-injection. Actually it's enough that you just specify return sb -> sb.decorator(corsDecorator);. But I guess we have a problem CORS with annotated services. Let me check it, then let you know. . I'm not sure but isn't it enough to specify ${QUERY_PORT:9411}?. Filed an issue: https://github.com/line/armeria/issues/1531. Really? I will take a look again! . Yes, you're right. But the orders for the other ArmeriaServerConfigurator beans are 0 which is a default when no @Order annotation is specified. And just specifying @Order makes its bean lowest precedence because the annotation's default value is Integer.MAX_VALUE (which means lowest precedence). \nSo it's okay to specify @Order only from here. But if you want to make their order clear, it would also be okay to specify order on all ArmeriaServerConfigurator beans. :-)\nSee: \n- https://github.com/line/armeria/pull/1535\n- https://line.github.io/armeria/apidocs/com/linecorp/armeria/spring/ArmeriaServerConfigurator.html#getOrder(). Ah, got it. I missed that. I guess there's no ordering issue between them, but I will change their order in order to make it clear. Thank you!. @adriancole I fixed here to pass the test https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/test/java/zipkin2/server/internal/ITZipkinServerHttpCollectorDisabled.java#L49-L56. From the previous version, 405 method not allowed returned because /api/v1/spans path exists but POST method is not allowed due to zipkin.collector.http.enabled=false. In this case 405 method not allowed seems more reasonable, so would it better to fix the test code? Please let me know what you think! :-). ",
    "qiufeng001": "Do you have a version with mongodb? Thanks. "
}
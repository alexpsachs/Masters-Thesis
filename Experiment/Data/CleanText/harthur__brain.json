{
    "harthur": "Thanks for filing. I realized this a couple weeks ago, I should have filed this issue so that I didn't waste your time, I mentioned it here: http://wiki.github.com/harthur/brain/todo.\nI hardly ever use IE and I'm not so familiar with how to get around this issue in IE, I think you could just have a getOutputs and setOutputs, which isn't as nice, but would work in IE too. I don't think defineProperty works in Firefox or Safari.\nIn order to turn the brain.js CommonJS module into a browser file you have to get rid of the \"exports.NeuralNetwork = NeuralNetwork\" line at the bottom, not sure if that's your problem? Unfortunately the process of turning it into a browser file isn't automated, and is going to get more complicated very soon , that's one of the things on my list (-:\n. Thanks for the pull request, why do you think toJSON() should return a string? I based it on the generic object toJSON() method https://developer.mozilla.org/en/json#toJSON%28%29_method. After some playing around with JSON.stringify() it seems like toJSON() it should return an object that can be serialized, also this seems to agree, but I'm not sure:\nhttp://mxr.mozilla.org/mozilla-central/source/testing/mozmill/mozmill/mozmill/extension/resource/stdlib/json2.js#30\n. Thanks! Totally merge-button merging this, though for future reference since the variable is an argument you don't have to do typeof to check for undefined you can just do if(callback).\n. Thanks again! Love the whitespace fixes.\n. Ah crap, I fixed this in my local but never pushed it. Thanks for the fix!\n. hmm, it looks like the error is coming from classify(), are you by any chance calling bayes.classify(\"some text\") without the callback argument?:\njavascript\nbayes.classify(\"some text\", function(category) {\n  console.log(\"text is in category: \" + category);\n});\nThe Redis backend requires asynchronous classification. hth!\n. Bayesian classifier has moved to https://github.com/harthur/classifier.\n. Are you using brain in the browser?\n. The Bayesian classifier has moved to: https://github.com/harthur/classifier\n. Should be fixed with new test runner: https://github.com/harthur/brain/tree/master/test\n. Wow, thanks for the catch!\n. Thanks for the pull request. Auth support has been checked in. Can you open a new request just for the connection part and put it behind an option?\n. Thanks!\n. Yeah, there's a reason there isn't one. The network can't be trained incrementally, you can only call train() once with all the data, so there's no point in keeping the network state around in Redis.\nYou can however collect the data incrementally, but that could be done outside the library and fed into train() once you've collected enough. If you just want to store the fully trained network in Redis, you can call network.toJSON() and put it in yourself.\n. That's the same as retraining from scratch. Unfortunately, the best way to deal with this is to build up the data, and re-train periodically with everything you've collected so far.\n. @christopherdebeer good point. I think more of them should actually move to the train() call, with maybe the hiddenLayers option sticking with the network initialization.\n. This would be a great feature for sure, but it would likely take the form of a separate implementation of another kind of neural network. And it wouldn't achieve as good a result as training once with all the data.\n. Calling this trainMore() with a second dataset would obliterate pretty much any state trained by the previous data set, I think.\nIf the data is similar to the previous data, it would just reach a local minimum (optimizing for the second set only) faster. If it's not similar, it might as well have been trained starting with random weights. Either way, you're not going to get results that differ much from training the network from a random state.\nI haven't heard of feed-forward back-propogation trained neural networks being trained in batches like you want. I totally understand the use case though, and there's probably another kind of neural network or training algorithm that is more suited to it.\n. I totally see the desire for this, but it would involve implementing a different type of neural network I believe. I want to keep this library as an implementation of the basic, feed foward neural network. Feel free to write your own library of course. cc @karpathy for info on any neural nets that can be trained online.\n. Closing because this won't be implemented in this library.\n@evolu8 File another issue if you want to discuss parallelization. Not sure how that would work myself, but maybe someone else will be able to chime in.\n. Thanks for the pull request. There's a reason the function is created this way actually. It's so that you can call toString() on the function and use that source, all by itself without a closure, to run a network. You can't do that without evaluating the body because you need to know what's in the network object.\nHonestly, toFunction() a bit redundant and confusing with toJSON(). It saves a bit of code, but it might go away.\nThanks anyways!\n. If you're talking about the color contrast demo (http://harthur.github.com/brain/), then you might be pressing \"train neural network\" before you've collected any samples by choosing between the blocks with black or white text.\nShould probably hide that button until at least one sample has been collected, sorry.\n. Fixed.\n. Should be fixed. npm install will now install dev dependencies.\n. Thanks a bunch for reporting this. It's been awhile since I've built, and there's a good chance I used browserbuild for 1.6.0. I'll figure it out later today and update the docs.\n. Updated README to point to new browser files.\n. Thanks for reporting this. Feel free to send a pull request (; If not, I'll still get around to this soon.\n. Yes, the demo is an example.\n. It's a feedforward neural network trained using backpropagation.\n. Training can definitely take a long time, depending on your data.\nIncreasing the number of samples could decrease or increase your training time. If the addition of samples lends more predictability to your data, then it could actually decrease the training time, as the training will need fewer iterations to reach an acceptable error rate.\nIt completely depends on the data, but yeah, the cases I've tried, adding more samples will increase the training time.\nA better measure would be training time versus iterations completed. train() returns the training results with an iterations count.\n. @nickpoorman there's no real way to make this work unfortunately, see #15. You have to have everything in memory at once.\nThe only option would be to radically change the neural network used. There are different kinds of neural networks, and some would support training incrementally.\n. Actually, I take that back, I think this could work, and it doesn't necessarily depend on #15.\n. Thanks for this functionality! Can you add a mocha test to the test/unit directory too?\n. @nickpoorman thanks for taking the time to add a test. Sorry to get to it late, had a busy week. I haven't looked over all the code changes, as there's a lot of indentation change. but I've left a few comments on the readme changes.\nI'm wondering what the advantage is to having a network itself be a write stream vs. the network just having a network.trainStream. On first think, the latter actually seems like it would be more self-explanatory when consuming the API. I think the former would make more sense if the network only had one function, but it does predicting as well as training. What were your thoughts?\n. Can you elaborate a bit on \"crazy binding calls\", give an example?\nHolistically, I'd rather not have the whole network be a stream just for training. The network has bulk predicting as well, what if you wanted that to be streamed?\nAdditionally, the code net.write(data) isn't very explicit about what it's doing, you'd have to add a code comment saying that by writing to the network, you're writing to a training stream. If we wanted to keep the network as a stream, we'd want to add a more explicitly-named function that wraps this.\n. Thanks a bunch for doing that. I'll take a look at this very soon.\n. This looks great, thanks Nick. I have just a couple comments.\n. Squashed and merged.\nThanks again for this. I appreciate the hard work and it looks like several other people do too.\n. Here's one using the XOR example: https://github.com/harthur/brain/blob/master/stream-example.js\n. I feel bad because you've done some work here, but unfortunately I don't accept style changes like this. Some of the changes here are style preferences, and the current maintainer (me right now) has chosen to stick with these preferences.\nThe other fixes to add missing semicolons are nice, but as the library is working, I don't want to take anyone's time to add them just for their own sake.\n. Thanks for the great pull request. Merged.\n. Added it, with some changes. Registered, let me know if it doesn't work as expected.\n. If brain used typed arrays it'd have to use Float64Arrays to retain precision.\nI decided against using Float64Arrays before, I think because they weren't implemented in all the browsers yet.\n. While I appreciate this, many of these changes are preferences. I'd rather keep the working code and avoid changing the git blame.\n. Thanks!\n. Thanks for this. It's my understanding that lodash is bigger in size than underscore. There won't be much speedup as underscore is only used in peripheral code and not in any critical paths. Let me know if there's another reason, however.\n. As brain is built into a browser file using browserify, I'm going to forgo making that file any bigger.\nCompatibility with other libraries shouldn't matter. With node the deps are local in the node_modules folder, and in the browser _ doesn't leak out. Correct me if I'm wrong.\n. @lavelle this is a feed-forward neural network trained with backpropagation, a standard textbook neural network.\nI used it in kittydar for recognition of cats in images, but I needed to use HOG descriptors as the features instead of raw pixels to get good results in the high 90s. The input vectors ended up being about 1000 dimensions. I also used a hidden layer of size 40. Cross-validation helps find the right parameters using brain.crossValidate, though that code isn't well documented sadly.\nI've heard convolutional neural networks are indeed better for image recognition. I have yet to use this myself, but check out karpathy/convnetjs for a JavaScript implementation. That's definitely worth a try I think.\n. Added.\n. Thanks for filing. I realized this a couple weeks ago, I should have filed this issue so that I didn't waste your time, I mentioned it here: http://wiki.github.com/harthur/brain/todo.\nI hardly ever use IE and I'm not so familiar with how to get around this issue in IE, I think you could just have a getOutputs and setOutputs, which isn't as nice, but would work in IE too. I don't think defineProperty works in Firefox or Safari.\nIn order to turn the brain.js CommonJS module into a browser file you have to get rid of the \"exports.NeuralNetwork = NeuralNetwork\" line at the bottom, not sure if that's your problem? Unfortunately the process of turning it into a browser file isn't automated, and is going to get more complicated very soon , that's one of the things on my list (-:\n. Thanks for the pull request, why do you think toJSON() should return a string? I based it on the generic object toJSON() method https://developer.mozilla.org/en/json#toJSON%28%29_method. After some playing around with JSON.stringify() it seems like toJSON() it should return an object that can be serialized, also this seems to agree, but I'm not sure:\nhttp://mxr.mozilla.org/mozilla-central/source/testing/mozmill/mozmill/mozmill/extension/resource/stdlib/json2.js#30\n. Thanks! Totally merge-button merging this, though for future reference since the variable is an argument you don't have to do typeof to check for undefined you can just do if(callback).\n. Thanks again! Love the whitespace fixes.\n. Ah crap, I fixed this in my local but never pushed it. Thanks for the fix!\n. hmm, it looks like the error is coming from classify(), are you by any chance calling bayes.classify(\"some text\") without the callback argument?:\njavascript\nbayes.classify(\"some text\", function(category) {\n  console.log(\"text is in category: \" + category);\n});\nThe Redis backend requires asynchronous classification. hth!\n. Bayesian classifier has moved to https://github.com/harthur/classifier.\n. Are you using brain in the browser?\n. The Bayesian classifier has moved to: https://github.com/harthur/classifier\n. Should be fixed with new test runner: https://github.com/harthur/brain/tree/master/test\n. Wow, thanks for the catch!\n. Thanks for the pull request. Auth support has been checked in. Can you open a new request just for the connection part and put it behind an option?\n. Thanks!\n. Yeah, there's a reason there isn't one. The network can't be trained incrementally, you can only call train() once with all the data, so there's no point in keeping the network state around in Redis.\nYou can however collect the data incrementally, but that could be done outside the library and fed into train() once you've collected enough. If you just want to store the fully trained network in Redis, you can call network.toJSON() and put it in yourself.\n. That's the same as retraining from scratch. Unfortunately, the best way to deal with this is to build up the data, and re-train periodically with everything you've collected so far.\n. @christopherdebeer good point. I think more of them should actually move to the train() call, with maybe the hiddenLayers option sticking with the network initialization.\n. This would be a great feature for sure, but it would likely take the form of a separate implementation of another kind of neural network. And it wouldn't achieve as good a result as training once with all the data.\n. Calling this trainMore() with a second dataset would obliterate pretty much any state trained by the previous data set, I think.\nIf the data is similar to the previous data, it would just reach a local minimum (optimizing for the second set only) faster. If it's not similar, it might as well have been trained starting with random weights. Either way, you're not going to get results that differ much from training the network from a random state.\nI haven't heard of feed-forward back-propogation trained neural networks being trained in batches like you want. I totally understand the use case though, and there's probably another kind of neural network or training algorithm that is more suited to it.\n. I totally see the desire for this, but it would involve implementing a different type of neural network I believe. I want to keep this library as an implementation of the basic, feed foward neural network. Feel free to write your own library of course. cc @karpathy for info on any neural nets that can be trained online.\n. Closing because this won't be implemented in this library.\n@evolu8 File another issue if you want to discuss parallelization. Not sure how that would work myself, but maybe someone else will be able to chime in.\n. Thanks for the pull request. There's a reason the function is created this way actually. It's so that you can call toString() on the function and use that source, all by itself without a closure, to run a network. You can't do that without evaluating the body because you need to know what's in the network object.\nHonestly, toFunction() a bit redundant and confusing with toJSON(). It saves a bit of code, but it might go away.\nThanks anyways!\n. If you're talking about the color contrast demo (http://harthur.github.com/brain/), then you might be pressing \"train neural network\" before you've collected any samples by choosing between the blocks with black or white text.\nShould probably hide that button until at least one sample has been collected, sorry.\n. Fixed.\n. Should be fixed. npm install will now install dev dependencies.\n. Thanks a bunch for reporting this. It's been awhile since I've built, and there's a good chance I used browserbuild for 1.6.0. I'll figure it out later today and update the docs.\n. Updated README to point to new browser files.\n. Thanks for reporting this. Feel free to send a pull request (; If not, I'll still get around to this soon.\n. Yes, the demo is an example.\n. It's a feedforward neural network trained using backpropagation.\n. Training can definitely take a long time, depending on your data.\nIncreasing the number of samples could decrease or increase your training time. If the addition of samples lends more predictability to your data, then it could actually decrease the training time, as the training will need fewer iterations to reach an acceptable error rate.\nIt completely depends on the data, but yeah, the cases I've tried, adding more samples will increase the training time.\nA better measure would be training time versus iterations completed. train() returns the training results with an iterations count.\n. @nickpoorman there's no real way to make this work unfortunately, see #15. You have to have everything in memory at once.\nThe only option would be to radically change the neural network used. There are different kinds of neural networks, and some would support training incrementally.\n. Actually, I take that back, I think this could work, and it doesn't necessarily depend on #15.\n. Thanks for this functionality! Can you add a mocha test to the test/unit directory too?\n. @nickpoorman thanks for taking the time to add a test. Sorry to get to it late, had a busy week. I haven't looked over all the code changes, as there's a lot of indentation change. but I've left a few comments on the readme changes.\nI'm wondering what the advantage is to having a network itself be a write stream vs. the network just having a network.trainStream. On first think, the latter actually seems like it would be more self-explanatory when consuming the API. I think the former would make more sense if the network only had one function, but it does predicting as well as training. What were your thoughts?\n. Can you elaborate a bit on \"crazy binding calls\", give an example?\nHolistically, I'd rather not have the whole network be a stream just for training. The network has bulk predicting as well, what if you wanted that to be streamed?\nAdditionally, the code net.write(data) isn't very explicit about what it's doing, you'd have to add a code comment saying that by writing to the network, you're writing to a training stream. If we wanted to keep the network as a stream, we'd want to add a more explicitly-named function that wraps this.\n. Thanks a bunch for doing that. I'll take a look at this very soon.\n. This looks great, thanks Nick. I have just a couple comments.\n. Squashed and merged.\nThanks again for this. I appreciate the hard work and it looks like several other people do too.\n. Here's one using the XOR example: https://github.com/harthur/brain/blob/master/stream-example.js\n. I feel bad because you've done some work here, but unfortunately I don't accept style changes like this. Some of the changes here are style preferences, and the current maintainer (me right now) has chosen to stick with these preferences.\nThe other fixes to add missing semicolons are nice, but as the library is working, I don't want to take anyone's time to add them just for their own sake.\n. Thanks for the great pull request. Merged.\n. Added it, with some changes. Registered, let me know if it doesn't work as expected.\n. If brain used typed arrays it'd have to use Float64Arrays to retain precision.\nI decided against using Float64Arrays before, I think because they weren't implemented in all the browsers yet.\n. While I appreciate this, many of these changes are preferences. I'd rather keep the working code and avoid changing the git blame.\n. Thanks!\n. Thanks for this. It's my understanding that lodash is bigger in size than underscore. There won't be much speedup as underscore is only used in peripheral code and not in any critical paths. Let me know if there's another reason, however.\n. As brain is built into a browser file using browserify, I'm going to forgo making that file any bigger.\nCompatibility with other libraries shouldn't matter. With node the deps are local in the node_modules folder, and in the browser _ doesn't leak out. Correct me if I'm wrong.\n. @lavelle this is a feed-forward neural network trained with backpropagation, a standard textbook neural network.\nI used it in kittydar for recognition of cats in images, but I needed to use HOG descriptors as the features instead of raw pixels to get good results in the high 90s. The input vectors ended up being about 1000 dimensions. I also used a hidden layer of size 40. Cross-validation helps find the right parameters using brain.crossValidate, though that code isn't well documented sadly.\nI've heard convolutional neural networks are indeed better for image recognition. I have yet to use this myself, but check out karpathy/convnetjs for a JavaScript implementation. That's definitely worth a try I think.\n. Added.\n. ",
    "techarch": "Hi Heather - I just made the changes on my local copy of brain-0.1.js and neuralnetwork.js and tested them. When I get home tonight I will check-them in against my fork and will send you a pull request.\n. Hi Heather - I just made the changes on my local copy of brain-0.1.js and neuralnetwork.js and tested them. When I get home tonight I will check-them in against my fork and will send you a pull request.\n. ",
    "KOLANICH": "ok\nthen fix http://harthur.github.com/brain/#neural-network\n(write that toJSON is a service function to be called by JSON.stringify)\nalso make export and import aliases for that\n. ok\nthen fix http://harthur.github.com/brain/#neural-network\n(write that toJSON is a service function to be called by JSON.stringify)\nalso make export and import aliases for that\n. ",
    "masterkain": "I have the same issue. Any word on this?\n. I have the same issue. Any word on this?\n. ",
    "acacio": "``` diff\ndiff --git a/lib/bayesian/backends/redis.js b/lib/bayesian/backends/redis.js\nindex ce4ca4c..eb2cdc8 100644\n--- a/lib/bayesian/backends/redis.js\n+++ b/lib/bayesian/backends/redis.js\n@@ -6,11 +6,14 @@ var RedisBackend = function(options) {\n   var port = options.port || 6379;\n   var host = options.hostname || \"localhost\";\n   var opts = options.options || {};\n+  var password = options.password || false;\nthis.client = function() {\n     var client = redis.createClient(port, host, opts);\n     if(options.error)\n       client.on('error', options.error);\n+    if (password)\n+      client.auth(password, options.error);\n     return client;\n   }\n@@ -114,4 +117,4 @@ RedisBackend.prototype = {\n   }\n }\n-exports.RedisBackend = RedisBackend;\n\\ No newline at end of file\n+exports.RedisBackend = RedisBackend;\ndiff --git a/package.json b/package.json\nindex 0314822..745f77b 100644\n--- a/package.json\n+++ b/package.json\n@@ -1,7 +1,7 @@\n {\n     \"name\": \"brain\",\n     \"description\": \"neural network and classifier library\",\n-    \"version\": \"0.3.5\",\n+    \"version\": \"0.3.7\",\n     \"author\": \"Heather Arthur fayearthur@gmail.com\",\n     \"repository\": {\n         \"type\": \"git\",\n. diff\ndiff --git a/lib/bayesian/backends/redis.js b/lib/bayesian/backends/redis.js\nindex ce4ca4c..eb2cdc8 100644\n--- a/lib/bayesian/backends/redis.js\n+++ b/lib/bayesian/backends/redis.js\n@@ -6,11 +6,14 @@ var RedisBackend = function(options) {\n   var port = options.port || 6379;\n   var host = options.hostname || \"localhost\";\n   var opts = options.options || {};\n+  var password = options.password || false;\nthis.client = function() {\n     var client = redis.createClient(port, host, opts);\n     if(options.error)\n       client.on('error', options.error);\n+    if (password)\n+      client.auth(password, options.error);\n     return client;\n   }\n@@ -114,4 +117,4 @@ RedisBackend.prototype = {\n   }\n }\n-exports.RedisBackend = RedisBackend;\n\\ No newline at end of file\n+exports.RedisBackend = RedisBackend;\ndiff --git a/package.json b/package.json\nindex 0314822..745f77b 100644\n--- a/package.json\n+++ b/package.json\n@@ -1,7 +1,7 @@\n {\n     \"name\": \"brain\",\n     \"description\": \"neural network and classifier library\",\n-    \"version\": \"0.3.5\",\n+    \"version\": \"0.3.7\",\n     \"author\": \"Heather Arthur fayearthur@gmail.com\",\n     \"repository\": {\n         \"type\": \"git\",\n```\n. ",
    "christopherdebeer": "I wasn't aware that a network couldnt be trained more than once - damn :( my bad. thanks for the reply.\n. Sorry quick question, after I posted my previous comment, i tried it out (training an already trained network) and it works (in so far as it doesn't throw any errors) Is this essentially re-training the network from scratch using the new data, or is it just screwing the network up?\n. Thanks :) clarification much appreciated.\n. I wasn't aware that a network couldnt be trained more than once - damn :( my bad. thanks for the reply.\n. Sorry quick question, after I posted my previous comment, i tried it out (training an already trained network) and it works (in so far as it doesn't throw any errors) Is this essentially re-training the network from scratch using the new data, or is it just screwing the network up?\n. Thanks :) clarification much appreciated.\n. ",
    "tlhunter": "I would like to see brain one day be able to train multiple times instead of just once. If that were to happen, it would make sense to keep the settings as they currently are.\n. My thought was that it would be nice if a machine could continuously learn throughout it's lifetime without keeping a database of past situations/outcomes, where it would be cheap to train one more experience instead of expensive to train all of them + 1.\nBut yeah, I may have to fork Brain for that to happen. I'm not too well versed with the various kinds of NN, the previous ones I had worked with had this capability.\n. I would like to see brain one day be able to train multiple times instead of just once. If that were to happen, it would make sense to keep the settings as they currently are.\n. My thought was that it would be nice if a machine could continuously learn throughout it's lifetime without keeping a database of past situations/outcomes, where it would be cheap to train one more experience instead of expensive to train all of them + 1.\nBut yeah, I may have to fork Brain for that to happen. I'm not too well versed with the various kinds of NN, the previous ones I had worked with had this capability.\n. ",
    "robertleeplummerjr": "@tlhunter see: https://github.com/harthur/brain/pull/63. Is this solved?. ty @miketheprogrammer!. I believe if I had access to the source, to reproduce, I could solve this easily.  Let me know if you can do that.. Solved here: https://github.com/harthur-org/brain.js/blob/3843729dd0ae5cb8e6be21b6040c254a9fcf4e0b/src/neural-network.js. I really like your thought here.  I believe this could easily be solved with the recurrent neural net implemented here: https://github.com/harthur-org/brain.js (the community continuation of the lib).  I believe the means of solving this would simply be iteration of every mathematical means of solving the puzzle.  The questions is, how would you predict the next move(s)?  Would you do one at a time, looking to the next, or would you solve from the beginning to end?\nHere is an example:\n```js\nimport LSTM from '../../src/recurrent/lstm';\nimport Vocab from '../../src/utilities/vocab';\nvar net = new LSTM({\n  vocab: Vocab.allPrintableSeparated()\n});\nnet.train([\n  {\n    input: 'hi',\n    output: 'mom!'\n  }, {\n    input: 'hello',\n    output: 'dad!'\n  }\n]);\nconsole.log(net.run('hi')); //some variation of \"mom!\"\nconsole.log(net.run('hello')); //some variation of \"dad!\"\n```. See https://github.com/harthur/brain/pull/63. I believe this is solved here:https://github.com/harthur-org/brain.js\nWe upgraded the syntax to es6.. ty sir!. Solved here: https://github.com/harthur-org/brain.js/blame/master/src/neural-network.js#L496 with es6. Would you consider closing this here, and reopening at: https://github.com/harthur-org/brain.js/pulls?\nIf so, we could add you as a collaborator.. Would you consider closing this here, and reopening at: https://github.com/harthur-org/brain.js/pulls?\nIf so, we could add you as a collaborator.. Sometime back I added a keepNetworkIntact argument, I believe it does exactly what you want: https://github.com/harthur-org/brain.js/blob/3843729dd0ae5cb8e6be21b6040c254a9fcf4e0b/src/neural-network.js#L41. Usage example:\njs\nconst net = brain.NeuralNetwork();\nnet.train([]);\n//sometime later\nnet.train([], { keepNetworkIntact: true });. We need to bump the version.  In the meantime, can you install via: npm i https://github.com/harthur-org/brain.js.git?. . Would you consider closing this here, and reopening at: https://github.com/harthur-org/brain.js/pulls?\nIf so, we could add you as a collaborator.. https://github.com/harthur/brain is unmaintained, https://github.com/harthur-org/brain.js is the community continuation of the library.. I'm curious what this looks like, as source, before and after.  Do you have the performance numbers, and the project they ran on?  Very interesting stuff!. Would you consider closing this here, and reopening at: https://github.com/harthur-org/brain.js/pulls?\nIf so, we could add you as a collaborator.. And mind blown.  Really nice work guys!\n. I think we'd like to make this type of thing easier in the future for other libs/projects, what would you guys like to see that would make it easier to implement?  Do you have some example code available that we could link up?  Off the top of my head I know exactly where these callbacks could go if you'd like them added.\n. @dasaki-gr dug into ~~your~~ the mentioned library, https://github.com/mcrowe/neurovis very cool.  I'll see what I can get running for you tonight :)\n. So digging further, https://github.com/mcrowe/neurovis is really its own full fledged neural network attached to vis.js, running on an amalgamation of server and client.  I could probably extend it to use brain.js, but seems a little awkward.\n. Would you consider closing this here, and reopening at: https://github.com/harthur-org/brain.js/pulls?\nIf so, we could add you as a collaborator.. Merged!: https://github.com/harthur-org/brain.js/pull/42\nConsider closing here?. I did contact @harthur about being a maintainer, no reply.  I did read her website, and why she ended up doing so, and totally respect it.  I would really like to continue in her absence, hoping the best for her.  There is a risk that she doesn't like any of the changes we do, but there is also the chance we may end up improving these fantastic libraries.  That being said, there are enough of us and these libraries are useful enough I went ahead and created: https://github.com/harthur-org and forked all libraries.  If you'd like access, open a pull request.\n. @ilanbiala, What if @harthur decides to re-maintain?  We open a single pull request, and merge, and then the org can go away.\n. Give me a few, I'll work it out.\n. I get what you intended to say.  I do it all the time for private repos based off of public ones.  Don't know why I didn't think about it, probably an A.D.D. moment.\n. Done for brain, and: https://github.com/harthur-org/brain/commit/388e5ea1840a0659156d560f3bb411acdf1ed535\n. @IonicaBizau I'd like to make you a fellow maintainer since you started this issue.  Seems only right.  Just pushed unit tests for the likely method.\n. :fist: :fire:\n. Added you to the \"with great power comes great responsibility\" team.\n. I know, the name of the team is a reminder for me :stuck_out_tongue: \n. Well stated @nickpoorman.\n. How would you guys feel about a :+1: policy?  Where at least one person that has contributed and that is not the committer must :+1: the PR moving forward.  When the :+1: is landed, we then can merge the PR.\n. when one has the capability to rewrite git history... \n. Devs, I believe a \"huzza\" is in order.\n. Huzza!!!!\n. Can you please open a new issue, that way this issue stays focused?\n. ping\n. merged some time back.. @1UnboundedSentience ping.  Also, nice work @IonicaBizau!\n. Consider closing as it is addressed in the community maintained version?. ty for finding this!. Would you consider closing this here, and reopening at: https://github.com/harthur-org/brain.js/pulls?\nIf so, we could add you as a collaborator.. I believe we implemented your work or similar from this pr: https://github.com/harthur-org/brain.js/pull/56 here: https://github.com/harthur-org/brain.js/commit/659ded832d06a39a2d3733ee08a718c860308967#diff-f0964eec5cfcdf6b9c5b85748849be07R492\n. Is this issue resolved?. @nickpoorman & @Dok11, the new repository does do matrix transforms via the recurrent neural net...\nExample from: https://github.com/harthur-org/brain.js/commit/338cf707a5a830c36aab3110fbf6cc33db321c5c#diff-04c6e90faac2675aa89e2176d2eec7d8R25\n```js\n//create a simple recurrent neural network\nvar net = new brain.recurrent.RNN();\nnet.train([{input: [0, 0], output: [0]},\n           {input: [0, 1], output: [1]},\n           {input: [1, 0], output: [1]},\n           {input: [1, 1], output: [0]}]);\nvar output = net.run([0, 0]);  // [0]\noutput = net.run([0, 1]);  // [1]\noutput = net.run([1, 0]);  // [1]\noutput = net.run([1, 1]);  // [0]\n``\n![](https://media.giphy.com/media/fGbbcXk14nqfe/giphy.gif). Your letting negative values return fromscaleNumberRange` which I assume is your means of normalizing values.\n```js\n/*\n * simple module to scale a number from one range to another\n /\nvar debug = require('debug')('scale-number-range');\nmodule.exports = function scaleNumberRange(number, oldMin, oldMax, newMin, newMax) {\n  if (process.env.SCALE_THROW_OOB_ERRORS) {\n    if (number < oldMin) {\n      debug('ERROR OOB - scale(%d, %d, %d, %d, %d)', number, oldMin, oldMax, newMin, newMax);\n      throw new Error('number is less than oldMin');\n    }\n    if (number > oldMax) {\n      debug('ERROR OOB - scale(%d, %d, %d, %d, %d)', number, oldMin, oldMax, newMin, newMax);\n      throw new Error('number is greater than oldMax');\n    }\n  }\n  const result = (((newMax - newMin) * (number - oldMin)) / (oldMax - oldMin)) + newMin;\n  console.log(result);\n  return result;\n}\n```\nOutputs:\n$ babel-node --presets es2015-node ./test\n-1\n-0.9953080375356997\n-1\n-1\n-0.9601572507619595\n-0.8707612283685106\n-1\n-0.9914394723724069\n-1\n-0.9979813272773151\n-0.5591789084541298\n{ '0': NaN }. If I *= -1 result, I still get NaN, so still investigating.. lol, beat me to it!  In all fairness, I was getting a haircut.. @tlhunter see: https://github.com/harthur/brain/pull/63. Is this solved?. ty @miketheprogrammer!. I believe if I had access to the source, to reproduce, I could solve this easily.  Let me know if you can do that.. Solved here: https://github.com/harthur-org/brain.js/blob/3843729dd0ae5cb8e6be21b6040c254a9fcf4e0b/src/neural-network.js. I really like your thought here.  I believe this could easily be solved with the recurrent neural net implemented here: https://github.com/harthur-org/brain.js (the community continuation of the lib).  I believe the means of solving this would simply be iteration of every mathematical means of solving the puzzle.  The questions is, how would you predict the next move(s)?  Would you do one at a time, looking to the next, or would you solve from the beginning to end?\nHere is an example:\n```js\nimport LSTM from '../../src/recurrent/lstm';\nimport Vocab from '../../src/utilities/vocab';\nvar net = new LSTM({\n  vocab: Vocab.allPrintableSeparated()\n});\nnet.train([\n  {\n    input: 'hi',\n    output: 'mom!'\n  }, {\n    input: 'hello',\n    output: 'dad!'\n  }\n]);\nconsole.log(net.run('hi')); //some variation of \"mom!\"\nconsole.log(net.run('hello')); //some variation of \"dad!\"\n```. See https://github.com/harthur/brain/pull/63. I believe this is solved here:https://github.com/harthur-org/brain.js\nWe upgraded the syntax to es6.. ty sir!. Solved here: https://github.com/harthur-org/brain.js/blame/master/src/neural-network.js#L496 with es6. Would you consider closing this here, and reopening at: https://github.com/harthur-org/brain.js/pulls?\nIf so, we could add you as a collaborator.. Would you consider closing this here, and reopening at: https://github.com/harthur-org/brain.js/pulls?\nIf so, we could add you as a collaborator.. Sometime back I added a keepNetworkIntact argument, I believe it does exactly what you want: https://github.com/harthur-org/brain.js/blob/3843729dd0ae5cb8e6be21b6040c254a9fcf4e0b/src/neural-network.js#L41. Usage example:\njs\nconst net = brain.NeuralNetwork();\nnet.train([]);\n//sometime later\nnet.train([], { keepNetworkIntact: true });. We need to bump the version.  In the meantime, can you install via: npm i https://github.com/harthur-org/brain.js.git?. . Would you consider closing this here, and reopening at: https://github.com/harthur-org/brain.js/pulls?\nIf so, we could add you as a collaborator.. https://github.com/harthur/brain is unmaintained, https://github.com/harthur-org/brain.js is the community continuation of the library.. I'm curious what this looks like, as source, before and after.  Do you have the performance numbers, and the project they ran on?  Very interesting stuff!. Would you consider closing this here, and reopening at: https://github.com/harthur-org/brain.js/pulls?\nIf so, we could add you as a collaborator.. And mind blown.  Really nice work guys!\n. I think we'd like to make this type of thing easier in the future for other libs/projects, what would you guys like to see that would make it easier to implement?  Do you have some example code available that we could link up?  Off the top of my head I know exactly where these callbacks could go if you'd like them added.\n. @dasaki-gr dug into ~~your~~ the mentioned library, https://github.com/mcrowe/neurovis very cool.  I'll see what I can get running for you tonight :)\n. So digging further, https://github.com/mcrowe/neurovis is really its own full fledged neural network attached to vis.js, running on an amalgamation of server and client.  I could probably extend it to use brain.js, but seems a little awkward.\n. Would you consider closing this here, and reopening at: https://github.com/harthur-org/brain.js/pulls?\nIf so, we could add you as a collaborator.. Merged!: https://github.com/harthur-org/brain.js/pull/42\nConsider closing here?. I did contact @harthur about being a maintainer, no reply.  I did read her website, and why she ended up doing so, and totally respect it.  I would really like to continue in her absence, hoping the best for her.  There is a risk that she doesn't like any of the changes we do, but there is also the chance we may end up improving these fantastic libraries.  That being said, there are enough of us and these libraries are useful enough I went ahead and created: https://github.com/harthur-org and forked all libraries.  If you'd like access, open a pull request.\n. @ilanbiala, What if @harthur decides to re-maintain?  We open a single pull request, and merge, and then the org can go away.\n. Give me a few, I'll work it out.\n. I get what you intended to say.  I do it all the time for private repos based off of public ones.  Don't know why I didn't think about it, probably an A.D.D. moment.\n. Done for brain, and: https://github.com/harthur-org/brain/commit/388e5ea1840a0659156d560f3bb411acdf1ed535\n. @IonicaBizau I'd like to make you a fellow maintainer since you started this issue.  Seems only right.  Just pushed unit tests for the likely method.\n. :fist: :fire:\n. Added you to the \"with great power comes great responsibility\" team.\n. I know, the name of the team is a reminder for me :stuck_out_tongue: \n. Well stated @nickpoorman.\n. How would you guys feel about a :+1: policy?  Where at least one person that has contributed and that is not the committer must :+1: the PR moving forward.  When the :+1: is landed, we then can merge the PR.\n. when one has the capability to rewrite git history... \n. Devs, I believe a \"huzza\" is in order.\n. Huzza!!!!\n. Can you please open a new issue, that way this issue stays focused?\n. ping\n. merged some time back.. @1UnboundedSentience ping.  Also, nice work @IonicaBizau!\n. Consider closing as it is addressed in the community maintained version?. ty for finding this!. Would you consider closing this here, and reopening at: https://github.com/harthur-org/brain.js/pulls?\nIf so, we could add you as a collaborator.. I believe we implemented your work or similar from this pr: https://github.com/harthur-org/brain.js/pull/56 here: https://github.com/harthur-org/brain.js/commit/659ded832d06a39a2d3733ee08a718c860308967#diff-f0964eec5cfcdf6b9c5b85748849be07R492\n. Is this issue resolved?. @nickpoorman & @Dok11, the new repository does do matrix transforms via the recurrent neural net...\nExample from: https://github.com/harthur-org/brain.js/commit/338cf707a5a830c36aab3110fbf6cc33db321c5c#diff-04c6e90faac2675aa89e2176d2eec7d8R25\n```js\n//create a simple recurrent neural network\nvar net = new brain.recurrent.RNN();\nnet.train([{input: [0, 0], output: [0]},\n           {input: [0, 1], output: [1]},\n           {input: [1, 0], output: [1]},\n           {input: [1, 1], output: [0]}]);\nvar output = net.run([0, 0]);  // [0]\noutput = net.run([0, 1]);  // [1]\noutput = net.run([1, 0]);  // [1]\noutput = net.run([1, 1]);  // [0]\n``\n![](https://media.giphy.com/media/fGbbcXk14nqfe/giphy.gif). Your letting negative values return fromscaleNumberRange` which I assume is your means of normalizing values.\n```js\n/*\n * simple module to scale a number from one range to another\n /\nvar debug = require('debug')('scale-number-range');\nmodule.exports = function scaleNumberRange(number, oldMin, oldMax, newMin, newMax) {\n  if (process.env.SCALE_THROW_OOB_ERRORS) {\n    if (number < oldMin) {\n      debug('ERROR OOB - scale(%d, %d, %d, %d, %d)', number, oldMin, oldMax, newMin, newMax);\n      throw new Error('number is less than oldMin');\n    }\n    if (number > oldMax) {\n      debug('ERROR OOB - scale(%d, %d, %d, %d, %d)', number, oldMin, oldMax, newMin, newMax);\n      throw new Error('number is greater than oldMax');\n    }\n  }\n  const result = (((newMax - newMin) * (number - oldMin)) / (oldMax - oldMin)) + newMin;\n  console.log(result);\n  return result;\n}\n```\nOutputs:\n$ babel-node --presets es2015-node ./test\n-1\n-0.9953080375356997\n-1\n-1\n-0.9601572507619595\n-0.8707612283685106\n-1\n-0.9914394723724069\n-1\n-0.9979813272773151\n-0.5591789084541298\n{ '0': NaN }. If I *= -1 result, I still get NaN, so still investigating.. lol, beat me to it!  In all fairness, I was getting a haircut.. ",
    "ali-mi": "I also need multiple trainings and although it is mentioned that this won't work, I wonder if adding a slightly modified version of the train function would give any useful results or not:\n``` javascript\ntrainMore: function(data, options) {\n    data = this.formatData(data);\n    options = options || {};\n    var iterations = options.iterations || 20000;\n    var errorThresh = options.errorThresh || 0.005;\n    var log = options.log || false;\n    var logPeriod = options.logPeriod || 10;\n    var callback = options.callback;\n    var callbackPeriod = options.callbackPeriod || 10;\n    var error = 1;\n    for (var i = 0; i < iterations && error > errorThresh; i++) {\n      var sum = 0;\n      for (var j = 0; j < data.length; j++) {\n        var err = this.trainPattern(data[j].input, data[j].output);\n        sum += err;\n      }\n      error = sum / data.length;\n  if (log && (i % logPeriod == 0)) {\n    console.log(\"iterations:\", i, \"training error:\", error);\n  }\n  if (callback && (i % callbackPeriod == 0)) {\n    callback({ error: error, iterations: i });\n  }\n}\n\nreturn {\n  error: error,\n  iterations: i\n};\n\n}\n```\n. I also need multiple trainings and although it is mentioned that this won't work, I wonder if adding a slightly modified version of the train function would give any useful results or not:\n``` javascript\ntrainMore: function(data, options) {\n    data = this.formatData(data);\n    options = options || {};\n    var iterations = options.iterations || 20000;\n    var errorThresh = options.errorThresh || 0.005;\n    var log = options.log || false;\n    var logPeriod = options.logPeriod || 10;\n    var callback = options.callback;\n    var callbackPeriod = options.callbackPeriod || 10;\n    var error = 1;\n    for (var i = 0; i < iterations && error > errorThresh; i++) {\n      var sum = 0;\n      for (var j = 0; j < data.length; j++) {\n        var err = this.trainPattern(data[j].input, data[j].output);\n        sum += err;\n      }\n      error = sum / data.length;\n  if (log && (i % logPeriod == 0)) {\n    console.log(\"iterations:\", i, \"training error:\", error);\n  }\n  if (callback && (i % callbackPeriod == 0)) {\n    callback({ error: error, iterations: i });\n  }\n}\n\nreturn {\n  error: error,\n  iterations: i\n};\n\n}\n```\n. ",
    "codename-": "+1\n. +1\n. ",
    "nickpoorman": "It would seem to me that you have to train the entire dataset at once. Although I wonder what the result would be if you were to take any new data and train just the new data into a network, then use the new network and the old network as nodes in a hidden layer of another new network.\nBy feeding one network into another network is how you develop complex networks.\n. One thing I do when getting any new data is train another network again in the background, then when it finishes hot-swap it for the running one.\n. fixes #29 \n. Ok. There's a mocha test in the test/unit directory now.\n. The additions in the readme are bit confusing, so I should probably get rid of them all together. They should really look at the test if for a more accurate way of using this anyway.\nAs far as the network being a WriteStream, I think it makes more sense that you can pipe directly to the network and emit events on it. Look at the stuff dominictarr puts out. If you really want to have network.trainStream then you are going to have to do some crazy binding calls throughout the implementation which I personally think will add more complexity to it.\n. Ok, I pulled the stream stuff into its own TrainStream class that can manipulate the network. The tests and README were also updated.\n``` bash\nnick : brain $ mocha test/unit/*\n\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\n27 passing (866 ms)\n```\n. Changes should be easy to see now: \nhttps://github.com/harthur/brain/pull/30/files \n. Ok. I fixed the comments.\n. The point of null in the stream is to signal the end of an epoch. You must continuously flood the stream with the same data if you want the network to train properly. A write ahead log would defeat the purpose of using streams (which is to train using a data size greater than available memory, or disk).\nThe rest of what your describing sounds awfully implementation specific. Much like a b-tree may be used to implement a database, you could use brain to implement what you are describing.\n. +1 - I'm all for this. I suspect there will be a speedup as well.\n. I use this with hundreds of features. If you're doing 100 you might try setting the learning rate to something like 0.03 and the error threshold to say 0.0055.\nAs for dimension reduction if you have a lot of zeros and are doing something such a PCA, the PCA might be choking on the zeros. You might try something like lasso in that case.\nOn Sat, Dec 6, 2014 at 8:20 AM, Giles Lavelle notifications@github.com\nwrote:\n\nI'm trying to use this library for an image recognition assignment. We're getting pretty poor results, around 40% accuracy, compared to the high 90% range for all the other techniques we've tested. That's with all our preprocessing and training over 100s of iterations with a very low error threshold.\nI'm wondering what type of neural network this is, and whether there's anything fundamental in its implementation that makes it poorly suited to image recognition? Convolutional NNs are generally great at it, but I was thinking that this library might be a different type.\nDoes it generally perform well on high dimensional data? We're using all features with non-zero variance from our image set, which is in the order of 100 or so pixels/features. The colour contrast example given only has 3 dimensions, so I was thinking we might be able to get better performance with some aggressive feature extraction/dimensionality reduction.\nReply to this email directly or view it on GitHub:\nhttps://github.com/harthur/brain/issues/52\n. yes\n. @harthur - If you would like to move this repo into an organization, I would be happy to maintain it.\n. I love that the community has started maintaining this project. One reason this project is so useful to people, is the simplicity of the code. There's no underlying BLAS library being called to do \"magic\" behind the scenes. In the spirt of this, I hope when these pull requests start being merged you keep that in mind, and document any new features added. Such as the \"likely\", which in my opinion should be an a separate util lib and not in the lib/neuralnetwork.js source (especially since convolutional neural networks are better suited for the example provided with that pull request).\n. @alexnix - Your input type and model are non-numeric values. You must normalize your data first. I suggest one-hot encoding them: https://github.com/nickpoorman/one-hot\n. @alexnix - Yes one-hot encoding solves the issue of having a feature(s) with \"categorical\" values. In your case for example, type will be expanded from a vertical column in the matrix, of \"Mazda\", \"Ford\", \"Volkswagen\", \"Renault\", \"Kia\", \"Hyundai\", etc... to horizontal, with a boolean flag of 1 if it is that type.\n\nFor example: Your first input is: { year: 2009, mileage: 311000, type: \"Mazda\", model: \"CX-7\" }\nSo it will become something like:\njavascript\n[2009, 311000, 1, 0, 0, 0, 0, 0, ...]\nWhere the header for the columns might be:\njavascript\n[year, mileage, type_mazda, type_ford, type_volkswagen, type_renault, type_kia, type_hyundai, ...]\nYou might also want to normalize your values between 0 and 1 for this library. Get the min and max value for each column and then scale them between 0 and 1. https://github.com/nickpoorman/scale-number-range\n. @Dok11, brain.js only allows for numeric values as inputs. One-Hot encoding allows you to transform Y-axis values into X-axis inputs with \"ON or OFF\" values.\nThis will naturally increase the dimensionality of the the inputs, so yes you will always end up with more columns. To reduce the number of columns, run your data set through dimensionality reduction via PCA, Lasso, or some other means.\nThis brain library is probably not what you want for highly dimensional data. Try using a library that can do matrix transforms quickly via BLAS or some other more efficient means.. @cawa-93 - I would have to take a look at the rest of your code - the setup of the network and how you are training the model. Another thing you should try is not using category mode. Simply supply your input vector as an array. Instead of this:\njs\n{\n  \"input\":{\n    \"albums\":0,\n    \"videos\":0.002345981232150143,\n    \"audios\":0,\n    \"notes\":0,\n    \"photos\":0.019921374619020275,\n    \"friends\":0.06461938581574472,\n    \"mutual_friends\":0,\n    \"followers\":0.004280263813796541,\n    \"subscriptions\":0,\n    \"pages\":0.0010093363613424174,\n    \"wall\":0.22041054577293512\n  },\n  \"output\":[0]\n}\ndo this:\njs\n{\n  \"input\":[\n    0,\n    0.002345981232150143,\n    0,\n    0,\n    0.019921374619020275,\n    0.06461938581574472,\n    0,\n    0.004280263813796541,\n    0,\n    0.0010093363613424174,\n    0.22041054577293512\n  ],\n  \"output\":[0]\n}\nI've been using this in production for three years (training millions of models and making billions of predictions monthly), I assure you there is nothing wrong with the library.. @cawa-93 - Two issues with your code. One you should filter out any user data that doesn't have the same shape. The following is going to cause issues. \njs\n{\n  \"id\": 305576398,\n  \"counters\": {\n    \"unknown\": true\n  }\n}\nTo do this use a filter:\n```js\nconst learnArray = users\n.filter(user => {\n  for (let key in maxRages) {\n    if (typeof user.counters[key] === 'undefined') {\n      return false\n    }\n  }\n  return true\n})\n.map(user => {\n  let result = {\n    input: {},\n    output: []\n  }\nfor (let c in user.counters) {\n    if (c !== 'messages' && c !== 'online_friends') {\n      result.input[c] = scale(user.counters[c], 0, maxRages[c], 0, 1)\n    }\n  }\nresult.output.push(user.counters.messages > 3 ? 1 : 0)\nreturn result\n})\n```\nAlso, you should scale to [0, 1] instead of [-1, -1].\nLastly, instead of using toFunction(), you should just use run to solve your NaN problem.\nI've updated some of the code in this gist: https://gist.github.com/nickpoorman/cd9465edca726df8dc06dbdd2937d153. It would seem to me that you have to train the entire dataset at once. Although I wonder what the result would be if you were to take any new data and train just the new data into a network, then use the new network and the old network as nodes in a hidden layer of another new network.\nBy feeding one network into another network is how you develop complex networks.\n. One thing I do when getting any new data is train another network again in the background, then when it finishes hot-swap it for the running one.\n. fixes #29 \n. Ok. There's a mocha test in the test/unit directory now.\n. The additions in the readme are bit confusing, so I should probably get rid of them all together. They should really look at the test if for a more accurate way of using this anyway.\nAs far as the network being a WriteStream, I think it makes more sense that you can pipe directly to the network and emit events on it. Look at the stuff dominictarr puts out. If you really want to have network.trainStream then you are going to have to do some crazy binding calls throughout the implementation which I personally think will add more complexity to it.\n. Ok, I pulled the stream stuff into its own TrainStream class that can manipulate the network. The tests and README were also updated.\n``` bash\nnick : brain $ mocha test/unit/*\n\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\n27 passing (866 ms)\n```\n. Changes should be easy to see now: \nhttps://github.com/harthur/brain/pull/30/files \n. Ok. I fixed the comments.\n. The point of null in the stream is to signal the end of an epoch. You must continuously flood the stream with the same data if you want the network to train properly. A write ahead log would defeat the purpose of using streams (which is to train using a data size greater than available memory, or disk).\nThe rest of what your describing sounds awfully implementation specific. Much like a b-tree may be used to implement a database, you could use brain to implement what you are describing.\n. +1 - I'm all for this. I suspect there will be a speedup as well.\n. I use this with hundreds of features. If you're doing 100 you might try setting the learning rate to something like 0.03 and the error threshold to say 0.0055.\nAs for dimension reduction if you have a lot of zeros and are doing something such a PCA, the PCA might be choking on the zeros. You might try something like lasso in that case.\nOn Sat, Dec 6, 2014 at 8:20 AM, Giles Lavelle notifications@github.com\nwrote:\n\nI'm trying to use this library for an image recognition assignment. We're getting pretty poor results, around 40% accuracy, compared to the high 90% range for all the other techniques we've tested. That's with all our preprocessing and training over 100s of iterations with a very low error threshold.\nI'm wondering what type of neural network this is, and whether there's anything fundamental in its implementation that makes it poorly suited to image recognition? Convolutional NNs are generally great at it, but I was thinking that this library might be a different type.\nDoes it generally perform well on high dimensional data? We're using all features with non-zero variance from our image set, which is in the order of 100 or so pixels/features. The colour contrast example given only has 3 dimensions, so I was thinking we might be able to get better performance with some aggressive feature extraction/dimensionality reduction.\nReply to this email directly or view it on GitHub:\nhttps://github.com/harthur/brain/issues/52\n. yes\n. @harthur - If you would like to move this repo into an organization, I would be happy to maintain it.\n. I love that the community has started maintaining this project. One reason this project is so useful to people, is the simplicity of the code. There's no underlying BLAS library being called to do \"magic\" behind the scenes. In the spirt of this, I hope when these pull requests start being merged you keep that in mind, and document any new features added. Such as the \"likely\", which in my opinion should be an a separate util lib and not in the lib/neuralnetwork.js source (especially since convolutional neural networks are better suited for the example provided with that pull request).\n. @alexnix - Your input type and model are non-numeric values. You must normalize your data first. I suggest one-hot encoding them: https://github.com/nickpoorman/one-hot\n. @alexnix - Yes one-hot encoding solves the issue of having a feature(s) with \"categorical\" values. In your case for example, type will be expanded from a vertical column in the matrix, of \"Mazda\", \"Ford\", \"Volkswagen\", \"Renault\", \"Kia\", \"Hyundai\", etc... to horizontal, with a boolean flag of 1 if it is that type.\n\nFor example: Your first input is: { year: 2009, mileage: 311000, type: \"Mazda\", model: \"CX-7\" }\nSo it will become something like:\njavascript\n[2009, 311000, 1, 0, 0, 0, 0, 0, ...]\nWhere the header for the columns might be:\njavascript\n[year, mileage, type_mazda, type_ford, type_volkswagen, type_renault, type_kia, type_hyundai, ...]\nYou might also want to normalize your values between 0 and 1 for this library. Get the min and max value for each column and then scale them between 0 and 1. https://github.com/nickpoorman/scale-number-range\n. @Dok11, brain.js only allows for numeric values as inputs. One-Hot encoding allows you to transform Y-axis values into X-axis inputs with \"ON or OFF\" values.\nThis will naturally increase the dimensionality of the the inputs, so yes you will always end up with more columns. To reduce the number of columns, run your data set through dimensionality reduction via PCA, Lasso, or some other means.\nThis brain library is probably not what you want for highly dimensional data. Try using a library that can do matrix transforms quickly via BLAS or some other more efficient means.. @cawa-93 - I would have to take a look at the rest of your code - the setup of the network and how you are training the model. Another thing you should try is not using category mode. Simply supply your input vector as an array. Instead of this:\njs\n{\n  \"input\":{\n    \"albums\":0,\n    \"videos\":0.002345981232150143,\n    \"audios\":0,\n    \"notes\":0,\n    \"photos\":0.019921374619020275,\n    \"friends\":0.06461938581574472,\n    \"mutual_friends\":0,\n    \"followers\":0.004280263813796541,\n    \"subscriptions\":0,\n    \"pages\":0.0010093363613424174,\n    \"wall\":0.22041054577293512\n  },\n  \"output\":[0]\n}\ndo this:\njs\n{\n  \"input\":[\n    0,\n    0.002345981232150143,\n    0,\n    0,\n    0.019921374619020275,\n    0.06461938581574472,\n    0,\n    0.004280263813796541,\n    0,\n    0.0010093363613424174,\n    0.22041054577293512\n  ],\n  \"output\":[0]\n}\nI've been using this in production for three years (training millions of models and making billions of predictions monthly), I assure you there is nothing wrong with the library.. @cawa-93 - Two issues with your code. One you should filter out any user data that doesn't have the same shape. The following is going to cause issues. \njs\n{\n  \"id\": 305576398,\n  \"counters\": {\n    \"unknown\": true\n  }\n}\nTo do this use a filter:\n```js\nconst learnArray = users\n.filter(user => {\n  for (let key in maxRages) {\n    if (typeof user.counters[key] === 'undefined') {\n      return false\n    }\n  }\n  return true\n})\n.map(user => {\n  let result = {\n    input: {},\n    output: []\n  }\nfor (let c in user.counters) {\n    if (c !== 'messages' && c !== 'online_friends') {\n      result.input[c] = scale(user.counters[c], 0, maxRages[c], 0, 1)\n    }\n  }\nresult.output.push(user.counters.messages > 3 ? 1 : 0)\nreturn result\n})\n```\nAlso, you should scale to [0, 1] instead of [-1, -1].\nLastly, instead of using toFunction(), you should just use run to solve your NaN problem.\nI've updated some of the code in this gist: https://gist.github.com/nickpoorman/cd9465edca726df8dc06dbdd2937d153. ",
    "aeosynth": ":+1: i would like to train continuously\n. :+1: i would like to train continuously\n. ",
    "JpEncausse": "+1\nMy little program learn when to switch on/off the light according to luminosity.\nI would also like to train continuously too\n- net.train() when user ask light on\n- net.run() when move detector ask light on\nDoes anybody have a best practices ? or a little work around function() ?\n. +1\nMy little program learn when to switch on/off the light according to luminosity.\nI would also like to train continuously too\n- net.train() when user ask light on\n- net.run() when move detector ask light on\nDoes anybody have a best practices ? or a little work around function() ?\n. ",
    "lafncow": "As I understand it, harthur is right.\nI've heard of starting with a small random sample of data and gradually adding more data to your model in some cases. However, you would want to do this additively, so you are still passing in all of the old data + some new data.\n. As I understand it, harthur is right.\nI've heard of starting with a small random sample of data and gradually adding more data to your model in some cases. However, you would want to do this additively, so you are still passing in all of the old data + some new data.\n. ",
    "erelsgl": "There are many training algorithms that are suitable for training continuously (the scientific name is \"online learning\"). \nThe most well-known is the perceptron algorithm, but there are also online versions of support-vector machines and more. See some papers here: http://www.citeulike.org/user/erelsegal-halevi/tag/online-learning\n. @PhilTeare you can find some online learners here: https://github.com/erelsgl/limdu \n. Thanks, I think it would be easier for new users and contributors to have this .gitignore bundled with the code, so that they don't have to add it themselves.\n. There are many training algorithms that are suitable for training continuously (the scientific name is \"online learning\"). \nThe most well-known is the perceptron algorithm, but there are also online versions of support-vector machines and more. See some papers here: http://www.citeulike.org/user/erelsegal-halevi/tag/online-learning\n. @PhilTeare you can find some online learners here: https://github.com/erelsgl/limdu \n. Thanks, I think it would be easier for new users and contributors to have this .gitignore bundled with the code, so that they don't have to add it themselves.\n. ",
    "PhilTeare": "I'm looking at an adaboost approach which would combine trained nets. Each net being something you could potentially train on new or different data. \nThoughts?\n. I'm looking at an adaboost approach which would combine trained nets. Each net being something you could potentially train on new or different data. \nThoughts?\n. ",
    "karpathy": "what @harthur said. It's not common to put adaboost on top of a neuron net. Instead, you train a larger neural net, or if you really want to you can use dropout in your neural net which can be interpreted as a form of ensemble learning (similar to adaboost). But more accurately just model averaging.\nIf you learn something and then get new data, it's not safe to simply train on the new data alone. Just because the net has \"learned\" something before doesn't mean it will retain it forever. In fact, it would gradually start to \"forget\" the old data as you feed the new data in. \nThat's why the correct thing to do is to add to your dataset and keep training with all the data; You're just expanding the dataset. Think of the neural net as needing to be reminded of your data all the time, otherwise it will just converge (even if sometimes very slowly, depending on the exact settings) around the most recent data it has seen. \n. what @harthur said. It's not common to put adaboost on top of a neuron net. Instead, you train a larger neural net, or if you really want to you can use dropout in your neural net which can be interpreted as a form of ensemble learning (similar to adaboost). But more accurately just model averaging.\nIf you learn something and then get new data, it's not safe to simply train on the new data alone. Just because the net has \"learned\" something before doesn't mean it will retain it forever. In fact, it would gradually start to \"forget\" the old data as you feed the new data in. \nThat's why the correct thing to do is to add to your dataset and keep training with all the data; You're just expanding the dataset. Think of the neural net as needing to be reminded of your data all the time, otherwise it will just converge (even if sometimes very slowly, depending on the exact settings) around the most recent data it has seen. \n. ",
    "evolu8": "Thanks all. @karpathy I intended to keep feeding the old data. @erelsgl Thank you! Excellent resource.\nSo any ideas on how this could be parallelised? \n(also is the a place for general discussions, such as this, or is this as good a place as any?)\n. Thanks all. @karpathy I intended to keep feeding the old data. @erelsgl Thank you! Excellent resource.\nSo any ideas on how this could be parallelised? \n(also is the a place for general discussions, such as this, or is this as good a place as any?)\n. ",
    "pvdz": "Haha, yes. And add a big note to select one of the two to continue ;)\n. Haha, yes. And add a big note to select one of the two to continue ;)\n. ",
    "ryansmith94": "Is this closed?\n. Is this closed?\n. ",
    "josher19": "Looks like the problem is that outputLookup is not getting set by fromJSON\n```\nJSON.stringify(net.outputLookup) !== JSON.stringify(net_clone.outputLookup);\nnull != net.outputLookup; // is a lookup Object\nnull == net_clone.outputLookup; // is undefined!\n```\nChecking for isArray or adding something like:\nelse if (i == this.outputLayer && layer[\"0\"] && layer[\"0\"].weights) {\n    this.outputLookup = lookup.lookupFromHash(layer);\n}\nto fromJSON might solve the problem (haven't tested it yet).\n. Solution above won't work if you are expecting the output to be an Array: \n```\nvar vet = new brain.NeuralNetwork();\nvet.train([{input: [0, 0], output: [0,0]},\n           {input: [0, 1], output: [0,1]},\n           {input: [1, 0], output: [1,0]},\n           {input: [1, 1], output: [1,1]}]);\nJSON.stringify(vet.run([1,1]))\n   ===\nJSON.stringify(brain.fromJSON(vet.toJSON()).run([1,1]))\n```\nAdding two new variables to toJSON would work:\nreturn { layers: layers, outputLookup:!!this.outputLookup, inputLookup:!!this.inputLookup };\nand then fix up fromJSON:\nif (i == 0 && (!layer[0] || json.inputLookup)) {\n    this.inputLookup = lookup.lookupFromHash(layer);\n  }\n  else if (i == this.outputLayer && (!layer[0] || json.outputLookup)) {\n    this.outputLookup = lookup.lookupFromHash(layer);\n  }\n. Fixed by #23 \n. Fixes #22\n. Looks like the problem is that outputLookup is not getting set by fromJSON\n```\nJSON.stringify(net.outputLookup) !== JSON.stringify(net_clone.outputLookup);\nnull != net.outputLookup; // is a lookup Object\nnull == net_clone.outputLookup; // is undefined!\n```\nChecking for isArray or adding something like:\nelse if (i == this.outputLayer && layer[\"0\"] && layer[\"0\"].weights) {\n    this.outputLookup = lookup.lookupFromHash(layer);\n}\nto fromJSON might solve the problem (haven't tested it yet).\n. Solution above won't work if you are expecting the output to be an Array: \n```\nvar vet = new brain.NeuralNetwork();\nvet.train([{input: [0, 0], output: [0,0]},\n           {input: [0, 1], output: [0,1]},\n           {input: [1, 0], output: [1,0]},\n           {input: [1, 1], output: [1,1]}]);\nJSON.stringify(vet.run([1,1]))\n   ===\nJSON.stringify(brain.fromJSON(vet.toJSON()).run([1,1]))\n```\nAdding two new variables to toJSON would work:\nreturn { layers: layers, outputLookup:!!this.outputLookup, inputLookup:!!this.inputLookup };\nand then fix up fromJSON:\nif (i == 0 && (!layer[0] || json.inputLookup)) {\n    this.inputLookup = lookup.lookupFromHash(layer);\n  }\n  else if (i == this.outputLayer && (!layer[0] || json.outputLookup)) {\n    this.outputLookup = lookup.lookupFromHash(layer);\n  }\n. Fixed by #23 \n. Fixes #22\n. ",
    "francescortiz": "it is a neuronal network and as such its only purpose is to make predictions. an example is the demo app that guesses which color is more readable given a random backgroind color. it is properly explained in the readme of this project. i believe that your question deserves a huge rtfm.\n. You can create your own .gitignore which ignores itself.\nCreate a .gitignore with this two records:\n.gitignore\nnode_modules\n. it is a neuronal network and as such its only purpose is to make predictions. an example is the demo app that guesses which color is more readable given a random backgroind color. it is properly explained in the readme of this project. i believe that your question deserves a huge rtfm.\n. You can create your own .gitignore which ignores itself.\nCreate a .gitignore with this two records:\n.gitignore\nnode_modules\n. ",
    "umerjamil16": "You can create your own .gitignore file and add the folders/file names you want to exclude from your git repositories in production.. You can create your own .gitignore file and add the folders/file names you want to exclude from your git repositories in production.. ",
    "timoxley": "+1. any updates on this?\n. +1. any updates on this?\n. ",
    "mikeumus": ":+1:  and watching. 8)\n. Woo hoo! \nThank you @harthur for the detailed merge.\nand thank you @nickpoorman for the pull-req! :+1:\n. :+1:  and watching. 8)\n. Woo hoo! \nThank you @harthur for the detailed merge.\nand thank you @nickpoorman for the pull-req! :+1:\n. ",
    "TeRq": "Waiting for it.\n. Jpiiii!\n. Waiting for it.\n. Jpiiii!\n. ",
    "nrox": "Your approach seams a bit strange. Can you specify a concrete example, why to you need to evolve neural networks with genetic algorithms? That is a case of what is referred to as meta-learning : when you need a network with optimal size to tackle specific problems repetitively, for example.\nI think you can use this library to do that, though, without any modifications.\nI started using brain.js recently and it looks very good. I am using this library for this test: https://assemblino.com/show/public20123372.html . Needs WebGL, and preferably Chrome.\n. I have some ideas for that, meanwhile check this out:\nhttp://nrox.github.io/q-learning.js/test2.html\nNN can be used in a similar way. Maybe NN are more suitable than Q-Learning if the number of states is too high.\n. Here you have it, brain.js applyed to something like a game agent trying to survive:\nhttp://nrox.github.io/brain-extension.js/test.html\nJust tweak it to achieve what you want. It's a short step.\n. Your approach seams a bit strange. Can you specify a concrete example, why to you need to evolve neural networks with genetic algorithms? That is a case of what is referred to as meta-learning : when you need a network with optimal size to tackle specific problems repetitively, for example.\nI think you can use this library to do that, though, without any modifications.\nI started using brain.js recently and it looks very good. I am using this library for this test: https://assemblino.com/show/public20123372.html . Needs WebGL, and preferably Chrome.\n. I have some ideas for that, meanwhile check this out:\nhttp://nrox.github.io/q-learning.js/test2.html\nNN can be used in a similar way. Maybe NN are more suitable than Q-Learning if the number of states is too high.\n. Here you have it, brain.js applyed to something like a game agent trying to survive:\nhttp://nrox.github.io/brain-extension.js/test.html\nJust tweak it to achieve what you want. It's a short step.\n. ",
    "farzher": "I was going to use that method for a little evolution simulation of creatures, that always move, and can only turn right and left.They'll have to eat food or whatever. The random neural network is how I'll get them smarter.\nI also want to use it for AI in a fighting game.\nAny suggestions? How are you using brain.js?\n. Awesome, it looks good, thanks.\n. I was going to use that method for a little evolution simulation of creatures, that always move, and can only turn right and left.They'll have to eat food or whatever. The random neural network is how I'll get them smarter.\nI also want to use it for AI in a fighting game.\nAny suggestions? How are you using brain.js?\n. Awesome, it looks good, thanks.\n. ",
    "twknab": "Thanks for a solid example of a .gitignore file! Found this file via another thread, thank you!. Thanks for a solid example of a .gitignore file! Found this file via another thread, thank you!. ",
    "miketheprogrammer": "I am not sure this is even relevant. Also I should have never said WAL, i meant just a simple buffer-stream. \"null\" is in fact a valid terminal character of streams, but i remember something about the streaming implementation was broken for me, though it was more than likely fixed.\nI think ill close this issue since i didnt exactly describe a problem and cases.. I am not sure this is even relevant. Also I should have never said WAL, i meant just a simple buffer-stream. \"null\" is in fact a valid terminal character of streams, but i remember something about the streaming implementation was broken for me, though it was more than likely fixed.\nI think ill close this issue since i didnt exactly describe a problem and cases.. ",
    "muattiyah": "@harthur No worries, don't feel bad or anything, it hardly took any time and I understand how you might have style preferences, thanks any ways for replying, cheers :)\n. @harthur No worries, don't feel bad or anything, it hardly took any time and I understand how you might have style preferences, thanks any ways for replying, cheers :)\n. ",
    "achalddave": "What does game.data.nndata look like? This works fine for me with the XOR example from the README.\n. What does game.data.nndata look like? This works fine for me with the XOR example from the README.\n. ",
    "kalyankdas": "Any idea, why the issue would be fixed, This is a big show stopper to use it for time consuming training.\n. Any idea, why the issue would be fixed, This is a big show stopper to use it for time consuming training.\n. ",
    "wbashir": "+1\n. +1\n. ",
    "No9": "http://nodejsreactions.tumblr.com/post/79664189233/when-the-frontend-guy-puts-a-bower-json-in-the-project\n. http://nodejsreactions.tumblr.com/post/79664189233/when-the-frontend-guy-puts-a-bower-json-in-the-project\n. ",
    "totty90": "+1\n. Seems that it loads lodash instead of underscore.\n. +1\n. Seems that it loads lodash instead of underscore.\n. ",
    "TheThirdOne": "Based on this table, I would say its implemented in all major browsers. (Sorry opera mini).\n. Based on this table, I would say its implemented in all major browsers. (Sorry opera mini).\n. ",
    "alexanderGugel": "Hey Fabian,\nwhat do you think of http://substack.net/task_automation_with_npm_run? #45 would solve this without any additional dependencies. Grunt isn't even necessary (kind of like the idea of how Backbone solves this).\n. Hey Fabian,\nwhat do you think of http://substack.net/task_automation_with_npm_run? #45 would solve this without any additional dependencies. Grunt isn't even necessary (kind of like the idea of how Backbone solves this).\n. ",
    "irony": "Please do a Pull Request? Thanks.\n. I also agree, it would be nice to get an response from @harthur\n. She have switched jobs to something secret so maybe she isn't even allowed to reply here: https://harthur.wordpress.com\n. Please do a Pull Request? Thanks.\n. I also agree, it would be nice to get an response from @harthur\n. She have switched jobs to something secret so maybe she isn't even allowed to reply here: https://harthur.wordpress.com\n. ",
    "kurttheviking": ":+1: \n. :+1: \n. ",
    "breadbaker": "Thats interesting you are right underscore is about a 3rd the size of\nlodash.\nThe main reason is compatibility.  The version of underscore currently in\nplace doesn't play well with other libraries.\nOn Fri, Nov 28, 2014 at 5:24 PM, Heather Arthur notifications@github.com\nwrote:\n\nThanks for this. It's my understanding that lodash is bigger in size than\nunderscore. There won't be much speedup as underscore is only used in\nperipheral code and not in any critical paths. Let me know if there's\nanother reason, however.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/harthur/brain/pull/50#issuecomment-64936149.\n. This is unfortunate as now I'd need to include two versions of underscore.\nBut whatever.\n\nOn Sat, Nov 29, 2014 at 11:24 AM, Heather Arthur notifications@github.com\nwrote:\n\nClosed #50 https://github.com/harthur/brain/pull/50.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/harthur/brain/pull/50#event-200170477.\n. Thats interesting you are right underscore is about a 3rd the size of\nlodash.\n\nThe main reason is compatibility.  The version of underscore currently in\nplace doesn't play well with other libraries.\nOn Fri, Nov 28, 2014 at 5:24 PM, Heather Arthur notifications@github.com\nwrote:\n\nThanks for this. It's my understanding that lodash is bigger in size than\nunderscore. There won't be much speedup as underscore is only used in\nperipheral code and not in any critical paths. Let me know if there's\nanother reason, however.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/harthur/brain/pull/50#issuecomment-64936149.\n. This is unfortunate as now I'd need to include two versions of underscore.\nBut whatever.\n\nOn Sat, Nov 29, 2014 at 11:24 AM, Heather Arthur notifications@github.com\nwrote:\n\nClosed #50 https://github.com/harthur/brain/pull/50.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/harthur/brain/pull/50#event-200170477.\n. \n",
    "lavelle": "cc @alishug\n. Fantastic, thank for the thorough response. I will definitely give that CNN a try, and have another go with brain.js and adjust the parameters as you recommend. \n. cc @alishug\n. Fantastic, thank for the thorough response. I will definitely give that CNN a try, and have another go with brain.js and adjust the parameters as you recommend. \n. ",
    "antoniodeluca": "Hi,\nI've submited two days ago a PR that should resolve this problem. \nThe submitted PR includes code to manage a new training option called \"inizialization\" that can be set to true or false. I thought that is very important to have this feature. In my case I always needed to mantain weights without loosing them on reiterated train calls.\nHope to seed the change integrated into the project.\nCompliment to Harthur and all Contributors for this beautiful and simple library.\n. @ClimbsRocks , yes it does exactly what you need. Hope it will help you.\n. @marcj, your code is not working because you are not passing the \"initialization\" parameter.\nbrainjs normally reinitializes the weights each time you call the train function (because the initialization parameter value defaults to \"true\").\nIf you want to preserve the weights for the next training sessions you should pass a \"false\" initialization parameter.\nThis is not needed on the first training session because it is supposed that the first time you train the neural network the weights have to be initialized.\nYou could see an example of the code looking into the commit test case.\nFor brevity here are two little example snippets:\nFirst training session\nnet.train(data, {\n    errorThresh: 0.2,\n    iterations: 100000\n});\nSecond training session\nnet.train(data, {\n    errorThresh: 0.2,\n    iterations: 1,\n    initialization: false\n})\n. @marcj, I do not think that the initialize function should be removed.\nIt is very good to have an initialization function that properly represents a point in which the system prepares the data structures and does eventual needed tasks.\n. @marcj, the second training is done with one iterarion just as an example.\nSorry if the example was very minimalist.\nThe intention was just to show the use of the initialization parameter.\nIn your real scenario you have to do as many iterations as you need.\n. Yes, sure. I do it today or tomorrow.. In the past I have used Brain.js and some other JavaScript based machine learning libraries but unfortunately I have found them not matching my needs. That's why, working on my personal projects, I have developed the idea of creating a different approach by myself. So, sorry to be here to talk about another project but I would really like to receive some opinions and suggestions from experienced people that, being into this specific field, could help me to set useful and shared expectations. By the way, thanks for the great support given to Brain.js so far. The library is called DN2A and is on https://github.com/dn2a/dn2a-javascript\n. Hi,\nI've submited two days ago a PR that should resolve this problem. \nThe submitted PR includes code to manage a new training option called \"inizialization\" that can be set to true or false. I thought that is very important to have this feature. In my case I always needed to mantain weights without loosing them on reiterated train calls.\nHope to seed the change integrated into the project.\nCompliment to Harthur and all Contributors for this beautiful and simple library.\n. @ClimbsRocks , yes it does exactly what you need. Hope it will help you.\n. @marcj, your code is not working because you are not passing the \"initialization\" parameter.\nbrainjs normally reinitializes the weights each time you call the train function (because the initialization parameter value defaults to \"true\").\nIf you want to preserve the weights for the next training sessions you should pass a \"false\" initialization parameter.\nThis is not needed on the first training session because it is supposed that the first time you train the neural network the weights have to be initialized.\nYou could see an example of the code looking into the commit test case.\nFor brevity here are two little example snippets:\nFirst training session\nnet.train(data, {\n    errorThresh: 0.2,\n    iterations: 100000\n});\nSecond training session\nnet.train(data, {\n    errorThresh: 0.2,\n    iterations: 1,\n    initialization: false\n})\n. @marcj, I do not think that the initialize function should be removed.\nIt is very good to have an initialization function that properly represents a point in which the system prepares the data structures and does eventual needed tasks.\n. @marcj, the second training is done with one iterarion just as an example.\nSorry if the example was very minimalist.\nThe intention was just to show the use of the initialization parameter.\nIn your real scenario you have to do as many iterations as you need.\n. Yes, sure. I do it today or tomorrow.. In the past I have used Brain.js and some other JavaScript based machine learning libraries but unfortunately I have found them not matching my needs. That's why, working on my personal projects, I have developed the idea of creating a different approach by myself. So, sorry to be here to talk about another project but I would really like to receive some opinions and suggestions from experienced people that, being into this specific field, could help me to set useful and shared expectations. By the way, thanks for the great support given to Brain.js so far. The library is called DN2A and is on https://github.com/dn2a/dn2a-javascript\n. ",
    "shamoons": "Love it! Thanks @antoniodeluca !\n. Love it! Thanks @antoniodeluca !\n. ",
    "ClimbsRocks": "It is- thanks!. @antoniodeluca , if this does what I think it does, you're my hero! \nSay I've been training a net for 2 days, and it's 84 iterations in by this point (turns out understanding human attraction is rather complex, even for a computer). I'd love to be able to train that same net even more at some point in the future, to take advantage of the two days worth of work that has already gone into training it. \nIf your PR does that, I'd be super happy! I'd been debating how complex this would be to implement myself; I'm glad I bothered checking out the PRs here to find you've already figured it out, and see that the tests for this are longer than the source code!\n. It already is! I created GridSearch for this library, and then at the end, I wanted to train the best classifier for significantly longer. Allowing the classifier to warm start at the point it had previously trained to has been awesome! \nIt's part of a larger project, but I hope to spin gridSearch out into it's own repo pretty soon to make harthur's awesome brainjs even more accessible to developers. I love her philosophy of abstracting away complexity, which pretty directly contrasts with scikit-learn's approach. I'm actually trying to build out my project to take that philosophy of avoiding unnecessary complexity to an even higher level. \nThanks again for this critical component of it!\n. It is- thanks!. @antoniodeluca , if this does what I think it does, you're my hero! \nSay I've been training a net for 2 days, and it's 84 iterations in by this point (turns out understanding human attraction is rather complex, even for a computer). I'd love to be able to train that same net even more at some point in the future, to take advantage of the two days worth of work that has already gone into training it. \nIf your PR does that, I'd be super happy! I'd been debating how complex this would be to implement myself; I'm glad I bothered checking out the PRs here to find you've already figured it out, and see that the tests for this are longer than the source code!\n. It already is! I created GridSearch for this library, and then at the end, I wanted to train the best classifier for significantly longer. Allowing the classifier to warm start at the point it had previously trained to has been awesome! \nIt's part of a larger project, but I hope to spin gridSearch out into it's own repo pretty soon to make harthur's awesome brainjs even more accessible to developers. I love her philosophy of abstracting away complexity, which pretty directly contrasts with scikit-learn's approach. I'm actually trying to build out my project to take that philosophy of avoiding unnecessary complexity to an even higher level. \nThanks again for this critical component of it!\n. ",
    "johnmarkos": "After installing cairo on a Mac with brew install, I was able to get npm install to work with brain.js by setting\nbash\nexport PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:/opt/X11/lib/pkgconfig\nas described here.\n. After installing cairo on a Mac with brew install, I was able to get npm install to work with brain.js by setting\nbash\nexport PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:/opt/X11/lib/pkgconfig\nas described here.\n. ",
    "MatthewClair": "Cairo seems to only be used for the cross validation test.  You don't need cairo outside running the tests.  If you run npm install --production it will skip the dev dependencies.\n. Cairo seems to only be used for the cross validation test.  You don't need cairo outside running the tests.  If you run npm install --production it will skip the dev dependencies.\n. ",
    "Zenohate": "Thanks a lot, I'll need this too and I prefer this as using streams or anything. gotta be useful in my IA development. You made my day.\n. Thanks a lot, I'll need this too and I prefer this as using streams or anything. gotta be useful in my IA development. You made my day.\n. ",
    "marcj": "For me this pull-request does not work. Here a simple example:\n``` javascript\nnet.train([{input: [1, 1], output: [0]}]); //trains 1,1 => 0\nnet.runInput([1, 1]); //results 0.0702838678761908 => correct\nnet.train([{input: [1, 0], output: [1]}]); //trains 1,0 => 1\nnet.runInput([1, 1]); //results 0.850795812504698 => incorrect\nnet.runInput([1, 0]); //results 0.929557447931592 => correct\n```\nI've simply commented out this this.initialize() call, so it never gets initialized.\nTo me its seems a lot of iterations to learn the new 1,0 => 1 overwrite simply the learned 1,1=>0 as the result of the learning, not through resetting the weights.\n. Well, as I said\n\nI've simply commented out this this.initialize() call, so it never gets initialized.\n\nWhat I mean with that is that I deleted the line this.initialize(sizes); manually, so the network.js does not reset anything when I call train().\nso, there's no need to pass the \"initialization\" parameter. :) I tested with this your approach of being able to continue the learning process by skipping the reset/initialize method. \n. Is there a reason why you only use one iteration for the second training? I guess with such low iterations the network has no chance to learn enough to get the errors low. Also with one iteration there's no need to pass any errorThreshold because the network has no chance to react with one iteration to a high error rate.\n. @antoniodeluca, well I called this.initialize() on my own in my userland code of course, so the network is initialized after is has been created. \nCan you confirm this behavior of this code?\n``` javascript\nnet.train([{input: [1, 1], output: [0]}]); //trains 1,1 => 0\nnet.runInput([1, 1]); //results 0.0702838678761908 => correct\nnet.train([{input: [1, 0], output: [1]}], {initialization: false}); //trains 1,0 => 1\nnet.runInput([1, 1]); //results 0.850795812504698 => incorrect\nnet.runInput([1, 0]); //results 0.929557447931592 => correct\n```\n. For me this pull-request does not work. Here a simple example:\n``` javascript\nnet.train([{input: [1, 1], output: [0]}]); //trains 1,1 => 0\nnet.runInput([1, 1]); //results 0.0702838678761908 => correct\nnet.train([{input: [1, 0], output: [1]}]); //trains 1,0 => 1\nnet.runInput([1, 1]); //results 0.850795812504698 => incorrect\nnet.runInput([1, 0]); //results 0.929557447931592 => correct\n```\nI've simply commented out this this.initialize() call, so it never gets initialized.\nTo me its seems a lot of iterations to learn the new 1,0 => 1 overwrite simply the learned 1,1=>0 as the result of the learning, not through resetting the weights.\n. Well, as I said\n\nI've simply commented out this this.initialize() call, so it never gets initialized.\n\nWhat I mean with that is that I deleted the line this.initialize(sizes); manually, so the network.js does not reset anything when I call train().\nso, there's no need to pass the \"initialization\" parameter. :) I tested with this your approach of being able to continue the learning process by skipping the reset/initialize method. \n. Is there a reason why you only use one iteration for the second training? I guess with such low iterations the network has no chance to learn enough to get the errors low. Also with one iteration there's no need to pass any errorThreshold because the network has no chance to react with one iteration to a high error rate.\n. @antoniodeluca, well I called this.initialize() on my own in my userland code of course, so the network is initialized after is has been created. \nCan you confirm this behavior of this code?\n``` javascript\nnet.train([{input: [1, 1], output: [0]}]); //trains 1,1 => 0\nnet.runInput([1, 1]); //results 0.0702838678761908 => correct\nnet.train([{input: [1, 0], output: [1]}], {initialization: false}); //trains 1,0 => 1\nnet.runInput([1, 1]); //results 0.850795812504698 => incorrect\nnet.runInput([1, 0]); //results 0.929557447931592 => correct\n```\n. ",
    "mindphlux1": "Hi, I'm trying to use your patch with loading a neural network from a file first, however it fails with the error \nrain/lib/neuralnetwork.js:180\n        this.errors[layer][node] = error;\n                   ^\nTypeError: Cannot read property '3' of undefined\nThe way I am doing is to load the file with fromJSON and then running train() with the 'initialization' parameter set to false, but it doesn't seem to work.\nApparently some default values are set in the init method that is disabled.\nWhat can I do? \nThanks\n. Hi, I'm trying to use your patch with loading a neural network from a file first, however it fails with the error \nrain/lib/neuralnetwork.js:180\n        this.errors[layer][node] = error;\n                   ^\nTypeError: Cannot read property '3' of undefined\nThe way I am doing is to load the file with fromJSON and then running train() with the 'initialization' parameter set to false, but it doesn't seem to work.\nApparently some default values are set in the init method that is disabled.\nWhat can I do? \nThanks\n. ",
    "Dok11": "\nSometime back I added a keepNetworkIntact argument\n\nI installed 'npm install brain.js' and these not have argument keepNetworkIntact. What?. @robertleeplummerjr thank you, that's work, and arg 'keepNetworkIntact' too ok!. @nickpoorman, can I ask you?\nYou wrote:\n\nWhere the header for the columns might be:\n[year, mileage, type_mazda, type_ford, type_volkswagen, type_renault, type_kia, type_hyundai, ...]\n\nIs it mean what topicstarter must set and \"model\" by your example? Like this:\n{model__maxda_cx_7: 1, model__bmw_x5: 0, model__nissan_xtrail: 0,...}\nIt's so many columns.. that's normal?\n. @robertleeplummerjr, that's cool, but not for this task, right?\np.s. ye, I do very similar nn, and this ask very interest for me ;). pps. Where I can see more examples?\nhttps://github.com/harthur-org/brain.js/wiki is empty.... > @nickpoorman: This will naturally increase the dimensionality of the the inputs, so yes you will always end up with more columns. To reduce the number of columns, run your data set through dimensionality reduction via PCA, Lasso, or some other means.\nWhat about set dictinary as:\n['cx-7', 'x5', 'x-trail', ...]\nand use keys in input array like:\n{... type: 0, ...}\nWill this right work?. > Sometime back I added a keepNetworkIntact argument\nI installed 'npm install brain.js' and these not have argument keepNetworkIntact. What?. @robertleeplummerjr thank you, that's work, and arg 'keepNetworkIntact' too ok!. @nickpoorman, can I ask you?\nYou wrote:\n\nWhere the header for the columns might be:\n[year, mileage, type_mazda, type_ford, type_volkswagen, type_renault, type_kia, type_hyundai, ...]\n\nIs it mean what topicstarter must set and \"model\" by your example? Like this:\n{model__maxda_cx_7: 1, model__bmw_x5: 0, model__nissan_xtrail: 0,...}\nIt's so many columns.. that's normal?\n. @robertleeplummerjr, that's cool, but not for this task, right?\np.s. ye, I do very similar nn, and this ask very interest for me ;). pps. Where I can see more examples?\nhttps://github.com/harthur-org/brain.js/wiki is empty.... > @nickpoorman: This will naturally increase the dimensionality of the the inputs, so yes you will always end up with more columns. To reduce the number of columns, run your data set through dimensionality reduction via PCA, Lasso, or some other means.\nWhat about set dictinary as:\n['cx-7', 'x5', 'x-trail', ...]\nand use keys in input array like:\n{... type: 0, ...}\nWill this right work?. ",
    "kkirsche": "I would. Any reason not to do both?. I would. Any reason not to do both?. ",
    "ch-hristov": ":+1: \n. :+1: \n. ",
    "DinerIsmail": "I'm actually working on something like that at the moment, and it's using brain.js. Here it is, in case you want to have a look. It's still under development, so there's still plenty of work to do and features to add: https://neuroplot.herokuapp.com/\n. Nice, good job! Let me know when you deploy it online, I'd be curious to have a look :)\n. Any update on this, guys? It would be awesome if this would be moved into an organization.\n. I also agree with @voxpelli \n. I'm actually working on something like that at the moment, and it's using brain.js. Here it is, in case you want to have a look. It's still under development, so there's still plenty of work to do and features to add: https://neuroplot.herokuapp.com/\n. Nice, good job! Let me know when you deploy it online, I'd be curious to have a look :)\n. Any update on this, guys? It would be awesome if this would be moved into an organization.\n. I also agree with @voxpelli \n. ",
    "dasaki-gr": "Very good !!!\nNice.\nI try something similar first with brain.js but finally with ConvNetJS. I'm not so good to present my result. I played with Snap.svg with ConvNetJS and i made the newrons draggables but when i realize the meaning of angular 2 and typescript i stoped every effort and i start learning them and try to convert ConvNetJS to angular 2 modules and components. When i will make something good looking i will present it to all.\nBelow is my first effort but it was with many errors.\n\n. Very good !!!\nNice.\nI try something similar first with brain.js but finally with ConvNetJS. I'm not so good to present my result. I played with Snap.svg with ConvNetJS and i made the newrons draggables but when i realize the meaning of angular 2 and typescript i stoped every effort and i start learning them and try to convert ConvNetJS to angular 2 modules and components. When i will make something good looking i will present it to all.\nBelow is my first effort but it was with many errors.\n\n. ",
    "IonicaBizau": "@nickpoorman I was going to suggest that. :smile: :+1: \n. :disappointed: \n\n\n. @FranzSkuffka I am not for forking and maintaining. It's much harder to maintain your own fork because most of the people will find this popular repository.\n\nI also vote for moving these projects in an organization, like @nickpoorman suggested. :dizzy: \n. @irony Oh, interesting read! :book: \n. I just emailed @harthur! Hope she will reply me and save these nice goodies. :grin: :gift: :pray: \n. No response from the author yet... :disappointed: \n. @michaelpittino I contacted GitHub support\u2013maybe they will be able to help us since @harthur had a talk at Passion Projects. Hopefully they will have a solution. :pray: \n. From GitHub support:\n\nUnfortunately we don't have any contact details for Heather Arthur from her Passion Projects appearance. Hopefully she'll reply to your email if she's able to help.\n. @robertleeplummerjr That's cool! :+1: \n\nI would still recommend having real source repositories, not forks (that means: delete forks, create repos, git push --all from local clones of @harthur's repos). I do not like forks because they have limitations like:\n- they are not searchable\n- commits in forks are not counted in the contributions calendar (big thing, right? :joy:)\nInstead we can explicitly state it's the repos are a continuation of the original projects.\n. @robertleeplummerjr Note we can always make pull requests between source repositories (not only between forks)\u2014I still think we should convert them in source repositories, not forking. :grin: \n. > Note we can always make pull requests between source repositories\nWell, actually that's wrong: I mean, we can synchronize the source repositories anyways: when the original owner is back and wants to maintain the source repo, we fork it, synchronize the fork (git pull ...) and then creating a PR on GitHub. :grin: \n. @robertleeplummerjr :cool: We now can start merging these PRs from this very repository in the @harthur-org account (by git pulling). :sparkles: \n. @robertleeplummerjr Thanks! :+1: Happy to help in my spare time. Not sure, but I may use this module in one of projects. :smile: \n. @robertleeplummerjr Thanks! I'm not evil. :four_leaf_clover: \n. :joy: \n. @nickpoorman I agree! :+1: Maybe for such things, open issues in the new repository. It's easier to track. :smile:\n. @FranzSkuffka I always had good experiences with the guys from npm support\u2014they transferred quite few packages on my account (mostly they were unmaintained/obsolete/bad quality/etc packages), but always they did a great job. I'd say we can give a try to ask them. Then we will get some traffic from the npm package. :joy: \n. @FranzSkuffka Check this out. :joy: \nPleeeease, without the f word!\n. >  ask at npmjs.org for permission to be added as package maintainers\nI contacted npm support and they did talk to @harthur. She wants to keep the npm packages in this state, so, we need to apply plan b): to publish the packages under different names. :disappointed: \n/cc @FranzSkuffka\n. My ideas are:\n- brain.js\n- brain2\n:grin: \n. @SoullessWaffle I don't think + can be used in the package names.\n. I npm published it under the brain.js name on npm. :rocket: \nIf any of you wants to have superpowers on npm, feel free to :bell: me and I will npm owner add you. :grin:  Also, if you want to contribute to the project we can add you as member in the @harthur-org organization.\n/cc https://github.com/harthur-org/brain.js/issues/4\n. @1UnboundedSentience Please check out the updated repo, since this repository is not yet maintained. I fixed this in https://github.com/harthur-org/brain/commit/f5467966e2408d8467bd354c2dc066b939c66e21.\n. @nickpoorman I was going to suggest that. :smile: :+1: \n. :disappointed: \n\n\n. @FranzSkuffka I am not for forking and maintaining. It's much harder to maintain your own fork because most of the people will find this popular repository.\n\nI also vote for moving these projects in an organization, like @nickpoorman suggested. :dizzy: \n. @irony Oh, interesting read! :book: \n. I just emailed @harthur! Hope she will reply me and save these nice goodies. :grin: :gift: :pray: \n. No response from the author yet... :disappointed: \n. @michaelpittino I contacted GitHub support\u2013maybe they will be able to help us since @harthur had a talk at Passion Projects. Hopefully they will have a solution. :pray: \n. From GitHub support:\n\nUnfortunately we don't have any contact details for Heather Arthur from her Passion Projects appearance. Hopefully she'll reply to your email if she's able to help.\n. @robertleeplummerjr That's cool! :+1: \n\nI would still recommend having real source repositories, not forks (that means: delete forks, create repos, git push --all from local clones of @harthur's repos). I do not like forks because they have limitations like:\n- they are not searchable\n- commits in forks are not counted in the contributions calendar (big thing, right? :joy:)\nInstead we can explicitly state it's the repos are a continuation of the original projects.\n. @robertleeplummerjr Note we can always make pull requests between source repositories (not only between forks)\u2014I still think we should convert them in source repositories, not forking. :grin: \n. > Note we can always make pull requests between source repositories\nWell, actually that's wrong: I mean, we can synchronize the source repositories anyways: when the original owner is back and wants to maintain the source repo, we fork it, synchronize the fork (git pull ...) and then creating a PR on GitHub. :grin: \n. @robertleeplummerjr :cool: We now can start merging these PRs from this very repository in the @harthur-org account (by git pulling). :sparkles: \n. @robertleeplummerjr Thanks! :+1: Happy to help in my spare time. Not sure, but I may use this module in one of projects. :smile: \n. @robertleeplummerjr Thanks! I'm not evil. :four_leaf_clover: \n. :joy: \n. @nickpoorman I agree! :+1: Maybe for such things, open issues in the new repository. It's easier to track. :smile:\n. @FranzSkuffka I always had good experiences with the guys from npm support\u2014they transferred quite few packages on my account (mostly they were unmaintained/obsolete/bad quality/etc packages), but always they did a great job. I'd say we can give a try to ask them. Then we will get some traffic from the npm package. :joy: \n. @FranzSkuffka Check this out. :joy: \nPleeeease, without the f word!\n. >  ask at npmjs.org for permission to be added as package maintainers\nI contacted npm support and they did talk to @harthur. She wants to keep the npm packages in this state, so, we need to apply plan b): to publish the packages under different names. :disappointed: \n/cc @FranzSkuffka\n. My ideas are:\n- brain.js\n- brain2\n:grin: \n. @SoullessWaffle I don't think + can be used in the package names.\n. I npm published it under the brain.js name on npm. :rocket: \nIf any of you wants to have superpowers on npm, feel free to :bell: me and I will npm owner add you. :grin:  Also, if you want to contribute to the project we can add you as member in the @harthur-org organization.\n/cc https://github.com/harthur-org/brain.js/issues/4\n. @1UnboundedSentience Please check out the updated repo, since this repository is not yet maintained. I fixed this in https://github.com/harthur-org/brain/commit/f5467966e2408d8467bd354c2dc066b939c66e21.\n. ",
    "FranzSkuffka": "That would be great. I am using this software in a project thesis because it is just the simplest solution out here :)\n. how about forking the repos and creating an organisation for maintenance during her absence?\n. +1, basically what I suggested already.\n. I agree with @IonicaBizau, that makes perfect sense.\n. Obviously harthur is the maintainer of the npm packages aswell. I have had experience with the npm removing coffeescript (which was not coffee-script) upon my request, because it was a confusing, unnecessarily publicated package.\nWe could either a.) ask at npmjs.org for permission to be added as package maintainers or b.) publish the packages with a prefix like harthur-org-brain.\n. @robertleeplummerjr I agree with you on establishing a consensus-based policy. However I would extend the pull-request cycle by two(?) days for example after the :+1:. During these two days anyone can throw in a veto, setting the earliest moment of the merge at one(?) day from the veto when a valid argument was given.\n. @IonicaBizau agreed, would be great to get some traffic from there!\nbtw, WTH did you do to your commit graph :laughing: \n. harthur-brain could be a good choice SEO-wise\n. Thank you @IonicaBizau \n. It's unmaintained. Please go to https://github.com/harthur-org/brain.js.. That would be great. I am using this software in a project thesis because it is just the simplest solution out here :)\n. how about forking the repos and creating an organisation for maintenance during her absence?\n. +1, basically what I suggested already.\n. I agree with @IonicaBizau, that makes perfect sense.\n. Obviously harthur is the maintainer of the npm packages aswell. I have had experience with the npm removing coffeescript (which was not coffee-script) upon my request, because it was a confusing, unnecessarily publicated package.\nWe could either a.) ask at npmjs.org for permission to be added as package maintainers or b.) publish the packages with a prefix like harthur-org-brain.\n. @robertleeplummerjr I agree with you on establishing a consensus-based policy. However I would extend the pull-request cycle by two(?) days for example after the :+1:. During these two days anyone can throw in a veto, setting the earliest moment of the merge at one(?) day from the veto when a valid argument was given.\n. @IonicaBizau agreed, would be great to get some traffic from there!\nbtw, WTH did you do to your commit graph :laughing: \n. harthur-brain could be a good choice SEO-wise\n. Thank you @IonicaBizau \n. It's unmaintained. Please go to https://github.com/harthur-org/brain.js.. ",
    "ilanbiala": "@harthur any response?\n. @harthur any response?\n. ",
    "jobsamuel": "I like that idea @nickpoorman :+1: \n. I like that idea @nickpoorman :+1: \n. ",
    "pablodenadai": "Thanks @IonicaBizau let us know how it goes.\n. There's an opportunity here for someone to take this over. Preferably someone who understands the codebase and is willing to move the project forward. That said, the project owner seems to have been the only major contributor in the past.\n. Fair point - I agree that moving this project to an org would be the best solution - it's a shame the owner isn't responding the messages. :disappointed: \n. Let's start a petition at change.org. :-)\n. Ok. Let's think of a good prefix for the package names.\nSample:\ncontrib-brain\ncommunity-brain\ncom-brain\n...\n. Cool - I didn't know there was a naming convention for it.\nWhere is the easiest place to start an online poll?\n. Thanks @SoullessWaffle.\nSee poll here.\nPs. It's okay if you guys decide to add more options to the poll though.\n. #### Results as of Apr 8, 2016, 10:43 AM GMT+1\n1. brain.js 10 votes (45%)\n2. js-brain 4 votes (18%)\n3. brain-plus 2 votes (9%)\n4. brain2 2 votes (9%)\n5. contrib-brain 1 vote (5%)\n6. community-brain 1 vote (5%)\n7. org-brain 1 vote (5%)\n8. brain-ng 1 vote (5%)\n9. com-brain 0 votes (0%)\n. The poll has been open for just over a week now.\nUpdated results:\n1. brain.js 13 votes (46%)\n2. js-brain 4 votes (14%)\n3. brain2 4 votes (14%)\n4. community-brain 2 votes (7%)\n5. brain-plus 2 votes (7%)\n6. contrib-brain 1 vote (4%)\n7. org-brain 1 vote (4%)\n8. brain-ng 1 vote (4%)\n9. com-brain 0 votes (0%)\n28 total votes\n@IonicaBizau I guess you could call brain.js the winner?\n. Thanks @IonicaBizau let us know how it goes.\n. There's an opportunity here for someone to take this over. Preferably someone who understands the codebase and is willing to move the project forward. That said, the project owner seems to have been the only major contributor in the past.\n. Fair point - I agree that moving this project to an org would be the best solution - it's a shame the owner isn't responding the messages. :disappointed: \n. Let's start a petition at change.org. :-)\n. Ok. Let's think of a good prefix for the package names.\nSample:\ncontrib-brain\ncommunity-brain\ncom-brain\n...\n. Cool - I didn't know there was a naming convention for it.\nWhere is the easiest place to start an online poll?\n. Thanks @SoullessWaffle.\nSee poll here.\nPs. It's okay if you guys decide to add more options to the poll though.\n. #### Results as of Apr 8, 2016, 10:43 AM GMT+1\n1. brain.js 10 votes (45%)\n2. js-brain 4 votes (18%)\n3. brain-plus 2 votes (9%)\n4. brain2 2 votes (9%)\n5. contrib-brain 1 vote (5%)\n6. community-brain 1 vote (5%)\n7. org-brain 1 vote (5%)\n8. brain-ng 1 vote (5%)\n9. com-brain 0 votes (0%)\n. The poll has been open for just over a week now.\nUpdated results:\n1. brain.js 13 votes (46%)\n2. js-brain 4 votes (14%)\n3. brain2 4 votes (14%)\n4. community-brain 2 votes (7%)\n5. brain-plus 2 votes (7%)\n6. contrib-brain 1 vote (4%)\n7. org-brain 1 vote (4%)\n8. brain-ng 1 vote (4%)\n9. com-brain 0 votes (0%)\n28 total votes\n@IonicaBizau I guess you could call brain.js the winner?\n. ",
    "scrussell24": "I would also like this!\n. I would also like this!\n. ",
    "michaelpittino": "Any update on this?\n. Yes but as @IonicaBizau said that's not easy to fork and maintain because of popularity of THIS repository.\n. Any update on this?\n. Yes but as @IonicaBizau said that's not easy to fork and maintain because of popularity of THIS repository.\n. ",
    "voxpelli": "Why not just create a fork, move development there, keep an eye on this repo and point people to the fork whenever they open issues or PR:s here and then if/when the owner of this repo gets back to you then this repo can be updated to point to that repo.\n. Why not just create a fork, move development there, keep an eye on this repo and point people to the fork whenever they open issues or PR:s here and then if/when the owner of this repo gets back to you then this repo can be updated to point to that repo.\n. ",
    "silentrob": "Ill throw out:\njs-brain\norg-brain\nI do like community-brain though :)\n. Ill throw out:\njs-brain\norg-brain\nI do like community-brain though :)\n. ",
    "Macil": "Total bike-shedding here, but I feel obligated to mention a few common fork naming conventions:\nbrain-ng\nbrain-plus\n(I'm not a big fan of contrib-brain because \"contrib\" is tainted in my mind by the multiple projects I've seen that have a \"contrib\" directory full of useful but unsupported and buggy 3rd-party patches that never got properly merged in.)\n. The author has discontinued their work on this project. Development of the codebase has been continued by the community at https://github.com/harthur-org/brain.js/.. Total bike-shedding here, but I feel obligated to mention a few common fork naming conventions:\nbrain-ng\nbrain-plus\n(I'm not a big fan of contrib-brain because \"contrib\" is tainted in my mind by the multiple projects I've seen that have a \"contrib\" directory full of useful but unsupported and buggy 3rd-party patches that never got properly merged in.)\n. The author has discontinued their work on this project. Development of the codebase has been continued by the community at https://github.com/harthur-org/brain.js/.. ",
    "mickdekkers": "@ghpabs You could try http://strawpoll.me/\n. Here's two more ideas:\nbrain-next\nbrain++ (as in c++; brain incremented by one)\n. @IonicaBizau you're right, perhaps we could use brainpp\n. @ghpabs You could try http://strawpoll.me/\n. Here's two more ideas:\nbrain-next\nbrain++ (as in c++; brain incremented by one)\n. @IonicaBizau you're right, perhaps we could use brainpp\n. ",
    "hadesara": "Can we decide on the date by which everyone should vote.\n. Can we decide on the date by which everyone should vote.\n. ",
    "roccomuso": "Great job guys. ;)\n. Great job guys. ;)\n. ",
    "alexnix": "One-hot encoding for type and model? \nI just learnt from YouTube Video that input values must be in [-1,1]. I am still wondering how to represent model and type, I could just label them with numbers like Audi 0.1, Opel 0.2 and so on but this will make the network find Audi and Opel similar because their labels are close to each other. Is there a god way to represent inputs with discrete, not correlated values? (such as car model, in my example)\nDisclaimer: noob in AI/ML/NN here.\n. Thank you for your advice, it was very useful indeed.\n. One-hot encoding for type and model? \nI just learnt from YouTube Video that input values must be in [-1,1]. I am still wondering how to represent model and type, I could just label them with numbers like Audi 0.1, Opel 0.2 and so on but this will make the network find Audi and Opel similar because their labels are close to each other. Is there a god way to represent inputs with discrete, not correlated values? (such as car model, in my example)\nDisclaimer: noob in AI/ML/NN here.\n. Thank you for your advice, it was very useful indeed.\n. ",
    "cawa-93": "I have the same problem, but all of the input signals are already normalize:\njavascript\nconst neural = require('../NeuralNetwork').toFunction() // In this directory are stored neural network and an array of learning\nneural({\n  albums: 0.011111111111111112,\n  videos: 0.016523867809057527,\n  audios: 0,\n  notes: 0,\n  photos: 0.00035337249878528203,\n  friends: 0.009302790837251175,\n  mutual_friends: 0,\n  followers: 0.007113002799187086,\n  subscriptions: 0,\n  pages: 0.0063083522583901085,\n  wall: 0.0005448000778285826\n}) // { '0': NaN }\nAn example of learning sample:\njson\n{\n  \"input\":{\n    \"albums\":0,\n    \"videos\":0.002345981232150143,\n    \"audios\":0,\n    \"notes\":0,\n    \"photos\":0.019921374619020275,\n    \"friends\":0.06461938581574472,\n    \"mutual_friends\":0,\n    \"followers\":0.004280263813796541,\n    \"subscriptions\":0,\n    \"pages\":0.0010093363613424174,\n    \"wall\":0.22041054577293512\n  },\n  \"output\":[0]\n}\nWhen training, I use only one an element of the training sample - then the network will take you back a numerical result. But if you use at least 2 Elements - the result is not a number\nFull train array in .json\nAll data were normalized using scale-number-range. @nickpoorman I create simple repository for you cawa-93/user-scaner\nI noticed if the train network objects, the numerical data stored in net.json, however, if the I train arrays, all values = Null. I have the same problem, but all of the input signals are already normalize:\njavascript\nconst neural = require('../NeuralNetwork').toFunction() // In this directory are stored neural network and an array of learning\nneural({\n  albums: 0.011111111111111112,\n  videos: 0.016523867809057527,\n  audios: 0,\n  notes: 0,\n  photos: 0.00035337249878528203,\n  friends: 0.009302790837251175,\n  mutual_friends: 0,\n  followers: 0.007113002799187086,\n  subscriptions: 0,\n  pages: 0.0063083522583901085,\n  wall: 0.0005448000778285826\n}) // { '0': NaN }\nAn example of learning sample:\njson\n{\n  \"input\":{\n    \"albums\":0,\n    \"videos\":0.002345981232150143,\n    \"audios\":0,\n    \"notes\":0,\n    \"photos\":0.019921374619020275,\n    \"friends\":0.06461938581574472,\n    \"mutual_friends\":0,\n    \"followers\":0.004280263813796541,\n    \"subscriptions\":0,\n    \"pages\":0.0010093363613424174,\n    \"wall\":0.22041054577293512\n  },\n  \"output\":[0]\n}\nWhen training, I use only one an element of the training sample - then the network will take you back a numerical result. But if you use at least 2 Elements - the result is not a number\nFull train array in .json\nAll data were normalized using scale-number-range. @nickpoorman I create simple repository for you cawa-93/user-scaner\nI noticed if the train network objects, the numerical data stored in net.json, however, if the I train arrays, all values = Null. ",
    "ewascent": "Just clone it\nOn Mon, Feb 13, 2017 at 11:50 AM, Jo\u00e3o Gabriel Lima \nnotifications@github.com wrote:\n\nI`ll do like to know: can I reopen this repository?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/harthur/brain/issues/78, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABZlDpu_6zhADHiAFPptiQYJyunrs2Mdks5rcLQHgaJpZM4L_pK2\n.\n\n\n-- \nEric Aldinger\n. Just clone it\nOn Mon, Feb 13, 2017 at 11:50 AM, Jo\u00e3o Gabriel Lima \nnotifications@github.com wrote:\n\nI`ll do like to know: can I reopen this repository?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/harthur/brain/issues/78, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABZlDpu_6zhADHiAFPptiQYJyunrs2Mdks5rcLQHgaJpZM4L_pK2\n.\n\n\n-- \nEric Aldinger\n. "
}
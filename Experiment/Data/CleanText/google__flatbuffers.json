{
    "gwvo": "Good point. I will put a check for that in place shortly.\n. Besides the comment above, the two are not that similar. FlatBuffers combines the flexibility of Protobufs with the efficiency of Cap'n Proto, in a sense a best of both worlds approach. Which is better depends on your use case.\n. FlatBuffers, like Protobuf, has the ability to leave out arbitrary fields from a table (better yet, it will automatically leave them out if they happen to have the default value). Experience with ProtoBuf has shown me that as data evolves, cases where lots of field get added/removed, or where a lot of fields are essentially empty are extremely common, making this a vital feature for efficiency, and for allowing people to add fields without instantly bloating things.\nCap'n'Proto doesn't have this functionality, and instead just writes zeroes for these fields, taking up space. They recommend using an additional zero-compression on top of the resulting buffer, but this kills the ability to read data without first unpacking it, which is a big selling point of both FlatBuffers and Cap'n'Proto. So Cap'n'Proto is either not truly \"zero-parse\", or it is rather inefficient/inflexible, take your pick.\n. If you're use case is small, simple, non-evolving objects, you're likely not going to notice much difference between the two. In that case you're better of picking which you prefer based on API, features, platform/compiler/language support, or whatever is important to you.\n. Thanks for reporting. This was a bug in Doxygen. New documentation has been uploaded and verified to work on Android.\n. += does an append, which is similar in performance to appending to a std::vector, and the fastest way to do string concatenation as far as I know (certainly better than ostringstream).\nWhy would it be copying the entire string? It typically only reallocates every power of two in size.\n. FlatBuffers is a very performance-conscious libary. That said, I don't think these reallocs will cause performance problems, even when generating large amounts of JSON. You could probably estimate an ideal buffer size given the size of the input FlatBuffer you're generating from.. this is something I could add, but I don't think it will make much of a difference.\nIf you want to output to cout, first generating the whole string would still be faster than calling cout for every substring.\n. You're right, it would be useful for that. Though I would hope that if you program for a memory constrained system, you don't use the text representation at all, and work with binary only.\n. We'd like bindings for as many languages as possible, and C would make sense to have. That said, since FlatBuffers derives its performance from being a super-thin inline-able layer, supporting a new language isn't a case of making \"bindings\" for flatbuffers.h, it is more like re-implementing flatbuffers.h (and providing a idl_gen_lang.cpp similar to idl_gen_cpp.cpp), which is a fair bit of work to do properly.\n. Noone is working on this as far as I know, and I'd be happy to take a PR if what you're making conforms to the standards of what's already there.\nWhat I am wondering about is flatbuffers.h, since it has tons of C++ top to bottom, adding C functionality sound to me like a LOT of ifdefs. Wouldn't a separate header for C make more sense? \nFeel free to point me to the code you have so far, so I can maybe comment on it.\n. yynil: if you want to help, check with diffuse above if there's something you can do.\n. Good news! :)\n. Ok, good to know. Just note on this issue if you do work on it again.\n. Good point, I will.\n. It really depends what the unsigned item represents how you want to deal\nwith it.\nYou can convert a byte to a short using: (short)(mybyte & 0xff)\nQuestion is, should the generated API always do that for you, or should you\ndo that only when necessary. I gravitate to the latter.\nOn Sat, Jun 21, 2014 at 5:17 AM, bml13 notifications@github.com wrote:\n\nOr may be you could use higher precision for unsigned types, for example\nfor ubyte in buffer use short in java and for ushort in buffer use int in\njava.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/9#issuecomment-46752207.\n. You're right, that makes more sense.\n\nBut uint as.. long? and ulong still as long?\n. Notice that failed test is trying to load a file. If you run flattests.exe from the root of the flatbuffers distribution (the parent of tests), all should work. I am not sure how under mingw the exe ended up in tests?\n. You can either put the sample binaries in the root of the FlatBuffers distribution, or you can make sure you run with the root as current directory, and then run the binaries from where ever they ended up, e.g. build\\somewhere\\flattests.exe\n. Embarrassing :) Fixed just now.\n. Thanks :)\n. I had to undo this merge, as for some reason it also renames the file into java178/flatbuffers179/FlatBufferBuilder.java. Why?\n. This \"dialect\" seems to be widely supported, and enhances readability (and compactness). Maybe I should make it an option?\n. I'll make it so.\n. fixed.\n. Good point, will fix.\n. Fixed, now supply -1 as indent_step to also remove all linefeeds.\n. I'm not sure what you're proposing to push here. The comment above refers to filetypes we don't use, and the actual commits appear to be just VS project setting changes?\n. Good find.. fixed just now.\n. Yes, simply place it ahead of startMonster and pass its return value to addName.\n. Sometimes you have to choose between the two :)\n. Thanks.. never tried -W4, will address these.\nOn Fri, Jun 27, 2014 at 7:03 AM, jffmarker notifications@github.com wrote:\n\nflatbuffers.h yields several compile warnings under VS2013 Update 2 with\nwarning level 4, both when used to compile flatc, as well as use a\ngenerated header file. Ignoring them when building flatc is less of an\nissue, but it'd be nice if including a generated header (which then\nincludes flatbuffers.h) didn't force us to back off or ignore warnings.\nFor example, when just including a generated header (haven't actually used\ncode from it yet):\n...flatbuffers.h(410) : error C2220: warning treated as error - no\n'object' file generated\n...flatbuffers.h(410) : warning C4244: 'argument' : conversion from\n'flatbuffers::uoffset_t' to 'flatbuffers::voffset_t', possible loss of data\n...flatbuffers.h(419) : warning C4244: 'argument' : conversion from\n'flatbuffers::uoffset_t' to 'flatbuffers::voffset_t', possible loss of data\nAnd when compiling flatc (also some come from CPP files):\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(410) : error C2220:\nwarning treated as error - no 'object' file generated\n(...flatbuffers\\src\\idl_gen_text.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(410) : warning C4244:\n'argument' : conversion from 'flatbuffers::uoffset_t' to\n'flatbuffers::voffset_t', possible loss of data\n(...flatbuffers\\src\\idl_gen_text.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(410) : error C2220:\nwarning treated as error - no 'object' file generated\n(...flatbuffers\\src\\flatc.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(410) : warning C4244:\n'argument' : conversion from 'flatbuffers::uoffset_t' to\n'flatbuffers::voffset_t', possible loss of data\n(...flatbuffers\\src\\flatc.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(419) : warning C4244:\n'argument' : conversion from 'flatbuffers::uoffset_t' to\n'flatbuffers::voffset_t', possible loss of data\n(...flatbuffers\\src\\idl_gen_text.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(419) : warning C4244:\n'argument' : conversion from 'flatbuffers::uoffset_t' to\n'flatbuffers::voffset_t', possible loss of data\n(...flatbuffers\\src\\flatc.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(410) : error C2220:\nwarning treated as error - no 'object' file generated\n(...flatbuffers\\src\\idl_gen_cpp.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(410) : warning C4244:\n'argument' : conversion from 'flatbuffers::uoffset_t' to\n'flatbuffers::voffset_t', possible loss of data\n(...flatbuffers\\src\\idl_gen_cpp.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(640) : warning C4100:\n'other' : unreferenced formal parameter\n(...flatbuffers\\src\\idl_gen_text.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(419) : warning C4244:\n'argument' : conversion from 'flatbuffers::uoffset_t' to\n'flatbuffers::voffset_t', possible loss of data\n(...flatbuffers\\src\\idl_gen_cpp.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(640) : warning C4100:\n'other' : unreferenced formal parameter (...flatbuffers\\src\\flatc.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(652) : warning C4245:\n'return' : conversion from 'int' to 'size_t', signed/unsigned mismatch\n(...flatbuffers\\src\\idl_gen_text.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(652) : warning C4245:\n'return' : conversion from 'int' to 'size_t', signed/unsigned mismatch\n(...flatbuffers\\src\\flatc.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(640) : warning C4100:\n'other' : unreferenced formal parameter (...flatbuffers\\src\\idl_gen_cpp.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(652) : warning C4245:\n'return' : conversion from 'int' to 'size_t', signed/unsigned mismatch\n(...flatbuffers\\src\\idl_gen_cpp.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(410) : error C2220:\nwarning treated as error - no 'object' file generated\n(...flatbuffers\\src\\idl_gen_java.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(410) : warning C4244:\n'argument' : conversion from 'flatbuffers::uoffset_t' to\n'flatbuffers::voffset_t', possible loss of data\n(...flatbuffers\\src\\idl_gen_java.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(419) : warning C4244:\n'argument' : conversion from 'flatbuffers::uoffset_t' to\n'flatbuffers::voffset_t', possible loss of data\n(...flatbuffers\\src\\idl_gen_java.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(640) : warning C4100:\n'other' : unreferenced formal parameter\n(...flatbuffers\\src\\idl_gen_java.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(410) : error C2220:\nwarning treated as error - no 'object' file generated\n(...flatbuffers\\src\\idl_parser.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(652) : warning C4245:\n'return' : conversion from 'int' to 'size_t', signed/unsigned mismatch\n(...flatbuffers\\src\\idl_gen_java.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(410) : warning C4244:\n'argument' : conversion from 'flatbuffers::uoffset_t' to\n'flatbuffers::voffset_t', possible loss of data\n(...flatbuffers\\src\\idl_parser.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(419) : warning C4244:\n'argument' : conversion from 'flatbuffers::uoffset_t' to\n'flatbuffers::voffset_t', possible loss of data\n(...flatbuffers\\src\\idl_parser.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(640) : warning C4100:\n'other' : unreferenced formal parameter (...flatbuffers\\src\\idl_parser.cpp)\n...flatbuffers\\include\\flatbuffers/flatbuffers.h(652) : warning C4245:\n'return' : conversion from 'int' to 'size_t', signed/unsigned mismatch\n(...flatbuffers\\src\\idl_parser.cpp)\n...flatbuffers\\src\\idl_gen_text.cpp(157) : warning C4244: 'argument' :\nconversion from 'const int' to 'flatbuffers::voffset_t', possible loss of\ndata\n...flatbuffers\\src\\idl_gen_text.cpp(158) : warning C4244: 'argument' :\nconversion from 'const int' to 'flatbuffers::voffset_t', possible loss of\ndata\n...flatbuffers\\src\\idl_gen_text.cpp(176) : warning C4244: 'argument' :\nconversion from 'int' to 'flatbuffers::voffset_t', possible loss of data\n...flatbuffers\\src\\idl_gen_text.cpp(204) : warning C4244: 'argument' :\nconversion from 'int' to 'flatbuffers::voffset_t', possible loss of data\n...flatbuffers\\src\\idl_parser.cpp(105) : warning C4244: 'argument' :\nconversion from 'int' to 'char', possible loss of data\n...flatbuffers\\src\\idl_gen_java.cpp(81) : warning C4244: 'argument' :\nconversion from 'int' to 'char', possible loss of data\n...flatbuffers\\src\\idl_gen_java.cpp(82) : warning C4244: 'argument' :\nconversion from 'int' to 'char', possible loss of data\n...flatbuffers\\src\\idl_parser.cpp(371) : warning C4244: 'argument' :\nconversion from 'const int' to 'flatbuffers::voffset_t', possible loss of\ndata\n...flatbuffers\\src\\idl_parser.cpp(417) : warning C4244: 'argument' :\nconversion from 'int' to 'flatbuffers::voffset_t', possible loss of data\n...flatbuffers\\src\\idl_parser.cpp(429) : warning C4244: 'argument' :\nconversion from 'int' to 'flatbuffers::voffset_t', possible loss of data\n...flatbuffers\\src\\idl_gen_java.cpp(376) : warning C4100: 'file_name' :\nunreferenced formal parameter\n...flatbuffers\\src\\idl_gen_text.cpp(141) : warning C4244: 'argument' :\nconversion from 'const int' to 'flatbuffers::voffset_t', possible loss of\ndata\n...flatbuffers\\src\\idl_gen_text.cpp(191) : see reference to function\ntemplate instantiation 'void flatbuffers::GenField(const\nflatbuffers::FieldDef &,const flatbuffers::Table ,bool,int,int,std::string\n)' being compiled\n...flatbuffers\\include\\flatbuffers/util.h(35) : warning C4127: conditional\nexpression is constant (...flatbuffers\\src\\idl_gen_cpp.cpp)\n...flatbuffers\\src\\idl_gen_cpp.cpp(96) : see reference to function\ntemplate instantiation 'std::string flatbuffers::NumToString(T)' being\ncompiled\nwith\n[\nT=int\n]\n...flatbuffers\\include\\flatbuffers/util.h(35) : warning C4127: conditional\nexpression is constant (...flatbuffers\\src\\idl_gen_java.cpp)\n...flatbuffers\\src\\idl_gen_java.cpp(106) : see reference to function\ntemplate instantiation 'std::string flatbuffers::NumToString(T)' being\ncompiled\nwith\n[\nT=int\n]\n...flatbuffers\\src\\idl_gen_text.cpp(37) : warning C4100: 'indent_step' :\nunreferenced formal parameter\n...flatbuffers\\src\\idl_gen_text.cpp(60) : see reference to function\ntemplate instantiation 'void\nflatbuffers::Print(T,flatbuffers::Type,int,int,flatbuffers::StructDef\n,std::string )' being compiled\nwith\n[\nT=unsigned char\n]\n...flatbuffers\\src\\idl_gen_text.cpp(126) : see reference to function\ntemplate instantiation 'void flatbuffers::PrintVector(const\nflatbuffers::Vector &,flatbuffers::Type,int,int,std::string )' being\ncompiled\n...flatbuffers\\src\\idl_gen_text.cpp(37) : warning C4100: 'indent' :\nunreferenced formal parameter\n...flatbuffers\\src\\idl_gen_text.cpp(37) : warning C4100: 'type' :\nunreferenced formal parameter\n...flatbuffers\\include\\flatbuffers/util.h(35) : warning C4127: conditional\nexpression is constant (...flatbuffers\\src\\idl_parser.cpp)\n...flatbuffers\\src\\idl_parser.cpp(48) : see reference to function template\ninstantiation 'std::string flatbuffers::NumToString(T)' being compiled\nwith\n[\nT=size_t\n]\n...flatbuffers\\include\\flatbuffers/util.h(35) : warning C4127: conditional\nexpression is constant (...flatbuffers\\src\\idl_gen_text.cpp)\n...flatbuffers\\src\\idl_gen_text.cpp(40) : see reference to function\ntemplate instantiation 'std::string flatbuffers::NumToString(T)' being\ncompiled\nwith\n[\nT=unsigned char\n]\n...flatbuffers\\src\\idl_gen_text.cpp(60) : see reference to function\ntemplate instantiation 'void\nflatbuffers::Print(T,flatbuffers::Type,int,int,flatbuffers::StructDef\n,std::string *)' being compiled\nwith\n[\nT=unsigned char\n]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/20.\n. Just fixed.\n. Thanks, will fix.\n. Just fixed.\n. The reason the current Vector class has a Get() function (as opposed to giving you a raw access to the value) is that it may do endianness swapping under the hood, deals with offsets and structs differently, and it is currently also read only.\n\nYour implementation should either be based on calling Get() internally, or replicate the functionality I mention somehow.\nNot sure about the more advanced use of C++11. We currently want to ensure compatibility with the relatively minimal C++11 in VS2010, and I'm worried it might choke on some of this. We'd need to test that.\n. Thanks, I'll pull this change. Do you mind squashing these two commits, and undoing the extra spacing you added for  < const T * > ?\n. cherry-picked this commit, thanks!\n. The way that iterator is currently implemented, you'd probably have to\nwrite:\nprintf(\"%i\\n\", (*member)->member());\nI'll see if that can be improved.\nOn Wed, Jul 16, 2014 at 5:09 AM, Jeff notifications@github.com wrote:\n\nHow can I use this functionality? Given this schema:\nstruct MyStruct {\nmember:uint;\n}\ntable MyTable {\nmembers:[MyStruct];\n}\nI expected to be able to do something like this:\nauto fb = GetRoot(NULL);\nfor (auto member : *fb->members())\n    printf(\"%i\\n\", member->member());\nBut MSVC 12 Update 2 gives this error:\n.../flatbuffers/flatbuffers.h(191) : error C2528: 'abstract declarator' :\npointer to reference is illegal\n...\\Test.cpp(381) : see reference to class template instantiation\n'flatbuffers::VectorIterator' being compiled\nwith\n[\nT=const MyStruct *\n]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/pull/22#issuecomment-49156136.\n. Ahh thanks. I'll push out an update shortly that fixes this error, and also\ndoesn't require the (*..)\n\nOn Thu, Jul 17, 2014 at 4:35 AM, Jeff notifications@github.com wrote:\n\nThe error is actually on the for loop, for (auto member : *fb->members())\nThe same error is generated if I expand that to for (auto iter =\nfb->members()->begin(); iter != fb->members()->end(); iter++) with a\nblank loop body.\nThis seems to be particular to Vectors of structs (vs. Vectors of PODs).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/pull/22#issuecomment-49295598.\n. Sorry, already made these changes locally, thanks for reporting them though!\n. Ouch. Will fix soon.\n. fixed.\n. Oops. Thanks for reporting, will fix.\n. Just fixed.\n. (discussed this with rw offline):\n\nWhile I am as big a hater of duplicated code as anyone, and while there's potentially a LOT of overlap between the Java version and C# / Go / whatever, the reason that the current C++ & Java generators are completely independent code is to make it easier to design the generated code to be tailor-made to the features of a particular language. We could have a more general generator interface that calls things like GenerateFieldAccessor etc, but then you also risk that this abstraction gets complicated because of all the exceptions different languages may need.\nHaving them all separate also facilitates independent development more.\nI think it is easier for now to continue down this path of having completely independent generators, then once they've matured a bit, look at commonalities and decide to abstract out functionality later.\nOpinions?\n. We may revisit this later.\n. Currently there is no Lua binding, so the only way you could use it is with C++ (since every Lua implementation is hosted in a C/C++ engine), and the values being passed on to Lua manually.\nWe cannot make any promises on what languages will be supported next, though community contributions can help here.\n. @DavidFeng : cool. If you can get it up to the standard of the other implementations, you could consider contributing it to the main project in a PR?\n. From http://google.github.io/flatbuffers/md__internals.html:\n\"On purpose, the format leaves a lot of details about where exactly things live in memory undefined, e.g. fields in a table can have any order, and objects to some extend can be stored in many orders. This is because the format doesn't need this information to be efficient, and it leaves room for optimization and extension (for example, fields can be packed in a way that is most compact). Instead, the format is defined in terms of offsets and adjacency only.\"\nThe C++ .bin file is generated by the JSON parser, which packs the table fields in a different order. Different order can also cause different alignment padding which can even make the size differ by a small amount.\nI agree this can be a bit of a surprise, though it is important that implementers of readers/writers understand this issue. I should probably explicitly mention below that paragraph that this can cause different binaries that are all compatible with one another. Any other suggestions welcome.\n. They are intended to be negative numbers, and I use hex for clarity. If this trips up compiler warnings somewhere, I'll use the decimal version.\n. Why? flatbuffers.h isn't a system header, quotes would seem the correct way?\n. Thanks for reporting, will fix.\n. fixed.\n. Fixed.\n. You're right, this is on my todo. Will get this in.\n. fixed\n. Go support is in! Please test it out and give feedback.\n. gedw99: let us know if you find any issues!\n. Nice work! Could you do me a favor and:\n1) if _MSC_VER, #define __builtin_bswap16 as _byteswap_ushort etc, instead of copy-pasting that entire block of code.\n2) squash these 3 commits into 1.\n. cherry-picked this commit, thanks!\n. This commit is in thru a cherry-pick.\n. How was this resolved? What platform?\n. fixed in recent commit\n. all of these are in..(cherry-picked, not merged). made a few changes in the cmake file (and renamed it), hope you don't mind :) Thanks for the contribution.\n. This commit is in thru a cherry-pick.\n. Thanks, will fix.\n. Just looked, but this was already in the current code. I can write field names with quotes. Anything in particular that's not working?\n. Ok, will add.\n. fixed in recent commit\n. regarding relative paths, what is the problem? There's commits on July 2 &\n21 related to path handling, are you up to date?\nAs for your call stack, I can't tell why it is crashing, the line it is\npointing at should not crash with the given type.\nIf you are using the unmodified monster.fbs in samples, I can run:\n./flatc -j samples/monster.fbs\nwithout crashing. Same if I first copy the fbs file to the parent dir. Tested on Linux & OS X.\n. Good find, and nice patch.. will add.\n. Good find, and nice patch.. will add.\n. This is in master through a cherry-pick, thanks!\n. If you look in the schema file, you'll see that the field at offset 12 in the vtable is a deprecated field. It is present in the vtable, but not in the object. If you go change these offsets, you'll break forwards/backwards compatability (see the \"internals\" document).\n. Ok, so your:\nint offset = Event.CreateDataVector(fbb, data);\nwould do the same as:        \nEvent.startDataVector(fbb, data.length);\nfor (byte i = data.length - 1; i >=0; i--) fbb.addByte(data[i]);\nint offset = fbb.endVector();\nI agree that's a welcome addition. However I think if we go that route, we should support it for all types of vectors where feasible, at a minimum all scalar types.\nAnother question is, wether arrays are sufficient, i.e. Vector may be common as well.\n. asLongBuffer() does not take any offset/size arguments, so seems it can only refer to the entire FlatBuffer. I see no way to do this without copying/allocating a new buffer.\n. fixed\n. Thanks.. fix forthcoming.\n. Fixed.\n. I agree, STL compatability was a bit of an afterthought. I suppose I could size at least as an alias.\n. fixed just now.\n. Oops. Will fix.\n. For the moment, know there's also a pre-made VS project in the build folder.\n. Fixed just now.\n. The first error is because 1 is by default unsigned, so instead writing 1u may also fix it.\nThe second errors are caused by me forgetting to regenerate monster_generated.h. Please run flatc -c monster.fbs in samples/.\nWill fix these both on Monday, thanks for noticing.\n. Ahh thanks.. already made a commit though that fixes this.\n. Thanks! already fixed elsewhere.\n. You'll have to give a more concrete example of how what you're trying to serialize does not fit with the schema structure, and what kind of code we could generate to fix that.\nI would think that most tree-shaped FlatBuffers are constructed from internal data that is similarly tree-shaped, so a pre-order traversal of it would allow it to be serialized without any additional bookkeeping.\nDo note that the FlatBuffers API has been designed for maximum efficiency first. The goals is to construct buffers with zero additional copying and allocation. It's not hard to make a friendlier API, for example, Protocol Buffers API gives you more flexibility in the order of serialization. But it also constructs an entire tree of objects.\n. On Mon, Aug 25, 2014 at 2:21 PM, greenrobot notifications@github.com\nwrote:\n\nIdeally, you could specify the root object and flatbuffers would do the\nrest.\nHow would this work? Would FlatBuffers generate object definitions for you?\nOr would it magically map any supplied objects to the schema?\nBut to be more concrete...\nTable A has String B. So we need to create B, store its offset, start A\nand add the string offset to A. This is always the same pattern and it\nshould be easy to automate this step.\nIn the case of a table with strings and scalars, I could generate a\nCreateA() function that takes all those elements, and hides the\nCreateString step. This is definitely on the todo list.\n\nBut in the general case, i.e. a table A that owns a B that owns a C this is\nnot possible, unless you put B and C in temporary objects (incurring\nallocation and copying).\n\nStoring vectors of tables are worse, because you need to maintain a list\nof offsets before you can add it the vector to another table. So, we need\nto allocate a list of offsets somewhere anyway.\nThis is indeed clumsy. But there's no cheap way to pass all that object\ndata to a function in such a way that we can abstract the vector\nconstruction in its entirety. What there should be though, is functions\nthat take an array, so that at least you don't have to write your own loop.\n. Agreed, though rather than just comparing it against C++, it be great to compare against Protocol Buffers for Java, and other similar solution.\n\nThis would be a great thing for an external contributor to try out (hint hint :)\n. I'll get you those files.\n. There's been no progress made on improving the benchmarks. The problem with releasing them is that a) they only work on Windows (Windows timing functions and projects, shouldn't be too hard to fix) and b) they depend on a bunch of external projects (protobuf etc), for which proper dependencies (submodules) and build dependencies need to be set up, preferably in a cross platform way (CMake).\nIf someone wants to to work on cleaning this mess up, I'd be happy to give them the code.\nFor Java, there's already something here: https://github.com/ennerf/flatbuffers-java-benchmark\n. Ok, I'll make an effort to get the benchmark code out there. Probably initially with the warts mentioned above, on a separate branch in the FlatBuffers repo, and then we can start cleaning it up from there (and add languages).\n. Ok, I'll make an effort to get the benchmark code out there. Probably initially with the warts mentioned above, on a separate branch in the FlatBuffers repo, and then we can start cleaning it up from there (and add languages).\n. The C++ benchmark code is now in the repo (in its own branch, see comment at the end of https://google.github.io/flatbuffers/md__benchmarks.html\nAnyone interested in integrating benchmarks for other languages, these could go in the same branch.\n. You don't need to even have it all in memory, using mmap you could very easily load only the parts that are touched. It can in theory even be streamed, but we'd need some code support for that.\nAs for indices, I agree that be awesome. It is already currently possible, i.e. you could construct a list of objects, then serialize also multiple vectors of offsets to these objects with different sorting orders. Alternatively, you can construct things like hashtables that are serializable (probably open addressing would be more space/time efficient than chaining in this case). I already had it on my idea list to be writing helper code for such things.\n. Thanks for the PR. I think what you've done is useful functionality, but to add it to master, I think I would prefer it to be done in a different way.\nIt seems your main reason for pulling in BOOST is std::conditional. Now there is only one use of that in the code base, so if supporting C++03 is a thing, I'd rather do that by rewriting that code to not rely on std::conditional at all, so we don't have an (optional) BOOST dependency.\nIt would seem to me that whatever compiler you're working with is not strictly C++03, since FlatBuffers makes liberal use of \"auto\", which is not a problem?\nAlso, for future PRs, it is much easier for me to work with if its a single commit rather than 12. It also gives a more readable history on master.\n. If you can look into this, that be great. If not, I will get to it eventually.\n. Thanks for reporting! Fixed this independently, using static_cast\n. You could even save indices inside the FlatBuffer.\nThe problem with use as a database is that it's not mutable, so currently to add a new object, you'd have to create a new FlatBuffer.\n. JavaScript support is indeed very desirable, we just haven't gotten to it yet. If someone wants to contribute code for it, that be helpful too.\n. If anyone tries this, I'd love to hear their experience :)\n. That's good to hear! thanks for testing :)\n. The JS implementation has been merged, please try it out. Currently lacking documentation however.\n. It may take me quite a while to get to, especially since I am not a JS expert :)\n@evanw is the original author.\nPR's welcome.\n. Cool, thanks!\n. Agreed, sounds like a good feature.\n. why space=this.bb.position() ? FlatBuffers writes to ByteBuffers in an absolute way, and does not rely on position() being anywhere in particular. space represents the amount of space left over at the beginning of the buffer, so should be set to the size of the backing array at the start.\n. Just added such a constructor as part of the last commit.\n. the clear() call only resets the writable area, it doesn't actually clear\nthe bytes. But yes, to create a FlatBuffer, you want to be using the entire\nByteBuffer, concatenating isn't supported.\n. ok, will add that.\n. 1. I already did that.\n2. The reason I didn't use put and get is that there are no absolute versions of those calls available, meaning you need to explicitly set the position.. though I suppose that is no big deal.\n3. Agreed.\n. That's indeed a bug, will create a proper fix.\n. just fixed.\n. I agree, this would be nice to have, and a logical fit for structs. It's a bit of a bigger feature though, since it lacks forwards compatibility on the schema side.\n. @falconair some of that is already above. To be more specific:\nA new type in the schema language. Before we had field : [float] as a way to declare arbitrary size vectors (allowed in tables only), now we'd have field : [float:4] to declare a fixed array (allowed inside structs only).\nOn a binary level, this represents exactly the same data as if you had written fieldN : float 4 times. It does not have a runtime size field, though it may have a generated code function/constant/enum to indicate the fixed size.\nRather than reusing the BASE_TYPE_VECTOR for this, I'd introduce a new BASE_TYPE_ARRAY in the C++ code. Then comes the fun work of ensuring this is taken into account in all places that deal with schema types, which is a lot.\n. I think a new fixed_length inside Type is ok. If you make it a short and place it after the element field, the Type object won't even grow in size :)\nshort is ok, because a struct is meant to be used inside a table, meaning it needs to be (way) smaller than 16bit anyway for the vtable offsets to fit.\n. I see that would be useful. The current lexical analyzer is greatly simplified by the fact that it can assume it is going to find a 0 byte before any memory it should not touch, having to check for end of buffer is going to require a lot of extra checks thruout the code.\nSo while I see this could be useful, I am not sure if I am a fan of it.\nFor your current use case, can't you temporarily set the end byte to 0, and set it back after you called Parse()?\n. It should be safe if the pointer you have is not const, the byte your zeroing is within the buffer, and you don't have other threads accessing that same buffer\n. Any table can be a root in a buffer. The root type is mostly for use by JSON, which doesn't specify any types, so we need to know where to start. The generated GetRoot functions are merely convenience.\n. 1) a commit was pushed yesterday that adds GetRootAs functions for all tables, not just the root_type.\n2) generally no. this is a strongly types system, meaning you need to know the kind of buffer you're dealing with. If you want to use this in a context where you want to have multiple different root types, you have these options:\na) make your root type a table that contains a union of all possible sub-roots.\nb) prefix flatbuffers with your own file header\nc) use flatbuffer's built-in file indentification feature, which hasn't been ported to Java yet. I'll get to that.\n3) That's a bug, the 1 should actually read: Any.Monster. I'll fix that.\n. getRootAs is a bit more descriptive and less likely to clash with other identifiers. I think I'll keep it for now.\nTo further my response to your issue #2, the built-in file_identifier functionality is now also available in Java. See the Schema docs, Java docs, and Java code.\n. In C++ there is the GetRoot templated function that works with any table type.\nGo appears to still be pending on: https://github.com/google/flatbuffers/pull/363\n. Your commit is in master.\n. Thanks! Some suggestions:\n- Please squash related commits into a single commit\n- Since FlatBuffers uses absolute positions for ByteBuffers, you do not need to save old_pos, and can simply reset position to 0 at the end for consistency (see an example of that in Table.java).\n- You may need to rebase, as I already made some of these similar changes.\n. You closed it? I'll cherry-pick 9ed40b2b4b782f79ad7171b8cd3bb8c57756a61b\n. cherry-picked.\n. Agreed for inheritance.\nUnions currently cannot contain structs for other reasons, however.\n. fixed\n. So you want the types to store their union enum as part of their definition? The problem with that is that a type can be part of multiple unions. Also, the advantage over using the current enums doesn't seem that great to me.\n. To distinguish multiple types of messages is what the union feature was designed for.. why not use it?\nAlternatively, if you want a more free-form solution, use the file_identifier feature.\n. It does not support structs (structs are inline, so they are not size-compatible with tables).\n. With file_identifier, you can simply call any number of XBufferHasIdentifier() calls to determine which type it is?\nsince you cannot use structs as the root of a FlatBuffer either, why not use unions as intended? that is much more compatible than your putInt solution (at least use addInt, as putInt may go out of bounds).\n. this is in.\n. That's an excellent idea.. I was thinking for convenience we should also be returning arrays, but that makes a copy. I like the idea of using a ByteBuffer instead.\nWould it make sense to return IntBuffer/FloatBuffer/etc instead, for more convenient access?\n. fixed\n. The problem with this is alignment, i.e. if you write arrays of bytes or shorts, you'll need to pre-align using the number of elements, to ensure it will match with the length field. I could simply shift the array at the end, but that could be costly. One thing we can do is to remove the need to specify the number of elements for all types with alignment >=4\n. Imagine you write an unknown amount of shorts, and that number ends up being 3. That is 6 bytes of data. Now you want to write the length field, which is an int, and needs to be 4-byte aligned. You must now write 2 padding bytes between the array data and the length field, causing the array data to not be adjacent to the length field. startVector takes care of that by aligning all the data before you write the array data.\n. cherry-picked already\n. Just compiled it on OS X with the included Xcode project, and get no such error.\nIs there anything in particular you're doing differently?\n. Robert: what is the complaint?\nOn Fri, Sep 12, 2014 at 10:13 AM, Robert Winslow notifications@github.com\nwrote:\n\nFWIW, I built this successfully just now on OSX 10.8.5 with a\ncustom-installed gcc 4.9.0: make clean all\n(Note that I had to remove '-stdlib=libc++' from CMakeLists.txt to keep\ng++\nfrom complaining.)\nOn Fri, Sep 12, 2014 at 9:22 AM, gwvo notifications@github.com wrote:\n\nJust compiled it on OS X with the included Xcode project, and get no\nsuch\nerror.\nIs there anything in particular you're doing differently?\nI'm guessing an #include  would fix it, but I wonder why it is\nnot needed in my case?\nOn Thu, Sep 11, 2014 at 11:35 PM, Valient Gough \nnotifications@github.com\nwrote:\n\nOn OSX:\nflatbuffers/util.h:152:3: error: use of undeclared identifier 'assert'\nassert(!(ucc & 0x80000000)); // Top bit can't be set.\n^\nincluding fixes it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/74.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/74#issuecomment-55426378.\n\n\nRobert Winslow\n@robert_winslow http://twitter.com/robert_winslow\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/74#issuecomment-55433323.\n. Robert: you said \"Note that I had to remove '-stdlib=libc++' from\nCMakeLists.txt to keep g++  from complaining.\"\n\nI wondered what the problem was.\nOn Fri, Sep 12, 2014 at 10:45 AM, Robert Winslow notifications@github.com\nwrote:\n\nNo complaint, I'm saying I get no header errors :-)\nOn Sep 12, 2014 10:27 AM, \"gwvo\" notifications@github.com wrote:\n\nRobert: what is the complaint?\nOn Fri, Sep 12, 2014 at 10:13 AM, Robert Winslow \nnotifications@github.com\nwrote:\n\nFWIW, I built this successfully just now on OSX 10.8.5 with a\ncustom-installed gcc 4.9.0: make clean all\n(Note that I had to remove '-stdlib=libc++' from CMakeLists.txt to\nkeep\ng++\nfrom complaining.)\nOn Fri, Sep 12, 2014 at 9:22 AM, gwvo notifications@github.com\nwrote:\n\nJust compiled it on OS X with the included Xcode project, and get no\nsuch\nerror.\nIs there anything in particular you're doing differently?\nI'm guessing an #include  would fix it, but I wonder why\nit\nis\nnot needed in my case?\nOn Thu, Sep 11, 2014 at 11:35 PM, Valient Gough \nnotifications@github.com\nwrote:\n\nOn OSX:\nflatbuffers/util.h:152:3: error: use of undeclared identifier\n'assert'\nassert(!(ucc & 0x80000000)); // Top bit can't be set.\n^\nincluding fixes it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/74.\n\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/google/flatbuffers/issues/74#issuecomment-55426378>.\n\n\nRobert Winslow\n@robert_winslow http://twitter.com/robert_winslow\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/74#issuecomment-55433323.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/74#issuecomment-55434997.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/74#issuecomment-55437171.\n. Ok, will add.\n\nOn Fri, Sep 12, 2014 at 8:37 PM, Valient Gough notifications@github.com\nwrote:\n\nThis may be C++11 specific -- my project uses C++11, which does not seem\nto be default for XCode yet.\nC++11 mode can be enabled via -std=c++11 (or -std=gnu++11).\nUsing XCode 5.1:\nApple LLVM version 5.1 (clang-503.0.40) (based on LLVM 3.4svn)\nTarget: x86_64-apple-darwin13.3.0\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/74#issuecomment-55480581.\n. fixed.\n. the reason all the addX functions are static is because they don't need an object to function. I'm trying to make the API as efficient as possible by not causing object allocation when none is necessary.\n\nThe chaining syntax would not work for all fields, since some already return an offset.\nI agree this would be lot nicer though.\n. I don't see the need for inheritance. In particular, Table's do not need to be dependent on FlatBufferBuilder, you don't need FlatBufferBuilder to read FlatBuffers.\n. Thanks, these seem useful additions. This commit is removing the headers, which we can't do, because we build on platforms that don't use cmake (e.g. Android).\nAlso, if you can squash fixup commits, that makes for a nicer history if I pull these in.\n. Also, there was already a recent commit that allows turning the tests off, you may have to rebase.\n. cherry-picked\n. Property getters may be doable, but setters currently less so, since the methods are static (they don't require a C# object to be created). I'll look into it.\n. All progress is right here on github, so no. The getters is something we want to change, and we'll get to it. Feel free to send us a PR is you're in a hurry.\n. Yes, that's a good summary of the issues. My solution was to simply not use properties, since having the two overloads have different names and be different things (properties vs methods) seemed inelegant. Also, the big benefit of properties is uniform reading & writing, whereas here we only read, so them not being properties seemed not quite as big a deal.\nAdditionally, writing to a FlatBuffer is completely separate from reading, so the two could never be combined in a single property. Having only half of that functionality in a property would maybe only add to the confusion.\nI'd love to hear more opinions from C# programmers.\n. Ok, it seems that this is important for C#.. it is a breaking change, but unless anyone has objections we'll go ahead with it.\nTwo things that would be good to have as part of this commit:\n- A paragraph in https://github.com/google/flatbuffers/blob/master/docs/source/JavaUsage.md showing how C# differs from Java.\n- While we're breaking the API, maybe make the externally facing getters in FlatBufferBuilder.cs (Offset & DataBuffer) also into properties, to be consistent?\n. Not sure about .position(), since the whole point of the C# ByteBuffer is to keep the interface the same as Java for easier code generation. I think only things that are user facing should be properties, internal implementation details are best kept as simple as possible.\n. Sorry, you're right on position(). New changes look good to me.\nCould you also update https://github.com/google/flatbuffers/blob/master/docs/source/JavaUsage.md showing how C# is different where appropriate? \n. Feel free to make a PR when ready.\n. XBog: that inferface is possible, but it would be very inefficient compared to what we currently have.\n. This is now in master: https://github.com/google/flatbuffers/commit/0ee1b99c5d38636c2ab8eb8e05dcd42b328c0cca\nThanks Petr.\n. Thanks.. currently looking at https://github.com/google/flatbuffers/pull/76 which does something similar, will see what we can adapt from yours after that.\n. The C++03 commit is not something we can merge as-is (see discussion on PR for that commit).\n. I'd be happy to try and support C++03, I'd just like to do it with as little #ifdefs as possible, and no boost.\nAs for your cmake commit, there were some changes to the cmake file not too long ago, which may conflict. If we can keep the cmake file as simple as possible while still supporting all your use cases, that be best.\n. Thanks. Yes, factoring out compile_flatbuffers_schema_to_cpp would be good.\nYes, the FLATBUFFERS_INSTALL option is probably not necessary if it doesn't do anything that could complicate matters at config time for certain users.\nThe new install code looks... complicated (I'm not a cmake pro). Any intuition why we need all these possible ways of doing things? Is there a way to simplify or converge on a best way of doing things? If not, a little more description on why all these different .cmake files are needed, where they end up, and how someone would use them would be welcome for users of this file.\n. Why do we need a FindFlatBuffers? (why is an add_subdirectory not sufficient)?\nGiven that FlatBuffers is so small (most basic usage is header only), I see little point to \"installing\" it, especially since you want to keep it in sync with generated code. Compiling it as part of a parent project makes more sense to me.\n. FlatBuffers is a tiny library, so I am not sure if having to rebuild on clean is the biggest concern in this case.\n. attributes may be anything, priority is an example of something that is not built-in. Will clarify that.\nnested_flatbuffer it looks like I forgot to document! will fix,\n. Fixed.\n. cherry-picked\n. I don't know anyone that's working on it.\nI'm not familiar enough with Python to be able to comment, but certainly anything that just represents an array of bytes is a good starting point. Convenient reading/writing of little endian scalars is helpful.\n. Cool to see progress!\n. FlatBuffers is really not meant to be used with a dynamically loaded schema. This is currently only somewhat possible in C++, since that has an implementation of the schema parser.\nThat said, for those that want to inspect a schema at run-time, the \"right\" way to do this is to have a schema that describes schemas (yes, you heard that right), such that the parser can output a FlatBuffer binary version of a schema, and then code in any language can load it. We have plans for this functionality but haven't gotten to it.\n. The verifier wasn't quite as urgently needed in Java, since there a malformed buffer would at most generate an out of bounds exception. What do you need the verifier for that you can't do by catching that exception?\n. Not sure how I didn't catch that. Will fix.\n. This was just fixed.\n. Yeah, the C++ buffers are already reusable, see FlatBufferBuilder::Clear().\n. Looks like it.. @rw ?\n. I agree that is unfortunate. Breaking the API is one thing, breaking the encoding is unacceptable at this point. I have some ideas on how to improve the API, though.\n. I don't see what functionality the code above provides that's not already in the current generated code? E.g. the monster() function doesn't help, since you'll still need to check its return value regardless.\nWhat would your visit function look like?\n. I'm not sure how that can work. If I pass a std::function<void (const Foo *)> it will give me a compiler error on the call with a Bar *.\nThe only way I can see this work is if we additionally generate something like:\nstruct FooBarDispatch {\n    virtual void f(const Foo *foo) {}\n    virtual void f(const Bar *bar) {}\n}\nYou then subclass one of these and pass them to your visit function. Though that sounds like a lot of work for avoiding a switch()/cast?\nI guess an alternative would be a visit function takes a std::function for each of the possible types, essentially implementing a typesafe switch().. though that be unwieldy given a big union, and also fragile.\nMaybe easier would be to add additional accessors, e.g. foobar_as_foo(), that returns null if it isn't a foo?\nAnyone else have an opinion?\n. It's relevant from a discussion point of view, but can probably be closed.\nWe're not going to change how unions are encoded in the format.\n. Haven't tried it with Unity, and it would be good to hear if it works.\nThe code is vanilla C# with no dependencies on MS's .Net implementation, so should in theory work.\n. xgalaxy: the FlatBuffers C# code doesn't use reflection or anything that would break AOT compilation.\n. xgalaxy: the FlatBuffers C# code doesn't use reflection or anything that would break AOT compilation.\n. The first short in a vtable is the size of the vtable, so it will fail early if buf_.data_at(*it) were to be shorter than vt1.\nI see no easy way to appease address sanitizer other than the setting you changed. I could write my own loop, but I'd rather not if that's not necessary.\n. I'm not sure how data_loc != vt1 would stop address sanitizer from complaining.\n. This was fixed in: https://github.com/google/flatbuffers/commit/6ca102e41302a3ad4000ff2fe129492e6497579e\n. CreateVector takes a uoffset_t * in this particular example, so you can simply pass it a pointer to an array of those. Alternatively, there's a variant of CreateVector that takes a std::vector<uoffset_t>\n. std::vector<uoffset_t> mlocVector, or better yet, std::vector<Offset<T>> mlocVector where T depends on what table you are creating (see the return value of whatever Create function you use to create the table.\n. thanks! will do\n. Fixed.\n. Just cherry-picked this for the moment: https://github.com/google/flatbuffers/commit/07d5965c812fa5e82dc4d3eb32b37540b7c91598\n. Fixed in: https://github.com/google/flatbuffers/commit/6ca102e41302a3ad4000ff2fe129492e6497579e\n. thanks, will replace with PATH for now.\n. Fixed.\n. Oops. Thanks!\n. Flatbuffers does not contain writable globals or statics.\nFlatBufferBuilder (used only when writing FlatBuffers, not when reading) is obviously not thread-safe, but if the FlatBufferBuilder object is local to one thread, it would be fine.\nA FlatBuffer that is being read is entirely const/read-only, so yes, can be read by multiple threads in any order.\nI can expand that documentation section to be more clear.\n. clarified in this commit: https://github.com/google/flatbuffers/commit/0ce53c96c30fe5438edf50251ca60b7655db1f2a\n. Yes, declaring would probably be the best. I'll make that happen.\n. user defined attributes now require a declaration: https://github.com/google/flatbuffers/commit/0952143971bdbb5ef20dae8a865e811a0e31b4b3\n. This is because unions are implemented as 2 fields, so they have to sit inside a parent. For backwards compatibility, that is not going to change any time soon, unfortunately.\n. Thanks.. I'll have a look.\nmemcmp should fail if the vtables are not equal size, since the first short is the size of the table, so it will never read outside of bounds. Seems like I may have to replace it anyway though, since it confuses static analysis.\n. Your nagging is welcome :)\n. Thanks for reporting. Fixed in:\nhttps://github.com/google/flatbuffers/commit/8ef6ee2a3e7f5c85e41fff07bf731fdb3b4bbc38\n. Thanks!\n. cherry-picked: https://github.com/google/flatbuffers/commit/118abc2871c05fb0b38b7fa8a3fe6278162e4b08\n. Table (and anything derived from it) refer to memory inside a FlatBuffer, which means they can't have a virtual table, meaning they can't have virtual or final methods.\nTable (and Monster etc) are \"handles\", helper classes to access information. I am not sure why anyone would want to inherit from them. I am not sure if there's an easy way to stop people from trying.\n. Table already has:\nprivate:\n// private constructor & copy constructor: you obtain instances of this\n// class by pointing to existing data only\nTable();\nTable(const Table &other);\nAre they not getting compile errors when trying to inherit?\n. I will investigate if all compilers we support have \"final\" for classes, and add it.\nThat said, there's a 3rd, more serious error made here: the assumption that you can cast raw memory to an object that needs a virtual table, without using placement new. If they had tried to use this object (call a method on it) before deleting, than they would have run into trouble even earlier. And this is a problem independent of FlatBuffers, i.e. could have happened with any library, and is not possible to protect against.\n. final added in this commit: https://github.com/google/flatbuffers/commit/285501f7bef048c21d0424653637f6f295b7d4fb\n. I agree, this would be handy. CheckField is the start of such a thing, but it needs generated code to be useful.\n. That's not correct behavior, since FlatBuffer creation in other languages will not store fields that are equal to the default (0 for any scalar fields that don't specify it), even if you explicitly set it. That would mean that in Python any 0 values would come back as None.\n. This functionality is now in:\nhttps://github.com/google/flatbuffers/commit/1fa803d187c244e8de0bb40f0d3c91f22408bde0\nThis allows you to call e.g.\nmymonster.CheckField(Monster::VT_HEALTH)\nto see if a field was stored at all. Note the caveat I mentioned still applies: you won't be able to tell the difference between a field that wasn't written vs a field that happens to have the default value this way, unless you use force_defaults. This is intrinsic to how FlatBuffers works.\n. Yes, that's an oversight.\n. I'll bump it up in priority, but I can't promise when I get to it.\nYou may have to use unique names for the moment.\n. This is still in the pipeline, I'll ping the brcooley.\n. Fix is in: https://github.com/google/flatbuffers/commit/ecb27817cab429925f311832cffb277df30a9908\n. windows.h clashes with the STL, which frankly is a problem with windows.h. It should always be preceded by #define NOMINMAX.\nSee e.g. http://stackoverflow.com/questions/11544073/how-do-i-deal-with-the-max-macro-in-windows-h-colliding-with-max-in-std\nYour #undef would be helpful, but I am not sure that that is the correct way to solve the problem. Anyone else opinions?\n. Yes, I can't merge this as-is. I love taking PRs, but they have to be neatly packaged in individual commits, changing one individual thing at a time, in a manner that fits with the existing code.\nIf you can rebase your commits such that I can cherry-pick an individual one that does not depend on the boost stuff, for example, I can get it in there and reduce the difference between the branches.\n. I'm not sure what the purpose of this modification is. If you wanted to test for the presence of the identifier, you'd use BufferHasIdentifier. Testing hard-coded for offset 4 like you do in the test is exactly what that function already does for you. What do you want to do with this string that you can't already do with BufferHasIdentifier ?\n. Ok, I guess it makes sense for that use case, yes.\nTo merge this, could you please:\n- make sure that both BufferHasIdentifier and Finish call your generated Identifer function, rather than using their own copy of the string.\n- Leave out the test case, as there is already a test for the presence of the identifer in the buffer, and a hardcoded offset is not desirable. If you want, you could add a test case simply comparing your new function with the string.\n- Make sure all of the above sits in a single commit.\n. There is already a test that covers that:\nTEST_EQ(MonsterBufferHasIdentifier(flatbuf.c_str()), true);\nYour change doesn't read/write the identifier, it just exposes it. So at best, your test should strcmp.\n. cherry-picked in https://github.com/google/flatbuffers/commit/f7babcfa10b515e2a5321e306dc53dfb89195756\n. Thanks, will have a look.\n. Cherry-picked in https://github.com/google/flatbuffers/commit/ced2cb6ce940f0ef0cb0d71f082f86bf082e6e8a\n. Thanks! Will have a look later.\n. this is in: https://github.com/google/flatbuffers/commit/aa46f0e4c23a353c9741f6d7e65e28013a0ca234\n. A joint PR would be great :)\n. Robert, do you want to do more to this commit, or is it \"good enough\" for now?\n. ready then? :)\n. Python2 tests:\nTraceback (most recent call last):\n  File \"py_test.py\", line 17, in \n    import MyGame.Example.Any  # refers to generated code\n  File \"/usr/local/google/home/wvo/rep/vendor/unbundled_google/libs/flatbuffers/tests/MyGame/Example/Any.py\", line 5, in \n    from enum import Enum\nImportError: No module named enum\n. Still get this: ImportError: No module named enum in __init__.py on all 3 tests. My actual Python version is 2.7.6. On Linux.\nAlso maybe have the files generates in tests rather than tests/py_gen, that way they line up with Java/C#.\n. For S_IRWXU etc: make sure you use the mkdir related function in util.h instead.\nThe nested_root thing I can fix.\n. Ok, merge at will :)\n. cherry-picked into: https://github.com/google/flatbuffers/commit/ae1763e226035e406ab5d0ac7174805a8e269e5e\n. cherry-picked into: https://github.com/google/flatbuffers/commit/aa46f0e4c23a353c9741f6d7e65e28013a0ca234\n. cherry-picked into: https://github.com/google/flatbuffers/commit/6f4b4c80a71a6e31bc16a53751ab122317eacf8a\n. On your original code, you should have gotten the error \"non-inline data write inside of object\" upon WalletStartCurrenciesVector, as you can't nest the construction of vectors (or tables). Is there anything that could have made that clearer?\n. Thanks... will fix.\nAlso here: https://github.com/google/flatbuffers/pull/113\n. should be fixed by https://github.com/google/flatbuffers/commit/ae1763e226035e406ab5d0ac7174805a8e269e5e\n. Try: nestedTable:[ubyte] (nested_flatbuffer: \"OtherTable\"). The quotes are not obvious from the docs, will fix.\n. They are for entirely different purposes. [tableName] is a vector of tables\n(part of the current FlatBuffer), and the other one is a vector of bytes\nthat contains an entire self-contained FlatBuffer. The latter is only for\nvery specific use cases (like container formats).\nOn Mon, Jan 5, 2015 at 6:15 PM, schwiet notifications@github.com wrote:\n\nThanks, yes that did the trick. Is the [ tableName ] syntax also legal?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/118#issuecomment-68816478.\n. cherry-picked into: https://github.com/google/flatbuffers/commit/3ec8d7f598b0319dc845830116252f051b45b2bc\n. No plans as of yet. There is interest in a C port, which may be an alternative path. What platform are you on / what compiler do you use?\n. It use lambdas here and there, which is not hard to work around. Same for the range-based for.\n\nThere may be template related features not supported by C++98... but this is hard to tell without trying it out.\nIs there any particular reason you can't update your version of GCC?\n. We can look to see if flatbuffers.h can be downgraded to C++03. Note that this would have to be done in a way that doesn't break / worsen the situation for the current API, and cannot rely on external libraries (such as boost) to make up for functionality.\nAlternatively, @mikkelfj has been working on an alternative FlatBuffer implementation in C, which would work with C++03: https://github.com/dvidelabs/flatcc\n. We now support STLPort (which is C++03), but the code still relies on C++11 isms here and there. I don't think at this rate we'll ever get to full C++03 support, in fact we hope to go in the opposite direction in the future, and support more of C++11/14/17/20.\nWhich means that for now the solution for C++03 and earlier is to use the C implementation: https://github.com/dvidelabs/flatcc. Ah... can't update to Yosemite yet, and doesn't happen on Xcode 6.1.. what symbols does it complain about?\n. Hmm, it appears to be linking some of the .cpp files.. twice?\nI've just managed to update from 6.1 to 6.1.1, and it is not giving me these errors, nor is it asking me to change my settings.\n. Remember it uses the same parser for both schemas and JSON, and it is really fast in general. So unless your schema is really big relative to the total amount of JSON you parse with it, you will not notice.\n. What problem is this fixing (what is an example when this goes wrong?)\nA better fix would be to find the end of the hex string, and then pass that address to StringToInt and strtoull, which does not require memory allocation\n. Ok, but can you modify it to not do a memory allocation, but instead pass the end pointer down to strtoull?\n. Creating the std::string still potentially does a memory allocation.\nYou can specify the length with strtoull, its just in the form of an end pointer.\n. Apologies, you're right.\n. cherry-picked in https://github.com/google/flatbuffers/commit/7cf74cb8644262f8ca02d69705bbc394d66514ce\n. Thanks, will merge.\n. cherry-picked into https://github.com/google/flatbuffers/commit/c95ad9cc5526de2cdd76ea5952a0ba47832e5ef9\n. Cherry-picked into https://github.com/google/flatbuffers/commit/c95ad9cc5526de2cdd76ea5952a0ba47832e5ef9\n. Cherry-picked into https://github.com/google/flatbuffers/commit/f5132b9ee190695de4fbfa833722ac4e016ba2cd\n. Thanks. We personally don't use Maven though, so I am a bit worried about having to support something I don't use. How can I tell this file is correct? Do you intend to keep this file up to date (and send me the occasional PR?)\n. Ok, thanks, I'll give it a go.\n. Cherry-picked into https://github.com/google/flatbuffers/commit/1263e9788e0a40ae1c0479c4478209d4991a35a7\n. Is there an automated way to do so from what is in this github repo?\n. cherry-picked in https://github.com/google/flatbuffers/commit/2b01247b309996f50d2facaa1d83519280911aba\n. Thanks, appreciated :)\n. cherry-picked in https://github.com/google/flatbuffers/commit/2b01247b309996f50d2facaa1d83519280911aba\n. I like the idea of biicode and wouldn't mind including support for it. My concerns:\nFlatBuffers itself doesn't have any dependencies, so seems the utility of biicode here is minimal (especially since we have to maintain the regular cmake code anyway). Users of FlatBuffers could benefit from biicode, but the default use of FlatBuffers involves just a single .h, so the only thing biicode would need to set up is its include dir. Then there's a more advanced use that involves several .cpp files (which shouldn't however be linked by default). The test code or samples should never be compiled when included in another project. Then of course the whole thing depends on a command line utility being compiled, to generate additional headers. I don't think your cmake code reflects the above. It is a very non-standard \"library\".\nSecond, assuming the above is addressed, the new cmake code appears to duplicate quite a bit of whats in the regular cmake code, which would worry me that we'd have to test with biicode and make changes in two places to ensure we're not breaking things for some users. The more the two parts can share things, the better.\n. I'm not so sure about having the .travis.yml file, because people might expect it to build vanilla FlatBuffers, but now it builds it using biicode.\nThe cmake file looks better. Is all this copying necessary? I understand it supports building to a different directory, but the existing cmake file doesn't support that either.\n. cherry-picked in https://github.com/google/flatbuffers/commit/da0f096ba220ec49bafc0743773a46524dd79832\n. Thanks!\nI already had a commit in the pipeline that reduced the memory allocations, but when I saw yours, I noticed I missed some functionality, which I then fixed. Your commit however has some additional code related to reusing a FlatBufferBuilder, which if you rebase it I can still cherry-pick, though do note that there's additional fields you have to reset beyond the ones you set already. Sorry about the mess.\nThe force defaults one looks good, though I would instead make it read x != d || force_defaults, which will be faster for the common case.\n. That surprises me, but if you tested it, we'll go with it :)\nI'll get the force defaults one in as-is.\n. Ok, seems all 3 of them need some rebasing, sorry.\n. I can see the use of it, but I do think it is a niche case, since it only makes sense with force_defaults, which most users don't want since it takes unnecessary space on the wire.\nEssentially, having force_defaults with a Has accessor grants you one additional bit of information (which is useful), but at the cost of 32 extra bits (or however much) when your value is at default. And yes, in some use cases it may well hardly ever be at default (such as with float data), so this can be a gain.\nCoupled with extra methods not being free on some platforms (such as Android), I think it should hide behind a command-line flag, such as --has-accessors.\nNote that there currently is no --force-defaults either (it is only available in code), so this may be worth adding too, such that when you convert JSON to binaries on the command line, it will write out the defaults.\nI was thinking for a second that maybe these two options should be one and the same, but then again I can picture situations where you may want to use them separately, so that is probably a bad idea.\n. I was hoping that for most users, bb.hasArray() is true, so this wouldn't be an issue..\nI'd be glad to see an improvement to this code, however. My primary concern is speed (the loop based version sounds like it could be slow). My secondary concern is simplicity (the vast majority of callers will want a string, so we should give them that). The third concern is code size / method count (adding additional methods isn't great unless the payoff is great).\nUsing a StringBuilder or CharSequence could be good, problem is, what do you do with one of these once you get it? If you need to convert it to String anyway to look at it, there's no point.\n. Hmm, that generates one more garbage object in that case, which isn't great, but I guess being multi-thread safe is valuable.\n__vector_as_bytebuffer also has this kind of code, though that can be improved by also using duplicate instead of slice.\n. Ahh.. yeah if it can have copying behavior, that be terrible with large buffers.\nIf the loop can be faster, I'm all for it, though including code to do our own utf-8 decoding sounds like a bad idea.\n. cherry-picked in https://github.com/google/flatbuffers/commit/4d3db99283199ffd8d5fdbe84933444d25b980f4\n. The C# implementation of ByteBuffer doesn't allow its position to be set while reading a FlatBuffer, and __string doesn't have the same problems as in Java.\n. cherry-picked in https://github.com/google/flatbuffers/commit/72b9501e69901abd2ba6fa7980996f3c1e244d93\n. cherry-picked in https://github.com/google/flatbuffers/commit/9c169083adb77b7ab046daa1600ba283a786e90f\n. We provide windows executables as part of the releases: https://github.com/google/flatbuffers/releases\nThey are not part of the repository, as that would require a windows build upon each commit. That means if you want to use the bleeding edge version, you need to build yourself (which is just a single action using the free visual studio express / community edition).\nI could make another interim release if that would be helpful.\n. Could you make one of the two call the other? This would reduce code & redundancy a tiny bit :)\n. whoops, misread the diff.\n. cherry-picked in https://github.com/google/flatbuffers/commit/9c169083adb77b7ab046daa1600ba283a786e90f\n. cherry-picked in https://github.com/google/flatbuffers/commit/4d3db99283199ffd8d5fdbe84933444d25b980f4\n. cherry-picked in https://github.com/google/flatbuffers/commit/c9a840e935e574852804205edebd4552a9d111f1\n. cherry-picked in https://github.com/google/flatbuffers/commit/f353fd886465f3fd6b96ed34804003f8cc3572fb\n. cherry-picked in https://github.com/google/flatbuffers/commit/71e97b712306980f95a86e6770b0dd18c9915593\n. The second option wouldn't work either, since conf is a pointer to the root object, which sits inside the buffer (not at the start).\nThe buffer is owned by vector_downward, which is owned by a FlatBufferBuilder which is owned by Parser. We can't really give you the raw memory owned by vector_downward since its buffer does not start at offset 0 either. vector_downward is meant for internal use only. We could potentially allow you to move the FlatBufferBuilder out of Parser but I am not sure how useful that is. \nIn the end, making a copy is easiest :(\n. I quite like it, I wasn't even aware that unique_ptr has a Deleter argument, that's quite elegant.\nI guess the low-fi alternative to this is a Release() in FlatBufferBuilder that simply returns two pointers. It has the advantage that it is simpler and easier to understand, but the unique_ptr trick is certainly more elegant.\nOne small issue with your code is that vector_downward assumes that buf_ is never null. There needs to be at least an if-check in the destructor because we can't assume that all allocators are null-tolerant. Also, once you call Release(), calling GetSize() now will return a garbage result, so we need an assert in there.\n. Lets go with the unique_ptr, please make it into a PR, taking into account the above issues.\nAs for the tests, I think it be better to reuse the existing test code as much as possible. So maybe, have the existing CreateFlatBufferTest return a unique_ptr, but also take a string arg, so we can demonstrate both ways for returning a buffer. Then AccessFlatBufferTest should take a raw pointer (& length), and be called twice.\n. https://github.com/google/flatbuffers/pull/146 is in master (since a long time ago :)\n. Thanks! I'll have to look at this later unfortunately (GDC). Any chance you can squash this PR into a single commit?\n. cherry-picked in https://github.com/google/flatbuffers/commit/a360958be3f998b66e12a2d3db0ea17a05a37757\n. Thanks :)\n. Apparently our code guidelines state that the copyright year should be when the file was created, not last modified. I'll fix that since some of your commit is still useful.\n. cherry-picked in https://github.com/google/flatbuffers/commit/432f3f26a40dbc51691c6d461d8a137c5e6c9f2e\n. Thanks! As long as I can cherry-pick them, then that's ok.\n. cherry-picked in https://github.com/google/flatbuffers/commit/c3807fa39dda5445924ef77253efb446459f6e56 and https://github.com/google/flatbuffers/commit/4464405250ee9151177d3f4b88fb9959c924ad37\n. Thanks, this looks indeed useful. I don't think we need to have the benchmark in the code, however.\n. cherry-picked in https://github.com/google/flatbuffers/commit/6a0126340a3767cc6f4988b861f1340ce6a747ea\n. Cherry-picked and refactored into https://github.com/google/flatbuffers/commit/6df9e1c537f566be082aa41c5e35628aa06733fc\n. Most cmake files I have seen would break if you used semicolons in filenames, but sure, this would be nice to fix.\n. Yup, that could easily be implemented by adding another generator to flatc. Anyone want to have a go at it?\n. Most C++ code out there is of the form namespace::symbol, not ::namespace::symbol, and thus is not protected against people making a mynamespace::namespace. I.e. imagine the errors you could cause by wrapping unsuspecting pieces of C++ code in namespaces like foo::std.\nAs such, I am not sure this is the right fix, since prepending :: to all namespaced symbols doesn't help readability. A better solution may be to blacklist \"flatbuffers\" (and \"std\"..) for namespace components. Then again, not sure if that's so great either, seeing as this is now a cross-language tool.\nOpinions welcome.\n. Sure, I can see the boost people using this, but pretty much all c++ code out there doesn't. Your commit does not prefix the std namespace, what happens you make a schema that says namespace grl.std ? More compile errors.\nI looked at the generated code from your patch, and its a lot messier than before. Now, generated code isn't necessarily meant to be read, but on occasion it will, and I don't think its worth it to allow people to use flatbuffers (and std) as part of their namespaces.\n. Closed for now, see discussion in issue #154 \n. Thanks, will look at both of these commits shortly. If they clash, can you make one depend on the other (2 commits in one PR)?\n. cherry-picked into https://github.com/google/flatbuffers/commit/3ad853630c6bf76a9c8fc2a15e3fc40cd52de691\n. cherry-picked into https://github.com/google/flatbuffers/commit/b8708beeec669371628eafbb6570cd278a0b96b9\n.  cherry-picked into https://github.com/google/flatbuffers/commit/c4a3e2f6bd90569637997c68a0a87d3a28e3db52\n. While FlatBuffers typically chooses efficiency over usability if the two can't be united, having to cast all the time doesn't sound great either. Another option is to generate accessors for both?\nMeanwhile let me get this change in.\n. cherry-picked into https://github.com/google/flatbuffers/commit/8b99bf614c3b91e3fa79e8b225bff7e6f03ca7f2\n. Thanks. Responded on that thread. We use Visual Studio 2010 compatible C++, will see if that can be upgraded.\n. looks like delete is a VS2013 feature, and we won't be able to upgrade to that soon (too many of our userbase still on 2010 or 2012).\n. You mean for example class Color in tests/MyGame/Example/Color.java should have a private Color() {} ?\n. fixed in #169 \n. cherry-picked into https://github.com/google/flatbuffers/commit/803f9bba27b9063b147d2751031e87ee55038bc9\n. So, do you want to change this so it generates accessors for both strings and byte vectors? Or do you think Go programmers will be happy with just the the byte vectors?\nAlso, if you can squash related commits, that's helpful in code review.\n. I'm fine either way, to me it really depends on how people typically use Go.\nAs far as collisions go, we already have this problem with \"Type\" and \"Length\" suffixes (depending on language), which is currently handled in an ad-hoc way in the parser by giving an error for any fields defined that would clash (see CheckClash() in idl_parser.cpp. We could add \"Bytes\" to that, though maybe \"ByteVector\" is even better, since it is less likely to clash.\n. In C++ we provide an allocator call-back, is there something similar that can be done in Go?\n. Yes.. though reusing FlatBufferBuilder objects seems preferable in most situations.\n. cherry-picked into https://github.com/google/flatbuffers/commit/9d368deb05f722c02d9d3a1e4cb1fd102f06a3b4\n. looked at the Python file, and I think it be cleaner to not have it at all, but instead have those change simply sit inside the files its targetting.\n. No, I don't think that should sit in the main readme.\nI love the idea of biicode, and I want to support it (as we are). At the same time, most people will be building with alternative methods, and I don't want to give the impression that biicode is required in any way. It should be available for people that want to use it, but as much out of the way (to not confuse people) for those who don't care about it.\n. Thanks! Cherry-picked into https://github.com/google/flatbuffers/commit/23f75f598e7876858faabc9f2bccac2cf83db235\n. Thanks!\nIs there any reason to have bool seal_classes ? i.e. in what case would I NOT want the code to be generated this way? If this is always sensible, lets not make it an option.\n. See my comment on #171. We could just make them static for now.\n. I think it is ok to apply your change to C# as well, until we have proper enums. Just like Petr says, static is more appropriate than sealed.\n. cherry-picked into https://github.com/google/flatbuffers/commit/557c57eb9d9d58a86fe1e80552219930c82fe7f4\nThis still uses sealed, since its being applied to all generated classes, where static wouldn't work.\n. I don't use Gradle myself, but that sounds useful. Being an external repo means its easier for you to maintain, I guess. I can always link to it from the Java docs.\n. @ragdroid : nice!\n. You are quite right, this is a remnant from it being derived from Java. This is a larger change though, since beside generating the enum, we need to make sure it uses the enum type in all the generated API, casts from/to the wire type etc.\nThus, it doesn't need to affect #162 yet.\n. Thanks!\n. Fixed in https://github.com/google/flatbuffers/commit/e3b432cba853f30ab2b4e6957f40d1aa63218cad\n. Please rebase, we just committed a fix for #162, feel free to improve.\nAlso note the two comments, in particular the superfluous Java casts.\n. Well, the cast were not there before, and even though it is generated code, it is confusing for those who do look at it. So I would like that functionality maintained, for both languages.\nThe GenTypeForUser is fine, I just wanted to make sure you were aware that something similar already exists.\n. Thanks! Cherry-picked into https://github.com/google/flatbuffers/commit/e3b432cba853f30ab2b4e6957f40d1aa63218cad\n. @mormegil-cz could it be that this PR caused this issue: https://groups.google.com/forum/#!topic/flatbuffers/Ftj03LKZD4I ?\n. Thanks! cherry-picked into https://github.com/google/flatbuffers/commit/a8d6962ac2fbf5075ee5f58877d488eb74ed32df\n. Could you be more specific in what you'd like to know? https://google.github.io/flatbuffers/index.html describes how you write a schema, how to use flatc to turn it into Java code, and how to use that Java code. I assume you know how to add Java code to your eclipse project (I personally don't use eclipse).\n. I have no personal experience working in eclipse, so I can't help you there. It should work the same as an IDE though: generate your C++ code from the schema using flatc (see documentation), and then import that code as well as the flatbuffers include/ directory into your eclipse project.\n. The root type is merely a convenience thing, you can actually use any table as a root if you want, using GetRoot() in C++.\n. It does not support as3 at this time. If anyone wants to contribute an as3 generator, we can consider that for integration.\n. The 1 is because Go doesn't have enums, though I agree we should look into some workaround.\n. That sounds good!\n. Hmm.. where would be pass/store this Allocator? Would like to keep the default interface simple.\nWhat would be ideal I suppose is to make the behavior you're suggesting the default, though I guess there might be issues with FlatBuffers assuming it can use TLS.\nIt is really unfortunate we have to make these copies at all.\nAnd also, yes, currently a corrupted FlatBuffer can provoke an ArrayindexOutOfBoundsexception or whatever getInt throws. It be good to catch that if you're reading untrusted data.\n. I'm very careful in adding additional methods. Besides the bulk in an additional generated code and clutter, there are platforms (not naming any names ;) that have method count limitations.\nAs an aside, if maximum efficiency matters for your use case, can't you use the AsByteBuffer?\nAs for TLS, currently use of FlatBuffers is entirely without global state, though I guess this particular use case would be transparent to the user. Who would deallocate that buffer though?\nA way to create a string directly from a ByteBuffer (that presumably already contains UTF-8) would make a lot of sense to me. That way you can just keep your string data around as ByteBuffers and never pay the string convertion cost.\n. as for the new byte[] in that code, I am starting to think that caching that in TLS as the default implementation may be preferable over yet another set of accessors.\nAnyone any objections? Or should we leave it as is for now?\n. (has been resolved elsewhere)\n. Thanks.. --gen-mutable needs to be added to the flatc command-line. I'll fix that today.\n. fixed in https://github.com/google/flatbuffers/commit/d4d7a84e11f4b6299ed13b7efa3872b81d675719\n. Whoops.. thanks! Cherry-picked in https://github.com/google/flatbuffers/commit/a50711ad13de9ae9082e19453f91c89c3f505bda\n. From the code:\nvoid NotNested() {\n  // If you hit this, you're trying to construct an object when another\n  // hasn't finished yet.\n  assert(!offsetbuf_.size());\n}\nRearrange your code such that CreateVector happens outside of MyDataBuilder mb(fbb) and mb.Finish() and it will work.\nI am guessing the build settings on Android are such that asserts are ignored.\n. The code you wrote actually works in most cases. The problem is that 16bit offsets are used for the object itself (from the vtable), so if the vector is >64k, this will blow up. Having the vector serialized outside the object fixes that, and the assert is meant to protect it.\n. Thanks, great work! Cherry-picked in https://github.com/google/flatbuffers/commit/0ee1b99c5d38636c2ab8eb8e05dcd42b328c0cca\n. Cherry-picked in https://github.com/google/flatbuffers/commit/e5a1a3129dba65de0bf5f49f34a574dd559d92eb\n. Cherry-picked in https://github.com/google/flatbuffers/commit/f7d24f60a20e628523d2c24b3dd2fd3fbfcfeea2\n. That's a lot nicer, yes. Cherry picked in https://github.com/google/flatbuffers/commit/221193eaa2a92d6c8f1e8c149c418b0add55b2a5\n. Yes, default is no-sorting. You need to call CreateVectorOfSortedTables instead to get the sort. See also \"Storing maps / dictionaries in a FlatBuffer\" in the C++ docs.\n. I appreciate the idea, but I am not sure how I feel about this functionality. Writing {locale: \"fr\", text: \"Bonjour a tous.\"}, is not that bad compared to \"fr\": \"Bonjour a tous.\",, especially when you consider that storing lots of string-based maps is an uncommon occurrence in FlatBuffers, since any non-open-ended map translates to a Table, not a vector.\nAlso, for future PRs, it is much easier to review single commits that contain an isolated piece of functionality, so squashing all these cleanup commits etc really helps.\n. Yeah, I think this is to special-purpose to go into master at the moment. If there's individual improvements you made unrelated to parsing/outputting a table as a single key/value, we can look at those.\n. Good catch, thanks!\n. Let me see how hard it would be to restructure the code to handle this. Note that FlatBuffers does not support nested definitions, so this would cause them to be moved to top level.\n. Ok, turns out it was quite easy to support nested FlatBuffers, just committed a fix: https://github.com/google/flatbuffers/commit/2abe24b9ddd22fd095d96f002fb8c16b4edc36d2\n. Good idea, thanks!\n. Did you check if srcData->status() == nullptr ?\n. Your use of FlatBuffers appears correct, Data() will get you a pointer to the start of data:[ubyte]. maybe something went wrong with constructing the buffer? I don't necessarily understand what you're doing in convertFBS2xProto though.\n. Thanks! I'll have a look.\n. Can we maybe stick the code related to this is its own cmake file (in CMake/) such that the main file stays overviewable? Also, it currently appears to be conditional on UNIX, can we wrap it in a bigger conditional (e.g. FLATBUFFERS_PACKAGE_DEBIAN, similar to the rest of the file) such that only people trying to build a debian package will ever run this code?\n. any progress?\n. That does look like a good idea, yes.\n. Thanks for catching this! I have PPC VM set up for testing, but so far it is not automated.. these are easy to miss. If you find anything else, please let me know.\n. Just pushed a commit that should fix it: https://github.com/google/flatbuffers/commit/b8681d80312f3e96a6093d7208753a97ee962499\n. Are you able to contribute an implementation?\n. Cool.. I'd start by looking at the C++ implementation... C++ gets a lots of efficiency from use of templates and in-line objects that can likely be used in D, that are not used in C#\n. I don't think there will be documentation on how to do that, because it is so different for every language. Following the C#/Java code generator could be done, but it may generate sub-optimal code for D.\nThe \"internals\" document gives a general overview of how FlatBuffers works, the API design for a particular language is up in the air, and can is best decided upon by reading the code generators of the closest language(s).\n. Very cool! Had a quick look at the code, and I guess the \"isn't pretty\" refers to the fact that it adds relatively many if-thens to the code generator. I bet a lot of those could be removed by creating additional items to the table of strings, or additional functions.\nAnything else that's missing from this implementation? How much work would it be to clean it up?\n. And it may be a lot faster if it uses intrinsics.\n. Someone else made a PR for D here: https://github.com/google/flatbuffers/pull/3856\nHelp reviewing this would be appreciated, especially by someone familiar with D.\n@ThomasBrixLarsen : how does it compare to your implementation?\n. That's certainly a nice interface, it just goes a bit against the grain of FlatBuffers (which is all about avoiding dynamic allocation), by first allocating a full object tree and then serializing it.\nI can see how it could be faster if Create calls down to C that knows how to quickly iterate the Python data. But using C complicates matters in terms of being easy to use on all sorts of platforms.\n. @rw is this PR still meant to go in?\n. Nice improvement, thanks :)\n. Good catch! If you look at the 3 occurrences of GenDefaultValue, you'll see that the first one generates a cast, the second one is the one you're trying to fix, and the third one doesn't have a cast yet either, but somehow doesn't generate a problem in your code? Does it make sense to put this functionality inside that function?\nAre you able to make a PR for this?\n. Yes, that's not ideal, because now it has to convert double -> float inside AddFloat(). If it was taken care of in the code generation, then the compiler will likely turn (float)1.0 into 1.0f at no runtime cost.\n. This is because default values (in this case 0) are not stored. So if you take the json file with the missing field and generate a binary from it, and then try to read the field, you'll correctly get 0 back.\nI agree that for communication with other software it be more useful if these were output in json. I'll change that.\n. Let's keep it open, as I want to address this.\n. Fixed, see commit: https://github.com/google/flatbuffers/commit/ecf5a6a580980af8f6ef83b513011f35dddc0c2b\n. @rw what about this PR?\n. status?\n. That was my error, and it has been fixed. You should be able to make this PR go green by just pushing any small change to it.\n. Accidentally included some test code in the last commit.. the joy of blindly doing git commit -a.\nThe offending commit has now been updated: https://github.com/google/flatbuffers/commit/ddb1d5ffe4bf6d6799662811d9383478ff3d36cc\nThanks for reporting!\n. status?\n. Ok, I guess we'll close it for now until someone can pick it up.\n. Are you saying it doesn't support /* xxx */ style comments? That's probably true, I can add those, though for now you may have to remove the comment manually.\nWhat is the problem with the floating point number? Java allows floats to be initialized with 3.14149 right?\n. (see my comment on the PR)\n. I appreciate the fix, but I feel writing #define NOMINMAX before you include windows.h is the better solution, it is what pretty much everyone does by default.\n. I very much doubt that Boost would include windows.h, and pollute the global namespace with min/max macros.. can you check to be sure who caused the include?\nAnd if you have a 3rd party header that blindly includes windows.h, it should still be possible to #define NOMINMAX before it.. though if it also relies on those min/max macros itself, them, well.. it is broken, frankly.\nAnyone else opinions on this matter?\n. See, the problem is not so much this single use of parentheses, the problem is that if you allow this, anyone using the STL min/max functions anywhere has to remind themselves to use extra parentheses, just in case someone somewhere includes windows.h improperly.\nMaybe a better solution would be something along these lines:\n```\nif defined(_MSC_VER) && defined(_INC_WINDOWS) && !defined(NOMINMAX)\n#error You've included windows.h without defining NOMINMAX\nendif\n```\n. Leaving it open for now.. other people may have opinions.\n. so we ended up fixing the call to max anyway here: https://github.com/google/flatbuffers/pull/602\n. Can you first ensure you're following Google code standard? There's tabs in the files, curly braces in the wrong places, etc. Also make sure you first rebase against master.\n. Can you resolve the merge conflict?\n. Thanks!\n. This was merged: https://github.com/google/flatbuffers/commit/a96bfdb369725ea12edc2cf9e2ef33841793eb82\nThanks!\n. Thanks! good find.\n. I'm not working on that yet, no.\nIf you do, have a look at the C++ commit that implements it for inspiration:\nhttps://github.com/google/flatbuffers/commit/3ec5dddb00dac9ae72a2f6808ed28b06cb64cdab\nIf you're just working in C#, it be very helpful if you tried as best you can to also make it work for Java, as it is the same code. If not, I can help with the Java side after you're done.\n. In a struct, no fields are optional, so the field can always be set, and thus the return value would always be true.\n. Good catch! I'll add static.\n. https://github.com/google/flatbuffers/commit/1e6f8f5b8c4d0407d7db750858e7863e07091958\n. LGTM\n. Thanks, much appreciated!\n. Thanks!\n. Not so much a bug, as an unclear error. Because it doesn't know Foo, it assumes it is a table that will be defined later. Not sure how to improve on this, as we do want to allow to refer to future types.\n. Odd.. Not sure why cygwin is different from both GCC on Linux and Visual Studio..\nEither way, the error is in flathash.cpp, but you added the #include to the header. Could you move it to the .cpp?\n. Yes, it hasn't been ported to other languages yet. It would require a subset of the functionality here: https://github.com/google/flatbuffers/commit/3550899.\nIt would not necessarily need a copy of the C++ Vector class, but just helper functions that knows how to lookup into & sort an array of a particular table type. If this can't be done generically with C# templates, then sorting call may have to be part of the CreateMyTableVector generated code.\n. Generally, I'd pick the more performant option, which sounds like the 4 generated functions. Then again we don't necessarily want lots of duplicated generated code either (though I guess this is only for tables that have a key attribute, so not the biggest deal).\n. I believe find by key is working in C#. What compiler do you use, and what version?\n. fixed that issue:\nhttps://github.com/google/flatbuffers/commit/12ca3e054e257e06e2c6e35e8824d5ae569bce4b\n. Thanks! Why are all the language generators in this library, though? presumably they're only useful for flatc. The only use case for a library is if you want to do JSON parsing/reading, which is covered by idl_parser.cpp and idl_gen_text.cpp.\n. Yes, best to keep idl_gen-* (except idl_gen.text.cpp) in FlatBuffers_Compiler_SRCS, since no library user will ever need them.\n. You mean idl_gen_cpp.cpp ? That generates the c++ code, which should never be needed at runtime.\n. Thanks :)\n. If someone makes an implementation in Haxe, we'd can consider it for inclusion.\n. Note that FlatBuffers is a cross-platform solution, so your code shouldn't really be platform dependent.\n. I would hope it doesn't matter, i.e. if Haxe faithfully implements all the byte-twiddling equally on all platforms, it shouldn't matter. And the whole point of using Haxe is being able to target multiple systems, so if the FlatBuffer implementation works with one target but misbehaves on another, it won't be very useful.\nDoes Haxe have the full set of exact-size int and float scalar types that FlatBuffers exposes? Can it read and write those from arbitrary locations in byte arrays? can you do so in little endian (only)?\nAs for targets, we already do C++ and Python natively, so JS sounds currently the most useful out of those (though we can currently target it with asm.js). But again, if it can't work on all of them, I wouldn't bother implementing it at all.\n. Pretty cool!\n. See \"text and schema parsing\" in here: https://google.github.io/flatbuffers/md__cpp_usage.html\n. You want to... get part of a schema in JSON-like syntax as opposed to its own syntax?\nWhat exactly are you trying to do?\nIf you're trying to do things with schemas dynamically, you may want to use the reflection functionality (see \"reflection\" in https://google.github.io/flatbuffers/md__cpp_usage.html ). That allows you to get a schema as a flatbuffer, which you can use to query it. It can even be turned into JSON (though not the format you descrive above).\n. This looks great!\nDid you run any performance tests to see if the small structs are close to the speed of ints? They should be, but it be nice to know for sure.\nDid you run generate code for Java to ensure nothing has changed there?\n. This will be a breaking change for C# users, but it will be for a good cause :)\n. Can either of you be sure that passing a byte[] to an IList parameter, then\naccessing it as an IList has zero performance overhead compared to just\nusing a byte[]?\nIf no, I'd vote for an overload.\nOn Mon, Aug 3, 2015 at 8:41 AM, John Belmonte notifications@github.com\nwrote:\n\nHi-- I'm not sure I understand your point about overload being better. An\nIList function parameter would allow your byte[], in addition to List etc.\nUsing overload also requires an uncomfortable choice: would the argument\nbe IList, which would be redundant with the existing array function, or\nList, which would be bad practice since it's expecting a specific\ncollection implementation.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/pull/242#issuecomment-127281892.\n. Thanks for checking that!\nYeah, that looks like it is going thru some wrapper object to make it work.\nThat's significant enough overhead that I feel we need specialized functions for arrays.\n. status?\n. Can you verify if/where we rely on the position being at the location of the last put method?\n\nThe java code has both relative and absolute puts. The code would be much cleaner with just absolute puts, so if we can make the code such that position is only used as the buffer start, that be fine by me.\n. Yes, making it behave just like Java would be the easiest I think.\n(and btw, it doesn't point to the vtable position. the beginning of a FlatBuffer is an offset to the root object, which then in turn starts with an offset to its vtable.)\n. The problem is of course that now you need such helper function for every possible type?\nAlso, as far as bools are concerned, this problem was solved in Java/C# for example that had the same issue. If you look in that generator, you'll see GenGetter etc. making a special case to convert between bool and byte.\n. As I mentioned in https://github.com/google/flatbuffers/issues/244, this may not be the right solution to the problem.\nInstead of ConvertBool, you want the generated code to accept bools natively, and then convert to and from bytes internally.\nand PrependVector only solved the problem for one type.\n. We're still at a stage where API changes, if they fix obviously broken or inefficient existing API, should be permitted, with care. It will break some people's code whenever they upgrade, but the fixes will be minor and very beneficial for everyone moving forward.\nYou could generate code akin to PrependVector, though I would say at least have a standard version available for all standard types could also be helpful. Sadly that means code duplication in Go.\n. Please re-open if you have time to work on it.\n. Your log doesn't tell me much. Did you check the SetRootType return value? is pStructDef valid?\n. In example 1, you use (parser.root_struct_def).fields.vec.begin() which should not compile, since root_struct_def is a pointer.\nIn example 2 & 3, SymbolTable fieldDef = pStructDef->fields; makes a copy of the fields, which, when destructed, probably frees some objects twice. I'm not sure why you want to copy them, you should always use all the data owned by Parser by reference.\nNote, if you want to access a schema programmatically, you may be better off with the reflection functionality, see \"Reflection\" in https://google.github.io/flatbuffers/md__cpp_usage.html\n. Thanks!\n. Excellent commit! Could you additionally describe this functionality in JavaUsage.md (similar to \"Mutating FlatBuffers\" in CppUsage.md).\n. Ok, sounds good.\n. Looks like there's a conflict, can you rebase?\n. Thanks, great work!\n. Wow, excellent work!\nWhat makes this Node specific? Is it the use of Buffer ? Can we somehow make this work on the client as well by using https://developer.mozilla.org/en-US/docs/Web/JavaScript/Typed_arrays or whatever is the most portable solution?\nCan you add at least a small document on JS usage?\n. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/DataView\nAllows reading writing of any sized little endian scalars at byte offsets, so it is ideal for FlatBuffers. Appears to be supported by all browser, and even Node.js, though on top of ArrayBuffer, not Buffer. Would be great to support, though I suppose that can happen in a follow-up commit as well.\n. Hmm, I'm not sure, since I am not that familiar with the JS world. It seems ArrayBuffer is available in all browsers and in Node, so it would be the logical choice, but in Node Buffer is more common, so the code should work well with that as well? Converting between the two involves a copy which is not ideal.\nAs for endianness, you should be able to call getInt16(offset, true) on a DataView to always read in little endian?\n. @evolutional : could you have a look at https://github.com/google/flatbuffers/pull/257 and together with @evanw  decide if you want to port some ideas from that PR into yours, or ideas from yours into his?\n. Yes, there's definitely a lot of features that are in the C++ version that haven't made it to other languages yet. To some extend that's ok, it was always intended to be the \"leading language\". But as these features trickle thru, it be nice to see how they compare. I could make a \"language status\" page or something.\nperformance stats would also be great.. to be able to see how the different implementations compare.\n. Ok, I just added the start of a feature matrix to the docs: https://github.com/google/flatbuffers/commit/201699288bed9be5f11d80024e9e52f63d5d467c\nIt be great if either of you could add/improve it. Do not be afraid to change how I've done things, this is meant to be just a start.\n. Fix was merged: https://github.com/google/flatbuffers/pull/253\n. Thanks for the fix!\n. but result is the binary version of that JSON, right? In FlatBuffer format. It cannot read JSON directly.\n. To be clear, you cannot pass JSON in any way (as byte[] or String) to the above code. It must be a binary FlatBuffer, i.e. the result of e.g. FlatBufferBuilder.sizedByteArray(), or flatc -b on the command line.\n. FlatBuffers is not able to parse JSON in Java. You have 3 options:\n1. (Best) make your server send you a FlatBuffer binary instead of JSON.\n2. Parse JSON in C++ thru JNI.\n3. Parse JSON using the flatc commandline tool.\n. That was intentional. int in Java seemed more comfortable to use for interfacing with unsigned types than short, requiring less casting on average. Do you see a problem?\n. I'll add a comment.\n. https://github.com/google/flatbuffers/commit/d97f628703b365491825acabf0ba798e1dfff800\n. Cherry-picked your commit locally to test with the other languages, and fix the conflict. I merged it here: https://github.com/google/flatbuffers/commit/ac10873e26f99b3d054d1cb89c00bad239ace691\nThanks for the fix!\n. Wow, that's both unfortunate (double work) but also useful.\nIt is nice being self-contained, though I'd worry that doing your own bit-twiddling is going to end up slower than using DataView/ArrayBuffer etc., any ideas about that? How well tested is your UTF-8 code?\nPreserving 64bit numbers is definitely a good thing! Hadn't noticed that in the other PR.\nSingle file is helpful too.\nNo experience with Google Closure yet, but that sounds very useful indeed.\nAny chance you and @evolutional can talk, and look at what functionality from one PR can be ported to the other PR such that we end up with a agreed upon best implementation that we can merge?\n. Ahh good.. sorry, I assumed from your first message that it was not using any kind of buffer :)\nAnd another language designer! Very cool.. I fiddle with this is my spare time: http://strlen.com/lobster :)\n. @evolutional : well, could you look and see if there's anything you've implemented that this implementation could use? Or @evanw, do you think you're covering functionality of what @evolutional  implemented? Either of you are much more familiar with JS than I am :) \n. @evanw: The unicode code in flatc doesn't treat the 16bit numbers as UTF-16, it treats them as unicode code points. That may be considered a bug.\nThough that's probably not what's going on in this case: you feed it an un-escaped UTF-8 string, and it comes back as an escaped UTF-8 string. At no point was it even UTF-16, since flatc only ever deals with UTF-8.\n. @evolutional: it may be preferable to have separate test & getTest if that is faster and avoids the conditional (having to test if an object is present). Though maybe that is over-optimizing things... what do you think, @evanw ?\n. That is suprising, but sounds like it's worth using the bit-twiddling version.\nI noticed the DataView version used this peek function to copy the bytes in a loop first, which wouldn't be needed in our case, but even after removing that the DataView is still 3x slower.\nAlso, using bit twiddling means you're not relying on DataView, meaning it is slightly more.. \"portable\" ? \nEndianness should not be much of an issue, since FlatBuffer is always little endian, meaning bit twiddling code to read an int from a byte array will look the same on any endianness and should thus be well-tested.\n. @evanw : how much work do you feel remains to get this PR in good shape?\n. Thanks Oli, your help would be useful regardless.\n. Great! @evolutional @rw wanna have a look at this?\nAlso, can you rebase?\n. Ok, it appears all issues are clear. I'll merge, any further improvements can be followups. Thanks for the hard work @evanw (and @evolutional )\n. On that note, anyone want to add some documentation specific to JS?\n. I wonder why it thinks that is so. It's being used with signed values, and compared against a negative number, so I can't see how making it unsigned is going to improve things. For reference:\nc++\n// Ensure that integer values we parse fit inside the declared integer type.\nstatic void CheckBitsFit(int64_t val, size_t bits) {\n  auto mask = (1ll << bits) - 1;  // Bits we allow to be used.\n  if (bits < 64 &&\n      (val & ~mask) != 0 &&  // Positive or unsigned.\n      (val |  mask) != -1)   // Negative.\n    Error(\"constant does not fit in a \" + NumToString(bits) + \"-bit field\");\n}\n. Well, I suppose I can shift it as unsigned, then cast to signed, if it makes clang happy.\n. Yes, gcc says:\nerror: comparison between signed and unsigned integer expressions [-Werror=sign-compare]\n           (val |  mask) != -1)   // Negative.\n                             ^\nInstead, I can do this:\nauto mask = static_cast<int64_t>((1ull << bits) - 1);\n. https://github.com/google/flatbuffers/commit/aeff09d724eed05eaa2ae50bbcc95bdb5572d569\n. This just went in https://github.com/google/flatbuffers/commit/d06b2736aad6190f18a9c267cb1e48b10f2e03f9\nIt adds basic mutation in Java/C#, which is similar to what was already in C++. C++ additionally has more extensive mutation support based on reflection. It is not \"planned\" for other languages, but is welcome to be added.\n. Note my two comments above.. Also, maybe at least some minimal documentation (you can adapt the Java one). Otherwise looks good, @rw ?\n. @mnmtanish you should not require to be using biicode, that should be entirely optional. If it's not optional, let me know, and we'll remove it.\n. Looks great to me. @rw wanna press the big green button?\n. I don't see why you'd need MutableStruct etc.. just mutate things in Monster like you're already doing. What I would recommend is to set the values back to their originals afterwards, then see if the original read test still passes.\n. status?\n. @rw can you comment or merge?\n. Oh, @mnmtanish, you'll need to rebase this PR it looks like..\n. Thanks!\n. You cannot \"set\" existing String objects in Java, so how do you propose to reuse them?\nWe could be returning StringBuffer objects instead, but that requires most users to convert to String somewhere along the line anyway.\nEven Better would be to have a StringUTF8 class, that can point directly to the UTF8 data stored inside a FlatBuffer (without copying), and allows common string operations.. though that still would involve copying to String very often.\n. Closed because lack of activity or resolution. Feel free to reopen when there's a new direction :)\n. That's something that could be fixed, it's mostly lazyness.\n. Just fixed: https://github.com/google/flatbuffers/commit/40ffbab31e053c76314f5bacebedcfdef336e4c4\n. the above commit was removed, and an improved version was just added: https://github.com/google/flatbuffers/commit/4d7810424c8f964dbcb8dd3179d8c46cd896c4dc\n. Thing is, FlatBuffers is language agnostic, and uses the schema as its source of truth. Additionally tables are more complex than simple structs because format evolution is very important. The method above is mostly useful because it allows you to declare both the \"schema\" and the use of it in C++ in one go, and is limited to structs, so I think the two approaches are mostly incompatible.\n. Ahh ok.. so you mean, it would call you back with \"here's a field of this type at this depth\" etc.?\nThis could be done in generated code, yes, but since it is a rather specific use-case, maybe how it is done currently (through reflection) is better. It gives you more flexibility, and it does not all this extra code to be generated. Do you have an example use case?\n. There's reflection in the FlatBuffers C++ implementation, not in C++, the language :)\nSee reflection.h, and test.cpp ReflectionTest()\n. Yup, pretty new :)\nThanks!\n. Sounds useful, but is something most people won't need. I am always careful when it comes to adding new generated code, since it can add a lot of bloat.\nAnyone else any opinion as to how useful this would be?\n. we already have --gen-mutable which generates the optional mutation functions, we can have --gen-compare additionally.\n. Note I suggested that we could add a --gen-compare for whoever wants to implement it. I did not say it is already available.\n. Copying a FlatBuffer table is a very heavy / non-standard operation (currently only possible in reflection::CopyTable()) since it entails a deep copy into another FlatBufferBuilder, and requiring schema information to do so. That's definitely not something that can be made to fit in a copy constructor.\n. And how is this function implemented? Does it call CopyTable? where does it get the schema from?\n. I can imagine how it is done in generated code, yes.\nI am referring to you saying \"I am currently using the following template code to provide a copy functionality\".. I am not understanding how that works.\n. Ah ok, understood.\nDo you want to make a PR for this functionality? I think it should be conditional on --gen-copy or something, since most people don't need this functionality.\n. If it is code generated, there's no need for the template, and it can be a regular method rather than a struct. Look towards the generated code for Verify() as an example of something that hierarchically touches all fields.\n. Thanks for this fix! Travis says it fails running the flattests though, can you check?\n. Had to modify your commit to get the original behavior for non-scientific output: https://github.com/google/flatbuffers/commit/a5c511576fb996f82c7887ceb877e9a688eb18ee\n. Thanks for the help! Maybe worth disabling the biicode builds on anything but Linux for now?\n. Thanks, that should fix it for the moment!\n. I grabbed your change locally, and fixed the test... it's now committed here: https://github.com/google/flatbuffers/commit/5de28c74f9131f437e407be147c221b57c6eca7e\n. If you want to use integers, they have to be integers, not strings. so\nthese should all work (and same with \"\" for field names):\nupdateType: 1,\nupdateType: Delete,\nupdateType: \"Delete\",\nBut this does not:\nupdateType: \"1\",\nI suppose I could try and parse an integer from the string.\n. This sounds like a good idea. I'm worried about the \"all\", though, in the sense that future changes to the code generation may cause warnings we won't get to see?\n. Ok, then maybe let's use that until any other issues pop up?\n. Thanks!\n. This could potentially be done as an option, as enum class is not supported by all our target compilers. Or maybe we should have --use-latest-cpp that enables this and possibly other things.\n. That be great. Make sure to make it conditional on --use-latest-cpp flag.\n. Otherwise very nice PR!\n. Yes, you're right. Though we can worry about that once we introduce such an option, because for the moment we are still using individual options for features, and we might combine them once things become unwieldy. I also have hope that we can change the defaults of these over time and then deprecate them, as the amount of people using compilers with subsets of C++11 shrinks.\n. Certainly, bool is represented as a uint8_t not just in the C++ API, but in the wire format, as used by all languages. So for backwards compatible reasons, we can't change this to bits, no.\nTo implement such a feature, you'd have to introduce a new type, i.e. a \"bit\" (combined with other bits when adjacent) or a \"bit array\" specifically. Though I'm not sure if I would agree that that would be worth it, since representing a bit array using any of the existing integer types is easy, especially in combination with enums that allow you to specify its members as powers of 2 (see documentation).\n. I was talking mostly about the wire representation. The size of bool in C++ is implementation dependent, so it cannot be used for that purpose.\nCertainly the interface to a bool FlatBuffer field should be in terms of the C++ bool type, and I see that it isn't at the moment. Feel free to make a PR for that, or I can hopefully get to fixing that shortly.\n. You are probably right. And as much as \"rewrite it all\" would be great, the reality is:\n- It is sometimes hard to anticipate every possible question people may have, especially when you don't have those questions yourself. The documentation is best improved through iteration.\n- FlatBuffers is an open source project, and some parts of the documentation (e.g. the one for Go) were made by external contributors, not large teams of professional writers at Google. Even the main documentation was written by a single programmer (me). Protobuf has had much more time for its documentation to mature.\nThat said, the best way to help us (and thus help us help you) is to give us precise examples of questions you don't feel are answered, which allows us to easily go add exactly the material to the documentation. Even better if these are reported as issues on a per-language basis, since different people work on different languages, but feel free to just add them to this issue for now.\nI agree additional \"step by step\" tutorials with more sample code would be great as well, and is something that be great to help from external contributors from. These could either be additions to the main documentation, or articles elsewhere we can point to.\n. On Tue, Sep 22, 2015 at 1:05 PM, maticmeznar notifications@github.com\nwrote:\n\nOk, I came up with the following example:\nFor the following pseudo JSON, how can someone (in Golang):\n- write a schema\nThis is all in: https://google.github.io/flatbuffers/md__schemas.html\nand is not dependent on the language you're using, so the information in it\nis not going to be repeated on the Go page. The Go page is purely for Go\nspecifics not found elsewhere.\n\nIf there's anything in that page that needs improving, let me know.\n\n\ncompile a schema\n\nSimilarly, that is here:\nhttps://google.github.io/flatbuffers/md__compiler.html\n- set each value\nThe Go documentation has an example on how to set values in the text from\n\"You can also construct these buffers in Go..\" onwards, which shows\nexamples for many different data types. The examples correspond to the\nexample schema in the pages above, so that should hopefully help.\n\nIt's very likely that this is not complete however, so let us know if\nanything is not answered by it.\n\n\nserialize the whole thing into a []byte\n\nThis is in the one before last paragraph on the Go page.\n- unserialize from []byte\nThat's what the Go page starts with. I'd agree that starting with\ndeserialization before serialization is maybe a bit odd, but starting with\nhow to read FlatBuffer data somehow made sense.\n- retrieve each value\nAlso at the start. Should probably show access to more kinds of values.\n-\n{\n\"email\": \"someone@example.com\", // utf8 string\n\"age\": 42, // uint8\n\"regTs\": 1421539200, // uint32\n\"hobbies\": [\"sprinting\", \"board games\", \"baking\"], // array\n\"alive\": true, // bool\n\"skills\": { // map\n\"golang\": 3, // uint8\n\"linux\": 6, // uint8\n\"itsec\": 6 // uint8\n},\n\"authHash\": \"b3VyZWhnMHI4ZmpzJ2QgZjl3YSB0OTQzdyA5IG9pZg==\", // []byte\n\"balance\": -664926102 // int64\n}\nMost of these are in the example.\n\nThe Go documentation is lacking an example of a vector of strings or\ntables, it only shows a vector of bytes.. that would be good to improve.\nWouter\n. We just pushed an overhauled version of the documentation that includes a tutorial shared by all languages:\nhttps://google.github.io/flatbuffers/flatbuffers_guide_tutorial.html\nThis tutorial plugs a few holes, and should make it easier to improve the documentation and keep all languages in sync. The language specific pages are now just to give language specific tips, build instructions etc.\nThose who wanted to contribute to the documentation can now do so based on this new structure.\n. I will close this particular issue, please open new issues for more specific documentation problems as you find them.\n. In your example, this Check(elem >= buf_ && elem <= end_ - elem_len) corresponds to Check(buf_ >= buf_ && buf_ <= buf_ - 4) (since buf_, end_ and elem are all the same), so buf_ <= buf_ - 4 would fail. \nThat would generally work unless buf_ - elem_len could become negative, which is possible when given a corrupt vector length for example, which would wrap it around as it is unsigned. E.g. an elem_len of 0xFFFFFFFF would end up with end_ - elem_len being end + 1 which allows elem to be outside the buffer.\nYour commit would fix that, but it doesn't correspond to your example, what am I missing?\n. You have a 256 byte buffer, and you ask if the first 4 bytes are inside that buffer, which should definitely be true.\nNote: print 0x7fffffffcf20 <= 0x7fffffffcf20 - 4 should be print 0x7fffffffce20 <= 0x7fffffffcf20 - 4.\n. just tried this:\nuint8_t buf[] = { 0 };\n  flatbuffers::Verifier verifier(buf, 0);\n  TEST_EQ(verifier.Verify<flatbuffers::uoffset_t>(buf), false);\nThis hits the assert in Verifier::Check (as Verify is not meant to ever return false in normal use), but Verify returns false, as expected.\nMaybe your testing framework catches this assert somehow?\n. Sure, we can have an assert for the buffer not being null. Though asking if a buffer starting at address 0 with length 0 can contain a certain data item is in some sense a legal query, since no data is actually stored there.\nI will still merge your PR, since it still fixes the potential loophole of a wrap-around length in the buffer that I mentioned earlier.\n. Yes, just move the CreateString calls above FeedBuilder.\n. The problem is that you're trying to create the string inside the object, it should be created before (move CreateString to above AppStart.\nBut the bigger issue is that the Python implementation doesn't protect you against that with a clear error if you try to nest objects.\n. You do not need push access to create a PR: https://help.github.com/articles/using-pull-requests/\n. I answered you here: http://stackoverflow.com/questions/32759826/how-can-i-rewrite-the-protobuf-scheam-with-flatbuffer-schema\nYou posted this question in 4 places. Next time just one will also get you an answer :)\n. Thanks!\n. That sounds really specialized functionality, can you explain when this is useful, and how others might need this too?\n. Hmm.. @rw, opinion?\n. It be interesting to know why it is so much slower, and what we can do about it.\n. The C++ benchmark code is now in the repo (in its own branch, see comment at the end of https://google.github.io/flatbuffers/md__benchmarks.html\nAnyone interested in integrating benchmarks for other languages, these could go in the same branch.\n. This was already fixed in this commit: https://github.com/google/flatbuffers/commit/feb481661094db2a73912f8e2d32bac9473ca47f\n. yes, the latest from master.\n. Can you paste the actual errors and references to generated code? It's hard to tell what's going on without that.\n. Either that, or describe which two identifiers clash, and where they are in the generated code.\n. Thanks, I guess the constructor arguments should be renamed, maybe .. _important_timestamp ? :) Are you able to make a PR for this?\n. Fixed in https://github.com/google/flatbuffers/commit/ec0c0b14cad95f8e0e5ee21763d0b064254e974d\n. Besides the comments above, yes please regenerate the C# code for the included tests in tests, and make them part of this PR.\nOtherwise looks great, thanks!\n. Sorry, didn't spot that you had updated the PR.\nLooks great, except one issue may be that Unity is an important target in C#, and there's evidence that it does not support Nullable structs correctly for all types:\nhttp://answers.unity3d.com/questions/362799/nullable-struct.html\nCan you (or anyone else) confirm that this code should work with Unity?\nFailing that, can we generate different code? It's nice that ArraySegment is a struct, not a class, so we should ideally preserve that. Can we set the internal array pointer to null to signal its not valid instead?\n. Yes, that sounds like it would work!\nCan you rebase, so I can merge?\n. Ok, I'll wait with the merge.. Thanks @evolutional !\n. Awesome :)\n. Great idea, this is long overdue.\nWhat software does this all need that Travis doesn't come with by default?\nCan we start with a test_all.sh that combines the existing tests?\n. These languages are now all tested by our AppVeyor CI. And since they're not as platform-sensitive as C++, testing them also in Travis would be nice to have but not strictly necessary.. We have a force_align attribute for structs, we could add this for other data as well.\nIt is however not quite as trivial as adding it as an extra parameter to CreateVector, since if someone forgets to specify it (or uses a language implementation that does not have that parameter), you now have an incompatible buffer.\nThis means it needs to be specified in the schema, and enforced from there (whenever you add the vector or string to a table) in all languages equally.\n. Can't promise when, no.\n. https://github.com/google/flatbuffers/commit/07da3fc216c62b18eb13a8bcb9afa95d7c325418\nThis allows you to override the alignment on vectors created in C++.\nThis was already possible in Java (through the alignment parameter on startVector).\n. Yes, we can probably make two versions of those functions, and make sure vector/string call the memcpy version, and the scalar/offset/struct ones call the existing version. Are you able to make a PR for this?\n. They're indeed highly optimized.. for larger blocks. They're so fast because they use alternate paths depending on the size of the block (and possibly alignment), but all those conditionals (and a function call) make it slower for copying 4 bytes.\n. We've been optimizing push for the various use case, this issue should be resolved.. FlatBuffers is designed to be both forwards and backwards compatible, and though the API may have changed between versions, the wire format has not changed since the very first version on github. Any version of FlatBuffers can read data from any other version.\nThat means there's likely something different going on. You say it has \"not worked\", can you be more precise about what is going on? Can you run a debug build of the 1.0.3 reading code and see where it goes wrong?\nNote that while the wire format has always been the same, you are responsible for schema evolution. For example, if your 1.0.3 program was compiled with an older version of your schema, and you then changed the schema in an incompatible way (for example, adding fields in the middle of a table without using field ids), then the serialized data will not be readable by the code that assumes your older schema.\n. What does the commit have to do with your question? Did you find the source of the incompatibility between the two versions?\n. Thanks for reporting this! That's indeed a bug in the verifier. I think I may have just fixed it in https://github.com/google/flatbuffers/commit/9c9fce96c7ce888a6b846612da985f5687bae638 please test again with that version.\n. Great!\n. Thanks!\n. That sounds great in general. @rw do you see any problems with that?\n. Thanks! More tests is super welcome.\nI will say though that the style of the tests that compare byte output of table construction etc are pretty fragile, in that there's multiple valid ways to construct a FlatBuffer, and those tests could break in the future.\nThat said, I see no harm them going in for the moment.\n. The root type can't finish the buffer itself, since the root type may be used in non-root contexts.\nFinish takes the root offset, because part of finishing a buffer is to write the root offset (which isn't simply the start of the buffer).\nThat said, that error you got could have a helpful hint in it, or maybe a way for there to be a check if external code calls Head() on an unfinished buffer.\n. builder.Finish() should be called exactly once, and on the root of whatever tree of data you've just serialized.\n. I added more explicit nesting/finishing checks to C++: https://github.com/google/flatbuffers/commit/d236dea13d2fdb9ad596679868eb4204c1562151\nAnd it sounds like these would be great to have in Python and other languages.\nAccidentally nesting or not finishing things is one of the most common bugs users have.\nAnother example: http://stackoverflow.com/questions/33482163/writing-a-vector-of-struct-with-flatbuffers\n. This issue was improved/fixed by https://github.com/google/flatbuffers/commit/3232727ace325e8cb2732cf208e6d902a1160454\n. A couple of issues:\n- The code doesn't appear to compile, see Travis log (size is not a variable).\n- The code does adhere to the style of the surrounding code, and to the Google C++ StyleGuide.\n- This introduces an if-then, which is undesirable. A better solution is to have 2 versions of push and fill, one that takes a template T parameter for when the size is know (and thus small) and uses the loop, and one for when the size is variable, and uses memset.\n. It can work with small amounts of data, but if your nested tables exceed 64k you'll run into trouble. Also your serialized data is less efficient this way (more vtables, more cache misses).\nSo instead of trying to ignore the error, serialize your data in the correct order, depth-first (post order), i.e. call createStudent before startClassInfo.\n. field stu needs to be a [Student], but you serialize it as a Student.\n. We support Visual Studio 2010 as our minimum C++11 level, and that doesn't have = delete.\n. Agreed, that would be great to have. We haven't gotten around to it. If anyone can contribute AppVeyor CI that be great.\n. That's interesting, have not seen that on any other compiler yet. Does it also happen if you don't cross-compile?\n. There isn't.\nProbably the closest you'd come is to declare a new union in the extension\nschema, that includes the old types and the new types.\nOn Tue, Oct 27, 2015 at 6:29 AM, claymore-minds notifications@github.com\nwrote:\n\nI note the \"union\" functionality in FlatBuffers, which allows you to\nspecify a finite list of table types that can be used in a particular slot\nin a schema.\nIs there any way to have a non-finite equivalent, i.e. for extensibility,\nwhere not all table types are known or declared in the schema, but are\ndeclared in other schemas and are acceptable?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/305.\n. No, the field's default value defaults to 0, and any field instance defaults to the default value :) Confusing, I know.\n\nIf you set a field to 0 whose default value is not 0, then the 0 value will be stored inside the FlatBuffer. I'm not sure what you mean by \"already assigned\".\nWe have a plan to add a has() method, yes, but it's not there yet. For now you'd have to compare against the default value.\n. You generally can't, there's no difference in FlatBuffers for a table in which ClassBuilder::addNumOfStudent has been called, and one which it hasn't. The idea is that you shouldn't need to. The value is 0, regardless of wether it is stored or set. Even if we add a hasNumOfStudent() method, it will return false if the value is 0.\n. @lirui-intel : yes, that's what my previous comment answers. There's no way to tell the difference between a field that has not been set and a field that has been set and has the default value, because they are both not stored in the wire format.\n. This functionality is now in:\nhttps://github.com/google/flatbuffers/commit/1fa803d187c244e8de0bb40f0d3c91f22408bde0\nThis allows you to call e.g.\nmymonster.CheckField(Monster::VT_HEALTH)\nto see if a field was stored at all. Note the caveat I mentioned still applies: you won't be able to tell the difference between a field that wasn't written vs a field that happens to have the default value this way, unless you use force_defaults. This is intrinsic to how FlatBuffers works.\n. Why do you need them to be in a single file?\n. Ok, that makes sense. We already have such a flag for C#, it can be extended to also work for C++ and other languages.\nAny of you able to contribute a PR for this?\n. This functionality is now in, using --gen-all\nSee https://github.com/google/flatbuffers/commit/45bda6e08de1436e8a25e791b776e0bcc38f232b\n. I have absolutely zero experience with PHP, but this looks pretty good from what I can tell. A good amount of tests, make sure you also test the utf8 code that is lurking in there somewhere.\nAnyone out there that knows PHP want to help code-review?\n. Ok, if noone else has comments, I'd say it looks good for merge.\nPHPUnit tests would be good if that dependency is easily available for whoever wants to be running the tests.\nCan you change the -l to --php? I've been meaning to stop the single character option madness, and your commit might as well be the first. I'll change all the other languages shortly, and deprecate the short ones.\n. @rw : it's nice to have, but it is a fragile test, so not required as far as I'm concerned.\n. @chobie Thanks! Once you feel you've addressed @rw's concerns, we can merge.\n. Ok, I'll merge, and any improvements to the PHP port can be follow-up PRs.\nThanks for your contribution!\n. status? @rw?\n. Thanks!\nCould you name it field_value or something though? The \"first\" in there is just because it is the first part of a pair, which is not really descriptive.\n. Great, thanks!\n. We definitely want to support Unity properly.\nSo am I understanding this correctly: in Mono 3.5 (which is C# 3.5 ?) enum types can be initialized with integers and it doesn't know how to parse casts in default parameters, whereas in the latest C# a cast is required?\nI'm guessing the proper solution is for the code to instead look like: CommandType type = CommandType.None ?\n. Thanks, this looks good!\n. You're missing fbb.Finish(monsterPos). getRootAsMonster tries to interpret the start of the buffer as an offset to the root object, which in no cases is valid.\nI'll add an error to fbb.dataBuffer() if it is ever called without the buffer being finished, to prevent these kinds of problems.\n. No, they're actually the same, finishMonsterBuffer calls fbb.Finish, but adds a file identifier if there is one.\n. It does: https://github.com/google/flatbuffers/blob/master/tests/MyGame/Example/Monster.java#L121\n. You probably don't have a root_type declaration in your schema.\n. Lists in FlatBuffers are called vectors, see https://google.github.io/flatbuffers/md__schemas.html\n. Yes, this is how all languages work, and so should JS.\nReturning null for a scalar would be bad, since the field not being present means it is equal to the default, not that it wasn't set.\nFor example, if your field is defined a size : uint = 6, and you explicitly write this field when constructing the FlatBuffer, initializing it to 6, it will not get written, since it is equal to the default. Then when you read it, getting null instead of 6 would be weird.\n. The easiest way to get a null default for an integer field is to wrap it in a struct:\nstruct myint { x:int; }\ntable mytable { scalar:myint; }\nthis will get you null if scalar isn't present. It also doesn't take up any more space on the wire than a regular int.\n. This functionality is now in:\nhttps://github.com/google/flatbuffers/commit/1fa803d187c244e8de0bb40f0d3c91f22408bde0\nThis allows you to call e.g.\nmymonster.CheckField(Monster::VT_HEALTH)\nto see if a field was stored at all. Note the caveat I mentioned still applies: you won't be able to tell the difference between a field that wasn't written vs a field that happens to have the default value this way, unless you use force_defaults. This is intrinsic to how FlatBuffers works.\n. Use IsFieldPresent instead of CheckField: https://github.com/google/flatbuffers/commit/995ee86a6ed1840190d5142089b5014b66d2a13d\n. Thanks. Fix in progress here: https://github.com/google/flatbuffers/pull/312\n. Fix has been merged: https://github.com/google/flatbuffers/commit/37e28d98ea1ef178e32be6acc0c5911a616ba021\n. Looks good, thanks!\n. Yes, we have JavaScript support now.\n. It was originally intended only the root type should have GetRootAs methods, but all of them is ok I guess, since some people may have schemas that serve multiple kinds of FlatBuffers. Also, C++ gets around that because GetRootAs is templated, but not all languages can do the same.\nCertainly all tables in all languages should have a Finish method, since that is what generates the vtable if needed. Not sure what's going on in Go there, @rw ?\n. This was fixed for Go and Python a while back.. I agree that this would be a nice feature. I think doing it in the schema would be better than on the command line, since other code will end up relying on those namespaces.\nNamespaces are also used to refer to things in schemas, and one parsed schema may end up generating code for multiple languages, so it be easiest if there was a base namespace for all of them, then we can use the attribute system in the parser to define prefixes for languages:\nnamespace protocol (java: \"com.company\", go: \"company\");\nIf you're able to make a PR for this, go ahead.\n. Great, thanks!\n. excellent!\n. I agree that this would be nice functionality, since once buffers get really big, collecting things all in one buffer is impractical.\nWhile it would be nice if FlatBuffers would provide this out of the box (particularly if it supported all languages), it is functionality that is FlatBuffer-agnostic in the sense that it is code that can concatenate any set of buffers into a stream by prefixing them with their size (regardless of whether they are FlatBuffers or not).\nAlso means we probably need a stream abstraction (with implementations as file and other things), which is a lot of code I'm not eager to get into.\nThat said, I'd be interested in well-made PRs for this kind of functionality.\n. We now have support for \"size prefixed FlatBuffers\" (in C++), which at least make this kind of streaming / concatenating easier. I do not foresee that we'll actually want to deal with file operations directly in FlatBuffers.. If the arduino supports a recent C++ compiler (e.g. GCC 4.8.2 or better), that should work.\nAs for minimizing network overhead, FlatBuffer is a lot more compact than JSON, but slightly less compact (but faster to read) than say Protobuf.\n. Sorry, these commits already got merged. Still curious how this will affect Windows though.\n. Thanks, that's the same as this issue: https://github.com/google/flatbuffers/issues/335\n. Yeah sorry, they're related but not the same.\n. Fixed in https://github.com/google/flatbuffers/commit/644bcbde9111cd0c17e721b78ae8b59fc35306a4\n. Ahh.. this is a related fix to https://github.com/google/flatbuffers/pull/312 but not quite the same.\nSince this changes the test generated code, can you please ensure to run flatc with these changes, which should make the generated code show up as changed in git, which you can then add to this commit. Ensure the tests run with this new code, of course.\nThanks!\n. That be great!\nIf you can, also test Java still generates the same code as before, as it is potentially affected.\n. Thanks for all the refactoring, that seems to be an improvement. Some small issues though.\n. Looks good now, thanks!\n. Actually, I had already recommend that we use structs in C# for accessor objects (e.g. everything that derives from Table and Struct) since they only store a ByteBuffer reference and an offset. This would be a great way to reduce on garbage generated, and would allow us to reduce the accessors from 2 to 1 (see e.g. Pos vs GetPos in Monster).\nFor some reason this never got implemented, so yes, a PR changing this would be great!\nIt will break the current API, but I believe that's worth it going forward.\nBe careful as it will diverge from the shared code with Java.\n. We don't have an official policy. My vague policy so far has been to allow API breakage if it makes a significant improvement to how FlatBuffers is used in that language. It will briefly annoy existing users, but in the end will be better for everyone.\n. @nietras Agreed. Note that for the moment, you can reduce gc churn by reusing your accessor object (the Get version of the accessor).\n. struct support was added a while ago.. Thanks for looking at this. The two features were made by separate authors and not considered together, so yes, this is indeed an issue.\nIn terms of violating the code standard, I'm not too worried there, since it simple_allocator is effectively equivalent to POD, since it has no instance variables and empty constructors/destructors.\nNote flatbuffers.h is \"header-only\" functionality, so this \"global\" should probably be static at file level?\n. Thanks for fixing this!\nThing is, this fixes the symptom, but not the cause. The bigger problem is that the if-statement at the start of Parser::Parse conflates include files and source files in its logic. It's all a bit messy.\nBetter yet, if you look at the uses of files_being_parsed_, it is meant to be a stack, but it isn't ever used in a properly recursive way (Parse calls itself recursively, but just restarts the function rather than continuing). Thus, the easier/cleaner fix may be to turn files_being_parsed_ from a vector into a string, and set it unconditionally at the start of Parse. Sound right?\n. This bug was popping up elsewhere too, so had to fix it:\nhttps://github.com/google/flatbuffers/commit/6267f8c6c002cb62d168fa7b7c6137e65f47b8a1\nThanks for reporting!\n. Thanks.. a corner case we clearly overlooked :)\n. Hi there! Cool you're working on this.\nFirst, it be good to collect your commits into a PR, which makes it easy for others to see what code is new, and to comment on it.\nAs for C++, I can answer specific questions, but for this to be successfully merged into master, we need language port authors to have a certain level of \"confidence\" over their understanding of the FlatBuffers internals and implementation to be able to write a correct generator. There are a lot of details to implementing FlatBuffers correctly.\nYour generator should probably be part of idl_gen_generic.cpp, since I am assuming Kotlin is somewhat similar to Java an C#.\nTesting is something you can make language specific, i.e. there is a script (generate_code.sh) that would now additionally generate Kotlin code, and should be tested as thoroughly as possible. Look at other languages for inspiration.\n. Differences can be resolved in idl_gen_general.cpp, for example, to resolve the type order in declaration, simply make a GenDecl function that takes a type and a variable name, and outputs them in the correct order for all languages, and call that from whereever needed. Double check by running the Java/C# generators that their code hasn't changed.\nIt's important that we keep idl_gen_general.cpp with as few if-thens as possible (if-then's isolated in functions). It's also very desirable to  have languages like Kotlin in there rather than in their own .cpp, as the more different generators we have, the more of a maintenance nightmare it becomes. In fact, I'd love to merge the Go generator back into the general one at some point. Opinions, @rw ?\nAs for wire types vs others, the types declared in idl.h for Java are the wire types, which is byte in case of bool, etc. Even if the function is called addBoolean, that is just for convenience/clarity, it still will store a byte in the buffer. What Kotlin does in terms of user-facing types is up to it, I presume it has the same lack of unsigned types that Java has?\nWhich brings me to another point, if Kotlin can call Java code, does it make sense to have it call into the Java runtime types (FlatBufferBuilder etc.?) It be great to reuse that code, greatly reducing the chances of errors in Kotlin.\nIf you feel there's something inconsistent happening in Java, you can clean that up, as long as the functionality remains the same. Though if it breaks the API, that be better left for a followup commit.\nDifferent syntax for casts: again, make a function that generates a cast, and does something different in Kotlin. Reduce the difference in languages to one single function.\n. You can make fixes to the same PR, rather than pulling it when you find a problem. This will allow people to start reviewing your code.\nto 2) above: I didn't suggest making Java the Kotlin API. I suggested just using the Java internals. For example, FlatBufferBuilder.endObject is a complex method that is not part of the user-facing API, so could be called by Kotlin code rather than reimplemented.\n. Thanks for those improvements.\nThat seems like a good approach. It'll be interesting to see if we can get other Kotlin users using your implementation.\n. That means there's a bug in your writing or reading code. Care to post snippets?\n. That's because you're calling .array() which gets the backing array (i.e. the whole buffer) of ByteBuffer. Since the vector you're trying to access is only a part of the parent buffer, privateKeyAsByteBuffer() returns a ByteBuffer that refers to only part of the array (i.e. its position will typically not be 0).  You'd have to use get() instead I think.\n. Thanks!\n. I presume the error here is that it doesn't know what Foo is?\nAre you able to make a PR for this? Or maybe @evolutional or @jonsimantov ?\nLikely the C++ generator handles this better, so worth looking at for inspiration. I wonder if it affects Java as well.\n. Thanks Oli!\n. @rw will know for Python/Go. @chobie for PHP.\nThe C# code is only shared with Java, not Go.\n. partial namespacing would be nice to have, but it isn't absolutely essential if it only applies to names outside of the current namespace I would say.\nJava namespaces work similarly, yes: http://stackoverflow.com/questions/9249357/difference-between-namespace-in-c-sharp-and-package-in-java\nJust seeing if the code compiles should be enough of a confirmation.\n. Thanks! @rw ?\n. @rw is the author of the Python port, and my Python knowledge is minimal, so I'd prefer him to ok it first.\n. Ah yes, forgot about that.. much appreciated!\n. Looks good to me, @rw ?\n. @rw?\n. status?\n. If you mean whether it is possible to have a single FlatBuffer data structure stored in multiple separate blocks of memory, the answer is currently no. Because everything is done through offsets (and some are shared), tracking and changing them all as their physical addresses change is non-trivial.\n. Thanks for the fix for str(), that is indeed problematic!\nAs for windows.h, we had a discussion here:\nhttps://github.com/google/flatbuffers/pull/215\nWhat do you think? Would using a warning like I suggest be better, or?\n. Ok, I guess we'll go with your solution for now. I strongly feel that people should not be polluting the global namespace with macros, but I guess it is hard to control.\n. Looks good!\nThis also adds new schemas, which adds a lot of files... but I guess that is necessary if we're to test that this works. Seems though that this is new code is not referenced from the tests in the various languages, though?\n. I'd say if this CL causes new files to be generated they should be included, because otherwise they end up in whatever next commit that is even less related.\n. Can you rebase?\n. Thanks!\n@rw : this introduces new generated code in all languages, I'll add tests for C++, maybe you can add them for Go/Python?\n. Thanks!\n. Yes, that's intended.\n. Thanks! Can you please rebase.\n. This PR seems very out of sync. Please re-open if you can reduce it to just the original issue.\n. @rw ?\n. Thanks. Can you please rebase.\n. Thanks!\n. Typically, if you want to be able to store multiple type of messages in a buffer, and you can't know from external information which it will be, you use a union.\n. Can you please rebase.\n. chobie@ any comments?\n. Thanks!\n. Can you please rebase.\nCommit looks good. I presume this was ported from Java?\n. Thanks!\n. We don't use generics in Java, because wrapping an int offset in an Object is very expensive, not a cost we want to pay. C# has by-value structs which makes this kind of wrapping cheap. If Kotlin implements generics like Java, I'd advice against using them.\n. First, lets keep the task of writing a Kotlin implementation separate from other tasks, such as factoring out general code generator functions.\nI don't think we need something like CodeWriter, it just wraps the current string concatenation for no obvious gain. Lets keep things simple with a very minimal, incremental transition.\nAlso, the formatting and indentation of the C++ code is all over the place. Maybe run clang-format (with the Google style) on it?\n. That may well be, but we have a lot of generators, and we want a transition path that is as incremental as possible. Your class is going to be a very intrusive change.\nWhat did it make worse? It enforces the Google style guide, which not everyone likes, but this project adheres to.\n. I suggested not using CodeWriter at all at this point. I would focus on just getting Kotlin right, and leave any general code generator refactorings till later commits. Or, suggest a refactoring for the existing generators as a PR that is completely independent from Kotlin, if you prefer.\n. \"starting from commit 40ad6f1\".. can you make the PR exactly what you're trying to merge, so it is easier for me to review?\nAgain, not an expert in Kotlin, so will only comment on general structure. Would like to have the ok from a more expert Kotlin user before merging.\n. I would guess a lambda is not inlined if it is passed around by value.\nSafety is a very worthy goal. In the case of FlatBuffers, efficiency trumps it though.\n. Thanks! Fixed here: https://github.com/google/flatbuffers/commit/b0d5bb1c4bafda99ffe37c2c304453f6ed45707c\n. The construction methods were placed in the same class, because they were static (avoiding an object needing to be allocated) and needed a home somewhere :)\nI know in C# we can make MonsterBuilder a struct, so it comes at no cost, but in Java we can't. Do you propose further specializing the code for these two languages?\n. Agree that this is nicer code. It would be another breaking API change though, and not necessarily a popular one, since people would have to change all their Monster.addName() calls to MonsterBuilder.addName() which is longer.\nAny existing Java/C# users with opinions?\n. Agree that this is nicer code. It would be another breaking API change though, and not necessarily a popular one, since people would have to change all their Monster.addName() calls to MonsterBuilder.addName() which is longer.\nAny existing Java/C# users with opinions?\n. Ok.. any further opinions? Anyone out there using FlatBuffers that would be seriously inconvenienced by this change?\n. The problem with a command-line option is that most people won't enable it, just deferring the problem.\nWe could have an option that enables the old-style code, but if you have time to add this option, you probably have time to fix your code. In fact, compile errors will be the first thing you see, and likely you'll fix those before you even find out there's an option to avoid them :)\nAlso having 2 paths in generated code complicates testing etc.\n. Yeah, I don't see the advantage of nested classes here either.\nAlso, for Java at least, I'm not a fan of the builder pattern, because that has to allocate an instance chain onto. The current functions are all static so don't require an instance.\n. If there's no further objections, go ahead with a PR :)\n. Nice improvement, but one issue: what is the efficiency of passing strings like that in Java in particular?\nJava does string pooling, but the way it appears to do that is to call String.intern(), meaning it would first have to construct a String object for that constant before it finds out it can be shared.\nIf this code causes any allocation, then I think we shouldn't merge it?\nhttp://stackoverflow.com/questions/3297867/difference-between-string-object-and-string-literal\n. Not too worried about memory usage, only about the amount of work being done to \"create\" that string reference.\nCan keep this open to see if anyone else has opinions.\n. Status?\n. Just uploaded a fix for this, thanks for reporting!\nhttps://github.com/google/flatbuffers/commit/6fba6b6e714c5b948e6b0c64d5157710668f302b\n. What about this commit? It seems to be missing the generated code.\n. ok, let me know.\n. Yes, would be good to add such a field. Also regenerate code for other languages when you do that, and run tests if you can.\n. status?\n. Thanks!\n. Yes, defaults are not quite as useful for floats as they are for ints. They are very useful for when the value was never even written, but not when it was computed.\nI'm not sure if we should be in the business of doing epsilon checking for the client. It could be convenient at times, but also produce unintended rounding, or cost performance.\n. Yeah, not sure why it is a double.. that may be considered a bug.\nDefaults for floats are a thing in all implementations, so I'd like to leave them in. Also think they're useful, as for any data where for example 0.0f is a common value, it will save space.\n. FlatBuffers is a completely general serialization solution, so anything can be converted to it, as long as you write a fitting schema for it. I am not familiar with OpenCV, but you could represent a matrix as a vector of floats, for example.\nSimilarly, FlatBuffers produces a byte buffer, so sending it through any protocol is possible.\n. @FrankStain : not sure where you get \"It's not recommended to be used in network interaction\", there's a lot of people out there using it exactly for that. Its wire size is typically bigger than protobuf, but the efficiency gained in packing/unpacking often offsets that.\n. @FrankStain np :)\n. Any idea @rw ?\n. Generally looks good!\nSo what I did see is that the unsafe implementation generally quite as much faster as I'd hope. So from that perspective, you could argue that leaving out the check is good, because people using the unsafe version are seeking maximum performance beyond the safe version. And they can always debug using the safe version.\nBetter yet, you can maybe document these defines, leave checks by default on, and say hey, if you want another 15% speed at the cost of checking, disable this define.\n. squashing not necessary anymore, I merge on github nowadays :)\n. The existing projects that are there work with VS2010, will this affect that?\n. Yup I guess that's fine.\n. Yes, this still seems useful as long as it keeps working with 2010 out of the box. I guess one additional worry is that this means if we change the project we need to ensure to change all of them?\n. issue: https://github.com/google/flatbuffers/issues/1741\n@rw ?\n. We're actually working on restructuring the documentation at the moment (with a tutorial that shows how to do things in all supported languages), so you may wanted to wait until that lands to contribute further improvements (which would still be much appreciated!)\n. Do we still want this?\n. Seems there is a compile error on linux.\n. any progress?\n. status?\n. Thanks!\n. That code (and possibly the original code) has two problems: constructing tables inside of the vector (they should be constructed before) and calling b.Finish() 3 times (it should only be called once, for the root object).\nI believe @rw has added checks to avoid these kinds of errors from happening, but they may have landed after 1.2.. can you check with tip?\n. Good point. I'll make another tagged release soon.\n. Thanks for reporting this! That was apparently not being tested at all.\nI just committed a fix: https://github.com/google/flatbuffers/commit/189153723649d3b6ad56f7b83662e30c480e124f\n. Yes, this functionality is sorely needed, thanks for the PR!\n. Awesome PR! Could you add a command-line option to flatc for it as well?\n. Apologies, but this PR will now have quite a few issues, as we just switched the parser to not use exception handling.\n. Very nice commit, thanks!\nYes, you're welcome to remove the lambdas, they're not used that much anyway. You should be able to declare the function objects directly next to where they're being called, which keeps it somewhat readable.\n. While I really appreciate the effort you've put into improving the API, I share similar concerns to @mfcollins3 \nWhat I would have liked to see is to make the existing Struct and Table into C# structs to get the gain of no GC, and all other changes (certainly ones that break user code) as absolutely minimal as possible. I understand there are advantages to these interfaces, but breaking as little as possible is a lot more important.\n. Yes, I'd love to hear opinions from other C# users. It may well be that your solution is already close to optimal given the use of structs.\nDiscussion is easier with this PR open though, I think.\n. I'm ambivalent to sharing things with the Java version. idl_gen_general.cpp was originally made since at the start the two were very similar, and it was hoped that further languages (Go etc.) could be merged into this implementation.\nNow we're at the stage where besides Java and C#, none of the other languages are sharing code, and Java and C# keep diverging. I'm wondering wether it is worth splitting up into two generators, as that would allow us to remove all the language specific if-thens, inline a lot of stuff, and thus simplify both. Then, the C# bytebuffer can be allowed to diverge from the Java API, and possibly be made simpler / faster, or its functionality inlined.\nOpinions?\n@rw : as maintainer of 2 more languages, how do you feel about this mess of code-generators I've gotten us all in? :)\n. Yeah, I think the amount of work to merge other languages into general will be very extensive (and error prone).\nI'm fine with splitting things up. It will make further work on improving either language simpler.\nThough maybe that should be a separate commit/PR, to keep the changes reviewable?\nMy main concern in C# is a) efficiency, b) not having to break the API again any time soon after this breaking change. @belldon and @evolutional can you come to a consensus about the exact class layout?\nYou can either make this PR go in largely as is, then do a splitting up PR, then do further work on C# alone, or if this works better, do a split up PR, and then rework this PR into the definitive C# API change.\n. Agreed. Sounds like we should do the split first. Anyone fancy doing that?\n. Sounds like a plan. @evolutional do you want to take the lead on this?\n. It is part of how FlatBuffers works that all scalars have a default value, even if not specified (it's 0), so short? would not make sense.\n. This still seems a worthwhile idea. Anyone want to have a crack at finishing this PR or write a similar one?\n. Ok, this PR was merged: https://github.com/google/flatbuffers/commit/52ca75506abd82b5616bdef4d28e9535262c1d65\nIt has accessors as structs and other improvement, though still can probably be improved further.\nIn the interest of not annoying C# users with a broken API multiple times, if any of you has ideas on how to further improve things, lets do PRs for them sooner rather than later.\nAs I've mentioned, I will be separating C#/Java generators to make future changes easier.\n. Nice idea. One things: please make these functions use C types and C linkage, that way it will be easier for other languages to use these same bindings in the future.\n. Very good point @evolutional, we definitely want regular FlatBuffer users to keep using C# without the build complexity of linking in C++ :)\n. Status?\n. sounds good.\n. This looks great! Some small comments. Also looks like CI is failing.\nCan you add a small section to the C# docs explaining how to use this functionality?\n. This looked like a pretty promising PR, are you going to finish it @chobie ?. Yes, there is already a nested table in there (field \"test\"), lets not new test files when not necessary.\n. Thanks!\n. What test fails? Did this PR really break something, or is the Python test overly conservative?\n. fixed: https://github.com/google/flatbuffers/commit/519dcdd0b2dbdec00e72a6be1894b850ce419955\n. This would be a nice addition, yes. See my comments about required fields. Also, make sure only custom attributes are written, any standard attributes (e.g. deprecated) should not, as that information is already in represented elsewhere.\n. Could you update this PR?\n. This functionality was already added here: https://github.com/google/flatbuffers/commit/72fc45aa6acbc11052c6baa462fac26c5075392a\nSee test.cpp for an example.\n. The actual C++ commit that enables it is https://github.com/google/flatbuffers/commit/995ee86a6ed1840190d5142089b5014b66d2a13d\nSince CheckField is privately inherit, the actual function is called IsFieldPresent, so maybe use the same in Go?\nNot sure why you're generating Has functions for each field, since you now have field constants, it is much more economical to simply call a single IsFieldPresent with one of these constants like in C++, especially since I consider this functionality to be quite peripheral compared to the other generated methods. Infact, I'd generally like to discourage their use.\nAlso, not sure why you added 2 fields to the schema, there's enough existing fields to test with?\n. Ah, sorry, didn't spot that.\n\"set\" is somewhat ambiguous, since you can \"set\" a field, which then still ends up not physically \"present\"  in the buffer. I personally wouldn't associate it with a field definition.\n. Status?\n. Correct, it is just the size of inline elements. Will fix. Wouldn't call this \"wildly inaccurate\" though :)\n. Some improvements to the internals doc: https://github.com/google/flatbuffers/commit/4e4a5142fb2d50c08856b3c3292bcf9c649ed2e7\n. hash.h is for automatic hashing of strings (string in JSON -> hash in FlatBuffer).\nCurrently, the recommended way to send a hashtable (or any kind of key/value data structure) is to create a vector of tables (that contain both key and value), and use CreateVectorOfSortedTables. The recipient can then use in-place binary search for lookup (see C++ docs).\nA proper hash table would be even faster of course, and I see no reason why you can't encode one in a vector, but you'd have to do it yourself at this point.\n. I do think this is a concern outside of FlatBuffers, i.e. the need for checksumming is independent of the construction method of the data transmitted.\nBut yes, sending a crc32 or similar along with the buffer sounds like a good way to start, depending on your needs. The verifier guarantees the buffer is safely readable, not that scalar data hasn't been corrupted.\n. Default should definitely be UTF-8. Looks good, @rw ?\n. Thanks!\n. Could you make that FLATBUFFERS_NO_ABSOLUTE_PATH_RESOLUTION to be sure?\nAlso indentation of # statements and the return statement do not correspond to the rest of the code.\n. Looks good, thanks :)\n. Thanks!\n. Not sure if I want VS2012 (and 13 / 15) projects, as updating them all becomes a hassle. What is the problem in just opening the 2010 project in 12 when needed?\n. Yeah sorry, I think I want to just maintain a single project.\n. If it is not too invasive I don't mind this being added.\n. Looks pretty good. Added some comments on how to improve it further.\n. Also, if you look at the travis output, there's errors related to your global redefinition of constexpr/nullptr. All the more reason to make sure these ONLY cover FlatBuffer code, not the standard headers, to ensure it compiles on newer gccs.\n. Sorry for the delay, was on holiday. Thanks for the fixes, looks good now!\n. This is not a bug, and is actually documented for the id attribute: https://google.github.io/flatbuffers/md__schemas.html\nIt is however extremely non-intuitive, and the error could be improved.\n. Yes, renumbering ids would be fragile in some cases. I think making the error checking/reporting more explicit is the way to go for now.\n. PR merged, thanks!\n. PR merged, thanks!\n. Can you maybe give an example, as I don't understand how making any of the FlatBuffers generated classes partial would be useful.\ncc: @evolutional \n. I personally don't understand why you'd want to add code to generated accessor classes (as opposed to refer to them), but I am not against making that possible, if people feel strongly about it.\nOne issue is that our accessors are meant to transition to being structs, does that affect this at all?\nAs for language specific options to flatc, that is not ideal, but allowed. Since this might differ per table, why not use FlatBuffers metadata system to mark individual tables as partial, and have the code generator check that?\n. Yes, maybe call the tag csharp_partial or something like that.\n. I'm not against a command-line flag either, it seemed that using the attribute has the additional advantage of being able to select on a per-class basis. Your call :)\n. @evolutional that looks great to me.\n. Support is in, thanks @evolutional https://github.com/google/flatbuffers/pull/3742\n. @rw ?\n. I guess this is abandoned. Anyone want to pick it up?\n. This is definitely an un-addressed problem.\nWe do want to keep generated accessors to be able to use the schema identifier (type() and not get_type() or whatever). I also would not want to restrict what you can use in one language because it happens to be a keyword in another language (e.g. it is pretty common in C++ to have a type field in C++ because it is not a keyword, and would be weird to not be able to use it, especially if you don't use Go.\nA more robust solution would be to escape keywords in each language individually, by e.g. making the generated code for type to be type_. Care must be taken, since _ is already used in some cases to escape identifiers (e.g. for struct constructor arguments in C++), so escaping them would result in a double escape: type__ etc. :(\nWe could additionally add a global blacklist for keywords that exist in the majority of languages (like if), though if we have a solid escaping function in all supported languages this may not even be needed.\nI'd welcome a PR for this.\n. Yup, those lists look like a good starting point. We'd have to make them into code tables that can be added to flatc, then create an Escape function per language that we wrap around any use of .name in any of the code generators.\n. Thanks! const std::vector<std::string> would be better off as const char *[]. Are you doing a PR?\n. const std::vector<std::string> would allocate a whole bunch of objects, whereas const char* ...[] is allocated purely statically.\n. yes, that sounds like a good idea.\n. identifiers starting with numbers are already not allowed.\n. Fixed elsewhere.. thanks :)\n. Thanks! Useful addition.\n. Nice, thanks.\n. Thanks!\n. How does this work? Where is FLATBUFFERS_FLATC_EXECUTABLE defined?\n. Ah ok, makes sense.. thanks!\n. Nice!\n. Hmm, TableKeyComparator has been in there for a while, why does it now suddenly give a warning? This is with the included project?\n. I appreciate the feature. The big problem is that it uses variadic templates which would break our support for Visual Studio 2010.\n. You shouldn't have a need to use the field id directly. What line in the test are you referring to, and what exactly are you trying to accomplish?\n. The explicit id test on line 318 is just for testing, you should never need this yourself.\nYes, highly recommend just using a vector.\n. Thanks!\n. It is intended that you call GetInAppPurchase always if you care about performance.\nI guess the reason that isn't stored in a class instance variable for you automatically is because there's potentially a lot of them (especially when you factor in vectors). And now we'd need a mechanism to set all that class back data back to null (or leak that data).\nWorse, the above mechanism isn't compatible with processing multiple FlatBuffers, as you'd still be pointing to the first when iterating the second. You could fix this somewhat by always calling GetInAppPurchase even when the object already exists, but that then in turn wouldn't work with concurrent access to 2 buffers (either interleaved, or in threads).\nThe problem may soon be moot, as we're expecting to replace accessor objects in C# with structs, which should be cheap to generate an copy always.\n. Yes, that's indeed what I'm describing.\n. There it works because you've made the cached value an instance member as opposed to a class instance member.\nTo make this work consistently though, you'd need cached copies of all references, including when you have a whole vector of them for example. That's a lot of data we don't really want to store.\n. Yes, I can see there's cases where #3 works well.. but for the sake of keeping the API managable I don't think we should support it (opinions welcome). #1 and #2 cover the spectrum of use cases pretty decently.\n. That was probably intended to be +=. Same for the Python version. @rw ?\n. Thanks!\n. The types are chosen such that they correspond closest to the FlatBuffers binary encoding.. though there would be nothing against the API convert to bool after it was read. The choice of []byte is likely because of efficiency, @rw ?\n. Thanks.. though I'm not sure how this would be used, since a lot of people use versions that are essentially a particular commit on github, where \"1.2.0\" would be incorrect, or at best meaningless. We do not want to have to increment the version number on every commit. A version number automatically derived from the commit would be great, but that seems impractical. __DATE__ is another option that is pretty imprecise. Opinions?\n. I understand versions are useful. I am asking what form that should take, since calling the versions at or near current HEAD \"1.2.0\" may be misleading.\n. I see. Just if an installed flatc happened to be 1.2, and HEAD would also be 1.2, then the conflict still wouldn't be obvious.\nI guess if I make versioned releases more often then that would help :)\n. Ok, so maybe \"1.2.0 (\" __DATE__ \")\" to help differentiate multiple builds?\n. Besides flatc for Windows, we don't do official builds, so HEAD can be left out I think.\n. sorry, one more change: could you remove -v ? It is often used for verbose, and having --version should be sufficient.\n. thanks!\n. To celebrate the addition of --version, I just made a 1.3.0 release: https://github.com/google/flatbuffers/releases\n. Why was this closed? Is this not needed anymore?\n. The reason all of the tests are bunched together is because they share\naccess to a schema and in some cases to test data (e.g. Java reads the\nbinary generated by C++).\nAnother thing is that I don't see why another project would want to include\nFlatBuffers.Test. Test code is meant to be local to the FlatBuffers\nproject, whereas the language directories are meant to be what you refer to\nin external projects.\nOn Wed, Feb 3, 2016 at 11:58 AM, Oli Wilkinson notifications@github.com\nwrote:\n\nIt makes me wonder if we should consider adopting this pattern for all\nlanguages, that way the Java/Go/Net and so on will have their ports in a\nsubtree of the project, which would make dealing with it a little simpler.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/3753#issuecomment-179429188\n.\n. I do like the idea of de-cluttering the test directory.\n\nI guess I really dislike build systems that have hard requirements on\ndirectory layout, since that makes it impossible to please everyone. But oh\nwell..\nIf you think building for NET Core is going to be useful, or if you think\nthat this generally is going to be a superior experience for C#, then go\nahead.\nWe'd need to make sure the docs are updated to reflect this:\nhttps://google.github.io/flatbuffers/flatbuffers_guide_use_java_c-sharp.html\n(come to think of it, now that we have the tutorial, it be good to split up\nJava and C# here too.)\nOn Wed, Feb 3, 2016 at 1:27 PM, Wouter van Oortmerssen wvo@google.com\nwrote:\n\nOn Wed, Feb 3, 2016 at 12:45 PM, Oli Wilkinson notifications@github.com\nwrote:\n\nUnderstood. From a .NET point of view, it would be a case of relocating\nthe code in /tests/FlatBuffers.Test and /tests/MyGame/*/.cs to the /net\nfolder to live alongside the existing /net/FlatBuffers/ folder. This is\na very common pattern in existing .NET library projects (the src/ and\ntests/ setup) and is essential for building the test project under Net\nCore.\nThe ideal distribution method for .NET would be to pack and publish a\nNuGet package from the main net/FlatBuffers/ folder. This wouldn't\nchange in either setup, it's obviously more geared up for it in the\nproject.json world.\nI'll keep trying to see if there's a way to make it work under the\ncurrent structure, but from what I've seen so far it's not likely to work.\nOf course, we could also say that NET Core isn't a current or desired\ntarget, in which case this question can be closed without further\nconsideration.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/3753#issuecomment-179456336\n.\n. > I'm trying to work around it - it's a known issue (dotnet/cli#966\nhttps://github.com/dotnet/cli/issues/966). Looks like the version of\nNET Core I'm on (RC2) is still a touch too early. I'll close this and come\nback after it's moved past the RC stage. It's been a worthwhile exercise\nthough.\n\nOk.\nAs for splitting up the docs - good idea. I'm also playing around with\nsome code locally to split the C#/Java generators into a 'general' base\nclass & 2 specializations.\nIf this can be simple & elegant, that's another option. If it ends up just\nas complex as the current table/if-then approach, but with more method\nindirection, then I am not so sure if it is helpful. As much as I hate code\nduplication, it is an option too.\n. What version of what compiler is this with? cursor_ is a const char * which is typically signed, you must be compiling in an environment/platform where it is unsigned.\n\nTry compiling with -fsigned-char.. if that fixes it, then maybe we need to add that to our build flags to cater for your platform.\n. If that fixes it, feel free to submit a PR for it.\n. It doesn't need to detect anything, since -f-signed-char is needed on all platforms (doesn't hurt setting it).\n. PR still coming? :)\n. Whoops :)\n. Yes, we have an intention to add checking for these kinds of clashes, but it hasn't been implemented yet.\n. Fixed here: https://github.com/google/flatbuffers/commit/8a58aafda12bd1b9f7f228dc5f0583cf4f4afe87\n. @rw?\n. We already have a composer.json in the repo. @chobie, did you already set this up, or should I add it to the above site?\n. I'll go find out who at Google owns that..\n. It's up: https://packagist.org/packages/google/flatbuffers\nIt is not auto-updating. I can set up a github hook to make this automatic, but I am not sure how desirable that is, in terms of versioning?\n. Ok, hook set up and appears to be working, thanks guys!\n. C-style unions would indeed be quite different from our existing union feature. I see their utility, but I'm a bit apprehensive to supporting two kinds of union features.\nNote that if you want to go to this level of efficiency, you can do so pretty easily in C++ at least, by simply defining the union yourself:\nunion { double d; struct { int a, b; } }\nYou would then define the schema purely in terms of the double (because it is a single element and would guarantee appropriate alignment), but whenever you receive the value, cast it to this union, and access whichever you wanted.\n. Thanks!\n. Thanks!\n. Did you resolve this? What was the issue?\n. This may help: https://github.com/google/flatbuffers/issues/3781\nThis needs to be better documented.\n. I have a commit that fixes it, should land later today.\n. Here's the fix, thanks for reporting: https://github.com/google/flatbuffers/commit/20c0082ee5bfeeecaa443c001a89934e9448ffa4\n. Yup, those were added recently: https://github.com/google/flatbuffers/commit/e848137ded3524c60faa6def429807faae5340e0\n. Can you add the new generated JS code to your commit? (run tests/generate_code.sh).\nOtherwise looks good to me. @evanw  / @evolutional ?\n. Ok, I guess noone has any complaints, so I'll merge. Thanks!\n. We have something that does this already, see reflection::CopyTable. Using reflection is potentially slower than if this were implemented using generated code, though if the frequency of such copies is low, it may be fast enough for your use case.\nAs you noted, the fastest will always be to use nested flatbuffers, but maybe not as elegant.\nI'm also open to the idea of generating code for this. Since it is not a frequent use case, we'd probably make this an option to flatc.\n. Ahh.. yeah, either porting the reflection code to those languages, or providing generated copy code for all languages is a lot of work. PRs would be appreciated :)\n. I am very much in favor of refactoring the code generators to share more code.\nPing other codegen authors @evolutional @evanw @chobie\nI think a way to transition there incrementally would be:\n- Create a CodeGeneratorContext that contains the Parser reference and the filename etc you mention.\n- Stick that in its own header file included by all generators.\n- Pass it to the various generator functions instead of the separate arguments we have now.\n- Move useful language-agnostic helper functions into that class, and have them be called by all generators where possible.\n- I would not suggest dictating inheritance from this object. If we inherit, we now need a header file per generator, which is visible to whoeever instantiates these generators, and it all gets a bit messy. Instead, the generator gets to choose how to to use this class, it can either pass it around like the Parser is currently, or it can create its own context class locally to the .cpp to hold all generation functions. Alternatively, if inheritance is desirable, we could do this local to the .cpp, and have a single instantiation function that returns a baseclass pointer.\n- Of all generators, the C++ one is the most feature complete, so should generally be the one to take code from to move into this class. The Java/C# one is also a good choice.\n- These refactorings should be somewhat safe, since all generated code is in git, meaning it is easy to check if the refactoring caused any of the output to change.\n- We should go for the lowest hanging fruit first (in terms of the easiest to de-duplicate). To remove redundancy 100% between these generators would require endless amounts of callbacks and such, which hurt our ability to evolve the individual generators.\nNote that there's a few other concerns that relate to such a refactoring:\n- We had decided that we maybe finally should split up the Java and C# generators. This will remove a lot of conditionals and simplify these generators, but of course generate two copies. The above mentioned refactoring would help clean up some of that. The question is, which should happen first? I'd think first split these two, then do the above refactoring would be slightly easier, though maybe there are generalizations in this particular generator that can be carried over to all languages.\n- Most languages deal with generating namespaces wrong, which will have to be fixed. Maybe though this can be a side effect of this refactoring. I just fixed it for C++: https://github.com/google/flatbuffers/commit/20c0082ee5bfeeecaa443c001a89934e9448ffa4 . Basically, the issue was that whatever the last namespace is is the default namespace that doesn't require any qualifiers, which completely screws up in the case of a file that has multiple namespaces inside of it. Note the namespace test has TableInC in namespace NamespaceC that most languages output in NamespaceA. If the way we handle namespaces is adopted from C++, that will fix it for all.\nOpinions? Anyone in particular want to have a first stab at something?\n. @Lakedaemon : I called my class a \"context\" because I didn't feel it should be a baseclass, but it is otherwise similar to your baseclass idea. Like I said, I am ok with code generators subclassing this shared class if that happens inside the .cpp, that way each generator can do it in their own way (it's important we can transition to this new system language by language).\nThe differences in generators should stay in the generators, i.e. any differences in how things are generated can be parametrized by arguments in the helper functions, but we don't want any if (lang == cpp) if (lang == python) stuff to creep into the generic code. If things are really too different, then I guess they won't share any code.\nSo far we don't have multiple language versions. We're really targeting lowest common denominator feature sets on most mature languages, so it can easily be used by all.\nHow individual functions look I don't know. I see this as an iterative process: recognize a code pattern we have in many generators, pull it into a nice helper method, apply to as much generator code as possible, test generated code still the same, repeat. I don't have a top-down \"design\" in mind.\nI'd recommend keeping callbacks to a minimum, as that will be hard to follow. I was thinking that the flow of control still belongs to the generator, and the context class is what is being called into. Maybe top-level callbacks like what @evolutional suggests can work, I'd just worry that different languages want to write entirely different things, or in a different order, and that this kind of structure becomes more of a burden than a help.\n. Thank you both for helping out.\nPlease make these refactorings as incremental as possible, i.e. prefer many small commits/PRs as opposed to one huge one. Easier to code review, and guarantee that it makes sense.\nAlso makes sure to double check your refactoring doesn't change the generated code. This should be easy as generated code changes show up in git diff. be sure to run generate_code.sh.\n. This was just fixed in this commit https://github.com/google/flatbuffers/commit/20c0082ee5bfeeecaa443c001a89934e9448ffa4, thanks for reporting!\n. @evanw ?\n. Merged, thanks!\n. To understand your issue, where exactly is it failing? On LoadFile or Parse?\nIf LoadFile, where are these dynamic files stored? LoadFile is not android specific (using normal C++ file loading), so it may not be able to find anything outside of assets. You'd be best off loading this file yourself (in Java) and then passing the file contents instead of the file name to C++.\nIf Parse, what is parser.error_ ?\n. Can you paste the JSON so I can see what the Parser is having trouble with?\n. Are you using FlatBuffers 1.3.0 at least? Parsing null as value for a field is a recent feature.\n. Yup the null looks fine. What version of FlatBuffers?\n. You need 1.3.0 for null parsing.\n. There's no emptyResponse field in your schema? You may need to add it.\nOr, if you want to ignore unknown fields, pass an IDLOptions object with skip_unexpected_fields_in_json set to true to the Parser constructor.\n. Thanks!\n. FlatBuffers by default will not write fields that are equal to the default value (for scalars), sometimes resulting in a significant space savings.\nThat however also means that testing whether a field is \"present\" is somewhat meaningless, since it does not tell you if the field was actually written by calling add_field, unless you're only interested in this information for non-default values.\nThe C++ FlatBufferBuilder has an option force_defaults that circumvents this behavior, and writes fields even if they are equal to the default. You can then use IsFieldPresent to query this.\nAnother option that works in all languages is to wrap a scalar field in a struct. This way it will return null if it is not present. The cool thing is that structs don't take up any more space than the scalar they represent.\n. Yes, you could do your own tracking of it using booleans, or better yet, a bit field in an integer. In C++, you can even use the generated field enums to populate the bitfield.\nYou can already tell presence of tables, so you'd only need it for scalars. You could make a struct Integer { i:int; } for example that works for all your integer fields.\nOf course a further option is to set the default at a value you're certain to never use if possible, like -1, or 0x7FFFFFFF, or whatever.\nYet another idea might be to simply port the force_defaults functionality to Java. If you search for it, it is really rather simple.\n. Tables (and structs/vectors/strings) return null when not present, and don't have defaults. Scalars have defaults and return the default when not present.\n. FlatBuffers wasn't designed for in-place modification of non-scalar\nobjects.. you're usually best off making sure any modification happens\nbefore you construct the buffer.\nThat said, there is a limited set of such modifications possible using the\nreflection functionality (reflection.h in C++).\nOn Mon, Feb 22, 2016 at 10:16 PM, Tylor Reynolds notifications@github.com\nwrote:\n\nI have a schema that looks something like this:\ntable Entity {\n  id:int (key);\n  children:[Entity];\n}\nWhat is the easiest way to remove an item from the list?\nMy first thought was to use MutateOffset with a value of nullptr but that\nappears to not work well with other parts of the flatbuffer code. My other\nidea is to swap the child to remove with the last item and then use\nResizeVector to shrink the vector by one.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/3778.\n. If it's not performance sensitive, protocol buffers has a nicer api for this, yes.\n. Can you explain me why you would ever want to mock a FlatBuffers generated API? To test against the real API, all you need is a single buffer, or a single file (no moving parts, simple). To create a mock, you'd have to implement all the accessors, which is not only way more work, but it would break as soon as the schema changes since it is generated code.\n. That's indeed unfortunate. We did discuss whether we should give them more complex names (e.g. MIN_VAL, but in the end it seemed best to keep them simple, and in-line with the NONE enums already generated by unions.\nWhat do you feel? Is this a small annoyance that can be fixed on your side, or do you think this is going to keep creating problems down the road?\nWe do need to do a better job of erroring-out on certain identifiers at the schema compiler level, but that is a separate issue.\n. Hmmm.. not emitting them if they already exist would solve the issue, but might also be a bit unpredictable if people use these identifiers without being aware of this \"feature\". Or people using MAX in your enum assuming it is always the biggest value.\n\nAlso not a fan of adding a flag for such a minor feature.\nAnyone else opinions?\n. @evanw ?\nMore context here: http://stackoverflow.com/questions/35507174/deserialise-google-flat-buffers\n. Agreed @ngbrown. That said, the function can be provided in Java, as long as users of it understand what it means for a field to not be present (that it is equal to the default).\n. That would be a better name, yes. I thought 'Present' is also good, since you're asking whether it is physically present in a buffer.\n. @rgilles : that function requires reflection to be present.. the IsFieldPresent in C++ works without reflection.\n. @rgilles : That's not necessary since it operates in quite a different way (using reflection). Though hasValue is not very exact, so maybe it is a good idea.\n. Thanks Evan!\n. Yup, once Maxim feels the port is solid, he should create a PR to have it merged into the main project (if he wants). @mzaks ?\nOne problem I see is that the building API seems to require you to create objects beforehand. The building API for all other languages avoid creating garbage objects by constructing objects in-place. For a high performance API that is kind of important.\n. Sounds good.\n. @mzaks: what's the status of this port?\n. It would be great if you could prioritize strong compatibility with the other FlatBuffer implementations. If you do things differently, and people exchange buffers between Swift and other languages, you're causing a world of pain for no good reason. In fact, something that is not compatible with FlatBuffers should not say it is an implementation of FlatBuffers, to avoid confusion.\nIf you can make it entirely compatible, we can work on integrating it into the main project, if you want.\n. @mzaks : I understand you can do some cool stuff with cycles, but we've discussed at length in the past why FlatBuffers doesn't support them.\nThis is a free world, and FlatBuffers is open source, so you're welcome to fork the format in whatever way works for you. But please make sure your users are aware that this is incompatible with FlatBuffers, and it isn't an implementation of FlatBuffers.\nFor most users, there's not a lot of gain of cycles, yet the downsides are huge.\nIt be great if you could make your work conformant however, and make Swift officially a language we support.\n. FlatBuffers is all about maximum performance serialization, and the API is as it is to allow in-place construction of serialized data. Your solution would create lots of temporary objects, causing unnecessary GC churn.\n. I'm not sure if I follow what you're suggesting, my familiarity with js is low. @evanw?\n. @evanw: supporting it in other languages natively (not thru C++) would mean duplicating a ton of the code in idl_parser.cpp and idl_gen_text.cpp. Given how much duplicated code we already have in FlatBuffers, that does not sound like a good direction.\nIn JS you don't really need a parser for JSON of course, but you still need to load a schema (like the binary schema format that can be loaded using FlatBuffers itself), which you could then use to traverse a JS object generically and generate a FlatBuffer on the fly. More elegant than porting parsing code, but still a lot of work.\nI see no easy way to get this done for most languages.\n. Hah, so you'd generate JS code from a binary schema that knows how to convert a JS object into a FlatBuffer? Sounds pretty cool.\n. @evanw: yes, I guess I assumed that value would be calculated.\nThis definitely looks like an awesome API, and the code (in index.js) looks simple enough.\nI can imagine it is still pretty quick, as the only overhead is object allocation. I could see a lot of people willing to pay that cost.\nI'd be for adding this an an optional way to do things to official API. What is missing is tests and some documentation (in JavaScriptUsage.md, since this API style is unique to JS).\n. @evanw: is there still interest in contributing this alternative API?\n. Aww.. what are you replacing it with and why?\nYes, renaming that package something others than flatbuffers would avoid people blindly getting that and thinking they go the official package.\n. Makes sense.. its not perfect for every use case. The overhead for a minimal message is quite large (20 bytes or so). I've thought about adding varints as an option, no reason we can't have them.\nIs there a doc that shows the binary layout of Kiwi?\n. Nice, that's pretty compact. Though it is close enough to the design of protobuf that I wonder why you didn't use that? Is this just a slightly smaller/simpler protobuf?\n. I'm no JS expert myself, and improvements to most languages typically rely on external contributions. Are you able to make a PR?\n. @evanw is the author, and @evolutional also worked on a JS port.\n. Thanks :)\n. Sorry, didn't see this PR was updated. Thanks!\n. Thanks!\n. Seems reasonable to me, @rw ?\n. Status?\n. Can you make a PR that adds a special case for FreeBSD in the CMakeLists.txt and test it? I do not have a FreeBSD install set up.\n. Merged https://github.com/google/flatbuffers/pull/3793\n. Thanks!\n. Thanks!\n. @rw ?\n. Hah. Will fix.\n. -0xFF is -255, which is 0xFFFFFF01, which as a byte is 0x01 :)\nI agree negation for hex is not the most obvious feature.\n. merged in https://github.com/google/flatbuffers/pull/3939\n. Google translate tells me:\nHello, when I define an array of objects inside, Offset I must first be calculated in order to add an array of go, this has led to my code there are many more such objects int array, as can a can as protobuf a added.\nI'm guessing you're saying that having to collect offsets in an array to construct a vector of tables is.. inefficient? Protobuf repeated fields will do the same, but internally. At least in FlatBuffers you can reuse the array. Or you don't like the API?\n. You appear to be saying:\nThis api design I really do not like. As long as the basic data types need to create an offset, protobuf the api can be used as a reference. I feel like that is a better design.\nThe reason the API design is like that is for efficiency. Protobuf requires you to allocate a tree of objects before you can serialize anything, and this memory allocation (and memory usage) is costly. FlatBuffer's API is written to not have to do that.\nIt also depends on your language. In C++, this is a huge speedup. In other languages, people don't seem to care quite as much how many objects they allocate.\n. There are ways two FlatBuffer serializers produce different binaries from the same data, if for example they serialize fields in a different order. It may of course also indicate a bug in the serializer, but that is hard to tell, because I don't know your code.\nWhat do you mean by \"wrong result\"? How does your parser work?\n. You appear to be doing you own ad-hoc schema parsing.. this is generally not advised, as you may end up with incompatibilities with the original parser. You're better off writing your code generator as an extension of the C++ parser and code generator.\nFlatBuffer binaries are cross-platform and cross-language, but they do allow the exact same data to be serialized differently, since ordering of fields is arbitrary.\nTo find out if there's anything wrong with the binaries you're generating, try checking them with the Verifier in C++, or by converting them back to JSON with flatc -t.\n. The assertion probably means that your binary is malformed, though hard to tell why you got that assertion, what was your command line?\nWhy that happened I cannot debug that for you. My best tips are: a) use a debugger to see what exactly it was trying to do when the above assertion hit, or b) debug the same binary using the Verifier (using FLATBUFFERS_DEBUG_VERIFICATION_FAILURE). c) Study how the code generated by your parser differs from how manual Java code constructs a buffer.\nWriting a FlatBuffer encoder is complicated, and since it is all just binary data, there is no easy way to tell what goes wrong.\n. I just ran flatc on your data with the above command-lines, and it worked just fine. You appear to be using 1.2, can you upgrade to 1.3, or better yet, latest from github?\n. There's currently no option for that yet, no.\nThe reason it does this is because .protos may contain identifiers that are keywords in FlatBuffers, etc.\nThe easiest way to fix it would just be compile .proto to .fbs, then fix the identifiers in the .fbs.\n. I guess we can have a command-line option for this. Maybe --escape-proto-identifiers. Are you able to make a PR?\n. That option does not exist yet, it's what I suggest would be added in a PR. Or I'll get to adding it myself eventually.\n. I replied to your stackoverflow question, so is this issue solved now?\nAs for \"easy to use incorrectly\", I agree in the case of Java, with all offsets being untyped ints, it is easy to create an illegal buffer. I don't have an easy answer there, since wrapping all ints in objects defeats FlatBuffers goals of efficiency. The API was designed around the features of C++ where this can be made typesafe for free.\n. If it needs to be dynamic at runtime, it will always amount to a union and a (safe) cast at best.\nAs for an example filling out an entire object, isn't the code in the body of CreateMyType() effectively an example that can be copied and modified?\n. Agreed. Will add.\n. Fixed: https://github.com/google/flatbuffers/commit/f779962e3ed93cc9e6d3d1a7f62cb8e08e69a3b8. flatc does not enforce filenames and/or paths to match namespace in C++ (unlike Java etc). Typically C++ programmers want to have full control over directory structure.\n. But that would require everyone to have a directory structure for their includes that matches the namespaces. That would work for some, but not others.\nI guess this could be yet another command-line flag.. more opinions?\n. there is now --keep-prefix that replicates the schema include dir into the generated code.. FlatBuffers do not encode their size (since typically the context knows the size, so this would be duplicate information). So yes, if you're dealing with a buffer in a way that the size is unknown, you'll need to pass it along somewhere.\n. Looks good to me. Opinions, @chobie ?\n. Thanks all :)\n. Yes, that should be safe if you call Clear in between buffers.\n. Hi, you may need to rebase, as this PR is a bit of a mess :)\n. Indeed, I'll make how to access the bytes more obvious in docs.\n. Indeed, thanks!\n. We already have the file_identifier feature (which you can declare in a schema). They are however not related to the type, they generate a global function:\ninline const char *MonsterIdentifier() { return \"MONS\"; }\nThat's perhaps an oversight, a static function in Monster would have been better, and would have allowed your use case.\nThese things are stored in the buffer, and can be checked against (in this case MonsterBufferHasIdentifier). The reason it is a check and not just returning the identifier, is that identifiers are not mandatory, and determinining if there's no identifier present is inexact.\nAdding fully qualified names to the generated code is another option, though I'm not sure if most people would like to have this by default, as it adds more strings to their binary.\nI'm not in favor of your BuilderOffset idea. This is only there to avoid passing a FlatBufferBuilder to send, and is unnecessary overhead for most use cases. I would even say that denoting what message to send by an offset into an unfinished buffer seems fragile and non-intuitive to me.\nYou could achieve something similar on your end by making a TypedBuffer template, that is instantiated with the root type as template parameter, and holds either a FlatBufferBuilder or maybe better, a const uint8_t * / size_t pair.\n. GetFullyQualifiedName would be better, to reduce the chances of conflicts. A switch would be nice, maybe --gen-name-strings or something, that way we can add further names to it in the future.\n. Sounds good, see my response there.\n. @notsimon : a table can be part of multiple unions, so having a single id may be problematic. Also, I don't understand in what situation it simplifies code?\n. Are you having this issue with the latest from the master branch? I recently fixed how namespacing is handled in c++, which may be related to this issue.\n. Ah sorry, I see the problem. Yes, it would make more sense if it instead would output the correct include statement.\nI guess the main reason I did it this way, is that some .protos we convert have hundreds of dependencies, which means you'd have to call flatc --proto on all of them separately (much like how code generation works for all other languages). So ideally, for .proto conversion, it would convert all imported .protos as well, just written as seperate files.\n. This is not a problem with FlatBuffers, this is how floating point numbers work in all languages and systems. Not all numbers can be represented precisely in 32bit floats (and even 64bit):\nhttps://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html\nWays around this:\n- Move to double (this only makes the issue less likely to happen, but doesn't avoid it entirely).\n- Use a fix-point integer representation such as 1012.\n- Use a string.\n. The answer is in that document :) Not all floating point numbers (in fact, very few of them) can be represented exactly. This has nothing to do with FlatBuffers.\n. Besides what is in the documentation (the \"Internals\" section) in particular, some details may indeed still be best understood from reading the source of the existing encoders. Probably the C++, Java/C# and Go ones are most mature at this point.\nI would wish that the Internals document would be enough to write an implementation so if you find particular issues not covered there (or ambiguous), I'd love to improve it.\nThat said, looking at existing implementations is a good idea anyway to get inspiration on the style of the API etc.\n. This PR is empty.\n. @rw?\n. Thanks!\n. Attributes have been available in the C++ Parser API for a while now, so they're more useful than just documentation :)\nThat said, I agree that not adding them to the reflection data is an oversight. Are you able to make a PR for it?\n. attributes were added to reflection here: https://github.com/google/flatbuffers/commit/72fc45aa6acbc11052c6baa462fac26c5075392a\n. Nice PR, thanks!\n. @rw?\n. Thanks!\n. That sounds like a good idea. I wonder why the original code was written this way, maybe to protect against integer wrap-around?\nAlso, this throws an ArgumentOutOfRangeException. Doesn't _buffer already throw some kind of out or range exception? If so, what's the value of this check at all?\n. Ok. Yes, please make a PR. \n. While I agree this would be a very welcome feature, we currently don't have a way to represent union values outside of tables. We could add those, but it would be a new type of value, which would need to be understood by all languages and code generators etc., so would be a more complex feature to add at this point.\n. Thanks!\n. Sorry, but this is not a FlatBuffers issue, as we don't distribute it as a shared library.\nThe real problem appears to be that these devices don't have a libgnustl_shared.so. You may need to copy / load it.\nAn easier and better solution is to link FlatBuffers and everything else statically.\n. Yes, don't run clang-format on code that's not new, makes it very confusing to review. Your command-line looks ok.\n. I'm not sure. What is the definition of FBUnitOrder? You're saying the length of orders == 5? I am not sure what you mean by \"(9 * 1.3) bytes\".\nYes you can save space by making FBUnitOrder a struct. NotiSync can't be a struct, no.\n. The size really depends on the use case, and things like alignment, etc.\nIf you want to understand how the bytes get generated, read the \"Internals\" part of the documentation.\n. Ok, this is indeed a confusing diff, easier to learn to apply the correct formatting yourself :) But ok, the change seems simple, as the only thing that is really different is putting the function in a class.\nLooks good, but why not also do this change for the remaining languages? Python / JS / PHP.\n. yes, generate_code.sh would cause any of the generated code files to show up as changed in your commit if you had broken anything about the code generation process.\nThanks!\n. Wow, nice find! That was a bit of an oversight.\n. Fixed here: https://github.com/google/flatbuffers/commit/4d7890c2c912d926f5e7a11cc2106bb37b5f0158\n. Yeah, oversight on my part..\n. Fixed in https://github.com/google/flatbuffers/pull/3848\n. Thanks!\n. Thanks!\n. Hadn't noticed this was still open, sorry.\n. You appear to be right! The fix would be to replace PutInt by AddInt. Can you make a PR for this?\n. PR still coming?\n. StartVector does not reserve space for the entire vector, it merely aligns the buffer.\n. @belldon: correct, but PutInt does not call Prep, only AddInt does. Hence my suggestion to use AddInt instead in the original code.\n. @belldon : ahhh, yes you're correct, hadn't spotted that. Prep effectively says, ensure you can write a size aligned quantity after additionalBytes. Ok, not a bug, then.\nIf anything, it assumes the code between startVector and endVector is correct. Using AddInt would be slightly safer, as otherwise a wrong count or element size would produce even harder to detect bugs.\n. Thanks! See my comments.\nNotice the CLA requirement.\nTwo things missing:\n- Generated D files need to be part of this commit. We check in generated files, such that whenever we change code generators, we can see what effect it has on generated files.\n- You need a D test (just the sample is not sufficient). See what's there in tests/ for e.g. Java, and replicate that.\n. Please respond to all my previous comments on how you have addressed them.\nI see you now have a test case, but it is very minimal. It should also test D buffer construction. Please look at tests for other languages, and do as thorough testing as possible.\nI still see no generated files.\n. Also, please add documentation, similar to the existing languages.\n. @WebFreak001 seems it would work on all platforms if indeed toLower wasn't used, and it was written as MyGame/Examples/ always.\n. The PR generally looks good. Anyone with D experience any last comments?\nCan't merge before there's some documentation akin to existing languages though.\n. What's the status of this PR? Still see no documentation, for example.\n. Where are we at? Anyone that can work on documentation?\n. I see there's activity on this PR, what is the plan?. I already responded to this in a similar email on the google groups.\n. Not sure I see why this is needed.\nYes, we don't enforce unique keys, this is something left to the caller.\nWhat application do you have for sorting in decreasing order?\nWhat is the problem with the current use of a table comparator?\n. So.. maybe we can make that an optional value to the key attribute? not specified / 0 means increasing, 1 means decreasing?\n. Not sure if that needs generated code, you could use helper functions like:\nuint32_t clear_bit(uint32_t i, uint32_t bitflag) { return i & ~bitflag; }\nI'm not sure if defining set_bit is even worth it, since it is just |. What do you think the interface should be here?\nAlso, you can declare enum's to be backed by up to a 64bit type.\n. I don't see how calling digitalInputs_DI_CLK1() is that big of a win over calling digitalInputs() & DI_CLK1, especially since the former requires a method per flag. This means that 10 fields of an enum type that has 20 flags generate 200 additional methods, for a small convenience.\nAnd this just tests the bit, if you also want methods for set/clear, things get even crazier.\nAnyone else opinions?\n. Yes, this would definitely be the way to go. The nested_flatbuffer annotation is just there to be helpful, you must indeed insert this object as an array of bytes. So you have to write the bytes that are the result of the previous builder there.\nThen when you access them, you call the function to retrieve the root on those bytes.\n. This is possible in any language (does not depend on nested_flatbuffer). I'm not well versed enough in JS to write you an actual code snippet, but I imagine it would look something like:\nbytes = bbuilder.asUint8Array()\nabuilder.startVector();\n// loop through bytes and add them here, see tutorial\noffset = abuilder.endVector();\nstartA(abuilder);\nA.addItem(offset);\n// etc.\nThe JS implementation should really have a helper function to create a vector directly from a JS array, that would make this a lot less painful.\n. Ahh ok. Yes, ideally you should be able to do something like getBRoot(myA.itemAsBytes()) or something. @evanw ?\n. That is indeed an oversight. There need to be non-const versions of Vector::Get(), Vector::operator[] and possibly VectorIterator. Are you able to make a PR for this?\n. Ok, I'll get to it.\n. You're right, making IndirectHelper support non-const is pretty hairy with a lot of duplicate code. For now I made this simple accessor that should at least allow access to the elements: https://github.com/google/flatbuffers/commit/2bdf44a25d435f5428afb79e18657d0ba03cb7c2\n. Thanks!\n. (sorry for the late response, back from holiday).\nPulling out functions like isEverythingGenerated into BaseGenerator is great, if that same code was also used in other generators.\n(btw, Google Style Guide requires that to be IsEverythingGenerated.\nI don't understand the use of splitting up the generation steps at this point. The code is easier to follow and modify (to me) if they are linear. I'd prefer it if functions are abstracted when there's a need for them (i.e. 2 callers), not before.\n. 1) Then please use the function in the other generators where you can. Duplicate code is best removed in the same commit that adds the functions.\n2) That may well be in the style guide, I would still want these functions to have a need to be pulled out before it happens.\n. (sorry, was on holiday).\nThanks for the fix (and the tests!). One small issue, see comment above.\n. Thanks!\n. Thanks for the fix!\nLike you said, I am not that we want to fix the signed/unsigned issue, as it is relatively benign, and is a breaking change.\n. Thanks!\n. You cannot tell the size of a FlatBuffer from its data, that has to be encoded externally.\n@rw?\n. This looks quite useful, but you have indeed already noted the problem with this code: pointer invalidation. Since that may happen quite randomly and lead to very hard to find bugs, I am hesitant to add this functionality.\nAt the very least, we'd have to give these functions much more obnoxious names than the innocent sounding GetObject, since I don't think the comment warning is enough. Maybe GetTemporaryPointer (and GetTemporaryMutablePointer) is better (no need to call it Object, as your code will work Vector/String etc as well). Maybe make the comment more shouty as well, if possible :)\n. Mind doing the suggested renamings? Then I think we can merge.\n. Great, thanks!\n. Yes, comments starting with // are supported.\nThe problem with includes is that a JSON file is always a single object, i.e. includes would have to happen inside {} to be modular. The current implementation requires includes to be at the start.\nAnother reason why includes might not be the greatest idea is that they promote data to be duplicated in multiple flatbuffers. It may be better to refer to them thru some other means.\n. Thanks for noticing, will add.\n. All these files now have the Apache v2 license: https://github.com/google/flatbuffers/commit/1a161a8333ec1fa382a06485b7ffad955c001151\n. Yup, this could be added. Even better if one called the other (since we'll need to keep the original implementation also.\n. I agree, not a fan of APIs that force string allocation when you pass a const char *.\n. Anyone able to make a PR for this?\n. That could be nice, if StringRef were to allow implicit conversions from all those types, and if it were to accept flatbuffers::String as well, such that it doesn't call CreateString at all if that is the case. That way we only need to generate code for a single function.\nYour overloads for statically sized strings look pretty clever, though may give undesirable results if buffers are passed that contain a 0-terminated string that only partially uses the buffer.\nAnd yes, current CreateString overloads similarly only call strlen if it is a naked const char *.\n. Given that there's specifically code in there that turns numbers into identifiers, I would say that's a feature  :)\nFrom a readability and editing POV, it's also a feature.\nThe parser automatically turns those back into numbers when converting back to FlatBuffers.\n. That would be great to have. Anyone wanting to contribute these?\n. Very cool! any initial numbers?\n. Thanks!\n. Looks like a good idea! Maybe mention this behavior in the comments of this function? Do we know that CharBuffer.wrap doesn't already perform this optimisation?\n. Thanks!\n. Can you please paste the full code, that includes string construction, buffer construction and (failing) verification all in one go?\n. Looks good, thanks!\n. Looks good. You have conflicts to resolve on this branch though.\n. You typically don't have to close a PR. Simply make an additional commit that fixes whatever is requested, and update the PR.\n. Decided it was not a good idea?\n. I don't think we want to re-license / dual-license any of the code.\nIt is the FSF's opinion that Apache v2 is not compatible with the GPLv2, but I don't think everyone agrees with that (as shown in your first link). I think you're fine using FlatBuffers in a GPLv2 project if the project owner agrees.\n. You're correct that I am more interested in spreading good tech than religious wars.\nInteresting what you say about protobuf. So what license is this? https://github.com/google/protobuf/blob/master/src/google/protobuf/message.h\n. I will ask internally. What project(s) is this for?\n. After much discussion, we decided to keep the project Apache v2 for now. It is a much more solid license in all sorts of ways (the old BSD license leaves a lot of questions open), and generally we feel that we are already GPL compatible.\n. I am guessing your PR is based on a branch that is not up to date with our master branch. Please update your local repo to update to the upstream master, then push that to your fork on github, then update your PR from that. Yes, I know, slightly convoluted process :)\n. It looks like the PR is actually good, there is one unrelated file in there that maybe was never regenerated, and your large number of files is simply because the comment changed in all of them.\nThanks, you reduced the total number of lines in FlatBuffers, that is the best kind of PR! :)\n. No docs for JavaScript?\nhttp://google.github.io/flatbuffers/flatbuffers_guide_use_javascript.html\nhttp://google.github.io/flatbuffers/flatbuffers_guide_tutorial.html (click JavaScript option).\nSome notes:\n- Your schema includes both an enum and a union for the type, this is not needed, as unions already come with a type field and enum.\n- You don't need the --binary for compiling schemas.\n- It may be that your message failing to decode has to do with using the enum instead of the union, remove the enum, and see if that fixes the problem. \n. Note that FlatBuffers currently compiles on compilers with very minimal C++11 functionality (Visual Studio 2010, older GCCs etc). enum class is not supported on those. So we somehow have to ensure this functionality works on those platforms, or somehow make it conditional.\n. Ahh.. you're ahead of me, sorry :)\nThanks!\n. Thanks, good fix!\n. Ahh.. I guess enums fail to use the type promotion we have for other datatypes, where a ubyte gets represented as a short. Are you able to make a PR?\n. You're right, it should actually generate color() / add_color() accessor based on short (as it does for any regular ubyte fields, but that doesn't fix the constants. But we can generate the constants as short as well I think?\nAnd enums out of range on the original type should be an error I think.\n. Thanks, good improvements.\n. Good fix, thanks :)\n. Wow! Looks like great work.\nI know @rw was working on a Rust port as well, maybe he can help review / suggest improvements or functionality from his port.\n@Lakedaemon : for now, the TD macro is how all generators work, so that's what this PR should follow as well, until we somehow change it for al generators.\n. ^ big fan of macros here ;)\n. Unions are only ever tables.\n. @josephDunne : thanks for the work so far! We can leave it open here, to see if anyone else wants to pick up the work in the mean time, or for you to come back to.\n. Rust is such a good fit for FlatBuffers, would be good to get this done! @rw, how about it? :). Correct, we have plans to filter certain keywords, but that is not trivial, as it needs to be the union of all protected keywords in all languages FlatBuffers compiles to. We could of course pre/suffix these identifiers, but at this point we probably don't want to.\nA warning may be best.\n. Thanks.. are you able to make a PR with \"proper\" fixes soon?\n. Ok, fixed here: https://github.com/google/flatbuffers/commit/e92ae5199d52fd59540a800bec7eef46cd778257\n. Looks like a nice commit, thanks for keeping it small. 2 small concerns.\nAlso if you can keep code movement to a minimum, that will make it easier to see what has changed. Your new CheckNameSpace function was hidden in what looks like a lage code movement :)\n. Thanks, this looks better now :)\n. Yes, a FlatBuffer doesn't encode length, and we can't add that now.\nWhat we can do is introduce the concept of a (uint32_t) length-prefixed buffer, intended to be chained. This is of course simple to do, but it be good if there was standard functionality for it in FlatBuffers, so people don't have to reinvent the wheel. It could be added as a flag to Finish, such that it can take care of ensuring the buffer is properly aligned.\nAre you able to make a PR?\n. Yes, the verifier could tell you the upper bound within which all data sits, bit it is slightly messy (it would need to know what the buffer is aligned to, which there is no way to tell). Without a guaranteed exact number, there's no way to reliably skip to the next message. And besides that it is an expensive way to compute the size, if you didn't need verification.\nYes, a PR. I typically suggest this, because me getting to have time to implement it may take a while, so if you're in a hurry..\n. Actually, alignment padding for the last element in a buffer only happens for strings, vectors with 16bit elements or less, so it IS possible to reliably figure out the buffer size this way. Basically it needs to account for all cases where flatbuffers.h calls PreAlign(). Even simpler, since a FlatBuffer is always at least 4-byte aligned, it can blindly increase the upper bound to be 4-byte aligned when it isn't, and this should be accurate. Any bigger elements (e.g. doubles, SIMD types) always produce a reliable upper bound.\nI'll make a note of that, this would be nice functionality to add to the verifier. It can even be controlled with a #define such that it doesn't slow down people that don't need it.\n. So https://github.com/google/flatbuffers/pull/3905 was merged, and allows the verifier to compute the size of a FlatBuffer in a larger buffer.\nLength-prefixed functionality has not been added yet, and I can't promise when I'll get to it. Meanwhile it is not that hard to do yourself.\n. You'll need to #define FLATBUFFERS_TRACK_VERIFIER_BUFFER_SIZE before you include flatbuffers.h or the generated code.\n. @promethe42 : looks nice!\n. btw, added some functionality to make size pre-fixing part of FlatBuffers: https://github.com/google/flatbuffers/commit/486c048a0d5963f83f3a0d6957e4dde41602e2e7\n. Good point! Are you able to make a PR?\n. Ok, I'll get to it.\n. You're right, that's an alternative way to solve the problem. Though keeping name and namespace separate may be more general.\n. I guess the objects vector is meant to be sorted by name only..\n. Ah indeed. Are you able to make a PR? I think you could just simplify it to return VectorIterator(data_, 1)\n. We've considered this quite a few times in the past. The reason we don't support this is that it goes against everything FlatBuffers is trying to achieve for the user: not allocating a tree of objects. Doing so is exactly what makes Protobuf slow, so supporting it would erase one of the major reasons to use FlatBuffers.\nThis is why we implemented the reflection functionality as a \"FlatBuffer friendly\" way of achieving this kind of functionality. But I agree it is rather clumsy.\nThat said, this would be useful for quick editing tasks in non-performance critical parts of an ecosystem that uses FlatBuffers generally, and one could argue that our support of slower languages like Python also means that not all uses of FlatBuffers always need to have maximum performance. So I have to admit I am open to the idea of optionally adding the kinds of interfaces you suggest.\nThe way I'd see the interface is that a new switch, --gen-objects that would add a TableObject *CreateTree() to each table, and then each TableObject derived class would have a Offset<Table> Serialize(FlatBufferBuilder &fbb) method. The cool thing is that this way you can do it partially, i.e. create a tree out of only part of a buffer, and serialize them back to part of a buffer.\n. Sounds good.\nThese PRs sometimes attract other people, so discussing here is good. That said, you could post on https://groups.google.com/forum/#!forum/flatbuffers to get more attention.\n. This kind of API is in the works, should be available shortly.\n. I'm expecting my first version to be out soon.\nIt uses unique_ptr for tables, and a custom generated union similar to std::any for unions.\n. Here you go: https://github.com/google/flatbuffers/commit/3101e327c04f14f9345f60fd184aa3034e2faadf\n. We originally had a virtual destructor and a unique_ptr for unions, but it\nwas deemed that the current solution was more efficient, since it poses no\nextra overhead on tables.\nThere is no copy constructor for unions, as these objects are not meant to\nbe copied anyway (they are full of unique_ptr's which we don't want to have\nto deep-copy.\nTo generate this code, you need --gen-object-api.\nOn Thu, Jul 21, 2016 at 7:02 AM, BogDan Vatra notifications@github.com\nwrote:\n\nOne last thing for today :)\nI can't find Pack method for tables, in CppUsage.md you have\nmonsterobj->Pack(fbb); but flatc doesn't generate it anymore. I also\nchecked the code and it's not there ...\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/3901#issuecomment-234262618,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AF3FePZ7QzHxFLORZx2fndmuO7rOWXLrks5qX3wKgaJpZM4IvmXC\n.\n. See my comments there. I don't think unique_ptr has any overhead.\n\nOn Mon, Jul 25, 2016 at 11:09 AM, BogDan Vatra notifications@github.com\nwrote:\n\nI created a pull request here #3961\nhttps://github.com/google/flatbuffers/pull/3961 which allows us to have\ndeep-copy of the structs ;-) without any smart pointers which it should be\na little bit faster.\nThe non-smart pointers version is also needed by upcoming Qt & QML\n--gen-object-api support, which I hope to be able to implement it in the\nnear future.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/3901#issuecomment-235034899,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AF3FePX3Cww8Tyg4cC620bgTj9gmNPuGks5qZPvNgaJpZM4IvmXC\n.\n. The folder structure is very redundant, there's no way to simplify this while still working with Maven?\n\nIt is fairly important that the tests share data and schemas with the other languages (in tests), and that these automatically update if schemas are updated (through tests\\generate_code.sh). Can we refer to those files instead of moving / creating new tests?\nThere's no changes to JavaTest.sh/JavaTest.bat, I would guess this PR breaks them?\n. It is fine to just append this PR, that way people can see the discussion and how things have changed.\n1) Is fine with me.\n2) I'm not that familiar with OSGi, or what advantages it would bring. FlatBuffers is not a service, and it isn't really a self-contained library (its API is pointless without the generated code), so there are few advantages I can see. Again, the simpler the better.\n3) If JUnit is not something we can rely on to automatically be available to anyone working on FlatBuffers on any OS (without including it in FlatBuffers itself), I'd rather not use it. The current unit tests are not pretty, but they are at least simple to use and functional. What would JUnit buy us? Maybe we can focus on working better with Maven first?\n. 2) Ok, that sounds fine (the manifest entries).\n3) I understand it is useful. But keeping FlatBuffers free of dependencies is even more important to me. The tests we have are mostly intended for those that modify the FlatBuffers implementation, not for users of the library. Can Maven not run non-JUnit tests?\nThis PR is now very simple, and can be merged. Or do you want to add 2) to it first?\n. Ok, I'll merge what you've done so far. Let me know of possible next steps. Thanks!\n. Thanks!\n. might as well fix the typos in the same line while you're at it: \"use 64bit machine\" -> \"use a 64bit machine\", and \"this is php limitations. please wait extension release.\" also doesn't run well :)\n. Ok, I guess I'll merge just this single fix for now :)\n. I agree, an extra 10% loss of speed is not a lot, but it is still worth the #ifdefs for performance reasons. No need to pay for functionality you don't need :)\n. Ok, all good now, thanks!\n. Thanks. In general, the grammar document is really out of date, and needs updating.\n. Fixed: https://github.com/google/flatbuffers/commit/00d726fc4c49690b82ec58efe4ee1e313b9227fe. Yup, grammar document is pretty out of date, will need some work.\n. Fixed: https://github.com/google/flatbuffers/commit/00d726fc4c49690b82ec58efe4ee1e313b9227fe. Yes, this PR is very hard to read now, since most changes are just clang format.\nI don't mind clang-format, but if you can put that into a separate PR, so we know those lines don't need code review.\n. Please rebase.\nTry make the next change even smaller, these large ones are hard to review. Smaller changes I'll be able to ok more quickly :)\n. Sorry :)\nclang-format does not fix everything, I don't think it fixes * placement.\nThis looks good to merge now (once travis finishes).\n. Merged, thanks!\n. This is how it is supposed to work, but could be improved. We noticed that a lot of JSON files had things like: \"mycount\": \"10\" in them, so we support parsing numbers out of strings. Though probably if the string is not a valid number, this should be an error.\nConversely, I'd be fine if mystring: 10 would parse, but this is a little bit trickier, because now you need to define what terminates the \"string\" in this case (presumably any whitespace, and something from the set of \",}]\").\n. Ok, will improve that.\n. Could you describe the functionality this would have (and how it would be implemented) a bit more?\nIf this is a frequent use case, and is not too unwieldy in implementation, it could be part of the core project, though having it as an external project is always an option too.\n. Hmm, still not clear to me when you would use this, and how it would be implemented. Can you go into more detail, with an example?\n. A visual editor for any FlatBuffer data would be awesome to have! However it does sound like a larger project, so maybe it would be more appropriate in its own project, with the FlatBuffer project referring to it?\n. watched and starred :)\n. We like to keep the core project with no dependencies where possible, i.e. we prefer our users not have to pull in additional packages to do basic testing.\n. Sure! Why don't you make a small PR with what you have in mind, so we can discuss it before you do a lot of work? Or give an example right here?\n. See my reply on #3910.\n. Hmm.. I am ashamed to say the union type is hard-coded to be a byte in all implementations (see idl.h line 39). Somehow in my infinite wisdom I never expected anyone to have that big a union.\nChanging this seems to me would be incompatible at an encoding level, so is not easy to fix.\n@Lakedaemon : how do you propose this to be fixed? What did you do in Kotlin?\nI'll look at your PR :)\n. I'm not sure how that can work. If you serialize a union type as a short, other implementations will see it as corrupt data.\n. Reaching all programmers, having them recompile all their code, regenerating all their data files and updating all their users is pretty much impossible at this point. We can't make backwards or even forwards incompatible changes to the binary format.\nUnless someone has a brilliant idea, the current state of things is that FlatBuffers unions are limited to 255 members.\nYes, there probably should be a compile error.\n. I'll make it a compile error.\nAnd yes, people making code generators that don't adhere to the spec is an ongoing concern. There's no easy way to solve that.\nYour \"handmade\" union is a workaround, but I don't want to have two different kinds of unions in FlatBuffers.\nWe can work at allowing the size of the union type to be specified. First step would then be to make all generators error-out if such a type is used, until they properly support it.\n. @gregschlom grammar fixed: https://github.com/google/flatbuffers/commit/00d726fc4c49690b82ec58efe4ee1e313b9227fe. Can you please just fix formatting issues individually, in the code that you changed? Running clang-format on everything causes massive amounts of changes that make history of things harder to read, and it makes undesirable formatting changes like putting pre-processor statements always in the first column, or not allowing single-line case statements.\n. I'm just asking you to ensure formatting that matches the existing code of your own (new) code. There's no hurry in this.\n. Yes, your new code simply has to match the existing code (which mostly complies to the Google code standard).\nYou don't need to make a new PR, you can simply amend this one.\n. What I was saying is it is preferred that you format your code as you write it, rather than using clang-format. Having unrelated formatting changes in a diff is not helpful.\nThis change is simple enough, so I'll merge it, thanks!\n. Nice improvement @YuryBandarchuk16 and thanks for the code review @Lakedaemon :)\n. Was already fixed by: https://github.com/google/flatbuffers/pull/3920\n. A lot of the union functionality depends on its members being tables. It's easier to simply wrap your string in a table to achieve this effect.\n. I'm not sure I follow. You can include schemas in other schemas, which should allow you to reuse any type. Can you give an example of what doesn't work?\n. Can you give a practical example why you'd want the type of something without the something?\nRather than a typed field, a current workaround is to use a regular integer field to store the union enum value.\n. Ok, that makes sense.\n@Lakedaemon : It is exposed in most languages, the problem for him is that there's no way to refer to that enum in the schema.\nWe'd need a special syntax to indicate you don't want the union, only the type. That would need changes in all generators and parsers, so is not cheap..\nThe workaround to use a regular integer field may be the best option for now.\n. How does this fix the build? strchr already returns a const char *.\n. Ahh.. I see on the mac strchr has a different signature than on other platforms: https://developer.apple.com/legacy/library/documentation/Darwin/Reference/ManPages/man3/strchr.3.html\n. Will merge, can you please sign the CLA though?\n. Thanks!\n. I am fine with names being public. Can you create a PR?\n. merged.\n. Thanks, two useful additions. Have a look at my comments.\n. Yup, looks good now, a few last fixes..\n. Awesome, thanks :)\n. Ok, thanks!\n. I agree they may be useful in other languages as well, but for a better byte array accessor it be better to actually generate code for an accessor.\n. @rw?\n. 4.6.3 is pretty ancient, but we do want to support it.\nconst Namespace *cur_name_space_ = nullptr; that nullptr should be moved to the constructor instead. @Lakedaemon wanna fix that?\n. This should really work, so if it doesn't, that is a bug in the Go code generator. It maybe should output namespaces for these types.. @rw?\n. Well, the code generator should do this automatically.\n. Was this fixed?. Yes, generally we try to keep to a very minimalistic use of C++11 to support older compilers, so we can't be using this kind of initialization anywhere. Sorry I didn't spot it in the original code review. Changing it elsewhere would be welcome.\n. Ah ok, nevermind, didn't see it was aligned with the constructor.\n. Thanks!\n. Nice, thanks!\n. You'll need to rebase.\nAlso, your PR has a lot of small commits with undescriptive commit messages. Any chance you can squash them all into a single commit?\n. This seems like a nice suggestion. Is this new code compatible with the old code (i.e. can use ColorType.ARGB as before?) Can Java code that uses these enums and is intended to be platform agnostic work with this? And names[] appears to be gone.\n. Sure. But I am saying if I have code that reads/writes FlatBuffers, that code will work regardless of wether it is compiled against a class generated with --android-java or without?\nIf so, go ahead and make a PR. I'd suggest calling the flag just --android, that way if we want to add Android specifics to C++ or whatever, we don't need yet another flag.\nAlso, please retain names[], as there is another PR ongoing to make it public. We want to keep these strings available on all platforms for uniformity.\n. I understand that. I wanted to know what the differences are for the client code using it, if any.\n. I don't think we want to ditch name/names, keeping backwards compatibility is somewhat important.\n. I guess that may be acceptable. The problem is with code that needs to work on both Android and desktop, now can't access names in a portable way.\nAny other Android/Java users have an opinion?\n. Not sure I agree.\nSay I write some code that does processing of FlatBuffer data, and it does by reading a FlatBuffer thru the generated Java API. Now I want to run this processing both in my Android app and on the server. Is it going to fail to compile on one of the other because the way enums are generated? That doesn't sound ideal to me.\n. I think I should be able to write code that deals with FlatBuffer data, and have it compile on Android and non-Android Java platforms alike, yes.\n. Ok, I suppose if people want to write portable code they'll just have to use the current API.\n. Sure.\n. flatc --json myschema.fbs -- mybinaryflatbufferfile.bin will generate mybinaryflatbufferfile.json.\n. Hmm, my JS knowledge is too limited to understand the consequences.. seems fine to me. Anyone else want to have an opinion? @evanw ? @evolutional ? @rw ?\n. Ok, will merge in that case :)\n. I believe this is for efficiency reasons, right @rw?\n. There is no \"inheritance\" between tables.\nThere's 2 ways you can do this. The simplest is to make File a union of all fields of the 4 table types. This is actually efficient, since fields in FlatBuffers are optional, and take little to no space if not present (or equal to their default value). It seems that your use case is very suited for this.\nThe alternative is to use unions. You'd make a union out of the specific types, then stick a reference to that in File. The one issue with this is that your JSON files would have to look a little different from now, i.e. fileTypeId would become an automatically generated field of the union, and the actual union fields need to be a sub-table.\n. No, I suggested to have just one table instead of 4. It is basically your VideoFile, since that contains all the fields any of them use. Call that File instead and remove the other 3. It should then parse your JSON as-is.\n. Can you show your serialization code?\nYou are likely encoding the FlatBuffer wrong. Make sure the string/vector is created before the table.\nC++ should give you an assert when you get this wrong. Run your code in debug mode.\nNo need to change voffset_t.\n. 1. It is not ok. As I said, an assert will happen if you try. You should have gotten the assert from the inner CreateString in your original code. You are apparently developing with asserts off (in release mode), which is a bad idea. Make sure that if you write assert(false) in your program, the program halts/crashes/stops the debugger.\n2. Yes, it is less convenient, but necessary for maximum efficiency.\n. 1. You will be encoding child-objects inside the table, which sorta works if the child objects are small, but not how the system was designed.\n2. All access to a table goes through a vtable, which uses 16bit offsets. 64k was meant to be enough for a table without its child objects.\n3. Your serialized data will typically be bigger and less efficient to access since you have more vtable duplication, and less cache efficient access.\n. Code looks good now, but apparently the tests actually fail. See Travis output.\nThe trick with typeid is cool, but maybe it is fragile if the compiler uses different type naming. You could also just pass \"float\" and \"int\" strings to TestValue, if that is easier.\n. Thanks!\n. Thanks :)\n. Or you could have written chat_type_:chat_type = chat_all; to have a different default than 0.\n. See my response to your issue https://github.com/google/flatbuffers/issues/3938. This change is not needed.\n. Thanks!\n. See the comment at the assert. Apparently your call to SetRootType was not succesful (maybe needs to be namespaced, depending on schema).\nNote that it is much better to set the root_type in the schema than to manually call SetRootType\n. Classes like Theme are there as a handle to access already populated FlatBuffers, they are not intended to construct/mutate buffers. Check the tutorial on how to create a buffer.\nC++ has a Verifier to check the integrity of buffers (necessary because there are no bound checks). In Java, if you can be reading corrupted buffers, currently IndexOutOfBoundsException is indeed how you'll hear about it.\n. Sorry, you can't construct FlatBuffers from empty using the accessor API. You'll have to use building API for that instead.\n. I don't see what the point is to go all the way thru the trouble of serializing an empty FlatBuffer just to access default values?\n. I think the solution is to write the \"boilerplate\" code you showed.. I don't think this functionality is common enough to have the code generator take care of it.\n. Efficiency is the #1 priority in FlatBuffers, even in languages like Java where that isn't easy. Adding an if-check in a very frequently executed part of the code is therefore not to be taken lightly.\nNow if that if-check would be a major improvement to the API, it could be considered, but in this case it appears the use of it is to save a single person, you, to have to write some extra code. That is not enough reason to make everyone else's code a tiny bit slower for no clear benefit.\n. Yes, we intend to support Maven directly. @rgilles just made a start with this in this PR: https://github.com/google/flatbuffers/pull/3902\nI appreciate the help, because I am not familiar with Maven myself.\n. I believe this take you are removing is referenced from https://github.com/google/flatbuffers/blob/master/docs/source/doxygen_layout.xml\n. Thanks!\n. Ahh yes, we intend to do a better job filtering these kinds of identifiers out, but haven't gotten to it.\n. Agreed, this is different.. but they both need to be fixed :)\nI'd say it is best if flatc made it an error, since programming languages typically don't allow field names the same as class types either.\n. It currently disallows field names that correspond to an existing table type.. To resolve that last error, also add grpc/src/compiler/cpp_generator.cpp  to the project. Feel free to make a PR, or otherwise I can fix it in a bit.\n. Ok, just fixed in https://github.com/google/flatbuffers/commit/ce3e7fbd72191da6602e910818c46018df08a515, thanks for reporting!\n. Thanks :)\n. Ok, we can merge this for now. Would be good to add tests, and to fix the handwritten code where it doesn't correctly detect UTF-8.\n. Thanks!\n. That looks right..\nAnd yes, non-scalars is a lot more complicated.\n. As for who can review this PR, anyone with more JS knowledge than me would be good :)\n@evanw wrote the JS implementation, but @evolutional and @rw are other contributors that qualify :)\nPlease add the generated code to your commit, so we can see how that has changed.\n. Yes. After you've built flatc with your modifications, cd tests and sh generate_code.sh should do it.\n. Can you add some tests that exercise these functions? See e.g. test.cpp MutateFlatBuffersTest().\n. Thanks! Would be nice to have vector/struct mutation too, but I guess that can be added later too.\nI'm fine to merge this, unless anyone else has any comments?\n. Ok, no other comments, I'll merge, thanks!\n. Hah, sorry about that.. it's new code :)\n. The offsets are 16bit. We go through offsets for all fields since all fields are optional, which is essential for forwards/backwards compatibility and general data flexibility.\nThe cost of a field isn't 2 + sizeof(field), since the offset sits in a vtable that is shared between similar objects, so the more objects you store the cost of offsets goes towards 0.\nFor more: http://google.github.io/flatbuffers/flatbuffers_internals.html\n. Nice contribution.. see my comments.\n. I'd say directly from the buffer. Part of the idea is that you can load a FlatBuffer, and do these kind of lookups without ever unpacking anything.. heck you could mmap it (or whatever the C# equivalent).\nIt should contain the least amount of new anything.. don't want to be creating extra garbage.\n. You shouldn't need to copy those functions.. the access of array elements should go via the table.\n. You shouldn't need to copy those functions.. the access of array elements should go via the table.\n. As far as I can tell, it is generating includes. Can you give a more specific test case / show your command line?\n. --gen-includes is maybe a bit of a misnomer.. it means it will generate include statements, not that it will do codegen for any included files. Try: flatc --cpp e1.fbs e2.fbs. I agree, that might be a nice feature.. See my comments. Some of your other changes are potentially nice though (like moving UnPack into MonsterT).\n. So far we've managed to keep everything at C++0x level. In terms of testing, a proliferation of kinds of code that can be generated is not desirable, so I would only want to introduce that if it gave huge benefits.\nI'm not sure if emplace_back is such a thing, most things we're putting in vectors are scalars, structs and unique_ptr's, and I bet most compilers generate equivalent code for that with push_back.\nIf anything, we should reserve the exact amount of space in these vectors before the loop :)\n. Ok, I suppose we can add such flags, as long as we can keep their use nice and isolated (no sprawling if-then's through the code :)\nWe don't need --c++0x since that will be the default for now (whatever is implemented by VS2010, essentially).\nI don't think we want to use std::any, as we do want the interface between all these version to stay the same. To have to have different documentation and tests for all these versions will get a little bit tiresome.\n. The mac build is also failing because of emplace back.\nDon't automatically enable C++11, the choice what code to generate may be independent of the compiler on the build/developer machine. Add a --cpp11 flag instead.\nThe Windows error:\nerror C2719: '_Val': formal parameter with __declspec(align('16')) won't be aligned\nc:\\projects\\flatbuffers\\tests\\monster_test_generated.h(437)\nIs caused by std::vector<MonsterT> testarrayoftables where MonsterT now contains a Vec3 pos; (which is 16byte aligned, whereas the Windows allocator defaults to 8) field in-line. That's not a good idea since these structs are optional, and if you in-line them there's not only no way to tell if they were present or not, but also they waste a lot of space if they were not present. So sadly they need to be a unique_ptr like they were before.\nI don't think std::vector<MonsterT> is a good idea either, as now a simple push_back onto that vector will cause a potentially enormous amount of deep-copying. Deep-copying would be nice to have as an option, but only if the user wants it... typically they don't.\n. Yes, agreed, that hopefully is mostly a move on a modern compiler, so maybe we can have the vector elements inline.\nunique_ptr is a C++0x-ish feature, and I believe the compilers that implement it also know how to move it :)\nThe object API is nicer to use, sure, but FlatBuffers derives its name from being able to do without a tree of objects.. so that will always be the focus.\n. Please see my comments above.\nThere's a lot of good improvements in this commit, but also a lot of ones that are unrelated, and could have been reviewed more quickly and easily if they had been in independent PRs.\nMost contentious for me at this point is the introduction of Optional, I was really hoping to keep the generated code as \"vanilla\" C++ as possible, by using standard types. I agree though that being able to deep copy when needed would be very useful. Other opinions welcome.\n. @Bklyn : that's a good point. If there really is not any difference, then {} just increases the chances that a compiler won't deal with it correctly. And generated (implementation) code is not there to be pretty, so I'd agree that using the most baseline code is best there, especially if it simplifies code generation.\nThe same can be said for range-for, there may not be any advantage to using it.\n. It's looking pretty good now, but the one thing I am unhappy about is Optional. While deep-copy is useful, it is also a niche use case, and for that we are asking users to deal with a non-standard smart pointer for all their regular accesses.\nI think the better trade-off is to have a standard pointers (unique_ptr) but a non-standard deep copy (a Clone method).\n. It's not just about the interface, the bigger problem is unfamiliar types bleeding into the rest of the codebase. This object API is such that people can choose to use it as the actual run-time data of their program, which means the types used will perculate through everywhere. I think it is nicer that people are able to receive or return a std::unique_ptr in their functions than the unfamiliar flatbuffers::Optional. They may already have APIs that want unique_ptrs. Maybe they are implementing an API themselves, and want to return a unique_ptr, and not the implementation dependent FlatBuffer type.\nYes, I hope that deep copy is rare enough to the point where the extra complications of Clone don't really matter, but the above concern is much more pertinent to me.\nI don't agree that your work is useless without them.. this PR contains a lot of improvements that are great to have regardless of how this issue is implemented. Like I said, I would have preferred them in separate commits so we can judge each change by its merits independently. We can still merge most of this commit, but we either have to revert or separate out this issue for the moment. We can reconsider it later, if more evidence appears that it is the better direction.\n. You can of course have your own fork any way you want.\nBut when you make a PR, you signal that you want to contribute to the main project. I invest my time reviewing your code. Just saying \"cherry-pick\" what I need is not very helpful for a PR as entangled as this.\nI don't think my reasons above for not wanting a custom Optional type are that crazy. Giving people standard C++ code to work with is a pretty important design goal of the object API.\n. I'm sorry you feel that way.\nOptional was introduced somewhere halfway this PR, it certainly wasn't obvious to me from the start you were planning to add them, otherwise we could have had this discussion earlier I guess.\n. I would gladly see a solution to make deep-copying easier, but introducing a new non-standard smart pointer (Optional above) is just not acceptable. The aim of this API is to look like idiomatic, standard C++, see discussion above.\nOne solution I could see (and which is useful in other ways) is to allow the pointer type to be selected in the schema, i.e. you could specify you want to use shared_ptr instead of unique_ptr.\nBesides Optional not being an acceptable solution, the other reason this PR can't be merged as-is, is because it contains a variety of not necessarily related changes. Some would be useful to have independently (and could be merged thru a separate PR).\n. Maybe I've forgotten the scope of the problem, but to what extend could we generate a copy constructor for these types that explicitly copies unique_ptr's?\n. @Bklyn : no, I was asking if we can make them copyable with an explicit copy constructor.\n@bog-dan-ro : yes, what at @Bklyn said, and use of {} constructors. If you look through the PR there's actually a lot of small changes (quite a few which are welcome) not related directly to making things copyable.\n. @Bklyn : yeah, I'm not happy generating tons of extra code either.. just exploring options.\nI don't think it was much of a value type to start with, it is a tree of objects, which I personally don't ever clone, but I'll believe you guys there's a need for it at times. It is pretty idiomatic to make trees of objects out of unique_ptr's these days, so this mimics that. Everyone out there using unique_ptrs manually has exactly the same problems as the current code does.\n. If we're going to make this an optional thing, then the way I suggest doing this is through field attributes rather than a flatc flag. The reason being is that we want to have a feature anyway to allow you to select the type of pointer (e.g. shared_ptr), and that you may want to select this on a per field basis.\ndynamic_cast won't work since these objects do not have virtual tables.\n. We have internal users for that use case.\nBut ok, I guess lets do it like this. Default is unique_ptr which can be overridden by a command-line flag, which in turn can be overriden per field if needed.\nI don't think switching based on the detected compiler is a good idea, since the machine that generates the code is not necesarily the one that compiles it. For switching based on C++ level a typedef is probably better.\nI can create a commit for pointer support, then you can see how Optional fits into that.\n. I just added this commit: https://github.com/google/flatbuffers/commit/d9fe4e2769045f0b90f7a82c03c7a3693fd14fff\nIt allows unique_ptr to be overridden both at the command-line and per field.\nSee if you can refactor this PR to allow your Optional to work with it. We can consider upstreaming it, since it will be optional (hah), so my concern of forcing a non-standard smart pointer on people has been addressed. Maybe stick it in its own header since it is optional?\nAlso, like others have noted, please either remove or put unrelated changes in their own PR whereever possible.\n. With copy/assignment, we may just have to hard-code it for now: emit them, unless type is unique_ptr or naked. If it is not one of those two, it is likely shared_ptr or your Optional, which support copying.\nNot sure how I feel about moving all sorts of this functionality into the schema. In the simplest case, just ensuring you have included your desired smart pointer before you include the generated code is sufficient. I know having a header file which is not \"self contained\" is bad style, but it sure is a lot simpler than making the code generator responsible for it. We could generate an include statement for the few pointers we recognize though. . I'm going to close this for now due to inactivity.. feel free to reopen when you have time to re-visit, or open a new one.. You're right.. can you make a PR for this?\n. This seems like a useful change, why close it?\n. Thanks!\n. Yes, you're right, I have no idea what it is doing there. Want to submit a PR to remove it?\n. I think someone else suggested making the names array public, so it can be used for such cases. This may be the easiest solution. Want to submit a PR for this?\n. What does it break?\nI guess Map uses more memory, the current array is more straight-forward, since enum->string is likely more common than the other way around.\n. It's generated code, so if they modify stuff in that array, it will only affect them. I don't think it is a big deal.\nWe didn't use Java enums exactly because they're inefficient (especially on Android), and because they don't correspond to the concept of an enum in other language (being just a fancy way to refer to integers).\nhttps://www.youtube.com/watch?v=Hzs6OBcvNQE\n. If we expose the array, then the choice of doing a linear search or converting to a map is left to the client.\n. It is a non-trivial change (the generated code needs to be aware of it), and I don't think it is that useful.\nWhat you have to see is that FlatBuffers is a cross-language binary encoding that defines things in a certain way, and it is up to the language to represent that as closely as possible to that definition. FlatBuffer enums are like integers and interoperable with integers. The fact that Java prefers to make them into objects is not going to change how FlatBuffers handles them.\nI am not entirely against a flag that allows you to generate them in the Java way, but I think just using the existing array is more productive.\n. Thanks for the fix!\n. Thanks!\n. I guess we've never though of supporting NaN or Inf. Are these standard JSON ways of writing things?\nIt should probably not be added to the code generators, but to the parser instead.\n. Yeah, that sounds a bit much. If people really want to store NaNs in FlatBuffers, they can always do so through the regular API.\n. For tags, you'd have to create a table that has all possible tags as fields. You cannot make an open-ended table.. well you can, but it would have a different structure in JSON.\nvalues cannot be parsed as-is, since you cannot have nested vectors. You'd have to replace the inner vector by a table with a string and a double field.\n. If you can't change the JSON, then yes, there's no way to parse it with FlatBuffers at the moment. You can't have a vector of unions either, unions have to sit in tables.\n. I don't think I understand your code, it appears to mix FlatBuffer construction code with reading code (they are so far entirely separate).\nNot sure I'd want to advocate generating methods for each individual enum, that could result in a LOT of code.\n. I think I prefer the current situation, as the extra complexity is not worth it I think. Anyone other opinions?\n. Very nice cleanup, thanks!\nYes, the \\x encoding is something I added early on when I wasn't really thinking about JSON compatibility yet.\nEnforcing UTF-8 is a good idea, since e.g. Java relies on string being parseable as UTF-8. If people want to store binary they should use vectors, but I guess your flag provides an escape hatch :)\n. The other generators don't deal with string literals afaik, i.e. they only deal with identifiers.\n. Nice clean-up! Looks good, @rw?\n. I'll defer to @rw on this one.\n. no, table is. Though Table is often used as the base class in implementation, but that should be namespaced\n. @rw?\n. See my comments..\n. Have you addressed all my past comments, where needed?\nBetter commit descriptions than \"Update\" would be helpful.\n. Looking pretty good now, a couple of last issues. Also can you maybe mention this functionality in the docs somewhere, probably similar to what is there for C++.\n. Ok, looks good now, thanks for your contribution!\n. Oh, forgot, there's still no docs for this.. would be nice if you could add that.\n. Had to fix a few issues with this, since it didn't work in Java 1.6: https://github.com/google/flatbuffers/commit/e1f8037cb55cbeac5d96ad63c2f5c70560737340\n. Looks very useful, thanks! See my comment. Also please ensure lines stay <80 columns.\n. I'm going to close this for now due to inactivity.. feel free to reopen when you have time to re-visit, or open a new one.. Have a look at the generated EnumNameMoveDirection(MoveDirection e) function. Its implementation compensates for the starting enum.\n. This is possible with the reflection code in C++ (see test.cpp ReflectionTest(), particularly the use of AddFlatBuffer).\nAlso, recreating buffers is easier with CopyTable.\nBut yes, generally, changing the serialized data after the fact is complicated and best avoided.\nAn alternative would be to store your Bars in seperate FlatBuffers, or to at least represent them as bytes rather than FlatBuffers:\ntable FlatBar { bardata:[ubyte]; }\nThat way, at least when recreating an array of FlatBar, you only have to deal with copying arrays, which is easier than the reflection methods above, and can work in any language. You can still access the inner buffer in-place with a GetRoot call.\n. Thanks for reporting! That apparently was never tested.. I made a quick fix here: https://github.com/google/flatbuffers/commit/3eebba789f6447b342a308b0ad38ed0a6fb51de4\n. We originally though this made most sense, given how flat most C++ include structures are, and sometimes these paths are not desirable. I agree that there are cases where you'd want the paths. We could fix this, but it would have to be through a code-generation option, to retain backwards compatibility.\n. You can now use --keep-prefix to change this behavior: https://github.com/google/flatbuffers/commit/93c0960c3abdd5f3f7cdc9c4981732fde7be5e4e. You're right, it be nicer to do that automatically. Are you able to make a PR?\n. No, I don't believe there's an easy way to do that, especially since objects are not contiguous things (some things are in-line, others referred to be offset) and because vtables may or may not be shared (so their cost is amortized).\nTo get a rough idea of how expensive things will be, it is worth understanding http://google.github.io/flatbuffers/flatbuffers_internals.html\nGeneral tips are to use structs instead of tables where reasonable (for simple, unchanging data types), to not nest tables unnecessarily, and to reduce your reliance on strings (use enums etc where possible). \n. I agree, that does look weird.. @rw?\n. Why not simply return obj to mean true and nil for false? Or is using pointers as booleans not a thing in Go? :)\n. Also, given that apparently noone has used this functionality yet, it sounds attractive to me to leave it out all-together. You can document that instead of nil, you can pass in a new(Person) yourself. Saving a conditional seems worth it to speed up the common case.\n. Or, if nice API is a concern, generate a second accessor that calls the existing one, with no Person parameter, that always allocates a new(Person). That's assuming Go has overloading.\n. This is a rather cool feature to add.\nIt is also a somewhat dangerous feature, as it adds an entirely new type. Unless you're catching all locations in all code generators where it is doing something with types, this commit potentially introduces bugs.\nWe'd need to add code generation for all languages.\n. I'm going to close this for now due to inactivity.. feel free to reopen when you have time to re-visit.. These should be covered by asserts (panics?) as much as possible, as we can't expect everyone to instantly understand FlatBuffers internals. But yes, we also need to have an example of both a single struct and a vector of structs added to a table, with some clear language as to how they're different from tables.\n. What's the followup?\n. Looks like a good idea, see my comments on your patch.\n. See my comments on that patch.\n. @rw wanna merge this?. Thanks!\n. If the call failing means no JSON was generated, then yes, callers should want to deal with it. And we just released a 1.4.0, so now is a good time to break the API :)\nIndeed, can you add a check for this bool to the call coming from flatc.cpp ? flatc can be used to turn any binary into JSON, so it be good to report an error here.\n. Ahh ok :)\n. See my response on your similar StackOverflow question: http://stackoverflow.com/questions/39060963/how-to-use-flatbuffer-generated-data-in-browser/39066630\nFeel free to post more complete code here if you want further help.\n. See my original comments on StackOverflow. You shouldn't be using serialize.\n. Please see discussion in https://github.com/google/flatbuffers/issues/3945 as to why this PR may not be a good idea.\n. @jojohello: Are you using the latest master, or 1.4? Can you paste the full error? What is it you're compiling?\n. This should fix it: https://github.com/google/flatbuffers/commit/b04e21db16fbf732d6b48359ba1a444c1eed0a74\n. Thanks, good idea :)\n. Had to undo this commit, since it didn't work with Java 1.6 (gives class loader errors). https://github.com/google/flatbuffers/commit/ee56418cefd891f7dbc2d156a955825fa5a15b3b\n. I got a different error, with class loader not being able to find Struct.class. I reverted it, because I didn't want to spend time figuring out how to fix it, sorry :)\nYou can make a new PR, which I can cherry-pick locally to test if it works. Though frankly if it can possibly cause any complications I'd rather not have it, as I don't feel this is a major problem to solve.\n. Your new PR has been merged.\n. Thanks!\n. If it returns 0, then the field must not be present. I can't tell from your code why that is. Maybe it gets transmitted badly? Can you show full code, including the part that writes and reads the FlatBuffer?\n. Your code looks ok to me. Have you tried running flatc --json myschema.fbs mybinary to see if the field is indeed present in the file? That way you know if the problem is on the reading or on the writing side.\nNote that this is a binary file, so not sure why in your code you use the extension .text.\n. The extension does not matter, no.\nAdding fields after the buffer has been constructed is difficult, it may be possible at some point with reflection, but it is better to organize your code such that you can construct the buffer in one go.\n. The big reason is that the fragile base class problem would be even more problematic here than in other systems. In FlatBuffers you can only add new fields at the end (or as the next higher id), which means that once you inherit, you can never add anything safely to the base class anymore.\n. I understand your use case, but it is a non-trivial feature (assuming we support it in all languages), and it is something that most users should not be using, since they'll get themselves into trouble. I think @promethe42 's solution is the best, assuming you need to pass part of a \"derived class\" to a function that expects a \"base class\".\n. You can't have virtual methods in the table accessor objects, since they point to inside the FlatBuffer.\nYou can still have polymorphism through templates (in C++), i.e. any two object that happen to have the same fields can be accessed this way. You can achieve this relatively simply by just having a schema feature that copies all the fields of a base table.\n. Please have a look (or rebase to) https://github.com/google/flatbuffers/commit/e1f8037cb55cbeac5d96ad63c2f5c70560737340 where I already fixed a few issues.\n. That typo would have been caught if there was a test for it...\n. For most use cases, you don't need a library at all (most people only need flatbuffers.h) or people may prefer static linking when they need the parser or reflection, since the code is so small. I guess noone has needed a shared library yet.\nCan you make a PR that adds those lines? Preferably surrounded by a conditional that is by default off.\n. Thanks for the tips. Fancy making a PR, since you seem to understand the issue better than me?\nThat said, I really recommend against Debian or any other OS having a standard FlatBuffers package that comes with a .so for people to use. That has all sorts of interesting versioning issues that static libs don't have.\nA Debian package that provides a precompiled flatc, headers and static lib of the major releases would of course be very useful.\n. You can already add /// comments to your schema, which will end up as // comments in the generated code. So adding some doxygen syntax to one of those should do the trick?\n. Ok, double-checked it works now, thanks!\nMaybe remove the commented line, and also delete the files at the end?\n. Thanks!\n. FlatBuffers does not support inheritance, neither in schemas nor in the generated code. Inheritance in schemas would be very fragile (can never add anything new to the base class). I don't see the point of inheriting from the generated code objects either.\n. Yes, FlatBuffers skips the parsing step, but is therefore slower than accessing native objects. I don't think it is fair to compare against native objects without including the time to parse/create them, no, and also not realistic, since you usually can't skip that step.\nThe only time when native objects are truly faster is when you have to access the data many times. In that case keeping things in FlatBuffers will actually be slower overal. If that is part of the performance critical part of your code, then you should copy data out of FlatBuffers.\n. It is possible that the JS implementation is particularly slow. I designed FlatBuffers originally for C++, and it may not fit JS quite as well (i.e. all the indirections confusing the JIT). Maybe the JS implementation can be optimized, or maybe a different API (that actually unpacks into JS objects would be better for this language.\nI believe @evanw (who implemented the JS portion) at some point experimented with an object based API, though I am not sure where that went. @evanw: can you point us to that? Maybe someone would like to continue it.\n. Thanks, that's some useful testing! Yes, indeed, looks like the manual UTF-8 decoding is slow.. I wonder how we can speed that up.\n. Hmm, using built-in functions would be great, bit worried that this is a two step conversion over octets, that might end up generating more garbage.. but who knows it is still faster than what we have. Also would need to check whether that works on Node.js.\nGood to hear FlatBuffers works so well for you :)\n. Hmm, it is hard for me to tell what effect this will have on the many platforms we support.\nCan we instead stick things in a var that people can override, but keep existing behavior?\n. Make --grpc a general flag, i.e. --python --grpc vs --cpp --grpc.\nNot sure why it is crashing from here. Can you debug it?\n. Why was the corresponding GRPC PR closed? We need the two projects to be in sync.\n. Ok, looks good now, just some unrelated formatting problems.\n. They're in the review above.\n. Just looking at the diff, still a few places where you removed spaces are not covered. Please look at the diff, where-ever your PR removes spaces, and undo that if it is not necessary.\n. I don't think we want to check in the generated .html files. Where are the the .md changes?\n. Ah, hadn't spotted that. So this PR is unnecessary?\n. Generated code uses int64_t.. can you point me to where you are seeing int32_t ?\n. Is it just the argument name, or also the accessor? Since the argument name we can fix with an _ or whatever, but the accessor is a bit harder.\nWe've had the plan to automatically escape all keyworlds in languages, but haven't gotten around to implementing that.\n@rw\n. I'd add the _ at the end.. and ideally flatc should do this automatically for you.\n. _ at the front is often used for system definitions, and _ at the end is something used for instance variables in the Google code style. The latter seems slightly more appropriate but it is just a preference.\nYes, flatc should have this functionality, but it doesn't yet.\n. This is a good start!\nTo your questions:\n1) Yes, we definitely would want the generated files to end up in the same location as the rest (probably java/com/google/flatbuffers/reflection. Probably the cleanest way would be to allow attributes to set on a namespace declaration, such that we can java_prefix tags and others.\n2) LookupByKey was just added by @TGIshib, maybe you can discuss with him how either API/docs can be improved?\n3) Yeah, that is inconvenient, but like you say, changing that at this point will be painful. It means we have to refer to it as reflection.Object, right?\n. The reflection functionality itself still looks minimal (only short fields). Let me know when you think it is ready for merge and you want me to review it.\n. this PR seems to contain a bad merge :)\n. This PR has 132 commits and 145 changed files.. It is probably not up to date with master? Can you please rebase?\n. Closing this for now due to inactivity, feel free to reopen if you have time to finish it.. Thanks for fixing that!\n. Thanks :)\n. Thanks! Though does this still allow it to be called in the old way (without an output directory)? We have a lot of code that depends on this function, and it be great if we didn't have to modify it. Alternatively, use a different name to allow the old style to still work.\n. Closing this for now due to inactivity, feel free to reopen if you have time to finish it.. The reason we don't have it is two-fold: first, string->enum is not as common as enum->string, and second, we don't want everyone to pay the price of a map/unordered_map.\nSo that means it either should be optional (yet another flag to flatc, not that great), or we can make it into a function that creates this map:\nvoid PopulateEnumTypeMap(std::map<EnumType, const char *> *mymap) { ... }\nThat way, the user is responsible for owning that map.\n. Returning the map causes a lot of allocation/deallocation if the compiler doesn't implement move semantics optimisation, so passing it as an argument seemed safer. They're otherwise equivalent, I don't see how returning it helps with \"static storage\", as neither will have that.\n. I guess I would never use a std::map statically, since you then have no way to delete it and detect memory leaks (unless you are confident that all implementations of map::clear will free up all dynamically allocated memory.\nI'm ok with it being a return value, as it is not a common use case, and most compiler should be able to move the memory.\n. Until I see evidence that all compilers we support can initialize a std::map without allocation, I wouldn't want it as a static. And seeing how some compilers don't even support the syntax for that, I am not being hopeful.. Yes sounds good.. does this pass the tests? E.g. there is in test.cpp: TEST_EQ_STR(root_table->name()->c_str(), \"Monster\");\n. cd tests\n../flattests\n. Looks good! But you still need to please googlebot somehow :)\n. The commit says it was authored with Xun Liu, so (s)he needs to also sign/ok.\n. Look at the commit. It says Xun Liu committed with paszea. Apparently there is a xun@pinterest.com that GitHub doesn't know about. There's 2 different accounts associated with this commit.\n. See the bots message. Did @xunl sign the license? and reply with \"I signed it\" or something?\n. Try I signed it! .. not sure how picky it is on spelling.\n. Ok, I am going to assume @googlebot is defective and merge it anyway. Commit authors both have consented and have a CLA.\n. And thanks for the fix :)\n. The main problem appears to be here:\nRaft.CueFB.finishCueFBBuffer(builder, offset);\nbuilder.finish();\nThe first call is a nice way to call finish. The second call calls it again, but this time without an offset (I guess JS doesn't warn about missing arguments?), which produces the extra zeroes.\nThe second problem is here:\nRaft.CueFB.addId(builder,id);                                    \nRaft.CueFB.addName(builder,id);\nNote you use id twice.\n. Why is the slice necessary? Does send not respect byteOffset?\n. Thanks for the fix!\n. Hah, that should be easy to fix, thanks for reporting.\n. Fixed here: https://github.com/google/flatbuffers/commit/ab51b030939e02e55cac6f9e779d8696013819a9\n. If you get that assert, it means that above the code you show, there is a call to startSession / startPageInfo / startVector or similar that hasn't yet been finished with the proper \"end\" call.\n. Can you show your full FlatBuffer construction code (in text, not image)? Then I'll point it out to you.\n. There doesn't appear to be a nesting problem in that code, so the problem must be elsewhere. Check between where you create mBuilder and this code if you started any table or vector without ending it.\nOr are you maybe reusing a FlatBufferBuilder from a previous buffer?\n. @seantcanavan : yes, you need to create strings and other objects outside of the table that references it, the error you got enforces that. See http://google.github.io/flatbuffers/flatbuffers_guide_tutorial.html\n. You can copy sub-tables out of a FlatBuffer using reflection::CopyTable in C++. A table may refer to non-contiguous data, so that's why this is a non-trivial operation.\nWhat is generally better is to make these nested FlatBuffers from the start (i.e. construct Header using its own FlatBufferBuilder, then store it inside the parent as a [ubyte] type). That way it is trivial to copy, and it works in any language.\n. Check out the union feature in the documentation.\n. Thanks :)\n. That seems useful, yes. This would be a traits style struct defined in flatbuffers.h that is specialized in generated code? That sounds useful, feel free to make a PR for that.\nAs for the helper code in that file, I am not sure what of that would make sense to bundle with the core FlatBuffers. String compares for sure. The concept or a Buffer, or a Root bundled with a Buffer doesn't make quite as much sense to me, because a) you're just wrapping std::vector, and b) use of these classes requires extra copies, which is what you mostly want to avoid.\n. You can also do it without a root_type, e.g.GetRoot<Testing::Test>().\nYou don't need to copy it into a std::vector, builder.GetBufferPointer() is pointer that can be passed to a GetRoot function.\n. Ok..\n. What compiler / version is this with? I'm guessing it is choking on final which is only enabled on GCC >= 4.7.\n. @TGIshib looks like this needs custom code for booleans.\n. Ah, for the library targets? I agree this is unfortunate, FlatBuffers should build with any standard lib. Can you make an independent PR for this?\n. merged it :)\n. The best solution is to upgrade to the latest FlatBuffers, as we have replaced the accessor objects with structs there, which can be copied rather than allocated, so the Get version of the method is gone.\n. Thanks!\n. This relates to my response here: http://stackoverflow.com/questions/39800023/add-existing-flatbuffers-object-to-flatbufferbuilder-c-sharp\nAgree that such code could be generated, but it is a little bit more niche usage than the existing generated code.\nOne more option is to generate Environment as its own buffer, and then nest it in the parent buffer, so it can be copied at a byte level when needed.\n. Looks like there was a bug with the map key sorting, can you try the latest version instead? https://github.com/google/flatbuffers/commit/6f37a52b740d540da4166804190c4ad86d955071\n. Thanks :)\n. Even though it doesn't matter because fbb never gets used, starting a table and never ending it is a bit weird. Please make the code an example of properly constructing a FlatBuffer, with its own FlatBufferBuilder if necessary.\n. Closing this for now due to inactivity, feel free to reopen if you have time to finish it.. This looks like a great fix, but can use some further cleanup, see above.\n. Closing this for now due to inactivity, feel free to reopen if you have time to finish it.. Thanks, this would indeed be great to have. It may be best to put all of this is a sub dir of tests to reduce clutter. The generated code from all languages also needs to be part of this commit (we commit generated code, since that makes it easy to see how any future code generator changes affect the generated code).\n. Hmmpf.. it is unfortunate that this adds 81 files.. but I guess there's no way to do this test with less tables...\n. Maybe we should not generate code for these tests for languages that do not have the map lookup functionality? that would reduce the size of this a bit.\n. I believe LookupByKey is only supported in C++/Java/C# so far.\n. Ok.. is this PR in sync with the LookupByKey PR? Should it maybe wait until that one goes in to merge this one?\n. Closing this for now due to inactivity, feel free to reopen if you have time to finish it.. The code above does not appear to have a problem. The problem could be in the way you create the sessions vector, its root, how your write or read the buffer, or obtain the root. You'll need to give complete code if you want me to debug your code for you :)\n. So.. you're saying the crash is happening when you read the data, but then when you restart the program, the program will read that exact same data just fine?\nOr it always crashes on certain files, when they're big?\n. None of FlatBuffers is thread safe (it doesn't do synchronisation for you). You can't write to a FlatBufferBuilder from two threads, you'd have to wrap it in a mutex first.\n. @gyanadeep : such crazy offsets means there's something wrong with the FlatBuffer data you're reading from. Either it was constructed in an illegal way, or it wasn't transmitted correctly. Hard to tell without seeing the code..\n. You cannot find out the length of the block without reading it, no.\nIf you know you have read more than a block already, you can figure out how long it is using the verifier, but that is not cheap.\nI recommend you keep it length-prefixed.\n. btw, added some functionality to make size pre-fixing part pf FlatBuffers: https://github.com/google/flatbuffers/commit/486c048a0d5963f83f3a0d6957e4dde41602e2e7\n. I don't think I understand. We use \"user facing\" signed types one size bigger whenever you read an unsigned value. So ubyte is presented as a short. If a ubyte contains 255, I would want that to be 255 as a short, not -1, right?\n. This PR gained a lot commits and changed files, something is not quite right?\n. Mostly looks good.. ready to go in?\n. Thanks!\n. Can you explain why this is necessary? If this is purely a code-size optimization I don't think it is a good idea, especially since usually the linker is able to de-duplicate.\n@andrewlu1 : How is that related to this PR? Note that Java API is \"unfriendly\" for efficiency reasons.\n. Thanks!\n. I presume you're using the latest master version, since changing the C# code to use structs is something that happened after the 1.4 release (the code may still say 1.4, since that is only updated upon release).\nDid you regenerate your code? In the new generated code, tables generated from your schema do not inherit from Table anymore, they have a Table member.\n. Yeah, the releases are all on github, we don't list those on the landing page.\n. I believe at best it allows you to add fields, it doesn't support the full optionality of any field like Protobuf and FlatBuffers do.\nI tried it at the time with VS2010, which wasn't working. Maybe there have been VS2010 fixes you can point to?\n. There is also a fix in progress here: https://github.com/google/flatbuffers/pull/4047\nCan you please cooperate with that person to ensure the LookupByKey functionality works well in all cases?\n. This should fix the issue, let me know if it does: https://github.com/google/flatbuffers/commit/424fc0c3acfeb8ce2d45192565108c7891626c7d\n. This was just fixed on master, thanks for reporting!\n. Can you be more exact about what does \"not work\"? What errors are you getting?\n. (sorry, already had fixed it before I saw this).\n. I just checked, but in our test, both with and without --scoped-enums, and it generates accessors for the enum type, not the underlying type. What accessor exactly uses uint16_t ?\n. Since pointers into the buffer are naked pointers, there's no place to store the buffer pointer along with it.\nI guess ideally FlatBuffers should have been designed such that the root pointer is always the same distance from the start of the buffer, but it isn't, as the root may store the vtable ahead of its start, which is variable size. It would be possible to generate a function that could compute it based on the vtable offset, the alignment, and the presence of a file_identifier.\nI can have a look to see if such a function can be generated.\n. Just added this commit which has such a function: https://github.com/google/flatbuffers/commit/6862b2ff08021c7ba474334a6e2a3f3b1fc0dee5\n. Thanks. This file is copied from the GPRC project, so this change will be lost next time it is updated. So really it should be fixed there, and should say something like // Generated by the gRPC C++ plugin.\n. The Create functions take long list of arguments that correspond to the field types. You can't make this into a generic function I think, but you're welcome to try :)\n. Note that you'll need 2 template arguments, one for MyType and one for MyTypeT. Not sure how that would help though, since the body of this Create function has to go thru and serialize each field, which is not something you can do \"automagically\" with templates.\n. Yup, we could add these as alternatives to the current code. Can you work on a PR?\n. Fixed elsewhere.. flatc is a command-line tool, so cannot be used on Android. But you can do the equivalent of what it does (JSON -> bin) using C++ on Android by using the Parser class.\nHere's an example how to do that from Java: http://frogermcs.github.io/json-parsing-with-flatbuffers-in-android/\n. This is the only way I know of. I see no log.\n. Yes there is. That is exactly what the link above is doing.\n. I agree with @msb-at-yahoo that this could also work with a general overloaded method, which would have the advantage of needing less template magic, and not having to bring things into the flatbuffers namespace.\n. Thanks!\n. Would it be better to stick the adding of the byteOffset to inside __vector, since every use of __vector would have this problem?\nDo any other accessors have this problem? None of the offsets add byteOffset, so that may generally not be supported?\nAlso if you make changes to generated code, make sure to add the generated code to the commit (run sh generated_code.sh).\n. Ah, that makes sense. Thanks!\n. We definitely have unsigned ints in our test schema, so I wonder how that can have been missing?\nEither way they should be equivalent to the signed version.. It looks like it may work just calling the signed version instead, since all it does is:\nflatbuffers.ByteBuffer.prototype.writeInt32 = function(offset, value) {\n  this.bytes_[offset] = value;\n  this.bytes_[offset + 1] = value >> 8;\n  this.bytes_[offset + 2] = value >> 16;\n  this.bytes_[offset + 3] = value >> 24;\n};\n. Note what it says here: http://google.github.io/flatbuffers/flatbuffers_guide_tutorial.html\n// This must be called after `Finish()`.\nvar buf = builder.DataBuffer; // Of type `FlatBuffers.ByteBuffer`.\n// The data in this ByteBuffer does NOT start at 0, but at buf.Position.\n// The end of the data is marked by buf.Length, so the size is\n// buf.Length - buf.Position.\n// Alternatively this copies the above data out of the ByteBuffer for you:\nbytes[] buf = builder.SizedByteArray();\n. If you name the field something that is not the same as the table name, the error will go away. Will add an error for this case in flatc to avoid this in the future.\n. yes, the interval of 2 is correct, since it is indexing into the vtable (which is made out of 16bit offsets). Those offsets then point to the actual fields. This is because fields are optional, and it only writes those that are present.\n. I'm not sure why it is not deserializing correctly, I would need to see full writing + reading code for that.\nChanging the value in __offset is definitely not fixing anything. If that is returning correct values then that is pure coincidence.\n. There is no file attached.\nPlease run the generated code unmodified. Like I said, your change doesn't fix anything, it makes things worse. The problem is likely elsewhere (e.g. with how you read/write buffers).\n. There appear to be compile errors, see CI.\n@rw?\n. @gonzaloserrano : sorry about that. Let's see if we can wake up @rw \n. I can only say whether it is acceptable once I see the full impact on flatbuffers.h. If that is minimal, then maybe. But right now I don't even see how this is possible without a major overhaul.. you'd need to reverse most of the code.\n. Yes, I would see if you can make your UPWARD implementation work with the standard test, to see what problems you'd run in to.\nCan you explain the need for upward building in more detail?\n. There currently is no guaranteed field order defined for FlatBuffers, and that is independent of buffer construction direction. All languages allow fields to be added one-by-one (in C++ thru the add_ methods), which means field order is under complete control of the caller. That means you could achieve any key ordering you wanted.\nThe JSON parser has its own ordering (it sorts by size of item, to reduce padding), as do the Create methods.\nNote as to your diagram, strings are not stored inline, they work over an offset. So contiguous keys out of multiple fields are only possible with scalars. And you can only enforce them being contiguous if you can enforce how the buffer is being constructed.\n. FlatBuffers does us a limited amount of C++11, and we sadly don't have a CI setup that tests with a variety of C++ versions, so this kind of thing happens.\nThe first error can easily be fixed, it's emitting a redundant , in code generation.\nThe second is the use of a lambda. This could be turned into a function probably.\nI could try to fix these, but I have no gcc 4.4.7 to test with, so who knows what other errors appear after :(\n. I just committed this which should fix the two errors you mentioned, let me know if there's more: https://github.com/google/flatbuffers/commit/290e9f270b93efc55d2124d6c73f3f48e033230c\n. We try to support older compilers as best as possible for the core of FlatBuffers (flatbuffers.h and base generated code), but I am not sure how easy it would be to fix the object API.\ngrpc/src/compiler/cpp_generator.cc is a file we use verbatim from the grpc project, that we would prefer not to fork.. Are you using C++ or Java?\nIn both cases it is a simple matter of adding the run-time and generated code to your project. Runtime C++ files are include/flatbuffers and in Java java/.\n. The android code has recently been updated with a gradle project.. This was already fixed a while ago. Which version are you using?\n. Thanks for the fix :)\n. I already responded to you here: http://stackoverflow.com/questions/40473817/dynamic-id-and-special-characters-for-serialization-with-flat-buffers/40474746#40474746\n. Hard to say without knowing how you tested. Is your Linux test somewhere here on github? Does Protobuf timing include allocating, populating and deallocating the object structure required for serialization?\n. Ah sorry, was not obvious this was ready for review.. Generally looks good to me, just some minor formatting.\n@rw any last comments?. Thanks! Looks great!. Thanks for the fix!\n. The vtable costs 2 bytes per field, and is shared among similar objects, so its costs goes towards 0 the bigger the buffer is.\nI'm not sure how you propose scan fields until you find the one you're looking for, as that would require field-id information to be present in the buffer, which currently doesn't exist. Adding that to the buffer would be about as costly as the vtable.\n. Where do you see this one vtable per object? If you look at FlatBufferBuilder::EndTable, you'll see that if it finds a vtable that's the same as the current one, it discards the current one. So there will be only a single vtable for Reference.\nAlso read the internals document.\nAnd again, how would you parse your variable length encoding? There are no type tags or field ids stored with data. This can only work if you always store all fields.\nFor storing Node, if they vary a lot, you may be better off with FlexBuffers: which can be more compact for variable data with lots of strings (work in progress here): https://github.com/google/flatbuffers/commit/8fb53aebf70b3cc0962a45bf7d094da0a028eaae\n. It's simple: vtables are shared if they have equal contents, irrespective of what type of table they're associated with.\nWhy does your data not get the full benefit?\nYour feature would often take more space than the current table mechanism, given optional fields, or fields that happen to have default values. I don't see a point introducing a 3rd type of object unless there was a very strong benefit. Generally introducing something new at the binary encoding level has huge costs (in having to support it in all our language implementations etc.) so has to be weighed carefully.\nFor special purpose encodings, I'd recommend storing those in a vector of bytes in whatever way you please.\n. Yes, the root must always be a table.\n. Yup there is, see the allocator argument to FlatBufferBuilder. You can implement this allocator such that you point it at pre-existing memory when FlatBufferBuilder asks for it.\n. Yes, it be nice if --gen-onefile would also be implemented for other languages.\n. That should generally be possible, but I doubt the Python implementation has an accessor for that. Since Python has no raw pointers, I am guessing this would have to take the form of \"copy vector data to string\", which could be added. @rw?\n. Anyone want to contribute a function for this?. On 2, I think it is ok to give people buffer access through an obnoxious name like getbuffer_little_endian or whatever.. then it is obvious that it is the clients responsibility to ensure they can deal with little endian data.\nAlternatively, only generate buffer access for [byte] / [ubyte].. Yup, such an accessor would be great. My Python is lacking, though.. what is the quickest way to memcpy something into another array? Anyone want to make a PR?. This has been implemented.. Are you able to read the other data (e.g. id and name) correctly? Or rather, what does the above JSON look like from the perspective of your app? Just to verify that the buffer is being read into the app correctly. If it is from a file, are you sure you're using binary mode? \n. The error appears to say: \"There is no member named 'back'\".\nstring::back is C++11.. what version of (I presume gcc) are you using? Are you able to upgrade your compiler?\n.back() could be replaced by [size() - 1], but I'd rather not modify that code since it is shared with the GRPC project.. Yes, it be nice to have GetPrefixedSize.. care to make PR? Not sure if we need PrefixedSizeByteCount, as that should be taken care of by other methods. Maybe GetFlatBufferFromSizePrefixed or some-such?. The reason we have no random access iterator is because the person that originally wrote it didn't think about it, and you're the first one to notice :)\nAre you able to make a PR?. Thanks :). This is using the unmodified benchmark in the benchmark branch, using what version of visual studio?\nThe original numbers were using VS2010 (our minspec target).\nSo some possible explanations:\n\nNewer VS has a faster allocator.\nNewer protobuf has optimized its memory usage.\nFlatBuffers has regressed in performance. I haven't tested this for a while myself, but somehow doubt it, as the core code that is exercised by the benchmark hasn't really changed in a long time.. I'm not sure what the System.Text.Encoding.Default is doing there. FlatBuffer data is binary data, so should not go through any text encoding changes.\n\nIf this came in over http, make sure it is application/octet-stream and that the data gets read into a byte array without ever being interpreted as text.. Your construction code appears to always write a params string, and your reading/writing code appears correct, so there's nothing in this code that indicates why it can be null. It is likely a problem elsewhere.. Tip: use explicit field ids, and/or --conform to check that you're not breaking compatibility rules.. I agree, [byte] would be better. Under C++, string is about the same, but in other languages it isn't. A PR to fix this would be welcome.. Fixed: https://github.com/google/flatbuffers/commit/69776b9e7eddd7aa9388f67d06f668e257f02704. Yes, --json means generate json, usually from a binary FlatBuffer, but an existing json works as well.\nYou can only specify default values on scalars in FlatBuffers, hence --defaults-json outputs scalar fields. The default for all non-scalar fields in null, not an actual value.. I agree it sounds useful, though first time I've heard of it :)\nSome possible paths:\n\nCreate a tool (in any language supported by FlatBuffers) that reads a .bfbs file (which you can generate with flatc -b --schema myschema.fbs, iterates thru that data and generates a sample json file.\nExtend the flatc JSON generator to generate empty objects (and vector and strings) whenever it encounters a null field, using a special command-line flag. Care has to be taken, since this can lead to infinite recursion :)\n. The way you output unions is not compatible with FlatBuffers, i.e. can't be read back in as-is. And yes, it would need to deal with recursion :). Thanks!. You can write them both in the same .fbs if you want. What language are you using? Did you follow the tutorial and read documentation? What exactly are you missing?. I agree a solution for sparse enums is needed. Current code simply doesn't output this mapping if an enum is deemed \"too sparse\". Like I said in https://github.com/google/flatbuffers/issues/4023 I am wary of forcing global objects on users of FlatBuffers that do memory allocation, so I'd propose that this functionality would take the form of:\nA static table of of enum/string pairs.\nA slow function that can lookup a string or an enum using that table (either direction).\nA function that turns that table into a std::map for faster lookups. Possibly in both directions.\n. @JRonak any idea?. This should be resolved by now.. As you can see here: https://google.github.io/flatbuffers/flatbuffers_guide_using_schema_compiler.html\nyou should put \"--\" between the binary file and all other arguments.\nThis is to avoid accidentally reading binary files, which could crash flatc.. Thanks :). The allocator was completely reworked in another PR, so possibly this PR doesn't apply anymore. Please start a new PR if additional fixes are needed.. This should be fixed by now.. Thanks!. What's the status on this? Seems useful to still merge this, @rw?. @zchee can you rebase?. This functionality is now partially covered by the above numpy support, though maybe we also want a version without numpy? Feel free to re-open.. We already merged this similar implementation, so this PR is now likely obsolete: https://github.com/google/flatbuffers/commit/3282a84e3068d2ff0ded5a683ceea0806da21ed6\nFeel free to make further improvements in another PR.. @evolutional may be working on this.. Identifiers were documented here: https://github.com/google/flatbuffers/commit/00d726fc4c49690b82ec58efe4ee1e313b9227fe. Ok!. This was fixed.. This was fixed.. Closing due to inactivity.. feel free to reopen.. Closed for inactivity, feel free to re-open if you can finish it.. This is already fixed.. This was fixed.. Fixed: https://github.com/google/flatbuffers/commit/ad0f48d7e798a16447c43185666ed06030b94304. was fixed here: https://github.com/google/flatbuffers/commit/dca33ddb75a035344cb6ea3e38b9123dcce9b5fd. was fixed here: https://github.com/google/flatbuffers/commit/dca33ddb75a035344cb6ea3e38b9123dcce9b5fd. Closed due to inactivity.. feel free to re-open.. Fixed here: https://github.com/google/flatbuffers/commit/dca33ddb75a035344cb6ea3e38b9123dcce9b5fd. Closed due to inactivity.. feel free to re-open.. Are you able to finish this?. Closed due to inactivity, feel free to re-open.. Fixed by: https://github.com/google/flatbuffers/pull/4404. @llchan @per-gron you may be interested in this up-coming blog post, which is based on your work: https://github.com/grpc/grpc.github.io/pull/550\n. Fixed: https://github.com/google/flatbuffers/commit/801e1b76995ade64fd89d17a7628b11c75d79967. Yes, totally needed. But as you say, potentially a bit of work. The sad part would be that CI would probably take 10x longer, since gRPC + dependencies dwarfs FlatBuffers.. Further discussion on the PR.. @evolutional may have an opinion.\n@Grimeh are you able to finish this?. Fixed here: https://github.com/google/flatbuffers/commit/dca33ddb75a035344cb6ea3e38b9123dcce9b5fd and here: https://github.com/google/flatbuffers/commit/432e7582c65aeb65b582724f601fc3202ecf9017. @per-gron is this PR still current?. Anyone else able to take it on?. Is there no more general #define for this libary? But sure, lets first do a\nPR for the best fix available.\n\nOn Tue, Jun 20, 2017 at 4:25 AM, Andrei Stoica notifications@github.com\nwrote:\n\nThe conditional check on ARDUINO at the moment fails to build with\nArduinoSTL when flatbuffers try to include utility.h instead of utility.\nOne potential fix, is to conditionally look for the include guard of\nArduinoSTL lib, i.e. ARDUINOSTL_M_H, which needs to be included anyway\nbefore the flatbuffers to pull in the STL headers.\nFor the cstdint, is a matter of user preference I guess whether it is\nincluded or not in the statically generated binary for the Arduino.\nIf you think this fix is good enough I can open a PR on it.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/4357#issuecomment-309724125,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AF3FeGwJ3g50T_7aAheHA9FHg87UzYtHks5sF6wZgaJpZM4N9dzG\n.\n. What's happening with this PR?. After createUserTracking do you call finish on fbb ?\n\nGenerally if you can't read a field correctly, something went wrong between creating the buffer and reading it again.. 1) Yup.. depends on the schema, but usually a fair bit bigger than the equivalent Proto.\n2) Hard to say what is going on here, without seeing code. What does StringToObject and ObjectToString mean? FlatBuffers serializes to a ByteBuffer, not a String. Also, FlatBuffers are not supposed to be de-serialized, so I am not sure what ToObject means. \nThat said, FlatBuffers was designed for maximum speed in languages like C++, and there are things Java doesn't support that makes it slower.\n3) The API is such that it writes the data linearly as you create it, which is most efficient, and yes, I agree, that is pretty restraining. But \"and later again check if you initialized offset or not\" that seems to be an artifact of your how you structured your code, since generally there is nothing to check. You can pass 0 offsets to a parent, and nothing will be serialized.. This was fixed.. Made some documentation fixes based on the above here: https://github.com/google/flatbuffers/commit/4b27c92910ebe57c1d21932df2a85c142516c7b4\nOther issues have been noted (Object API for more languages, Java performance, removal of deprecated fields) but will take longer to materialize :). High level:\n- Yup, no way to get an STL container to point to existing memory.\n- Protos can be parsed without knowing the schema (though the printout won't contain field names).. which you can't do with FlatBuffers. Yes, we could solve it using generated code, which would be nice.. I'll look into it.\n. - I don't think we'd want to support both forward and reverse order, as that would create too much complexity in the code base. We'd have to switch entirely to forward building, but this would be an API breaking change. Worth considering, but it's a pretty big step at this point.\nVersioning:\n- Yes, new fields are just ignored by old code, required or not. Even if the old uses the verifier, it was compiled from an older schema that didn't have the field, so it won't require it to be present.\n- Unless the deprecated field was required, all fields are implicitly optional, so any old code should have been testing for their presence before access (if it was a non-scalar.. scalars will just be a default value).\n- You currently can't remove it from the .fbs. The best I can do is allow you to write something like deprecated_field where right now you must write some_name:some_type (deprecated). The schema compiler needs to know there used to be a field there.\nPerformance:\n- It is likely other languages can still be made faster, we just haven't done much work on them personally. A binding through C could be a speedup for really slow languages like Python, but for decently fast languages like Java, going through JNI is likely slower than just doing it in Java, since the most basic field lookup is just using 2 offsets. Also, doing it all in Java is friendlier since Java users likely do not want to have to deal with having platform specific C++ code to compile and distribute.\n- It really is a very different encoding. We always store scalars as the declared type, so a long will always take 8 bytes, whereas Protobuf uses varints. Protobuf needs one or two bytes preceding each value for type/id, whereas we have vtables (which may be amortized or not), alignment etc. So it strongly depends on your schema and your actual data how efficient that will be.\n. @rouzier: @mzaks notes that this PR may not be complete if doesn't test a vector of bools.. he notes that additional changes in e.g. IsTypedVectorElementType were necessary.. That is not strictly necessary (you can just use TYPE_VECTOR, but it would be nice to have also, yes, to not store all the type fields.. That is correct, it would only need to change if TYPE_VECTOR_BOOL were added.. Ok, I'm a bit confused about what you're suggesting we do.\nYou're saying you want both a typed vector of bool (that's store as a uint8) and a bitfield?\nCurrently there's no typed vector of bool, so I am not sure what the problem is with IsTypedVectorElementType.\nAs for bitfields, I would not call them TYPE_VECTOR_BOOL since they will be entirely incompatible with other typed vector. They're better off as TYPE_BITFIELD or something. TYPE_VECTOR_BOOL could be used for a typed vector of non-bit bools, but having both may be overkill, especially since you can already use bools with TYPE_VECTOR if you want. So maybe adding TYPE_BITFIELD is sufficient.\nAlso, your asBool appears incorrect, it should return true for everything !=0, not nil.\n. fbb.Vector with a std::vector would currently fail, because the code assumes all scalars are available as a typed vector, which bools currently are not. So this would be a good reason to add TYPE_VECTOR_BOOL with uint8_t (not bit) elements, for consistency.\nI don't think vectors of bools should become typed vectors of uint if we have an actual bool type. We should definitely have a typed version, since paying 2 bytes per bool would be a bit wasteful.\nSo my recommendation, for now, is to fix things by adding a TYPE_VECTOR_BOOL. We can add a TYPE_BITFIELD additionally later, but that can/should be in its own PR.\n. I agree that the current binary representation isn't very good. I am just saying that:\na) I'd hope people don't use JSON with large binary blobs,\nand more importantly:\nb) I am not sure that it makes sense supporting an encoding that is not part of JSON proper, and currently will only work with glTF. It could be a FlatBuffers feature that people start to adopt, but I am not sure why we should be in the business of encouraging more efficient binary blobs in JSON :)\nThat said, not totally against it either, I just don't see a lot of evidence that it is useful to FlatBuffers.\n. @vrachels are you able to finish this, see my comments?. Looks good now.. but would be nice if the tests for these languages at least one case where they're being used. See https://github.com/google/flatbuffers/blob/master/tests/test.cpp#L1456 for inspiration.. Thanks, very nice contribution!. Running sh generate_code.sh seemed to miss a few changes in generated code, I added them here: https://github.com/google/flatbuffers/commit/ffddbdc7ab7c9a8b143c8fcb16ee083fa54dde89. Thanks!. I believe we fixed this recently: https://github.com/google/flatbuffers/commit/0e85eeef2c6ed3eb9ec201aaea6caa62612a8522\nAre you using the latest master version?. Looks good, thanks!. Noone should really ever include windows.h without first #define NOMINMAX.\nBut I don't mind merging this, if it helps people that don't know this :). I'm not sure why you'd want to verify a part of a FlatBuffer, or verify\nduring construction.\nIf you don't specify a root, no root helper functions are generated. But\nall the verify functions for individual types are still generated.\nOn Tue, Nov 7, 2017 at 6:28 AM, Andrew Hundt notifications@github.com\nwrote:\n\nIs it a good approach to verify parts of a flatbuffer or partially\ncompleted flatbuffers? I've noticed a verify function is not generated\nunless a root identifier is specified for that type.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/4478#issuecomment-342498559,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AF3FeFZ1q9rDW3DCZvMD-R6BuyZwYWv4ks5s0GkPgaJpZM4QO9aJ\n.\n. On Mon, Nov 6, 2017 at 9:16 PM, Sergey Avseyev notifications@github.com\nwrote:\nThe core of FlatBuffers is headers-only, and was generally made with\nstatic linking in mind.. any reason why you need this versioned shared\nlibrary?\nStatic libraries discouraged in Linux distributions, and they will track\nABI anyway.\nThis only matters if we wanted to distribute FlatBuffers as a stand-alone\nlibrary in a Linux distribution, which I would strongly recommend against.\nAgain, FlatBuffers is a tiny, mostly header-only, in active development\nlibrary. There is no advantage to dynamically link it, only disadvantages.\nIt does not make sense for it to have the same treatment in Linux as\nbig/stable things like OpenGL, GTK, FreeType, Zlib etc.\n\nWouter\n. I don't see how that reuses the accessor. Besides, using such high-level features is bound to bring its own inefficiencies.. FlatBuffers is intended to be as fast as possible.. We currently require CMake 2.8.. if possible, we'd like to keep the required version low, since bumping it may force a lot of people to update.. likely not.. JS can't even represent 64 bit ints in a single number, it needs 2 of them (since it uses doubles underneath).. I take it this is compatible with our current JS output? Any performance implications?\nIf not, a PR is welcome :). It looks like the reason that if is there is because the GenField call beneath it does not correctly look up the default value of a field. So your fix will generate the wrong value if the default isn't 0.\nThe correct fix is thus to remove the if condition and the else case (like your fix kinda does), but also add a default value lookup to the second GetField call inside GenField. The default value is available in fd.value.constant, but is a string, so needs to correctly be converted to the proper scalar type. We have a StringToInt but no StringToFloat yet.\nSo yes, a bit messy, which is probably why it wasn't implemented. Wanna have a go at it and make a PR?. There's already similar functionality in DestinationType/DestinationMask/DestinationCast.. why is this entirely new function needed?\n. Is this the fastest way to do a default argument in JS? Would it be worth generating two methods instead to avoid the if?\n. C# uses Pos() and GetPos(), so might as well use a similar scheme?\n. No, just wanted to make sure you were aware of it. If merging the two gets too convoluted, leave as-is.\n. yes, that would work.\n. This appears to be a bounds check on the whole buffer? That seems rather inefficient, given that it does not protect against most writes in wrong locations. Also it returns false if it fails, which hides the error, it should really be an assertion failure (whatever that is in Go).\n. You could roll the looking up of the offset and the if-then into MutateByte, rather than it being in generated code.\n. let's use --use-latest-cpp or something similar.. there may be other recent C++ niceties, and I don't think we should clutter the options with a flag for each. I'm expecting that people that want to use as modern C++ as possible to want to opt in to all of them. Sounds good?\n. Ok, I think you're right, it may be better to have the individual flag you have now. We can always add a --use-latest-cpp later that enables multiple options.\n. Is this correct? Doesn't pos point to the length of the vector, instead of the scalar data?\n. can you put this conditional inside __vector_as_arraysegment ? Also, rather than returning a blank segment, returning null would make more sense to me, which is also what the Java version does.\n. Oh, I wasn't aware of that. I do think we should promote the use of structs in C#, so feel free to use the empty struct approach. Maybe add this to the documentation?\n. Does it ever happen in JS that you data that is already UTF-8 somewhere (or ascii) that you may want to serialize (maybe coming from another FlatBuffer)? Or generally want to create a string from bytes? Should we also have a method for that usecase?\n. Ahh.. I guess we can't avoid an object here because JS can't represent longs? But then what is the user going to do with it? JS essentially only has doubles, so if the user has to convert this long object into a double to be able to access it anyway, does it make sense to return that directly?\n. is this endian-safe?\n. similar to my comment above, since all this decoding is potentially expensive, is there a way to represent a UTF-8 value directly in JS?\n. I guess this works too, though I guess all other languages add these to the table in idl.h, so that you can use a single macro instead of switches to generate these. Fine either way though.\n. love it that you ported these as well!\n. While I guess there's a lot of utility to these style of comments, it does make things a bit verbose, and the generated code long. I guess that's worth it?\n. Ok, agreed.\n. Looks like you are correct. flatbuffers.Long keeps the lower 32bits in low regardless of endianness, even though the bits inside low may be in big endian order. I guess that had me confused.\n. No, lets keep it, it will be useful for further development.\n. Where did the other build types go, i.e. does it still build in debug for C++?\n. This doesn't seem to do anything other than reorder the if-else?\n. Does this file have to be in the root? We'd like to keep it as uncluttered as possible.\n. you're already calling this from a context where the language is C#, no need to check double\n. pull StringToInt out of loop?\n. This is not a good default, as this will generate code like \"Color.2\" which will give weird errors. Maybe have this function also generate the \"Color.\" part, so the default doesn't need to have it.\n. ok, makes sense\n. a union always starts with 0, but an enum may start with any number, so result may be incorrect. Instead, remove this if, and just return field.value.constant as the default value. I believe it should be impossible for your loop not find the value, so this should be ok. An assert/error would be better, but we don't have a mechanism for errors in codegen.\n. I added PORTABLE_LOCAL_PATH at some point because Windows ndk-build only works with relative paths. Does this change work on Windows?\n. Since this removes the & operation in Java, I presume you tested this as well, and it still does the same thing? It seems to make sense to me, i.e. it is going to take the lower 32bits of the long and interpret them as signed anyway.\n. why does this take an array of int? The type in the schema is ubyte.\n. cast not required anymore?\n. Yes, but for vectors this seems like a bad idea, since they now take 4x as much memory. Also, they are likely to be used with other code that already has things stored as an array of bytes. So please ensure this generates code as before.\n. If this is the same in all languages so far, why add it?\n. Can you check all code still fits in 80 columns :P\n. Not a big deal either way. Could be useful in the future, then again the only language I know that doesn't use \".\" is C++.\n. This seems overly complex code for no reason. It be way simpler to simply set the new length to the size of the current table. Powers of two in theory powers cause less allocations, but in practice this difference is minimal.\n. I guess sticking the #if inside of AssertOffsetAndLength is not going to yield the performance gain? (C# compiler doesn't cull empty methods?)\nAlso please indent the #if if you don't mind, so much more readable.\n. That sounds right.\n. why this indirection? Lets make sure that allocator is always valid instead.\n. if you use a lambda instead, you'd cut down on a lot of boilerplate code.\n. Note this has to compile on compilers as low as gcc 4.6.3 and visual studio 10, so use of newer C++11 features (move) are risky.\n. brace style is not compliant with google c++ style guide\n. assert instead?\n. return result ? 0 : -1;\n. values are not required, for example \"deprecated\" is an attribute without a value. in that case the value field should not be written at all (null)\n. these should not be required either, since most things don't have attributes, you should not write this field for them if there's 0 elements.\n. please document what this turns off (and indent it)\n. isxdigit takes an int, and we supply a char. what warning exactly is this giving?\n. Ok, which call(s) cause this warning? If its few, can we fix it instead by pre-fixing (void) ?\n. Please double check all code for styleguide issues: {\n. why Method? operator() is more canonical\n. keyboards? :)\n. not needed, the whole file is C++\n. not needed, VS doesn't define GNUC.\n. code style: use snake_case for identifiers\n. I think it be even shorter if instance variables be turned into arguments. Or at least more readable.\n. why do we need this function?\n. Let's keep this code on a diet then :P\n. Instead of these two in-progress languages, maybe say And more in progress generically.\n. Oh, you mean to avoid writing \"csharp_partial\" here? I don't think that's the biggest deal.\n. Setting this globally may cause problems with compilers that are not gcc/clang, such as visual studio. I would just add this flag to the gcc/clang lines below (I'll refactor these flags someday).\n. Maybe call this \"How to use FlatBuffers in C\", as the current wording equivalent to the other languages would appear to suggest that you can use it with C using code generated from the main compiler.\n. Why no Windows? I highly suggest your fix this.\n. Please keep the segments specific to C minimal and non-redundant. The issue you are explaining here is not specific to C and is explained elsewhere, so should not be repeated here just for C. All code snippets should be similar to their C++ equivalents, any explanation specific to C would be better off in CUsage.md\n. This is all very long. Please keep this file for the most common usage of the C interface, and move any discussion of alternatives and API options to CUsage.md if needed.\n. This is a really weird way to do namespaces in C.. I'd recommend doing simple prefixing in the code generator instead. Yes, it makes for longer code, but it is a lot more readable.\n. .c\n. FlatBuffers tries really hard to be cross-platform, and this would be good for any projects we officially endorse (in our docs) as well. Otherwise we'd have to qualify that we \"support C\".\nHaving a Windows VM around for testing is not hard, though if that is too much work an alternative may be to simply use one of the Windows CI solutions out there (e.g. AppVeyor) to build your project on Windows. With CMake, you can debug a cross-platform build entirely on your non-Windows platform of choice, before even trying it with Visual Studio.\n. Ok, that's weird, since the comment holds for all languages. Feel free to move it outside the C++ div. Though the general principle still holds, we want to keep per-language segments as small and as uniform as possible, for maintainability\n. I'd say it is more readable without the macro, since the macro results in weird looking code allover.\n. Let's actually remove both these in progress languages, I don't like making promises.\n. Maybe instead of Yes, write the lowest compiler version you support it working with?\n. The original ordering reflected when these ports were added, which also tends to reflect how much they've been used (maturity).\n. don't make promises for later, description of these features can be in later commits.\n. several things here not adhering to google style, e.g. indentation of public, _ in constructor arguments (they need to be on the instance variables)\n. Why was this function added? I see it only called once.\n. indent?\n. don't put these kinds of explanations in code.. put them in the commit message or discuss them elsewhere\n. I presume this allocates another ByteBuffer (though with shared backing array). Is that faster than modifying bb directly and making sure its position etc is reset afterwards?\n. The big issue I see with this commit is that we're leaving this buffer and associated static data permanently allocated for each table type that has a string in it. This is more serious than the builder case, which would only have one copy. maxCharsPerByte presumably is also conservative, making things worse. Maybe for starters leave out the Math.max(, 128) ?\n. inline this function? especially since calling it twice does not give you two ByteBuffers, it's not that useful for other callers.\n. good point!\n. if this happens on the main thread of a program, that data will be allocated till the end. But yes, you're right, it is shared for all tables.\nI mean, don't have a minimum of 128 bytes even. Let the minimum be the size of the first string. Saves memory in the case of small strings, and shouldn't hurt case that read lots of strings.\n. please translate\n. does this if also need to be static?\n. factor out range check into function?\n. also, you need to check the endianness of the current machine.\n. factor out if-condition into a function?\n. is there no D equivalent of memcmp you can add to bytebuffer?\n. should be before kMAX\n. if this is a sample, it should maybe be samples/sample_binary.d to conform to the other languages\n. You copied this from Java/C#, where this function switches between upper/lower case. In D, this function is constant. So please remove this function, and put the actual character in place for the callers.\n. Memory leak? Where is this deleted?\nMaybe just allocate it on the stack?\n. sizeof(flatbuffers::soffset_t) is not the correct way to specify this. The offset is indeed 4, but that is actually because it is sizeof(flatbuffers::voffset_t) * 2 (the first field after the 2 fixed vtable fields). You can make this code slightly cleaner by instead calling FieldIndexToOffset(0) to compute the offset, and for extra documentation declare the field as F:string (id: 0);\nThis isn't the cleanest\n. This may end up in all compilation units.. instead make a BaseGenerator::FlatBuffersGeneratedWarning() that returns the string?\n. You appear to also be changing the Python type?\nAlso: align columns.\n. What is this file?\n. can this code pattern be abstracted? no need for the generated code to be any more repetitive than need be\n. All these test files are also in tests/\nThey should probably not be also under rust/ if possible.\n. Why does it generate these? Can this not be generic code that is part of the standard classes?\n. limit can never be 0? otherwise j may go to 0xFFFFFFFF\n. maybe a better name for s1/s2, or some explanation of what they represent, or how your algorithm works? Same below.\n. Would be slightly cleaner if you defined FLATC_SCHEMA_ARGS outside (before) the if, and then made the set inside the if just append to the existing variable.\n. init upper_bound_ to buf, so you don't need a conditional here.\n. not needed\n. please put this code in a #ifdef FLATBUFFERS_TRACK_VERIFIER_BUFFER_SIZE since verifier speed is important, and most people don't need this functionality\nuppper_bound -> upper_bound\nuse auto for types.\n. change the 4 into sizeof(uoffset_t), explain why this works\n. mutable not needed\n. Adding a random value here is not great. Maybe run the verifier a second time for the purpose of computing the size, and comment what you're doing.\nSee, this here shows that this functionality is problematic. If we don't know length, we have to pass an arbitrary number that is the biggest we think the buffer can be, to allow it to compute the real size. But in doing so, it may touch unallocated memory beyond the buffer. So rather than an arbitrary number, you MUST pass the size here of the buffer it sits in.\nSo a more accurate test would be to put the given FlatBuffer into a std::vector twice, append additional random data (e.g. a string), and then pass the size of that vector to the verifier, and see that it comes back with the length of the original string.\n. Apparently that last byte is never needed to be read, which is all it checks for :)\n. These kinds of comments are unnecessary for a method called GetBuffer inside a Verifier class.\n. no space between * and GetBuffer\n. Maybe remove const from that method instead, since the verifier already had other writable fields (depth_ etc).\n. maybe make this GetComputedSize to make it obvious how it was obtained.\n. This still needs to be #ifdef-ed. + may be cheap, but if isn't always.\n. Maybe wrap this logic into Verifier::VerifyIdentifier or something, so this doesn't need to go into all generated code. Also if you modify generated code, make sure to run tests\\generate_code.sh to make the generated code changes part of your commit\n. Keep this original test. Then put your test beneath it and make clear that this specifically tests the ability to compute the size from the data algorithmically.\n. This all seems very arbitrary. All we need is a vector, then append the flatbuffer twice, then check only that the computed length == original length.\n. Then I guess it shouldn't use it.\n. Why Internal? Computed or Calculated seems more obvious where this comes from. And GetBuffer doesn't need to be renamed I think.\n. Comments are only needed if there's something to clarify.\n. They test for different things, and as I said, this size computation should be optional.\n. It makes more sense to remove const where needed, since Verifier objects are typically never const to start with.\n. empty line here?\n. Accessing unaligned data like that is not even supported by FlatBuffers, so no need to test it.\nAnd in general, add some comments why you test this way.\n. spaces inside {}\nno space after *\nsame elsewhere\n. Thanks for the quick fix! This has a few very minor issues though:\n- index is a bit generic, line_length maybe?\n- use C++ casts like static_cast\n- type needs to be size_t, not int\n- spacing around + and -\n. Simplify this as suggested in https://github.com/google/flatbuffers/issues/3900 ?\n. no empty lines around this.\n. spaces around == and ?\n. make fit in 80 columns\n. This entire block of code appears copy pasted from the regular codegen for Create? Lift out into a function with a bool for gen_vector_pars\n. Please run generate_code.sh|.bat in tests to include the changed generated code in this commit, so we can see what this code changes.\n. normally is good to keep bug fix commits separate from new features rather than mixing them in one commit.\n. Ah yes, ignore my comment.\n. weird indentation?\n. No need to pass this variable, it is already in parser.opts.escape_proto_identifiers\n. keep it lined up if possible. --escape-proto-ids ?\n. indent\n. please fix your editor settings, it seems a lot of lines have changed indentation but no actual code changes. indent == 2 spaces, no trailing spaces.\n. can this be in a separate commit?\n. no need for *start_2, just cursor_[1]\n. Add a unit test for this case?\n. 3 spaces here.. make it 4?\n. make these vectors const\n. maybe just leave out the redundant == nullptr part?\n. This is going to add fields in the generated code for a lot of languages (which are not in your commit).\nMuch simpler to add a case to ValueTest() in test.cpp\n. I meant, instead of name == nullptr ? 0 : _fbb.CreateString(name) output name ? 0 : _fbb.CreateString(name)\n. Testing == against floats is very fragile, please use FloatCompare like the other cases.\nEven better would be to make TestValue templated so it can use both float and int, so you can do an int comparison with ==.\n. use ? :\n. indent #ifdef\n. indent #ifdef\n. ? :\n. Move this code into Verifier::VerifyBuffer. You can pass the file_identifier (or nullptr) as an argument.\nYou may also not need the InternalBufferSize calls etc. anymore.\n. Remove the +1 +3 +4.. no need to test on un-aligned buffers, these aren't supported anyway.\nOh, and only 2 copies (rather than 4) are needed to test this.\nMaybe use different verifier names rather than your use of {}\n. The variable doesn't have to be guarded really, the most important is the code in Check/Verify.\n. No, computed size is better, it indicates where this data came from\n. How about just return mb_detect_encoding($bytes, 'UTF-8', true) ?\n. we would like to keep the base objects without overhead of a virtual table, hence why the unions work the way they work.\n. Note that we compile on older compiler such as GCC 4.6.3 and VS2010, so this syntax won't fly.\n. I meant, instead of the 2 returns just use 1.\n. Yes... you can add the parser argument back if needed.\n. should the name have the word Sorted in there somewhere?\n. Why is the sorting done at reading time? If you look at the C++ implementation, it saves the tables in sorted order into the buffer. Then when reading, no sorting is required and the binary search can be done in-place.\n. see if you can move parts of these larger functions into a standard helper function, rather than generating code for all of it.\n. This { indentation is non-standard for Java.\n. my suggestion was to write return (bool) mb_detect_encoding($bytes, 'UTF-8', true) inside the if, because the way you've written it now, if mb_detect_encoding returns false, it is still going run the code underneath, which is not necessary.\nAlso, maybe it is useful to add a test case for this function, one with and one without utf-8 data?\n. I agree it is nicer.. but we already have some users of this code that really prefer to keep things non-virtual.\n. I guess you can temporarily just comment out the if to test the old code, that should have already been working.\n. virtual still needs to be removed..\n. this may need to be auto & for when the vector contains e.g. structs, which we don't want to copy. Better yet, remove the variable val and stick *it straight in the push_back.\n. this help text is a bit specific, since we'll likely extend this functionality well beyond just emplace_back and {}\nwould be better to say:\nc++0x (default): Minimal c++11 functionality at the level of VS2010 / GCC 4.6.2).\nc++11: Code for a fully compliant c++11 compiler (VS2015 / GCC 4.?).\n. This functionality still needs to reinstated, either with unique_ptr or your Optional.\n. maybe we can have a struct that contains all the different tokens between the C++ variants, this will cut down on the number of conditionals we'll need now and in the future. See for example struct LanguageParameters in idl_gen_general.cpp as an example.\n. not sure why these are here.. can you rebase?\n. I believe we have a constant for the size of an offset somewhere.\n. Please explain what this does and why it is needed.\n. Since this is so performance sensitive, and the new method introduces another conditional, maybe leave the original method as-is.\n. please explain the invoked_static.\n. same as in Java\n. Use auto for variable declarations where possible.\nKeep within 80 columns, see the Google C++ Style Guide.\n. There's a LOT of language conditionals in this code, making this hard to read.\nQuite a few can be added to the language parameters struct at the top of this file. Some that cannot, and that are used frequently, could be made into a function.\n. This method doesn't seem to do anything. Can the caller just call CompareTo ?\n. Maybe an idea for a future improvement: Since Name allocates a string, if we could somehow compare directly against what is in the byte array, this would be faster. Though I guess that means val would have to be supplied as UTF-8 bytes as well.\n. Since we are generating this binary search specifically for this type, we might as well in-line KeyCompareWithValue\n. All our other #defines are in all caps, so maybe this one should be as well?\n. Not sure, why is it needed in Java?\n. Is there no way around this? This will allocate an object for each integer.\n. It seems inelegant to rely on reflection for this.. maybe it would be better to add a createSortedTableVector to Table which the real implementation can override, and this function can all.\n. maybe make it non-static for the purpose of being able to override it?\n. ok, I guess leave it as-is.\n. Since it is only needed to create sorted vectors (not normal vectors), and since you likely already have such an object from creating the elements of the vector, I think this is ok.\n. Adding it in the middle rather than at the end will break all reflection files out there.\n. same here, this is an incompatible change. has to be at the end, or use ids. or maybe reuse index for this purpose.\n. asserts just produce a crash for the user. if people start making schemas with arrays in them, then unsupported languages need to get a proper error message somehow.\n. in reality the size needs to be even less in most cases, as a struct is stored in-line in a table, which needs to be addressable over 16bit vtable offsets, so an array of doubles can only have 2048 elements minus whatever table elements come after it. But we can't necessarily compute this exactly, since there may be multiple structs in the same table, etc.\n. we can have structs in structs, so arrays of structs in structs would be logical to support.\n. so the current syntax is field : int 4.. I guess that's cool. I would even recommend writing it as field : int4 in examples/docs, which has a similarity to HLSL/GLSL vector types.\n. Though I guess that would need support in the lexical analyzer or type parser, so maybe not.\nMaybe field : [int 4] would be better, to make it similar to vectors and clear it is an array. \n. never mind, just saw the syntax below. I guess that's cool.\n. we may want to test a few more types, in a few different alignment scenarios. Like in the current case you can't tell if you're doing padding correctly.\n. please also put this in MyGame.Example namespace, so once support for other languages arrives we don't have an explosion of folders.\n. note that data is stored in little endian format in structs so this is not great. We should provide a function with an index. If we want to provide a pointer accessor, we should maybe call it b_LE or something to make the risk more obvious.\n. we're missing documentation. use of arrays should be added to the schema docs, and the tutorial.\n. subclass\n. createSortedVectorOfTables would be more consistent with the method above it.\n. What if it is a very long vector? Having to copy the offsets out of the vector seems really unpractical, it be better if lookupByKey directly worked on the vector in the buffer.\n. Yes, that would be best.\n. It's nicer to handle errors in a centralized location, so I'm afraid we're going to have to pass an error back. To make that simpler you can stick a std::string error_ in BaseGenerator that you then check for not being empty in flatc.cpp.  And return early from this function.\n. the performance of this function is entirely down to this loop, so it may be worth caching bb.array() outside of the loop just in case the compiler is too dumb to do it.\nAlso, this code does not support UTF-8 or unicode at all. bb.array() may contain unicode sequences, which will typically cause the comparison to fail, or worse yet, it could actually cause a false positive.\nTo do this correctly, we'd either have to do UTF-8 parsing on the bytes as we compare them (probably fastest since we don't have to unpack all characters that way, but I'd hate to add yet another UTF-8 parser to the code). Alternatively, we unpack the entire string into a String first. This is easier, but is very inefficient, and will cause large amounts of garbage generated by a binary search.\nThe third alternative is to leave it as-is and document that this lookup only works with ASCII subset. I doubt that will be acceptable.\nActually, a 4th option is to first convert the search string (key) into a UTF-8 byte array, and then do bytewise comparison. This is decently efficient, since it only needs to be done once for the entire search. This sounds the best to me. You can early out if lengths are different, and maybe use Arrays.equals for the actual comparison.\n. Please use the style of the rest of the project, i.e. * instead of *.\n. Maybe put the changes in this file in its own header? they have no dependencies on the rest of idl.h, and if it is its own header, then it can be included/parsed as a C header.\n. Why did you remove the namespace around these types? could have renamed grpc_cpp_generator into grpc_generator, because entirely without namespace is a bit unsafe.\n. this (and the decl below) are already in schema_interface.h?\n. already included by cpp_generator.h\n. If you could improve the Python/Go generators while you're at it, that be best I guess.\n. maybe round it up? right now, if you pass 4 you will get 0, which will cause problems.\n. How does .NET Core affect us? It appears to have reduced libraries (which we don't use much anyway) but should have the full language functionality?\n. Any idea if my use of interfaces is safe? Did you run your benchmark?\n. interesting that the input to strtoll is const, but the end ptr is not.. I guess we'll inherit that weird behavior.\n. please use the style of the existing code, e.g. char *end.\n. if (!*end) ?\n. Thanks for testing. Decode has the biggest speedup, but isn't that essentially a no-op? Isn't all the object access in Traverse? If so, the speedup in Traverse is not particularly impressive, though it may also mean it wasn't really bottlenecked by accessor creation to start with (was your old code re-using accessor objects?)\nYes, it breaks the API somewhat, which is unfortunate, but I need to represent the optional nature of things somehow, and felt that doing so with Nullable was the most idiomatic. I am not sure why they chose to require the .Value accessor, but so be it.\nYes, AOT will likely be awesome. Never been a fan of JITs and VMs :)\n. Ok, will do.\n. please use the style of the existing code, e.g. char *end.\n. We shouldn't do all this hard-coding. A better way would be to have an additional function pointer in the structures above (which may null), for the GRPC code generation.\n. \"Generate GRPC interfaces for the specified language.\"\n. Why a map? just do the same as generator_enabled.\nActually, grpc_enabled can be a single bool, as it simply works for any language that has generator_enabled\n. we don't need to store \"--grpc\" as it is the same for all of them.\n. All Python files are in the namespaced directory Mygame\\Example, so should this file.\n. the loop should go to found on the first match. The way you've written it the Error is never executed!\n. this should be checked by itself, not inside this loop.\n. outside the loop.\n. this was already checked above?\n. has_grpc_lang_set is not needed. just use grpc_enabled\n. These tests should run unattended, i.e. it should quit automatically after 1 rpc has succesfully been sent back and forth.\n. Maybe make this a command line argument?\n. This FlatBuffer is unfinished?\n. name is an offset to inside an unfinished FlatBuffer.. what is that supposed to do?\n. we'll need more extensive testing than this, see e.g. the C++ code.\n. Are you going to move these files to java/com/google/flatbuffers/reflection ?\nAlso, please add a line to reflection/generate_code.sh to automatically generate these files.\n. Google code style guide says non-const arguments like namespaces should be passed by *\n. Why not pass the vector as argument like the previous function? that avoids potentially costly copying.\n. are there any other places where ns,components is used that this needs to be called instead?\n. please use Google C++ style instead of Java style: component_start. Same for other variables.\n. use auto for local variables where possible.\n. use for (;;) { .. if (..) break; .. } instead of while to avoid duplicating code like searching for .\n. Google C++ style guide: indentation = 2 space\n. enum_value\n. Sounds good. Just making sure :)\n. ok!\n. Ah yes, we can do that as required (not all languages use namespacing like Java). My comment was more about the use in Java, making sure we cover all cases.\n. probably any_generator should not be set here, since this relates to languages.\n. indent\n. indent / align\n. instead give error here: \"GRPC interface generation not implemented for this language\"\n. \" for \" should be part of the previous string\n. This error check can be removed entirely, along with that variable. If someone writes --grpc without a language, you'll get no options: specify at least one generator. already.\n. Yes, less generated code is better, assuming it doesn't affect performance much.\n. this cannot be called so can be removed?\n. a line with no effect?\n. what does this do?\n. Yes, but maybe give it a more specific name if only used by reflection, and also use __ like the other methods.\n. For methods in Table, starting with __ is better to avoid clashes with any field names.\n. missing newline?\nalso in the .bat version?\n. nice!\n. How does this relate to the current PR? If this was a fix, could this have been made into a seperate PR that can be approved seperately? Smaller PRs for individual items are easier and quicker to review.\n. indentation\n. maybe remove this function altogether?\n. If you make a seperate PR, and we merge that, then the change would automatically \"dissapear\" from the diff of this PR since it is already in there.\n. This doesn't look correct to me, you should be testing the result of __offset(26) against 0, much like the methods above it. What is the - 4? And using the offset from the end of the array is incorrect, it is better to modify lookupByKey such that it can use the offset returned by __vector directly.\nIn other words, the code should probably look like this (with a fixed lookupByKey):\nint o = __offset(26); return o != 0 ? Monster.lookupByKey(__vector(o), key, bb) : null;\n. The person that implemented that was using an offset to vector from when the buffer was constructed (these offsets are always relative to the end of the buffer). This is a bug though, since usually this offset is not available during reading :)\nSo yes, please fix that. Do not worry about breaking anything, since the current code is entirely broken already.\n. please finish the buffer as well.\n. here it is adding both the L and an (int) cast, that may be a bit superfluous?\n. This may not be needed in C#\n. that's not a very clean way of doing things.. please instead pass a bool to not have bb or 0!=bb not be generated in the first place.\n. indent.\n. The problem you're fixing may also be present in C#. It be nice to fix them both equally.\n. Where is __lookupByStringKey ?\n. Functions like this (two conditionals) are going to make binary search very expensive in Java. I'd prefer it if the binary search implementation would directly use < and = inline rather than these functions. I understand that doesn't work correctly for unsigned numbers, but I think their usage will be rare (strings and signed numbers make more sense).\n. Ooh, looks like the person that added this variant introduced a bug. Good find!\n. Hmm.. this is a really terrible level of duplication of code, I think I preferred it as generated code, easier to maintain.\n. remove spaces in ()\n. rather than modifying the existing code (and breaking the current API), please just make the templated version an alternative way of calling the same thing.\n. The amount of changes appear pretty minimal, most of the code is identical. Besides maintenance, the code bloat is worse, since now every user of FlatBuffers gets 11 copies of this function, versus only the users of the key feature. That is likely a lot less on average. Also, if we want to update C# to match Java, we need yet another 11 copies of this code, whereas with the generated code this is much easier to keep in sync.\n. please use spacing conform https://google.github.io/styleguide/cppguide.html\n. Why was this needed? There's already the function FullNamespace\n. don't see any types in this file, why is there a copy?\ntsTest -> ts_test\n. please just use all the existing files rather than making copies.\n. yes, all these if-thens can use some cleanup. prefer to stick them in a table (see LanguageParameters in idl_gen_general.cpp for an example) where possible, and otherwise think how to represent the code with as few if-thens as possible by sticking them into functions where needed.\n. if (field.value.type.base_type == BASE_TYPE_BOOL)\n. This is not about inheritance, it is about duplication of code. I think it was fine as generated code.\n. I think the code maintenance issues are a bigger deal than a little bit more JIT work (and only for those people that actually use it), but that's just my view.\nKey lookup through reflection is currently not supported in any language, not even C++. I suppose it could be made to work with some generic binary search (convert each key type to string?) but I haven't given it much thought yet.\n. maybe inline this since it is not being called anywhere else.\n. Hmm, that's a shame it has to be in the flatbuffers namespace, but I guess there's no other way.\n. Inline this function since it is only called once?\n. has this code been tested? this if asks if the len to be written is bigger than the existing data, which is almost always true. You probably end up with a huge buffer this way :)\n. A lot of this code is the same as for the downward case, would need to be refactored to have less duplicate code.\n. How does this work? This function returns a pointer to write to. It points to above the new space, not the start of the new space.\n. Please create a test. I don't think what you're doing here can work at all. It would write all elements in reverse order, which would not create a valid FlatBuffer, as a lot of items rely on adjency. See the internals document.\n. To be very consistent, maybe.. but they're also rather big. I guess you had to put it here to make SetNameSpace calls be readable, so it is ok to leave it as-is\n. Rather than another top level function, maybe put this under grpc/test_go\n. Can you make your sample work from the existing monster_test.fbs file, much like the C++ test?\n. rather than just grpc.go, this probably needs to be prefixed by the schema name, e.g. monster_test_grpc.go, and may need to live in the tests dir so we can easily regenerate it along with all the other generated code.\n. @rw does this file and the following one look good to you?\n. this could go in grpc/test_go\n. would prefer a test based on the existing test schema.\n. this file is copied verbatim from grpc-go?\n. what is this needed for?\n. not sure why you reformatted this, as it breaks the Google C++ style guide.\n. please don't reformat things unrelated to the current PR.\n. Undo\n. Undo\n. Undo\n. why was this removed?\n. these classes appear identical to the ones used for C++, why are they duplicated?\n. This should probably be renamed to have Go in its name, and the above function can then be inlined.\n. Not sure what is customary in Go, but one file seems better to me.\n. To allow future updates from grpc-go, it is important that this file remains unchanged. If you can factor out those changes such that they happen in different files, or if there are ProtoBuf specific things in this file, they should be factored out with a PR to grpc-go\n. But that code now only exists for Go, not for C++\n. what do you mean? if changes are needed, refactor such that both can use it.\n. No, I meant rename this function with Go, since it appears Go specific. Then you can inline the function above.\n. why is this functionality removed?\n. again, why is this called GenerateGRPC when it only calls the go generator?\n. Again, this appears the same as the C++ version. Please share as much code as possible.\n. Ok, typically when you ask people to review a PR, it not be in an unfinished test state. If there are things missing, you clearly mark them as such. From the current code, I can't tell what you still intend to change, and I've already commented on these before.\nAs to what languages get generated, when you specify e.g. --cpp --grpc it will only generate code for C++, not for Go. Maybe follow the pattern that is in the Python PR in progress: https://github.com/google/flatbuffers/pull/4014/files\n. See my comment below.\n. please format this code. does Go have a line limit as part of its code standard? if not, please use 80?. keep style of surrounding code, space around operators.. Going via this fieldGetter is slower, and requires us to convert floats to ints etc.. all to avoid generating code for this function? The generated code version was fast, simple, and no duplicated code to maintain.. The generated code version is still here also? Why don't we use just this?. indentation. indentation. whitespace formatting of if/else, please check Google C++ Style Guide, or use clang-format (on only your changes) withe the Google style.. maybe put a - in front of these lines so it is obvious there are 2 examples, not 1.. Maybe fix the english of this example.. besides language, maybe ask people to also mention other relevant things, like compiler version, OS, FlatBuffers version (1.4 vs master), etc.\nSo an example could be:\n- Crash when accessing FlatBuffer [C++, gcc 4.8, OS X, master]. Any more detail under what circumstances this happens? Compilation units should be independent, so this is really weird.. some comments to document this function?. maybe call obj root, to indicate that it must be the root type?. it be better to pass a buffer (const uint8_t *, size_t) instead of a Table, since the latter fails to verify the root pointer. If you do that, then you can also create the Verifier internal to this call.. as much as I like asserts, the caller is calling this function because is_struct was true, so this one is a bit redundant.. Please use Google C++ Style Guide for all indentifiers, so in this case parent_table.. check for required centrally (i.e. where you're iterating thru the fields), which would remove the code duplication between the if/else here and below.. scalar fields don't need to be verified. All you need to check is for the whole vector to fit in bounds.. fit more arguments on one line :). Rather than introduce a new boolean, you can just check below if typefield is non-scalar.. Hmm.. would be worth generating more stable identifiers then.. or leaving these changes out of the PR?. ",
    "julienschmidt": "See the Benchmarks section in the docs:\n\nCap'n'Proto promises to reduce Protocol Buffers much like FlatBuffers does, though with a more complicated binary encoding and less flexibility (no optional fields to allow deprecating fields or serializing with missing fields for which defaults exist). It currently also isn't fully cross-platform portable (lack of VS support).\n. \n",
    "jbenet": "Useful for comparisons https://kentonv.github.io/capnproto/news/2014-06-17-capnproto-flatbuffers-sbe.html\n. ",
    "lilith": "@gwvo Could you explain how flatbuffers is more flexible? I'm having a hard time understanding (at an API level) how flatbuffers improves on cap'n'proto. For my use case, perf differences between the two are negligible.  Perhaps an example of a common scenario which flatbuffers handles better than Cap'n'proto could shed clearer light on this?\n. Thanks for the clarification! I'd read this, but wondered if there were other aspects to consider. My use case is for representation of a graph of small nodes, so there aren't any god objects likely to grow extensively (and since I'm using it as IPC and not over the wire, space is not a concern). \n. ",
    "nopcoder": "Add - #include \n. ",
    "bvgastel": "Profiling a similar case showed a lot of overhead, but maybe it is 'just' the doubling of the buffer each time, instead of reallocing each time something is appended. Or I was confused about the exact meaning of +=.\nBut there is still an optimization opportunity here: is there any way to estimate a reasonable amount of bytes beforehand to minimize the number of realloc needed?\nAlso I would like to see the option to direct output it to std::cout.\n. I agree. I was confused and too quick to judge. My apologies.\nI still would like to see a streaming API to deal with large datasets on memory constrained systems (both reading and writing), maybe I will open a different ticket for it (as it not performance related).\n. I have one project that depends on an external JSON REST API, which sends over 1.5 Mb of decompressed JSON. The devices that should be supported are Android phones with 20 Mb of heap space available (so a continuous block of 1.5 Mb is almost impossible). Only because of the streaming APIs it is possible to even run on such devices. I can also imagine the use on constrained embedded systems like Arduino.\n. ",
    "diffuse": "Is anyone working on this?  I've started working on this and I've created an idl_gen_c.cpp and added the necessary code to flatbuffers.h and to flatc.cpp.   The code generation piece is far from finished, but I was wondering if anyone is already working on this functionality.\n. I've had a little more time to work on the C binding.  I am working on creating a C version of flatbuffers.h.  It will exist as flatbuffers_c.h.  I will be creating a fork within the next week so I can start getting some input for the C binding. \n. I haven't worked on the C binding in a long while.  I don't see me completing it any time soon as I have become too busy.  Perhaps when I get some downtime I can begin work on it again.\n. Do you know if anyone else is working a C port?\nOn Mon, Jan 5, 2015 at 5:38 PM, gwvo notifications@github.com wrote:\n\nOk, good to know. Just note on this issue if you do work on it again.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/7#issuecomment-68795139.\n. \n",
    "yynil": "C binding is very important for some special cases like iOS SDK prior to iOS 5.0, which doesn't support c++11. I'm also wondering if i could help to add a C binding. \n. ",
    "rw": "Given Go's simplicity (i.e. lack of expressivity), the Go port might be a good point from which to begin writing the C port. The C++ version is more concise and thus maybe harder to port.\n. @mikkelfj Interesting!\n. I made the license a comment, not an echo. Makes sense :-)\n. This is a large and non-urgent change. I'm happy for it to just initiate discussion.\n. Please let me know your thoughts on it: https://github.com/google/flatbuffers/pull/36\n. Unlike the Java version, the Go port does not overwrite data when deduplicating vtables. This is a step towards basing Builder on io.Writer, instead of a byte slice, for better generality.\n. I've force-pushed some small updates: code cleanup in go_test.go and removing all unnecessary calls to panic.\n. The cherry-picked commit is: https://github.com/google/flatbuffers/commit/74d5f3701fd19ca13b8fe69d1cf54002e11416da\n. Nice!\nOn Sep 16, 2014 4:14 PM, \"gwvo\" notifications@github.com wrote:\n\nthis is in.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/pull/70#issuecomment-55827970.\n. FWIW, I built this successfully just now on OSX 10.8.5 with a\ncustom-installed gcc 4.9.0: make clean all\n\n(Note that I had to remove '-stdlib=libc++' from CMakeLists.txt to keep g++\nfrom complaining.)\nOn Fri, Sep 12, 2014 at 9:22 AM, gwvo notifications@github.com wrote:\n\nJust compiled it on OS X with the included Xcode project, and get no such\nerror.\nIs there anything in particular you're doing differently?\nI'm guessing an #include  would fix it, but I wonder why it is\nnot needed in my case?\nOn Thu, Sep 11, 2014 at 11:35 PM, Valient Gough notifications@github.com\nwrote:\n\nOn OSX:\nflatbuffers/util.h:152:3: error: use of undeclared identifier 'assert'\nassert(!(ucc & 0x80000000)); // Top bit can't be set.\n^\nincluding fixes it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/74.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/74#issuecomment-55426378.\n\n\nRobert Winslow\n@robert_winslow http://twitter.com/robert_winslow\n. No complaint, I'm saying I get no header errors :-)\nOn Sep 12, 2014 10:27 AM, \"gwvo\" notifications@github.com wrote:\n\nRobert: what is the complaint?\nOn Fri, Sep 12, 2014 at 10:13 AM, Robert Winslow notifications@github.com\nwrote:\n\nFWIW, I built this successfully just now on OSX 10.8.5 with a\ncustom-installed gcc 4.9.0: make clean all\n(Note that I had to remove '-stdlib=libc++' from CMakeLists.txt to keep\ng++\nfrom complaining.)\nOn Fri, Sep 12, 2014 at 9:22 AM, gwvo notifications@github.com wrote:\n\nJust compiled it on OS X with the included Xcode project, and get no\nsuch\nerror.\nIs there anything in particular you're doing differently?\nI'm guessing an #include  would fix it, but I wonder why it\nis\nnot needed in my case?\nOn Thu, Sep 11, 2014 at 11:35 PM, Valient Gough \nnotifications@github.com\nwrote:\n\nOn OSX:\nflatbuffers/util.h:152:3: error: use of undeclared identifier\n'assert'\nassert(!(ucc & 0x80000000)); // Top bit can't be set.\n^\nincluding fixes it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/74.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/74#issuecomment-55426378.\n\n\nRobert Winslow\n@robert_winslow http://twitter.com/robert_winslow\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/74#issuecomment-55433323.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/74#issuecomment-55434997.\n. @shaxbee thanks for your work on the Python port. You mentioned that help is welcome. To make it easier for us to collaborate, could you please list the status of the different aspects of the Python port? e.g. Code generation, runtime library, and test suite.\n. I'm taking a look at contributing some Python code. I'll know more soon.\n. We've got Python bindings!\n. I'm interested in adding it. What would the function signatures look like? FlatBuffers are not self-describing, so the usual Marshal/Unmarshal interface may be insufficient.\n. @xysu When you say \"programmatic way/interface to access fields\", do you mean storing data in a struct? Or in a map...?\n. I am strongly in favor of something like this. Would you like to to try making a pull request?\n. Yeah, Reset seems to make this unneeded.\nOn Aug 10, 2015 19:22, \"Wouter van Oortmerssen\" notifications@github.com\nwrote:\nLooks like it.. @rw https://github.com/rw ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/85#issuecomment-129534941.\n. Closing since this is fixed by #165.\n. Is this issue still relevant?\n. Has this been addressed?\n. Interesting. I'd like to see much more testing on this. Have you looked at the read tests in the C++ version? Fuzz-testing would be appropriate here.\n\nKeep it up!\n. Please compare with the Python 2/3 port I pushed today: #112. Maybe we can combine these?\n. This is a feature-complete Python port, derived from the Go version. I'm not trying to collide with other PRs, but instead provide more options for whatever ends up being merged.\n. Please provide feedback on my use of ctypes; it seems mostly unnecessary now but I would benefit from additional opinions.\n. Nice microbenchmarks.\nI am personally more interested in correctness than speed at this point,\nhence my emphasis on quite a bit of testing.\nOn Dec 22, 2014 10:33 PM, \"Zbigniew Mandziejewicz\" notifications@github.com\nwrote:\n\nI've tried ctypes and ended up using struct because it enforces value\nranges automatically, takes care of endianess and returns native python\ndata types. Also it seems to be much faster:\nctypes:\n%%timeit raw = struct.pack('<i', 534217); head = 0\nn = 0\nn |= raw[head]\nn |= raw[head + 1] << 8\nn |= raw[head + 2] << 16\nn |= raw[head + 3] << 24\nctypes.c_int32(n).value\n   ....:\n1000000 loops, best of 3: 994 ns per loop\nstruct:\n%%timeit raw = struct.pack('<i', 534217); fmt = struct.Struct('<i'); offset = 0\nfmt.unpack_from(raw, offset)[0]\n   ....:\n1000000 loops, best of 3: 268 ns per loop\nI'll steal the idea of explicit type definitions though.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/pull/112#issuecomment-67925661.\n. Also, what do you mean by steal?\nOn Dec 22, 2014 11:23 PM, \"Robert Winslow\" me@rwinslow.com wrote:\nNice microbenchmarks.\nI am personally more interested in correctness than speed at this point,\nhence my emphasis on quite a bit of testing.\nOn Dec 22, 2014 10:33 PM, \"Zbigniew Mandziejewicz\" \nnotifications@github.com wrote:\n\nI've tried ctypes and ended up using struct because it enforces value\nranges automatically, takes care of endianess and returns native python\ndata types. Also it seems to be much faster:\nctypes:\n%%timeit raw = struct.pack('<i', 534217); head = 0\nn = 0\nn |= raw[head]\nn |= raw[head + 1] << 8\nn |= raw[head + 2] << 16\nn |= raw[head + 3] << 24\nctypes.c_int32(n).value\n   ....:\n1000000 loops, best of 3: 994 ns per loop\nstruct:\n%%timeit raw = struct.pack('<i', 534217); fmt = struct.Struct('<i'); offset = 0\nfmt.unpack_from(raw, offset)[0]\n   ....:\n1000000 loops, best of 3: 268 ns per loop\nI'll steal the idea of explicit type definitions though.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/pull/112#issuecomment-67925661.\n. I'd rather you keep your PR at its current scope...\"lifting\" code is not\nthe kind way to do things.\nOn Dec 23, 2014 6:45 AM, \"Zbigniew Mandziejewicz\" notifications@github.com\nwrote:\n\nIf you don't mind I'll use parts of your builder implementation and put\ndatatype definitions in separate module as you did and remove hardcoded\ndatatype sizes.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/pull/112#issuecomment-67943562.\n. I think we are both miscommunicating. Reconciling our efforts is my goal.\nI'd rather not have flames here :-)\nOn Dec 23, 2014 7:01 AM, \"Zbigniew Mandziejewicz\" notifications@github.com\nwrote:\nI don't understand - you've mentioned combining my PR with yours in #110\nhttps://github.com/google/flatbuffers/pull/110. I'll go ahead and\nprovide own implementation of Builder then...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/pull/112#issuecomment-67944574.\n. Where do our implementations overlap?\n. That's important, but not as much as architecture choices. I'm more comfortable with the very strict byte-layout focused test suite #112 is based on. @shaxbee feel like working with me on a joint PR?\n\nAnyone else--thoughts?\n. @shaxbee Yes, please email me. Address is on my profile page. :-)\n. Beta testers: please evaluate this PR for merging.\nI force-pushed some updates:\n1. Add read benchmark for the gold example data (currently at 2100+ traversals/sec).\n2. Add write benchmark for the gold example data (currently at 1300+ builds/sec).\n3. Add the PythonUsage.md documentation file.\n4. Include dependencies when installing with setup.py install.\n5. Add code coverage reports to the test runner (currently reports 87%).\n6. Use faster struct packing.\nThis branch has feature parity with the Java and Go versions, supports both Python 2 and 3, and is thoroughly tested.\n. - Reified exceptions into their own types (in the exceptions.py file).\n- Sped up read traversals by 50%, to 3000/sec, by removing a supermethod call in the packer.py hotpath.\n. Increase code coverage to 97%: Add cases to generate and test conditionals not traversed with the 'gold' example Monster data.\n. @gwvo I'll address your comments in this thread, squash, and this should be good to go. I'll comment here when that's ready. Ideally we'd do a round of speed work afterwards, similar to #165.\n. Pushed some updates:\n1) Rebase on top of latest master.\n2) Update use of GenComment to match the new type signature.\n3) Squash commits.\n4) Comment out the 'typed Python'-style asserts.\n. @gwvo Yep!\n. Improved compatibility:\n- Remove all external dependencies for the runtime library and generated code.\n- Add Python 2.6 compatibility. CPython 2.6, 2.7, 3+, and PyPy are now passing, using the same generated code. \n. More tweaks:\nLift generated Python files into tests/MyGame, like we do for the other ports.\nTests will run if at least one Python interpreter is found.\nTests will run if no coverage utility is found.\nDisable benchmarks by default.\n. Now we disable Go/Java comparison checks by default.\n. Thanks @layzerar! Pushed.\n. @layzerar Almost there, incorporating some out-of-band feedback.\n. Factored out some duplicated code, made the runtime library PEP8-compliant, and made a number of other stylistic fixes. The structure of the code has not changed.\nThis is looking OK to me. Anyone else want to give feedback?\n. @layzerar Thanks, fixed string catenation in py_test.py's error message.\n. @gwvo Switched from mkdir to EnsureDirExists (from util.h).\n. We've got it on PyPI:\n$ pip install flatbuffers\n. Thanks for finding this case. LGTM.\n. Thanks, LGTM. FWIW, I'm considering changing all string accessors to return byte slices, since strings always cause an allocation.\n. Given that the allocation issue has been outstanding for years, I doubt they will be fixing it soon. Even if they do, we'll have to wait (at least) for the 1.5 release.\nWe should bite the bullet and just return byte slices pointing directly into the FlatBuffer data, and require the caller to copy or cast it, as they wish.\n. The problem with generating multiple accessors is that we can't guarantee they won't collide with other schema-defined fields. For example, a field called 'Name' would get 'Name' and 'NameBytes' as accessors. If there is a 'NameBytes' field (say, a [uchar]), then we'd get a collision.\nPersonally, I'd rather let the user make the choice whether to allocate a string, at the cost of some syntax noise. That means returning byte slices in all cases.\nAnyone else want to share an opinion?\n. - Removed remaining allocs during building, as reporting during the gold data benchmarks.\n- Added collision check in CheckClash for _byte_vector and ByteVector for the BASE_TYPE_STRING type.\n. Reduced number of backing buffer growth events, bringing BuildGold speed to within a factor of 2.5x ParseGold.\n. I'm hearing that users want to be able to provide their own backing buffer, to have more control over memory usage, and potentially reduce allocs. Something like \ngo\nNewBuilderFromSlice(...)\nor\ngo\nbldr:= &Builder{Bytes: myslice}\nbldr.Reset() // ensures correct initial state of the byte slice\nis what is being asked for.\nThis approach isn't as simple as it seems, though, because a Go Builder actually has three GC'ed objects: the Bytes slice that holds the flatbuf data, the vtables slice that holds already-seen vtables, and vtable which holds the current (if applicable) vtable.\n(Note that to set any of these values directly would require making them publicly visible. I'd rather not make vtables and vtable publicly visible, since they are just for the internal state of the Builder.)\nSo, if users want this, we have a few paths.\n1) Change the Builder constructor to take a Config-like object, then look inside of it to pull out any pre-existing buffers the user gave us. Each field would be optional. You'd use it like this:\ngo\nmyCfg := BuilderConfig{\n        InitialBytesBuffer: mySlice1,\n        InitialVTableBuffer: mySlice2,\n        InitialVTablesBuffer: mySlice3,\n}\nbldr := NewBuilder(&myCfg)\n(Simple use cases will just pass nil for the config.)\n2) Make each alloc'ed field publicly visible, then call Reset on an already-existing Builder, so that users could pass in anything they want. It would look like this:\ngo\nbldr := &Builder{Bytes: mySlice, VTables: mySlice2}\nbldr.Reset() // Sets up bookkeeping\nI'm not a fan of this because, like I said, we'd have to make the internal state fields public so that users could set them.\nFeedback?\n. @dgnorton That's an interesting idea. As a caller, that's simpler (but less flexible) than using a Config. One issue I see is that it makes internal bookkeeping code more complicated. Another issue is that the Bytes slice (the largest of these three objects) will lose its exact power-of-2 growth behavior.\n@gwvo Any thoughts?\n. An allocator callback in Go would be something like this:\n``` go\nbldr := NewBuilder(myAllocator)\nfunc myAllocator(existingSlice []byte, desiredCapacity int) []byte {\n  // Look in a sync.Pool or somewhere else for a suitable slice.\n// Or make a new one on the heap:\n  if existingSlice == nil {\n    return make([]byte, desiredCapacity)\n  }\nexistingSlice = existingSlice[:cap(existingSlice)]\n  extension := make([]byte, desiredCapacity - cap(existingSlice))\n  extended := append(existingSlice, extension...)\n  return extended\n}\n```\n(This was edited.)\nThis has the added advantage that we could use it for growing the byte buffer, too.\n@gwvo Is this the kind of thing you mean?\n@dgnorton Would you use this?\n. (Keep in mind that at this point, you could just have a pool of Builder objects and reuse them.)\n. I'd like to handle a public interface for allocations in a separate feature branch. Let's get these speed improvements shipped.\nA workaround (for those who want it) is to manually manage the publicly-accessible Bytes buffer, combined with judicious use of the new Reset function.\n. We should change the code generator to make a new type for the enum, and declare const values for it. For example:\n``` go\ntype TestType byte\nconst (\n    Red TestType = 0\n    Green TestType = 1\n    Blue TestType = 2\n)\n``\n. I suppose it would beTestTypeRed`, actually.\n. See #197 \n. @eproxus This would be a great patch. Are you up to contribute it?\n. @eproxus How's this going? :-)\n. Please have a go!\nOn Mon, Jan 11, 2016 at 6:19 PM, Eric Lagergren notifications@github.com\nwrote:\n\nFunny enough I'm fiddling with this right now. I could pick this up if\nyou'd like. I think I might be able to finish most of it tonight.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/197#issuecomment-170760811.\n\n\nRobert Winslow\n@robert_winslow http://twitter.com/robert_winslow\n. Solved by #4843. Pretty cool! How hard would it be to add tests?\n. @Downchuck I'd like to see this get in, but not sure where @ThomasBrixLarsen is on the port (code gen cleanup, tests, etc.).\n. @layzerar Sounds interesting, please open a pull request. It's easier to comment on changes when there is code to compare against. :-)\n. @layzerar Would you say this is a higher-level API that builds on the existing implementation?\n. @gwvo I agree. It was that kind of reasoning that made me ask about the 'layering' of API calls.\n@layzerar Do you think you can make this approach more cooperative with the way the code currently works?\n. @layzerar From the documentation comment for Finish:\nFinish finalizes a buffer, pointing to the given `rootTable`.\nIt's used to create the information that the generated GetRootAs* functions need.\nIn that example, mon2 is reachable through mon's test field.\n. Note that the benchmarks for PyPy are unfair, because (I think) its JIT hasn't warmed up yet. If you increase the iterations to 100,000 you should see a difference.\n. @layzerar Did you have any luck with this? Let us know if you want to talk through the design of it.\n. @layzerar Sorry to hear about that! I hope you're doing better now.\nA Cython port seems like a good idea. How do we make Cython optional? I don't want to make people install Cython if they don't want it.\n. @layzerar What happens if a user installs Cython after installing flatbuffers? Will the detection happen at install time or at run time?\n. @layzerar It sounds like you are saying that it won't work if a user installs Cython after installing flatbuffers.\n. @layzerar Shall we close this issue?\n. @layzerar Please run Python tests and commit the updated MyGame/Example/*.py files. That way we can see the effect of the changes.\n. Yes :-)\n. It would be nice to have this, but there are no tests. @splhack Would you add tests for this so we can merge?\n. I verified this works as expected. @splhack Please rebase (git rebase master) and we'll merge it!\n. @splhack Please take a look at the TravisCI result, looks like a whitespace issue?\n. great!\n. To test this, you might consider using reflect to check that the type is Color:\nhttp://stackoverflow.com/questions/20170275/how-to-find-a-type-of-a-object-in-golang/20170564#20170564\n. Or even:\n``` go\npackage main\nimport \"fmt\"\ntype Foo int\nfunc main() {\n    var x Foo\n    t := fmt.Sprintf(\"%T\", x)\n    fmt.Printf(\"is x a main.Foo? %v\", t == \"main.Foo\")\n}\n```\n. @eproxus Are you still working on this? I'd like to merge this once there are tests for it.\n. Solved by #4843. Anyone want to sanity check this?\n. Thanks for finding this. Would you like to contribute a patch?\nOn Jul 13, 2015 10:46 AM, \"Franken\" notifications@github.com wrote:\n\nFound two misconceptions at builder.py\nhttps://github.com/google/flatbuffers/blob/master/python/flatbuffers/builder.py\nfile.\nBoth are about 2Gb margin size of buffer. Actual sizes are much less than\n2Gb!\nFirst one here\nhttps://github.com/google/flatbuffers/blob/master/python/flatbuffers/builder.py#L96\n.\nUOffsetTFlags.bytewidth equals to 4, so this check throws exception if i\ntry to initialize builder with more then 16 bytes initial size.\nSecond one is here\nhttps://github.com/google/flatbuffers/blob/master/python/flatbuffers/builder.py#L241\n.\n2**20 == 1Mb, but not 2Gb! When builder tries to inflate buffer over 1Mb,\nit throws exception.\nAnd third one hidden between first two. Both checks of buffer size are\ninconsistent which may lead other bugs.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/234.\n. This looks fixed now that #235 is merged. Confirm?\n. Looks good to me!\n. @Hinidu done: https://pypi.python.org/pypi/flatbuffers/2015.08.19.0\n. This is a good idea. Would you contribute a pull request for it?\nOn Aug 4, 2015 2:49 PM, \"Eric Lagergren\" notifications@github.com wrote:\nI started using fb the other day and noticed I was doing this a lot:\nTypeStartVector(builder, len(buf))\nfor i := len(buf) - 1; i >= 0; i-- {\n    builder.PrependByte(buf[i])\n}\noff := builder.EndVector(len(buf))\nWhich is a lot of boilerplate if you have a handful of byte slices.\nA simple but useful utility function would be something like this:\nfunc PrependSlice(b *flatbuffers.Builder, buf []byte) flatbuffers.UOffsetT {\n    for i := len(b) - 1; i >= 0; i-- {\n        b.PrependByte(buf[i])\n    }\n    return b.EndVector(len(buf))\n}\nwhich would turn the first bit of code into:\nTypeStartVector(builder, len(buf))\nend := PrependSlice(b, buf)\nAdditionally, the when a boolean is inside the .fbs file the generated\ncode is something like:\nfunc TypeAddBoolValue(builder *flatbuffers.Builder, boolValue byte) { ...\nwhich is rather unfortunate because I then have to convert my bools to\nbytes which means I have this boilerplate:\nvar foo byte\nif boolValue {\n    foo = 1\n}\nNone of this boilerplate is incredibly terrible, but when I try to marshal\na bigger struct into a flatbuffer it gets a bit annoying to type all it\nout. Obviously I abstracted it into my own functions, but I figured others\nmight've ran into this issue as well.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/244.\n. Everything means code duplication in Go :-)\nOn Aug 10, 2015 18:26, \"Wouter van Oortmerssen\" notifications@github.com\nwrote:\nWe're still at a stage where API changes, if they fix obviously broken or\ninefficient existing API, should be permitted, with care. It will break\nsome people's code whenever they upgrade, but the fixes will be minor and\nvery beneficial for everyone moving forward.\nYou could generate code akin to PrependVector, though I would say at\nleast have a standard version available for all standard types could also\nbe helpful. Sadly that means code duplication in Go.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/pull/245#issuecomment-129517284.\n. @EricLagerg any thoughts on what @gwvo said above?\n. @EricLagergren Did you get a chance to try out codegen?\n. This is exciting! Thanks for the contribution. I think this is a great step forward.\n\nA few thoughts:\n- I'd really like to see more tests. JS has a weak type system and idiosyncratic casting rules. So, please port the tests from the Go or Python versions. Especially the byte layout ones, and the fuzzing tests. Code coverage reporting would also be good.\n- If this can run both in the browser and Node (without any dependencies) that would be extra awesome. Otherwise, we could make two JS libraries: the vanilla JS version, and a Node wrapper around it.\n(Edited to remove an irrelevant thing.)\n. Yeah, documentation about testing would be nice. I like your breakdown of the different types.\nThat said, users aren't expected to know or care how well-tested a port is. That's up to us, the implementors. We get to set our own (hopefully high) standards for code quality and test coverage.\n(The Go and Python tests are essentially the same IIRC.)\n. (One thing you didn't mention are the Go and Python tests that perform byte-level checks on the serialization primitives. They are extremely useful for developing and hacking on the code, but don't help as much once a port is done. They primarily are there to help the implementor add features piecemeal, instead of all-or-nothing.)\n. @evanw I'm getting a test failure when running ./JavaScriptTest.sh with Node 4.1.2. The traceback looks like this:\n```\n$ ./JavaScriptTest.sh\nassert.js:89\n  throw new assert.AssertionError({\n  ^\nAssertionError:  (./flatbuffers/tests/JavaScriptTest.js:259:1)\n    at Module._compile (module.js:434:26)\n    at Object.Module._extensions..js (module.js:452:10)\n    at Module.load (module.js:355:32)\n    at Function.Module._load (module.js:310:12)\n    at Function.Module.runMain (module.js:475:10)\n    at startup (node.js:118:18)\n    at node.js:952:3\n```\nI don't see an email address for you anywhere. Since it's hard to debug issues in a GH thread, would you send me an email and we can take this offline?\n. My local test failures were due to bad cloning from GitHub, all clear from here!\n. Interesting, thanks for opening this PR. Two things:\n1) We need to think about the pros and cons of enabling in-place mutation. @gwvo are there any mutation features planned for any FlatBuffers ports?\n2) @mnmtanish Please add thorough tests.\n. Yeah I'm fine with this, thanks @mnmtanish. Just need to see Travis passing again...\n. I'm ready to hit the big green button, the only thing that makes me uneasy is that this adds a lot of untested code to the project. In particular, only these functions are tested according to the Go cover tool:\nMutateInt8\nMutateInt16\nMutateFloat32\nMutateFloat64\nMutateInt16Slot\nHow simple is it to add unit tests for the other functions?\n. How about adding unit tests for each function? Make a builder, add a slot with a value (or the default!), mutate the slot, then check for the new value.\n. @mnmtanish Were you able to write these tests? Let us know how we can help!\n. Something like:\ngo\nt := &flatbuffers.Table{Bytes: b.Bytes, Pos: b.Head()}\n. (I'd prefer to have unit tests that are separated by function, though.)\n. Ah, my previous comment was opaque. I followed up by looking at the fuzz test, and using the field offset logic from there. The following code works for me:\n``` go\nfunc TestFoo(t *testing.T) {\n    b := flatbuffers.NewBuilder(0)\n    b.StartObject(12)\n    b.PrependBoolSlot(0, true, false)\n    b.PrependByteSlot(1, 1, 0)\n    b.PrependUint8Slot(2, 1, 0)\n    b.PrependUint16Slot(3, 1, 0)\n    b.PrependUint32Slot(4, 1, 0)\n    b.PrependUint64Slot(5, 1, 0)\n    b.PrependInt8Slot(6, 1, 0)\n    b.PrependInt16Slot(7, 1, 0)\n    b.PrependInt32Slot(8, 1, 0)\n    b.PrependInt64Slot(9, 1, 0)\n    b.PrependFloat32Slot(10, 1, 0)\n    b.PrependFloat64Slot(11, 1, 0)\n    offset := b.EndObject()\ntab := &flatbuffers.Table{\n    Bytes: b.Bytes,\n    Pos:   flatbuffers.UOffsetT(len(b.Bytes)) - offset,\n}\n\ncalc_offset := func(n int) flatbuffers.VOffsetT {\n    return flatbuffers.VOffsetT((flatbuffers.VtableMetadataFields + n) * flatbuffers.SizeVOffsetT)\n}\n\ntab.GetBoolSlot(calc_offset(0), false)\ntab.GetByteSlot(calc_offset(1), 0)\ntab.GetUint8Slot(calc_offset(2), 0)\ntab.GetUint16Slot(calc_offset(3), 0)\ntab.GetUint32Slot(calc_offset(4), 0)\ntab.GetUint64Slot(calc_offset(5), 0)\ntab.GetInt8Slot(calc_offset(6), 0)\ntab.GetInt16Slot(calc_offset(7), 0)\ntab.GetInt32Slot(calc_offset(8), 0)\ntab.GetInt64Slot(calc_offset(9), 0)\ntab.GetFloat32Slot(calc_offset(10), 0)\ntab.GetFloat64Slot(calc_offset(11), 0)\n\n}\n```\nThe calc_offset is necessary because there are hidden fields used to store object metadata. We don't normally call these methods, they are for generated code to use. Let me know how it goes!\n. In theory, the format is flexible. You can read about it here: https://google.github.io/flatbuffers/md__internals.html\nIn practice, we use the same format in all the ports. IMO that is best explained by looking at the byte-level tests I introduced in the Go and Python ports: https://github.com/google/flatbuffers/blob/master/tests/go_test.go#L433\n. What happens when the first write was the default case, therefore no data is written in the field? Is there an error when you try to mutate it?\n. @mnmtanish Okay, great. Please add some comments explaining the many tests in CheckMutate. In particular, please make it easier for me to tell if all combinations of tests are being performed. The general sections I'm looking for are:\n- Expected false && the value is unchanged, because there is no room in the flatbuf\n- Expected false && the value is unchanged, because the new value is the same as the old value (is this correct?)\n- Expected true && the value is changed, because there is room in the flatbuf\n. Nice work! I'd like to see the test code cleaned up to use table-driven tests (https://github.com/golang/go/wiki/TableDrivenTests), and the following cases covered:\nReturning false:\nMutateBoolSlot\nMutateByteSlot\nMutateInt8Slot\nMutateUint8Slot\nMutateUint16Slot\nMutateInt32Slot\nMutateUint32Slot\nMutateInt64Slot\nMutateUint64Slot\nMutateFloat32Slot\nMutateFloat64Slot\nMutateVOffsetTSlot\nAnd the true case:\nMutateVOffsetTSlot\nI found these by using the Go cover tool. For your convenience, I modified the GoTest.sh file to look like this:\n```\nGOPATH=${go_path} go test flatbuffers_test \\\n                     --test.coverpkg=github.com/google/flatbuffers/go \\\n                     --test.coverprofile=coverage.out \\\n                     --cpp_data=${test_dir}/monsterdata_test.mon \\\n                     --out_data=${test_dir}/monsterdata_go_wire.mon \\\n                     --test.bench=. \\\n                     --test.benchtime=3s \\\n                     --fuzz=true \\\n                     --fuzz_fields=4 \\\n                     --fuzz_objects=10000\nGOPATH=${go_path} go tool cover -html=coverage.out\n```\n. @mnmtanish LGTM, thanks! After rebase we can merge this as-is.\n. Excellent!\nOn Tue, Jan 19, 2016 at 3:51 PM, Wouter van Oortmerssen \nnotifications@github.com wrote:\n\nI will close this particular issue, please open new issues for more\nspecific documentation problems as you find them.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/273#issuecomment-173026740.\n\n\nRobert Winslow\n@robert_winslow http://twitter.com/robert_winslow\n. Thanks @brunoqc \n. Looking into this, @brunoqc any guesses?\n. I've reproduced the error on 14.04.3 LTS 32bit. Looking into it.\n. @brunoqc Please confirm it's fixed for you.\n. Great! Thanks for reporting this.\n. @tguo-aa Would you like to contribute a PR that adds the following line to CreateString (make sure to add a test)?\nself.assertNotNested()\nThis is taken from StartVector.\n. @tguo-aa Yes, that sounds good.\n. @tguo-aa merged.\n. Released a new version on PyPI that contains this: https://pypi.python.org/pypi/flatbuffers/2015.12.22.1\n. This is interesting, and I see why you'd want to do it. On the other hand, if we try to add this to Java or Go, will it typecheck without using reflection?\n. Would you like to contribute a PR that implements Reset? The Go version has it already, you could start there (make sure to test it).\n. @Downchuck Any luck here?\n. bump :-)\n. I think this is interesting. It would be much simpler to convert the existing Python version over to Cython, however.\n. I think this is interesting. It would be much simpler to convert the existing Python version over to Cython, however.\n. I'm encouraged by the collaboration here. :-)\n. Thanks!\n. I also wrote about this in an existing Travis issue: https://github.com/travis-ci/travis-ci/issues/4090#issuecomment-152558948\n. Yes, this would be great.\n. Hey @abuchanan, how is this going?\n. Seems like some search-and-replace would take care of the method name changes. Monster.start_inventory_vector looks fine to me.\nCompiling docs happens with doxygen:\n$ cd docs/source\n$ doxygen\n. We can keep additional data in the Builder object to make error messages better. The Builder is a state machine, and treating it like one might make all this stuff more transparent for users.\n. Released a new version on PyPI that contains the fix for this: https://pypi.python.org/pypi/flatbuffers/2015.12.22.1\n. Cool! It doesn't appear to test Cython and Python. How do I do that?\n. @layzerar How can I choose the plain-Python code when I have Cython installed?\n. @layzerar I got it to test locally. This looks interesting. I like seeing such a speedup!\nA few things I'd like to see:\n1) The diff is confusing and seems to have extra changes (like whitespace modifications in py_test.py). Could you work on cleaning up the diff?\n2) We need to be able to run CPython and Cython tests from the same script. Would you look into modifying PythonTest.sh and py_test.py so that it happens?\nKeep up the good work!\n. I'd like to see the generated bytes for the example data, and verify that they are exactly the same as the Java bytes. This is done in the Go and Python versions. Is that too strict, @gwvo?\n. @chobie Thanks for replying to my line comments.\nI think that users should be prevented from writing values that are too big for the schema datatype. For example, I should not be allowed to write 0xFFFF into a FlatBuffers field of type uint8.\nWe have checks for this in the Python port. What do you think?\n. I only made this PR public so I could get Travis to run the tests. Those were all WIP.\n. I like this, let's rebase and get it merged.\n. Can we rebase this? It's wildly out of sync now.. @buchanae could you try rebasing on top of latest master? The diffs may not be as bad as you think.. @danring @mmastrac Sorry just getting to this issue! It's on my radar now. Would either one of you like to contribute a patch for Python and/or Go? Please email me (in my GitHub profile) to discuss specifics.\n. @dictav @aardappel Go's import paths are restrictive. Read here for more: https://golang.org/doc/code.html#ImportPaths\nIn particular, Go expects you to use the domain name in packages that are not in the standard library. For example, this is the Go import path for flatbuffers: github.com/google/flatbuffers/go.\nMaybe the answer is to have a command-line flag for Go that specifies a path prefix. That way, in the schema, you could specify: namespace protocol (java: \"com.company\", go: \"company\");, and for Go you would also have a flag that gives you the domain prefix: github.com.\nDoes that help?\n. @aardappel What I'm saying is that the host of the source code is expected to be present in the package name. In your example, it would look something like:\nnamespace protocol (java: \"com.company\", go: \"github.com/company.com\");. > To allow companies to share schemas without forking?\nYes, that's a good example of why this is needed. In general, if you fork a Go project (so that it lives at a different path) you have to do a big find-and-replace.. @dictav thanks for writing this up! I think a command-line flag is the best option. @aardappel what do you think, and what would be the best set of flag(s) to use if you agree?. @prideout We of course test 2.7, check out https://github.com/google/flatbuffers/blob/master/tests/PythonTest.sh\nThe issue you are having seems to only be for older versions of 2.7. Are you on 2.7.5?\nThe PR https://github.com/google/flatbuffers/pull/2098 is a better fix because it only changes things for older 2.7 versions, so I'm closing this.\n. Thanks for submitting this PR. Please add tests that verify all of these work correctly for the example Monster (and so that we can prevent regressions). Otherwise LGTM.\n. Bump @danring \n. Thank you, merged!\n. @faizanrashid Thanks for noting this issue! I like your suggestion. Please submit a PR (with full test coverage if you can) and I'll be happy to review it!\n. According to the changelog at https://hg.python.org/cpython/raw-file/15c95b7d81dc/Misc/NEWS (found via https://www.python.org/downloads/release/python-2710/) this may be a result of this issue in the release notes for 2.7.4rc1:\n- Issue #10212: cStringIO and struct.unpack support new buffer objects.\nI'm happy to take a further look at it, but @faizanrashid what would you like the behavior to be?\n. Released a new version on PyPI that contains the fix for this: https://pypi.python.org/pypi/flatbuffers/2015.12.22.1\n. Seems fine to me. There is no null in Go, though. What about calling it IsNilField?\n@gwvo does this look close to how the original feature is implemented? I'd like to keep the approach as similar as possible to the C++ version.\n. LGTM, thanks!\n. Hi,\nThis is known and expected. One tradeoff we made with the current\nFlatbuffers implementations is that buffers are written \"back to front\".\nThis means creating a buffer with elements in the order ABCD will result in\nreading them in the order DCBA. The solution to your problem will be to\nwrite the objects with this property in mind.\nDoes that help?\nOn Jan 18, 2016 3:08 PM, \"Philip Rideout\" notifications@github.com wrote:\n\nThis might be due to our ignorance rather than an actual issue, but our\nC++ client (which is a buffer consumer) is seeing arrays reversed from our\nPython server (which is a buffer producer).\nHere's a small example that creates an array-of-struct with two items in\nthe array. The first item is 0x1111 and the second item is 0x2222.\ncd tests\ncat > wombat.fbs < wombat.py <<EOF\nimport flatbuffers, binascii\nfrom Fauna import Wombat, Foo\nbuilder = flatbuffers.Builder(0)\nWombat.WombatStartFoosVector(builder, 2)\nFoo.CreateFoo(builder, 0x1111)\nFoo.CreateFoo(builder, 0x2222)\nfoos = builder.EndVector(2)\nWombat.WombatStart(builder)\nWombat.WombatAddFoos(builder, foos)\nbuilder.Finish(Wombat.WombatEnd(builder))\nprint binascii.hexlify(builder.Bytes)\nEOF\npython wombat.py\nThis produces console output that ends with:\n22221111\nThere's a flatbuffers doc page that says \"The current implementation\nconstructs these buffers backwards\", so maybe this makes sense. However it\nis unexpected that that the C++ reader API is reversed from the Python\nwriter API.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/3711.\n. 1. bool should certainly be made a true Boolean. @jmcguirk I'd welcome a patch to this effect. Start with the Testbool function in the example monster data (at  https://github.com/google/flatbuffers/blob/master/tests/MyGame/Example/Monster.go). The code generator should use GetBool, not GetByte.\n2. A string causes a heap allocation, so we decided to leave this as a []byte slice. The user can cast it to a string. Note that there is also the String function in https://github.com/google/flatbuffers/blob/master/go/table.go, however it is unused by the code generator.\n\n@jmcguirk If you'd like to chat about a patch, my email is in my profile.\n. Putting this on my queue.\n. @dmuir Thanks for the report, here's the PR: https://github.com/google/flatbuffers/pull/3760\n. We definitely have an opportunity to refactor now that we have so many language ports. I like what you're thinking here.\nIt's going to be an effort to implement what you're considering, though, and we may not need to. I think it's possible that each port will become more specialized, not more generic, as new features are stabilized and added.\nJust my 2c; @gwvo makes the call here.\n. Closeable?. @verokarhu can you give a code example of how an interface would save you effort? I agree with @gwvo that this does not seem like it would help.\n. Sounds like a great opportunity to submit a pull request @verokarhu :-)\n. @mmastrac Made a comment on your PR: https://github.com/google/flatbuffers/pull/3791#issuecomment-207549357\n. @mmastrac I agree with the post above, can you make it const? Then I'll merge straightaway. Thanks for this!\n. @mmastrac You're right. This is a bit of a quandary. However, e8ac0f2 is not yet used by the code generator, so we could safely remove it. What would you prefer?\nTBH I'm not sure how people write fast Go code on App Engine, because there are spurious heap allocations than can only be avoided with unsafe.\n. @EricLagergren That's a good idea! Want to take a shot at implementing that?\n. I buy it :-) Thanks!\n. @goagain There is no enum mapping like what you describe. Would you like to add it to the code generator? It would just be generating a map[string]int object, mapping enum names to their int values. The object would live in the toplevel of the type's file. For example, this would be the new version of https://github.com/google/flatbuffers/blob/master/tests/MyGame/Example/Color.go:\n``` go\n// automatically generated, do not modify\npackage Example\nconst (\n    ColorRed   = 1\n    ColorGreen = 2\n    ColorBlue  = 8\n)\nvar (\n    ColorEnumMapping = map[string]int{\n        \"ColorRed\":   1,\n        \"ColorGreen\": 2,\n        \"ColorBlue\":  8,\n    }\n)\n```\n(Note that I strongly disapprove of Go not having enum types--this would be a 'hack'.)\n. @vladfi1 Would you paste (at least part of) your schema? I would expect the root type to have a generated function, for example in https://github.com/google/flatbuffers/blob/master/tests/MyGame/Example/Monster.py we have:\npython\n    @classmethod\n    def GetRootAsMonster(cls, buf, offset):\n        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)\n        x = Monster()\n        x.Init(buf, n + offset)\n        return x\n. @googlebot I signed it!\n. You'll need a \"framing format\" to convey metadata. Sending a length, like you did, is normal and appropriate.\n. Great to see this! Let me know when you are ready for review. \n. @josephDunne My biggest piece of feedback on this version is that I need to see fuzz testing. The C++, Go, and Python versions all have fuzzers, so there are lots of examples. Let me know if that makes sense to you.\nAnother piece of feedback is that instead of using unimplemented! for the conversion from integer to enum, it should return an option type.\nOtherwise, a quick read of the runtime and generated code raises no red flags at this time, nice work!\n. @josephDunne I'd like to see 1-to-1 equivalency, first.\nAfter that, though, QC is great. (My only concern is that QC's default Arbitrary impls probably can't provide the test cases we care about. Generator functions will have to be created.)\n. @josephDunne great progress!\nRegarding fuzzing, it seems it would be most straightforward to skip QuickCheck for now and use the code from the Go port: https://github.com/google/flatbuffers/blob/master/tests/go_test.go#L283-L426 That uses a simple LCG RNG to drive some serialization cases. Do you think it would be straightforward to port to Rust?\nAlso, do you know if any Rust code makes heap allocations besides the Vec that backs the Builder?\n. @josephDunne I'd like to get this through sooner if possible; waiting on the Rust core team to fix a bug sounds like a recipe for waiting a long time.\nDo you see any way around it for now?\n. @josephDunne Any luck with the fuzzing?\nHappy to talk over email about it if you have any questions.\n. @blt Does this still require nightly? Does it have thorough tests now?. @blt FYI: after reading over this PR a few times, I believe that adding comprehensive tests will be tricky and time-consuming. So please be careful of spending too much time on this.. @blt Good luck. I already tried to do that and cut myself off after too much time was used :-). @wesm I've resumed working on the Rust port. I'm planning to have feature parity with C++, including a solid test suite. Please stay tuned.. I'm impressed with the progress that has been made on this PR. Especially the thorough use of macros, which seem like an expressive and idiomatic way to do our FlatBuffers code generation in Rust.\nAs always, my primary concern is with testing:\n\n[ ] unit testing (especially wire format bytes),\n[ ] fuzz testing (like in almost all the ports), and\n[ ] integration testing (interoperating with the payloads generated by existing ports).\n\nFurthermore, I see that an issue has been opened on the repo for this PR's fork that talks about testing: https://github.com/josephDunne/flatbuffers/issues/4. It says that achieving test and feature parity with Go will be \"quite a task\". I hope that implementing those tests is not as hard as it may seem.\nHowever, one of my ongoing concerns with this PR (going back to the original PR in 2016) is that the tests seem to have been a secondary concern. When we authored the original Go and Python ports, we wrote them in a test-driven-development way. This was so we could learn the gritty details of the code generator and wire format. It was also because the FlatBuffers combination of compiler, code generator, and runtime library is powerful... and therefore can have unexpected emergent behavior if we are not very careful in our engineering.\n@josephDunne @hollinwilkins thoughts?. Hi all!\nI'm glad there's still interest in a Rust port. I know it's been a disappointing ride over the last few years for those of you wanting to use FlatBuffers in Rust, and seeing it not come together.\nAbout a month ago, I dug in full-time for a few weeks and made another attempt at writing a solid Rust port. As some of you may recall, I've tried this before. This time around, though, my Rust-fu is much better, and I'm doing this out of personal necessity. I want to use FlatBuffers in Rust to develop other software where speed is critical: storing data in a write-ahead transaction log in a NoSQL datastore. Using Rust with FlatBuffers is the optimal choice for that project.\nHere are some goals of this unpublished WIP port:\n\nExtensive test coverage: this is non-negotiable and includes the types I listed above. Unit, fuzz, integration, etc.\nFeature parity with the C++ code generator: This includes things like the builder convenience functions and the verification API. I might wait to implement the reflection API and flexbuffers support.\nNo user-facing macros: While macros are nice to write as a language designer, I find them to be a problem when asking new users to get familiar with a codebase. (I am assuming that it is good for our users to understand how the generated code really works.) I like how easy it is in Go, for example, to learn how something operates by reading its code. In my experience, macros make this a lot harder.\nTo be as fast as the C++ port: this includes the ability to unsafely interpret FlatBuffers memory as Rust objects.\nIdiomatic Rust.\n\nHere is the progress so far:\n+ Cargo-compatible package format.\n+ Code generation (based on the C++ generator) is ~complete.\n+ Generated code for the test schemas compiles and imports successfully.\n+ Tests are being ported one at a time from the C++ test suite. This is in the early stages.\n+ The runtime library is being built in tandem with the test suite (TDD).\n+ 181 git commits so far.\nI intend to publish the WIP branch of this in the upcoming weeks so that I can start getting feedback on the user experience of the generated code. Thanks for your patience!\n. Below is a link to an experimental, currently-broken WIP branch. I'm putting the code up here only to show progress for those of you who are curious. (Also, to force myself to develop in the open.)\nhttps://github.com/rw/flatbuffers/tree/2018-02--rust\nThe generated code is at https://github.com/rw/flatbuffers/blob/2018-02--rust/tests/rust_usage_test/src/monster_test_generated.rs\nPlease bear with me as I get this into shape.. Generated code typechecks and is importable as a Rust crate; some wire format tests are starting to pass.. @andygrove I use the following command:\nmake all test && ((cd tests && ./RustTest.sh ) ; cd ..)\nBut, I can't support your usage right now since the core code is in flux. I'm just giving status updates here because it seems a lot of people are waiting for this to land.. Got 10 byte layout tests passing, and tomorrow I'll be implementing the start/end functions for creating Tables.\n. Byte layout tests (from Go and Python) all pass.. Fuzz test of scalar serialization inside vtables passes.. Tests pass that verify generated Rust code can correctly interpret the example serialized files created by the C++, Go, Java, and Python test suites. (Except for vector data, since I'm still working on the ergonomics of vectors.). Creation of example serialized data (the gold \"Monster\") in Rust partially passes: library code and generated code can both create correct tables with scalar values, strings, and table unions.. Non-user-visible status update: almost done with a code generator refactor. This is a way to eliminate long-tail bugs, and I can do it now that I'm over most of the conceptual hurdles and can create better abstractions.. I solved a type system problem: we now lift Flatbuffer Offset traversals into the typesystem. This gets the ergonomics closer to the C++ port (which is a good thing).\nProof of concept: https://play.rust-lang.org/?gist=20b4ab8ec5b82c409d431f3658eadd89&version=stable&mode=debug&edition=2015\n\nEdit: improved version: https://play.rust-lang.org/?gist=648af791b28a91cd4c705897c5499eed&version=stable&mode=debug&edition=2015. This code is still WIP (I very much need to clean up a lot of things, and rebase) but this branch has all (70+) tests passing. If you're feeling adventurous, try it out: https://github.com/rw/flatbuffers/tree/2018-08-12--all-tests-passing\nUpdate: As of commit e6e634d, this code is up-to-date with master.. Final checklist before PR:\n- [x] Better union ergonomics in generated code.\n- [x] More code comments.\n- [x] Usage docs.\n- [x] Final generator and runtime code cleanup.\n- [x] VTable deduplication.\n- [x] Basic benchmarks.\nEdit: we aren't going to do this for this version: \"Included/nested generated code imports (the code is generated, but the mod imports don't work yet).\". Pull Request submitted: https://github.com/google/flatbuffers/pull/4898. @josephDunne Thank you for your hard work on this in 2016. You helped us start the long journey to getting a Rust port contributed to FlatBuffers. We really appreciate your efforts, and hope you stay involved in the project! \nClosing in favor of #4898.. @josephDunne Would you please send me an email about us getting control of the Cargo project? I don't see the notification you mentioned from April.. @llchan Interesting idea. So it would be a bunch of constants at the top of each generated Python file? That should be straightforward to implement; the names would be _offset_<function name>.\n. Thanks for writing up this issue. The absolute-path import requirement is a problem.\nProposal 1\nIt may be possible to keep all generated code in a flat namespace inside the internal directory (which is blessed in Go terminology), then re-import them into user-facing imports. That way, each user-facing file does not need to make any external imports, just internal. However, I don't know if you can re-export types in a useful way in this scenario.\nProposal 2\nFlatten the whole namespace into one import.\n@gonzaloserrano would you please comment on either approach?\n. I have no opinion on this :-)\n. The first part of this was done in https://github.com/google/flatbuffers/pull/3859, which returns a string without allocating anything on the heap. However, the code generator and tests have not been updated to use it.\nIt's on my TODO to update the code generator and tests, but... it's a small change and would be a good way for someone (@gonzaloserrano?) to contribute to the project. Please let me know if you'd like to take it on.\n. Keep in mind that strings are normally always owned by the Go GC. Currently, we choose to return a []byte so that the user can decide what to do with the data:\n1) Safely cast to string, causing a heap allocation,\n2) Unsafely cast to string, skipping a heap allocation,\n3) Use directly as a byte-slice.\n. LGTM thanks!\n. Sounds good... is 'Table' a reserved word in the FlatBuffers schema language?\n. 1) I'd like a simple test on the Table accessor for each generated type.\n2) Why is the interface called Message as opposed to anything else? The word \"message\" is RPC-centric, which will be confusing to users who are (say) storing FlatBuffers in files.\n3) Is there a meaningful way to test the Message interface?\n. Sorry about that @gonzaloserrano, I'll try to do better. Thanks for your PR!. Thanks @americanjeff, I agree. Options for solutions are:\n1) Remove the (currently broken) attempt to make a heap allocation when given a nil object, or\n2) Update the function signature to permit a correctly-returned heap-allocated object.\nI suspect most people want to use this function with a single object that's reused, which is why we haven't seen more issues about this.\n@americanjeff If we choose (2) do you have a suggestion for the new function signature? I was thinking func (rcv *Roster) People(obj *Person, j int) (*Person, bool).\n. I'm amenable to just leaving it out. @americanjeff ?\n. Alright, we'll leave it out. Let's just document that you need to pass in an object. @americanjeff If you'd like to make a PR I'd be happy to review it, do you have time for that?\n. Pull request :-) \n. These items are inline. The following change works for me:\nBefore:\ngo\noffsets := []flatbuffers.UOffsetT{\n        CreateStruct(builder, data[0].a, data[0].b),\n        CreateStruct(builder, data[1].a, data[1].b),\n}\nTableStartStructsVector(builder, 2)\nbuilder.PrependUOffsetT(offsets[1])\nbuilder.PrependUOffsetT(offsets[0])\nstructs := builder.EndVector(2)\nAfter:\ngo\nTableStartStructsVector(builder, 2)\nCreateStruct(builder, data[1].a, data[1].b)\nCreateStruct(builder, data[0].a, data[0].b)\nstructs := builder.EndVector(2)\n. @americanjeff I'd like to improve the docs or error handling to make this easier to debug. Do you have any suggestions for how to do that?\n. I don't understand why these files are in the root of the project? . Can we have someone verify this PR on GCE?. @gonzaloserrano Could you describe your use case a little more for me? I'd like to know the underlying problem you're trying to solve.\n. @jchiu0 This now provided by @gonzaloserrano's patch, so you can do:\ngo\nmy_table.Table().Bytes\nTo get the underlying []byte.\nThanks!. 1) I think you'd be better off using numpy.frombuffer.\n2) There's no way to do what you want with the current Python interface. Keep in mind that we are endian-safe, so if flatbuffers gives you access to the underlying memory, then we'll violate that promise. For that reason, we can't always give you 'memcpy'-like performance.\n3) That said, there should be adequate ways to integrate ctypes or numpy types so that we can have accessors that provide direct, correct views into the data. Would you like to try adding that feature? I think what it would take is a) generating numpy-style types for any fixed-length (int, struct) flatbuffers type, and b) adding that to the code generator and adding some tests.. @zplzpl Is this resolved?. Fixed. Merged #4112 which is a dupe of this. Thanks!. Premature PR. Passing this one since the CLA check passed. Thanks!. I added review comments on this a few weeks and it seems either they never showed up (thanks to me misunderstanding the UI) or they weren't addressed.. @wrigby we're looking for a good way to work more tightly with numpy. This is cool.\nShort term, supporting fast array access is a priority and I'd welcome a PR from you.\nLonger term, there won't be endianness problems if we can use ctypes (or equivalent) to specify the byte ordering. What do you think about doing that?. @wrigby Great start. I need you to add tests to this. It would be good to update PythonTest.sh to use --gen-numpy and add checks for every Monster field.. @wrigby Great. In general we're not going to merge a feature unless it's thoroughly tested. So, reach out to me if you want more details on how to do that.. @wrigby I think it's justified to put numpy support into the code generator. . Should this handle collisions? e.g. If my schema has:\nvar\nvar_\nThen this PR will currently turn the keywords into:\nvar_\nvar_\nWhich is a collision.. LGTM!. Yes, this would be nice to have in Python. I know the C++ port has this feature. I do not have the time at the moment to do this, but I encourage others to give it a try.. @BAndiT1983 Thanks for raising this. Your suggested approach of adding type Mode int does the job. I think you will end up 1) adding a line to the generator that emits \"type \" + enumname + \" int\\n\", and 2) change the type of the enum values. You will have to update the types in the test suite to match.. One option is to use an encoder like zstd, which supports custom model \"dictionaries\", on individual values before writing them to a FlatBuffer. That way you minimize overhead.. @jlafon Thanks for pointing this out. The fix is straightforward: update the code generator to emit bytes() instead of \"\". Would you like to make a tiny PR with this change?. @jlafon Great, put a note here when that's ready.. This is promising!\nWould you be able to add more tests? In particular I want to make sure that multi-byte scalar types (like uint16) are read back correctly.\n(I see you're pre-specifying the byte order, which is good! I just want test coverage.). @kbrose Great. Please do that for the other supported numeric types, especially the float ones. This will help us catch issues down the road if e.g. numpy changes its behavior. Bonus points if you write a small fuzzer for testing this feature.\nWe should be aggressive when writing tests, since we want to be paranoid about correctness.\n. @kbrose I wasn't specific enough I see, my apologies. I didn't mean for you to create those new schema fields, I was thinking of something simpler: constructing the buffer manually then testing that the correct value gets read back. This is similar to the 'wire format' tests we have in the test suite.\nHow about you undo those big schema changes, and we can add more regression tests in another followup PR?. I just read the diff again and it's comprehensible as-is.\n1) Is there a good way to add Python2.7 support to the appveyor runner, perhaps by using conda again? I'll follow up and handle PythonTest.sh.\n2) I'm fine with this PR. Since other languages are involved I'd like to hear from @gwvo also.. @mikeholler We need an automated way to create a PyPI. I haven't figured out how to set that up yet. If you or someone you know would like to help us with that, we'd appreciate it!. @kbrose Thanks for your offer, we'll take you up on it. Our pain paint is that shipping a new pypi package is a 100% manual process right now. Do you know of any good ways to partially or totally automate that?. Ideally we would not put setup.py into the root, because the semantics don't make sense: we only want the contents of the python directory to be part of the python package.\nWe could have a separate tracking repo for this purpose.. LGTM. We have functions for reading byte arrays (from https://github.com/google/flatbuffers/pull/4390) but not yet for writing them. \nI've created a PR that implements your request: https://github.com/google/flatbuffers/pull/4453\nPlease try it out and provide feedback.\n(Also, you'll want to default to installing the python package from git (using pip3 install -e git+ssh://...) to get more up-to-date code.). It's not tested as much as CreateString but that's okay--the functionality is almost identical.. @jfinken Does this meet your needs?. @jfinken Glad to hear it!. So do we just want a panic with a good error message?. Hi @mikeholler, I'm the Python maintainer. We've let releases lag, since 1) it seems users typically end up pulling from master,  and 2) some changes should bump the Python package version even if they aren't a change to the FlatBuffers core code, and we have not explicitly decided how to handle that case. Ideally, we'd ship a new Python package at least as often as core FlatBuffers does.\nTo answer your questions:\n+ I have access to the PyPI project for FlatBuffers.\n+ Publications are totally manual: just uploading an sdist.\n+ Complications are that it's time-consuming.\n+ Semvar would be great.\n+ Do you know of a good way to automate this, even partially? This ties into a bigger project we've been working on to get complete continuous integration for all ports.. Thank you @mikeholler, very cool! Let me try to plug this in this week.. @mikeholler Do you know how we would combine this with the Python test suite, so that we only publish a new package if tests pass?. @mikeholler Thanks! We'll create a separate account for this.. Assigned to me: the fix is to make the code generator explicitly annotate the type of the const value with the enum's underlying type.. As I said above, when the example monster has more tested fields, this LGTM.. Resolved by PR #4772 . @binary132 We can definitely add it, but as @aardappel said above, the benefits might not be that numerous. Could you give me a sense of how urgent this is for you?. I think what we can do is pull in the namespace traversal from another port to use to create these import statements using fully-qualified paths: https://github.com/google/flatbuffers/blob/master/src/idl_gen_python.cpp#L299. @GStones did you get your issue resolved? If not, please post your schema file so we can take a look.. Closeable?. @nairb774 's code change brings the library code more in line with what is idiomatic Go code (for better or worse). Here's how it's done in the standard library (note the bounds-check trick is there, too):\nhttps://golang.org/src/encoding/binary/binary.go\nAnd here's an old Rob Pike blog post about it:\nhttps://commandcenter.blogspot.com/2012/04/byte-order-fallacy.html\n(discussion: https://news.ycombinator.com/item?id=3796378)\nI'm in favor of this PR because it's more idiomatic.\n@binary132 if you'd like to run some benchmarks with your method, I'm happy to accept a PR that changes the order of writes.\n@aardappel If we use the unsafe functionality we might be able to speed this up on little-endian machines by using unsafe pointers. However,\n1) there's no canonical way to detect the endianness of the host machine (https://www.reddit.com/r/golang/comments/3a88ay/does_go_need_native_endianess/), and\n2) we've had users have trouble with using flatbuffers on Google App Engine when we use unsafe, so we'd have to maintain two code-paths: a) safe little-endian/safe big-endian, b) unsafe little-endian/safe big-endian. Going down that road would be fighting the language too much to make it worth it right now, IMHO.. @senior7515 thanks for writing that article!. @felixfrank This looks really useful! Thanks for making this PR. I left some comments for you to reply to.. @felixfrank Our CI doesn't test numpy, so I need to do this manually. If that goes well (I expect it will) then we can merge!. @aardappel @guoqibi Closeable?. @victorstewart Interesting! It sounds like a way to try this out would be to use FlatBuffers inside Redis via Lua. We don't yet have an official Lua port, but there seem to be other ones on Github you could try out.. @victorstewart Redis is single-threaded, so how about this: keep a FlatBufferBuilder around, and reset its state after every time you build a new flatbuffer. That way, you are not paying any allocation costs (as the builder capacity grows). Then, updating a key is just a memcpy.. Hi @ElliotHYLee, not all flags to flatc are supported in all of the code generators. Is this an important issue for you? I can move it up in our queue of features to work on. (Or, if you'd like to try contributing, I can guide you in making a PR.). @kostya-sh Thanks--we know about this and I'll fix it :-). We should synthesize with #4853 #4891. @phayes I included it in the PR for people who don't want to run the code but still want to see the output of tests passing.. @aardappel Thanks for the feedback, I incorporated it all. Also, I resolved an alignment issue that I didn't catch before, and made the Push trait more useful.. @barweiss We didn't get around to it. Feel free to go for that, and take a look at these older issues: https://github.com/google/flatbuffers/search?q=pep8&type=Issues. @cholcombe973 Could you provide more detail on the errors you see? I import flatbuffers in every mod, like this:\nhttps://github.com/google/flatbuffers/blob/master/tests/monster_test_generated.rs#L11. @cholcombe973 Looks like I didn't cover the case where there is no namespace! Thanks. Do you want to polish the PR (by regenerating the code, allowing unused imports, doing indentation in C++) or do you want me to take it over?. @cholcombe973 Those GRPC warnings are okay. Please follow the directions in the PR template text (below your PR text) and we can go from there.. @cholcombe973 I think you'll need an attribute in Rust to allow unused imports on that newly-generated code. Would you add that please?. @cholcombe973 great! If you could add a line of whitespace after that import block, we should be good to go.. @cholcombe973 Could you rebase and make this into one commit with one commit message? GitHub doesn't offer much flexibility with how to squash PRs, unfortunately. Thanks for helping keep our commit history clean :-). @cholcombe973 Thanks!. @gwvo  is this supported in C++?\n@ehsanul In the meantime, you can use the get_root function like this:\n```rust\n[inline]\npub fn get_root_as_baz<'a>(buf: &'a [u8]) -> Baz<'a> {\n  flatbuffers::get_root::>(buf)\n}\n``. I have a weakly-held opinion: only generate accessors for the declaredroot_type, and let users who know what they're doing useflatbuffers::get_root` if they want to.\nI'll let @ehsanul chime in.. Closing in favor of this tracking issue so that we can have more discussion about whether we want to support this or not: https://github.com/google/flatbuffers/issues/4918. @ehsanul please confirm latest master fixes this.. @mmastrac @ehsanul does https://github.com/google/flatbuffers/pull/4912 fix this?. @ehsanul Does this fix it? :-) https://github.com/google/flatbuffers/pull/4917. @mmastrac  How about this? https://github.com/google/flatbuffers/pull/4923\nThis lifetimes change to builders was fairly recent, and was an attempt to improve the ergonomics around builders so that users don't always have to specify ..Default::default() because of a PhantomData. Thanks for helping me find the corner cases!. @mmastrac @ehsanul please reopen this if you find another issue!. CLA can't be signed for edits using the Github UI (because it clobbers the email address). Closing.. cc @ry. @pwrdwnsys Not that I know of, but @aardappel would be able to chime in. I suspect it would be straightforward to do that.. @jean-airoldie and @tymcauley: You both have been making excellent contributions to the Rust port. Would either or both of you be interested in working on the Rust Verifier functionality? I would've emailed you personally, but I don't know how to get in touch with either of you :-) If you want to email me, link is in my profile.. What is involved in implementing this?. @kzvi If I understand correctly, wouldn't it be most appropriate to have a child lifetime bound?. @kzvi I mean 'a: 'b, 'b. @kzvi @mmastrac please confirm this is fixed by @kzvi's PR.. @kzvi @mmastrac alright I'm open to another PR for that! Same thing: change the code in the smallest way possible, and write tests for it.. @kzvi It took some thought... I've convinced myself that this change is okay. :-) The reason it was not obviously correct to me is because we use unsafe in some places to read data. Would you add a test case (in a new mod) for this? It can go right after the generated_constants module here: https://github.com/google/flatbuffers/blob/master/tests/rust_usage_test/tests/integration_test.rs#L237\nThe test case can be the 'static lifetime example you gave in the original issue thread.\nThanks!. @kzvi I see you went ahead with using the full example data. Would now make it two tests? 1) The one you created that uses generated code. 2) Using just table::get as you used in the issue you opened.\nNote that a valid string can be represented by this byte data: [0, 0, 0, 0].. @kzvi OK, LGTM! Please squash your commits into one easy-to-read commit, and make the CLA bot happy if you can figure out what it's asking for.. Hi @kzvi, I'd be surprised if this PR passes tests, because the return type for required fields has changed. Would you please reduce the code size of your patch, and run tests?. @kzvi I added a comment with an idea for how to simplify the code and abstract out the functionality.. @kzvi bump! :-). I'm not an expert on the C++ port but are you saying you want the FBB to not write the image data into its owned buffer?. @cholcombe973 AFAICT the output indicates the operation took 5322 nanoseconds (5.322 microseconds). I determined that by looking up the definition of subsec_nanos. Is that correct?. @cholcombe973 :-). @jean-airoldie Good find! The function here is the culprit, would you want to author a PR? The change involves modifying the predicate when deciding to add an underscore. \nhttps://github.com/google/flatbuffers/blob/master/src/idl_gen_rust.cpp#L33. @jean-airoldie I updated CI to include Rust tests. Please fix the failures in the test suite (they have to do with underscores in the generated code).. @jean-airoldie It's a matter of taste, but using two underscores to indicate namespaces seems okay for now. Please link to the generated C++ code you're referencing.. @jean-airoldie \n1) You mentioned test_as_my_game___example_2___monster which I think is unavoidable since we need to show the user what namespace the casted object is in. Here is how the C++ code does it: test_as_MyGame_Example2_Monster.\n2) You can link to line numbers in Github diffs btw. :-)\n3) The snake case function gets used all over the place, so maybe we should parameterize it according to whether or not the original underscores need to be preserved?. @jean-airoldie Does this fix your original issue of namespaces declared as \"foo_bar\" in the schema?. Yes, thanks @tymcauley for getting this to work!. @barweiss Thanks for submitting this PR. I like some of the ideas; it's going to be hard for us to accept a PR that makes this many changes, though. Would you be able to focus on the PEP8-compliance in the code generator, then we can potentially make the other changes you are suggesting at a later date?. @barweiss Okay, I've got an open mind and am happy to see how this goes. :-)\nWould you please commit the updated generated code? (Or confirm that the generated code from your new generator is identical to the code generated by the current Python generator.). @barweiss I'm looking to see the test suite pass with your new code, so that would require running ./generate_code.sh to make your new-style python files.. @barweiss bump! Want to run that code generator like I mentioned above, and see if tests pass using your new code? :-]. @mmastrac Go ahead and rebase!. @mmastrac Is there consensus that rustfmt is really the best way to format all code? For example, I dislike how it handles long function signatures by not aligning to the name of the function.. @zhangsoledad @mmastrac do we have consensus? turn off clippy in generated code for now?. @zhangsoledad @mattfs Want to make a PR to turn off clippy in generated code?. @laur89 Vectors are not 'smart' in the same way tables are. Vectors just contain fixed-width elements. In the case of adding a field to structs that you are storing inside a vector, you would need to rewrite the data. This is because structs explicitly do not support backwards compatibility (for efficiency). Instead, if you want vector elements to be backwards-compatible, you should use a vector that stores table offsets. In this way, you can create a vector of tables, and get the advantages of default fields, backwards compatibility, etc in your elements.\nDoes that make sense? :-]. @kostya-sh thanks! would you add some test cases for these new bool fields? perhaps check the value of Testbool and MutateTestBool?. @kostya-sh could you change it multiple times? IIUC that would handle the cases we are interested in.. LGTM. @aardappel is it okay that @kostya-sh changed the test data?. @mariocao is that a nested table? what is your schema?. @mariocao Want to try making a PR for some of these?. @rumatoest If you use the latest master version of flatbuffers, does this problem still happen?. @whatisaphone @rumatoest would you please post the code you have written that causes these errors? I ask because debugging lifetimes can be tricky without all the context (because of the constraint-satisfaction methods Rust seems to use to resolve lifetimes).\nMost importantly, code samples can help us write regression tests so that this doesn't happen in the future.. @Tommytrg Does this happen on Rust stable? The .map is on an Option -- there is no iterator there -- not sure how that could possibly happen.. @whatisaphone bump. 1) @Tommytrg @whatisaphone Could you please post all the code that causes E0495? I need this to include your schema, the generated flatbuffers code, and the code that calls the generated code.\n2) @Tommytrg Thanks for digging into the required issue about unions! I think I understand the problem: the typed union accessors assume that the underlying data is wrapped in an Option. I can open a PR to fix it--but would you like to try? I'd appreciate the help! The change would be to skip the .map if the field is required: you would make the change in the Rust code generator.. @Tommytrg bump!. @csmoe Want to try to make a PR to update this? AFAIK this doesn't cause anyone problems, because of the allow directives.. LGTM, @gwvo ?. @itsMeBrice did you figure this out? :-) I admit the Python may be a bit confusing regarding strings/bytes because that is the major difference between Python 2 and 3.. @rumatoest The *Args structs currently still take Option for that kind of field:\nhttps://github.com/google/flatbuffers/blob/master/tests/monster_test_generated.rs#L1127\nBut, you can see that the required property is used at both write and read time:\nhttps://github.com/google/flatbuffers/blob/master/tests/monster_test_generated.rs#L1400\nhttps://github.com/google/flatbuffers/blob/master/tests/monster_test_generated.rs#L915\nWould you like to take a shot at making a small PR that removed Option from required fields in the *Args structs?. @rumatoest @fbenkstein The current semantics of the language ports, Rust included, is that the code is allowed to assume that the underlying flatbuffer is valid. If the data is not valid, then the different ports have different ways of dealing with this (generally they panic on out-of-bounds access, except in Rust/C/C++ where they may perform undefined behavior during a reinterpret_cast-type of call).\nThe most robust solution to handle untrusted data is to run the Verifier (currently only in C++) to validate the correctness of a flatbuffer. We're working on a Verifier for Rust so that we can have the same developer experience in both of these languages.. @aardappel @rumatoest @fbenkstein I'd like to close this as non-constructive. Maybe we need better docs on these design decisions?. @rumatoest \n1) Is the 190ms here referring to the line below or above?\n2) I'd like to see the definition of finish_service_response_buffer.\n3) Are you resetting the same builder between calls, or are you initializing a new one each time?\n4) Can you create a self-contained program (main.rs) demonstrating the slowness?. @rumatoest I see you are creating a new builder each time:\nrust\n    let mut fb = flatbuffers::FlatBufferBuilder::new();\nTry re-using it by using the reset method. There is documentation here: https://docs.rs/flatbuffers/0.5.0/flatbuffers/struct.FlatBufferBuilder.html#method.reset. @rumatoest is this resolved?. @rumatoest bump. @rumatoest  Try using create_vector_direct. From the docs:\n\nCreate a vector by memcpy'ing. This is much faster than calling create_vector, but the underlying type must be represented as little-endian on the host machine. This property is encoded in the type system through the SafeSliceAccess trait. The following types are always safe, on any platform: bool, u8, i8, and any FlatBuffers-generated struct.\n\nhttps://docs.rs/flatbuffers/0.5.0/flatbuffers/struct.FlatBufferBuilder.html#method.create_vector_direct. @rumatoest If you want further help, please post your project in a self-contained git repo so that I can look at it further. This should include the steps to replicate, including the cargo commands you are using. Otherwise, I'll just continue making suggestions that might not be a fix for your problem. :-). @rumatoest Bump? Are you using reset and create_vector_direct?. It seems to me that using a combination of reset, create_vector_direct, and compiling in release mode should be enough to see speedups. Please re-open this issue if needed.. @eguendelman That's annoying! Would you be able to make a self-contained program I could use to debug this?. @aardappel Typo, fixed :-). Turns out, for schemas with namespaced objects with conflicting names, that --gen-onefile creates invalid Go code. This is because Go does not permit multiple namespaces in a single file. The journey continues.... @csmoe please sign the CLA. @jean-airoldie \n1) If I understand you correctly, you are trying to tell the Builder that some &[u8]s you have are FlatBuffer objects, is this correct?\n2) For maximum clarity, would you please post code showing how you are doing it now, and how you would like to do it?. @victorstewart\n1) Maybe try zstandard?\n2) FlatBuffers uses a form of compression internally: it doesn't write fields that are the default value, and it deduplicates vtables (a type of metadata) if you write multiple objects in the same FlatBuffer.\n3) When compressing FlatBuffers, you lose the 'mmap-ability' that the format provides. Perhaps you should try compressing the data inside the [ubyte] fields instead.. @victorstewart \n1) Yep, the zstd dictionary training is convenient!\n2) Maybe you can train new zstd dictionaries periodically, and use FlatBuffers to send the dictionaries to clients? Then, you can reference them by a \"dictionary ID\" to instruct the client on which dictionary to use to decompress them.\n3) For another approach, see this heuristic compressor by the Redis author: https://news.ycombinator.com/item?id=540048. @victorstewart I hope you found a good compression solution. Closing the issue since this question is (technically) about a framing format, instead of FlatBuffers itself.. @vglavnyy I recommend including recent changes from master to help your CI runs pass!. @andygrove You can see here that we already emit these directives in some cases:\nhttps://github.com/google/flatbuffers/blob/980a6d66d38e3267c601dfa2d33901dfcca73feb/tests/monster_test_generated.rs#L1252\nWould you be able to point out where else the directives should be emitted? Thanks!. @aardappel Do you have any thoughts on how to re-use functionality for this from the other language ports?. @Tommytrg also please regenerate the code, if applicable, using ./generate_code.sh. @Tommytrg I discussed this with @aardappel and we've decided that this would benefit from updating the main test schema with a required union field so that we can test this situation in all language ports. I can guide you in updating that schema and adding tests, or I can do that myself. Do you have a preference? It would make this PR more robust!\nThe line in the monster_test.fbs file I want to add is:\nany_required:Any (id:48, required);. @Tommytrg yes! You can create a new test mod for this, maybe call it: required_fields.. @Tommytrg Sorry I took a long time to get back to you -- this PR looks like it's on track! One change that would make merging this a lot easier would be to put the new schema field into a separate flatbuffers testing schema, not the main monster.fbs. I spoke with @aardappel and he pointed out that putting a new required field in the main testing schema will cause all the other language port tests to break.\nWe do this already with other language features, such as union vectors. The steps will be:\n1) Create a new test schema in the tests/ directory, perhaps called \"required_union.fbs\". Here's an example: https://github.com/google/flatbuffers/blob/master/tests/monster_extra.fbs\n2) Add a line to generate_code.sh to generate only Rust code with this new schema. Here's an example: https://github.com/google/flatbuffers/blob/master/tests/generate_code.sh#L20\n3) Update the test suite to import these new generated files.\nThanks again for your help and your patience, we appreciate your work.\n. @aardappel Your review would be appreciated!. @rockmenjack are you able to achieve what you want without using --gen-onefile? We are considering deprecating the onefile feature because Go is restrictive with what you can have in one file.. @oakad I apologize for the delay. I'll put these Go issues higher up in my queue. Thanks for understanding. Also, if you're looking to get involved in the project, we are happy to mentor potential contributors, and guide them through the PR process. Please let us know!. @oakad Wow, this is a lot of effort! Thanks for thinking through this and getting into the code.\nSo that we can discuss this with more feedback, would you be able to come up with a design document for the changes you want to make? Ideally, you'd have sections that state before and after for the changes you are proposing. As-is, it's not clear how to evaluate the design decisions apart from the code changes.. 1) We merged #5097 ([Go] Namespaced imports fix) which is relevant here.\n2) I agree with @aardappel that allocations are to be avoided. In particular, even if the Go spec doesn't provide strict rules, we know how the most popular toolchain behaves w.r.t. escape analysis. There's no reason to ignore that information.\n3) It is still true that safely reading a string from a FlatBuffer datum requires an allocation in Go, and it is still true that safely reading a []byte does not. (Note that we have had code that uses unsafe to read string data with no allocation.). @oakad Could we interest you in working on some important, specific contributions? Go still needs API improvements, we just want to write them in a more piecemeal (and consensus-driven) way.\n1) Maybe a \"Builder API\" like what is in Rust:\nhttps://github.com/google/flatbuffers/blob/master/tests/monster_test_generated.rs#L940\nhttps://github.com/google/flatbuffers/blob/master/tests/monster_test_generated.rs#L1343\n2) Definitely an \"Object API\" like what is in C++:\nhttps://github.com/google/flatbuffers/blob/master/tests/monster_test_generated.h#L1015\nI'm happy to guide / review anything you want related to those.. This is cool stuff! Thanks for taking the time to write this PR.\n@rjkat We recently added a way to run language test suites in CI using Docker. Would you add a Dockerfile to to test Julia? It will go here: https://github.com/google/flatbuffers/tree/master/tests/docker/languages\nYou can use the official images at https://hub.docker.com/_/julia/.. Is there benefit in trying to get the PR into a merge-able state, even if it isn't completely feature-complete? That way others can work on improving it?. @tymcauley Thanks for being pro-active and trying to help me, I appreciate it!\nI think I implemented your suggestion, but I'm still getting build errors.\nLog: https://travis-ci.org/google/flatbuffers/jobs/477008045\nDockerfile for testing: https://github.com/google/flatbuffers/blob/412cd098410270af5c075fdb9d38ad9b55abe509/tests/docker/languages/Dockerfile.testing.rust.big_endian.1_30_1\nDo you have any other tips on getting this to work?. @tymcauley Woohoo, thanks! Want to sign the CLA so we can get your name on this PR along with mine?. CLA bump. @tymcauley Would that run inside Docker? Could you create a review suggestion with it, if so?. @tymcauley Do you have any suggestions on how to improve this situation, ideally by keeping the ergonomics simple?\n1) We could make it so all users need to access vector data using Vector. Then, if they are on little-endian, we can allow them to convert the Vector to a slice.\n2) Or, if we can override the the get method for our special instances of &[u16], we could do the endian conversion in the get calls.\n3) Or, if we change all serialized scalar types to be wrapped in something named like le_u16, and have a DeRef function on big-endian systems that does the conversion, that might let us keep the slice syntax. (But it would look like &[le_u16].). @tymcauley Thanks for posting your thoughts. I did a little more digging...\n1) There's a core tension with this, because the best ergonomics are \"slice-like\" ergonomics; but, slices are supposed to be memcpy-able. We can't really do this even if we wanted to, because we can't change the implementation of Copy.\n2) The C++ port of FlatBuffers wraps every vector in a Vector type that just provides iteration and indexing: https://github.com/google/flatbuffers/blob/master/include/flatbuffers/flatbuffers.h#L204\n3) We already have a Vector type in Rust FlatBuffers, like what is in C++, but there is some lack of clarity around when to use slices and when to use Vector.\n4) I think we should:\na) Merge this PR with one change: don't run the Rust test suite on big-endian, just check that it compiles.\nb) Convert all slices in the generated code and in the test suite to use flatbuffers::Vector<T> instead of &[T].\nc) Change Vector::safe_slice to be called Vector::le_slice, and that function will only be available in little-endian code.\nd) Maybe simplify the trait magic that's going on with the SafeSliceAccess trait.\ne) Turn on Rust big-endian testing.\nWDYT? cc @aardappel . @tymcauley That's some good sleuthing! :-)\nHowever, I think that commenting out the tests based on endian-ness is only half (less than half?) of the battle. This is because we want our users to run into (at worst) compilation errors, not runtime incorrectness, when using the library on big-endian machines. Does that clarify what we're trying to do with this fix?. @tymcauley You're pushing this boulder up the mountain, thank you! Would you want to open a PR (potentially with \"WIP\" in the title) so we can discuss your changes further?. @tymcauley missed this! No need to make a PR to my personal fork. I mean: please open a PR to the official flatbuffers repo so that we can go from there.. Solved by #5229 . @vglavnyy Would you please add Python tests for this?. @vglavnyy Python tests LGTM.. @vglavnyy We probably need Java and C# tests too, although I don't know those codebases as well, so @aardappel would know better.. @jean-airoldie Good catch! Would you like to try making a PR to fix this? I believe the update will be to edit the following conditional in C++ so that the import statements are written even if there is no namespace at all: https://github.com/google/flatbuffers/blob/master/src/idl_gen_rust.cpp#L1775. @jean-airoldie I'm fine with that -- we already silence warnings about unused imports.. @jean-airoldie What is the compiler error for your delegated function?. @jean-airoldie Great!. @jean-airoldie Thanks for digging into this! I admit I'm not entirely sure I understand.\n\nLifetimes and their bounds are pretty complicated in Rust FlatBuffers. For example, here is generated code that takes a moment to grok:\nhttps://github.com/jean-airoldie/fbs-payload-example/blob/master/src/payload_generated.rs#L119\n\nrust\n    pub fn create<'bldr: 'args, 'args: 'mut_bldr, 'mut_bldr>(\n        _fbb: &'mut_bldr mut flatbuffers::FlatBufferBuilder<'bldr>,\n        args: &'args MessageArgs) -> flatbuffers::WIPOffset<Message<'bldr>> {\nFWIW, understanding that function signature might be useful in solving this lifetime problem.\n\nYou said\n\n\nThe current problem is that the lifetime of WIPOffset<_> returned from FlatBufferBuilder::create_vector() must be bound to the lifetime of the builder. This is a problem when your application reuses the same builder by periodically calling reset on it because its lifetime will necessarily outlast those of objects it serializes.\n\nFrom my perspective (again, maybe I don't fully understand what you're trying to do), reusing that WIPOffset<_> would be a logic error if the builder gets reset. Instead, you would want to use the generated accessor functions to read the data.\nWould you please restate for me, in a sentence, how you want to use the WIPOffsets between calls to builder.reset()?\n\nAbout WIPOffsets...\n\n\nConceptually I think the WIPOffset / slice to bytes within the builder should be bound to something other than the builder and that calling FlatBufferBuilder::reset() (or the builder going out of scope) invalidates the references. This way we could bind the lifetime of a function that does the serialization to the lifetime of the bytes in the builder.\n\nThat might be nice, but I'm not sure that would be possible with compile-time checking.\n\nOverall, I'm amenable to improving this API. I just want to make sure we fully grok the implications of what any lifetime changes would mean for safety and for ergonomics.. @jean-airoldie \n\n1) What can you do with WIPOffset that you can't do with get_root_as_* functions?\nhttps://github.com/google/flatbuffers/blob/master/tests/monster_test_generated.rs#L1864\n2) I notice you aren't yet using the root_type directive in your flatbuffers schema. You may want to add it. Example: https://github.com/google/flatbuffers/blob/master/tests/monster_test.fbs#L127\n3) I think the mismatch that is confusing to me is this: how could you use a WIPOffset in a valid way, if the builder it is bound to has been reset?\n:-). @jean-airoldie I have a guess that reborrowing from structs is what is causing the issue here. Do you think it would work (as an example for us to learn more) if you used plain functions?. @jean-airoldie I'm asking for a \"struct-less\" version that just uses function calls. So it would be\nrust\nsend_things(&mut builder, things);\nnot\nrust\nsend_things(&mut server.builder, things);. @jean-airoldie I made a minimal PR to your repo that may solve your problem. My changes have the following reasoning behind them:\n1) In the \"delegated\" builder function signatures, the WIPOffset lifetimes need to be tied to the builder. The previous function signature you used did not constrain it in that way, because the lifetime was elided from the function definition.\n2) The functions (probably) should not return byte slices from builder.finished_data(), because then the builder can panic if you forget to reset it. Plus, lifetimes might continue to be problematic.\n3) I added a sink parameter to show how I would write the builder.finished_data() into an object that implemented io::Write.\n$ cargo run --quiet\n[12, 0, 0, 0, 0, 0, 6, 0, 8, 0, 4, 0, 6, 0, 0, 0, 4, 0, 0, 0, 1, 0, 0, 0, 8, 0, 0, 0, 4, 0, 4, 0, 4, 0, 0, 0]\nPlease let me know if that helps you do what you are trying to do :-). @jean-airoldie Would you be interested in writing a FAQ/troubleshooting section for the Rust FlatBuffers documentation, to share what you learned?. @neomantra What do you think about making a PR to add a clang build to our suite of Docker tests? You can see examples here: https://github.com/google/flatbuffers/tree/master/tests/docker/languages. @pyottamus We've needed this feature for a while, thanks! If you can add tests then we can merge this.. @aardappel I mentioned that in the PR review :-]. @aardappel Can you confirm that the file identifier can be arbitrary bytes, not just ASCII or UTF-8? The regexp on this page makes it seem like that is the case: https://google.github.io/flatbuffers/flatbuffers_grammar.html. @pyottamus Looks like you'll want to fix the conflict with the tests/monsterdata_python_wire.mon file.. @neomantra This Dockerfile needs to be in the subdir -- you can see it wasn't run in CI: https://travis-ci.org/google/flatbuffers/jobs/481548580. @neomantra I think you'll want to put the Docker files in the languages subdir so that the CI script picks them up: https://github.com/google/flatbuffers/tree/master/tests/docker/languages. Can we get CI to pass with a bump? :-]. @aardappel I don't know yet.. Probably a good fit for the object API, if/when it gets developed.. @naure Thanks for looking into this and filing a PR! Would you be able to fix the CI issues that were flagged?. @naure The CI message in Travis is:\nERROR: ********************************************************\nERROR: The following differences were found after running the\nERROR: tests/generate_code.sh script.  Maybe you forgot to run\nERROR: it after making changes in a generator or schema?\nERROR: ********************************************************\nSo, if that's correct, maybe simply generating all the code again will fix the CI alert.. @naure Looks like an isolated AppVeyor networking issue -- could you push a no-op commit to force CI to run again?. Thanks @naure! \ud83c\udf89 . @jaynus LGTM! Would you add a no-op commit to get CI to run again? Looks like Appveyor had an error.. @jaynus Would you please add a test for this? Perhaps as a sub-mod under the roundtrip_vectors module in the test suite? Link: https://github.com/google/flatbuffers/blob/master/tests/rust_usage_test/tests/integration_test.rs#L774. @jaynus Or, minimally, by adding a vector iterator assert to the serialized_example_is_accessible_and_correct function. Link: https://github.com/google/flatbuffers/blob/master/tests/rust_usage_test/tests/integration_test.rs#L185. @jaynus Cool! Let's add your vector collect check to some more tests, so we can test more situations:\nvector_of_string_store_helper_build\nvector_of_string_store_manual_build\nvector_of_f64_store\nvector_of_table_store\nvector_of_bool_store\nvector_of_struct_store. @jaynus Thanks for opening this PR! I'd like to see TravisCI passing. Here is the error the occurred:\nERROR: ********************************************************\nERROR: The following differences were found after running the\nERROR: tests/generate_code.sh script.  Maybe you forgot to run\nERROR: it after making changes in a generator or schema?\nERROR: ********************************************************\nI also noticed that flatbuffers was spelled as fflatbuffers in some code.. @rida914-4 Do you think this is an issue in the Rust library or the Rust generated code? If you do, please upload your code to a repository so that I can take a look!. @dnr Works for me! Want to make a PR? . @bspeice Please push a no-op commit to trigger CI again.. @bspeice Is there a way to keep silencing the original dead code warnings? e.g. https://travis-ci.org/google/flatbuffers/jobs/500122232#L1248. @bspeice Thanks for your PR! Hope it helps other Bazel users :-]. LGTM, @aardappel is there anything tricky about letting enums be hashed or ordered?. @TethysSvensson thank you! \ud83c\udf89 \ud83c\udf89 . @rjsberry We don't have any strong opinions on how to do it yet! Do you think there is a clear best way to do the codegen build.rs?. @aardappel Any thoughts?. @rjsberry No additional comments, I appreciate the thought you're putting into this. It sounds like a good idea!. See also https://github.com/google/flatbuffers/pull/5166. @Sebdevar Thanks for the thorough issue description. Would you like to try making a PR for this?. Thank you @dnr !. Looks like TravisCI is failing for some networking-related reason (https://travis-ci.org/google/flatbuffers/jobs/502253317). Would you try pushing a no-op commit to rebuild? @jean-airoldie . @jean-airoldie and now appveyor timed out -- ! Bump again? Thanks for your patience with our CI.. @jean-airoldie thank you!. @tymcauley This is looking good! Given the complexity of the traits involved, do you see any clear opportunities to simplify the way the code is organized? Especially with how the extra impls are created when the target is little-endian?. Okay I'm happy. @aardappel WDYT?. CLA note: rw and tymcauley are the only ones who worked on this, and the bot says we both signed CLAs. I think the CLA bot is unhappy because we used two different forks/branches to get this done.. @dnr This looks great! Thank you for working within our existing structures so well, like in the C++ generation code and in the Go test suite. @aardappel any thoughts?. @dnr Responding to your PR questions:\n1) I'm fine with global maps, but if a user has a very large enum, it might be a lot of overhead. See also point (5) below. @aardappel any thoughts?\n2) Using sync.Once could move the latency from program start, to a hot path, which would be surprising to our users. \n3) Wrapping this all in a function might be useful, but we can delay that, perhaps to another PR.\n4) Stringer-style heuristics would be cool, but I think we can wait and see if we need them.\nNew idea:\n5) Instead of a map, we could use a sorted slice of (name, value). That way, we can use binary search on it, which would be fast, and without the initialization overhead of a map. We could even store that all in one contiguous byte slice (using unsafe to turn []byte into string), and thereby skip a lot of allocations.\nOverall, I'm inclined to accept this PR as-is, and make the above optimizations in a followup PR.. @dnr\n2) The latency of the first sync.Once call is variable, IIUC, because it needs to allocate the map and all its data. The following sync.Once calls are basically a no-op though.\n3) If we have a way to make the UX better then it's okay to change the APIs, IMHO. The main commitment the project has is to the file format itself, which must be -- and is -- extraordinarily stable.. is the idea that this helps with defensive programming? Because IIUC we would not expect to see negative values in the wild.. Shouldn't this test still use the old way of making the object? In effect, it seems you want to add a second, property-based API. It will need to be tested along with the existing API. Do I understand correctly?\n. Please make this a constant on the class, so that we do not incur a function call each time.\n. @gwvo The comments could be used to add type-checking later on, e.g. with TypeScript.\n. This seems useful, why is it commented out?\n. Can PHP not encode these overflowing ints?\n. Is this check for 32 versus 64 bit platforms?\n. Old mention of Javascript here.\n. Is this a way to make the value signed? A comment or helper function would be good here.\n. Should this also reset vtables and other variables?\n. The big endian code path seems untested, is that correct?\n. Why is this signed?\n. I understand why this is here... is there a library function we can use instead?\n. Since it's here it should be tested.\n. This is the same code as putInt. How does it work?\n. I always use coreutils so I never noticed this. Blech OSX!\n. Because this is a constant, should we lift it up into a global variable instead? That way users don't have to use string equality on it.\n. Why is this private?\n. Can we avoid this alloc, or at least have a reusable version too?\n. Same concern regarding allocs.\n. Agree\n. I need this to be integrated into the test suite somehow.\n. Is there a reasonable way to incorporate fuzz testing, or are these happy-path tests enough? I don't know enough about GRPC to decide this on my own.\n. (i.e. using testing)\n. @JRonak Sure, if this is global and the same for every user, you can put it in a subdirectory of the go lib.. resolved. Please change this to $PYPI_USERNAME. Could you run the full Monster tests here? That way we can exercise more than just a few generated functions.. Where are the *_pb2_grpc.py files? :-). I like the generated comments!. I second @aardappel's thoughts: users who want to pickle will use pickle directly.. Is there a way to avoid byteswap (regardless of whether in_place is True or False)? If so, we can avoid allocation and also avoid modifying the input data. The intuition I have is that the write to the Flatbuffer on line 474 could use tobytes if the data are little endian, but otherwise it could push each element to the Flatbuffer individually, to preserve endianness. What do you think? Maybe put this behavior behind a flag?. Thanks for this feedback. Do you see a way to improve assertRaises to make this testable?. @bsoniam Would you be able to use CheckReadBuffer (or something like it that is comprehensive) here? That's what I mean. Source: https://github.com/google/flatbuffers/blob/master/tests/py_test.py#L80. @bsoniam want to delete this import?. Please undo these whitespace changes.. Please move this constant to encode.py.. Please import this from a pre-existing definition.. Thanks for testing the raiseing codepaths! :-D. Please add a test that checks the example monster data for the MONS file identifier. You can put this assertion in the pre-existing test function that reads the example monster data. Here is how it is done in Java: https://github.com/google/flatbuffers/blob/master/tests/JavaTest.java#L86. Please add a check for the False case. It can be as simple as:\npython\nself.assertFalse(flatbuffers.Table.HasFileIdentifier(buf, \"FOOO\".encode())). Please add tests for some other scalar dtypes (uint16, int64) and float and double types.. Please add an additional check that asserts that the dtype is a number, as well as a test for it. It's possible that users will try to pass in complicated objects (strings, etc) and we want to be clear that, right now, we won't support that.. Updated. Having the compiler check my logic in this way was helpful during development. I'm fine with removing it if you think it's a good idea.. Updated. Good eye! This is actually okay because it's a trait method, so unless users are importing Push, they cannot access it. Furthermore, I added the trait methods we use to the reserved keywords list, so a field called follow would be generated as follow_.. This is a great idea.. Wrote a macro for this.\n. Added a comment explaining why we can't do this (the Args struct takes a reference to a Vec3 so we have to make sure it lives long enough).. Removed it. Updated to use size_of. Please don't refactor this function unless you absolutely need to. I'd like you to add an .unwrap() for required fields instead. Within the switch blocks.. Upon further thought, what do you think about making a C++ utility function with this signature?\nc++\nstd::string WrapInOptionIfNotRequired(std::string, bool)\nThat encapsulates this logic? that way we can abstract out all the if (!field.required) stuff and also simplify the patch.\nYour code changes would then look like:\nc++\nreturn WrapInOptionIfNotRequired(\"&\" + lifetime + \" \" + typname, field.required);. Please commit the generated code. @jean-airoldie why did you remove these indents?. Please put } else if {s on the same line.. @jean-airoldie Please keep the PR as simple as possible. Indentation is a bigger problem, anyway, in the Rust generator, because (as you see) nested modules are at the same indentation level. :-). Looks like you need to commit the updated generated files! :-). Needs to handle the case when the given string is empty.. Is there a way to make this easier to understand? Using continue to skip underscores, then writing them in other branches, is hard to process mentally.. @aardappel Is it okay to remove None from the set of comparable values?. Please make this a more descriptive test name.. This would be clearer as monster.enemy().unwrap().name(), WDYT?. Okay, either way is fine by me. I think you should at least add more explanation that that particular code is designed intentionally to check a lifetime property. That will help future contributors/maintainers.. @mmastrac nice!\n. @felix any response to this?. @Tommytrg what is this line for? . This would return None even if the data is present -- not what we want!. This is where the conditional on required needs to be: if the field is required, don't do the Option map.. My thoughts:\n1) Ideally, we'd run everything on Appveyor, too.\n2) But, if we keep these Windows tests, Appveyor CI times will continue to stay very long (60 minutes wall clock time). Maybe we could try to parallelize our Appveyor builds?\n3) If a port is using Windows-specific features, then maybe we should keep the port test in Appveyor.\n4) If a port is not using Windows features, it seems justified (to me) that we can move its testing entirely over to Linux -- we don't need to test that the language itself runs on Windows like it does on Linux.\nThe least-risky thing to do here is to keep the Appveyor tests as-is. @aardappel WDYT?. @aardappel added them back to appveyor!. @oakad Not quite true: strings require heap allocations and byte slices don't. (Unless we use the unsafe functionality.). Should be if file_identifier is not None. Please write a comment explaining the type string <BBBB. I think there needs to be a conditional test here that checks that the file identifier exists, ideally using generated code.\nHere is example generated code in Java:\nhttps://github.com/google/flatbuffers/blob/master/tests/MyGame/Example/Monster.java#L16\nAnd its test:\nhttps://github.com/google/flatbuffers/blob/master/tests/JavaTest.java#L87. Please describe why we do this twice, for future maintainers.. While we're here -- would you add a parameter to this function that inserts n spaces to each line, then use it appropriately in the various places GenNamespaceImports is called? You can see that in the code from before this PR, the imports were prefixed by spaces.. Let's delete this whitespace line.. Let's manually create one extra whitespace line here (code_ += \"\"). @aardappel Can you look at this C++?. Please run the C++ test binary as the final step.. Please check the not case: that MonsterBufferHasIdentifier returns False.. When calling functions with keyword arguments: please provide the arguments as keyword arguments, not positional arguments.. Please call this using keyword arguments, not positional, for size_prefixed.. Please also check GetBufferIdentifier and HasBufferIdentifier.. Please fix typos. Why is this bytewidth?. Right, I misread :-). @pyottamus @aardappel Do you have consensus on this string encoding stuff?. = style. Please write a comment on this unusual range iteration.. No spacing between = for kwargs args.. Style of =. = style. Needs spacing: offset +=. No spacing for kwargs =. Please break this out into 2-3 lines by assigning the values to intermediate variables.. esapedID is misspelled. = style. Indentation needs fixing I think. = style. = style. @pyottamus yes!. Style. Style. Readability (break up long line). @dnr @aardappel maybe we can do that in another PR?. Is the idea with this that you've identified a case where the data are not read correctly, and you are encoding it in a test?. I think the real solution here is to use the struct constructor function, not to create it with the default initializer, like I seem to have done here.. @tymcauley Nothing as complicated as that -- just to use a new function, like we do with Vec3 here: https://github.com/google/flatbuffers/blob/master/tests/monster_test_generated.rs#L556. This is superseded by other other conversation in this review.. Please add a comment describing that we're acting as if we are the code generator by making FooStruct. @tymcauley If you think it would be better to use the real generated Vec3 for this, too, I'm fine with that.. @tymcauley perfect!. @tymcauley Just to be safe, want to add an assert here for using a Vector to access the u8 data? For both endiannesses.. @tymcauley Just to be safe, want to add an assert here for using a Vector to access the u8 data? For both endiannesses.. Indeed they do!. @dnr Did you think about having this generated function return the string value from the EnumNamesAnyUniqueAliases map? That way we don't need to import strconv, nor cause heap allocations by using strconv. If the key is not in the map, we don't want to cause a panic, so we could return a hardcoded string like AnyUniqueAliases_UNKNOWN.. I should've read more closely! You can keep the current logic, since, as you said, it shouldn't happen unless the data is bad.. Let's just use #[inline] here -- it's more idiomatic and gives the compiler more flexibility.. Can you explain this \"double-colon\" syntax to me please?. Why use cap, wouldn't that be unsafe if the buffer is reused?. Can you talk me through why you removed this branch of the if statement?. When I say \"reused\" I mean during the normal course of using a Builder in a hot path, where users are repeatedly calling Reset. My understanding, and please correct me if this is not true, is that using cap here would mean the users would get bogus data that they didn't write, if they wrote a payload that didn't use up all of the slice's capacity.. Thanks! I was not familiar with that syntax. Bigger picture... what is the purpose of changing the capacity? It seems like this would be surprising to users, and make thinking about allocations trickier.. Please also check that Pos(nil).Test2() is equal to the strongly-type enum value example.ColorGreen. (And add the same logic for the other test, too.). Right, thanks--juggling too many PR reviews here I guess :-)\n. Interesting! So you're essentially using the slice header (in Go-speak) to store the metadata needed to access the data?. ",
    "mikkelfj": "https://github.com/dvidelabs/flatcc just released\n. Mentioned on google groups, proposing using FNV-1 hash little endian as a default pre-calculated identifier for a given type:\nhttps://groups.google.com/forum/#!topic/flatbuffers/FsgsUOPj2Zg\n. For reference, the following file implements the type hash, but normally it would just be a generated constant.\n https://github.com/dvidelabs/flatcc/blob/master/include/flatcc/flatcc_identifier.h\n. I also doubt the usefulness, but an example usecase would be a handler function that accepts any number of table pointers and a hash type (the numeric equivalent to the type_identifier string). The table could originate from a union in a message envelope table, or directly as a root table, or by other means such as a UI event. In this case it is convenient to be able to map the union type to to a hash type similar to mapping a union type to an enum name. The solution would be a simple extension ot the union api: GetHashTypeFromUnionType, or mytable.myunion.HashType(), where 0 is NONE. Of course there is a risk of hash type conflicts, but it is managable.\n. flatcc 0.3.1 just released with Windows support. Documentation updated accordingly.\n. Haven't read through it all, but to the original point at top\n\nWe agreed on making the vector of union into 2 parallel fields: a vector of types and a vector of values. This will not break any existing encoding and has efficient storage. The downside is, iterating the vector has more work, as there are two underlying vectors to read from instead of just one.\n\nUsing two vectors is likely very efficient. First, the type field is a byte so it packs better. In the cpu it will fit comfortably in data cache and you avoid 3 wasted bytes of padding both in cpu cache and in buffer format. Second, you are possibly only interested in specific types in the union. If so, you need to access much less memory and can find the offset to the actual object with a simple array lookup. Furthermore this lookup is more likely to be in lookahead cache because it is more densely packed. The initial lookup could be more costly to feed the cache with both type and vector, but if it is a small vector, both are likely cached. If it is a large vector, then having the separate is all the more important.. It is probably too late now. But one thing worth considering it requiring the layout to be a type vector padded and immediately followed by the table vector. This will enable an implementation to reference the union vector with just one pointer which is highly practical while still allowing the union vector to appear as two normal vectors to other code. A verifier would enforce this. Otherwise the C implementation needs to define a struct to carry a dereferenced union vector.. Sorting, searching, key attribute if all tables have a key field of same type (and possibly same name)?. I tested out a meson branch on flatcc version 0.3.5 and made a sample project using it: https://github.com/dvidelabs/flatcc-meson-sample\nI added some flags to generate dependencies with flatcc - after doing this builds were much more exact than with CMake - change a second level included schema file and the necesary code rebuilds. I have never gotten CMake close to this. The Ninja build system used by Meson has a weakness with not handling a tool producing more than one output, but you can still drive dependencies by choosing one output file.\nBut - there are some issues:\n\nThe AppVeyor build wasn't out of the box - couldn't find MSVC compiler (didn't bother finding a fix)\nCompile flags are pervasive, so subprojects share your compiler flag settings making it difficult to include other source with different settings, e.g. old source needing different flags. Not a big issue for flatbuffers tools, but a concern with the Meson tool.\nPyhton 3.4 dependency: Meson does not use any external packages which is good, but it is still a minor concern. My meson sample project actually installs Meson via a shell script, so it is not a huge deal.\nThe external compiler support (such as flatc or flatcc) is a bit clumsy, but does work.\n\nOverall I would also like to fully support Meson, but so far there isn't sufficient interest, and the maintenance is an issue. I might add it anyway, because it is much better when dealing with code generators.\nThe other concern is that choosing Meson would not get rid of CMake.. The flags I added to flatcc are:\n-d                         Dependency file like gcc -MMD\n  --depfile=<file>           Dependency file like gcc -MF.\n  --deptarget=<file>         Override --depfile target like gcc -MT.\nand here are the build rules activating the --depfile flag:\nhttps://github.com/dvidelabs/flatcc/blob/meson/rules/meson.build. Right. Many projects are moving to Meson.\nI wouldn't want to support two build systems.\nMy take is that among all the new / old systems, Meson and CMake are two to choose between because of momentum and generality.\nFor CMake: Boost has just decided to move to CMake.\nFor Meson: systemd is now building with Meson on Fedora. Many Gnome projects are migrating if they haven't already. X Server and Wayland are using Meson. It seems that many projects currently using autotools are switching to Meson.\nBy comparison, the latest version of premake has been in alfa for the last four years or so and some other big-player systems depend on JVM which is a non-starter.\nI consider eventually switching to Meson if users do not object. The improved dependency managemement of generated code and the better Ninja backend is the main motivation for me. But it is also important that cross compilation is well supported, especially for flatcc that has a number of esoteric target systems to consider.. I'm also not too keen on providing build products but I am not entirely ruling it out either if someone can support it. See also:\nhttps://github.com/dvidelabs/flatcc/issues/66. I think it makes sense to support a json_base64 attribute on ubyte vectors. It allows generic flatbuffers to be converted to json. The alternative is hex escaped strings and I believe base64 would be more practical here.\nNotably REST application interfaces might want a simple JSON representation of data. \nEven if Flatbuffers is the better storage option, just the knowledge of being able to sensible export as JSON might help the migration to FlatBuffers.\nIt also solves the general problem of JSON not being very good at handling binary data. Having to store binary data trailing requires a lot of implicit knowledge that would not be good for interop but where the relatively low 33% overhead of base64 is fully acceptable.. Perhaps the attribute should have a json prefix so it isn't assumed that the ubyte stores base64 in binary buffers. For example attribute \"json-base64\";\nAlso the base64 encoding should possibly be urlsafe always, or there should be an attribute for it  like attribute \"json-urlsafe-base64\"; urlsafe replaces '-' with '+' and '/' with '_' and drops '='.\nhttps://www.npmjs.com/package/urlsafe-base64\nhttps://jb64.org/specification/\nThere is a whole separate discussion on unquoted multiline base64 which would be convenient for things like storing sprites in game objects, but it is probably best to stick with plain JSON strings in the official definition. Specific parsers could then have non-portable relaxed formats to help content producing humans.. It might be a good idea to allow whitespace \\r\\n\\t within the string. The two character escaped symbols and plain space, not the binary values 0x09, 0x0a, 0x0d, 0x20.. Base64 parsers frequently support white space between encoded symbols, such as when parsing multi-line email content. Therefore it is likely that there are base64 strings floating around which do contain whitespace. But if the majority of JSON base64 parsers object to whitespace, that would go against allowing whitespace.\nOn base64 vs base64url - if there are many JSON parsers that expect standard base64, then base64url should be a separate attribute, if supported at all.\nI believer '/' is permitted, but not required, to be escaped in JSON.. > Attribute base64 like hash attribute for string, it doesn't have any meaning outside of JSON parser.\nI believe hash is an experimental leftover, but it also ought to be prefixed json-hash IMHO.\nAttributes can be used in a number of contexts that has nothing to do with JSON, and other users might be confused when reading a schema without considering JSON serialization.. Come think of it, base64 could also apply to csv import / export, so perhaps a json-base64 is not clever after all?. On RFC - I wouldn't take it too literally, for example it is paranoid about secret messages in whitespace, not sure how widely it has been reviewed, but it is a good baseline.\nOn whitespace, JSON does not allow 0x09, 0x0A, 0x0D unless escaped. So it is fine to allow and ignore these after text escape has been processed, but we can't formally allow them in the string and call it JSON.\nOn can be omitted - needs to be clear about reading and writing. I printing cannot allow '=' in base64url but it would be fine to parse it. In fact, I believe parse should accept both base64 and base64url and optionally have a runtime option to fail non-strict format.\nI'm being general here because I too have a FlatBuffer JSON parser in the FlatBuffers C code generator FlatCC.\nOn [ubyte]: it could be applicable to other vector formats, but endianness would be an issue - but without higher aligned [ubyte] you will miss alignment. ubyte is a good start though. Perhaps vector alignment should be addressed separately.\nOn import/export prefix - that is perhaps too much detail and a bit long. Not really sure here.\nOn jb64 ambiguity - I don't think that is an issue. The format is probably not particularly official, just someone who already addressed the problem space and worth looking into. We don't want date formats mixed in with this.. I think we should at least allow base64 on both [ubyte] and string types.\nWhile strings are supposed to be utf-8 they are not always. Rather than having to deal with non-standard hex escapes supported in the current FlatBuffers JSON format, or with more complex, but standard unicode escapes which don't cover all possible byte values anyway, it would be good to allow a base64 encoding instead because even if it does violates utf-8, it does not violate JSON.. I have seen this form but I'm not sure how standardized it is. I also hesistate because it might lead to ambigouties with future extensions to FlatBuffers and FlatBuffers JSON or other syntax extensions. I am working on some extensions that are not fully pinned down.\nFurthermore, if this form were to be supported, it likely would have to be for all strings. I don't think it directly conflicts with string vectors but that is also a concern. So for now I think I the safest solution is to just support a single string, but it could be extended later, and it might not be a bad idea, I just don't have the overview.\nAs to \"\\t\\r\\n\\020\", this would not provide multiline directly, but only via external sources. So my reason to support them would be other tools converting imported strings.\nThere is a further complication:\nA few moments ago I convinced myself the logic must be: first expand the string and all escapes, and if array syntax, concatenate strings. Then optionally decode by base64. Thus, the decoder should choose if it supports the binary 0x09 etc. and not \"\\t\" etc. although of course it is free to combine these steps intenally.\nHowever, this circumvents the idea of having precise binary control over the input. Thus, I think we should strictly disallow any escape codes in the input. I wouldn't mind 0x09, 0x0A, 0xD, 0x20 except that only 0x20 is valid JSON.\nSo regardless of what I have set earlier, I think I prefer your original proposal for now and we could ponder on the array extension later.\nIf anyone wants a nice multiline entry - this can still be supported via custom syntax - as I am working on - but we cannot use this for portable data exchange while I simple string with only base64 or base64url works perfectly for that.. @gwvo can you have a look?\n. Wouter will have to guide there.\nOther: I think string type should also be supported, and that the base64u attribute should be renamed base64url.. I'm adding 'base64' and 'base64url' as predefined attributes to flatcc: listing them as an attribute is valid but not necessary. They cannot have a value, cannot coexist, and are only allowed on [uint8] vectors. ([uint8] is the same as [ubyte]).\nI only support uint8 vectors, but wonder if int8 should also be supported.\nI dropped strings, at least for now, because it added some complexity and corner cases.\nIf an uint8 vector has a  nested_buffer attribute with either base64 or base64url, it will be parsed and printed as if it was not nested, but the nested attribute is still used for generating accessors, verification etc.. Actually, flatcc generates an error if a predefined attribute is specified as userdefined attribute, so\nattribute \"base64\";\ngenerates an error, but the attribute is recognized on fields.. Pushed to flatcc master\nhttps://github.com/dvidelabs/flatcc/commit/efa54155e25387b233fa61b55a8b0fcb79ba2feb\n. Documented here (I'm not sure why there are so many RFC's related to base64?):\nhttps://github.com/dvidelabs/flatcc#base64-encoding\nTest case here:\nhttps://github.com/dvidelabs/flatcc/blob/be642b3146542c032731cc3d58e86667e11432ce/test/json_test/test_json.c#L312-L348\nAnd schema here:\nhttps://github.com/dvidelabs/flatcc/blob/be642b3146542c032731cc3d58e86667e11432ce/test/monster_test/monster_test.fbs#L104-L109\nNote: I decided to make base64 and base64url two internally defined attributes. We need to agree on this or something else, otherwise the schema will fail on flatcc. Notably flatcc will not accept attributes \"base64\"; because the attribute is already defined internally.\nI'm planning a release of flatcc v0.5.0 in the not so distant future, so I'd like to hear about any objections soon.\nAnother issue: it would be good to support (force_align: n) attributes on vectors, especially ubyte, because it allows for larger values to be loaded in base64 while the JSON parser does not have to guess about signs and endianness in the format. This is mentioned in the doc section linked above. However, I'm not sure if this makes it into 0.5.0.\nQuestion: if force_align is used, should the tail of the vector be padded to alignment?. Thanks for the update.\nWhy is padding mandatory for decode in base64? It is not necessary when you know the string length which you do with quoted strings. It certainly should be permitted but I don't think it should be mandatory - but if not padded, it should be checked that the data is zero in unused bits - my implementation does that.\nIn a sense padding is also redundant for encoding but it makes it possible to copy the text as is and concatenate with other text, so it is good style to do it.. Just a thought on padding:\nIt might make sense to skip padding for base64url, but since it is sometimes encoded with % escapes, it is best to leave it in so url encoders can do their thing. It is simpler to strip off later if necessary hence FlatBuffer JSON should always add padding.\nHowever, the JSON printer could have a runtime flag that says to skip padding for base64url or base64 without affecting the schema or the default behavior. This is not currently supported by FlatCC but the FlatCC JSON printer do have runtime flags that could support this. This is also an argument for always accepting unpadded input.\nLikewise, if, for some reason, the base64 parser should enforce padding in input, this could be supplied as a runtime flag to the parser.\nIn conclusion, I think the optional padding (by default) in parsing and mandatory padding (by default) in printing is the best behavior. RFC 4648 does permit omitting padding when this is documented so there is no violation.. I added unpadded test cases and fixed an unpadded decode bug at the same time.\nhttps://github.com/dvidelabs/flatcc/blob/f2e88383c35e0321dc7aae68dea9454cd4f70301/test/json_test/test_json.c#L305-L358. Can you please make sure to add base64 and base64url to known attributes? flatcc (for C) won't accept a schema with attribute \"base64\"; because it is already defined internally.\nEDIT: you already did that, you just didn't link the change in the above post:\nhttps://github.com/vglavnyy/flatbuffers/blob/1cd765a0f440e767045703d45cf2d6be6fc21f92/include/flatbuffers/idl.h#L541-L542. I just did, but nothing changed. I did manually copy the generated monster test thing in the original commit before seeing the PR message.. Note that the implementation maps the ALIAS type to the original token type. This could cause some confusing error messages but I could not provoke any easily. On the other hand it is likely to have very limited impact on backends. There is, apparenlty, no distinction between tokens and types in the parser, otherwise I would likely have created separate tokens in AST instead of remapping them.. Now I see a lot of changes - maybe it was due to building in a subdir before?. Forced push a clean update.. yup. ta. FYI: flatcc now supports this syntax. Not sure exactly how what causes the problem, but I encountered similar issues with flatc when developing flatcc and early documentation mentioned this as a difference.\nFlatcc does some cycle and dependency analysis for structs which I don't think is needed for tables where a simple forward declare works. Structs need to be put out in the right order if one struct is defined by another. The analysis is a bit tricky and time consuming to implement due to the mental load, but overall not that big of a deal. It's essentially just a depth first search.\nAs to include guards, I'd recommend using #ifndef FILENAME_H over #pragma once in part because it makes it possible to concatenate all files into a single file or to stdout - I used that feature today when implementing a script that expands macros and extract documentation. Another reason is that paths are not unique so pragma once might fail - as has been reported by some.\nflatc has relatively recently added support for mutually recursive schema file inclusion. This is not supported by flatcc and I'm not sure it is possible why ensuring that there is only one unique way to generate a header file for schema file, but haven't looked that hard.\n. I think this is where the magic happens: \nhttps://github.com/dvidelabs/flatcc/blob/95969b42898af1124fc23cd9dc2b07acc3bdc9e1/src/compiler/semantics.c#L461-L608\n. BTW: I don't think this was ever coordinated between flatcc and flatc, but flatcc ignores case and path when deciding if a schema file has already been processed - the reason is largely the same as for pragem once: a path is not reliable measure of identity. I.e. it looks at the uppercase base name. Therefore include guards can use the upper case basename. Maybe there is an odd edge case where this could cause a problem but I haven't heard of any.. Forward declarations won't work in the general case because a struct cannot by defined by another struct via fw decl. unless the member is a pointer, which it isn't. At least if FB structs are implemented as C/C++ structs or classes (at least that is the case of C and would think it also applies to C++).. Actually flatcc (for C) will not compile foo.fbs. This is intended. The reason is that flatcc creates a file for each schema file and the output is always the same regards of how a file is included. flatcc keeps track of the \"visibility\" of types.\nEdit: forgot not in \"will not compile\". > We have been discussing support for fixed size vectors in structs, though not sure if that would solve your problem. This is the most likely to actually happen, as it has been frequently requested. @mikkelfj\nI wrote up a summary of recent chat under possibly future things:\nhttps://github.com/dvidelabs/flatcc/blob/master/doc/binary-format.md#fixed-length-vectors\nAs to multi-dimension, at least for fixed length vectors I'd imagine a syntax like\nstruct {\n   foo : [int:4:2:16];\n}\nBut I haven't thought hard about it. It might co-exist with with [[int]] syntax for varlen because you can have multi-dim varlen like [[int:4:2:16]]. But it is a bit far out perhaps.\nAs to unions I also wrote a note here: https://github.com/dvidelabs/flatcc#union-scope-resolution\n\nThese limitations cause to create small adhoc struct or even table types to generally store 1 data field.\nAre there reasons relative to code generation for some languages ?\n\nOne reason is that unions are stored as two fields an one field holds a type or a type vector, and the other holds an offset to an object or to a vector of offsets objects, but you seem to be aware of this already from the above discussion.\nIt would be possible to have unions of type int, but they would be stored external to the table, or some basic assumptions would be broken. Structs are, or can be, generated with create calls, so you can create an int struct quite easily.\nIt has been a huge workload to add support for variable type unions in flatcc, especially due to the implications for code generation for JSON parsers. The type system is very complex. The schema parser semantics are trivial in comparison.. flatcc (for C) implements union vectors as a \"_type\" field which is a vector of types and the union value field which is a vector of values, each value being an offset to something outside the table, including structs. Because the type field can appear after the value field, backtracing must be supported. Note that the _type is not _types, to keep it consistent with single valued unions.\nBoth vectors must be present or absent and have the same length. Any union of type NONE must have null value in the corresponding value vector.\nSo e.g.\n{ test_type: [\"Monster\", \"NONE\"], test: [{name: \"Shreck\"}, null] }\nIn addition flatcc is considering an alternative notation that does not require a _type field nor backtracking, but Wouter and I do not entirely agree on this.. Some tests:\nhttps://github.com/dvidelabs/flatcc/blob/8aa10ad970dc8a97a704f8ec16c255bd82f6da26/test/json_test/test_json.c#L479-L503\nThe test macro lists the parsed input first, then the expected printed output from the parsed result.\nTo make full sense of the test case, the flatcc schema is needed, notably the Alt table with the manyany field:\nhttps://github.com/dvidelabs/flatcc/blob/a0cb4d71cd5290657ee091f5f920b6b43118ae20/test/monster_test/monster_test.fbs. Also some info on union vectors here: https://github.com/dvidelabs/flatcc/blob/master/doc/binary-format.md#unions. @shivendra14 You should be able to use flatcc to convert fb to json and vice versa, also in unions.\nhttps://github.com/dvidelabs/flatcc\n. I can't confirm by reading the code, but I added the test case for input and wrote the expected output in padded strict JSON. This passes the flatcc json test suite including verifying the nested buffer (which is kind of lucky since the monster_test.fbs are not exactly identical).\nhttps://github.com/dvidelabs/flatcc/commit/9ed2733455093817741b3c5889b26091b2f4c242\nNote that the test fails if flatcc compiles with UQ off (unquoted), meaning the test case fails when it expects strict json, but that is to be expected.\nI have released my implementation in v0.5.0 earlier today so the above is not in the release.. Note that I did not add runtime flags to control printing with or without padding - mostly because it was messy in the input, as you said. But it could be added later. I think what we have is fine for interop.. BTW: the TEST macro in my test takes two string arguments, the first is the input of a monster_test.fbs json file and the second argument is the expected json printed output without spaces if the parse is successful.\n  . I tried to test the schema, but some semantics have diverged a bit on namespaces and language specific attributes. Removing these from the schema and the schema parses cleanly with base64, base64url attributes.. @aardappel reservation in some form is necessary, otherwise the base64 decoder becomes very complex. The output size can be estimated conservatively within a small margin allowing the decoder to run without having to check output boundaries and manage flushing.. I just realized that this also needs to be tested with union strings and string vectors, and union vectors with strings.. I got confused - I forgot it was only ubyte and [ubyte] cannot be part of unions.\nI was thinking of a use case like loading multiple PEM certificates where each cert type would have different union type so the following could be handled\nhttp://how2ssl.com/articles/working_with_pem_files/\nThis would currently require each cert ot be loaded into a separate table, but may that is not a bad thing.. I\\m not sure exactly what the latest changes do, but it appears to enforcing padding.\nflatcc isn't very configurable right now, but I believe it accepts unpadded input and in the future will be allowed to generate unpadded.\nThere are some JSON base64 formats floating around that define base64 to  be unpadded, so I don't think it is wise to reject it, given that the string is fully delimited.. This is protocol buffers base64: \"JSON value will be the data encoded as a string using standard base64 encoding with paddings. Either standard or URL-safe base64 encoding with/without paddings are accepted.\" \nhttps://developers.google.com/protocol-buffers/docs/proto3#json. @vglavnyy note there is one new suggestion: that base64 decodes also base64url and that base64url also decodes base64 unless explicitly required not to by some optional configuration\nCurrently I'm not sure if flatcc does this, but I know that the implementation is prepared for it and I'd be happy to make this update later.. So, I know Wouters position is different here. But in my view, the \"reserve\" facility in STL i exactly designed for the case where you feed in a known quantity of data or need to process that data in-memory. So while I can that in most cases messing with reserve is just code obfuscation, I think that does not apply here. Especially considering that a base64 string could be in the size of many megabytes.. Hex notation for binary float avoids loss of precision, but it is not standard JSON, so it can't be relied upon in general. It can, however, be used as a configuration option. Also, many JSON parsers inadvertently support binary float because the strtod function does.\nI would advise against using hexfloat (binary float) as a general fallback. It should rather be either a general chosen option to accept and to print, and/or possible an attribute on specific fields in the schema.\nAs to correctness of float/double, this is very hard to deal with because there are often no finite sequence of decimal digits that represent a computer words representation of float or double. The best one can do is to print enough digits to not loose data in roundtrip, but even here there is no single unique representation, but the general consensus is to chose on a shortest possible representation as the more correct form. I could say a lot more about this in the context of fast Grisu3 parsing/printing I adapted for flatcc, but I'd leave it at: just because it looks different does not necessarily make it less correct.\nFor flatcc with grisu3 numbers are currently always printed in double, also for floats, even though it is longer than necessary - I tried to round down floats, but it cost too much performance. Without grisu3 it is just printf with 7/17 digits.\nThe grisu3 printer uses non-scientific notation when that format is shorter and so does printf g formatting option, I believe.. Here this the print logic used by flatcc\nhttps://github.com/dvidelabs/flatcc/blob/master/include/flatcc/portable/pprintfp.h\n. I don't think that format specification is necessarily a good idea. It assumes the use of printf which is one of the slowest part of JSON printing.\nIn FlatCC I work with grisu3 although users can override with their own printing logic. Grisu3 has no format parser. I'm also discussing with the designer of the new Ryu algorithm about controlling printing precision but that will highly likely not result in in a format string.\nMore like number of significant digits or so. The information I got from Ryu was that it might be easy (and faster) to print a fixed number of digits as opposed to the shortest possible string, which I find is a good solution. It would be nice with both a length limit and the shortest option if is shorter than the limit, but that is a nice to have. I can live with 4.0000 and 3.1415. I'm not entirely opposed to the idea of some attribute control but I think it is bit early to settle on a very flexible format. Perhaps use runtime or compile time flags until things settle a bit.. https://github.com/ulfjack/ryu. Yes, hex floats are faster and easier in many ways. I think I got some hex float logic,  but I never got around to fully integrate and test it.\nSome users cannot use hex floats due to interop (which is why you use JSON I guess). One user chose to override the print logic in flatcc (see link printer in above comment) to get the precision necessary, at the expense of performance.\nI have wondered about how precision would work with hex floats. I would assume you just print until leading hex digits are zero and don't bother with explicit precision control - so you never loose precsion with hex.. See also https://github.com/dvidelabs/flatcc/issues/90\n. Wouters attributes make sense, but I still think we should wait til Ryu matures to see what is practical because it is going til kill all other float printers but it can't have all the bells and whistles.\nI agree with generally not having this in schema - this is also why I don't think we should rush this. But sometimes you might need more control over the format on specific fields. Having a general tool specific switch ought to be the preferred approach in general. In my case by overriding pparsefp.h when compiling the JSON parser.. When doing this, could you also consider parsing \"Infinity\", \"Inf\", \"-Infinity\", \"-Inf\", \"NaN\" case in-sensitive? That should cover how most non-conforming JSON parsers deal with numeric exceptions.. Wrt. Infinity, NaN in my previous comment, I think all the forms should be parsed, but for printing it makes sense to follow protobuf \"Infinity\", \"-Infinity\", \"NaN\", or null if strict JSON is configured. https://developers.google.com/protocol-buffers/docs/proto3. There is a section in the README explaining this. It is lack of access to Windows for testing, secondarily that MSVC is very focused on C++ and not a likely consumer of the generated C, given that C++ exists. There is support for Windows in the include/flatcc/portable library, if someone will test it.\nhttps://github.com/dvidelabs/flatcc#limited-windows-support\n. It was copied from a specific C++ comment, so I assumed it wasn't relevant for other languages. Hence copying was the only way to also include C. Please advise.\n. I will try to abbreviate.\n. The generated code is already prefixed. The ns() style is merely a convenience that users can avoid, especially when namespaces are absent or short. In this case, all identifiers are prefixed MyGame_Sample_, which is too long to be readable. For example, the following are equivalent. This is explained in flatcc README.\nns(Monster_name_create_str(B, \"Orc\"));\nMyGame_Sample_Monster_name_create_str(B, \"Orc\");\nThe Reflection sample project is implemented this way, in it looks clean because the prefix is short and to the point:\nhttps://github.com/dvidelabs/flatcc/blob/master/samples/reflection/bfbs2json.c\n. right\n. No problem.\n. I'll keep it for now. It is still v 0.3 and breaks nothing to remove if user feedback indicates they prefer the other way. For me it quickly becomes very tedious to write those long prefixes even if I agree this isn't ideal.\n. FlatCC also tries really hard, hence the include/flatcc/portable library, but everything can't be covered at once by one developer. I'll look into getting a Windows VM running. It would be great if Google could somehow provide a big-endian testing platform.\n. I can do that, but the thinking is that whereever it is used would be more consistent with existing definitions and require less change if more aliases are updated. I'm happy either way.. Sorry, I misread you comment I though you meant GEN_TYPES_ALIAS -> GEN_TYPES_ALIAS_SCALAR.\nIf I used GEN_TYPES I would affect all current and future backends. I preferred to not touch all backend code generators, and I think it would be better to separate out the macro to one per backend.\nThe other issue was originally that I needed a separate token type, but that could be shoehorned into the above - but it would not be clean since the other types are backend types and only the first (ENUM) is a token.. ",
    "lbensaad": "Or may be you could use higher precision for unsigned types, for example for ubyte in buffer use short in java and for ushort in buffer use int in java.\n. I think it is the API who should do that always, since the user defined a field as unsigned (eg. ubyte) then he should get it as unsigned value, because if he wanted it signed then he should define as signed (eg. byte). I propose to to return both ushort and ubyte as int. Or it should be an option in the compiler.\n. Yes you can return uint as long, and for ulong you can say in the documentation that it is not supported only long is supported.\n. Good, especially if this library is described as \"Memory Efficient Serialization Library\"  so adding this feature should be a core feature, I also plan to use it with Netty which use pooling also.\nIt could be something like this:\npublic FlatBufferBuilder(ByteBuffer bb)\n    {\n        this.bb = bb;\n        this.bb.order(ByteOrder.LITTLE_ENDIAN);\n        space=this.bb.position();\n    }\n. That is good, thank you,\n. I see that you clear the Buffer before using it, what if the buffer contains other data? But i don't know if this is a possible scenario!\n. I think there is another problem with direct buffers, they don't have array(), and the FlatBufferBuilder uses it a lot so it will cause a problem.\nI think you should throw UnsuportedOperationException if the the provided buffer has no array (make a call to hasArray() to know that)\n. 1. It could be a better solution to use bb.capacity() instead of  dd.array().length; I did a Replace All in my code editor and test it and it did work, you can do more testing.\n2. You should also avoid the use of bb.array() and System.arrayCopy like in fbb.growByteBuffer() and fbb.createString. You should use bb.put(byte[]) and bb.get(byte[]) instead.\n3. i think growByteBuffer() and newByteBuffer() should be static functions.\n. Ok, thanks. But in Java, \n1. If i have a ByteBuffer of a non root table how do read that using FlatBuffersBuilder. The only way I know is to use the __init() function, is there another way?\n2. Is there a way to know the type of the table from the binary data?\n3. How to use the addXXXType function, in your JavaTest.java you used:         Monster.addTestType(fbb, (byte)1); how do you get the type of a table, in this case 1 which it is hard-coded ?\n. Thanks\n1) But i think the name should be just \"get\" or \"init\" instead of getRootAs\n. Ok, thanks, i will try that, in fact i tried to merge my commits in one commit but i did not know how, so i will try again.\n. Is it suitable Now\n. I am sorry, i am not familiar with Git, i got confused between commit, sqush, rebase. so i closed it and wanted to start again.\n. Yes, that the right commit. Thanks.\n. What i meant by type is not a union enum but sort of an ID assigned by the user or the compiler to tables and structs like the field id. Because if two peers are exchanging messages, then there is no way to know what type of message is contained in the data if there is no ID in the data (header).\n. Does the union accept structs now? or will it does?\nMay be i will use the file_identifier or use my own IDs.\n. I think the file_id works with only one table, so how to identify other tables?\nI end up using my own IDs class and removed the wrapping Message table defined above.\nSo my code now looks as follows\njava\npublic class MessageType{\n    public static final int CHALLENGE=1;\n    public static final int POINT=2;\n}\nand use the builder as follows:\njava\n        FlatBufferBuilder fbb = new FlatBufferBuilder(100);\n        Challenge.startChallenge(fbb);\n        Challenge.addX(fbb, x);\n        Challenge.addY(fbb, y);\n        Challenge.addToken(fbb, t);\n        int c = Challenge.endChallenge(fbb);\n        fbb.finish(c);\n        fbb.putInt(MessageTyte.CHALLENGE)\nso after I call fbb.finish() I then call fbb.putInt(type) for example to add the message type;\nFor parsing, now i use something like: \njava\n        int t=bb.getInt(0);\n        int offset=4;\n        switch(t){\n           case MessageType.POINT:\n                Point p=Point.getRootAsPoint(bb, offset);\n                processPoint(p);\n                break;\n           case MessageType.CHALLENGE:\n                Challenge ch = Challenge.getRootAsChallenge(bb, offset)\n                processChallenge(ch);\n                break;\n        }\nThe reverse writing in the ByteBuffer is a very good idea as you can see, wee can add header information without the need to copy in new buffer, and use only one write operation later to write the whole message to the socket/file. I am also planning to add the \"length\" of the message the same way as adding the type of message, like that i will know how many bytes to read before start parsing them, especially if the stream contains many messages. \n. Ok thanks, I will try that.\n. May be you can start now with returning just ByteBuffer, and the user can convert it as he wants with the ByteBuffer.asXxxBuffer() functions.\nYou can add the following lines to the file idl_gen_java.cpp inside the \"if\" after the line 286:\ncpp\n    code += \"  public ByteBuffer \" + MakeCamel(field.name, false) + \"AsByteBuffer(\";\n    code += offset_prefix;\n    code+=\"(ByteBuffer)(((ByteBuffer)(bb.position(__vector(o)))).slice().limit(__vector_len(o))) : null;}\\n\";\n. What the problem with alignment and why it is needed? Could you please explain it to me with example if possible. I saw it in the code but did not have time to fully analyse its role and i did not understand why it is there.\n. Yes I agree about efficiency, what about class inheritance model, did you think about it? \n. I don't think hex should be signed because the sign is a bit which is already included in the hex and bin representations. \nIf you accept signs in hex, then how would you interpret: n:byte = -0xFF?\nA byte that is equal to 0xFF is -1 in decimal, so what about -0xFF?\nIt will be complicated and confusing\n. ",
    "patlecat": "Ahhhh okay. Thanks, that did it. I usually put the makefiles into the build directory (with CMake) unless instructed to do otherwise. And so I had no clue where to put it.\nAnd how to use the sample binaries and in which directory do they go?\n. ",
    "splitcells": "It seems the Firefox addon Mouseless Browsing renamed that file by accident.\n. ",
    "RichardSteele": "That would be a viable option. If it's possible to choose between relaxed and strict syntax, the latter doesn't even have to be default.\n. ",
    "AlexStocks": "Good job. I appreciate your so fast response.\n. When you build a FlatBuffers object, you must start up from constructing its members. When you have build up all its members, you can create the root object. If you build the root object from the scratch, FlatBuffers do not know its memory size which depends on its member content. If you really do not like this idea, you can use protobuf instead by which you can build the root object firstly.\n. ",
    "jffmarker": "Much better; thanks!\n. How can I use this functionality? Given this schema:\nstruct MyStruct {\n    member:uint;\n}\ntable MyTable {\n    members:[MyStruct];\n}\nI expected to be able to do something like this:\nauto fb = GetRoot<MyTable>(NULL);\nfor (auto member : *fb->members())\n    printf(\"%i\\n\", member->member());\nBut MSVC 12 Update 2 gives this error:\n.../flatbuffers/flatbuffers.h(191) : error C2528: 'abstract declarator' : pointer to reference is illegal\n        ...\\Test.cpp(381) : see reference to class template instantiation 'flatbuffers::VectorIterator' being compiled\n        with\n        [\n            T=const MyStruct *\n        ]\n. The error is actually on the for loop, for (auto member : *fb->members())\nThe same error is generated if I expand that to for (auto iter = fb->members()->begin(); iter != fb->members()->end(); iter++) with a blank loop body.\nThis seems to be particular to Vectors of structs (vs. Vectors of PODs).\n. Thanks for investigating. I've never tried to write custom iterators, and appreciate someone more knowledgable taking a look.\n. ",
    "baszalmstra": "Thanks! Appreciate the quick response!\n. I understand the reason Get is there. My iterator implementation calls IndirectHelper directly, replicating Get's functionality. I could also implement the iterator by calling Get but that will give the Iterator a little more overhead. I think this implementation is cleaner.\nI justed tested my implementation with VS2010 and can confirm that it compiles and runs.\n. Per your request I removed the spaces and squashed the commits.\n. ",
    "evolutional": "I'm going to take a look at this. I recently ported the code to .NET (C#) as close to straight port of the Java version. However there's a few things in the codegen I'd like to change, so anything to make it easier would be interesting.\n. I know this is ~11 months old, but are these Benchmarks something that we can use today? I'm looking at porting them to C# so we can compare C++/Java/C# - ideally add other languages too.\n. As a simple first step, a unified benchmark to compare FB implementations against each other may be good\n. I threw together a quick port of the benchmark to .NET (https://github.com/evolutional/flatbuffers-java-benchmark/commits/cs-port)\nI didn't have the .fbs so crafted my own from the generated java code.\nI'm not getting anywhere near the Java numbers above on the PC I ran it on (Win7 x64 Intel Xeon E5-1650v2 @ 3.5Ghz / .NET FX 3.5)\nName                                  Mean   StdD  Unit     1M/sec\nEncode                               0.003  0.001 ms/op        3.4\nDecode                               0.000  0.000 ms/op        0.0\nTraverse                             0.002  0.001 ms/op        1.7\n(10 warm up, 50 measurement iterations)\nEither:\na) my benchmarks are bad (possible - we don't have JMH for .NET, I used a lambda for the test action, may have added overhead) \nor \nb) the .NET version of Flatbuffers needs optimization :) (possible)\nI'll do some tinkering; re-run the benchmark on my main PC, across .NET 3.5, 4.0 & 4.5 and with/without the \"unsafe\" version of the bytebuffer. I'll run the java benchmark too, so I can compare them.\n. Thanks!\nNot being too familiar with the Java benchmark suite, the \"score\" - is that the measured average? \nSo 1.137 us/op would be 1 operation in 1.137 microseconds?\n. I think I was measuring the timings wrongly. I've adopted the approach here (http://stackoverflow.com/questions/1206367/c-sharp-time-in-microseconds) to measure in microseconds and am now getting these figures:\n50 iterations - Win7 x64 Intel Xeon E5-1650v2 @ 3.5Ghz & .NET FX 3.5\nSafe ByteBuffer\nName                                  Mean   StdD  Unit\nEncode                               1.264  0.486 us/op\nDecode                               0.018  0.039 us/op\nTraverse                             0.476  0.181 us/op\nUnsafe ByteBuffer\nName                                  Mean   StdD  Unit\nEncode                               1.068  0.377 us/op\nDecode                               0.022  0.042 us/op\nTraverse                             0.366  0.183 us/op\nThe unsafe bytebuffer is noticeably faster on the encode action. The performance still appears to be lower than the figures you posted for the java version, though - but I can't verify that until I can run the same Java benchmarks on this machine.\n. Cracking out dotTrace from Jetbrains, it looks like the hottest function is the FlatBufferBuilder.CreateString() method, taking over 50% of the time within the Flatbuffers assembly. \nSpecifically, the call to Encoding.UTF8.GetBytes() is expensive. This is the function that takes the .NET wide strings and converts to UTF8.\nThe next most expensive call is FlatBufferBuilder.EndObject(), but it's nowhere near as expensive as CreateString().\n. Made an optimization to CreateString (PR inbound) which changes the timings to:\nSafe Bytebuffer\nName                                  Mean   StdD  Unit\nEncode                               1.108  0.372 us/op\nDecode                               0.018  0.039 us/op\nTraverse                             0.478  0.249 us/op\nUnsafe Bytebuffer\nName                                  Mean   StdD  Unit\nEncode                               0.988  0.386 us/op\nDecode                               0.018  0.039 us/op\nTraverse                             0.380  0.218 us/op\n. Making a few more tweaks to how Pad/Prep is called and I can save a few more cycles on the benchmark (I'll prepare another PR)\nSafe\nName                                  Mean   StdD  Unit\nEncode                               1.042  0.358 us/op\nDecode                               0.018  0.039 us/op\nTraverse                             0.486  0.246 us/op\nUnsafe\nName                                  Mean   StdD  Unit\nEncode                               0.906  0.374 us/op\nDecode                               0.020  0.040 us/op\nTraverse                             0.358  0.205 us/op\n. Ran the Java and C# version of the benchmarks on the same machine (Intel i7-4770K @ 3.50Ghz Win10 x64). \nResults in here https://gist.github.com/evolutional/85b9c6fac33a8455945d\nWith the optimizations I discuss in the gist, we achieve roughly equivalent performance (with Java direct bb being slightly quicker).\n. Nice, I'll pull over the C# benchmarks\nOn Wed, Jan 6, 2016 at 5:02 PM -0800, \"Wouter van Oortmerssen\" notifications@github.com wrote:\nThe C++ benchmark code is now in the repo (in its own branch, see comment at the end of https://google.github.io/flatbuffers/md__benchmarks.html\nAnyone interested in integrating benchmarks for other languages, these could go in the same branch.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/google/flatbuffers/issues/55#issuecomment-169513322\n. Won't the C# version also suffer from the same problem?\n. Related PR: #1253\n. If we add IList support, I would prefer it to be IEnumerable if possible, as it provides more flexibility.\n. Forgot to verify the build with cmake; will resubmit\n. Yeah; it's the use of Buffer (and I guess require()?).\nI was considering what would be needed for browser support - it would mostly be around providing support for read/write LE data. This is built into the Buffer class by default. One thought I had was to allow an injectable buffer io object, so that in node it can use buffer or in the browser one crafted from the (u)int8 typed array with added read/writeLE methods.\n. @rw It was based initially on the Python port (the generators are similar and could conceivably be merged in a future update). It then modelled some characteristics of the C# port (because that's the one I worked on first/most). So it's something of a hybrid.\nI agree 100% about the more tests; I was going to raise a discussion about testing of FB ports, as there's no clear standard on what we should follow. For something as low level as FB, then the tests you suggested definitely make sense.\nI'll update to run in a browser. Indeed, I was going to work on submitting that after this, but it makes sense to do it now.\n. @gwvo - would you prefer the DataView to be used everywhere, replacing the nodejs buffer? I'd have to do an initial endian check (like the example) and implement my own read/writeLE routines. Itd mean more if checks, I think - unless I don't set them on the prototype but rather construct my buffer io object in a function eg:\njavascript\nvar bufferIO = function(bytes) {\n    if (isLittleEndian) {\n       this.readInt16 = function(o) { /* no byteswap */ };\n    }\n    else {\n       this.readInt16 = function(o) { /* byteswap */ };\n    }\n};\n. @gwvo You're right, I didn't realize there was the optional endianness on the dataview.\nBuffer is more performant than Int8Array. I'll run some perf tests to see the difference in this implementation\n. I'm updating it to be usable in both node and in the browser.\n. The first two categories do help end users - if we have a clear way of verifying them of course. Something that springs to mind is that there's a couple of features missing from the js port (required fields, mutators, etc) that are in other ports. Having a way of knowing this from the tests would be really nice.\n. If this implementation is more fully featured (and tested) than mine, I'm happy to withdraw my PR in favour of this.\n. I've been working on my local copy, so will comment on the main differences:\n- I still don't have clean UTF-16 <-> UTF-8 conversions; this is good to see here\n- I was adding more explicit node/browser module handling (eg: my version can be used in the browser and node without anything else)\n- Having the code emit as a single file really helps the dependency problems I had to code around\n- The annotations are nice\n- This version has 64-bit numbers; I was using the native JS number, which I think is a 53bit double internally\n- I was designing my tests to not need node / requireJS - but it's not an issue if it does\nFunctionally, it looks the same.\nI think we should still follow @rw's recommendation and add fuzz / performance tests to this. I haven't written any of those for PR 250.\n. Another difference; in my local version I have support for things like test() and getTest(obj).\nThis version supports test(obj) and will do the conditional around the obj param and create a new one if not present.\n@gwvo - Are you ok with this?\n. If needs be, I can always grab it and fix it up if @evanw isn't available.\n\nOn 13 Oct 2015, at 13:52, Wouter van Oortmerssen notifications@github.com wrote:\n@evanw : how much work do you feel remains to get this PR in good shape?\n\u2014\nReply to this email directly or view it on GitHub.\n. The Mono Compiler in Unity is based on an old version; it's not good to assume that it'll work without testing in Unity. I'll grab your PR and test it in the next couple of days unless you can do it in the meantime.\n. I tested the code in gist (https://gist.github.com/evolutional/72ccf61a76479be97601) in Unity 4.6.9f1 and Unity 5.2.2f1 and had no issues. Everything compiled ok and the methods all got called correctly. I even tested with my own custom struct to see if I could see the bug in the linked thread, but I didn't encounter an issue. It's likely they fixed the bug, or maybe it occurs for certain types of structs - but I didn't see it.\n\n@gwvo - I think we're ok\n. You're welcome @mfcollins3 :)\n. There's a bunch more that can be ported over, namely the fuzzing of the generated monster code, but this is a start.\n. @gwvo - do we have a policy for API breaking changes in the non C++ generators?\n. I may have time to take a look on the weekend if nobody else is able to.\n. I took a look at the generated code in each of the languages:\n- C++: Uses fully qualified names; no problems anticipated.\n- C#: No \"using\" statement or use of fully qualified name; issue is as described originally\n- Java: No \"import\" statement or use of fully qualified name; may be a problem?\n- Go: No \"import\" statement or use of fully qualified name; may be a problem?\n- PHP: No \"use\" statement or use of fully qualified name; may be a problem?\n- Python: No \"import\" statement or use of fully qualified name; may be a problem?\n- Javascript: Uses fully qualified names; no problems anticipated.\nI think there's likely to be a problem in most of the implementations except C++ and Javascript.\nI started looking at a C# specific fix, but as that code is shared between Java and Go, I need to go and study how Java and Go handles name qualifiers to ensure that the \"fix\" doesn't break anyone else. Ideally, I'd use a fully scoped name in all of them - will this work in Java and Go?\nPHP and Python may also have problems - I don't know if the authors of these generators could take a look and confirm?\n. I've been looking at a fix for the general generator (C#/Java) and have a decision to make. We can either fully qualify the namespace or try and partially qualify it from the current NS.\nEg:\nOption 1\ncs\nnamespace NamespaceA\n{\n   public class MyType\n   {\n       public NamespaceA.NamespaceB.SomeType GetMyThing(...) {}\n   }\n}\nor:\nOption 2\ncs\nnamespace NamespaceA\n{\n   public class MyType\n   {\n       public NamespaceB.SomeType GetMyThing(...) {}\n   }\n}\nI have the first one working fine but it looks a bit ugly. The second will take some more work, but it's more common in C#.\nAn alternative is to put using NamespaceA.NamespaceB statements in the code, but that could be problematic with symbol clashes.\nEDIT: It's worth noting that it will only fully qualify a NS if it's different to the one the current definition is in\nEDIT2: The Java code generated for Option 1 is as follows:\njava\npublic NamespaceA.NamespaceB.TableInNestedNS foo() { return foo(new NamespaceA.NamespaceB.TableInNestedNS()); }\nI don't know enough Java to know if this is correct. I assume that fully qualifying a name is the same in Java as C++/C#?\n. I referenced the code in the C# project (for compilation sake), but not the others. I'll take a look at adding it to the Java test application, at least. I'll change the C# code to emit a single file, to cut down on the generated files - I could also omit the non-C# / Java code as it isn't part of this fix/change.\nI can't confirm if the issues I talked about in #360 are present in the other languages (Go, php, Python), but the generated code in this CL will allow someone to confirm?\n. I think Travis is stuck\n. done!\nedit: merge conflicts - I'll sort\n. I appear to have made a bit of a mess! My fork wasn't fully up to date so my changes are now in the \"past\" after the latest merge. Any further rebase brings in all these other changes. Any idea how I fix this up?\n. Well that was fun!\n. It doesn't fix the issues in Go/Python. Maybe we need a new issue to test/fix them in each?\n. It's not about allocation, more about usability. When I'm using the generated API (eg: monster), it's not as intuitive as I'd have liked. The \"XXXBuilder\" class would remain the same (eg: static methods, etc) but be a class in it's own right. This would bring the C#/Java APIs more in line with the C++/Python code which does have separate builder/accessors.\n. so we'd have...\njava\npublic final class MonsterBuilder {\n  public static void startMonster(FlatBufferBuilder builder) { /* etc */ }\n  public static int endMonster(FlatBufferBuilder builder) { /* etc */ }\n}\ncs\npublic static class MonsterBuilder {\n  public static void StartMonster(FlatBufferBuilder builder) { /* etc */ }\n  public static int EndMonster(FlatBufferBuilder builder) { /* etc */ }\n}\nAnd then the regular Table-based classes...\njava\npublic final class Monster extends Table {\n  public static Monster getRootAsMonster(ByteBuffer _bb) { /* etc */ }\n  public Vec3 pos() { /* etc */ }\n  /* etc */\ncs\npublic sealed class Monster : Table {\n  public static Monster GetRootAsMonster(ByteBuffer _bb) { /* etc */ }\n  public Vec3 Pos { get { /* etc */ } }\n  /* etc */\nIf we wanted to ever swap the C# accessor classes for structs, we could - the builder wouldn't need to change.\n. We could gate it with a command-line option if we went ahead.\n. I think the Bloch builder pattern is more common in Java  - I haven't really experienced it a lot in C#, but it makes. I'd be in favour of that pattern. It does, however, differ from the C++ / Python implementations.\n. Java & C# have similar methods of string literal interning (https://msdn.microsoft.com/en-us/library/system.string.intern.aspx)\nHow do we go about measuring the allocations in Java? Perhaps we need a benchmark for alloc/GC in the managed languages.\nGranted, having an extra string will mean that memory is used to hold the string contents - more than the current implementation for sure. If this is a concern, then I agree it's not worth merging (even though the current error messages aren't ideal).\n. I think we left it to see if others had opinions. Seems like nobody really minds (or cares), perhaps just close?\n. I think I'd prefer if the C# code emitted the correct float value 1.0f rather than (float)1.0\n. I was thinking about this the other day. Ideally, the user would specify the epsilon but there's nothing in the Flatbuffers spec to handle this.\n@gwvo - thoughts?\n. I agree with @chobie that the AddFloat() method should be taking the value and default as a float, not a double.\nI'm not sure of the best way of checking for it being set without an epsilon check; or if they're not so useful on the AddFloat methods maybe we consider removing the default?\n. Thanks to @ennerf for their Java benchmarking which helped lead me to this tweak.\n. Have been experimenting with the C# performance. There's some stats in a gist here - https://gist.github.com/evolutional/85b9c6fac33a8455945d\nThe CreateString() + Pad() optimizations result in approx 20-25% gains in the safe version of the encode benchmark and around 17% on the unsafe version.\nThere's one more optimization I have experimented with; using a #define to remove the AssertOffsetAndLength() checks on the ByteBuffer access. This yields a good performance gain in the unsafe build (very little effect to the safe build), but obviously is more risky. \nWould people be interested in seeing that additional change in this PR?\n. I'll prepare a PR with the define that removes the bounds checking. Do you want me to rebase & squash into a single commit?\nThe code for the benchmarking is here - https://github.com/evolutional/flatbuffers-java-benchmark/tree/cs-port - It's a port of just the flatbuffers part of @ennerf 's java benchmarking to C#.\n. This has an issue - will resolve and resubmit.\n. Possibly; I'll verify.\n. I'm cleaning this up (and rebasing - didn't realise I had this in master). I'm going to trash the 3.0 project.\nThe solution is fine in 2010. The only project that isn't compatible is the .NET 4.5, which makes sense as this is the newer framework version. What do you want to do in this case?\n. It'd be nice to build these into a nuget package at some point and release it every major update; that way people can consume as binaries. They'd need a flatc.exe or equivalent.\n. Closing this off (moving to another branch so I can have master up to date). If you're still interested I'll issue another PR.\n. You need to build the inner-most data for the table before you use it.\nFor example, you're building a Mesh object - you need to first build the vectors for the properties, positions, etc outside the mesh table before using them. Before you can build the vectors, you need to build the structs into the buffer first.\nHere's an annotated code example in C#:\n``` cs\n            var fbb = new FlatBufferBuilder(128);\n        // Let's build a mesh\n\n        // We want to build the members of the Mesh from 'in' to 'out'\n\n        // Create the id in the buffer\n        var meshIdOffset = fbb.CreateString(\"mesh01\");\n\n        // This will create a vector of the Vertex Attributes in the buffer\n        // there's a helper for these, as they're scalar types\n        var meshAttributesVectorOffset = Mesh.CreateAttributesVector(fbb,\n            new[] { VertexAttributes.POSITION, VertexAttributes.NORMAL });\n\n        // Create your vector for positions, you have to 'Create' these\n        // and do it in reverse order\n        Mesh.StartPositionsVector(fbb, 2);\n        var pos1 = Vector3.CreateVector3(fbb, 10, 20, 30);\n        var pos2 = Vector3.CreateVector3(fbb, 40, 50, 60);\n        var positionsVectorOffset = fbb.EndVector();\n\n        // Do the same for normals, tangents, uv0, uv1, etc\n\n        // Now start the mesh table...\n        Mesh.StartMesh(fbb);\n        Mesh.AddId(fbb, meshIdOffset);\n\n        // Add the position vector to the vtable\n        Mesh.AddPositions(fbb, positionsVectorOffset);\n\n        // Do the same for normals, tangents, uv0, uv1, etc\n\n        // finish the mesh\n        var mesh01Offset = Mesh.EndMesh(fbb);\n\n        // Now we're thinking about building the DabModel, same principle applies\n        // -> build in to out\n\n        // Create your data for the model first\n        //id\n        var modelIdOffset = fbb.CreateString(\"model01\");\n        // create a vector which points to the mesh we made earlier\n        var meshesVector = DabModel.CreateMeshesVector(fbb, new[] {mesh01Offset});\n\n        DabModel.StartDabModel(fbb);\n        DabModel.AddId(fbb, modelIdOffset);\n        DabModel.AddMeshes(fbb, meshesVector);\n        var modelOffset = DabModel.EndDabModel(fbb);\n\n        DabModel.FinishDabModelBuffer(fbb, modelOffset);\n\n        // Now read it back\n        var dabModel = DabModel.GetRootAsDabModel(fbb.DataBuffer);\n\n        Console.WriteLine(\"Model id: {0}\", dabModel.Id);\n        Console.WriteLine(\"  Mesh Count: {0}\", dabModel.MeshesLength);\n        for (var i = 0; i < dabModel.MeshesLength; ++i)\n        {\n            var mesh = dabModel.GetMeshes(i);\n            Console.WriteLine(\"    {0}: Mesh Id: {1}\", i, mesh.Id);\n            Console.WriteLine(\"      Positions: {0}\", mesh.PositionsLength);\n            for (var j = 0; j < mesh.PositionsLength; ++j)\n            {\n                var pos = mesh.GetPositions(j);\n                Console.WriteLine(\"        {0}: X:{1} Y:{2} Z:{3}\", j, pos.X, pos.Y, pos.Z);\n            }\n        }\n\n```\nI have been working on a tutorial for the C# generator and code, but progress is slow.\nHope this helps get you going!\n. It does result in better performance, however the caveat above still applies.\n. I agree with @mfcollins3 - I think a struct to hold essentially the \"pointer\" (bb offset & byte buffer) and then a series of static helper methods would be the way to go here.\nAs a general thought, would this pattern also apply to the Java version? I have concerns about diverging the two APIs considerably as right now it is easy to keep feature parity due to the similarities.\n. Well, I can see two approaches in addition to the route you took.\nThe first would essentially mean taking the ByteBuffer and bb_pos from the base (Table/Struct) and generating it inline with the new structs. The methods that existed on Table would be put into a TableHelpers static class.\nSample as follows:\n``` cs\n    public static class TableHelpers\n    {\n        /// \n        /// Refactored from __offset\n        /// \n        public static int GetOffset(ByteBuffer bb, int bb_pos, int slot)\n        {\n            return 0;\n        }\n    public static string GetString(ByteBuffer bb, int offset)\n    {\n        return \"\";\n    }\n}\n\npublic struct Vec3\n{\n    private int _pos;\n    private ByteBuffer _bb;\n\n    // c'tor replaces __init\n    public Vec3(int pos, ByteBuffer bb)\n    {\n        _pos = pos;\n        _bb = bb;\n    }\n\n    public float X {  get { return _bb.GetFloat(_pos + 0); } }\n    public void MutateX(float x) { _bb.PutFloat(_pos + 0, x); }\n    // etc\n}\n\npublic struct Stat\n{\n    private int _pos;\n    private ByteBuffer _bb;\n\n    // c'tor replaces __init\n    public Stat(int pos, ByteBuffer bb)\n    {\n        _pos = pos;\n        _bb = bb;\n    }\n\n    public string Id\n    {\n        get\n        {\n            var o = TableHelpers.GetOffset(_bb, _pos, 0);\n            return o != 0 ? TableHelpers.GetString(_bb, o + _pos) : null;\n        }\n    }\n}\n\n```\nA second method would involve taking the ByteBuffer and bb_pos and wrapping them in their own struct which encapsulated the buffer & position, eg: \ncs\n    public struct BufferPosition\n    {\n        public int Position;\n        public ByteBuffer Buffer;\n    }\nWe'd then pass this around to the TableHelper methods. The trouble is with this is that it makes the get/mutate accessors pretty ugly, so we may want to consider moving the stuff in that static TableHelper class into methods on the BufferPosition struct instead. eg:\n``` cs\n    public struct BufferPosition\n    {\n        public int Position;\n        public ByteBuffer Buffer;\n    public int GetOffset(int slot)\n    {\n        // etc\n    }\n\n    // Method created from the logic of the get accessors (eg: __offset + __string)\n    public string GetStringAtOffset(int offset)\n    {\n        // etc\n    }\n}\n\n```\nThe accessor methods now start to look like this:\ncs\n       public string Id\n        {\n            get\n            {\n                return _buffer.GetStringAtOffset(0);\n            }\n        }\nAt this point, we'd likely change the name of that struct.\nOf course, we've now diverged massively from the Java implementation.\n. Should we look to be merging Go/Python or the other languages into the general generator? I mean there is benefit in keeping 'general' around as it allows the languages to maintain feature parity. I do have a concern that they'd diverge in feature support if they were split. \nThat being said, there's a lot about the generated C# I don't like (formatting, mutators not being 'setters', etc) that being a C# developer by trade rile me every time I use it. The per language branching statements do make it less readable and doesn't guarantee that a change to Java will make it into C#, for example. \nPerhaps we can fight back the feature divergence by creating language specific issues in github for every time a change is made to C++. That way we'd have a per-language backlog and sight over needs what.\nIf we did split the generators, we should seek to build out the 'common' library so that we have functionality that be shared between generators easily. In my last job, I used to maintain our code generators and much of the language quirkiness was pushed into helpers which left the actual template having a lot of the language specific stuff hidden.\n. I think to come up with a consensus, we'd need more opinions from other C# users. I might go ahead and hand roll a demonstration of my approach to see if it a) works in practice and b) has the performance benefits we expect.\nSplitting up the generators should 100% be a separate PR that leaves the C# & Java generated code 100% identical, with only the backend changing.\n. You make a good point about the Unions @belldon \n. Sure, I can take a look.\nOn Wed, Jan 6, 2016 at 12:36 PM -0800, \"Wouter van Oortmerssen\" notifications@github.com wrote:\nSounds like a plan. @evolutional do you want to take the lead on this?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/google/flatbuffers/pull/2777#issuecomment-169453463\n. I would consider adding the Parser to a separate assembly/cs project called FlatBuffers.Utils.csproj or maybe FlatBuffers.Parser.csproj\nThe rationale is that I see this as an extension for \"power users\" that will require a separate exe/dll build. If people want this feature, they'll know (or need) steps to set it up - or we distribute binaries with the compiled assembly via NuGet. \nThe idea of the main assembly being dependent on a separate binary doesn't sit too well with me.\nOther than that, it's a nice feature - one that I can see would be useful to some people.\n. Looking at the monster example, it looks like this is handled by deliberate skipping of the field with id: 7 (the union is declared with id: 8)\n. What are you thoughts on explicitly renumbering based on the union type fields being present?\nIf we have:\nA:int (id: 1)\n   B:uniontype (id: 0)\nIt would end up renumbering to:\nB_type: 0\n   B: 1\n   A: 2\nIt depends if the id attribute is to provide intent or actual explicit ids. If the latter, then I guess we just make it more clear that this is what you need to consider when numbering explicitly, and maybe tidy up the error message.\n. I don't see much of an issue in providing an option to allow this in the code gen. Obviously it won't be defaulted to on as it'll break back compatibility. Downside is that it'll apply to all tables in a flatc execution, which may be an issue if only one or two tables need it.\nOne thing about this is that it's very c# specific and how we feel about adding per language options on the command line?\nOn Tue, Jan 19, 2016 at 11:34 AM -0800, \"Jason McGuirk\" notifications@github.com wrote:\nSure thing - I went ahead and just swapped \"SEALED\" for partial for our own purposes, but seems like it would be generally useful - and Protobuf-net spits out partial classes fwiw.\nThe primary use case is when you want your data classes to also be able to also have business logic attached to them.\nFor instance, consider a game where buildings and other things can be harvested. If you're using an entity/component framework, you might encapsulate configuration for such behavior inside a class called \"HarvestComponent\"\n```\n/// \n/// Harvest component, encapsulates a harvestable entity\n/// \npublic class HarvestComponent : BaseComponent\n{\n        /// \n        /// The resource harvested\n        /// \n        public string HarvestResource;\n    /// <summary>\n    /// The amount of the HarvestResource that can be stored\n    /// </summary>\n    public double HarvestCapacity;\n\n    /// <summary>\n    /// The minimum time after collection before a harvest can be attempted again\n    /// </summary>\n    public double MinHarvestTime;\n\n    /// <summary>\n    /// The rate of resource generation (resources per minute)\n    /// </summary>\n    public double HarvestRate;\n\n}\n```\nCool - now we have the configuration data. Where should we put our business logic in game for powering the harvest (for instance, asking how much of a particular resource can be harvested right now for a particular object?)\nThere's two options here - we either stick it on the configuration bean (rich data model) or expose a manager to own handling that data (service model).\nHere's an example of the former.\n```\n/// \n/// Harvest component, encapsulates a harvestable entity\n/// \npublic class HarvestComponent : BaseComponent\n{\n        /// \n        /// The resource harvested\n        /// \n        public string HarvestResource;\n    /// <summary>\n    /// The amount of the HarvestResource that can be stored\n    /// </summary>\n    public double HarvestCapacity;\n\n    /// <summary>\n    /// The minimum time after collection before a harvest can be attempted again\n    /// </summary>\n    public double MinHarvestTime;\n\n    /// <summary>\n    /// The rate of resource generation (resources per minute)\n    /// </summary>\n    public double HarvestRate;\n\n    /// <summary>\n    /// Gets the amount of resource that can be currently harvested\n    /// </summary>\n    public double GetCurrentHarvestValue(EntityPersistentData persistData){\n            double timeElapsed = (ServiceLocator.time.projectedServerTimeInMilliseconds/1000 - persistData.Harvest.LastHarvestTime);\n             return Math.Min(HarvestCapacity, Math.Floor(HarvestRate * timeElapsed));\n    }\n\n}\n```\nAnd here's an example of the latter (Service model)\n/// <summary>\n/// Service that manages business logic for harvesting\n/// </summary>\npublic class HarvestManager\n{\n        /// <summary>\n        /// Gets the amount of resource that can be currently harvested\n        /// </summary>\n        public static double GetCurrentHarvestValue(HarvestComponent, hc, EntityPersistentData persistData){\n                double timeElapsed = (ServiceLocator.time.projectedServerTimeInMilliseconds/1000 - persistData.Harvest.LastHarvestTime);\n                return Math.Min(hc.HarvestCapacity, Math.Floor(hc.HarvestRate * timeElapsed));\n        }\n}\nThis is by and large a stylistic choice (folks from a primarily java background will tend to favor the service model in my experience), but making the class partial enables both styles - like so\nSchema:\ntable HarvestComponent{\n        HarvestResource:string;\n        HarvestCapacity:float;\n        MinHarvestTime:float;\n        HarvestRate:float;\n}\nPartial:\n/// <summary>\n/// Extension of HarvestComponent data bean that encapsulates biz logic and other convenience methods\n/// </summary>\npublic partial class HarvestComponent\n{\n        /// <summary>\n        /// Gets the amount of resource that can be currently harvested\n        /// </summary>\n        public double GetCurrentHarvestValue(EntityPersistentData persistData){\n                double timeElapsed = (ServiceLocator.time.projectedServerTimeInMilliseconds/1000 - persistData.Harvest.LastHarvestTime);\n                return Math.Min(this.HarvestCapacity, Math.Floor(this.HarvestRate * timeElapsed));\n        }\n}\nThere's a fair bit of other stuff thats useful to actually paint on the data classes as well - namely Transient or Cached variables and convenience methods\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/google/flatbuffers/issues/3507#issuecomment-172961134\n. One question I have here is whether you actually need the benefits of FlatBuffers in this context. Eg, would you prefer to deserialize from a buffer into a pure POCO that unity can use and then discard the buffer?\nOn Tue, Jan 19, 2016 at 11:34 AM -0800, \"Jason McGuirk\" notifications@github.com wrote:\nSure thing - I went ahead and just swapped \"SEALED\" for partial for our own purposes, but seems like it would be generally useful - and Protobuf-net spits out partial classes fwiw.\nThe primary use case is when you want your data classes to also be able to also have business logic attached to them.\nFor instance, consider a game where buildings and other things can be harvested. If you're using an entity/component framework, you might encapsulate configuration for such behavior inside a class called \"HarvestComponent\"\n```\n/// \n/// Harvest component, encapsulates a harvestable entity\n/// \npublic class HarvestComponent : BaseComponent\n{\n        /// \n        /// The resource harvested\n        /// \n        public string HarvestResource;\n    /// <summary>\n    /// The amount of the HarvestResource that can be stored\n    /// </summary>\n    public double HarvestCapacity;\n\n    /// <summary>\n    /// The minimum time after collection before a harvest can be attempted again\n    /// </summary>\n    public double MinHarvestTime;\n\n    /// <summary>\n    /// The rate of resource generation (resources per minute)\n    /// </summary>\n    public double HarvestRate;\n\n}\n```\nCool - now we have the configuration data. Where should we put our business logic in game for powering the harvest (for instance, asking how much of a particular resource can be harvested right now for a particular object?)\nThere's two options here - we either stick it on the configuration bean (rich data model) or expose a manager to own handling that data (service model).\nHere's an example of the former.\n```\n/// \n/// Harvest component, encapsulates a harvestable entity\n/// \npublic class HarvestComponent : BaseComponent\n{\n        /// \n        /// The resource harvested\n        /// \n        public string HarvestResource;\n    /// <summary>\n    /// The amount of the HarvestResource that can be stored\n    /// </summary>\n    public double HarvestCapacity;\n\n    /// <summary>\n    /// The minimum time after collection before a harvest can be attempted again\n    /// </summary>\n    public double MinHarvestTime;\n\n    /// <summary>\n    /// The rate of resource generation (resources per minute)\n    /// </summary>\n    public double HarvestRate;\n\n    /// <summary>\n    /// Gets the amount of resource that can be currently harvested\n    /// </summary>\n    public double GetCurrentHarvestValue(EntityPersistentData persistData){\n            double timeElapsed = (ServiceLocator.time.projectedServerTimeInMilliseconds/1000 - persistData.Harvest.LastHarvestTime);\n             return Math.Min(HarvestCapacity, Math.Floor(HarvestRate * timeElapsed));\n    }\n\n}\n```\nAnd here's an example of the latter (Service model)\n/// <summary>\n/// Service that manages business logic for harvesting\n/// </summary>\npublic class HarvestManager\n{\n        /// <summary>\n        /// Gets the amount of resource that can be currently harvested\n        /// </summary>\n        public static double GetCurrentHarvestValue(HarvestComponent, hc, EntityPersistentData persistData){\n                double timeElapsed = (ServiceLocator.time.projectedServerTimeInMilliseconds/1000 - persistData.Harvest.LastHarvestTime);\n                return Math.Min(hc.HarvestCapacity, Math.Floor(hc.HarvestRate * timeElapsed));\n        }\n}\nThis is by and large a stylistic choice (folks from a primarily java background will tend to favor the service model in my experience), but making the class partial enables both styles - like so\nSchema:\ntable HarvestComponent{\n        HarvestResource:string;\n        HarvestCapacity:float;\n        MinHarvestTime:float;\n        HarvestRate:float;\n}\nPartial:\n/// <summary>\n/// Extension of HarvestComponent data bean that encapsulates biz logic and other convenience methods\n/// </summary>\npublic partial class HarvestComponent\n{\n        /// <summary>\n        /// Gets the amount of resource that can be currently harvested\n        /// </summary>\n        public double GetCurrentHarvestValue(EntityPersistentData persistData){\n                double timeElapsed = (ServiceLocator.time.projectedServerTimeInMilliseconds/1000 - persistData.Harvest.LastHarvestTime);\n                return Math.Min(this.HarvestCapacity, Math.Floor(this.HarvestRate * timeElapsed));\n        }\n}\nThere's a fair bit of other stuff thats useful to actually paint on the data classes as well - namely Transient or Cached variables and convenience methods\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/google/flatbuffers/issues/3507#issuecomment-172961134\n. One issue is that our accessors are meant to transition to being structs, does that affect this at all?\nOff the top of my head, I don't think so - you can still have partial struct declarations in C#.\nSince this might differ per table, why not use FlatBuffers metadata system to mark individual tables as partial, and have the code generator check that?\nThat is an option - it does feel a little strange putting language-specific attributes into the schema though. If you're ok with that, we could go with that.\n. (I didn't realise commits in my branches ended up referenced on the actual issues here!)\nI've created a simple partial switch for all classes in my branch (https://github.com/evolutional/flatbuffers/commit/fc1958b11ea749f4ac65d89e14f7348f87ee723e).\nI can happily adapt to use the attribute you mentioned.\n. @gwvo - I agree, having a per-table/struct option is much more powerful and useful.\nI pushed a commit here (https://github.com/evolutional/flatbuffers/commit/dbf670237125947bf50457786a79ec95b1a3605e). If it's ok with you guys, I'll send a PR.\n. It might we worth taking a cross-section of the keywords from each of the major languages, else we may end up with a schema that's fine in Go but not C#, for example.\n. Yes. I pulled the latest from master and my flatc project failed to compile without that change. I wasn't too far behind head at the time.\n. It makes me wonder if we should consider adopting this pattern for all languages, that way the Java/Go/Net and so on will have their ports in a subtree of the project, which would make dealing with it a little simpler.\n. Understood. \nFrom a .NET point of view, it would be a case of relocating the code in /tests/FlatBuffers.Test and /tests/MyGame/**/*.cs to the /net folder to live alongside the existing /net/FlatBuffers/ folder. This is a very common pattern in existing .NET library projects (the src/ and tests/ setup) and is essential for building the test project under Net Core. \nThe ideal distribution method for .NET would be to pack and publish a NuGet package from the main net/FlatBuffers/ folder. This wouldn't change in either setup, it's obviously more geared up for it in the project.json world.\nI'll keep trying to see if there's a way to make it work under the current structure, but from what I've seen so far it's not likely to work.\nOf course, we could also say that NET Core isn't a current or desired target, in which case this question can be closed without further consideration.\n. I'm trying to work around it - it's a known issue (https://github.com/dotnet/cli/issues/966). Looks like the version of NET Core I'm on (RC2) is still a touch too early. I'll close this and come back after it's moved past the RC stage. It's been a worthwhile exercise though.\nAs for splitting up the docs - good idea. I'm also playing around with some code locally to split the C#/Java generators into a 'general' base class & 2 specializations.\n. I started looking at splitting the Java and C# generators and using a more baseclass-centric approach. I've not pushed it anywhere as it's all local right now.\nI had a base IDLGeneratorBase which had a public Generate method that took the various arguments that make up the context. This baseclass then declared virtual methods for common actions like WriteStruct and WriteEnum. This would probably also extend out to WriteStructBuilder too, when we separate out builders and accessors.\nI did experiment with having a layer between this base and the actual Java/C# to cover the common cases, because there was a lot of similar code in there - but for early versions we could easily trash that and duplicate where we need - avoiding the overabstraction and complication.\nI actually abstracted the concept of TextWriter out to cover things like tabs vs spaces/indentation sizes, line endings - so I could so stuff like writer.IncreaseIndent(); writer.WriteLine(\"stuff\");.\n. I'm in the middle of a big move at the moment, so I might be a bit slow or unresponsive. I'll see about pushing up my prototype branch this weekend; but I like the idea of working with @Lakedaemon on this\n. Me either :)\n. Ship it!\n. Sorry, I only just saw this - my notification emails were hitting the spam box. I see no issue to make them partial.. I think an official NuGet package would be a great idea. I'm a C#/C++ dev by trade and consume 99% of my dependencies as binaries from NuGet.\nIf we were to publish an official NuGet package, I'd suggest we create a solution with various .NET projects in it. These projects would be set up with the various defines & safe/unsafe code builds - we'd likely publish these as separate packages, allowing people to select & consume the one they need.\nI would publish the NuGet packages at the point of creating an official FlatBuffers release - @aardappel I'm not sure how you prepare the releases or what steps you go through, but it'd be a good place to publish NuGet packages.. @aardappel you might wanna check with the Protobuf team on how they handle their NuGet packages on there.. Not sure I agree with a Debug.Assert, as that would crash production code. Perhaps an exception would be better?. I started it and got a good way into it, will have to check up on where I was and see what is needed to complete\n\nFrom: Wouter van Oortmerssen notifications@github.com\nSent: Wednesday, June 6, 2018 12:07:33 AM\nTo: google/flatbuffers\nCc: Oli Wilkinson; Mention\nSubject: Re: [google/flatbuffers] Support --gen-object-api for C#/Java (#4769)\nI agree that be nice to have.. last I heard @evolutionalhttps://github.com/evolutional mentioned he was interested in working on it, but haven't heard anything recently.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/google/flatbuffers/issues/4769#issuecomment-394888452, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ACJL7jdjKF0ij75MGlEtDhHm2c4HS-keks5t5w81gaJpZM4Ubpoe.\n. Yes; I considered this but did it this way to conform more to the C# way of doing it. We would have to provide different names for the ones where we want to populate an object, any suggestions?\ninitPos(obj) ?\n. pos() for the one which creates a new T() and getPos(obj), like the C# convention?\n. I debated not adding it - but went for it anyway. Can remove if you like?\n. Will do.\n. Yeah; agreed.\n. I should downgrade this so that it works correctly in VS2012\n. Just ran the tests with just the method body optimized out; getting roughly the same timings as with the individual calls defined out (there's 0.02us or so in it). It's probably best to define out the body only for the sake of maintenance. Thoughts?\n. If we're changing this, we should consider changing the code to be more C# formatting compliant - eg: public fields are in PascalCase. The reason is that it will trip a number of static/style analyzers.\n. Same. Public methods such as this really should conform to the C# style guide (Offset() or similar).\n. return flatbuffers_parser_parse(m_parser, source) == 0 ? true : false; would be more concise\n. You never check the return value here\n. Nor here\n. We should consider try/catch/finally exception handling in case any of this fails, that way we don't leave that string buffer dangling.\n. The size param doesn't need to be a ref IntPtr, it should be sufficient to use ref int size (or long, for 64-bit)\n. Why is size both an out param and a return type?\n. I wonder if we need a known attribute table here, to prevent copy paste of attribute names. It would apply to all known attributes\n. This is not idomatic C#, it really should be.\nvoid Init(int i, Bytebuffer bb)\n. Idiomatic C# would call this IAccessor\n. As these are public, we may want to adopt a more idiomatic C# convention which is to use PascalCase names for members.\n. Same, re: PascalCase naming\n. Again, public methods named with prefix warts is frowned upon in C# \n. Issue with struct implementing an interface is that we may start running into boxing (and thus performance) issues.\nhttps://blogs.msdn.microsoft.com/abhinaba/2005/10/05/c-structs-and-interface/\n. I think I'd want to take this change and run through the benchmarking I set up a while back \nNote to self: I should really get this cleaned up into a PR for everyone to use\n. Did you mean to change the java output?\n. The name Accessor (or IAccessor) doesn't quite sit right with me, as it's only really providing an Initialisation method. \n. Perhaps take the configuration as a param to the bat file so we can use debug builds?\n. We also now have NET Core 1.0 to consider\n. Not sure about your use of interfaces, haven't looked into it yet - but I did get chance to look at the benchmarking.\nYour change does have an impact on performance (code pushed to here - https://github.com/evolutional/flatbuffers/tree/cs-benchmark)\nFrom what I can tell, the 'encode' test is slightly slower and the 'decode' test is much faster. Both of these have a lower std deviation, which seems to indicate that performance is more consistent - at least in the stuff the benchmark is doing.\nBefore:\nName                                  Mean   StdD  Unit\nEncode                               3.853 1.190 us/op\nDecode                               0.416 1.701 us/op\nTraverse                             2.428 0.389 us/op\nAfter:\nName                                  Mean   StdD  Unit\nEncode                               4.006  0.952 us/op\nDecode                               0.090  0.139 us/op\nTraverse                             2.224  0.426 us/op\nThis benchmark is fairly old now, ported from the Java one - it'd be a good exercise to bring them all in line with your C++ one to get a true comparison.\nIt does seem that using structs here results in faster decode performance; the consistency boost is very likely from the reduced garbage collection overhead.\nNote that it was a breaking change, however - I had to modify any code which accessed a struct to get the .Value member.\nEDIT: I totally missed your comment about the break in the api - sorry!\n. As for Net Core, we should very definitely start looking at this - it's at 1.0 RTM now and is the cross platform version of .NET (Mac, Linux, Windows). It's a whole new compiler & runtime with a reduced library set - which you're right, we don't use - however there may be differences in performance. There's an AOT compiler (not sure if that's standard or optional), which will have a bearing (likely, positive!).\nI've not used it much at all, so really can't say much more than that, unfortunately. \n. Definitely consider calling it IAccessor, to be idiomatic - it'd hurt my eyes otherwise :)\nProbably something like IFlatbufferObject if it's the base for all tables/structs.\n. (Naming things is hard!)\n. I've just realised that those numbers were pulled from a Debug build\n. Release build:\nBefore change:\nName                                  Mean   StdD  Unit\nEncode                               2.104  0.872 us/op\nDecode                               0.048  0.071 us/op\nTraverse                             1.008  0.172 us/op\nWith change:\nName                                  Mean   StdD  Unit\nEncode                               1.736  0.448 us/op\nDecode                               0.040  0.049 us/op\nTraverse                             1.040  0.165 us/op\nMade a bit of a difference.\n. This is more in line with what you'd expect. Traverse largely the same (as it re-uses most of the objects), Encode is improved and Decode is about the same (as you said, 'Decode' does nothing of note).\n. Hmm, you're right. Just looked over the 'Encode' code and it should all be in-place, low alloc stuff (string creation is the only one). I'll run the benchmark on another machine.\n. ",
    "DavidFeng": "@cmzx3444 \nI am writing a flatbuffer library for Lua right now, it can read flatbuffer messge for now,\nplease see: https://github.com/DavidFeng/lua-flatbuffers\n. @gwvo ok, I will improve it to meet the quality to merge into the main project\n. ",
    "vgough": "Thanks!\n. This may be C++11 specific -- my project uses C++11, which does not seem to be default for XCode yet.\nC++11 mode can be enabled via -std=c++11 (or -std=gnu++11).\nUsing XCode 5.1:\nApple LLVM version 5.1 (clang-503.0.40) (based on LLVM 3.4svn)\nTarget: x86_64-apple-darwin13.3.0\n. ",
    "AndrewJDR": "\nPlease provide an option to parse binary and product text, so that it is easy to decode binary blobs produced in flatbuffer format.\n\n@gwvo This was labeled 'fixed'. What commandline option can be used to do this? I could not find it in the documentation. Thanks!. Got it figured out. For others' reference, you need to use the --raw-binary flag and need to take care to use -- correctly.\nE.g.:\n./flatc --raw-binary -t myschema.fbs -- my_flatbuffer.bin. ",
    "ghost": "Universe is being good to me today. Will be glad to use and improve.\n. i will be trying this out !\n. I don't see the problem. A serialised flat buffer is still generated from the same object.\nLike pojo etc.\nMaybe I am missing something ...\n. URL about glTF\nhttps://github.com/KhronosGroup/glTF\n. I wonder if the rust implementation can be compiled to JavaScript via emscripten.\n. I was looking around. Did this ever get implemented? I am new to Flat Buffers and this is one of the reasons I liked protobufs so much.. Thanks a lot. It got solved.\nI did some tests in python and found out that if the data is relatively small, say, several bytes, the pack and unpacking speed compared to json is slower. Is this expected?\nOf course if the data is large, take 1MB as an example, although the packing is slower than json but the unpacking is tremendously faster than json.\n. I'd love to do that. It is my honour to be able to contribute to this project. Do I have permission to push my branch to this remote in order to start a PR?\nRegarding the test case, I'd like to add it into TestExceptions class in tests/py_test.py. Does it seem OK?\n. https://github.com/google/flatbuffers/pull/286\nPlease review\n. Hi! Any chance to have this merged somehow? Would help to use gRPC+flatbuffers in Go. Thanks!\n. This improvement looks promising. The lack of unicode handling prevents me from daily use of python. But I am not sure we can make it better if we can port protobuf's escape routine which looks like more practical.\nOr maybe I need switch to C++ implementation and use something like ctypes or boost.python/pybind11 to bridge C++ code and python code for production.\n. @gwvo I have checked with the current tip b974e95ce45d1a8e0e2053a32c0569c3d9b41200 and everything works as expected. The unit test was adjusted to reflect the correct way of serializing an array of tables. I am closing this issue.\nWith this said, IMO it is a good idea to tag this development officially (e.g. v1.2.1) to help folks like me who go for the latest official release and not for the tip in master branch.\n. Is this for \"optional\" value?\nI don't need \"forward and backwards compatibility\". \nSo I try to use \"struct\", but it can not contains other struct vector.\nAnd \"required\" attribute does not affect to the size.\n. It was so omitted. Sorry for that.\nI thought when 'orders' (in NotiSync) is empty, it may spend maximum 4bytes.\nSo, short(2) + byte(1) + short(2) + emptyorder(4) = 9bytes\nAnd the benchmark document say \"Wire format size\" will increase less then 30%.\n(http://google.github.io/flatbuffers/flatbuffers_benchmarks.html)\n344(flatbuffers binary) / 312(raw struct)\nSo, I thought that it will be maxium 13 bytes.\nAnd another tables.\n```\ntable FBUnitOrder {\n    from:short;\n    target:short;\n    direction:FBVec2;\n    position:FBVec2;\n    state:byte;\n    steps:[FBVec2];\n    to:FBVec2;\n}\nstruct FBVec2 {\n    x:short;\n    z:short;\n}\n```\n. Also is this permitted if one of overloaded type name is vector, something like this:\nTable Yy{\n     name:string Re [SomeOtherVector]\n}\n?\n. Ah sorry got it now thanks, I just stumbled upon raw flatbuffer grammar, perhaps you could write that to something more readable human friendly PEG grammar :)\n. +1 on this\n. ok. \"length-prefixed\" is best now\nthanks for the answer!\n. wow! great!\n. Well, in fact, I don't know what PR exactly means in your sentence. . OK. did you see flatc --scoped-enums flag when generating source file? https://github.com/google/flatbuffers/blob/master/src/flatc.cpp. closed as. Turn out I was missing ! in if check part :). ",
    "mendsley": "Updated and rebased as a single commit\n. ",
    "eile": "ping - addressed both comments.\n. > It would seem to me that whatever compiler you're working with is not strictly C++03, since FlatBuffers makes liberal use of \"auto\", which is not a problem?\nThe problem is '-stdlib=libc++' in clang, since this breaks compatibility with projects linked against the old stdlib since they use different memory layouts.\n. > Also, for future PRs, it is much easier for me to work with if its a single commit rather than 12. It also gives a more readable history on master.\nNo problem: once we're done I (or you) can squash into a single commit before merging into master.\n. > so if supporting C++03 is a thing, I'd rather do that by rewriting that code to not rely on std::conditional\nSure - although I would have to look into how this is to be done. So for my changes stayed on the surface without actually needing to understand the code.\n. +1 for this. By coincidence I created the same in the train yesterday....\n. @biddisco: The discussion on #57 was about getting rid of these three functions by reimplementing the functionality differently. I won't get to it before November.\n. In any case, as @biddisco outlined above, the correct way to find a CMake project is via config mode, not through a FindFoo.cmake finder. The latter is legacy and should only be used for non-cmake projects.\n. Don't try externalproject_add. It's horrible to use compared to add_subdirectory.\nrebuilding: Use ccache.\nIn general, flatbuffers should install its export config, as this PR did.\n. Ok, will look into it.\n. #108 is the first one.\n. ",
    "dyu": "Sorry, false alarm. I edited test/monsterdata_test.golden and quoted the fields, which gave errors when running flattests (I assumed it was the parser that balked).\nTurns out, I had to add this line in ParseAndGenerateTextTest for the equality test:\nopts.strict_json = true;\n. Thanks!\n. Where's the benchmark code?\nOn Wed, Jan 28, 2015 at 11:49 AM, Florian Enner notifications@github.com\nwrote:\n\nI've implemented a first version. Note that\n1) I currently don't have any information about GC triggers, so the\nnumbers (especially for encoding/decoding protobufs) may be worse in real\nworld use.\n2) FlatBuffers-Java still requires some optimizations\n3) The test-data may not reflect your use case!\nSo far the numbers (sec for 1M operations) look like the following:\nOS: Windows 8.1\nJDK: Oracle JDK 1.8.0_20-b26\nCPU: Intel i5-3427U @ 1.80 Ghz\nDecode + Traverse\n(FlatBuf direct) 0 / 0.639 = 0.639\n(FlatBuf heap) 0 / 0.732 = 0.732\n(ProtoBuf) 6.903 / 0.037 = 6.94\nEncode\n(FlatBuf direct) 1.137\n(FlatBuf heap) 1.576\n(ProtoBuf) 1.652\nPreliminary Result\nThe data looks quite a bit different than the C++ implementations.\nTraversing a ByteBuffer is very expensive compared to traversing objects\ndirectly. However, the deserialization time still ruins the overall\nperformance of Protobuf. As long as you don't fully traverse each message\nmore than 10 times, FlatBuffers will be faster (for similar use cases).\nSerializing data via flatbuffers is faster as well, but not by a huge\nmargin.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/55#issuecomment-71776794.\n\n\nWhen the cat is away, the mouse is alone.\n- David Yu\n. Btw, the assertion failure will trigger by editing samples/monsterdata.json to this:\n{\n  pos: {\n    x: 1,\n    y: 2,\n    z: 3\n  },\n  hp: 80,\n  hp: 80,\n  name: \"MyMonster\"\n}\nThe error on this one would be \"struct field appearing out of order: y\" (struct failing first/early)\n{\n  pos: {\n    x: 1,\n    y: 2,\n    y: 2,\n    z: 3\n  },\n  hp: 80,\n  hp: 80,\n  name: \"MyMonster\"\n}\n. \"For your current use case, can't you temporarily set the end byte to 0, and set it back after you called Parse()?\"\nI guess so. (Didn't think of that).  Guess I didn't want to write something not owned by a malloc'ed buffer.\nI'm no c++ expert, so I'm gonna trust you that its perfectly safe to do :-)\n. Yes. Its now updated to using 10UL instead of cast.\n. Yea I initially thought about making the generic function call the non-generic one.\nUpdated.\n. Duplicate of https://github.com/google/flatbuffers/pull/113\n. Usecase:\njson rpc with flatbuffers' json parser/generator and data stored in a datastore in binary form (a.k.a flatbuffer)\nI could always create a parser per thread, but that also means parsing the static schema for each thread repeatedly.\nMy suggestion was to load the schema once and copy the internal datastructures (already loaded with the schema) of that parser per thread.\nIts similar to constructing a parser with AST (datastructure) vs a text file (schema)\n. \"Have you measure a significant startup cost?\"\nNo not yet.  But I'll definitely try hacking the parser to have a construct from an existing parser.\n\"If you're handling JSON RPCs, then why not use the same schema for parsing\nand generation?\"\nThat's exactly how I'm using flatbuffer's parser.\nAnd no, I'm not reinitializing it everytime.  I'm using it like you said.\n. ",
    "jacobwpeng": "Thanks, I do forget that deprecated field...\n. ",
    "harningt": "Another nice option would be to expose vectors as primitive buffers, ex: ByteBuffer, LongBuffer, IntBuffer,etc... CharBuffer would be nice, but I'm pretty sure that's only for storing Java's 2-byte chars.\nThis would result in only the construction of a new buffer object pointing to the original ByteBuffer as a backing store... Would also provide a useful way to fill an in-progress flatbuffer.\nIf my data benchmarks vs protobuf storage where I want ByteBuffer access (instead of the many copies of byte arrays) reveals that flatbuffers work out better, I may provide a patch for the Java generator...\n. ",
    "zarianw": "Thanks. I was able to make progress after the compilation went through.\n. The CLA page keeps failing with the error:\nA server error occurred, please try your request again.\nNot sure if it is intermittent error, will try again in the morning to sign it.. I signed it!. @gwvo It seems the build machine has an old version of cmake which does not recognize target_compile_definitions which was added in cmake v3.0.2. Should I upgrade the travis yml file to install a newer cmake version. target_compile_definitions offers the best and cleanest way to handle the FLATBUFFERS_TRACK_VERIFIER_BUFFER_SIZE macro issue that we talked about earlier. Or we can use the dist: trusty (Ubuntu 14) which already has the newer cmake pre-installed. BTW I also tried the fix you suggested to make the field always available but that caused the following compile time error\n/Users/travis/build/google/flatbuffers/include/flatbuffers/flatbuffers.h:1456:26: error: private field 'upper_bound_' is not used [-Werror,-Wunused-private-field]. If I remove the macros from everywhere (struct definition and code) then it would not matter. But I thought you meant to remove it just from the structure definition. I tried that and build started failing with error: \n/Users/travis/build/google/flatbuffers/include/flatbuffers/flatbuffers.h:1456:26: error: private field 'upper_bound_' is not used [-Werror,-Wunused-private-field]\nThe failed build link is:\nhttps://travis-ci.org/google/flatbuffers/jobs/183507818\nDid you mean to remove it from everywhere?. I have made the changes you requested. IMHO the suggested fix is fragile and will break in some other case. For example I still had to move test.cpp to be the first file in cmake to make tests work(on windows atleast)  which is weird. I understand why you don't want to upgrade the cmake version but in this case upgrading cmake version and using target_compile_definitions is probably the right thing to do. But its your call. The build is still in progress lets see if it is successful this time.. Well I guess I have a solution for our problem. I was able to use COMPILE_DEFINITIONS (https://cmake.org/Wiki/CMake_2.6_Notes) which is available in cmake 2.6 and above. The fix seems to be working and the build has passed as well. I also took care of your other comment. Let me know if there is anything else.. This macro has caused me a lot of grief :) and this change is actually not the right one now that I have looked into it a bit more. The core issue is that test.cpp and reflection.cpp both include flatbuffers.h (either directly or transitively) and test.cpp is defining the macro and reflection.cpp is not. So the end result is that both translation units see a different version of the header as they are getting compiled. The latest issue I saw while incorporating code review changes was stack corruption. The reason was that class Verifier is of different size in test.cpp and reflection.cpp. If I do sizeof(Verifier) in test.cpp its 28 bytes (32bit build) and 24 bytes in reflection.cpp. The reason is again this macro if I look inside the Verifier class then I see the same macro\n```\nifdef FLATBUFFERS_TRACK_VERIFIER_BUFFER_SIZE\nmutable const uint8_t *upper_bound_;\nendif\n```\nAnd this causes the difference. I can start looking into how we can propagate this macro to all translation units or maybe if you have some suggestion then please share that. I think cmake has something for this but I would have to look again.. Will do. Will do. Will do once the above macro issue is resolved because using Verifier inside reflection.cpp is causing stack corruption.. Will do. The switch is on the vector element type and this verify function overload is internally calling VerifyVector which is doing exactly what you suggested. Am I missing something?. Because of the nature of the check we would always need an if/else either inside or outside the function. Currently VerifyStruct and VerifyVectorOfStructs can handle both required and optional fields. If I make this change then they would only be able to handle required fields. And the place where I would be iterating the fields would look like\n```\nauto offset = table->GetOptionalFieldOffset(fieldDef->offset());\nif (!offset && fieldDef->required()) {\n  return false;\n}\nif (!VerifyStruct(v, table, fieldDef->offset(), childObj)) {\n  return false;\n}\n```\nWhich is not very pretty either. If you really want it I can make this change but my preference would be to encapsulate this check inside the function like it is right now.. This will no longer be required as we will create verifier internally inside reflection.cpp Verify function. But even there I have to create verifier like above because inorder to create it inline the VerifyObject function needs to take in verifier as const& but later I call function table->VerifyTableStart which expects a non const& to verifier and I don't want to use const cast there to make things work.. @gwvo can you confirm if this is the right way to get the type of the union. I am assuming that a union field always has its type before itself in the vtable. Is this a right assumption?. Will do. Actually it is, can you try it on your side. I could not get the tests to pass without this (on windows with visual studio atleast). The Release build tests were failing.. The is_struct field is defined in table Object. We can probably get the index from field.type()->index() but the objects vector is defined in the schema table which we don't have in this function. Is there any other way to test if the passed in field is a struct?. ",
    "apurvam": "Yep I realized it after typing. I just created a pull request with the patch for the first error.\n. I submitted pull request https://github.com/google/flatbuffers/pull/53  which fixes both errors.\n. ",
    "7a6": "This fixes one error, but there are also errors in the sample code break the build on linux.\n. ",
    "greenrobot": "Ideally, you could specify the root object and flatbuffers would do the rest.\nBut to be more concrete...\nTable A has String B. So we need to create B, store its offset, start A and add the string offset to A. This is always the same pattern and it should be easy to automate this step.\nStoring vectors of tables are worse, because you need to maintain a list of offsets before you can add it  the vector to another table. So, we need to allocate a list of offsets somewhere anyway.\n. @aardappel Can you point me to those tests? So you are verifying you get the data out you put in?\nI saw this happening using clang++-3.8 on a 64 bit Linux machine.\nWill try to collect some more data.. That's about what my code is doing:\nflatbuffers::Offset<Thing> singleOffset = thing->makeFlat(fbb);\nauto vectorOffset = fbb.CreateVector<flatbuffers::Offset<Thing>>(&singleOffset, 1);\nHasThingsBuilder builder(fbb); // the generated builder\nbuilder.add_things(vectorOffset);\n\nIs there something odd?. Also checked running your test, passes fine.. ",
    "ennerf": "I was planning on writing some internal benchmarks comparing the Java implementations for FlatBuffers and Protobuf within the next few weeks. \nIf you give me access to the schema and maybe the source files you've used in http://google.github.io/flatbuffers/md__benchmarks.html, I may be able to provide the (non-Android) Java numbers.\n. I've implemented a first version. Note that,\n1) I currently don't have any information about GC triggers, so the numbers (especially for encoding/decoding protobufs) may be worse in real world use.\n2) FlatBuffers-Java still needs some optimization\n3) The test-data may not reflect your use case (!!!)\nedit: 4) The encode/decode numbers look a bit better than I'd expect. There may be some optimizations that I'm not properly accounting for.\nSo far the numbers (sec for 1M operations) look like the following:\nOS: Windows 8.1\nJDK: Oracle JDK 1.8.0_20-b26\nCPU: Intel i5-3427U @ 1.80 Ghz\nDecode + Traverse\n(FlatBuf direct) 0 + 0.639 = 0.639 us\n(FlatBuf heap) 0 + 0.732 = 0.732 us\n(ProtoBuf-Java)  6.903 + 0.037  = 6.94 us\n(ProtoBuf-JavaNano) 1.101 + 0.024 = 1.125 us\nEncode\n(FlatBuf direct) 1.137 us\n(FlatBuf heap)  1.576 us\n(ProtoBuf-Java)  1.652 us\n(ProtoBuf-JavaNano) 1.379 us\nPreliminary Result\nTraversing a ByteBuffer is very expensive compared to traversing objects directly. However, the deserialization time still ruins the overall performance of Protobuf.\n[Edit 2015-12-10]\nadded units and results for protobuf-javanano\n. benchmark\n. Btw. there may be some optimizations that I'm not accounting for. The protobuf encode/decode numbers look better than I'd expect. Let me know in case you go through the code and find something wrong with it.\n. I've based the Java benchmark on the original C++ benchmark. I unfortunately don't know whether or not the Java benchmark would still run, but please feel free to use the code in any way you want.\n. Here is the corresponding fbs https://gist.github.com/ennerf/cb7999bd30973acaf794\n. I've uploaded some numbers for my current Desktop machine in case you'd like more data for comparison: https://gist.github.com/ennerf/d108294623a8c52f984f\n. The JMH score depends on the settings. In this case, it does represent the average time.\n@BenchmarkMode(Mode.AverageTime)\n. Ok, I'll get the latest changes in and create another pull request when I'm done.\nI placed the check to force_defaults first because I thought it might aid branch prediction. I just wrote a benchmark and found that on my machine \"force_defaults || x !=d\" was consistently faster than \"x !=d || force_defaults\", no matter whether it's set to true or false.\nThe code and results are in https://gist.github.com/ennerf/e4078e0aa9c41fa3de49\n. I was thinking the same thing, but I think those two options need to be separate. However, --has-accessors may imply --force-defaults.\nI'd probably vote for making it an option in the schema file instead of a commandline argument, similar to what they did in protobuf-java-nano. https://github.com/google/protobuf/tree/master/javanano at \"Nano Generator Options\".\nSince it doesn't seem like it's a high priority item, I'll keep my changes internal for now. If other people start requesting this functionality, I'll revisit creating a proper patch.\n. Good find @pjulien \nI'd vote for using StringBuilder in combination with reading characters byte by byte, i.e., something like\nStringBuilder string = new StringBuilder(length);\nfor(int i=0; i<length; i++){\n    char val = getUtf8CharAt(vector, i);\n    string.append(val);\n}\nreturn string.toString();\n. Yes. I think the first step should be to just implement a byte-by-byte read into the byte[] array though. Implementing a custom getUtf8CharAt() function would be quite a bit of work, and there is other stuff that's probably higher on the priority list.\n. @pjulien HeapByteBuffer#get and DirectByteBuffer#get both overwrite that function and use System#arrayCopy and Unsafe#copyMemory internally. I've looked at some of my recent benchmarks and byte-by-byte copy is about 10-25x slower than the alternative.\nAssuming that the String is only a small part of an entire message, the performance of duplicate (1. allocate buffer, 2. memcopy of full message, 3. memcopy of string) is going to be much worse than iterating a for loop.\nedit: duplicate() creates another wrapper object onto the same buffer, and does not actually copy the data.\n@gwvo I didn't mean to replace the String with CharSequence. I was just proposing of doing the conversion without any additional (unnecessary) memory allocations. I'd expect that the performance would actually improve over the current implementation.\n. Oh sorry, my mistake. You are correct.\n. @pdca compiled flatc for windows\n. It should already do that. Can you let me know which line you don't like?\n. ",
    "LouisCAD": "Would be nice to have a comparison with json parsed/serialized by LoganSquare too (this library doesn't use reflection, so a benchmark would be fair)\n. Why is this issue closed? Returning a CharSequence pointing directly to a flatbuffers would be great as Android supports Charsequence in many places!\n. I just saw Java uses utf-16 in memory, which is different from utf-8. I understand why this issue is closed now. Thanks @Lakedaemon !\n. This new code would be compatible with old one on Android, but would not compile without IntDefs which are specific to Android. That's why I suggested this code style to be an option to generate Android optimized java code (--android-java). And yes, names[] is gone because It's not really useful unless you want to display the enum names to users IMHO (from my use in java on Android). That's why I suggested names[] to be generated only if an option such as --gen-enum-names is specified on flatc.\n. I just forgot to tell that java interfaces (with or without the @ that makes them an annotation) don't support methods, so the names()would have to be ditched, but that would not be a major problem as all fields, including the names array are public in interfaces, which would then allow access to them (but without having to call a method, which is more expensive than just accessing the array on Android anyway)\n. @gwvo But if it's an option, people who would use the --android option would know what to expect and would see how the generated code is if using the names(\u2026)method.\nHere's how it woudl look like with --android option:\n``` java\npackage xyz.louiscad.common.model;\nimport android.support.annotation.IntDef;\nimport java.lang.annotation.Documented;\nimport java.lang.annotation.Retention;\nimport static java.lang.annotation.RetentionPolicy.SOURCE;\n@Documented\n@Retention(SOURCE)\n@IntDef({ColorType.ARGB, ColorType.MATERIAL_COLOR_KEY})\npublic @interface ColorType {\n    int ARGB = 0;\n    int MATERIAL_COLOR_KEY = 1;\nString[] names = { \"ARGB\", \"MD_COLOR_KEY\", };\n\n}\n```\nI'd then just need a ColorType.names[ColorType.ARGB]call to get the name. So basically just replace parenthesis with brackets.\n. I personally think it's not a problem to generate java code for non Android runtime, and again for Android runtime. Non Android java runtimes will probably be servers, and it's as if the server was not running java, you generate for the language that the runtime understand. Android is quite different from other JVMs.\nI mean, users should not run some code designed for Android on non Android runtimes, and vice-versa, hence the suggested --android option to exclude if the code is not going to run on Android.\n. To do this, you would just have to omit the --android option. I really don't understand your concern about providing the ability to optimize the code for Android, a mobile platform, which is very different from other java environments. Please, if there's a real reason why you wouldn't want Flatbuffers to be optimized for Android, tell us.\nAgain a comparison between code vs platform: iOS libraries are developed in Objective-C and/or Swift, and nobody complains they don't work on other Objective-C systems such as macOS, watchOS or Objective-C compatible Linux distributions, so I don't see why should be different for Android.\n. And of course, isn't the flatbuffers .fbs file that is supposed to be platform agnostic? The code can be generated at any time from command line, so ne need to on top of this, requiring the code to be platform agnostic too.\n. So I can consider this feature request accepted and start looking how I can implement it?\n. So a decision must be taken. Either add the line of code I suggested so creating an empty flatbuffers is easy as ABC, or add a private constructor to each generated class so nobody can try to create a broken flatbuffers.\nI would personally prefer the first option to don't add complexity where not needed.\nHow to create an empty Flatbuffers to get default values:\nFirst solution:\njava\nmTheme = new Theme(); // Requires only one extra line in Table#__offset() to return 0 if bb is null.\nSecond solution:\njava\nFlatBufferBuilder themeBuilder = new FlatBufferBuilder(0);\nTheme.startTheme(themeBuilder);\nfinal int o = Theme.endTheme(themeBuilder);\nTheme.finishThemeBuffer(themeBuilder, o);\nmTheme = Theme.getRootAsTheme(themeBuilder.dataBuffer());\nI guess it's pretty obvious which one requires less boilerplate from developers\u2026\n. I don't use kotlin yet, but I don't see the benefit of flatbuffers default values if I have to write a boilerplate friendly class myself, which anyway, can't read flatbuffers default values as it will always get an NPE. I mean, if I create other tables, I have to rewrite each default values for each table in a wrapper class? I don't get how you find it more practical that the ability to instantiate a working empty flatbuffers table with the new keyword\u2026\nPlease, tell me, if I want to use it in java, I have to write another class manually and copy paste all the default values and created delegates for the class under the wrapper, or stop using java? Don't you think it deviates from flatbuffers philosophy which is closer to Don't Repeat Yourself and which aims to keep things simple?\nAnyway, I'm lucky that it's so easy to edit flatbuffers behavior as there's no read-only dependency, so I added the line I was talking about in the Table class which checks if bb is null, and it works flawlessly.\nI'm ok to do a PR if you think it's ok to add a null check in the __offset(\u2026) method.\n. @Lakedaemon Don't structs default to null when not set? If yes, there should be a way to return null if bb is null?\n. I'll try adding a struct to my flatbuffers to see how this could be addressed\n. @Lakedaemon Where did you see that structs are never null? From what I see by reading generated code, they can be, and the line I added is struct safe in this regard.\n. @Lakedaemon It made me check that it would work properly with structs, so thanks! Last thing to check is for unions and their type. Do you have an idea?\n. @Lakedaemon I don't think checking if a reference points to null can be considered as overhead. I mean, it's a very cheap operation, which maybe kotlin runtime overpasses \ud83d\ude09 . I'd benchmark without and without if I was afraid of a performance hit though.\nAnd about kotlin, this is only about java, so maybe if this problem is present in kotlin too, there'd be another way to address it, by hiding the FlatbuffersBuilder calling code in the constructor for example?\nBTW, glad to see it'd work on unions too and that it's absolutely safe for the whole flatbuffers grammar\n. @gwvo So can it be simplified for cases when you want to leave all default values untouched?\nWhat about something like the following that would generate all the boilerplate code needed to build a flatbuffer with just default values?\nCode that the programmer needs to write\njava\nmTheme = Theme.createDefaultTheme();\nmMonster = Monster.createDefaultMonster();\nGenerated boilerplate code\n``` java\npublic class Theme {\n    private Theme() {}\npublic static Theme createDefaultTheme() {\n    FlatBufferBuilder themeBuilder = new FlatBufferBuilder(0);\n    Theme.startTheme(themeBuilder);\n    final int o = Theme.endTheme(themeBuilder);\n    Theme.finishThemeBuffer(themeBuilder, o);\n    return Theme.getRootAsTheme(themeBuilder.dataBuffer());\n}\n\n...\n}\npublic class Monster {\n    private Monster() {}\npublic static Monster createDefaultMonster() {\n    FlatBufferBuilder monsterBuilder = new FlatBufferBuilder(0);\n    Monster.startMonster(monsterBuilder);\n    final int o = Monster.endMonster(monsterBuilder);\n    Monster.finishMonsterBuffer(monsterBuilder, o);\n    return Monster.getRootAsMonster(monsterBuilder.dataBuffer());\n}\n...\n\n}\n``\n. Mine is to edit theTable.java` to add a null check. I think it'll be hard for us to agree if you keep advocating for boilerplate with no visible reason, which seems to counter flatbuffers policy.\nBTW, we have no real way to know if some feature will be used, but adding it is pretty simple, and shouldn't negatively impact Flatbuffers users.\nAnd about use cases:\nAny app that uses Flatbuffers for some configuration having defaults that may be left untouched, or that may need to work with default values until a configuration is fetched from an async, potentially failing operation such as from network.\nThink IOT, wearables...\n. It seems that you were a little uncomfortable with the possible performance hit of adding a null check in the Table class (if (bb == null) return 0; // This line I added allows empty flatbuffers).\n1. We are talking about java. People who want brute performance will choose Go or C++, not java.\n2. I did a little research about this, and the \"performance hit\" that a null check adds can in no way be called overhead. In this stackoverflow answer, a guy did 2,000,000,000 null checks, and the worst he could get was less than 3ns for a null check (2.574).\n3. Considering java apps are probably not games, but more likely to be Android UI/background apps, or server apps, how could a 3ns impact performance in any way? Even if the null check is done multiple times, if the developer did this on the UI thread in and Android app, it would need to be done 5,333,333 times to cause a frame drop by itself. Of course in Android, heavy jobs are not done on the UI thread if the developer follows best practices, so I think we should give this a try.\n4. Finally, it would solve the problem of the default constructor which instead of creating a broken object, would create a default one, which would work.\nI'm doing the 1-line-of-code pull request, hope it'll be accepted\n. It's an average per loop, done with 2Billion loops. So multiply 3ns by\n2Billion\nOn Mon, Aug 22, 2016, 11:24 PM Olivier Binda notifications@github.com\nwrote:\n\nAlso, how can 2,000,000,000 null checks only take 3ns when a cpu only has\nabout 3,000,000,000 cycles in a second ?\nThose null checks in the test might have been ignored by the compiler and\nI wouldn't trust that benchmark...\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/3945#issuecomment-241555364,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGpvBXsVrd5RHA4hB-dRaQ58BoR85rBSks5qihOTgaJpZM4JM6B5\n.\n. \n",
    "biddisco": "FindFlatBuffers.cmake should not exist at all. This is not required when flatbuffers generates a config using cmake. I have added such a config generatio in PR #78 \n. The version here supports 3 modes of using the project which are documented in the cmake, but the commit message was not very clear and the text below is better\n+# Support 3 methods of using FlatBuffers\n+# find_package(FlatBuffers) - pointing to the INSTALL location\n+# find_package(FlatBuffers) - pointing to the BUILD directory\n+# add_subdirectory(FlatBuffers) - where the project using\n+# FlatBuffers can simply set FlatBuffers_DIR to the build subdirectoy\n+# The main difference is that the build dir versions need the source tree\n+# include locations instead of the install tree.\n. I have cherry picked libstdc++ support from #57, integrated it with this branch and will redo the pull request shortly with some other minor changes\n. I cherry picked (and squashed) about 4 commits from the bluebrain #57 libstdc++ PR and also added another commit for a new fix that was required to compile against latest master.\n. I found a small error when I compiled on another machine and fixed it by force pushing to corrected patch\n. That is a great shame. The patch provides alternatives to std::conditional, std::move and std::function which are only switched in when the user explicitly enables the FLATBUFFERS_USE_CXX03_STDLIB option. Without this the code is unaffected. We would like to use Flatbuffers on BlueGene/Cray and other machines where we cannot always get a c++11 compliant toolset. Are you sure you won't reconsider? Providing boost alternatives is far easier (for us) and far cleaner (for you) than making other more intrusive changes to the code.\n. I have removed the libstdc++ changes from this PR and rebased the patch onto the current master so it contains only cmake/install related fixes. \nThe install rules implemented follow the recommendations of http://www.cmake.org/cmake/help/git-master/manual/cmake-packages.7.html#\nThe master branch defines a function to wrap the *.fbs files and generate a header, this was also introduced in the #57 branch but placed in the install generated config file which allows downstream projects to use it. I did not remove the version from the master CMakelists but will do so if requested and place the function into a separate cmake file which can be included by the main project and also installed for downstream projects. I did however rename the function in the config version to match the main CMakeLists version.\n(It is not clear to me why the project requires an option for FLATBUFFERS_INSTALL. If you do not call \"make install\", then nothing is installed. Removing the install rules saves a fraction of a second at config time, but other than that does not seem very useful - is there another reason for it?)\n. ",
    "rodrigob": "On paper, the generated C++ should work out of the box with emscripten (to be tried).\n. Just had flatbuffers working via emscripten. Works like a charm.\nMoved from a custom serialization format (from dlib.net); the resulting file is smaller and it is parsing is much faster. Flatbuffers for the win !\nOverall I only had excellent experiences with emscripten.\nGet a zero dependencies project to compile with cmake. Then launch emcmake cmake . && make and tada!, you are good to go.\nSince flatbuffers has no run-time dependencies (only needs to include flatbuffer.h at compilation time), it is a perfect fit for the emscripten scenario.\n. On which branch should we follow changes on this issue ?\nhttps://github.com/shaxbee/flatbuffers/tree/python_support_v2 ?\nOut the box with Ubuntu 14.04 and gcc (Ubuntu 4.8.2-19ubuntu1) 4.8.2 I get simple errors like\nflatbuffers_python/src/idl_gen_python.cpp:500:32: error: \u2018ostream_iterator\u2019 is not a member of \u2018std\u2019\n. After fixing the ostream_iterator error (by including <iterator>), the generated python code is not accepted by the parser:\n```\nPython 2.7.6 (default, Mar 22 2014, 22:59:56) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport my_test_fbs.py\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"my_test_fbs.py\", line 53\n    _format = Struct(\"\n                     ^\nSyntaxError: EOL while scanning string literal\n```\n\n\n\nand\n```\nPython 3.4.0 (default, Apr 11 2014, 13:05:11) \n[GCC 4.8.2] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport my_test_fbs.py\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: source code string cannot contain null bytes\n``\n. The problem of line 53 is_format = Struct(\"\\00\\00f4xx\")the\\00` are the issue.\n\n\n\nThe corresponding structure definition is\nstruct MyTestStruct {\n  idx1:ulong;\n  idx2:ulong;\n  thresh:float;\n}\nit seems like the python port has problems handling ulong types. Any idea of the proper fix ?\n. Traced back the issue  to static char GenPyFmtChar(const Type& type) which does not cover the ulong case indeed.\nWhich format char should be returned for ulong ?\nI guess this function should implement \nhttps://docs.python.org/2/library/struct.html#format-characters ?\nI also notice there\ndefault: assert(\"Unexpected scalar type\"); return 0;\nnotice that the assert will never raise. Is this a bug or a feature ?\n. @shaxbee thanks for the efforts on this feature.\nGot things almost running (after adding the missing long/ulong entries in GenPyFmtChar):\na) The generated python code was missing import functools\nb) Now I have no clue how to read my data, I tried\ninput_data = open(path_to_fbs_dot_bin, \"rb\")\n    my_struct = my_generated_fbs_library.MyRootDataStructure(input_data)\n    print(len(my_struct.some_vector_entry))\nand I get\nFile \"flatbuffers_python/python/flatbuffers/reader.py\", line 102, in read_field\n    view, base, vtable = self\nValueError: too many values to unpack\nany suggestion on how to read the data from the python side ?\n. With current head I get \nmy_generated_fbs.py\", line 92, in forests\n    return self._read_indirect_field(1, functools.partial(flatbuffers.indirectvector, Forest.unpack_from))\n  File \"python/flatbuffers/reader.py\", line 111, in _read_indirect_field\n    view, base, vtable = self\nValueError: too many values to unpack\nnot clear what is wrong there (similar bug than before). \n. But keep in mind I do not know how to instanciate the read. Maybe I am doing it all wrong.\nFor now I do: instance = MyClass(input_data.read()); print(len(instance.field_I_want_to_access))\nIs MyClass(input_data.read()) the right thing to do ? Looking at the code it seems not.\n. I had tried unpack_from; maybe I was missing that the code is meant for python3 instead of python2.7 ?\nIn python2.7 when doing instance = MyClass.unpack_from(input_file.read(), 0)\nI get \nFile \"flatbuffers_python/python/flatbuffers/reader.py\", line 13, in <lambda>\n    _frombytes = lambda arr, view: arr.fromstring(view.tobytes())\nAttributeError: 'str' object has no attribute 'tobytes'\nwhen using python3 I get\nFile \"flatbuffers_python/python/flatbuffers/reader.py\", line 27, in read_scalar\n    return scalar_fmt.unpack_from(view, offset)[0]\nstruct.error: unpack_from requires a buffer of at least 4 bytes\n(where view is a large bytes object of len(view) == 70231324, and offset is a suspiciously high 3173276464)\n. @shaxbee can you give me some guidance on what needs to be done/fixed ? I could look into it tomorrow and try to \"get it done\".\nOtherwise, I will end up side-stepping flat-buffers by using protocol-buffers for the python exchange, and flat-buffers for the rest, which is sad.\n. @shaxbee  thanks for the progress.\nNow my root object is being read, and I can access some members, however it seems that I am not using the indirectvector object properly. How is this object supposed to be used ?\n. @shaxbee yes I am trying to access vectors.\nVector of float fails with \nFile \"generated_code_for_fbs.py\", line 88, in the_vector_of_floats_vector\n    return self._read_field(0, functools.partial(flatbuffers.indirect, flatbuffers.read_float_vector))\n  File \"flatbuffers_python/python/flatbuffers/reader.py\", line 135, in _read_field\n    return target(view, offset) if offset != 0 else default\n  File \"flatbuffers_python/python/flatbuffers/reader.py\", line 86, in indirect\n    return target(view, read_uint(view))\n  File \"flatbuffers_python/python/flatbuffers/reader.py\", line 35, in read_vector\n    size = read_uint(view, offset)\n  File \"flatbuffers_python/python/flatbuffers/reader.py\", line 32, in read_scalar\n    return scalar_fmt.unpack_from(view, offset)[0]\nstruct.error: unpack_from requires a buffer of at least 4 bytes\nand vector of Table objects seems simply wrong. type(x) for x in list(root_object.the_vector_of_table_objects_field) returns\n [<class 'method'>, <class 'memoryview'>, <class 'array.array'>] which is most likely not was is intended.\nThe first element is <bound method type.unpack_from of <class 'generated_code_for_fbs.TheTableObjectClassOfTheField'>>.\n. For now I used capnp for the python part and a c++ capnp to flatbuffers convertion program.\nIt is double work, but at least I got thing running.\nLooking forward for a better python support for flatbuffers in the future.\n. Last tests I did still failed, I have now moved to using flatbuffers only in c++, the python branch is yet not mature enough for my needs.\nI guess the test cases need to be significantly increased. I could not get the \"vector of objects\" scenario working.\n. Yey !\n. ",
    "louisremi": "Would it be possible to have the emscripten port distributed as an NPM package?\n. You can build such helpers to convert from a JS object to a flat buffer, but this buffer will have to be specific to your flatbuffer schema.\n. Nope. Who did most of the work on the JS port? Contracting is worth considering.\n. ",
    "joeblew99": "@gwvo thanks. Will kick the tires\n. ",
    "michaelbpaulson": "Quick question, I see that --gen-mutable is not supported in JS.  Is there a plan to support this?  I see nothing in the issues. @gwvo \n. I may have some time to explore for a bit.  I'll see what I can come up with shortly.\n. I have actually been working on these.  Its under this project:  https://github.com/michaelbpaulson/fb-node-hello\nI can move them over once I complete them more.  It involves a comparison between JSON and FBs with serialization and server talk / mutation.  Anyways.\n. I'll start uploading asap.  I am going to combine three projects and start getting numbers out ASAP.  \nBtw, renamed the project from fb-node-hello to https://github.com/michaelbpaulson/flatbuffers-benchmarks\n. I am going to build the Scalar values first, as I understand them.  They are simple :)  Just requires a simple offset !== 0 and returning either true or false based on success.  Should be simple.\nHere is an example of the code I believe needs to be generated:\n``` javascript\nfunction mutate_*(value) {\n    var offset = this.bb.__offset(this.bb_pos, );\nif (offset !== 0) {\n    this.bb.writeInt32(this.bb_pos + offset, value);\n    return true;\n}\n\nreturn false;\n\n}\n```\nThe * is meant to represent the name of the value and not a generator.\n. I just realized there is very little tests with any mutation setting.  So this will be I'll tool around with getting some set of tests.\n. Related to #60 and #3955 \n. I'll figure out the travis issue in a bit, until then i'll leave PR open.\n. @gwvo silly question.  To what do I generate code for?  I have executed make|make test|make all| etc etc.  Are you asking me to generate the JS bindings for the test folder?\n. The code generation is a bit out of date, hence all the other changes.\n. Merged and repushed.  Should be cleaned up now.\n. @gwvo I copied the available tests into javascript.  The mutation tests themselves were rather bare so the coverage is not ideal, but it keeps the same level of effort as the cpp tests.\n. Just let me know if there is anything else.\n. Ohh wow.  The fowards/backwards compatibility makes sense.\nThank you\n. 3 things:\n\\1. My primary intentions to put the onus on the library, instead of the implementor, to generate bit addition and evaluating fields.  \nTake the first function addBitFieldBadging.  It takes in a Badging enum value and adds it to the existing value, or writes in the first value. (since multiple bit flags can be present).\nTo me, it makes coding a bit nicer, granted not by much.\n``` javascript\n// ... in a function far far away ...\nvar isHD = true;\nvar isUHD = false;\nvar is Dolby5_1 = true;\n// ... somewhere where the flatbuffer is being constructed ...\nVideo.startVideo(bb);\n...\nif (isHD) {\n    Video.addBitFieldBadging(bb, Badging.HD);\n}\nif (isUHD) {\n    Video.addBitFieldBadging(bb, Badging.UHD);\n}\n// you get the idea.\n```\nIf this helper method is not available then this happens.\n``` javascript\nvar value = 0;\nif (isHD) {\n    value |= Badging.HD;\n}\nif (isUHD) {\n    value |= Badging.UHD;\n}\nVideo.addBadging(bb, value);\n```\n\\2.  reading the data (the prototype method).\nIf I wanted to know if the badging contains HD i do the following, as of now.\n``` javascript\nfunction doStuffWithVideo(video) {\n    var badging = video.badging();\nif (badging & Badging.HD) {\n    // Do Something.\n}\n\n}\n```\nOnce again, I know its not a lot, but it would be nice to have the following:\njavascript\nfunction doStuffWithVideo(video) {\n    if (video.isBadgingHD()) {\n        // Do Something.\n    }\n}\n\\3.  You are right, it is a lot of code, therefore could it be hidden behind a --gen-bit-field-bindings flag at generation.\nFinally I like the idea for ease of use, but I also understand its solving a rather simple problem.  But its a problem that is easier to accidentally screw up when programming by hand as opposed to a library generating it.\nIf you feel that this has no place, ill be glad to close this ticket.\n. @gwvo just a quick PR.  I noticed while playing with my change I introduced lint.  Forgot 2 semi-colons per generation.\n. @gwvo previous commit just removed parser from GenStruct.  Based on other idl generators, parser is used to tell if its --gen-mutable request.  Please correct me if I am wrong. \n. ",
    "shaxbee": "+1 very useful for stuff like transformation matrices etc.\n. Updated pull request - squashed into one commit and rebased with changes from master. Generated files are back in repository as well.\n. @biddisco: yes, if you include project via add_subdirectory it adds install targets developer might not want.\n. You could try externalproject_add instead.\nOn Tue, Nov 8, 2016, 5:37 PM Louis-Paul CORDIER notifications@github.com\nwrote:\n\nThe problem I have with add_subdirectory command is if you are cleaning\nyour project, you'll have to rebuild all project that have been added with\nadd_subdirectory. Using the binary form (libflatbufffers) is time saving\nwhen you have to build your project 10 times for 3 different plateforms for\ninstance... I think it would be great to automatically generate a\nFindLibFlatbuffers when the FLATBUFFERS_BUILD_FLATLIB is enabled. Does it\nmake sense to you?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/78#issuecomment-259089356,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE_mfYNDIbt4vopj1W-MPORa2wJb202Tks5q8ELUgaJpZM4Ck-p7\n.\n. cb91e03c6bc1a176848c2a5064496b5617f34fda:\n\nBasic primitives reading - scalar / vector / offsets implemented.\nEnum and fixed structure generator implemented.\nPending: tables and union generator\n. 7a4c374ab7e84a226753f31f2ddea2001a04611f:\nPython code generator for tables\nSkeleton of builder implementation\nUnit tests for reading scalars\n. Hi Rodrigo, it's still work in progress :-) assert is there in case some\nnew scalar type gets added / to suppress unhandled enum value warning.\nOn Tue, Nov 4, 2014, 18:02 Rodrigo Benenson notifications@github.com\nwrote:\n\nTraced back the issue to static char GenPyFmtChar(const Type& type) which\ndoes not cover the ulong case indeed.\nWhich format char should be returned for ulong ?\nI guess this function should implement\nhttps://docs.python.org/2/library/struct.html#format-characters ?\nI also notice there\ndefault: assert(\"Unexpected scalar type\"); return 0;\nnotice that the assert will never raise. Is this a bug or a feature ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/81#issuecomment-61617252.\n. Oh I see now. I guess I should signal error somehow. I'll look into that on\nweekend.\n\nOn Tue, Nov 4, 2014, 18:11 Zbigniew Mandziejewicz shaxbee@gmail.com wrote:\n\nHi Rodrigo, it's still work in progress :-) assert is there in case some\nnew scalar type gets added / to suppress unhandled enum value warning.\nOn Tue, Nov 4, 2014, 18:02 Rodrigo Benenson notifications@github.com\nwrote:\n\nTraced back the issue to static char GenPyFmtChar(const Type& type)\nwhich does not cover the ulong case indeed.\nWhich format char should be returned for ulong ?\nI guess this function should implement\nhttps://docs.python.org/2/library/struct.html#format-characters ?\nI also notice there\ndefault: assert(\"Unexpected scalar type\"); return 0;\nnotice that the assert will never raise. Is this a bug or a feature ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/81#issuecomment-61617252.\n. @rodrigob I'm in the process of writing tests and ironing out bugs for reading - you'll see some working stuff on weekend.\n. @rodrigob I've pushed some bugfixes - any help w. r. t reading flatbuffers would be welcome - feel free to fork and/or report errors here :-)\n. Use MyClass.unpack_from instead and now that table reading was not tested\nyet. BTW I've added missing import and type codes in latest commit.\n\n\nOn Wed, Nov 5, 2014, 08:27 Rodrigo Benenson notifications@github.com\nwrote:\n\n@shaxbee https://github.com/shaxbee thanks for the efforts on this\nfeature.\nGot things almost running (after adding the missing long/ulong entries in\nGenPyFmtChar):\na) The generated python code was missing import functools\nb) Now I have no clue how to read my data, I tried\ninput_data = open(path_to_fbs_dot_bin, \"rb\")\nmy_struct = my_generated_fbs_library.MyRootDataStructure(input_data)\nprint(len(my_struct.some_vector_entry))\nand I get\nFile \"flatbuffers_python/python/flatbuffers/reader.py\", line 102, in read_field\n    view, base, vtable = self\nValueError: too many values to unpack\nany suggestion on how to read the data from the python side ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/81#issuecomment-61620192.\n. @Rodrigo: I'll work on it together with tests :-)\n\nOn Wed, Nov 5, 2014, 15:00 Rodrigo Benenson notifications@github.com\nwrote:\n\nI had tried unpack_from; maybe I was missing that the code is meant for\npython3 instead of python2.7 ?\nIn python2.7 when doing instance = MyClass.unpack_from(input_file.read(),\n0)\nI get\nFile \"flatbuffers_python/python/flatbuffers/reader.py\", line 13, in \n    _frombytes = lambda arr, view: arr.fromstring(view.tobytes())\nAttributeError: 'str' object has no attribute 'tobytes'\nwhen using python3 I get\nFile \"flatbuffers_python/python/flatbuffers/reader.py\", line 27, in read_scalar\n    return scalar_fmt.unpack_from(view, offset)[0]\nstruct.error: unpack_from requires a buffer of at least 4 bytes\n(where view is a large bytes object of len(view) == 70231324, and offset\nis a suspiciously high 3173276464)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/81#issuecomment-61767435.\n. Codegen is done in src/idl_gen_python.cpp, table base class is defined in\npython/flatbuffers/reader.py.\nMaybe you are trying to read root message? I didn't add code for that yet\n:-( Sorry for no updates but my daily job is keeping me busy in past two\nweeks.\nOn Mon Nov 17 2014 at 1:19:21 AM Rodrigo Benenson notifications@github.com\nwrote:\n@shaxbee https://github.com/shaxbee can you give me some guidance on\nwhat needs to be done/fixed ? I could look into it tomorrow and try to \"get\nit done\".\nOtherwise, I will end up side-stepping flat-buffers by using\nprotocol-buffers for the python exchange, and flat-buffers for the rest,\nwhich is sad.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/81#issuecomment-63228286.\n. Just pushed bunch of bugfixes.\n\nFeatures missing / bugs:\n- ~~nested fixed structs do not deserialize properly~~ done\n- no builder/verifier codegen\n- unit tests for scalar vectors - they pass integration test though\n- union properties are incorrect\nDone:\n- codegen for reader\n- scalar serialization and unit tests\n- reading integration test (ported from Go)\nDesign decisions were made based on benchmarks covered on shaxbee.github.io\n. I'm not sure about packaging for python as well - there seem to be three options now:\n- generate all classes to one file and add imports to __init__.py (supports generating into existing directory structure, class names contain extra element though)\n- each class gets its own file (might be expensive)\n- generated file name is taken from last namespace element (class names are clean, doesn't work with pre-existing source trees)\n. Please note that to run integration tests you need to install nose and enum34 (only on Python 2.x - backport of Python3 standard enum) packages, after that you can simply run python python_test.py.\nUnit tests are run as usual via python setup.py test or python setup.py nosetests for more verbose output\n. It should be used internally in generated code. Interface wise it is compatible with Sequence. I'm not warranting it works though - do you have issue with field containing vector of strings/tables?\nEdit: Minimum example when something doesn't work would help.\n. I've uploaded some fixes two days ago. Might be worth checking.\nOn Thu, 27 Nov 2014 07:26 Rodrigo Benenson notifications@github.com wrote:\n\nFor now I used capnp for the python part and a c++ capnp to flatbuffers\nconvertion program.\nIt is double work, but at least I got thing running.\nLooking forward for a better python support for flatbuffers in the future.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/81#issuecomment-64725527.\n. Would be nice if you provide minimal example of what fails.\n\nOn Tue, 16 Dec 2014 08:03 Rodrigo Benenson notifications@github.com wrote:\n\nLast tests I did still failed, I have now moved to using flatbuffers only\nin c++, the python branch is yet not mature enough for my needs.\nI guess the test cases need to be significantly increased. I could not get\nthe \"vector of objects\" scenario working.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/81#issuecomment-67090294.\n. I've reproduced and fixed problem with array of tables. \nIt does not seem to be covered by tests using tests/monsterdata_test.mon.\n@gwvo, @rw - Would it be possible for you to add some data and unit tests on testarrayoftables field?\n. OK. Since we cannot modify existing encoding could we simply return stack-allocated union handle object with interface similar to one above? Also generic visit function accepting any callable and calling it with appropriate type would be useful :-) If that sounds good I can make a branch and mend some code.\n. I guess the visit method would be templated so we can pass any callable,\nthen visit will call it with appropriate type. Eg.\n\ncpp\nstruct Monster {\n  // ...\n  template <typename F>\n  public void visit_foobar(F f) {\n    auto foobar = GetPointer<const void*>(20);\n    switch(GetField<uint8_t>(18, 0)) {\n      case Foobar_Foo: f(reinterpret_cast<const Foo*>(foobar)); break;\n      case Foobar_Bar: f(reinterpret_cast<const Bar*>(foobar)); break;\n      default: return;\n    }\n  }\n. This can be fixed by adding check for cursor position being at the beginning of source file. Attached pull request would fix that.\n. On side note I don't know if this is correct behaviour or not but Python port is returning None value for fields which are not set and don't have default value - including scalars.\n. Sure. I'll be able to do that only after new year though.\nOn Fri, 12 Dec 2014 15:29 Robert notifications@github.com wrote:\n\nInteresting. I'd like to see much more testing on this. Have you looked at\nthe read tests in the C++ version? Fuzz-testing would be appropriate here.\nKeep it up!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/pull/110#issuecomment-66740493.\n. @rw: Working on fuzz tests now - they require builder support for tables / scalar fields, I've aligned Python builder interface with C++/Go as seen in  747030d\n. Fuzz testing, fixed formatting, added more docstrings. I'll work on builder codegen now.\n. Hello, go ahead and close it, I'll work on struct based read/write and some other perf optimizations once #112 is merged.\n. I've tried ctypes and ended up using struct because it enforces value ranges automatically, takes care of endianess and returns native python data types. Also it seems to be much faster:\n\nctypes:\n%%timeit raw = struct.pack('<i', 534217); head = 0\nn = 0\nn |= raw[head]\nn |= raw[head + 1] << 8\nn |= raw[head + 2] << 16\nn |= raw[head + 3] << 24\nctypes.c_int32(n).value\n   ....:\n1000000 loops, best of 3: 994 ns per loop\nstruct:\n%%timeit raw = struct.pack('<i', 534217); fmt = struct.Struct('<i'); offset = 0\nfmt.unpack_from(raw, offset)[0]\n   ....:\n1000000 loops, best of 3: 268 ns per loop\nI'll steal the idea of explicit type definitions though.\n. If you don't mind I'll use parts of your builder implementation and put datatype definitions in separate module as you did and remove hardcoded datatype sizes.\n. I don't understand - you've mentioned combining my PR with yours in #110. I'll go ahead and provide own implementation of Builder then...\n. I think we are talking about same thing then :-)\n. They don't seem to overlap that much as #110 is based on C++ API and uses struct / array heavily.\nMy focus was on performance and therefore I reduced the scope of v1 features to reading.\n. Sure. How do you want to do that? Merge tests and builder codegen from #112 to #110?\n. @rw: I've merged encode / numtypes code, using struct module but keeping the methods. Could we sync via email?\n. +1 for this PR, i don't have capacity at the moment to finish up #110.\nOn Mon, 9 Mar 2015 at 14:12 Robert notifications@github.com wrote:\n\nIncrease code coverage to 97%: Add cases to generate and test conditionals\nnot traversed with the 'gold' example Monster data.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/pull/112#issuecomment-77805123.\n. @rw Congrats on getting this through! :-)\n. Does not work on Python 3 though. What about using __all__ ?\n. Seems like leftover/unused function as Table.__new__ reads vtable already.\n. Those are high-order functions that return reader callable processing buffer (source, offset).\nTake a look at generated code - essentially indirect reads offset and forwards it to 'target' callable, eg. property of type vector of strings would be constructed by indirect(vector(indirect(read_unicode))).\n. Immutability and speed of property access - same trick is being used in collections.namedtuple.\nYou can find more details about that here: of http://shaxbee.github.io/python-binary-part2-struct-unpack/\n. Comments are being generated - see Monster.testofarraytables\n. Should the rights be reserved to Google Inc?\n. I guess i could make generated code more straightforward though.\n. \n",
    "orientalpers": "Yeah, i vote for this feature too.\n. ",
    "falconair": "Any word on this feature? I have a large definition file where my definitions have strings of pre-specified length (my target is mainly c++). I would love to be able to use just structs, rather than tables.\n. I almost created a new ticket for this, but it is probably enough just to add a comment here. It would be nice to be able to define a 'static' string attribute in a schema. For example:\ntable TestEvent{\nevent_id :string = \"test_event\" ;\n}\nThis seems to be the kind of thing that should be added whenever static length strings/vectors are implemented (just like numbers can be defaulted).\n. @gwvo could you start to provide some guidance on how this feature might be implemented. Will there need to be custom fixed size vector and string classes? How will the schema definitions change, etc.? Perhaps if you add more information on how expect this feature to be implemented, some C++ dev will take a crack (unfortunately I'm not a c++ guy).\n. Zero out the unused bits. This way,  single check to see if the value is non-zero will tell me if any of the bits are true. I can also compare two bit vectors and quickly find out if any (0 to 64) fields are true.\n. That seems odd, since bool is a fundamental type in almost every language. If I declare something 'bool' in FB, I expect the equivalent bool or boolean type in the generated code. Note that I am talking about the language interface, if it is represented as 8 bits on-wire, that is not as big of an issue.\nIn any case, am I correct in assuming that false will mean a zero 8bit int and true will mean a 1 (or greater than zero)?\n. I was initially going to paste an example, but I would have to paste proprietary code. I'll try to create a minimal example which will recreate this issue. Not sure if I'll be able to do that immediately.\n. It looks like flatc is generate a header file where a method name and constructor argument name are the same, which gcc doesn't like.\nFor example, in the definition file, I create a struct\ncpp\nstruct Timesamps{\n  ...\n  important_timestamp: ulong;\n  ...\n}\nThe generated header looks something like this:\n``` cpp\nMANUALLY_ALIGNED_STRUCT(8) Timestamps FLATBUFFERS_FINAL_CLASS{\n  ...\n  uint64_t important_timestamp_;\n ...\npublic:\n    Timestamps(..., uint64_t important_timestamp,...): ...{\n      ...\n      uint64_t important_timestamp() const { return flatbuffers::EndianScalar(important_timestamp); }\n      ...\n    }\n}\n```\nAs far as I can tell, important_timestamp shows up as a constructor argument as well as a class method and is the cause of the warning.\n. I'm afraid I wont' be able to get to it for a while, and you don't want me writing c++ code anyway!\n. ",
    "ahundt": "sounds very useful!\n. +1 for integrating this, it would make flatbuffers a much easier sell to my software groups\n. @specialforest How did converting to C++03 go? Any chance there will be something worth merging soon?\n. ah well thanks for replying\n. I'd appreciate this as well, or perhaps instructions to make the jar\n. @velvia those look pretty out of date unless I'm mistaken the current release is 1.3.0 and it seems your latest is 0.2.0\n. Quick google search also turned up this for publishing to maven central:\nhttps://www.theguardian.com/info/developer-blog/2014/sep/16/shipping-from-github-to-maven-central-and-s3-using-travis-ci\n. Does move work for FlatBufferBuilder? That would be very useful to me because I'm using asio for networking so I have to move the buffers around. Right now I'm using a shared_ptr but that introduces overhead.\n. I agree it isn't the prettiest solution, but it does solve the problem completely and is widely used by expert C++ developers. \nTo back up my claim, I just searched the boost 1.57.0 /include directory for instances of ::boost:: and found 11,124 results. Some of those are documentation and such, but a quick look through shows about half is legitimate code using :: to select the global namespace.\n. Perhaps that is a bit different than the idea I was trying to get across. What could be useful is being able to write a function that iterates over each data member at compile time, so that they can be accessed, modified, transported, or represented as a \"view\" to another representation easily and efficiently for all data members and at multiple \"depths\". By \"depth\" I mean object.shallow vs object.that.is.very.deep, which can be a bit panful to access.\nThat could just be a useful feature of the generated code for the C++ implementation of the flatbuffers protocol. Hopefully my explanation makes sense\n. There is a current way to do that with reflection with C++? I know C++ doesn't generally have reflection but perhaps you have some mechanism already set up I missed?\n. Oh neat, I will definitely take a look. Thanks! I just found it in the docs since you mentioned it and it looks promising. Perhaps it is relatively new since I haven't checked out the docs in a while?\nIn this case you can probably close this issue and I'll only post on this thread or create a new issue if there is something worth discussion. Thanks again, what a great tool!\n. \"latest cpp\" sounds like a moving target that can become incorrect in the future. Perhaps it should be \"--use-cpp11\" or \"--use-cpp14\"?\n. @gwvo great! Is there a PR or could you point me into the code?\nI actually might be able to use this in the very near future, might it be easy to add to python as well?. Awesome, thanks!. Well, I'm actually hoping to verify that messages are correctly received at the destination and figuring out the best way to do so. I'm thinking of sending a flatbuffer message containing another flatbuffer message and a checksum. I could then recompute the checksum, verify it, then send a reply with the checksum so it is confirmed as received.\nCan any existing flatbuffers code help with this, do you have any suggestions, or perhaps know of existing code I could use as an example elsewhere?\n. tl;dr for others: crc, checksums, hashing, and hash tables of serialized buffers are not built in explicitly and crc/checksums are currently out of scope.\nThanks that clears up what's in the current code for me! \n. Perhaps this answers my question?\nhttps://github.com/google/flatbuffers/issues/333\n. seems to be there for c++ as IsFieldPresent:\nhttps://github.com/google/flatbuffers/blob/9dfc3d61ee41c6d92e0b3be61fa5522e3273e840/include/flatbuffers/flatbuffers.h#L1330\nTested as follows in test.cpp:\n// Checking for presence of fields:\n  TEST_EQ(flatbuffers::IsFieldPresent(monster, Monster::VT_HP), true);\n  TEST_EQ(flatbuffers::IsFieldPresent(monster, Monster::VT_MANA), false);\nHowever, I haven't yet located the Java equivalent.\n. I looked through the java folder and there doesn't appear to be an equivalent:\nhttps://github.com/google/flatbuffers/tree/master/java/com/google/flatbuffers\nI also checked classes generated in java with the latest version and there isn't any equivalent there either.\n. Interesting, I probably want to stick with the default behavior. \nDo you then recommend I have a boolean indicating yes/no When I want to differentiate between no change and the default value?\nThe following may be moot considering the above but in at least some cases there is a way to achieve the an equivalent to IsFieldPresent in java:\nIt looks like fields in a table can be handled by initializing objects to null and passing them to the function that updates the objects:\njava\n  public Inertia inertia(Inertia obj) { int o = __offset(10); return o != 0 ? obj.__init(o + bb_pos, bb) : null; }\nIf they're still null when they come back out then they weren't there.\n. > Another option that works in all languages is to wrap a scalar field in a struct. This way it will return null if it is not present. The cool thing is that structs don't take up any more space than the scalar they represent.\nI have a mix of scalars and table fields across a variety of flatbuffer structs and tables. Is there a single way to differentiate between \"default\" and \"no update\" in all cases (including perhaps the struct wrappers for scalars when appropriate). Or if it requires slightly different usage for each case could you explain the best way to do that for each?\n. > You can already tell presence of tables, so you'd only need it for scalars. You could make a struct Integer { i:int; } for example that works for all your integer fields.\nTo clarify: for a table you can tell the difference, but for a scalar in a table you cannot?\n. Would it then make more sense for the function to be something like isFieldDefault() in both languages? (though the backwards compatibility concern may be more pressing)\n. I've narrowed it down, the issue is actually in C++. I was missing one layer of table which goes into the union. I've added that and now I serialize then immediately deserialize to debug things all in C++, and I'm getting junk out.\nInput Value and output Junk:\nKukaJAVAdriver sending armposition command:[1, 0, 0, 0, 0, 0, 1]\nre-extracted 7 joint angles: 0 11 02 03 04 05 06 1\nFilling out the relevant portion in C++:\n``` c++\n              case flatbuffer::ArmState::ArmState_MoveArmJointServo: {\n            /// @todo when new\n            JointScalar                          armPosVelAccelEmpty;\n            auto armPositionBuffer = fbbP->CreateVector(armPosition_.data(),armPosition_.size());\n            auto goalJointState = grl::flatbuffer::CreateJointState(*fbbP,armPositionBuffer);\n            auto moveArmJointServo = grl::flatbuffer::CreateMoveArmJointServo(*fbbP,goalJointState);\n            controlState = flatbuffer::CreateArmControlState(*fbbP,bns,sequenceNumber++,duration,armControlMode_,moveArmJointServo.Union());\n            std::cout << \"KukaJAVAdriver sending armposition command:\" <<armPosition_<<\"\\n\";\n             break;\n          }\n\n```\nNote that I checked my code against the sample CPP for both writing and reading the buffer. I've also tried to correctly imitate the creation of the message with  the use of a .Union().\nCode to re-extract and print the result, which returns junk:\n``` c++\n          grl::flatbuffer::FinishKUKAiiwaStatesBuffer(*fbbP, states);\n      flatbuffers::Verifier verifier(fbbP->GetBufferPointer(),fbbP->GetSize());\n      BOOST_VERIFY(grl::flatbuffer::VerifyKUKAiiwaStatesBuffer(verifier));\n\n      if(armControlMode_ == flatbuffer::ArmState::ArmState_MoveArmJointServo)\n      {\n          auto states2 = flatbuffer::GetKUKAiiwaStates(fbbP->GetBufferPointer());\n          auto movearm = static_cast<const flatbuffer::MoveArmJointServo*>(states2->states()->Get(0)->armControlState()->state());\n          std::cout << \"re-extracted \" << movearm->goal()->position()->size() << \" joint angles: \";\n          for(std::size_t i = 0; i <  movearm->goal()->position()->size(); ++i)\n          {\n            std::cout << i << \" \" << movearm->goal()->position()->Get(i);\n          }\n          std::cout << \"\\n\";\n      }\n\n```\nHelp with figuring out where I'm going wrong would be appreciated!\nAs my experience has increased with flatbuffers, and now that I'm using more complicated messages in Java & C++, I've been a bit surprised by how hard flatbuffers is to use correctly and how easy it is to use incorrectly. Sorry about that being a discouraging sentiment, I wonder if there is a way things could be made easier to use...\n. After double checking the readme I've also decided to create a stackoverflow post regarding this issue: https://stackoverflow.com/questions/35924332/flatbuffers-encoding-then-decoding-c-double-array-table-union-returns-junk\n. The issue I ran into is resolved thanks! I do think the ease of use can be improved and I've thought of two possible approaches.\nI believe the use of unions in the C++ API is not type safe because it casts things to void. Trickier mechanisms for this that will fail at compile time are possible which would allow compile time failure when things are done incorrectly through mechanisms like boost variant. I'm not suggesting using boost itself, instead I'm throwing out an idea suggesting one possible approach that can preventing such issues. http://www.boost.org/doc/libs/1_60_0/doc/html/variant.html\nAnother way to make it much more usable would be to generate examples filling out the whole object. using zero defaults when the .fb is run. That would actually save enormous amounts of time. The balance of that is I'm sure it would be a pain to write. \nDo either of those sound practical or achievable?\nAlso, as a side note I just accepted an internship at google for this summer!\n. If the type has a deep dependency tree on other flatbuffers then CreateMyType() doesn't capture it.\n. I signed it!\n. Well, a duplicate function with a rename would be easy enough. Alternately could use cmake_parse_args to make the parameters optional and retain the old behavior if they're not specified. What's preferred?. @gwvo good advice, thanks!\n. Sorry, this unintentionally included a rollback. I may just create a separate pull request with only the relevant patches squashed together\n. @gwvo created it\n. numpy is very ubiquitous I'm very interested in this feature, especially if it is available by default without any new flags to add.. @wrigby Cool! you might want to look at #4390 before you start again. I think I had been interested in similar helper messages in the past.. I suggest that if there must be a flatc flag it should be --disable-numpy, so it is available by default.\nnumpy is very ubiquitous and I'm very interested in this feature, especially if it is available by default without any new flags to add. (sorry for the slight duplication I just saw this newer issue).. Ah but could it be done in a way which loads all the data in numpy by default with the current API? In other words have numpy data returned by default, without any API changes?\nAlso, thanks for the great work!. whoops, then what you're doing sounds right. I just assumed it was like the C++ API, I'm just getting started with the python one as of today. Sorry!. I wonder how hard a numba or cython api would be, since flatbuffers already compiles with C++ that's probably not too high a barrier for many users, as long as it would be an optional feature. Wishful thinking. :-). Looks like travis might be able to do that:\nhttps://docs.travis-ci.com/user/deployment/pypi/\nhttp://www.robinandeer.com/blog/2016/09/01/automated-pypi-releases/. @mikeholler perhaps @gwvo (@aardappel) is willing to simply move setup.py to the root location?. FYI I've seen other big projects that support multiple languages and use PyPi put setup.py in the root directory without issue. One example I can think of off the top of my head is https://github.com/bulletphysics/bullet3 and its pypi package pybullet. I don't have strong feelings, just wanted to give an example that works.. found the answer, the parameter is --escape-proto-ids. ah, what if the default simply added an extra namespace scope?\ncartographer.transform.proto -> flatbuffer.cartographer.transform.proto\nThen it could just be imported / re-scoped to wherever people want with the correct settings, and without name changes (at least in C++ and perhaps python).. that sounds like a good approach. Are there any best practices of memory management with flatbuffers recorded somewhere? I've seen people struggle a lot with creating their buffers and accessing their buffer when they receive it from somewhere else. I think the bad practices could be avoided and some good ones made easier with convenience functions and examples that help with this.\n\nWe already have a way to bundle a buffer with a root pointer in BufferRef. This class makes it optional to own the buffer (and free it), which I think is a better design. In many cases, you don't even need to pass the buffer around, since you can call GetBufferStartFromRootPointer to retrieve it.\n\nIf you copy a BufferRef with the free boolean set to true, won't it delete twice? Ideally, couldn't something be set up where lifetime is managed by std::shared_ptr or std::unique_ptr?\nPerhaps Buffer could be modified to store one of those rather than a vector, or just be used for inspiration because some of those member functions are fairly convenient.\nThanks for your consideration!. > add more example code for these languages (e.g. writing to a file).\nExamples are always a great help with API confusion! If nothing else, this would be great to have.\nAnother feature that might help immensely is if example code could be generated. Basically, just generate a main function that explicitly fills out all the user-defined tables in a higher level table with the default values (not the zero/null value), that someone can copy/paste into their program and just add in their real values. That might save users many hours of filling out Create() calls. Probably not fun to implement the generator though...\n\nshared_ptr can of course work, but it is relatively heavy-weight, and requires all other users of the buffer to also be using shared_ptr, which you can't always dictate.\n\nYou can always get the raw pointer out and manage the lifetime separately, or just not use the ref/buffer object, right?. > Not sure about generating example code. \nIt might provide a good opportunity to provide a fairly high performance default people could utilize with ease.\n\nIn the end the FlatBuffers API is a little harder to use for necessary reasons, and I'd worry that people using it \"blindly\" without understanding what the code does, does not improve their situation.\n\nI wish I had data so this wasn't so hypothetical, but could it improve their situation if it isn't an extremely performance sensitive use case which helps them get going faster, and that this might in fact be the most common situation for most code written?\nI expect even with such a compromise the result would likely still be usefully faster than a protobuf, for example.. Thanks!\nWould this still work if there was another create call in between?\nauto off = CreateDataMessage(fbb, ...);\nauto off2 = CreateSomeOtherMessage(fbb, ...)\nauto dc = reinterpret_cast<const DataMessage *>(fbb.GetCurrentBufferPointer() + fbb.GetSize() - off.o);\ndc.time();\nI'm not sure if the allocations would mess with it.. Answer to my question: Check the order of initialization, make sure you aren't passing the wrong object instance. There is also a convenient assert if you run in a debugger.. In my case any builder will only finish a specific buffer. If the builder could optionally be templated on the type it is expected to build, and would fail to finish any other type then it could easily detect and point out the bug at compile time. I'm being a little loose with the syntax but it should get the idea across. \nHere is the fbs:\nmytable {\n  myvalues:[double]\n}\nHere is the conceptual C++:\n```C++\nFlatBufferBuilder fbb;\nv = fbb.CreateVector(...);\nmt = Createmytable(fbb, v);\n// error! I meant to pass mt\nfbb.Finish(v)\n```\nOf course, in the real code fbb wasn't templated. The bug was a bit tricky to find but obvious once located.. Actually, we've been doing additional testing and I haven't completely fixed my issue. Verifier success is intermittent at the moment, I suspect it will be another bug like the one above further down insinde my message's data.. When searching for the source of a verify failure, is it a good approach to verify parts of a flatbuffer or partially completed flatbuffers? I've noticed a verify function is not generated unless a root identifier is specified for that type.. The second issue is that my buffers were getting too big. It may make sense for verify to check that.. > I'm not sure why you'd want to verify a part of a FlatBuffer, or verify\nduring construction.\nIf you're creating a massive buffer that takes hundreds or thousands of lines, being able to verify parts of a buffer means the manual search time it takes to find the mistake goes from ~O(n) down to ~O(log(n)).. Thanks!\nI'd like to clarify my understanding from your post regarding limits with @Chunting (whom I work with).\n\nmax_depth = 64, max_tables = 1000000 \n\nDoes this mean that a table containing a vector of tables with depth 2 can only have 500,000 elements in it?\nWe are writing log files that contain a table with a vector of tables ~1.8-2.5Kb in size each, these are probably depth ~4-6, and they accumulate at 250 buffers per second from a sensor. Unfortunately, this pushes me up against the hard limits in surprisingly little time, with 250Mb of data accumulating 1 million buffers in just 1.1 hours if table depth is included in the the 1 million limit, in 10 minutes if it is. This seems like it might be the source of our problem.\nI was hoping to write out files entirely supported by the existing API without any custom header+size entries that I would need to re-write for every language, but I didn't realize I would hit the limits so easily. Would you suggest or consider one of the following options for this type of use case? \n\nconsider supporting a multi-buffer file (or stream) in the flatbuffers API\nIf I must create custom files, is there a way I can create fixed size flatbuffers to write my headers?\nShould I do compile time configuration of flatbuffers to increase the existing limits (I'd like to avoid the problems I know users of my logs will encounter with this option)\n\nFor 1 and 2 the design could be something like the following, assuming these two data types would be of fixed size:\n```\nstruct BufferStreamHeader {\n// magic fixed size identifier for this buffer stream\nid[uint64] = 0;\nversion[uint64] = 1;\n}\nstruct BufferStreamMessageSize {\nfixed64 size = 1;\n}\n```\nThe file could look like this, for whatever buffer \"Message\" happens to be:\n[BufferStreamHeader][BufferStreamMessageSize][Message][BufferStreamMessageSize][Message][BufferStreamMessageSize][Message]...\nThe messages themselves can be a fixed type since it could easily be a table containing a union.\nWhat do you recommend? Also, thanks for all your help and consideration in solving these issues!. > There is already a size-prefixed buffer support in a number of languages. \nCool, where can I find documentation for creating size prefixed buffers?\nIs there also a way to guarantee the size of a buffer?\nI also created the following PR documenting the Verifier to clear things up:\nhttps://github.com/google/flatbuffers/pull/4564\n. ok well I think that closes out this issue! Thanks!. FYI I've seen other big projects that support multiple languages and use PyPi put setup.py in the root directory without issue. One example I can think of off the top of my head is https://github.com/bulletphysics/bullet3 and its pypi package pybullet. . Also why did travis fail when I just added some comments?. I totally agree DRY (Don't repeat yourself) is an excellent principle for the code itself. Please consider that it is equally worthwhile to help users find the critical information they need where & when they need it. In other words, if someone makes a mistake the code, documentation, and error messages should try to solve the problem for them.\nKeras is a library makes for an excellent documentation example:\n - api design philosophy: https://blog.keras.io/user-experience-design-for-apis.html\n - Keras documentation: https://keras.io/\nI added this comment to this specific location because it exactly is where I looked, and it could have fixed my original problem. If this comment had been here it would have saved me several days of time. It might save you time too because there won't be new questions like https://github.com/google/flatbuffers/issues/4478.  :-)\n. ",
    "idoroshev": "Where can we store the length of such array after type parsing?\nWe could create a new field in Type struct or create an inheriting struct with this field, but I doubt, these solutions are great.\n. I agree with using my commits in this PR.\n. I suppose, it will be good for the prefix increment, because postfix increment should return old value of object and olny then incremented. Correct me, if I am wrong. \n. ",
    "let4be": "So... How do I access non root_type buffers, can't find anything like Get*** in c++ generated files and the same problem in go lang.\ngenerated code has huge differencies between languages and it's really hard to work with\n. Tried both latest master and latest release versions\n. ",
    "gaurimaheshwari": "please explain me how to build flatc executable?. ",
    "aardappel": "@gaurimaheshwari : https://google.github.io/flatbuffers/flatbuffers_guide_building.html. We currently have field:type = val to define a default, or field:type to imply = 0. So maybe if we added field:type = none or something similar as a way to indicate a field doesn't have a default, and should thus never be omitted. That way a has_ method will work on it more meaningfully.. The problem with being schema based is implementation: this needs to be supported in the code generators for all languages, otherwise you'll get inconsistent results. It'll be a fair bit of work.. @JonathanHope such a function should really be returning a DetachedBuffer, since it is asking the caller to own the memory. The called can then retrieve the root from it.. It doesn't look like anyone ever did, no... Hah, hadn't thought of that.. thanks for noticing.\n. I doubt there are many people out there instantiating these classes, and if they do, it probably be good for them to hear about a problem in their code :)\nI'd prefer if you didn't make it an option, that's not worth the added complexity.\n. https://github.com/google/flatbuffers/commit/321a1c9dc0bb2205feb1bb9fd079546403aa852f\nVerified it does search parent namespaces for the type.\nAlso, it the error message now hints you should check the namespacing.. Wow, surprising we haven't caught this earlier. I guess when you always test on x86, this can happen. We also test on arm, but that can compile to instructions that allow unaligned access depending, or maybe it just got lucky.\nYour fix was merged.. much appreciated!\n. Good find!\n. merged, thanks!\n. thanks!\n. That's indeed a bug. Will look at your fix.\n. The Go scripts does a bunch of copying to satisfy some Go runtime requirements which I don't quite understand. Would be good to have a .bat version of it, yes.\nI'll fetch your change and test it locally for now, otherwise looks great!\n. Tests all passed locally, merged. Thanks!\n. Nice, thanks! Could you run tests/generate_code.sh (which will update the test code, which you should include in this commit). To test, run JavaTest.sh.. the tests shouldn't need any changes since they don't rely on the argument name.\n. Thanks!\n. are you using --defaults-json ? You adding || true does something similar to my commit, except it doesn't check that the field is scalar, which doesn't surprise me that it would crash.\n. @troyedwardsjr Very cool! but why implement your own schema parser? It's a lot of work to guarantee feature parity, and integration with the main project is much harder this way.. Closing due to inactivity. Feel free to re-open.. Because documentation often doesn't get updated, in fact, that should have been updated with this PR. Care to fix it?\nI don't know that anyone is working on mutation for those languages, no.. Your code looks fine, so the problem must be that result does not represent a valid FlatBuffer. This could be because something went wrong during construction, or transmission of the bytes.\n. The server could run flatc -b on the JSON, but better yet, don't generate JSON at all, and have the server use a FlatBufferBuilder to construct its data. That way it will be efficient all the way thru.\n. Ah, I assumed your server was also in Java. In that case, you may find this interesting: https://github.com/google/flatbuffers/pull/250\n. No, I don't think this was ever attempted. You're welcome to create a PR for it :). Was also requested here: https://github.com/google/flatbuffers/issues/3766. Rather than generating code for it, this can probably be implemented now based on the mini-reflection functionality: https://github.com/google/flatbuffers/blob/master/include/flatbuffers/minireflect.h. @dominikandreas it is not native functionality because.. noone has implemented it yet.\nIt probably hasn't come up that much yet because FlatBuffers whole reason of existence is to use data in-place, and avoid copies. Unlike, say Protobuf (or the FlatBuffers object API) where data is a tree of objects, which you may use as you internal program representation, so copying is much likely to be needed.\nThat said, it would probably a nice addition for the base API as well. It could either be code generated, or a generic function that makes use of the mini reflection tables. The latter sounds more elegant to me, but will be slightly slower.\n(Note that, for the purpose of copying tables out of larger buffers, nested flatbuffers are often the neatest way of doing this).. hash-X: go ahead :)\n. Can you please check with the latest FlatBuffers from master, since we just committed much improved support for .proto parsing: https://github.com/google/flatbuffers/commit/94680f5483593b1a48c79b516d153fd432b3f2e8\n. I looked at CodedOutputStream, and that just seems to deal with encoding values.. I am not sure what you are suggesting.\n. Use FlatBufferBuilder.sizedByteArray() to get the bytes, much simpler.\nHow you write those to file is up to you.. what you do above looks similar to writeDelimitedTo, but I've never used it.\n. It seems someone should rebase PR #310 for this to happen... do we still want this in?. @rw : you liked this, if it is a simple rebase, can you merge it?. No, this issue still covers that mostly. Can you make a PR for this? @rw?. @kartynnik Maybe I was mistaken above (looking at past PRs, I think I may have confused it with size prefixing functionality). We only just got this for Go: https://github.com/google/flatbuffers/pull/4720/files and looking at the Python code there appears to be no such functionality there.\n@rw who wrote the Python implementation.. I don't think I understand's Go's namespace restrictions.. @rw, how does this relate to my proposed attributes above?. Why not namespace protocol (java: \"com.company\", go: \"company.com\"); ? Why also have a command-line flag?. Ok, that's all sorts of f-ed up.\nBut why do we need a command-line flag? To allow companies to share schemas without forking?. Sounds like for Go we'll need a command-line flag regardless. For other languages though, it could be an attribute in the schema.. which would be friendlier since it forces every user to agree on the namespace?\nEither way, you can start with a command-line option for Go, since presumably they would all be concatenated anyway.. Fix was merged: https://github.com/google/flatbuffers/pull/4222. @xiaohaoliang see the discussion above.. apparently in Go command-line is better because the namespace may be different per user of the package.\nThough I guess there's no harm in also supporting the schema version, @rw?. @xiaohaoliang if for Java and C# you can already specify it in the schema, why do you also need it on the command-line? I thought this was only necessary for Go.\nAnd if we'd also support it for Java and C#, it be good to use the same option/var as the one used for Go.. Yes, but what is wrong with namespace package_A (java: \"com.company\") ?\nAnd if we also want a command-line argument, I say lets change --go-namespace into just `--namespace and make it work for any language.. @mikemccarty-vertex sure, that's a good first step. Any reason not to also add the in-schema attributes?. And we should change this code https://github.com/google/flatbuffers/pull/4222 to use a cross-language option.. Ah yes you're right, Go is special in that way :(. @stefansullivan I agree this is a pain. I'm not sure if there would be a builtin way that would make everyone happy, but for now having prefixing options at least let people work around namespace differences.\nAs for code styles, we have tried to do automatic conversion of identifier styles (e.g. translation to camelCase in Java), but I am not sure how consistent we are in this.. Here's the code: https://github.com/google/flatbuffers/blob/master/include/flatbuffers/flatbuffers.h#L1475\nAnd yes, should be easy to add to python, if you follow the alignment caveats mentioned in the C++ code.. There has not been progress for a while, @Lakedaemon are you still planning to finish this? Or anyone else want to pick this up?. Closing due to inactivity. Feel free to re-open.. @suraj1291993 @evolutional this was last discussed in 2015, so by now if we wanted to make this change, we'd have to make it a command-line flag that is by default off, since the amount of people we'd break might be a bit much by now.. Ok, I'll merge it, thanks!\n. Closing due to inactivity. Feel free to re-open.. Ok, since this PR was stalled, I ended up implementing it myself. Please check it out here:\nhttps://github.com/google/flatbuffers/pull/3998\nI've probably taken a different approach than this PR, choosing to try to change as little as possible.\n@evolutional @Lakedaemon @belldon any comments?\nWe still have room for additional improvements.. I am considering we may at some point want to split the C# and Java generators, as this PR was really a lot more painful than it should be.\n. @chobie: can you fix this PR?. Here is a proposal to make them all partial, if anyone is still following: https://github.com/google/flatbuffers/issues/4154. Closing due to inactivity. Feel free to re-open.. Can't make promises, but will see what I can do. Maybe @alexames? Anyone else want to make a PR would be welcome to.\nNote that fixing this doesn't make the above code working, it merely moves the error from the C++ compiler to the schema compiler.. @KhasanovBI see #3781 . This is a duplicated issue of https://github.com/google/flatbuffers/issues/264. @m339606: We would like to support Swift in the main FlatBuffers project, but as discussed above, @mzaks's implementation is incompatible with the other language implementations. So either those need to be addressed, or maybe someone else will make a compatible implementation. If that doesn't happen, I can't promise when or if I can make a Swift implementation myself.\n@mzaks: please re-read the discussion above. Support for cycles will need to be removed for your implementation to be compatible with FlatBuffers.. An option could work, if it is indeed off, and it comes with a very clear disclaimer that this is a feature currently a) unique to swift, b) not part of the official FlatBuffers spec, and c) generates incompatible binaries that may generate verifier failures or crashes on current and future implementations depending on language.\nFeel free to make a PR for Swift, that will make discussing changes easier :). I have not seen any progress in making Swift officially supported in the main FlatBuffers project. PRs still welcome :). Did anyone look at the differences between @mzaks and @TonyStark106 implementations? Looks like the latter actually works with flatc, so would be easier to integrate into this repo.. @kevzettler this thread is about converting a JS object to FlatBuffers, akin to the \"object API\" that now exists in C++. This would make a lot of sense for JS.\nParsing JSON is a different matter (even if JS they are closely related). Implementing a parser and a binary generator is no simple matter, certainly a lot more complicated than an object API (in JS, the latter would be more generally useful, since it already has a JSON parser).. No it isn't available yet. If you see @evanw 's comments above he already had something like that going, but it never made it into the official FlatBuffers project. It be great for someone to pick up his work and make a PR.. You can add it :)\nWorth noting that there is some basic version of this functionality already in for Java/C#, which still in progress being refactored, see e.g. https://github.com/google/flatbuffers/pull/4055 and https://github.com/google/flatbuffers/pull/4047. @wesm I believe @rw has been working on a Rust port, it be good to check with him.. @josephDunne @hollinwilkins Thanks for continuing this!\n@rw want to chime in how this relates to your work?. @rw?. @rw this is awesome news! can't wait :). The grammar is severely out of date, it needs to be updated.. union values out of range is now an error: https://github.com/google/flatbuffers/commit/9d01bfaea3befaa0e12b0b310cadc1ed036ecfe3. Closing due to inactivity. Feel free to re-open.. @atamgp which of @rw's proposals sounds like the best solution?\nWho can take this on?. @mikemccarty-vertex I am not sure that those are the same problem, but they are similar, yes. It is a question of the code generator not calling the appropriate namespace generating function, but just using the name directly.. @mikemccarty-vertex I don't think this is a parser problem, it is a code-generator problem. To fix, find where in the code generator it is outputting the type for the request & response. If it is not using WrapInNameSpace in is likely incorrect. Same for the other languages.. @mikemccarty-vertex in idl_gen_grpc.cpp I see class JavaGRPCGenerator : public flatbuffers::BaseGenerator ?. @dopsun There is a plan, but noone has worked on it yet.. Ok. Again, not familiar with Maven. Are you saying there is a single key for all of Google that we likely already have? Or does it support sub-domains?. @jchensc I agree that is a bad situation. Again, I don't use Java / Maven, so I don't know what exactly is involved, but I can ask around.. Ok, ongoing progress here: https://issues.sonatype.org/browse/OSSRH-35510. It should be available now or soon: http://repo1.maven.org/maven2/com/google/flatbuffers/flatbuffers-java/1.7.2/\nLet me know if you are successfully able to depend on this.\n. The updated pom.xml: https://github.com/google/flatbuffers/commit/f3f113b24a9b14b8d288ef80f66f9743ae518bf7\n. Ok, it is now visible: https://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.google.flatbuffers%22\n@dopsun @jchensc @sdeleuze @loveyoupeng can any of you confirm that this solves your dependency problem?\n. Ahh lots of red, the best kind of diff :)\nI declare this issue resolved them.. Closing due to inactivity. Feel free to re-open.. Closing due to inactivity. Feel free to re-open.. @Guillaume227 No, there's no plan to support nested vectors. We could, but it is a significant undertaking of supporting all languages. We recently added support for vectors of unions for example, but that has yet to be implemented beyond C++.\nYour schema can of course currently parse if the data is:\n[{ x: 1, y: 2}, ...]\n\nNow, there's nothing stopping us from also parsing [1,2] if we're expecting a s 2 struct field, since structs are often used as fixed-length vectors.\nThe only reason why I'd be hesitant is because there are a lot of possible JSON formats out there that would need some massaging to fit into FlatBuffers, and I don't want to support all of them. Besides parser complexity, there's the possibility that errors become more obscure.\nI'm fine with this one though, feel like making a PR for it?\nThe example @bbczeuz gave is a string / double pair, which means it would have to a vector of a table of 2 such fields (strings are not allowed in structs).\n. Probably simplest to add a test in test.cpp that has its own schema, similar to e.g. EnumStringsTest(). Status?. @vglavnyy would you like to finish this PR?\nNot sure what you mean by testing in Release.. asserts are not supposed to run in release, but I suppose if you really want to that can also be made overridable.\nI don't think preserving formatting is important.. FLATBUFFERS_ASSERT would be most consistent.. Yes, go ahead! :). Status?. Status?. @rw can this be merged?. @rw ?. @rw: I presume you mean GAE?\nI presume this is for testing the unsafe stuff works? Surely there's got to be an easier way to do that?. ping @rw . Thanks!\n. Yes, you actually need to write the bytes contained in that object. See the StackOverflow answer.\n. I signed it!\n. Thanks for the comments, @evolutional !\n. Ok, changed Accessor to IFlatbufferObject.\n@evolutional, if you're cool with the new API and the performance implications, I'll merge this, and we can do further tweaks in follow-ups when needed.\nI think I first want to seperate C#/Java codegen.\n. Thanks for reporting, will look into it.\n. Thanks!\n. Thanks, looks good :)\n. Status?. Closing due to inactivity. Feel free to re-open.. Status?. @royalharsh : it seems that PR is currently not building. It be great if you can resolve whatever build/merge issues there are, and then have someone from the grpc team review it (I am not part of that team).. @kapilsh your help would be appreciated!. @kapilsh yes, that seems like the right starting point.. What's happening to this PR?. Still 145 commits.. you need to rebase.. Status?. Sounds like the latter would be less duplicate code. But yes, as long as the old behavior works unmodified.. Closing due to inactivity. Feel free to re-open.. Closing due to inactivity. Feel free to re-open.. I made my own fix that at least makes the test string case work for Java and C#. Likely needs further work for scalars: https://github.com/google/flatbuffers/commit/8468ea1ab410ffaa1eed99750e4c77b496b72fd5. This should be fixed :). Status?. Status?. It seems like generated code is back in, but there's still a lot of code in there for the old approach?. Closing due to inactivity. Feel free to re-open.. Closing due to inactivity. Feel free to re-open.. Neither this nor #4047 have been merged yet.. . CI seems to be passing at the moment, and this code hasn't changed in a while. Are you sure you are on the latest master, @MrSampson ?. Like I said, I cannot reproduce that error locally, as it was fixed a long time ago.\nIf you look at e.g. your first error which points to line 599 and complains about Monster::UnPack.\nIn my current version, those functions appear on lines 707 and 765, and have a different signature from yours, as well as taking care of the unused parameter problem:\ninline MonsterT *Monster::UnPack(const flatbuffers::resolver_function_t *resolver) const {\n  (void)resolver;\nSo somehow your code is not up to date.. not sure why.. Closing due to inactivity. Feel free to re-open.. Status?. This is probably obsolete now that this was merged: https://github.com/google/flatbuffers/pull/4232. Status?. @lp35 : I do feel these templates should sit in their original namespace. Which means it is up to the user that if they want to generically call Create, this will only work for tables in the same namespace.\nI don't think it is a good idea to suddenly put all these Create functions in e.g. the flatbuffers namespace, do you?. You're right, the C++ FlatBuffers API was not designed with highly generic use in mind. But it would be nice to fix that where we can.\nWe already have a Create method inside the generated type: it is called Pack. That should work in your use case, right?. Cool! So I guess this PR can be closed?. https://github.com/google/flatbuffers/commit/e9f1f4d9b7fb41f9f66a24ef03ccbea1780d2674. It sounds like this PR is not needed anymore, or?. @smiles who set up the Android CI.\nSetting up the Android SDK & NDK is a bit more complicated than just apt-get I'm afraid. Best is to get the SDK, then open the SDK Manager and make sure API 25 and the NDK are selected.. As the first error states, we don't support that GCC version.. @fbenkstein yes, I agree it looks like this has been addressed, thanks!. PR was merged.. Closing due to inactivity. Feel free to re-open.. Closing due to inactivity. Feel free to re-open.. Do we still need FLATBUFFERS_TRACK_VERIFIER_BUFFER_SIZE in CMake if we apply the fix I suggested above (to make the verifier field non-conditional)?. no, I meant removing it from the field declaration (and possibly from the constructor initialization, the lack of which probably generates that warning). But keep it where the variables actually get used. The idea of that #ifdef is to not have to have the overhead of tracking the size when not needed.. I didn't say I did not want to upgrade CMake.. if that is what is required, then we can do that. I was just hoping it wasn't necessary.\nIf it is order dependent on Windows, we may be running into https://en.wikipedia.org/wiki/One_Definition_Rule issues.. if that is the case, then I agree we have to set that flag globally. Is there no way to set it in a portable fashion prior to 3.0.2?. Closing due to inactivity. Feel free to re-open.. C++ / Go are supported, Python is in progress, other languages no progress yet.. @linwukang for Python:\nProgress on the grpc side of things: https://github.com/grpc/grpc/pull/8063\nOn the FlatBuffers side (stalled for the moment): https://github.com/google/flatbuffers/pull/4014. Are you passing a flatbuffers.Long value to this function, or something else?. Yes, it sounds like 999 should instead be a flatbuffers.Long.. Ok, thanks :). Thanks for the fix.. see errors above though.. As the message says, there's a different commit author (ygli) from the PR author (leeygang) and both need to sign.. Yeah, it should not output that function for long/ulong types, which I think I just fixed: https://github.com/google/flatbuffers/commit/d1e8899310d8e8758d9900628de448a3a43bd050\n. Status?. The CI error is indeed not your fault, fixed that independently.\nThanks!. See my earlier comments on how to modify this PR.. I've clarified my comments, sorry :). Looks like you are correct on the upstream of the go generator, I didn't know we had to translate this from Go. So, ok, don't worry about upstreaming that. Go ahead with your final modifications.\nAnd no worries, my Japanese is much worse than your English ;). Yes, that's a bug.. it should detect there's no arguments for the 3rd constructor and not generate it.\nJust pushed a commit that should fix it.\nAs for it taking 12 bytes, yes, it is not optimized for empty files :) That is 4 bytes of the root pointer, 4 bytes of vtable offset, and 4 bytes of vtable.. @WiSaGaN This is the commit I was referring to: https://github.com/google/flatbuffers/commit/5fd0fefab644caa2bb395cda79fbee6103035818\nThat just doesn't generate the per-field constructor if there are no fields (the 3rd one in the example above). What error are you getting, exactly?. @WiSaGaN oh weird, that commit somehow got undone by later refactors.\nI've re-applied the fix here: https://github.com/google/flatbuffers/commit/7dd5cfb5103a0aeba126c663fc1ab99ff3a450a0\n. @WiSaGaN yes. Though feel free to use master or cherry-pick now if you need it, as there's no definitive date for versioned releases.. @nicholasbishop argh.. that is different problem from what my commit fixes. An empty struct in C++ will get sizeof(T) == 1 which fails. This problem was also in the original commit.\nI'll create a fix.. I'm realizing this is a larger problem. C++ has the rule that a struct has a minimum size of 1 (such that an array of them doesn't have 0 size, and have them all at the same address, which would break all sorts of code). Now I can adopt that same rule for FlatBuffers (so far we haven't explicitly dealt with this issue), but now this problem would extend beyond C++, and every language would need to know to write a single byte when serializing an empty struct. This seems fragile to me.\nAn alternative is to define empty structs as being zero in size (and update the above assert to accommodate for that) but this will be incompatible with C++, and thus also fragile.\nThe simpler option is of course to simply give an error on such structs, which I would prefer. It sounds like that would make you guys unhappy though.\nFact is, empty structs, though accepted by the parser, are currently not correctly supported. Other languages, when you ask them to serialize an array of empty structs, probably currently write 0 bytes. So turning it into 1 byte would be backwards incompatible for the format, which we can't allow. So disallowing them seems to me the only viable option.\nDo you have strong arguments why we should support them?. @nicholasbishop when you come across such an empty struct, can you omit it?\nOk, I'll add an error for now.. They're now disallowed: https://github.com/google/flatbuffers/commit/160e8f2fdc9d5989e652709fae3fac0bd9aaed14. Can you describe what you're trying to do here?\ncpp_generator.cc is a file copied from the GRPC project, so it would be good to submit your change there first.\nmonster_test.grpc.fb.cc is a generated file with changes.. yet I don't see changes to the code generator in this commit.. Apologies, didn't spot you had also changed the schema. I can merge this for now, but like I said it would be good to upstream your change to cpp_generator.cc to the grpc project, otherwise this problem will come back next time the file is updated.. Thanks for the fix!\n(The Windows error was fixed independently).. I agree that would be nice. And such accessors could assert() on trying to access the wrong type. Not sure about them being templates though, union_name_subtype() to me sounds nicer than union_name<mynamespace::subtype>(), though the latter can be of use in other templates.\nOpinions?. We can't call the templated version union_name since that is already used for the current accessor, which we have to keep for backwards compatibility.\nThe templates would have to use template specialization to implement the various sub types.\nA PR would be welcome.. Yup that sounds good!. I don't think there's any way to avoid this, unless you want to get into the business of common prefix stripping, i.e. union { n.A, n.B } can be simplified, but union { A, n.A } can't.\nI'd say, leave it as is. If people feel that they need to use namespaces in unions, then it makes sense for them being in the name.. I think we can close this then.. please open a new one if there's something to change about @FrankStain 's implementation.. Nice fix, thanks!. I'm not sure what the solution is here. Rather than trying to detect a library's availability (which, unless CMake has that built-in, sounds tricky), I imagine we do this through a CMake or environment variable which can default to the common case, and allows overriding for other cases?. It looks like it should be using the unsigned version of _strtoi64 for that case instead. Are you able to make a PR?. Hopefully fixed in: https://github.com/google/flatbuffers/commit/29574282a283ddc7904d096d27b783b794da7e91. Thanks! That seems like a good idea indeed. You say it is measurable, do you have any numbers for this performance improvement?. Can you share some data on how much improvement you saw, and with what kind of data? On what platform? It is essential that this is not a performance degradation for buffers without any large data items.. The biggest thing it is being used on are strings and structs, but it is mostly used on scalars and offsets, which are often 4 bytes.\nIn fact, if this is a speed bottleneck, I recommend you add something like:\ntemplate<typename T> void push_scalar(T t) {  // Note: t must already be little endian data.\n  auto dest = make_space(sizeof(T));\n  *reinterpret_cast<T>(dest) = t;\n}\n\nand call that from PushElement instead of push. That will generate special purpose copying code for all scalars and offsets (which is the majority). The rest of all the calls to push (which includes strings and structs) can then keep calling your memcpy based version.\n. Ok, I'll benchmark this myself at some point.\nWill leave this PR open just in case someone else can confirm it is a speed up for average data and/or wants to try out my suggestion above.. push has already been further optimized for single scalars, see push_small. Thanks!. So the problem is that there is an extra . in the C++ name compared to the Go one?\nI'm not familiar enough with GRPC to know which is correct. @ctiller can you point us in the right direction?\n@JRonak who made the Go GRPC implementation for FlatBuffers.. Ok, quick fix here: https://github.com/google/flatbuffers/commit/8c1a723ba55d1574590eba801d64afab9c49e017. Some of these fixes are in generated code files, yet I didn't spot any changes to the code generator (idl_gen_go.cpp). Without changes, these fixes will get lost in a future commit.\n@rw any other comments?. We'd prefer to simply generate well-formatted code, so these changes should be applied to the code generator.. If you don't want to fix those, simply leave the generated code files out of this PR, and we can still merge the other files.. Ok, thanks!. My guess it is because it is using LoadFileRaw which exists only in the .cpp.\nThe .cpp was introduced later, so that is why it is inconsistent. Frankly more of it should be moved into the .cpp.\nDo note that these are utility functions for the rest of FlatBuffers, they are not intended to be core API functionality. You can use them, but \"loading a file\" is not something you need FlatBuffers for.. sample_binary.cpp is such an example, what would you like to add?\nWe don't have helpers for writing a buffer to disk, etc, since there are so many ways you can send/store/receive/write buffers.. Yes, there's no way around the lack of nesting, this is required for the encoding to work properly.\nUsually it is is not much more code though, i.e. it is a question of collecting offsets (in e.g. a vector) and then using those offsets when constructing the parent.\nOne thing would be to make sure to use vectors where possible, i.e. the above schema should be:\n```\ntable FB_Subsubdata {\n  dataContainer:[FB_DataContainer];\n}\ntable FB_Subdata {\n  section:[FB_Subsubdata];\n}\ntable FB_DataPacket {\n  db:[FB_Subdata];\n}\n```\nThat should simplify your code a lot.. Good to hear.. good luck with your project :). There is no automatic way in Java to copy a table from one buffer to another. (In C++ there is CopyTable when using reflection). You'll need to manually re-create b1 in builder, e.g.:\nB.startB(builder);\nB.addField(b1.field());  // for each field.\nB.endB(builder);\nOne alternative is to store all of bytes in the new builder using a [ubyte].. \"but still could not make it\".. can you provide details of any problems you encountered?. builder.putB(bytes);  int o1 = builder.offset();\nThis doesn't work, because o1 points to the start of a buffer, not the start of a B. You're also adding the offset as if its a vector, but it doesn't point to a vector.\nInstead, if you want to do it this way, represent b as b:[ubyte] in A. Then write the B bytes as a vector. To read it, you get the bytes out again, and then use GetRootAsB to access it.\nBut like I said, simpler would be to reconstruct B as in my original post.. Yes, you may assume this. One caveat is that the data that you're copying is little-endian, so this would not work on a big endian machine (e.g. Xbox 360).. By default, the parser works best if the type precedes the value, like you do here. We then actually have some special purpose code to deal with the type being after the value, which is what you're getting the error from. So somehow (in the last test case), it is not recognizing \"bar_type\" : \"Bar1\" as a union type, even though it was fine with the very similar baz case. Can't tell why, I'll debug it locally, thanks for the test case!. I believe I've fixed it here, thanks for reporting! https://github.com/google/flatbuffers/commit/eac2905568ec764f2d6fb0864ff95acec419d163. You're correct.. that makes no sense :). Hah, didn't think we'd ever compile on a 16bit platform. I'm impressed it even fits :)\nWe can make that variable (and max_tables_ and max_depth_) into uoffset_t (which is a uint32_t), which is kind of the size_t of a FlatBuffer (which is limited in size to 32bits anyway).\nCan you make a PR for this? Maybe #ifndef ARDUINO or something more generic for those 2 includes (what causes them to not be present?) . @mikkelfj you may be interested in this PR, since we want to add support for other languages eventually.. @bei, oh, and besides the comments above, some minimal additions to the docs would be good. This can be very minimal (in Schemas.md) since it is only supported by C++, and should be noted as such.\nAlso, JSON parsing/generating would be awesome in this PR or a followup.. @mikkelfj : glad you agree! but yes, besides efficiency, a big motivation was that a pair of type/union would require an entirely new kind of type in FlatBuffers, whereas this doesn't.. @mikkelfj : I see your point, but such a special encoding requires significant new support code across languages. The beauty of the current implementation is that it is both efficient and a very minor extension of the code that is already there.. The reason we have this \"superior language support\" is because we've had lots of nice folk contribute these implementations.. which also means that not all APIs are equally complete. So the lack of \"serialize byte array all at once\" is not because we think it is a bad idea, it is because noone's added it yet.\nWhich language are you using? I'm guessing Python?\nI wouldn't know what a fast way to deal with byte arrays is in Python, anyone?\nAre you able to make a PR for this?. @rw who made the original Python implementation.. That be great :). @wrigby : cool that you made it work, but this type of low level work should really be done by the API itself.\nIf you only generate such functions for byte arrays there should be no endian issues, but I guess people may want to use it for other arrays as well.. That indeed sounds like a problem.. can you make a PR?. File it to this project. Fork https://github.com/google/flatbuffers/ on github, clone your fork locally, make fixes, push to your fork, and your fork should have a \"create PR\" button on top of it on github.. @rw does this look good to you?. We definitely want to support all features in all languages, but yes, usually C++ is first.\nI'll see if I can wake up @rw :). Thanks!. Sorry, the root of a buffer currently always has to be a table.. What does this workaround look like?\nI mean, you can do this pretty easily by taking the Offset>, get the offset value out of it, and then create an Offset out of it, and call Finish(). Then again do a similar cast when reading.\nIt is possible to support vectors as roots, but it is relatively complicated because there's a lot of code that assumes it is going to be a table, in many languages.. Yup that's what I had in mind.. it works without casts even.. easier than I thought. Maybe this is something we can start supporting in the schema... root_type [int] :)\nAgreed on the pre-allocation. Can you make a PR for that?\n. No, it might be useful for direct binary use as well. Another one that sometimes may be useful is to have a struct as root.. I'm not entirely sure what you're trying to do.. you say, \"I build nodes into an instance of fbb\", but then the code doesn't use fbb but instead appears to be copying data from a FlatBuffer into a C++ data structure.. which I am not sure how that relates to creating a vector of tables.\nYour loop below there should work, assuming serialize returns the result of a call to CreateNode. Both the individual nodes and and the Nodes table should indeed go into the same FlatBufferBuilder to be able to refer to eachother.\nIf I missed what you're trying to do, try posting an as complete as possible code snippet, and I'll point out where it is going wrong, if anything.. @rw @wrigby \nTo what extend would we want this to work independently of numpy? Or is numpy so ubiquitous in Python that if you're using large arrays you're likely using it (or don't mind using it)?\nWhat about use cases where people store other sorts of buffers (other serialized formats) inside FlatBuffer arrays?. I agree being able to use it in-place will be great for Python. We may need to agree other use cases later. Agree with moving code out of the code-generator and into the library is generally good.. especially if it means we can do without the extra flatc flag.. Thanks!. If there really aren't any performance or other downsides, I'm ok with it.\nAre you sure it has no consequences though? In most languages, supporting some kind of interface implies a dynamic feature like a virtual table. In C++ it is all done at compile time. In C#, not sure which is used?\nAs long as a partial struct is as good as a struct when not extended, I suppose.. You're right, that sounds like a low risk :) Go ahead then. Any C# users please complain if needed :). B seems cleaner, and I am fine removing the attribute, because as you say, it should affect very few people.\nIn fact, here's the original issue: https://github.com/google/flatbuffers/issues/3507 and PR: https://github.com/google/flatbuffers/pull/3742\nOne curious thing is that @evolutional mentioned this change as not being backwards compatible, I wonder why so?\nIf it is indeed so, we should just go for A.. This sounds like a positive change, if the enumerator is about as efficient the current interface, and doesn't replace the current interface.. Yup, that is stale.. a fix would be appreciated.. We don't really have a good solution for streaming a single FlatBuffer, i.e. there's no easy way to make sure access to a partial buffer is valid. This is because we refer to data via offsets, which may point anywhere within the buffer.\nAs for your second scenario, if RecyclableMemoryStream manages byte[] arrays, can't we just get access to those? I'd be worried that having to go through an interface like https://msdn.microsoft.com/en-us/library/system.io.memorystream(v=vs.110).aspx would pose overhead, since FlatBuffers does a LOT of scalar reads, that would all have to go through ReadByte (many calls), or Read (extra copy into byte[]).. So.. you're saying that if the FlatBuffers code tries to read from a certain byte in the buffer that hasn't been streamed in yet, it is going to block until that byte is read?\nI guess that is possible, sure, but efficiency wise seems to potentially nullify the advantages of FlatBuffers, which assumes reading values on fly is very fast.\nIt be great to hear some numbers on what all this costs.\nAs an alternative, have you looked into using the equivalent of mmap() in C#? That could give you the advantages of streaming without its overhead.\n. Ok, I thought the whole point of using a streaming interface was to be able to start reading it as bytes are streaming in.\nIf you're only going to read the buffer once all bytes have been read, they need to all be in memory, and thus there's no advantage to streaming. You'd be better off with: prefix with size of buffer, allocate buffer, stream all bytes into buffer, read buffer (with existing API).\nI know discontiguous memory has some advantages, but it is too much of a cost on FlatBuffers to be worth a new API for. You're better off modifying your streaming to work with a contiguous buffer. Even copying is going to be faster.. This was all to avoid clashes with any field names. But since most of these methods now sit in a struct that is not inherited, we may actually be able to rename them to normal C# function names.\nThe reason this hasn't been done is because the code that generates calls to these functions is shared with Java (idl_gen_general.cpp), and we wanted to avoid yet more conditionals in there. What my plan was to actually split up that file in two, such that further divergence of the two code generators becomes easier.. No problem, it is good to give these visibility.. Agreed.\nIf anyone wants to take this on, it should be a matter of copying idl_gen_general.cpp to two files, and then removing/simplifying/inlining as much as possible the code in either one now that the choice of language is constant.\nThis has to be done carefully to not break the existing functionality, though hopefully running generate_code.sh gives enough coverage to see nothing has changed vs the test code that is git.. Thanks, will fix.. Fixed.. Can you please run generate_code.sh in tests/ (and add any changes to this PR) so we can see how this affects the generated code?. I mean add the changes to this PR as a commit. That way we can see what code has changed as part of the diff.. Yes, that only has the code-generator change in it.\nIf you run generate_code.sh in tests/, do any files show up as changed in git? If not, maybe your change doesn't affect any existing tests, but it sounds like it should.. The split view also shows all lines changed.. did you maybe change all the line-endings or something (are you on Windows maybe?). Thanks!. Not sure who created that, but it isn't maintained by us. Maybe contact DevZH?\nShould we have an official one? Is there a way Nuget can pull from our official repo automatically?. @petertiedemann thanks for the help, because I am definitely not familiar with nuget.\nSo far, we use AppVeyor for CI only.. it does not build any distribution files. Even if it did, wouldn't we want to to only update the nuget package for major version releases as opposed to every commit?\nCurrently, when I release a new tagged version, I do an npm publish manually for JS. Other languages do not get an automatic package currently, I have yet to set this up for Maven (if anyone wants to help, that be welcome).\nWe may have a nuget account, but Google has a lot of teams.. will have to see who owns this.\nAs for your change, we can't move to the VS2017 image, since it drops support for VS2010, which we're still testing against.\nHelp making our project files \"nuget ready\" would be appreciated indeed.\n@evolutional who has worked the most on C# and may have an opinion.\n. @evolutional thanks! Do either of you want to set up these projects such that at release time all I have to do is run some kind of command? I'll go dig up our account.. why was this closed?. FlatBuffers is not thread-safe by design. It is too low level that access level thread-safety would make sense, so it is up to the user to encapsulate if needed, much like other low-level data structures.\nIn C#, a ByteBuffer is part of what you use to read a buffer, so should be covered by whatever mutex governs the buffer, which would mean inthelper and friends should not be a problem. I agree that people may make the assumption that you can share a ByteBuffer between threads, since its use appears to be read-only. It may be helpful to document this. I'd say though, making assumptions about classes not updating their state is a very error-prone thing in general, a lot of classes employ caching techniques.\nIf we wanted to commit to making the accessors safe for multi-threaded reading, we could think about using TLS (e.g. https://msdn.microsoft.com/en-us/library/dd642243(v=vs.110).aspx) for the variables we write to? This may be more expensive what is there now, which would defeat the point.. Not officially no, but there are some C# benchmarks in here: https://github.com/google/flatbuffers/issues/55\n. The root type is declared in the schema, see: https://google.github.io/flatbuffers/flatbuffers_guide_writing_schema.html. There isn't, at least not from the command-line. A temporary workaround may be to put each table that can be a root in its own schema file, and then include one in the other when necessary.. Thanks, that would be great. I am not that familiar with npm, presumably you can specify the version you want to depend on? How do we make that refer to major FlatBuffers release versions?\nThe comment states he'd be happy to give us control of the name, so maybe we should do that. @evanw, do you mind to release the flatbuffers npm name to us?\nFor author.. @evanw wrote the original code, but he may not want to have people bug him about it anymore. Maybe you can simply make it \"The FlatBuffers project\" for now, or something generic.\n. @evanw: thanks.. just created an account \"aardappel\", please transfer it to that.. @dictav : it is possible that a different version of flatc generates code that does not work with flatbuffers.js.. not super likely, but it is better if they are in sync.. Thanks @evanw !\n@dictav : we now need to replace the content on the flatbuffers package, which still refers to @evanw 's alternative implementation.. @dictav : added you, thanks :). Thanks!\nSo what is now the process for releasing another version with this?. Thanks! Made a note of that for when we do the next release (1.6 should be out shortly). Please run sh generate_code.sh in tests/, and add the resulting changes to the PR, so we can see how the generated code has changed.. Thanks!. Hmm.. not sure.. I feel that doxygen for all methods we generate would create massive amounts of code, and also bloat up our code generator.\nThe object API unpacks things into objects, so that that involves memory allocation is hopefully no suprise. Ideally people use the root UnPackMonster that returns a unique_ptr.. The API docs already tell you to use the  UnPackMyRoot version.. but yes, there are a lot of publicly available methods that may or may not be useful to the programmer. Not sure what to do about that.\nIf anyone programs FlatBuffers by just using IDE auto-complete rather than the docs, then there's a lot that could go wrong :(. And identifier may not contain -, as that wouldn't work in any of the programming languages we support. Identifiers are [A-Za-z_][A-Za-z0-9_]*\nAnd yes, I should probably document that.. Since the names of enums are turned into identifiers in programming languages, no there currently is no way to do that.\nWe'd have to add an option to turn - into _ or something when trying to parse enums.\nFor the moment, the only way is to change the JSON ahead of time.. @TGIshib (who I believe wrote this code): it appears there's some invalid code generation going on inside GenKeyGetter (in idl_gen_general.cpp) that does not work for uint keys... 1) Do you have the latest GRPC? It seems that the current signature for Deserialize has 2 arguments, which changed recently.\n2) I see there are memory errors, but those could also result from your usage. For example, you're calling std::move on a raw pointer (the result of UnPack), which I am not even sure what that would do. You're better of calling UnpackSampleRoot instead, which gives you a unique_ptr, which is probably safer.\nAlso, not sure why you're using the object API, the standard API is much more efficient for your use case.\nWhat would your suggested change to ObjectUnion::Reset() improve? You'd be calling delete on the wrong pointer type. These types have no vtables to dynamically call the right destructor.. Your SIGSEGV you posted is happening while reading a FlatBuffer, so has unrelated to the union. You may fist want to find out why that is happening. It sounds like the memory pointed to by request is somehow not valid, or contains something of a different type.\nWhy ***Union is not extend \"flatbuffers::NativeTable\"?\nBecause unions are not tables. They are a pair of a type and a table pointer.\nDoing a delete nullptr is valid C++ (see e.g. http://stackoverflow.com/questions/6731331/is-it-still-safe-to-delete-nullptr-in-c0x), so I am not sure why we should add an if-check just to please valgrind (are these messages coming from valgrind?).\nI still don't understand where your leak comes from. Do you have more information?\n. Thanks @lebdron :). Could you comment on or fix the other issues?. Ok, will merge this to fix the problem, but the code could use further cleanup and testing (see my comments).. I made my own fix that at least makes the test string case work for Java and C#. Likely needs further work for scalars: https://github.com/google/flatbuffers/commit/8468ea1ab410ffaa1eed99750e4c77b496b72fd5. Partial answer here: https://github.com/google/flatbuffers/issues/4169\nI guess the other problem is that it has {} around the value, which we can't parse as an enum value either,  as that is meant to represent a sub-object. Looks like your best bet is to modify the JSON if possible.. We already support parsing numeric data inside strings when the type of the field is numeric, so both of those should work with the type set to int.\nWe could additionally support parsing 70 as a string for a string field.. though using an int field would obviously be better in this case.. It sounds like you'll have to make the field a string, since this custom unit parsing is something you'll have to do in your own code.\nLike I said, I agree that we should be enable parsing non-strings as strings for string fields. This should be an easy change to make if you want to create a PR.\nCurious that it is in your data though, since 70 is not valid JSON.\n. You'll need to actually send a table that contains the union as part of the message:\ntable Request { request:RequestMessage; }\n. We have no way to parse generic untyped JSON data at the moment. All you can do is skip unknown fields (with --unknown-json).\nSupport for keeping this kind of data likely comes once we add FlexBuffers (https://google.github.io/flatbuffers/flexbuffers.html) support to the JSON parser.. The second is definitely the way to go. The way that would work, is to define the field like:\nextra:[ubyte] (flexbuffer)\nThe parser would notice the tag and switch to constructing a FlexBuffer for the following JSON value, and store the result in the vector. It should also generate an accessor for that field that directly returns the root as a FlexBuffer Reference.\nThis change is a bit bigger, since there is no FlexBuffer JSON parsing yet. It's not particularly hard since it reuses a lot of the current parsing code, but not trivial either.\n. Yup, you're totally right that this would be desirable (and possible), we just haven't gotten around to it. Are you willing to have a crack at it?. Thanks!. Thanks.. can you make this fix in the form of a PR?. You're right, that would be safer. I guess we test with VS2010, and this is a new error for VS2012. We should really set up tests with later versions as well.\nCan you make a PR for this?. Pull Request.\nFork FlatBuffers on GitHub, make change to your fork, press \"create pull request\" button.. I will then merge it into the project if it is all ok.. Thanks for your fix.. and the test update!. Thanks, looks good now!. Parse should really give an error on an empty file, since that is not a valid JSON object, and does not parse anything (and thus does not produce a FlatBuffer.. Yup, a simple if-check before that line with an error would fix it. Mind making a PR?. Thanks for the fix!. Nice improvement :). Hmm I wonder on what platforms these are needed.. OS X?. This could be done more cleanly if we knew where these flags are needed.. but for now this is ok.. It looks like in GenerateGRPC path is ignored.. if you were to prefix path + to uses of file_name this should work.. can you make a PR?. I merged it, thanks!. @Peterfly : because we support compilers that don't support enum class.\nTo be fully correct, the parser should check all enum values are in range, which it currently may not.. Great fix, thanks!. Acknowledged.. will fix.. As noted here, it should already give an error when you pass a vector of structs to CreateVector: http://stackoverflow.com/questions/42468489/flatbuffers-error-when-creating-vector-of-struct/42474167#42474167\nIt could potentially be silent for structs of size 4, which was fixed here: https://github.com/google/flatbuffers/commit/349a3912081c9d9e7f5f065122fe0194cc26aa5a. Yes, this is possible, we just don't have a good example for it in the docs, which we should fix.\nThe gist of it is (haven't checked this code):\nfbb.StartVector(16, numelems, 8);  // FIXME: should not require these.\nfor(..) vertex.createVertex(......);\nvec = EndVector();. This now finally has been properly documented: https://github.com/google/flatbuffers/commit/262e1d7bf962dc796e47b2186bad6944515ac833. Agreed that there's no reason this can't be supported for structs. I guess we originally didn't think of it because strings are the most common key type, and those aren't available in structs.\n\nAre you able to make a PR for this? Should be a relatively simple change.. Where exactly does it segfault (did you run it under a debugger?)\nIt be best to make the Java/C# generator just ignore structs for the moment, with a comment saying so.. The FlatBuffer encoding for string is (and will always be) UTF-8. I believe that is common among serialization/storage/networking formats.\nFor the moment, you could use [short] (or whatever type it is equivalent to wchar_t on your platform, since it is platform dependent) to store your wstring without conversion, if you wanted. You'd then cast your wchar_t * to a short * and call CreateVector instead of CreateString. You wouldn't be storing the 0 terminator unless you did length + 1. Maybe add a static_assert that these types are indeed the same size :)\nNot sure if it be worth-while to add a wstring type to FlatBuffers directly, because a) the datatype is non-standard, so still would need conversion depending on which underlying  type you choose (oh, and additionally be subjects to endianness conversion). Conversion means losing in-place access which is what FlatBuffers is about. And b) most people would regard it as a wasteful / non-standard way to store strings.. Thanks!. Thanks, nice improvement!. Thanks!. I haven't seen the if in EndianSwap give that warning before, I wonder why it appears for this new code. Are you compiling at a higher warning level than whatever CMake defaults to?. Thanks!. Please run sh generate_code.sh in tests, and add the resulting changes to your commit.\nPlease adhere to the style of the current code, e.g. the * goes next to the name, not the type.\nWith your current changes, EnumNames is still generated for non-sparse enums, but now goes unused.\nI'd prefer it if you generated the switch only for sparse enums (since it is less efficient/generates more code). You could add a range-check to the array based version as well. I think it would be nicer to return a nullptr if the name is out of range, letting the client code deal with out of range values.. What exactly is the error that happens when generating JSON? Maybe the generator relies on the last namespace when it shouldn't, so this could be fixed in an even more robust way.. The mixed parsing is because flatc accepts a set of file and it discovers whether something is a schema or json by parsing it, rather than relying on the extension or something.\nIt does sound like it is working as intended. If anything, the fact that specifying an unqualified root name works could be considered a bug :) Or maybe it would have been better if SetRootType asserts, rather than returning a bool that is easy to ignore?. Hmm, yeah I agree that is also problematic.\nSo it is not easy to completely remove that statement, since all other decls would have to check if this file doesn't have a namespace yet, which would have to happen in many places.\nI also don't think we can blindly remove the namespace at the end of the file again, since if it is a schema, things may refer to it.\nSo we have 2 options, both a bit hacky:\n\n\nSimply remove the last namespace if it is empty only at the end of JSON parsing.\n\n\nMove JSON parsing into its own function, like you said. Still call it from where the code originally was, because currently we apparently support JSON files with schema includes etc, which we may not want to break. Then, at the start of parsing, provide an early out for JSON parsing, kind of in this new order:\n\nECHECK(SkipByteOrderMark()); NEXT();\nif (Is('{') { ParseJSON(); return; }\nnamespaces_.push_back(new Namespace());\n. Reworked namespace handling: https://github.com/google/flatbuffers/commit/321a1c9dc0bb2205feb1bb9fd079546403aa852f\n\nNo empty namespaces are allocated or inserted anymore.. Thanks for the fix!. Thanks :). Yes, union types are always represented by a byte. The schema compiler should give you an error when using values like that.. this needs to be fixed.. I will make it generate an error. For now you'll have to manually ensure your values are inside the [0..255] range. It is fairly hard, since all implementations in all languages make this assumption. So if you changed it to a uint16_t, you'd create an incompatible FlatBuffers fork for your own use.\nWe could support this in the future by it being specified per union, but it is a non-trivial change as all  languages and parsers etc will have to support it.. Tables can have around 16K fields, but a table with hundreds of fields and only one set will be very inefficient, so no, not recommended.. The inefficiency is mostly the larger wire size, caused by big vtables for all those unused fields. If having 2K of unused data just to store a single value is \"no big deal\", then I guess so :)\nYou could also consider a hierarchical union, i.e. a set of sub-unions all <256 elements, with one root union to decide between them.. union values out of range is now an error: https://github.com/google/flatbuffers/commit/9d01bfaea3befaa0e12b0b310cadc1ed036ecfe3. Thanks for the fix. It is a bit verbose just doing it for this one function, so lets instead make it cover the entire file. This will make sure that any future occurrences are covered, and it will give us a spot to add additional un-helpful warnings.\nPut the start block at line.. 109 it looks like (after all the other ifdefs), and the end block at the end of the file.\nWe should also maybe add /W4 /WX to the CMake file, since any other warnings will not be caught otherwise.. Thanks!. Thanks.. can you make a PR to fix it?. Sorry, just saw you did.. merged, thanks!. Ok, will keep the casts as-is for now, thanks :). I'm not familiar with C++/Python interop, but really all you should have to do is convert the C++ buffer to a Python byte array, at which point you can use the existing API.\nYour call to PyCapsule_New only takes a pointer, and not a length, so clearly isn't constructing a byte array, so likely there isn't a way for the Python API to get at the bytes.. Yes, it IS a pointer into the underlying buffer. I agree the copy constructor should be deleted (or maybe just declared and not implemented, to support older compilers). Can you make a PR?. Merged, thanks!. Thanks, good addition! Since String derives from Vector, that should be covered too.. Thanks, nice addition :). Nice addition.. I was thinking that each language needs this functionality, and that maybe would should implement this generally for all languages.. but it doesn't look like there's much gain to that.\nSo I'm cool with this being merged, if @rw is.. Looks good now, thanks!. Thanks!. wait, why was this closed?. Yes, the functions with static were pulled out of the class, and I forgot to make them inline. Can you make a PR?. Yes, it is intended to be header-only.. Your buffer data does not appear to be a valid FlatBuffer. On the sending side, are you using GetBufferPointer ? If you are accessing the buffer by some other means you may be sending the empty part of the buffer along.. Verify only checks that a buffer can be accessed without going out of bounds, it has no idea what data is supposed to be in your buffer. It so happens that a buffer that starts with a bunch of zeroes just looks like a buffer with an empty root table.\nBut yes, we could additionally add rejection of 0-valued offsets in the verifier, that would catch these cases as well.. Verifier will now catch these kinds of issues: https://github.com/google/flatbuffers/commit/8b92122f33c2e2aa07e335341503ce19b4989abb. Yes, please do create a PR, that generally looks good, and the extra tests are very welcome.\nThings I spotted so far:\n\n< may have to be <= ? Same for >.\nKeep to the Google C++ coding standard, e.g. camelCase -> snake_case for variables.\nThere was a check in there for 64 bits which is now gone. Does the new code work correctly with (un)signed 64bit? worth adding a check.. If you look through old issues, I believe we have had people complain about this before, so it would be good to fix... but breaking everyone is not great.\n\nWould be good to get opinions from actual JS users, as I personally have no clue in terms of what would be the best path.. As someone with no experience with JS, I have a hard time evaluating whether this is a good idea. Anyone else want to weigh in?. Ok, this was deemed ok by a JS-using colleague, so I'm merging it! If this causes anyone problems let me know.. Sorry, had to undo this PR: https://github.com/google/flatbuffers/commit/b627b7c6c60f5fb24d73c8915b73e4d9e0fd176d\nThis fails in certain JS environments (exports being unknown, etc.). It may be the Closure compiler that is generating those errors.. Agreed one PR per concern is better, but it is not a big deal here since both are simple.\nAgree using length() is a nice optimization.. Looks good, one small nit.. Looks great, thanks!. There is already a force_defaults argument to FlatBufferBuilder for this purpose.. I'm not necessarily against this kind of functionality, but the cost has to be considered, especially if not commonly used. E.g. adding an extra argument to all add functions does not sound great from that perspective, and neither does generating a lot of extra code (even if hidden behind a switch it has cost).\nActually, (at cost to you) you can already force defaults on a per field basis by simply calling FlatBufferBuilder::ForceDefaults before and after. Not elegant but it works.. Yes, but remember those extra args will be added to all fields for all users of FlatBuffers, none of whom so far have requested this functionality :) I know, small, but at this scale it matters.. Thanks, this seems like it would be a useful addition!\nSince TypeScript mostly overlaps with JavaScript, this would be better an an option to the existing JavaScript generator (in IDLOptions), rather than a copy of the existing generator (since now we have have to maintain 2 of them).\nAlso please add some description to your PR?. Well, yes, you'd have to factor the code such to keep the amount of if-thens at a minimum. But sharing the code still seems very useful to me.. Yes, please do.. This PR looks pretty reasonable to me now. It appears to already have absorbed some changes from https://github.com/google/flatbuffers/pull/4244. @fasterthanlime are you happy with the state of this PR?. I agree tests would be nice. Can you translate JavaScriptTest.sh to TS? (and JavaScriptTest.sh). If possible, also add that test to appveyor.xml (there is already a JS test in there using node).. @fasterthanlime : thanks for checking. @krojew would be good to fix that too, though can also be in a follow-up PR if you want. Testing specific to TS would be good to have in this PR though.. This also makes some changes to the JS generated code, can you explain what that is for, and the consequences?\nOther than that this looks good to me. @fasterthanlime ?. @krojew The output from translating TS code shouldn't overwrite the JS code, and it shouldn't be in this commit. We want to be able to track changes to JS code seperately.. Thanks! I can merge, if there are no further comments.. A couple of issues when running this locally. \n when you run sh generate_code.sh it produces changed .ts files. This is probably because it is being run without --no-fb-import. We need to be able to run this script with no new changes.\n Your test script fails for me. I get permission denied on the first npm command. It would really be better if this test can run locally without having install and de-install npm packages, and just use the local files.\n* Renaming and overwriting the existing JS code is also very fragile. Maybe better to rename the .ts file so it doesn't have the same base name as the .js code.. Why? the JS test runs locally, and it be great to do that for TS too.\nStill, it needs to output the same code as generate_code.sh does. And renaming of the .ts is better than overwriting the .js.. Not being a JS/TS user, I guess I don't understand the requirements. Doesn't TS already have types in the source? Why do they need to be external, and why do they need to be retrieved through npm?. So why don't we also have that file in this git repo? If people make changes to the TS implementation, which presumably also changes the .d.ts, they need to simultaneously do a PR here and update files elsewhere? Wouldn't it be more convenient if that was all in one place?. So to make changes to TS, you have to make a PR to 2 repositories? Seems inconvenient to me, but if that is standard.. I guess so.\nEither way, maybe you can address my other 2 suggestions above (the non-npm ones). . @liukun thanks.. I was wondering that too.. @JoshGalvin I'm no JS/TS expert, but this may well have been inherited from the JS implementation. There were at least 2 attempts before (one is here: https://github.com/google/flatbuffers/pull/3750) to address this, but the last had to be rolled back due to incompatibility with some environments I seem to remember (Google Closure compiler)?. Thanks!. Nice!. yes, that is an unfortunate collision. The parser would be better off without hard keywords, which would be a good idea.. Hard keywords removed from the parser, which solves these kinds of problems: https://github.com/google/flatbuffers/commit/03e2899985849dbffa3178ae76c00288f7aca79b. Yes, this a valid use of FlatBuffers. Simply serialize a B, then pass the same offset to B when you're constructing A1 and A2.\nThere's currently no way to do this through JSON. That might indeed be nice to add.\n. We'd need to decide on some syntax for paths (or an existing one) that is clearly parseable separate from data. That should not be hard since most JSON data is quoted, so starting with a $ like in JSONPath would be fairly unambiguous.\nThe FlatBuffers JSON parser parses JSON directly into a binary FlatBuffer without an intermediate representation, so if you recognize a path a field value, besides parsing the path, the hard part is to then follow that path in the half-constructed binary. Not only is there no such code yet (you'd have to implement it based on the parsers schema information), you'd have to do it while being aware of the partial nature of the data. Of course that means you can only refer to objects that have already been parsed.\nIt would require a fair bit of understanding of how the JSON parser works.. so maybe a good start having a look.. I had no plans to add this.. but agree it would be nice to have. If someone wants to work on a PR that would be welcome.. That be useful yes! Especially since .tmbundle is something that works with a lot of editors out there I believe, so useful to create beyond just GitHub.. Tables have no \"position\" in anything, so they can be renamed freely as long as you don't mind breaking the users of the generated code. If a table gets renamed, this is compatible with existing binary data.. I think if --conform doesn't already do it, it can check that two types are binary equivalent. It doesn't even need a new attribute.. --conform tests 2 specific schema instances. So if for a particular field the type is different, rather than giving an error, it could test those 2 specific types for conformance.. Well, it needs to know that those 2 tables are supposed to be related. Are they both the root_type of the schema? If so, that's something we should support.. Can you make a PR to fix this? If you look at Parser::ConformTo, it would be a question of pulling out the code that compares two tables into a function, then also calling that function if root_type on both Parser objects is set.. The Java FlatBuffers implementation cannot parse JSON directly. You are passing a buffer containing JSON to code expecting a binary FlatBuffer file.\nYou must first convert the JSON to binary. You can do this with e.g.  flatc -b myschema.fbs myjson.json, or if you want to do it at runtime, you'd have to call the C++ JSON parser over JNI, which is more complicated.. Confirmed.. Yes, you could simply have an #ifdef in flatbuffers.h that selects the desired bit-width, that could even be set from the generated code if declared in the schema.\nI guess the reason we have so far only hinted at possibility rather than supporting it directly is because it generates multiple kinds of FlatBuffer binaries that are incompatible with eachother, which is generally an undesirable situation, and noone has needed FlatBuffers this big yet :)\nWe can't really deal with 32 vs 64 bit at runtime, since that would be too slow.\nIf we're going to be introducing a 64 bit FlatBuffer, I'd suggest making it on purpose incompatible with the existing format, such that you can't accidentally confuse the two, e.g.:\nCurrent FlatBuffers always start with a 32 bit root offset.\nA 64 bit FlatBuffer could start with a magic 64 bit value that cannot be an offset (e.g. 0xFFFFFFFFFFFFFFFF or something slightly more unique), followed by a 64 bit root offset.\nThat way the 32bit GetRoot() can assert at that value, and a 64 bit version can require it.. Oh, and we could support 16 bit FlatBuffers with a similar idea.. which could also use 8 bit vtable offsets for additional space savings.\nNot sure about 128 bits.. I'm going to utter famous last words and say noone will ever need it :P. I hadn't planned another release soon, are you able to work with current HEAD for a bit?. If we place them outside of the namespace, does that not force the contents of the FlatBuffers namespace upon users of the generated code? E.g. the FlatBuffers namespace contains an Offset, could that not clash with any user Offset?. Ok, makes sense! Are you able to create a PR for this?. Interesting.. that indeed sounds even better.. You can't disassemble without a schema.. since if you just look at a binary, you have no idea what type some field represents.\nThe easiest \"disassembler\" we have right now is the JSON output (flatc -t). A more technical version that is more explicit about offsets would also be possible, but I don't know that one exists.. @fasterthanlime : Thanks for your help in getting this functionality in, regardless of whether this version or the one from @krojew.\nBesides correctness, efficiency, and an appropriate API for the language, I share your concern for keeping the code simple, i.e. reducing the amount of if-thens needed to accomplish this as much as possible, by factoring them into functions, or even better, constant lookups.. The diff for this one seems a bit harder to read since there's a lot of spacing (indentation?) changes. Maybe that can be kept seperate.. Also, like the other PR, generated code should be included in the PR.. Thanks!. C++ is probably your best bet, or if you prefer, you can use the C API.. Thanks! Can you run generate_code.sh (or bat) such that the C# generated code files are updated, and add them to your PR?. Thanks!. All getValues does is get an offset to a value table. It does not do any memory allocations.\n50 microseconds for 500 iterations, which means a single iteration takes 1/10millionth of a second. That sounds decently fast given that PHP is a pretty slow language to start with. What are you expecting? If you need faster access than that, maybe use a sub-process that uses C++? :)\n. This appears to say the function you're using outputs microseconds: http://php.net/manual/en/function.microtime.php. Thanks!. Thanks!. Yup, that is terribly out of date. I'll update it.. Thanks for the fix! I believe it may be a bit too complicated though. The problem in #4252 is caused by the fact that the root type is set in the first file and never reset. A simpler fix may be to do root_struct_def_ = nullptr at the start of Parser::DoParse ? I certainly wouldn't want to add this many test files for a small issue.. There's a lot of tests in here that can break in the future, as implementations can pad things different, write in different order etc, which would change all offsets.\nI think the better way to test this is to have implementations read buffers of other implementations, which we don't even do well in FlatBuffers (e.g. many of them read the C++ test buffer, but not vice versa). Not sure what the best solution there is.. I didn't mean that the problem with padding bytes is what their value is (they're always 0), but how many there are.\nFlexBuffers probably has less things that can be encoded differently than FlatBuffers, since fields always have a fixed order, so these tests are not quite as bad, so maybe we can merge these for the moment, but I'm worried that if the future your \"large map test\" fails because we're doing something differently, we may just have to delete it :)\nMaybe at least, in your check() macro, you can output exactly which byte (at which offset) failed the test.\nAlso please add the extra .cpp to the Android.mk\nMaybe you can move some of the code that's the same as in test.cpp into test.h, such that it can be shared. . Padding bytes should be 0. Where are they not 0? flexbuffers::Builder::Align always writes 0.. Well, that looks like a bug in the C++ implementation to me.. let me look into it.. Ok, that was a real bug in FlexBuffers which occurs on 32-bit system writing 64-bit vectors. It should be fixed here: https://github.com/google/flatbuffers/commit/340d1a3447cf821924aaaeefa5e6aef0fcdbf0c0\nPlease rebase and see if that makes the unit test happy.. There is no simple way to extract a nested table from a FlatBuffer, since it is not necessarily contiguous. You have 2 options: using a nested byte buffer instead (with the nested_flatbuffer attribute that generates a convenient accessor), or alternatively use the reflection functionality with CopyTable. I recommend the former . Looks like the main test is failing because the root_type gets cleared when doing JSON parsing. So a better solution is:\nif (!Is('{')) {\n  root_struct_def_ = nullptr;\n}\n\nAt line 1880 or so.. That's because it generates include statements without a path, which in this case would be #include \"ProgressBar_generated.h\", which is the same as the current file. It filters these out to avoid recursive includes.\nIt could warn about this I guess.\nI'd recommend against giving different concepts the same name though, sounds very confusing. I had a hard time understanding the schema :). A binary FlatBuffer is much harder to dump than a Protobuf, since you have no type information at all. You can decode the root table pretty easily (since you know its a table), but then decoding its fields will be hard. You'll have to guess wether a 32 bit value is an offset to a table/vector/string or a float/int just by guessing which is more likely, by looking if you assumed it was a table, wether it would have a valid vtable (all offsets inside the buffer, etc). I've thought about writing such a dumper, and its output would be more useful than a hex dump, but it would not be able to be very exact. Things like a vector of tables will be pretty hard to detect.. A better way is to create a \"registry\" of all possible schemas you know about, and then use the file_identifier to find the schema when you come across a buffer. If the buffer contains no file_identifier, you are SOL though.. No, I haven't. Like I said, I think it would be minimally useful. It would be interesting to attempt, though.\nI'd highly recommend your clients to at least put a file_identifier in all their schemas, that way you can recognize what type it is, and dump it in very readable JSON. You could use the functionality in registry.h, or even more lightweight is the buffer dumping in minireflect.h.. Thanks!. Agreed to having that as built-in functionality.. Where does this warning occur? Isn't it better to use a static_cast, or better yet, ensure better variable types are used?. Thanks!. Thanks, nice fix!. This does not appear to be a problem with our CI which runs VS2015 build and uses CMake. Can you paste some of the errors you're getting?. There appears to nothing in this PR. Also a description more informative than \"update\" is welcome.. Thanks, that's an improvement!\nOne thing, the way you've added --no-fb-import to the script turns it off for JS as well? If so, you should add the changed .js files to this PR after running the script. Does the JS test still function?. Thanks!. What errors does this PR solve?\nAs for email addresses, you're talking to a bot. It apparently has detected the commit author is different. Maybe redo the PR from the same account?. Thanks for your fix!. Thanks :). Yup, it can be performance problem if accessed more than once. FlatBuffers was really designed to be used in-place, but Java doesn't allow that.\nI'm not sure that caching a copy is the right solution however, as storing these String references in the accessor object makes the accessor object almost the same size as a real object (and more expensive to reuse, as all these fields have to be reset).\nFor repeated access of an object, the better solution is to unpack the FlatBuffer in its entirety to a Java object. That involves a whole lot more allocation, but at a certain number of repetitions, that becomes cheaper.. Not sure what is causing this, but {anonymous} maybe hints at those functions being defined or declared in the wrong namespace?. @royalharsh yes, it should do. Does monster.fbs contain and rpc_service declarations?. Can you put those changes into the PR, that makes it easier to review.. I'm not sure what you're asking. Bytes contains the FlatBuffer data (starting from Head() until the end of Bytes). That's the only data that should be sent over an RPC.. Only a slice.. Please.. look at the code before asking questions, it really is not that hard.. Thanks!. Thanks for the PR, but I'm not sure I agree this is a good idea:\n- Unlike Protobuf, the presence of a field is not that meaningful, since fields get omitted whenever they are equal to their default value, so testing for the fields presence is typically not something you want to do. And if you do, there is already IsFieldPresent.\n- Vector fields return a vector handle, that can do much more than just indexing and size. I don't think adding these convenience methods is worth it, especially since adding more special purpose names like _size increases the chance of name clashes.. I agree easier migration from Protobuf is worth-while, just not sure if that is worth these extra methods. If anything, I'd hide it behind it behind a --proto-compat flag or something, since most people don't need it.\nThe name clashes: if anyone has create a field something_size already, this new feature may create name clashes.\nOpinions, anyone?. I agree, those kinds of functions should be generated for you. @rw?\nThese can only be generated if none of the fields are a struct, since those have to be generated in-line, which is not possible as an argument in Python. But for most tables it should be possible.. Hmm, requiring a global Pack isn't the greatest thing.. but I couldn't come up with a better solution given it needs to be callable from generated code, and you may not be able to modify the type itself.. Good idea, added 2015 to AppVeyor here: https://github.com/google/flatbuffers/commit/0c80b3a7ccdc81878504bbd76042ac8c50a313e0\nAnd the actual fix here: https://github.com/google/flatbuffers/commit/e93a5652d03286af2b327ce8a16864c5224d4c87\nThanks for reporting :). There's no reason why a Clear() method does not exist. Can you make a PR to add one?. It does not expect an identifier at the start, if present, the identifier is at bytes 4..8.\nPreviously it simply didn't check for this.\nIt looks like your buffer simply doesn't contain an indentifier, even though your schema has one. So check where you are generating the buffer that your are calling the correct Finish function, e.g. FinishMonsterBuffer, or manually fbb.Finish(root, MonsterIdentifier()).. Can you post your schema and construction code? Again, unless you are using one of the two forms of finishing a buffer above, you will not get the identifier. If you call fbb.Finish without the identifier argument, it will not be inserted.. If you define an identifier in your schema, it is an error for a buffer to not have one. The recommended way to construct a buffer is using the calls I mentioned above that insert one automatically. Yes, it is possible to call the underlying functions directly and thus not insert one, but that doesn't mean it is optional or correct. The main library code can't know what the identifier is, so that's why this is stored in the generated code.\nWhat is the problem with having your Java code insert an identifier?. I guess I really don't understand the problem. Even Java has a finishMonsterBuffer which enforces the insertion of the identifier. If an API allows the creation of a malformed buffer, that is a bad API, and we try to avoid that as much as possible to the extend that the language allows it.. Will fix.. Sorry, cannot make any promises on anything. As I suggested on SO, you could give it a go yourself.. Was fixed here: https://github.com/google/flatbuffers/pull/4316. That's nice, but probably somewhat meaningless in the case of FlatBuffers. Our code contains a fair amount of public methods that are intended for generated code, so can't be private or friend-ed. These methods aren't really part of the API.. These are not easy to filter by regexp. It be a long list.. Yes, as you found out, it only tried to read from file if it hasn't parsed those files yet, so parsing them in the right order will work. I agree this could be better documented, contributions welcome!\nThere currently is no way to read a binary schema back into the parser state. This would be desirable functionality to have. Shouldn't be too complicated to add, but is a bit of work.. @arnetheduck I agree that would be very cool. It could be included as a dump of a .bfbs file.. or better yet, as completely static data structures that are actually readable as code.\nThen you still need a way to pull that data into the parser, to be able to parse JSON with it. But that is somewhat wasteful, as the parser representation is bigger than the .bfbs or static representation, because all data structures are dynamic. Meaning it would be better if the parser could work directly from either representation.. using a very lightweight interface of some sort.. @arnetheduck reflection.cpp has GetAnyValueS which turns any value into a string, but is currently somewhat incomplete. This could probably be improved to produce nice loggable strings.\nYou'll have to explain what exactly you want to be custom about it. If it is too custom, then just implementing your own version of that function may be easiest.. No, no need to open a new one :). Thanks.. though I'm not so sure this is much of an improvement. This brings 2 new headers in, and uses a bunch of C++11 that may break people on older compilers, for no huge gain in usability.\nAPI so far: parser.Parse(src1, nullptr, filename1) && parser.Parse(src2, nullptr, filename2)\nNew API: parser.Parse({ { filename1, src1 }, { filename2, src2 }, nullptr)\nOpinions anyone?. There is no cost no calling Parse many times. It already calls itself recursively if it finds includes it hasn't parsed yet.\nNone of the C++11 is that much of an issue, it just seems a relatively high cost for the small gain.. Thanks! So on this platform __BIG_ENDIAN__ is not defined, but __BIG_ENDIAN is? Maybe only add __BIG_ENDIAN, and leave out the platform define, since that doesn't add anything new?. Thanks, that looks like a nice change!. This is again breaking the Closure compiler:\nERROR - assignment to property bb of SomeTable\nfound   : undefined\nrequired: (flatbuffers.ByteBuffer|null)\n  this.bb = undefined;\n  ^^^^^^^^^^^^^^^^^^^. Reverted the above assignment: https://github.com/google/flatbuffers/commit/c7bfe06c54bcd43312ff017e8ca32bc54833d524. You can simply create a fix on top of what is currently in master. As long as it can avoid the above Closure error.\nFor me, TypeScriptTest.sh completes succesfully, so maybe it should also test \"strict TS\" if we're supporting that?. It seems strange to me for a tool to require specific tags to be present in auto generated code, because by its very definition, you don't have control over what is in auto generated code. Would have been better to allow these files to be excluded externally.\nThat said, if this is the way the C# world works, feel free to add such tags to the C# code generator via a PR.. Sounds good.. see my review there.. This was fixed.. Thanks!\nI'm a bit ambivalent about this: one the one hand I don't mind adding this is if it makes integrating FlatBuffers easier for people, on the other hand I don't know any Meson users, so it is likely this file will not be maintained over time.\nWhen it comes to build systems for C++, it seems that, no matter how hacky it is, CMake has \"won\", and from the dozens of alternative there is no clear one that is getting more usage than the others. So from that perspective, I am not sure supporting Meson is any more worthwhile than the alternatives.\nOpinions?. I wasn't arguing for or against Meson based on qualities as a build system, merely that supporting yet another build system has its cost, and we shouldn't do that if (almost) noone is using it.. If you have data where every key is dynamic, then no, you can't write a schema for it, and probably FlatBuffers isn't the right library to use, since it derives a lot of its efficiency and convenience from being strongly typed.\nYou can emulate key-value pairs of course, and FlatBuffers does have in-place binary search for it, but it won't map directly to the JSON you expect, and generally throws a lot of FlatBuffers advantages.\nIf you can, I'd recommend trying to organize your data such that it is more structured.\nFor schema-less data, I recommend https://google.github.io/flatbuffers/flexbuffers.html, but the JSON parser for that is still forth-coming.. Elements of a vector have to be all the same type, so yes, this could work if they're both always strings. Though in that case, this would also work:\ntable KeyValue { key:string (key); value:string; }\ntable Root { dict:[KeyValue]; }\n\nThis has the advantage that you can use the in-place binary search functionality to look them up.\nThis is still not a great use of FlatBuffers, since you're turning everything into strings. It is only marginally better than using JSON.\nThe absolute best solution is to turn all keys into fields, and all values into the most appropriate type (e.g. int/float where possible).. You need to surround that JSON value with { dict: .. }, since the root is a table, not a vector.. Yes, no reason why we couldn't have those. Would be a question of calling ParseMetaData also for EnumVal. Are you able to make a PR for this?. We typically do not want to add anything to the generated code that may have runtime / memory cost. It sounds like GeneratedCodeAttribute would have some cost.. This may generate a statically allocated string in all compilation modules, depending on compiler. The original was guaranteed to only generate temporary strings.\nYou can move the newlines to the client(s) I guess.. @schoetbi according to @sharwell the closing tag location is fine. That leaves only my comment about Java to be addressed.. Looks good, thanks!. Agreed! Will add.. https://github.com/google/flatbuffers/commit/d7ac3788e81724d92d29364d719769a034f87829. Thanks, appreciated!. Seems to work for me (and I'm also on Chrome). Anyone else have this problem?. Also, all documentation is included in the repo in docs/source which you can read offline.. There's still some formatting problems, but we can merge this for now. Thanks for the fix :). Thanks, nice fix!. There already is a 1.6.0 release.. what exactly are you asking for?. Ahh.. updated: https://www.npmjs.com/package/flatbuffers\nNote there's a fair few JS changes (and TS support) post 1.6, but that will come with an official 1.7 release.. I agree it is dangerous, though I would hope that people moving it to their home directory and then executing it to be a rare event (and even then, it is generally deleting files that can be regenerated).\nWhat is your suggested fix? Maybe we can guard that command with a check that we are still in the tests dir? I don't think enumerating all .class files by name is very productive.. Actually we can include the MyGame/Example path in the find command, which should generally fail everywhere else.. This appears fixed.. Thanks! Can you sign the CLA?. Thanks!. @krojew ?. I don't think those 2 buffers were actually generated from that client code. For one, they contain different strings.. Can you post your C++ code that writes the buffer? Maybe you are writing in text mode or something?\nThere may also be something wrong on the JS side. A flatBuffer is always at least a multiple of 4 in size, and 270 is not a multiple of 4. Why does Buffer.from create a different length from data ? What type is data ? What is the length on the C++ side?. @mindori Can you explain us what the problem was, for future people that may run into it?\nLooks to me a JS String is not a safe place to store binary data :). Ah, good to hear. If only there was a simpler way to detect these kinds of problems.. Thanks, much appreciated you're fixing this! A few issues:\n\nIf you had to make changes to the copied GRPC files to make them work with FlatBuffers, it means something diverged, and it would be good to upstream those changes to the GRPC project as well. In the end, we want those files to be sharable between both projects. For now, we can merge your changes here first, though.\nThe Go changes will not work, since they now have C++ types in them. Have a look at the changes to the Go code, and you'll see a C++ type as result of your changes. If possible, make that change not apply to Go.\nThere are unrelated JS/TS changes in your commit as a result of other commits that haven't added the generated code. I would remove these from your PR to keep it clean, but no big deal either way.. Looks good now, thanks for your help!. I guess its fine that people that use FlatBuffers master also require GRPC master. Not great to have to synchronize releases though, but I guess for now while both APIs are not 100% stable this will occasionally cause problems.. Anyone want to pick this up?. As to when it is safe to reuse a buffer, that sounds like a GRPC question to me.. maybe you can ask it there?\n\nI'm sure we can make a better API. The current one is as simple as it gets, and was created with \"what seemed to work\". Besides reuse (which I would verify with GRPC first), I'm indeed very interested in making this API zero-copy.\n. That may well be.. @krojew who recently implemented this.. Hmm.. this may already be the case in JS.\nThe issue is probably that a vector of structs has all structs inline, as opposed to a vector of tables, which is just a vector of offsets to previously created tables. This means that you have to create the structs in a loop, since there's no way in JS/TS to pass an already serialized struct by value, e.g. (pseudo code):\nbuilder.StartVector(..); for(..) { CreateStruct(..); }; builder.EndVector(). @krojew who may have an opinion :). C++ allows structs to be created by value such that they can be memcpy'ed into the serialized data. In JS, this would imply allocating N JS objects to be serialized later. The base JS API tries to avoid object allocation in favor of serializing data directly, to keep with the \"flat\" nature of the library :)\nThis is a pretty clumsy API for JS, and it be nice to have an alternative API that serializes JS objects into a FlatBuffer, for when convenience is more important than speed. But such an API should cover all of FlatBuffers, not just vectors of structs.. This now finally has been properly documented: https://github.com/google/flatbuffers/commit/262e1d7bf962dc796e47b2186bad6944515ac833. @krojew \nSeems weird.. why is there support for passing null instead of a vector as argument? Would be much cleaner if it always required a non-null vector. And if data.length === 0 you still want to create a vector, vectors of length 0 do exist :). I personally would say it should not accept null, but I'm not a JS programmers, so I don't know what common patterns are.. please do.. Nice work.. see my comments.\nOh, and not sure why the const on the allocator. That could be removed.. Note: this was just merged: https://github.com/google/flatbuffers/pull/4305. I agree that inheriting is not the nicest way of doing things, but if this gets us the simplest/cleanest code, I'm ok with it. Certainly forwarding all methods is a mess I'd not be happy with. I guess the alternative would be for MessageBuilder to own a FlatBufferBuilder, and once it has set up the correct allocator, allows the user to retrieve a reference to the FlatBufferBuilder. Any disadvantage in that compared to inheritance?\nYes, my fault for pushing to not make FlatBufferBuilder into a template. Maybe I am being too conservative, and if others agree that templates would bring strong benefits, I can be overruled :) .. we only just changed the API, after all.. @per-gron : Not sure I agree the verifier should be default on. While typically you'd want to use it for untrusted network traffic, it should still be a conscious decision to use it, and what you'd want to do if it fails will differ by the use-case. People also use GRPC between trusted servers in the same internal network / data center, and may not want to pay the (small) cost of the verifier when maximum thru-put is the goal.\nWe could add it to the examples/comments though, indicating people should consider using it. . @per-gron I think that depends on your use case. Edit - commented on #4336 - we can go ahead with a safe default, but please document.. Ok, this all looks good to me! Any additional improvements lets add them in follow-up PRs.\nThanks for the hard work :). This generally sounds good, but I'm worried a little about breaking the API, since I know there's people out there using this functionality. Moving the whole of FlatBufferBuilder into a template doesn't sound too elegant to me.\nSame with using std::allocator. This would be cleaner, but it would break things for people I'd have a slight preference to keeping the current allocator.\nstd::move to keep the allocator state sounds like a good idea.. but again requires people to implement a move constructor, which may break things.\nOpinions?\nRelated past PRs that never went in: https://github.com/google/flatbuffers/pull/4117 and https://github.com/google/flatbuffers/pull/2500\n. I typically squash PRs into a single commit on merge (a github feature), so it doesn't matter either way.. Thanks! Left comments on the PR.. @llchan no problem! I appreciate your work on this. Let me know when I should review again.\nBtw the flag is not about C++98 language support.. we do require a minimal amount of C++11 (whatever VS2010 supports). The the particular use case is Android, where game developers often still compile against STLPort (which has no C++11 library features, but is compiled with a more modern compiler). We currently only support this use-case for flatbuffers.h + generated code files, a lot of the other code in the FlatBuffers project already doesn't compile with STLPort. Can't wait for the day when this is not necessary anymore.\nI agree we really need CI for this. I've been meaning to look into Android CI but haven't gotten around to it, as with so many things.\n. The use case for release() is for people to be able to hang on to a buffer they just constructed without having to memcpy it elsewhere, and without having to hold on to a dynamically allocated FlatBufferBuilder (which contains other data not needed after construction).. Ok, this now seems good in its current form, any further issues can be addressed in follow-ups. Thanks @llchan !. There have been breaking changes before between subsequent 1.x.0 releases. So This would be in 1.7.0. I agree it would be good to do that release only once the dust has settled around the GRPC work too.. Thanks!\n. Yup, we're trying to make sure those files are shared between the projects, such that any changes the GRPC folk make are easy to integrate into FlatBuffers later. We should really have GRPC as a dependency, but since FlatBuffers is a tiny repo with no dependencies so far and GRPC is huge, we've resorted to copying for the moment :)\nThe initial work I had to do in C++ is to remove the hardcoded dependency on Protobuf. I did this by making the code generator call into some interfaces that are then seperately implemented for FlatBuffers and Protobuf (see schema_interface.h)\nSo step one is to do this for csharp_generator.cc and its headers. Find any Protobuf types and replace them with those interface types. Make sure it doesn't change the generated code for the Protobuf case, and make a PR to the GRPC project.\nStep two is then to copy those files into FlatBuffers, and integrate them similarly to the other languages. Make a C# test case. Some docs if you're feeling generous :)\n. No idea. Worth checking the GRPC docs.. @7anner that looks like its going in the right direction, you'll need to get a GRPC member to review it.\nNote: there appear to be many changes that are purely formatting (e.g. 2 space indent -> 4 spaces) which not only make it harder to review, but are against the Google C++ Style Guide. You'll want to undo them.. I wouldn't call it a \"category of languages\". It is a language that can describe a data format. It doesn't necessarily describe a \"language\" since it can be used entirely without syntax (by binary construction) and offer no functionality beyond plain data.\n\"IDL\" is a very common way to refer to these kinds of systems.\nI also disagree that phrase implies the schema is JSON. It merely implies the parser is able to handle the superset of syntactic forms of JSON and the schema language.\nThe schema language is much more succint than JSON. Would you rather write:\nmyfield:int = 10 (required);\n\nor:\n{ \"name\": \"myfield\", \"type\": \"int\", \"default\": \"10\", \"attributes\": [ { \"required\": \"true\" } ] }\n\n?. No, it's not using that one specifically. The FlatBuffer schema language is entirely custom.. Thanks, looking good!. We should probably add a section :)\nThe basic usage is identical to OS X though. You may want to build FlatBuffers for OS X to get the flatc compiler for OS X. Then write your schema and convert it to a C++ header. Your XCode project needs to be able to include this header and the ones in the FlatBuffers include dir, much like on OS X.. There is also: https://github.com/mzaks/FlatBuffersSwift\nNeither project is part of the official distribution, so hasn't been code-reviewed, and I can't say much about the quality of these projects, sadly.\nAn alternative is using the C version (which is likely easier to call from Swift than C++): https://github.com/dvidelabs/flatcc. Thanks!. Looks like a null string pointer somewhere. What are you passing to Parse ? Can you check in a debugger what the values of the strings are being passed at line 1900?\nIf you link it all statically does it go away?\nIf there is also an error on Linux, there's probably a problem with how you're calling this. What's the error there?. Thanks! Not sure if this is the right fix though, since we generally want this pattern to work without the client instantiating their own allocator. Some related work going on in this PR: https://github.com/google/flatbuffers/pull/4312\n. Ok, we can merge this, then fix the API later.\nIt doesn't build though, there's a flatbuffers:: missing in front of unique_ptr_t flatbuf.. The allocator has been reworked in: https://github.com/google/flatbuffers/pull/4312\nIt now uses a single static default allocator, so the issue this PR addresses should be fixed.. We changed the API to be more C#-like.. it's possible we never updated the sample. I'll have a look.. Fixed: https://github.com/google/flatbuffers/commit/398ae0cb6b0fe421adb26b52902d21efddd03ae2. @per-gron BufferRef was created because the GRPC API expected some kind of object to hold on to, and this was the stand in for it on the FlatBuffers side. It was just the simplest thing that could work, and I agree it is not very robust for general use.\nGenerally we don't like to break the API of course, but the GRPC code is still young and immature, and I feel it is warranted wherever needed.\nGood idea to focus improvement discussion on @llchan's PR :). A lot of work on FlatBuffers is done by external contributors.\nI've done most of the core functionality of FlatBuffers, but I mostly work on the C++ implementation.  What I work on is often driven by internal needs at Google. So while I may eventually get around to implement GRPC support for other languages myself, it may take a while, especially since I'm not an expert in some of them.. Yes, unions are intended for this purpose.\nYou could indeed copy it, from another FlatBufferBuilder.\nIt is also possible to without copying, but there is no nice API for that at the moment. You would do:\nfbb.Finish(CreateMyRootObject(..));\n\nAt this point, you have a complete FlatBuffer in the builder, but you can still prefix it.\nauto size = fbb.GetSize();\nfbb.StartVector(size, sizeof(uint8_t));\nassert(size == fbb.GetSize());\nauto vec = fbb.EndVector(size);\n\nThe StartVector won't do anything, since there is nothing to align. You can now finish the buffer a second time.\n. Thanks! Could you make a PR for this?. Most people would use the CreateMonster call, which is even more convenient than the syntax you suggest.\nI'm trying to remember why we never added this. I'm guessing it was from a desire of the smallest possible overhead (not having a return value that's possibly not used), but that's probably misguided.\nI don't think I have anything against this being added. Care to make a PR?\nNote that the return value should probably be a reference.. Thanks, see my comments on the PR.. Thanks!. Thanks for the PR, I agree a required field should not have a ? (nullable) type in C#.\nHowever, we can't just remove the check for a 0 offset. It is still possible that some implementation disrespects the required flag, and when that happens, something \"debuggable\" has to happen. With this PR, a string gets constructed from garbage data, I think returning null is much better for types that are implicitly nullable (like string in C# and everything in Java).\nFor explicitly nullable types (structs) we can't do that, so we should have a Debug.Assert in there I think?. No, I suggest that where possible, we retain the existing behavior of returning null. Only for structs in C# I suggest using an assert, since we have no other option.. @linwukang FlexBuffers so far is C++ only.. @evolutional an exception would be good.. @Grimeh that works for me.. I agree that this is more logical.\nNot sure if I would call it an \"unreasonable\" dependency, however, since it is a single header, and it just gives you a bunch more stuff in the flatbuffers namespace.\nThe small downside of this refactor is that the minimum use case of FlatBuffers used to be a single header, and now its 2 :)\nAnyone any opinions?. Ok, thanks :). @vglavnyy @AntonYudintsev I don't remember when we added this, or what it is needed for, but generally no pop would be bad because it affects anyone using FlatBuffers. It would be best to try and remove this pragma, see what it is needed for, and make it more local.\nIf either of you has access to Windows, wanna give that a go?. WebAssembly is an intermediate format target for languages like C/C++/Rust etc, so you'd typically use FlatBuffers in those languages directly. While theoretically you could generate WebAssembly code directly, there would be no easy way to refer to it, and you'd miss out on the high level typing those languages give you.. This seemed to already have been mentioned: https://github.com/google/flatbuffers/issues/197\nA PR for it was attempted but never completed: https://github.com/google/flatbuffers/pull/212\nAnyone want to get this merged? @rw?\n. The existing PR would be good to start from.. I'm no Go expert, @rw can give guidance :)\nWhat's the GSoC project?. Yes, they have evolved separately :). made some comments on your commit.. though they are about C++, not Go :P. Agreed, that's a bug. Should simply be a matter of resetting these variables at the start of DoParse.. care to make a PR?. The verifier is indeed fast on small buffers, because all it ends up doing is pulling data into cache that you're about to access anyway. I've never benchmarked it though. Care to share a breakdown of cost from your testing? Would be useful for others to know.\nYou make a strong case that people that build on this tech and don't know that they need to decide on verification are better off with the safe default. I think I agree. We should just document very clearly that by default it is on, that it will result in so-and-so GRPC error if it happens (and how to check that indeed it was caused by the verifier, not something else), and here's how to turn it off if rpc's are your bottleneck :). Ok, looks good, thanks!. Thanks, that makes sense. But.. now we have these two:\n   VectorIterator(const VectorIterator &other) : data_(other.data_) {}\n   VectorIterator(VectorIterator &&other) : data_(other.data_) {}\n\nForgive my ignorance of move semantics, but does it still make sense to have the second one as && (instead of just &) ? We're clearly not moving anything here.. Thanks! Yeah, not sure whey we had it, data_ is const no less. Let's see if we're breaking anyone's code.. 3..2..1... Sure, removing it sounds like a good idea. Can you make a PR for it?. Thanks!. Thanks for the review! Yes, this iterator was contributed, and me not having a lot of experience writing custom iterators I never noticed any of this.\nPlease do make a PR for whatever you can fix, as you seem to know better than me :). Not sure if I'm a fan of the GetRoot() overloads. Getting the root from a buffer is not a simple dereference, and having that verbose call in there makes it obvious what is happening.\nAs for copying.. if there are any reasons why a copy could be \"surprising\" or error prone, then an explicit Clone() or similar would be better. . @ctiller could you confirm this PR is safe (see discussion above)?. @llchan I don't agree about the operators. I don't think it is needless noise when the two kinds of pointers describe fundamentally different concepts (buffer vs root). Hiding that behind a simple dereference is error-prone, we've already had plenty of people confused on how the two differ, and I don't want to make that worse.. @llchan a shared_ptr<T> derefences to T *, which is just the start of the buffer it holds. A Message<T> is really like a shared_ptr<uint8_t>, and then GetRoot() gets you a T * that points somewhere different than the uint8_t * it is managing. Not quite as straightforward. The distinction between those two pointers is a core FlatBuffers thing. . btw I can merge this, it looks good to me, was just waiting if @ctiller is still going to chime in.. Well, I am arguing that the user should be aware of those details :). Thanks for the fix! I was going to do a 1.7.0 release that would have this new GRPC functionality in, so it be good to know we don't have any memory leaks before that :)\n. I bet it generically generates a loop. Care to make a PR to replace it with BlockCopy when possible?. Yeah, the buffer is built backwards :)\nSo the best thing would be to add a new AddBytes function in the builder, that reserves space for all bytes (using Prep) and then copies all at once. Then replace the generated loop with just that call.. This is mostly a historical decision, as the reasons we originally had are not that relevant anymore (streaming etc). It's slightly easier backwards (no back-patching), but also clumsier in some ways. Can't change it anymore though :). Thanks! Maybe instead make capacity return a size_t though?. @llchan . @llchan should work without Docker?. @berner it be great to hear some anecdotal numbers on what kind of speedup you're getting, if any (here's hoping the rest of gRPC does not become a bottleneck :)\n@llchan now with this bug fixed, I'll do a 1.7.0 release of FlatBuffers soon, that will include this functionality, so we can get more people using this code. We may post about it.. @cberner That's a pretty exciting speedup already! Please keep us updated if you can.. Just tried doing this, and the problem is that in the latest VS2017 image, they dropped support for VS2010 (our minimum spec), which the 2015 image still does support.. Thanks guys!. 1. Just use FlatBufferBuilder. It has a clear method that you can use to reset it and reuse it, thereby reducing GC usage. You should not have to deal with calling newByteBuffer unless you want to.\n\nIn C++ you can point a class at pre-existing data, in Java you can't. But notice the second way of calling it, you can supply your own Message rather than relying on the generated code to call new. This way, you can reuse the Message object many times and reduce GC.. Thanks! You can update existing PRs btw, rather than creating a new one.. It doesn't use memcpy underneath because we have go through PushElement that does correct endianess swapping when on a big-endian machine and it deals with non-scalars (offsets) that need are fixed to their location when serialized.\n\nThis situation would be good to special-case however. We could overload CreateVector for Offset<T> and T much like the underlying PushElement, and then for the T case we could use an #if FLATBUFFERS_LITTLEENDIAN to directly use PushBytes (memcpy).\nCare to make a PR?. Thanks!. Looks like your build settings include -Werror=implicit-fallthrough= which we don't have in ours (I don't think -Wall and friends include it.\nWe could add -Werror=implicit-fallthrough= to our CMakeLists.txt, such that this warning is avoided from now on.. care to make a PR that add it, and adds whatever annotation GCC expects to allow fall-through?. Here will work: https://github.com/google/flatbuffers/blob/master/CMakeLists.txt#L120. Hmm that's odd. Maybe it is not detecting your version of gcc correctly? Can you add it to the options just above there instead?. 2 seems the right option. But from which GCC is this warning available?. If so, we must ensure in CMake that it is only enabled on 7+. feel free to just add the right comments. but like I said, if we don't add the warning, you may see regressions.. Odd.. so the only options are to disable the warning or.. not use fall-thrus ?. Not sure, I don't personally know which library Arduino users are likely to use. Does the conditional fix pose a problem when used with ArduinoSTL? Can we conditionally detect which lib is in use?. That would be serious. We have testing code that constructs a vector of tables though, that works fine (on 3 different compilers and platforms), Can you share the code that constructs aVec ? What compiler are you using?. Are you by chance serializing a vector<offset_t> as opposed to vector<Offset<..>> ?\nThis is an example of serializing a vector of tables (ignore the sorting, it still calls the regular CreateVector underneath): https://github.com/google/flatbuffers/blob/master/tests/test.cpp#L138\nThat then gets read and verified here: https://github.com/google/flatbuffers/blob/master/tests/test.cpp#L277. You can test by just building the FlatBuffers tests. If that fails, we have a possible issue with the compiler. If it succeeds, there's something different about your code compared to our tests.\nWe test with clang, but on OS X.. Can you try sticking it in a vector<Offset<Thing>> and see if that fixes it? I wonder why it is picking the wrong template overload... Ok, reproduced it. If you change your call from fbb.CreateVector<flatbuffers::Offset<Thing>>(&singleOffset, 1) to fbb.CreateVector(&singleOffset, 1) the problem goes away.\nThe way I can explain this is that both overloads have the same template parameter, T, but are overloaded on the first argument. So without the template parameter, C++ picks the correct overload. But with the template parameter, you're saying, \"I want T to set to Offset<Thing>\", which means the first overload now matches also.\nNow the big question is how common it is for people to explicitly specify the template parameter, since those people will get themselves into trouble with this overload.\n@cberner who wrote this.\nWe may need to a 1.7.1 bug-fix release.. Not even sure how to change it such that this problem can't happen, apart from creating a differently named function.. Ok, for now, added this assert to make sure that people that have code similar to yours get a compile time error: https://github.com/google/flatbuffers/commit/25a15950f5a24d7217689739ed8f6dac64912d62. Made a 1.7.1 bug fix release just in-case: https://github.com/google/flatbuffers/releases. long is a 64-bit integer which JS can't represent directly, so it is split in 2 halves, high and low.\nYour example value doesn't need a long, an int would suffice (32bit).. You can represent a 64-bit integer as two 32-bit integers, so (in hexadecimal, to show the effect):\nbuilder.createLong(0xDEADBEEF, 0xABADCAFE)\n\nWhich represents the value 0xABADCAFEDEADBEEF, which you can't type in JS directly, because it not representable as an exact integer in JS's 64-bit floats.. I agree that would be nice, and generating such a schema in flatc should not be hard. Does anyone wants to make a PR for it?. It be good to generate the JSON schema from the default schema we use for tests: monster_test.fbs. That way you can just add --json-schema or whatever to generate_test.sh.\nNot sure how to test it on the command-line, unless there is a command-line verifier. You could of course compare it against a golden file but that is very fragile. Maybe for a first iteration test it manually.\nYou typically make PRs early, so I can comment on them.. Thanks!. Agreed, we should compile this file with --gen-object-api. Can you make a PR that adds this to reflection/generate_code.sh and includes the new generated code?. I guess originally the thought was that you'd never need to use this with the object API, since there is no point in modifying any of the data. But I guess it can happen when used in other contexts.. Should be easy to fix: requires that we suspend checking of table references until the end of the \"main\" file (currently also done at the end of each include file).. I don't understand why that is needed.\nEach file on the command-line is its own independent compilation (Parser instance) so they should be independent.\nFor one particular file, when we generate code, we don't want to be generating code for its includes.. Merged your PR, thanks!. Thanks, looks good now!. There still appear to be Travis errors?. I'd prefer our CI to keep working with the older CMake, as it means we are not forcing everyone else to update their CMake just to use FlatBuffers.. What is the minimum CMake version to allow this feature? Do we have any data on what versions of CMake are commonly in use?. Can you add a description?. Please describe.. Note you have some CI errors to address.. The CI runs through VS2010, so if it is happy, you know it works there. And yes, VS2010 supports auto and a partial set of other C++11 features.. Thanks!. You specified the type of things to be a vector of Thing, not a single element, and that is exactly what it is requesting. Either remove the [] around Thing, or actually create a vector, i.e. CreateVector(&mything, 1), which will return the desired Offset<Vector<Offset<Thing>>> type.. CreateThing returns an Offset<Thing>, not a Thing. So when you When call CreateVector with it, T is a Offset<Thing> too.\nSo you want: std::vector<Offset<Thing>> things. Then you can things.push_back(CreateThing(..)) etc.\nNot sure what GetThing is supposed to do in the code above. If that is a root accessor from a buffer containing a Thing as root, then that won't work. Creating and reading a FlatBuffer are two entirely seperate APIs, and you can't just mix them. To construct a vector, you must use offsets to objects in the current buffer, you can't use pointers to objects in other buffers.. What would the issue be? This is by design :). Yes, that is pretty fundamental. If you read the tutorial you'll never see the two APIs being used interchangably.\nUnlike Cap'n Proto, FlatBuffers doesn't store fields that are not set or are not default, so being able to modify them after creation doesn't make much sense (it would require a very complicated resizing operation). So creation of a buffer is a linear, write-once affair.\nYou were also trying to point to objects of different buffers, which would never work, not even in Cap'n Proto. Luckily, as you find out, the API is strongly typed enough that it doesn't allow you to do those kinds of things.\nNot sure how we can document this better. We can add some of the above wording to the tutorial, but then again, people skim docs, and in the end the API is the last defense :). The mutable interface is meant to be used with buffers that have already been constructed (not buffers currently being created), and indeed, it fails if called on a field that is not present. So it is very limited in its use.\nUsing reflection, you can mutate more things, and it can even copy entire tables from other buffers. Tables are not necessarily contiguous blocks of memory (their vtables can be shared, and its contents are not nested, subobjects can also be shared etc), so it is a somewhat more complex operation than in Cap'n Proto.. cast to size_t is most appropriate.. but put it where the variable is initialized, not where it is used.. fbb.dataBuffer() does not serialize, it merely gives you access to the already serialized data. FlatBuffers serializes on the fly. Similarly, object.getRootAsXXXX(byteBuffer) does not de-serialized. It merely gets you a friendly pointer into the existing buffer.\nThe key difference is that FlatBuffers doesn't de-serialize at all, ever. It allows you to access the serialized data in-place.\nFlatBuffers only supports writing to a ByteBuffer, yes.\nYour timing doesn't test anything. You're testing the time it takes to write to the stream. You should instead test the time from where you create the FlatBufferBuilder until you call finish on it, that does all the serialization work.\nYou can't time de-serialization because it is always 0. Instead, to compare against Protobuf, you could compare the time it takes to de-serialize protobuf and access all its fields, against FlatBuffers accessing all fields.\n. You can use any existing compressor on a FlatBuffer, which will give some savings, but it is not optimal because offsets are relatively random and thus not very compressible. A special purpose compression schema could be invented, but I don't think anyone has tried that yet. FlatBuffers was on purpose designed for speed of access over size.. @binary132 if wireformat size is your concern above all else, then indeed I would not choose FlatBuffers. You can run a compressor on top of FlatBuffers, but since offsets don't compress well, this doesn't gain as much as you'd get from using, say, Protobuf.\nI guess FlatBuffers was designed for use cases where memory usage and speed matter, such as games loading lots of data, or high performance RPC between services in a data center (where cpu can often be a bigger bottleneck than the network!)\nOther than using a good compressor, not sure what to do if you already bought in to FlatBuffers and you want it to be smaller. I could imagine a special purpose transform that would make FlatBuffers more compressible, not sure if that's worth it.. I don't have access to this version of clang, but lets see:\nunsigned integer overflow: 18446744073709551615 + 1 cannot be represented in type 'unsigned long'\n\nThat number is 0x7FFFFFFFFFFFFFFF, and 0x7FFFFFFFFFFFFFFF + 1 should just fit just fine in an unsigned long (aka uint64_t in this case).\nAlso it appears to happen inside this function:\ninline size_t PaddingBytes(size_t buf_size, size_t scalar_size) {\n  return ((~buf_size) + 1) & (scalar_size - 1);\n}\n\nNeither argument will ever be such a big number, so not sure what the error even means. Is it doing fuzzing? Can you get a callstack with variable dump from this?\nNot sure what the fix is. Worth trying to replace both 1 with 1UL and see if that makes it happier.. @alexames do you have time to see if you can reproduce this?. As you say, the only useful artifact out of a FlatBuffers build is flatc (there are no shared libraries typically used, as the minimal use case of FlatBuffers involves just headers).\nI agree it is useful to have pre-built flatc executables available. We do this for every release for Windows, as users they are the least likely to want to build them themselves :) So far most mac/linux folk seem ok to build them themselves, but it doesn't hurt providing them, if it can be automated.\nCan you educate me why we would want to use Docker for this? Since it is a single executable that requires no particular environment, isn't it easier to make these the by-product of CI or something?. I think a regular pre-built flatc may be preferable, since not everyone is using Docker to build stuff.. @neomantra thanks for providing this!. @ghorbanzade see my comments above.. if we're going to provide pre-made binaries, I don't see why they  should be in a docker image.. it be more widely useful if it was a regular binary.. Yes, a docker image is in theory more convenient, assuming the user is already using docker. For everyone else, it is much less convenient.\nI was assuming a statically linked 64-bit linux flatc executable would be usable by most people (on Linux), but maybe I am missing something? Certainly on Windows and OS X this is easy.\n. You're right, this is pretty much an oversight. We just added support for parsing nested FlatBuffers and FlexBuffers, and outputting nested FlexBuffers, so this is the missing part of the puzzle.\nHere is where we output FlexBuffers as JSON: https://github.com/google/flatbuffers/blob/master/src/idl_gen_text.cpp#L178\nWe'd need a similar check there for nested FlatBuffers. Similarly, we'd want to turn the check for the nested FlatBuffer into a boolean, so we don't pay the lookup cost for every field. Yes, that's where to introduce the bool. Make sure to also replace other lookups to nested_flatbuffer by the bool while you're at it.\nIf you see other places that access nested_flatbuffer, you'll see that they get the struct to be used from an attribute, not from the type (since the type is typically just a vector of ubyte).\nI wouldn't store it in the type (as per your second message) since that creates a weird type that we haven't had before.\nJSON parsing would be nice as well. You could use the existing Parser, the only problem you'd have is that when you call Parse, you end up filling the builder_ variable with the FlatBuffer, but that one is currently already being filled for the parent FlatBuffer! So you'd need to somehow swap out the current buffer, stick a new one in temporarily, etc. There's probably an efficient way to make this happen, but to keep things simple, you could try achieving it just by copying the bytes in and out. There may be other variables besides builder_ that need backing up, like field_stack_.\n. Hah, nice workaround, but going via FlexBuffers AND JSON probably not a good idea :)\nThough you're applying a trick I hadn't thought of: instantiating a new Parser with just the root, using the definitions of the current parser. That wouldn't work for all kinds of parsing since it is missing the list of StructDefs etc, but works in limited cases. It would not work for a doubly nested FlatBuffer for example :)\nYou should be able to do it without FlexBuffers if instead you do:\n\nRemember the current Parser::cursor_\nSkipAnyJsonValue()\nCopy the string in between to a new string, then pass that to the new parser.\n. I'm not quite sure what you want to do. Can you give an example of what you want the calling code to look like?\n\nCan't you just call the generic method and cast it to IFlatbufferObject or something?\n. what is myInterfaceMethod in this example? A field that is present in all tables of the union?. Sorry, my C# is failing me here.. can you cast typeAStruct to a IMyInterface above, and then still call the method on it? Can the method access the fields?. You don't need to encode binary with base64 or whatever to send it as part of a GET request.. simply use a content type of application/octet-stream.\nAgain, it sounds like you're doing extra work that is not typical of how you'd use FlatBuffers.. Thanks, nice improvement!. Yup, that is functionality that should eventually exist.. it would function similarly to creating a FlatBuffer from JSON.\nAs far as interop is concerned, I would not expect this to be a frequent use case however, typically you decide which of the two is best suited to store the kind of data you have, and I wouldn't expect you to want to move back and forth a lot, especially since accessing FlexBuffer data directly is already pretty efficient.\nAnd there may be use cases for FlatBuffer to FlexBuffer as well, which would be slightly simpler.. @gabyx I do not know of anyone working on it.\nIf you have the option between union of tables and FlexBuffers, but you still want to FlexBuffers to be convertable to FlatBuffers, I would just use the union of tables, and save yourself the trouble. FlexBuffers really is best used for when you need the flexibility of arbitrary data.. is $schema a convention used by most users of JSON schemas, not just MS? If so, we can explicitly skip such fields, even without --unknown-json. FlatBuffers can't deal with JSON that has $ in a field name anyway.. Sounds good.. care to make a PR?. Thanks!. Then I would suggest we make a new android_stlport target, that only uses the basic generated code (without --object-api) and the parser, and revert all STLPort changes to the rest of the code base.\nLike I said, it is not about the cost in code for you to make these changes, it is about the cost to future PRs having to deal with writing STLPort compatible code and using our wrappers correctly, relying on CI to tell them. I believe that burden should be as minimal as possible.\nIn the case of the object API in particular, we make the promise that this translates FlatBuffers into objects using \"idiomatic C++\", so much so that you can use them as your runtime data structures. If suddenly something as basic as std::unique_ptr becomes a different type, that can polute into downstream code or require conversions, which defies the point of this being idiomatic. So if the object API has to be made STLPort compatible, I'd suggest #ifdefs in the generated code such that non-STLPort users can still have their std types. But better yet let's not have this compatibility for the object API at all, since it is optional anyway.\n. Seems the only main disagreement we have left is wether STLPort compatibility should be inflicted on parts on the code base that currently don't need it. Can we have some other people weigh in?. @alexames yup, that would actually be better, since it reduces the friction with people not familiar with STLPort. Maybe nice for a followup-PR?\nOne risk is that if people use a new function not available in this emulation class (but generally available even in STLPort), they'll now be forced to add to the emulation class, whereas before that just worked. So the emulation better be somewhat complete.. Ok, I guess this can be merged if everyone is happy.. I don't see a reason why they can't be.. not sure why the original author chose to do it that way, maybe to be similar to some other language implementation? @evanw ?. Thanks for your extensive feedback!\nYou had to switch to Protobuf because we don't have GRPC bindings for all the languages that you use?\nComments on your bullets in the same order:\n- You have to understand that until recently, all of Google's massive code-base was using Protobuf uniformly. Data exchange formats have kind of a viral effect. So Protobuf is and always will be hugely important to Google, regardless of how awesome FlatBuffers is :) We have quite a few internal users, but adoption is slow going because of that reason. We have more internal contributors than just me, but commits may appear authored by me because of the way we move things from our non-git source control. Action is mostly in C++ since that is what we use, other languages are mostly external contributions.\n- We made a conscious decision to sacrifice simplicity of the API where needed for the sake of speed. We have an API that is as easy to use as Protobuf, it is the object API, and just like Protobuf is inefficient. We'd recommend to only use it in non-speed sensitive cases.\n- The Python binding was made by an external contributor, and since we don't use it internally, it doesn't get as much love. A GRPC binding for Python was started but never completed.\n- The object API can totally work for other languages, in fact, it is more appropriate for other languages. In slow languages like Python, an object API may even be faster, since the ratio between the cost of object allocation vs the cost of computation is different.\n- To have a DebugString style function, we would need access to the schema, so it would pretty much be a wrapper around the current JSON generator. Protobuf messages can be parsed (without field names) even when you don't know the schema (since it contains types). FlatBuffers doesn't contain types so this is generally not possible.\n- Most people would use CreateVector which adds all elements in one go. If doing it by element is not document clearly, this is an oversight, please file an issue.\nVersioning:\n- A new required field can fail the C++ verifier on old data. In all other use cases it fine. We should probably document that too, yes.\n- Yes, you can deprecate a required field, that will just stop requiring it.\n- Yeah, we could add a dedicated syntax for deprecated fields. For now, you could rename the field into something more obviously deprecated.\nPerformance:\n- FlatBuffers was definitely designed for languages like C++, so Java and Go may not be as good a fit. A big issue in Java for example is that it can't read UTF-8 strings in-place. FlatBuffers replaces object allocation with a small amount of work during field access, which may not always be the right trade-off. Of course, It is hard to tell without looking at these benchmarks if they're not doing something silly, you should always do tests for your own use-case.\n- In our testing, FlatBuffer binaries are typically 30% bigger, but YMMV.\n. @zejal yes, when using ids, we could simply leave out fields entirely.\nHowever, we still need a way to do that when not using ids.\nAlso, even when using ids, because it is quite important that ids are assigned sequentially for performance, it is nice to be able to check that with an error. For example, if you add a new field and skip an id, there's no way to tell the difference between that being an error or an intentional deprecated field.. Yes, a placeholder would be fine. In fact that was one of the suggestions above: the ability to have placeholders without a name or a type. Using some kind of type specifically for this purpose sounds like an ok alternative.. @maingoh You asked for a DebugString that works without loading a schema, it just arrived here: https://github.com/google/flatbuffers/commit/72a99abfb7db64dc49720b28b41f382b5ec7cde0. @maingoh : this new code converts a FlatBuffer to a string which as close as possible to JSON (though at the moment, FlatBuffers' more permissive JSON, i.e. there are no string quotes around field names).\nThere is no parsing of JSON to FlatBuffers using this mini-reflection yet, which would be possible, but a fair bit more complicated. For the moment, best to use the existing Parser for that purpose.. @zejal yes, there's been bi-directional FlatBuffers <-> JSON right from the start. But it involves parsing a schema at run-time.\nThis new functionality does FlatBuffers -> JSON without loading or parsing a schema.. @mfarrugi I agree we should have it for every language, it just hasn't happened yet. Contributions welcome.. Thanks!\nNot sure how I feel about this, as so far we intended for booleans to be the same as ints, to keep things simple. I.e. FlexBuffers stores things as they are represented, not what they mean.\nThen again, we already have an explicit null too, and maybe it would help interop with JSON to not lose \"true\" and \"false\".\nAnyone have an opinion?\n. I agree that would be nice. Let me have a look... Thanks, nice improvement!. So yes, we have been supporting varying levels of old compilers that may have caused this. In fact, we're just adding an STLPort PR that only has a basic level of unique_ptr emulation, and no shared_ptr.\nThat said, while I am a fan of unique_ptr, I'm not as much of a fan of shared_ptr, as it is pretty expensive (I'd only use it if ownership wasn't clear). The SymbolTable could own unique_ptrs, but there will be plenty of weak (naked) references.\n. The schema extension has to be .fbs. That seems inconvenient, yes. @rw ?. Maybe string fields should have accessors for both (bytes and str) since .decode('utf-8') is bound to be an expensive operation, that some users may like to avoid if they can deal with bytes directly.\nSince the current ones are for bytes, we could add an accessor with Str appended?. It makes sense that string would convert to a Python string, we do the same (convert from UTF-8) in other languages, e.g. Java. @rw probably chose to use bytes for efficiency, and we may want to be backwards compatible, hence my suggestion above.. I agree numpy support would be nice. I do think it should be optional though, if not every Python install comes with it. You could add a --numpy if needed.. @kbrose can numpy support in the library code be factored into its own file?. @rw can you review before this gets merged?. That is pretty cool. I think being able to receive a FlatBuffer, and then without copying pass large chunks of data in it to numpy will open up all sorts of new applications.. I agree that testing is currently clumsy because they all share one schema, making it harder to test language individual features. It also has advantages though.\nI don't think we need to go too crazy on tests to protect against numpy changing in backwards incompatible ways, I find that fairly unlikely.. Yes, I'm fine with the extra fields. Since @rw is ok, I'll merge.. please follow-up if necessary.. @rw for the moment, can we push 1.7 to it so it is more up to date? Or does @mikeholler specifically want to use this PR? That would be in a 1.8 whenever it comes.. 1.7 is already out, it's the current official release.. @kbrose  while the numpy based tests help make sure this functionality works, they now made our tests in general flakey, see the http timeout on this test run: https://ci.appveyor.com/project/gwvo/flatbuffers/build/1.0.982\nIt's pulling in a LOT of dependencies, including things like openssl which I don't think we need. Can these tests be made less heavy by requiring only numpy? If not, can we make numpy tests optional (with an option to PythonTest.sh or whatever?). @rw may also have some ideas.. I'm fine with disabling it for now until we can do better. Can you make a PR for that?\nWe can try the --no-deps and see what happens. I still don't understand why it has those dependencies though, surely numpy itself does not rely on openssl etc.. Ok, I can fix that.. disabled numpy tests for now: https://github.com/google/flatbuffers/commit/1f0bd1285130503000ef2cb615e6ecb01aaa90fd. There's no schedule for that.. currently it seems to happen once every 6 months or so, or whenever a significant chunk of new functionality has accumulated.\nI'd like to help you out by making releases whenever people ask for them, but thats difficult, as different people need different features.\nThe releases really aren't a lot more stable than regular commits, if you need this feature I'd really recommend to just use FlatBuffers at this commit.. Thanks!. Thanks, that appears to be introduced by a recent commit by @stewartmiles.\nAny reason you use the lambda version btw, instead of just passing the vector as-is?. I'd say if this is not something JSON generally supports, that would be a bit special purpose.\nI would hope that a FlatBuffer storing vertex data is not represented in JSON for anything performance sensitive.\nYou could consider storing vertex attributes as a union of vector of structs, that way you actually get readable float data out of it in JSON, etc.. glTF is a nice library, but if that is the only place where it is used, I'm not sure if this would be a good thing to add to FlatBuffers. Anyone else an opinion?\nDoes glTF actually use JSON with big vertex buffers as a runtime representation for assets? I thought the JSON was mostly for \"metadata\", where the JSON would refer to binary assets to contain the bulk of the data.. Wow, both of those are kind hacky, I'd say. They should have had the JSON refer to external files, much like it does with textures and shaders I'm sure.\nBut the second option sounds better to use. Storing large amount of binary in text will always suck, no matter what the format.. @vglavnyy can you make a PR? makes it much easier for me to see the changes.. Inside PrintVector makes sense, if it is limited to [ubyte].\nAs for accessing attributes, I think it may simply be a case of passing a const FieldDef * to those functions. I say *, since it may be called from contexts that are not fields, so may be nullptr.. Ah, good catch!. That would be nice.\nOne issue we have with clang-format is that it undoes all the indentation of #preprocessor directives, and there's no way to stop that other than bracketing them all with // clang-format off etc.\nIt also doesn't catch everything, e.g. use of camelCase, location of * and & etc.\n. Yeah, the way this is implemented is a bit unfortunate, but not doing so can generate compile errors.. I believe this was added because in the example protos I used for testing, it was possible to protobuf fields that were FlatBuffers schema language keywords, not because of namespaces.\nI guess the correct solution would be to catch just those keywords and escape only them.. Thanks, that was indeed missing :). This appears to contain code from other commits.. btw, be sure to run sh generate_code.sh as it says in the PR instructions, since your change has effects on other languages too.. There should be changes, since you change the .fbs file, which gets generated into all sorts of languages.\nI think its fine to use the original tokens for these new types for now.. Ok, looks good to me now. Shall I merge?. Can you rebase?. Thanks!. Yeah, I'm impressed too.. we definitely do check for namespaces when handling this kind of code. I'm suprised this hasn't come up before since it is a common pattern. Are you by chance on a cutting-edge version of gcc or clang?\nWithout looking at the code, my theory would be that is generating another Namespace object in the parser because its in a new file, and then at codegen we compare namespaces by pointer rather than value.\nSince Namespace objects are not unique, the easy way to fix this is to compare by value. The harder (but better) way would be to change namespaces_ from a vector to a map, make sure Namespace objects are unique, and track the current namespace more consistently (currently, the last element of that vector). It has caused other namespace bugs in the past and would be good to clean up.\n@alexames have time to look at this? If not I can do so soon.\nA quick workaround would be to give Vector3 its own namespace for now, or move it into the same file for now.. Pretty sure this will have fixed it (namespaces are now unique): https://github.com/google/flatbuffers/commit/321a1c9dc0bb2205feb1bb9fd079546403aa852f. You call GenerateText. \nAPI: https://github.com/google/flatbuffers/blob/master/include/flatbuffers/idl.h#L692\nExample call: https://github.com/google/flatbuffers/blob/master/tests/test.cpp#L562. You supply an empty parser. You need to make sure the parser contains a schema, otherwise it doesn't know how to generate text.\nSo add something like parser.Parse(myschema.c_str()) where myschema is the loaded schema.. Works here (Linux):\njava version \"1.6.0_45\"\nJava(TM) SE Runtime Environment (build 1.6.0_45-b06)\nJava HotSpot(TM) 64-Bit Server VM (build 20.45-b01, mixed mode)\nFlatBuffers test: completed successfully\n\nNot sure why this would fail with Java 8... Ahh, yes, that should be a simple fix of copying the required flag into the auto generated field. Can you make a PR?. I'm guessing the problem is caused by C1_generated.h including C2_generated.h.\nCan you see if the problem goes away with --no-includes ? This is (unfortunately) what we tested with, and does not automatically generate include statements.. \"Any backwards offsets will be explicitly marked as such.\" means \"all offsets are towards a higher memory location, unless specifically documented that they don't\".\nSo no, cycles are forbidden by design. Note that most serialization formats don't even allow DAGs (which FlatBuffers does support), so I don't think this is a great loss, and simplifies things a lot.. @shivendra14 as I asked above, does this problem exist for you with the latest master? Does it also happen when using --no-includes ?\nI don't think anyone has worked on this recently, and I myself am a bit back-logged, so not sure when I will get around to address this.. @shivendra14 well, can you test with latest master?\nWith --no-includes, you are responsible for including things yourself in the right order.. @shivendra14 no, ideally the user shouldn't have to forward declare anything. I'd be fine with a solution that moves all forward decls before the includes (but after including flatbuffers.h), if that makes things more robust. I am not sure if this makes all forms of cyclic dependencies possible though.. Someone made an unoffical port here: https://github.com/DavidFeng/lua-flatbuffers\nThere's no plan to support Lua, but we welcome a PR to make it part of the main project.\nSee also: https://github.com/google/flatbuffers/issues/27. I am aware of that, I come from game development :). Ah, related, I just \"fixed\" the Go tests as part of this commit: https://github.com/google/flatbuffers/commit/ac1015e3c417ecb18d8f449a4e6aaaff3c4f53b9 (seach for GoTest.sh).\nIt was compiling the gRPC files even though they weren't being used in that particular test, removing them made the error of golang.org/x/net/context not existing go away. That said, it should still be fixed for the gRPC code.. Thanks! Rather than deleting it, I would modify it as At the time of testing it wasn't fully cross-platform portable (lack of VS support, recent version support VS2015 and up).. That was fixed recently: https://github.com/google/flatbuffers/pull/4398\nYes, you can use FlexBuffers without lambdas, simply use StartVector and EndVector instead.. I agree that should not have leaked into user code, it needs to only be set for the test. I'll have a look.. Fixed: https://github.com/google/flatbuffers/commit/42611f9a8344833286e03cc748fbd37330b69996. Not sure I am following. inst is static, thus a global, and deallocated only on program termination.\nIt would typically not be thread-safe to share, but this once has no instance variables, so should be ok.\n@llchan wrote this new allocator design.. Sorry, I still don't see what the actual problem is in the above code. You shouldn't really be relying on the initialization order of statics anyway. And this static has no instance variables and doesn't destruct anything, so what exactly is going wrong?. Ok, I think I get it now, sorry.\nMy confusion comes from the fact that DefaultAllocator is an empty object, containing just a vtable, so the destructor is a no-op, so in theory order of destruction shouldn't matter to it. But of course in debug mode, a compiler (VS or otherwise) is allowed to overwrite the memory upon destruction, in an attempt to trip up use-after-free code, which in this case overwrites the vtable, and cause any calls to crash.\nIs that what is happening, in the error you mention?\nHolding a FlatBufferBuilder in a static is not exactly a common use case, so one could argue that documenting (\"you must call instance() first, or you must reset() before exiting main() / the thread\") could be sufficient.\nFixing it properly could be done by putting a DefaultAllocator instance in each FlatBufferBuilder. The cost of that is pretty low. We definitely don't want to get into dynamically allocating it.\n. @llchan we can't really be leaking memory though, it will show up on the various leak checkers out there.. (1) is acceptable. But could be in FlatBufferBuilder & DetachedBuffer instead?\n(2) sounds better.. maybe a few changes, but ends up with cleaner code at the end?\n(3) not a fan of this.. it's another allocation, and it be our first use of std::shared_ptr. We're trying to keep our STL usage in the core code simple, given that we want to support STLPort which doesn't have C++11 types.. Google Translate says:\nThank official support tpyescript, now large javascrit projects are basically written with typescript, but only the official version does not provide a flatbuffers.js ts version. Want to be able to provide it, so I wrote the TP projects will not see the red error.\nNot what that means.. what exactly is missing where? What is the error message?. FlatBuffers does support TypeScript, see --ts option.\nSee also: https://github.com/google/flatbuffers/pull/4232\nLast message according to Google Translate:\nWhen flatbuffers.js introduced to the typescript project when he is not a standard typescript syntax, leading to smart hints are not, there is a need to configure the typescript support and js mixed. I mean that the official can provide flatbuffers.js his Ts version. Or whether there are plans to include this in subsequent releases\nI still don't know what that means, I'm afraid Google Translate is not that good for communicating across languages. @krojew who wrote the TS support may know :). You mean the T ? Yeah that could easily be made configurable.\nYes for a PR, just add 2 strings to IDLOptions. Your example command-line arguments sound fine. Will not affect other languages.. For a 3D array like that, best to store it in a 1D array of 4096 elements. Should be easy to (de)serialize by just looping through in a given XYZ or ZYX order always.\nThe alternative is wrapping vectors in tables, and then making further vectors of them, but that sounds a lot clumsier.. Actually, beyond tables, structs and strings can be part of a union (only implemented in C++ so far).. just enums cannot. Enums are simple scalar values.. like other scalar values, they have to wrapped in a struct or table to be used in a union.. Thanks!. Makes sense!. I agree flatc should ideally catch these things.\nA lot of languages have limitations on using the same name for different kinds of \"things\", so this reuse of names is generally a bad idea. I guess the best for flatc would be to disallow any combination.. No, it can't. As the name implies, FlatBuffers wants to create the entire data structure as one flat block of memory. It does not support pointers to external memory.\nOne thing you can do however is construct a FlatBuffer which contains a vector created by CreateUninitializedVector, then later get the address of that vector in the finished buffer and use that to fill the vector. That does not allow resizing though.. Thanks :)\n. There currently isn't.. flatc does have the -M option, which prints out make rules for generated files, currently for C++ and some others (Java and C# for some reason), but will likely not work for Go. We could extend this functionality for Go. PRs welcome :). Yes, the reflection API does not specifically have such accessors, but it is pretty easy doing it yourself, by calling GetRoot<Table>(byte_address) for FlatBuffers, or similarly GetRoot for FlexBuffers.\nreflection::Obj is the type. To get that for a nested FlatBuffer, you'd have to iterate the attributes of the field, and find the nested_flatbuffer one. Sorry, that is a bit clumsy, and should really be wrapped in a helper function. We welcome a PR for that.\n. Note that flatbuffers::GetAnyFieldAddressOf< flatbuffers::Table >(*parent_table_ptr, *field_ptr); is unlikely to be correct. If the field is a vector of bytes, the start of that vector is not the same as the root table. You need to call GetAnyRoot after.\nThe correct way (for this and FlexBuffers) is to call GetFieldV<uint8_t>() on the field, then call .Data() on that, then GetAnyRoot (FlatBuffers) or GetRoot (FlexBuffers) on the result.\nThe attribute is flexbuffer, not flex_buffer. How many attributes are there for this field, and what is in them?\n. Sorry, I just go by your incomplete snippets of code, I don't have time to create test cases for every issue that gets reported.\nAs for the attributes, I just had a look, and it appears that we are only saving custom attributes to the .bfbs file, and we don't have fields in the schema to store this information. If you could make a PR to add them, that be great.. A file can only have one root.\nYou can however split it up in two files (each with their own root) where one includes the other.\nAdditionally, in e.g. C++ you can call GetRoot<Type>(..) even on things that are not declared as root_type. root_type in the end is only important for JSON parsing and some utility functions that get generated.. Yes, it would be easy to allow another extension (--schema-ext E or somehing), should be easy to add. Can you make a PR?\nIt cannot be just any extension, as it uses it to distinguish between schemas and other kinds of files.. You can also file a bug with Bitbucket to not be so inflexible with extensions :). Yup, somehow missed doing that.. sloppy! https://github.com/google/flatbuffers/commit/5fa00630afaaf676595bebbc7348c960d6c0de5a. Thanks! As noted in the PR instructions, please run sh generate_code.sh (or .bat) in tests and then add any changed generated code to your commit.. Ah yes, it is possible we forgot to update the .bat at some point, yes. .sh is the the reference :). Thanks! Yes, cd-ing back to the root dir seems like a good idea.. Sorry, somehow missed merging this! Thanks!. Ah, so we would replace the existing overloaded methods by this?\nAny other consequences from this change?\nFor example, we can't actually implement the generic template<typename T> void Add(T i), so how do we make calling Add with an unsupported type an error?\nMaybe it is safer to make a generic AddT call the existing overloaded Add ? (better name suggestions)?\n. In terms of generic programming, if I have a generic function over T, why would I prefer to call AddT<T>() over just Add() ?. Ok, then lets start with a generic As function?. That is not my understanding.  If you'd call the destructor on other, you will call the destructor twice, since the local variable in the caller's scope will also destruct it. What am I missing?\nIt seems that swap is a pretty common implementation of move, given that it means you don't need to worry about what you're overwriting, since the caller will deallocate it for you.\n@llchan who wrote this code.\n. There are only longer lifetimes if the DetachedBuffer you are overwriting actually owned a buffer, which is not the common case here.\nThe nice thing about std::swap is that it really simplifies the code. I do not believe there are actual performance issues in this case, though I am not against a PR that changes them into delete + default init of other.. @Voultapher a FlatBufferBuilder was intended as something you typically don't copy or move (that's what the DetachedBuffer is for. Not sure if there are any good use cases to do so.. We generate a function that looks up from an enum value to string, which would not work if two enums had the same value. Though I suppose we could simply not generate that function in such cases.\nI'm not sure if there were other reasons. We could consider allowing arbitrary orderings, if the above is taken into account.. No, that is not part of core functionality, so not emitting it for enums that are not contiguous is fine for the moment.\nNot sure what you mean by \"making everything a table\".. I'm fine with the current naming, it clearly expresses what is going on. Sadly the STL naming is not that clear.. Does this change any of the generated code?. ok, thanks!. Totally agree, this is problematic. I need to make the parser keyword-less.. Parser made keyword-less, which will solve these kinds of problems: https://github.com/google/flatbuffers/commit/03e2899985849dbffa3178ae76c00288f7aca79b. Thanks!. Note that (2) doesn't actually take more space, beyond the schema/code complexity.\nAll scalars have a default value. If you don't declare it, it is 0, and values of 0 will be omitted. The Field::default_integer is thus always the correct value.\nI have no better solutions than the struct trick, or force_defaults. FlatBuffers wasn't designed to have a has function with the same semantics as say, Protobuf. We valued simplicity (you can always access a field even if it is not stored) and efficiency (the most common value has no storage cost) more important at the time.\n. @IlldianX the way currently binary schemas are written (see the call to reflection::CreateField in idl_parser.cpp) will always write the default value, so force_defaults won't make any changes there. You could \"fix\" that by changing that call to instead manually calling add_myfield per field, but even then, you can't detect whether someone wrote = 0 in the schema this way.\nWe could add a flag to FieldDef stating wether a default was explicitly parsed and then store that in the binary schema too pretty easily, but I am not sure that makes any sense for anyone but you, since it has been defined from day 1 that not declaring a default is the same as writing = 0.\n\"a field is present iff there's data wired into the buffer, or the field has an explicitly declared default value\".. that wouldn't be the case either, because if the field has a declared default value but a value was explicitly not written, you're still going to conclude it is present.\nAdditionally, making use of reflection in your main code path of reading fields is slow and clumsy, not something I'd recommend.\n@schoetbi I'm not sure I understand what you mean by a version. Versioning is built-in by design in FlatBuffers. Also, forcing scalars already exists, it is the aforementioned force_defaults.\n. I'm not exactly sure what you're trying to do.\nIf you're trying to access serialized objects in a partially finished buffer, there is currently no API for that, even though it would of course be possible.. Thanks!. @lp35 didn't we conclude that the overloaded Add was sufficient for generic programming? What is still missing?. As the PR instructions above say, can you please run the code generator and include the changed Python code?\n@rw does this PR look good to you?. You can test under Linux with Mono, see tests/FlatBuffers.Test/NetTest.sh.\nYou may assume size 4, yes. It be nice to use a constant, though, either and existing or new one in Constants.java and FlatBufferConstants.cs. I was thinking, that rather than introducing all these new generated methods (which few people will use, and may confuse), why don't we simply have non-generated functions that can either advance a size prefixed ByteBuffer, or slice into a new ByteBuffer? There is nothing type specific about these functions. You'd then simply call getRootAsMonster(bb.FromSizePrefixed()) or whatever.. @robert-schmidtke do you have plans to finish this PR, and/or can @yazdan (see https://github.com/google/flatbuffers/issues/4632) help you get it merged?. Sorry to be a pain, but still not happy with the addition of off_. It is adding additions etc to pretty much all core functions that need to be very fast. Maybe the JIT is smart enough, maybe it is not, I'd rather not take this risk that this is going to make every single FlatBuffers access slower just for the benefit of more efficient slices. The core functionality simply is more imporant than slices.. Otherwise looks good to me now!. No, I'll squash it on merge.\nJust waiting for CI. I presume you are \"done\" with this PR?. Thanks for sticking with it :). @rw what do you think?. We should really just add a special purpose function for byte arrays.\nI believe this was discussed elsewhere too, not sure why it didn't happen.. @rw?. Fixed here: https://github.com/google/flatbuffers/pull/4453. Which, in particular? You mean for struct types that only have 1 field?. Ahh ok. Can you create a PR that adds explicit there ?. There is map feature in FlatBuffers implemented similarly. So from the example here: https://developers.google.com/protocol-buffers/docs/proto3#maps\nmap<key_type, value_type> map_field = N;\n\nis equivalent to:\nmessage MapFieldEntry {\n  key_type key = 1;\n  value_type value = 2;\n}\n\nrepeated MapFieldEntry map_field = N;\n\nWhich in FlatBuffers would become:\ntable MapFieldEntry {\n      key:key_type (key);\n      value:value_type;\n    }\nmap_field:[MapFieldEntry];\n\nThe (key) is what allows binary search lookups.\nNow --proto could translate that automatically, of course, but for the moment if it is blocking you, you could consider just modifying the .proto.. Yup, if you want to make a PR for it, that be most welcome.. Can you re-generate the files with --clang-nullable off, and removed from the .sh file?. Ok, looks good, thanks!. Thanks! As it says in the PR instructions, please run the code generator script as well.. Yes.. the files need to be part of the PR.. Thanks!. Great! LGTM. I think there may be another out-standing issue or PR that was also asking for this.. Thanks :). Thanks!. I presume this is in Go. A buffer shorter than 4 bytes is corrupted, where do these buffers come from, and why are the passed to FlatBuffers?\nI'm fine with changing how these errors are reported, if that makes sense. @rw?\nIn C++ we have a verifier (because C++ has no bounds checking like Go) which can catch these problems earlier.. Which line of that log should I be looking at? Since there are a lot of errors in there, most unrelated to this issue.\nwindows.h has a LOT of such defines, so just undefining these two is not solving the bigger problem. The better solution is moving that include into util.cpp. Could you make a PR?\nWhat exactly is happening? Do you have a field named ABSOLUTE ?. @DariuszOstolski thanks for checking!\nBy default, enums are prefixed, so that would be Position_ABSOLUTE, unless you use --no-prefix.. Hey, thanks for reporting!\nYes, it's unfortunate that these can get out of sync. We process our docs with doxygen, not sure if that has a way to pull in a file from elsewhere. But even if we did, all the rest of the tutorial text can get out of sync.\nI think for now just fixing these two issues is easiest, I'm hoping it won't happen that often :) Can you make a PR?. Ah yes, that is confusing. This relates to the more verbose MonsterBuilder version below, so this paragraph should probably be moved down and updated. CreateMonster already enforces the ordering the paragraph talks about.. Thanks! can you cd tests && sh generate_code.sh.. this will likely result in a modified monster_generated.h etc.. Most of these utilities don't exist in the main FlatBuffers repo, and some of them are certainly very welcome to be contributed, such as the string comparison, and anything else that leads to a canonical nicer interface that doesn't have any design trade-offs.\nNot sure about Buffer (and the Root that includes it). This simply wraps around a std::vector containing a FlatBuffer. Two issues with this: first, it promotes copying. A core feature of FlatBuffers is that you should be able to use it whereever you have access to a buffer owned by something else without copying. Second, things that are wrappers around simple std containers often have to duplicate a lot of the container they're wrapping with little benefit.\nWe already have a way to bundle a buffer with a root pointer in BufferRef. This class makes it optional to own the buffer (and free it), which I think is a better design. In many cases, you don't even need to pass the buffer around, since you can call GetBufferStartFromRootPointer to retrieve it.. Well, a FlatBuffer is simply an array of bytes, so in theory that should be something every programming language handles well. In my experience, people have trouble with FlatBuffers when they are not used to working with binary representations, and do things like write it to a file in text mode, or use some kind of array representation in their language which aren't actually bytes. I am not sure how to get around that other than maybe add more example code for these languages (e.g. writing to a file).\nIn the end, other serialization systems have the advantage that they convert things into native language objects, allowing their users to not have a clue what serialization even does.\nCorrect, BufferRef is a bit unsafe right now, it should probably have an explicit copy constructor that asserts if the free boolean is set. Or it should optionally store a length such that it can clone, but that may be undesirable.\nshared_ptr can of course work, but it is relatively heavy-weight, and requires all other users of the buffer to also be using shared_ptr, which you can't always dictate.. Not sure about generating example code. In the end the FlatBuffers API is a little harder to use for necessary reasons, and I'd worry that people using it \"blindly\" without understanding what the code does, does not improve their situation.\nYes, you can have a buffer type that's based on shared_ptr, if that is useful. I'm just saying that is a common (and desirable) case where you receive a FlatBuffer from somewhere that you don't own, and you have no control over how they manage its lifetime. It is then most useful for the default buffer type to be able to work with this.. They could use the object API, which I definitely recommend if ease of use is more important than raw speed.. This is certainly possible, though it is not supported nicely in the API. You could do something like:\nauto off = CreateDataMessage(..);\nauto dc = reinterpret_cast<const DataMessage *>(fbb.GetCurrentBufferPointer() + fbb.GetSize() - off.o);\ndc.time();\n\nIt be nice to wrap that middle line in a helper method, if you want to contribute it :). Yes, that will still work even if CreateSomeOtherMessage causes the buffer to resize, the offset is agnostic to that. Of course, the pointer isn't, so don't be creating any objects between obtaining the pointer and its use.. @spmckenney: They look generally good, I would change:\n\nI would give them a more \"obnoxious\" name, given that they are for a very specialized use case, e.g. GetPartialBufferPointer, since I wouldn't want people using that thinking it is the normal way to access objects.\nOffset should be passed by value, which also means you need 1 less variant.\nIf made member functions of FlatBufferBuilder, replace GetCurrentBufferPointer and GetSize with their implementation.\n(Make the style correspond to the rest of FlatBuffers code).. @rajukp there generally isn't an API to get the address of scalar members of a table, even on regular finished buffers. It's possible but would require some major hackery. Why do you need this?. You are best off simply making sure that for those fields you want to reuse, you still have the original source of the data (outside of FlatBuffers).\n\nIf that is not possible, I'd use code similar to what is discussed above. If these fields are scalars, you do not need the address of them, their value should be sufficient.. merged, thanks!. Merged, thanks!. Thanks!. I'm not entirely sure what you're asking.\nFirst, for the actual request/response messages, those will need to be in FlatBuffer format, so there is no automatic way to create those from a UObject / UProperties currently. You'll have to write code to construct a FlatBuffer from whatever is in your game objects explicitly.\nFor the client, there is no class structure requirement, you can put the code that creates a request wherever you want, including presumably inside these Unreal classes.\nFor the server, you need to inherit from the generated interface (in the sample case that is Greeter::Service, and implement the methods with the same interface. It may be possible to use multiple inheritance to add this directly to an Unreal class, but if not, you may be better of with a seperate server implementation that then calls into the apropriate Unreal classes afterwards.. That file is verbatim from the gRPC project.. to modify it, you'd make a PR with them first I guess. Not sure if they'd welcome an Unreal specific code generator.\nWhat is ugly about it?\nInstead you may want to just add an Unreal specific generator to FlatBuffers. Have a look to see how all the current generators work to get some inspiration for how to do that.. @krojew any idea?. Using a normal constructor would most certainly be faster for small structs than memcpy.\nThe reason we use memcpy is to retain little-endian data regardless of platform. On a big endian platform, with a regular constructor, if you read a float, it will be a little-endian float and thus temporarily garbage data. Now it gets written back into the new object, for which accessors do endianness conversion, so should in theory be ok..\nThe alternative would have been to use the accessors such that it does proper endian swap, which is of course potentially slow on big endian.\nThe memcpy is there to side-step these questions :)\nI'd be ok to change to a normal constructor if there really is no downside to having temporary garbage data being transferred.. What I mean with garbage data is that:\nVec3(const Vec3 &_o) : x_(o.x_), ... {}\n\nOn big endian, the float being copied is in little endian format, thus is potentially invalid (or even NaN etc). Though frankly I don't think this will ever be a problem, as EndianScalar causes similar floats temporarily.\nI'm not sure if we'd break something by removing the copy constructor all-together, it may well have been added for a reason. You can try, and if you make a PR, the CI will test it on a bunch of platforms and compilers.\n. Our C++11 support situation is complex. We originally supported whatever VS2010 did, which is a minimal amount of C++11. But recently we had to reduce our requirements to support STLPort in some files. \nBut no, we don't have such a #define.. Thanks! Yes, they occasionally break stuff... Typically one does not version generated files. However, FlatBuffers is a project whose functionality includes generating files. I.e., to know whether FlatBuffers is operating correctly, we must be able to see changes in generated files alongside changes in the generator in diffs. Hence they are versioned.\nAnd yes, this also means that the traditional \"out of source\" doesn't work so well. We could support it by copying that file explicitly (which would just copy over itself in the in-place version), but there are a lot of generated files, and this will get error prone.\nInstead, the approach for users of FlatBuffers that want to do out of source builds is to not build the flattest exe (which I don't see the use for if you're using it as a library). There is a FLATBUFFERS_BUILD_TESTS CMake option for that.. Maybe we should add a CMake error that if FLATBUFFERS_BUILD_TESTS is on, \"out of source\" is not supported.. Looks good, thanks!. Can you show what exactly went wrong, just in-case there's a way to do this better?\nAnd yes, would recommend always running in an environment where asserts are available, as we rely heavily on them to guard against common errors.. I agree having Finish be templated would be nice. We could template the whole of FlatBufferBuilder, though a simpler change might be to just add a second templated FinishAs that calls Finish. If you get used to using that instead of Finish in your code base, it will at least force you to think about what the root type should be, and protect against refactoring errors.. Well, you should be able to call the verifier on part of a buffer, we just don't have a nice API for it.\nauto st = CreateSomeSubTable(..);\nauto mytable = reinterpret_cast<MyTable *>(fbb.GetCurrentBufferPointer() + fbb.GetSize() - st.o);\nmytable->Verify(..)\n\nWe should really have a nice helper method for the middle part of that :). No, the two numbers are independent. They are also just default arguments that work for \"most people\", but if you don't want to be subject to max_tables simply pass MAX_INT or whatever, since the ultimate limit is the 2GB size of the buffer.\nThere is already a size-prefixed buffer support in a number of languages. Combining those into a single stream or log file is not something FlatBuffers provides, since we didn't want to get into the business of abstracting over file IO and such.\n. There probably is no documentation for this functionality yet beyond what you get searching for SizePrefixed in flatbuffers.h. Help improving documentation is very welcome.. though see my comment on your PR. . While we obviously try to avoid any API changes, they do happen, as the surface area is pretty big. We don't really track ABI changes at all.\nThe core of FlatBuffers is headers-only, and was generally made with static linking in mind.. any reason why you need this versioned shared library?\nI cannot guarantee that the major version will go up on an ABI change. Is there any point in having the version follow along with FlatBuffers releases then?. @avsej see my explanation in my previous comment.\nWhat distribution are you trying to add this to, and what for?. I understand static libraries are discouraged in a Linux distribution. My point is that there's no good reason for them to be available in a Linux distribution at all, since most users should want to link FlatBuffers statically (and get it from github rather than apt-get). That's what everyone so far has been doing.\nAnd yes, FlatBuffers is headers-only for the majority of use cases, no object files required. You only need those for more niche use cases such as doing on-the-fly JSON parsing, or reflection etc.\nI didn't know about Gentoo, not sure why they decided to add it. Are there any users of that .so?. Either way, I'm not going to stop anyone from doing so, this is just my opinion :)\n. Makes sense.. not sure how that happened.. Nice, thanks!. Agree that would be nice. We are currently not set up to produce build artifacts for all platforms though, so we'd need to do that first.. @icexelloss I don't think anyone is working on this, would you (or @laurentgo) be able to help making this happen? My understanding of Maven & co is very minimal, so no idea what the best way is to make this happen. Like I said, you'd need to produce this executable somehow.. so either Maven can spawn that, or an existing binary (from CI or github releases?) should be used.. The ByKey function should be generated for whatever field stores the dictionary, in this case e.g. myfield:[AssetInfo], not on the dictionary item itself.. We have traditionally supported GCC as far back as 4.4.x I think, but we do not actively test with multiple GCC versions, so things will break.\nIf you can make a PR that fixes these, that be very welcome. I could try and fix them (certainly the first error is easy to fix :) but I do not have access to your version of GCC here.\nThe errors all appear to be in flexbuffers.h, which is only needed for the test target, so if you ran CMake with FLATBUFFERS_BUILD_TESTS off, you may be able to get around it also.. Typically you want to use createX whenever it is available (it is sometimes not available when using e.g. structs, since those can't be created \"inline\" when passed as an argument).\nThe only reason to use startX / endX may be when you set very few fields (with createX you have to pass default for unused fields), or when they're subject to complex conditionals etc.. Hah, didn't know that was a thing in Java :)\nI presume this is for a generated createMyTable call. We can easily fix this by changing idl_gen_general.cpp:1185 that says if (has_no_struct_fields && num_fields) into e.g. if (has_no_struct_fields && num_fields && num_fields < 256). Can you make a PR for that?. That is fine too :)\nI hope noone wants to actually fill in that many values in a single call anyway :). Thanks :). I believe there has been some work on a Dart implementation, but it was never released. I don't know of anyone actively working on it, no.. I'd generally recommend supporting it through the official compiler (flatc), as that'll make it more likely the implementation is correct, complete, and will have some future support. If anyone wants to work on a PR I'll happily review it. . @dnfield the npm package is generated from code in this repo, would it work similarly for Dart?. I push to npm whenever I make a versioned release. If doing the same for Dart is just updating a version number and running a single command, I can do it as well :). flatc currently has no plugin system. On the positive side, any code generators part of flatc are pretty much guaranteed to be kept portable to all desktop platforms and compilers.. Can't load that page (has been giving me GitHub's Unicorn error for hours now). I seem to remember we are ready to merge, if there are no further comments.. It's nicer for \"history's sake\" to merge the original PR, give me a few hrs. I've contacted github about it.. It just loaded, so I merged! Congrats to all involved :)\nSince this is still fresh, if people feel we need API/code improvements, now is the time for follow-up PRs.. @isoos I haven't published anything. If I need to be the person to do that, I'd appreciate instruction on what to exactly upon a release.. I know there's a version to be update in the yamspec, and probably a script to run.\nThe docs on http://google.github.io/flatbuffers are updated manually (usually upon a release), not automatically following each PR.\n. I ran the script and it succeeded.\nIt suggested:\n* Author \"Konstantin Scheglov\" in pubspec.yaml should have an email address\n  (e.g. \"name <email>\").\n* Author \"Paul Berry\" in pubspec.yaml should have an email address\n  (e.g. \"name <email>\").\n. Yes, FlatBuffers was at its core designed to not by modified after they have been created.. it is a serialization format after all. In some languages we have options to mutate, but it is definitely no replacement for regular objects.\nIn your case, if sharing data between a sim thread and a render thread means you are making a copy of all your game state, using FlatBuffers can still make sense since the render thread typically does not mutate, so can use buffers as is. It really depends on the situation wether this is faster than just actually making a copy of all your game objects. Typically FlatBuffers will be more cache friendly and possibly more compact, but it is also more work to produce them.\nIn some languages we also have an object API that automates all this, but not in JS yet.. Yeah, you'll have to try it out.. hard to tell what will be efficient from a distance :). Ah, thanks :) Can you make a PR to fix it?. Nice fix!. Thanks!. Thanks!. I personally do not understand JS import intricacies, or why this was added like this. @krojew may though... Thanks!. Thanks :). The reason it is not there is because it is potentially very inefficient: It would have to allocate one Weapon accessor for each element, plus the array. The current Java API encourages you to re-use accessor objects to produce as little GC churn as possible.. Attributes are not part of a type, they are part of a field.\nThe wrapper would certainly work, and no less efficient than directly using a ulong.\nThere's no reason why we couldn't support the hashing attributes on vectors of integer types.. i.e. the correct way is to write: children:[ulong] (hash: \"fnv1a_64\", cpp_type: \"NodeT\"). Ok.. I do not know of the top of my head where all in the code generator this would need changes :). Thanks!. The underlying .buffer you refer to doesn't necessarily contain the data at offset 0. If you want to look at the underlying buffer, you'd also have to use .byteOffset. slice presumably makes a copy, so there it will always start at 0.. The only way to take care of it would be to make a copy. The nice thing about Uint8Array is exactly that it can refer to any slice of another buffer, which is very efficient.. merged #4510 instead.. Can you please run generate_code.sh and add changed files to the PR, so we can see what the effect of the codegen is. You may need to add a field that is a vector of hashes to the schema.. Yes, please rebase.. sorry, looks like another rebase is needed.. Ok, Thanks!. @amitsaini12345 just mentioned on https://gitter.im/google/flatbuffers that this PR broke the VS C# build (missing Referrable.cs etc).. Yes, generally data needs to be present for it to be mutable (and not equal to the default value).. Not being a JS expect, it is not obvious to me that this supports all JS versions on all platforms, in/outside browser, TypeScript, Windows etc. This makes use of some scripting to generate flatbuffers.js which makes me uneasy, can you explain how this supports all possible platforms?\n@krojew for TS. Any other JS users want to chime in?. I'd prefer it if this keeps working for people who work directly with our repo, I am sure there are some :)\nIf the cp in package.json only needs to work for the person uploading to npm (me), then I guess it is ok if it doesn't work on Windows I guess, but better yet if all this copying wasn't required at all.\nWe've tried to make changes to importing before, and it broke people using the Closure compiler. Is this likely to affect it? (again, not a JS expert :)\n. I'm confused, are you going to make changes such that flatbuffers.js is not removed/ignored?\n. Ok, so now I wonder, given that flatbuffers.js is the source of truth for future modifications, how are those changes supposed to end up in the other 2 copies? Aren't we better off if the script copies from flatbuffers.js instead? That way we don't even need to have the copies in the repo?. Well, I'd prefer it if flatbuffers.js is the source of truth, and the other 2 are copied as needed by the script, since that fits what we've been doing so far. Having 3 identical files is very confusing for contributors. The copied files don't need to be in the repo, I think.. @trxcllnt I've sent you a hangouts invite.\nApologies if this is drawn-out, my understanding of the JS eco-system is minimal, and I am wary of breaking existing users (and copies of in repos :)\nIs there no way to if-then around the offending statement?\nIf awk is not guaranteed to be installed, is there another tool? sed? We already concluded this wasn't going to run on Windows I think.. LGTM!\nI previously just used npm publish upon new version releases, is that any different now?. I'm on 3.10.10 for some reason. Noted :)\nThanks for your persistence :). What do you mean by 64bit offsets? All versions use 32bit offsets. Did you fork FlatBuffers by changing uoffset_t ?. @vishvananda We have some Python related CI in travis.yml, but it's not enabled because it currently fails (intermittently I think). If someone knows how to make it robust we can enable it.\n@tarehart I can totally make a manual release.. if someone would spell out the incantations to me that would be appreciated, since I'm not that familiar with the Python ecosystem. Better yet if @rw or someone else can do it since I didn't even set up the original.. @Tinche Thanks for the instructions!\nOk, produced the archive, and have a working twine. Made a pypy account (user: wvo). Now @rw just needs to wake up and make me a maintainer :). Ok, it has been uploaded.. but it still defaults to showing the 2015 version by default, because of the naming? There appears to be no way to change that.. There are only options for delete (which would probably be a bad idea), not hide.\nWould be good to fix this, as this could have a lot of people thinking the old versions are current, and then going thru the pain of finding out stuff doesn't work.\nKinda surprised pypi has no way to fix this.. @tarehart it seems PyPi can't really deal with how we changed the versioning scheme.\nAt this point it seems almost better to me to delete the old versions. That would cause people that depend on just flatbuffers (no version) to be silently upgraded though.. which may cause them subtle errors? @rw?. @Tinche sure. Can future releases then be wheel-only? Does publishing a wheel do anything to correct the current mess?. Thanks for the guidance, got as far as:\n```\nwvo@wvo ~/r/v/u/l/f/python> . .venv/bin/activate.fish\n(.venv) wvo@wvo ~/r/v/u/l/f/python> pip install dist/flatbuffers-1.9.tar.gz\nProcessing ./dist/flatbuffers-1.9.tar.gz\nBuilding wheels for collected packages: flatbuffers\n  Running setup.py bdist_wheel for flatbuffers ... error\n  Complete output from command python/.venv/bin/python3 -u -c \"import setuptools, tokenize;file='/tmp/pip-b8a92e4t-build/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" bdist_wheel -d /tmp/tmpgyvmbmiipip-wheel- --python-tag cp35:\n  VERSION environment variable not set, using datetime instead: 20180625194528\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\nerror: invalid command 'bdist_wheel'\n```\n. (and haven't uploaded the .whl yet). @vishvananda yes, I got:\n-$ pip install wheel\nRequirement already satisfied: wheel in /usr/lib/python2.7/dist-packages (0.29.0)\nI guess I also need to do this in the virtual env.. that wasn't obvious to me :)\n. Ok, getting similar output to you, flatbuffers (20180625205540) for the sdist, flatbuffers (1.9) for the wheel. So should I upload the wheel?. Ok, uploaded.. To commemorate the 1.10.0 releases I am doing right now, I also deleted the old releases from 2015 that kept showing up on pypi as most recent: https://pypi.org/manage/project/flatbuffers/releases/\nI know I am potentially breaking people, but it didn't seem like there were any other options.. @amiralia Yes, FlatBuffers are both forward and backwards compatible between the earliest releases and the latest versions, of all languages.. LGTM.. @bluekyu do you agree we should merge this instead of #4501 ?. Ok, thank you all, will merge.. Hmm.. not sure what the right thing is, since it does make sense to me that \"installing\" is something you only do on release builds. At the same time, dependent tools should be available regardless, though possibly not installed. Can you not refer to flatc at its build destination as opposed to its install location?\nPossibly related: changes to install paths happening in #4501 . I believe the meaning of \"install\" in CMake is probably intended to be the same as it has been in the Makefiles that it replaces: to copy from working directory to a system directory where it is available for all. \nThat can also involve debug versions of libraries I guess, but I presume they'd have to be named differently.\nThis is about an executable / tool though. I don't see why you'd ever want to \"install\" a debug build, unless you want to change the meaning of \"install\" from the above.\nAs I asked.. if you want to be running a debug build of flatc, why not access it from where it is built?\nAnyway, seems we're talking about semantics here.. I'd love for anyone else with some knowledge of CMake to chime in what the right course of action is.. You still haven't answered why in your use case you can't just run the debug flatc from where it is built.\nBut yes, other opinions very welcome. I am not necessarily an expert on this topic :). @lfjones: So, what is the \"proper\" way to do this? I can't imagine you want to end up with debug binaries in /usr/local/bin. CMake defaults to debug, and has a really clumsy way to select release, can't do much about that.\n. I wouldn't want to default CMake to Release.. I'd rather keep it the way people are expecting it to behave.\nIf you want to add some notes to that page, that be very welcome, please create a PR.. I suppose we can make it a (default off) option, sure.. It seems there is a consensus that people don't mind installing debug binaries, so lets go ahead and make that happen. Anyone want to create a PR for this?. Can you give some more context of where in the code this error is coming from? With what kind of FlatBuffer data?. What style guide wants namespaces in UpperCamelCase? Both the google one and most C++ out there (and all of FlatBuffers) uses lower case only.. That does not specify the format of a namespace. The example may be UpperCamelCase, but that is not stated as required, as far as I can tell.. Ah sorry, I missed that I put that in there.\nI don't think we'll make that auto-convert at this point though, as that would break too much code. So if you want lower-case, you'll need to use it explicitly.. That's the best grammar we have. It is probably out of date compared to the parser, so the parser is the source of truth at the moment. It'd be great to update it.. Can you be more specific?. @rw?\n. LGTM. @rw @mikeholler this indeed appears to be breaking our travis build on every commit. Is there a quick fix, or should we revert this PR?. Disabled it for now, so we can have a working CI while we figure out a fix: https://github.com/google/flatbuffers/commit/b24f2016a19a66403f3e15b5f480d9aa03a361b0. Thanks, but.. not sure how I feel about this solution. It duplicates a bunch of code (would be good to at least wrap that in a function), and doesn't fix the core bug that caused this (that there was no proper default value lookup). Also, your copy doesn't test for opts.output_enum_identifiers.. You can leave it open. Maybe someone will pick it up and improve it.. @yfinkelstein you're right.. maybe that can be fixed in https://github.com/google/flatbuffers/pull/4527 which is about to be merged :). This was closed, because?. See my comment son your PR.. Ok, looking better now. Can you add an entry in CppUsage.md, in Object based API section?. Thanks!. Looks good, thanks!. When you access builder.DataBuffer.Data you are assuming the buffer starts at offsets 0, which is not true, use .Position instead.\nIf you need to make a copy, SizedByteArray() can do that for you.. Are you able to create a PR for it?. Why should it be in its own repo? If it is in a different repo, I definitely won't be maintaining it :)\nWhat do you mean by \"conan recipe\" ? Surely Conan should keep working with older Conan files?\nIf you mean updating the FlatBuffers version in the conan file, I can of course do that.. Conan support was merged.. Looks like this failed in CI on Android STLPort. You may have to surround these constructors with #ifndef FLATBUFFERS_CPP98_STL, sorry.. That should probably be fixed. Looks like M_PI isn't standard. Might as well not include math.h and define our own constant. Can you make a PR?. Yup, looks like that commit introduced a bug. string_back should return a char &, and filepath_last_character should be a char & also. Care to make a PR?\n@smiles who authored the commit.\n. Interesting.. the problem appears to be caused by vector::data() that can legally return nullptr, and memcpy not accepting a nullptr. This is certainly very inconsistent given the semantics of an empty buffer in C/C++.\nIf we are going to fix this, I'd prefer it we'd do so further upsteam from push, since that is a frequently called function, and this fix is specific to vector::data (for example, flatbuffers::Vector::data does not have this problem, nor do many other sources of buffers.\nActually, I see that for STLPort compatibility, we use these helper functions:\ntemplate <typename T> const T* data(const std::vector<T> &v) {\n  return v.empty() ? nullptr : &v.front();\n}\ntemplate <typename T> T* data(std::vector<T> &v) {\n  return v.empty() ? nullptr : &v.front();\n}\n\nSo maybe better to fix these functions. We could replace nullptr by reinterpret_cast<T *>(&v), which I believe would be legal (since the pointer should never be accessed). Such a cast could violate strict aliasing, but again since it is never accessed that should not matter.\n@stewartmiles who wrote these functions.\n. This looks to fix the same issue as PR https://github.com/google/flatbuffers/pull/4517\nA description of a cleaner way to fix this is in issue https://github.com/google/flatbuffers/issues/4509\n. @vglavnyy thanks, but your proposed implementation is not the cleanest way to do it. I'd prefer it if this issue was fixed in the manner described by my comment in #4509. Like I said:\nThe correct fix is thus to remove the if condition and the else case (like your fix kinda does), but also add a default value lookup to the second GetField call inside GenField. The default value is available in fd.value.constant, but is a string, so needs to correctly be converted to the proper scalar type. We have a StringToInt but no StringToFloat yet.. Looks good now! Can you please rebase?. As @yfinkelstein mentions in https://github.com/google/flatbuffers/pull/4517#issuecomment-354374875 enum values should really be quoted (with \"\").. The Travis fail is flaky, by the looks of it.\nRest looks good, thanks!. Nice fix, thanks!. Sorry, there is no Java implementation of FlexBuffers yet. I guess constructing the buffer through JNI would be your only option.\nAlternatively, you can use a union in FlatBuffers, though that is not as flexible as FlexBuffers.. @Bharathvarsha FlexBuffers so far is not implemented in Go.. @Bharathvarsha the string key in that map is a limited set, you want to make those keys into fields of a table rather than a map. This is way more efficient than using dynamic map lookups. The value (the interface) should be a union.\nIf it actually has to an unknown set of keys, you can do something like this table Elem { k:string (key); v:my_union; } where my_union is a union containing all the things interface{} can point to. You then store a vector ([Elem]) sorted by key so you can binary search them like a map. Hmm.. looks like the key attribute so far is C++/Java/C# only, sorry.\n. I don't know of anyone working on it at the moment.. Why is the protocol text based? That doesn't seem to align with FlatBuffers that well.\nWriting a generator is typically not hard.. you'll get the parsed version of the rpc_server declarations and can generate whatever code you want.\nAs for other communication patterns, I personally don't have a lot of experience with pub/sub systems in how they differ from rpc's, so a motivating example would be welcome :). That sounds great! What is all involved? Can you make a PR that changes the travis file accordingly?. @krojew is this worth fixing?. CreateUninitializedVector should work just fine for structs.. Yes, you may have to cast the return type. I suppose we could add a CreateUninitializedVectorOfStructs that does that already. Care to create a PR?. Thanks for the multiple PRs, that will make it much quicker to review and merge. Start with the less contentious ones :)\nWe can keep 3 & 4 separate yes.. Oh, I am sure it can be done, but setting that up is a bit of work. Sadly FlatBuffers doesn't have quite the headcount that Protobuf does :). I'm closing this to indicating it will not be merged, in favor of individual PRs. Feel free to keep the discussion going though :). @yfinkelstein I certainly agree this would be great to have, but I don't think I have the bandwidth to create and maintain such services here internally.\nIf anything, I wonder if it wouldn't be easier to use the existing Travis and AppVeyor CI that we have to generate build artifacts for flatc on every run, than a script used by Maven can pull from.\nI don't see the point in using Maven for other languages, as they tend to have their own build/deploy systems. Using a foreign such system in addition to your own is usually unwelcome.\n. Thanks for the fix!. Agree we don't want dependencies for regular FlatBuffers users.\nWe recently started pushing jars to maven central for each version release, what does the work by @davidmoten add that we don't have yet? flatc? Note that I have very minimal understanding of Maven.\nAs for the C++11 issue that is unfortunate. We do want to keep supporting (and testing) with VS2010, so commenting that out is not a good solution. If the range based for is the only problem, can you submit a PR to grpc to replace it with a regular iterator based loop?\n. Sounds good.\nIf you can get me exact maven commands to run, I can help test. Again, not familiar with it.. @cliveseldon what still needs to be done? I believe artifacts were recently updated for Maven (see https://github.com/google/flatbuffers/issues/4681).. Hmm, maybe I forgot this when I uploaded the regular Java files.\nJust did a mvn deploy in the grpc dir, and that has finished with success. It appears to have uploaded a bunch of stuff to sonatype.org :) Presumably it will take some time to be visible.. I guess there typically would not be any reason to access them, since you're the one setting them. Can you show me an example how this is useful?. As you say, this method does not exist because its inability to indicate an actual sub-buffer makes it less than useful. Also, even if you had this method, what it would return would not be a valid FlatBuffer, as it misses a stored root offset (and possibly a file_identifier).\nWhat do you need it for?. If you want to be able to copy children, the best way is to store in a nested FlatBuffer.\nNo, parent-child offsets are unsigned, so a child is guaranteed to always be later in the buffer. However, the order of siblings is not guaranteed. Furthermore, the offsets may form a DAG.. flatc has had nested_flatbuffer for a long time, which is where flatcc got it from.\nAnd signedness is not language specific. I may use C++ as a point of reference in https://google.github.io/flatbuffers/flatbuffers_internals.html, but that is simply because C++ was the first (and most complete) implementation, and C++ makes it easy to be exact about things. For example, Java has no way to express the format exactly because it doesn't have unsigned types.. memcpy takes a size in bytes, not elements.. Ok, that sounds like an oversight, could you create a PR fix?\nNot that a root_type declaration was originally to know how to start parsing JSON, and in some generators it generates additional utility functions. There is nothing special about it beyond that. Any table can be used as the root of a FlatBuffer.. No, not quite. root_type is merely a default. Anything can be a root type. If a generator needs to do something for multiple possible roots, it should simply do it for all tables, rather than relying on root_struct_def_. There should not be multiple root_type declaration. If generating code for rpc_service needs to do something for each request/response, then either that needs to be generated for all tables (not just the root), or, if desirable, for all tables used as request/response types.. Yes, this simply hasn't been implemented for Java yet, and would be useful. @evolutional at some point said he was looking into it.\nYes, the current API sacrifices any ease of use for performance, as that has always been FlatBuffer's selling point. A friendlier API would be nice to have as an alternative.. As Kotlin can call any Java, yes, this should be straightforward.\nNot sure about using a serialization framework, as most of those assume there is an unpacking step into objects, which FlatBuffers doesn't have. You're better off using the existing API as-is if you care about efficiency.\nThere was an effort to write a native Kotlin FlatBuffers implementation at some point, but that stalled.. @rockerhieu do you know who could make a Kotlin port?. Yup, if someone can pick up @Lakedaemon's work, that be cool :). @Lakedaemon I was already fine having Kotlin support 3 years ago, otherwise I would have not reviewed the PR back then. But the PR as-is still needed work that was never finished.\nJava's API is not great because the language is really limiting in what it can represent efficiently. Efficiency is more important than API design in FlatBuffers.\nYour caveats:\n1. They are slightly un-ergonomic, I would agree, but that's the way they fitted best in FlatBuffers 1.0.\n2. Not only do other serialization formats almost universally use UTF-8 (e.g. Protobuf), in general UTF-8 has \"won\" as a format. It unfortunate there are still languages/systems that use UTF-16. I do not see a way this could have been handled better. Certainly UTF-16 would have not been a good default, and supporting both would also not have been helpful, as now you need to deal with multiple formats depending on who wrote the data. If someone wants to use FlatBuffers entirely with UTF-16 languages then of course that would be better to keep it in UTF-16, but most use cases of FlatBuffers are more cross-platform or cross-language than that. And people can store UTF-16 in an [short] if they really want to.\n3. Not sure how Kotlin implements them, but they should be integer constants, not classes.\n4. Most work on FlatBuffers is done by the community. There are endless languages and features, they are implemented when someone wants to. . We have so far merged support for 10 or so languages, and all of them have had to meet certain quality standards before they got merged. Nothing goes into FlatBuffers without review being completed, we cannot merge unfinished code that still has issues, because a) people will start using it, and if the code still has bugs or the API will still change, that has to be avoided as much as possible, experimental tag or not, and b) because many PRs are made by volunteers and we can't count on them finishing the job. They have to have a minimum bar of finished-ness when merged.\nSo I am sorry to hear the long review burned you out, but there is no other way to maintain quality and direction in a project like this. We cannot be managing levels of stability for each different feature, and we only have one branch.\nA long review is also a sign that the PR submitter wants to do things his own way rather than follow the direction of the project. For pretty much all PRs that were abandoned after long review that is the case, and it certainly is for the one you linked.\nAsking for somehow who is \"somewhat of an expert\" in a new language is very different from expecting Stroustrup to review. I am not a Kotlin expert so getting a second opinion before we commit to a certain style of API is important, especially since we didn't seem to agree on some of it.\nAnd it is cool that Go and Kotlin are about productivity etc, but FlatBuffers isn't. The whole point of FlatBuffers is efficiency. If I wouldn't need efficiency, I would not be using FlatBuffers, there are probably \"easier\" serialization systems out there.\nThere are cases where a feature can be both easy to use and also fast. That's great. But if the two are at odds in FlatBuffers, the latter has priority.\n. @Lakedaemon \nNope, if you check my previous review comments, almost none were about Kotlin, but instead about how you were implementing the generator (in C++), and you didn't appear to want to address.\nAt the time, I knew of 0 people at Google involved with Kotlin.\nAgain, you want to contribute to a project that is about maximum efficiency, and then go your own direction, and ignore code review comments. That won't work. I may not be a Kotlin user, but I know a fair bit about programming language implementation, to say the least. Until you can prove to me with a benchmark that Kotlin does magic, unnecessary dynamic allocations are not welcome in FlatBuffers :)\nYour generator does not have \"a minimum bar of finished-ness\" just because you're using it, if it ignores the direction of the FlatBuffers project. We've had probably over a 100 individual contributors, and usually code review is very friendly and it is easy to reach agreement.\nWhile the C++ implementation is always the benchmark, many of the other language ports are actually pretty solid. They have been used by large projects without ever any serious bugs (ones that would affect serialized data integrity).\nAnd no, we don't have to adhere to what is considered a friendly API in a given language. We don't do so in any of the other implementations either, they look quite foreign to how a normal object based API in those languages would look like. That is by design. FlatBuffers is not going to change that direction for Kotlin's sake.. Since this issue became mostly a rant about the past PR, if someone wants to kickstart the Kotlin port, please open a new issue and/or PR.. Can you show us the .fbs files involved? What is the flatc command-line?. Thanks! :). I'm not sure what this would do, since this code mixes reading a FlatBuffer with creating one. Is entries() referring to something in a different FlatBuffer or in the one currently being constructed? If the former, then this won't work since you can't refer to offsets across buffers. If the latter, then this can theoretically work, but I then you're probably better off doing these searches in whatever data you constructed the FlatBuffer from.. Yes, you'd have to copy the entry for this to be valid, since they're in different buffers. So even if you had the offset, that wouldn't be sufficient.\nThere is a CopyTable in reflection.h, but that is pretty complicated to use. You're probably better of just calling CreateEntry(builder, CreateString(entry->id), CreateString(entry->term)). Thanks!. fields is a pointer, so you may have to write *fields for this to work.. Looks good, thanks!. @yfinkelstein do you mind removing the date from the generated code? with it, now every PR shows an unrelated change for the java gRPC files, see e.g. https://github.com/google/flatbuffers/pull/4586/files\nGenerated code should ideally be deterministic. . That be great, yes.. They can both be in the root, as clang-format probably is only used for C++ and is explicitly run, and .editor-config just needs a `[*.{cpp,cc,h}] in front of the C++ settings, as per the example on http://editorconfig.org/\nAnd yes, we are currently not using clang format on the code. Hence why I said that I question how useful this file is, as you'll need to run it from your editor to apply only to changed lines.. We could choose to make that the standard, but then we'd first have to create a commit that clang-formats all code, since we wouldn't want to do that as part of your commit. We haven't done that so far since clang-format has some undesirable side-effects on e.g. #ifdef indentation.\nDoes your editor clang-format things outside of your control? I don't understand what the problem is to not clang-format your PRs.. I agree it is not ideal.. we can re-consider.\nI guess we can merge these files for now. If someone is going to do any formatting, at least they will have the right settings.\nAlso note there is git clang-format which only touches your changes, which would be the preferable way at the moment.. This will make you happy: https://github.com/google/flatbuffers/commit/ced00278e1e2866a69b676b81b068438198eb5f3. Sorry, I mean, this: https://github.com/google/flatbuffers/commit/89711c9c470b8ad4e2a4cbc890743852ea9d7235. If the only problem is snake_case to camelCase, can't you do that algorithmically? We already do that in some cases in the Java code generator.\nOf course, the \"mini\" in mini reflection means it doesn't offer all data the text or binary schemas do.\nAttributes could be added, I suppose, with another flag (--reflect-full ?). I say full, because if we're going to add attributes, it be worth seeing what else is being omitted and also add it.\nYou could create a PR for this if you want.\n@RobertBColton not sure what type safety you're referring to.. Thanks!. The textual version of a .bfbs is.. a .fbs file! You can load them at runtime by including the Parser in your project, and you can also use the parser to iterate through all schema information.\nI suppose converting it to JSON is an other option, but yes, that will get you a very verbose schema.\nYour segfault is because you're passing text/json to a function expecting a binary schema. You'd first need to convert it back to a binary schema in memory using the Parser. But to do that, you also need reflection.fbs. To me, sounds easier to just parse the original .fbs file.. correct.. Yes, I am very unaware of the principles of Maven, I am merely stating what makes sense to me. If this is how things are typically done then I guess leave it be.\nThe gRPC tests should typically be independent of the regular FlatBuffer tests since we don't want the main project to depend on gRPC. So calling it from TestAll.sh is probably not a good idea. \nIt will be really awkward to reference generated files located in /tests from within /grpc/tests Why? I am not familiar with limitations on directory structure in Maven.. I guess we can have a .sh in grpc for now? Yes, CMake is C++ only.. Ok, let me know exactly what Maven commands to run to test this.\n. For release of the main project, so far I have done some combination of:\nmvn clean deploy (or directly: mvn clean deploy -P release)\nmvn release:clean release:prepare\nmvn release:perform\n\nIt's hard to tell which is correct, since there's a lot of output with always at least some error, but then later it appears to have succeeded in uploading the new version anyway.\nJust now, on just mvn deploy (from grpc) it does a lot of downloading and uploading, then ends with:\n[ERROR] Rule failure while trying to close staging repository with ID \"comgoogleflatbuffers-1008\".\n[ERROR] \n[ERROR] Nexus Staging Rules Failure Report\n[ERROR] ==================================\n[ERROR] \n[ERROR] Repository \"comgoogleflatbuffers-1008\" failures\n[ERROR]   Rule \"pom-staging\" failures\n[ERROR]     * Invalid POM: /com/google/flatbuffers/flatbuffers-parent/1.8.0/flatbuffers-parent-1.8.0.pom: Project name missing, Project URL missing\n[ERROR] \n[ERROR] \n[ERROR] Cleaning up local stage directory after a Rule failure during close of staging repositories: [comgoogleflatbuffers-1008]\n[ERROR]  * Deleting context 80b35fbef5a0a.properties\n[ERROR] Cleaning up remote stage repositories after a Rule failure during close of staging repositories: [comgoogleflatbuffers-1008]\n[ERROR]  * Dropping failed staging repository with ID \"comgoogleflatbuffers-1008\" (Rule failure during close of staging repositories: [comgoogleflatbuffers-1008]).\n[ERROR] Rule failure while trying to close staging repository with ID \"comgoogleflatbuffers-1008\".\n[ERROR] \n[ERROR] Nexus Staging Rules Failure Report\n[ERROR] ==================================\n[ERROR] \n[ERROR] Repository \"comgoogleflatbuffers-1008\" failures\n[ERROR]   Rule \"pom-staging\" failures\n[ERROR]     * Invalid POM: /com/google/flatbuffers/flatbuffers-parent/1.8.0/flatbuffers-parent-1.8.0.pom: Project name missing, Project URL missing\n[ERROR] \n[ERROR] \n[ERROR] Cleaning up local stage directory after a Rule failure during close of staging repositories: [comgoogleflatbuffers-1008]\n[ERROR]  * Deleting context 80b35fbef5a0a.properties\n[ERROR] Cleaning up remote stage repositories after a Rule failure during close of staging repositories: [comgoogleflatbuffers-1008]\n[ERROR]  * Dropping failed staging repository with ID \"comgoogleflatbuffers-1008\" (Rule failure during close of staging repositories: [comgoogleflatbuffers-1008]).\n[ERROR] Failed to execute goal org.sonatype.plugins:nexus-staging-maven-plugin:1.6.7:deploy (injected-nexus-deploy) on project flatbuffers-java-grpc: Remote staging failed: Staging rules failure! -> [Help 1]\n. I hadn't tried the test script yet, since you said I needed to build/install first :)\nRunning it (with your latest commit) succeeds.\nmvn deploy now also succeeds without errors.\n. Is this good to be merged?. Thanks!. tables can be in any order, but other things, like unions, are declare before use. So make un.fbs not include anything and it should work.\nroot_type was never required. It's useful for JSON parsing and causes some extra utility functions generated for the indicated type.\n. This appears to be a bug in the \"vectors of unions\" feature that it doesn't check for null. The generated code for CarNet::BSMFrame::VerifyContentcontentVector should contain if (!values) return true at the start.\nAre you able to create a PR for this?\nunions are represented as two fields (or in this case, two vectors), so for simplicity it was decided way back when that only tables can be roots.. Fix here: https://github.com/google/flatbuffers/commit/0c86929e39cdbd7ee64da5e594833df017a40c11. Hmm.. this function is called by FileExists, whose only use is for to check where to load an include file from (if you include foo.fbs, it prefixes it with every specified include path and see if it exists).\nI have no idea how that possibly could take 33s of cpu time. How many include paths do you have? How many include statements in the file (and in the files included from it)?\nIt may be bad behavior with recursive or redundant includes, since it first checks if the file exists before it checks if it has already included this file, which could possibly be inverted.\nYou could try substituting some OS-specific code in there from e.g. https://stackoverflow.com/questions/18320486/how-to-check-if-file-exists-in-c-in-a-portable-way and see if that changes the speed in a major way.. That's still an impressive number of .fbs files :)\nLike I said, the real solution is for the code that calls FileExists in idl_parser.cpp:2277 (DoParse) to be moved further down (below if (included_files_.find(filepath) == included_files_.end())), and then make the code above depend on name instead of filepath. That would reduce the number of calls significantly.. Yes, that would be a case where this would go wrong, probably why we chose to first resolve names before including them in the map. That said though, you probably would want to be systematic about wether a path is included in paths or in names :). 1) Depends what you imagine that to look like :) I don't think we're looking to support new FlatBuffer types.\n2) Creating data in FlatBuffers is mostly possible only very sequentially, though you could create a circular buffer inside a CreateUninitializedVector if your elements were structs/scalars. Generally, creating multiple buffers sounds like a better idea though.\n3) Yes, you can specify an allocator for a builder, and that allocator can assign memory however it wants.. Yes, that is an ongoing problem, where new code generation was added but someone forgot to call a wrapping function. Thanks for finding it! PR welcome.. @ahundt : travis was broken in general.. that should be fixed now.. Great, thanks!. Should be fixed by: https://github.com/google/flatbuffers/pull/4572. Hah, I think that was just added by @yfinkelstein .. maybe that should indeed be removed.. Yes, there's no reason why we couldn't support scalars in unions. The main reason there are sometime some ad-hoc limitations is simplicity, as we've tried to limit the number of features that require special code generation given the number of languages supported. A smaller reason probably is that a union element is addressed over an offset, which is not very efficient for scalars, so it is a use case we'd want to discourage.. Features that are larger changes spanning multiple languages may still happen, it is just a question of time, and contributions.\nHaving scalars in unions doesn't seem that urgent to me, since it can already be achieved with structs.\nStructs in unions are also a very recent feature (that's currently only supported in C/C++), so yes that claim may be out of date, though still holds for all other uses of structs.. Struct types are the way they are because:\n\nThey are meant to be stored in-line in the parent table, or in a vector. For that, small size and fixed size is required.\nIn the first implementation language, C++, they are meant to be copied/stored by value. That arguably is less important in other languages, except maybe C/Rust.\nSince they can't be changed once defined, it does not make sense to use them for complex things. They were mostly envisioned to make things like 2..4 component vectors efficient, things that have a definition that never changes.. Support for (variable size) vectors in structs is unlikely at this point.\n\nWe have been discussing support for fixed size vectors in structs, though not sure if that would solve your problem. This is the most likely to actually happen, as it has been frequently requested. @mikkelfj \nSupporting nested or 2D vectors directly is another possibility that would be a logical extension of what we already have, i.e. table other { arr2 : [[int]]; }, but requires more changes in more places.. Since there may be multiple strings in a union, and since string may be a reserved word, you need to actually name them, e.g. union u { MyString : string }. Support for vectors of unions has not been implemented in the JSON parser yet, so the error is at least partially correct.. No, the \"partially\" referred to the strange u in the error message, but I guess that's the name of the type :) So ignore that.\nYes, sounds like that function should be split up (and/or inlined) if that's the case.. Given that there's no standard for binary float in JSON we had said that \"whatever strtod supports\" is the standard :). @zejal the place to implement this would be in Parser::ParseVector. If that is parsing a vector of union values, it is going to need the types to do so. In the simplest case, you would somehow pass the previously parsed field to this function, and if it is a vector of union types, you can use those to know what to parse.\nThings get a LOT more complicated to support the type field appearing after the union, which we do support for single values (see Parser::ParseAnyValue) but for vectors this would get even messier. It would definitely be acceptable to do this in two stages, to first error-out if the type field hasn't been parsed yet, and deal with that later.. So far, this project doesn't publish any pre-made binaries anywhere except for what is on the releases page. Not sure how it got on brew.\nAs long as it can be automated, I'm fine with making them available elsewhere.. CMake is used on all platforms, and pretty platform neutral.. So far, all languages that are supported is because someone wrote the implementation for them. Noone has attempted this for Objective-C yet. You are welcome to be the first!\nThat said, since FlatBuffers is all about speed, and Objective-C being a superset of C, using the C implementation sounds like the best way to go.. You are probably right.. not doing this correctly is probably just lazyness on my part :) Care to make a PR?. Thanks!. As for wether the flags should be incremental or independent, I don't feel super strongly either way. That said, the existing 2 flags are incremental, and I am not too worried about future flags, so to me incremental seems nicer.\nAs for the use of auto, that is already the code style of the rest of the project, so this PR is not the place to go argue its merits :)\n. That's a shame as a fair bit of effort was put into this already. Hope someone else will pick it up.\nAs FlatBuffers not having the same reflection facilities as Protobuf, that is likely so. Do note that this API is called \"minireflect\" for a reason, as its meant to be smaller/more efficient as the full-fat reflection API we also have.. I think that be a great idea (open sourcing the BUILD files).. @stewartmiles what do you think?. @jschaf note that FlatBuffers does not have any dependencies. It can work with GRPC but does not directly depend on it. It doesn't even use Google Test :). Yup, thanks @jschaf !. @jschaf can you give this a go?. @AustinSchuh yes, if you could finish this, that be very welcome. @jschaf do you care which way?. Thanks for the fix!. Thanks!. Thanks.. though I don't know if adding one file is worth it compared to the likely-hood people will benefit from not having this commit show up when using this specific tool... Thanks!. Ah, that's unfortunate.. I already had worries this was going to introduce side effects during code review of that commit, and this is an example.\nI guess the best fix is to add an explicit overload for char *.. care to make a PR for that?. I agree it would be. It would be need to be #ifdef-ed somehow though, since we support older compilers that do not have std::string_view.. Indeed it was.. closing!. The reason we don't have inheritance is that it is even more fragile than having it in a programming language, in terms of adding fields to the parent. A programming language can get away with this since code is typically re-compiled when parent classes change, but in FlatBuffers, we can't \"re-compile\" all data, so adding to a parent would simply not be possible.\nWhat do you suggest happens when someone adds a field to Parent ?. In C/C++ you cannot change change a file without causing all its dependents to be re-built, unless you're working with code that sits in a pre-compiled binary that is not updated, which is rare (typically dll/so's are versioned). Same for Java with all its dependency management.\nIn FlatBuffers however, working with code and data that has not been updated since a schema change is the default, rather than an exception. The consequences of persistent data being corrupt are also much greater than that of transient data. Hence the consequences of breaking schema changes are much more serious than code changes in most programming languages.\nThere was a discussion recently about safe inheritance in FlatBuffers in the gitter channel, if you're interested.. Thanks!. Thanks!. Yes, the include semantics are such that they are relative to the main file, and any include paths you set.\nThis is not necessarily wrong, e.g. C/C++ do the same thing. If you #include \"somepath/foo.h\" and then inside of there #include \"bar.h\", it will not look in somepath for you.\nThe solution is -I dirb.. I'm not sure what all this splitting up buys us. My exact problem with the current PR is that there's an mismatch: the core routines have C-style interfaces that require copying by the user. I proposed the opposite: that the core routines directly work with the types already used by the parser/generator, such that there is less translation steps. So if anything, less splitting up, more integration.. I don't think I understand. Why not just fix this PR with my suggestions?. Yes, you'll likely need templates. Break what encapsulation? There's no need for encapsulation of anything here.. Why don't you simply address my code review suggestions? From your other code you are clearly capable of doing so. The biggest waste of time is creating a feature and then not merging it because you don't want to make changes to your code.. All my suggestions are in my past code-review comments.. That's indeed weird code. The value is used to index into an array though, so negative numbers certainly won't work regardless.\nMy preference would be to leave out the cast entirely, and index directly using the enum, if the compiler(s) allow that.\nFailing that, auto index = static_cast<size_t>(e) would be best, since the indexing operator by default wants a size_t. And auto to not duplicate things.. Nice fix, thanks! Sorry for the delay.... please describe your issue in more detail. That does look like a bug, we should indeed support scientific notation.\nThere was a reason for us using fixed, though I don't recall what exactly. Maybe some of consumers of that data didn't support scientific notation, or it was merely an attempt to get readable floating point numbers (if you have 0.0 with some rounding error, it may show up as scientific notation that looks nothing like 0.0).\nWhat I am surprised by is that this is showing -0.0. Since it is a double, I'd expect it to show -0.000000178814 instead. There is nothing in that code that truncates it to a certain amount of digits, so the real question is why that is happening.\nAh, looks like the default precision is only 6, so that should be doubled for doubles, maybe as a default argument to NumToString that is then passed to std::setprecision.\nCare to make a PR?\n. Can you confirm this fixes it for you: https://github.com/google/flatbuffers/commit/bbf4dac6a3f88a8ff26f2851f183a7cef3e709c4. I think at least by default we'd want to keep fixed, as we likely have people depending on this.\nAs for the default precision, which is 6 in C++, may be because if you use 7, it is possible to get undesirable digits in some cases, i.e. it printing .9999999 when you'd expect .0.. Including @mikkelfj who had wanted to support binary float notation in flatcc, so he may have opinions.\nI'm fine with default fixed, with options for either scientific or binary float.\n. This is reworking floats and also adds hexfloat support, would that also fix this issue? https://github.com/google/flatbuffers/pull/4895. Maybe start with (float_format: scientific, float_precision: 17) kind of attributes?\nThough I am not sure I agree that this kind of thing should be in the schema at all. Maybe I get data from someone else, and their schema doesn't have attributes, but I need my output to be in a certain format. Though I guess that could be fixed with an override in IDLOptions / command line?. So.. Conan needs to inject itself into CMake files using Python to work? Doesn't sound very elegant to me, but oh well.\nAlso, as for submitting to Bintray, having binary packages available seems useful to me, though only on release versions, not every commit?. We have a repo on bintray: https://bintray.com/aardappel/flatbuffers, and travis/appveyor environment keys have been set.. @uilianries Conan appears to breaking unrelated PRs, see e.g. https://travis-ci.org/google/flatbuffers/jobs/337010509\nCan you fix?. CI that relies on heavy software installation is just a bad idea.. but that wasn't your fault.\nI guess we'll need to live with it occasionally being flakey.. @uilianries the Python code in here appears to be generating CI failures: https://travis-ci.org/google/flatbuffers/jobs/367231754\nAny ideas how that can be improved?. rotten, really? Software may have bugs, through no bad intent.\nDefault values were intended for use with tables (where fields can be omitted), I don't think we even considered them for use with structs, hence the above issue.\nWe could actually use the default values in the default constructor, or we could error on defaults in the schema parser. Given that languages other than C++ have no way to use these declared defaults, I am leaning towards the latter.. https://github.com/google/flatbuffers/commit/3694ae08171c676b0a142488303824b7d589b5e5. Fixed: https://github.com/google/flatbuffers/commit/f431a96523520617358b571911362cca2cb7abec. Yes, feel free to move all biicode references.. This was already done a while ago.. Thanks for all the work! Some notes:\n\nNot super happy with the large amounts of Python being introduced, because when any of this breaks, I am not sure I'll have time to figure out how to fix it. Is this how you normally build things with Conan? Why is this necessary, and why can't Conan do more of this out of the box?\nAs for binary build matrix, what is most useful is that people can rely on a downloadable flatc executable, which can be just x64 release mode, and does't need all other combinations. Pre-built binaries for libs is not as useful, since FlatBuffers is small and mostly intended to be statically linked, and in most use-cases is actually header-only. But I suppose it doesn't hurt to have them, if you feel strongly about it.\nIn particular, it would be great if this causes flatc for the 3 platforms to be available at a predictable url, since recently e.g. @yfinkelstein has been setting up automatic packaging for Java/Maven, where being able to pack in those binaries would be super useful. Similarly, we may want to supply these binaries for other languages / packaging systems (e.g. npm, pip..).. Ah.. for scripts (other language package managers) is there a way to programmatically figure out this URL?\n\nFor humans, I can link to these URLs from the releases page, at least it will save me building them myself.\nWhat is the state of this PR? Would you prefer to merge it and improve later?. We have a repo on bintray: https://bintray.com/aardappel/flatbuffers, and travis/appveyor environment keys have been set.. You're saying I should set up additional CI because bintray has hard to use URLs? Why should we integrate with bintray at all then?. @memsharded .tgz should not be a problem.. even on Windows most archivers support it.. @memsharded you LGTM as well?\nNot that familiar with Conan, so when y'all are LGTM I can merge.. Thanks everyone!. @uilianries I had done this already a while ago (see above), and checking in travis shows that it is set. Do these get invalidated every so often?. @Croydon we can probably do a release sometime soon. Though maybe worth it to manually trigger one for 1.9?. I've made a note to get 1.10 going.. but can't make promises, it may be a few weeks, since I am going on a trip.. 1.10 just happened (can conanfile.py was bumped just before).. @Croydon Can't find such a button.. do you mean \"Set Me Up!\" ? :). Doh! Done.. Added description/urls, and an image. FlatBuffers doesn't really have a logo, the image on http://google.github.io/flatbuffers/ is the one for the team where it was created, but I guess it works for now :). Thanks!. I think this fixes it: https://github.com/google/flatbuffers/commit/fee9afd80b6358a63b92b6991d858604da524e2b\n. You make a pull request, then we can review your changes.. That's a bug, fixed here: https://github.com/google/flatbuffers/commit/020012e69c22440d89525fd8ad7974636dc85360\nThanks for reporting!. Thanks!. Not sure what's going on with this commit.. you're trying to update the benchmark branch?. Please redo this PR by rebasing to HEAD first.. If anything, --gen-mutable should probably be implied for structs when using --gen-object-api, since codegen for structs is shared between the base API and the object API, unlike tables.. Care to make a PR?. There are two different bits of functionality here.\nFLATBUFFERS_TRACK_VERIFIER_BUFFER_SIZE computes the size of the buffer as part of verification, by tracking the furthest data it touches as it verifies. It does not store a size field in the buffer itself, and works on regular buffers, so should not be used together with the SizePrefixed functions.\nThe SizePrefixed functions store the size ahead of a regular buffer. If you use this, you should not need FLATBUFFERS_TRACK_VERIFIER_BUFFER_SIZE. Important to note that you should not mix size prefixed and regular buffers/functions, as they are incompatible, e.g. make sure you created the buffer with FinishSizePrefixed etc.\nWhile we have GetSizePrefixedRoot, I see we don't actually have a GetSizePrefixedSize or whatever, which is bad. The size is the first 32-bit of the buffer though, you can grab it yourself.. VerifySizePrefixedBuffer does exactly the same as VerifyBuffer except it expects the buffer to be prefixed by a size.\nAnd yes, the rest is correct.. This is tricky because Parser::Next is written such that it can always safely read the next character, as long as it doesn't accept '\\0'. To work non-terminated, Every cursor_++ needs to go along with a check for end of buffer, though maybe that can be wrapped in a macro. It will be harder to guarantee the code is correct.\nTypically in C++ when you pass the length of a string you do NOT include the 0 byte, so +1 should not be needed.\nWhat does Java do when it passes one of its strings through the FFI to some C code? Make a copy?\nDon't both Java and C# represent strings as UTF-16? Since FlatBuffers uses UTF-8, doesn't that imply a copy anyway?. Yes, all tests passing is not a guarantee that the code will be correct, as you'd need to prove that there's no combination of characters that, when followed by end of buffer, can allow to be reading past the buffer. We do not have tests right now that stress this.\nAnd yes, I would say this feature would only become important once \"copy and add a zero byte\" becomes a performance problem. If these languages are doing UTF-8 conversion and copying anyway, I do not feel it is an issue yet.. @smiles can you have a look?. As for the error you're getting, flatbuffers/minireflect.h does not appear in the public headers, adding it should fix it?. @smiles can you give this a final review?. flatc is a tool that you get by building this repository. See https://google.github.io/flatbuffers/flatbuffers_guide_building.html. We already have a different such tag for C#: https://github.com/google/flatbuffers/issues/4287\nHow does this standard differ from the one we already have?. Ok, do you want to create a PR to add this?. SGTM. Yes, if you declare e.g. struct MyStruct (force_align: 64) in the schema, the generated code will have _Pragma(\"pack(1)\") struct __attribute__((aligned(64))).. You're getting this error with which compiler, which version (as it passes our CI) ?. It seems the CI breakage is unrelated to this commit, will merge, thanks!. Yes, this was functionality that was necessary for us internally, and isn't documented.\nThese functions have been kept on purpose very generic, in that it is entirely up to you how you want to do the lookups, so not sure what is there is to explain.. This functionality was originally created for when you want pointers in a FlatBuffer to point to things in a different FlatBuffer (or some other kind of shared data). It was never intended to patch up things inside a single FlatBuffer.\nIt's indeed intended to run on the fly, since we wouldn't want to store a vector of things to resolve. It may be possible to do this yourself though, by storing the pointer and the hash, rather than resolving them, then resolve them later.\nAs for the short version, I'm not entirely clear what is going on there.. what do you mean by \"cannot be bound to the resolver\" ?. cool :). Thanks!. Makes total sense to me it would respect cpp_ptr_type if specified. Go ahead with a PR.. FlatBuffers wasn't really designed to append information incrementally, after it was already finished once.\nBest would be to store each set of 100 in a separate buffer.. @robatussum Not necessarily. It would use the above schema as-is (which stores elements in a vector, not linked), and to store a list/vector of buffers.. that entirely depends on the code external to FlatBuffers serialization, as it is just managing buffers.. @ronakypatel As I said above, FlatBuffers is designed for maximum efficiency of writing data just once, so appending an existing buffer is not easy, and generally is not planned as a feature. If you need to append, you can do so in the following ways at the moment:\n\nMake sure that the \"chunks\" you are appending are actually separate FlatBuffers, such that your overal data is simply a list of (size prefixed) FlatBuffers. This is the recommended way. There is size-prefixing functionality in most languages, especially for this and general streaming use cases.\nUnpack buffer with Object API, add item, and repack. While simple and convenient, this will (un)pack the whole buffer so can thus be wildly inefficient depending on the size of your data.\nThere is actually a way to append to an existing buffer without unpacking it, using the reflection functionality. This would be a reasonably efficient way to to append, but has a very cumbersome API, and has its limitations, you have been warned. Examples of modifying a buffer in-place are here: https://github.com/google/flatbuffers/blob/73a648b685bb335a0fa14254e304883979580ff9/tests/test.cpp#L781-L837. If you can add this, that be great. Maybe post in #342 rather than use a new issue?. Hmm.. I guess this is a new error in gcc? Does this mean this was entirely broken before (was not being correctly aligned) or somehow magically picked the aligned new for you?\n\nOther than 32-bit Windows, I believe most allocators are already 16-byte aligned, so this likely never showed up before.\nPossible solutions:\n\nAdd -faligned-new to the build for gcc.. not sure if this is needed on other compilers.\nExplicitly call a placement new or whatever is required to get the aligned version in that generated code.\nAdd overloaded new operators for Vec3 and similar types.\n\nCertainly that last option seems very undesirable to me. The first seems easiest, but not sure if that fixes it for all cases.. Ok.. are you able to create a PR for this?. Not seen that before. string_pool effectively contains just a bunch of uint32_t 's, so it is hard to see how that could cause a segfault on clear() unless its memory was being corrupted from the outside. I'd run under valgrind/asan & friends to see if you can see any such corruption.. Thanks!. @vitalyisaev2 created this functionality in https://github.com/google/flatbuffers/pull/4001, he probably knows best why this is the case.. Thanks!. FlatBuffers values performance above everything, including convenience and features.\nA function such as bar reads a field from a buffer on the fly, so it is super important that this is very fast. An await essentially splits up a method into multiple (resumable) functions, which is very expensive.\nIt seems to me that doing something as high-level as async on something as low level as a single FlatBuffer field access is a mismatch in abstraction levels, and it be better to do this at a higher level.\nI'm not familiar enough with JS to know if an equivalent of mmap is available, but that sounds like what you'd need.. I don't think there's a delay.. do the commits have a different email address from the one asssociated with your github account? Maybe re-create the commit?. Thanks!. Thanks! Those were indeed missing.. Ouch.. that sounds like --no-prefix is fundamentally broken for use with unions.\nNot sure how to fix this elegantly, since we can't change the name of the table. We may just have to make it such that --no-prefix only applies to enums, not unions?\nA workaround may be to stick the union in a seperate namespace.. The problem is that the fields in a oneof can be any fields, including scalars and other types that are currently not supported in a FlatBuffers union. It could be done optionally if all fields are tables like in your example.. Yes, this was probably done to circumvent Java's checked exception requirement. Not sure why we didn't go with RuntimeException, but that does seem more appropropriate.. I guess since the Error couldn't be caught, noone has code that tries to handle this, hence it is not actually much of a breaking API change?. I am estimating this is not breaking anyone, but lets see... Looks good, but needs a rebase.. Thanks!. I don't think we've ported that functionality to Python yet, unfortunately. You can try doing it manually, but it is a little bit more tricky than just pre-pending the size, since if the buffer upon Finish() has for example an 8-byte alignment, it will not have such an aligment anymore with the size. This then can cause crashes when trying to read the buffer on some platforms.\nThe best solution is to actually port the functions to Python, and ensure it has the same logic as the C++ code.. Yes, if you can help that PR to become mergable, that be great... see my comment there.. Thanks!. I typically only update Maven (and all other package managers) upon versioned releases. Not sure when 1.9 will happen, hopefully soon.. @krojew do you know what makes TS special here?. Are you going to redo this PR?. We haven't done plugins so far, supporting new languages/systems typically just goes into the main C++ code. For a new rpc system, doing something similar to our current GRPC support would definitely be easiest.\nGoing via the reflection data would be another option, and we definitely should have service data in there. Are you able to make a PR to add this data? It is a matter of adding to reflection/reflection.fbs, and extending the code in Parser::Serialize() etc.. GRPC doesn't extend the CppGenerator either, it simply outputs its own header that includes the main header. Any reason that doesn't work for you?. Thanks!. I had to temp disable these, since this was failing on some of our internal platforms due to -Wc++98-c++11-compat. I am not entirely sure what needs to be done to make it compatible, since using that flag locally does not repro the error (local variables in constexpr not being compatible with older C++ standard) for me.\nhttps://github.com/google/flatbuffers/commit/ca68d8b0433e39d788aefcd4c01a93cadcdc43f4\n. I'm definitely in favor of improving the feature detection in general.\nNot sure if just going by the official defines will work though, as e.g. for Visual Studio __cplusplus >= 201103L may be false for quite a few versions even though the feature in question is available. We can't regress in what we support.\nAlso problematic is our support of STLPort, where the compiler may be very new but none of the STL features will be available.. I actually had to disable the hash functions being constexpr at all recently (it is on master) as it broke things internally for us as well: https://github.com/google/flatbuffers/commit/ca68d8b0433e39d788aefcd4c01a93cadcdc43f4 .  It would definitely need better feature detection to be turned back on, and work with -Wc++98-c++11-compat.\nI welcome any PRs that can improve the situation.. Thanks.. can you add it?. Yes, I'm afraid we need FlatBuffers as a whole to build under STLPort. We don't have a way to do shared_ptr so this needs to somehow be #ifdeffed out, but we have no way to do that in generated code. We could stick these tests in a separate schema, but I'd prefer not to add complexity just for this. Or we could the unthinkable and leave it untested with shared_ptr, and only test it with unique_ptr or naked.. Sounds good!. Thanks!. Hah, this PR contains similar functionality: https://github.com/google/flatbuffers/pull/4445\n@robert-schmidtke @desgaz maybe check which of the two would be better to merge?. I really don't have a name preference :) I guess --size-prefixed works better for both reading and writing, as --prefix-size sounds like it is write only.\nIt would make sense to me that while reading a binary that is size-prefixed to automatically read any following buffers also. The down-side of doing that automatically is not detecting when a buffer has the wrong size in error, so maybe --size-prefixed-stream (which would imply --size-prefixed) may be better for such a feature. Would it write multiple JSON files, or use one of the formats mentioned on https://en.wikipedia.org/wiki/JSON_streaming ? Which is most common? Maybe this feature should sit in a follow-up PR.\nIn terms of API calls, it would have been cleanest to have a single call that turns a size-prefixed buffer into a regular one, and then use all existing functions. But we've already gone down the route of duplicating all these functions, so I guess we'll continue with that :(\n. Thanks!. We still support VS2010 and relatively old versions of GCC that don't have this feature. Also, since it is generated code, it being slightly prettier looking is not that much of a win against losing compatibility with older compilers.. Yes, I'd love to move minimum requirements forwards. It will take some time, sadly.. Yes, that be great. In fact, maybe stick it in idl_parser.cpp since that does currently not depend on util.h.. Thanks!. Nice, thanks!. This PR was broken in both Java and C#. In both, it had a , where a : was needed, but C# was more fundamentally broken, because it did not take into account that our handles are (value) structs.\nIt is kind of assumed that if you modify code for languages, you also test with them.\nI made a quick fix here: https://github.com/google/flatbuffers/commit/4bc6de9a881ea2f31f8f5ee302f78e1b101fa68f\nThis is also a failing of our CI, somehow our AppVeyor CI didn't run, which tests both C# and Java.. Did you miss the example? And isn't this a problem with Babel, not FlatBuffers?. Since using a newer version of CMake may force users of FlatBuffers to go through the trouble to update their CMake install (which is not always possible), I favor making it 2.8 compatible, unless there are very compelling reasons to upgrade. Can you fix the line ending?. Can you submit a PR?. FlatBuffers allows use of custom allocators, yes, but I doubt we'd want to endorse any particular one. That is up to the user.\nAlso note that nowadays FlatBufferBuilder can get by with just a single allocation, so using custom allocators for performance reasons is likely not that useful. We have the custom allocator hook for case where people want to dictate where buffers get built, to allow zero-copy scenarios etc. (as we do with GRPC).. Can you check who exactly defines these macros? I suppose this is something iOS specific as it doesn't happen on OS X.\nIt's hard to protect against macros in headers. We could either just rename the enum (which is only used internally, so should not break anyone). I suggest TYPE_INT -> FBT_INT.\nAlternatively, some #ifdef -> #undef sequence for iOS.. Yes, renaming would be best. Whoever wants to, go ahead with a PR.. Merged.. thanks @KageKirin !. How exactly is it \"hanging\"? Hanging means it is in some kind of infinite loop, which is generally not possible with FlatBuffers since all parent->child offsets must point forwards, meaning that garbage data results in an array out of bounds sooner or later.\nWe generally cannot check for all possible kinds of corrupt data, as that would be prohibitively costly, and also against semantics (some references can't be nil). We also don't want to burden the user with having to do additional error checking.\nFor C++ we have a verifier (since C++ doesn't have bounds checks), and most other languages we rely on bound check exceptions (that the user can catch if garbage data is something that can be reasonably expected). Not sure how that works in Go.. Do you have any debugging tools that allow you to break the process and get a stack-trace, to see who is hanging? Hard to advice otherwise.. Thanks!. This was fixed here, please try again: https://github.com/google/flatbuffers/commit/4bc6de9a881ea2f31f8f5ee302f78e1b101fa68f. Thanks! :). I guess that's an error in the docs, since we indeed don't support struct as root_type. Care to fix?. For the moment, I am afraid that is your only option. I'd consider making Monster a table though, its not much bigger and will be much more extensible.. What is UB about this line, exactly? regardless of wether **in is signed or unsigned, it would have the exact same result once you apply & 0x80. All bits beyond the first 8 never contribute to the end result.\nIf this needs fixing, the argument type should stay const char *, but a local that is unsigned could be introduced.. I actually suggested to keep the signature the same, and fixing it locally with an extra local. Can you make a PR?. Thanks!. Yes, sadly the way we support GRPC means we don't automatically stay in sync, we have to manually update the files in grpc/src whenever they make major changes. Can you create a PR to update it?. It's something like that, yes. Though we want to keep that file as unmodified as possible, so that namechange you mention should probably happen in the FlatBuffers specific code somewhere. And it should probably update all .cc/.h in that dir, not just the one for C++. And there may be other small changes that need to happen (something related to the GRPC_CUSTOM_STRING definition I seem to remember).. There's a bit of discussion about the issues here: https://groups.google.com/forum/#!topic/flatbuffers/UL9mgLPxdfk\nI said:\n\nThe original reason was problems around where to store the default value. A default string has to return a value of type flatbuffers::String. We can decide to store this as a static in generated code (adding statics to headers isn't great, and some code may assume that a flatbuffers::String * always points to inside the buffer). Or we can put it inside the buffer, but that bloats each buffer, even if the value isn't used.\nProbably statics is still the preferable solution, and I am not against a PR that implements it, but yes, it would have to do something useful for all languages, since we can't have one language change from returning null to returning a string later. That, or there needs to be an error in flatc when using a string default with an unimplemented language (like we already have for vectors of unions).. Thanks!. Ugh.. seems the googlebot is not happy about the CLA yet... Ok, Thanks!. Not easily, no. The binary schemas (.bfbs) files can easily be read in any language since they're just a FlatBuffer. But we don't have any supporting code in C# to any actual operations on a FlatBuffer using a binary schema, similar to the functionality that is in reflection.h in C++. If possible, linking in some C++ to do the actual work may be the quickest option.. Ah, thanks :). Quick note: anything that ends in _generated.h is a generated file, so your changes will be overwritten unless they're also changed in idl_gen_cpp.cpp.\n\nAlso cpp_generator.cc is copied from the GRPC project, so any changes there should be in the upstream project first, or will also be overwritten.. Can you rebase, so the CI can run?. @rw author of the Python port. @kbrose sounds useful to me! Maybe as part of some special tips section in the python document?\n. @rw?. Adding to the end is backwards but not forwards compatible.\nChanging any existing ordering is neither, of course :). Thanks!\nWe should really be running Bazel in our CI so it doesn't break that easily.\n. Should this be in the JS doc since there's a lot shared between them? Or would an individual doc work better?\nIt should also be mentioned on the main page :)\nAre you making a PR for this?. Thanks :). Sorry, updated now: http://google.github.io/flatbuffers/index.html\nI see Typescript still isn't linked from the main page, or the index on the side etc.. Yes, that most definitely should work... What warnings are you getting, and what exactly would need to change to fix them?\nAre you able to create a PR for this?. Made some initial comments. And yes, we need a test, docs, and generated code in the repo. Have a look at tests for some other language (e.g. Java) to see what is required.. Yes, would be great to get some feedback from @scheglov. It's hard to benchmark anything related to GC, since if there's no GC happening between the moment you create the garbage and the timing, it will indeed be cheap (most allocations are a pointer increment). It is also hard to take the effect on the cpu cache into account fairly.\nOn the one hand I am fine with high level languages (compared to C++) having a higher level (and slower API). On the other hand, the reason for FlatBuffers existence (and why people use it) is speed over comfort, so I try to default to that.. We're still missing:\n\nDocs (similar to existing languages).\nTests (again, similar to existing languages).\nGenerated code as in the repo (add your flag to generate_code.sh/.bat).\n\nAlso there was a file you deleted that you didn't intend to. Let me know when you think you have addressed all previous concerns.. @dnfield there is no need to close it, it can stay open as long as you have intentions to finish it.. Other than the license and file locations/copies no major issues at this point. Again, hard to tell since I am no Dart programmer, but we iterate after this PR.. The version should be 1.9.0, it will only be bumped when the release actually happens.. Ok, if noone has further comments, I can merge.. @krojew any idea?. The fix by @krojew above didn't change anything? Are you using a version newer than this?. Can you give a specific example how you're using the enum in the second schema?\n@krojew 's fix seems to specifically fix it for when the second schema has a field containing that enum.. in TS.. @SupJoeeeeeey let us know if this fixes it for you.. I'd encourage the author of that code to extend support for writing, and maybe contribute it to the main project to integrate with our existing parsing & code generation.. Next time just modify the existing PR instead, so comments are preserved. They are here for posterity: https://github.com/google/flatbuffers/pull/4667\nThanks for the improvements!. Awesome, much appreciated!\nWill push a docs update to https://google.github.io/flatbuffers/ so this is searchable.. 1. We typically don't make promises. But yes, we should get around to making a release.\n2. So far it has been.. 3x a year or so?\n3. Google tends to not make much promises on future support either, especially open source projects.. 1. https://github.com/google/flatbuffers/releases/tag/v1.9.0. @MitchelLabonte didn't know it hadn't arrived there.. I did run the Maven incantations that for previous releases have worked. I can try running them again?\nSadly I am not smart enough to understand what Maven all does, or at least not enough time to learn it properly.. ```\n-$ mvn release:perform\n[INFO] Scanning for projects...\n[INFO] Inspecting build with total of 2 modules...\n[INFO] Installing Nexus Staging features:\n[INFO]   ... total of 2 executions of maven-deploy-plugin replaced with nexus-staging-maven-plugin\n[INFO] ------------------------------------------------------------------------\n[INFO] Detecting the operating system and CPU architecture\n[INFO] ------------------------------------------------------------------------\n[INFO] os.detected.name: linux\n[INFO] os.detected.arch: x86_64\n[INFO] os.detected.version: 4.9\n[INFO] os.detected.version.major: 4\n[INFO] os.detected.version.minor: 9\n[INFO] os.detected.release: debian\n[INFO] os.detected.release.like.debian: true\n[INFO] os.detected.classifier: linux-x86_64\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Build Order:\n[INFO] \n[INFO] flatbuffers-parent\n[INFO] flatbuffers-java-grpc\n[INFO]                                                                       \n[INFO] ------------------------------------------------------------------------\n[INFO] Building flatbuffers-parent 1.8.0\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-release-plugin:2.5.3:perform (default-cli) @ flatbuffers-parent ---\n[ERROR] No SCM URL was provided to perform the release from\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] flatbuffers-parent ................................. FAILURE [  0.426 s]\n[INFO] flatbuffers-java-grpc .............................. SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 1.494 s\n[INFO] Finished at: 2018-05-04T08:49:59-07:00\n[INFO] Final Memory: 29M/1545M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-release-plugin:2.5.3:perform (default-cli) on project flatbuffers-parent: No SCM URL was provided to perform the release from -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n```\nNo idea what any of it means. Running with -e does not give more information on why it failed.. The pom.xml contains 1.9.. but it looks like there's various GRPC files that are still at 1.8.. I updated them just now.\n\nAlso, flatbuffers-java is not in the modules, do you normally deploy this artifact separately?\n\nI have no idea. The maven files have been authored by others.\nRe-running from the root, it now only complains about No SCM URL was provided to perform the release from. If instead I do mvn clean deploy -P release instead I get Failed to execute goal org.sonatype.plugins:nexus-staging-maven-plugin:1.6.7:deploy (injected-nexus-deploy) on project flatbuffers-java: Execution injected-nexus-deploy of goal org.sonatype.plugins:nexus-staging-maven-plugin:1.6.7:deploy failed: Server credentials with ID \"ossrh\" not found!. Sorry for the delay.. it looks like I managed to make it run succesfully..\n[INFO] Remote staging repositories released.\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 01:51 min\n[INFO] Finished at: 2018-06-07T14:25:11-07:00\n[INFO] Final Memory: 51M/2337M\n[INFO] ------------------------------------------------------------------------\n[WARNING] The requested profile \"release\" could not be activated because it does not exist.\nNot up on Maven Central yet, that usually takes a while?. Yay we have a 1.9.0!. 1. FlatBuffers just provides information in a sequence of bytes, how that is processed in IO libraries, networking or mmap-ing functionality is very platform dependent and outside the scope of the library.\n2. When read into memory you pay the cost for any paging all at once, and you fill (and possibly flush) cache. When using mmap, you typically pay paging cost on first access. With really big data sets, you may also get pages paged out over time. Even load all at once may incur paging, of course. The effects of this are very system dependent (not just the OS, hardware, amount of ram, data size etc, but also current system load). Typically I would expect a mmap-ed FlatBuffer to give about the same performance as reading it all at once with the added benefit of lower start-up time.\n3. This is a little bit harder but can be done. Essentially you can mmap a large buffer, then pass a custom allocator to FlatBufferBuilder, which will only return this block of memory.. No, how much to allocate depends entirely on your use case. You'll need to pick something that is always bigger than any data you expect to write.\nI have no experience with mmap on iOS.. Thanks!. The fact that it ignores the inner namespace is most definitely a bug. Should not be too hard to replicate the logic from the C++ generator. Can you make a PR?. Appreciated!. A \"FlatBuffer document\" is a binary file that is not self-describing, so no, in the general case you cannot tell if it is schema compliant by looking at it. In C++, we have the code-generated verifier, which can tell you if a given binary can be safely accessed with code derived from that same schema. That is not the same as being schema compliant however, for example it has no way to tell if some piece of binary data is a float or an int, a string or a vector of bytes, table A or table B, or generally any 2 things which could accidentally be \"compatible\" at a binary level.\nThis is generally the case for any binary files, i.e. you can tell if it is parseable as a certain file format, but you cannot say for 100% sure it is that file format, and only that file format.\nComparing two binaries is feasible, this could either be done with code based on reflection (for any schema), or with specific generated code for a schema. Currently there is no such code however.. A github issue is the best place for a feature request, yes.. @dnfield correct, you can have a comparison function succeed even when the binaries are different. I did not get from the OP that comparison was meant to say anything about the schema compliance.. It is documented here: https://google.github.io/flatbuffers/flatbuffers_internals.html\nWhile FlatBuffers' offset based structure (and padding) makes it very different from other formats, I wouldn't say that not being binary comparable is unique to FlatBuffers. In Protobuf for example, fields are not guaranteed to be in any particular order, so if they are binary comparable, that is a side-effect of a particular implementation, not something that is guaranteed.\nLike you say, even JSON doesn't guarantee order.. Yes, the attribute decl should really only accept valid indentifiers (or\nthe attribute usage should also allow string constants).\nAnyone care to make a PR for this?\nOn Sat, Mar 31, 2018 at 6:17 AM DanEble notifications@github.com wrote:\n\nThe problem is in attribute usage\nThat's a problem. Consider: if an attribute can not be used, why accept\nits declaration? Flatc should anticipate the inability to use the attribute\nand complain at the point of declaration.\nAs a solution - remove dot symbol form the attribute name.\nExactly. This is the approximate level of detail that flatc could and\nshould provide in its diagnostic message.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/4687#issuecomment-377692358,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AEcpYL7SEHYvm_Ed9cd5fk-gKis4vWNhks5tj4HOgaJpZM4TAx4W\n.\n. Apologies, didn't see there was already a PR out. Thanks!\n\nOn Mon, Apr 2, 2018 at 11:50 AM Wouter van Oortmerssen wvo@google.com\nwrote:\n\nYes, the attribute decl should really only accept valid indentifiers (or\nthe attribute usage should also allow string constants).\nAnyone care to make a PR for this?\nOn Sat, Mar 31, 2018 at 6:17 AM DanEble notifications@github.com wrote:\n\nThe problem is in attribute usage\nThat's a problem. Consider: if an attribute can not be used, why\naccept its declaration? Flatc should anticipate the inability to use the\nattribute and complain at the point of declaration.\nAs a solution - remove dot symbol form the attribute name.\nExactly. This is the approximate level of detail that flatc could and\nshould provide in its diagnostic message.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/4687#issuecomment-377692358,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AEcpYL7SEHYvm_Ed9cd5fk-gKis4vWNhks5tj4HOgaJpZM4TAx4W\n.\n\n\n. Ahh.. yes that may well be an unfortunate side effect of it being generated by the grpc code generator that we copy from the grpc repo. Not sure if there's an easy fix without forking that generator.. Thanks, this indeed fixes it. The solution is unfortunate though, since now we have something that looks like a string but can't be a string. Would it be cleaner to allow either string or identifier in attribute usage, and make the error message there clearly say that it expects \"either a single identifier or a string\"?. Apologies, what I meant is even simpler: both at the attribute declaration, and the use of an attribute, accept both an identifier and a string. If it is a string, no need to check it conforms with identifier rules, allow it to be anything.. Close. I believe the value of the use of an attribute is always a string.\n\nSo:\nattribute a;  // ident\nattribute \"b\";  // string, happens to be valid ident also\nattribute \"1\"; // string, not a valid ident.\n\nusages allowed: a, \"a\", b, \"b\", \"1\"\nusages disallowed: 1\n\nThere's no need to check if something is a valid ident. Simply accept ident or string at both definition and use.. Oh.. we could add a test for all possible combinations, but I think this code is simple enough that would be overkill.. Sounds like your understanding of CMake is well beyond mine. The answer to why we're doing things we do is that it grew this way :)\nAlso, we support relatively old versions of CMake, so I am not sure the \"modern\" way of doing things is supported by those? There is a cost to requiring newer versions, as it may mean things stop working for people with their currently installed version, which is especially annoying on OSes/distros where fixing that is more work than a simple apt-get.\nSo I'd love PRs that improve any of the above, especially if they can be done with our current CMake version requirements, and are not likely to break people. Changes that require bumping that version should be a little more considerate and have really strong benefits we can't have at the current version.. Thanks!. if you do flatc -t schema.fbs -- binary.bin then any imports in schema.fbs will also be available, so no need to specify them seperately. The single schema makes sense, since flatc needs to know what the root of the binary is.. There is no way to use the Parser and GenerateText without schema files, currently.\nIt can work without disk, though. You just need to call Parser.Parse with schema files in memory, in order of includes.. Thanks!. Frankly, both projects are equally to blame, since doing a #define of a relatively simple name like STRUCT_END is prone to clashes. Pretty much all #define in FlatBuffers start with FLATBUFFERS_, but somehow we missed this one.\nCare to make a PR for this? This would require changing the definitions of STRUCT_END (and probably MANUALLY_ALIGNED_STRUCT for consistency) in flatbuffers.h, their use in idl_gen_cpp.cpp, and re-running generate_code.sh.. Perfect!. Ah.. there was some generated code in this one though.. . Thanks!. I changed FlatBuffers_Library_SONAME_FULL to correspond to our current version.\nFlatBuffers makes such minor breaking changes frequently, and was not designed to be used as a stable shared library interface. It was designed for static linking (all the core code is headers-only). I allowed FLATBUFFERS_BUILD_SHAREDLIB to be added for convenience of those who insist on using shared libraries for some reason, but if is going to rely on API stability of the headers, then maybe it should be removed, as it does not make sense.\n. It depends how you use it. The default generated code and all code it relies on is all headers-only. Some people opt to do things like want to parse schemas or JSON at run-time, which is not headers only.. Thanks!. Yes, FlatBuffers was designed for backwards and forwards compatibility, and we intend to never break binary compatibility (as long as users follow safe schema evolution rules).. Yeah, not sure if this is main branch worthy.\nWhy are you including util.h on an embedded system? Are you using the parser? Ideally in a minimal environment you'd use just the generated code + flatbuffers.h\nCertainly it would need to be a lot better factored, the code is very repetitive at the moment, Would be better to isolate the differences in one place.. Precision is right in the code, and are 6 and 12 currently.\nFloatToString is kind of intended to be internal, so using it with a non-float type is \"undefined\". You do not need to cater for this case, though compile error or assert is ok.. Yes, that is not ideal code, but it is the correct way I feel. Otherwise you end up with a string that has 2 terminators and whose length is incorrect.. Yes, c_str() pretty much guarantees that all implementations have to store this terminator.. Looks good, thanks!. Thanks, looks good! You changed the code generation also, did you run generate_code.sh to see if it affects any files?. Thanks!. UnPackType(buf) is just a shortcut for GetType(buf)->UnPack() (or even more generically, GetRoot<Type>(buf)->UnPack()).. To be able to communicate with other languages and systems, and in many cases for compactness, I'd highly recommend to store data inside FlatBuffers as UTF-8.\nIf your project is Java-only, I suppose using a char type would make sense. I can imagine we could add an attribute hint (e.g. java_type) that would indicate a desirable alternative type for the API to convert to and from.. Why would you need a library? It is built-in to Java (CharsetEncode.encode, using Charset.forName(\"UTF-8\")). In fact, FlatBuffers already does this for you, if you pass a UTF-16 Java String to FlatBuffers you get it stored as UTF-8 and vice-versa.. What would that mean? Vector elements are always \"required\" the way I see it, i.e. they cannot be null.. That is probably unintentional. The way I intended FlatBuffers is for these elements to never be null. The C++ code for example will assert if you try to create a vector with a null offset. If any implementations allow null then we should fix that, rather than making it legal.. It isn't breaking in C++, because there is no way to create vectors with nulls in them, so the verifier can be made stricter.\nIn other languages, e.g. Java, while there is no assert, an offset of 0 would create an offset in the buffer that points past the end of the buffer, so would blow up the C++ verifier in a different way.\nPut differently, there is no way to even represent null in a vector, since offsets are integers, and even the value 0 \"points\" somewhere. Making the verifier stricter will potentially uncover buffers that are already corrupt, if they exist at all. Asserting on 0 offsets would be nice to add to other languages.\nSo I don't think it is a breaking change in that sense, and would be welcome to fix.\n. Looks like you're correct.. eek. Interesting that fuzzing never caught this. A fix for the above very welcome.. Thanks.. see my comments there.. @bsoniam thanks for continuing this work! Looks like we have consent from @royalharsh. Do you feel the code is in a mergable state now? Once you do, I'll review it (and maybe @rw, the author of the Python port, can have a look too).. @bsoniam did you see code review I did for the previous PR?. Yes, and @rw's ok.. I don't see the import pickle being removed. Other than that I think this is ready to merge, once @rw gives his ok regarding tests.. This should be an option (a field attribute that maybe reads java_type: char), not the default. We still want to be able to read most shorts as short.. Seems like the JSON standard allows for unicode chars to encoded naturally, so we should also support that as a flag.. Looks like you are still using the FlatBufferBuilder.cs from 1.8?. I think best would be to rename any flags in there to have identical naming to the flatc options (most already do) and then point people to the documentation for flatc. We certainly wouldn't want to document things 3 times (its already in flatc.cpp and Compiler.md).. I think having half the file having natural utf-8 and the other half having it escaped because of an illegal character in the middle is a bit weird, and not very useful. I think it is better to only output that character as \\x escaped. Alternatively, output it as-is.. Ok, thanks!. That means that the C++ code generator is not calling Name (or EscapeKeyword) when generating union_type.. can you make a PR for this?. Well, the _type field is just a field like any other, and fields are also a Definition, so escaping it should generally work.\nWhat's happening is that when union_type gets defined it does not need escaping, and then when it is used it is created from union (which is first escaped) and then concatenated with _type. So all that needs to be done is make the order of escaping and concatenating consistent between these two. Seems to me to first concatenate and then escape is the right order.. CI shows a segfault in the tests, can you investigate? https://travis-ci.org/google/flatbuffers/jobs/370278046. Looks good, thanks!. Ahhh.. some code-generators have functions like EscapeKeyword in the C++ generator to deal with this.. we'd need to add one for Python.\n@rw who wrote the Python generator.. Thanks!. Sounds like an alignment issue, yes. Floats inside the buffer are guaranteed to be 4-byte aligned, but that assumes the buffer itself is aligned. How do you allocate the buffer?\nIt could also be a float ABI issue: https://www.raspberrypi.org/forums/viewtopic.php?t=102400. The above code is for serializing. When reading, what alignment does the buffer have?. The address of p ends in a (10), which is 2-byte aligned, whereas a float on your platform needs to be 4-byte aligned. FlatBuffers guarantees floats are 4-byte aligned relative to the start of the buffer. So this means your buffer is placed somewhere in memory that is not properly aligned.. It looks like the buffer is being allocated by gRPC. You'd have to check if they provide a way to override the allocation behavior, or, failing that, first copy the message into your own buffer (which would be wasteful).\nYou could open an issue in the gRPC project asking about this.. I don't know anyone that is working on it, no.\nIn most languages it is easier to catch an out of bounds exception, not sure how well that works in Go.\nA verifier could be a nice addition.\n@rw who wrote the Go implementation.. Can you describe the actual problem in more detail?\n@rw who wrote the Python implementation.. Is either of you able to contribute a PR for a fix?. Ok, looking good, thanks!. Thanks.. somehow we did not spot that :). Awesome, thanks!. That indeed sounds like a bug, a bool internally in FlatBuffers is represented as 0 or 1, but should definitely be cast/converted to the language's bool type when accessed.\nNon-scalar types (strings, vectors, tables) are defined as not having a default, so should return a \"null\" value, rather than an empty value, since this distinguishes it from an actual empty vector that is present. I guess in Python that would be None ?\nSo your fixes look good except for the vector default.\n@rw who wrote the Python port.\n. This fixes the non-scalar case: https://github.com/google/flatbuffers/pull/4733. @Tinche appreciated :). Agreed, such annotations should not be on required fields. Does Java also have Non-nullable? And yes, an exception would be fine.\nCan you make a PR for this?. Correct, there is some special \"patch up\" code present for tables/structs, but not for unions/enums. Would be nice to add.. Well that's nice and simple!\nDid you run sh generate_code.sh to see if it has any effect on generated code files?. Thanks!. @krojew who worked on this recently. \"has expired\" ? says who? do you run some kind of script? :). @PublicParadise : this is a project where a lot of work is done by external contributors, so implementations for various languages are in constant evolution in terms of how well supported they are (TypeScript is one of the newest). We certainly don't have any deadlines for anyone. Sorry if that doesn't meet your expectations of how a project should be maintained.. Pushed new docs.. Thanks!. Would be good if it worked for JSON parsing and generation too, yes.. Yup, structs are always the same size no-matter what.. You have conflicts with your own other PR, please rebase.. I think the name is fine, since we already have force_defaults elsewhere.\nAttributes could maybe work, for scalars.. but that would still be quite clumsy since now all code that writes tables needs to additionally iterate thru fields to see if theres one with that attribute, know if it hasn't been emitted yet, etc. That may be too much work for such niche functionality.. Can you improve the description of --force-defaults as I asked above?. You updated the docs, but not the corresponding help text in the cpp.\nI'd be fine with a change to --emit-defaults and all other uses of \"force defaults\" in the project if you feel strongly about it. Feel free to do that in this PR or in a followup.. Thanks!. Ah yes, you're right, that is unfortunate. Though most important is the case where string_view is not used to not break. Would be nice to add C++17 CI but at this point we have so many possible levels of language/std/compiler support it is hard to test all possible combinations.. If we can do a one-off CI, we can also just do a permanent CI. It could possibly be added to Travis by pulling in a newer gcc manually.. Ok, looks good, thanks!. @paulreimer hey, in the code it does __cplusplus > 201402, is that maybe meant to be >= ? Some compilers that report 201402 apparently have string_view.\nAlso, this page https://en.cppreference.com/w/User:D41D8CD98F/feature_testing_macros notes very different versions for string_view (and the experimental version), where do your versions come from?\n. Yeah, not sure either what is the canonical way. I'd hope that removing the version checks and just trusting has_include would work?. This may not be that hard to add, since it already have the ability to output a .fbs file from a .proto, e.g. flatc --proto myschema.proto outputs myschema.fbs. It does this by just regenerating the internal schema representation it has just parsed, so would do exactly what you want regarding includes. It may thus be enough to add a --fbs flag or so that also calls GenerateFBS. Note of course this makes you lose all original formatting etc.. Thanks!\nThis is potentially a breaking change for some, but I feel it is necessary to correct this.\nActually, do you mind adding [BREAKING CHANGE] to the start of the commit message? Will make it easier to announce this when we release the next version.. Hah, turns out I can do that myself :). FYI @rw who wrote the Python port.. Thanks for adding the C# version!\nI don't see any tests though, can you add code to create and read a vector of unions in the regular test?. Hmm, that's unfortunate. Since it builds and no-one will be using it yet, I think we can merge, but someone trying to use this may run into unknown problems.\nIf anyone feels like adding the missing test, please go ahead, otherwise I'll get to them eventually.. Thanks!. Thanks for the fix, and thanks for explaining how this likely won't break people! That's just what I needed to hear :). Well, FlatBuffers doesn't include ctype.h either, so one of your other includes must include it transitively?\nIt is really unfortunate that ctype.h dumps these in the global namespace.. if you were controlling it yourself, cctype may work better. We can of course escape X more aggressively (two underscores?), but it is really hard to protect against all possible #define's in all C headers for all possible field names.\nA quick workaround maybe to just put #undef _X before your generated FlatBuffers file.. Ah didn't know string includes it, that's worse.. Any reason you need ./ in there?\nI guess most language have an \"include file directory\", and this is implicitly defined as being the one for the root schema being compiled. In C/C++ for example, if you do the above scenario, you will also include folder_1/children_file1.h' even if you are already infolder_2, without../`.\nYou could use -I to add more include roots. We could add a feature that automatically adds the current non-root file as a root, but not sure if that could cause ambiguous situations.. Yes, sounds like this was already fixed. So far we've never backported fixes to releases (I don't think we'd have the bandwidth), and just relied on people using master instead.. --cpp-str-type is for use with the \"object API\". The String type in the base API is just a placeholder for data in the buffer that has a specific encoding, and as such cannot be replaced by another string type.. Yes, the order of destruction between statics is undefined. DefaultAllocator is static so cannot be used together with other static things. This certainly is unfortunate.\nWhy do you need to have a Parser static though? It's a big complicated class with lots of allocations, making that static sounds.. problematic.\nI believe we've talked thru options of making DefaultAllocator non-static in the past, but somehow the alternatives seem worse.. I agree this is not ideal. I don't think we can enforce this is code. We should add it to the docs, but I don't think that will stop people from doing these kinds of things.. Hmm.. maybe I could rewrite it such that rather than using a default allocator, it simply does the default if the allocator is nullptr. That way we can avoid static all-together.. This removes the static DefaultAllocator: https://github.com/google/flatbuffers/commit/0848f58cdd848d2353b1c0a070f1f6c932d2458f. Ouch, that's not good. The proposed fix sounds good (for both callers), care to make a PR?. Fixed in https://github.com/google/flatbuffers/pull/4751. You can most definitely add fields, but you cannot remove them (you can at best rename them).. @schoetbi who wrote this functionality.. I bet that use of target was generated by .name rather than Name() in idl_gen_cpp.cpp. Care to create a PR?. Fixed in https://github.com/google/flatbuffers/pull/4752. @johngull yes, I am good at guessing these things :). The generated X_nested_root() function is simply a convenience function that gives you the root table pointer. It is equivalent to GetRoot<X>(t->X()->data()). If instead you wanted the nested FlatBuffer as a byte array, you just use the regular X() function.. In my example X is the field. _nested_root is what is appended for the root accessor, have a look at the generated code.. You also asked it here: https://stackoverflow.com/questions/50401412/struct-of-arrays-in-flatbuffer\nNo need to ask a question in 2 places.. Yes, it is intended to panic on invalid data, as it may have offsets that point to outside the buffer. We do not want to do manual error checking on each access for performance and API simplicity.\nIn C++ we have an additional verifier that can tell if a buffer is safe to read before accessing it (because C++ has no bounds checking at all). This could be ported to Go if that is useful.\n@rw who wrote the Go port.. Thanks, much appreciated!. Awesome, thanks :). Yes, that would make sense. Care to add that in a PR?\nPossibly related: https://github.com/google/flatbuffers/issues/4511. Thanks!. I agree it should default to what the others do, for now.. Thanks!. Thanks!. That is a false positive in ASAN, as the code in question can never use an uninitialized value.\nI just pushed a fix that should silence those warnings: https://github.com/google/flatbuffers/commit/3a2f6d53008dcf5f5ca43ebcae144f285b44dcc5\n. Could you make a PR to fix this?. We have looked into it in the past, and the amount of work is probably not that bad given that we already have a mode that is compatible STLPort (which is C++98). A bigger question is support going forward, we'd need C++98 CI to have any hope of maintaining this.\nMeanwhile, others are eager to start using features of newer C++ versions with FlatBuffers (e.g. we just added string_view support), so such support with cause yet more #ifdeffing, which will make more extensive CI even more important.. Yes, CI failure is unrelated, and this seems safe to merge. Thanks for the fix!. @KageKirin have a look at my comments in that PR, that may give you an impression what to fix about the PR.. Please explain why you would need this, because I don't think you do.\nYou shouldn't be using CreateUnitializedVector with offsets, as I explained on gitter.. For that, you should use the regular CreateVector call. There is no way to avoid collecting the offsets first. Note that offsets from the vector to tables must always go upwards in memory (they're unsigned), so calling CreateUnitializedVector before the tables is simply not supported by the format.\nHave your profiled that vector<Offset<..>> offsets(known_size) is what is the bottleneck in your program? Can you reuse that vector?. Added an assert to help future users of this function: https://github.com/google/flatbuffers/commit/4cfe36ae8e4dda3e3ede2ca8a6333347fd4e81da. Thanks!. I'm personally not familiar with the Python eco-system. The script in that commit kept breaking our CI though, so had to be disabled.. if someone knows how to make it more robust, it can be re-enabled.\nMore discussion here: https://github.com/google/flatbuffers/pull/4390\n@rw who wrote the Python port, @kbrose who wrote the numpy support.. Ah yes, that was discussed at length here: https://github.com/google/flatbuffers/issues/4507 and apparently there is no easy way to fix that. I think that after 1.9 has been out for a while, we should delete the old YYYYmmdd versions, as that will be the only long term fix.. Best is to simply keep fbb_ptr around, rather than the root.\nIf you have to pass root around, there is GetBufferStartFromRootPointer to recover the root pointer.\nAnd yes, adding a GenerateTextFromTable (it doesn't have to be the root!) would be a useful addition too.. Yes, would make sense to me that this is more STL compliant.. makes no sense for them to uoffset_t. Can you make a PR?. Thanks!\nI was going to mark this a \"breaking change\", but given that this functionality wasn't even working correctly before, I think it hopefully shouldn't break anything.. @vglavnyy I meant breaking for other people, i.e. if someone has code that looks like uoffset_t i = it - inventory->begin() it may now give a compilation error (on 64-bit builds). But I think that is acceptable.. I agree that be nice to have.. last I heard @evolutional mentioned he was interested in working on it, but haven't heard anything recently.. @evolutional that be great :)\n. CreateUninitializedVector was only ever intended to work with scalars, see my explanation here: https://github.com/google/flatbuffers/issues/4763\nStructs should work though. For structs, the way Offset<> refers to them is different, so it would need a CreateUninitializedVectorOfStructs function. Feel free to create a PR for them. Or as a workaround, just cast the Offset to the correct type.. Vectors of unions is a relatively new feature, which is not supported in all languages yet, and I guess also not in JSON output / parsing. PRs welcome.\nIn your workaround, you shouldn't need type if X Y Z are tables, since you can tell which one is present by it being non-null.\nI am not sure if the alternative is faster.. now you have 3 different vtables rather than 1, and it is the same number of indirections to access.. so it may well be worse.. case 2 and 3 have the same amount of indirection. A union creates two fields, a type field and a table field, so this is not an extra indirection.. Thanks for refactoring that! The travis failure looks to be a flake, I'll look into it.. Thanks!. The call stack at the moment of assert should tell you exactly what the problem is, since the caller will be a particular field in a particular table.\nThat said, debugging a verifier failure should only ever be necessary in exceptional situations, it should normally be necessary to run after construction.. No, these are just defaults, nothing more. Set them to whatever works for you.. Not sure I understand what this does that the schema type [ubyte] together with CreateVector and CreateUninitializedVector doesn't already do?\nGenerally, adding a new type to the schema language should not be taken lightly, as that really needs support in all languages to be useful.\nA quick look at your change shows that this would break backwards compatibility in reflection. Also, some of your Create functions return String offsets, which seems weird? May need more testing :). You are using the object API, which is an optional convenience API which creates a tree of objects out of a FlatBuffer for convenience and mutation (Pack/Unpack). It is not recommended to be used when efficiency matters.\nIn the base API, you can access vectors and strings with zero copies. FlatBuffers has been designed for zero copy from the start.. Why would it be useful to have such paths stored in FlexBuffers (as opposed to being passed to a function that knows how to navigate a FlexBuffer)? Can you give me a typical use case?\nAs for external or not, I guess that depends on the complexity, speed, and how useful it is. Right now, you can chain calls to navigate from the root, e.g. root.AsMap()[\"field\"].AsVector()[2], which you intend to replace with `root.Path(\".field[2]\") or something, and then parse that string at runtime? Parsing at runtime sounds.. inefficient and not really a match for an efficiency oriented library?. There doesn't appear to be anything wrong with the braces (other than indentation).\nIt complains about binary, not Java, which sounds like you are combining -b and -j --grpc on the same commandline?. I would definitely keep code that depends on reflection separate from the base API, it should be entirely optional. Reflection is there for special use cases and is generally not recommended to be used unless you absolutely need it (since it is slow and cumbersome).. You are using 2 different accounts or email addresses.. on the commit it says \"Shivendra Agarwal\", and on the PR it says \"shivendra14\" and they have 2 different pictures. I can probably merge it since you are clearly the same person, but try to not do this for any future PRs.\nDon't add another struct to the schema, there's already a vector of structs in the monster tests.\nAlso please use the same style of code as the surrounding style (google C++ guidelines). See my comments in the PR you just closed.\n(Rather than creating and closing PRs, you could just replace the commits in the existing one). Rather than overwriting that test case, you can just make a new test case that creates a Monster with just that field, and tests it. That's better than creating new fields.\nPlease have a look and compare your code to surrounding code, I think you'll be able to spot differences. That's quicker than me spelling things out.. See my review above.. Looks good!. Thanks!. Thanks!. That looks exactly what is needed.\nCould you make a PR to fix this?\nMight need something similar for nested FlexBuffers. That doesn't have a min alignment variable currently, so could be hardcoded to sizeof(largest_scalar_t) for now.. Thanks for the fix!. Thanks!. I have no idea how one uses C++ from Node, but from the FlatBuffers side, does that require anything other than the current C++ generator?. That may not work as well as you think, as both the \"library\" (FlatBufferBuilder etc) and the generated code have a fairly large API surface consisting of lots of tiny functions that do very little. Very likely the overhead of going through these V8 bindings dwarfs the work many of these functions do, and you'd risk it actually being slower than pure JS.\nThe only way I can imagine C++ (over a language binding) will be faster if it somehow does all the work at once, e.g. pass some data from JS to C++ which then constructs a buffer all in one go.. Yes, please make a PR. @jronak who wrote the GRPC Go generator.. Thanks, that is indeed more correct!. Not sure what you mean by \"blindfolded\". Scalar fields have defaults that are for binary size optimization, and are largely an implementation detail you should not care about. Knowing the presence of a scalar field is not a feature that FlatBuffers generally supports.\nThe current conversion to JSON is correct in the sense that if you convert it back, you get the exact same FlatBuffer data back out of it. It may be inconvenient for interop with other systems that are not aware of these defaults, which is why we already have --defaults-json and output_default_scalars_in_json in IDLOptions.. > This option enables to write default fields in JSON which are already there in flatbuffer binary.\nNo, it also outputs fields not present in the binary.\nI don't think I understand what you want that is different from that option.. 1. Yes, see Reference::ToString.\n2. Yes, this is even automatic. Annotate the field with the attribute flexbuffer, and when converting the FlatBuffer to JSON the nested FlexBuffer will become JSON also. Or of course call ToString yourself if you only want the nested part.\n3. This is currently not automatic, but you can call GenerateText on the nested buffer yourself.. I'd think that a FlatBuffer holding a FlexBuffer (strongly typed structure with dynamic leaves) is a combination that makes a lot of sense, and is very useful. The other way around does not make a lot of sense to me.\nThe headers are designed to be relatively self-contained, so flexbuffers.h relying on the parser (and thus being not header-only anymore) is not a good idea.. Have you tried making a map in FlatBuffers using the key attribute and vector sorting facilities (see docs/tests)? Doing multiple nesting levels between 2 serialization systems just to get maps doesn't seem ideal to me.. I'd recommend staying inside FlatBuffers :). It is pretty common practice in C++ that something which is purely \"programmer error\" is guarded by assertions only. Developing in C++ with assertions off is a dangerous idea.\nError codes would be reserved for functions that have to respond to errors in data. We do this for parsing text, obviously, but not for parsing binary since it is impossible to in a way that is always accurate, fast, and does not complicate the API.\nA highly re-usable library like FlatBuffers can't use exception handling, since it has to be usable in the many C++ projects that don't use exceptions.\nLogging could be used if it was behind an interface (since we can't make assumptions about output), but this is less than useful since the core of FlatBuffers is intended to be very high performance, and can't use logging.\nIn summary: FlatBuffers error handling is by design.\n. Let me know if this fixes it: https://github.com/google/flatbuffers/commit/a2fe49b498955bc70c0cafc037a2ccd8b1b843d0. I think having both an assert and an if is not a good idea. The assert to me makes more sense because the if hides the problem that you're working with a schema without a root. Then again, you could argue that having no root is legit, but then we should remove the assert.. Ah thanks!. Yes, we have flakey CI sometimes, sorry.\nSeems like a good fix. If anything, you may want to apply this when field.value.constant != \"0\" as well, e.g. if in the schema you write a default = true, field.value.constant will be \"1\" which does not sound what you want in Dart.. Thanks!. I'm not sure what exactly you're trying to do. If you're trying to cut up a binary FlatBuffer file depending on its contents, that is generally not possible, as tables inside a buffer are not nested self contained structures, they are structured over offsets and may share data.\nIf you wish to copy a table out of a larger buffer, the easiest is to create a nested_flatbuffer (this works in all languages). Alternatively, there is CopyTable in C++ reflection.. In Go you can only use nested flatbuffers. Simply declare your field as [ubyte], the serialize it with the contents of another flatbuffer (containing the sub-table you want copy out later). Add the attribute nested_flatbuffer to the field, which in some languages generates more convenient accessors.. Thanks!. I don't know anyone that is planning to work on this right now.. FlatBuffer serialized data has been binary compatible since v1.0, and is intended to always stay that way. Only the API may change occasionally.\nLatest master should be no worse than 1.9 :). @vglavnyy that be a nice solution, yes!. Thanks!. Not sure I understand your graphs.. what does p50 etc mean? X axis is time taken or thruput? e2e.. end 2 end presumably, differs from the other 2 tests how?\nAdding a Reserve() method that re-allocates a buffer sounds fine to me. I don't think SafeRelease() makes sense, as most users or Release() would not want allocations to happen afterwards. I expect Reserve() to be fairly niche usage (since it is only barely faster than just copying a buffer in most cases).\nTip: you can replace the 8 in your code with GetBufferMinAlignment().. As for the other issues, I am not sure if we can require identifiers to not start with _ now without breaking people. If we'd want to change that, we'd first have to make identifiers starting with _ a warning. Best solution for now is to make identifiers used in generated code like this more unique, so they're less likely to clash. . Thanks! We can address other issues later.. Thanks!. @dbaileychess would it be possible to use a Lua string as a byte buffer, at least when reading? Seems you can read individual bytes from it with string.byte. Not sure if that would be faster.. @dbaileychess instead, can you try and debug it? If it doesn't crash in a branch that doesn't have your changes, it will be related to your change. Can you run it under a debugger, you should be able to get some kind of stack trace.. Maybe add a little documentation? Mention it is supported in the main docs, translate tutorial if you feel so inclined.. @dbaileychess Probably leave it as is for now, since changing that schema affects many languages. That be better in a follow-up.. Ok, looks good.. shall I merge?. np!\n. I'm afraid I haven't heard of anyone working on it.. contributions welcome.. I don't think it will segfault by itself, but it is still a pretty bad bug! Quick fix here: https://github.com/google/flatbuffers/commit/b188fde27eeb2093ce8c5198de3eb46ee1eedabb. Ahh good point. Care to make a PR?. FlatBuffers has been designed around accessing serialized data directly from a flat buffer, as opposed to unpacking into a tree of objects, like Protobuf does. This is for efficiency. That has the downside that for anything other than similar scalars, it is hard to change the contents of a buffer after construction. Again, that is by design. Though Dart could support setters for scalars, if it wants to (several other languages have them).. That is an \"object API\" which could be implemented for Dart on top of the base API. However, generally such APIs are not recommended for use with FlatBuffers unless performance doesn't matter.. Not a fan of having many build systems in the main repo, but in C++ that seems unavoidable. We already have CMake, Bazel and Conan so far (and for Android, make).\nMy biggest problem with this is maintenance, as I will typically not test with anything other than CMake, and our CI is currently CMake only as well. When adding a new .cpp, making sure these get added to the various files is not hard, but there is the possibility of things breaking unless someone using the build system pulls upstream regularly.\nThat said, I am fine someone adding (and hopefully maintaining) gn files in the main repo. . What are the consequences of this change? Is this still usable by.. mono on Linux/mac? Unity users? Older VS setups?. yes, some assurance that we're not breaking some obvious group of people with this change would be good.\nSeems .Net standard is supported from Mono 4.6 onwards, and Unity just got 4.6 in Unity 2017.1, so we'd be excluding any older versions of Unity it seems.\nThis may not be the biggest disaster, there is always the option of including files manually.. @damienpontifex I'd presume people can also include this project you're modifying? Sorry, not much of a C# user. If including them manually is common then I guess your change will certainly do no harm.\nNot familiar with Nuget, but I am fine with including that.. Ok, lets merge this, and if it doesn't work for people we can fix/undo later!\nThanks @damienpontifex and @davidfowl, would appreciate help with nuget.. @damienpontifex @davidfowl : for some users on Unity 5.6.2 this change is causing the following error:\nnet\\FlatBuffers\\FlatBuffers.csproj(1,1): error MSB4041: The default XML namespace of the project must be the MSBuild XML namespace. If the project is authored in the MSBuild 2003 format, please add xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\" to the <Project> element. If the project has been authored in the old 1.0 or 1.2 format, please convert it to MSBuild 2003 format.\nAny idea why that is?. @davidfowl I am not sure, no.. I'll ask them to comment.. I've had to revert this PR for now, since none of us know enough about the .net ecosystem to fix this: https://github.com/google/flatbuffers/commit/705577de51961a5f2ab776a11a5ded3d03727b53\n. @davidfowl yes, please do.. @HoLLy-HaCKeR yes, we still test with VS2010 (for C++, and by extension C#).. FlatBuffers can read and write more \"liberal\" JSON than the standard. You can use options like --strict-json (https://google.github.io/flatbuffers/flatbuffers_guide_using_schema_compiler.html) to control this.. At this point, changing the default would not be easy.. CMAKE_BUILD_TYPE=Debug is the default, I believe? If not you can pass it one the CMake commandline with -D.. Yes, PR welcome.. I guess for fixed vectors it doesn't seem that useful since they're often small and thus work well with FixedTypedVector(const T *elems, size_t len). But if there's a use case where using a lambda is required, it can be added.\nYes, with those flags you should be able to create a FixedTypedVector. Using the specialized constructors is just faster since it doesn't have to use the very general CreateVector, but instead can use ScalarVector.\nWould be good to add an assert to disallow typed=false, fixed=true.\n. There might be general code paths that require you to go through the general CreateVector even with those flags.. That is indeed odd.. should probably have !fixed || inside the assert. Care to make a PR?. Yes, this is by design, since to support more fixed lengths we need more types.\nThe use case for these types are typical 2d/3d/4d vector data, colors etc.\n. In most programming languages and libraries that deal with vectors, storing x and y together is preferred, since they can be copied/operated upon in one go, as opposed to repeating code for x and y.\nA fixed vector doesn't store a size field, it is implicit in the type.. Yes, IsTypedVector etc would be welcome.. You don't always want release mode, but it is a good default for most users, and good to document that you may want to specify it.\nThanks!. I'm fine with these changes if they indeed provide performance, these are very core functions that need to fast and are allowed to have obscure implementations.\nFor especially 64-bit values, rather than 8 separate writes, does Go not have something akin to memcpy ? in C/C++, the compiler is smart enough to turn memcpy into single writes, if the size is a small constant.. @rw who wrote the Go port.. Agreed that a safe version would make sense, bounds checking could probably be added to the last function. Maybe empty string would be a better default as its easier to test against, and still will not cause printing code to crash.. Appreciated!. Would be good to add the use of these functions in the string output?. Ok, merging for now. Using these functions in the string output would still be a good idea.. Thanks!. Thanks!. Thanks!. Thoughts:\n- Thanks for the article.. always cool to see actual tests and performance data. And agreed this is treacherous territory.\n  - That first benchmark is broken btw. It makes an unnecessary copy into a std::vector (and an unnecessary allocation) and calls ReleaseBufferPointer which is unnecessary and unused.\n  - Also must say FlatBuffers vs Cap'n Proto is apples to oranges, as the former is way more flexible in terms of data layout and evolution, something you pay a slight cost for. The latter is mostly naked structs which are simply not suitable for a lot of applications without compression and other tricks (defeating zero copy). FlatBuffers value is that it gets you most of the flexibility of Protobuf at about the speed of Cap'n Proto. Best of both worlds.\n- When creating large buffers, I'd definitely hope people would use a larger initial size than 1024.. I'd hope people constructing large buffers are not on super memory constrained devices and can pick a size that fits \"most buffers\". A benchmark which largely measures the cost of reallocs is a bit pointless.\n- When your goals are maximum speed and zero copy, why use the object API at all? It is there for convenience, but certainly throws out a lot of FlatBuffer advantages.\n- For that same reason, not sure how I'd feel about a size counter method (which presumably would be generated code), as I'd expect the intersection of people that want to pay the cost of the object API and those who are worried about the number of reallocs (and cannot estimate a higher initial size) to be very small.\n. Large buffers - I wasn't suggesting computing exact sizes. I meant that for most use cases you'd know a standard distribution of possible sizes, and then you'd pick an initial size at the 75th percentile or something, and pay the cost of a realloc only infrequently.\nObject API - I'm not sure if it is a common use case, but using the object API as storage way before you even want to serialize any of it does sound convenient. Most such uses I'd expect people to already have their own data structures though, especially if they need to be queried. For simpler use cases, I'd expect people to want to accumulate data directly in a FlatBufferBuilder.\nEstimating Size - I can see the advantages for your use case, and indeed being able to detach without wasting memory is a nice bonus. I just haven't seen this combination of requirement before, so it seems niche to me.\nIt would also be really fragile, as it entirely depends on the order the object API does things in, since serializing things in different order can result in different sizes (because of alignment). So the size code would have to evolve in lock-step with the object API, and not be able to be used with any other ways of building data (base API, JSON, parser, other languages..).\nAs you said, not sure how you'd estimate vtable sizes without actually emulating serialization of all vtables, since even vtables of unrelated types may share if they happen to be binary equal. So you need a buffer of vtables to be able to compute the size. To create a vtable in the first place you need to track all fields much like the actual serializer.\nDefault values of all fields would need to be tested, since these fields are not stored.\nThe sizing code would almost be identical to the currents serialization code minus actually writing to the buffer for non-vtable data. You'd be duplicating a lot of code from flatbuffers.h (and idl_gen_cpp.cpp) in interesting ways.\nSo, very high software engineering complexity/maintenance cost, which needs the appropriate high rewards :)\nI was thinking for a second that using the mini-reflection data for this purpose might be cool (since no generated code required), but that could be even more fragile than generated code, since it is not as easy to check that it is in sync with the object API serializer.\n. The advantage of getting size exactly right is that now the start of the buffer is also the start of the FlatBuffer. Also, it is hard to define what the margin of error should be, since if an encoder repeatedly causes more alignment bytes for each element in a vector, this could be boundless. FlatBuffers does not dictate an ordering and there area already implementations that differ (some sort by field size, some just use field order, some allow the programmer to use an arbitrary order).\nI'd read FlatBufferBuilder::EndTable (or the same in any other language) + https://google.github.io/flatbuffers/flatbuffers_internals.html to understand more about vtables.. Yes, that code just uses the existing Pack, but with a default buffer size that will get realloced. I am not sure how that relates to your posts above?\nAlso, using ForceDefaults such that you can keep changing the default values sounds like a bad idea. You're wasting space on the wire, and if at some point you create some data in some language without this flag you'll be creating incompatible data. Not sure why you need this.\n. Thanks!. LGTM if @rw is ok.. Oh, does any of this affect users of older versions of C#, e.g. Unity users?. Ok, sounds good!. This change appears to breaking tests/FlatBuffers.Test/NetTest.sh (mono on Linux):\n../../net/FlatBuffers/FlatBufferBuilder.cs(191,26): error CS1502: The best overloaded method match for `FlatBuffers.ByteBuffer.Put(int, byte)' has some invalid arguments\n../../net/FlatBuffers/ByteBuffer.cs(283,21): (Location of the symbol related to previous error)\n../../net/FlatBuffers/FlatBufferBuilder.cs(191,38): error CS1503: Argument `#2' cannot convert `T[]' expression to type `byte'\nByteBufferTests.cs(403,31): error CS1502: The best overloaded method match for `FlatBuffers.ByteBuffer.Put(int, byte)' has some invalid arguments\n../../net/FlatBuffers/ByteBuffer.cs(283,21): (Location of the symbol related to previous error)\nByteBufferTests.cs(403,41): error CS1503: Argument `#2' cannot convert `T[]' expression to type `byte'\nByteBufferTests.cs(580,60): error CS1502: The best overloaded method match for `FlatBuffers.ByteBuffer.Put(int, byte)' has some invalid arguments\n../../net/FlatBuffers/ByteBuffer.cs(283,21): (Location of the symbol related to previous error)\nByteBufferTests.cs(580,70): error CS1503: Argument `#2' cannot convert `float[]' expression to type `byte'\nByteBufferTests.cs(591,56): error CS1502: The best overloaded method match for `FlatBuffers.ByteBuffer.Put(int, byte)' has some invalid arguments\n../../net/FlatBuffers/ByteBuffer.cs(283,21): (Location of the symbol related to previous error)\nByteBufferTests.cs(591,66): error CS1503: Argument `#2' cannot convert `float[]' expression to type `byte'\nByteBufferTests.cs(609,56): error CS1502: The best overloaded method match for `FlatBuffers.ByteBuffer.Put(int, byte)' has some invalid arguments\n../../net/FlatBuffers/ByteBuffer.cs(283,21): (Location of the symbol related to previous error)\nByteBufferTests.cs(609,66): error CS1503: Argument `#2' cannot convert `FlatBuffers.Test.ByteBufferTests.dummyStruct[]' expression to type `byte'\n@dbaileychess Can you create a fix?. Just reverted this PR so we're not breaking anyone while this is being fixed, please re-apply once tested: https://github.com/google/flatbuffers/commit/1f5eae5d6a135ff6811724f6c57f911d1f46bb15. I have: Mono JIT compiler version 4.6.2 (Debian 4.6.2.7+dfsg-1). Ah.. simple.\nThat may be needed, if github regards this one as closed.. Thanks!. Thanks again :). Thanks!. Thanks! I bet it is in other ports that forked off these ones... Just fixed it for Lua :). Like @shivendra14 said there is no need to specify scalars to be required, since they are always readable (and thus @shivendra14 workaround doesn't buy you anything either).\nWhat exactly are you trying to do?. @NN I'm afraid there's currently no way to do that, and I see no easy way to implement that, as you can't tell the difference between an unset value and a set value that happens to be equal to the default (by design).\nYou can use features like ForceDefaults() and IsFieldPresent() in C++ to work around this, at the cost of bigger binaries, but that is at read time, not construction time.\nAPI-wise, if you use the Create functions for a table type, those will force you to supply a parameter for each field, as opposed to using the add_field functions individually.. @dnfield. Are there no compilers that support template aliases but do not have __cpp_alias_templates defined (feature flags seems to be also a recent feature)?. That may be safer in that case, don't want to break people who are not affected.. Yes, some implementations (e.g. Java) will assume string means UTF-8, and unpack it into their native string representation. So [ubyte] is still recommended as the default if you care about interop.\nIf you only work in C++ and and your code uses strings to store data thruout, you may be better off with string however.. If you can guaranteed sizes don't change, I would bypass reflection (it is indeed heavyweight) and directly modify memory. You are totally safe to call data() on a string, const-cast it, and write to it (up to size() obviously).\nWe actually have multiple Mutate functions already in Vector (which String inherits from), but not MutableData or similar (or just an overload to data). You'd be welcome to add them.. This is somewhat of a mess, since these started out with typical C++ syntax (my_table : MyTable) which we then automatically converted to what is typical in other languages when we added support for them later.\nSo no, there is no general mechanism to avoid such situations, and they are language specific, so would need to be fixed on a per generator basis.. Thanks!. Thanks!. published: https://pub.dartlang.org/packages/flat_buffers. Thanks!. Yup, fix forthcoming.. Should be fixed here: https://github.com/google/flatbuffers/pull/4871. That code is indeed broken, it should check the vtable size first, like all normal accessor functions do.\nFixed it here:\nhttps://github.com/google/flatbuffers/commit/c80f8d18c15c1bbe85b47bb3943bd772f0e5841e\nAs you can see the fix is to call GetOptionalFieldOffset which checks the size correctly.\nThanks for reporting!. This is getting kinda multi-layered though, we created DetachedBuffer so people could hold on to the memory inside a FlatBufferBuilder. This made sense at at the time since FlatBuffer Builder stored extra memory for book-keeping, but since then that book-keeping has been removed, so now a FlatBufferBuilder and a DetachedBuffer use a similar amount of memory (one dynamic alloc each)!\nSo rather than creating 3 layers of buffer, I'd say if this functionality is needed, we should simply add a FlatBufferBuilder::ReleaseRaw instead.\nWe could almost remove DetachedBuffer at this point, but I guess people rely on it.. And actually, we'd have to remove at least a triple: buffer, size, and start offset, since the latter is usually not 0. It is for this reason we have this custom class anyway (a unique_ptr with a custom deleter caused too much trouble).. @gabyx yup, though since we're still compiling on older compilers, a traditional API with some & args may work better :). @paulreimer yes, both classes are essentially a single dynamic memory allocation + a bunch of inline scalar/pointer sized fields (DetachedBuffer 6 of them, FlatBufferBuilder 10 or so, if you include what is in vector_downward). So unless your buffer is really small the memory usage is identical.\nThe only practical difference may be that FlatBufferBuilder may miss e.g. move constructor, if you need it?. Sure, a PR is welcome, but again, I wouldn't use a tuple.. @sutambe I'll have a look there.. Thanks for also updating the docs!\n2.8.11 should be fine, yes.. If there's one thing that FlatBuffers was never designed to work well for, it is in-place resizing updates. Reflection works, but it is fairly slow and limited, though I suppose it is ok if you only want to touch one field at a time.\nI don't really have any brilliant ideas on how to improve it. Trying to reserve space inside a FlatBuffer like you suggest sounds difficult to manage, and requires this compression pass.. I'd stick with the existing reflection in that case.\nIf you touch many fields, the object API may actually be faster, but of course that requires specific knowledge of fields. That could be solved by having a callback, i.e. the user of the API would be given a FlatBuffer, and has to return one. It can then do that transformation either entirely in-place when possible (mutation functions), with reflection, or with the object API (which is just a simple UnPack -> change stuff -> Pack).\n. @victorstewart we already have such an offset table, it is called the vtable (more here: https://google.github.io/flatbuffers/flatbuffers_internals.html ). We also already have the ability to remap the offsets, this is exactly what the reflection code does when resizing.\nIn addition to that though, for your idea, we'd need to track what amount of empty space is present beyond an array, and then later compact that.. Thanks.. this file is copied from upstream GRPC project though, so unless it is also fixed there, this fix may regress.. The AppVeyor one I think I just fixed. The Travis one is a bigger issue I need to look at soon, ignore the 3rd test.\nOtherwise this looks fine to me, but what does it buy us? Will these binaries be available somewhere?. Yes, I have nothing against this being supported, since we already allow it for sub-tables.. The main page has a link to GitHub? http://google.github.io/flatbuffers/index.html\nIf you want, can you submit a PR to add it to the building page also?. Yup, we should. Care to make a PR?. If it is caused by just a few bytes being added or removed, it sounds like the Dart implementation has an alignment problem of some sort.. @dnfield can you have a look at this?. just ran publish.sh. Though the parser in general was always meant to accept more than JSON standard, it would indeed be nice to clean some of these up that are just sloppy and serve no purpose.\nCertainly just - is very nonsensical, and anything that will not be accepted by strtod we use internally should probably be prohibited as well. I don't see a problem with .2 and 1. though, which are fine in quite a few languages.\n\nMy guess is no, especially not if stroll etc interpret it as octal.\nIf strtod accepts it then I am fine accepting it. Again, we're not trying to validate JSON, merely to consume it. Then again I'd be fine to disallow it if people feel strongly against this.\nI would say yes. The grammar is likely out of date and would be nice to update together with whatever parser fixes we make.. Ok, thanks!\nWe should not support octal if JSON doesn't.. I would vote to accept both if strtod does. Like I said, I'd like to be on the liberal side of what we accept, which means including C/C++ style literals.\n\nTightening what we accept now also has the problem that we might break people, so unless it is complete nonsense like -, I suggest we don't.\nThat is just my sense though, if anyone has other opinions, I'd be happy to hear.. First: This was simply to support existing JSON data where everything is put in \"\" regardless of the data type.\nSecond: that is merely an oversight, and should probably also be supported, for consistency.\nThird: Correct.. This will likely affect the generated code, so please be sure to run generate_code.sh and include the resulting changes.\n@dnfield does this seem like a good change for Dart?. Thanks!. Hmm.. though I can see that being useful, we do currently not support private types for any of the languages we support, and have no way to indicate that in the schema.. though it shouldn't be hard really (can be a private attribute).\nDo you feel like making a PR for this?. Thanks!\nYou might as well add this feature to Java while you're at it.. Is it not possible to have a cached ByteBuffer and swap the byte[] in and out of it?\nI'd like the idea of naked byte[] accessors but at this point that is far too backwards incompatible. ByteBuffer was chosen to not have to deal with creating scalars out of bytes ourselves (and the hope that this was under the hood faster than lots of << and | ops).\nCertainly accommodating both at runtime is a no-go since we wouldn't want to create overhead, no matter how small. A double set of base classes sound bad too. Sad that Java doesn't have proper templates or macros or something.\nWhat do you mean by the \"instanceof\" approach?\n. I'd be very hesitant to rely on the optimization ability of a particular Java implementation.\nHave you tried benchmarking on Android for example?. Not that I know.. I'm afraid we don't. We have some Android tests but they're not benchmarks.. That would still need to be checked, though. We really can't afford code that regresses performance on any platform, and if-thens inside such a low level functions would have used impact if not guaranteed static.. @toddlipcon It can be done, it just needs someone to put in the effort to prove it safe with some numbers to back it up for all platforms.. Hmm.. that code has been in there for quite a while and has worked on all sorts of Linuxes. Could it be some non-standard libc? Your compiler is somewhat old, but that code doesn't use any new features afaik.\nThis suggest (re)installing libstdc++ https://stackoverflow.com/questions/34117122/undefined-reference-to-stdistreamseekglong-long-std-ios-seekdir. Not sure then, sorry.\nCan you compile a little test program that uses tellg? Can you build FlatBuffers on a different Linux you may have access to?. Thanks!. Hah, wasn't aware Bazel even supported Windows, I guess that is new? Someone who knows Bazel want to fix this? @jschaf maybe?. If they can be removed, that would certainly be cleanest.\n-Wno-implicit-fallthrough is on in newer gcc by default, and off in older versions, so I presume it is there to avoid regressions.\n. Thanks!. Thanks!. Thanks!. Thanks!. Thanks!. It's not super obvious from the error what the compiler has trouble with, but I am guessing it doesn't recognize MANUALLY_ALIGNED_STRUCT. It's defined in flatbuffers.h for VS, Clang and GCC, and will #error for anything else.\nWhat compiler is this \"Visual Micro compiler\" based on? If GCC, what version?. That sounds weird, then. Can you look at the definition of MANUALLY_ALIGNED_STRUCT and do some tests to isolate the problem? Like check which of the pre-processor defines used in that code are available on your platform?. You need to maintain the include structure as it is in the repo (include/flatbuffers/*.h), then set an include dir to include/.. @krojew any idea?. We have so far done these one a per language basis: C++, Dart, Go, Lua, Python have them, Java, C#, JS, TS, PhP do not. So it just needs someone to add them for JS/TS.. Thanks!. We certainly want to keep supporting older C# compilers too (e.g. Unity tends to behind), so an #ifdef may be needed. Also we want to be careful to not slow things down with extra indirections or conditionals. Would Span be slower than always directly using byte[] ?. We'd like FlatBuffers to run on a wide variety of JS platforms, so does TS translate let into var for pre-ES2015 JS targets?\nOther than that, sure, PRs welcome.. Yes, that looks like it was only implemented for C++ so far.. That is likely quicker, yes. Of course, rather than calling the above function, you can call startVector yourself and do an arraycopy, though it would be nice if that was wrapped in a helper function and/or the generated code is updated for cases where this is possible.. Sure, we can generate special purpose code just for byte.. Looks great, thanks! Would be nice to add to other languages as well.. I would just keep it super simple, e.g. uint8_t *DetachRaw(size_t *size, size_t *offset). This is very low-level access, so the API doesn't need to be pretty.. 7/include/flatbuffers/flatbuffers.h:611:54: error: declaration of \u2018size\u2019 shadows a member of 'this' [-Werror=shadow]\n   uint8_t *release_raw(size_t &size, size_t &offset) {. see travis output. Looks good, thanks!. Thanks!. Thanks!. Thanks, agree this sounds right.. lets see :). It looks like this simply isn't implemented. Reference::ToString should have an if (IsTypedVector()).. etc at the end. Wanna have a go at this?. Oops, that may well be. Wanna fix it?\nWould it be cool to make the index unsigned, or is that going to clash with a lot of existing C# code?\nFaster than checking 2 bounds may be casting to unsigned and then a single bounds check. This may sound incorrect, but you could argue that an index is really meant to be unsigned. Not sure if this would be ok in C#, it appears common in C++.. Yes, I can imagine casting to unsigned would be weird in C#.\nThough I must say it is also weird that C# opted to have unsigned types, and then chose not to use them for one of their most obvious use cases (indices)? Unlike Java, which simply doesn't have unsigned at all.\nC# specifics are in this unfortunately shared file with Java, which frankly should be split up: https://github.com/google/flatbuffers/blob/master/docs/source/JavaCsharpUsage.md. @rw . This certainly looks reasonable.. the amount of verbosity is unfortunate but unavoidable I agree.\nI don't think we have any GRPC specific unit tests yet, in fact, we don't even run GRPC tests by default, since we don't want to depend on GRPC being present. But adding some, that could maybe be run as part of the existing grpctest.cpp would be very welcome.. I remember looking at hunter once, and it seems like a cool idea. Still, supporting something which is not likely to be used by other GRPC users is not ideal.. The main CMakeLists.txt has option(FLATBUFFERS_BUILD_GRPCTEST \"Enable the build of grpctest\" OFF). It would be cool if @llchan could review, since he is responsible for a lot of previous gRPC and allocator improvements.. Thanks!. We should really have some iOS specific docs, or at least some hints in the Mac specific docs.\nIt appears iOS needs some CMake specific hackery we don't have: https://stackoverflow.com/questions/10530849/how-to-generate-the-xcode-project-of-static-libs-for-ios-using-cmake\nOr use: https://github.com/leetal/ios-cmake. Well, I am also away for 2 weeks, so no problem! :P. It is generally looking really good, but still some things to improve. The locale thing makes me sad. It is getting to be a really big PR which sadly is harder to review.. Splitting it would be nice, though by now I've reviewed most of it.. https://github.com/google/flatbuffers/pull/4902 was merged, so you can now use its test functions.. I would say that for projects using FlatBuffers, expecting it to pick up headers from the build dir makes sense, since they are generated artifacts.\nthe files in tests are an exception though, as we have are using a header that for purposes of the test is not a build artifact, it is part of the test and part of git (since we use it to test code generation changes also). It thus makes sense for this case specifically to pick up headers from the source directory.. Thanks! Amazing work!. Can you send the smallest possible schema that reproduces the issue?\nOr can you run it under GDB so we can get a stack trace?\n. The bitwise & between the LoadFile and Parse calls should be &&, though not sure how that should affect things in release mode.. @gabyx does the detach related changes look good to you?\n@rw please merge once everyone is happy if I am not around.. @sutambe looks ready to be merged.. see my last comment above though.. @sutambe do you want to address that last comment, or do you feel strongly about leaving it as is?. I guess it is fine, we can use it as is.\nThanks for your work!. @sutambe @vglavnyy I did not spot the assert being removed, I'd agree we want to keep that. Can you put it back?. @rw . In C++ we only generate these convenience functions only for the declared root_type, others should use the underlying generic function directly. Other languages actually generate these functions for all types. Rust can go either way with this.. Adding a method to ByteBufferFactory sounds like a good solution to me.. the default can simply do nothing. It of course makes the assumption that the caller of growByteBuffer stops using the bb arg.. LGTM. Thanks!. LGTM. @eolivelli looks like the use of default creates problems when building with Java 1.7:\n```\nflatbuffers/FlatBufferBuilder.java:180: error: default methods are not supported in -source 1.7 default void releaseByteBuffer(ByteBuffer bb) { ^ (use -source 8 or higher to enable default methods) \nflatbuffers/java/com/google/flatbuffers/Utf8Old.java:46: error: lambda expressions are not supported in -source 1.7 ThreadLocal.withInitial(() -> new Cache()); ^ (use -source 8 or higher to enable lambda expressions)\n```\nrelated breakage: https://github.com/google/flatbuffers/pull/5035. There are people on Android apparently still using 1.7, I am not too familiar with why or how.\nWould this breaking of the API just be of the API we just introduced? That may be ok, since there's not likely a lot of people using it yet?. @eolivelli I think that is acceptable.. @eolivelli please do :). You could, but just a PR would be sufficient.. It be nice to run a test to see what speedup Rust gets from not using bounds checks.\nBesides that, another advantage of a Verifier (as opposed to catching an out of bound error) is that it is all in one place (catching bounds checks is hard to make modular). Also may be easier to debug.. Would be worth looking at past gRPC / FlatBuffers language PRs, but it seems that every language is supported a bit differently in gRPC, so hard to say.. If the C++ code can read the buffer correctly (and thus the buffer is ok), and given that I think the JS code normally knows how to read a whole string, I am guessing the culprit is in how you actually read the buffer in JS. Is it coming from file or network? Are you reading this in a binary mode?. packet.slice(8, packet.length - 8) reduces the size of packet by 16, which is probably not what you intended.. Already responded here: https://stackoverflow.com/questions/52192977/in-javascript-google-flatbuffers-how-does-one-write-a-ulong/52193251?noredirect=1\nBigints are pretty new to JS, but it would be awesome to have direct support for them in FlatBuffers.\n. @rw ?. @rw : PRs in this repo auto-squash, so need to have people do that manually? Or is this to improve the commit message?. Generally FlatBuffers cannot refer to external data.\nOne thing you could try is:\n\nCreateUninitializedVector (so it is the last thing in the buffer, buffers are written backwards)\nCreate and Finish the rest of the buffer\nObtain the data() pointer of the vector.\nSend ZeroMQ the FlatBuffer up to that pointer, followed by the image data.\n\nThis avoids the memcpy, assuming ZeroMQ can pull its data from 2 buffers.. that is actually hardcoded in the code generator, and instead should probably correspond to whatever the actual enum type is. Care to make  PR?. Ah, I missed that. I guess we have to hard-code it to int64_t then (not uint64_t as signed values are pretty common). This is a shame, since the vast majority of enums don't need this bit-width, but oh-well.. See CI results, this does not seem to work: error: \u2018experimental\u2019 in namespace \u2018std\u2019 does not name a type\n         typedef std::experimental::string_view string_view;\nI'm guessing it has the header, but the actual implementation does not use the experimental namespace anymore??. Looks good, though you may have to change the constructors to work with older compilers (see CI).. Thanks!. That's definitely neat, but in general in FlatBuffers the \"value\" of such a key-value map is considered to be \"all the other fields that are not the key\", not just a single field. So the second element should probably be the whole table.\nAlso, this would need to be behind a flag, since we support older compilers.. Thanks!. Not just in Rust, also for C & C++. I guess other languages are safe, since they tend to implement scalar writing with shifts or using explicit endianness aware functions.. Thanks!. We already have our own hash functions in hash.h, so lets use those.. Fixed: https://github.com/google/flatbuffers/commit/4b10656f9bd0d5b7fa2b068af6db3ad62c2a7267. Care to make a PR to fix this?\nIs there no way to put this in a central location in C#? Or better yet, outside the source-controlled code?. @eerhardt the define.. yes, it be nice to set this in build/project files instead of source code.. I am not aware of any non-determinism when encoding tables, can you give an example? . that isn't non-determinism though. The same serializer will produce the exact same bytes every time given the same input. So i'd like to understand how your tests fail, given that your inputs are not changing.\nOf course, testing against binary output like this is not a great idea to start with. Why do they appear as binary in your JSON output? If you are using the nested_flatbuffer attribute, any nested table should be output as readable JSON as well.\n. I'll merge this for now.\nYes, in cases where we can only afford a single type, it would understood that large unsigned values are stored bitwise (as negative numbers). Not sure if there's a better solution that is still elegant/efficient.. @akhilman thanks, help is appreciated! I suggest taking these features one at a time.\n@rw who wrote the Python port.. Thanks!. That looks a lot nicer :). Nice!. I was planning to a 1.10 release very soon. Since this change changes a lot of core functionality, it may be best to merge it just after 1.10, such that if it breaks anything for anyone, we have time to fix it. Does that sound reasonable to you?. Yes, maybe factoring the local stuff out if it is not too much work would be nice.. FlatBuffers 1.10 has happened, so lets prepare this one to land... That is certainly somewhat tricky code, but it doesn't bother me :). Ok, we're going to merge this, any \"fallout\" from it can be addressed afterwards.\n@vglavnyy Thanks for your hard work :). @fbenkstein not all users of flatc (that still may be involved in generating Rust) may have Rust installed. There is some value in flatc being able to do the correct thing as a standalone tool.. Yes, that's a good point, there are test (in particular the ones that test for non-null) that assume the code does not continue beyond the first failure.. Looks good, thanks!. It really shouldn't matter, unless you use it with very small values. >= is more elegant, I say.. First 1.11 commit! ;). Nice!. This is great! Worried that it will slow down our CI yet more, but hey, this is important to test. Thanks for making it work!. They are there because AfterHash only works correctly outside functions, and because we'd rather have BeforeHash :)\nTwo bugs I've filed for it:\nhttps://bugs.llvm.org/show_bug.cgi?id=36019\nhttps://bugs.llvm.org/show_bug.cgi?id=36020\nWe could consider adding clang-format to the build.. though that would still have to be conditional I think, for people that don't have it installed. I'd also worry that it would interact badly with the build system or an IDE (in terms of change detection).\nHow do other projects deal with this?. The strange checkout command is because we don't want to format generated files.\nThis script was used to make the code base initially clang-formatted but probably hasn't been run since.\nUse of git clang-format should be fine I think.\nThough note, for use with CMake, it is not a given that on e.g. Windows either of those tools in path.\nDidn't know about this blank line requirement.. that sucks.\n. Both bugs I listed will need to be fixed before we can remove clang format off, which would be nice.\nYour bug: https://bugs.llvm.org/show_bug.cgi?id=39164\nSo what do you suggest we do for now? Add blank lines in 63 locations? Or wait with whole-code-base clang-formatting until some of these bugs are fixed, and use git clang-format only for now?\n. Ok, sounds good.\nLets leave this issue open for reference, and to report if there's any progress on any of the above bugs.. Yup, FlatBuffers breaks the Google Style Guide on this. FlatBuffers uses a LOT of preprocessor directives (sadly), and I don't want to all move them to column 1.\nReplied on issue 36020.. Nice, this make sense.. @laur89 FlatBuffers does not have the concept of updating or creating a delta of a FlatBuffer. If you want incremental updates, you explicitly need to define a schema that allows those to be encoded.. Thanks!. I had to change this, since it wasn't compatible with the Google Closure compiler decls we use internally: https://github.com/google/flatbuffers/commit/4f066c39cedf44eff2ae925e9a30dcdbba417e2f\nNot sure if this is the best solution, comments welcome.. Thanks!\n. @rw?. @rw @kostya-sh I guess that is not ideal since it generates noise unrelated to this PR, but if it is needed it is ok.. We'll need more information, like the schema, as I don't know what the types of the variables you mention are. What do you mean \"empty\"? data() always returns a valid point assuming its parent is valid.. Why do you use EnumValuesIngredient()[1] instead of Ingredient_carotte ?\nYou can't nest vectors inside tables. You should call builder.CreateVector(types) before reciHBuilder recipe. Or instead of using the builder, use CreatereciH instead, in which case this error can't happen.\n. builder.GetBufferPointer() creates a dangling pointer, since builder owns the data you're pointing to. You could move builder to a parent where it outlives your C# use. Or allocate it with new and return that instead. Or return the result of builder.ReleaseBufferPointer().. builder.ReleaseBufferPointer().data() again produces a dangling reference. You should use builder.ReleaseBufferPointer().. Thanks!. I can see the usefulness, but this is definitely more complex than with structs, where it is a mere memory mapping.\nChanging the signature of these functions at this point doesn't sound like a great idea, we'd break potentially a lot of code.\nI understand you're not happy having UnPack return a Eigen::MatrixXd *, but that is the only thing that would be compatible with the current API. To declare it just Eigen::MatrixXd we'd also have to specify native_inline which is currently not supported for tables. So maybe we should look at that first.\n. Default values for tables are more difficult since they are possibly large and recursive. Tables have a default value.. it is just null rather than empty object. It seemed to make sense at the time.\n\"The current struct unpack is not suitable for std::unique_ptr case\" how so?\nAs for the Pack function, does this not work?\n```\ninclude \"my_pack_prototypes.h\"\ninclude \"generated.h\"\n// My pack implementations follows here.\n```. Yes, that is a bit clumsy. This feature hasn't seen a lot of testing I think. Glad that at least it can work.\nI'm not sure about the generated code up into multiple headers, which would be a bit of invasive change at this point. Same with defaulting to native_inline. We'll need a solution that incrementally improves what is currently there.. Yes, it may well be that so far all users using this functionality have always used both attributes, so defaulting to native_inline can't possibly break them. If that is indeed the case we can change this.\nOf course saying native_inline + native_type:\"std::unique_ptr<User_t>\" is a bit weird, but ok.. hopefully it is not needed. I guess the whole point of a native type is using memory-compatible structs which you always want inline.\nDo you want to make a PR for this?. @iceb0y yes, that would be a nice option to have (since now the type can be inline), though we'd have to provide our own implementation which I am not a fan of.. We have zero dependencies so far, and I don't think we're about to take one on to include Abseil.\nAnd an empty table often makes a lot less sense than an empty string. I think we'd want to keep the optionality explicit there, by default, and probably don't want to break the existing API.. How would such a view based API be different from the base API we already have? To me, the biggest gain of the object API is to enable mutation with standard C++ types.. Why would you want to remove them?\nThe reason for their existence is compile time error checking, i.e. functions that require a Offset<MyTable> are much harder to use incorrectly than ones requiring an int, and accidentally passing the wrong int results in corrupt binaries that are very hard to debug.\nDid you test if your int version is any faster? I remember we concluded that the speed was similar when we introduced these types.\n. Ah yes, the dir names are shared between languages, so this will be hard to fix. Maybe a --force-lc-namespace or something, so people can at least use it in their own code?. If in Bazel you define FLATBUFFERS_TEST_PATH_PREFIX you should be able to fix this without changing the code.. Thanks!\n. Why does this PR include js/flatbuffers-tests.ts which appears to contain a minimal copy of some generated code ?\nShould tests/TypeScriptTest.sh be updated to not do an npm install ?. Yes, @fbenkstein can you see what happens in @vglavnyy's PR? Which PR should go in first?. https://github.com/google/flatbuffers/pull/4948 has been merged.. Ok, later changes can be in another PR, thanks!. Nice!. Thanks!. Ok, LGTM. @sutambe @vglavnyy Yes I agree the return value of ReleaseRaw not being used looks suspect.. if that is intended, then a comment would be good.. Thanks!. Nice!. Can you make a PR for this?. The blob type is typically used for binary data (if it happened to be all printable you'd use string instead), so appending it to a string is going to produce garbage. If we actually want to print it, it will have to thru a function that escapes non-printable code, like EscapeString.\nAlso, the way you wrote it, there's a potential memory allocation and copy involved. Luckily, EscapeString can be called without such a copy.. Not a Python user, but afaik the value returned is a buffer, not a string, and this is how Python prints those. @rw?. Thanks! Can you run sh generate_code.sh to make sure generated code is updated.. Thanks!. @stewartmiles @riklund \nAdded missing code that makes it conditional here: https://github.com/google/flatbuffers/commit/21591916afea4f50bb448fd071c3fccbc1d8034f\n. Ah, I am guessing this is a new warning in newer GCCs. Could indeed be as simple as adding U to all unsigned default values. Can you make a PR for this?. For generated code, we --scoped-enums to enable these manually, so we could use that for this too? Though I guess doing it automatically for compilers that support it is also nice.\nBut certainly the use of typename T::FlatBuffersVTableOffset is a very nice solution, I am suprised VS2010 accepts it :)\nI'll merge this for now, we can improve later.. Thanks!. There are plenty of places in the generators for many languages where the case of characters in the field name is changed to suit the conventions of the language, so having 2 fields that differ only by case is dangerous.\nI agree that can be problematic in working with pre-existing data (though using both e and E as a field is pretty crazy). Certainly pre-processing the JSON data seems like the best solution here.\nI could imagine a way to specify an alternative field name in the schema just for parsing JSON, but frankly that feels pretty hacky.. The parser has IDLOptions::skip_unexpected_fields_in_json which can be set to skip these fields.\nI am not sure I understand your second problem. Can you not simply add an email field to the schema and re-generate your Java & C++ code?. The Java FlatBuffers API is non-mutable. The \"objects\" you get from this API are are just handles, the actual data is stored in the buffer, and modifying something there that changes the size is difficult.\nThe best way around that is using something like the C++ object API if you really need it, which at some point was planned for Java but hasn't materialized yet: https://github.com/google/flatbuffers/issues/4769. Thanks!. Can it be that your flatc and the library files (*.cs) are out of sync? They need to both be of the same version.. CI failure looks like a fluke..\nThanks!. @fbenkstein awesome! This cleans up the code a lot, not sure why we originally went with std::function, as indeed it is not necessary here. Should be faster too.. LGTM, thanks!. @rw?. FlatBuffer binaries can change between different writers because of field ordering etc, though I am not sure what causes this difference, since it is always generated by flatc. It seems release and debug builds differ, which sounds bad.. maybe std::sort behaves differently.\nI should really figure that out.. in the mean time disabling the binary diff would be ok.\nCan you add a message upon diff fail? I fear people will see this CI breaking and not understand what they have done wrong. Something like: generated code differs - please run tests/generate_code.sh or something.. Awesome! This is a very nice addition to the CI.\nAnd thanks for fixing the Dart ordering issue.. No, that's probably a simple omission. Please add it.. In what situation does it panic? array out of bounds? that is probably intentional as we wouldn't want to have user code check that upon every access. I believe a verifier is the reasonable alternative when corrupt data has to be dealt with gracefully.. It's not just get_root_as, every accessor would need to be returning Result. We don't do this in any of the language APIs, the result of accessing a corrupt buffer is an index out of bounds exception in most languages, or simply undefined behavior in C/C++. In no language is this error propagated as part of the API, which would be very expensive and clumsy. The verifier in C++ means you can have a check for a corrupt buffer in a single location, such that during actual access such errors are guaranteed not to occur. It would make a lot of sense for Rust to follow the same model, since it is also performance oriented.\n. Languages have different classes of errors. Does array indexing in Rust return a Result ? Does the division operator?. > the contract of the flatbuffers Rust library could be that panics do not occur if verify_ has been called?\nYes, that makes sense to me. That's the contract you get in C++ (no segfaults in that case).. @rw I think it is important to clarify that going out of bounds on a FlatBuffer should be treated similarly to how the language treats going out of bounds of an array, for a buffer that is corrupt and has not been verified.\nBut yes, we can close it.. Yes, I think the big issue is that binary schemas were never intended to have the full set of information that the parser stores, as they initially were only needed for reflection.\nWe could add more data to the reflection data to have the full coverage the JSON and language code-gens need.. I am just afraid it is going to clutter the schema a lot. For that reason, for most bools that are derived from attributes, I'd say store those as attributes and only turn them back into bools upon reading them back into the parser state. I know that uses way more space, but is probably acceptable for attributes that are used infrequently.. There is now a second PR trying to do the same: https://github.com/google/flatbuffers/pull/5077\nCan you compare?. Since it has an explicit check for 0 I guess it was intentional, though I do not remember why.\nThe difference appears to be that lang is the current language being generated, and lang_to_generate is all languages for the current flatc invocation. I am guessing that was needed because when it parses the schema, it does not know yet which languages it will be used for.\nIt doesn't look like parsing and generating of vectors of unions is working (just looking at the code)?\n. Yes, support for structs in unions and vectors of unions should probably be tested seperately. Care to fix this?. Well, the parser can be called from flatc (in which case we wan't errors for unsupported features) and at runtime by any program that links it in for say JSON generation (in which case we don't want these errors since no such flags will be set). Unless we'd require the latter users to specify the kJson or kBinary flags, but that might be backwards incompatible.. Can you say what is not clear? I thought my comments here explained it.. Since the same function call is used for both, I must assume that it is the same set of languages. But that be worth verifying.\nAs for the == 0, that's what I was referring to here:\n\nWell, the parser can be called from flatc (in which case we wan't errors for unsupported features) and at runtime by any program that links it in for say JSON generation (in which case we don't want these errors since no such flags will be set). Unless we'd require the latter users to specify the kJson or kBinary flags, but that might be backwards incompatible.. To see if a language supports things other than tables in a union, you'd have to check the corresponding code generator and see if the code it generates for accessing that union deals with multiple types properly.\n\nThat, or more simply, see if the language works with the schema in tests/union_vector. It appears that currently that dir contains code for C++/C#/Java/PHP/JS/TS (and pretty sure C supports it too). If the tests for the given language import files in this dir, it is pretty sure the feature is supported.\nSince the test is the same, it appears these languages have always supported both \"vector of union\" and \"union of struct/string\" at the same time. So it may actually be sufficient renaming SupportsVectorOfUnions into SupportsAdvancedUnionFeatures for now?. @dalnew I don't think there is, no. Can you make one?\nhttps://github.com/google/flatbuffers/pull/4143\n. ilBuilder.Clear(); not needed.\nCan you look at the call-stack for the verifier in debug mode? It should give you an assert that should make it relatively easy to identify what information it thinks is missing.\nIt needing an enum sounds like an oversight.. where does that happen exactly? FlatBuffers doesn't use exceptions.\n. Yes, it looks I marked the enums field as required, which is incorrect. Not sure why.\nYou say you weren't able to create an empty vector of EnumVal ? What is the error there?\nTo your points:\n- If a schema has a file_identifier, the verifier will expect it to be present.\n- If you called Finish on your buffer, both Get calls are the same. If you haven't, GetBufferPointer will assert. You should really only be using GetBufferPointer.\n- flatc may add other data like documentation or attributes. Easy to verify by dumping both buffers to JSON.\n. I believe I finally fixed this here: https://github.com/google/flatbuffers/commit/f575b02fda04fe579fb23442234feb8129b77ee2. I agree this would be very nice to have.\nSo far, we are supporting as far back as VS2010, and gcc 4.x (not sure what x at this point), which is useful to us because FlatBuffers has a fairly wide user base, that includes embedded developers, game developers, which are always a few compilers behind. So testing with older compilers is helpful.\nIt be best to have both. Maybe we can add a \"cutting edge\" build to Travis to enable these tests etc?\n. The current PR defaults to sanitize on for flatc on all clang/gcc builds that support it it seems, which seems excessive for those who build flatc for regular production / build system (non testing) usage. I think it should be default off unless running under CI (we can pass a -D to cmake in Travis?). For flattest I'd be fine if it was always on, and for that is more valueable since it exercises everything including the parser. What is unique to flatc is the code generators, which, while in need of testing, don't need the same level of sanitizing as the rest of the code :)\nI am not sure how it would simplify #5005 since these kind of issue come up when people build their own programs where they typically don't have sanitizers on. Though I guess your CMake changes make it slightly easier to do so, if they use CMake.\nAnd yes, we need to fix the UB im 4982.\nSlightly longer CI I am willing to have (for flattest only).\n. Thanks, this is a better default.\nI'm fine adding newer compiler to CI, LLVM 7 sounds good for starters.\nAs for the leak, lets first see what @sutambe thinks.\n. @thecsapprentice who implemented this flag.. Thanks!\n(AppVeyor CI appears to be experiencing problems).. To me, one file per schema makes a lot of sense and reduces clutter, for languages that can handle it (not Java). If it helps with Go's crazy imports all the better.\nI don't understand 1) though. If in Rust you used one file per type, what does that tell us about Go?. May have been my fault, since I published it. But what does the script append to an existing file for? That sounds very error prone and worth fixing.\n\"sed \\\"s/this.flatbuffers = flatbuffers;/export { flatbuffers };/\\\" js/flatbuffers.js >> js/flatbuffers.mjs\"\nI can try and publish a 1.10.1 after deleting the .mjs file.\n. Ok, published a version that does not have the duplicate code: https://www.npmjs.com/package/flatbuffers/v/1.10.2\nFixed npm publish not appending: https://github.com/google/flatbuffers/commit/b78c4332be45a488390bdb966e02831f3ebba90a\n. Nice, but.. arrows haven't always been in the language, right? So with this PR we're cutting off support for older platforms to reduce the size of a function definition? :). You are best off creating a table NestedValue { buf:[ubyte] (nested_flatbuffer: Value); } and then creating a vector of those. This way, you can copy all the invididual Value buffers as is without unpacking, and access of the resulting buffer will also be efficient.. LGTM, Thanks!. CI appears to be failing on C# project missing the new files, I can fix that later for you if you don't have access to the tools to do so, though you may just be able to add it to FlatBuffers.Test.csproj by hand.\nBut first see my comments above.. Ok, so this is ready to be merged? Any trait changes can be proposed in a different PR... Thanks!. Thanks!\nHave a look at the googlebot message though.. Ok, now we have an unused variable raw, this solution seems a bit OTT for a test case. The point about ReleaseRaw, as its name implies, is that it is the lowest level way to access this memory, for when DetachedBuffer returned by Release is too high level for you, so wrapping it in such high level machinery seems odd. Just fixing the memory leak with manual code would be better?\nNote that the first iteration of Release returned a unique_ptr with a custom deleter, which then created too many issues that eventually were solved by DetachedBuffer. So I am getting a sense of going in circles with the above code :). @sutambe I wasn't suggesting changing ReleaseRaw, in fact I am very much saying lets keep it in its simple form. I was just giving some context to how we got here, and suggesting simpler code would be better for this test. Though I am fine to merge as-is, just to get the leak out of the way.. Ok, merging this just to have the leak fixed. Please consider removing the shared_ptr at some later point.. The first link is JS code and the second link is C# ?. Not the biggest expert on CMake (and Conan) either, maybe @uilianries or @Croydon knows?. LGTM.. anyone else?. Thanks!. @uilianries mind having a look at what Conan-related failure is happening here (on a recent PR): https://travis-ci.org/google/flatbuffers/jobs/457911893. Generally lookls good to me, thanks! Ready to go in?. Hmm, if you're sure this getenv works on all platforms, it is worth trying. Remember we support VS2010 (C++0x) and Android with STLPort. Rather than #ifdeffing those out, your current code may be more universal?. Thanks for the other fixes.\nThe Rust error is unrelated.\nYes, it be great to move some functionality in util.h to util.cpp to move #includes, in particular windows.h. Merging this, util.h refactor can go in other PR.. Can you give an example of what you want, since I don't think I understand. The code you link to is a very different implementation of FlatBuffers that appears to use C# reflection, which we make no use of so far.. Sorry, still have no idea what you're trying to do. \"I want conveniently identify that message at low-level before deserialization, directly using a constant field with predefined unique ID that was generated from a scheme\". Outside the context of your project I have no idea what that means.\nIt am guessing though that you'd want fields in a FlatBuffer message that are unused by serialization/de-serialization. That doesn't make a lot of sense in the case of FlatBuffers, since something like SpawnMessage is just a handle to buffer of serialized data. It is not an object that you can use for storage of variables unrelated to serialization.\nThough theoretically it can be done for scalars using the mutable API, we don't have this feature in any language, and I don't see a point in supporting it.. I think you're not realizing that the whole reason FlatBuffers even exists is because it does things radically different than the others. The others unpack into C# objects, which could store extra fields. FlatBuffers does no unpacking at all, it directly accesses serialized data as-is, using handles.. Looks like the appveyor error has nothing to do with this PR, so I'll merge.. thanks!. Yes, that is looking like it couldn't resolve the name. That should give an error at the end of parsing the schema though (since it allows declarations in any order).. Ah! I guess the nested typename was never meant to be namespaced.\nLookupCreateStruct already does this looking up in parent namespaces that you are proposing to add, so potentially a simpler fix would be to replace all of this with:\nfield->nested_flatbuffer = LookupCreateStruct(nested->constant);\nBut I'm not sure why we had that double lookup in there in the first place.\n. @omalley:\nThis is generally awesome, since string conversion can be Java's bottleneck, this is very valuable.\nA few concerns:\n\nThe comments mention this implements a relatively strict decoder. I'd be worried that someone's use of FlatBuffers in the field suddenly breaks because they have sloppy UTF-8 data stored. What is the likelyhood of this? Any idea how it might differ in behavior vs the Java SDK one? If invalid UTF-8 is found, how does that affect the String returned?\nAlso uneasy about the use of sun.misc.Unsafe, there are rumors it will be removed (https://www.javaworld.com/article/2952869/java-platform/understanding-sun-misc-unsafe.html)? Is it even supported on all platforms (e.g. all version of Android?, https://gitter.im/scala-android/sbt-android/archives/2018/01/09) Would it simply fall back on the slower path?\nIf it is going to fall back, it would be good that this happens without class warnings.\nThere's also logging statements in the code, would prefer to not have those.\nHow fast is the non-unsafe fallback path? If it is slower than the Java SDK code, that would be a problem. If it is almost as fast as the unsafe code, we could greatly simplify this code by just always using this path. \nWhy is unsafe even needed? UTF-8 is byte based parsing, surely it can't be that slow to just read bytes out of a byte[] ? UnsafeUtil is full of scalar accessors that are not needed.\nGenerally, while I understand that there's an advantage to copying these files as-is from Protobuf just in case we'd want to update them in the future, it is also a LOT of code, quite a bit of which doesn't seem necessary for the FlatBuffers use case. If possible, I'd prefer reduces special case code with less dependencies.\n\n@shivendra14: The C++ implementation currently does no UTF-8 encoding or decoding, UTF-8 is its native format.\n. It also depends on the speed. If we need Unsafe to reach good speeds, then using the files mostly as-is makes sense. If it is not a big difference with the safe path then getting rid of the unsafe path would be a huge simplification, and worth doing, imho.. My gut feeling says that for simplicity, testing and dependencies sake it be nice to go with just the safe encoder. It's a huge improvement over the original already, and that extra 10% (or 3%, from the perspective of the original) is not worth the extra complexity.\nThanks for testing this!. This looks great! Thanks for factoring out the unsafe stuff. I can merge, though not sure if we need to be changing the version number between releases.. Merged! Lets see if it has a (positive) impact on Java users :). @omalley looks like the use of lambdas creates problems with Java 1.7, any way we can work around that?\n```\nflatbuffers/FlatBufferBuilder.java:180: error: default methods are not supported in -source 1.7 default void releaseByteBuffer(ByteBuffer bb) { ^ (use -source 8 or higher to enable default methods) \nflatbuffers/java/com/google/flatbuffers/Utf8Old.java:46: error: lambda expressions are not supported in -source 1.7 ThreadLocal.withInitial(() -> new Cache()); ^ (use -source 8 or higher to enable lambda expressions)\n```\nrelated breakage: https://github.com/google/flatbuffers/pull/4914\n. Looks like that code was never tested with a big endian machine (is there even one that C# supports?).\nStill, would be good to fix by pulling the value write out of the if... care to make a PR?. Ah, that's an oversight, since we've only tested with GetAnyRoot. As a work-around you could use a C style cast (const flatbuffers::Table *) which should always work. Would be nice to add a helper function for this instead though.\nThe private inheritance is intentional.. Perfect!. Thanks!. opnode.mutable_builtin_options() is a void * since it is a union, it is not known statically which type it is. You can cast it to the type indicated by opnode.builtin_options_type(), or use a function like opnode.mutable_builtin_options_as_Conv2DOptions() to do that for you.. Can you run generate_code.sh to include the new generated functions? Also, please rebase, you have unrelated commits, and some code shared with your other PR.. Yes, that is Rust related.\nCan you address my comment here: https://github.com/google/flatbuffers/pull/5033#event-1970487546. Ah ok.. but the BUILD file does.. Ah sorry, hadn't seen your update.. Thanks!. Thanks!. Thanks!. I thought I had done this. So just now ran mvn deploy again, but it errored out saying it could not update 1.9.0. The only references to 1.9.0 were in cached files, so after a git clean -dxf ran it again, and this time it succeeded.\nSo I guess at release time I didn't catch this error. Is Maven really this fragile that it can't cache things correctly?\nIt hasn't shown up on https://mvnrepository.com/artifact/com.google.flatbuffers/flatbuffers-java-grpc but I presume it will soon.. It is there now.. I guess it just takes a LONG time to update?. Can you describe what exactly this fixes? Would this install debug binaries into the system folders (which I believe is undesirable)?\nPrevious discussion: https://github.com/google/flatbuffers/issues/4511 and https://github.com/google/flatbuffers/issues/4753\nAlso, see the googlebot message.. With debug I mean an un-optimized executable (no -O2 or whatever). I would claim you would not want such binaries in your system directories, especially not when speed can matter (which can be the case for flatc, e.g. when it needs to parse large amounts of JSON).. Ok, fine, lets allow it to install debug binaries then.\nI'm still somewhat worried that people not paying attention (that are not your use case) will accidentally end up with slow binaries in their system.. what do you think about forcing the build to Release if not specified?\nif (NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)\n  message(STATUS \"No build type selected, default to Release\")\n  set(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"Build type (default Release)\" FORCE)\nendif()\n(similar code is used by other projects, e.g. LLVM).\n. Also, while the Travis failure appears unrelated to your change, can you check the CLA message above and fix it.. Thanks @uilianries ! So it was related to this PR, my bad.. > IMHO, only developers/maintainers would build on their own, and they should know what they're doing.\n\nPlain users should only install from their distro.\n\nNot sure how realistic that is. Distro's are often several years behind, and for flatc it is important that it matches the runtime code. So if you have a project that relies on FlatBuffers via git submodule or whatever, and thus takes the code from there, it should also build flatc from there, and if an install gets invoked, users may end up with slow debug binaries in their system.\nFrankly, rather than making release and debug function the same, I'd argue that flatc (and lib/include) should NEVER be installed in system dirs, since you may have multiple projects relying on multiple versions of FlatBuffers, and you'll create a mess trying to compile them on the same system.\nYou cannot compare FlatBuffers with libraries installed into Linux system dirs (that have super stable APIs/ABIs) that way. Not only does FlatBuffers evolve more, there is an API boundary between generated code and runtime code that changes on occasion, all in headers.\nBut hey, if I am the only one that thinks that way, then so be it.\n. It sounds like you live in a perfect world where there is only Linux. When building portable apps that rely on 3 desktop OSes (and maybe 2 mobile ones), relying on a distro package manager is impossible, and relying on the github repo is the only way to go.. Windows has a package manager? What, Chocolatey? The Linux subsystem for Windows? vcpkg? And you think you can depend on developers all using those, and getting your dependencies thru them?\nNot even on Mac is usage of package managers universal. No package manager for iOS/Android either. It is a non-starter.\nEven on Linux, there is no apt-get install flatbuffers, and FlatBuffers is not a small project.\nI have no idea how anyone can say Why not just using the package managers of the target platforms ? unless they only use very popular libraries and think there is no world outside of Linux.. Hmm, regardless of whether you are right about how these platforms suck for not having standardized package management, the fact is they don't, and that's what I take into account for FlatBuffers.. Ok, part of what this PR was about was merged in https://github.com/google/flatbuffers/pull/5180\n. Also note the in progress column here (can't link to cards??) https://github.com/google/flatbuffers/projects/6. I changed this repo to rolling builds in the appveyor settings.. Since FlatBuffers started with C++, it was assumed that schemas would be written in snake_case for fields (which corresponds to most C++ style guides), and then we added ways to convert those to various camels for Java/C# etc. But yes, we don't enforce this, and we don't do the reverse if someone writes their schemas in a camel case. Maybe we should.. Also note the FlatBuffers style guide says things like field names should already be snake_case: https://google.github.io/flatbuffers/flatbuffers_guide_writing_schema.html (under \"style guide\"). Which identifiers have the wrong case?. I am not sure what the problem is. Your schemas have e.g. namespace org.apache.arrow.flatbuf, which I presume generate the corresponding package name in Rust, what needs to be changed about this?\nWhy do the files have to be renamed?\nso I had to manually search and replace to changetype__typetotype_type. A union appendstypeto a field. I guess the base field name got renamed totypebecausetype` is reserved? I guess you could special-case this, but I'm not sure it is worth it.. Well, presumably it breaks because you can't be adding required fields without breaking backwards compatibility..\nI am not a fan of adding lots of small schemas for features either.. that's going to generate a ton of extra test files for all languages.. This is not likely to happen. We'd need to add not just unions in structs (and omitting union types), but also scalars in unions, which are currently not supported.\nYou may be better of with just: table PropertyObject { hash:int; i:int; f:float; s:string; a:[float]; }. Note that vtables are shared, so this will generate 4 different kinds of vtables (for a table with only 2 of the fields set, hash + one of the union), so their cost accross a million objects is zero. The cost of storing an int or float property would be 12 bytes: a vtable offset + hash + int data, which is pretty good.\nIf it has to be even smaller, just store all properties as a vector of ints, with the int being either an int directly, a float bit representation, or index into a vector of strings etc.. Yes, like I said such a feature is not likely to happen.. std::lexicographical_compare seems to be at least C++17, so that would require conditional compilation anyway.. so probably not worth it.. Thanks!. LGTM, thanks!. I am not sure myself either, for Java at least. I agree generally I'd like to stay as close to upstream as possible, but maybe that should be a seperate effort. For now, lets merge this as-is, thanks!. What is the error?\nI guess the cast is changing.. calling convention? No idea why it is there.. As I was suspecting, there's a __cdecl in there that it is casting away, and I am guessing it doesn't like that.\nNot sure what the cleanest fix is. An #ifdef _WIN32 that adds __cdecl to the Pack_t type? Maybe there's a better way?\n. LGTM!. LGTM. @vglavnyy what do you think is the best way to handle this? It be nice if we could avoid having to special case such values for all languages, by using.. hex floats? I am guessing there may be languages that don't even have a way to produce a NaN.\nWhile we generally should support such values for JSON parsing, I am not sure how useful I find it for default values, if it requires special purpose code in each language to support.\nRelated: https://github.com/google/flatbuffers/pull/4948 (float parsing overhaul).. My recommendation: wrap it in a struct, which comes at no serialization cost (same size binary), and will allow very clean detection of not being present.\nAs for JSON, we should have solid printing and parsing of any kind of float, @vglavnyy can comment.. Wow, nice overview :)\nOk, we can at least special-purpose the printing of these values in generated code for these languages then.. See discussion here: https://github.com/google/flatbuffers/issues/5069. According to the code (flatc.cpp:407), it should at least output Unable to generate json for <filename>, since the generator functions return a bool, which is false when there is no root type (idl_gen_text.cpp:287). Does it not?\nIt would be nice to change the generators to return an enum so that multiple error types can be specified, care to make a PR?\n. Agreed, that be nice!. Yes, Go was one of the first ports, so misses many niceties that have been introduced since.\n@rw who wrote the initial port.. Nice! Merge at will.. Can you write an example of what is going on with a code sample in this issue, rather than linking to a zip?. Ah yes, the calls to GetInt inside CreateSortedVectorOfTable2 should really just refer to the generated accessors (in this just K()) which include testing for default value.\nCan you create a PR for a fix?. Thanks!. Thanks! This looks like a pretty good implementation, but how is it different from https://github.com/google/flatbuffers/pull/5001 by @fbenkstein ? Could you two compare implementations and make sure we end up with the best of both somehow?. Thanks, looking good now!. I would have preferred if you had fixed the original PR instead of starting a new one, as that would preserve discussion better: https://github.com/google/flatbuffers/pull/5050\nLike I asked on the other PR, can you move the new flatbuffers.pc.in out of the root dir, like CMake/flatbuffers.pc.in or similar?\nAnd this one still has a CLA problem, see above.\n. Thanks, looks good! Can you fix the CLA issue?. Looks like you should rebase... Thanks!. Yup, FlatBuffers uses 32-bit offsets. We could offer a 64-bit version of the format, but never got around to it. See notes on FlatBuffers64 here: https://github.com/google/flatbuffers/projects/10\nI'm not sure what your change is intended to do (since std::vector will not create any serialized data), but it won't get around the 2GB limit.\nYour only option for now is to split up your data in smaller buffers. Or help make FlatBuffers64 a reality :)\n . @oakad \nAnother change you are making is \"builder objects\", which causes an object to be allocated for each object constructed. The whole point of FlatBuffers is to get away from allocating objects and keep things as efficient as possible.\nYou are also introducing an object API, so why not reserve nice convenient API features for that (since it is meant for uses where convenience is more important than maximum speed) and leave the base API biased towards speed over convenience? In fact, if you go that way, you can keep @rw's base API, and simply make the object API use it.. that's how it works in C++.. @oakad \n\nThis is incorrect. Most of the time those objects are simply pushed to stack\n\nIn the case of Java I am aware of e.g. Hotspot doing \"escape analysis\" to achieve this, but I am not sure that every implementation does this, for example Android, where efficiency is the most important.\nIn the case of Go, can you point me to a list or rules that guarantees that in this case this optimization will always happen?\n\nI would also like to point out that \"reducing the number of allocations\" while desirable, should not become a fetish in memory managed languages\n\nI am not interested in starting another \"the cost of GC\" discussion here again. Fact is that the whole point of FlatBuffers is to replace objects by a nice compact byte array. Whether that is an optimization worth having probably depends on the language and use case, but should not be thrown away by default. I have nothing against an object API, but I think it should be the choice of the programmer to opt-in to it.. @binary132 thanks, that's what I thought.\n@oakad people use FlatBuffers because (part of) their tech stack requires certain performance characteristics. I am sure they can handle a slightly less pretty looking API in exchange for that, as many have in languages other than Go.\nAs for strings, I suggest to simply allow both.. you can default to strings if you wish, but a parallel byte slice accessor would be good.\nI suggest benchmarking against @rw's existing API to ensure we're not losing any past performance. There is already a benchmark I believe.\n. @oakad if you're not willing to make compromises on your PR we are requesting, then yes we can't merge it.. Closing.. if anyone wants to continue this work, please open a new PR that attempts to address the issues raised above.. Duplicate from here: https://stackoverflow.com/questions/53646959/flatbuffers-how-to-generate-name-method-in-python\nLooks like we'll need to add names to the generator.\n@rw . @dbaileychess any idea?. You can close it too :). Thanks!. Thanks!. Very cool.. give me some time to go thru all of this :). So far, all language implementations are self-contained in this repo, and this repo has no dependencies. I don't think we'll want to change this for Julia. Since it is just a few files, I think they can be copied.. We don't use git submodules in FlatBuffers yet, so would prefer to keep it that way.. @rjkat I am not sure what you mean by a patch here. I'd say in general we'd want flatc to be able to generate code for the tests that just works. We wouldn't want programmers to have to fix generated code manually, or have restrictions on schemas if Julia is one of the output languages.\nI'm sure there is a solution, it may just require a different code structure in the generator or in the generated code. If all these languages can deal with the current test schemas, so can Julia I'm sure.. Can you explain more why you need this? This seems a very niche feature that should not really be needed. If your software is reading vtables manually (without going thru the generated API), then you already need to check against the length of the vtable, to accomodate reading data from older versions (regardless of whether they are currently zeroes or not), so writing those extra zeroes doesn't buy you anything.. Thanks!. string in FlatBuffers is meant to be UTF-8, since many language APIs (not just Java) will decode it upon access. Using it for byte storage is thus ill-adviced. Use a vector of ubyte [ubyte] instead.\n. I'm afraid that is currently the only way to define a 2d array, yes, unless all those arrays are the same size, when you could use a single array to store them all.\nA \"string list\" is not necessarily a contiguous representation in memory, and even when it does it mixes meta-data (lengths, offsets, alignment) with string data. As such getting a ByteBuffer to it would have little use.. Sorry, we're not going to add a new API because it makes sense in just one person's code base, especially as I have explained above it does not make sense in the general case.. Note that FlatBuffers is a binary protocol, so \\n would certainly not work.\nMany languages already have SizePrefixed versions of Finish and GetRoot, I would use that if possible. Then you can simply (binary) concatenate all buffers.. Yes, sadly we have a fork of some of the gRPC code (in grpc/src/compiler), so this occasionally happens. If you could help by updating those files to the latest gRPC and submitting a PR for it, that be very welcome.. Possibly related: https://github.com/google/flatbuffers/issues/5099. You're not supposed to access the buffer (by calling GetRoot) until after the verifier succeeds. Call VerifysometestBuffer instead of ptr->Verify.. Does it relate to https://github.com/google/flatbuffers/issues/5096 ? On first glance, it does not, but may have the same root cause. A PR would be welcome.. Thanks for looking at this! There seem to be some compile errors in CI though.. Also in your original version it seems you were missing addInnerType ?. Thanks.. this is very nicely done, but I have one major issue with it: AddElement is on the \"hot path\" of FlatBuffer construction, and now for most values of float/double fields it will doing a conditional branch and calling isnan. That to me is a high price to pay for supporting these constants, and I feel we shouldn't support it by default, since most users have no need for it, so shouldn't pay this cost.\nWe could conditionally compile support for this, but that is messy too. Simpler, I'd prefer to define it such that a NaN as a default value does whatever IEEE == does, meaning you probably don't want it.. Yes, they should really never be called by application code.\nI don't think we want to add _nan versions of all field methods. That's way too much clutter for such a small feature.\nI'd suggest keeping it simple and inside AddElement do an #ifdef FLATBUFFERS_NAN_DEFAULTS if (isnan(def)) return.. kinda thing?. Ok, this looks good to me, thanks! Ready to merge?. I don't think I understand what is the problem with partial language support. If you look at generate_code.sh you see that for e.g. union_vector.fbs only 4 languages support it and generate code for it. We'd need to do something similar for NaN's maybe?. I'll merge this, further changes can be in a follow-up.. __init is not a function the user should be calling.. but yes, I don't see see a reason why it can't be moved to the base class.. care to make a PR?\nI don't understand the use case for getByteBufferPosition.. can you give an example?\nStruct has almost nothing in it.. I suppose we can make a base class for it, but it wouldn't help much, especially as this is not user facing code.\nYou cannot generically iterate a FlatBuffer without type information (not beyond generic fields in the root object anyway).. Rather than getByteBufferPosition it would probably better to have an explicit clone method?\nI am not sure I understand your use of min & max in this context.\n. Ok, I suppose it doesn't hurt to have a getByteBufferPosition for such uses. Care to make a PR?. Nice, thanks for fixing. Will be good to have these running in CI.. No, looks good now, sorry.. We could fail on leaks, though so far leaks haven't really ever happened, its a surprisingly infrequent bug (probably good for a library that is all about avoiding allocation ;)\n. Awesome! long overdue cleanup :). https://github.com/google/flatbuffers/projects/6#card-14479553. Thanks!. Hah, didn't spot that, those definitely should be in there. Either of you care to fix it?. Thanks!. The file_indentifier is not meant to be stored in the JSON file, it comes from the .fbs file, and will be re-generated when creating a binary from JSON.. Yes, please do. Both the declared static final and any accessor methods that read or write this enum should use short, there should already be code in the Java generator that does this for regular ubyte fields (non-enum).. I guess this is a new warning.. I wonder what is non-trivial about it. Maybe because it has private members? or because of the alignment decl?\nAnyway, I suppose we can instead generate individual field initializers.. want to make a PR for this?. Notice that in C++ a FlatBuffer struct and a C++ struct are the same thing, the generated C++ struct can actually be used as a datatype in C++ code even when not constructing a FlatBuffer.\nIn C# however the generated struct type is pointing to serialized data. I do not see why this new version is useful.. Yes, that is not exposed, and yes, I can see that could be useful.\nMaybe just add a method to FlatBuffers.Table that just does public static bool __equals(Table t) { return bb_pos == t.bb_pos && bb == t.bb; } ? You should then be able to call that with any 2 table accessors?\nThe docs may be out of date because in C# accessors are structs which do not generate garbage at all.. Ok, thanks!. @vglavnyy can you email me?. Thanks!. Changing the API like that will potentially break people.\nI suggest adding a second accessor for strings, e.g. FieldString() alongside the existing Field() method.\nAlso, changing nil into \"\" is not quite the same either. Typically, if a field is not present, it should return nil (this is in line with the other language APIs), though it could be \"\" if the field is required.\nDoes \"\" in Go allocate?\n@rw?. Thanks :). I don't see a test that checks if the file identifier is actually the expected one when reading?. the schema compiler will currently accept any bytes for file_identifier (since it is a string constant where escape characters are permitted), but the intent was for it to only be printable ascii characters (32..126). \nThe reason being that when such an identifier is missing, the memory being tested may contain the start of a vtable (a 16bit vtable size & object size). With the smallest ascii characters, you'd need a vtable size >= 0x2020 (> 4000 fields) before you run the risk that BufferHasIdentifier would accidentally return true for the wrong kind of buffer. This is still an issue, I guess we could say that the use of the file_identifier feature is not safe for users of really big tables. This is bad design, caused by adding this feature to the format retroactively. It should not bite anyone in practice though.\nSo in summary, I'd be in favor to restricting file_identifier to printable ascii where possible. I'd even be in favor in retro-actively adding this to flatc, maybe at first as a deprecation warning, just in case anyone actually uses this.. Added this to https://github.com/google/flatbuffers/projects/7. We want to be compatible with old code and old binaries, so sticking zeroes in there won't work. Even if we could we wouldn't want to waste that space.. @pyottamus it is not compatible with old binaries since we can't guarantee any zeroes in that location for them. We have to remain compatible with all FlatBuffer data ever produced in the past.. @rw @pyottamus yes, the filtering of non-ascii file_identifiers needs to happen in flatc during schema parsing (should be an error). Anyone feeling like picking that up go ahead.. Ahh nice, that sounds very useful.\nWe typically don't explain external tools in the documentation itself, but adding a link to it would be good. Here https://google.github.io/flatbuffers/ we have a Useful documentation created by others, so maybe there could be a Useful tools created by others :)\nOr maybe such a section specifically in the RustUsage.md.. This appears to be purely a GRPC issue? FlatBuffers does not rely directly on GRPC by default, and certainly does not rely on any protobuf headers. Not sure where the path flatbuffers/include/grpcpp/codegen/config_protobuf.h comes from, this repo has no such path.. No idea about that specific integration. Have you tried first just to build GRPC by itself with Unreal?. Thanks!. Thanks!. @vglavnyy No, seems like a good idea to me.\nAnd yes, optimizing CI such that they don't time out so often would be good :). @rw can merge once he's happy :). @neomantra apologies.. just pushed a fix for that.. Thanks for the cleanup!. Can Serde support zero-copy style serialization?. Thanks!. I'm afraid we don't have verifier functionality in C# yet.\nYour best bet is to test for the presence of the correct file_identifier (using the MyTypeBufferHasIdentifier method).\nYou should never get a \"crash\" though if you access an invalid buffer.. you may get an index out of array bounds exception, which you could catch.. To be clear, you're only getting std::string usage if you use the parser at runtime. Regular use of FlatBuffers (including reading non-scalar fields) all happen with 0 memory allocations.. Thanks! I don't understand the message change though, as this feature is implemented using template specialization.. Yes, I don't think specification was intended.. CI error appears unrelated, merging, thanks!. Would be better to just reopen the existing issue, as it has more context.\nAnyway, thanks for the repro. @krojew mind having a look at this?. Thanks!. To me a field level option makes most sense, since it does have a cost, and really depends on the kind of data stored in a field whether it makes sense or not. It should really be off for most strings, so any global option (compile or runtime) makes less sense to me.. Related: https://github.com/google/flatbuffers/issues/5145. I agree this would be better. I don't think we should break existing users over it though, and having a double set of methods is also not great. A flag also clumsy.. anyone opinions?\n@krojew for TS side of things.. Is IAR an entirely new compiler, or is it based on gcc or clang?\nA PR adding your compiler's defines to these headers would be welcome.. @harshshah903 you posted this question in 3 locations by now.. maybe one would have sufficed.. Yes, this would be cool to add.\nInitial discussion: https://stackoverflow.com/questions/54436292/is-it-possible-to-optimize-flatbuffers-serialization-when-constructing-from-nati/54446116?noredirect=1#comment95709117_54446116\nRelated: https://github.com/google/flatbuffers/issues/5141\nWe already use native_ as a prefix for attributes related to the object API, so native_shared and native_sorted would probably be good.\n. Well, we currently don't even have an explicit CreateVectorOfSharedStrings function, so unless you or someone else has a use case for it, I'd say no for the moment.. Can you make a PR for this?. Thanks for fixing this :). Thanks, that's a nice improvement :). This function is generated automatically, but yes it does not make sense without setting the test field also, which you currently can't with these methods. So this method should probably not be generated.. Both failures are unrelated. Thanks for the PR!. clang-format is not enforced in any way, so likely there's a lot of code that does not conform.\nIt is mostly there so you can do git clang-format on only your own code.. Thanks for adding all the other missing command-line flags! Though in a seperate PR would have been neater. Same for any clang-formatting to unrelated code.. lets do that in a seperate PR.. Not familiar with this process in Go, @rw any idea?. Thanks! I think this header is also used in the GRPC test which is failing CI: https://travis-ci.org/google/flatbuffers/jobs/488670301#L2494\nSo likely it needs using MyGame::Example::Vec3; and maybe others.. Thanks!. Looks like it is causing some errors: https://ci.appveyor.com/project/gwvo/flatbuffers/builds/22129657/job/fxilhjpbp94glyo8. Thanks!\nAnd yes, a fix for the pom would be good, if it can be done elegantly. The java implementation has been developed for the longest time without relying on Maven or any other such tool.. There are no vectors of static length in FlatBuffers.. we've considered add them (https://github.com/google/flatbuffers/projects/7#card-14478675) but so far this has not happened. Unless you have a very size critical use case, using the variable sized arrays (vectors) only costs you 4 bytes more.\nYou can get the max of an enum in most languages in the generated code, but not in the schema language.. Thanks!. Agree with @iceb0y that this is kinda fringe functionality, though if its small and elegant I suppose we could add it. That is not the case right now, as this PR contains lots of unrelated code in the diff. Can you rebase?. Hah, that's unfortunate.. looks like we need a special case there?. I'm not sure how that helps, since the type of the enum is only known at runtime (in the parser).\nI think the changes can be entirely in code. We can obtain the desired data with reinterpret_cast<uint64_t> or a union.. @iceb0y I don't think this can work.. Now EnumDef needs to be templated as well (since it contains EnumVals), and now we can't have a type for the list of EnumDefs in Parser since each is different. You'll end up dispatching and casting all over the place. Better to just cast the actual value storage.. Thanks @vglavnyy !. I am not sure how I feel about adding this functionality to flatc, as turning binaries into code is very general functionality that can also be done by external tools, and is not necessarily something flatc should do.\nAnyone else any opinions?. Just searching for convert binary to C source brings up a ton of such tools.. on Linux there's even a built-in way apparently (xxd -i). I'd suggest using one of such tools as part of the build system?. I don't mind outputting it as .h, since that simply gives you flexibility: you can add it to whatever compilation unit that you wish, or multiple in one, etc. A .cpp forces you to update your build files to use it, and needs another header or extern to refer to it, which may be undesirable.\nNot sure what good it would do to split up flatc. That would have advantages if the whole would be too resource intensive, but it is lean enough as it is. And being able to just copy a single exe somewhere to have your FlatBuffers \"toolchain\" ready is nice.\nMost of IDLOptions is needed for the parser.. splitting things up would not do a lot to make things less complicated. Most compilers have a ton of options, and so do most tools (e.g. git).. I don't think it is a problem.\nAs for adding direct bfbs -> source functionality to flatc.. as I said, I am not a huge fan of it, but if you both think it is useful I am ok adding it.. Ok, lets agree to add this functionality then, as a stand-alone header (see my comment above).\n. Yes, that naming sounds fine, assuming the schema is monster.fbs.\nI didn't meant the option should be hidden, just that it shouldn't be on for our tests.. Not too familiar with Bazel.. maybe @jschaf or @AustinSchuh know?. @AustinSchuh Yes, I'd love some Bazel CI, please do! @rw also dockerized some of our CI, so it can potentially run as part of that.. Apologies, already fixed it. Actually, the point of the previous commit was to remove those #ifdefs since having a member variable #ifdeffed like that is very error prone (if different compilation units compile with different settings).. @krojew who implemented this in https://github.com/google/flatbuffers/pull/4735. Yes, looks like using __union to implement this feature is incorrect, as that is specific to loading unions from tables. Looks like there are no tests in that PR, I shouldn't have allowed that to merge like that :(\nBut yes, your fix would work, though maybe slightly better add a __union_from_vector specific to this use case so we don't add/sub bb_pos twice. Can either of you make a PR?. @sheinbergon I am working towards 1.11 definitely.. cannot give a timeline since it depends on some internal progress.. Not sure if there was an explicit reason.. @krojew ?. This is hard to review since there's a lot of unrelated formatting changes in the diff.. @krojew are you ok with this PR?. Thanks for reviewing @krojew :). @ccifra can you rebase?. Thanks!. Apologies, I broke this, and fixed it just now before I got to these PRs.. like I said on https://github.com/google/flatbuffers/pull/5164 : The point of the previous commit was to remove those #ifdefs since having a member variable #ifdeffed like that is very error prone (if different compilation units compile with different settings).. Yup, my bad.. That is currently not supported, and I don't think it ever will be. We'd need to update all languages to be aware that vector elements can be null, and it wouldn't be forwards compatible.\nYou're best off working around it. Use empty strings/tables. Use a secondary vector that indicates null-ness, etc.. Thanks!. Thanks!. @vitalyisaev2 can you review?. Thanks both!. This was discussed at length in https://github.com/google/flatbuffers/pull/5050\nI was under the assumptions that you wouldn't want debug binaries in your system dirs, but given the fact that people keep asking for it, I guess I am wrong.\nAre there any other changes in the PR that are still useful?\nI don't see how it fixes https://github.com/google/flatbuffers/issues/4753 btw, since that one doesn't talk about build types.\nCI failures are unrelated.. Thanks for the extensive testing, everyone :)\nI'm surprised the existing UNSAFE_BYTEBUFFER is so slow.. didn't know about the need for Dispose() (I'm not much of a C# programmer).\nThe addByte loop may also be good to clean up.\nI'm certainly for cleaning this up and giving the user options, as long as the default way stays the fastest possible.\nNot sure if I'd be a fan of splitting up FlatBufferBuilder, especially if that requires API changes or different API usage to be efficient. Not that I fully understand why it is needed.. @eerhardt that all sounds fine to me.. Yes, CI currently looks to be broken.\nThanks for the fix, I am going to assume it works for now :). @vglavnyy does this look good to you?. Ok, thank you both!. I already responded here: https://stackoverflow.com/questions/54654961/google-flatbuffer-ios/54655894\nYou'll have to ask a much more specific question. flatc can convert JSON to FlatBuffers, and so can the C++ library.\nAs for official Swift support, I've suggested the author of the Swift implementation to contribute to this repo, but so far this hasn't happened.. Looks like the C++ generator isn't checking the deprecated flag for these fields.. can you make a PR with a fix?. Thanks.. the .csproj doesn't seem to have much in it, what does it change?\nShould a .user file be added to the repo?. Thanks!. Thanks!. Thanks!. There appear to be related Java errors: https://ci.appveyor.com/project/gwvo/flatbuffers/builds/22447072/job/6gbxwp3ef32xp6go\n. Thanks!. Thanks for adding the Benchmarks also.\nAny ideas on how we could add to our AppVeyor CI to maybe run with these #defines on and off?. I mean the existing tests. Right now, the code inside those defines isn't tested so if anyone makes a change that affects them we won't find out.. Thanks for the reviews, @ccifra and @stephentoub!\nThere's indeed plenty that can be refactored about the C# implementation, but that can also happen in a next PR. See also my comment about ByteBuffer.. @stephentoub seems to already have approved.. I'll merge once the CI finishes.. The one CI failure is a flake.. merging!. This sounds like it is related to https://github.com/google/flatbuffers/issues/5165 .. @krojew ?. @krojew Which commit has fixed the C# version? https://github.com/google/flatbuffers/pull/5190 has no C# code, if it is related. Our AppVeyor CI runs C# tests, so anything added to that could show it is fixed or not, even if you don't have C# yourself.. I'm not sure what you're suggesting.. are you wanting to store this offset for every field somehow?. That would make all accessor objects \"fatter\", but yes that would probably speed up field access. I guess the design was copied from C++ where something like bb.GetInt gets inlined into a single instruction, but in Java it probably doesn't.\nI agree this would be worth trying out, preferably with a benchmark.. Ok, merged it! It is an interesting trade-off, but I think this will be mostly good for most users.\nI think it may be worth porting this change to other languages where vtable access is not as cheap as C++\n. Added it here: https://github.com/google/flatbuffers/projects/5#card-18099043. Agree, would be good to get rid of this duplicate.. #ifdef __BIG__ENDIAN__ does seem to work on other platforms, so maybe we can change it to #if defined(__BIG__ENDIAN__) || __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ to catch both cases? Can you try that out and make a PR if it works for you?. Ah good point, yes, that looks reasonable.. Apologies, looks like you're the first to compile on big-endian for a while, and apparently this stuff broke at some point. Bear with us.\nThe problem here likely is that T is an Offset<..>, which is not a scalar, even though it is a simple scalar wrapper.\nThe problem originates here, where this type for some reason returns true for is_scalar: https://github.com/google/flatbuffers/blob/master/include/flatbuffers/reflection.h#L424-L426\nThe good news is that this is an isolated case, so should be fixable.\nThe easiest fix may be to provide an overload for WriteScalar https://github.com/google/flatbuffers/blob/master/include/flatbuffers/base.h#L358-L363\nSomething like:\ntemplate<typename T> __supress_ubsan__(\"alignment\") void WriteScalar<Offset<T>>(void *p, Offset<T> t) {\n  *reinterpret_cast<uoffset_t *>(p) = EndianScalar(t.o);\n}\nCan you try this? Sadly don't have an easy way to even simulate big-endian here.. If you want a quicker fix, I am guessing disabling this code will also make this error go away: https://github.com/google/flatbuffers/blob/master/tests/test.cpp#L805-L843. Sorry, didn't test that code locally. \nThis seems to build for me (keeping it in base.h):\ntemplate<typename T> struct Offset;\ntemplate<typename T> __supress_ubsan__(\"alignment\") void WriteScalar(void *p, Offset<T> t) {\n  *reinterpret_cast<uoffset_t *>(p) = EndianScalar(t.o);\n}\n. I guess I am on a newer compiler.. so hard to tell how to fix this.\nIf you can't get this to work, then for now an #if !FLATBUFFERS_LITTLEENDIAN around the reflection test code in https://github.com/google/flatbuffers/blob/master/tests/test.cpp#L805-L843 (with maybe a comment as to why) would be a temp fix to just get it working for you.. Cool, that's a better fix then. Mind putting it in a PR?. That sounds like there's somehow illegal characters in those files?? Also there are missing arguments to the code shown.. I am guessing the code generator is emitting random binary instead of a string, but no idea how that can relate to your change or the fact that you're on a big endian machine.. That's @vglavnyy's change. Maybe that change introduces number parsing (for e.g. defaults in the code above) that somehow doesn't work on big endian?\nWe really need some big endian CI to avoid this in the future?. Thanks for the fixes!\nThis is indeed a very messy situation and pretty treacherous, it is not the first time a signed/unsigned char issue has bitten us.\nI personally don't see the point in e.g. static_assert(std::is_signed<int8_t>, \"text\");, to me int8_t by definition says that it is signed so is always true. If not that would break almost all code out there.\nThese fixes were a number of endian related issues that went uncaught because noone ever tests with it. It be so cool if we had a big-endian test in our CI.. anyone any ideas how to accomplish this? @rw maybe?. Project link: https://github.com/google/flatbuffers/projects/16#card-18099193. Not sure I follow, all our current uses of ReadScalar/WriteScalar need proper alignment?. reinterpret_cast is potentially not strict aliasing safe, but I still do not understand how this affects alignment. I believe all our calls to these functions are aligned to at least their own size.\nWe could replace reinterpret_cast with a memcpy on platforms that are smart enough to make that into a regular load operation with no overhead (which in my testing is most platforms, only exception I have seen so far is MSVC in Debug mode). That would have the added advantage of strict aliasing safety.. Thanks!. Thanks!. Yes, no reason for that not to exist.. PR welcome!. What is idiomatic instead? Does that have the same performance characteristics?. Thanks!. Ouch, that was a pretty bad oversight..\nThanks!. Thanks, this appears to be a nice speedup!. Nice!. @rw I presume this only means you can obtain the hash from an enum, not that anything about the enums themselves changes, so should be ok.. Thanks!. Not too familiar with the Rust build process.. is writing build scripts in Rust itself the regular way to do it?\nAlso while I understand that for Rustaceans being able to pull flatc via Cargo is awesome, do note that bad things can happen if your flatc is not in sync with the rest of your Rust FlatBuffers code or generated code.. so you'd want to somehow enforce that.. What I mean it must be ensured that whatever Cargo pulls in is the same version as whatever flatc is being built from. If the user is entirely independently installing flatc then this will likely not be the case, which will create problems.\nflatc is not something you install thru your systems package manager. It must be built along with the runtime code, whenever you update the runtime code.. Yes, that would probably be ideal (compiling flatc). The version to build of flatc needs to match the exact commit of whatever the runtime files are. So if crates.io only ever gets updated upon official releases (which are infrequent), then the version number would suffice. If you want to be able to update crates.io more frequently then.. don't know. Does crates.io store the commit id it was derived from?\nDepends what is involved in such an official crate. The dependency I mentioned above has to always be maintained. To me, the even more desirable way would be for whatever build process creates the crate to also include pre-built binaries of flatc, but I'm not sure if the Rust world supports that. Or failing that, that it would include the source to build flatc. In either case you wouldn't need any version matching since it all comes from the same build.. If you can tie a crate to a git commit, that would be more robust than versions.. Thanks!. Hmm, I think just adding empty() to the requirements would have been better, it is more canonical to use empty(), and changing all the code to look clumsier to save string classes implementing empty() does not seem like a good tradeoff.. I agree that is unfortunate, and indeed string_view is the natural choice for this. In fact, I'd prefer str() not to exist at all, since a forced copy is so not FlatBuffers.\nBut indeed we cannot break that API at this point, so it would need.. yet another option or attribute. Not sure if you feel that is worth the trouble.. I think B) is the best, since most custom strings will already have such a constructor, and it is the most flexible.\nC) is problematic because it may require for a your custom string header to depend on flatbuffers.h. Thanks!. Apologies, not sure what went wrong there. I couldn't find it either, so I manually squashed and merged your PR https://github.com/google/flatbuffers/commit/eb2a81f73db9893fbe2eec53662fa7f9358e54a8\nThis does mean it does not show you as the author of the commit, though it does refer to you in the commit message. Sorry about that.. Why is the cast needed?\nAlso, space between void and *.. Ok, thanks!. Yup, this has been considered in the past, the trade-off is with trying to reduce the amount of accessor objects needed in Java, and the burden for the user having to reuse them to be efficient. But sure, used correctly, for longer vectors that would be more efficient.\nWe could potentially add this kind of API as an alternative way to access vectors (we still have to support the existing way as well).. Nice!\nShould we have a badge on the main page for this CI?. @rw I defer to you on this one :). Thanks :). Does this make source files also have crlf on Linux etc? Or do these rules only apply to conversion happening on Windows?. Ah cool, thanks!. Thanks! not sure why this was a template at first... Well, there are 2 Bar namespaces here, and it always prefers the one closest to the current namespace (it goes inside-out).\nThe full details of how name resolution works is,.. well.. this monstrosity: https://github.com/google/flatbuffers/blob/407fb5d5379a52e21002b9d887b4bbf36980efd5/src/idl_parser.cpp#L1566-L1617\nIn particular, here it looks thru all parents, based on the current name: https://github.com/google/flatbuffers/blob/407fb5d5379a52e21002b9d887b4bbf36980efd5/src/idl_parser.cpp#L1590-L1595\nThis will cause it to first look for Foo.Bar.Bar.Person, then Foo.Bar.Person (which it finds), and then Bar.Person would have been next.\nI believe that is similar to how C++ works?. That looks correct to me :)\nYou're making a Haskell implementation? How cool! Consider contributing it to the main repo when done.\nWhy are you needing to figure out these namespaces though? You should let flatc handle that, like all the other languages do. Haven't looked at your code yet, but I hope you're not implementing your own schema parser.. making sure you stay in sync with the main one will not be fun.\n. @rw generally looks nice to me.. Global maps would make me uneasy in C++, but in Go it is probably more natural.. You are likely compiling code generated with an older version of FlatBuffers flatc with newer runtime files. Make sure all files are from the same version.\nFor reference, the commit that introduced this functionality: https://github.com/google/flatbuffers/commit/4e5152d886de963162ae81282240bc5377fa10ce. Yes, master is typically ahead of whatever released version is in Maven, so they cannot be used together.. As for JSON in Android, this still needs C++, for example like so: http://frogermcs.github.io/json-parsing-with-flatbuffers-in-android/. Can you check the exact version you are using? What is on line 368 util.h? . My guess is that your code is including windows.h without #defining NOMINMAX before it, before including the flatbuffer headers.\nWe can protect against this in FlatBuffers by changing the above line to *val = (flatbuffers::numeric_limits<uint64_t>::max)(); (we do this elsewhere in the code base for std::max), though obviously the windows header is terribly broken.. @DrYSG: Can you submit a PR with that fix?\n\nFunny thing, we are also using FB for serious gaming R&D work. And quite happy with it.\n\nThat's great to hear! What is the R&D about?\n. That sounds pretty cool.. hope FlatBuffers helps :)\nNot sure why the firewall would block it (you can access git over https). Has been fixed on master, thanks for reporting!. Has been fixed on master, thanks for reporting!. Apologies, already fixed it before I saw this one.. Basic usage of FlatBuffers is a header-only affair, so libflatbuffers is indeed a CMake construct that is only needed for a small percentage of users. The major feature it gives the user is the Parser, for runtime JSON or schema parsing. Unlike many *nix C libraries you may be used to, libflatbuffers is not the definition of the project.\nAs for the C++ API, the tutorial https://google.github.io/flatbuffers/flatbuffers_guide_tutorial.html, C++ usage https://google.github.io/flatbuffers/flatbuffers_guide_use_cpp.html and reference https://google.github.io/flatbuffers/classflatbuffers_1_1_flat_buffer_builder.html should cover it?\nflatc cannot be skipped, as it generates code ahead of the compilation process, which is never part of any lib.\n. Thanks for reporting!\nFixed on master: https://github.com/google/flatbuffers/commit/11198f10f597654c6b0fcbd1299f8761b6ea2390\nThis kind of shadowing is legal in more modern C++, somehow it escaped me it would still be a warning on older compilers.. I guess for now you can leave the loop, as it seems Arrays.equals doesn't take offsets. Converting String to UTF-8 should happen before the loop in LookupByKey.\n. These usings shouldn't be needed.\n. will do\n. It's the base class of all accessor objects. I named it for what it represents, not what it does :)\nFeel free to suggest a better name.\n. Yes.\n. Thanks, was not aware of this. This interface is mostly needed for unions, where we need to be able to initialize any accessor object type generically. Since structs don't support inheritance, I used an interface, which apparently has the same problem.\nSo how do I do anything to a template type which is a struct?\nAlternatively, we could generate union accessors for each possible value of the union, but using templates would be cleaner.\n. Since this is in implementation (they are public, but not intended for the users of the API), lets do that in a followup commit, since this would cause yet more differentiation with the Java implementation. I think it would be good to split them both up, then we can do further cleanup on C# much more easily.\n. So nothing is ever cast to Accessor or transferred to a variable of that type, so maybe this is safe. It invokes a method coming from Accessor, but still on its own type (inside __union). It is then later cast to a Nullable of its own type (when __union returns).\nYes, running your benchmark would be helpful. One thing that worries me that even if the benchmark says speed is good, do we know this is for all implementations (MS vs mono)? And maybe my code is faster than the old code because of struct use, but still masks an allocation for unions.\n. what does this do? it currently only really runs for 2 languages.\n. The Go version will never be merged into this I don't think, so yes, remove it.\n. Ok, I'm confused.. why is Encode faster? I thought this PR would only affect Traverse :)\nTraverse is still a win, since we now don't require people to do object reuse anymore.\n. You're right, that is actually pretty bad. My preferred fix would be to make that field always available. Feel free to do so as part of this PR or a separate one.. Maybe for structs it is not ideal, but I generally see repetitive code here that I would like to see better factored. The simplest part is that v.Verify(reinterpret_cast<const uint8_t*>(&parentTable) + offset, obj.bytesize()); appears twice, which I don't think is necessary, and the second part is checking for isRequired which I think at least for some fields can be done more centrally.. Ah ok, I missed that.. I don't think we want a comment like this in a sample, this is fairly implementation dependent.. Yes, that is actually a correct assumption since the schema compiler always inserts these type fields before the actual field. That said, this is very fragile code. For now, it be better to call the existing GetUnionType, which is slow and clumsy, but better protected.\nBetter yet, the first LookupByKey in that function could be moved to inside an assert, to test it is equal to the way you compute the offset.. Here and elsewhere: & next to variable name. This should not be the case anymore, so can you revert all changes to this file?. You can remove these superfluous {} now. Also, put more arguments on one line (up to 80).. you must also test the is_struct flag.. this file is shared with the GRPC project, so formatting changes here are not desirable, they will make future updates/merges much harder.. This file is auto generated, so any changes here should be because of code generation, not gofmt.. You're right, it would be a bit clumsy to have to pass in the schema just for this check. Though without it, it is possible to call it on a field which is in fact a table, and then things will go badly wrong.\nThis is really my fault, because rather than the is_struct flag, it would have been better as a separate type.. but that is hard to change right now.\nI guess at the minimum, stick a comment in front of the assert, like: \"TODO: this does NOT check if the field is a table or struct, but we'd need access to the schema to check the is_struct flag.\". Sorry. my comment was not about this line, it is about the whole file. I'm saying it is better to change go_generator.cc in upstream project.. Ah ok, good.. maybe prefix this with \"By default\". comment doesn't say anything new :). Note comment above (space around -). 1 << 0 :). we cannot be adding vectors of unions to the samples, since this sample is used by all languages. It has to be a separate schema and test until support is ported to all.. make sure lines fit in 80 columns (google c++ style guide). since the sizes of these arrays are created by whoever builds these arrays, returning false is better than an assert. It be nice to add an assert in the builder code, but I am not sure where.. assert.. this function should never be called with a language that is not in the table.. why 2 instead of sizeof(language_parameters) / sizeof(LanguageParameters) ?.. also never use int.. it needs to be size_t or uoffset_t depending on what it is addressing.. return a reference so you don't need to change all those . into -> below?. it could be: if (lang == java) return .. else { assert(lang == C#); return .. }. auto & ?. else if. has to be an error instead of assert, since it comes from flags supplied by user?. note that you're parsing this schema, but it is not used in the code below. for this to be useful, you should also load a json file with such a vector in it?. use auto whereever possible. leave out comments that don't add to what the code already says.. The task of the verifier is to find out if a buffer is safe or corrupt, and do so quickly, and by, default, without asserts. If a buffer is corrupt, that could have all sorts of reasons, including bad transmissions etc, so errors are not necessarily that useful.\nBut yes, rather than just return, you should wrap the condition in Verifier::Check, which allows anyone to debug a verifier issue.\n. \"Vectors of unions are not yet supported in all the specified programming languages\". please use the same style as the rest of the file, which means & and * next to the variable, not the type. Same below.. This will add new generated code in all languages, which you'd have to add to the PR. Since the change was already in 2 cases, I'd leave these changes to the .fbs file out.. Can you remove it from the PR then?. You'll likely have to make the changes on your local branch, then push again.. const std::string & - to avoid a copy.. part of this code is copy pasted from the function just above it.. please share that code in a sub-function.. style: } else. GenTypeBasic(type, false) != \"byte\" -> SizeOf(type.base_type) != 1 ?\nWhat does this code do? I don't understand how the += works. I know we can't have test cases for keys of every possible type, but can you verify this generates legit code?. Should bb.array().length not be bb.limit() to be totally correct? Just in case the backing array isn't fully used?. Can you explain what this does? If vtable is pointing to the start of the object, this is probably not working. Why is this code different from the other version of __offset?. If an expression is used twice, use a variable?. Please use the style of the rest of the code, a space between the type and *. rather than store in a vector, just iterate through the fields again below.. please add the generated code to the PR (run sh generate_code.sh in the tests dir.. depth_ and max_tables_ also will need to be changed, otherwise you'll get warnings on other platforms when these values are compared. Besides, on Arduino, num_tables_ could wrap around before it reaches max_tables_.. Please adhere to the Google C++ Style Guide, which would make this IsFinished. But note you read the vtable offset with getShort.. whereas it is an int. Again, the code should correspond to what the __offset function above it does.. This also parses schemas, so maybe \"Input file is empty\" would be better.. maybe you can also disable 4512 in these files and remove it from CMakeList.txt? That way it will compile for others regardless of their build settings.. this seems a bit crazy having 2 nested casts.. if you just remove the U from the original code, does that work?. same here. @pjulian : if you have time to help @TGIshib correct this fix, that may be helpful.. Not so sure about this change, since it potentially creates problems and ambiguities with the other CreateString overloads. I suggest leaving this one as-is, and adding a template one with a different name, e.g. CreateStringFrom or something.. There is already a copy of IDLOptions present in the Parser, why are we passing a second copy?. I agree that having to change the generated code for this is undesirable. I guess I am worried about breaking any existing code out there, since we also have overloads for const char * etc, and some compilers may have trouble deciding which one to call. It also makes for hard to follow errors.\nIf C++ guarantees it will always prefer the overloads before it tries the template, then we can merge this. In fact if that is the case we can even leave the std::string overload in.. There's already a lot of generator related options in the IDLOptions in Parser, so yes, that is normal.. this may be quicker with an if?\nif (type <= ULong) ..\nelse if (type <= Double) ..\nelse..\n?. Thanks for checking! For now, please keep the \"dead code\" std::string version in there, to be sure. Also please move the template to after all the non-templated versions, just in case that matters.. You've already checked that it is scalar above, so this can lose the is IsFloat check (or become .. else { assert(IsFloat(... this if-then can be simplified I think :). CreateVectorOfSortedStructs. please use a struct StructKeyComparator much like the ones for tables below. That way we don't have to put an #ifdef for use of lambdas around this (we support some older compilers).. how about return static_cast<int>(key > val) - static_cast<int>(key < val) ?. maybe compare against the actual version of visual studio to make sure it work on later version, e.g. http://stackoverflow.com/questions/18387640/how-to-deal-with-noexcept-in-visual-studio. rather than generate code for this every time, stick it in flatbuffers.h (and call it FLATBUFFERS_NOEXCEPT to avoid clashes).. why not just val < min || val > max, that way you can inline OutOfRangeErrorMsg. string &op, like the rest of the file. I'd be worried some compiler will give a warning saying that this is always false for 64 bit, but I guess all our current compilers are cool with it. Essentially for anything 64bit we are not doing error checking, since we use strtoll, which presumably just gives 0 if you give it a number that doesn't fit in 64 bit. In fact I am suprised it works for uint64_t as shown by your tests. . please double-check the list of instance variables, as I think you're missing some of them, like nested etc.. vecofstructs->size() already returns a uoffset_t. So if you just change the type of i, you don't need any casts.. Maybe move this to util.h ?. Yes, makes sense.. No need for the extra -----------------, just add the // <auto-generated>\\n.\nYou can still call FlatBuffersGeneratedWarning(); on the next line.\nAlso, I believe https://github.com/google/flatbuffers/issues/4287 showed that there should be a </auto-generated> at the end of the file?. They client ?\nMaybe: They cache large amounts of data on the client, which needs to be fast loading and low memory overhead.. I'm sure it's just looking for the <auto-generated>.. Please use similar syntax to the rest of FlatBuffers, i.e. indentation, spacing etc.. Pretty sure it wouldn't fail to recognize it as generated code because of missing ----. See https://github.com/google/flatbuffers/issues/4287\nBesides that, I had 2 other comments you could address:\nYou can still call FlatBuffersGeneratedWarning(); on the next line.\nAlso, I believe #4287 showed that there should be a  at the end of the file?\n. @sharwell. Don't think we need output on the server. Just use the id() in the generation of a response.. this Clear() is redundant. do we need a second ClientContext? a second FlatBufferBuilder?. Fred is not a verb :). nice find!. As I mentioned in your other PR, this would be cleaner by simply adding the s390x's big endian flag to the existing ifdef, e.g. #if defined(__BIG_ENDIAN__) || defined(__BIG_ENDIAN) rather than adding a machine specific define.. Why was this moved here? Seems an entirely independent change from what you're trying to accomplish, and actually makes things worse.. What build failures, exactly? Travis doesn't build for s390x. Sorry to be pedantic, but:\n- This file is used for Java code gen too. Unless Java has the same convention, it probably shouldn't go in there.\n- Why is the closing tag </auto-generated> directly after the warning? From the discussion elsewhere I was expecting the closing tag to either be at the end of the file (enclosing all generated code), or omitted entirely.. Ok, looks like no endian define is on by default: https://www.ibm.com/developerworks/library/l-systemz/\nSo we'll merge this as-is.. Maybe as part of this PR you can also add some documentation, explaining people that they should use MessageBuilder instead of FlatBufferBuilder for GRPC, and what the consequences are.\n. Yeah, no problem with this. There's generally no access to vector_downwards by users anyway.. Nice catch.\nThe lambda will have a reference to allocator_, which I guess is what the old code also did, but it means that people that delete the FlatBufferBuilder before the unique_ptr are going to run into trouble. Worth capturing by copy?. Yeah, this wasn't essential.. Not too familar with GRPC slice semantics to know if all this below works.. Generally, where it really doesn't matter, we like to keep our usage of C++11 as low as possible to support an as wide range of compilers as possible. Our CI includes VS2010 which may not like this new initialization syntax.. We may run in environments that have exception handling turned off. Prefer asserts whenever these are programmer errors (as opposed to data errors).. Maybe some comments here (and elsewhere in this file, where mildly tricky things happen). E.g. how does the slice allocator handle the fact that FlatBuffers leaves empty space at the start of a buffer? Doesn't GRPC assume that data starts at the start of a slice?. some comments as the the exact consequences of this code.. some more comments here too. I'll ask someone... Ah yes, good point. I'm realizing we don't have the GRPC code as part of our CI.. which is bad.. but yes, for the moment,.. we're cool with whatever C++11 GRPC uses :). Why not merge Allocator and DefaultAllocator? Specifically, reallocate_downward is not meant to be overridden, so this is not really an interface. Alternatively, it could be cleaner to leave the logic of what reallocate_downward does outside of this class, unless you have hopes that some allocator somewhere actually can extend downwards in-place.. I'd say the equivalent of = delete in older code is to simply declare it but not implement it (resulting in a linker error). Making it private, as you say, has side effects, and even private: func; public: is not entirely neutral, but maybe slightly better. Just a note, don't feel very strongly one way or another.. Any use of unique_ptr has to be bracketed in #ifndef FLATBUFFERS_CPP98_STL since we're still trying to support some very ancient platforms with this header.. We're cool with those platforms simply not having release(). nit: please use the code style of the rest of this file, which means & goes next to the variable name.. Not so sure about this. If we were to provide our own unique_ptr emulation, I'd stick that in its own header, stl_emulation.h or similar. Though maybe its simpler to just not have this available on old platforms.\nIn either case, it be good to make sure one single define governs this (we already have FLATBUFFERS_CPP98_STL as opposed to making this MSVC specific.. Here we now have a core class dependent on newer STL, this may be hard to ifdef out. Could consider reverting this back to naked pointers, and documenting clearly that we want to own this after it is passed in (at the FlatBufferBuilder level). ok. ok. Not necessarily the compiler config stuff.. but this emulation code is only for a small number of users so could be tucked in a corner.. I think that's a bit much for something that's for very few users and dying out. A typedef to have either naked or unique would be sufficient I think.. This entire first loop (and vector) are not necessary. Just make this iteration part of the second loop.. space after if :). please use the Google C++ Style Guide (e.g. snake_case instead of camelCase). you can compare the field pointers, instead of the names. This goes through the entire stack, which means it can possibly find fields in parent objects. You probably want something like auto pf_it = field_stack_.end() - fieldn instead?. this is one more dynamic memory allocation we didn't have before..  weren't we going to make this part of vector_downward?. That's a lot of complexity to define this DEPRECATED feature which is not used anywhere else in the code base. I'd like to keep this simple if possible. Just mark things deprecated in a comment instead.. Do we still need this one? Not likely to be used externally?. Why do we need a proxy? Why not use the allocator directly? I feel this code is getting a bit complicated :). I appreciate your effort in making this more widely available, but this is a lot of code we're adding for a nice feature. We already had the situation where people on older compilers simply didn't have the \"detach\" functionality available, so we're not losing anything by using unique_ptr and ifdeffing it. Also, I'd like the \"cost\" (extra dynamic memory allocations) of using allocators and detaching to be paid by those using these features, and not everyone else (the 99% :). I don't see where this function is used.. It probably won't find it anymore now that you are comparing field objects, which are different. It should still happen if you compare names instead. Also, it can happen with a recursive table, like table A { b:B (required); a:A; }. Here, it could pick up a missing b from an out object, since the field pointers will actually be equal.. For things that are very unlikely to be used externally, I think that is preferable, yes.. If we don't own the allocator, I'd rather that just be denoted by a bool, and make the client responsible for knowing when it is safe to deallocate the allocator. I agree that proper reference counting is nicer and safer, but we're talking about a niche use of a niche feature here, so simplicity is preferable.. We haven't had problems with unique_ptr on VS2010 (==1600) before (including the code you're replacing), so not sure what the problem is here?\nWe do run the full set of tests on VS2010, which needs this functionality, so we cannot define for CPP98_STL for VS2010.. Ah.. might be nice to add it in the GRPC PR then, so we can see how caller and callee interact. For example, I do not see from this code why we don't simply keep the existing allocator when we reset?. Yes, as static default would be a nice savings for most users.\nI'm generally a fan of solving problems with templates over inheritance.. but again, trying to keep the code nicest for the default case. I know I'm being peculiar.. While the name makes logical sense, DetachedBuffer was better, because a) a FlatBuffer refers to a single block of memory in the FlatBuffer representation, whereas this contains much more than just that. And b) the name would give people the impression that this is the main way you want to deal with FlatBuffers, or its main API, whereas really it is a non-essential helper class.. This may trip up older compilers. VS2010 is cool with it apparently, but I wonder if this is going to increase the lowest gcc on which we can compile this. I guess we can keep it for now, and maybe ifdef it later if need be.\nI guess getting rid of the move constructor would require re-introducing the unique_ptr, so maybe not that great.\nSpeaking of which, is it worth type-deffing this class as unique_ptr_t for the moment, for backwards compatibility? Or is that too nasty since it really has nothing to do with unique_ptr? Maybe not worth it, as any compile errors resulting from this are easy to fix.. Agreed, these are nice to have.. Style nit: keep multiple of these on one line where possible within 80 chars.. * next to variable name.. As above, the Google C++ Style Guide actually wants to put multiple items per line :). The Google C++ Style Guide has a built-in preset in clang-format :). Ok, sounds good.. Not sure either. There's a lot in the style guide I disagree with. In this case I would say it is generally good to just keep the existing code, your diff becomes more complicated with \"drive by\" code style improvements that may just be a preference :). This should now be made into if (it->second.empty()) ?. Why not simplify this method to assume we keep the same allocator, since it is only used from GRPC code at the moment? In fact, it is called with the exact same parameters than on construction, so I don't see why we're reallocating the buffer.. If we're going to be using inheritance, neither of these methods are necessary for access, and add unnecessary public functionality. This is a neat pattern, but since this is mostly used internally, I think a bool would be sufficient too.. Hmm, what a clumsy API. Maybe maybe one function that returns either slice_.data.refcounted or slice_.data.inlined and use it twice?. nice!. rather than this hack, what would be the downside of making both base classes members, and adding a GetFlatBufferBuilder() ? is calling such an extra function unwelcome verbosity? :). I don't think we need inline in front of inline methods, they're already guaranteed inline.. use auto where possible.. Google style: no need for all these empty lines between each statement. Like I said, even if deallocate is just an unref, there's still an allocation in reset that is not needed, and will just get deallocated shortly after if this MessageBuilder is abandoned.. What is this class needed for? Can we inline this code?. Ok, I now see that this is kind of nice, since you now have one point of error checking when you receive the message. Once concern is though that if you check for status and get a ::grpc::StatusCode::DATA_LOSS, you must be able to know that this is a verifier failure, not some generic networking error. So we must clearly document that a) you can set FLATBUFFERS_GRPC_AUTO_VERIFY, and b) that if you do, you should listen for that code. if DATA_LOSS is used also by other GRPC conditions, there must be a way to find out if it was a verifier failure without running the verifier again. Storing a bool in Message doesn't seem that elegant.\nWhen you add it all up, it seems almost nice to just suggest to the user they Verify on the message themselves when they receive it, what do you think?. Ah ok, so it is overrideable. So now we have both a pre-processor flag and an overrideable template, just to save the user from calling a function manually.. this may be a bit much. If we have Verify in all examples and documentation, that should be enough for users to consider if they want it.. I have no problem with protected.. It's both more readable and also less, since it took me a bit of time to figure out why the types were needed :). Ah, thanks @ctiller :). Ok, I guess so.. There's no such rule in the Google style guide, but the FlatBuffers projects uses it whereever it is valid.. @ctiller is ::grpc::StatusCode::DATA_LOSS appropriate for when the serializer finds corrupt data? Or is there a better way to handle this?. Why this function declaration syntax?. please put the } on the next line, and maybe omit the last ,. what's with the \\\\ ?. this logic is copied from just above here, maybe add a roundup() function?. There is still an allocation here.. As I understand it, in the single use case for this function, we reset this buffer and then throw it away.. Should these two defines be merged? copy constructor and assignment typically go together.\nDo we need them at all? If they just cause a ref increase, is there any harm in allowing it?. auto. Sounds like we could just inline this class, since if you're not going to be using the verifier, there really isn't anything else to replace it with, so no reason to override this. The define takes care of all.. We should check the status here specifically for verifier failure (and only verifier failure, not other occurrences of DATA_LOSS).. I think we can make that assumption for now. Prefer to usually not make abstractions that have 0 users.. easy to add later if needed.. yes.. agreed its good to behave the same. still would be nice to be able to know if its a verifier failure or something else, though.. maybe some explanation comments here?. base.h is shared between FlatBuffers and FlexBuffers. If this is not used by FlexBuffers it doesn't need to be here. In fact, I was expecting it to be just a static class method since it is only used inside one class.. The style thru-out the FlatBuffers code is to check pointers without != nullptr. So rather than adding it in a few more places, it would be more consistent to remove it everywhere.. Actually it appears to be only used in one place now, so it can be inlined again :). use ?: ?. auto reserved = old_reserved + (std::max)(len, old_reserved == 0 ? initial_size_ : growth_policy(old_reserved));\nAnd why the cast? make initial_size_ a size_t?\nWhy both reserved and reserved_? harder to follow.\nIn fact, since we can already tell frombuf_ there is no buffer, we don't need reserved_ to signal there's no buffer, and could have kept the older, simpler logic.. as I asked before, why is this necessary?. Indent pre-processor statements. I know that's odd, but that's what the rest of the code does.. ok!\nI guess in the future making comment in code as to why you probably don't want to make copies is helpful.. Please keep lines within 80 chars.. Use style of surrounding code (e.g. } else {. use style of surrounding code, { goes on previous line.. No need for const.. Generalize comment: \"We allow tables both as JSON object {..} with field names or vector [..] with all fields in order.\" or similar.. Less blank lines through the code, keep style of rest of the code.. just NEXT will do as you already know what it is.. invert condition. you can put the ?: inside the Is. let's not call these nested array, but maybe table vector?. ?: inside. since use of these is nested, why do we need to support [] at root level?. what is this for?. please don't reformat things that are not part of this change.. I was referring to the unusual function declaration syntax, as opposed to say inline const Color *EnumValuesColor() { or similar. I've frankly never seen anyone use () around the function signature, or [] after it. Can this be made more readable? I agree that we don't want to rely on std::array. Again, not sure why you can't just use _MIN and _MAX.. Thanks! Docs are better than no docs, so we can leave this as-is for this PR.. Since we want this to be portable, eventually this should become a CMakefile.txt.. but we can leave it for now if you want.. Thanks for adding another example!\nNot sure about the top level examples dir, since we already have samples we could put it under, or better yet use grpc/samples since we already are keeping everything grpc related under there, including tests.. style: location of *, or better yet, auto.. style, template on same line as the rest.. if its not performance sensitive, maybe use std::function for callbacks? or maybe no callback at all, since in an example, simple straightline code to follow is more important than modularity/factoring concerns.. if this is an instance variable because you want to reuse it, you need to be calling Reset on it.. auto &, here and elsewhere.. what is this needed for? there's no constexpr anywhere else in the code, so I'd rather not have it for the sake of older compilers, for the moment.. remove constexpr if possible.. This appears to be a duplicate of Clear below, so can be removed entirely.. I mean Clear :). It's not a big deal either way, there's certainly some inconsistencies in style due to external contributors, things I didn't catch.\nNo idea how to express that in clang-format. There's quite a few things the Google C++ Style Guide leaves unspecified, so it defaults to \"how the project has always done it\".\nI'm a big fan of clang-format, but running it will typically polute your commit with things from older changes. Unless we enforced running it on merge somehow it's hard to rely on it. It also doesn't support the pre-processor indentation and a few other things. Usefully, some editor integrations allow you to clang-format just a selection.. you're right, we do.. yeah, nevermind.. Can one reuse the other, or can code be shared?. but both the * and & on the right?. Ok, good point.. less unneeded empty lines please, here and elsewhere.. Keep style of surrounding code for *. format of else. else... like I said, I see no good use cases for allowing this at the top level, especially with just one field. Let's remove this.. this if can be removed, as Offset is always 4 bytes.. Does the old slice that is being overwritten need its ref-count decreased?. what is this repo and why is it needed. seems very fragile depending on getting a specific version of cmake. why does this not work with the travis cmake? does this bump up our minimum required cmake version (which would not be desirable)?. This PR seems to make a lot of possibly unrelated improvements, like e.g. here getting this version from git. Would it be possible to first make a PR that does the absolute minimum to get the main thing you're trying to accomplish done, and then the rest in follow-ups? This will help a cmake amateur like myself have some confidence that this is not going to break something somewhere. This seems again an unrelated change. Maybe it is an improvement, but I would like to see it separate, with some description of what it does. And wether it affects the minimum cmake version we require.. Can some of these improvements not be made with whatever version travis has? We rely on cmake on a lot of platforms, and requiring people to upgrade what they have installed (especially if it is not under their control) is really not desirable. Would like to keep cmake requirements low unless there is a VERY compelling argument.. documentation for this function says that it only works on initialized slices. Does that mean it works on a grpc_empty_slice() ?. lets leave out the -S ?. is ? a correct way to do this, or are we missing types?. Try to follow the style of the rest of the code-base, thus template on same line.. no need for blank line, here and elsewhere. not that I mind too much, but doesn't seem using stringstream has any advantages here over using stting. this if-else can be simplified :). ooh fancy :). simplify. maybe nice to add some indentation, for readability?. I don't think we want a todo in the root. Can you instead add this to a github issue?. bit_flags should be handled just like regular enums (they are numbers). all bit_flags does is assign those numbers differently. Why is this making a copy? you can still use auto &. Why not for (auto it = namespaces.begin(); ... ?. Also, use snake_case instead of camelCase. use auto where possible. There is no code anywhere else that outputs to cerr, so this is not a good error handling mechanism. It be better to simply handle all types in some way. What happens with e.g. BASE_TYPE_STRUCT, which is equivalent to a JSON object?. Why is this vector copied?. indent these 2 lines?. if description is empty, should it maybe be left out to reduce clutter? Same of any other fields that are optional?. Since this is what 99% of user will want, no need to state it is to remain consistent with previous versions.. This one is probably not needed, since it is already covered by the other methods.. why is this public? would a user ever want to have a stream to a partial FlatBuffer? Maybe inline into the function below?. just put the return inside the try and catch.. Rather than copying this test code, put it in a method?. Yes, it may well be that the original public was unintentional. It could probably be inlined.. this wasn't addressed. The reason to put it in a method is such that you can share the code with where you copied it from. There should only be one copy of this test code.. indentation. lets not hand-code our own string-compares. Do people actually use $SCHEMA or $Schema ? If not, do a straight string compare. If all 3 exist, I'd say enter all 3 into a set auto_skip_fields created in the parser at initialization, and test against that.. You should actually expect a string here, if that is the only valid value for a schema.. Like I said, please replace by EXPECT(kTokenString). Can we call this data() instead? So far when a function or type is meant to emulate an STL or C function/type, we have given it similar naming also.. Please indent preprocessor statements similarly to the rest of the file/project.. arg type should be & vector ? The majority of its usage should be const.. I am aware of the Google Style Guide for non-const, but since this is trying to replace an STL function, it should do so with as little changes as possible.. can you explain? So far we've had std::unique_ptr in the code base working for VS2010 just fine. Would be great if we could do without a wrapper for this compiler. Can we stick this in flatbuffers/stl_emulation.h or something?. This appears to copy code from the existing CreateVectorOfStructs.. can the two share code without being inefficient?. See, I am guessing this is caused by the VS2010 wrapper. I'd really prefer it if the 99% of the world that doesn't care about STLPort anymore can just refer to std::unique_ptr and doesn't have to worry about why we brought it into our namespace and what changes that could bring.. does your comment evaluate as part of a static const variable? if so should we do it?. Kinda bad we need this gradle cruft in 2 places? I guess that's how they roll?. Should some of this be moved into a stl emulation header?. This is a lot of boilerplate just to be able to keep using a lambda. Maybe it should be a macro? :). Besides being test code, this code is also an example of how to use the API. It's not great that this is now more clumsy to support a compiler hardly anyone uses.. maybe better if it was just ifdeffed out.. I think having non-STLPort users be able to usestd` types is pretty important, so yes, lets go this way.\nI'd love to drop VS2010, since the pressure of people wanting to use more of C++11 is pretty great. This is something we can consider seperately. But first order of business is not requiring them use even less of C++11 because of STLPort :). I guess I'd be fine adding such functions, since if this call is ever speed-sensitive, that will be about the loop, not about the pre/post code. Also, a user will likely only ever use one of the two functions in a code base, so inlining is likely.. Sorry, I was under the assumption the compiler would tolerate log in regular const expressions, that indeed doesn't make sense.. Hard to tell which is worse without seeing the code, but begin/end style methods to factor out some of the common code maybe preferrable. I used a lambda because it was the simplest way to factor out the common code, but if a single use of a lambda requires all this helper code, it's not good value for money anymore.. yes, a duplicate test under an #ifdef would be better.. My intent was that this #include be conditional on #ifdef FLATBUFFERS_CPP98_STL, such that the file is generally not needed at all.. Should these be private?. I believe the majority of occurrences of template in this code base put them on the same line.. In my last review I asked if this could be called data (to match the STL function). It already sits in the flatbuffer namespace, so I don't feel this needs vector_ to be clear.\nAlso, I think the argument should be by & rather than *, to match STL, and since most uses are const anyway.. indent similarly to the rest of the project. indent similarly to rest of project, here and elsewhere. We're replacing the STL, so STL style > Google style in this case. So both & would be best. Failing that, just the const one.. Then the whole project is in violation. Consistency is more important than adhering to a style however, so this should be indented until such time that the whole project is fixed, if that is to happen. I don't think it is a good idea to leave the project in an inconsistent state.. See my comment elsewhere.. consistency > any style guide. If this is to be fixed, then all at once, and not in this PR.. Just call ReadInt64 here?. Put this behind all the AsInt functions?. The body could be if (type_ == TYPE_STRING) { .. } else return AsInt64() != 0\nActually I'm not sure how I feel about this string special-case.. if the JSON contains \"true\" as a string, it be better to serialize it as a TYPE_BOOL, and then we wouldn't need this code here at all.. again just call ReadInt64(data_, parent_width_) directly.. ReadInt64(data_, parent_width_). just static_cast<int64_t>(b). That's a bit of a hack :)\nThe better way is to introduce a kTokenBooleanConstant, and to check the uses of kTokenIntegerConstant to also deal with this new token (in some cases, not all). Also, in ParseFlexBufferValue, you should check for strings true/false. These comment don't explain anything extra.. same below.. put & and * next to the name, and use snake_case not camelCase, like the rest of the file.. maybe rather than bool make this field a StructDef * ?. use same indentation as the rest of the file. this makes for a weird type, a [ubyte] that has a StructDef set. Better to store it in nested_flatbuffer. else after { like rest of file.. EXPECT('{'). snake_case here and elsewhere. inline this function if not used elsewhere, same with cleanup. I don't think all of these need to be copied for JSON parsing to work, probably only enums_.. Yes, but you're creating a new kind of type that's a mix between [ubyte] and my_nested_type which is a bit weird. If storing it in nested_flatbuffer works, that is probably better.. Then they would copy the same things from the parent, so that should be ok? Frankly recursive nested flatbuffers sounds a bit crazy :) I'd say keep the code as simple as possible for now.. maybe skip these tests if numpy is not present?. maybe stick the error in a function?. Thanks!. what does this do?. * next to var.. This doesn't look correct anymore. This kind of comment doesn't really say anything.\nIn general, comments should be: // Like this. not //like this, and only when they say something new. No need for empty lines before/after comments.. yes that was the idea.. though if they're in seperate files that may not be as easy?. Like I said, this code looks incorrect. Why is it still looking up the attribute? Why is it getting the type from nested->type.struct_def instead of field.nested_flatbuffer? Are you running generate_code.sh, because I think nested->type.struct_def would be null?. Yes, please update such names.. not sure how they got in there.. (sorry for the late reply, somehow missed this)\nYeah, this is indeed tricky. I guess one could argue that if someone writes \"true\" instead of true then maybe that is what they want, or if they didn't, then the fact that they get costlier string storage is their own fault.\nIf we go that way though, you could also argue that a string should always be false, regardless of wether it contains \"true\", since a string is not a boolean. Or maybe, \"\" should be false and all other strings true, analogous to how it converts strings/vectors to int. That is already implied by return AsInt64() != 0. I guess I don't like this string special case.\nIf were to go with having the parser convert \"true\" to a boolean value, then ToString would get the correct string back, but AsString wouldn't. So yeah, maybe lets not go with that.\nI'm kind of in favour of the simplest solution here, meaning no special case at all.. As discussed above, lets not do this.. these tests can be removed if we're not supporting booleans in strings. Why not stick this information in FLATBUFFERS_GEN_TYPES_SCALAR ? less duplication that way, and probably simpler code.. yes, this will be hard to review with different spacing. Please make sure the only code that is changed is code that needs to be changed.. Why is this? What effect will this have on other users? @krojew @JoshGalvin . The fact that it requires the macro to be changed is unfortunate, but it is a simple change.\nI do think it's the cleaner way, since the macro has one IDLTYPE parameter, and now we'd simply have two. The parser can then check for both in the same loop.. please remove it then.. I'm not sure what you're saying. I'm not applying a formatter. You should make this PR in a form that shows only non-whitespace changes to the code you're changing.. return (type_ == TYPE_BOOL ? .. : ..) != 0. return type_ == TYPE_BOOL && ... Generated code should be part of the commit. We do that such that we can spot the effect of future changes to the code  generators.. indent case. why does this keep changing?. line between \" and \" should be <80 chars.. type -> prefix | suffix ?. No idea.. you'll know more about Python & gRPC by now than me :). why, what should they do? How would that affect C++/Go?. please remove the space between * and AsWeapon. well, what is the cleanest way to lift out the protobuf specificness? Should that happen here or in the grpc project?. On one line please. Can you add at least one use of As() in test.cpp?. Why are these not inside Reference ?. constant?. This code has a lot of overlap with the code above, any ideas to refactor?. maybe at least pull the file identifier arg out of the if?. Not sure why we're creating a new ByteBuffer here, ideally this refers to the existing one?. I'd prefer it if we keep the test simple without needing yet another schema. You could simply generate a size-prefixed buffer in code, and then see if you can access it correctly.. Why is this a generated function? It is identical for all, so could be a global function elsewhere.\nActually, even GetSizePrefixedRootAsMonster is not necessary, you could instead just call GetRootAsMonster(GetBufFromSizePrefixed(buf)) or something, especially since this use case is not as common.. I'm aware we're not copying the buffer, but I generally like to avoid object allocations where possible.\nI guess the + then - trick would be ok, but really we shouldn't need to touch the position at all if you see how the existing getRootAsMonster works, you can just generate a similar method that does _bb.position() + 4 on the fly. . I'm not sure that an intra-language test is that important, since all we'd be checking is if a language uses the right size (32 bits)... pointers. why is this necessary?. This particular file needs to be compile with other compilers. Either --clang-nullable should be by default off for these test files, or we should #define _Nullable for non-clang compilers. . Is this supported by every implementation of Java (and Android)? Should it be conditional upon gen_nullable?. These 5 lines are copied from Allocator::reallocate_downward.. either it needs to refer to that function, or that function should be removed?. can you please use the same pre-processor indentation style as the rest of the file.. Given that the current version is 1.7.1, maybe set it to that?\nSince I do releases, I'll make a note to update this with all the other versions I have to update :). We can't duplicate all these files from what is under tests/.. it should just refer to the existing files.. again, can't duplicate this.. refer to elsewhere.. You're serializing a builder? what does that do?. what are these 0 and 1 elements? Can we have some comments in this file so people can learn how the API works from it?. why is this needed?. But now you're serializing the result of one serializer + its support data structures with another serializer, which is a really bad idea. Please make sure it directly uses the serialized FlatBuffer data.. yes, monster_grpc_fb.py should still remain under grpc/tests/python much like the other languages.. I don't understand why this has to be in a method though. What other values can this method return? Why is it relying on the name of the builder variable?. The FlatBuffer data is already bytes. By using pickle you are wrapping it (and any data in the builder it doesn't need) a second time. This needs to be removed.. This was previously only executed if FLATBUFFERS_INSTALL was on. Will this work on e.g. Windows / Visual Studio?. should we check here that if it is not scalar, it is actually a vector?. why is this moved down from its original location?. can you run generate_code.sh to see if your changes have any effect on the current code? Moving where code is generated would show such changes. It's best if you can keep such changes to a minimum. please use indentation to match the rest of the code. spaces around /. indentation. else on same line as }. {} around else block.. move * much like rest of this code.. Can you instead move the operator new further down?. can you put this right after the constructor?. nullptr. please document above what this warning is.. can't do this.. Why was this indented and reformatted? Please undo.. What does this comparison do? If the value is \"0\" then StringToInt will do the right thing, so it is unnecessary.\nWhat is necessary however is to distinguish between int and float types, since the contant may be a value like \"3.14\" which currently would get truncated. You'll need to inspect fd.value.type, and if its float/double, parse a float instead (strtod).. the alignment is actually Right for this project. We also don't run clang-format on unchanged code, so I'm not sure having this file is a good idea.. If we're going to have this file, please make it language specific. This repo contains multiple languages that do not follow the same rules.. Why can't this be under grpc/ like everything else? Also, it seems like this is conflating 2 issues: making flatc available for Java users, and support for gRPC in Java, ideally those would be independent, and independent PRs. This makes it harder to review. It is hard for me to see if using flatc in Java without gRPC is possible, for example.. If this means it is going to require me to build these on 3 platforms, then pull them back to Linux to be able to run Maven, then that is very inconvenient, and not sure that is something I want to commit to.. Can this be in grpc/tests like the other languages so far, and use the existing schema, again like the other languages?. Sorry, we cannot have this kind of duplication, they'll have to be pulled from their original location.. It be preferable to keep things in existing locations, and keep things testable without Maven. I do not know what the requirements are for pom files.. most build systems I know can address files from anywhere, is that not the case in Maven?. please refrain from making formatting changes unrelate to your change, makes it much harder to review.. No need to add this function, just use strtod directly.. indent. Probably better to remove this test.. users shouldn't be calling AddElement directly, or use hard-coded offsets.. Instead, just load the default schema used by other tests?. rename this directory from flatbuffers-java-grpc to just java ? The other two parts are obvious from context. directory-wise, here main/java also seems redundant. Since this is a runtime file, it would make sense for me to be stored in the root java folder instead, much like grpc.h in C++ is stored with the non-grpc headers.. this should probably be in grpc/tests'. does every dir containing Java need one of these files? this cannot be centralized?. Thanks, but these examples seem a bit out of place. In-line documentation should be short, to the point, and non-redundant (no need to remind people of facts stated elsewhere in this file). More extensive explanation and examples should instead go into the .md files.. MaybeAdd minimal type/name/attribute reflectionto make it obvious this is incremental.. Maybemini_reflect_namesetc to more easily see that these are related.. These functions look very wasteful, allocating memory from what are otherwise nice static structures. The list of attributes is typically very short so linear search on them should be ok, or if not, we can make sure they're always written in sorted order so we can optionally do binary search.. I don't see the point of this helper.. please remove.. This should probably also be a pointer since it is typically not present.. These previously were not generated if the values are equal to their indices, and now they are. Please check what is causing that..goes next to the name.. This seems like a wasteful encoding if it is so sparse, since each unused item is 3 * sizeof(void *). Maybe better to make these pointers too, so unused ones can be nullptr. see comment above.. this should also include names.. We're not likely to add a keys field just for this use case. The order should not matter, in fact, I would say it be a good idea to ensure sorted order, so just use dict.. space before.. or clang-format.. both StructDef and EnumDef inherit from Definition which holds attributes, so this this code can be simplified.. use&&=.. this is not a bitwise op.. just stick this check in the if below?. please use auto where possible.. I would say only user-defined attributes should be generated.. Sure, but you should also appreciate that the problems people may encounter with FlatBuffers vary wildly based on their background, and we can't haveflatbuffers.hturn into a tutorial for all sorts of cases that may not be relevant to the current reader. Hence my original request.. thiscounttemplate really doesn't have much use, and causes some unneeded typedeffing and casting below, let's remove it.. my idea was that this field would be a pointer too, but I guess an array of size 1 can work too.. has this been addressed?. make this argument aconst char to not force unnecessary string allocations on the user.. this function is not needed. same as above for string args. replace this with apairsince it is just for temp data?. I'd recommend against macros in the generated code, as this doesn't help the compiler nor the reader, if any.. this is going to generate a lot of code in all languages, for a small feature that currently only affects C++. Please instead add a bunch of attributes to an existing field.. remove(). some comments as to where these constants come from, and/or make them into named constants.. please runsh generate_code.sh, this will ensure the generated code in this commit will have the same settings as before.. where does this formatting come from? unless done by clang-format, please don't reformat things that are unrelated to your PR.. missinge. rather than using template specialization for this, just check for the right type at the call site.. useautofor local variables where possible.. throughout this code, we are simply appending totext.. thisreserveserves no purpose, since I doubt you will get a perceptable speed difference, but it does obfuscate the code, so please simplify.. why does this function encode into avectorand not astring? If it took astring, we could pass ittext, and it could append straight where it is needed. See similarly howEscapeStringis called.. why is this alignment needed? if this is to align it with cache lines for performance, please remove. prefer simple code over optimizations that have not been shown to be a clear bottleneck.. more comments / better identifier names please. This code is pretty cryptic.. move testing code to test.cpp. can you in-line this table? reduces the amount of new files generated.. Thanks for adding these parameters.. they have to be used as well, though, thememcpybelow is now invalid, it has to do the same as the twomemcpystatements inAllocator::reallocate_downward. Better yet, pull those 2 mempy's into a method and call it from here.. We're not using exceptions anymore.. why is any of the tests and samples stuff exported?. what's the need for this path?. why do we need a library for the tests?. is this file strictly needed? always looking for ways to not clutter the root dir :). Please make sure thegoes next to the name. Also,Defaultdoesn't sound quite right, since there are no alternatives. JustTypeTablewould be too prone to clashes. Maybe..ThisTypeTable?GetTypeTable?TheTypeTable? Or go verbose and sayMiniReflectTypeTable..?. Please use the style of the rest of this file, e.g.{not on its own line. Use clang-format if needed.. spaces around=. No need for empty lines. just add an arg here rather than default args.. This is going to allocate a std::string for every number parsed in a vector. Why don't you make the argument toParseSingleValuea*so the the null-check can happen only when the error occurs?. maybe call this \"field\", which is what it almost always is.. why were these spaces removed?. string on the same line?. change these into an enum?. Just make it rely onFieldDefdirectly rather than this template trick. You can includeidl.hsince this this file is included in places whereidl.hwas already included anyway.. Comments are sentences: start with a capital and end with a.. field. please refactor these functions.Base64EncodedSizeis not needed, since the destination is a vector (or string), so we can simplypush_back. There's no need for an intermediate vector either. Just pass_texttoBase64Encode`.\nI'm going to stop my review here since it is clear my comments from last time haven't been addressed. Please confirm you have addressed all my comments (from both reviews) thruout the code before asking me to review again.. maybe keep the existing behavior of returning null, which is easier to test if a field is available or not.. keep these calls in the same order as the method above it for readability.. FlatBuffer data is always little endian, so no #if required.. As per the comment above, it should point at the 4, not at the start.. Funny that your tests still work.. it is because you've overlapped the vtable with the vtable offset, and they're both 0 :). Beyond this test, there is no functionality that actually needs such an empty table. I'd rather this would be part of a bigger PR that actually does something with this functionality.. We didn't need this _off in Java, why here? It is both not great for performance (since Position is used a lot), and makes it harder to tell if this code is correct, i.e. if _off is taken into account in all the right places.. Maybe note that this only applies to parsed JSON?. builder_.Finish(toff, file_identifier_.length() ? file_identifier_.c_str() : nullptr, opts.prefix_size), no if-then needed :). same here. why is this needed separate from FLATBUFFERS_CONSTEXPR ?. yes, that code looks more correct :). sorry, did not catch this earlier. This is the Java/C# generator, which shouldn't be in the library.. see below.. ok, makes sense.. Why not make this the only version of this method, and have the accessor function add new Monster() as the first argument for the original function?. I don't think so. Methods starting with __ are not intended to be used as part of the API, and are sometimes public because they need to be accessed by internal code.. or just because whoever implemented forgot to make them private :(\nSo maybe the original method can pass null and __lookup_by_key can check for it? That's an additional conditional, but I think that is acceptable given the complexity of the method.. True.. just thinking of a way this can be clearer. Maybe wire format (-b) instead of buffers ?. debug mode is the default in CMake. Why is this added? Seems unrelated to --size-prefixed so should be in a different PR if at all.. use spacing 2 * like the rest of the code. For clarity, maybe better to call the other function than to repeat code.. Same here. This reads size prefixed. Code for writing size prefixed seems missing.. I'd vote for one option. I don't see a lot of reasons why 2 would be better.. We may not have clang-formatted in a while, so yeah, maybe leave that out for now. There is a way to use git to clang-format only your changes, which would be good. There is no reason for a nested_flatbuffer to have a size prefix, since its length is known from the vector it sits in.. Why is Data removed? was this unused?. some methods here are virtual and others not.. any reason? I don't think we intend this class to be inherited from, certainly not if it affects performance for basic functions like GetInt etc, which it may.. what's the use of this one? Why would anyone ever want the unused part of the buffer copied?. Where did this function come from / where is it called from?. This function is called a LOT, so it potentially becoming slower by being virtual I am afraid is not acceptable.. The surrounding code does not use camerCase. Actually just put the cast in-line, and no need for the comment.. You say this is on your Mac.. does that mean these timings are from Mono, or the MS implementation?\nIf this is using the MS JIT, then indeed you have a good point that performance is acceptable for _off. Though part of this may be that the C# ByteBuffer is already quite slow.\nI think we can do better though. If you look at how getting the root works in C# generated code:\npublic static Monster GetRootAsMonster(ByteBuffer _bb, Monster obj) { return (obj.__assign(_bb.GetInt(_bb.Position) + _bb.Position, _bb)); }\nThis tells me, that to create a root from a size-prefixed buffer, all we need to do is pass it a ByteBuffer whose _pos has been moved forward by 4 bytes. All buffer accesses from there on are absolute, using the bb_pos stored in these table accessor structs as the starting point, so no need to keep adding _off at each access. And likely more robust since we can't forget to add _off somewhere.\nAm I missing something?\n. Supposedly parts of MS's implementation are portable now and run on Linux, and I presume OS X as well, but haven't looked into it. It is entirely possible that Mono is not quite as optimized.\nI'd prefer it if we use just a single variable, for efficency (possibly), especially since size-prefixing is such a minor feature compared to the base functionality. To screw this up, someone would have to load a size-prefixed buffer, slice it to obtain a new one, then call Reset on it, and somehow not understand that will get you your size prefixing back? That sounds unlikely to me.\nI'm not sure if we're getting the correct results here. You're only calling bb.GetInt which doesn't even touch _pos! It directly uses its argument into _buffer.. just make this an if in the caller rather than template specialization.. move to test.cpp. don't pass it opts etc, only call this function if base64 is requested.. check here if base64 is requested, not inside the function. all the complexity in this function is unnecessary. a std::string can have variable amount of characters added. So remove this code for computing the size, and inline this function with B64EncodeImpl and simplify. Same for decode. This is the third time I am asking. I will stop my review here. Please do not ask me to review without addressing comments.. Please do not introduce new local variables for such minor level of repetition, this makes code harder to read (since you now have to understand the lifetime of the variable, and what it was defined as). Here and elsewhere.. * goes next to the name. Have you tested that this gets correctly re-generated?. Do we have any Dart experts who can help review this file in particular?. this looks expensive. In C++, string pooling is optional, is that the case here?. Why is this kind of special purpose class for bools needed? FlatBuffers vectors (and strings) are typically not compatible with the host language version of these (except in C++ to some extend), so if this creates overhead compared to a more direct way of accessing FlatBuffers data, it may well not be desirable. FlatBuffers is all about avoiding heap allocation, so accessor objects should be as lightweight as possible.. why are the ones for bools singled out here?. bools can be null in Dart??. Does Dart guarantee by-value semantics for small wrapper objects like this? In Java for example our offsets are naked ints, to not pay the cost of object allocation everywhere. This sadly of course is less-typesafe.. Dart appears to have genericity, so why are these functions duplicated per type?. there's currently no support for string defaults in FlatBuffers, so this can probably be simplified.. How and when are these reader classes used? Reading a single field needs to be super fast, so allocating objects is not desirable.. The builder above seems to keep multiple of these around. In most other implementations, there is always at most 1 copy of these tails/offsets around at any one time, and they are reused. To compare vtables, you compare directly against vtables in the buffer, rather than keeping a copy of these lists.. No need for this new variable.. Please check elsewhere too.. Wether duplicates are likely really depends on the use case. There are plenty of cases where strings don't duplicate at all, and then you end up copying all strings into a map for no reason. FlatBuffers should by default be as low allocation as possible. Since in C++ it is opt-in for that reason, I suggest we do the same in Dart (most other languages have no string pooling feature at this time).. It was not made based on benchmarks, no. But allocating an object for every offset generated (which there are a lot of) seems excessive.. If its faster then definitely keep them.. Generating binaries that are 100% compliant with the other implementations is by far the most important concern of all. So yes, please remove this special case. In fact, as part of the test, also reading the binary generated by C++ is a small step in helping to verify that.. Allocation is cheap, GC is not. You pay the price elsewhere.\nBeyond that, there's the fact that FlatBuffers purpose is to reduce the need for object allocation and copying. So even if Dart's memory management is indeed magically cheaper than other GC languages, it looks weird to have a library like FlatBuffers generate a ton of garbage.. in this case, you have a list of _VTables that live as long as the builder does. I propose to only let one _VTable object live that long (and inline it into the builder). That should always be an improvement.. There's also a Conan build file to be updated.. There are language specific files in docs/, can you add this file there instead, including links to it from the main docs?. we have 'sample/' that have samples for all languages, all from the same schema, please add this there. Then in 'tests/' is where the tests for all languages (again from a single schema) go.. This API style seems to hide the FlatBufferBuilder that exists in other languages. That means it either doesn't have one and these calls allocate their own memory (which would be inefficient), or it is global?. should this be null if its not being used (by default)?. Why are these two at the top, and other scalar readers further down below?. would it make sense to in-line this data into the Builder above? It has no use anywhere else.. we're at 1.8.0. should move to tests/. This file contains a lot of low level tests, which is awesome! Ideally we should have tests that mirror what the other languages in tests/ do too though.. Ah sorry you have those already. Maybe point people to docs/source/DartUsage.md specifically also. Or maybe better just make this file purely a pointer to that file?. Please use similar code style as the rest of the C++ code, which means GenerateEnums etc.. A lot of these comments appear superfluous.. please use auto where possible.. Is this the only way to do enums in Dart? Can't we have integer constants instead somehow (without a class)?. Ok, I'm confused about a bunch of things here:\n\nWhy does there need to be an abstract class and impl, rather than just 1 class?\nWhy double? Are these 32bit in Dart?\nDoes the declaration of double _x mean an actual variable is stored? If this an accessor, it should really store zero things, as the data is in the buffer.\nFor my education, what is this ??= doing?. This appears to be creating a BufferContext for each Vec3, or? what exactly is this function meant to do? Why are bytes stored in ints?. Builder classes in other languages don't actually store anything, they directly write to a buffer. The whole design of FlatBuffer is about avoid object construction / copying, which this ignores.. The internstrings bool should really be in the builder object that contains the string map?. Ah, never mind, looks like Conan deals with this generically.. Ok, that's fine, leave it here then.. I certainly don't want copies. Dart packages can't pull from different directories?\n\nI'd prefer it in sample/ since there it can share the schema, and it is useful for people to see how FlatBuffers is used in different languages all in one place. Also, generally it is a structure we've used so far, and would be nice to not break for the sake of Dart.. I agree that it is not likely to be an issue for Dart, but it is more of a style issue. FlatBuffers is about creating and reading buffers in-place, and is about efficiency. Even in Python (probably the slowest language of all) we adhere to this pattern.. Then why not make the null-check indicate if interning is used, and not use an intern var at all?\nYou'd definitely want just one of these per buffer you're creating.. I have to maintain version numbers for a lot of languages, so it is more meaningful if they are in sync.. symlinks are problematic in Windows.\nComplicated how? Again I'd expect a packaging tool to be able to pull from different places.. It is more important that it matches the semantics of FlatBuffers I feel, which represents them as integers. If enums cause object allocation, that be quite odd. The other languages also prefer speed over convenience whereever applicable.\nIf you feel strongly about convenience, a seperate \"object API\" (that sits on top of the base API) can be added.. one exists for C++ and is planned for other languages.. 1. in what SDK code? I don't think it makes much sense to inherit from automatically generated classes that represent data in a FlatBuffer.\n3. With allocated, I meant stored, sorry. So yes, these variables shouldn't exist.\n4. So this is attempting to cache all values? I'd think it should just load from the buffer on each occasion.. Still not getting it.. where do these bytes come from and what do they represent?. Again, as per the above, I don't think any values should be stored. There really is no reason to.. It be fine to determine that globally, I think.. I'd be fine with a publish script that copies things around, as long as they're in git in one location.. these two seem largely equivalent, so might as well use string for both cases.. we're compiling on older compilers, so prefer not to use newer C++11 syntax like {} constructors.. prefer to use 2 += statements instead of a single one with an + for efficiency. Also don't need to construct a string from . here first.. reduce amount of blank lines.. this seems all quite verbose.. we have no need to count the digit count of a number for the other implementation, why here? Can this be simplified/removed for the current use cases?. return on the same line, here and elsewhere?. Yes, that be fine, especially since it is not the default path.\nMight as well write it to a char buffer, then create the string from its contents in the end rather than resizing (that way the result is small when it can be).. I'd get rid of the intern variable at all and just check _strings to see if the string needs to be pooled.. I guess that is much more acceptable then. If this is idiomatic for doing enums in Dart I guess it is ok.\nMy thinking is that you should want a representation that matches FlatBuffers semantics (which is cross-language) rather than trying to force the language way of doing things. . I don't see the use case.. why would a Dart user be constructing a Vec3 out of bytes rather than floats? A flatBuffer never contains only Vec3's, at the very minimum it needs a root table to hold them.. It would be good to do that.. I didn't suggest global variables, I mean to store this information just in the builder for a buffer, of which there should be one instance.. That creates something that is not a FlatBuffer, and is not something that is supported by most implementations afaik. I'd remove it, as it could confuse people, and there really is no need for it.. If this is going to be a way to decode whole buffers, then yes, allowing it on all tables is nice to have.. force this in flatc.cpp, not here.. entities?. instead add extra arguments to ParseAndGenerateTextTest to test both?. Definition also contains a name and doc_comment already, so these can be removed below.. why is generate_code.sh changed (can't comment on that actual file). Having these two being an Object requires RPCCalls to be serialized after objects.. which is fine I guess. An alternative would be an index into objects much like elsewhere.. Inserting fields in the middle like this is an incompatible change and would make this schema incompatible with all pre-existing binary schema files. Add at the end, or give all fields a correct id attribute.. I hate to ask, but can we make this flag shorter so it lines up? :(. was this caused by clang-format? if not, prefer not to reformat unchanged code.. There is no problem, merely noting the consequences of this choice.. See my comments on the previous PR. pickle takes data that is already serialized (already a byte array) and then serializes again, together with unneeded object data. Please remove the use of it, and use the FlatBuffers byte array data directly.. No need to check for null, since Get never returns null (see my last comment here: https://github.com/google/flatbuffers/issues/4704). E.g. if you serialize an offset of 0 in a vector (which asserts in C++, but maybe not in other languages), the pointer returned by Get would be a pointer to the location of the offset (see IndirectHelper on how this is implemented). We could check for this specific pointer but I do not think that is necessary, since it is just one of many possible pointers that point inside the buffer, but at the wrong location.. T is the table type here, which does not represent the size to be checked, as the size of a table can only be known dynamically.\nIf table is out of bounds, it will actually get rejected by VerifyTableStart() (called by table->Verify()), so unlike what I said in https://github.com/google/flatbuffers/issues/4704 the original code is actually correct (since table is never read from before it fails verification).\n. @englercj No, this makes no difference in terms of size, since objects are serialized once regardless.. rather than silently failing if the length is incorrect, this should probably be a panic?. why are these 0-bytes written? these are not written in the other implementations. This now aligns to a multiple of 8, which is not necessary, as this needs to align to 4. So just pass either one.. If you made this \"incorrect file identifier length\" you can drop the fmt dependency (and who knows how much code).. No, that code is not quite the same. In Java, it says, \"after I write these data items, I need the alignment to be minalign\" (which is 4, or 8 if there's any long/double fields). Your code says \"after I write 0 items, align to 8\" (which is usually correct, but wasteful). So it be better to copy what Java does, and use minalign.. would prefer this to be part of the root .gitignore, that way it is easier to overview what is all being ignored. Not sure if this file can be here. Google has a requirement for copyright assignment when doing a PR to a Google repo. Since this is from code originally from someone at Google, can we let this fall under the root Apache 2 license? Can you ask @scheglov or whoever you got the base code from? . My knowledge of Dart is minimal, so I'll leave it to you to judge. That said, we cannot have any duplicated files, and we already have 8 or so languages conforming to a particular pattern where tests and samples are in a particular locations and share particular files/schemas, so it be really good not to have to make an exception for Dart's sake.. for example, this files can't be copied, regardless of what structure you choose for the other files.. has this happened?. I guess if it comes from Chromium then it will be fine for Google. As far as being included in this folder, it could also be copied from the root by your script, if the same license can be used.. I see the test/sample files themselves are still in dart/ though, are you going to move them as well?. ah :(. Can you add this to the doc too?. As you said, this is not for fields that are not present, merely for default values. maybe \"emit default values in binary from json\" as a more precise description.. If it passes the CI, then its all good, so feel free to try that if it is cleaner. I'm also fine with the macro, since it is used just in implementation code.. Please indent the #preprocessor commands like the rest of the file does.. Now that the code below is changed, do we still need iostream and this ifdef at all?. Please indent #preprocessor commands here and elsewhere. use // clang-format: off where needed. std::string already comes with null-termination built-in (right after size(), so I don't think the +1 is needed.. this doesn't look correct.. if precision == 0, this means it wants default precision for the type, not 0. I think the if can be removed, and also remove the default arguments = 0 from these functions.. remove blank lines from this and other functions. For very simple If's like this, make them one-liners.. Looks more appropriate to stick this in AbsolutePath.. actually that already has FLATBUFFERS_NO_ABSOLUTE_PATH_RESOLUTION. Indent these + clang format comment, like elsewhere.. If you stick this code on top of the include handling, you don't need a #else since this uses return.. Do we really need this file?. I know this is additional testing burden, but might it be worth it generating this code for C# as well while you're at it? that be cleaner than all these language conditionals. There's a test script that runs with mono on linux/mac, or a test project for VS.. according to the jitpack website, I should be able to do this without such a file. Worse, the only thing this file appears to be doing is specifying a particular kind and version of jdk, which doesn't seem like a good thing to do. Can we remove this file?. It is needed whereever it indented, since clang-format will put them back at column 0.. Hmpf, yet another top-level file.. ok then.. Thanks :). I don't think so.. this fixes the case for reading fields, but what about for writing (serializing) them?. put the if only around field.value.constant and False ?. please use code style of surrounding code, e.g. is_bool, and no random blank lines.. use auto where possible. Ah seems bools are already handled there.. Can you also add this to Compiler.md ?. This can also be removed.. This globally allocates a whole bunch of strings, even if they're not used. Make them const char *.. see idlgencpp.cpp for an example.. Would be better to stick this in PythonGenerator so it is only initialized when used.. This function should ideally be called everywhere a .name gets put in generated code, not just here.. please use the same style as the surrounding code, e.g. default_value. use style of surrounding code: *.. Use style of surrounding code: snake_case for variables, here and below.. { on previous line.. Thanks for the refactor! Even though is_string will never be true for structs, it is cleaner this way. And nice to use the trick with - for both.. These files should actually be in git. Reason being is that future changes to Lua codegen we'll be able to see exactly how it affects the output.. Why are these ignored?. Can we have slightly more commenting? Trying to read this code I am still not exactly sure what it represents and how it does it. Maybe some comments on how this is the most efficient way to do a binary array in Lua.. lots of hardcoded values in here that could use some clarification. This really makes a speed difference in Lua?. invert the condition?. use same indentation as surrounding code. const char * ? prefer to not do memory allocations at the static level. no, only names which if used as a field/meyjod would produce an error.. like I said, the generated code should be included in this PR. This seems unrelated to your PR, how did this end up in here?. also in the .sh version. not at this location. Just a general reminder.. Yes, constructing the std::string where you are using it is still better than global.. This is very Windows specific. Maybe better to leave this line out, and assume that the files have already been generated prior to running this test (that's how it works for most Languages).. The whole point of Add() is that it is overloaded, so that it can be used in contexts where the type of the argument is not know (e.g. a template). The version you added is not overloaded (by type) so has no such use. In that case, using Null is better as it is much more informative.. This is a very specific comment that is not useful to the majority of users of this very generic function. I am suspecting you had trouble writing a blob, but this is not the way to document this. The fact that you cannot pass (const void*, size_t) to a T & is probably already clear.. Overloading in C++ is generally perilous, which is hard to avoid when supporting with very weakly typed things like const char * etc. As I said, it is best used with templates, for explicit construction you are better off calling Builder::Blob.. That doesn't seem necessary to me.. can you put these in the opposite order? First IsVector etc. It is nice if the reader can go from most general to most specific for vectors.. IsTypedVectorElementType is still in the wrong order?. does this still allow this file to be used with Python installations that don't have numpy?. please use the C++ code style here ({ on same line etc.). Should this not return the VectorOffset of the new array?. I don't understand how this is so much faster when it allocates + copies? Is that because the overhead of going thru a function call to access an element is bigger? Did you benchmark vs a version of the old code that reuses the accessor object?. this file is weird to me. It doesn't actually test anything, and has a bunch of unused code. Can it be cleaned up? removed? replaced with just using test.cpp?. did you test this code? sizeof(data) is the size of the pointer?. Same here.. name() returns a flatbuffers::String which I am pretty sure ostream doesn't understand. You still need the c_str().. Can you make FlatBufMethod store a BaseGenerator & such that some of namespace functions in there can be reused here?. Lets not have yet another class for this. The idea was a simple function that just gives you a raw pointer back, + size/offset. Is this worth special casing? If it is not a common occurrences, just letting Prep/Put do nothing may on average be faster and clearer.. this is pretty awesome.. Should this function maybe always be available, for backwards compatibility?. actually never mind, the user has to explicitly enable ENABLE_SPAN_T so it is ok. If C# supports it, can we have these directives indented, here and elsewhere?. move comment out of #if. These *._generator.cc files are copied from the grpc project, so we should avoid making changes in them, as this requires more difficult merging later.. auto. maybe put these 3 lines in a clear_allocator and also call it from the destructor?. Ah I guess leave it for now then.. this looks inefficient, creating a new list. Can you somehow roll this into the loop below?. It's ok to do multiple strings on one line, efficiency is not as big a deal here.. this seems like a weird check. A file identifier is associated with a given root type, so you should output it if this table is the root type for this file.\nAlso, what is with the repeating SaveFile, are you overwriting?. { on prev line, same below. IsTypedVectorElementType doesn't seem of concern to the user most of the time?. Why do we have these here? If they are meant as documentation they should maybe be comments?\nLast I worked on this code, such include and link dirs are not necessary if you make install gRPC, if I remember correctly.. Does step 2 not happen automatically as a consequence of 1? I never remember building protobuf manually.. If this is actually needed (which I don't think it is, see above), it would be better to create a CMake variable for the install path, and make all of these relative to it, such that users don't need to modify this file.. none of this was needed last time I tried.. just ./grpctest.. Why are you allocating a DefaultAllocator explicitly?. Nice!. If these env vars are not specified, we get.. /include ?. Like I mentioned earlier, if you a regular make install without the custom dir, none of these environment vars appear to be necessary, is that the case? Should that be mentioned in this doc as an easier way? Also, I don't remember having to build protobuf seperately.. why are we doing this? if we are currently already accepting .2, then now rejecting it may be problematic.. That's not a confidence inspiring statement.. what compilers do NOT support these feature? Are they above whatever minspec we compile with (VS2010, gcc 4.8 or whatever is the current minimum?) If these are older compilers, lets not have this statement.. Ah I guess that answers my question. This is unfortunate. Maybe the docs can say this explicitly, but as a note, e.g. features X Y and Z can be parsed (note if you are using a compiler older than VS201X or gcc 4.X then these may not be available). Can you explain why we need this?. This has a lot of code duplication with the signed version. Can this be lifted out into a shared function (or template) ?. this seems clumsy.. we have a function that returns 2 things, but then needs to unpack them into 2 variables. better to have 2 accessor functions, or no accessor at all.. We can remove InvalidNumber if it does nothing.. use auto wherever possible.. why are these separate from the statement above? constructing error messages is not performance sensitive.. That's some extensive testing, thanks!. is this portable??. please also add this to BUILD. this phrase is a bit weird.. what exactly happens if an offset in the buffer that points outside of the buffer is turned into a slice, and then accessed? How is that different from a raw pointer?. \"Rust may provide this in a future version\" ?. Given that users of the generated API likely aren't calling push_slot themselves, this message may be confusing? In fact, it may be easier to just have one message for nesting violations.. function doesn't appear to do anything?. what should be simplified about it? maybe worth mentioning in the comment.. do Rust people have a typical line limit?. should this be shared with the code above?. hah, that's a neato way to write multi-step definitions!. and another occurrence. rust doesn't have a way to query these from a type??. Wow, not being a rustacean, all this follow/phantom/slice slice stuff is pretty mind-boggling. Doesn't need to be part of this PR, but would be good to at some point explain this stuff somewhere, not just for interested readers, but also for users that end up running into implementation details. Or to sell the specific benefit of what Rust + FlatBuffers allows, which may be beyond other languages.. Hmm.. while it doesn't matter, not a huge fan of having multiple ways to look at types. I'd have preferred it if this code either used the existing types, or improved what is not ideal about the. Also, a seperate enum value for each vector sub-type seems weird to me.\nThen again, don't feel super strongly about it, so don't see this as a demand to change it. Other reviewers?. more on one line?. use auto wherever possible. please change .bat as well. should this be _follow if there is a possibility of a clash with generated fields?. seems weird to include this in the repo?. inline in the create call?. should bad fieldname be generated with a macro?. or just use assert_eq ?. fair enough.. we can merge then.. This file is cloned from the same file in the gRPC repo, so modifying it is undesirable. Not sure if this can be done any other way.. Update the description.\nShorter if possible.\nAlso in Compiler.md. This is a bit very hacky. I know its a lot of layers to plumb thru, and I like to be pragmatic, but maybe grpc_generator::Method can have a reference to grpc_generator::File which can have a reference to IDLOptions ?. So this is a breaking change, would be good to put [BREAKING] in the title or something.\n. This isn't generated code.\nFor actually generated code, we don't support line-limits.. this is a file we copy from gRPC, sp ideally it be fixed upstream as well... please do not reformat unrelated code, keep #preprocessor things indented. If this resulted from clang-format, please add // clang-format: off as required.. lets keep our \"testing framework\" header only if possible. since there is an assert(0) in there, it is intended to halt on the first test failure.. maybe call this file test_builder.h so it sorts with the other cpp test files.. just stick all this code in the header for simplicity? if not, there may be other build files that need updating, e.g. BUILD, Android etc.. Ok, that seems indeed nicer than what we had before.. change the minimum possible.. ahh yes, nice to share this code.. Yes, that be great. Or at the same time. Whatever works best.. No, this meant it is nice this code is shared between the main test and the gRPC test.\nI still prefer things to be in headers, but I can be convinced otherwise.. @sutambe I don't think I know who should review it.. I'd leave it to their triage and see what happens :). why is this named differently from the define above?. indent please. why is this by default on?. This is not safe against future extension, if someone adds a new type to a union, then this will return equal in old code for new values that are not equal. So maybe default false, but add Equipment_None to be true?. please use code style of surrounding code, { on same line etc.. did you mean to remove --rust ?. this is a bit ugly, mixing them with struct pre-declares. Can you give them their own section below?. Add at least one test which uses these operators?. well, if it compiles for all platforms it is fine to use.. I meant, since it is intended to halt on first failure, there's no need to count fails, and no need for a global, thus this code can be kept in the header.. no need for the cast here?. this seems overcomplicated logic. Wouldn't if(c == '_') { /*ignore*/ } else if(upper(c)) { if(i) s += '_'; s += lower(c); } else s += c; suffice?. Yes, please refactor. Same with all similar looking code below.. It may be used by others, but I suppose this is ok.. ah, didn't spot those.. This could probably be moved to idl.h, as base.h does not assume a parser is even present.. this sounds very generic.. FLATBUFFERS_HAS_NEW_STRTOD or something?. Google C++ guidelines want member vars at the end.. remove the enum?. I wasn't aware of this, and the fact that we need all this is very unfortunate. It be nice if we could parse floats correctly regardless of the locale settings on a build machine.. but that involves writing our own strtod? Are there any better solutions?\nI've personally never understood locales.. the idea that code should parse things differently depending on the local language seems very error prone to me.. can we detect this automatically to not have to bother the user with it?. yes, or util.cpp as much as possible.. these are pretty big templates, to what extend can we factor some of this out to go into the .cpp? If it becomes too messy then ignore this comment.. why does this need to be duplicated from the one in the root?. this PR that is about to land has factored out these test macros, maybe you could use the same? https://github.com/google/flatbuffers/pull/4902. Maybe some comments on what all the new code in this file does and why it is needed?. nice.. thanks for adding all these tests.. These files previously weren't part of the repo, as they aren't for most languages? why change this now?. this file seems duplicated from the definitions above?. why is this function here? maybe better to be in code_generators.cpp?. Does it make sense to switch to a single file for Python?. That seems totally fine to me. I don't think key as an attribute currently implies required, but it probably should, which means the value to compare against should also be present.. this code can easily be de-duplicated with a template.. Yeah, it sounds like there is always some locale involved.. Indent. please use rpt_type and msg or similar.. remove 1 + and make it >= ?. You probably don't want these in the PR. Should these maybe be in a seperate table? I can imagine JS gets optimized better if theres only one type of key in a map. Then again performance may not be a concern here?. Well, is it safer then to generate two tables? Anyone else an opinion?. I asked a JS/V8 expert, who said mixing types like this is unlikely a performance issue. So we can merge as-is.. Please indent preprocessor statements to match the rest of the project, use // clang-format: off wrapping.. it's confusing that this pointer type has the same they key. maybe call it default_ptr_type ? And maybe also mention this in CppUsage.md under # Using different pointer types. ?. We still compile FlatBuffers on some relatively old compilers, so please use a traditional constructor here.. flexbuffers.h:346:13: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]\n       if (i < v.size() - 1) s += \", \";. You can simply do if (i) if you move this statement to above ToString :). We still want these kinds of values to be representable, so the correct behavior is to store them bitwise in a signed number. { on next line.. Why use varargs? that seems to make it error prone. Just move the default arg to last in the _ version.. probably don't want this file checked in?. Ok, I guess we'll leave it like this for now.. This seems from another PR? rebase?. Please place * like surrounding code.. I don't think I see the point of this.. this adds a lot of code, and all it does is cast to the inaccessible base class? I understand the current IsFieldPresent can cast anything, which is less safe, but there has to be a better solution. Maybe we can do another inheritance level for example, make a BaseTable that inherits from private flatbuffers::Table and then generated classes can publicly inherit from BaseTable ? Once we have that, there may be other places in the code base that currently use Table that can then use BaseTable instead.. use all caps for variables like the rest of the file?. Why do we need these? I don't see them being used anywhere.. I think the PR template does mention this.\nSuch a CI check would be awesome :). I'd be hesitant to rename Table, as that will break a fair bit of code. Maybe call the middle level GeneratedTable, or is that too verbose? Whatever makes sense.. I don't see a use for boolean enums, since bool in FlatBuffers always takes one byte. So you get the same as using ubyte except now your enum is not extensible. I'd suggest to not allow bool.. If this entirely subsumes the shell scripts in this dir, should we remove those?. Please explain why this is needed (in comments). There's 2 other fuzzers in this dir (for parser and verifier), please mention them in this readme, and maybe show how ti run those also?. Please explain what is going on here and why this is need. @sutambe who started this file.. Please use the existing code style for *.. please rebase?. why not reuse the existing flag for this?. use auto where possible.. Generally code that is never used should not be there, even if it would make sense to be there.. ah, makes sense.. I suppose we could allow these as integers in any context (by moving them to be parsed as integers in the lexical analysis), though I don't see the need / use case for this.. Windows abs paths do not start with \\ (though network paths do), they start with [a-zA-Z]: .. though not sure if we want to parse all that stuff here, especially if GetFullPathNameA works with both. Why was this change needed? If it is an optimisation I'd say leave it out, or if only realpath (or GetFullPathNameA) don't work correctly with paths that are already absolute, I'd say move it to where they are called.. Did this change have any effect on any of the generated files?. again, this doesn't look like it tests anything meaningful on Windows.. Why was this ./ here before? Remember this code should work with JS in all sorts of environments, browser and server, very old versions etc. If you aren't sure nobody will be affected, you should leave it in.. You didn't actually address my comment above. Also there are still formatting problems ({ should be on the same line).. While this is a more lenient system, it does introduce the problem that the moment you add a second instance of a type in your union, all your code generation changes. Before, your code generation would change the moment you switch to using aliases, which is a little more predictable.\nNot saying your approach is wrong, but we should be aware of this. It shouldn't affect too many people, but I wonder if it is worth it, being less predictable.. Also the ident is a bit confusing, since it could sound like \"type aliases\" are ambiguous, whereas only the types are. So maybe uses_multiple_type_instances or something of that nature would be more informative.. So far the testing of aliases was tests/union_vector, since it was coupled with other union features not all languages support. Moving it into this schema enables it for all languages, and it seems that it mostly works (since it is just renaming), but we have to make sure. Alternatively these new types could be moved into union_vector.fbs instead.. Internally they are normalized to using /, but we don't do any processing for Windows drive letters. So no, this is not posix.. I certainly hope people don't use absolute paths in include statements. I think we can assume they are relative already.. I know close to zero what is required for JS, certainly taking into account older versions etc. If you don't know for sure either, we should get someone else to review this PR. Maybe @krojew knows?. Ah interesting.. @rw will probably want to enable this test for Rust.. Other people may use those traits to map from type to enum? What do you mean by \"always providing the declaration\" (example)?. Ah ok, didn't understand. That does sound good, yes, though _NONE is also the union's null value, and it should be possible to produce such a value, which I think becomes impossible if we remove this case. Of course, having to use a random unused type to produce it is really bad, not sure if anyone relies on it.. If you don't understand, ask, don't ignore :)\nHow about:\nif (i) s += \", \";\n      v[i].ToString(true, keys_quoted, s);. Please also change in BUILD and Android.mk files. Is this always how Bazel does paths? Would it be nicer to do -DBAZEL_TEST_DATA_PATH=../com_github_google_flatbuffers/tests/ so this string is not hardcoded in the the source?. That was just an idea. If this is really always the case then I guess it is fine as is.. We copy this file from the grpc project, so not sure how useful changes here are.. We already have a FLATBUFFERS_CONSTEXPR elsewhere. Please use same style as rest of the file, { on previous line, snake_case instead of camelCase, * on the right etc.. use ? :. why are both sides converted to string?. You can delete this NONE case.. This file copies a lot of test harness code that is already test_assert.h. Either factor that out, or use the existing test.cpp for this purpose (I'd strongly prefer the latter).. Please move this to CMake ?. Please add some comments to what this does and why it is needed.. Why would we remove it? I'm not sure if we're still needing to support GCC 4.4, but maybe keep it in until we have stronger reasons not to support it.. Why change this to a switch? I'm pretty sure even old compilers will optimize a constant if, but I am less confident about switch. Since this code only runs on big endian (which we don't test) we might be making things slower without knowing for certain users.. again, these files are copied.. don't think it is worth making changes here.. I think originally this was put here to avoid clashes with the include files above.. how are we sure this compiles, since we don't build with FLATBUFFERS_MEMORY_LEAK_TRACKING on?. In general, this PR seems to be fixing a LOT of unrelated things, certainly unrelated to the title of the PR. While I appreciate you're cleaning up code as you find them, this all would be a lot easier to review if util.h changes were split from other #include optimisations, const related changes, etc.. anything template doesn't require an inline ?. this file is copied from the GRPC project.. are these fixing from upstream? if not, worth seeing if it has been fixed upstream and pulling in the latest file?. DooM!. You probably don't need both.. I meant the CMake dir. Less files in the root the better.. Sorry, I am no CMake expert, and neither is everyone else.. Why are we deleting these? they run on a different CI service, on a different OS (Windows). These languages should be mostly OS agnostic, but it could catch something? Are we confident this is 100% superfluous now?. I still see plenty of code that doesn't match the code style in the tests. Please have a look.. Why is this file deleted? We have generated files in the repo to review code generator changes. Please leave it in.. What is that 60 minutes due to? If its due to Python with Numpy, maybe just disable those? I bet the tests that rely purely on pre-installed runtimes are fast?\nI'm pretty sure no Windows specific features are being used. Coverage of testsin different environments seems like a good idea to me, even in theory they're identical. And who knows at some point docker starts acting up again, then its nice to still have these tests.\nI'd keep them, though we could trim them if one is particularly slow.\n. Sorry, I have no idea what starlark does, so I didn't know there was a conflict. Having this file not be deleted is definitely the most important. Surely you must be able to generate this file somewhere else, and still build with the existing test.cpp ? This being a build system I'd hope you can specify an additional include dir somewhere.. Please use snake_case, not camelCase, here and elsewhere.. Also, lines <80 chars, { on same line.. please read Google C++ Style Guide and apply to your code.. I appreciate that you've made a sample, but really this would be better off in the tests, since these samples are not run as part of testing. Also, would be better to use the existing .bfbs file rather than generating it anew.. no empty lines needed, unless function is really long.. Ok, that sounds good.. thanks for explaining.. This was using byte for a reason I think (efficiency).. It might be nice to make the initial version of this not change the tests, so we know you're not breaking the API. Are you breaking the API? if so, how, and where? Should maybe be behind a flag if it is a big change?\nAlso, are you running sh generatecode.sh and adding the result to this PR? or is the generated code not changing?. I'd recommend not having this option. Having 2 kinds of binary schema files doesn't help anyone.. If you're going to indent the comment, then also the code following it.. Nice, wasn't aware. Those should definitely be inline.. Hmm.. While I am sure we can improve a lot about the API, I am not we'd want to break all of it. And maintaining 2 side by side generators (and test/sample code, etc etc) sounds pretty bad too. I defer to @rw on this.. Using the verifier to decide doesn't sound great, better would be to use reflection::SchemaBufferHasIdentifier, and when that fails, try again assuming the first 4 bytes are a size.\nI guess to me --size-prefixed is a niche feature, so --schema-size-prefixed is a niche of niche, and would prefer to not polute the options with it if we can reasonably either demand only regular FlatBuffers are used for binary schemas, or silently detect both with the above option.. There is still an import pickle here?. Why this change?. Would be nice to point to the main docs and tutorial in docs/ or https://google.github.io/flatbuffers/. It be good to have most of the docs under docs/ that are user facing, so they end up as part of the website etc.. Please move these files to be in test/ to be analogous to all other languages.. This is nice and short, but.. as I understand it, this generates an \"object API\", where you construct an object which then gets serialized. The whole point of FlatBuffers design is that it can create (and in particular, read) serialized data without creating language objects first, but directly.. As part of this PR, you need to sign the Google CLA, which as I understand it, means that code that goes into this repo, won't be your copyright anymore.. This is a bit unfortunate, since for all languages code generation from a single .fbs is supposed to be a stateless operation. If it is so that we can't get around this, we better document super clearly that code generation for Julia has additional rules.. use size_t.. unsigned int will give warnings on 64-bit compilers.. use auto where possible, here and elsewhere.. indent. ahh, looks like they're simply duplicates, so remove them here.. these are handwritten tests? Should also be in tests/, though maybe under tests/julia'.. Ok.. I guess we haven't been doing that so far, but ok for now.. Yes, it is entirely about performance. If Julia's meta programming / JIT can make this just as fast as an API that constructs serialized data directly, then that is great also.\n.--no-includesis for not generating include statements in the generated code. Afaik all generators do not generate code for definitions coming from included files, instead it relies on those being generated already when flatc was invoked on those other files. It be great if Julia could follow that pattern as closely as possible.. We are only just adding official support for Julia to FlatBuffers, so I'd say breaking the API should not be a concern, doing the right thing is more important now. Those existing users should probably continue to use the existing code and the upgrade at their convenience.. Certainly I don't want duplication of anything.. but moving them would be great. Don't want Julia to be the exception :). You should be able to callflatconnamespace_test1.fbs, and get a stand-alone working Julia file (or files). Then you compilenamespace_test2.fbswhich produces include statements to pull in the previous files, and only definitions for what is innamespace_test2.fbs`.\nIf you look at existing generators, they check .generated on any struct/table/enum to skip them, these are the ones from the includes.\nI am not familiar with how Julia does namespaces so I can't tell why it is not possible to follow this pattern. Some language allow namespaces to be \"re-opened\" (like C++, JS..), others use prefixing (like Python) to achieve the same.. Why is this needed? Would prefer to keep this warning on.. Why is this necessary? This should be default on most systems, no?. Style: placement of *, space before {. This cast seems funny.. if ByteBuffer::ByteBufferPointer is the new way they're writing it, maybe make the above function also take one? Or inline the old function?. There already is:\nif(FLATBUFFERS_BUILD_GRPCTEST)\n   if(CMAKE_COMPILER_IS_GNUCXX)\n     set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-unused-parameter -Wno-shadow\")\n   endif()\n...\nlets keep it specific to GRPC.\n. Thanks, I'll ping someone to review it.. Hmm, I'm afraid I am not much help there. Have you asked around with Julia experts how they would solve this?. clearing these bytes seems expensive, why not just leave them as-is?. what is this needed for? It doesn't appear to be used anywhere.. might be cleaner to reset this one also? if you clear a data structure, you expect it to be functionally equivalent to creating a new one. So it shouldn't retain state.. can we just give it an empty default constructor rather than adding all these platform specific warning disables?. Ah, it doesn't like the const reference member.. maybe make it non-const or not a reference?. Why is this in util.h? This is also an inefficient way of doing it, better keep this code to where it is actually used.. This is the only place where your new \"force\" feature is used. Why is it needed? Can it instead be solved locally?. What are the consequences of this? I don't think we want to require our users to specify this flag for things to work correctly.. I presume this is somehow better/faster than just *(iter - 1) ?. I guess this can stay in.. if we want the file_indentifier to not contain these character, that's the responsiblity of the schema compiler, not the code generator.. don't think Python needs () inside if.. Please add this to the docs as well.. Can you wrap this in a function, such that the code can just be e.g. code += \"  return \" + struct_def.name + \".end\" + Verbose(struct_def) + \"(builder);\\n\"; ?. this line can be lifted outside of the if-then? In fact only CreateString / CreateSharedString needs to be inside of it.. Same here.. maybe also mention it in the object api section below.. Not sure about the name now, since you're using it for both the object api (all start with native_) and the base API (the direct function). So I think we need a name that reflects that.. maybe just shared ?\nAlso, please mention here exactly where this currently happens, i.e. in Direct functions and Pack.. Ahh sorry forgot about that.. Why is this disabled? What if it is in use by someone?. Can you take the description from the current main page? We support a lot more languages nowadays :). What are the consequences of this?. Is this person really the maintained for the Redhat package?. ByteBuffer was put in originally to make it easier to share the code generator between Java and C#, as they were deemed similar. Since then, the two have diverged more and more, and there was a plan at some point to fork the code generator for the 2 cases, to make it easier for C# to go its own path. That way \"inlining\" ByteBuffer would also become an option.. Ah, hadn't realized we had exposed it. Still, some amount of inlining/refactoring is probably possible if needed, especially if we fork the generator.. The one sad part of this is that is will generate a lot of overhead if you loop through a vector, since it obtains the vector every time. But with the current API there is no alternative I guess, and it is better to have the option than not.. NULL. probably makes more sense to require a size_t argument. Not sure why it is doing this, but this change is not desirable. It may need a blank line somewhere to work, I seem to recall.. remove (). make sire all lines are <80 columns. This is not great, as it creates more code for all users, even though it is needed for a tiny minority. Would be better to leave the original in there for most cases.. Typically prefer git clang-format to limit changes to the new code, yes.. Aren't indexing functions in Java typically called at? Or is get more typical?. maybe get the vector just once, to not promote people to copy this style? :). while we're caching, could cache the length as well?. There doesn't appear anything specific to tables in this class, it could easily work for a vector of doubles or whatever too. Why not add support for vectors of other types while we're at it? I'd think that the gain of only looking up the vector just once is most pronounced with iterating a scalar vector.. Would be nice to also output code for C#, if you have access to a C# compiler.. But that's what I am saying: the current TableVector could actually work for other types, and would not need a copy per type.\nIt be fine to leave out the mutation functions for this particular class if it makes things complicated, since if someone  wants to mutate, they can still use the existing functions.\nGenerally it be nicer to not add copies of such classes to generated code whenever possible (efficiency wise).\nIn C++, the Vector template always has mutation function, irrespective of whether any mutation generated code exists.. Fine, leave it out for now then.. I understand we'd actually want to test this functionality, but this is going this is going to generate a lot of churn whenever this binary changes. Since it is somewhat niche functionality, I'd almost prefer it if by default we do not turn this on for the code generated in this repo.\nAlso, I'd think it be even better if it sat in its own header (since then it is easy to not pay the overhead for all the users of this header that don't use this function call). The way it is now, this will generate a lot of data in each compilation unit it is included in, some which will be optimized by the linker, some will not. When I said that it is ok to put this data in a header, I meant for it to be the ONLY thing in the header.. not even with a inline function around it (it be good if you got an error if you included this twice :). Both code and comments are repeated 3x below.. maybe stick it in a function?. When can field be null? maybe better such that it is never null?. this does kinda the same. Maybe a better way to factor this is one function that does Field->std::string * for the vector type, which returns null for not present, that way this logic can be shared.. ",
    "XBog": "any changes/progress in the c# code generator? =)\nhttp://www.gamedev.net/blog/49/entry-2259791-flatbuffers-in-net/\neverything that makes the FB creation easier is welcome ...\n. hey momegil-cz! nice work.\nI had a bit more drastical changes in mind :)\nBut as gwvo already said :  setters currently less so, since the methods are static (they don't require a C# object to be created)\nThe perfect generated code for me to use, would be:\nMonster monster = new Monster(); \nmonster.Mana = 100; \n....\nbyte[] example = monster.Bytes;\nBut that just seems not  (so easy) \"possible\".\n. efficient > usability. But optional? As i said, \"for me\" it would be perfect, because i`m using c# only for flatbuffer bytedata generation that don't change at runtime. For that i'm currently writing wrapper classes around the flatbuffer classes, to be able to use the normal \"automatic\" c# properties reflection/serialization. For the dynamic data i'll use c++ . \nSo yes, if i would need efficiency, i would stay with the current solution. \nit's just my laziness :)\n. ok, it could be seen as an issue if you are a beginner :) but it can be done like this:\nhttps://groups.google.com/forum/#!topic/flatbuffers/-TAa87iDyCg\n. ",
    "mormegil-cz": "The problem here is that each non-scalar field gets two read accessors: One parameterless, that could (should) be converted to property, but there is also the second accessor which receives an instance from outside as a performance optimization, and that (obviously) needs to be a method. However, in C#, you can\u2019t have a property and a method with the same name. We could prefix the method, e.g. to GetYyy (MyType Yyy {get {\u2026}}, MyType GetYyy(MyType obj) {\u2026}), but this could lead to naming collisions (I could have Yyy and GetYyy in my schema). We could use some \u201cugly\u201d prefixes like __GetYyy, but that is not really C# style (but could be acceptable for such members used only for performance optimization?). Or we could use properties only for scalar types where this problem does not occur, but that would be a bit strange and user unfriendly, IMHO.\nBut in fact, there are already possible naming collisions with methods like StartXxx and GetRootAsXxx. So we might as well just use GetYyy and be done with it. Objections?\n. Take a look at my bcdd5941 which implements this change. I feel the properties are a better/more idiomatic way how to do that in C#, even with those downsides. (E.g., when I needed to debug my flatbuffer-powered communication, I tried to dump the incoming structure in LINQPad, only to learn there is nothing to be seen, as the structure has no properties, and I need to manually call each accessor method instead of just calling .Dump().)\nAlso, note that the process of reading and writing a flatbuffer are usually quite separate in your code, so the difference between x = MyObj.Prop; and ObjType.AddProp(x); is not really confusing, IMHO.\nWhat do you think?\n. Right, and especially the Java-styled ByteBuffer.position() which I was almost shocked to find. ;-)\nHowever, I am still a bit torn\u2026 The current result is quite a hodgepodge\u2026 Isn\u2019t there a way to do that in a really simple, unified and idiomatic C# style?\n. Well, I am using .position() in my client code using FlatBuffers API, exactly according to the documentation:\n\nThe buffer is now ready to be transmitted. It is contained in the ByteBuffer\nwhich you can obtain from fbb.dataBuffer(). Importantly, the valid data does\nnot start from offset 0 in this buffer, but from fbb.dataBuffer().position()\n(this is because the data was built backwards in memory).\nIt ends at fbb.capacity().\n\nAnd, AFAICT, the only alternative to using .position() would be to make a copy of the buffer with .SizedByteArray(), which is quite unnecessary when e.g. writing the buffer to a stream. Or, is there a better way (in which case, the documentation should be updated)?\n. I made a few more unificating changes, so that all accessor methods start are named GetXXX, and I changed the API classes to use properties per the above comments. The end result (and its usage) now looks almost acceptably, I\u2019d say\u2026 :-)\nBut an opinion from another C# programmer would be most welcome. What do you think, @XBog?\n. @gwvo: I think the code (including documentation) are ready in my branch, do I need to squash it for a PR, or is that unnecessary, given you cherry-pick it to Gerrit, anyway?\n. I have squashed the commits and created the PR, see above.\n. Enums in C# should not be sealed classes. If anything, they should be static classes. But in fact, they should not be classes at all, they should be native enums, see #171.\n. Correct, this is quite a bit more work, I'm working on it in my branch; I believe it works already, it just needs a bit of cleanup and squashing.\n. Well, yes, I noted it generates unnecessary casts, both in Java and in C#, I just thought the logic to differentiate between cast-needed and cast-not-needed would needlessly complicate the compiler code, given the unnecessary casts would probably just be ignored by the Java/C# compiler.\nBut if you think cleanness of the generated code is important as well, it should be doable. Or, a compromise version where these unnecessary casts are left for C# and excluded in Java.\nRe DestinationType: Note that DestinationType expresses the destination type in terms of FlatBuffers types, i.e. a Type instance, while GenTypeForUser (like GenTypeBasic) returns just a string to express a type in the target language; I think the enum type per se does not have a Type instance at all, so this would be impossible. Doing some larger refactoring to somehow merge the logic of both functions could be possible, but I am not sure if it is needed and/or I am up to the task.\n. I have updated the patch; the current version does not change the Java version at all.\n. @gwvo Yep, that seems quite plausible. I\u2019ll take a look at it.\n. A fix (+test) for this added at PR #253.\n. ",
    "lp35": "Hi,\nDigging this issue out of the grave. \nI noticed there is no FindFlatBuffers for the library part. Actually there is only FindFlatBuffers for the scheme compiler. I would like to know if anyone already tried to create a real FindFlatBuffers.\nIf not, I can do a PR for this.\nBR.\nEDIT: the best would be to create a Cmake folder while executing INSTALL, and put all cmake useful find-tools here.\n. The problem I have with add_subdirectory command is if you are cleaning your project, you'll have to rebuild all projects that have been included with add_subdirectory. Using the binary form (libflatbufffers) is time saving when you have to build your project 10 times for 3 different plateforms for instance... I think it would be great to automatically generate a FindLibFlatbuffers when the FLATBUFFERS_BUILD_FLATLIB is enabled. Does it make sense to you?\n. Hi,\nIndeed, I up-vote this feature that might be very useful. Maybe adding a special flag to each structure/table in the FBS in order to not implement all == operators.\n. Hi,\nA quick message to upvote this feature.. Dear bog-dan-ro, dear gwvo,\nI tried the flatbuffers fork of bog-dan-ro, and the deep-copy on structure is just a must have in cpp. Actually I can't use generated structure (MonsterT) of 'table' in standard containers like map because of unique_ptr, and that is a real handicap for my program. Is there any chance to see this pull request to be merged?\nOtherwise, is there any workaround that would allow me to use generated structure with unique_ptr<> members in std::map for instance?\nBest regards,\n. Hi again,\nI would be glad to see this upstream. If this feature is too disturbing, a concession maybe possible, like adding an extra flag on the compiler (--enable-cpp-deepcopy for instance).\nBR.\nEDIT:\nI noticed few warnings though while using flatbuffers.h using Visual Studio 2015, line 1593 \n  inline operator bool() const { return val_; }\nwarning C4800: 'Video_Mode const ': forcing value to bool 'true' or 'false' (performance warning)\n. I now it's not very performance efficient, but having a dynamic_castflatbuffers::NativeTable could identify this kind of structure, isn't it?\n. Hi,\nAny update or ETA?\nAnyway thank you very much for considering this feature!\nBR\n. Actually they are multiple overloaded methods that are generated:\ninline flatbuffers::Offset<Monster> CreateMonster(flatbuffers::FlatBufferBuilder &_fbb, const MonsterT *_o, const flatbuffers::rehasher_function_t *rehasher)\nand \ninline flatbuffers::Offset<Monster> CreateMonsterDirect(flatbuffers::FlatBufferBuilder &_fbb,\n    const Vec3 *pos = 0,\n    int16_t mana = 150,\n    int16_t hp = 100,\n    const char *name = nullptr,\n    const std::vector<uint8_t> *inventory = nullptr,....)\nMy interest point on the first version, that can be changed into the following:\ntemplate<typename MyType>\ninline flatbuffers::Offset<MyType> Create(flatbuffers::FlatBufferBuilder &_fbb, const MyTypeT *_o, const flatbuffers::rehasher_function_t *rehasher)\nI know it is a very specific request, but it can avoid a lot of programming and copy pasting efforts in my use case.\n. Of course, but it is actually possible to use template specialization in another file.\nFor instance, it is possible to add this declaration in flatbuffers.h:\ntemplate<typename T> \ninline flatbuffers::Offset<T> Create(flatbuffers::FlatBufferBuilder &_fbb, const TT *_o, const flatbuffers::rehasher_function_t *rehasher)\n{\n  //Generate compile time error\n  #error Trying to create an existing flatbuffers.\n}\nAnd then, while generating the header for each file, it is possible to specialize the template, as following:\ntemplate<typename Monster> \ninline flatbuffers::Offset<Monster> Create(flatbuffers::FlatBufferBuilder &_fbb, const MonsterT *_o, const flatbuffers::rehasher_function_t *rehasher)\n{\n  return CreateMonster(_fbb, ...);\n}\n. Sure, I'll try to do it before the end of the week. Regards\n. I don't know why my PR is not passing the travis build system... My static_assert is triggered although working like a charm on my VS2015, should I try with a GCC compiler under Linux?\nEDIT: my bad, I realized that my build system was broken. I'll fix it and commit a new version.\n. Hi,\nLatest commit is working on all compilers. I'm waiting for you suggestion.\nBR,\n. Hi, few reason for this choice:\n- In my application, this new Create template will be inside a template method.\n- I'm using flatbuffers::GetRoot< T > to deserialize my data, so I would like to keep coherency and readability between serialization and deserialization.\n- Personal taste: I prefer to explicitly force the type with template than letting the compiler decide for me.\nBR\n. up?\n. Hi,\nI took some time to investigate the overloaded solution. My only question is: in which namespace this overloaded method takes place? \nFor the monster example, the namespace is MyGame::Sample. This prevent from using the Create method in generic template for serialization, because the namespace must prefix the create method (e.g: MyGame::Sample::Create. And it bring us back to the original issue solved by this pull request.\nExample:\n\nGenerated headers:\n\n```\nnamespace MyGame{\nnamespace Sample{\ninline flatbuffers::Offset Create(...) {}\n}}\nnamespace MyNamespace{\ninline flatbuffers::Offset Create(...) {}\n}\n```\n\nGeneric template:\nint serializeFlatBuffers<T>(const T& val)\n{\n   ...\n  flatbuffers::FlatBufferBuilder fbb;\n   auto off1 = Create(fbb, val); //Won't compile because of namespace issue\n}. Hi,\n\nI agree with you that my solution is surely improvable and that it is not a good design to contaminate the flatbuffers namespace. \nAnother solution in that comes to my mind (not tested) is to add the create method as a static method to the generated struct/table, so the flatbuffers will not be modified.\nThat would give something like this:\nint serializeFlatBuffers<Native, NativeT>(const NativeT& val)\n{\n   ...\n  flatbuffers::FlatBufferBuilder fbb;\n  auto off1 = Native::Create(fbb, &val);\n}\nAlso I am currently having to maintain a complex descriptor system that stores pointers to 2 differents template methods (one for flatbuffers table, another one for flatbuffers struct - MANUALLY_ALIGNED_STRUCT). Indeed as C++ does not embed an RTTI system, there is no way to identify if the passed object is a table or a struct (and these 2 differents types are not using the same serialization/deserialization methods).\nActually I don't like the non-symetric API design of Flatbuffers. Flatbuffers is using a namespace-restricted Create method, but it needs a the templated GetRoot in the flatbuffers namespace for deserialization...\nI think flatbuffers can be a powerful generic serialization library for highly modulable frameworks, but I'm currently scarifying performances by proceeding to tricky stuff to get it working...\nI hope I'm clear enough.  If not, just ping me for an example.\nBR,\n. Hi,\nIndeed, it should fit to my needs, thanks! I was building an old version of flatbuffers on which the Pack() method was not present. Seems the feature has been added on bc2ec7119bd95230c4eb9a730a5439472efe01fa\n. I tried with the Pack() method yesterday. Work like a charm, you can close this PR. Thanks!. I have seen few people creating an intermediate flatbuffers to workaround this.\nDo you think it is feasible? If ye, I can eventually work on a pull request if you provide me enough information.. The workaround is to create an intermediate flatbuffers ModuleList, as shown in the first post.\nI did a little code snippet using your suggestion. That's currently working, but I want to be sure this what you had in mind.\n```\n    std::vector vec1;\n    vec1.push_back(10);\n    vec1.push_back(11);\n    vec1.push_back(12);\nfbb.Finish(fbb.CreateVector<uint8_t>(vec1));\n\n// Simulate network transfer.\nuint8_t* buf = (uint8_t*) malloc(fbb.GetSize());\nmemcpy(buf, fbb.GetBufferPointer(), fbb.GetSize());\n\n// Deserialize\nconst flatbuffers::Vector<uint8_t>* tout = flatbuffers::GetRoot<flatbuffers::Vector<uint8_t>>(buf);\n\nstd::vector<uint8_t> deserialized;\nfor (flatbuffers::uoffset_t _i = 0; _i < tout->size(); _i++) \n{ \n    deserialized.push_back(tout->Get(_i));\n}\n\nfree(buf);\n\n```\nPS: I've seen a little optimization to do in your generated monster example.\n// method: inline MonsterT *Monster::UnPack(const flatbuffers::resolver_function_t *resolver)\n// ...\n{ auto _e = testarrayoftables(); if (_e) { for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->testarrayoftables.push_back(std::unique_ptr<MonsterT>(_e->Get(_i)->UnPack(resolver))); } } };\ntestarrayoftables.push_back() might realloc vector memory buffer each time it is called. Pre-allocating the vector buffer would increase the performances of the std::vector generation:\n```\n// method: inline MonsterT Monster::UnPack(const flatbuffers::resolver_function_t resolver)\n// ...\n{ auto _e = testarrayoftables(); _o->testarrayoftables.reserve(_e->size());  if (_e) { for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->testarrayoftables[_i] = (std::unique_ptr(_e->Get(_i)->UnPack(resolver))); } } };\n```\n. having a root_type [Table] would be awesome indeed! But to my understanding, this is only useful for JSON parsing, right?\nI am digging into flexbuffers today, maybe it will be easier to integrate this into the later?\nOk for the PR.. Yes, it makes sense. But maybe it can be added to the API guide of flatbuffers? I don't know if it is possible to have this kind of generic documentation, as each method has a different name when generated.. Then an if statement missing before https://github.com/google/flatbuffers/blob/ebcfbbadf0c18c6b59ab0b50473eacc8e3c38c5f/src/idl_parser.cpp#L1930, isn' it? \nIf EOF is the first token, the while loop is jumped over. No error is returned. Tested on flatbuffers january 1.5 release, by empting the content of monsterdata_test.golden.. Yes sure.. I signed it!. I apologize, seems I haven't typed the right keywords in the github search bar... Thank you very much for your time!. AddT can work, yes. Anyway you have to specify the template argument if you want to use the same name as the overloaded version + template implementation. So you can keep the actual implementation.\nI.E:\n```\nuint16_t test = 2;\nfb.Add(test);  // call the template\nfb.Add(test); // Call the overloaded method.\n```\nIf you try to call Add with an unimplemented type, you will get a compilation error (the best thing to do to my mind).\nAddT is the best to not misleading devs IMO.. Actually Add() is not the problem. AsXXX() is the problem, because you can't create a template that uses different names for functions calling.\nI.e:\n```\ntemplate\nvoid myDeserializationFunction(std::vector iSerialized, void out)\n{\n  auto root = flexbuffers::GetRoot(iSerialized);\n  static_cast(oVal) = root.As(); \n  // Without templated As(), we have to create specialize each template (for AsUint32(), AsInt(), etc...)\n}\nAnd for the sake of API symmetry, adding AddT would be more for clearity. But it's not mandatory.\n```. Sorry for the many commits - I haven't any Linux distros right now.. Thanks for merging. Should I create a templated version for Create() method as well?. I'm not sure to well understand what do you mean by \"garbage data being transfered\". Can you provide me a concrete example?\nIMO, if you use a copy constructor, you can't get garbage data if the object you are copying is valid.\nAfter reading the code in base.h, and more specifically \n```\ntemplate T EndianScalar(T t) {\n  #if FLATBUFFERS_LITTLEENDIAN\n    return t;\n  #else\n    return EndianSwap(t);\n  #endif\n}\ntemplate T ReadScalar(const void p) {\n  return EndianScalar(reinterpret_cast(p));\n}\ntemplate void WriteScalar(void p, T t) {\n  reinterpret_cast(p) = EndianScalar(t);\n}\n```\nFlatbuffers is already doing EndianSwap() on Big endian platform, thus it is already slow on big endian, isn't it?\nI have few arguments for removing this default copy constructor:\n1. It doesn't change anything at the end as the memory layout of flatbuffers structs in big endian and little endian is exactly the same.\n\nWhen used in std::vector, having this kind of copy constructor may prevent the compiler to optimize vector copying using std::copy. (https://stackoverflow.com/a/19885838 and http://andreoffringa.org/?q=uvector).\n\nWith copy constructor (std::is_trivially_copyable::value == false):\nfor(int k = 0; k < in.size(); k++)\n{\n  out[k] = T(in[k]); // equivalent to memcpy(&out[k], &in[k], sizeof(T)\n}\nWithout a copy constructor  (std::is_trivially_copyable::value == true):, compiler will detect that the structure is trivially copiable, and then optimize by using a memcpy:\nmemcpy(&out[0], &in[0], in.size()*sizeof(T));\n\n(as said before) Make flatbuffers compliant with the notion of trivially copiable of the C++ standard.\n\nEdit 1:\nBut anyway I don't have the complete architectural vision of flatbuffers you might have, so removing this default constructor might break something in flatbuffers I'm not aware of...\nEdit 2:\nI can PR if you want.. Hi again,\nTo my last update, flatbuffers is only compatible with C++11 compliant compilers. Althought, it seems that it has changed since the last time I saw this information, because it seems to compile under GCC 4.9 according to the continuous integration script.\nI would like to know if you have any #define in flatbuffers indicating if the compiler does respect the full c++11 standard. I added a test to check the trivially copiable capabilities, but it seems that GCC 4.9 and Visual Studio 2010 doesn't support SFINAE.\nAny clues for solving this issue?\nBR. Finally found that GCC < 5 does not support full C++11 standard... Ready to be merged if everything is ok on your side!. I did multiple attempt to avoid that but it was not clean or was not working...\n. I agree for inline, but in this case other methods like GenTablePost(), GenTable(), etc. must be inlined as well, isn't it?\n. AFAIK, template specialization can't be added inside class scope :/\nhttp://rextester.com/PDUXKQ82485. Sure I will. Thank you for taking time to comment.. ",
    "layzerar": "How is it going?\n. @rw I found a little bug in the setup script, the following patch should fix.\n``` diff\n python/setup.py | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\ndiff --git a/python/setup.py b/python/setup.py\nindex f067b38..44f5ee7 100644\n--- a/python/setup.py\n+++ b/python/setup.py\n@@ -8,7 +8,7 @@ setup(\n     author_email='me@rwinslow.com',\n     url='https://github.com/python/flatbuffers/python',\n     long_description='Python runtime library and code generator for use with the Flatbuffers serialization format.',\n-    packages=['flatbuffers'],\n+    packages=['flatbuffers', 'flatbuffers.vendor'],\n     include_package_data=True,\n     requires=[],\n     description='Runtime library and code generator for use with the Flatbuffers serialization format.',\n``\n. After reinstall this package, everything is OK for me.\n. Life is short, how about this?\n. -flatc` failed to compile in VS2013, error log is here:\n1>------ Rebuild All started: Project: flatc, Configuration: Release Win32 ------\n2>------ Rebuild All started: Project: flatsamplebinary, Configuration: Release Win32 ------\n3>------ Rebuild All started: Project: flatsampletext, Configuration: Release Win32 ------\n4>------ Rebuild All started: Project: flattests, Configuration: Release Win32 ------\n1>  idl_gen_fbs.cpp\n2>  sample_binary.cpp\n4>  idl_gen_fbs.cpp\n3>  idl_parser.cpp\n1>  idl_gen_general.cpp\n4>  idl_gen_general.cpp\n3>  idl_gen_text.cpp\n1>  idl_gen_go.cpp\n4>  idl_parser.cpp\n3>  sample_text.cpp\n2>  flatsamplebinary.vcxproj -> C:\\Users\\layz\\Desktop\\flatbuffers\\flatbuffers\\build\\VS2010\\Release\\flatsamplebinary.exe\n1>  idl_gen_python.cpp\n3>  Generating Code...\n4>  idl_gen_text.cpp\n1>..\\..\\src\\idl_gen_python.cpp(577): error C2065: 'S_IRWXU' : undeclared identifier\n1>..\\..\\src\\idl_gen_python.cpp(577): error C2065: 'S_IRGRP' : undeclared identifier\n1>..\\..\\src\\idl_gen_python.cpp(577): error C2065: 'S_IXGRP' : undeclared identifier\n1>..\\..\\src\\idl_gen_python.cpp(577): error C2065: 'S_IROTH' : undeclared identifier\n1>..\\..\\src\\idl_gen_python.cpp(577): error C2065: 'S_IXOTH' : undeclared identifier\n1>  idl_parser.cpp\n4>  test.cpp\n1>  idl_gen_cpp.cpp\n4>  Generating Code...\n1>..\\..\\src\\idl_gen_cpp.cpp(276): error C2220: warning treated as error - no 'object' file generated\n1>..\\..\\src\\idl_gen_cpp.cpp(276): warning C4189: 'nested_root' : local variable is initialized but not referenced\n1>  idl_gen_text.cpp\n3>  flatsampletext.vcxproj -> C:\\Users\\layz\\Desktop\\flatbuffers\\flatbuffers\\build\\VS2010\\Release\\flatsampletext.exe\n1>  flatc.cpp\n1>  Generating Code...\n4>  flattests.vcxproj -> C:\\Users\\layz\\Desktop\\flatbuffers\\flatbuffers\\build\\VS2010\\Release\\flattests.exe\n========== Rebuild All: 3 succeeded, 1 failed, 0 skipped ==========\n- py_test.py run with following error (Python2.6) :\nTraceback (most recent call last):\n  File \"C:\\Users\\layz\\Desktop\\flatbuffers\\flatbuffers\\tests\\py_test.py\", line 1331, in <module>\n    main()\n  File \"C:\\Users\\layz\\Desktop\\flatbuffers\\flatbuffers\\tests\\py_test.py\", line 1294, in main\n    ('<benchmark read count> <benchmark build count>')\nTypeError: 'str' object is not callable\n- this patch would fix:\n``` diff\n tests/py_test.py | 14 +++++++-------\n 1 file changed, 7 insertions(+), 7 deletions(-)\ndiff --git a/tests/py_test.py b/tests/py_test.py\nindex c6861f1..2934394 100644\n--- a/tests/py_test.py\n+++ b/tests/py_test.py\n@@ -1290,13 +1290,13 @@ def main():\n     import os\n     import sys\n     if not len(sys.argv) == 4:\n-       sys.stderr.write(('Usage: %s ')\n-                        (' ')\n-                        ('\\n' % sys.argv[0]))\n-       sys.stderr.write(('       Provide COMPARE_GENERATED_TO_GO=1   to check')\n-                        ('for bytewise comparison to Go data.\\n'))\n-       sys.stderr.write(('       Provide COMPARE_GENERATED_TO_JAVA=1 to check')\n-                        ('for bytewise comparison to Java data.\\n'))\n+       sys.stderr.write('Usage: %s '\n+                        ' '\n+                        '\\n' % sys.argv[0])\n+       sys.stderr.write('       Provide COMPARE_GENERATED_TO_GO=1   to check'\n+                        'for bytewise comparison to Go data.\\n')\n+       sys.stderr.write('       Provide COMPARE_GENERATED_TO_JAVA=1 to check'\n+                        'for bytewise comparison to Java data.\\n')\n        sys.stderr.flush()\n        sys.exit(1)\n``\n. Briliant!\n. @rw What do you think?\n. I test it in:\n- PyPy 2.5.1 (Python 2.7.9)\n- Python 2.7.6\n- Python 3.4.3\n. It seems likepypydoesn't faster thanpython, will it be better to useBytesIOinstead ofbytearray`.\n```\n$ pypy py_test.py 1000 1000 1000   \n\nRan 57 tests in 0.184s\nOK\nvtable deduplication rate: 7607.04/sec\ntraversed 1000 512-byte flatbuffers in 0.38sec: 2626.74/sec, 1.28MB/sec\nbuilt 1000 512-byte flatbuffers in 0.54sec: 1838.09/sec, 0.90MB/sec\n$ python py_test.py 1000 1000 1000\n\nRan 57 tests in 0.057s\nOK\nvtable deduplication rate: 13472.56/sec\ntraversed 1000 512-byte flatbuffers in 0.20sec: 4881.96/sec, 2.38MB/sec\nbuilt 1000 512-byte flatbuffers in 0.47sec: 2132.52/sec, 1.04MB/sec\n$ python3 py_test.py 1000 1000 1000\n\nRan 57 tests in 0.052s\nOK\nvtable deduplication rate: 14469.87/sec\ntraversed 1000 512-byte flatbuffers in 0.19sec: 5219.65/sec, 2.55MB/sec\nbuilt 1000 512-byte flatbuffers in 0.43sec: 2325.05/sec, 1.14MB/sec\n``\n. @rw Yes. The the newflatcwould generate new code, the old code is still compatible.\n. @rw I already adapted the code in py_test.py\n. @gwvo @rw \nI see. I would get back the old API soon.\n. @rw I got confused about when I should or should not callbuilder.Finish()`, can you help?\nThe following code is a part of py_test.py. Why does b.Finish(mon2) not be called here?\n``` python\n    b = flatbuffers.Builder(0)\n    string = b.CreateString(\"MyMonster\")\n    test1 = b.CreateString(\"test1\")\n    test2 = b.CreateString(\"test2\")\n    fred = b.CreateString(\"Fred\")\nMonster.MonsterStartInventoryVector(b, 5)\nb.PrependByte(4)\nb.PrependByte(3)\nb.PrependByte(2)\nb.PrependByte(1)\nb.PrependByte(0)\ninv = b.EndVector(5)\n\nMonster.MonsterStart(b)\nMonster.MonsterAddName(b, fred)\nmon2 = Monster.MonsterEnd(b)\n# b.Finish(mon2) not be called, why? except union?\n\nMonster.MonsterStartTest4Vector(b, 2)\nTest.CreateTest(b, 10, 20)\nTest.CreateTest(b, 30, 40)\ntest4 = b.EndVector(2)\n\nMonster.MonsterStartTestarrayofstringVector(b, 2)\nb.PrependUOffsetTRelative(test2)\nb.PrependUOffsetTRelative(test1)\ntestArrayOfString = b.EndVector(2)\n\nMonster.MonsterStart(b)\n\npos = Vec3.CreateVec3(b, 1.0, 2.0, 3.0, 3.0, 2, 5, 6)\nMonster.MonsterAddPos(b, pos)\n\nMonster.MonsterAddHp(b, 80)\nMonster.MonsterAddName(b, string)\nMonster.MonsterAddInventory(b, inv)\nMonster.MonsterAddTestType(b, 1)\nMonster.MonsterAddTest(b, mon2)\nMonster.MonsterAddTest4(b, test4)\nMonster.MonsterAddTestarrayofstring(b, testArrayOfString)\nmon = Monster.MonsterEnd(b)\n\nb.Finish(mon)\n\n```\n. @rw Sorry for the late reply. I was busy and suffered from kidney stones recently. So a lot of things were postponed.\nI think I should give up this pull request temporarily. Because this idea seems not mature right now.\nAnd I plan to add a cython extension for the python port. The builder part which got about 25 times performance improvement has been done. I will submit a pull request when it's ready.\n. @rw It's easy. If people don't have cython or compile failed, just fall back to the pure python version.\n. @rw Cython would compile .pyx to a python library (.dll, .so etc) at install time, so we just need to  detect cython at install time.\n. @rw Yes, It would work after reinstall flatbuffers.\n. Forced update!\n- Fix compilation issue in VS2013\n- Fix indentation\n- Fix a bug of range check in builder.py\n. Forced update!\n- Get back the builder functions\n- Add test to compare new API and old API\n- Add some documentation comments\n. @rw Fix two small bugs. Can you help review this?\n. @rw pushed!\n- The first bug: mentioned 2 gigabytes, but 2**UOffsetTFlags.bytewidth - 1 == 15\n```\n\n\n\nfrom flatbuffers import builder\nx = builder.Builder(16)\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"flatbuffers/builder.py\", line 98, in init\n    raise BuilderSizeError(msg)\nflatbuffers.builder.BuilderSizeError: flatbuffers: Cannot create Builder larger than 2 gigabytes.\n```\n- The second bug:\n\n\n\n/// this documentation comment ahead of table will break the generated python file\ntable Monster {\n}\n. @rw Force update. Just two small above-mentioned bug fixs. I\u2018m sure this is OK. Can you help to merge this.\n. Force update, one of the bugs seems have been fixed.\n. @gwvo \nThe third one generate the builder function, the prototype of functions are:\nvoid FlatBufferBuilder::AddFloat(int o, float x, double d)\nvoid FlatBufferBuilder::AddDouble(int o, double x, double d)\nSo it doesn't generate a problem. Is this a mistake?\n. @Downchuck \nI am very sorry for the late reply.\nI upload my earlier work here.\nSimple benchmark:\n``` shell\n$ python py_test.py 10000 10000 10000\n\nRan 57 tests in 0.011s\nOK\nvtable deduplication rate: 447827.12/sec\ntraversed 10000 288-byte flatbuffers in 0.40sec: 24759.78/sec, 6.80MB/sec\nbuilt 10000 288-byte flatbuffers in 0.15sec: 65484.94/sec, 17.99MB/sec\n```\n. @Downchuck \nEm, you can try first, but debugging a cython module is not easy.\n. @abuchanan \nI have written a version which passes all the tests. You can find it here https://github.com/layzerar/flatbuffers/tree/cython_extension.\n. @abuchanan \nThat's good. I will try to got it available for everyone recently.\n@Downchuck \nHow are things going?\n. https://github.com/google/flatbuffers/issues/284\n. @rw @gwvo \n. Force pushed! Add MANIFEST.in file, so that the *.h and *.pyx will be packed into the source distribution. \n. @rw import flatbuffers, if flatbuffers.fastcodec  is not None means that you're using cython version currently.\n. @rw  Yes, It passes all the tests.\nIn the python version of Builder, self.Bytes means the whole buffer, self.Head() means the offset of the payload, so self.Output() is equal to self.Bytes[self.Head():].\nThe cython version is based on the C++ implement, access the whole buffer is not a good idea. So self.Bytes is always equal to self.Output(), and self.Head() always return 0.\n. You can try to remove fastcodec.so file in the package or just use from flatbuffers.builder import Builder instead of from flatbuffers import Builder, then it will fallback pure-python version.\nSwitching between cython and pure-python is very easy to implement. Can I make a pull request after this pull request? This pull request contains too much.\n. Update.\n1. Rebase master.\n2. We can switch between two implementations by flatbuffers.force_implementation('python') and flatbuffers.force_implementation('cython').\n3. When Cython extension building to fail, fallback to plain Python version.\n. Sorry, doesn't have much time to complete this.\n. @rw As the building order is too complicated for the end users, so I remove the the old API, the new API would translate it to equivalent code automatically. As the result, the old API is no longer available for the end users. If we need more tests, we can just add more tests for the raw builder API.\n. ",
    "thedrow": "@layzerar Check #112 and #110.\n. Should this be closed in favour of #112?\n. @shaxbee I'm not a maintainer.\nI'm just trying to keep this alive since I'm interested in this feature. Only OP and maintainers can close the issue.\n. This PR needs to be rebased.\n. Woot!\n. ",
    "matrixik": "Hmmm, that's a problem.\nMaybe some code generation tool that will make sure everything is written/read in the correct order?\nSome examples:\nhttps://github.com/youtube/vitess/tree/master/go/cmd/bsongen\nfor http://godoc.org/github.com/youtube/vitess/go/bson\nCode example: https://github.com/alecthomas/go_serialization_benchmarks/blob/master/vitess_test.go\nhttps://github.com/philhofer/msgp\nCode example: https://github.com/philhofer/go_serialization_benchmarks/blob/master/msgp_gen.go\n. ",
    "xysu": "Just a bit curious about the status of this ticket. Is anyone currently working on it?\nSince FlatBuffers are not self-describing, besides the provided interface{} objects, the Marshal/Unmarshal interface would expect something that describes the layout of the schema, e.g., a schema file or certain schema object. When we were designing our own zero-copy serialization protocol, we chose the schema object approach in our API because such schema object is already available in our design (generated from the schema files). Would be nice to hear your opinions. Thanks!\n. The \"schema that describes schemas\" is called the meta-schema in our protocol. Yes, we allow users to send encoded meta-schema over the wire and has a schema parser to decode it on the other end of the wire.\nAnother issue with the current Go implementation of FlatBuffers is that there is no programmatic way/interface to access fields. This is less efficient than through the generated \"accessor\" code but sometimes can be handy, for example, when implementing Marshal/Unmarshal with dynamically loaded schemas. Generating separate Marshal/Unmarshal code for different schemas seems to be a bit tedious. I guess the real question is, if the support for Go Marshal/Unmarshal is needed in many use cases...\nA different topic -- I'm wondering what could be a good starting point to contribute to the Go implementation of FlatBuffers :).\n. For example, GetInt8(p Path) where p is some human readable format such as \"b[3].x\" or {\"b\", \"3\", \"x\"}. One can work with the loaded schema to verify the path and then extract the value from the right offset of the binary (still need to use vtables). \nThis doesn't require us to make a copy of and store the encoded binary in a struct. I agree it is less efficient than the generated code, but at least provide a convenient way to achieve certain things at run-time. For instance, with this in hand, it would be straightforward to inspect an object using reflection at run-time and marshal it into binary. Similarly for the unmarshal side.\n. ",
    "newtack": "There is another option to solve the original problem of avoiding too much garbage collection: The ability to reuse a builder for other serializations. To do this, there would need to be a way re-initialize the internals, without re-creating objects, e.g. clearing the vtables without removing the allocated capacity. \n. ",
    "cbandy": "It looks like #165 has code for this.\n. ",
    "erikdubbelboer": "Since #165 is merged can't this issue be closed?\n. ",
    "xgalaxy": "It's more a question of JIT than it is of just using straight vanilla C#. Because straight vanilla C# can still cause a JIT cycle on iOS device and crash.\n. Good to know. Reflection itself isn't a problem either. Its only the emitting parts of reflection that can't be used - aside from the slowness factor. Thanks for the info.\n. Good to know. Reflection itself isn't a problem either. Its only the emitting parts of reflection that can't be used - aside from the slowness factor. Thanks for the info.\n. ",
    "jesta88": "It works great with Unity, so this issue should be closed.\n. ",
    "resistor": "For my particular application, the error goes away if I turn off address sanitizer's strict memcmp check, which indicates that for my particular use case, the issue would not manifest dynamically because a comparison failure would have caused the memcmp() to terminate early.  However, I don't see anything in the code that would prevent us from hitting the bad case in a situation where the memcmp() didn't terminate early.\n. I believe you can workaround it like so:\nauto data_loc = reinterpret_cast<voffset_t *>(buf_.data_at(*it));\nif (!vt1_size ||\n    *data_loc != *vt1 ||\n    memcmp(data_loc+1, vt1+1, vt1_size - sizeof(voffset_t)) continue;\n. ",
    "joker-eph": "I think the point is that if the current vtable does not have the same size, there is no need for comparing the content. And then ASAN wrapper for memcmp() will never be called with a vt1_size larger than the allocation data_loc belongs to.\n. This fixes #88 (I didn't notice the issue before launching this PR).\n. ",
    "daiday": "@gwvo I've found the way to create the table array, and it's exactly like what you said! Thank you very much! Flatbuffer is easy to create, but I think there's little inconvenient to fill the data, such as you can't nest the table creation. Maybe it's a good way to do that!Anyway, thanks again!\n. ",
    "onewayheroic": "@daiday so i can see you had solve the problem and I meet the same problem, can i have a example of your code? I just do not understand what @gwvo said...\nthank you very much\n. std::vector<???what type??> mlocVector\nmlocVector.push_back(mloc2)\nlike this? Then, what type was mlocVector?\n. ",
    "dicta": "See: http://www.cmake.org/cmake/help/v3.0/command/get_filename_component.html\nDIRECTORY syntax was added in CMake 2.8.12. PATH existed previously and can be used as a replacement alias for compatibility with 2.8.11 and earlier.  \nNote that major Linux distros (e.g. Debian stable, RHEL6) in widespread production use still have cmake versions < 2.8.12, so it's probably better to use PATH to maintain compatibility vs. requiring a very new version of cmake.\n. ",
    "maglar0": "Sorry if I'm nagging you, but I'm still not convinced about that argument about memcmp(). The size of the table is the first short written in EndTable(), but since you're actually writing \"backwards\"/\"down\", it will not be the first short that memcmp() comes across. For example, if the table being written now is \"x, y, z, size1\" and an earlier table is \"x, size2\", memcmp() will read out of bounds of the earlier table if y=size2. Is it not so?\nEDIT: Never mind, I see now that you're reserving space for the field data, then writing the size (at the lowest memory address of the table), and finally filling in the field data in the reserved space. So memcmp() should start comparing the sizes and thus stop before going out of bounds.\n. For my particular use case, I have a \"router\" class where other code register identifiers together with a \"handler\" to call for each incoming packet with a particular identifier. The router has a hash table with identifiers to do the handler lookup. If I would use BufferHasIdentifier it would require a linear search, and the router would have to be aware of all packet types or do each test with an indirect call to different BufferHasIdentifier.\nI figured having access to the identifier would be useful in many situations where one would want to handle different flatbuffers in a more generic way, but maybe I'm wrong.\n. I'll try to figure out how to do that.\nBut why don't you like the test case I provided? The documentation clearly states that the identifier will be stored at offset 4-7 in the buffer, surely there should be a test to verify that it is working as described?\n\nThese 4 characters will end up as bytes at offsets 4-7 (inclusive) in the buffer.\n. That test doesn't check that the identifier is at offset 4 in the buffer, only that the flatbuffer code stores it somewhere in the buffer. \n\nBut ok, I think I have managed to change it into a single commit doing what you wanted. For some reason the old diff disappeared in this \"thread\", but I guess that's a \"feature\" of github.\n. ",
    "luna-duclos": "The attached PR fixes this issue\n. ",
    "bazhenovc": "Yes, I understand. But I've already reviewed 2 junior programmers and told them not to inherit from tables... Adding a final to the generated class will be the easiest way to stop people from doing that.\n. Private constructors do not forbid anyone to actually inherit from the Table, it forbids only allocating a Table. My problem was that one guy inherited from Monster, allocated raw memory using the allocator and returned the pointer, while the other guy tried to delete the pointer.\nThe code looked like this:\n```\nstruct Ogre : private Monster // First error here\n{\n  virtual ~Ogre() {}\nstatic Ogre New() { return (Ogre)Memory::Allocate(sizeof(Ogre)); } // Second error here(poor C++ knowledge)\n};\n```\nThe first guy's mistake was, obviously, inheriting from the generated table class, which lead to completely unexpected error in a different place, where the other guy tried to delete that pointer.\nHere is the code:\nOgre* ogre = Ogre::New();\n...\ndelete ogre; // Third error\nAs you can see, the programmer initially made a mistake inheriting from the Table, which in turn led to 2 more mistakes in different places. If Monster class was final - the first mistake would be impossible to make, thus avoiding 2 additional mistakes and saving some time to review it.\nAfter I've added final keyword manually, I've seen several additional places with the same wrong approach, so this change is actually useful (at least for us) :)\nThe problem is that Monsters and Tables belong to the FlatBuffers library, while Ogre is a part of the engine - which implies different usage and different perception of these two entities. Adding a final to the generated classes will make this workflow less error-prone.\n. Thanks!\n\nThat said, there's a 3rd, more serious error made here: the assumption that you can cast raw memory to an object that needs a virtual table, without using placement new\n\nUnfortunately this happens a lot here and there, and we can do nothing from our side to fix that (we can't just lay off everyone and hire 300 C++ gurus).\n. ",
    "rockerhieu": "Monster is private in the generated header hence mymonster.CheckField(Monster::VT_HEALTH) doesn't work. Please reopen this.. Nevermind, it was because f3SecondTable was missing the field f2Int.. Now that kotlin-native and kotlin-multiplatform are becoming more popular, please support this.. @Lakedaemon this is a great start.\nI just want to add that your work is a kotlin/jvm implementation of flatbuffers. There are still kotlin/native, kotlin/js, kotlin/objc on the list.. CheckField is not public too:\nerror: 'bool flatbuffers::Table::CheckField(flatbuffers::voffset_t) const' is inaccessible. Turned out I can use flatbuffer::IsFieldPresent(mymonster, Monster::VT_HEALTH).. ",
    "variadic-template": "Hi Gwvo,\nDo you think someone in your team will address this sooner rather than later?\nJust curious. \nThank you\n. ",
    "brcooley": "I've started work on this, a fix should be landing soon.\n. ",
    "vyadzhak": "Thank you for info)\n. ",
    "gkogan": "Looks good to me.\n. ",
    "jordan52": "this looks to be in really good shape.\n. ",
    "Downchuck": "Is ./flatc still needed to do code gen? I'm hitting issues with its python output.\nEDIT: My bad -- looks like \"namespace\" is required when generating python output.\n. @rw What's the status on this commit?\n. Regarding a D compiler: it may be more \"D-appropriate\" to simply generate code during compile time, as is done with the https://github.com/msoucy/dproto project.\nI won't be able to pick this up in the near-future, either, as we're stuck in some other languages for the time being.\n. @layzerar Where did you wind up on a Cython port? As a user I really don't mind searching/installing  \"cflatbuffers\" from pip, or whatever other name you'd like to use.\n. Here's an example of Cythonize in a try statement:\nhttps://github.com/stephan-hof/pyrocksdb/commit/8481bc2481e3341a0399f360b26df97af7617229\n. In this particular case I am processing messages in protobuf and\ntranslating them to flatbuf. That said, this technique is the same for\ntransforming across most serialization formats.\nI am working with large batches of data, and manually typing out each add\ncommand with the key name is really time consuming, verbose and practically\nunnecessary given that code gen can handle it, and I could just use a loop.\n. This technique sped up writes in the capnproto target significantly; still working on building it for flatbuffers. \n. Bit verbose on the cpp side:\n```\ndiff --git a/src/idl_gen_python.cpp b/src/idl_gen_python.cpp\nindex 96f9c42..ee747e1 100644\n--- a/src/idl_gen_python.cpp\n+++ b/src/idl_gen_python.cpp\n@@ -370,24 +370,41 @@ static void GetStartOfTable(const StructDef &struct_def,\n static void BuildFieldOfTable(const StructDef &struct_def,\n                               const FieldDef &field,\n                               const size_t offset,\n-                              std::string code_ptr) {\n+                              std::string code_ptr,\n+                             bool asindex = false) {\n   std::string &code = *code_ptr;\n-  code += \"def \" + struct_def.name + \"Add\" + MakeCamel(field.name);\n-  code += \"(builder, \";\n-  code += MakeCamel(field.name, false);\n-  code += \"): \";\n+  if (asindex) {\n+   code += \" elif idx == \" + NumToString(offset) + \": \";\n+  }\n+  else {\n+   code += \"def \" + struct_def.name + \"Add\" + MakeCamel(field.name);\n+   code += \"(builder, \";\n+   code += MakeCamel(field.name, false);\n+   code += \"): \";\n+  }\n   code += \"builder.Prepend\";\n   code += GenMethod(field) + \"Slot(\";\n   code += NumToString(offset) + \", \";\n   if (!IsScalar(field.value.type.base_type) && (!struct_def.fixed)) {\n     code += \"flatbuffers.number_types.UOffsetTFlags.py_type\";\n     code += \"(\";\n-    code += MakeCamel(field.name, false) + \")\";\n+    if(asindex) {\n+     code += \"datum)\";\n+    }\n+    else {\n+     code += MakeCamel(field.name, false) + \")\";\n+    }\n   } else {\n-    code += MakeCamel(field.name, false);\n+    if(asindex) {\n+     code += \"datum\";\n+    }\n+    else {\n+     code += MakeCamel(field.name, false);\n+    }\n   }\n   code += \", \" + field.value.constant;\n   code += \")\\n\";\n+\n }\n// Set the value of one of the members of a table's vector.\n@@ -484,6 +501,21 @@ static void GenTableBuilders(const StructDef &struct_def,\n     }\n   }\n\nstd::string &code = *code_ptr;\ncode += \"def \" + struct_def.name + \"SetIndex(builder, idx, datum):\\n\";\ncode += \" if idx == -1: return\\n\";\n+\nfor (auto it = struct_def.fields.vec.begin();\nit != struct_def.fields.vec.end();\n++it) {\nauto &field = **it;\nif (field.deprecated) continue;\n+\nauto offset = it - struct_def.fields.vec.begin();\nBuildFieldOfTable(struct_def, field, offset, code_ptr, true);\n}\ncode += \"\\n\";\n+\n   GetEndOffsetOnTable(struct_def, code_ptr);\n }\n```\n. I think that Java will handle boxing just fine\n\nvoid tblSetIndex(FlatBufferBuilder, int idx, Object datum)\nI don't think this optimization is worth it:\nvoid tblSetIndex(FlatBufferBuilder, int idx, int datum)\nvoid tblSetIndex(FlatBufferBuilder, int idx, long datum)\n... etc ...\n. Having implemented this work, I can give the following feedback:\nThe classname prefixes for static methods are a problem; they break any means of general use of the library. I need to duplicate my code blocks because the method names are dynamic.\nThe lack of cython interface is a deal-breaker.  This runs a magnitude slower than it would under cython compatibility. Docs on the API seem a little out of date.\nIt takes 10x as long right now to run an encode into flatbuffers compared to capnproto. flatbuffers is appropriate for the use case I have (isolated data, safe, many null fields) -- file sizes are significantly smaller. My concern is that the flatbuffers/python project is meant as a demonstration implementation, more proof of concept than a module targeted toward python use.\nAnd that's completely normal, and reasonable. Based on what I've seen in protobuf, it seems that it would be beneficial to have a separate cython flatbuffers project and repo.\n. I got much better usability from the code gen once I got rid of the camel case prefix:\nfrom tblNameSetIndex to AddDatum, and tblNameStart to BeginObject and tblNameEnd to EndObject\nOn the speed issues, I think it's just easier to expose the C++ library via Cython. I did compile the pure python library, but it is still quite slow.\n. What do you mean by slower?\nIt's slower to use Python setattr vs an index.\nThere are no methods to dynamically work with the code gen from Python;\nthus adding AddDatum, BeginObject and EndObject to the class code gen makes\nit fast and easy.\nOn Monday, September 28, 2015, Wouter van Oortmerssen \nnotifications@github.com wrote:\n\nIt be interesting to know why it is so much slower, and what we can do\nabout it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/281#issuecomment-143797084.\n. This is called Clear() in the C++ library, looks like it tries to do a few more things.\n. Cython looks approximately like:\n\nfrom setuptools import setup\n+from distutils.extension import Extension\n+\n+sources = ['flatbuffers/*.py']\n+try:\n+    from Cython.Build import cythonize\n+    from Cython.Distutils import build_ext\n+    sources = [Extension(\"flatbuffers.builder\", [\"flatbuffers/builder.c\"]), Extension(\"flatbuffers.table\", [\"flatbuffers/table.c\"]), Extension(\"flatbuffers.number_types\", [\"flatbuffers/number_types.c\"])]\n+    # sources = \"flatbuffers/*.py\" (ifdef USE_CYTHON)\n+except ImportError:\n+    def cythonize(extensions): return sources\n+\n...\n+    cmdclass = {'build_ext': build_ext},\n+    ext_modules = cythonize(sources),\n. On creating it all on pyx:\nbuffer.py is the main class. compat, packer and encode can be written in as cdef inline.\nAdding appropriate type annotations to buffer.pyx may make for a very performant encoder.\nMost of that work needs to be done anyway to support the Python API on top of the C++ layer.\nIf passing over to the C++ library, here's a start:\ncdef extern from \"flatbuffers.h\" namespace flatbuffers:\n        cdef cppclass FlatBufferBuilder:\n                FlatBufferBuilder(int initialLength) except +\n                void Clear()\n                void StartTable()\n                void EndTable(int, int)\n                void StartVector()\n                int EndVector(int, int)\n                int* Data()\n                int CreateString(char*)\nWe only need to expose the methods used by the python code gen, and the core Buffer methods.\nThis work will cut down processing significantly, making python flatbuffers useful for batch processing. \n. @rw I'm not sure which is more simple. Passing to the C++ library will require less code, as we only need to expose the public interfaces.\nI've spent a little time porting buffers.py WriteVtable to Cython. I think it's the heavy lifter. It's tedious: number types needs to be ported first. Then it's fairly easy to look at the coverage analyzer:\ncython --cplus  --annotate-coverage coverage.xml builder.pyx\nMy guess is that with just a few parts of builder converted, there will be big performance gains on encoding. \n. Going to try pxd instead.. I'll see if that one works out better.\nhttps://github.com/cython/cython/wiki/pure\n. Having tried three methods now: it occurs to me that simply generating a pyx file which interfaces directly with the cpp library is the most appropriate method. This is similar to what the CapnProto cython project does. Doing so provides the best performance for the objects, as they are fully compiled and targeting the core C++ library, and it provides the best flexibility, as a developer can simply code gen the python and cython output, supporting pure python and cython distribution.\nSo, I'm giving that a shot. It still requires supporting the basic Builder and Table interfaces in Cython cdef.\n. This is the base that would be in the flatbuffers Python module:\n```\nmakecython++ test.pyx \"\" \"-std=gnu++11\"\nfrom libc.stdint cimport uint32_t, uint16_t, uint8_t, int32_t, int64_t\nctypedef uint32_t uoffset_t\nctypedef uint16_t voffset_t\ncdef extern from \"flatbuffers/flatbuffers.h\" namespace \"flatbuffers\":\n        @staticmethod\n        cdef T GetRootT\n        cdef cppclass FlatBufferBuilder:\n                FlatBufferBuilder() except +\n                uoffset_t GetSize()\n                void Clear()\n                uint8_t GetBufferPointer()\n                void FinishT\n                Offset CreateString(char str, size_t len)\n                void StartVector(size_t len, size_t elemsize)\n                uoffset_t EndVector(size_t len)\n                uoffset_t StartTable()\n                uoffset_t EndTable(uoffset_t start, voffset_t numfields)\n                # look into this, if needed.\n                void ReleaseBufferPointer()\n        cdef cppclass Offset[T]:\n                Offset(T) except +\n                uoffset_t o;\ncdef class flatbuffer:\n        cdef FlatBufferBuilder fbb\n        def cinit(self):\n                self.fbb = new FlatBufferBuilder()\n        def size(self):\n                return self.fbb.GetSize()\n        def data(self):\n                cdef unsigned char buf = &(self.fbb.GetBufferPointer()[0])\n                cdef Py_ssize_t length = self.fbb.GetSize()\n                return buf[:length]\n        def finish(self, uoffset_t offset):\n                self.fbb.Finish(Offsetuoffset_t)\n        def CreateString(self, blob):\n                return self.fbb.CreateString(blob, len(blob)).o\n        def StartVector(self, size_t vlen, size_t elemsize):\n                self.fbb.StartVector(vlen, elemsize)\n        def EndVector(self, size_t vlen):\n                return self.fbb.EndVector(vlen)\n        def StartTable(self):\n                return self.fbb.StartTable()\n        def EndTable(self, uoffset_t start, voffset_t numfields):\n                return self.fbb.EndTable(start, numfields)\n```\nThen there would be a flatc compilation flag to generate Cython bindings for the generated CPP code.\nIt's output would look something like this:\ncdef extern from \"flat_generated.h\" namespace \"flat\":\n        cdef cppclass flat:\n                int64_t ID()\n        cdef cppclass flatBuilder:\n                flatBuilder(FlatBufferBuilder fbb)\n                void add_ID(int64_t ID)\n                Offset Finish()\nFollowed by the usual cython wrapper for a public class:\ncdef class flatview:\n        cdef names *ptr\n        @staticmethod\n        cdef root(void *buf):\n                instance = flatview()\n                instance.ptr = <names *> GetRoot[flat](buf)\n                return instance\n        @staticmethod\n        def create(buf):\n                return flatview.root(<unsigned char*> buf)\nAnd builder:\ncdef class flatbuilder:\n        cdef namesBuilder *ptr\n        def __cinit__(self, flatbuffer buf):\n                self.ptr = new flatBuilder(cython.operator.dereference(<FlatBufferBuilder *>buf.fbb))\n        def ID(self, int64_t ID):\n                self.ptr.add_ID(ID)\n        def Finish(self):\n                return self.ptr.Finish().o\n. @layzerar Thanks, great work.\nI like that you caught some of the discrepancies like flatbuffers.table.Table to flatbuffers.Table, in the python code generation.\nWhat are your thoughts on the direction I took, to enable \"pyx\" code generation?\nYour patch will speed up \".py\" code gen,  mine will enable users who want to generate \".pyx\" to do so with minimal indirection. They seem complementary.\n. @layzerar Packaging is a rather strange experience as well.\nI had success with basic schemas, with round trip encode/decode. But, I manually generated the code -- I still need to create a new code gen for \"pyx.\", and I have not tested for proper deallocation.\n. @layzerar I can not work on the project this month. If I can get more time allotted I can pick up on the code generator that @abuchanan started.\n. ",
    "schwiet": "The documentation says you'd use the attribute nested_flatbuffer: table_name, but I've found that flatc doesn't actually recognize this. I happened to try something like:\ntable Currency{\n    id:int;\n    amount:int;\n} \ntable Wallet{\n    tag:int;\n    currencies:[Currency]\n}\nand flatc compiled that. The generated code adds a:\nvoid add_currencies(...) to the WalletBuilder, just look at your generated.h file.\n**EDIT: just realized you were asking about Go. I'd imagine there's a similar function in the builder, still.\n. Thanks, yes that did the trick. Is the [ tableName ] syntax also legal?\n. ",
    "easlee": "Yes, the generated file has this method, and i already used it in my code\ngo\nWalletAddCurrencies(builder, currencies_vec)\nset a vector is ok, but my problem is how to add a table into a vector. in testcase of google\uff0c it ignore  this case.\nEDIT: add a struct can use CreateXXX method, it works good\ngo\nWalletStartCurrenciesVector(builder, 3)\nfor j := 2; j>=0;j-- {\n    CreateCurrency(builder, j+1, 100)\n}\ncurrencies_vec := builder.EndVector(3)\n. I read the source code of C++ and Go, tried many times, finally solved the problem.\n``` go\nwallets_count := 3\nbuilder := flatbuffers.NewBuilder(0)\nwallet_slice := make([]fb.UOffsetT, wallets_count, wallets_count)\n//NOTE: MUST FROM 0 TO N\nfor i := 0; i =0;j-- {\n        CreateCurrency(builder, j+1, 100)\n    }\n    currencies_vec := builder.EndVector(3)\n    WalletStart(builder)\n    WalletAddTag(i+1)\n    WalletAddCurrencies(builder, currencies_vec)\n    wallets_slice[i] = WalletEnd(builder)\n}\nUserStartWalletsVector(builder,wallets_count)\n//NOTE:MUST FROM N TO 0\nfor i := wallets_count-1; i >= 0; i--{\n    builder.PrependUOffsetT(wallets_slice[i])\n}\nwallets_vec := builder.EndVector(wallets_count)\nUserStart(builder)\nUserAddId(builder, 19)\nUserAddWalletsVector(builder, wallets_vec)\nuser := UserEnd(builder)\nbuilder.Finish(user)\nbuffer := builder.Bytes[builder:Head():]\n```\n. ",
    "mtariq61": "Can somebody translate this code to Java for me please.\n. ",
    "stewartmiles": "Looks ok, any chance you could change the config option to\nFLATBUFFERS_BUILD_FLATC\nin order to be easier to read?\nCheers,\nStewart\n. I'm not sure what exactly you're trying to achieve.  You must be trying to\nparse a whole lot of json.\nThe parser contains state that cannot be shared between threads.  If you\ncreate multiple parsers, one per thread to parse the same JSON you're going\nto be out of luck since each parser parses the entire JSON state.  Why not\nbreak the data into separate blobs of JSON and parse them each on a\nseparate thread (if that really is a bottleneck - I'm not sure how)?\nAlternatively why not generate binary flatbuffer data and just load it in\nso no parsing is required?\nIn terms of JSON generation, it's similar to the parsing case split the\ndata up into discrete chunks (e.g one flatbuffer per generator) then\ngenerate the JSON for each lump of data per thread.  Again though I'm not\nsure why you need to do this.\nIt would be useful if you describe your use case.\nCheers,\nStewart\nOn Thu, Jan 8, 2015 at 6:33 AM, David Yu notifications@github.com wrote:\n\nI'm using the parser to parse/generate json from multiple threads.\nI only need to call parser::Parse once to load the schema (and then it\nwould be cloned for n threads).\nThen SetRootType for json parse/generate.\nNot really experienced in C++.\nIf you give me some pointers, I'll try to submit a PR\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/123.\n. Ah I see, I'm not sure you're saving that much time by avoiding the process\nof parsing the schema on each thread, unless you have a huge schema.\nHave you measure a significant startup cost?\n\nIf you're handling JSON RPCs, then why not use the same schema for parsing\nand generation?  In order to interpret received data it will need to be in\na form that matches the schema anyway, so use a single schema.  The you'll\nbe initializing the parser on each thread once and parsing the received\ndata each time, no need to reinitialize it.\nYou can test this out with flatc, if you do something like this...\nflatc -t samples/monster.fbs samples/monsterdata.json\nsamples/monsterdata2.json samples/monsterdata3.json\nflatc will construct one parser, parse the schema and then parse the json\nand dump the resulting json to the current directory.\nCheers,\nStewart\nOn Thu, Jan 8, 2015 at 9:38 PM, David Yu notifications@github.com wrote:\n\nUsecase:\njson rpc with flatbuffers' json parser/generator and data stored in a\ndatastore in binary form (a.k.a flatbuffer)\nI could always create a parser per thread, but that also means parsing the\nstatic schema for each thread repeatedly.\nMy suggestion was to load the schema once and copy the internal\ndatastructures (already loaded with the schema) of that parser per thread.\nIts similar to constructing a parser with AST (datastructure) vs a text\nfile (schema)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/123#issuecomment-69295539.\n. I don't think you need to modify the parser for your use case, just\nconstruct and initialize it on each thread from the same schema, the\noverhead should be pretty small for setup unless you're firing off and\ntearing down threads on a regular basis.\n\nOn Fri, Jan 9, 2015 at 1:14 PM, David Yu notifications@github.com wrote:\n\n\"Have you measure a significant startup cost?\"\nNo not yet. But I'll definitely try hacking the parser to have a construct\nfrom an existing parser.\n\"If you're handling JSON RPCs, then why not use the same schema for parsing\nand generation?\"\nThat's exactly how I'm using flatbuffer's parser.\nAnd no, I'm not reinitializing it everytime. I'm using it like you said.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/123#issuecomment-69399903.\n. Wouter asked me to take a look at this and I'm a little concerned how this could become messy pretty quickly.  In terms of using biicode in the Travis project, I guess that's ok as long as it's factored out into a separate section of the travis.yml at the moment the primary install target installs biicode which seems really odd.  As I mention in my comments on the CMakeLists.txt file, it would be more scalable - AFAIK - to put the biicode stuff in a subdirectory so that when someone comes along with support for the next package management hotness we don't just lump it into the core project file.\n\nDoes this make sense?  Sorry to churn you on this, we would just like to get this right.\nCheers,\nStewart\n. Looking way better, any chance you could squash this series into one commit (git rebase -i) ?\nWhat happened to the Travis build config?  Going to keep that in a fork?\n. I'm sorry to hear you feel like this.  Are there particular areas you feel\nneed improvement?\nOn Fri, Sep 18, 2015 at 8:40 AM, maticmeznar notifications@github.com\nwrote:\n\nAfter learning the basics of Protocol Buffers a few months ago from the\nexcellent documentation on Google's website, I decided to try FlatBuffers\ntoday. Needless to say, things are VERY POORLY explained, if at all. It\nreally should be done better.\nPlease look at official Protocol Buffer documentation on how this should\nbe done.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/273.\n. It shouldn't (famous last words) make a difference\nsince local-source-file-path is used to expand all source file paths in the\nndk-build makefiles.\n\nOn Mon, Nov 16, 2015 at 2:14 PM, Wouter van Oortmerssen \nnotifications@github.com wrote:\n\nSorry, these commits already got merged. Still curious how this will\naffect Windows though.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/pull/347#issuecomment-157188166.\n. LGTM\n. Any progress on this?. Looks like I need to get this working with VS2010 as well :/. Responses to the top level comment inline below:\n\nThanks for the mountain of work to support STLPort. I've got some nits and one bigger issue:\nSo far we've promised that STLPort is supported for flatbuffers.h and the generated code (without --object-api), since otherwise the changes would be too sprawling (as you just found out). We never enforced this, which your change now does, which is awesome.\nIs your project using STLPort also using the parser, flexbuffers, reflection, and the object API?\n\n\nWe're using the parser, but the cost of adding support for the other components was minimal.\n\n\nIf not, I feel this change could have been simplified a lot by simply making a new test target for Android STLPort that only includes the basics.\n\n\nI disagree, supporting STLport requires - TBH - minimal changes to the code base.  Most of the C++11 features utilized are simply syntactic sugar that only serve to narrow the user base of this library and components that depend upon it.\n\n\nWhile it could be argued that STLPort compatibility is a \"nice to have\" for the entire code-base, it puts undue burden on contributors to have to support it. They typically want to use full C++11 in their changes, and then have to deal with CI telling them it is not supported on VS2010. Luckily, VS2010 supports a lot of the basics, but STLPort is much worse, also because most contributors won't have a local test-setup for it, so are forced to play a very slow ping-pong with the CI to get their PR in shape.\n\n\nThe tests are already in place.  Android CI is now setup for this, VS2010 is humming along the limits on STL usage are minimal.\n\n\nAnd all that for a build config that is dying out rapidly, and very few people are using. I feel it should only be supported where absolutely necessary, and impact the modern C++ experience as little as possible.\n\n\nSee the data I forwarded you, basically it disagrees strongly with your statement.\n. Ok:\n I've moved the cruft into stl_emulation.h \n Renamed VectorData etc. methods.\n Cleaned up flatbuffers::unique_ptr in <= MSVC2012\n Restored the std::function code that constructs flexbuffers in the test.  Though I did narrow the capture to just the builder, it's interesting how similar the code looks.  . I agree that std::unique_ptr --> flatbuffers::unique_ptr with <= MSVC2012 is suboptimal.  I strongly disagree that about your assessment associated with maintaining stlport compatibility, most of this CL is build logic and stl_emulation.h.\n\n\nw.r.t unique_ptr in the object API code gen, how about I move flatc's default back to using std::unique_ptr and I change the build logic for the tests / samples to generate code with flatbuffers::unique_ptr?  This will keep the non-stlport code gen using std::unique_ptr so that we can support <= MSVC2012 in a canonical way (note this is the only affected compiler by flatbuffers::unique_ptr vs. std::unique_ptr).  WDYT?. Ok by default the object API generates code with std::unique_ptr now.. @alexames and @aardappel , extending std::vector and std::string in stl_emulation.h sounds like a neat idea.  This does have the issue that flatbuffers::vector != std::vector when using stlport which means that code like std::vector v = someflatbuffersvector will fail to compile as there is no conversion between the derived class and the base class.  This is a similar issue to the horribleness that occurs when using flatbuffers::unique_ptr in <= MSVC2012 builds.  On the up side this does limit the pain to users of stlport - who should move away from it anyway.  If you folks think I should make the change, I'll do so.\n@alexames moving flatbuffers::unique_ptr into the std:: namespace isn't feasible as I've already - unfortunately - run across some projects using stlport that decide to do exactly the same resulting in duplicate type definitions for std::unique_ptr.  . I took a first stab at moving to flatbuffers::vector and flatbuffers::string to get rid of the compatibility methods and the change ends up inconsistently bubbling up into the public API so we end up with a load of occasional flatbuffers::vector and flatbuffers::string in the API surface.  In order to do this properly we would need to change all public APIs to use flatbuffers::vector or flatbuffers::string instead which is a far larger change.  I would definitely prefer to keep what we have in place and add custom vector / string types in a follow up commit.. Looking at it.... Sorry about that.  It's fixed in f064a6cc60b0978509280002f93dcb9d08a543d8. Appveyor is hanging so I tested locally with MSVC 2010.. Thanks @a-maurice !. Ok I've fixed the build failures due to f2b3705. @aardappel seems possible though I can imagine some of what we maintain in our internal BUILD files may not work with Bazel and may refer to projects we can't expose.  In short it sounds like work, @jschaf (if you're the Googler of the same name) seems like we may need to maintain a couple of BUILD files, one that is open sourced and another that remains internal, you game for it?. Sounds fair to me, probably worth refactoring our internal BUILD file since it's almost there :). Well it looks like the problems the CI was supposed to catch, it's now catching which means that uses of std::move among other things will need to be cleaned up first.. @sutambe did you rebase with my most recent commit to fix the Android build issues with std::move etc.? https://github.com/google/flatbuffers/pull/4970. As discussed on another thread, we'll need a way to use this without injection of these annotations as they add a dependency upon jsr250 on Android which some folks - like our project - do not want.  @riklund any chance you could make this conditional ?. Just a small thing, we follow the Google C++ style guide\nhttp://google-styleguide.googlecode.com/svn/trunk/cppguide.html\nTo adhere to this, you'll need to line up the arguments followed by the function definition to the opening parenthesis.\nBTW: I tried pasting a formatted version in here but failed :(\n. Need to add a license and copyright header to each file.\n. Need license and copyright header\n. \n   extern bool GeneratePython(const Parser &parser,\n                              const std::string &path,\n                              const std::string &file_name,\n                              const GeneratorOptions &opts);\n\n. No need for the dot before the module name.\ne.g\n\nfrom builder import Builder\n\nshould be fine.\n. FYI: We follow the Google python style guide.\nhttp://google-styleguide.googlecode.com/svn/trunk/pyguide.html\nSo it may be worth running this through pylint to make sure everything looks ok.\n. Constants like this should be upper case \nhttp://google-styleguide.googlecode.com/svn/trunk/pyguide.html?showone=Global_variables#Naming\ne.g\n\n_LITTLE_ENDIAN = sys.byteorder == 'little'\n\n. It would be great if you could add some pydoc comments describing this class and its methods.\n. Since padding is just a set of zeros would it make sense to share this across all instances of DownwardArray rather than creating a separate padding buffer for each DownwardArray?\n. Need license and copyright header\n. Could you alphabetically sort the import list?\n. Could you wrap long lines http://google-styleguide.googlecode.com/svn/trunk/pyguide.html#Line_length ?\n\ndef ensure_view(source):\n    return (source if type(source) is memoryview else\n            memoryview(source))\n\n. I'm not sure whether this will work since make_fixed_vector() currently returns None.\n. Why does this need to have a nested impl() function?\n. nit: How about calculating the data offset once here?\n\nvector_elements = read_uint(view, offset)\ndata_offset = offset + 4\narr = array.array(fmt)\nreturn read_array(arr, view[data_offset:data_offset + (size * arr.itemsize)])\n\n. How about reusing read_byte_vector() ?\n. Worth adding a comment to describe why this is implemented using a nested function?\n. Similar to indirect, why does this need an additional layer of indirection here?\n. Why use a tuple rather than declare the members of this class via the init() method?  Seems a little bit odd to me at the moment.\nI have similar reservations about Struct and Table.\n. For unit tests could use you the vanilla python unittest module instead to keep the dependencies down?\n. Please limit lines to 80 characters\n. It would be great to see comments in the generated code as well.  You have implemented code to generate doc strings but don't cover the code in the golden file test.\n. Please use vanilla python unittest module\n. Why does this need to depend upon biicode, what are we getting for this?\n. I'm not a big fan of distribution specific code in the main cmakelists file.  If this was factored out so that biicode was part of an optionally included file it may not be so bad.  Right now if we start adding rules for each distribution mechanism for the library this could get out of hand pretty quickly.\n. What is this file for?  Could you add comments?\nWould it be possible to put everything related to biicode in a subdirectory for example?\n. If we're going to setup Travis for Flatbuffers we should probably have it associated with a Flatbuffers maintainer's account instead.\n. sg, done. done. fixed. sg, done. Ok will do, I'll ping this comment when I have an update since this is a larger job.. This forwards unique_ptr into the flatbuffers namespace, I've updated the comment to describe the rationale.. Apart from creating a pair of CreateVectorOfStructStart() / CreateVectorOfStructEnd() methods to handle the couple of lines of pre / post code around the loop.  I don't think so, that of course adds the potential for the compiler to not inline this code in addition makes it a little more tricky to read.  Any other suggestions?. Can't do this as some users of stlport (we have some internal users) have defined std::unique_ptr as well so it's not safe to just inject the type into the std namespace.  flatbuffers::unique_ptr is an alias of std::unique_ptr in all implementations except for stlport and <= MSVC2012. \nIn <= MSVC2012 the derived class is a little tricky as this results in the user doing...\nflatbuffers::unique_ptr fbp;\nstd::unique_ptr sup(fbp.release());\nwhich I concede is inconvenient.  AFAIK this is the closest to type aliasing that's possible with <= MSVC2012.\nBTW It's most definitely not 99% of the world not caring about stlport I'm afraid.  I've pinged you the data.\nI guess another way to look at this is why support MSVC2010 / MSVC2012?. Yep, that's the canonical way to distribute gradle based projects.. Sure, see earlier comment.  I'll ping when I've moved it.. Unfortunately it can't be a macro as a lambda in <= MSVC2012 can't just be cast to a function pointer it seems to be a pretty broken state of affairs.  I could split the logic out into static methods rather than using lambdas but it makes the code harder to follow WDYT?. I still wanted to make sure the new code worked.  I guess I could add a version that works with old skool function pointers and new lambda / closures, WDYT?. How would we do that?  log AFAIK is not constexpr.  Even if it had constexpr support in some STLs what about those that don't?. I've been pondering this a bit more and perhaps I can handle stlport using macros in the generated code e.g\nin stl_emulation.h do..\nnamespace std {\nclass flatbuffers_unique_ptr;\n}\nThen in the generated code...\n`\n// generated header\nifdef FLATBUFFERS_CPP98_STL\ndefine unique_ptr flatbuffers_unique_ptr\nendif  // FLATBUFFERS_CPP98_STL\n// generated code that uses unique_ptr\nifdef FLATBUFFERS_CPP98_STL\nundef unique_ptr\nendif  // FLATBUFFERS_CPP98_STL\n`\nThis is a little bit of a pain since all internal uses of std::unique_ptr will also need to be wrapped in a similar set of boilerplate which is pretty unfortunate.. Ok it's moved.. Did you see the latest patch?  I've ditched this wrapper code in favor of #ifdef'in the typedef for the function pointer which is far cleaner.. Ok done. Not possible, as this uses type aliases for internal uses of unique_ptr etc.. Correct, the existing use cases are in violation of Google style.  Rather than proliferate the problem I've added code that adheres to Google style.  The idea is that a follow up can consist of a pass to align style so that we can use auto formatting tools like clang-format to keep things in check.. Sure done, though I noticed a load of other stuff under protected that probably shouldn't be there.  Probably worth a future clean up.. Regarding\nvector_data(vector* v) vs. vector_data(const vector& v)\nthere are mutable uses of vector.data()  Since it's possible for the result to be mutable, following Google style, it can't be a reference.  This makes it clear at the point of use that the data could be mutated.  The alternative would be to have\nT vector_data(vector v) \nT* vector_data(const vector& v)\nwhich I feel is worse.  If you want me to go that way, happy to do so.. I'm afraid that would be a Google style violation.  I can see the case for indentation where there is a lot of nested preprocessor logic, however this is a very clear case.. Ok we'll do that.. I've replaced everything with a reference though I really don't like it.  It simply aids to obfuscate the code at the call site.  This is something that could be resolved during a style pass.. Done, we'll resolve in a follow up.. Ok, we can resolve in a follow up.. Done. Done. ",
    "LeanderBB": "Changed applied as requested :)\n. ",
    "specialforest": "Linux/GCC. I see the library does use \"auto\". This should be pretty straightforward to workaround. Do you know offhand which other C++ 11 specific features are used?\n. I've created a branch https://github.com/specialforest/flatbuffers/tree/c++03 and made some changes to compile it with non C++11 compiler. I had to use boost headers though. All tests are passing.\n. Since then I haven't synced the branch with the mainline. I doubt it will ever be merged.\n. ",
    "nickva": "Would also like C++ 98 support. Sometimes updating GCC is not easy when it is part of a larger infrastructure or certain configurations have been tested and certified.\nMy platform is RHEL 6 (CentOS6). GCC version 4.4.7\n. ",
    "dvj": "I would like to use FlatBuffers with the TI compiler (http://software-dl.ti.com/codegen/non-esd/downloads/index.htm) which does not support C++11 (http://processors.wiki.ti.com/index.php/C%2B%2B_Support_in_TI_Compilers)\nIn my case (and I would think for most people) this would only need C++98/03 support for the flatbuffers.h file, not for flatc which could be built with a C++11 compliment compiler.\n. ",
    "charsyam": "@dyu Thanks. \n. ",
    "ondovb": "A bunch...here is the output:\n```\nLd flatc normal x86_64\n    cd \"/Users/ondovb/install/flatbuffers-1.0.3 2\"\n    export MACOSX_DEPLOYMENT_TARGET=10.9\n    /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/clang++ -arch x86_64 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.9.sdk -L/Users/ondovb/install/flatbuffers-1.0.3\\ 2 -F/Users/ondovb/install/flatbuffers-1.0.3\\ 2 -filelist /Users/ondovb/install/flatbuffers-1.0.3\\ 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/flatc.LinkFileList -mmacosx-version-min=10.9 -Wl,-search_paths_first -Wl,-headerpad_max_install_names -Xlinker -dependency_info -Xlinker /Users/ondovb/install/flatbuffers-1.0.3\\ 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/flatc_dependency_info.dat -o /Users/ondovb/install/flatbuffers-1.0.3\\ 2/flatc\nduplicate symbol __ZN11flatbuffers7NewLineERKNS_16GeneratorOptionsE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_text-FD3CEDECEA406A94.o\nduplicate symbol __ZN11flatbuffers6IndentERKNS_16GeneratorOptionsE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_text-FD3CEDECEA406A94.o\nduplicate symbol __ZN11flatbuffers16OutputIdentifierERKNSt3__112basic_stringIcNS0_11char_traitsIcEENS0_9allocatorIcEEEERKNS_16GeneratorOptionsEPS6_ in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_text-FD3CEDECEA406A94.o\nduplicate symbol __ZN11flatbuffers5PrintIPKvEEvT_NS_4TypeEiPNS_9StructDefERKNS_16GeneratorOptionsEPNSt3__112basic_stringIcNSA_11char_traitsIcEENSA_9allocatorIcEEEE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_text-FD3CEDECEA406A94.o\nduplicate symbol __ZN11flatbuffers12GenerateTextERKNS_6ParserEPKvRKNS_16GeneratorOptionsEPNSt3__112basic_stringIcNS8_11char_traitsIcEENS8_9allocatorIcEEEE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_text-FD3CEDECEA406A94.o\nduplicate symbol __ZN11flatbuffers14GenerateBinaryERKNS_6ParserERKNSt3__112basic_stringIcNS3_11char_traitsIcEENS3_9allocatorIcEEEESB_RKNS_16GeneratorOptionsE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/flatc-59072AAE264CC47D.o\nduplicate symbol __ZN11flatbuffers16GenerateTextFileERKNS_6ParserERKNSt3__112basic_stringIcNS3_11char_traitsIcEENS3_9allocatorIcEEEESB_RKNS_16GeneratorOptionsE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/flatc-59072AAE264CC47D.o\nduplicate symbol program_name in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/flatc-59072AAE264CC47D.o\nduplicate symbol _main in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/flatc-59072AAE264CC47D.o\nduplicate symbol __ZN11flatbuffers2go12OffsetPrefixERKNS_8FieldDefE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_go-614DA3E823EBA56B.o\nduplicate symbol __ZN11flatbuffers10GenerateGoERKNS_6ParserERKNSt3__112basic_stringIcNS3_11char_traitsIcEENS3_9allocatorIcEEEESB_RKNS_16GeneratorOptionsE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_go-614DA3E823EBA56B.o\nduplicate symbol __ZN11flatbuffers6Parser11ParseHexNumEi in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser4NextEv in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser6IsNextEi in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser6ExpectEi in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser14ParseTypeIdentERNS_4TypeE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser18LookupCreateStructERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser9ParseTypeERNS_4TypeE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser8AddFieldERNS_9StructDefERKNSt3__112basic_stringIcNS3_11char_traitsIcEENS3_9allocatorIcEEEERKNS_4TypeE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser10ParseFieldERNS_9StructDefE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser16ParseSingleValueERNS_5ValueE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser13ParseMetaDataERNS_10DefinitionE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser22ParseIntegerFromStringERNS_4TypeE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser13TryTypedValueEibRNS_5ValueENS_8BaseTypeE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser13ParseAnyValueERNS_5ValueEPNS_8FieldDefE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser10ParseTableERKNS_9StructDefE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser11ParseVectorERKNS_4TypeE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser15SerializeStructERKNS_9StructDefERKNS_5ValueE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers10kTypeSizesE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers10kTypeNamesE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser9ParseEnumEb in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser11StartStructEv in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser9ParseDeclEv in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser11SetRootTypeEPKc in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser13MarkGeneratedEv in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser14ParseNamespaceEv in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser14ParseProtoDeclEv in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser22ParseTypeFromProtoTypeEv in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers6Parser5ParseEPKcPS2_S2 in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_parser-503CAA09FBB97042.o\nduplicate symbol __ZN11flatbuffers3cpp17GenUnderlyingCastERKNS_6ParserERKNS_8FieldDefEbRKNSt3__112basic_stringIcNS7_11char_traitsIcEENS7_9allocatorIcEEEE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_cpp-F7C986698F837568.o\nduplicate symbol __ZN11flatbuffers3cpp24GenerateNestedNameSpacesEPNS_9NamespaceEPNSt3__112basic_stringIcNS3_11char_traitsIcEENS3_9allocatorIcEEEE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_cpp-F7C986698F837568.o\nduplicate symbol __ZN11flatbuffers3cpp21CloseNestedNameSpacesEPNS_9NamespaceEPNSt3__112basic_stringIcNS3_11char_traitsIcEENS3_9allocatorIcEEEE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_cpp-F7C986698F837568.o\nduplicate symbol __ZN11flatbuffers11GenerateCPPERKNS_6ParserERKNSt3__112basic_stringIcNS3_11char_traitsIcEENS3_9allocatorIcEEEERKNS_16GeneratorOptionsE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_cpp-F7C986698F837568.o\nduplicate symbol __ZN11flatbuffers11GenerateCPPERKNS_6ParserERKNSt3__112basic_stringIcNS3_11char_traitsIcEENS3_9allocatorIcEEEESB_RKNS_16GeneratorOptionsE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_cpp-F7C986698F837568.o\nduplicate symbol __ZN11flatbuffers11GenerateFBSERKNS_6ParserERKNSt3__112basic_stringIcNS3_11char_traitsIcEENS3_9allocatorIcEEEERKNS_16GeneratorOptionsE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_fbs-704AE593C0FB8569.o\nduplicate symbol __ZN11flatbuffers11GenerateFBSERKNS_6ParserERKNSt3__112basic_stringIcNS3_11char_traitsIcEENS3_9allocatorIcEEEESB_RKNS_16GeneratorOptionsE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_fbs-704AE593C0FB8569.o\nduplicate symbol __ZN11flatbuffers9MakeCamelERKNSt3__112basic_stringIcNS0_11char_traitsIcEENS0_9allocatorIcEEEEb in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_general-358562755FF8CF6A.o\nduplicate symbol __ZN11flatbuffers10GenCommentERKNSt3__16vectorINS0_12basic_stringIcNS0_11char_traitsIcEENS0_9allocatorIcEEEENS5_IS7_EEEEPS7_PKc in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_general-358562755FF8CF6A.o\nduplicate symbol __ZN11flatbuffers15GenerateGeneralERKNS_6ParserERKNSt3__112basic_stringIcNS3_11char_traitsIcEENS3_9allocatorIcEEEESB_RKNS_16GeneratorOptionsE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_general-358562755FF8CF6A.o\nduplicate symbol __ZN11flatbuffers19language_parametersE in:\n    /Users/ondovb/install/flatbuffers-1.0.3 2/build/FlatBuffers.build/Debug/flatc.build/Objects-normal/x86_64/idl_gen_general-358562755FF8CF6A.o\nld: 50 duplicate symbols for architecture x86_64\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\n```\n. ",
    "matsuhiro": "When json data include multi bytes string, StringToInt() does not return expected value.\nFor example,\n{\n\"value\":\"\\u304230000\"\n}\nWhen cursor_ have \"30423000\" as value, and StringToInt() returns 809644032, but expected value is 12354.\n12354 is 3042 in hexadecimal.\n. Thank you for your reply.\nBut, strtoull don't have input parameter to specify string length.\nSo, I improved my code, please check it.\n. You mean that an end pointer is endptr of \nunsigned long long int strtoull (const char* str, char** endptr, int base);\n?\nendptr is output parameter that have pointer of  the next character in str after the numerical value.\n. ",
    "pjulien": "PR is here\nhttps://github.com/google/flatbuffers/pull/127\n. @gwvo \nI don't like maven either, I just used it because that's what proto does.\nTo validate it, just do \"mvn install\" in the Java source directory and you'll see a jar will be created.  Yes, I do plan to maintain it.  That being said, eventually if there is no objection I would:\n- Move to Gradle\n- Confirm the source directories to what Maven/Gradle want\nAs noted in the earlier comment, this is just to get a jar in your local repository, it's not good enough to publish to Maven Central\n. @gwvo You OK with this patch?\n. Good stuff @ennerf was about to write this one but really glad you did.\n. @ennerf @gwvo \nI think a StringBuilder might be good if we allow a user to supply their own instance so they can recycle them.  Something akin to ByteBuffer variants that are available today.\n. @gwvo \n- Looking at ByteBuffer#get, the one that takes the full array, it seems to be doing a loop there too. Would need Unsafe to avoid the loop\n- Yes, string\n- I'm thinking for the first version of replacing this:\nbyte[] copy = new byte[bb.getInt(offset)];\nwith this:\nByteBuffer bb = this.bb.duplicate();\n      byte[] copy = new byte[bb.getInt(offset)];\nIt will fix the issue without committing to a direction.  This way, you can decide later on what you want to do.  Does that sound fair?\n. @gwvo There is no copy, duplicate will only copy the satellite data.  The backing buffer is shared between both buffers\n. Well technically it would be version 1.0.0-SNAPSHOT since that's the version on the front page.  I just didn't know if @gwvo would be OK would that.  @gwvo if you are I can change the version.\n. @ennerf Good stuff.  You're systemically fixing all my issues with the library.  Although the library does a good job avoiding allocations itself, it's hard to use it today and have the calling code stay allocation free.\n. @gwvo \n\nHmm.. where would be pass/store this Allocator? Would like to keep the default interface simple.\n\nA flat buffer object today that has someString and someStringAsByteBuffer methods would also get the equivalent methods that take an allocator as an argument.\nThis also means that existing methods would just forward to these new methods with the default allocator.\n\nWhat would be ideal I suppose is to make the behavior you're suggesting the default, though I guess > there might be issues with FlatBuffers assuming it can use TLS.\n\nHow so?  TLS is specified by the original Java spec and it's also part of Android 1.0.  Are you talking about availability or something else?  I can draw up a patch if that's the direction you're looking to go.\n\nAnd also, yes, currently a corrupted FlatBuffer can provoke an ArrayindexOutOfBoundsexception or > whatever getInt throws. It be good to catch that if you're reading untrusted data.\n\nYes but this is not what I was referring too.  Here you can provoke an OutOfMemoryError in there because the byte[] is allocated before the byte buffer's limits are set (position and limit).  If using the byte buffer to validate first, we'll get a proper runtime exception.  If not, the heap can fill up with large byte[] objects\n\nIt is really unfortunate we have to make these copies at all.\n\nYes, very.  I am aware of a patch that is floating around that has better support for interoperability between the two.  The patch is on the Java 9 queue but from what I can tell was also in the Java 8 queue back in the day of that release.\nAfter profiling, moving things around from one flat buffer to another hurts a lot more than this.  I have a filling and draining buffer and I'm re-assembling these buffers to be faster by making sure individual elements all fit on a single cache line for later processing.  Unfortunately, converting from bytes to string to bytes requires too many allocations.\nBased on these numbers, my other proposal would be adding a copyString method in FlatBufferBuilder that just takes a ByteBuffer.  This would be even more meaningful and improve scenarios where you're only working with flat buffers in the first place.\n. @gwvo \n\nAs an aside, if maximum efficiency matters for your use case, can't you use the AsByteBuffer?\n\nI do, I read it as a ByteBuffer but I can't write to the destination flat buffer today, there's no copyString method that takes a byte[].  Adding FlatBufferBuilder#copyString(ByteBuffer) would be more than good for today.\n\nA way to create a string directly from a ByteBuffer (that presumably already contains UTF-8) would make a lot of sense to me. That way you can just keep your string data around as ByteBuffers and never pay the string convertion cost.\n\nPatch coming up.\n. @gwvo \n\nas for the new byte[] in that code, I am starting to think that caching that in TLS as the default implementation may be preferable over yet another set of accessors.\n\nPersonally, here is the patches I am doing base on my profiling:\n- Added createString that takes a ByteBuffer to FlatBufferBuilder.  This saves me so much in terms of allocations.  However, I'm planning on submitting on a different ticket.\n- For this, per your earlier point, don't know how to remove the byte[] when a thread is done without additional methods.\n. @gwvo \nOh and almost forgot, being able to add vectors that take an offset and length is the other thing I'm doing.  Will submit in a different ticket too, that way, can keep reusing the same int[] when adding a vector because it doesn't have to be the exact size.\n. @gwvo np\n. PR was merged\n. @gwvo \n\nDo we know that CharBuffer.wrap doesn't already perform this optimisation?\n\nIt does not, it creates a StringCharBuffer internally every time. When profiling, was seeing the CharBuffer objects constantly.\nIt's possible escape analysis take cares of the wrapper itself but it's extremely hard to say since the profiler has the effect of turning escape analysis off.\n. typo.  \"safe\" -> \"save\"\n. No, it's because if you don't duplicate it, it's not thread safe to multiple concurrent readers.  So in-between changing its position/limit and then resetting, any other reader that is using the same buffer would be susceptible to getting strange exceptions such as buffer underflow/overflow exceptions.\n. Well technically, it's already inlined, that's one thing the JVM excels at. I think it makes it less clear what the input to createString is.  Let me known\n. Why do you say it's permanent?  If the thread dies, it will be garbage collected.\n\nMaybe for starters leave out the Math.max(, 128) ?\n\nWhat do you mean?\n. Also,\n\ny allocated for each table type that has a string in it\n\nThis is incorrect, it's one buffer per thread, the number of tables is irrelevant\n. This will break for many different type of byte buffers.  Both heap and off-heap.\n\nIf the byte buffer is a slice and on-heap, this will fail because arrayOffset needs to be added.  \nIf the byte buffer is off-heap this will also throw\n\nUsing limit might not be correct either depending on the context.. I am seeing similar issues in all the new code since I've last looked at this, is there any reason why array() is accessed directly?  Are you specifically telling the world that only non-sliced on-heap buffers are usable for this feature?\nIf you are, it should be documented better but I would argue such documentation will mostly be ignored and people will complain on the list about the exception they're getting... if they're lucky to get them.  Off-heap buffers will throw immediately when using the array method but on-heap slices will most likely only hit at run time.\nI am also concerned about the use of getShort too.  This call will not align correctly so I don't understand why it's being used at all.\n. @aardappel\n\nif you have time to help @TGIshib correct this fix, that may be helpful.\n\nyep\n. ",
    "velvia": "Hey guys, it'd be great if we could publish flatbuffers jar to a central repo.  That said it's really not that many files, but...\n. If anybody is looking for flat buffers jars, I have them here:\nhttps://bintray.com/velvia/maven/flatbuffers/view https://bintray.com/velvia/maven/flatbuffers/view\nI\u2019m too lazy to publish to Maven Central.\n\nOn Feb 19, 2016, at 12:07 AM, Andrew Hundt notifications@github.com wrote:\nI'd appreciate this as well, or perhaps instructions to make the jar\n\u2014\nReply to this email directly or view it on GitHub https://github.com/google/flatbuffers/pull/129#issuecomment-186111086.\n. No the version is not related to the Flatbuffers version at all.   It\u2019s from a few months ago, and the Java support hasn\u2019t changed that much.  The version is for my own project.\nOn Feb 22, 2016, at 2:00 PM, Andrew Hundt notifications@github.com wrote:\n@velvia https://github.com/velvia those look pretty out of date unless I'm mistaken the current release is 1.3.0 and it seems your latest is 0.2.0\n\u2014\nReply to this email directly or view it on GitHub https://github.com/google/flatbuffers/pull/129#issuecomment-187405438.\n. \n",
    "qumeta": "great job\nfrom my iPhone\n\n\u5728 2015\u5e741\u670822\u65e5\uff0c23:41\uff0cFrancisco Ram\u00edrez notifications@github.com \u5199\u9053\uff1a\nHi,\nMy name is Francisco Ramirez and I write you from biicode a small start-up that it\u2019s developing a C and C++ dependency manager; just like Maven and Maven Central for Java. Our tool aims to allow our users to set-up a project with just #includes.\nWe're uploading as many interesting libraries as we can. I have forked your repository to support biicode. Our CMakeLists.txt code is on IF(BIICODE) statement, so it's not interfering with the original one (if you want to compile it without biicode).\nI have tested it on Windows with MinGW and Visual Studio, Linux with GCC and OSX with Clang.\nIf you want to try it with biicode follow this steps:\nInstall biicode\nCreate new project folder:\n$ bii init flatbuffers\n$ cd flatbuffers\nCheckout the github repository in block folder:\n$ git clone git@github.com:franramirez688/flatbuffers.git blocks/google/flatbuffers\n$ bii cpp:build\n$ ./bin/google_flatbuffers_tests_test\nYou just created a biicode block named google/flatbuffers (username/blockname), you've compiled it and run an example. Now, if you'd want to publish it, sign up and run the command:\n$ bii publish --tag STABLE \nBy the momment, you could take a look to this example, which is reusing your code (uploaded in fenix account user)\nIf you want to know more about biicode or need help just write me! francisco.ramirez@biicode.com\nFlatbuffers doc example\nThanks you so much in advance\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/google/flatbuffers/pull/133\nCommit Summary\nadded biicode.conf and adapted CMakeLists.txt for using flatbuffers with biicode\nFile Changes\nM CMakeLists.txt (29)\nA biicode.conf (15)\nPatch Links:\nhttps://github.com/google/flatbuffers/pull/133.patch\nhttps://github.com/google/flatbuffers/pull/133.diff\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "franramirez688": "Thanks a lot for your feedback! I'll update my MR with changes in the CMakeLists.txt to modify the minimum code, respecting the original one and adding a CI file (travis or appveyor) to check biicode building isn't breaking to make  the maintenance easier.\nGive it a try and let us know what do you think\n. I modified CMakeLists.txt and biicode.conf to get a simpler solution. On the other hand, I've added a .travis.yml and travis badge to check building with biicode was right. The last ones are referenced to my biicode and github user, but if you decide to maintain your lib in biicode these would point to your account names\n. I just rebased and squashed my commits. I modified the original CMakeLists.txt and added a biicode.cmake into CMake/ folder to separate biicode's building. I commented the biicode.conf file too.\n.travis.yml file to build with biicode has been currently removed but I can replace it for a .travis.yml file to build \"as usual\"\nHope this fits you best,\nCheers,\nFran\n. I removed Travis file because I'm working on a flatbuffers' build config with and without biicode, so, you could check both at same building time. As soon as it's ready, I'll open another PR or add it here to merge it.\nCheers,\nFran\n. Travis build status is generic, with and without biicode, it's referenced to my account but I thought you could change it with your one. If you don't want to have biicode badge in your main Readme.md I'll bring it to biicode/Readme.md.\nThat Python file is a hook for biicode. It's a simple script which is executed when all the project files are processed (bii work, bii build, bii configure, etc. commands) and it's in charged of adding/deleting biicode.conf file into your root project folder and adding 4 lines to your main CMakeLists.txt\n. I just modified biicode/README.md file, biicode badge was removed from the main readme.md and Travis image is ponting to your future account.\n. Ok! I'm going to change it. So, would it be a problem if I back to add my biicode badge in your main readme.md?\n. I just deleted the Python file and sat all the changes inside the respective files.\nDon't worry about it because I totally understand your position ;) \nI'm glad that you like the idea of biicode!\n. ",
    "AEtherSurfer": "I want this as an option in the schema file.. ",
    "pdca": "@ennerf \nOur company firewall block all exe/msi..., could you provide *.zip flatc download link?\n. @gwvo \nIt is impossible to download & install software by staff freely because company policy you know.\nIt is great job you provide flatc-< version >.zip +  source-< version >.zip + binary-< version >.zip\nMany thanks your flatc.zip, so I can begin to try to use it now.\n. ",
    "hyjin": "@gwvo Thank you!\n. ",
    "gregoire-astruc": "I've updated the PR according to your comment.\nBoth approaches make sense to me: consistent types for either side of the ternary branches, or consistency with the return value.\nAs long as I can use FlatBuffers on linux without resorting to manage my own fork... :)\n. That's too bad, really...\nI'm currently looking at the code, and wondering if a Release() method along the lines of std::unique_ptr.release() would do the trick.\n`` cpp\n// AssumingConfiguration` is a FlatBuffer schema.\nconst Configuration *loadConfiguration(const std::string &filename)\n{\n    flatbuffers::Parser parser;\n    // parser.Parse(schema)\n    // parser.Parse(file)...\n    // error checking...\nreturn GetConfiguration(parser.bulder_.Release())\n\n}\n```\nHere, the FlatBufferBuilder's vector_downward would relinquish its allocated memory to the caller. That means it is now up to him to free the memory when done.\nNow given vector_downward may have a not so simple allocator, perhaps there's a std::unique_ptr to throw in there somewhere, with its deleter bound to the vector_downward's allocator (or maybe a copy of it)?\nI am just throwing ideas here, and I know FlatBuffers are fast on their own, but I still feel like it is missing a release()-like option, to allow the caller to take ownership of the created object(s).\n. I quickly hacked around and came up with something functional, tough still quite not to my liking :)\nChanges were quite simple (thanks to a nice codebase!):\n- allow vector_downward to release its data.\n  It returns a unique_ptr which points to vector_downward.cur_, but will release vector_downard.buf_ upon destruction using the same allocator (well, a copy of it).\n- enable FlatBufferBuilder to expose such released pointer.\n- throw in some tests for good measure.\nNow, for integration with client code, I believe that the generated function GetXYZ() may need a equivalent that would explicitely states that the pointer is released to the caller. ReleaseXYZ() or TakeXYZ()...\nStill, the code provided lacks the ability to either reset/clear the FlatBufferBuilder and its members (especially the vector_downward).\nHopefully you like the idea :)\n. Well, if you take a look at PR #146 I submitted a patch to allow a caller to take ownership of the data buffer away from the FlatBufferBuilder.\nThis is done through a unique_ptr with a custom deleter (because we cannot make assumption about the allocator that is used). There's also overhead, but it's a lot more limited than with a shared_ptr approach.\nIt also enable callers to create/delete FlatBufferBuilder on the stack (inside functions for instance) while retaining the data.\nHopefully this answers your question and may fit your needs :)\n. Well lucky you!\nI squashed it all down to a single commit. I've also added a tidy bit in the documentation.\n. As far as I remember, bitshift is UB on anything but unsigned values... \n. Does it even generates warnings if mask is unsigned? Given you only use it in bit manipulation, I wonder. \n. Release() or ReleaseBufferPointer()? The latter is consistent with the Get version.\n. Other possible meaningful names are buffer_t or memory_t, which less emphasis the uniqueness, and more what it represents.\n. ",
    "JonathanHope": "Unfortunately it remains unclear to me how to do exactly what the OP asked for. I saw that a Release function was added which returns a DetachedBuffer. I also know that a ReleaseXYZ style function was discussed in the comments of that issue which I do not believe was ever implemented.  Updating the code for the new API:\n// Assuming `Configuration` is a FlatBuffer schema.\nconst Configuration *loadConfiguration(const std::string &filename)\n{\n   flatbuffers::Parser parser;\n   // parser.Parse(schema), parser.Parse(file)...\n   auto buffer = parser.builder.Release().data();\n   // What to do here?\n   return parser.builder.Release().; \n}\nIs there a recommended way to do this use case?. ",
    "yacoder": "I probably should have created 2 separate branches for each of the changes?.. I didn't realize pull requests take all the changes from the branch. On the other hand, one of the changes is trivial (the warning fix), and the second one depends on it (sort of), so I suppose it might be ok to consider them both together. Not sure what's the best practice in such cases :)  Let me know if there is a better way.\n. ",
    "bmharper": "I have removed the benchmark and squashed it into a single commit.\n. Wow, I didn't know that. However, surely one would want the flatbuffer type \"string\" to be represented by the idiomatic utf-8 string type of the language? If one has to cast the byte slice to a string before using it, then would you even be winning anything? I found this issue https://github.com/golang/go/issues/6714 which discusses []byte -> string cast allocations.\nWhen doing this work, I was wondering whether I shouldn't just use \"string\" to represent an arbitrary buffer, but (without actually doing the homework) I was worried that some language such as Java would end up representing \"string\" as UTF16, which would be very painful.\n. ",
    "reynolma2": "Please review this proposed update, it was found during some testing of the C# library and usage with Table that contained BOOLs\n. ",
    "mgerhardy": "http://code.google.com/p/protobuf-wireshark/\n. I signed it!\n. thanks\n. It's about the line 4872.\nC:\\projects\\engine\\gen-protocol\\network\\ServerMessages_generated.h(33): error C2059: syntax error: 'constant' (compiling source file C:\\projects\\engine\\src\\modules\\network\\ClientNetwork.cpp) [C:\\projects\\engine\\src\\modules\\network\\network.vcxproj]\n         C:\\projects\\engine\\gen-protocol\\network\\ServerMessages_generated.h(33): error C3805: 'constant': unexpected token, expected either '}' or a ',' (compiling source file C:\\projects\\engine\\src\\modules\\network\\ClientNetwork.cpp) [C:\\projects\\engine\\src\\modules\\network\\network.vcxproj]\n         C:\\projects\\engine\\gen-protocol\\network\\ServerMessages_generated.h(41): error C2589: 'constant': illegal token on right side of '::' (compiling source file C:\\projects\\engine\\src\\modules\\network\\ClientNetwork.cpp) [C:\\projects\\engine\\src\\modules\\network\\network.vcxproj]\n         C:\\projects\\engine\\gen-protocol\\network\\ServerMessages_generated.h(42): error C2062: type 'unknown-type' unexpected (compiling source file C:\\projects\\engine\\src\\modules\\network\\ClientNetwork.cpp) [C:\\projects\\engine\\src\\modules\\network\\network.vcxproj]\n         C:\\projects\\engine\\gen-protocol\\network\\ServerMessages_generated.h(42): error C2143: syntax error: missing ';' before '}' (compiling source file C:\\projects\\engine\\src\\modules\\network\\ClientNetwork.cpp) [C:\\projects\\engine\\src\\modules\\network\\network.vcxproj]\n         C:\\projects\\engine\\gen-protocol\\network\\ServerMessages_generated.h(43): error C2059: syntax error: 'return' (compiling source file C:\\projects\\engine\\src\\modules\\network\\ClientNetwork.cpp) [C:\\projects\\engine\\src\\modules\\network\\network.vcxproj]\n         C:\\projects\\engine\\gen-protocol\\network\\ServerMessages_generated.h(55): error C2065: 'AttribMode': undeclared identifier (compiling source file C:\\projects\\engine\\src\\modules\\network\\ClientNetwork.cpp) [C:\\projects\\engine\\src\\modules\\network\\network.vcxproj]\n         C:\\projects\\engine\\gen-protocol\\network\\ServerMessages_generated.h(55): error C2146: syntax error: missing ')' before identifier 'e' (compiling source file C:\\projects\\engine\\src\\modules\\network\\ClientNetwork.cpp) [C:\\projects\\engine\\src\\modules\\network\\network.vcxproj]\n         C:\\projects\\engine\\gen-protocol\\network\\ServerMessages_generated.h(55): error C2143: syntax error: missing ';' before '{' (compiling source file C:\\projects\\engine\\src\\modules\\network\\ClientNetwork.cpp) [C:\\projects\\engine\\src\\modules\\network\\network.vcxproj]\n         C:\\projects\\engine\\gen-protocol\\network\\ServerMessages_generated.h(55): error C2447: '{': missing function header (old-style formal list?) (compiling source file C:\\projects\\engine\\src\\modules\\network\\ClientNetwork.cpp) [C:\\projects\\engine\\src\\modules\\network\\network.vcxproj]\nHere is how I call flatc: https://github.com/mgerhardy/engine/blob/master/cmake/macros.cmake#L160\nflatc -c -I ${CMAKE_CURRENT_SOURCE_DIR}/../attrib/definitions --scoped-enums -o ${GEN_DIR} ${CMAKE_CURRENT_SOURCE_DIR}/definitions/${DEFINITION}. Thanks for having a look. \nIt contains: \n enet headers - and they \"only\" contain\n#include <winsock2.h>\n SDL headers.\nI'm unsure what they include.\nI'll try to find the cause and will update the issue once I know.\n. Ok, the order matters - first including the flatbuffers generated headers fixed the issue.. CMAKE_CXX_COMPILER_VERSION can be anything - not just gcc, no?. ",
    "advayDev1": "Pulled from proper branch in request 157 instead:\nhttps://github.com/google/flatbuffers/pull/157\n. If #158 is merged first, this will need to be modified (or vice-versa).\n. Sure thing - merged both feature branches back into this single PR.  You can close out #158 if that works for you.\n. yes\n. see pull #169 \n. fixes #162 \n. This change is technically api incompatible.  You could have existing\nclients subclassing or instantiating generated classes.  So if you follow\nsemantic versioning or similar I'd check in with a flag and later set to\ndefault true and eliminate flag when you release 2.x\nOn Mon, Apr 6, 2015, 5:38 PM gwvo notifications@github.com wrote:\n\nThanks!\nIs there any reason to have bool seal_classes ? i.e. in what case would I\nNOT want the code to be generated this way? If this is always sensible,\nlets not make it an option.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/pull/169#issuecomment-90297777.\n. sgtm, done in squashed eedd313402331730ec7322fd79c819e3ce9ef528\n\nthe fact that it is a breaking change is noted in the commit message instead.\n. i think this still makes sense for Java though (it has no notion of static classes).  shall i go ahead with a java-only change?\n. Understood, I thought it might be useful since Gradle is the build process used in Android Studio, and it looks like that is still recommended over bazel for that purpose.\n. ",
    "dgnorton": "@rw how hard would it be for the builder to take just one buffer and segment it as needed internally so that implementation details are hidden from client code?  If any one of the segments (bytes, vtable, vtables, etc.) needed to grow, allocate a whole new buffer, copy each of the segments into the new buffer, and continue.\n. @rw I'm not sure a pool of builders would work.  E.g., in our case...\n- get buf from a pool of []byte\n- read flat data from socket into buf\n- get builder from a pool of flatbuffer builders\n- traverse buf normalizing data into new flatbuffer using the builder\n- pass newly constructed buf down the data pipe and return builder to pool\nWe have a problem at that last step because the builder still owns the memory that we passed down the pipe.  We could pass the builder down the pipe but that seems awkward.\nMaybe I'm thinking about it wrong and there's a better way?\n. ",
    "onecityuni": "I65b6ea7d208b0ccd6a0b34761162fed6ba391fc5\n. ",
    "Klaim": "Could you be more explicit? I don't see any commit with this sha.\n. ",
    "ragdroid": "We tried something of this sort: https://github.com/ophio/flatbuffer-gradle-plugin\n. ",
    "iamandi": "Hi gwvo,\nI too am working embedded project for which library and c/c++ APIs are in eclipse luna. So working on eclipse is must. Now what I would like to know how to install and integrate flatbuffers with eclipse? So that I can integrate them with my project APIs and produce hex files. Let me know if you want to know more. Thanks\n. ",
    "deajosha": "Thank you very much,  I have resolved the problem.\n. ",
    "hantaogo": "many web game client use as3\n. thank you very much\n. ",
    "generalzgd": "i hope as3 too\n. ",
    "eproxus": "That would change the code to example.MonsterAddTestType(builder, TestType.Red), right?\n. @rw I'll try. :-) No guarantees on when it will be ready though.\n. Was some time since I worked on it, but I got stuck on generating the getters/setters and their respective types.\n. So, it's a bit more complex than I first anticipated. The enum types have to be used in the getters/setters of structs where enums are used, otherwise the resulting Go code will not compile.\n. Not anymore, I kind of got stuck on the getter setter code generation.\n. Unfortunately, I don't work with Go any more currently and so have no time to spend on this.\n. @googlebot I signed it!\n. ",
    "loachfish": "Thank you for your reply.\nIt works with your advice, and it is really that asserts are ignored in the Android.\nIt also works with the sample_text.\nIf there is any impact as asserts are ignored?\nB.R\n. in the function GenerateStruct **file - idl_gen_text.cpp\ntable->CheckField(fd.value.offset)\nif (struct_def.fixed || table->CheckField(fd.value.offset)) {\n}\nif the value of the field is 0 and not generate struct\ngoto the  CheckField()  **file - flatbuffers.h\n  bool CheckField(voffset_t field) const {\n    return GetOptionalFieldOffset(field) != 0;\n  }\nso I modify the condition of the Judge and not check field's value and always generate the struct.\nIt works.\nThen i want to know the risk assessment.\n. to gwvo,\nThank you for your reply and advice.\nB.R\n. maybe it was caused as that not fill the field btnInfo.\nSo the case is that when the vector is not filled, it will crash to gen?\n. the function\n// Specialization of Print above for pointer types.\ntemplate<> void Print(const void val,\n                                    Type type, int indent,\n                                    StructDef union_sd,\n                                    const GeneratorOptions &opts,\n                                    std::string *_text);\nthe val maybe nullptr, when the vector is not filled.\nwhen goto the function PrintVector, the pointer is nullptr.\nSo it add the condition: in the begine of function of Print\n  if (val == nullptr) {\n    return;\n  }\nit runs OK, but the output is like this:\n{\n  layoutPattern: MasterSlave,\n  layoutID: Slave,\n  overwriteFlag: 1,\n  cursorControlType: DestinationMode,\n  btnCount: 0,\n  btnInfo: \n}\n. maybe we should fill the meta text when the val == nullptr,\nto ensure  the format.\n. it works when add the options.\n    flatbuffers::GeneratorOptions opts;\n    opts.strict_json = true;\n    opts.output_default_scalars_in_json = true; // key\n    GenerateText(parser, fbs_str.c_str(), opts, &jsongen);\nanother syntax error when make in my env:\nflatbuffers/src/flathash.cpp:26:17:  the enum  items cannot end with a comma:\nenum OutputFormat {\n  kDecimal,\n  kHexadecimal,\n  kHexadecimal0x,  ///< \n};\nMaybe it works well in some env, but it will affects the portability .\n. g++ (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3\nUbuntu 12.04\n. I get it. Thanks a lot.\nI want to get the fbs schemafile with the json format.\nmaybe my fbs file has many tables, and via the root table to get the schmafile.\ne.g.\n/* NDBody.fbs */\n/* NDBody */\ntable NDBody\n{\n    arm:string; // info of arm\n    leg:string;  // info of leg\n    eye:string; // info of eye\n}\n// xxxxxxxxxxx\ntable  NDArm\n{\n   finger:ubyte; //xxxx\n   a:NDType; //xxxx\n   b:short; //xxxxx\n}\nenum NDType : ubyte\n{\n    Zero = 0,\n    One = 1,\n    Two = 2\n}\nwhen I pass the table name NDArm, I want to get:\n{\n   \"finger\": ubyte,\n   \"a\": NDType,\n   \"b\": short,\n}\nwhen I input NDType, I want to get:\n{\n    Zero = 0,\n    One = 1,\n    Two = 2\n}\nWhether the flatbuffers support the method?\n. test 1:\nint main()\n{\n  std::string schemafile;\n  bool ok = flatbuffers::LoadFile(\"monster.fbs\", false, &schemafile);\n  if (!ok) {\n    printf(\"couldn't load files!\\n\");\n    return 1;\n  }\n  // parse schema first, so we can use it to parse the data after\n  flatbuffers::Parser parser;\n  const char *include_directories[] = { nullptr };\n  ok = parser.Parse(schemafile.c_str(), include_directories);\n  assert(ok);\nok = parser.SetRootType(\"Monster\");\n  printf(\"ok=%d\\n\", ok);\nStructDef *pStructDef = parser.root_struct_def;\n  if (!pStructDef) {\n    printf(\"pstructdef is nullptr.\\n\");\n  }\n  printf(\"pStructDef[%p]\\n\", pStructDef);\nstd::vectorstd::string namesp = pStructDef->defined_namespace->components;\n  printf(\"sz[%lu]\\n\", namesp.size());\n  for (int spz = 0; spz < namesp.size(); spz++) {\n    printf(\"%s\\t\", (namesp[spz]).c_str());\n  }\n  printf(\"\\n\");\n// SymbolTable fieldDef = pStructDef->fields;\n  // for (auto it = fieldDef.vec.begin(); it != fieldDef.vec.end(); ++it) {\n  for (auto it = (_parser.root_struct_def).fields.vec.begin();\n    it != (_parser.root_struct_def).fields.vec.end(); ++it) {\n    flatbuffers::FieldDef &fd = **it;\n```\n// printf(\"fd[%s]\\t\", fd.name.c_str());\nprintf(\"ty:[%d]\\t\", fd.value.type.base_type);\n```\n}\n  printf(\"\\nend\\n\");\nreturn 0;\n}\nlogout:\nok=1\npStructDef[0xfb0f80]\nsz[2]\nMyGame  Sample  \nty:[15] ty:[5]  ty:[5]  ty:[13] ty:[14] ty:[3]  \nend\n. test 2:\nint main()\n{\n  std::string schemafile;\n  bool ok = flatbuffers::LoadFile(\"monster.fbs\", false, &schemafile);\n  if (!ok) {\n    printf(\"couldn't load files!\\n\");\n    return 1;\n  }\n// parse schema first, so we can use it to parse the data after\n  flatbuffers::Parser parser;\n  const char *include_directories[] = { nullptr };\n  ok = parser.Parse(schemafile.c_str(), include_directories);\n  assert(ok);\nok = parser.SetRootType(\"Monster\");\n  printf(\"ok=%d\\n\", ok);\nStructDef *pStructDef = parser.root_struct_def;\n  if (!pStructDef) {\n    printf(\"pstructdef is nullptr.\\n\");\n  }\n  printf(\"pStructDef[%p]\\n\", pStructDef);\nstd::vectorstd::string namesp = pStructDef->defined_namespace->components;\n  printf(\"sz[%lu]\\n\", namesp.size());\n  for (int spz = 0; spz < namesp.size(); spz++) {\n    printf(\"%s\\t\", (namesp[spz]).c_str());\n  }\n  printf(\"\\n\");\nSymbolTable fieldDef = pStructDef->fields;\n  // for (auto it = fieldDef.vec.begin(); it != fieldDef.vec.end(); ++it) {\n  // for (auto it = (_parser.root_struct_def).fields.vec.begin();\n  //   it != (_parser.root_struct_def).fields.vec.end(); ++it) {\n  //   flatbuffers::FieldDef &fd = **it;\n//   // printf(\"fd[%s]\\t\", fd.name.c_str());\n//   printf(\"ty:[%d]\\t\", fd.value.type.base_type);\n// }\n  // printf(\"\\nend\\n\");\nreturn 0;\n}\nlogout\nok=1\npStructDef[0x119af80]\nsz[2]\nMyGame  Sample  \nsegment fault (core dump)\n. test 3\nint main()\n{\n  std::string schemafile;\n  bool ok = flatbuffers::LoadFile(\"monster.fbs\", false, &schemafile);\n  if (!ok) {\n    printf(\"couldn't load files!\\n\");\n    return 1;\n  }\n// parse schema first, so we can use it to parse the data after\n  flatbuffers::Parser parser;\n  const char *include_directories[] = { nullptr };\n  ok = parser.Parse(schemafile.c_str(), include_directories);\n  assert(ok);\nok = parser.SetRootType(\"Monster\");\n  printf(\"ok=%d\\n\", ok);\nStructDef *pStructDef = parser.root_struct_def;\n  if (!pStructDef) {\n    printf(\"pstructdef is nullptr.\\n\");\n  }\n  printf(\"pStructDef[%p]\\n\", pStructDef);\nstd::vectorstd::string namesp = pStructDef->defined_namespace->components;\n  printf(\"sz[%lu]\\n\", namesp.size());\n  for (int spz = 0; spz < namesp.size(); spz++) {\n    printf(\"%s\\t\", (namesp[spz]).c_str());\n  }\n  printf(\"\\n\");\nSymbolTable fieldDef = pStructDef->fields;\n  for (auto it = fieldDef.vec.begin(); it != fieldDef.vec.end(); ++it) {\n  // for (auto it = (_parser.root_struct_def).fields.vec.begin();\n  //   it != (_parser.root_struct_def).fields.vec.end(); ++it) {\n    flatbuffers::FieldDef &fd = **it;\n```\n// printf(\"fd[%s]\\t\", fd.name.c_str());\nprintf(\"ty:[%d]\\t\", fd.value.type.base_type);\n```\n}\n  printf(\"\\nend\\n\");\nreturn 0;\n}\nlogout:\nok=1\npStructDef[0x20c6f80]\nsz[2]\nMyGame  Sample  \nty:[15] ty:[5]  ty:[5]  ty:[13] ty:[14] ty:[3]  \nend\nsegment fault (core dump)\n. the root_struct_def and pStructDef is OK,\nif use it to hold the fielddef is will crash when in the  destructor?\n. cannot the json with strict ?\nwhen i change the json\n{\n  \"notifyNo\": 111,\n  \"updateType\": 1,\n  \"item\": {\n    \"itemId\": 171,\n    \"itemType\": 1,\n    \"selectionSts\": 2,\n    \"continuousValue\": 10,\n    \"charCode\": 0,\n    \"settingItemName\": \"abc\",\n    \"OptList\": [\n      \"a\",\n      \"ab\",\n      \"abc\"\n    ],\n    \"settingName\":\"def\"\n  }\n}\nthen to flatbuffers and use this flatbuffers to fbs:\n{\n  notifyNo: 111,\n  updateType: Delete,\n  item: {\n    itemId: -127,\n    itemType: OnOff,\n    selectionSts: Off,\n    continuousValue: 10,\n    charCode: 0,\n    settingItemName: \"abc\",\n    OptList: [\n      \"a\",\n      \"ab\",\n      \"abc\"\n    ],\n    settingName: \"def\"\n  }\n}\nIt is OK.\nhow to set the input json with strict format ?\n. when my source file is:\n```\nstd::vector vollist;\nfor (int i = 0; i < size; ++i) {\n    const NDVolphoneInfo *tmp = new NDVolphoneInfo(i + 1, i + 2);\n    vollist.push_back(tmp);\n}\nauto list_1 = builder.CreateVector(vollist.data(), size);\nauto volphone = CreateNDBroadcastIndividualVolStatus(builder, 9, list_1);\nbuilder.Finish(volphone);\n```\nit can pass the verifer, but the string is NG.\n. debug to find that the value of  verifier.Verify(vollist())  is false,\n. the fbs generated header file shows:\nwhen the vector contains struct, the type to fill the field is a contain pointer's flatbuffers' Vector, such as:\nvoid add_vollist(flatbuffers::Offset< flatbuffers::Vector < const NDVolphoneInfo *>> vollist);\nbut when the vector contains table the fill type is:\nvoid add_vollist(flatbuffers::Offsetflatbuffers::Vector> vollist)\n. to find that when verifier the vector of struct:\ntemplate < typename T > bool Verify(const Vector < T >  *vec) const \nthat the T is a pointer type, the sizeof(T) is 4 in 32bit PC, and 8 in 64bit, it may not be equal the sizeof \nstruct, when the sizeof(struct T) is less sizeof(pointer), the verifier will be failure.\nif to modify the fbs file to:\nstruct NDVolphoneInfo\n{\nincoming:ubyte;\nphone:ubyte;\nplaceholder1:int;\nplaceholder2:int;\n}\nthe verifier will pass.\n. PASS\uff01\nthx!\n. found that:\nin class FlatBufferBuilder the function:\nuoffset_t EndTable(uoffset_t start, voffset_t numfields)\nauto pos = static_cast(vtableoffsetloc - field_location->off);\ncast the postision from uoffset_t(uint32_t) to voffset_t (uint16_t).\ncan I redefine the  voffset_t to uint32_t?\n. if want to fill the fields size > 64KB, how to deal with this.\nI. modify to support filed size > 64KB\nin flatbuffers.h\n// format forks to save a bit of space if desired.\ntypedef uint32_t voffset_t; // redefine voffset_t\nin function:\nuoffset_t EndTable(uoffset_t start, voffset_t numfields) \nassert(table_object_size < 0xffff0000);  // Vtable use 32bit offsets.\nII. use the macro to get the selection in Makefile, like,\nifdef USE_8BIT\ntypedef uint8_t voffset_t;\nelse if USE_16BIT\ntypedef uint16_t voffset_t;\nesle if USE_32BIT\ntypedef uint32_t voffset_t;\nelse\ntypedef uint16_t voffset_t;\nelse\n\n. NDxProtocolData.fbs\n```\nenum NDxCommandType : ubyte\n{\n    Unknown = -1,\n    PointToPoint = 1,\n    GroupBroadcast = 2,\n    Broadcast = 3\n}\ntable NDxDataHeader\n{\n    cmd:uint = 4294967295;\n    from:uint = 4294967295;\n    to:uint = 4294967295;\n    cmdType:NDxCommandType = Unknown;\n    devNo:uint = 4294967295;\n    fromMac:uint = 4294967295;\n    toMac:uint = 4294967295;\n}\ntable NDxProtocolData\n{\n    version:string;\n    head:NDxDataHeader;\n    data:string;\n}\nroot_type NDxProtocolData;\n```\nNDySubCommand.fbs\n```\ntable NDySubCommand\n{\n    dataLen:uint;\n    data:[byte];\n}\nroot_type NDySubCommand;\n```\nEncode\nvoid sendySubCommand(uint32_t size_)\n{\n    flatbuffers::FlatBufferBuilder fbb;\n    NDxProtocolDataBuilder pdb(fbb);\n    pdb.add_head(CreateNDxDataHeader(fbb, 3/*code1*/, 10000/*from*/, 10086/*to*/, NDxCommandType_PointToPoint, 12/*devNo*/, 168/*fromMac*/, 172/*toMac*/));\n    {\n        uint32_t buffSize = size_;\n        char* buff = new char[buffSize];\n        flatbuffers::FlatBufferBuilder fbb2;\n        fbb2.Finish(CreateNDySubCommand(fbb2, buffSize, fbb2.CreateVector((int8_t*)(buff), buffSize)));\n        pdb.add_data(fbb.CreateString((char*)fbb2.GetBufferPointer(), fbb2.GetSize()));\n        delete[]buff;\n        buff = NULL;\n    }\n    fbb.Finish(pdb.Finish());\n    // to send data\n    // if (NULL != m_sender) {\n    //     m_sender->sendData((char*)fbb.GetBufferPointer(), fbb.GetSize());\n    // }\n}\n. It's really the encoding problem.\nthe right encode sequence:\n```\nstd::vector vec;\nvec.clear();\nfor (int i = 0; i < size_; i++) {\n    vec.push_back('D');\n}\nflatbuffers::FlatBufferBuilder fbb;\nauto fbVec = fbb.CreateVector(vec);\nNDySubCommandBuilder vr_builer(fbb);\nvr_builer.add_dataLen(size_);\nvr_builer.add_data(fbVec);\nfbb.Finish(vr_builer.Finish());\nstd::string vrStr(\"\");\nvrStr.append((char*)fbb.GetBufferPointer(), fbb.GetSize());\nflatbuffers::FlatBufferBuilder fbb_2;\nauto fbStr = fbb_2.CreateString(vrStr.c_str(), vrStr.size());\nauto header = extavc::CreateNDxDataHeader(fbb_2,\n                3, 10000, 10086, NDxCommandType_PointToPoint, \n                12, 168, 172);\nNDxProtocolDataBuilder pdb(fbb_2);\npdb.add_head(header);\n{\n    pdb.add_data(fbStr);\n}\nfbb_2.Finish(pdb.Finish());\nstd::string outData((char*)fbb_2.GetBufferPointer(), fbb_2.GetSize());\n```\n. 1. why it's ok in first encode sequence when the vector size is less then 0x0000FFFF;\n2. this special encode sequence maybe not easy-using in some scene,\n. thx for your reply.\n. The final is that the wrong encoding sequence, and bring out the NotNested assert.\nwhile there are some new questions:\n1. disable the assert, what's the influence when encoding in nested.\n2. in the condition 1(close the assert), we pack/unpack the flatbuffers' all right in the first encoding sequence when the size is less than 64K, either the verifier check. while the size is out of 64K, verifier shows the mistakes. why is 64K?\n3. if make sure the size is less than 64K, and encoding in the first method, and the assert is closed, If there are other hidden problems ? \n. got it!\nthx.\n. got it!\nthx.\n. the first field's value must be 0 in the declaration for a enum.\nyou should be define the fbs file:\nenum chat_type:int\n{\n    chat_default = 0,\n    chat_all = 1,\n    chat_news = 2,\n    chat_whisper = 3,\n}\n. ",
    "Jiboo": "\nYou need to call CreateVectorOfSortedTables instead to get the sort.\n\nSorry I wasn't clear, I'm not responsible for that call, I was talking about the JSON to binary feature in the flatc tool, and idl.h's Parser::Parse which in the c++ docs under \"Text & schema parsing\".\nThey will both in the end use Parser::ParseVector to serialize a vector, which don't check that the vector is a table with a key, and just calls CreateVector and PushElement offset to your entries in the order they were parsed.\nIf you confirm that this is a a bug, I could extract it from #187 and make a separate PR.\n. Hi, yeah, I knew thus would be too many changes, sorry.\nIn my case I want users to write JSON configuration files, I'll use flatbuffer as a way to \"compile\" their JSON and be able to read it as fast as possible.\nI designed the format thinking about how I'd write it in json, and I used maps of string->tables. Thus tables also contains similar maps (to do some kind of subconfiguration, enabled in some conditions, the properties of the subconfiguration will be merged in it's parent, no limit on this level of nesting).\nThis \"nesting\" and the overhead of the entries and vector opening/closing, and the need to always specify the name of the key and the value, motivated this attribute.\nThis was mainly for my use, I just wondered if you'd be interested. I could try to split in separates PR if something's worth merging.\n. The FlatBufferBuilder constructor in CPP also takes an initial size, but it defaults to 1024. As Java doesn't support default values for parameters, maybe it would be better to add a constructor with no parameters in the Java class instead of doing the fix in the docs.\nhttps://github.com/google/flatbuffers/blob/master/include/flatbuffers/flatbuffers.h#L486\nhttps://github.com/google/flatbuffers/blob/master/java/com/google/flatbuffers/FlatBufferBuilder.java#L49\n. ",
    "hash-X": "Yeah ,I have the same issue with you. In my project , many many of  nested messages I will find. You can see it in the website. I think it is very improtant , cause existing code a lot of are protobuf. So, rewrite is urgent.\nhttp://stackoverflow.com/questions/32759826/how-can-i-rewrite-the-protobuf-scheam-with-flatbuffer-schema/32763527?noredirect=1#comment53411545_32763527\n. Thank you .My bro. Recently, I really need this features. Thank you for your efforts.\n. @aardappel , If you don't mind ,I am glad to write some document about java example in FlatBuffer. Because in most China technical forum I see. The example about java I have seldom to see. I can not see any example about Java used in Flatbuffer in our major technical forum (51cto, CSDN) . But I think I am good at java used in FlatBuffer and I can write more detail example in java used in FlatBuffer. But I don't know how to do next. From where to write these document ?\n. may be like this :+1: \ntable Yy {\n  name:string;\n}\ntable Xx {\n  id:uint (required);\n  name:string;\n  y:[Yy];\n}\n. OK , thank you. \n. OK , thank you. \n. OK, Thank you. I think I should download the latest source code (master branch) and recompile the code. Am I right ? \n. Thank you. Fow now, I think I should download the latest source code( master branch ), and recompile it. Am I right ?\nFrom: Wouter van Oortmerssen [mailto:notifications@github.com]\nSent: Tuesday, September 29, 2015 12:55 AM\nTo: google/flatbuffers\nCc: Zhang, Minglei L\nSubject: Re: [flatbuffers] Attention,there is a bug in flatc. (#285)\nThis was already fixed in this commit: feb4816https://github.com/google/flatbuffers/commit/feb481661094db2a73912f8e2d32bac9473ca47f\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/google/flatbuffers/issues/285#issuecomment-143803358.\n. FlatBuffer complier support Protobuf is not enough at this level. \n. @aardappel  I try it and without error and I think it works. Thank you. \n. @aardappel One more question, I think we should have some OutputStream for FlatBuffer Object. Just like protobuf have CodedOutputStream.java. Since at some special needs in saving FlatBuffer data to disk. and Deserialization, loading these data from disk. Although FlatBuffer is based on Memory Serialization Library. Do you think ? \n. @aardappel hi, I mean, you know, here is some code that protobuf previously use, and it support a method called writeDelimitedTo. The method is in AbstractMessageLite.java ,below is the detail about writeDelimitedTo, My question is how can I save my FlatBuffer object to disk ? Which output stream can I use ?\npublic void writeDelimitedTo(final OutputStream output) throws IOException {\n    final int serialized = getSerializedSize();\n    final int bufferSize = CodedOutputStream.computePreferredBufferSize(\n        CodedOutputStream.computeRawVarint32Size(serialized) + serialized);\n    final CodedOutputStream codedOutput =\n        CodedOutputStream.newInstance(output, bufferSize);\n    codedOutput.writeRawVarint32(serialized);\n    writeTo(codedOutput);\n    codedOutput.flush();\n  }\nNameSystemSection.Builder b = NameSystemSection.newBuilder()\n          .setGenstampV1(blockIdManager.getGenerationStampV1())\n          .setGenstampV1Limit(blockIdManager.getGenerationStampV1Limit());\nNameSystemSection s = b.build();\n      s.writeDelimitedTo(out);\nBelow is OK ? Will it have the same function as `writeDelimitedTo` ? If I use DataOutputStream\nIs something error will happen ? Semantic may be changed ? I am confused.\nIntelNameSystemSection.\n          createIntelNameSystemSection(fbb, fsn.unprotectedGetNamespaceInfo().getNamespaceID(),\n              blockIdManager.getGenerationStampV1());\nByteBuffer byteBuffer = fbb.dataBuffer();\n      byteBuffer = fbb.dataBuffer();\n      int serializedLength = byteBuffer.capacity() - byteBuffer.position();\n      byte[] bytes = new byte[serializedLength];\n      byteBuffer.get(bytes);\n      DataOutputStream dos = new DataOutputStream(out);\n      dos.write(serializedLength);\n      dos.write(bytes);\n      dos.flush();\n. @aardappel  OK , I will try it. Thank you !\n. methodTwo(fbb) will throw an error, I am not sure this message is necessary. I am confused.\nException in thread \"main\" java.lang.AssertionError: FlatBuffers: object serialization must not be nested.\n    at com.google.flatbuffers.FlatBufferBuilder.notNested(FlatBufferBuilder.java:293)\n    at com.google.flatbuffers.FlatBufferBuilder.startVector(FlatBufferBuilder.java:239)\n    at com.google.flatbuffers.FlatBufferBuilder.createString(FlatBufferBuilder.java:266)\n    at test.genstudent.TestStudent1.methodTwo(TestStudent1.java:32)\n    at test.genstudent.TestStudent1.main(TestStudent1.java:45)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)\n. @gwvo  Thank you for your reply. I just fixed my code according to your suggestion. But It seems still have problem. I don't know why ? Here is the latest code. Could you help me ? How can I fix the code and it looks correctly ?\n```\nstatic void methodTwo(FlatBufferBuilder fbb) {\n    int stuOffset = Student.createStudent(fbb,\n        fbb.createString(\"lisi\"), (short)19, 1);\n    ClassInfo.startClassInfo(fbb);\n    ClassInfo.addStu(fbb, stuOffset);\n    ClassInfo.addNumTeacher(fbb, (short) 3);\n    int env = ClassInfo.endClassInfo(fbb);\n    ClassInfo.finishClassInfoBuffer(fbb, env);\n    ClassInfo classInfo = ClassInfo.getRootAsClassInfo(fbb.dataBuffer());\n    short num = classInfo.numTeacher();\n    System.out.println(num);\n    System.out.print(classInfo.stu(0).name());\n  }\n```\nAnd here is the error\nException in thread \"main\" java.lang.IndexOutOfBoundsException\n    at java.nio.Buffer.checkIndex(Buffer.java:538)\n    at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:359)\n    at com.google.flatbuffers.Table.__offset(Table.java:33)\n    at src.genstudent.Student.name(Student.java:16)\n    at test.genstudent.TestStudent1.methodTwo(TestStudent1.java:43)\n    at test.genstudent.TestStudent1.main(TestStudent1.java:49)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)\n. @gwvo , OK, I know how to fix my code and run it collectly. Thank you very much.\nint[] data =\n        {\n            Student.createStudent(fbb, fbb.createString(\" gwvo \"),\n                (short)19,\n                1)};\n. @gwvo , you say , I am not sure \"already assigned\". Ok, I mean if I have a field of numOfStudent in the table Class. Just like below schema. I set this filed to 0 and default value is 0 too even if I don't set this value to 0. How can I judge this filed has already set ? This filed has already assigned by myself. Not the default value. Although they are equal. So, I should use hasNumOfStudent() method to judge. But FlatBuffer don't support it. If the field is not primitive type, so I can judge whether it is NULL to judge that. To primitive type, I am confusing.  For 0, it doesn't mean that it does not exist. But NULL can show this filed is not exist. Do you understand what I mean yet?\ntable Class {\n      numOfStudent : short = 0;\n}\n. @gwvo OK, It seems look I just judge that by whether it is 0 or not. but Thanks anyway.\n. OK, thanks a lot. \n. ",
    "vitalyisaev2": "Ok! I will try to do it.\n. @gwvo unfortunately I had no time to fix it. I'm sorry :(\n. @dvolosnykh sorry, actually I've prepared that patch a very long time ago (#196), and I haven't worked with cmake since that time. Please feel free to review and fix it.. @mockbutler I've tried to build DEB package with CPack from your fork, and unfortunately it seems to be broken. Steps to reproduce: \nbash\ndocker run -it --name=debuild debian\napt-get update\napt-get install g++ cmake git\ngit clone https://github.com/mockbutler/flatbuffers\ncd flatbuffers\ncmake .\ncpack -G DEB\nThis results in error:\n```bash\nCPack: Create package using DEB\nCPack: Install projects\nCPack: - Run preinstall target for: FlatBuffers\nCPack: - Install project: FlatBuffers\nCPack: Create package\nCMake Error at /usr/share/cmake-3.7/Modules/CPackDeb.cmake:811 (message):\n  CPackDeb: Debian package requires a maintainer for a package, set\n  CPACK_PACKAGE_CONTACT or CPACK_DEBIAN_PACKAGE_MAINTAINER\nCall Stack (most recent call first):\n  /usr/share/cmake-3.7/Modules/CPackDeb.cmake:1040 (cpack_deb_prepare_package_vars)\nCPack Error: Error while execution CPackDeb.cmake\nCPack Error: Problem compressing the directory\nCPack Error: Error when generating package: FlatBuffers\n```\nI'm sorry if I made mistakes in cmake commands, because I'm very far away from packaging now. Please check it.. @mockbutler thanks for a prompt response. The only problem is CPACK_RPM_PACKAGE_MAINTAINER now, all the rest is fine.. looks good to me. 1. It worth noting that flatbuffers package already exists on Fedora Koji, so you can either take this package, or take src.rpm and rebuild it for any RHEL-based distribution you need. The only meaningful reason to add CPack-based RPM package here is that if you use RPM packages in your CI and you need to have flatbuffer's master. In case if you need stable version, I'd prefer to use packages provided by Fedora team.\n2. I believe you should put your name and email here instead of mine :) . ",
    "ericlagergren": "Funny enough I'm fiddling with this right now. I could pick this up if you'd like. I think I might be able to finish most of it tonight.\n. Sure thing. I'll put in a pr when I go to lunch in an hour or so.\n. > Instead of ConvertBool, you want the generated code to accept bools natively, and then convert to and from bytes internally.\nTrue. I fiddled around with the C++ for a while but decided against including it in the PR because it looked like an API change.\n\nand PrependVector only solved the problem for one type.\n\nThen couldn't a function similar to PrependVector be generated? That seems to make the most sense, since writing out for loops for each slice that needs to be serialized is a lot of boilerplate code.\n. @rw I think generation is the way to go. I'd be willing to work on that, although I wouldn't be able to start working on it for around a week.\n. Unfortunately I've been slammed at work lately and haven't had much time to\nwork on it. Sorry for dropping the ball.\n\nEric Lagergren\nericlagergren.com http://www.ericlagergren.com\nOn Mon, Sep 28, 2015 at 5:16 PM, Robert notifications@github.com wrote:\n\n@EricLagergren https://github.com/EricLagergren Did you get a chance to\ntry out codegen?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/pull/245#issuecomment-143908733.\n. @gonzaloserrano It's deleted. We didn't want to continue to maintain it with google/flatbuffers tip plus there was a little bug with unions and enums that we didn't have time to work around.\n\nI can send you the .patch files if you'd like :smile: \n. > It might we worth taking a cross-section of the keywords from each of the major languages, else we may end up with a schema that's fine in Go but not C#, for example.\nMy opinion is to create a shared table of the reserved identifiers across all the languages and load that + a language-specific table based on the flags provided (e.g., --go, --cpp, etc.)\n. Shared keywords across all supported languages (ignoring Python since it's not very C-like):\n\"else\", \"default\", \"continue\", \"case\", \"const\"\n    \"goto\", \"switch\", \"break\", \"for\", \"if\", \"return\"\nIf you drop Go you get something like four more common keywords.\nI don't quite see the point in a shared table, tbh.\nThis list of keywords (it's a .zip with 7 files that each have one keyword per line) and this little Go program can generate a table of keywords for each language (assuming C++11 for the vector initialization).\n. > We could additionally add a global blacklist for keywords that exist in the majority of languages (like if), though if we have a solid escaping function in all supported languages this may not even be needed.\n@gwvo My last comment has a little Go program that generates a list of all the keywords for each language flatbuffers supports.\nIMO the proper table could be selected at runtime and then underscores tacked on if the field name matches any of the specific table's contents.\n. Here's some tables:\nhttps://gist.github.com/EricLagergren/4ec6defd29065dbb9112\n. @gwvo I could submit a PR, but it might have to wait until this weekend.\nWhy const char* ...[] instead of const std::vector<std::string> ? (C++ isn't my daily language.)\nedit: new tables: https://gist.github.com/EricLagergren/4ec6defd29065dbb9112\n. Oh, yeah. True. Makes sense!\nI updated my previous comment with the tables.\nRegarding the PR, just appending underscores should work, correct?\n. @rw 'fast' is subjective. :-)\nAnyway, one idea that I like is to use build tags. Use // +build unsafe for unsafe code, but default to safe. That way people (like me) who don't use GAE can use unsafe and the default version is nice, safe Go.\n. @rw yes\n. > If that change for only GAE, I think use appengine tag is better.\nsome people dislike having 'unsafe' in their code, I suppose.. @aardappel fwiw I do have a CLA signed. I've contributed to other Google projects.\nedit: I changed my GitHub email\u2014I think the CLA is signed with ericscottlagergren@gmail.com. Me neither, I can't imagine why or how I did that.\nOn Thu, Jan 26, 2017 at 8:48 PM Robert notifications@github.com wrote:\n\nI don't understand why these files are in the root of the project?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/3991#issuecomment-275588389,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AFnwZ-cpdLGDaBwxSPEFGfVLP86zUCA-ks5rWXcwgaJpZM4Jk4-Y\n.\n.  Not sure why git got a little screwy there. . \n",
    "WebFreak001": "I can try, D has a lot in common with C++ and C#\n. Its pretty complicated to write the D compiler for flatc because there is no real wiki page/documentation on how to create a new language thats like C# or Java. I tried just duplicating the C# element from the array from the top but that didnt seem to work. The D library for it is done though, just didnt test it yet because of flatc.\n. Instead of the huge reverseBytes functions here: https://github.com/ThomasBrixLarsen/flatbuffers/blob/master/d/source/flatbuffers.d#L586\nI would use std.bitmanip.swapEndian which has exactly the same function content but it is more clean.\n. The D programming language has unittests already built in, so you can just do\nD\nunittest {\n    // load flatbuffer\n    assert(monster.hp == 5);\n}\n. @gwvo actually the tests would break for windows users because of case insensitive folders. Right now his implementation generates to mygame/examples/ instead of MyGame/Examples/\nWhile this actually produces two completely different folders on case sensitive filesystems, it will probably just put them in the same folder on case insensitive ones. This is no issue if you specify the path of every generated D file in the command line call, but with package managers/build systems like dub it will just add the root folder of it as import path. So basically you can use import foo.bar; and it will try to get foo/bar.d in all import folders or from the specified files.\nHowever there is a difference between import Foo.bar; and import foo.bar; as they try to search in folders with different cases. This is no problem for case insensitive filesystems but it will fail to find the file on case sensitive ones if one does not exist.\nSo one solution would be putting the example in a separate folder or alternatively just removing the toLower method from the generate function. Doing the latter could make the generated code not comply with the phobos (D stdlib) naming standard but it would give the user more freedom and it would result in more expected names.\nimo just removing the toLower calls would be better as it leaves the user with more freedom and some people actually name their modules like that. It also just works then and fixes the samples\n. its still missing tests for nested namespaces & nested buffers though\n. shouldn't this be just byte instead of ubyte? (byte = int8 in D)\n. Either this should give you a message \"must not be zero\" or check for <= 0\nThe Java implementation just sets it to 1 instead of throwing an exception\n. any reason for not using std.traits.isNumeric!T || is(T == bool) instead of all these is(T==type) checks?\n. you need some /// ditto here and on some other places to apply the documentation to all the members\n. it comes from the defintion in the flatbuffers file. It should not force it to D style as some people prefer uppercase enum members\n. ",
    "ThomasBrixLarsen": "I have a D fork: https://github.com/ThomasBrixLarsen/flatbuffers\nThe implementation is made inside the C#/Java code generator. It isn't pretty but it generates working D code.\n. I have removed a lot of the kD checks and the reverseBytes methods. I don't think anything is missing, but the code should probably be changed to use ranges at some point.\nI can look into adding the C# tests as unittests and converting the samples.\n. I haven't worked on this code in a while. The project I was using it with has been put on hold. I don't expect to work on the project again for at least a couple of months.\nI remember I had some problems with generation of files for namespaces. But the rest seemed to work well.\n. Enum members should be lower case according to the D style guide.\n. Consider running dfmt on all the D code.\n. ",
    "zoujiaqing": "Can use it?\n. @gwvo merge pull request?\n. Of course, because this is an open source project, we want more participants to support flatbuffers.\nI am very sorry that my team is very busy recently :(\n. ",
    "splhack": "I added a test and the type of EnumNames is now map[int]string.\n. Thanks! rebased.\n. @rw The build issue seems like the same as https://travis-ci.org/google/flatbuffers/jobs/146261105\nCXX_FLAGS has unintentional line break\n```\n$ cat CMakeFiles/flatbuffers.dir/flags.make\nCMAKE generated file: DO NOT EDIT!\nGenerated by \"Unix Makefiles\" Generator, CMake Version 3.3\ncompile CXX with /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++\nCXX_FLAGS =  -std=c++11 -stdlib=libc++ -Wall -pedantic -Werror\n                        -Wextra -g -I/Users/sakamoto/Source/go/src/github.com/splhack/flatbuffers/include -I/Users/sakamoto/Source/go/src/github.com/splhack/flatbuffers/grpc -I/Users/sakamoto/Source/go/src/github.com/splhack/flatbuffers/tests -I/Users/sakamoto/Source/go/src/github.com/splhack/flatbuffers/samples\nCXX_DEFINES =\n```\n. thanks! rebased.\n. \ud83d\udc4d \n. ",
    "googlebot": "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\nGooglers can find more info about SignCLA and this PR by following this link.\n need_author_cla \n. CLAs look good, thanks!\nGooglers can find more info about SignCLA and this PR by following this link.\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\nGooglers can find more info about SignCLA and this PR by following this link.\n need_author_cla \n. CLAs look good, thanks!\nGooglers can find more info about SignCLA and this PR by following this link.\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\nGooglers can find more info about SignCLA and this PR by following this link.\n need_sender_cla \n. CLAs look good, thanks!\nGooglers can find more info about SignCLA and this PR by following this link.\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\nGooglers can find more info about SignCLA and this PR by following this link.\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n\u2139\ufe0f Googlers: Go here for more info.\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\u2139\ufe0f Googlers: Go here for more info.\n need_author_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n\u2139\ufe0f Googlers: Go here for more info.\n need_sender_cla \n. CLAs look good, thanks!\n\u2139\ufe0f Googlers: Go here for more info.\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n\u2139\ufe0f Googlers: Go here for more info.\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\u2139\ufe0f Googlers: Go here for more info.\n need_author_cla \n. CLAs look good, thanks!\n\u2139\ufe0f Googlers: Go here for more info.\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n\u2139\ufe0f Googlers: Go here for more info.\n need_sender_cla \n. CLAs look good, thanks!\n\u2139\ufe0f Googlers: Go here for more info.\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n\u2139\ufe0f Googlers: Go here for more info.\n need_author_consent \n. ",
    "chenleijava": "en ha  . support  /* xxx */ style comments will be better. float value=3.14149 is error.   value =3.14149f right. in java \n. ",
    "vreid": "I didn't know if I had to open an issue before submitting the pull request; if opening the issue wasn't necessary I'm sorry for doing that. Fairly new at this.\n. I'm pretty sure the Windows include was somewhere in the boost headers in the project that I stumbled upon this.\nThere surely is a way to get boost to include it differently that I don't know of (yet).\nBut isn't it a good idea to make flatbuffers.h more resilient against this case? Maybe someone can't use \"#define NOMINMAX\" because some other part of the code depends on those macros?\n. I don't have access to the codebase, which prompted me to make this request, until the 15th so I can't make any valid arguments at the moment.\nI'm pretty sure the windows.h use wasn't coming from the company includes and the only other thing we are using is boost and flatbuffers (evaluating).\nI agree with your reasoning though, using std::min/max without parentheses is perfectly valid C++ code; it's the use of the windows.h that messes things up.\n. ",
    "amoldeshpande": "ok, I've made the fixes you pointed out. thanks.\n. sorry about the merges. still trying to figure out git merging.\n. ",
    "aeneid": "jesta88, are you working on this one? If not, I might pick it up myself\n. By the way, is there any particular reason as to why Table mutators return boolean while Struct mutators return void in the C++ generated code?\n. Thanks!\n. I had a feeling that was the right thing to do. Anyhow, done.\nAbout it behaving differently, I agree it's strange but I've always said C is a fickle beast.\n. This issue seems to be affecting the Python & Go generated classes as well.\nIs there any reason why the generate_code.sh script does not generate python test code?\n. I think i have the Python and Go class generators nailed as well. Thing is I'm a little stuck trying to run their tests on my Windows machine (You only have a .bat file for the Java tests). Any suggestions?\n. All of the C# ByteBuffer absolute put methods would change the buffer's position, which is not a behavior you would expect (Indeed it does not occur in Java's ByteBuffer).\nFor example:\nhttps://github.com/google/flatbuffers/blob/master/net/FlatBuffers/ByteBuffer.cs#L140-L145\nThat method would be used every time one would mutate a byte field resulting in the buffer's position being advanced.\nThe way I see it, once you call X.FinishXBuffer the buffer's position isn't suppose to change as it should point to the beginning of that buffer's vtable position.\n. This wasn't intentional. I believe simply removing the change in buffer position (as in the safe case) will do the trick. I'll have to run the tests at home just to make sure.\n. I just noticed I did not add support for vector mutation. I think I should get that done before you merge this PR\n. I've added vector mutation and updated JavaUsage.md\n. Done.\n. I actually fixed this locally and then spent a few hours trying to cope with all the required casting to make it work. It's wiser to leave it as-is.\nI will, however, leave a comment saying that it is intentional.\n. DestinationCast sort of does the opposite of what SourceCast does. Basically it upcasts unsigned types so that the correct value will be returned via the accessor. What SourceCast does is the opposite. It downcasts parameters representing unsigned integers before writing them onto the buffer.\nI could achieve this with DestinationCast if I added some boolean parameter determining whether or not it should downcast or upcast but I didn't think that would be any better.\nNonetheless, I'm totally open to suggestions on this one.\n. ",
    "evanw": "It looks like memory growth via reserved_ += std::max(len, growth_policy(reserved_)); in vector_downward::make_space aligns inside growth_policy instead of outside std::max. This means the current offset is aligned relative to the end of the buffer but is not relative to the start of the allocation, which is what's causing the undefined behavior. Is this intentional?\n. I've got a reduced test case demonstrating undefined behavior: https://gist.github.com/evanw/bb945599bfe974955299.\n. Thanks! Good questions.\nThe unicode implementation comes from the library of a programming language I'm developing (code here, tests here). I was surprised that the flatbuffers project doesn't have unicode tests and was planning on adding some.\nI'm not sure what you mean about using bit-twiddling instead of DataView. I'm using DataView for all ByteBuffer data accesses. Unicode involves encoding and decoding logic since it's a variable-length encoding and DataView doesn't have support for unicode, so that code has to combine use of DataView with bit operations.\nBesides optimizations, I have found Google Closure Compiler really helpful for ensuring that I'm using the generated code correctly (it does type checking and validates argument counts among other things).\nWhat do you think @evolutional? I'm happy to help figure this out.\n. Hmm, my unicode encoding and decoding routines check out fine but it actually looks like the current flatc binary doesn't handle unicode correctly. It doesn't pass my tests for unicode characters that need two code units to represent in UTF-16. The JSON input below was converted to binary with flatc and the monster_test.fbs schema, then converted back to JSON using the --strict-json flag.\nInput: { \"name\": \"\ud844\uddd9\ud834\udf06\" }\nExpected: { \"name\": \"\\uD844\\uDDD9\\uD834\\uDF06\" }\nObserved: { \"name\": \"\\xF0\\xA1\\x87\\x99\\xF0\\x9D\\x8C\\x86\" } (not valid UTF-16, not valid JSON either)\nAlso, thanks for the pointer to Lobster. Very interesting, especially the type inference.\n. I just ported the low-level fuzzer code from C++. It found an issue where writing 64-bit integers was done with the wrong endianness, which is now fixed.\n. The JSON format (http://json.org/) doesn't specify an \"\\xNN\" escape sequence, so JSON parsers that adhere to the spec will correctly reject the generated pseudo-JSON as invalid. Only the \"\\uNNNN\" escape sequence is allowed. This isn't an issue for me since I only need the psuedo-JSON output for debugging purposes. I just pointed it out because it's not actually generating valid JSON data, which is what I would expect with a flag called --strict-json.\nI think collapsing both getters into one leads to a better API and shouldn't have a significant performance impact. The generated code doesn't access the \"arguments\" value to check for argument presence, which is the only thing that could cause a noticeable performance impact here as far as I know.\n. That's true. Care would have to be taken about endianness though in case someone tries to run it on an obscure platform.\n. I'm definitely willing to help but I thought it was already in good shape, sorry. It sounds like you'd like it to be rewritten to avoid using DataView. I'll do that now.\n. Ok, done. I was able to get away with the same code path for both big and little endian platforms except for 64-bit float handling, where I think separate code paths are probably required. I'm pretty sure I did it right but I don't have a big endian platform to test on, so someone please review the code first.\n. Huh, that's interesting. I haven't tried node 4x yet but I can't imagine a reason why it would behave differently. I'll send you an email when I get home.\n. I agree that it's nice to be able to know if a field is present or not. However, that doesn't appear to be how the flatbuffers spec is designed, so I think the generated JavaScript code is working as intended.\nI'm looking at http://google.github.io/flatbuffers/md__schemas.html and the closest thing I can find is \"non-scalar (string/vector/table) fields default to NULL when not present\" although I can't find anything about scalar values defaulting to 0 when not present. Maybe @gwvo can clarify? I imagine cross-language consistency would be the desired behavior here.\nIn our project (C++ code), we solved this problem by using invalid sentinel values as defaults for scalar fields: 255 for bools and enums, INT_MIN for ints, and FLT_MIN for floats.\n. Browser environments don't have the \"module\" variable so this pull request will cause the generated code to crash on initialization when used directly in a browser.\n. Oh whoops sorry, just saw this. This looks better. Why use window instead of this though? Doesn't this work in more JavaScript environments instead of just in the browser? For example, V8 has a shell called \"d8\" that doesn't define window. Other than that it looks good.\n. Yes, this is an issue with packaging. Right now the generated code assumes dependencies have been figured out already. I would prefer to leave the generated code independent of flatbuffers.js to preserve flexibility. That way it will automatically work in multiple environments and without a package manager. I'll fix it now.\n. I would prefer to have the compiler keep my enum values and not emit additional MIN and MAX values when that would cause compile errors. For now I renamed them MIN_ and MAX_ in our code to get it compiling again and move on. However, having the automatically-generated enum values there is now a recipe for bugs. I could easily see someone using the wrong ones and I could also see that not getting caught in code review. It seems better for the compiler to just not emit them if they conflict with user-generated values. What do you think?\n. There are examples in https://github.com/google/flatbuffers/blob/master/tests/JavaScriptTest.js. To serialize a flatbuffer:\nvar fbb = new flatbuffers.Builder();\n// ... build the flatbuffer ...\nvar uint8Array = fbb.asUint8Array();\nTo deserialize a flatbuffer:\nvar bb = new flatbuffers.ByteBuffer(uint8Array);\nvar drinks = MyAlcoholist.Drinks.getRootAsDrinks(bb);\nIf you're using node and need a Buffer object, it should be trivial to convert between Uint8Array and Buffer. Is that what you were looking for?\n. It's totally possible to convert from js objects to flatbuffers and back because flatbuffers can be converted to JSON and all JSON is valid JavaScript. However, right now the only way to generate JSON is using the command-line app.\n@gwvo, do you have any plans for adding JSON serialization as an option to the code generators for different language targets? Right now the only way to do this is by cross-compiling the command-line app using emscripten. I assume that works but I haven't tried it.\nThe other option would be to create a script that could consume a schema and convert back and forth to and from JSON. I just saw that flatbuffers now has a reflection schema so this seems pretty easy to do.\n. Yup that's what I meant by \"create a script that could consume a schema and convert back and forth to and from JSON\". I also just realized that since JavaScript uses a JIT, you could write a JavaScript library that generates efficient FlatBuffer code on the fly given a schema. Then everything could be done in JavaScript and there doesn't have to be a dependency on C++ at all. This would also help us with some stuff we're doing so I'll try to write a prototype tonight as an experiment.\n. @gwvo I almost got something working but then I ran into a problem. The format in reflection.fbs doesn't encode the per-field padding value but that value is required to be able to generate buffers containing structs. Is there a reason why the value was omitted? Or is it something that should be added?\n. Actually never mind, I can hack around it: fields[i].padding = fields[i + 1].offset - fields[i].offset - fields[i].inline_size.\n. I have an initial version of the library I wanted: https://github.com/evanw/node-flatbuffers/. It converts between FlatBuffers and plain JavaScript objects. I made it because it's useful for what I'm currently working on. We're experimenting with lots of schema changes and writing all of that serialization code by hand every time was a huge pain. Other people have brought up the verbosity of the FlatBuffers API and this library can help with that too.\nAlthough it trades off performance for ease of use, it's probably still reasonably efficient. Building JSON on the fly is slow in other languages but in JavaScript the JIT often uses hidden class transitions to generate very efficient code. There also isn't always extra GC overhead (your problem may require you to construct objects in memory anyway, this just does it for you).\n@darklinden I'm not sure what you're asking. Does this library solve what you were trying to do? Note: the library I linked to is brand new and while it appears to be working, there may still be an issue or two that I haven't encountered yet.\n. @gwvo We're actually moving off of flatbuffers so I'm not interested in spending the time to contribute a JavaScript-only API at this time. I think https://github.com/evanw/node-flatbuffers/ is a great start and it's been working fine for us (we've been using it at Figma). Right now that package is using the \"flatbuffers\" npm name but I'm happy to give control of that name to you if you'd like. Just let me know.\n. Fixed, sorry about that. I didn't put \"https://\" before it and GitHub thought it was a relative URL for some reason. The correct URL is of course https://github.com/evanw/kiwi.\n. The binary layout hasn't been documented yet unfortunately. I've been meaning to do that since you asked but I haven't found the time so far. There are only a few primitive types (byte, int, float, and string) and the serialization routines are very simple so it should be pretty easy to understand the details of the format from the code. Here's the simple version in the meantime:\n- Variable-length integers use LEB128 just like Protocol Buffers, including the part where the sign bit is the least significant bit for signed integers. 32-bit integers can take up to 5 bytes to encode this way. Kiwi doesn't support 64-bit integers because JavaScript doesn't support them.\n- Floats are stored as 32-bit values but their bits are rotated so the 8-bit exponent comes first. If the exponent is zero (+0, -0, and denormalized numbers), the other 24 bits (the sign and mantissa) aren't encoded. This means all floats encode to 4 bytes except for 0, which takes up 1 byte.\n- Strings are stored using a null-terminated UTF-8 encoding.\n- Arrays are stored as a single variable-length unsigned integer length followed by that many elements.\n- Structs are stored by writing out the value for each field in order, with no other metadata.\n- Messages are stored as a sequence of (field ID, value) pairs terminated by a single null byte. Each field ID is stored as a variable-length unsigned integer.\nThere's a live demo at http://evanw.github.io/kiwi/ where you can see which bytes get produced for different data types.\n. Yeah, like I said before I'm happy to transfer the name. Just let me know which npm account to transfer it to.\n. @aardappel: done!. It's kind of a bad idea to npm unpublish because that will break legitimate packages that other people have written which use the unpublished package and are otherwise working fine. See this for npm's philosophy on this. I would recommend just publishing a new major version and leaving the old published packages as-is. This should be fine, right?\n. It doesn't happen for me but I could imagine the use case and it would be easy to add. I think the generated code should still return JavaScript strings though since that's what everyone will expect. I'll add generation of parallel getters and setters for UTF-8. I'm thinking name/addName for JavaScript strings and rawName/addRawName for Uint8Arrays representing the raw data from the FlatBuffer.\n. I definitely don't think it makes sense to return a double here. I've encountered a nasty issue in production before where a ruby server was using 64-bit integer IDs that were fully populated for sharding purposes and communicating that to a node server over JSON using 64-bit doubles. It was very painful to figure out exactly what was going on and I don't think it would be a good idea to encourage that. I've added flatbuffers.Long.prototype.toFloat64() for this purpose but it has to be an explicit call. I'd probably use something like goog.math.Long normally but I think avoiding dependencies here is more valuable.\n. No that is not. Good catch, thanks. I'll fix it soon.\n. The type checking that Google Closure Compiler provides was very useful for me in porting the library to JavaScript and having extra confidence that things were wired up correctly. It caught quite a few issues around boolean <=> integer conversions, for example. That said, now that the library is complete I guess it's ok to remove them if you want. It's sort of like removing tests: the tests have already provided their usefulness and it's fine to remove them as long as you don't need to make modifications later on. It's up to you, so I'll wait to hear what you decide.\n. Whoops I didn't answer your other question. I've only seen 64-bit numbers used for identifiers, in which case this makes perfect sense. The user can read and write them so they can transfer them to and from FlatBuffers, they can compare them with flatbuffers.Long.prototype.equals(), and they can get the low and high integers and use them for whatever they need. For example, users could map longs to objects using the string long.low + ':' + long.high as the key and a JavaScript object as the map.\n. Actually, wait. I think this is fine. FlatBuffers is always little-endian and the flatbuffers.Long object isn't endian-specific since it's a JavaScript object. Both readInt64() and writeInt64() read and write the low and high words in little-endian as well and use readInt32() and writeInt32() which should do the right thing on big-endian platforms. I don't see how a big-endian platform could behave differently here. I really wish I had a big-endian platform to test on though because I'm second guessing myself. Does that sound right?\n. I just added support for this. I ended up going with an optional encoding argument to the string getters instead of separate methods because it's simpler and generates less code. Getters return strings by default but return Uint8Array slices instead when in UTF-8 mode. The createString() function accepts either strings or Uint8Arrays now and treats Uint8Arrays as encoded UTF-8 data. See commit 4bef5e8 for details.\n. ",
    "TGIshib": "Is it okay to make 4 generated functions in each class (KeyCompareWithValue,KeyCompareLessThan,CreateMyTableVector, LookupByKey), or it would be better to make 3 generated functions (KeyCompareWithValue,KeyCompareLessThan,CreateMyTableVector) and templated LookupByKey? In the second case KeyCompareWithValue,KeyCompareLessThan virtual functions will be added to the Table class and the functions generator will be a bit more complicated.\n. Is it better to take the key field data for comparison directly from the buffer, than to do something like this?\nArray.Sort(... => new Monster().__init(...).KeyCompareLessThan(new Monster().__init(...\n. I'll think about lookups without unpacking. Now I'm almost done with searching in the ByteBuffer and there is one thing: I need __offset and __string functions for accessing fields in the buffer array, so I had to clone them from the Table class into FlatBufferBuilder or ByteBuffer. Unfortunately in Java __string function is rather big and this duplication looks quite bad.\n. Yes, I have tried to remake this in accordance with all remarks.\n. Thank you!\n. Will add docs soon!\n. Fixed typo, added checking before appending byte array.\n. Here\n. Seems so.\n. Hello,\nfield(String key) will work correctly only when our vector of Objects will be a field of Object, because in the other cases there will be another constant at __offset(6). Not sure that it is a good idea, but maybe we could store offsets of the vectors of tables with the key field somewhere?\n. Sorry, I hadn't spotted that. It looks good, but why do we need - 4 here?\n. Thanks, I get it.\n. I will correct.\n. Ok.\n. Will be done after the end of exams.. I use ByteBuffer to get the key field while sorting, but in Java I can't access it before the buffer is finished. Through this method I can get ByteBuffer for createMySortedTableVector.\n. This will be done in C#. But may be it would be better to leave it in Java?\n. Not sure I know how to implement this. \u0441reateSortedTableVector should be static, so I can't override it or get the ByteBuffer.\n. In Java compareTo doesn't work with primitives. I can use compare method of the corresponding wrapper class or make the comparison in the lambda expression, if it would be better.\n. The only thing I can offer is to use stream, but not sure it would be better.\n. Then it will be necessary to create a Monster object while building the buffer just to create a vector. If it is okay, I can implement this.\n. So lookupByKey should get a vector offset as an argument?\npublic static Monster lookupByKey(int vectorOffset, String key)\n. We should compare only a segment of the bb.array() , so I can get it before the loop in lookupByKey, pass it to compareStrings as an argument and then use Arrays.equals. Either I can get this segment  in compareStrings, but I don't think it is a good idea. Or maybe would be better to leave the comparison without Arrays.equals?\n. When the Go version will be released, absence of this check will cause errors. Remove it?\n. Oh, yes, dumb checking. Sorry for that.\n. Done.. Done.. I have verified. Works for all key types except for boolean.. This checks if the the key field is default. This is similar to other version of __offset, but without vtableOffset < bb.GetShort(vtable) check since key field always presents.\nThis is similar to other version of __offset + check in the generated field getter.\nDoes vtable that is pointing to the start of the object could be in the vector of tables?. ",
    "FrankStain": "Yep. Submitted code is only small fix that solve only described mistakes.\nWhile inspecting 'growByteBuffer' function i found that buffer size can be easily inflated over 2Gb without throwing exceptions. Thinking about fix that mistakes and submit more useful pull request.\n. Yep, it's fixed. I already had test it in my code.\n. @rw Great!\nBTW, what about #234 now?\n. @gwvo It's simple.\nAmalgamated header is pretty simple to include in project. Also no needs to additional includes when new file format generated.\nToday a lot of libraries distributed only in amalgamated state: single header + single source file.\nWhen you have 1-3 .fbs files, it's not so important how much generated headers you have, how much files you have to include into build pipeline for every platform you develop and how much files you forget to include. But if count of .fbs files becomes above 20, count of generated files becomes important and amalgamation becomes great feature.\nAlso, future amalgamation becomes more easy if some sources already amalgamated.\nI don't wish combine my .fbs files, every separate format is complex and it will be unable to support if all formats will be in one file.\nExternal amalgamation currently used as additional pre-build step in build pipeline for every platforms. It's additional script, that is complex too. And i always think about what i need to do if this script fails amalgamation in one day.\n. @gwvo Sry, i can't in this time. I'd be rather suggest this PR, if could, instead of asking feature. :)\n. UPD:\nhttps://github.com/google/flatbuffers/blob/master/src/flatc.cpp#L271\n.cpp\nopts.lang = generators[i].lang;\nAll looks like here must be another code, smth like\n.cpp\nparser->opts.lang = generators[i].lang;\n. @lucmichalski Hi!\nFlatbuffers is designed as flexible file formatting library, it strongly oriented on files and zero memory consumption when accessing data from such files. It's not recommended to be used in network interaction (my thoughts), preferring to using of protobuf for that purposes.\nSo for transfering the data protobuf will be more useful. But what about converting raw data from OpenCV (and to), Flatbuffers may become useful.\nIf you need translate some raw data (byte stream) from OpenCV into protobuf, you may firstly just map this data into Flatbuffers file format and then translate it into protobuf structures.\n. @gwvo Sry, it's only my thoughts about purposes of using protobuf and flatbuffers. Seems i need look on it widely. :)\n. @aardappel , hi there! :)\nWhile working on my wrapper over Android JNI i had a lot of hard work with its Get###Field, Call###Method, New###Array etc. Since my wrapper is strongly templated, i had forced to wrap all such functions of JNIEnv in its templates.\nSo, it would be great if new union accessors will be automatically templated by compiler or duplicated with templates.\nSmth like this:\n```.cpp\nUnionContentType union_name_type() const ...;\nNestedA union_name_NestedA() const ...;\nNestedB union_name_NestedB() const ...;\nNestedC union_name_NestedC() const ...;\n// with static instantiations to corresponded \"union_name_TNestedType\" functions.\ntemplate< typename TNestedType >\nTNestedType union_name() const;\n```\nCurrently i had already made some templates to make accessing the content of unions more careful and safe. But i look into idl_gen_cpp.cpp and think it's not so easy to implement my decision at level of compiler.\nI can try to implement this stuff with some help.. @aardappel \n\nWe can't call the templated version union_name since that is already used for the current accessor, which we have to keep for backwards compatibility.\n\nYep, i forgot about it. Also i already thought about better name for such template and union_name_as looks more appropriate. How it sounds for you?. @aardappel \nSo, finally i had found the time for this feature. Sorry for my huge lateness.. ```\nunion Foo\n{\n    A,\n    B,\n    C,\n}\ntable Bar\n{\n    u:  Foo;\n}\n```\nSo, for this fbs schema, the C++ generator will produce next C++ code.\n.cpp\n  Foo u_type() const {\n    return static_cast<Foo>(GetField<uint8_t>(VT_U_TYPE, 0));\n  }\n  const void *u() const {\n    return GetPointer<const void *>(VT_U);\n  }\n  template<typename T> const T *u_as() const;\n  const A *u_as_A() const {\n    return (u_type() == Foo::A)? static_cast<const A *>(u()) : nullptr;\n  }\n  template<> const A *u_as<A>() const {\n    return u_as_A();\n  }\n  const B *u_as_B() const {\n    return (u_type() == Foo::B)? static_cast<const B *>(u()) : nullptr;\n  }\n  template<> const B *u_as<B>() const {\n    return u_as_B();\n  }\n  const C *u_as_C() const {\n    return (u_type() == Foo::C)? static_cast<const C *>(u()) : nullptr;\n  }\n  template<> const C *u_as<C>() const {\n    return u_as_C();\n  }\nBut names of additional accessors will become more complex once the union contains types from different namespaces. Smth like next code.\n.cpp\n  const fbs::test::a::A *u_as_fbs_test_a_A() const {\n    return (u_type() == fbs::test::b::Foo::fbs_test_a_A)? static_cast<const fbs::test::a::A *>(u()) : nullptr;\n  }\n  template<> const fbs::test::a::A *u_as<fbs::test::a::A>() const {\n    return u_as_fbs_test_a_A();\n  }\n  const fbs::test::a::B *u_as_fbs_test_a_B() const {\n    return (u_type() == fbs::test::b::Foo::fbs_test_a_B)? static_cast<const fbs::test::a::B *>(u()) : nullptr;\n  }\n  template<> const fbs::test::a::B *u_as<fbs::test::a::B>() const {\n    return u_as_fbs_test_a_B();\n  }\n  const fbs::test::a::C *u_as_fbs_test_a_C() const {\n    return (u_type() == fbs::test::b::Foo::fbs_test_a_C)? static_cast<const fbs::test::a::C *>(u()) : nullptr;\n  }\n  template<> const fbs::test::a::C *u_as<fbs::test::a::C>() const {\n    return u_as_fbs_test_a_C();\n  }\nAny suggestions?. @Olipro , latest master already contains this feature. Take a look on it.. CLA Is signed.. @aardappel , glad to work with you. ). In fact, uint8_t is used currently to store the type of union value.\nhttps://github.com/google/flatbuffers/blob/bb223da2583dc2a9ec005bae6ade265197fb96f5/tests/monster_test_generated.h#L578\n@slavslavov , can you confirm that no warning/error was produced by 'flatc' while parsing your schema?. Had missed this check. Tnx for help!\n. My bad. According to next note, this line removed.. Yep. Fixed.. Generated code included into PR.. ",
    "Hinidu": "ping @rw\nWhat's the status of this PR? Why is it still not merged in?\n. @rw thank you for merging! Do you have any estimates when we will have flatbuffers package with this fix on pypi?\n. @rw thank you again!\n. +1 we found the same problem today.\n. This change and the one above lead to the incorrect behavior: we can call growByteBuffer as many times as we want and it will never raise BuilderSizeError because newSize can't become more than Builder.MAX_BUFFER_SIZE.\nI suggest to fix it that way:\n``` python\nif len(self.Bytes) == Builder.MAX_BUFFER_SIZE:\n    msg = \"flatbuffers: cannot grow buffer beyond 2 gigabytes\"\n    raise BuilderSizeError(msg)\nnewSize = min(len(self.Bytes) * 2, Builder.MAX_BUFFER_SIZE)\n```\nIt ensures that the last growing will make buffer as big as possible and that succeeding calls to growByteBuffer will raise an exception.\n. ",
    "martell": "Hi :)\nI based it from what cocos2d-x was using in its game engine\nhttps://github.com/cocos2d/cocos2d-x-3rd-party-libs-bin/blob/v3/flatbuffers/CMakeLists.txt\nI didn't actually look at the sources when I made it heh.\nI can move them into flatc_sources if you want ?\nThanks for taking the time to look also :)\n. done :)\n. Yeah I realized after looking at it which is why I removed the comment :)\n. ",
    "Antoine-Lassauzay": "I would like to help with this. There should be some planning to do ahead. @fortegames what haxe platforms do you think should be prioritized (if not all) ?\n. @gwvo You are right. To be precise I should have said which Haxe \"targets\", given that the language compiles to JS, CPP, Flash, Python, etc. each of which can be executed on a variety of platforms. On the one hand Haxe guarantees consistent behaviour on all targets, but given the nature of FlatBuffers I would expect some challenges along the way.\n. I have a unit test that passes on at least 6 haxe targets (CPP, JS, Neko, Flash, Python3, PHP) and I assume would work elsewhere (haven't tried). Specifically, I implemented the Struct/Table reading (from the test file test_data.mon), now heading to the Builder implementation and finally the code generation part.\nMaking slow progress on this as it's not work-related, but I should get there soon ! \n. Hi, yes. I completely abandoned this since I started my new job but nevertheless I pushed it on Github.\nhttps://github.com/Antoine-Lassauzay/flatbuffers/commit/ea16422c01e18a4d5239dd5ae27e1f99aa0721f5\nIt contains base classes for Table, Structs and tests ported from C#. What would be missing I think is the code generator, better enum support and endianess checks.\n. ",
    "vnedilko": "Hi, @Antoine-Lassauzay It is amazing if you can share current version of flatbuffers for haxe. Can you do it?\n. ",
    "troyedwardsjr": "If anyone is still looking for this:\nI've started a port for FlatBuffers in Haxe, with a compiler written in Haxe, and recently released the 1.0 version.\nhttps://github.com/troyedwardsjr/flatbuffers-haxe. ",
    "jdonaldson": "Another interested haxe developer chiming in.  I'm actually more interested in Arrow, which will rely on flatbuffers to a certain degree.\nHaxe can support the fbs format by externing existing platform code, or providing its own implementation.  Both approaches are equally useful imho.  Haxe also has a really nice travis testing library which helps keep on top of the dozen or so targets that haxe supports.\n. ",
    "RevenantX": "\nDid you run generate code for Java to ensure nothing has changed there?\n\nYes. In Java nothing changed.\n\nDid you run any performance tests to see if the small structs are close to the speed of ints? They should be, but it be nice to know for sure.\n\nNo. Maybe speed difference exists. But very liitle.\n. Maybe better create overloaded function, than replace existing? For me in many cases - byte[] more comfortable than IList\n. You can write something like:\nvar recipeOffset = Example.Recipe.EndRecipe(fbb);\nExample.Recipe.FinishRecipeBuffer(fbb, off);\n. ",
    "belm0": "By the way, not clear to me why tests/monster_test.bfbs gets modified when tests/generate_code.sh is run.\n. Hi-- I'm not sure I understand your point about overload being better. An IList function parameter would allow your byte[], in addition to List etc.\nUsing overload also requires an uncomfortable choice:  would the argument be IList, which would be redundant with the existing array function, or List, which would be bad practice since it's expecting a specific collection implementation.\n. Indeed, IList looks a bit slower.\nDoArray took 0.0219289 seconds.\nDoList took 0.0455862 seconds.\nDoIList took 0.1182848 seconds.\nhttp://volatileread.com/utilitylibrary/snippetcompiler?id=26939\n. ",
    "belldon": "It looks like this wasn't fixed for any of the \"unsafe\" put methods (PutInt, PutLong, etc.).  Was this deliberate?  As is, the C# mutator tests fail when UNSAFE_BYTEBUFFER is defined.\n. Got it, thanks.  I've run the tests with the changes in buffer position removed and everything seemed to work just fine.  I just wanted to check and make sure there wasn't a specific reason the rest of the methods weren't changed.  I'll submit a PR with the changes.\n. I missed that, thanks.\nI'm going to take the opportunity to re-work the fix a little bit.  I think I can consolidate a few of the \"get generated type\" functions and make the whole thing a little easier to understand.\n. Finally done with the fix.  Turned into a slightly bigger job than expected. :)  I made sure to include the updated generated test files.\n. Made the requested changes and re-worked the fix one more time to be closer to the original code.\n. Done.\n. Rebased.\nI hadn't checked the Java version until now, but yes, the Java version does pretty much the same thing.\n. I think I'm OK with it either way, but if we are going to place the builder methods in a separate class, isn't it a more common pattern to have the builder class be an inner class?\n. I think the primary reason for making a builder class an inner class is to give them access to private fields of the containing class.  This is useful when you have objects that require multi-step construction and you don't want to allow users to be able to partially construct objects.\nIf the builder class is a separate class, it can only interact with the class being built through public methods.  This means that the builder class can never really be anything more than a convenience class.\nAll that said, in our case, having the builder class be just a convenience class is perfectly OK.  But, moving the builder methods to another class will forego any functionality that requires private member access.\n. I thought of another way to generate the union members that's more similar to the current method.  It's a tiny bit clunkier and requires another interface, but may be worth it.  I'll merge it in.\n. @mfcollins3 \nDefinitely a lot of change here.\nThe IFieldGroup, ITable, and IStruct intefaces allow the generated structs to have a similar \"inheritance\" hierarchy to the generated classes, which were able to inherit from Table and Struct.  This can be useful in a few cases.  For example, the interfaces make it possible to write generic methods that constrain a method to only work with Table access classes.\nLike you suggest, keeping the Table and Struct classes and changing all the methods to public static would also have worked.  But, it seems the only benefit to this approach is keeping the Table/Struct class names the same (which can sometimes be reason enough).  On the other hand, the generated classes would have to always pass all of their state to these methods and, any semantic relationship between the generated classes and Table/Struct is somewhat lost.  Instead, the TablePos and StructPos classes are an attempt to model the previous relationship using composition (vs inheritance) and interfaces.\nRemoving ByteBuffer from the Table access struct interface is an attempt to keep the publicly exposed interface limited to just Table/Struct accessor properties/methods.  As it stands, there's an implicit restriction on naming any of your schema fields ByteBuffer.  That said, it's very easy to add back if that's the consensus.\n. I'm not sure the interfaces really matter at all when it comes to breaking changes.  By switching to immutable structs, we're guaranteed the following will break in any implementation:\n- Table/Struct inheritance\n- __init method\n- object re-use methods\n- union accessor method, which works via object re-use\n- return values for Table/Struct fields\nThe only optional breaking change, which was deliberate, was changing the accessibility of the ByteBuffer property.  But, this can easily be changed back.\nTo provide a bit more rationale of some of the changes:\nUsing composition with the TablePos and StructPos classes allowed the C# code to continue to share most of the accessor logic with the Java code.  Since the variables (bb and bb_pos) are named the same, all the generator has to do is generate a prefix (this.pos.).  If the existing Table/Struct class methods were changed to public static methods, the C# code would need to diverge further from the Java code. Also, any changes to which state is stored (e.g., if the vtable offset were cached instead of looked up every time), would require changes in the generator and the Table/Struct classes.\nThe interfaces were added because having some sort of hierarchy is generally useful and interfaces are the only way to do that with C# structs.  \nHaving the structs explicitly implement the interfaces is effectively the same thing as prefixing those properties/methods with a double underscore: it was just meant to separate them from the accessor properties and methods and not have to worry about name clashes.\nI'm going to close the PR for the time being.  It seems like a bit more discussion needs to take place before it will be ready to go.\n. Thanks for the feedback.  I have a couple of questions.\nWhen you refer to a struct to hold the \"pointer\", are you referring to the generated struct or a separate struct contained by the generated struct?\nCompatibility with the Java code was one of the main reasons for coming up with the Table/StructPos structures.  But, another reason was consolidating the stored state into a separate structure.  This allows us to potentially change stored state without changing the generator.\nAlso, apparently I'm in the minority on this, but I'm not sure I get why having a separate static class with helper methods is a better approach.  Can anyone explain?\n. Since keeping the C# and Java generated code in sync is no longer a priority, I'm OK with closing this PR.  I think there are definitely other, cleaner ways to do it.\nOf the two options proposed by @evolutional, I throw my support behind the option with the BufferPosition struct. :)\nOne issue to consider, however, is how union accessors are implemented.  To have a single accessor, will require the use of generics and interfaces, which is the approach I settled on.  The alternative, which I started with, is to have an accessor per union member.\n. Shouldn't the preceding call to StartVector() prevent the ByteBuffer from running out of space?\npublic void StartVector(int elemSize, int count, int alignment)\n{\n    NotNested();\n    _vectorNumElems = count;\n    // prepares for write of vector size\n    Prep(sizeof(int), elemSize * count);\n    Prep(alignment, elemSize * count); // Just in case alignment > int.\n}\n. In the .NET FlatBufferBuilder, it looks like the Prep() method reserves space and aligns the buffer.  So, if I'm reading the code correctly, the following call to Prep() in StartVector():\nPrep(sizeof(int), elemSize * count);\nshould reserve enough space in the buffer.  \nSee the code for the .NET version of FlatBufferBuilder.Prep() below.\n``\npublic void Prep(int size, int additionalBytes)\n{\n    // Track the biggest thing we've ever aligned to.\n    if (size > _minAlign)\n        _minAlign = size;\n    // Find the amount of alignment needed such thatsizeis properly\n    // aligned afteradditional_bytes`\n    var alignSize =\n        ((~((int)_bb.Length - _space + additionalBytes)) + 1) &\n        (size - 1);\n    // Reallocate the buffer if needed.\n    while (_space < alignSize + size + additionalBytes)\n    {\n        var oldBufSize = (int)_bb.Length;\n        GrowBuffer();\n        _space += (int)_bb.Length - oldBufSize;\n}\nif (alignSize > 0)\n    Pad(alignSize);\n\n}\n``\n. I think the following call toPrepmade withinStartVector(seeStartVectorimplementation copy/pasted a few posts up) already accounts for the length that's eventually written by thePutIntcall inEndVector`:\nPrep(sizeof(int), elemSize * count);\nI don't think the usage of PutInt within EndVector is a bug.  I'm not convinced the issue, as originally described, can actually occur if StartVector is called and the appropriate Put/Add* methods are called the correct number of times.\n. Right.  I think the cast is only required when going from unsigned to the larger signed type (on the way out of the buffer) to undo any sign extension that may have occurred.\n. From what I can tell, the existing logic for returning all unsigned types smaller than int in Java is to convert to int.  This fix makes the create vector method consistent with the accessors, which were already using int.\n. Yep, the cast definitely isn't required.  On it.  Thanks for the feedback.\n. Got it.  Will fix.\n. Removed.\n. ",
    "xgdgsc": "Why is the documentation page  still mark Simple mutation of Java and C# as WIP ?  And any plans on Python and Nodejs mutation?. It is confusing to see the github version number lower than pypi version. Glad to see this is going to be fixed soon!. I mean the style guide of flatbuffers https://google.github.io/flatbuffers/flatbuffers_guide_writing_schema.html wants namespaces in UpperCamelCase.  \nBut the style section gave me the feeling that the code generator for c++ would be automatically translating to c++ style when I use UpperCamelCase . There could be some room for improvements in the code generator or the doc.\n\n. The scripts looks fine. Have you double checked your PYPI_PASSWORD and PYPI_USERNAME variables?. Like this:\nitem.fbs\nnamespace my.namespace;\ntable Item {\n  item_id: string;\n  item_value: string;\n}\nroot_type Item;\nitemManager.fbs\ninclude \"item.fbs\";\nnamespace my.namespace;\ntable ItemManager {\n  account_id: string;\n  item_list: [Item];\n}\nroot_type ItemManager;\nflatc --cpp --scoped-enums --python --js  -o output/ item.fbs itemManager.fbs\n. I workaround this by using the --gen-all flag for now.. ",
    "pavlospt": "Hello. Just a simple question. With this JS support will we be able to send data through an endpoint in FlatBuffer binary format or just use it internally on Node ?\n. The result in its String representation from bytes, is the same as the JSON I posted. Do I need to do something else server side?\n. Result is a byte array that in its String representation is equal to the JSON I posted.\n. Do I need to convert the byte array in another specific FlatBuffer format ?\n. \u039f\u03ba so I have to parse the byte array I receive from server through a FlatBufferBuilder in order for it to work ? The use case I am into is like the following:i  receive the bytes from server and then wrap it on the ByteBuffer \n. So there is no case like receiving the bytes from server and converting it on the runtime inside the application ? For the 1st case you mean that the server would run the ./flatc -b command and then send the file to the client?\n. Hmmm I don't see any support for JavaScript and therefore for Node.js so I guess that this is not possible at all. The only way around is to have the server produce the binary after having constructed the JSON. \n. Yes thanks a lot I will check it. Hope we would be able to produce an NPM package for this case. \n. ",
    "bman654": "FYI accessing data in an ArrayBuffer via DataView is significantly slower than accessing via a TypedArray, though TypedArrays must be aligned correctly, while DataViews needn't be.\nIf the flatbuffer data is always aligned (I think it is?), then having code which checks the current environment for little-endianness and uses TypedArrays instead of DataView will make it alot faster...\nhttp://jsperf.com/dataview-vs-typed-array-views\n. ",
    "mnmtanish": "Hi @rw,\nI just wrote some tests in Go. Please let me know if I missed anything. I also wanted to ask, shall I make setters return an error if it fails? like when trying to mutate a field which is not available in the table (e.g. Mana field).\n. Hi @rw and @gwvo,\nI've modified the code to make it work like https://github.com/google/flatbuffers/commit/d06b2736aad6190f18a9c267cb1e48b10f2e03f9. Please let me know if there's anything else to do.\n. I've made those changes. I'll write some docs tomorrow.\n. @rw Build fails on OSX because of some issues with the .travis.yml file. I've opened another pull request trying to fix them https://github.com/google/flatbuffers/pull/266 but it's not ready.\n@gwvo I'm haivng some issues installing biicode on OSX. I'll update it there if I got it to work.\n. @gwvo Sorry about the confusion. The biicode issue was about https://github.com/google/flatbuffers/pull/266.\nI just rebased this on top of https://github.com/google/flatbuffers/pull/266 to run tests on travis.\n. Hi @gwvo, can you let me know if there's anything else.\n. Can't write unit tests for some fields with current monster_data.fbs. I think I can cover all Mutate functions if it's okay to add one or more fields to the Monster table here.\n```\ntable Monster {\n  pos:Vec3 (id: 0);\n  ...\n  testmutablestruct:MutableStruct (id:24);\n  testmutabletable:MutableTable (id:25);\n}\nstruct MutableStruct {\n  bool\n  byte\n  ...\n}\ntable MutableTable {\n  bool\n  byte\n  ...\n}\n``\n. @rw sorry I was a bit busy last few days. I'll try to finish it this weekend.\nCan you help me with this one, how to create aTable` from a builder?\n```\nb = flatbuffers.NewBuilder(0)\nb.StartObject(12)\nb.PrependBoolSlot(0, true, false)\nb.PrependByteSlot(1, 1, 0)\nb.PrependUint8Slot(2, 1, 0)\nb.PrependUint16Slot(3, 1, 0)\nb.PrependUint32Slot(4, 1, 0)\nb.PrependUint64Slot(5, 1, 0)\nb.PrependInt8Slot(6, 1, 0)\nb.PrependInt16Slot(7, 1, 0)\nb.PrependInt32Slot(8, 1, 0)\nb.PrependInt64Slot(9, 1, 0)\nb.PrependFloat32Slot(10, 1, 0)\nb.PrependFloat64Slot(11, 1, 0)\nb.EndObject()\nt = &flatbuffers.Table{\n  Bytes: ?,\n  Pos:   ?,\n}\n``\n. I'm getting an error (panics withslice bounds out of range). Just tried printing a few values to debug the issue. I don't know much about how the builder works but thevtable` value looks weird.\n\n. Thanks for the calc_offset function. Where can I read more about flatbuffer internals?\nWrote unit tests for all mutate methods.  Got the code coverage upto 93% from 82%.\n. Mutate methods return a boolean value. It'll return false if it didn't write.\n. @rw Sorry for the delay. I just added some comments to explain tests inside CheckMutateBuffer. About the second scenario (old value == new value), it does not check values so it always returns true if the flatbuffer has space. This way, if the return value is true, we can expect the buffer to have the new value. Is this way fine? or is it necessary to know whether the values has actually changed?\n. I'm extremely sorry for the delay guys, I'll start working on this tonight and try to complete this pull request ASAP. Hopefully It'll be ready before Monday.\n. @rw fixed tests and code coverage issues\n- using table based tests to reduce duplicate test code\n- testing Mutate_Slot methods where it should return false\nFinally, it has these mutation methods\n- MutateBool\n- MutateByte\n- MutateUint8\n- MutateUint16\n- MutateUint32\n- MutateUint64\n- MutateInt8\n- MutateInt16\n- MutateInt32\n- MutateInt64\n- MutateFloat32\n- MutateFloat64\n- MutateUOffsetT\n- MutateVOffsetT\n- MutateSOffsetT\n- MutateBoolSlot\n- MutateByteSlot\n- MutateInt8Slot\n- MutateUint8Slot\n- MutateInt16Slot\n- MutateUint16Slot\n- MutateInt32Slot\n- MutateUint32Slot\n- MutateInt64Slot\n- MutateUint64Slot\n- MutateFloat32Slot\n- MutateFloat64Slot\nI didn't write Mutate_OffsetTSlot methods because I'm not sure whether they should exist and how they should work. Removed previous MutateVOffsetTSlot method. For an example: GetUOffsetTSlot and GetSOffsetTSlot does not exist and the getter function for GetVOffsetTSlot looks like below:\nfunc (t *Table) GetVOffsetTSlot(slot VOffsetT, d VOffsetT) VOffsetT {\n        off := t.Offset(slot)\n        if off == 0 {\n                return d\n        }\n        return VOffsetT(off)\n}\n. Let me know if there's anything more to do.\n. @gwvo @rw rebased :)\n. Made travis run biicode only on Linux.\nI have no idea whether it's important or not :)\nI don't get to code with c++ often.\n. I think we can use these methods.\n. Didn't think of that. I'll remove the if condition.\n. TODO:\n- [ ] Find a way to install biicode on OSX\n. ",
    "pcallewaert": "Any update on this? Would really be a nice feature!\n. ",
    "Lakedaemon": "On 08/24/2015 07:33 PM, Wouter van Oortmerssen wrote:\n\nYou cannot \"set\" existing String objects in Java, so how do you \npropose to reuse them?\nWe could be returning StringBuffer objects instead, but that requires \nmost users to convert to String somewhere along the line anyway.\nEven Better would be to have a StringUTF8 class, that can point \ndirectly to the UTF8 data stored inside a FlatBuffer (without \ncopying), and allows common string operations.. though that still \nwould involve copying to String very often.\nFor java, returning a bytebuffer pointing to the utf8 data is quite nice \nbut there aren't many api that can consume this (a shame :/)\n\nWhat about returning a CharSequence ?\nIt's not immutable and you'll still have to use toString to use some Api :/\nStoring text in the java variant of utf16 would be ideal not to have to \ndo any parsing,  but that would defeat the purpose of being a cross \npurpose solution, I guess :)\n\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/google/flatbuffers/issues/260#issuecomment-134310097.\n. I'll probably try to implement a mechanism to reuse strings in kotlin in the following way : \nyou ask for a string for field whose record is at vtableOffset + offset\noffset == 0 -> null\noffset !=0 -> check a (flatbuffer instance assigned) map for a string at int vTableOffset + offset. If there is such a string return it, if not create a String, put it in the map and return it\n\nclear the map, when you wrap your type over another bytebuffer\n. A string is stored as utf8 bytes in a flatbuffer. So, you have to parse it into chars to get a CharSequence.\nIf you only use your flatbuffers for java and don't care for crosslanguage support, \nyou can store a string in a flatbuffer like a vector of shorts (you cast chars to shorts)\nand then, to read it, you can get the bytebuffer that holds it, view it as a CharBuffer that implements CharSequence... no parsing involved\n. why not add an option in the schema for implementing equal, hashCode, toString ?\nlike \"data\" in kotlin \nIn Java, if you implement ==, you need to implement hashCode() too...\nPersonnaly, I wrap my flatbuffers in an interface (that also hide the flatbuffer methods) that implements equals and hashCode()\nThat way, I only handle the interface and have different concrete class : a flatbuffer for serialization/deserialization... a standard class for construction...\nand I can test round trips...\n. Ok, I managed to make the switch statements work\nI am compiling the Monster & cie flatbuffers :  a few issues remain...\nI basicaly hacked the go code, as go might be ressembling kotlin the most.\nFor more complicated stuff, I'll look at the Java code...\nNow starting to work on enums (and next on unions)...\n. Mmmh, this is going to be problematic : \nunion Any { Monster, TestSimpleTableWithEnum }  // TODO: add more elements\n\"Any\" is a language type in kotlin (it's what corresponds to java's Object)\nlet's try to use a full name (with package)\n. ok... managed to setup a script to compile a test with kotlinc and execute it with the java vm.\nNow, let's write tests in kotlin (converting those from java) and debug...\nAlso, got to finish enums/unions...\nmmh, another problem with Stat : it defines a val field which is a reserved name in kotlin\n. I have reached a new milestone. \nThe test and the classes produced by flatc compile. A lot of tests pass (first fail is in testExtendedBuffer()). I now have to make all tests pass : let the fun begin !  :)\n. Hello there. \nI'm planning to debug my code, clean it a lot (reusing stuff in idl_gen_generic.cpp now that I have more understanding of what flatc is and does) when it'll be passing the tests, using the google standards, and submit a squashed pr when it's ready for review. \nAlso, for confidence. I don't have a complete understanding (yet) of the flatbuffers format (I need to have a very long and deep look at how vtables are layed out and about padding) but, I understand what it does and I can look at/reuse the java code so... time will tell... ^^\nI'll look at generate_code.sh and see what I can make of it, thanks for the tip\n(at the moment I'm using a script and a test class, inspired by the java code).\nThanks for the kind answer (and for the flatc code :)\n. Now, all tests (converted from JavaTest.java) pass for Kotlin, which just means that we should be able to do the same things in kotlin than in java with flatbuffers and that it kinda works (yet it still isn't ready for production use).\nNow, I know for sure that there are a few places where I have messed things up and the tests haven't caught me red-handed which means that there are opportunities to improve the (java/kotlin/other language ?) test coverage. ^^\nFor example : \nIn some string accessors, I'm returning \"\" instead of null where the vtable offset is 0 which is wrong -> default values should be tested in case of missing fields (or vector values) \nNext, I'm going to work on :\n- improving my code to make it cleaner/more reliable/reuse stuff defined in idl_general\n- writing documentation for the kotlin api\n- writing some text to help fellow language porters\nConcerning flatbuffer support, Kotlin behaves mostly like C++, java, C# and Go \nbut has a few peculiar points with :\n1) Enums (a bit like C#)\n2) Types appear after parameters and functions (a bit like Go)\n3) val and var properties instead of functions for field accessors in struct/tables ( a bit like C#)\n4) Null safety (type String? is nullable when type String isn't)\n5) Companion objects (instead of static methods)\n6) Extension functions (a bit like Go)\nThis makes me think that the kotlin code can reuse quite a bit of what is defined in idl_general... \nand that it needs a specialized engine in idl_kotlin as most stuff in idl_general has hardcoded types before parameters like this : \ncode += \"  public \" + struct_def.name;\n  code += \" __init(int _i, ByteBuffer _bb) \";\n  code += \"{ bb_pos = _i; bb = _bb; return this; }\\n\\n\";\nI guess it could be done with stuff like \n    if (lang.language != GeneratorOptions::kKotlin) {\n    ..\n    }\nBut they are going to be all over the place and with the increasing numbers of supported languages, \nidl_gen_general is going to become a big mammoth.\nSo, for the moment, I think I'm going to stay with \nKotlinCodeGenerator  extends/specializes the  GeneralBaseCodeGenerator\nBy the way, is there an grammar for the flatbuffer schema ? (update : yes, there is one in the doc folder)\n. Ok, I have tried reusing stuff in idl_gen_general.cpp like the GenTypeForUser methods, \nwhich looked like a very good candidate for reuse in generator for target languages BUT\nas it is declared static, it is not visible outside of idl_gen_general.cpp which means it can't be reused...\nAs all methods in that file are static, we will have to reimplement them for all target languages except if we somehow integreate the generators in  idl_gen_general.cpp, but it will be awkward for Kotlin as : \nFor uChar/uShort..., Kotlin doesn't cast, it widens bits with value.toByte()\nFor enums, Kotlin converts them with a function YourEnum.from(value)\nBut as we only have the SourceCast method that prepends the value, it won't do for source\nFor destination, DestinationMask (after the value) and DestinationCast (before the value) would do.... but their name would be misleading for kotlin...\nPondering what to do...\n(update : removing static in idl_gen_general.cpp fixes the issue and makes the code reusable)\n. After struggling today to align my code with idl_gen_general, \nI'm finally totally confused by the types in flatc...\nI was expecting to mainly have : \nA type for the user facing api (Int for USHORT)\nA type for the wire/storage  (Short for USHORT)\n(as well as a type for the create methods (offset for pointer types)... but that's beside the point)\nBUT GenTypeBasic is neither as Java has :\nboolean for BOOL -> user facing | not a wire/storage\nshort for USHORT -> not user facing | wire/storage\nAre there some inconsistent definitions in flatc or am I misunderstanding something ? \nCould you enlighten me ? \nI could use more documentation (say an explanation of what they are exactly for, and the subtle difference between them) for \nGenTypeGet, GenTypeBasic, GenTypeNameDest, GenTypeForUser, DestinationType...\n. Thinking it through, given theorical types (USHORT), I was expecting to have \nType representation in the language (Int) <- conversion -> Type used for storage (Short) <-- read/write--> storage (byteBuffer.putShort/getShort)\nNow, in flatc, the \"inconsistent?\" behaviour above is mitigated/amplified by the fact that : \nIn java, addBoolean has been added to FlatBufferBuilder (so actually Boolean is a wire type for storage)\nAnd in the getters and setters, a boolean can be read/written from bytebuffers (through a byte), so the conversion actually happens in the getters/setters (GenGetter, GenSetter)---> wrongly overstepping responsabilities\nAlso, for each language, you can tamper with things in idl_gen_general by adding switches like \nif (lang.language == GeneratorOptions::kCSharp) { do something different in there }\nconversion between types differ also from language to language.\nthe C/java way is to make it like (int) value and 0xFFFF\nWhen the Kotlin way is to write it like value.toInt().and(0xFFFF)   (<--- this isn't a cast, it widens the bits of the type)\nInstead of having casts (before) and masks (after) to hardcode it like cast + value + mask, it would be nicer to have convertToSource(value) and convertFromSource(value) method\nEspecially since source only has the cast (and not the mask), so there is no way to do \nEnum.from( value ) with sourceCast + value\nAlso, what works 0!= value to create booleans doesn't for Kotlin, we need 0.toByte() != value\nso DefaultValue needs some clanguage customization too\nAlso, (this is a matter of taste, not a real issue) bb and bb_pos are used throughout flatc, \nI would rather use _byteBuffer and _position in Table/Struct. \nIt would be nice if there was an languageOption to customize that (to better respect the language customs)\nTo wrap it up, I would love the code to become more simple and general to avoid code duplication/unnecessary headaches (especially if more complex features are to be implemented on top of the flatbuffer format (like maybee caching, mutating in memory and writing on copy, binarySearch on vectors of Struct/tableswit multi keys...)\nOn a bright side, the code is brilliant and there are very smart ideas in there (using flatbuffers for reflection representation, awesome :) ). \n. I studied those functions in the code and started (documenting them and) reusing them.\nOn the latest commit, the code builds and tests pass (yet, it still isn't ready for production).\nI have started to write some documentation as well as a guide for others to add support for other languages to flatbuffer.\nI still have a lot of clean up to do before it is ready for a Pull Request.\nAlso, before that happens, I'll change the kotlin api to make it more in the spirit of Kotlin (concise & fun to use) :\nThere are some unnecessary long method names like Monster.createMonster(....) or \nrootAsBytebuffer(...) that have duplicate info in them or type info that any decent IDE (like intellij idea) will give you along with code completion... (but well, I bet C++ doesn't have that and needs the verbosity, to allow devs to understand what they use)\n. I cleaned my code. Next, I'm going to revert the changes I did to idl_gen_general.cpp\nMost methods in there can't be shared, even the FunctionStart function ! (I had to duplicate them...like the go people) and I don't want to increase the complexity of this file by adding Kotlin specific stuff... \n. Answering your points in a different order : \n1) Kotlin as Java doesn't have unsigned types.\n2) Making Kotlin call the Java code would forsake most of it's benefits over Java.\nIt would be a bit like forcing C++ dev to only write C.\nAs an illustration, I'm building a monster with the latest version of the Kotlin api like this : \n         with (builder) {with (Monster) {\n        val mon = start()\n                    .addPos(with(Vec3) { createVec3(1.0f, 2.0f, 3.0f, 3.0, Color.Green, 5.toShort(), 6.toByte()) })\n                    .addHp(80.toShort())\n                    .addName(str)\n                    .addInventory(inv)\n                    .addTestType(Example.Any.Monster)\n                    .addTest(mon2)\n                    .addTest4(test4)\n                    .addTestarrayofstring(testArrayOfString)\n                    .addTestbool(false)\n                    .addTesthashu32Fnv1(Integer.MAX_VALUE + 1L)\n                    .end()\n            finishBuffer(mon)\n        }\n        }\nand I'm working to improve this api further : I'll remove the \"add\" prefix, the end() and start() method calls... and hopefully also the \"with\" parts (this is the difficult bit)\n3) If I have to write a GenDecl function that work for ALL languages, and test all code generators.\nSomehow, I'm already in maintainance hell. \nI should only have to worry about the Kotlin code generator. The fact that I have to modify the C++/Java/Csharp code generators to make some room for the Kotlin Code generator is problematic\nOf course, for casts and declaration order, it's easy and doable (But what about nullability, fluent apis, companion objects, Extension functions, properties... there are a lot of differences) : \ntransformSource(value) {\n    if (isKotlin) return beforeSource + value + afterSource else return othersCast + value;\n} \nBut each time a language is added to flatc, it will happen again and again...\nAnd later on, someone is bound to change the Kotlin code to make room for another language...\nThis also explains some inconsistencies in the idl_gen_general.cpp code and the if/then hacks.\nIMO, ideally idl_gen_general.cpp should just contain a completely general and abstract class, \nproviding truly general stuff like comments, fields, utility functions...\nAnd each language should have it's own file (separation of concern) implementing a subclass and overriding the  methods they need\nThis is also just what has been done by the Go/python/javascript and Kotlin guys... ^^\n. Sorry about the pull request : this is my first time. I did sign the CLA but I forgot to squash the commits into a single commit. I'll try to fix that today when I find time.\nAlso, I did a git rebase -i (but it had some trouble so I had to retry a few times and do some git edit-todo thingies, so I'm not sure it worked)\nI don't know if I can cancel a pull request. I'll see what I can do to do a better job...\n. Just implemented a (proof of concept) toString() code generation. \nIt should suck performance-wise as it does a lot of allocation. But it should be quite nice for debugging/test purposes.\nGot to think how to make that efficient given that you may have tables of vector of tables of vector of tables... It'll probably involve recursivity, variations of te _offset/_indirect... methods that take a (parent) byteBuffer and an offset (relative to the parent)...\n. I have started eating my own dog found which gives me some more coverage for the code generation and I found out that I was wrongly using the code to generate \"createStruct\" to generate Tables... \nSo I cancelled my pull request and I'm working to fix this : \nMainly by implementing the same methods that for java with the added benefits that, as Kotlin has lambdas, we can probably create inlined structs also...\nKotlin has default values for metod parameters too, and can pass parameters by names in functions. \nSo, I'll revamp the constructor api, to : \ntake required parameters first (-> I got the impression that this requirement is only check at a flatbuffer's creation and doesn't impact code generation for getting values (which is probably wrong))\nwithout default value,so you have to explicitly give them\ntake optionnal parameters next (with defaults)...\ntake a lambda last, to write the struct fields inline\nI'll try to implement that and see how it goes...\nLast but not least... the tests don't fail if you don't implement these create methods ---> improve the tests \n. Thanks for the tips. I didn't know. Maybee I can reopen the PR.\nI am actually reusing a lot of the Java internal code (the Table/Struct/FlatBufferBuilder classes in com.google.flatbuffers), that I converted it to Kotlin to get Null safety on top of the nice helpers functions.\nThis gave me a very big headstart (I'm really thankful).\nI have only slightly  altered the api (not the internals) to make it more convenient, more safe or more aligned with what users expect.\nFor example, thanks to inlined lambdas, we can \"hide\" the calls of startObject() and endObject(), while making sure they are both called at the right places, with no impact on  performance.\n. as a follow up to the issue about Table/struct create constructor, thanks to inlinable lambda functions, \nI think that it might be possible to have constructors in the same spirit than the C++ ones : \n- have nice structured initializer (nice clean user-friendly api), \n- without the performance hit of creating objects (the compiler does all the dirty work)\n- with named parameters\n- with optional values\n- with required values\nOn the bad side, users that wouldn't use the standard/built-in way to build a struct would shoot themselves in the foot :/\n(at this time, it doesn't look possible to subclass a function in Kotlin and to inline it :/) \n. Just pushed a bug fix for some create methods for vector of scalars (with vararg) : the destination types were wrong (it's really hard to understand what types you get when you call the methods that originates from idl_general that use like 3 or 4 composed method calls... I think that I'll simplify and document those, next time, because it is crazy stuff that makes fixing the most simple bugs a gamble)\nAlso, strangely, in the latest Kotlin 1.0.0 RC, I got some error because of the one assert in FlatbufferBuilder (looks like assert might have been disabled by the kotlin team...which is kinda surprising)... Fixed that too.\nAs I don't have much time to work on flatbuffers right now (though I'm using it more and more), \nI'll just watch from the shadow to see how/if the Csharp/Java split happens and if there is some code that can be shared to simplify the kotlin generator... If there isn't, I'll write something.\nBut, as long as Kotlin 1.0.0 isn't released, there is no hurry...\n. Kotlin 1.0.0 might not be that far away (maybee 1 month, 2 months away).\nWhen it is released, I'll make sure my code works with it and contact the nice jetbrains people to ask if one of them could review this pr (mostly the code produced by the generator). \nAlso, as I feared, there might be a defensive array copy when the spread operator * is used to pass an array to a vararg function (http://blog.jetbrains.com/kotlin/2015/10/kotlin-1-0-beta-candidate-is-out/). Which means that there is a performance hit with the create functions I wrote with vararg.\nI'll fix that and revert to just using arrays. The vararg methods weren't that nice anyway when building flatbuffers with big complex arrays that have to be computed\n. I made another pull request with a squashed commit\n. I started using this in my code and I found 1 bug in the create functions (I used the go code).\nI am investigating this. At the moment, I found a workaround (I use the add functions)\n-> I'll try to find why this happen, make a test, fix the code, test some more with my stuff and make a pull request later when the bug has disappeared\nI'm cancelling the pull request for now\n. Working on improving the create Struct and Table Api... \n. I managed to improve the create api, now it is possible to build a complex table with structs in one command. The test file now looks like this : \n    with (builder) {\n            with(Monster) {\n        val mon = monsterOf(of(\"MyMonster\"), \n                    hp =80.toShort(),\n                    inventoryOf = inventoryOf(0, 1, 2, 3, 4),\n                    testType = Example.Any.Monster,\n                    testOf = monsterOf(of(\"Fred\")), \n                    test4Of = test4Of(2) {testOf(10.toShort(), 20.toByte());testOf(30.toShort(), 40.toByte())},\n                    testarrayofstringOf = testarrayofstringOf(of(\"test1\"), of(\"test2\")), \n                    testbool = false,\n                    testhashu32Fnv1 = Integer.MAX_VALUE + 1L, \n            posDef = vec3Def(1.0f, 2.0f, 3.0f, 3.0, Color.Green, testDef(5.toShort(), 6.toByte())))\nfinishBuffer(mon)\n        }\n    }\nMost helper methods (Of) returns an offset (an Int) and Structs have also a special method (Def) deffered till it is used to build a table, \nI don't think the methods with start/end/fields are needed anymore (am I wrong ?), \nso I will remove them next\nSadly, I see no way of returning an int with the type Offset, without an allocation... :/ (do the Csharp guys pay for this or is it free for them ?)\nI would just love to be able to inline such a thing but I don't think this is possible at the moment with java/kotlin.\nSo a safer build process would impact performance a bit :(\nI'm going to clean my code (again :/) and push the progress I made...\n. Could you have a look at the latest code ? \nI'm quite happy with the new api ^^\n. Now that we have constructor methods for structs/tables/vectors...\nI should probably inline all CustomType.addField  methods in the corresponding constructor and remove them... (-> less code generation = less bloat = less method count on android). \nThat way, as name clash wouldn't be possible anymore, I could remove all those \"Of\" at the end of all the methods (and maybee rename \"FlatBufferBuilder.of\" to \"FlatBufferBuilder.string\" ? or not ? )\nFor structs : structDef -> struct \n                    structOf -> don't return offset && find a new proper name for the method ??? inlineStruct ? writeStruct ? ... pondering...\n. Implemented the previous changes. Constructing a nested monster now looks like : \n    with (builder) {\n            with(Monster) {\n        val mon = monster(of(\"MyMonster\"), \n                    hp = 80.toShort(),\n                    inventory = inventory(0, 1, 2, 3, 4),\n                    testType = Example.Any.Monster,\n                    test = monster(of(\"Fred\")), \n                    test4 = test4(2) {testRaw(10.toShort(), 20.toByte());testRaw(30.toShort(), 40.toByte())},\n                    testarrayofstring = testarrayofstring(of(\"test1\"), of(\"test2\")), \n                    testbool = false,\n                    testhashu32Fnv1 = Integer.MAX_VALUE + 1L, \n            pos = vec3(1.0f, 2.0f, 3.0f, 3.0, Color.Green, test(5.toShort(), 6.toByte())))\nfinishBuffer(mon)\n        }\n    }\nI don't think I can improve that further.\nAlso, It looks like type aliasing might be implemented in Kotlin after version 1.0\nThis might help to make the kotlin api more type safe, with regards to offsets, without allocating stuff, by inlining lambdas that have some particular type alias. We shall see... but for now, Ints should be enough.\nI'm going to slow work on the flatbuffers/kotlin api from now on... \nfixing things (like namespaces)... and waiting review...\n. arhh damn... I messed up the merge... it doesn't compile on the php side \nStupid me that didn't verify this before pushing... :/\nupdate : the php code doesn't compile because...making room for kotlin in \ndefine FLATBUFFERS_GEN_TYPES_SCALAR(TD)\nbreaks the code for php relying on this definition that was added later...\nthis should be fixed (hopefully) by the later push\n. I'm closing this pull request. I messed up non-kotlin files when I first started hacking Kotlin support and it makes Travis's test fail.\nI'm opening another pull request, with less modifications of non-kotlin files\n. With packages fixed, kotlin support now works properly. \nNext, I should improve the performance of Enums with at least : \n- a function for Unions or values in an arithmetric progression\n- a switch for say less than 8 enums\n- a map or a binary search otherwise ?\nI would also like for travis to launch and check the kotlin tests but don't know how to do that (or if I even can)\n. Improved the performance of enums when they are in an arithmetic progression (function) and with a map. I included some new enums in the monster schema to build the new implementations but haven't written tests for them yet. :/\nBy the way, why does flatbuffers serialize/deserialize enums with their value instead of their ordinal ?\n. A change to accomodate the fact that the recently released Kotlin 1.0 Beta 3 undeprecated values() and deprecated values : https://github.com/JetBrains/kotlin/releases\nHopefully Kotlin 1.0 will be released soon and quit being a moving target (it's not that difficult to hit, but still...) \nAlso, structRaw now returns an offset to the struct (so that you can create a flatbuffer that is a struct conveniently)\n. We can just ask the people at jetbrain to look at it. They are really helpfull and are usually really happy whenever a new library pops up for Kotlin. \nAlso, though we all want to avoid code duplication and maintenance hell. I still don't see how the solution involving idl_general.cpp and ifs (spagetty code instead of overloading) enables that : \nespecially if there are going to be code generators for java 1.7, java 1.8, java 1.9, java 1.10, kotlin 1.0, kotlin 2.0...\nHaving cleanly separate files for each language makes it so much more easier to implement features/fix things\n. I found and fixed a few issues with my code related to enums and to downsizing constants... \n(-2.toShort() produces an error in Kotlin when (-2).toShort() doesn't, which feels slightly weird to me)\nI added a few fields to the Monster Table at the end.\nNext, I should test mutations of these fields, implement enums with a sparse array like in C++ (and depending on the range of enum values use that, binarySearch or a map).\nMaybee we should have all types of vectors/fields in the Monster table to test all generated methods...\n. Finally, I wrote a base class for a Code Generator and rebased the Kotlin Code Generator on it.\nI used the opportunity to clean the code wherever I could. But there are still improvements to do.\nI would like to get feedback about it\nNext, I'll make the base class on a diet.\nIt will probably be very general, with just :\na public constructor,\na public method with signature bool generate() \n6 const protected members (parser, path, file_name, keywords, namespace_components, namespace_dir)\nOn top of that, we might write (for example) a StronglyTypedCodeGenerator class/trait \nwith more utility functions like the safeName method that avoids name clash for fields\nor a safeType method that would avoid name clash for types...\nAnd then the various Code Generators for various language\nlike Kotlin1, Kotlin 1.2...\n. Split the former base class into the most simple/general class for code generation. Namely\n  class BaseGenerator {\n    public:\n      BaseGenerator(const Parser & parser_,const std::string & path_,const std::string & file_name_) : parser(parser_), path(path_), file_name(file_name_) {};\n      virtual bool generate() = 0;\n    protected:\n      virtual ~BaseGenerator() {};\nconst Parser & parser;\n  const std::string & path;\n  const std::string & file_name;\n};\nAll code generators can migrate very easily to this new class (this can be done in less than an hour, without hassle and the intervention of any code generator maintainer)\nThough it is simple, there are some immediate benefits : \n1) No need to pass those parser, path, file_name arguments in all functions.\n2) Easier for code generator writers to start writing a code generator for their exotic language : \nthey aren't  necessary fluent in C++, it took me a lot of time/frustration at first to understand how to make those paranoid errors like \"parameter isn't used\" disappear...\n3) Standardisation (nice for cross pollenisation) : now all code generators will start looking the same (which is good), with the important variables named the same way...\n4) Inheritance is needed to be able to share code between code generators for different versions of the same language \nIn my opinion, In the end, it will make developing, maintaining and sharing code easier\nAlso, it might  help in one of your goal : \"somehow, make it so that a feature of flatbuffers can be implemented simply in parallel in a lot of languages\"\nThis might come with the help of other orthogonal classes/interfaces\n. Kotlin 1.0.0 has been released today (much sooner than I expected).\nI'll make the modification today to use arrays instead of varargs, make a last push an ask some of the jet brain people for a review...they'll get delighted for another library to get native kotlin support on day 1\n. I also simplified some method's names\nnames for methods aren't final. I would like them to be as simple and explicit as possible. \nAlso, I'm waiting for the feedback of others to rename them\nI'll be asking some of the jetbrains people to look at this pr now, for a review\n. Now that the latest code pass the Travis check, I asked in a tweet the help of a kotlin expert to help review this pull request. Let's just hope now that I have made a good job at it. ^^\n. today, because I wasn't using named parameters in the constructor, I got burned by this simple schema : \nnamespace org.lakedaemon.setup;\ntable LongsCache {\n    size:int; // file size\n    blueprint : string (required); // tells if the cache is valid\n    longs:long;\n}\nroot_type LongsCache;\nThis happens because offsets are standard ints in the kotlin generator.\n-> I think that I'll start writing a kotlin generator that uses Generics (like the csharp generator, to differenciates between offsets for table, vectors and standard ints).\nThis will be a good experiment to see if I can reuse code from the kotlin 1.0 generator\nIt will allow people to check their generated code (by swaping the different kotlin generators, the safe/slow one and the less safe/fast one)\nAlso, when/if kotlin implements type aliasing, it might bring improved safety without performance penalty\nIn my next commit, I'll add the generated kotlin classes (I see now the wisdom of that)\nSadly, I have had no news from the Jet brain people yet, for a review of this pr.\n. Kotlin implements Generics mostly like java (except that they behave way better when inlined), this is why I used Int for offsets in the kotlin 1.0.0 generator.\nIn the generator for kotlin 1.0.0, The create functions for Tables return a lambda (with the signature FlatbufferBuilder.() -> Int) instead of an Int because they can be inlined (no performance penalty) and allow a very nice way to create a Table in one line\nIf/when the people at Jet Brains implement type alias for the next version of Kotlin (they wrote that they were interested in doing that), it might be possible to make those create function return a type alias instead (like Offset) for this lambda bringing a lot more type safety (again without performance penalty/without object creation) \nAt least, this is my hope. It remains to be seen if it can be done.\nMaybee I should wait before experimenting with that...\n. Saddly, I haven't been able to peek at the work of @evolutional yet. So, I gave a try at writing a CodeWriter class and refactored the kotlin code generator so that it uses it.\nIt works and produces code that is more beautiful land easier to maintain. \nIt only supports indentation for now, I'll give a go at supported C++ like namespaces later.\nI'm partially satisfied with the use of operator += and +\nI'll probably have to switch to an operator that is left-assossiative like << \nbecause code << aaa << bbb is better than \ncode = code + aaa + bbb\nOnce I do this switch and I am satisfied, I'm going to look at splitting and refactoring the java code generator \n. concatenation for std::string won't give you semi-automatic indentation, semi-automatic namespace, dispatch code to different files (say 1 file for a class, 1 file for top level methods...)\nWith std::string, I had to put \\n and \\t everywhere and the code generated still wasn't as nice as the last batch.\nI'm experimenting in my branch to see what api would be best for code generator maintainers.\nI don't really expect anymore this pull request to be merged as it, indeed, does too many things.\nIt is nice to get some feedback though.\nIf I somehow manage to produce a nice simple way to factor out general code generators and if you are interested to merge it in the flatbuffers codebase, it'll still be possible to split it in small, manageable pull requests\nIn the mean time, I have to experiment and try to \"port\" another generator to see if the ideas behind the code are sound (also eating my own dog food is a good way to see if I'm doing something wrong or not and to find issues early)\nI didn't know about clang-format. I will look into it : I use no ide for C++, just jedit and syntax highlighting. Manually maintaining indentation is really tedious, so I gave up\n. I setup clang-format-3.7 (on ubuntu)\ntried : clang-format-3.7 -style=Google -i ./idl_gen_kotlin.cpp\nAnd it made things worse... (good bye indentations....)\nI tried again after : clang-format-3.7 -style=google -dump-config > .clang-format\nNo improvements\n. You are right. I guess that I am among the people that don't like the way code looks with the Google style guide. \nMy future prs will follow the Google style guide as this project adheres to it (I knew it before but I didn't know about clang-format). \nAlso, I will make it so (or die trying) that the transition path is as smooth/simple and unobstrusive as possible. \nI have gained quite a lot of experience transitioning my code to this CodeWriter and I can use that to make it simple to code generators maintainers.\nAs of now, transitioning mostly means : \n1) removing lots of \"\\n\" \"\\t\" in strings and replacing them by NL (newline), TAB (tab to the right) and BAT (pop the last tab amount). I can change the names if you don't think they are the best for the job) \nThe python guys are already doing it this way I think\n2) in some methods signature, replacing std::string with CodeWriter\n3) instancing a CodeWriter somewhere and saving to file at the end of the process\nLater on, the C++ guys will gain better namespace formatting, for free.\nI think that I'll let the += and + operators overrides in CodeWriter \n(nice for transition) \nbut I'll probably add a << override to append stuff (and command tabs/namespaces) to the codeWriter\nThe problem with += and + is that you have unexpected results if you do something like \ncode = a + b + c  instead of code = code a+b +c \ncode += a + b instead of code += a; code += b \ncode + a instead of code += a;\nYou don't have those problems with << \nAlso, I know that I am annoying ^^\nPlease bear with me. I'll soon propose in a new PR branch some small PR to start the transitioning process and you shall judge (and probably ask changes).\n. Well, then I will work under your supervision to get the Kotlin code generator right in a git branch, \nwithout the CodeWriter (I'll still develop and use it for my own personal projects so, if you ever warm to it, it'll be available).\nCould you guide me on what needs to be done to get it right/merged ? \n(starting from commit https://github.com/google/flatbuffers/pull/1190/commits/40ad6f195986b81d1b022ac7629e9a08093b119b)\nFeature wise, the Kotlin code generator should be on par with the Java code generator (compare the generated classes ?) or slightly better (toString method, better enums/unions, better api...). \nI have been using  in my own projects for a few months and I'm happy with it. \nI would really welcome some feedback from other users\n. I found a way to make the kotlin api even safer with the createStruct/Tables/Vectors methods.\nFor offset, instead of asking for an Int (error prone, not safe) or for a lambda ()->Int (nice for generating structs inline but still error prone) I am now asking for a lambda (Struct?)->Int.\nSo, the compiler will now make sure that the right offsets are used in the right places.\nAlso, there is still no performance penalty because the lambda is inlined and it is really simple.\nI have already implemented that for tables and structs. \nIt will take me a little more time to implement it for vectors/unions. \nOnce it is done, I'll try to make a clean pr (google style) without the CodeWriter. And I'll ask the JetBrains people again for a review.\n. On 04/19/2016 12:52 AM, Wouter van Oortmerssen wrote:\n\nI would guess a lambda is not inlined if it is passed around by value.\n\nYes indeed. This is something I'm thinking about (especially for String).\nval mon = monster(of(\"MyMonster\"), // <-------- i'm considering making \nstring offsets safe, not sure yet.\n                     pos = test(2.toShort(), 3.toByte()) // <---- this \nstruct is safer with a lambda that is inlined\n)\nval pos = test(2.toShort(), 3.toByte()) // <---- there is object \ncreation there\nThe object constructed is very light though as the lambda is just {offset}\nval mon = monster(of(\"MyMonster\"), // <-------- i'm considering making \nstring offsets safe\n                     pos = pos, // <--- the lambda is inlined there\n)\nIt's kind of hard to get the right balance between safety, \nuser-friendliness and efficiency\nI would guess that building a Flatbuffer is quite an heavy operation and \nI'm not sure if the cost of creating a very thin wrapper around an Int : \nthe JVM should be quite heavily optimize use and garbage collection of \nvery short objects\n(also kotlin doesn't have int (primitive type), it only has Int (object \ntype)) is really detrimental to performances.\nWe would need the advice of a Kotlin expert for this/profiling.\n\nSafety is a very worthy goal. In the case of FlatBuffers, efficiency \ntrumps it though.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub \nhttps://github.com/google/flatbuffers/pull/1190#issuecomment-211618074\n. On 12/30/2015 04:20 PM, Oli Wilkinson wrote:\nShould we look to be merging Go/Python or the other languages into the \ngeneral generator?\nNo, please no.\nI mean there /is/ benefit in keeping 'general' around as it allows the \nlanguages to maintain feature parity. I do have a concern that they'd \ndiverge in feature support if they were split.\nOf course they should diverge, you can't expect languages with so many \ndifferences to behave the same.\nTrying to build a simple/clean master generator that rules all languages \n(and there are gazillions of languages out there) is not possible and is \nholding features implementation back. (It hurts to integrate code into \ngeneral, right ?)\n\nYou share what you can. You don't coerce languages to use a common \nfacility that they shouldn't use.\n\nThat being said, there's a lot about the generated C# I don't like \n(formatting, mutators not being 'setters', etc) that being a C# \ndeveloper by trade rile me every time I use it.\n\nRight !\nFlatbuffers is mostly the wire specification and the C++ parser\nCode generators should be cleanly separated (and versioned)\nThis would speed up flatbuffer features development\n\nThe per language branching statements do make it less readable\nMy point\nand doesn't guarantee that a change to Java will make it into C#, for \nexample.\nAnd code will break when someone makes a change and merging code will \nbecome harder\nPerhaps we can fight back the feature divergence\n\nThis is actually good. feature divergence means that new features can be \nimplemented for one language...\nand ported to others later\n\nby creating language specific issues in github for every time a change \nis made to C++. That way we'd have a per-language backlog and sight \nover needs what.\nIf we did split the generators, we should seek to build out the \n'common' library so that we have functionality that be shared between \ngenerators easily.\n\nPlease do that. We needed that months ago.\n\nIn my last job, I used to maintain our code generators and much of the \nlanguage quirkiness was pushed into helpers which left the actual \ntemplate having a lot of the language specific stuff hidden.\n\nyou mean like a clean base abstract class with overrides to implement \nthe languages quirkiness ?\nhint hint...\n\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/google/flatbuffers/pull/2777#issuecomment-168018427.\n. Update : I answered the previous mail without having read the previous message from gwno (sorry).\nNow that I have read it : yes, please, split the generators, it is going to be worth it.\nThe respective api and code for Csharp and java will be better, simpler \n\nFor the transition (we don't want to break code), \nwe could write a base abstract class providing the common stuff/utils (you can recycle parts of the general file). Each language could then start using this base class at their leasure.\nEach time it is done, it should reduce file lengths by like 33% and also complexity.\nPlease remove the TD macro and provide some alternativ like maybee abstract methods like \nUserTypeForByte() : String\nUserTypeForUByte() : String\n...\nI'm not sure all scalar types may use a static  type name provided by an array (enums usually don't)\n...\nto enable metaprogramming/generics (returning a type name defined in the field), I guess\nUserTypeForEnum(field:Field) : String...\nupdate2 : I'll help however I can\n. On 01/12/2016 09:06 AM, Eric Lagergren wrote:\n\n|type:Type = User;|\nGenerates invalid Go code because |type| is a reserved identifier.\n\nThe kotlin generator had the same problem :\nin the Monster example, Any is a reserved type.\nAlso, some field names used by users should be avoided as they are \nreserved words in the language :\nfun, val, var...\nThere were some issues with enums too (had to fully specify types)...\nAll languages should expect to have such a problem and should work \naround that\nIn the kotlin generator,\nfields ending with a \"\" gets another \"-\"\nother fields with a reserved name get a \"\"\nother fields without a reserved name stay the same\nso we have (flatbuffer schema generated)\nval -> val_\nval_ -> val__\ngah -> gah\n\nI think it'd be fairly easy to throw an error if any one of these \nkeywords https://golang.org/ref/spec#Keywords are used as field names.\nIs this worth a PR?\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/google/flatbuffers/issues/3509.\n. On 01/12/2016 02:51 PM, Oli Wilkinson wrote:\nIt might we worth taking a cross-section of the keywords from each of \nthe major languages, else we may end up with a schema that's fine in \nGo but not C#, for example.t\n\nI'm not sure about that.\nYou should be able to specify the schema you want. You should not have \nto inspect any other languages out there to choose the variable names \nyou can use\ncode generators should remap types/names in case of clash.\nThis is a code generator - language specific issue\n\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/google/flatbuffers/issues/3509#issuecomment-170917405.\n. this is a bug I had to fix for the kotlin code generator (which was heavily based on the go code generator).\nIndeed, it was intended to be a +=\n. The users will want to know what version their (language specific) code generator is at too. \nIt's not just about the parser. \n(imo, it is mostly informative but nice to have : to make sure you are using the right version when you have flatc in your path, etc...)\n\nUsually people use a specific version of flatc to generate their code once. \nThe only incentive to update to a more recent version would be for a bug fix or to get a particularly interesting feature. \nYou don't want to constantly rewrite your code to keep up to date with the latest improvements\n. date and version are fine by me, if a user really needs the git sha, it should not be overly complicated to get from those\n. @rw we definetely want to avoid efforts and pain. The aim here is to enpower/help code generator writers, to reduce the  burden of maintainers, while improving the flatbuffers lib.\nPlease give me your advise/guidance to limit refactoring to what is really needed/usefull !\nAlso, I agree that each port will become more specialized (to fully use the capacities of the target language, give the best api/performance, spect the coding conventions...) but this only concerns the exported string/bits to the language file. The C++ code doesn't change that much and can be shared/generalized a lot across all code generators.\nI looked at the code for the go/python/javascript/php/c++ code generators (which all spawned from a common original generator, the C++ one, right ?) and they are all rewriting those methods : \n\nstatic std::string GenGetter(const Type &type);\nstatic std::string GenMethod(const FieldDef &field);\nstatic void GenStructBuilder(const StructDef &struct_def, std::string code_ptr);\nstatic void GenReceiver(const StructDef &struct_def, std::string code_ptr);\nstatic std::string GenTypeBasic(const Type &type);\nstatic std::string GenTypeGet(const Type &type);\nstatic std::string TypeName(const FieldDef &field);\n\nThey also all have variants of this method that should be shared : \nbool GeneratePython(const Parser &parser,const std::string &path,const std::string & /*file_name*/) {\nfor (auto it = parser.enums_.vec.begin();\nit != parser.enums_.vec.end(); ++it) {\nstd::string enumcode;\npython::GenEnum(**it, &enumcode);\nif (!python::SaveType(parser, **it, enumcode, path, false))\nreturn false;\n}\nfor (auto it = parser.structs_.vec.begin();\nit != parser.structs_.vec.end(); ++it) {\nstd::string declcode;\npython::GenStruct(**it, &declcode, parser.root_struct_def_);\nif (!python::SaveType(parser, **it, declcode, path, true))\nreturn false;\n}\nreturn true;\n}\nOr this one :\n// Generate struct or table methods.\nstatic void GenStruct(const StructDef &struct_def,std::string *code_ptr,StructDef *root_struct_def) {\nif (struct_def.generated) return;\nGenComment(struct_def.doc_comment, code_ptr, nullptr, \"# \");\nBeginClass(struct_def, code_ptr);\nif (&struct_def == root_struct_def) {\n// Generate a special accessor for the table that has been declared as\n// the root type.\nNewRootTypeFromBuffer(struct_def, code_ptr);\n}\n// Generate the Init method that sets the field in a pre-existing\n// accessor object. This is to allow object reuse.\nInitializeExisting(struct_def, code_ptr);\nfor (auto it = struct_def.fields.vec.begin();\nit != struct_def.fields.vec.end();\n++it) {\nauto &field = **it;\nif (field.deprecated) continue;\n`GenStructAccessor(struct_def, field, code_ptr);`\n}\nif (struct_def.fixed) {\n// create a struct constructor function\nGenStructBuilder(struct_def, code_ptr);\n} else {\n// Create a set of functions that allow table construction.\nGenTableBuilders(struct_def, code_ptr);\n}\n}\n. Also in the various code generators, the string that is being built is sometimes passed through a pointer std::string *code_ptr or with a reference and is mostly used through a reference\nWhich means that there are useless std::string &code = *code_ptr; all other the place\n(just pass everything by reference, it's cleaner)\n. code generators could also benefit if they could tap (a base class) for methods that : \n1) return a field name (and that escapes it in case of a clash with a target language keyword) \n2) return a user type name for a struct/table/union/enum (and that fully qualifies it in case of a clash with a target language built-in type... didn't have to use that yet though)\n3) save a binary/text string to file\n4) detects if an enum can be implemented through an arithmetic progression (memory and performance benetifs vs a sparse array or a map). Unions always can.\n5) return user types for getter, user types for create methods, user types for create array of offsets methods\n6) upsize wireType to userType, downsize userType to wireType\n7) transform a schema constant in a user typed constant...\n. I completely agree with transition points 2, 3, 4, 6, 7 and would like to discuss/get more details about point 1 & 5\nAbout the split, I would vote for splitting the two first.\nIf nobody has worked on that yet. I might give it a go in a \"split\" branch.\nWhenever someone fixes a \"shared C++ related logic\" bug for his language generator, it will propagate the fix to all others. This means that the refactoring will give all of us the benefits of your namespace fix, for free (thanks for that :) ).\nAbout point 5.  With the class refactoring solution, we don't need 1 header file by code generator.\nLike you suggest, we can just use 1 header file (for any shared base classes, they should be few in number) : \n- Flatc stays like it is now (no modification)\n- the class for each code generator is declared and implemented in it's cpp file (under it's language namespace)\n- the code generator is launched through the usual call back like this \n  bool GenerateLanguage(const Parser &parser,const std::string &path,const std::string & file_name) {\nlanguage::LanguageGenerator * generator = new language::LanguageGenerator(parser, path, file_name);\nreturn generator->generate();\n}\n  }  // namespace flatbuffers`\nMy gut feeling is that we need inheritance. I have thought delegation too but haven't found yet how it could be used in this situation : I'm quite new at this code generator thing, so I need more experience.\nI'm open to any solution that makes our job and the code better.\nAbout point 1. Could you develop a bit more about this CodeGeneratorContext ? \nI only see vaguely what you mean (and the evil might be in the details) and I would like to know more.\nMy concerns are : \n- I guess that the C++ and the Python CodeGeneratorContext will be very different. \n  How will we be able to set the differences in the codeGeneratorContext ? (types, wire type to user type transformation, imports, ...)\n- say language 1.0 has some more features than language 1.1. \n  How can I implement a generator for language 1.1 that reuses some (good) methods of generator 1.0 and  replaces others with new implementations ? \n- how will the following method (signature and body) look if we use a CodeGeneratorContext \n    void getStructFieldOfStruct(const FieldDef &field, std::string & code) {\nauto structName = field.value.type.struct_def->name;\ncode += \"\\tval \" + name(field) + \" : \" + structName + \" get() = \" ;\ncode += name(field) + \"(\" + structName + \"())\\n\";\ncode += \"\\tfun \" + name(field) + \"(reuse : \" + structName + \") : \" + structName + \" = \";\ncode += \"reuse.wrap(bb, bb_pos + \" + NumToString(field.value.offset) + \")\\n\";\n}\n. I forgot  about point 8. If I understand correctly, we want to keep things simple.\nThe goal here is not to implement an overly complicated behemoth that tries to cover 100% of the cases (and fails with style :) ). \nIn my eyes, a code generator .cpp file feels a bit like a .php file where the C++ code plays the part of php and the strings we append play the part of the html markup\nIn my opinion, declaring the following methods (that concern strings and not C++ logic)\nBeginClassFile\nEndClassFile\nwill lead to endless amount of callbacks and still need to be overriden anyway\nBecause some static methods would need to be implemented in other file/outside of a class in some language, some language might not even have classes (why not access a flatbuffer from assembler ?)\nBesides using start/endMethods is nice for xml but is a bit weird, when you can use the C++ language to nest methods with the language itself. It's way more powerfull and flexible\n. Sounds very nice to me and a bit like what I did, except that you went the extra mile and added indentation management (nice for python & beautiful presentation) on top of that (I guess that C++ nested namespaces can be managed in the same manner by your TextWriter Concept instead of string)\nI would love to see what you wrote and to help/collaborate if I can/if it's the direction we are going.\nBy the way, I don't remember what a string_constant is in the flatbuffers grammar spec ? \nDoes flatbuffers support unicode for field's name / type's name ? (in which case, we should use wstring instead of string, right ?) \n. @gwvo ...and I mostly agree with all that you said. Thanks for the detailed explanation, it helps me.\nNow that I have a clearer picture of what to do, I'll resume work and try to help where I can.\n@evolutional If you want, I can help you port more generators to your approach and/or help split the java/csharp generators (I can help with java but not with csharp)\nI might also experiment with porting another code generator (either C++, Python, Java or Go) to my codebase... In the case, I would do that, I'll make it so that it is easy to align this work later with @evolutional's contribution (as his approach supports more features) and with the method/names other code writers use (mostly derived from the C++ names)\n. you can avoid an array copy and don't need the ByteArrayOutputStream if you do something like this : \nint offset = 0;\nint read = 0;\nwhile ((read = fis.read(buf, offset, buf.length - offset)!= -1)) {\n   offset += read;\n}\n// you might also want to check that you read all the file (and it's not truncated)\nAlso, it seems to me like you are creating your ByteBuffer in a weird way, as ByteBuffer has no public constructor. You should create one with this :\nByteBuffer bb = ByteBuffer.wrap(buf);\nThen, in this post : http://frogermcs.github.io/json-parsing-with-flatbuffers-in-android/\nit says that \n\nThere currently is no support for parsing text (Schema\u2019s and JSON) directly from Java\n\nYet, it explains how to do that through C++ and the NDK on android \n. Sorry. This pr is messed up : \n1. The indentations are all wrong and make merging a hell. I did it with clang-format-3.7 style=google -i ...\nWhat should I do instead ?\n1. I forgot to remove some imports in code_generarors.h\nOnce this is fixed, I'll rebase\n. I managed to implement a short macro for jedit that calls clang-format on selected text for me. \nSo, I'll clang-format all code that I paste/move/write and there should not be problems with indent anymore (hopefully).\nAlso, thanks to jedit's powerfull find and replace dialogs, it won't be that hard to append all class members with \"_\"\n. Hello. I submitted the aforementioned changes.\nCould you tell me if there are still things wrong with the latest commit or if you want more changes ?\n. I already did the changes for the other languages in my local repository. I was waiting for your approval to push them upstream. Will it be a problem for merging if I push them now ?\nThe following changes should be simple too :\nmostly refactoring very long methods into manageable simple short methods (as per the Google C++ style) and starting to share code among the various code generators.\n. There. I fixed the memory leaks (didn't know how to instantiate a class on the stack before, this is quite different from java).\nAlso, I added js/python/php code generators to the refactoring process.\ndoing sh generate_code.sh is enough to check that all is well ? as well as flattests ?\n. 1) Please tell if you want different names for the methods and I'll change them.\n(this is important because all code generators will have those functions and share those names)\n2) the fileWarning methods should be shared between code generators (and moved to the base class).\nWouldn't it be nice if the warning could also have metadata about how the file was computed (version, date, flags sent to the compiler (mutable, etc..)) instead of just saying this : \n\"// automatically generated by the FlatBuffers compiler,\"\n        \" do not modify\\n\\n\";\n. 3) I have some code that implements this TODO\nvoid CheckNameSpace(const Definition &def, std::string *code_ptr) {\n  // Set up the correct namespace. Only open a namespace if\n  // the existing one is different.\n  // TODO: this could be done more intelligently, by sorting to\n  // namespace path and only opening/closing what is necessary, but that's\n  // quite a bit more complexity.\nThe code to fix that is actually a bit simpler and shorter (with 2 less functions). Should I do a PR ? \n. A) If what is done in this commit is ok with you, I can split the generate function in other generators and reuse the same functions names.\nThere will just be slight differences in code generators depending if they save a single (C++) or many files (java), etc... But it should make it easier to understand, compare, maintain code generators and port features.\nB) After that, the next step will be to slowly but steadily move inside the functions that are outside the class and share in the base class what can be shared \n. (no worry, hope you had a great time !)\n1) The javascript code generator can already use IsEverythingGenerated, what it does instead is \n// Only output file-level code if there were any declarations.\n    if (enum_code.length() || struct_code.length()) {....}\nAlso, the other code generators should use it (the kotlin code generator doesn't but it should exit early if there is nothing to do)\n2) At this point, splitting the generation steps helps others navigate and understand the various code generators, cross pollenization, feature implementation, evaluating what features are still missing from their generator, ...\nEven if the method bodies aren't shared later (I still hope to share most of them), \nsharing their signature will help a lot when maintaining/writing the code\nAlso it enforces the C++ style guide : \n\nWrite Short Functions\nPrefer small and focused functions.\nWe recognize that long functions are sometimes appropriate, so no hard limit is placed on functions length. If a function exceeds about 40 lines, think about whether it can be broken up without harming > the structure of the program.\nEven if your long function works perfectly now, someone modifying it in a few months may add new  behavior. This could result in bugs that are hard to find. Keeping your functions short and simple makes it easier for other people to read and modify your code.\nYou could find long and complicated functions when working with some code. Do not be intimidated by modifying existing code: if working with such a function proves to be difficult, you find that errors are hard to debug, or you want to use a piece of it in several different contexts, consider breaking up the function into smaller and more manageable pieces.\n\nAlso you might also see it this way : \nthough generateAccessor is only called once in the C++ generator, it is called 7+ times accross all code generators\n. Ok, then I'll close this pr and send new pr along these lines next.\n. ok, I'll do that.\nThe methods that use it will have either to take a generator parameter or to move  (with the method that call them) into the class though : \nMoving into the class is not a problem as it will have to happen eventually in the refactoring process but it will make this pr less trivial\n. In order to implement  FlatBuffersGeneratedWarning() inside the class, I had to pull beginFile() and saveType() in the python/go/php generators inside the class. \nThis actually has a really nice consequence : \nwe can now easily share  big chunks of the saveType/saveClass methods (the code that compute the namespace, the namespace_dir, etc...)\n. Thanks for pointing that out. I'll fix that as soon as possible (might take a while, I still have a lot to learn about the git/github workflow).\n. I rewrote this commit, closing this\n. I pulled and shared generating the directories (ensuring they exist) and the namespace-string into the base class.\nI also fixed the comment for the package name. Which means that the python files will be slightly different\n. arh, damn, I screwed it up...\n. On my side, It looks like the issue is with the google/master repository : \nif I clone the latest flatbuffers repository in a clean directory with \ngit clone https://github.com/google/flatbuffers.git\nand compile it (on ubuntu) with \ncmake -G \"Unix Makefiles\"\nmake\nI get this file (with mutation methods) : \n// automatically generated by the FlatBuffers compiler, do not modify\nifndef FLATBUFFERS_GENERATED_MONSTER_MYGAME_SAMPLE_H_\ndefine FLATBUFFERS_GENERATED_MONSTER_MYGAME_SAMPLE_H_\ninclude \"flatbuffers/flatbuffers.h\"\nnamespace MyGame {\nnamespace Sample {\nstruct Vec3;\nstruct Monster;\nstruct Weapon;\nenum Color {\n  Color_Red = 0,\n  Color_Green = 1,\n  Color_Blue = 2,\n  Color_MIN = Color_Red,\n  Color_MAX = Color_Blue\n};\ninline const char *EnumNamesColor() {\n  static const char names[] = { \"Red\", \"Green\", \"Blue\", nullptr };\n  return names;\n}\ninline const char *EnumNameColor(Color e) { return EnumNamesColor()[static_cast(e)]; }\nenum Equipment {\n  Equipment_NONE = 0,\n  Equipment_Weapon = 1,\n  Equipment_MIN = Equipment_NONE,\n  Equipment_MAX = Equipment_Weapon\n};\ninline const char *EnumNamesEquipment() {\n  static const char names[] = { \"NONE\", \"Weapon\", nullptr };\n  return names;\n}\ninline const char *EnumNameEquipment(Equipment e) { return EnumNamesEquipment()[static_cast(e)]; }\ninline bool VerifyEquipment(flatbuffers::Verifier &verifier, const void *union_obj, Equipment type);\nMANUALLY_ALIGNED_STRUCT(4) Vec3 FLATBUFFERS_FINAL_CLASS {\n private:\n  float x_;\n  float y_;\n  float z_;\npublic:\n  Vec3(float x, float _y, float _z)\n    : x(flatbuffers::EndianScalar(x)), y(flatbuffers::EndianScalar(y)), z(flatbuffers::EndianScalar(_z)) { }\nfloat x() const { return flatbuffers::EndianScalar(x_); }\n  void mutate_x(float x) { flatbuffers::WriteScalar(&x, x); }\n  float y() const { return flatbuffers::EndianScalar(y); }\n  void mutate_y(float y) { flatbuffers::WriteScalar(&y, y); }\n  float z() const { return flatbuffers::EndianScalar(z); }\n  void mutate_z(float z) { flatbuffers::WriteScalar(&z, _z); }\n};\nSTRUCT_END(Vec3, 12);\nstruct Monster FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {\n  enum {\n    VT_POS = 4,\n    VT_MANA = 6,\n    VT_HP = 8,\n    VT_NAME = 10,\n    VT_INVENTORY = 14,\n    VT_COLOR = 16,\n    VT_WEAPONS = 18,\n    VT_EQUIPPED_TYPE = 20,\n    VT_EQUIPPED = 22\n  };\n  const Vec3 pos() const { return GetStruct(VT_POS); }\n  Vec3 mutable_pos() { return GetStruct(VT_POS); }\n  int16_t mana() const { return GetField(VT_MANA, 150); }\n  bool mutate_mana(int16_t _mana) { return SetField(VT_MANA, _mana); }\n  int16_t hp() const { return GetField(VT_HP, 100); }\n  bool mutate_hp(int16_t _hp) { return SetField(VT_HP, _hp); }\n  const flatbuffers::String name() const { return GetPointer(VT_NAME); }\n  flatbuffers::String mutable_name() { return GetPointer(VT_NAME); }\n  const flatbuffers::Vector inventory() const { return GetPointer >(VT_INVENTORY); }\n  flatbuffers::Vector mutable_inventory() { return GetPointerflatbuffers::Vector(VT_INVENTORY); }\n  Color color() const { return static_cast(GetField(VT_COLOR, 2)); }\n  bool mutate_color(Color _color) { return SetField(VT_COLOR, static_cast(_color)); }\n  const flatbuffers::Vectorflatbuffers::Offset weapons() const { return GetPointer> >(VT_WEAPONS); }\n  flatbuffers::Vectorflatbuffers::Offset mutable_weapons() { return GetPointerflatbuffers::Vector >(VT_WEAPONS); }\n  Equipment equipped_type() const { return static_cast(GetField(VT_EQUIPPED_TYPE, 0)); }\n  bool mutate_equipped_type(Equipment _equipped_type) { return SetField(VT_EQUIPPED_TYPE, static_cast(_equipped_type)); }\n  const void equipped() const { return GetPointer(VT_EQUIPPED); }\n  void *mutable_equipped() { return GetPointer(VT_EQUIPPED); }\n  bool Verify(flatbuffers::Verifier &verifier) const {\n    return VerifyTableStart(verifier) &&\n           VerifyField(verifier, VT_POS) &&\n           VerifyField(verifier, VT_MANA) &&\n           VerifyField(verifier, VT_HP) &&\n           VerifyFieldflatbuffers::uoffset_t(verifier, VT_NAME) &&\n           verifier.Verify(name()) &&\n           VerifyFieldflatbuffers::uoffset_t(verifier, VT_INVENTORY) &&\n           verifier.Verify(inventory()) &&\n           VerifyField(verifier, VT_COLOR) &&\n           VerifyFieldflatbuffers::uoffset_t(verifier, VT_WEAPONS) &&\n           verifier.Verify(weapons()) &&\n           verifier.VerifyVectorOfTables(weapons()) &&\n           VerifyField(verifier, VT_EQUIPPED_TYPE) &&\n           VerifyFieldflatbuffers::uoffset_t(verifier, VT_EQUIPPED) &&\n           VerifyEquipment(verifier, equipped(), equipped_type()) &&\n           verifier.EndTable();\n  }\n};\nstruct MonsterBuilder {\n  flatbuffers::FlatBufferBuilder &fbb_;\n  flatbuffers::uoffset_t start_;\n  void add_pos(const Vec3 *pos) { fbb_.AddStruct(Monster::VT_POS, pos); }\n  void add_mana(int16_t mana) { fbb_.AddElement(Monster::VT_MANA, mana, 150); }\n  void add_hp(int16_t hp) { fbb_.AddElement(Monster::VT_HP, hp, 100); }\n  void add_name(flatbuffers::Offsetflatbuffers::String name) { fbb_.AddOffset(Monster::VT_NAME, name); }\n  void add_inventory(flatbuffers::Offsetflatbuffers::Vector inventory) { fbb_.AddOffset(Monster::VT_INVENTORY, inventory); }\n  void add_color(Color color) { fbb_.AddElement(Monster::VT_COLOR, static_cast(color), 2); }\n  void add_weapons(flatbuffers::Offsetflatbuffers::Vector> weapons) { fbb_.AddOffset(Monster::VT_WEAPONS, weapons); }\n  void add_equipped_type(Equipment equipped_type) { fbb_.AddElement(Monster::VT_EQUIPPED_TYPE, static_cast(equipped_type), 0); }\n  void add_equipped(flatbuffers::Offset equipped) { fbb_.AddOffset(Monster::VT_EQUIPPED, equipped); }\n  MonsterBuilder(flatbuffers::FlatBufferBuilder &fbb) : fbb(fbb) { start = fbb_.StartTable(); }\n  MonsterBuilder &operator=(const MonsterBuilder &);\n  flatbuffers::Offset Finish() {\n    auto o = flatbuffers::Offset(fbb_.EndTable(start_, 10));\n    return o;\n  }\n};\ninline flatbuffers::Offset CreateMonster(flatbuffers::FlatBufferBuilder &fbb,\n   const Vec3 *pos = 0,\n   int16_t mana = 150,\n   int16_t hp = 100,\n   flatbuffers::Offsetflatbuffers::String name = 0,\n   flatbuffers::Offsetflatbuffers::Vector inventory = 0,\n   Color color = Color_Blue,\n   flatbuffers::Offsetflatbuffers::Vector> weapons = 0,\n   Equipment equipped_type = Equipment_NONE,\n   flatbuffers::Offset equipped = 0) {\n  MonsterBuilder builder(fbb);\n  builder.add_equipped(equipped);\n  builder_.add_weapons(weapons);\n  builder_.add_inventory(inventory);\n  builder_.add_name(name);\n  builder_.add_pos(pos);\n  builder_.add_hp(hp);\n  builder_.add_mana(mana);\n  builder_.add_equipped_type(equipped_type);\n  builder_.add_color(color);\n  return builder_.Finish();\n}\nstruct Weapon FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {\n  enum {\n    VT_NAME = 4,\n    VT_DAMAGE = 6\n  };\n  const flatbuffers::String name() const { return GetPointer(VT_NAME); }\n  flatbuffers::String mutable_name() { return GetPointer(VT_NAME); }\n  int16_t damage() const { return GetField(VT_DAMAGE, 0); }\n  bool mutate_damage(int16_t _damage) { return SetField(VT_DAMAGE, _damage); }\n  bool Verify(flatbuffers::Verifier &verifier) const {\n    return VerifyTableStart(verifier) &&\n           VerifyFieldflatbuffers::uoffset_t(verifier, VT_NAME) &&\n           verifier.Verify(name()) &&\n           VerifyField(verifier, VT_DAMAGE) &&\n           verifier.EndTable();\n  }\n};\nstruct WeaponBuilder {\n  flatbuffers::FlatBufferBuilder &fbb_;\n  flatbuffers::uoffset_t start_;\n  void add_name(flatbuffers::Offsetflatbuffers::String name) { fbb_.AddOffset(Weapon::VT_NAME, name); }\n  void add_damage(int16_t damage) { fbb_.AddElement(Weapon::VT_DAMAGE, damage, 0); }\n  WeaponBuilder(flatbuffers::FlatBufferBuilder &fbb) : fbb(fbb) { start = fbb_.StartTable(); }\n  WeaponBuilder &operator=(const WeaponBuilder &);\n  flatbuffers::Offset Finish() {\n    auto o = flatbuffers::Offset(fbb_.EndTable(start_, 2));\n    return o;\n  }\n};\ninline flatbuffers::Offset CreateWeapon(flatbuffers::FlatBufferBuilder &fbb,\n   flatbuffers::Offsetflatbuffers::String name = 0,\n   int16_t damage = 0) {\n  WeaponBuilder builder(fbb);\n  builder.add_name(name);\n  builder_.add_damage(damage);\n  return builder_.Finish();\n}\ninline bool VerifyEquipment(flatbuffers::Verifier &verifier, const void *union_obj, Equipment type) {\n  switch (type) {\n    case Equipment_NONE: return true;\n    case Equipment_Weapon: return verifier.VerifyTable(reinterpret_cast(union_obj));\n    default: return false;\n  }\n}\ninline const MyGame::Sample::Monster GetMonster(const void buf) { return flatbuffers::GetRootMyGame::Sample::Monster(buf); }\ninline Monster GetMutableMonster(void buf) { return flatbuffers::GetMutableRoot(buf); }\ninline bool VerifyMonsterBuffer(flatbuffers::Verifier &verifier) { return verifier.VerifyBufferMyGame::Sample::Monster(); }\ninline void FinishMonsterBuffer(flatbuffers::FlatBufferBuilder &fbb, flatbuffers::OffsetMyGame::Sample::Monster root) { fbb.Finish(root); }\n}  // namespace Sample\n}  // namespace MyGame\nendif  // FLATBUFFERS_GENERATED_MONSTER_MYGAME_SAMPLE_H_\n. Regarding the refactoring effort going on for the flatbuffers library (to make it simpler and better for you code generator writer and for the flatbuffers library maintainer), it would be great in my opinion, if you could remove the dependency on the TD macro (that forces you to touch the parser file, as well as the files of other code generators) and make the rust code generator self-contained in idl_gen_rust.cpp\nYou would mostly have to define your own GetXType methods\nYou can look how it is done in the javascript code generator (no TD in there) or in the Python, php, go code generators\n. Le 02/06/2016 08:52, Joseph Dunne a \u00e9crit :\n\nSure I will do that.\nIf its going to be self contained should I just implement it entirely \nin Rust...in a separate repo perhaps?\nno, your C++ code is fine. @gwvo will tell you if he wants changes (or \nnot) to merge your code in the library.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub \nhttps://github.com/google/flatbuffers/pull/3894#issuecomment-223210095, \nor mute the thread \nhttps://github.com/notifications/unsubscribe/AAGTB97QrMpYN7gKeU5tsuwpDwTqrSjMks5qHn2SgaJpZM4Ir42e.\n. @gwvo I'm not surprised :p\n\nActually, idl_gen_js.cpp doesn't use the TD macro. They use GenType method that is really easy to understand (I'm a big fan) : \nstatic std::string GenType(const Type &type) {\n  switch (type.base_type) {\n    case BASE_TYPE_BOOL:\n    case BASE_TYPE_CHAR: return \"Int8\";\n    case BASE_TYPE_UTYPE:\n    case BASE_TYPE_UCHAR: return \"Uint8\";\n    case BASE_TYPE_SHORT: return \"Int16\";\n    case BASE_TYPE_USHORT: return \"Uint16\";\n    case BASE_TYPE_INT: return \"Int32\";\n    case BASE_TYPE_UINT: return \"Uint32\";\n    case BASE_TYPE_LONG: return \"Int64\";\n    case BASE_TYPE_ULONG: return \"Uint64\";\n    case BASE_TYPE_FLOAT: return \"Float32\";\n    case BASE_TYPE_DOUBLE: return \"Float64\";\n    case BASE_TYPE_STRING: return \"String\";\n    case BASE_TYPE_VECTOR: return GenType(type.VectorType());\n    case BASE_TYPE_STRUCT: return type.struct_def->name;\n    default: return \"Table\";\n  }\n}\nAlso, the go/python/... folks could also avoid the TD macro by defining static const char _ctypename[]  inside their idl_gen__**.cpp file, making their GenBasic method easier to understand\nIMO, idl.h should not contain language specific stuff. I'll fight the TD macro or die in the process. :)\n. I'll send a pr to fix this issue soon (it requires moving methods inside the code generator class and whence the merging of https://github.com/google/flatbuffers/pull/3908). My simple approach (that works for Kotlin) is :\n\"accessor_\" is sanitized to \"accessor__\"\nvalid \"accessor\" is sanitized to \"accessor\"\ninvalid \"accessor\" is sanitized to \"accessor_\"\nIt doesn't require  an union of all protected keywords as it only uses a list of forbidden keywords for each language. And also, it is customizable, which means that it can be replaced by a better approach and permits (in the future) remapping the accessor names to user defined accessor names.\n. I documented the algorithm and gave the variables more meaningful names.\nI also changed the method name from CheckNameSpace to SetNameSpace (as it is what it really does)\nYou are right about code movements : \nit mostly happens when : \n 1) I move methods inside the class \n 2) I clang-format the result\n 3) I do some modifications\nI think that I should do that in 3 separate commits instead of 1. \nBecause the clang-format step is really messing up diffs\nI'll try to rework my next prs that way to see if it makes a difference\n. This pr shares the WrapInNameSpace methods. \nI had to introduce a virtual CurrentNameSpace() method because, js & php always fully qualify their types, while java & csharp only do if the name space of the file is different from the namespace of the type, while C++ uses curr_name_space_...\nI'll wait for your feedback before eventually applying clang-format to all the changes, so as not to pollute the diffs\n. Finally I applied clang format to this pr.\nAs the php and general code generators were using very long lines, this adds quite a few lines \n. I just dropped the last commit making the pr easier to read.\nI'll put the clang formating in a later pr (I have to wait for the code to be merged, right ?) or let you do the clang formating\n. It has just taken me 3 hours to make the rebase (it was a first for me and I hadn't realised that you had changed the namespace related directory where things are saved).\nI fixed the space at the place you pointed out but not anywhere else because we are supposed to clang format the whole generator class anyway (because all methods moved inside) and this will fix all spaces related issues.\nIt would be nice if you could merge this soon because I would really really like to avoid another expensive rebasing operation (and as this pr touches all code of the generators, this is bound to happen for any trivial commit).\n. It's doable (I did it for the Kotlin code generator). I'll soon propose \na fix for enums in java/csharp/C++ ...\nI need that work https://github.com/google/flatbuffers/pull/3908 to be \nmerged before though\nOlivier\nLe 21/06/2016 17:51, johngalt131 a \u00e9crit :\n\nLooking more closely, C++ would pretend to work, but internally it's \nusing int8_t as well.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub \nhttps://github.com/google/flatbuffers/issues/3914#issuecomment-227484034, \nor mute the thread \nhttps://github.com/notifications/unsubscribe/AAGTB8MFPCKsxh0-En_isuzsWCunZKNFks5qOAhlgaJpZM4I6yvl.\n. In the Kotlin Generator, \n- enums have the type enum_def.name \n-  uses userScalarType(enum_def.underlying_type.base_type) for internal representation.\n  where userScalarType upscales the underlying schema type (so we use a signed int to store an unsigned short)\n- serializes it as as wireScalarType(enum_def.underlying_type.base_type) where  wireScalarType uses just the right number of bits to store the value (so a USHORT uses a short on the wire)\n\nAlso Unions are serialized as  enums in an arithmetic progression which means that they can get serialized and deserialized in a very efficient (speed and memory) way with a simple function call, \nWe avoid using a sparse array (which would be rally costly for enums with large values)\nI implemented it that way because I thought that was what you had in mind with your enum_def.underlying_type.base_type (having enough bits to encode the enums). Now, I really hope that you aren't saying that  the spec says that enums shgould be encoded with a uint8. ^^\n. What if we were fixing all implementations at the same time ? \nThe code generated before the fix would still break. \nWould it be acceptable to ask users to update their code ? Or is the only alternativ to forbid enums with more than 256 values ? \n. Sending a 256+ union to a serveur that only supports at most 255 unions will not end well. \nAnd there is no way to work around that without updating the code on the server. \nThere is also something really disturbing with the fact that some bad code generators implementations running for a while should result in crippling the spec/library for everyone, forever. \nWhat if we get things right for 50 code generators and someone somewhere does a bad job at creating the 51th one ? \nAlso users can still have (unofficial) unions with 256+ types by doing what we should have been doing for them : in a Table, put an uint field for their unionType and a [ubyte] field for the union data, init the [ubyte] with the right type, cast and return. If they have to write this code and we refuse to do it for them, they are really going to hate us.\nWhy not forbidding 256+ unions with an error by default \nand allowing 256+ (with a big warning) with a switch for developers that are able to update their code/who write code that will only use next gen flatbuffers, who knows what they do, etc...\nAlso, as a side matter, it would be nice to have a way to version/improve/move forward the flatbuffers library spec, to get rid of some quirks it may have (while preserving backward/forward compatibility ?) \n. I'm not sure I can (big crisis with the wife today)\nCan you do it ? \nAt worst, I can indent things, but I'm not sure I can do much more than that\n. I'll do my best then\n. I reread your comments and I think I understand better what I have to do now but to be sure : \n1) I shouldn't clang format the whole files\n2) I should indent correctly the code that moved inside the class (removing a few constant spaces is easy)\n3) I should reformat (sometimes remove some \\n, add/remove spaces) the method signatures (other's code) that have become shorter because I removed parameters\n4) I should fully format the code I wrote\n5) This means that I should close this pr and make another one\nright ? \n. ok. I'll amend the commit and slowly but steadily improve the formatting of the code I touched then (do expect more commits).\nThat way, it'll also allow me to implement in parallel the fix for csharp/java name clash problem\n. I tried to rebase but there were conflicts and I couldn't make sense of the code in the merge process (messed functions). So I started anew the clang-formating process.\nSo, I first clang-formated the base class -> it produced a nice diff\nThen I clang-formated the whole cpp generator -> it produced a nice diff (but it also formatted stuff I didn't touch... the result is quite nice so I will wait for your feedback if you want to merge it or if you want me to ammend the formatting changes to code I didn't touch)\nThe js, php and general files will require particular care though, so it will not be possible to clang format the whole file and expect nice diffs -> I will manually and slowly implement the changes\n. Also, I'm adding the generated classes for java that haven't been added by previous commits (change for enums)\n. double is 8 bytes in java...\n. The enum type of the VehicleData Union is exposed in the \ncsharp/java/kotlin generators. Probably in the other generators too (if \nI'm not mistaken).\nWhat language are you using flatbuffers for ?\nOlivier\nLe 02/07/2016 01:04, Hari Shankar a \u00e9crit :\n\nOk, here is a contrived example:\nSay there is a union called VehicleData, which is\nunion VehicleData {\nCarData,\nBusData\n}\ntable Vehicles {\nvehicleId: int,\nvehicleData: VehicleData\n}\nLet's say we store Vehicles in some file, and I want to define schema \nfor a query language which can query to get all Vehicles that are \ncars. So the schema for the query might be something like:\ntable Query {\nvehicleType: VehicleDataType\n}\nVehicleDataType and VehicleData have a one-to-one mapping, so if the \nenum was exposed, I could have used it directly.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub \nhttps://github.com/google/flatbuffers/issues/3919#issuecomment-230065241, \nor mute the thread \nhttps://github.com/notifications/unsubscribe/AAGTB64WE6HCc3vgnnCSBjPHL_pSciamks5qRZ0VgaJpZM4JCo8D.\n. ah I see. Thanks for the explanation.\n. I build it on ubuntu 16.04 with these exact commands (without sudo \nbefore make).\n\nI think that constexpr is a C++ 11 feature.\nDoes your version of make support this ?\nMaybee youneed to update your build tools ?\nI have those versions :\ncmake --version\ncmake version 3.5.1\nmake --version\nGNU Make 4.1\nLe 07/07/2016 22:28, Snake Plissken a \u00e9crit :\n\nany help would be great!!!\nsteps and errors follow...:\n$ git clone https://github.com/google/flatbuffers.git\n$ cd flatbuffers\n$ cmake -G \"Unix Makefiles\"\n-- The C compiler identification is GNU 4.6.3\n-- The CXX compiler identification is GNU 4.6.3\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Configuring done\n-- Generating done\n-- Build files have been written to: \n/home/rattlesnake/test/GOOGLE_FLAT_BUFFERS/flatbuffers\n$ sudo make\nsudo make\nScanning dependencies of target flatbuffers\n[ 2%] Building CXX object CMakeFiles/flatbuffers.dir/src/idl_parser.cpp.o\n[ 5%] Building CXX object \nCMakeFiles/flatbuffers.dir/src/idl_gen_text.cpp.o\n[ 8%] Building CXX object CMakeFiles/flatbuffers.dir/src/reflection.cpp.o\n[ 11%] Building CXX object CMakeFiles/flatbuffers.dir/src/util.cpp.o\nLinking CXX static library libflatbuffers.a\n[ 11%] Built target flatbuffers\nScanning dependencies of target flatc\n[ 14%] Building CXX object CMakeFiles/flatc.dir/src/idl_parser.cpp.o\n[ 17%] Building CXX object CMakeFiles/flatc.dir/src/idl_gen_text.cpp.o\n[ 20%] Building CXX object CMakeFiles/flatc.dir/src/reflection.cpp.o\n[ 23%] Building CXX object CMakeFiles/flatc.dir/src/util.cpp.o\n[ 26%] Building CXX object CMakeFiles/flatc.dir/src/idl_gen_cpp.cpp.o\n/home/rattlesnake/test/GOOGLE_FLAT_BUFFERS/flatbuffers/src/idl_gen_cpp.cpp:209:38: \nsorry, unimplemented: non-static data member initializers\n/home/rattlesnake/test/GOOGLE_FLAT_BUFFERS/flatbuffers/src/idl_gen_cpp.cpp:209:38: \nerror: \u2018constexpr\u2019 needed for in-class initialization of static data \nmember \u2018cur_name_space_\u2019 of non-integral type\n/home/rattelsnake/test/GOOGLE_FLAT_BUFFERS/flatbuffers/src/idl_gen_cpp.cpp: \nIn member function \u2018virtual bool \nflatbuffers::cpp::CppGenerator::generate()\u2019:\n/home/rattlesnake/test/GOOGLE_FLAT_BUFFERS/flatbuffers/src/idl_gen_cpp.cpp:89:5: \nerror: \u2018cur_name_space_\u2019 was not declared in this scope\n/home/rattlesnake/test/GOOGLE_FLAT_BUFFERS/flatbuffers/src/idl_gen_cpp.cpp: \nIn member function \u2018virtual const flatbuffers::Namespace* \nflatbuffers::cpp::CppGenerator::CurrentNameSpace()\u2019:\n/home/rattlesnake/test/GOOGLE_FLAT_BUFFERS/flatbuffers/src/idl_gen_cpp.cpp:211:48: \nerror: \u2018cur_name_space_\u2019 was not declared in this scope\n/home/rattlesnake/test/GOOGLE_FLAT_BUFFERS/flatbuffers/src/idl_gen_cpp.cpp: \nIn member function \u2018void \nflatbuffers::cpp::CppGenerator::SetNameSpace(const \nflatbuffers::Namespace/, std::string/)\u2019:\n/home/rattlesnake/test/GOOGLE_FLAT_BUFFERS/flatbuffers/src/idl_gen_cpp.cpp:854:9: \nerror: \u2018cur_name_space_\u2019 was not declared in this scope\n/home/rattlesnake/test/GOOGLE_FLAT_BUFFERS/flatbuffers/src/idl_gen_cpp.cpp:860:9: \nerror: \u2018cur_name_space_\u2019 was not declared in this scope\n/home/rattlesnake/test/GOOGLE_FLAT_BUFFERS/flatbuffers/src/idl_gen_cpp.cpp:860:75: \nerror: unable to deduce \u2018auto\u2019 from \u2018\u2019\n/home/rattlesnake/test/GOOGLE_FLAT_BUFFERS/flatbuffers/src/idl_gen_cpp.cpp:869:19: \nerror: unable to deduce \u2018auto\u2019 from \u2018old_size\u2019\n/home/rattlesnake/test/GOOGLE_FLAT_BUFFERS/flatbuffers/src/idl_gen_cpp.cpp: \nIn member function \u2018virtual const flatbuffers::Namespace* \nflatbuffers::cpp::CppGenerator::CurrentNameSpace()\u2019:\n/home/rattlesnake/test/GOOGLE_FLAT_BUFFERS/flatbuffers/src/idl_gen_cpp.cpp:211:65: \nerror: control reaches end of non-void function [-Werror=return-type]\ncc1plus: all warnings being treated as errors\nmake[2]: * [CMakeFiles/flatc.dir/src/idl_gen_cpp.cpp.o] Error 1\nmake[1]: * [CMakeFiles/flatc.dir/all] Error 2\nmake: *** [all] Error 2\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub \nhttps://github.com/google/flatbuffers/issues/3926, or mute the \nthread \nhttps://github.com/notifications/unsubscribe/AAGTB0bkotP5pD2OehTk3e5iYXqztNVDks5qTWGBgaJpZM4JHd9U.\n. I'm willing to (also the issue comes from my code). I'll try to send a pr tonight. \n. @Rattelsnake could you please retry with the latest snapshot and report if it builds for your configuration (and if not, paste the error message) ?\n. I'll do it in all other places then. Thanks for the quick feedback\n. That's the way clang-format formats it for me.\nHow should it be formated ?\n. Le 13/07/2016 18:32, Wouter van Oortmerssen a \u00e9crit :\nSure. But I am saying if I have code that reads/writes FlatBuffers, \nthat code will work regardless of wether it is compiled against a \nclass generated with |--android-java| or without?\nIntDef, @Nullable @NotNull, etc.. only affect the generated code and not \nthe serialized bits.\nThe java code that is used for Android is slightly different from \nstandard java code\n(because android only uses the java api, not the java binaries, the java \nclasses/bytecodes are translated to dex/dalvik/art bytecode)\nIf so, go ahead and make a PR. I'd suggest calling the flag just \n|--android|, that way if we want to add Android specifics to C++ or \nwhatever, we don't need yet another flag.\nAlso, please retain |names[]|, as there is another PR ongoing to make \nit public. We want to keep these strings available on all platforms \nfor uniformity.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub \nhttps://github.com/google/flatbuffers/issues/3933#issuecomment-232411259, \nor mute the thread \nhttps://github.com/notifications/unsubscribe/AAGTB5C1qyvcXKMt44kfHz04lOIAhzoqks5qVRMOgaJpZM4JItqe.\n. Le 15/07/2016 12:52, Louis Cognault a \u00e9crit :\nI personally think it's not a problem to generate java code for non \nAndroid runtime, and again for Android runtime. Non Android java \nruntimes will probably be servers, and it's as if the server was not \nrunning java, you generate for the language that the runtime \nunderstand. Android is quite different from other JVMs.\n\nThis makes sense to me.\nAlso, the aim of introducing a base class for code generators was to \nallow reuse/avoid code duplication and simplify the code base.\nIn this aspect, it might be interesting to have a class that holds most \nof the common java stuff\nand to just override a few methods to fine tune the code generator for :\na) java with true enums (some people might want the safety)\nb) java for android (some people might want the nice \nNullable/NotNull/IntDef features)\n...\nc) java with int instead of enums (some people might want the performance)\nd) who knows with java 9, java 10 coming it might be interesting to \nchange the public api to make flatbuffers a lot easier/safer to use (I \nsaw an effort to make the api more pythonic, the kotlin api is \ndefinately simpler with just 1 line to create a complex flatbuffer...)\n\nI mean, users should not run some code designed for Android on non \nAndroid runtimes, and vice-versa, hence the suggested |--android| \noption to exclude if the code is not going to run on Android.\nI can see the point of @gwo though :\nwhat if somebody want to generate C++ code for the android ndk ?\nThey might want to have nice android features kbaked into their \ngenerated code\n\nand --cpp --android avoids a switch compared to --cpp-android\nAlso, cpp/java is for language and  android for a platform (somewhat). \nThese might be 2 orthogonal concerns\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub \nhttps://github.com/google/flatbuffers/issues/3933#issuecomment-232921745, \nor mute the thread \nhttps://github.com/notifications/unsubscribe-auth/AAGTBwq0KDc-6wLlthzY0Qo_fozkv1nyks5qV2aCgaJpZM4JItqe.\n. As @gwo said the generated code is there to handle already populated flatbuffers (in java, bytebuffer rightly written by the flatbuffers generated code....ByteBuffer.allocate(4) might not be a rightly built bytebuffer; it might not work for complex flatbuffers that will access bytes fartyher than the first 4 bytes)\n\nYou should probably handle your problem differently : \nIn your case, I would probably use a ThemeWrapper class that would wrap a private Theme class \nThe Them class is used to read/write flatbuffers while the ThemeWrapper class hides all the Theme method and exposes only the api you need\nhandling a null bytebuffer (and defaults) could then be done at the ThemeWrapper stage\n. Also, if you code for android and if you use kotlin, you might want to use the kotlin code generator implemented there https://github.com/google/flatbuffers/pull/1190\n (If you do, I would love to get feedback), it is much easier to use, terser and you get null safety \n. First, please wait for what @gwo has to say. I happened to write the \nkotlin code generator for flatbuffers but I don't have his wisdom when \nit comes to the library, so I can only give you my opinion (which is not \nofficial) :\nLe 15/07/2016 15:46, Louis Cognault a \u00e9crit :\n\nI don't use kotlin yet, but I don't see the benefit of flatbuffers \ndefault values if I have to write a boilerplate friendly class myself, \nwhich anyway, can't read flatbuffers default values as it will always \nget an NPE.\nThe problem is that, at the moment, the generated classes are designed \nto read from an existing (not null) message\nI mean, if I create other tables, I have to rewrite each default \nvalues for each table in a wrapper class?\nyou want to implement your defaults in the schema file and only use that.\nThe best solution I know at the moment is the first one that you \nsuggested (create a rightly built bytebuffer)\nI don't get how you find it more practical that the ability to \ninstantiate a working empty flatbuffers table with the |new| keyword\nPlease, tell me, if I want to use it in java, I have to write another \nclass manually and copy paste all the default values and created \ndelegates for the class under the wrapper, or\nit would work but it would probably be a lot more work and hassle as \nusing your first solution for a complex flatbuffer\nstop using java?\nno, the problem will be the same with kotlin. Sorry for misleading you.\nDon't you think it deviates from flatbuffers philosophy which is \ncloser to Don't Repeat Yourself and which aims to keep things simple?\n\nI see your point and I think that I understand your need.\n\nAnyway, I'm lucky that it's so easy to edit flatbuffers behavior as \nthere's no read-only dependency, so I added the line I was talking \nabout in the |Table| class which checks if |bb| is null, and it works \nflawlessly.\nThis should work for scalars, strings and vectors in a Table but what \nabout structs though ?\nThey are inlined in a flatbuffer and don't have defaults...\nmethinks the generated code will try to look inside the bytebuffer to \nfetch the values\nI'm ok to do a PR if you think it's ok to add a null check in the \n|__offset(\u2026)| method.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub \nhttps://github.com/google/flatbuffers/issues/3945#issuecomment-232954839, \nor mute the thread \nhttps://github.com/notifications/unsubscribe-auth/AAGTB3d0gOjSgsAYisPROYdJOIwuxHXSks5qV489gaJpZM4JM6B5.\n. No, they don't. A struct is never null. They are inlined. If the root of your flatbuffer is a struct that holds an int, you get a bytebuffer with at least 4 bytes (might even be more because of other flatbuffer features (file identifyer)) .\n. My bad, you are right (I tried to be of help but failed miserably, sorry).\n\n1) In a Table, a Struct field can be null.\n2) as only tables can be root types (and not struct... I had forgotten \nabout that),\nit looks like you can get your defaults for an empty flatbuffer with \nyour single line.\nLe 16/07/2016 16:47, Louis Cognault a \u00e9crit :\n\n@Lakedaemon https://github.com/Lakedaemon Where did you see that \nstructs are never null? From what I see by reading generated code, \nthey can be, and the line I added is struct safe in this regard.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub \nhttps://github.com/google/flatbuffers/issues/3945#issuecomment-233133735, \nor mute the thread \nhttps://github.com/notifications/unsubscribe-auth/AAGTBwjVBDRfOzl9yOAUr5No1k1brGljks5qWO8dgaJpZM4JM6B5.\n. Le 16/07/2016 18:02, Louis Cognault a \u00e9crit :\n@Lakedaemon https://github.com/Lakedaemon Thanks to you, I checked \nthat it would work properly with structs, so thanks! Last think to \ncheck is for unions and their type. Do you have an idea?\n\nApparently, with java generated code, with offset = 0\nunions return null and unionType returns 0 = NONE\nSo, I guess that it would work.\nStill, modifying the offset function that way would incur a slight \nperformance hit\nIt's trading off convenience against performance :\nyou save a few lines of code and a small bytebuffer allocation when \ncreating defaults at start (once),\nbut you pay the price every time you access a flatbuffer field (and some \npeople access million+ times their flatbuffers)\nAlso, another reason not to do it this way (though it works) is that you \nlose null safety :\nThe kotlin generator is garanted by the language and the  compiler to be \nnull safe.\nIf it was allowed for the bytebuffer to be null, I would have to put \nnull checks everywhere to make the generated code compile (and that \nwould be another performance hit).\nThe java code generator doesn't have this problem as java doesn't care \nat all about null safety\n(given a Table variable, it could be null or not null, you have no way \nto know till you test it)\n(in Kotlin, you have Table that are garanted to be not null and Table? \nthat can be null)\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub \nhttps://github.com/google/flatbuffers/issues/3945#issuecomment-233137150, \nor mute the thread \nhttps://github.com/notifications/unsubscribe-auth/AAGTB6Jw491Z5BTqQQuB5pA4CftKPBgSks5qWQB-gaJpZM4JM6B5.\n. Also, how can 2,000,000,000 null checks only take 3ns when a cpu only has about 3,000,000,000 cycles in a second ? \n\nThose null checks in the test might have been ignored by the compiler and I wouldn't trust that benchmark...\n. It's a slightly different issue than what was suggested before though :\nInstead of having a name of the field clash with a keyword of the target \nlanguage,\nit looks like the name of the field clashes with the name of a user \ndefined (parent) flatbuffer type.\n(I may be wrong since I just had a fast peek at the code)\nIn this case, should the library fix it for the user ? should flatc \nissue an error ?\nshould the library or the user fix it by slightly changing the name of \nthe field ? (or of the type ?)\nby appending, say, an underscore...\nLe 18/07/2016 22:15, Wouter van Oortmerssen a \u00e9crit :\n\nAhh yes, we intend to do a better job filtering these kinds of \nidentifiers out, but haven't gotten to it.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub \nhttps://github.com/google/flatbuffers/issues/3949#issuecomment-233444842, \nor mute the thread \nhttps://github.com/notifications/unsubscribe-auth/AAGTB7vfbd156iNMmmeudynpGRHDQUx-ks5qW97hgaJpZM4JOGcW.\n. I ported flatbufffer to kotlin (0.9) in mai 2016, see there https://github.com/google/flatbuffers/pull/1190\n(closed for lack of activity). Feel free to use it.\n\nI tried to get it merged, but there were way too many obstacles for that so I lost interest. And I won't spend anymore time trying to get it merged but, If a kotlin port gets eventually merged, I'll gladly contribute\nThe kotlin port is fully working and also more type safe (avoiding integers for flatbuffer construction) than the java branch of flatbuffers. Also, it is much simpler to use as it has a kotlin friendly api. \nA kotlin user should definetely NOT use the java branch of flatbuffers (but you can if you like pain)\nWith the release of kotlin 1.0 and lately 1.3 (typealiases, inline classes), there is still some room to make the port even better.\nAlso, If I'm not wrong, some of my merged commits in the past allow supporting different versions of kotlin for flatbuffer (so you can have fb for kotlin 1.0, fb for kotlin 1.3, ..)\nLastly, the last commit of my port might not be the best as I introduced a Writer class which (retroactively) was probably unneeded. But the commits before that implemented kotlin for flatbuffer without a Writer in a clean way. So, don't hesitate to look at all commits to see what you can salvage and what you can't.\nBest regards\nOlivier binda. I had a quick look at the flatbuffer codebase and it looked like they warmed up to the idea of having (language specific) code writers (there is a CodeWriter class now that wasn't there 3 years ago). \nSo, my last commit should be good too with minor modifications.\nI would really like to see kotlin supported for flatbuffer as it would benefit kotlin and the fast expending kotlin community (3 years ago, kotlin was a new language so maybee it was not a prioriry to the flatbuffer guys to get it supported. Now that google has endorsed kotlin and uses flatbuffer in tenserflow lite, maybee it will be easier to get it merged as flatbuffer looks like a google backed project (though it may not be)).\nWith (really) minor changes my fork should be up and running in no time and support the latest features of Kotlin. I'm available and willing by mail to discuss technical details/changes\nI forgot that Kotlin got support for unsigned values in version 1.3. \nAs it wasn't available 3 years ago, I had to implement it using the available bit operations of the time,\nbut now, proper unsigned value support (powered by inline classes underneath) should be implemented for kotlin in flatbuffer\nAlso, another reminder : DO NOT use the java branch of flatbuffer inside kotlin software : \n1) it is not null safe (the kotlin port is)\n2) it will prevent you kotlin code to be compiled natively or for the browser (the kotlin port enables that)\n3) the Java api is gruesome (it will still take a few java iterations to get java's api on par with kotlin, now that they are on the right track), look at the documentation part of the kotlin branch to look at how a complicated flatbuffer should sanely be constructed\nAlso, there are some really bright ideas inside flatbuffers :\n1) no parsing, mmap and everything is ready for computations\n2) a flatbuffer may (somewhat) implement a type system\n3) schema are flatbuffer (this one is mindboggingly awesome)\nThere are lots of things that may be possible with flatbuffer as an underlying format\nBut there are a few caveats though : \n1) Unions suffer from a bad design and we all have to work around that now :/\n2) strings : flatbuffers want to be an universal format for all languages, but languages have very varying features and don't always use the same format (Java use modified utf16, C++ use utf8). As a result, the flatbuffer api for strings will be useless for many languages. You'll have to work around that with a buffer of other types (shorts for java, kotlin, groovy, scala and friends) and a cast (to chars and CharSequence)\n3) There might be a performance issue with enums but I'm not sure (it was 3 years ago, I don't remember, I may be wrong)\n4) Flatbuffer has initially be released in 2014. One wonders why it takes so much time to support more languages and features, if it is so groundbreaking and revolutionary. Especially since implementing support only requires writing strings. \n. Yeah. You are right about that. \nMost of the work already done should be (with non-major effort/pain) easily transformed into pure kotlin though\n(as flatbuffers is mostly about turning a binary ByteBuffer (blob), into a structured Type with thje right interfaces, lots of bits operations and integer computations (for index, padding, width, complicated but luckily already done).\nThere should still be some work with adapting the api/structures to the various platform.\nBut that's the fun part (actually writing a kotlin code generator for flatbuffer is fun work)\nThere should be great benefits for (hopefully) no major effort.. I would like to amend the previous comment. \nThough I understand the theory about multiplatform kotlin project and though I have played a little bit with that, I don't have enough experience with multiplatform project and I don't have the necessary knowledge and practical experience about the js, objc and native platforms to give good enough advice about those platforms.\nFor example, though I said that it is possible to write pure kotlin code for multi-platform fb, it won't have the best performance on all platforms. To get that, it will (obviously) be necessary to adapt the api and the classes to each target. \nOther fb targets, like might be very good inspiration on how to do things on non-jvm targets like \nC++ or go for kotlin/native and kotlin/objc\njavascript for kotlin/js\nSo, it is not that bad as it is not like starting from scratch.\n. Your made your points. I can understand that.\nOn my side, my first PR to add (experimental) kotlin support was made the 28 Nov 2015.\nI quit after the 19 Apr 2016, without managing to get kotlin support merged because I was burning out (the barrier to contribute to fb is too high, IMO)  and also, because of \n\nDo you know anyone in the Kotlin community that is somewhat of an expert on the language that would want to help with a code review of this PR?\n\n(and though I did contact andrey Breslav to ask him for a review, the jet brain team never did one).\nImagine if you had to go and ask Bjarne Stroustrup for a review, to contribute to some project on the internet...\nIt could have gone differently : \n1) merge the code with an experimental tag (warning that it may not be ready for production use), that way other kotlin users would have used it and sent feedback, improvements, bug reports, code reviews from experts like you wanted\n2) IF the code was good enough and battle tested, it could then be promoted to \"stable\"\n3) by now, you could have had 3 years of test and feedback for the kotlin support\nBut it is your right to lead fb differently. This guy https://github.com/google/flatbuffers/pull/5082 and his pr won't probably come back though ;)\n. Also, the Go and Kotlin \"revolutions\" are mostly about api design, productivity, safety, ...\nIf all we wanted was absolute performance, we would still be developping in C++.. 1) The Kotlin code produced by the generator was never reviewed for the reasons you mentioned (there is no knowledge of kotlin among the mergers of fb). Yet, what preventing merging and made reviews so long was apparently the kotlin side. Isn't something is wrong here ?\nAmong all the people working at google, it was not possible to ask to a googler that knows a bit of kotlin to do a proper review ? for a google supported project ? that serves the interest of google ?\nOr maybee the google-employed fb mergers cannot ask other googlers for help but contributors should ?\nWhen you pay an item in a store, you come with a banker that vouchs for you or you let the credit card device call the bank to make sure there is money on the card ?\n2) With no knowledge nor experience of a target langage, arguing that a long time user of the target language wants to make things his way (maybee the way the target language is meant to be used and not the C++ way) doesn't feel of ? \n3) performance and allocations : \nIf the target language doesn't have primitives (like kotlin) but only objects allocated on the heap (like Kotlin), you can't avoid allocations, even for ints because ints are objects in kotlin. \nWhat you want to avoid is BIG allocations (there is a lot of reuse in the short allocations, they are fast)\nAnd so, allocating memory to have an int, or allocating memory to have an (Type)-> Int function whose code is only \"return int\" is the same performancewise but the later makes things type safer for fb construction especially since lambdas can be inlined in kotlin (if you use them the right way, like when building a complete fb in one line). So you get the same performance as int but with added type safety, and nice completion from the ide (you don't get that with ints)\n4) 10 target languages barely makes fb a cross language serialization/deserialization library, like it is marketted. If you have code that isn't written in these 10 langages you hit a wall.\nAlso, some of those code gererators are old :  java 11, go 2, etc.. are coming.\nDoes fb support language versioning ?\nThe efficiency of flatbuffer comes within the limits of the target language. \n5) But everything comes down to \n\na) people will start using it, and if the code still has bugs or the API will still change, that has to be avoided as much as possible, experimental tag or not, and b) because many PRs are made by volunteers and we can't count on them finishing the job. They have to have a minimum bar of finished-ness when merged.\n\nI would argue that a working code generator that is used for production has a minimum bar of finished-ness.\nMost code generators are written by volunteers, most would maintain them (because they use them) and keep on improving them (because they use them in projects) if given a (real) chance. Just don't drive them away.\nIf fb can't count on volunters, fb is screwed because the fb people cannot write code generators  nor maintain them (fb people don't use the other weird languages) nor improve them.\nI bet that, among the 10, the only decent fb code generator is the C++ one (because it has a contributor, with deep C++ knowledge and merging powers). \n6) and \n\nAnd it is cool that Go and Kotlin are about productivity etc, but FlatBuffers isn't. The whole point of FlatBuffers is efficiency.\n\nThe whole point of fb is efficiency WITHIN the limits of the target language. \nThe flatbuffer api for the target language MUST satisfy the necessary language-friendliness to be usable in the language (or we would just usea C code generator and link it's foreign generated code)\nC++ or Java verbosity and 10 lines to do something that is not safe and ARGUABLY more efficient is a big no no (and using UNSAFE methods like it was suggested in the go thread is not acceptable for a lot of business out there). . > There are cases where a feature can be both easy to use and also fast. That's great. But if the two are at odds in FlatBuffers, the latter has priority.\nBut you could have the best of both world. \nYou could have a safe and usable code generator 1 for language A\nAnd a mega fast, unsafe code generator 2 for language A\nIt is not duplication of code (some code can be shared) as it brings 2 different functionnalities\nAnd you wouldn't have double the work, because you would not be the one maintaining it\n. Anyway, good luck to fb. (end of rant). ok, it makes sense\n. I will fix indentation, etc...\nsigh... transition just became harder because of the \"_\" at the end of instance variable. \nNow we will have to rename most of the variables used by all code generator writers in all files...\n. Yes. You are right\n. I cut this code and pasted it there (this is moved old code). \nAs I didn't want clang-format to mess this old code, so that git could correctly display was has changed and what hasn't (it still doesn't look like I thought it would look though).\nI'll indent it with clang-format then (everything should be indented 2 spaces to the right as this function was moved inside a class) but I fear that it will mess things for the pr. :/\n. It hasn't been added. It currently lives at lines 897-900 in your current code base in the idl_gen_cpp.cpp file\nIt has just been moved before the class because it is used inside it.\n. Yes, sorry about that. I'll fix it as soon as possible. \n. limit can be equal to 0 and it happens :  at the start and the end of the C++ file, it is supposed to be equal to 0.\nAs we always have 0 <= limit <= s1 and 0<= limit <= s2p, \nthe conditions j = s1 and  j > limit ensures that we always have --j >= 0 in the for loop\n. I gave more meaningful names to the local variables and added a few comments describing the method. Hopefully, it'll be enough (or I'll add more explanations).\n. ",
    "cogumbreiro": "For what is worth, in C++ it also makes sense to define the hash operator.\nSince Flatbuffers data structures can be converted to JSON, adding a toString method that emits JSON doesn't seem to be a big gap.\nI am wondering, since there is interest in maintaining the generated code base small, then maybe it would beneficial to add support for extension points in code generation? In which case, the base functionality is the ability to write add-ons/plugins that add a native method to the generated code.\n. Indeed I  do not think a copy constructor to be the best idea.\nI am currently using the following template code to provide a copy functionality:\nc++\n // Copy a FBS object.\ntemplate<typename T>\nstruct copy {\n    flatbuffers::Offset<T> operator()(flatbuffers::FlatBufferBuilder &, const T&) const;\n};\nSo, the copy operator expects a reference to a FlatBufferBuilder instance and a reference to the object being copied.\nIf the copying functionality is added to the generated code, then a method flatbuffers::Offset<T> T::Copy(flatbuffers::Offset<T> operator()(flatbuffers::FlatBufferBuilder&) const seems more appropriate to me.\n. I am imagining the code generation for method Copy(flatbuffers::Offset<T> operator()(flatbuffers::FlatBufferBuilder&) to be almost the same as what's generated for Create${Table}.\nSimilar in the sense that _builder._add_${field} would be akin to Copy${Table}(${field}, _builder).\nAm I missing something obvious? Is the problem circular dependencies?\n. Sorry, I wasn't clear enough.\nThe project where I am using flatbuffers has a requirement where Flatbuffer tables must be copiable.\nTo this end I defined that template function that each table types used in the application must specialize.\nThen I can use an instance of copy<T> to \"clone\" any flatbuffers table (which generates an offset instance).\n. The bug is way more subtle than I had thought.\nThe test case below is failing, which means that Verify is yielding true when it shouldn't.\nTEST_CASE(\"get_root\") {\n    uint8_t data[256];\n    memset(data, 0, sizeof(data));\n    auto check = flatbuffers::Verifier(data, sizeof(data));\n    REQUIRE(!check.Verify<flatbuffers::uoffset_t>(data));\n}\nNow, if I run gdb I get the following behaviour, which I fail to understand. I've tried\n(gdb) print buf_\n$43 = (const uint8_t *) 0x7fffffffce20 \"\"\n(gdb) print elem\n$23 = (const void *) 0x7fffffffce20\n(gdb) print end_\n$24 = (const uint8_t *) 0x7fffffffcf20 \"\"\n(gdb) print 0x7fffffffcf20 <= 0x7fffffffcf20 - 4\n$30 = false\nprint elem <= end_ - elem_len\n$26 = true\nThe last expression  should  yield false and it is not.\nI have also tried rewriting the expression as follows to no avail:\nc++\n    return Check(elem >= buf_ && reinterpret_cast<const uint8_t*>(elem) + elem_len <= end_);\nAny ideas?\n. Sorry, ignore that last comment, I misread the e as an f, thus viewing buf_ and  end_ as the same address.\nThe test case  I  was  using is wrong. I should be providing an empty buffer instead of one with  size 256,  as that is what triggered the error on my code.\n. The error results in a segfault/valgrind, and it is not catched by a test unfortunately.\nThe error is triggered in  this code:\nc++\nvector<uint8_t> buffer;  // given a buffer of size  0\nauto data = buffer.data(); \nauto check = flatbuffers::Verifier(data, buff.size());\nreturn check.VerifyBuffer<T>() ? flatbuffers::GetRoot<T>(data) : nullptr;\nThe code  crashes inside check.VerifyBuffer<T>().  Maybe I'm invoking the wrong method?\n. OK, I got it. The problem is on my end, by providing a NULL buffer.\nDo you think that Verifier should  account for that case?\n. Yes. I've added a guard before assigning the data variable that returns a nullptr in case buffer.empty().\nThanks for merging the code.\n. ",
    "krysanify": "Hi, I'm using flatc.exe 1.3.0 and I don't see --gen-compare option.\nAny word when it will be available? Thanks.\n. ",
    "q10": "Upvoting this feature as well.  I wouldn't mind if it had an interface similar to google::protobuf::util::MessageDifferencer in protobuf.. ",
    "pungoyal": "Was this ever implemented? Cant find a reference in flatc documentation.. Was this ever implemented? Cant find a reference in flatc documentation.. ",
    "aldanor": "Also encountered this recently; would be definitely nice to have this, even if as a standalone 'deep-copy' function/method.. ",
    "dominikandreas": "From an end-user perspective, I can't quite understand how this is not a native functionality. Sure, maybe it's necessary to generate more code to support it, but considering how much of a crucial part it would be to some projects makes it definitely worth it from my perspective. I was hoping to making the switch from protobuf in a small project and can't invest the time to implement this myself, but I'm sure there are many others like me who would love to have this. ",
    "Reimerei": "I adjusted the pull request.\nThe tests do not pass any more. I could not figure out where I need to adjust them.\n. ",
    "teemuandersen": "\"all\" seems to be the usual way with generated code but \"unused\" is enough to solve this issue.\n. Changed it to \"unused\"\n. ",
    "vijairaj": "Yes, either way it would be great to have this feature.\n. How do we get this done? Shall I send a pull request?\n. Implemented in #271.\n. @maticmeznar, You might want to have a look at the tests. At least for C++, I found the test cases more useful than the docs.\n. @maticmeznar, You might want to have a look at the tests. At least for C++, I found the test cases more useful than the docs.\n. Since there was already an option --no-prefix to control enum generation I was inclined to use another one specific to enum to make it explicit. How about replacing --no-prefix with something like --gen-enums noprefix|scoped though that would be a breaking change. If that doesn't go well, I can change the option per your suggestion to --use-latest-cpp or --gen-latest-cpp.\n. ",
    "S-YOU": "how will you use the rest of bits, btw?\n. ",
    "maticmeznar": "The entire documentation needs to be overhauled and rewritten for a common developer, not an expert. Some of the things that are not well (or not at all) explained are: root_type, vector field accessor, vector, getroot, offset, builder, classes in Go, attribute, enum, union, default values, scalar, non-scalar (why bother a developer with the concept of \"scalars\" and endinness). There is a huge lack of examples with explanations given line-by-line. There is simply not enough information to understand how to use FlatBuffers properly. Godocs are useless.\n. I just spent another hour trying to use Flatbuffers in Go and I have nothing but frustration and anger to show for it.\n. Ok, I came up with the following example:\nFor the following pseudo JSON, how can someone (in Golang):\n- write a schema\n- compile a schema\n- set each value\n- serialize the whole thing into a []byte\n- unserialize from []byte\n- retrieve each value \n{\n    \"email\": \"someone@example.com\", // utf8 string\n    \"age\": 42, // uint8\n    \"regTs\": 1421539200, // uint32\n    \"hobbies\": [\"sprinting\", \"board games\", \"baking\"], // array\n    \"alive\": true, // bool\n    \"skills\": { // map\n            \"golang\": 3, // uint8\n        \"linux\": 6, // uint8\n        \"itsec\": 6 // uint8\n    },\n    \"authHash\": \"b3VyZWhnMHI4ZmpzJ2QgZjl3YSB0OTQzdyA5IG9pZg==\", // []byte\n    \"balance\": -664926102 // int64\n}\n. ",
    "rubber-duck": "I agree with maticmeznar - I had to wade trough tests to figure out how stuff works just enough to get something working - still don't understand the logic behind the API.\nFor example one non-obvious thing is why does vector of tables generate c++ definition as vector> - from what I seen you can iterate using iterators (ie. for(auto it = vector.begin( ) ...) ) but using for(auto& object : vector) will iterate trough offsets and it's not obvious why or how to transform offset to table instance.\n. I agree with maticmeznar - I had to wade trough tests to figure out how stuff works just enough to get something working - still don't understand the logic behind the API.\nFor example one non-obvious thing is why does vector of tables generate c++ definition as vector> - from what I seen you can iterate using iterators (ie. for(auto it = vector.begin( ) ...) ) but using for(auto& object : vector) will iterate trough offsets and it's not obvious why or how to transform offset to table instance.\n. Wouldn't it be better if Vec3.create_Vec3() just became Vec3.create() (also for Monster.get_root()) ? It seems like the whole builder API after your refactor is designed to be used trough module eg. Monster.start() (which is great :+1: ) so might as well be consistent.\nAlso I don't see why you're leaving CamelCase in builder methods when you converted to snake_case - ie. prepend_Uint32 is kind of an eyesore and PEP8 says all lowercase - admittedly it makes prepend_uoffsett_relative_slot look even more cryptic but still isn't worth breaking consistency for IMO - this could be bikeshedding :smiley: \nOtherwise why is this not being merged ? It would clean my python exporter code a lot, especially the 20 lines of import statements at the top of my file, instead of from foo.bar.Type import * I could just from foo.bar import Type and have builder functions properly namespaced not polluting my module.\n. I think Vec3() will collide with Vec3 type definition ?\n:+1: on Type.from_root as it also isolates read methods in to Type\nI feel like syntactically the API should follow python standards - not C++/Java/etc, it already deviates by creating a module for every type and builder objects. Semantically it should remain as close to other languages as possible - which is not being changed here.\nI also think you should do it all at once because I'm not fond of the idea of updating dynamically typed python code all the time.\nAnyway up to the dev team - I just feel like this would make the API cleaner to use, so :+1: on PR from me\n. Typo Monster.start ?\n. ",
    "dehengxu": "void NotNested() {\n    // If you hit this, you're trying to construct an object when another\n    // hasn't finished yet.\n    assert(!offsetbuf_.size());\n  }\nI commented assert , it looks works well. \n. ",
    "bbigras": "I don't know why the failed test says it's line 69. It seems to happen in checkFuzz(), called from:\nhttps://github.com/google/flatbuffers/blob/811a5c3389d375fad8e0e367ec72cdfb2a634672/tests/go_test.go#L118\n. The problem happens (is detected) at https://github.com/google/flatbuffers/blob/811a5c3389d375fad8e0e367ec72cdfb2a634672/tests/go_test.go#L375\n. I can reproduce on Ubuntu 15.04 (32-bit).\n. Yes, it's fixed. Thanks!\nbbigras@ubuntunew:~/flatbuffers/tests$ ./GoTest.sh\nwarning: no packages being tested depend on github.com/google/flatbuffers/go\nPASS\nBenchmarkVtableDeduplication-2   2000000              1760 ns/op\nBenchmarkParseGold-2             3000000              1587 ns/op         181.45 MB/s           0 B/op          0 allocs/op\nBenchmarkBuildGold-2              500000              8078 ns/op          35.65 MB/s           0 B/op          0 allocs/op\ncoverage: 95.2% of statements in github.com/google/flatbuffers/go\nok      flatbuffers_test        16.582s\nOK: Go tests passed.\n. ",
    "bryaan": "Hey I just tried this in Python and it slows down significantly.  Is there any other option to reuse the buffer in Python?. ",
    "cliveseldon": "Can you explain what you mean @bryaan \nI'm also interested in reusing the builder if this makes sense or do we think python's garbage collection of the bytearrays is ok?. Is there any update on this or the status of the Maven artifact?. Ok Great. I just didn't find any docs or official releases just one some time ago about grpc+flatbuffers for C++ and some open issues here and on gRPC Github. If its stable and ready to be used great!. Should there be a 1.9.0 release for flatbuffers-java-grpc to Maven?. \nUsually these things only take a few hours to propagate to Maven Central.. I think any advantage might only be for cases where the source data is in a byte[] as I haven't found a way to transfer data from a double[] to a byte[] that doesn't involve a loop over each double at some point.. ",
    "buchanae": "I have a first step here: https://github.com/abuchanan/flatbuffers/commit/5632679a5a10e93ba4554aff59180072f05034b2\n. @layzerar I should have read this thread and your code more carefully. Great stuff!\nAnyway, I started a code generator to wrap the generated CPP code in Cython here: https://github.com/abuchanan/flatbuffers/tree/cython\nMy original goal was to try to completely wrap the C++ API in order to remove the need for all the Python code, but I can see that's not going to work now, and the FastBuilder/FastTable stuff is pretty nice. Not sure I'll continue on the generator, but maybe it could be useful for @Downchuck \n. The changes here https://github.com/abuchanan/flatbuffers/commit/7347aa2cd074d26bb83235a7e40dd22513bc710a pass the tests and make an decent improvement to the generated APIs. There is more that could be done:\n- update all the internal APIs, Builder.PrependInt16Slot becomes Builder.prepend_int16_slot\n- clean up the generated builder helpers somehow. Currently you end up with something like,\n``` python\nfrom fb_models import Monster\nbuilder = Builder(0)\nMonster.Monster_start(builder)\nMonster.Monster_start_inventory_vector(b, 10)\n```\nI'm not sure exactly what an improved version would look like yet. Maybe just remove the \"Monster_\" prefix from the functions, so you have Monster.start_inventory_vector.\nI've edited the docs. Do I need to compile them? If so, what's the command for that?\nNote, this probably conflicts with https://github.com/google/flatbuffers/pull/304\n. Is builder.Finish() only to be used on root types? \nAsked another way: say I have two types, CSVRowCollection (the root type) and CSVRow.  Should I be calling builder.Finish() on every CSVRow, or only on the collection?\n. Oh, and Vec3.CreateVec3() becomes Vec3.create_Vec3()\n. I agree that Vec3.create() and Monster.get_root() would be improvements, and they should be simple changes. At some point, I didn't want to get too crazy with changing the API and starting to diverge from the other implementations, so that's why I left get_root_as_Monster as-is. Really, Vec3() would be better/best, as well as Monster.from_root(), so I'm not sure where to draw the line. I favored getting the most basic improvements committed and avoiding the more complex issues of an ideal, very pythonic API.\nSame for prepend_UInt32. I was actually intending this on my first pass, but I discovered that the existing code does something magical to automatically get the names from C++ types or something. I didn't understand it immediately, so I didn't touch it. Again, aiming for a quick, incremental improvement rather than a shot at an ideal API.\nOn the other hand, now that I'm revisiting the idea \u2013 when it comes to API changes, it's probably better to do it once, than to do it lots of times.\nOf course, if the consensus is that I should shoot for the ideal API, I'm happy to explore that. There's still lots of room for improvement and I have ideas, but it will take a lot longer.\n. Whew, this was awhile ago :) I can see how rebase goes, tonight.. I did take a crack at rebasing, but it's so old I was thinking of starting over. Sorry for the slow pace on my end, pretty busy lately. I'll try again this weekend. \n\nOn Jan 26, 2017, at 8:58 PM, Robert notifications@github.com wrote:\nCan we rebase this? It's wildly out of sync now.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Sorry guys, I don't have the time or motivation to redo this work. Closing. . Yes, fixed. Thank you for checking carefully!\n. \n",
    "mfcollins3": "I tested this against my application that is using the FlatBuffers library. I was not sure if I should modify the sample test case to add something like a monster image that could be read. I will be happy to contribute that code to the test case though if you need me to.\n. I've never played with Unity, but I'm downloading it now and will see if I can verify whether or not this works on Unity. I'll change it if necessary to make it work.\n. Unfortunately, I've never used Unity before, so this may take me some time to verify. I did take the sample project that came with it and threw in an \"ArraySegment?\" value set to null and it compiled without issue and the test game seemed to run. I also tested it out on the Mac with the latest version of the Mono compiler and that sample program compiled and ran successfully. Is this good enough for a verification?\nIf the nullable ArraySegment is going to be a problem, I can change the code to return an ArraySegment that points to an empty array and has zero length.\nThoughts?\n. I rebased my branch. I tested the code using .NET Framework 4.5 and the latest version of Mono for Windows. I didn't see any compiler or runtime errors with the nullable ArraySegment return value.\n. @evolutional I have no Unity experience. I don't think that I can do an accurate test. If you can test it, that would be great.\n. Thank you very much for your help verifying this @evolutional . I greatly appreciate it.\n. I like the proposed changes. We just started using C# flat buffers in my product and are not too far down the road. I do think that it will make our code better in the long term. It makes more sense to me to have the builder methods moved out into their own type because it seems strange that Monster is used to both represent a Monster and build a Monster.\n. I read the article, but not sure that I'm in favor of having the builder as a nested class. I haven't done Java in a while and don't know if this is the norm, but it's not really a pattern that you see in .NET. I don't think that I'd really like to see it done this way in the .NET API.\nMy issue with nested public classes would be that it makes using them and inheriting from them kind of funky. My preference is to leave nested classes as an implementation detail of the parent class, something that should be hidden such as a node type for a tree or linked list.\nI think that it would be great if the builder is broken out from the main entity class to be able to use inheritance or allow my users to use inheritance. For example, my product is an enterprise messaging product and I have a table called Message. Under this proposal, I would get a Message and a MessageBuilder class. I can see utility in being able to create subclasses of MessageBuilder in certain scenarios to add special capabilities such as encrypting my message body when storing it in the Message.Body field or formatting the message body in a specific way. Yes, I could subclass a public nested class, but in my opinion it just looks ugly in the code to do that. Right now I implement this by writing a builder layer on top of the generated builder code, but it would be great to not have to do that.\n. I don't think that access to private data is a concern in this specific case. The builder is going to be writing data to a byte buffer and then will eventually transfer the byte buffer over to the Table object in the constructor. There shouldn't be any need for accessing private members of the Table object.\n. A couple of questions since the C# implementation is currently the primary means through which I am using flat buffers:\n- I'm getting thrown by the IFieldGroup, ITable, and IStruct interfaces. Why were these necessary?\n- I understand that with the change to structs, the generated table structs could not derive from Table. Changing the methods in Table to public static was necessary. Instead of renaming the base classes to TablePos and StructPos, maybe they should be renamed back to their original names. What about generating the structs to hold the byte buffer and starting position for the table/struct and passing those values to the updated methods in Table/Struct.\nMy concern after reviewing the code is that there seems to be too much change here and maybe this solution holds too much complexity. Was it really necessary to move the byte buffer out of the generated table, for example? I think some more details as to why you went down this path would be helpful to evaluate the changes that you are proposing. I can see from your comments what you did, but I'm not sure that I understand the \"why\" for some of these changes.\n. Yes, it's correct. The code for __vector() is on line 65 in the code above. It returns the index of the first byte of the vector data, following the 4-byte length.\n. Yes, I can put the conditional inside __vector_as_arraysegment. My initial thought on returning the blank segment is that ArraySegment is a struct value and not a class reference. I thought at the time it made more sense than making it nullable, but I'll change it to return a null value instead. Thanks for the feedback.\n. ",
    "Thomasdezeeuw": "This could be travis file for Go:\nsudo: false\nlanguage: go\ngo:\n  - 1.3\n  - 1.4\n  - 1.5\n  - tip\nI should note that I haven't tested flatbuffers with older versions of Go.\n. ",
    "xplicit": "For CS you can look at the travis configuration for ms bond serializer. It uses various languages (cs, c++, etc)  https://github.com/Microsoft/bond/blob/master/.travis.yml\nIf you don't want to use \"csharp\" language definition in travis.yml file, you can use any other language and install mono prerequisites manually. \nHere the sample how It can be done for OSX: \nhttps://github.com/xplicit/bond/blob/travis2/.travis.yml\nHere the sample how it can be done for Linux:\nhttps://github.com/xplicit/HyperFastCgi/blob/2f92e4d1a0a87927c1c022176e9b4be45a0eb1ec/.travis.yml\n. ",
    "mmastrac": "Nevermind -- after reading the reflection source, I realized that I completely missed that index points at the correct type:\n// If base_type == Object, index into \"objects\" below.\n. Sorry -- I might not have been clear: all languages generate \"End\" (ie: the vtable method), but the \"Finish\" method (ie: the one that writes the first four bytes of a buffer that are eventually read by GetRootAsXXX) is missing in Go.\nMultiple root types would be great, but there would have to be some thought put into how the identifier is associated with different root types.\n. Oh, I missed that it was implemented for C++. I'm using Java client-side and Go server-side. \nI agree that's it wouldn't be a common case and isn't worth generating by default. If I find some bandwidth I may be able to take a stab at implementing this.\n. FWIW this isn't sufficient for removing unsafe after https://github.com/google/flatbuffers/commit/e8ac0f293e5e8a957e95c5ae72eefb0bf4d1be0e\n. I'm seeing the same thing w/master. In addition, tables with no nested tables appear to have code generated in create that incorrectly assumes a lifetime parameter on the struct. It suggests that there's an accidentally-inverted boolean test somewhere.. I can open another issue for this, but the lifetime parameter for a table w/a union of tables is still busted. \n```\ntable Foo {\n}\nunion Bar {\n  Foo,\n}\ntable Baz {\n    bar:Bar;\n}\n```\nSee the unused 'a here:\npub struct BazArgs<'a> {\n    pub bar_type: Bar,\n    pub bar: Option<flatbuffers::WIPOffset<flatbuffers::UnionWIPOffset>>,\n}. A related issue to this: this causes extra lifetime inflation on the getters for tables-in-tables.\n#[inline]\n        pub fn subtable(&'a self) -> Option<SubTableProto<'a>> {\n            self._tab\n                .get::<flatbuffers::ForwardsUOffset<SubTableProto<'a>>>(TestProto::VT_SUBTABLE, None)\n        }\nShould be something like:\n#[inline]\n        pub fn subtable(&self) -> Option<SubTableProto<'a>> {\n            self._tab\n                .get::<flatbuffers::ForwardsUOffset<SubTableProto<'a>>>(TestProto::VT_SUBTABLE, None)\n        }\n... but because Table::get requires a borrow of self for the lifetime of the underlying buffer we need to do the same thing here (even though the other object is technically just locating the other object and can go away afterward).. I confirmed that this PR compiles with my private Rust codebase w/a fairly complex object structure, and that it allows loosening of nested object lifetimes as well. Once this lands, I'll rebase https://github.com/google/flatbuffers/pull/4949 on top of it.. New test passes and re-confirmed still works on my local codebase. The test I added confirms that the lifetime of the output object is disconnected from the lifetime of self, as it is required to outlive the temporary created in the map function.\nWith the old-style code, the test fails with a borrow-checker error:\n#[inline]\n  pub fn name(&'a self) -> &'a str {\n    self._tab.get::<flatbuffers::ForwardsUOffset<&str>>(Monster::VT_NAME, None).unwrap()\n  }\nerror[E0597]: `e` does not live long enough\n   --> tests/integration_test.rs:273:57\n    |\n273 |         let enemy_of_my_enemy = monster.enemy().map(|e| e.name());\n    |                                                         ^      - `e` dropped here while still borrowed\n    |                                                         |\n    |                                                         borrowed value does not live long enough\n274 |         assert_eq!(enemy_of_my_enemy, Some(\"Fred\"));\n275 |     }\n    |     - borrowed value needs to live until here. I cleaned up some that extra bfbs file that got committed somehow. Also renamed the test to table_object_self_lifetime_is_short_lived and added more commentary around it.. I can't say for sure that the Rust ecosystem is backing rustfmt, but I believe it is a Rust ecosystem component (ie: https://github.com/rust-lang-nursery/fmt-rfcs) and as such is basically the \"canonical\" format for Rust code. I assume that go fmt has the same weight in that ecosystem.\n@steveklabnik might be able to weigh in with more authority.. This turned out to be an issue in practice in our use. As it stands right now enums allow negative values. In our case we had a somewhat complicated issue where they needed to interop w/another system that required tight-packing of these enum values. The only way for us to fit our enum in an 8-byte value was to use negative numbers, which surfaced this issue w/names causing a panic.\nNote that changing an enum to ubyte to avoid negative numbers has other issues (specifically w/Java) and could use a pass over the rest of the code-generators and libraries as well. . Roger, will do.. You are correct, but in this case I'm specifically testing a failing pattern that won't compile. It'll work if there's no closure in place as the temporary consumed from the Option will be allowed live to the end of the function. The closure \"forces the issue\".. I could modify the test to more explicitly use { and } instead of a closure. For example, this would probably fail too:\nlet buf = load_file(\"../monsterdata_test.mon\");\n        let monster = my_game::example::get_root_as_monster(&buf);\n        // This line won't compile if \"self\" is required to live for the lifetime of buf above as the borrow disappears at the end of the closure.\n        let enemy_of_my_enemy = {\n            let enemy = monster.enemy();\n            e.unwrap().name()\n        }\n        .... I gave it another shot to see if I could explain better - let me know what you think of this! \ud83d\ude04 . ",
    "arthur1977": "Thanks for your answer.\nI agree, your way is much safe. Can we expect this feature in future versions?\n. 1abbe40\n. ",
    "chenkai036": "@gwvo @arthur1977  I'm kind of curious that based on what the statement (using for-loop is better than memcpy for small bytes) is established.  I would simply use memcpy or std::copy/std::fill, as I believe they're highly optimized(hopefully it's still true as of today).  Am I missing something?\n. Thanks, I see.  I would have noticed the failure if I had built it on Windows.  Maybe it's a good idea to setup/enable a MSBuild CI for the project on Windows, e.g. AppVeyor ?\n. ",
    "pcmoritz": "It would be really great if we had this, hope somebody wants to take another stab at it! It'd be much more Pythonic than the current API :+1: . ",
    "lirui-apache": "Hi @gwvo , I think the question here is that is there a way to tell whether an optional field exists? Checking the default value won't help, because the user still doesn't know if the field hasn't been set, or it's just set to the default value.\nThanks.\n. I see. Thanks @gwvo .\n. ",
    "sbrednikhin": "I need that thing too.\n. ",
    "chobie": "Okay, I've finished basic implementation and tests. now ready to review.\ninformation about php implementation: \nhttps://github.com/google/flatbuffers/commit/e7fb54bc0d50d7fd34ef7f9129edd8f3c07dffb1\nI'd like to release this as experimental support PHP as It's easy to introduce\nflatbuffers to php users who got involved. \nOur code respects PSR-2 and PSR-4 coding standards\n(these are PHP community driven coding standards). It doesn't have big api design issue.\nref: http://www.php-fig.org/psr/psr-2/ http://www.php-fig.org/psr/psr-4/\nLimitations:\n\nPHP's integer is platform dependent. we strongly recommend use 64bit machine\n  and don't use uint, ulong types as prevent overflow issue.\n  ref: http://php.net/manual/en/language.types.integer.php\nphp don't support float type. floating point numbers are always parsed as double precision internally.\n  ref: http://php.net/manual/en/language.types.float.php\nByteBuffer is little bit slow implementation due to many chr/ord function calls. Especially encoding objects.\n  This is expected performance as PHP5 has parsing arguments overhead. probably we'll add C-extension.\n\nphp doc comment is still work in progress. I'll prettify it during experimental phase (probably in this year.) \nOriginal commits:\nhttps://github.com/chobie/flatbuffers/compare/b6cc8cd7b5ea352a0ceee0e47d466d8038cb0397...8d670f6da7b7eeb8b708a962b585779883f8b4c5\n. Thanks reviewing my code. I'll take a look into that when I back home.\n. @gwvo fixed several things, includes -l to --php flags. can you check commits?\n\nPHPUnit tests would be good if that dependency is easily available for whoever wants to be running the tests.\nOkay, I'll switch to PHPUnit when I added travis support. \n\n@rw Yea, It would be nice, LL users often forgot type systems. I'll take a look into your implementation later.\n. @gwvo yea, I've implemented that. now, squashed & forced pushed php branch.\ntravis seems not good at this time, probably their system trouble. ill check travis tomorrow.\noriginal branch is here.\nhttps://github.com/chobie/flatbuffers/compare/master...chobie:php-origin-20151118\nI'm planning to do remaining PHP tasks (e.g mutation, json parsing, reflection... etc). before do these, I need to play C++ implementation as I should know more internal implementation.\nuntil then, my main contribution will be C#(Unity) :)\n. okay, fixed and forced pushed. \n. > So am I understanding this correctly: in Mono 3.5 (which is C# 3.5 ?) enum types can be initialized with integers and it doesn't know how to parse casts in default parameters, whereas in the latest C# a cast is required?\ngot it, 0 is special case. my previous commit has compile error when default value is not zero.\nWe should use enum value (like CommandType.None) instead of cast for supporting Unity. \nmaybe latest C# is able to cast enum value from int. (I'm not C# guy, not sure which C# version accept this)\n\nI'm guessing the proper solution is for the code to instead look like: CommandType type = CommandType.None ?\n\nYes. \nFor now, I've just implemented it and do FlatBuffers.Test works fine.\n. thanks!\nI've updated branch. Would it be all right?\n. oops, I've forgotten about union type. can you check again?\nThis is just little fix, and there are waiting users. so ,it's no problem to throw away this PR and fixing yourself If I missed something. It would be faster.\n. CI failed but it's not related this PR. just package issue E: Couldn't find any package by regex 'g++-4.9'\n. ah, got it, I just confused it as FinishRecipeBuffer always includes file_identifier If fbs defines that, \nso I always use fbb.Finish instead of it. thanks\n. AFAIK, almost generators haven't supported include statement yet (except c++).,\nmaybe we need to implement that first, and consider namespace problem.\n. @evolutional thanks pointing the issue! I've missed that.\n@gwvo ah, sorry, I went sleep before describe the commit. first commit added (float) cast. then second commit removed type cast and fixed correct float value (also, fixed related code which uses GenDefaultValue).\n\nIt seems to be missing the generated code.\n\nCurrent test code haven't used table default value for float type. so its no effect for existing generated code. (note: struct seems not use default value.)\nI'll do more check in this week and update comment.\n. https://gist.github.com/chobie/61fab9086081be6b76c8\nSorry for late update. probably it is fine. should I add field to monster_test.fbs? \n(for example, luck:float = 1 (id:25);)\n. oops, sorry. I'm having a hectic half a year.\nI'll to this in this weekend.\n. I've checked FloatDefaultValue table with d70f5ac6b02f250dca8e2c6e8f59a4223d1f66f6.\nthis issue has been fixed. sorry about the mess here.\ngenerated c# file is below:\n```\n// automatically generated by the FlatBuffers compiler, do not modify\nusing System;\nusing FlatBuffers;\npublic sealed class FloatDefaultValue : Table {\n  public static FloatDefaultValue GetRootAsFloatDefaultValue(ByteBuffer _bb) { return GetRootAsFloatDefaultValue(_bb, new FloatDefaultValue()); }\n  public static FloatDefaultValue GetRootAsFloatDefaultValue(ByteBuffer _bb, FloatDefaultValue obj) { return (obj.__init(_bb.GetInt(_bb.Position) + _bb.Position, _bb)); }\n  public FloatDefaultValue __init(int _i, ByteBuffer _bb) { bb_pos = _i; bb = _bb; return this; }\npublic float Value { get { int o = __offset(4); return o != 0 ? bb.GetFloat(o + bb_pos) : (float)1.0f; } }\npublic static Offset CreateFloatDefaultValue(FlatBufferBuilder builder,\n      float value = 1.0f) {\n    builder.StartObject(1);\n    FloatDefaultValue.AddValue(builder, value);\n    return FloatDefaultValue.EndFloatDefaultValue(builder);\n  }\npublic static void StartFloatDefaultValue(FlatBufferBuilder builder) { builder.StartObject(1); }\n  public static void AddValue(FlatBufferBuilder builder, float value) { builder.AddFloat(0, value, 1.0f); }\n  public static Offset EndFloatDefaultValue(FlatBufferBuilder builder) {\n    int o = builder.EndObject();\n    return new Offset(o);\n  }\n};\n```\nI'm closing this issue. thanks\n. @gwvo thanks taking look this. I'll consider parser api design as this lacks several methods.\nI'll update PR note and ping you if I finished.\n@evolutional Definitely agree, FlatBuffers.Parser.csproj would be nice. I'll add csproj soon.\n. sorry for late response. \nwe've used parser api in Unity and it looks good. \nparsing json file improves our development iterations.\nfirst of all, I'd like to send PR about accessing ParserState::line_ member before fix this.\nas we have 5k lines json file. it's tough to find to find what is the problem if we broke json.\nmaybe I'll update this PR in this Augst.\n. okay, field test seems any type, i'll add enemy entry to monsterdata_test.json when i back home.\n. Hmm, packagist has been changed their vendoring rules. (AFAIK, we could register any vendor name)\nI don't have google vendor permission so I can't submit this package to it.\n\n@gwvo Can you submit https://github.com/google/flatbuffers to packagist (https://packagist.org/)?\nWe have to ask someone who has google vendor name permission If you can't.\nhttps://packagist.org/packages/google/\n@kornrunner copmoser also supports vcs type. you can use flatterbuffers with following composer.json. \n{\n  repositories: [{\n    \"type\": \"vcs\",\n     \"url\": \"https://github.com/google/flatbuffers\"\n  }],\n  require: {\n    \"google/flatbuffers\": \"*\"\n  }\n}\n. Thanks!\n. Thanks @armen, can you add simple assertion test about getInventoryBytes?\n. @armen Thanks, looks good to me.\n. Yea, Basically, we have to put composer.json at top level of repository.\n(little bit off topic) I've tested sub directory version which using github svn support.\nit works good on local machine. but packagist detects the url as git :( \ne.g) https://github.com/chobie/composer-test\nBut now, current PHP implementation is still experimental. we can postpone this decision till stable release.\n. You know this test is javascript port, but I can't find unicode_test.mon on tests directory.\nI'll implement this when I have chance to setup node.js.\n. ~ int types are okay since PHP's integer type is signed long (platform dependent).\nAFAIK, over uint types requires too much implementation when converting it on userland code due to overflowing integer automatically converted as double internally and lacks functions. \n(also I'm not sure how can i encode those value correctly)\nOne interesting thing, php-mongo-driver has (https://github.com/mongodb/mongo-php-driver/commit/fa414d10189e48c57c926833d111a2412b733778) dropped uint and ulong types as majority users doesn't use them.\nWe can those values with extension. maybe I'll write it when ByteBuffer API has been fixed.\n. There are mbstring extension and polyfill-mbstring.\nhttp://php.net/manual/en/book.mbstring.php\nhttps://github.com/symfony/polyfill-mbstring\nmbstring depends current detecting order (http://php.net/manual/en/function.mb-detect-order.php) when detecting passed string encoding.\nSometimes it returns wrong encoding when order is not good. Especially, Japanese language often returns wrong encoding with mb_detect_encoding as it has several encoding and similar pattern.\nI'll add utf8 test case later.\n. I agree, It should be add some comments and put error check on getUint.\nCurrently, https://github.com/chobie/flatbuffers/commit/e7fb54bc0d50d7fd34ef7f9129edd8f3c07dffb1#diff-66510972612fc4e230137135eefbc6c8R225 checks overflowing value.\nOn 32bit machine (and Windows): \noverflowing unsigned integer can't put with this API as design.\nOn 64bit machine: unsigned integer value are supported natively, so it doesn't need special task when encoding to bytes.\nIt needs unpack function when decoding value.\nhttps://github.com/chobie/flatbuffers/commit/e7fb54bc0d50d7fd34ef7f9129edd8f3c07dffb1#diff-8bca56a260f1ea2ed9560137c20536d5R376\nbut I get little bit concerned. I'll make sure this behaviour. \n. Yes, same as https://github.com/google/flatbuffers/pull/308#discussion_r44659373 comment.\nhelper function would be nice. I'll add that soon.\n. Since I've referenced CSharp and Java implementations.\nhttps://github.com/google/flatbuffers/blob/master/net/FlatBuffers/FlatBufferBuilder.cs#L211\nhttps://github.com/google/flatbuffers/blob/master/java/com/google/flatbuffers/FlatBufferBuilder.java#L253\nYou are right, these should be uint.\n. Yes. specifically it checks long size. windows long type is 32 bit on 32/64bit machine.\nUsually we use PHP_INT_SIZE constant when checking int size.\n. nope. this came from Java's Buffer API (http://docs.oracle.com/javase/8/docs/api/java/nio/Buffer.html#reset--).\nC# implementation also clear pos only. (https://github.com/google/flatbuffers/blob/master/net/FlatBuffers/ByteBuffer.cs#L51)\nbtw, Java and C#'s FlatBuffersBuilder doesn't have reset API.\nhttps://github.com/google/flatbuffers/blob/master/java/com/google/flatbuffers/FlatBufferBuilder.java#L436\nhttps://github.com/google/flatbuffers/blob/master/net/FlatBuffers/FlatBufferBuilder.cs#L351\nI've used C#(Unity) and PHP, For now, I'm following C# implementations.\n. yea, maybe it's okay. codes are same as C#.\nhttps://github.com/google/flatbuffers/blob/master/net/FlatBuffers/ByteBuffer.cs#L102\nI'll add test case later.\n. thanks pointing this, I'll re-read google c++ style guide.\nmaybe still contains not compliant codes. i'll check with cpplint.py\n. Am i correct in understanding in that?\nhttps://github.com/google/flatbuffers/blob/ac9041b7e96470f8ecb8ae833076dec0c1b90891/net/FlatBuffers/Parser.cs#L41\nCurrently, caller should handle exceptions when calling GeneratJson/GenerateBuffer method.\n. thanks, fixed\n. It doesn't have clear reason. In my experience, some OSS library used this signature (out param and return type) so just use this.\nCertainly, it makes no sense, i'll fix this later.\n. ",
    "kornrunner": "Could you please add package to https://packagist.org?\n. Thank you very much.\nSetting up a github hook would make things easier for you (as you wouldn't need to go to packagist to update package with every new release). You're still in charge of versioning in terms of issuing new releases (and they'll be automatically available to packagist/composer users). End users are in charge of which version they will install/use (it doesn't affect them if you publish a new release, but makes it simple for them to upgrade, if they wish).\nI hope this sheds some light.\n. Unfortunately yes, if we intend to make it available as a Composer package via Packagist - de facto standard dependency manager in PHP world.\n. ",
    "krisskross": "Cheers, that worked. I suppose the Java documentation is outdated also since it suggest calling finish on Monster.finishMonsterBuffer() instead of fbb.finish(monsterPos).\n. The generated code does not provide a finishMonsterBuffer method?\n. Maybe i'm missing a flag or two to flatc?\nflatc -j -o src/main/java src/main/flatbuffers/schema.fbs\nMy generated Monster class is very small compared to what you refer to.\n``` java\npublic final class Monster extends Table {\n  public static Monster getRootAsMonster(ByteBuffer _bb) {\n    return getRootAsMonster(_bb, new Monster());\n  }\n  public static Monster getRootAsMonster(ByteBuffer _bb, Monster obj) {\n    _bb.order(ByteOrder.LITTLE_ENDIAN);\n    return (obj.__init(_bb.getInt(_bb.position()) + _bb.position(), _bb));\n  }\n  public Monster __init(int _i, ByteBuffer _bb) {\n    bb_pos = _i; bb = _bb;\n    return this;\n  }\npublic String name() { int o = __offset(4); return o != 0 ? __string(o + bb_pos) : null; }\n  public ByteBuffer nameAsByteBuffer() { return __vector_as_bytebuffer(4, 1); }\npublic static int createMonster(FlatBufferBuilder builder,\n      int name) {\n    builder.startObject(1);\n    Monster.addName(builder, name);\n    return Monster.endMonster(builder);\n  }\npublic static void startMonster(FlatBufferBuilder builder) { builder.startObject(1); }\n  public static void addName(FlatBufferBuilder builder, int nameOffset) { builder.addOffset(0, nameOffset, 0); }\n  public static int endMonster(FlatBufferBuilder builder) {\n    int o = builder.endObject();\n    return o;\n  }\n};\n```\n. Yep, that was it. Not immediately obvious from the documentation, this flow of corrections I made in order to make it work. \nThanks for you help!\n. Final question (and forgive me not coming from a C++ vocabulary), how do I express a list of something in the schema grammar?\n. ",
    "zz85": "Hmm, that's an interesting behaviour. In which case, I'm interested to know if there could be a null default for integers and whether it make any sense for have an api which returns whether the field is default?\nOtherwise, I think Evan's approach seems reasonable for most cases, or an additional field could be defined to indicate the presence of a field.\n. That mighht change the way the code is called, but thanks for the suggestion!\n. ",
    "dudehook": "Table::CheckField is not visible to clients of the table object.\nI'm hacking the code to generate a \"has_xxx\" method to make the call to CheckField.\ni.e.   mymonster.has_health()\nI'll create a PR if you want it, but I'm only modifying the cpp generator since that's all we're doing.\n. Ah, excellent. I missed that.\nOne more question: will this have a \"false negative\" if the field is set, but happens to be set with the default value?  I think I'm seeing that happening...\n. Yes, never mind.  I see that in the code comments.\n. ",
    "danring": "Generated Python also doesn't include GetRootAs methods (see #363).  It seems the Go and Python libraries themselves don't handle file_identifier in their Finish methods.\n. I will fix #363 tonight\n. #363 is now rebased\n. @rw Apologies, I was offline for a week.  I'll add tests soon.\n. @rw Are the added tests sufficient?\n. Bump @rw -- Any further tests needed?\n. ",
    "ibloat": "I rebased @danring 's #363 since it does not apply cleanly to master anymore.\nhttps://github.com/google/flatbuffers/compare/master...ibloat:fix-341\nIf there is interest still I will create a PR if @danring is unavailable.\nWe have been using this patch for a few months now without problems.\n. Thanks for submitting this! I was still undecided after reading #3991 but let's go ahead, it can be closed again if it is of no use.\nAdding the email address to the CLA in a moment. It's signed and the email address used to commit has been added to 'Alternative emails'.. Cheers. No way to trigger a recheck I guess.. You are probably referring to #3748, this PR doesn't have its problems in the browser but adds around 250 bytes to the build.\nMade the export named for commonjs again to not break existing code.\nThe named vs default discussion warrants an issue all to itself, will create one some other time.. If exports is undefined it should just fall through.\nI would be interested to hear where this is not working as expected.. That must be it. Sorry about that.\nI did test that flatbuffers.js survives the pass through closure-compiler with default options but it seems that it fails with many of the optimization/module flags.. ",
    "nealkruis": "Checking on the status of this issue. It looks like the remaining issue is handling file_identifier in Finish (and *BufferHasIdentifier) methods in Python (and other languages)? Should this be moved to a separate issue?. I've got a lot of catching up to do before I'm comfortable putting together a PR. I might poke around a bit at the code in my hobby time, but I wouldn't expect anything soon. This feature is more for convenience than functionality.. ",
    "kartynnik": "@gwvo Can you please point to the fixed file_identifier handling in Python (the issue reported by @nealkruis)?. ",
    "dictav": "Hello,\nFirst, I don't speak English well, I'm sorry.\nI tried to update Parser to support the base namespace of NameSpace on following:\nhttps://github.com/dictav/flatbuffers/tree/namespace-additional-info\nBecause I want to make GoGenerator work well in nested namespaces (#3927). I'm working on following:\nhttps://github.com/dictav/flatbuffers/tree/fix-go-import\nI have noticed. In Go, the base namespace must be only the one per a generation. Because the Go's import path, the directory hierarchy and the other repositories are tightly relative. They cannot separate.\nI think that it is better to use the namespace command line option\nor to add the new token for the base namespace.\n. @aardappel @rw \nI have thought about some proposals again. As I thought, I think that it is better to use the command line flag without changing IDL.\nRead here for more:\nhttps://gist.github.com/dictav/e30d87c9c3fbb41142929dc67c2393b9\n(Again, I don't speak English well, I'm sorry.). That's good.\nThank you.. I have built flatc (version 1.5.0 (Jan 10 2017)) and checked.\nThat's good.\nThank you.. > How do we make that refer to major FlatBuffers release versions?\nI was wondering if it is better to manage flatc version and js/ flatbuffers.js version separately.\nBut users do not get confused if they are the same. I will do that.. I think that we need to do the following:\n\nPublish the new version (npm publish)\nMake sure the new flatbuffers package is available at npm\n~~Delete the evanw's implementation versions~~\n~~npm unpublish flatbuffers@0.0.1~~\n~~npm unpublish flatbuffers@0.1.0~~\n\n@evanw How about you?\n@aardappel If you want me to do this work, please add me to the package owner (https://www.npmjs.com/package/flatbuffers/access). Then I can do it.. @evanw You are right. Thank you.. @aardappel \nI have tested about importing and bundling. And our npm package works.\nhttps://github.com/dictav/js-flatbuffers-test\nI have finished my work of publishing.\nhttps://www.npmjs.com/package/flatbuffers\nSo I removed \"WIP\". If you are OK please merge this PR.. @aardappel \nWe can do it easily as following:\n```sh\n$ cd /path/to/google/flatbuffers\n$ EDITOR package.json\nUpdate \"version\". Or we are able to use npm version command\n$ npm publish\n```. ",
    "xiaohaoliang": "My wish is that:     namespace protocol (java: \"com.company.package\", c: \"package\"); \na command-line option for Go is good. But, I think doing it in the schema would be better than on the command line.\nNow for compatibility, I may  need \n--general-namespace Generate the overrided namespace in Java and CSharp.\\n\"\n@aardappel \n(My English is poor, I'm sorry.) . @aardappel  yes, Now  command-line is better , I will to do the command-line:\n   --general-namespace Generate the overrided namespace in Java and CSharp.\\n\nso, should I push the code to master ?    \nhttps://github.com/xiaohaoliang/flatbuffers/commit/79c4342be6348af243ad6677a41bada6acf00f77. @aardappel  the one schema like namespace package_A  to generate cpp files and java files . \nthe wish is :  cpp like namespace package_A{...} , but java need like package com.company.package_A; .\nThe same idl file to generate different paths and namespaces. So need command-line--general-namespace, like Go but different.\nGo : command-line:--go-namespace com.company.package_A\n       path: com/company/package_A/\n       generate source_file:package package_A. @aardappel \n lets change --go-namespace into just --namespace and make it work for any language.\nI think it is good ! \nnamespace package_A (java: \"com.company\", go: \"com.company\"); that is perfect!\n. I signed it!.  I suggest turning the warning off,too. \nEndianSwap(T t) in include/flatbuffers/flatbuffers.h: 173-181    is the same.\n@chronoxor @aardappel \nhttp://stackoverflow.com/questions/28985515/is-warning-c4127-conditional-expression-is-constant-ever-helpful    . Maybe #define FLATBUFFERS_LITTLEENDIAN 1, so EndianSwap  was not compiled.  @aardappel . ",
    "mikemccarty-vertex": "I have a working version that adds a --namespace-prefix <prefix> commandline option.  Are you interested?. Re #4222:  I don't think that option is compatible with what I'm doing.  That option is an override rather than a prefix.  I couldn't see how to incorporate it without changing its meaning.. Re: in-schema\nI was less confident about how to go about doing that.  ;). @aardappel @stefansullivan I implemented in a fork a --namespace-prefix commandline option that would prepend a user-specified namespace prefix (e.g. \"com.\" for Java).  I got it to work but I stopped using it in production when I realized it broke gRPC compatibility between different languages.  I haven't had time to investigate whether there are reasonable workarounds for this problem.  In the meantime, the fork still exists but my motivation to see it merged to master has waned.... @zats I have a version that works (or appears to) across the supported languages in the sense that it generates reasonable compilable code.  Like you, I'm not comfortable enough with all of the languages to say it officially works for all of them -- C++ and Java were my main focus.  \nIn any case, as I said above, once I discovered that my changes broke gRPC ABI compatibility across languages, I essentially abandoned it because that is a critical use case for me and I don't know that the problem is fixable.  If this feature is ever incorporated into mainstream flatc, it will have to come with a big gRPC disclaimer.\nI would say, if you have something that is ready to ship right now, you should probably submit it.  The fork I have is several months behind the head and I don't have the time to maintain it at the moment.  If you want to incorporate some of the changes I have made into your implementation, I have no problem with that.  The changes I made are not that sophisticated.  ;). This problem is not limited to the Go language.  I can reproduce with both C++ and Java:\nthings.fbs:\nnamespace my.company.things;\ntable InThing\n{\n    thing : int;\n}\ntable OutThing\n{\n    thing : int;\n}\nservice.fbs:\ninclude \"things.fbs\";\nnamespace my.company.service;\nrpc_service ThingDoer\n{\n        DoThings(my.company.things.InThing) : my.company.things.OutThing;\n}\nCompiling this with:\nflatc --java --cpp --grpc service.fbs\nleads to generated code like this:\nJava:\nprivate static volatile io.grpc.MethodDescriptor<my.company.service.InThing,\n                                                 my.company.service.OutThing> getDoThingsMethod;\nNote what should've been typed ...things.OutThing is instead typed ...service.OutThing.\nC++:\nvirtual ::grpc::Status DoThings(::grpc::ClientContext* context, \n                                const flatbuffers::grpc::Message<InThing>& request, \n                                flatbuffers::grpc::Message<OutThing>* response) = 0;\nHere, OutThing is not namespace qualified at all and so is implicitly typed ...service.OutThing.\nThis appears to only be a problem for the --grpc output.. Any hints on how best to fix this would be greatly appreciated.  It appears to me the grpc support is not well integrated with the parser.. @aardappel right, therein lies the problem -- AFIACT, the grpc generator code (e.g. grpc_java_generator) has no access to any BaseGenerator derived object.  I suppose one could be passed along but I'm not sure that fixes all of the problems.. Yes, but it isn't really used and even if it were, there is another set of interfaces (grpc/src/schema_interface.h) that serves as yet another layer of abstraction to be fought through (this is what I meant by 'not well integrated').  This all might have a relatively simple solution but I currently don't have the bandwidth (or background knowledge) to figure it all out.. Yes, sorry...  I've been working on some higher priority things here lately.  I promise I'll get around to it soon.. ",
    "stefansullivan": "I just wanted to chime in here. I've started using flatbuffers for python, C++, and Java in at least 3 different repos for multiple projects. Now obviously flatbuffers are inherently a serialization/deserialization protocol so their purpose is to be able to cross memory/language/application boundaries. \nI really want to be able to use some sort of consistent namespace convention across all those languages, but they all have their own packaging requirements. For example, java requires com.company.packname.nestedpackage naming convention, while C++ doesn't tend to use namespace company::package. Further, pythons package names are identical to folders, which has been an issue with Mac OS's case-insensitive filesystem more than once \ud83d\ude2d. There's also commonly-accepted coding styles that differ between these languages (e.g. CapitalCamelCase in C++ vs under_scores in python)\nIt would be really nice to be able to generate a schema that used one namespace, but had some convention/language feature to specify how those namespace get translated into directories, packages, etc. for each language\nnamespace Company.Package.Subpackage (java: com.company, python: company);\n\nOr maybe there could even be a notion of packaging that is required for certain languages? e.g.\nnamespace package.subpackage (cpp: Company);\npackage java:com.company, python:company, go:github.com/company;\n\nI don't know. Just now I decided not to use the raw generated sources as part of my build process because I need to rewrite the package names per-language. @zats @mikemccarty-vertex I can attest to the fact that the fork for --namespace-prefix works for a non-gRPC use case. I'm struggling with a python bug with importing from nested namesoaces that doesn't appear to come from this branch. But I'm guessing with a different schema structure it would be fine. ",
    "zats": "@mikemccarty-vertex hey I was working on cpp override for the namespaces when I found this issue.\nIntuitively I understand that it's better to implement a language independent command line argument allowing to override namespace per specified language but I definitely don't have enough domain specific knowledge to cover all the languages supported outside of cpp\nWhat's the current status of this effort? Are you planing to open a pull request any time soon or should I proceed with --cpp-namespace override pull request?\nThanks. ",
    "cruelbob": "@gwvo This is not same issue\nSee example of generated c# code(AddFoo method. It's getting Foo value instead of VectorOffset):\nBar.fbs:\n```\nenum Foo: byte\n{\n    A,\n    B,\n    C\n}\ntable Bar\n{\n    Foo:[Foo];\n}\nroot_type Bar;\n```\nBar.cs:\n``` c#\n// automatically generated, do not modify\nusing FlatBuffers;\npublic enum Foo : sbyte\n{\n A = 0,\n B = 1,\n C = 2,\n};\npublic sealed class Bar : Table {\n  public static Bar GetRootAsBar(ByteBuffer _bb) { return GetRootAsBar(_bb, new Bar()); }\n  public static Bar GetRootAsBar(ByteBuffer _bb, Bar obj) { return (obj.__init(_bb.GetInt(_bb.Position) + _bb.Position, _bb)); }\n  public Bar __init(int _i, ByteBuffer _bb) { bb_pos = _i; bb = _bb; return this; }\npublic Foo GetFoo(int j) { int o = __offset(4); return o != 0 ? (Foo)bb.GetSbyte(__vector(o) + j * 1) : (Foo)0; }\n  public int FooLength { get { int o = __offset(4); return o != 0 ? __vector_len(o) : 0; } }\npublic static Offset CreateBar(FlatBufferBuilder builder,\n      Foo Foo = Foo.A) {\n    builder.StartObject(1);\n    Bar.AddFoo(builder, Foo);\n    return Bar.EndBar(builder);\n  }\npublic static void StartBar(FlatBufferBuilder builder) { builder.StartObject(1); }\n  public static void AddFoo(FlatBufferBuilder builder, Foo FooOffset) { builder.AddOffset(0, (VectorOffset)(FooOffset).Value, 0); }\n  public static VectorOffset CreateFooVector(FlatBufferBuilder builder, sbyte[] data) { builder.StartVector(1, data.Length, 1); for (int i = data.Length - 1; i >= 0; i--) builder.AddSbyte(data[i]); return builder.EndVector(); }\n  public static void StartFooVector(FlatBufferBuilder builder, int numElems) { builder.StartVector(1, numElems, 1); }\n  public static Offset EndBar(FlatBufferBuilder builder) {\n    int o = builder.EndObject();\n    return new Offset(o);\n  }\n  public static void FinishBarBuffer(FlatBufferBuilder builder, Offset offset) { builder.Finish(offset.Value); }\n};\n```\n. @evolutional have you any progress with this task?\n. ",
    "nietras": "We would very much like this to be the case as well, since this generates unnecessary GC pressure.\n. ",
    "samkusin": "FYI, I've managed to author a fix that doesn't use static/global variables.  It's not yet production ready as it still has some debugging/naming issues in it.  But it does address the original issue and passes the tests application.  I've included it in case it's useful.   I'll submit a pull request once it's ready.\nhttps://github.com/samkusin/flatbuffers/commit/4a03dab4f9fb650dc3d2a9aabfd53b7c5849b012\n. I've created a pull request for my changes.  \nhttps://github.com/google/flatbuffers/pull/2500\n. ",
    "4ntoine": "Since \"kotlin\" branch can't be found anymore (https://github.com/Lakedaemon/flatbuffers/tree/kotlin) and the last commit is on June2, 2016 i wonder if the project is still alive?. ",
    "xuefli": "see issue #357\n. //schema:\nnamespace sample.protocol;\ntable Sample {\n    publicKey:[byte];\n    privateKey:[byte];\n}\nroot_type Sample;\n/++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++/\n//clang:\nvoid testMarshall() {\n    flatbuffers::FlatBufferBuilder builder;\n    string publicKey, privateKey;\n    OpensslWapper::generateRSAKey(1024, publicKey, privateKey);\n    vector temp(publicKey.begin(), publicKey.end());\n    auto auto4pub = builder.CreateVector(temp);\n    temp.clear();\n    temp.assign(privateKey.begin(), privateKey.end());\n    cout << \"base64(publickey):\" << Base64::encode(publicKey) << endl;\n    cout << \"base64(privateKey):\" << Base64::encode(privateKey) << endl;\n    auto auto4private = builder.CreateVector(temp);\n    auto storeInstance = CreateSample(builder, auto4pub, auto4private);\n    builder.Finish(storeInstance);\n    flatbuffers::SaveFile(\"test.bin\", reinterpret_cast(builder.GetBufferPointer()), builder.GetSize(), true);\n}\nvoid OpensslWapper::generateRSAKey(int strengLen, string& publicKey, string& privateKey) {\n    RSA* keyPair = RSA_new();\n    BIGNUM* bne = BN_new();\n    BN_set_word(bne, RSA_F4);\n    RSA_generate_key_ex(keyPair, strengLen, bne, NULL);\n    /*\n    RSA* keyPair = RSA_generate_key(strengLen, RSA_F4, NULL, NULL);\n    if(keyPair == NULL) {\n        return;\n    }\n    /\n    unsigned char buffer[2048];\n    unsigned char bufferPtr = buffer;\n    int len = 0;\n    len = i2d_RSA_PUBKEY(keyPair, &bufferPtr);\n    //len = i2d_RSAPublicKey(keyPair, &bufferPtr);\n    bufferPtr = buffer;\n    publicKey.assign(bufferPtr, bufferPtr + len);\n    len = i2d_RSAPrivateKey(keyPair, &bufferPtr);\n    bufferPtr = buffer;\n    privateKey.assign(bufferPtr, bufferPtr + len);\nRSA_free(keyPair);\nkeyPair = NULL;\nBN_clear_free(bne);\nbne = NULL;\n}\n/++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++/\n//java\n    public static void testSample() {\n        byte[] data = null;\n        File file = new File(\"test.bin\");\n        RandomAccessFile f = null;\n        try {\n            f = new RandomAccessFile(file, \"r\");\n            data = new byte[(int)f.length()];\n            f.readFully(data);\n            f.close();\n        } catch(java.io.IOException e) {\n            System.out.println(\"FlatBuffers test: couldn't read file\");\n            return;\n        }\n```\n    ByteBuffer binaryBuffer = ByteBuffer.wrap(data);\n    Sample instance = Sample.getRootAsSample(binaryBuffer);\n    System.out.println(instance.publicKeyLength());\n    System.out.println(Base64.toBase64String(instance.publicKeyAsByteBuffer().array()));\n    System.out.println(instance.privateKeyLength());\n    System.out.println(Base64.toBase64String(instance.privateKeyAsByteBuffer().array()));\n}\n```\n/-------------------------------------------------/\n//result of clang\nbase64(publickey):MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCZiGM9F7Nta/lxnMrCnmppWVTDTf9PFo3k18hrcA75omQ8BqVXjCymLsljeEch7n1JhY7AF5SXYtQCOK9R/7B9nWCyv9uvnBVwq1bUid1RNbX9fmsy5ADXQBqVHAaHmc0hltM9D8ExvGDySzaHDfUJNTOA1U9ttmLFnj9naR7BvwIDAQAB\nbase64(privateKey):MIICXQIBAAKBgQCZiGM9F7Nta/lxnMrCnmppWVTDTf9PFo3k18hrcA75omQ8BqVXjCymLsljeEch7n1JhY7AF5SXYtQCOK9R/7B9nWCyv9uvnBVwq1bUid1RNbX9fmsy5ADXQBqVHAaHmc0hltM9D8ExvGDySzaHDfUJNTOA1U9ttmLFnj9naR7BvwIDAQABAoGBAIXKemBEhRE3ZFoC/hX2olhHhjvjfjZXOzNObtJhBku3nY1JZVATUN1ILCrIJ4SEJR2qiykN42jAsGt3JiBYplyW+C9/8/BjCgWslq5QLR76gcGnARMME/QWHCgfB2pGvc8q1Y0a3eEc9IELZIZ0tK8u7j+KAi5ajaFdkkiewsuBAkEAyL+N/ytpFlkzRfUxXsk1uONFLS5O+qvYJ0SfttculwlBF5LQVCkF3C9mcbogg7azw9pVjZyx/3bT/TJqGd+nHwJBAMPKGCNaRW7of2LDEQ8ud/3h1+8caMdirTM1zdOjQuSfuIujZntxvy/8MdS1nemJuOUb498YEJWZJqlc7lCssWECQBH3GpUQZHosh2smU0HBTf560FEHyOekA1cYTBbH0RKRMHMUn3I4rZ6T214oIwrGifXoDyn6MO/EyZnqLhacrk8CQDLhJuqw/9A1nT0GIj3Gvqn8ccgSouwFqargNY/lInIVS7kGCpDT23JV4IE9fT7+ak0ntUz5jP3DYferTccrSaECQQCv4cTv9Rv4rzSib9HeMmXOfx8MPMWkv2AQd//OS37exRFZ/Uz2PAR3G2rRcGaAsEqQcQ5nbMTrfPhfYdkqpF6d\n//result of java\n162\nDAAAAAgADAAEAAgACAAAAHACAAAEAAAAYQIAADCCAl0CAQACgYEAmYhjPRezbWv5cZzKwp5qaVlUw03/TxaN5NfIa3AO+aJkPAalV4wspi7JY3hHIe59SYWOwBeUl2LUAjivUf+wfZ1gsr/br5wVcKtW1IndUTW1/X5rMuQA10AalRwGh5nNIZbTPQ/BMbxg8ks2hw31CTUzgNVPbbZixZ4/Z2kewb8CAwEAAQKBgQCFynpgRIURN2RaAv4V9qJYR4Y74342VzszTm7SYQZLt52NSWVQE1DdSCwqyCeEhCUdqospDeNowLBrdyYgWKZclvgvf/PwYwoFrJauUC0e+oHBpwETDBP0FhwoHwdqRr3PKtWNGt3hHPSBC2SGdLSvLu4/igIuWo2hXZJInsLLgQJBAMi/jf8raRZZM0X1MV7JNbjjRS0uTvqr2CdEn7bXLpcJQReS0FQpBdwvZnG6IIO2s8PaVY2csf920/0yahnfpx8CQQDDyhgjWkVu6H9iwxEPLnf94dfvHGjHYq0zNc3To0Lkn7iLo2Z7cb8v/DHUtZ3pibjlG+PfGBCVmSapXO5QrLFhAkAR9xqVEGR6LIdrJlNBwU3+etBRB8jnpANXGEwWx9ESkTBzFJ9yOK2ek9teKCMKxon16A8p+jDvxMmZ6i4WnK5PAkAy4SbqsP/QNZ09BiI9xr6p/HHIEqLsBamq4DWP5SJyFUu5BgqQ09tyVeCBPX0+/mpNJ7VM+Yz9w2H3q03HK0mhAkEAr+HE7/Ub+K80om/R3jJlzn8fDDzFpL9gEHf/zkt+3sURWf1M9jwEdxtq0XBmgLBKkHEOZ2zE63z4X2HZKqRenQAAAKIAAAAwgZ8wDQYJKoZIhvcNAQEBBQADgY0AMIGJAoGBAJmIYz0Xs21r+XGcysKeamlZVMNN/08WjeTXyGtwDvmiZDwGpVeMLKYuyWN4RyHufUmFjsAXlJdi1AI4r1H/sH2dYLK/26+cFXCrVtSJ3VE1tf1+azLkANdAGpUcBoeZzSGW0z0PwTG8YPJLNocN9Qk1M4DVT222YsWeP2dpHsG/AgMBAAEAAA==\n609\nDAAAAAgADAAEAAgACAAAAHACAAAEAAAAYQIAADCCAl0CAQACgYEAmYhjPRezbWv5cZzKwp5qaVlUw03/TxaN5NfIa3AO+aJkPAalV4wspi7JY3hHIe59SYWOwBeUl2LUAjivUf+wfZ1gsr/br5wVcKtW1IndUTW1/X5rMuQA10AalRwGh5nNIZbTPQ/BMbxg8ks2hw31CTUzgNVPbbZixZ4/Z2kewb8CAwEAAQKBgQCFynpgRIURN2RaAv4V9qJYR4Y74342VzszTm7SYQZLt52NSWVQE1DdSCwqyCeEhCUdqospDeNowLBrdyYgWKZclvgvf/PwYwoFrJauUC0e+oHBpwETDBP0FhwoHwdqRr3PKtWNGt3hHPSBC2SGdLSvLu4/igIuWo2hXZJInsLLgQJBAMi/jf8raRZZM0X1MV7JNbjjRS0uTvqr2CdEn7bXLpcJQReS0FQpBdwvZnG6IIO2s8PaVY2csf920/0yahnfpx8CQQDDyhgjWkVu6H9iwxEPLnf94dfvHGjHYq0zNc3To0Lkn7iLo2Z7cb8v/DHUtZ3pibjlG+PfGBCVmSapXO5QrLFhAkAR9xqVEGR6LIdrJlNBwU3+etBRB8jnpANXGEwWx9ESkTBzFJ9yOK2ek9teKCMKxon16A8p+jDvxMmZ6i4WnK5PAkAy4SbqsP/QNZ09BiI9xr6p/HHIEqLsBamq4DWP5SJyFUu5BgqQ09tyVeCBPX0+/mpNJ7VM+Yz9w2H3q03HK0mhAkEAr+HE7/Ub+K80om/R3jJlzn8fDDzFpL9gEHf/zkt+3sURWf1M9jwEdxtq0XBmgLBKkHEOZ2zE63z4X2HZKqRenQAAAKIAAAAwgZ8wDQYJKoZIhvcNAQEBBQADgY0AMIGJAoGBAJmIYz0Xs21r+XGcysKeamlZVMNN/08WjeTXyGtwDvmiZDwGpVeMLKYuyWN4RyHufUmFjsAXlJdi1AI4r1H/sH2dYLK/26+cFXCrVtSJ3VE1tf1+azLkANdAGpUcBoeZzSGW0z0PwTG8YPJLNocN9Qk1M4DVT222YsWeP2dpHsG/AgMBAAEAAA==\n. //if I using the followwing code instead of instance.publicKeyAsByteBuffer().array(), there is no problem\n```\n    byte[] content = new byte[instance.publicKeyLength()];\n    for(int i = 0; i < content.length; i++) {\n        content[i] = instance.publicKey(i);\n    }\nSystem.out.println(\"base64(publickey):\" + Base64.toBase64String(content));\nSystem.out.println(\"hex(publickey):\" + Hex.toHexString(content));\n\n```\n. //if I using the following code instead of instance.publicKeyAsByteBuffer().array(), there is no problem\n        byte[] content = new byte[instance.publicKeyLength()];\n        for(int i = 0; i < content.length; i++) {\n            content[i] = instance.publicKey(i);\n        }\n        ByteBuffer bb = instance.publicKeyAsByteBuffer();\n        bb.get(content);\n        System.out.println(\"base64(publickey):\" + Base64.toBase64String(content));\n        System.out.println(\"hex(publickey):\" + Hex.toHexString(content));\ncontent = new byte[instance.privateKeyLength()];\n    bb = instance.privateKeyAsByteBuffer();\n    bb.get(content);\n    System.out.println(\"base64(privateKey):\" + Base64.toBase64String(content));\n    System.out.println(\"hex(privateKey):\" + Hex.toHexString(content));\n. ",
    "prideout": "@rw and @gwvo : This change is straightforward and useful, since Python 2.7 is fairly mainstream nowadays.\n. @gwvo can we merge this?  We'd rather not continue maintaining a fork of flatbuffers.\n. Ah, thanks for the fix!  We're on 2.7.3.\n. Thanks, that helps.\n. ",
    "uwydoc": "the \"GetRootAs\" methods are really convenient and necessary for reading messages consisting only non-root_type object.\n. ",
    "chrisstaite": "This is the issue with implementing code in headers I suppose.  Either fix should be implemented, but it causes a build error so as is (even though it's not the code itself's fault) so it needs something.  In my case the Windows.h import comes from Python.h.\nI see the point that people should disable the stupid macro definition from Windows.h, however adding additional macro definitions to my code is something I would rather not do.  Please decide one way or another though.\n. I agree with your feeling, but unfortunately Microsoft are still catching up with namespaces and love their backwards compatibility.\n. ",
    "faizanrashid": "@rw Any recommendations on how to handle this issue?\n. I would expect python 2.7.x versions where x >=5 to use memoryview while the rest to use buffer.\n. ",
    "aiert": "thanks, gwvo, so using a union and put all the request-response tables in it is the usual way we use flatbuffers? \n. now i know how flatbuffers works , root type in go is just a example to show me how to get data from a []byte type variable. so now i don't use root type anymore, thanks gwvo.\n. ",
    "armen": "@gwvo done\n. @chobie sure, added the following. Do you think that's sufficient?\n$assert->strictEqual(bin2hex($monster->GetInventoryBytes()), \"0001020304\");\nTo refresh your mind, the following is the initial value for the Inventory:\n$inv = \\MyGame\\Example\\Monster::CreateInventoryVector($fbb, array(0, 1, 2, 3, 4));\n. ",
    "suraj1291993": "Any update on this request ? I am hoping to see something similar to that of Objects generated by protobuf.. I think even providing this in an off mode will be helpful for new comers. They might opt for an easier usage. Also, existing projects might prefer to slowly move to builder pattern for cleaner implementation.. ",
    "vinaysshenoy": "@evolutional I was actually able to figure it out through a lot of trial and error a few hours before you replied. Thanks for the sample, anyway!\nI'm actually planning to submit a PR when I get a little time that reworks the samples in the project with more complexity and shows how to do this sort of stuff, which I feel is sorely needed.\n. @gwvo That's good news! I'll wait until the documentation is done then.\nClosing this now.\n. ",
    "ncpenke": "No problem! I'll sync to the latest and update the PR. The change to no longer use exception handling will actually be quite beneficial, thanks for making it!\nI also have another change that I did not submit a PR or open a issue for, but if you think it'll be useful to project I can post it. It's to rewrite the lambda methods to use explicit function objects; I'm working on an older compiler so needed to do that. But it definitely decreases the readability (lambdas are nice for that).\n. Yea, I saw that but didn't get a chance to revisit. I think your suggestion to move things to flatbuffers.h will simplify this effort greatly. Thanks for all the feedback, will incorporate it shortly!\n. Using local types as templates is only supported post gcc 4.5 (https://gcc.gnu.org/projects/cxx0x.html), so in order to support 4.4, this will have to be defined outside of the function. However, changed this to be a struct.\n. Since RndDef is a locally declared struct, also kept AddToSchemaAndInstances as a locally defined struct. However, made this more compact than the previous iteration.\n. operator() cannot be static, and std::function is unable to be initialized from a local function object.\n. Sorry, fixed.\n. Will do\n. Ok, wasn't sure. I was a little thrown off that clang defined it.\n. Done. Thanks for bearing with me on these.\n. oops :)\n. Seemed like a step backwards to pass the arguments in all places below, so added a convenience macro and a static method for type checking.\n. Found this at https://en.wikibooks.org/wiki/More_C%2B%2B_Idioms/nullptr. It'll be easier to maintain the compatability easier with this idiom, since things like field_stack_.push_back(std::make_pair(val, nullptr)); will no longer generate a compiler error because the type will be inferred. I also removed the member pointer cast since it's not used in this code anywhere. \nPlease let me know if an attribution needs to be added somewhere.\n. It seemed a bit silly to use the struct Padding::Method approach earlier, so moved these out.\n. ",
    "Arlorean": "I'd like to suggest ALL value/struct types on tables be Nullable so we can determine if the field was added or not,  unless a default is specified in the schema. Our workaround for now is to wrap up all base types into structs so they are Nullable. This is what I'd like to see in the generated C# code:\npublic struct Monster {\n    public Vector? pos; // Nullable since no default\n    public short mana; // Not nullable since 150 is the default if not present\n}\npublic struct Weapon {\n    public short? damage; // Nullable since no default\n}\nBufferPosition struct gets my vote too from the current thread.\nUPDATE: Ah, the spec says that all scalar values default to 0 if the default is not specified meaning there the struct approach is the way to do this anyway. Sorry for not fully understanding this before posting.\n. ",
    "philip-napofearth": "This PR broke the Python unit tests with \"improve indirect buffer test\" (7fbd9b8).\n. Hi, here's what I see when running PythonTest.sh:\n```\nprideout ~/git/flatbuffers/tests $ sh PythonTest.sh \nTesting with interpreter: python2.6\n..........................................................F\n======================================================================\nFAIL: test_wire_format (main.TestWireFormat)\n\nTraceback (most recent call last):\n  File \"py_test.py\", line 66, in test_wire_format\n    CheckReadBuffer(bytearray(canonicalWireData), 0)\n  File \"py_test.py\", line 155, in CheckReadBuffer\n    asserter(monster.Enemy() is None)\n  File \"py_test.py\", line 81, in asserter\n    raise AssertionError('CheckReadBuffer case failed')\nAssertionError: CheckReadBuffer case failed\n```\nI think the monster.Enemy() is None check is failing due to the addition of Fred to the test dataset; after I localled reverted the aforementioned commit, the test starting passing again.\n. ",
    "glycerine": "@rw Thank you.\nThe motivating discussion was about preserving database NULLs, so that's what came to mind.  \nAlso nil implied pointers. I figured NULL would signal that we aren't talking about pointers.\nThat said, I don't feel super strongly about this name. \nAlternatives could be: IsUnsetField(), or IsFieldSet(). I think protobuf3 uses HasField().\nI see the go bindings don't provide ForceDefaults() -- it is c++ only at the moment, so to make the IsNullField() check really useful, we could look at adding that as well.\n\nImplementation wise, I basically just copied the C++ implementation in terms of generating the constants, and made the constants/method names idiomatically Go-like.\n. We're not generating Has functions for each field, that just happens to be the start of name of the two new fields in the schema; their raison d'etre is just that they have defaults, and in particular known and fixed values for defaults that include one zero default. I was hoping their names would suggest that.  The Go API generates a getter for all fields already, I didn't change that.\nIn Go the zero value is somewhat special, so I wanted to check both zero defaults and non-zero (e.g. 3 as a default) when checking the ForceDefaults functionality. I setup new fields to avoid any test crosstalk. I could eliminate hasthreedefault (in favor of hp or mana, the only other fields with defaults) if its important.\nI don't mind renaming to FieldIsPresent() from FieldIsSet() for consistency. FieldIsPresent() says to me its a check if the field is defined at all in the schema, where FieldIsSet() allows less room for mis-interpretation, but I think being consistent is more important to simplify the documentation.\n. Abandoned... feel free to merge or modify at will.\n. ",
    "parnic": "Sure thing.\nHow do you want the indentation done on the nested if/else?\nedit: took a guess. :)\n. I just figured since there was a VS2010 folder that it was harmless to add the 2012 ones I've been using. The only problem is that VS2010 projects use the VS2010 toolchain which is no good if you don't  have both 2012 and 2010 installed. I am happy to keep my 2012 stuff local if you'd prefer.\n. This is already being done down on line 353:\nwhile (isxdigit(static_cast<unsigned char>(*cursor_))) cursor_++;\nThe exact warning is:\nidl_parser.cpp(194): warning : C6330: 'const char' passed as _Param_(1) when 'unsigned char' is required in call to 'isxdigit'.\n. Only the _mkdir call. That's why suppress is used instead of disable. Yes, prefixing (void) works, though I'm not sure I knew that was an option. I'll make the change.\n. ",
    "jmcguirk": "Sure thing - I went ahead and just swapped \"SEALED\" for partial for our own purposes, but seems like it would be generally useful - and Protobuf-net spits out partial classes fwiw.\nThe primary use case is when you want your data classes to also be able to also have business logic attached to them.\nFor instance, consider a game where buildings and other things can be harvested. If you're using an entity/component framework, you might encapsulate configuration for such behavior inside a class called \"HarvestComponent\"\n```\n/// \n/// Harvest component, encapsulates a harvestable entity\n/// \npublic class HarvestComponent : BaseComponent\n{\n    /// \n    /// The resource harvested\n    /// \n    public string HarvestResource;\n/// <summary>\n/// The amount of the HarvestResource that can be stored\n/// </summary>\npublic double HarvestCapacity;\n\n/// <summary>\n/// The minimum time after collection before a harvest can be attempted again\n/// </summary>\npublic double MinHarvestTime;\n\n/// <summary>\n/// The rate of resource generation (resources per minute)\n/// </summary>\npublic double HarvestRate;\n\n}\n```\nCool - now we have the configuration data. Where should we put our business logic in game for powering the harvest (for instance, asking how much of a particular resource can be harvested right now for a particular object?)\nThere's two options here - we either stick it on the configuration bean (rich data model) or expose a manager to own handling that data (service model). \nHere's an example of the former.\n```\n/// \n/// Harvest component, encapsulates a harvestable entity\n/// \npublic class HarvestComponent : BaseComponent\n{\n    /// \n    /// The resource harvested\n    /// \n    public string HarvestResource;\n/// <summary>\n/// The amount of the HarvestResource that can be stored\n/// </summary>\npublic double HarvestCapacity;\n\n/// <summary>\n/// The minimum time after collection before a harvest can be attempted again\n/// </summary>\npublic double MinHarvestTime;\n\n/// <summary>\n/// The rate of resource generation (resources per minute)\n/// </summary>\npublic double HarvestRate;\n\n/// <summary>\n/// Gets the amount of resource that can be currently harvested\n/// </summary>\npublic double GetCurrentHarvestValue(EntityPersistentData persistData){\n    double timeElapsed = (ServiceLocator.time.projectedServerTimeInMilliseconds/1000 - persistData.Harvest.LastHarvestTime);\n     return Math.Min(HarvestCapacity, Math.Floor(HarvestRate * timeElapsed));\n}\n\n}\n```\nAnd here's an example of the latter (Service model)\n/// <summary>\n/// Service that manages business logic for harvesting\n/// </summary>\npublic class HarvestManager\n{\n    /// <summary>\n    /// Gets the amount of resource that can be currently harvested\n    /// </summary>  \n    public static double GetCurrentHarvestValue(HarvestComponent, hc, EntityPersistentData persistData){\n        double timeElapsed = (ServiceLocator.time.projectedServerTimeInMilliseconds/1000 - persistData.Harvest.LastHarvestTime);\n        return Math.Min(hc.HarvestCapacity, Math.Floor(hc.HarvestRate * timeElapsed));\n    }\n}\nThis is by and large a stylistic choice (folks from a primarily java background will tend to favor the service model in my experience), but making the class partial enables both styles - like so\nSchema: \ntable HarvestComponent{\n    HarvestResource:string;\n    HarvestCapacity:float;\n    MinHarvestTime:float;\n    HarvestRate:float;  \n}\nPartial:\n/// <summary>\n/// Extension of HarvestComponent data bean that encapsulates biz logic and other convenience methods\n/// </summary>\npublic partial class HarvestComponent\n{\n    /// <summary>\n    /// Gets the amount of resource that can be currently harvested\n    /// </summary>  \n    public double GetCurrentHarvestValue(EntityPersistentData persistData){\n        double timeElapsed = (ServiceLocator.time.projectedServerTimeInMilliseconds/1000 - persistData.Harvest.LastHarvestTime);\n        return Math.Min(this.HarvestCapacity, Math.Floor(this.HarvestRate * timeElapsed));\n    }\n}\nThere's a fair bit of other stuff thats useful to actually paint on the data classes as well - namely Transient or Cached variables and convenience methods\n. So to give a little bit of background, we're currently storing all our Game Data (roughly 30,000 distinct JSON files covering things from quests to items to hero stats - we call these EntityTemplates) as JSON and then compiling it into Unity game objects and shipping them to our mobile client. A particular user will only query between 10 and 15% of the content catalog (based on progression) and will do so in a mostly random access way (with a few intense bursts here and there). When a particular EntityTemplate is loaded, probably on average 70-80% of it is read (with some components not being immediately relevant for the context in which it has been loaded).\nThis works decently well but has some pretty real overhead - namely they're for the most part simple POCOs (aside from the biz logic I've outlined in a few instances) but they incur the penalties of being Unity game objects (Updates on their components are called and there's issues around script compatibility between major versions). \nWe're looking to pivot away to an alternative serialized format for this data (runtime JSON parsing is a no-go) and have basically narrowed it between protobufs and flatbuffs. \nMy hunch says we'll likely wind up with protobufs since we query so much of any given EntityTemplate in one go - but the pivot from GameObjects to Protobufs/FlatBuffs shares a lot of work and I thought I'd start with something thats more geared towards GameDevs out of the box :)\nAll that being said - I think partials are definitely nice to have for FlatBuffs\n. Cool - definitely appreciate the background and insight into it.\nOur specific use case (and I wonder if this might be the highway use case?) is that its acceptable that the class member variables stay resident in memory after reading as this represents configuration data thats queried on demand. Optimally we'd clean this up, but it represents acceptable loss for us for significantly better usability\nI'm trying to wrap my head around this bit:\nWorse, the above mechanism isn't compatible with processing multiple FlatBuffers, as you'd still be pointing to the first when iterating the second\nIs this roughly what you're describing?\n1. Have a schema defining two tables. A and B. A references B.\n2. Implement the above caching code, when A.B is queried, cache the instance of B.\n3. Open up a flat buffer (X) with a byte[] that represents an instance of A. Query A.B\n4. Open up a different flat buffer (Y) with a byte[] that represents an instance of A. Query A.B -- Result: X's instance of B is returned. Expectation: Y's instance of B is returned. \nThat is to say, the class instances would themselves be shared across multiple flat buffer reads?\nIf so - definitely agree that would be a dealbreaker. We happen to have a singleton flatbuffer that contains all our catalog data so potentially less of a concern.\n. Hmm - I'm wondering if this is actually the case, though its possible I'm not understanding how others are using flatbuffs in practice. \nJust taking a look at the code it seems like everytime GetRootAsX is called a new instance is returned, meaning the cached values should be held separate and should remain sane.\nHere's a test harness I put together with reference caching code in place including test output\nhttps://gist.github.com/jmcguirk/d7705f6b92ea4c57c045\n. Ah okay - sorry for the confusion. Yeah I think it makes sense to store the cached reference as a member variable.\nThe cached members should get ejected from memory as soon as the root object is no longer referenced - meaning the only time they're superfluous in memory is when you no longer care about a particular subtree of a root, but still care about another part of that tree - which can be true if your root is long lived and covers a lot of data. This actually happens to be exactly our use case - but, for us, its vastly preferable to managing our own references in calling code.\nThe vector references are interesting as well - and agree there'd need to be a solution here for there to be a consistent expectation. On our end, we're actually reading the entire vector into a member variable on first query. This was necessary to expedite integration and most of our use cases wound up traversing the entire vector anyways - but I realize this may not match up to the average use case and if the convenience methods were to adopt reference caching it probably makes more sense to cache (lazily) by index rather than create all the instances in one go.\nSo to sum up - It seems like there's 3 potential options here (2 of which are currently supported)\n1.) Use the existing A.B convenience method. Pros: Convenience, easy integration. Cons: GC churn, equality checks, can't cache other member variables via partials.\n2.) Use the existing A.GetB(MyInstance) method. Pros: No excess GC generation. Cons: Harder integration path, more boiler plate code to manage references\n3.) Use a [New] A.B convenience method that caches references. Pros: Convenience, easy integration, no GC churn, equality checks and caching via partials work. Cons: Dangling references created until root is no longer referenced\nIn all instances, I agree that #2 yields the best - most predictable performance at the cost of more boiler plate code.\nFor the convenience methods - our use case definitely favors the semantics of #3 here, but totally understand if its not consistent with your vision for FB\n. Cool sounds good - we'll maintain our own fork for this behaviour :)\n. ",
    "SermoDigitalDeployer1000": "We've been using this in https://github.com/SermoDigital/flatbuffers for a while without issues.\n. var should be const since the types won't ever change. Also allows for them to be used as array lengths.\n(Posting as @ericlagergren.)\n. ",
    "gonzaloserrano": "Also interested in this PR.\n@SermoDigitalDeployer1000 is that a private repo?\n. We have solved this case sending the in first two bytes the size of the flatbuffer data.\nThen we use the deserialization proposed here https://github.com/google/flatbuffers/pull/3977 to handle any type of flatbuffer.\n. I think protobufs has an option to define the go package https://developers.google.com/protocol-buffers/docs/reference/go-generated#package. More info https://github.com/golang/protobuf/issues/39\n. Agree, but i think its not very straightforward. Looks like people who uses protobufs even have made wrappers to do it, e.g https://github.com/peter-edge/protoeasy-go#go\n. The flatbuffers documentation describes:\n\nstring, which may only hold UTF-8 or 7-bit ASCII. For other text encodings or general binary data use vectors ([byte] or [ubyte]) instead.\n\nSo i think its semantically more correct that if a field has a string type then its accessor function should return a string, not a []byte, even when in go a string is in fact a byte array (but the opposite is not true). If i wanted a byte array i would indeed use the [byte] flatbuffer notation.\nAbout the efficiency, the code which will use them will need to type cast anyway, since usually domain models work with strings, not byte arrays. \n. Rebased and ready to merge.\nedit: the diff is far more readeable with the ignore-whitespace option ?w=1: https://github.com/google/flatbuffers/pull/3977/files?w=1\n. 1. will do\n2. as i linked before, its modelled after proto.Message interface (code). I'll change it to Flatbuffer or any other suggestion you would have.\n3. it's tested partially when using the new function GetRootAs which receives a Message here. The methods of that interface are already tested since they are part of the current code.\n. Hi guys, sorry for the delay. As requested i have:\n- added a test for accessing a struct and table types of flatbuffer here https://github.com/google/flatbuffers/pull/3977/files#diff-7d8db511fdeeb3e19e602b2726e3525fR1170\n- renamed Message to FlatBuffer\n. Hi guys, it's been a couple of weeks. Do you think this code can be merged or it needs more iterations? Thx.\n. Closing since no one seems to really care.. I think you'll find this PR useful https://github.com/google/flatbuffers/pull/3977/files even it doesn't look to be merged soon. Basically it does the same thing and provides an interface.\nedit: plus my PR provides the GetRootAs method and has tests\n. @rw i guess you were referring to this PR instead of mine at  https://github.com/google/flatbuffers/pull/3977 right? My use case is explained in the 3977's description.\n@jchiu0 did you check my PR? There you can found a generic GetRootAs() func that works thanks to the new interface.\n. this is to make it mac-compatible\n. ",
    "wingyippp": "Appending underscores should work for reserved names. But the field name starts with digits, such as 320size : long, does not work. What about making the field names starts with underscores?\n. ",
    "fichter": "I am no cmake expert, but FLATBUFFERS_FLATC_EXECUTABLE seems to be defined in CMake/FindFlatBuffers.cmake:\n    # Find the flatbuffers schema compiler\n    # Output Variables:\n    # * FLATBUFFERS_FLATC_EXECUTABLE the flatc compiler executable\n    ...\n    find_program(FLATBUFFERS_FLATC_EXECUTABLE NAMES flatc)\nI stumbled upon the issue when I was trying to build flatbuffers through the Yocto Project.\nThe lines \"COMMAND flatc ...\" in CMakeLists.txt seems to simply try to execute flatc from the system path. However, when building natively on the same machine using the following commands it works fine: cmake -G \"Unix Makefiles\" && make\nYocto build log follows:\n...\n| [ 51%] Built target flatc\n| make -f CMakeFiles/flatsamplebinary.dir/build.make CMakeFiles/flatsamplebinary.dir/depend\n| make -f CMakeFiles/flatsampletext.dir/build.make CMakeFiles/flatsampletext.dir/depend\n| make -f CMakeFiles/flattests.dir/build.make CMakeFiles/flattests.dir/depend\n| make[2]: Entering directory '/home/rfichter/yocto/qmx6/tmp/work/x86_64-linux/flatbuffers-native/1.2.0-r0/build'\n| /home/rfichter/yocto/qmx6/tmp/sysroots/x86_64-linux/usr/bin/cmake -E cmake_progress_report /home/rfichter/yocto/qmx6/tmp/work/x86_64-linux/flatbuffers-native/1.2.0-r0/build/CMakeFiles 15\n| make[2]: Entering directory '/home/rfichter/yocto/qmx6/tmp/work/x86_64-linux/flatbuffers-native/1.2.0-r0/build'\n| /home/rfichter/yocto/qmx6/tmp/sysroots/x86_64-linux/usr/bin/cmake -E cmake_progress_report /home/rfichter/yocto/qmx6/tmp/work/x86_64-linux/flatbuffers-native/1.2.0-r0/build/CMakeFiles 17\n| make[2]: Entering directory '/home/rfichter/yocto/qmx6/tmp/work/x86_64-linux/flatbuffers-native/1.2.0-r0/build'\n| /home/rfichter/yocto/qmx6/tmp/sysroots/x86_64-linux/usr/bin/cmake -E cmake_progress_report /home/rfichter/yocto/qmx6/tmp/work/x86_64-linux/flatbuffers-native/1.2.0-r0/build/CMakeFiles 21\n| [ 55%] [ 59%] [ 62%] Generating samples/monster_generated.h\n| Generating samples/monster_generated.h\n| flatc -c --no-includes --gen-mutable -o samples /home/rfichter/yocto/qmx6/tmp/work/x86_64-linux/flatbuffers-native/1.2.0-r0/git/samples/monster.fbs\n| Generating tests/monster_test_generated.h\n| flatc -c --no-includes --gen-mutable -o samples /home/rfichter/yocto/qmx6/tmp/work/x86_64-linux/flatbuffers-native/1.2.0-r0/git/samples/monster.fbs\n| make[2]: flatc: Command not found\n| CMakeFiles/flatsamplebinary.dir/build.make:56: recipe for target 'samples/monster_generated.h' failed\n| make[2]: *** [samples/monster_generated.h] Error 127\n. ",
    "opsenas": "i was referring to test.cpp ~ln318, i was attempting to find a way to access member by some kind of offset in example, i have schema:\ntable Struct1\n{\n    ...\n}\ntable Root\n{\n    value1 : Struct1;\n    value2 : Struct2;\n    value3 : Struct3;\n}\ni was searching for a way to access \"valueN\" by offset, id or other way, without using the vector or array, but in the end result i ended up just using vector\n. thank you, seems i had an old version, updated and found\n. The idea is that there is progress bar element in progress bar layout +\nheaders are kept in different folders and are re parsed to fix includes, so\nin code it isn't that confusing since they are in different folders...\nOn Apr 8, 2017 3:45 AM, \"Wouter van Oortmerssen\" notifications@github.com\nwrote:\n\nThat's because it generates include statements without a path, which in\nthis case would be #include \"ProgressBar_generated.h\", which is the same\nas the current file. It filters these out to avoid recursive includes.\nIt could warn about this I guess.\nI'd recommend against giving different concepts the same name though,\nsounds very confusing. I had a hard time understanding the schema :)\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/4257#issuecomment-292682735,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACT07OszpBr14VCQqdh3-x7d6_LcVb8Uks5rtti4gaJpZM4M0eML\n.\n. \n",
    "Mandar-Shinde": "\n. \n.  It\u2019s my pleasure.. :blush:\n. ",
    "escholtz": "Oops. Will close and reopen with proper exports.\n. Yea, doesn't seem like there's a perfect solution (unless you want to modify the build script to pull git sha which adds complexity). Could also include some combination of date and version # too.\nThe problem I was running into was that the test script was using the version of flatc that was brew installed instead of the version built from master. The brew installed version was crashing with the flatbuffer test inputs from master.\nIt's not a major issue and I quickly figured out what was wrong, but I'm just trying to note the pain points as a first time user (even if they're small). For people evaluating the project, enough small pain points and they might give up and go with an alternative.\n. Would that solve the issue you described in the previous comment (head vs non-head)? Wouldn't you want a HEAD flag in there \"1.2.0 HEAD (Built \" __DATE__ \")\" and then remove the HEAD when you're  releasing official builds?\n. Updated PR based on feedback.\n. Removed -v\n. ",
    "andrei-pokrovsky-nv": "It's 64-bit arm, aarch64, g++-aarch64-linux-gnu\u200b. I think you are right about -fsigned-char.\n. ",
    "Chippiewill": "@gwvo Just ran into this while compiling for RPi and -f-signed-char seems to resolve this. I'd open a PR but my cmake-foo isn't so great and I can't work out how to make it detect ARM specifically.\n. Ok, I'll open a PR for it in a bit.\n. You already merged it https://github.com/google/flatbuffers/pull/3789\n. ",
    "geraldstanje": "hi, any idea when that gets fixed?. ",
    "smandy": "I think I've hit the same problem. Just doing initial experiments with Flatbuffers. I can write flatbuffers from C++, read them into python, but no joy on the js side.\nI've got a complete example at https://github.com/smandy/flatBufferExperiment\nI've committed the actual flatbuffer file as well. cd into js directory and try node experiment2.js. The object deserialised ( rehydrated?!) from the flatbuffer appears empty. i.e vector has zero length and title is null.\n. ",
    "KhasanovBI": "Does this issue resolved? I have the same problem too. When I pass serialized data from java over websocket to browser js, get ArrayBuffer, pass it to Uint8Array, and feed to flatbuffers.ByteBuffer and nothing in data.. ",
    "gaurav03jain": "Any update on this issue ?\n. Its too hectic process to fix all identifiers in .fbs file if it is huge. why can't we have some supplier flag during compilation to distinguish the same ?\n. I tried, but didn't find any such option \"--escape-proto-identifiers\". If its there can you please tell me usage.\nBTW i am running following command  \"flatc --escape-proto-identifiers --proto test.proto\"\n. Yes, latest commit id: 20c0082ee5bfeeecaa443c001a89934e9448ffa4,\nWhich contains some namespace changes.\n. Ideally this is the only correct way, compiler should convert/compile each dependent files separately and import all of them in one root file. Combining all the code into single file potentially create issues at other places.\n. ",
    "Mischanix": "Added.\n. Any update?\n. ",
    "kennir": "same problem here\n. ",
    "amarDwattamwar": "Thanks for replying.Loading part is working fine after debugging I am seeing required string contents are there.its failing when parser is parsing.I already tried by loading file in JAVA code I ran into similar issue.\nparser.error_:\n02-22 14:45:22.801 31168-31257/score_center.debug E/jni/FromParsingLibrary: 1:0: error: expecting: [ instead got: identifier: null\nI double checked JSON content nothing wrong in that.\n. JSON :\n{\n  \"status\": \"success\",\n  \"timestamp\": \"2016-02-22T19:44:54Z\",\n  \"calendar\": {\n    \"endDate\": \"2016-11-04T06:59:59Z\",\n    \"eventDates\": {\n      \"dates\": [\n        \"2016-02-21T08:00:00Z\",\n        \"2016-02-22T08:00:00Z\",\n        \"2016-02-23T08:00:00Z\",\n        \"2016-02-24T08:00:00Z\",\n        \"2016-02-25T08:00:00Z\",\n        \"2016-02-26T08:00:00Z\",\n        \"2016-02-27T08:00:00Z\",\n        \"2016-07-11T07:00:00Z\",\n        \"2016-07-12T07:00:00Z\",\n        \"2016-07-13T07:00:00Z\",\n        \"2016-07-14T07:00:00Z\",\n        \"2016-10-03T07:00:00Z\",\n        \"2016-10-04T07:00:00Z\",\n        \"2016-10-05T07:00:00Z\",\n        \"2016-10-06T07:00:00Z\",\n        \"2016-10-07T07:00:00Z\",\n        \"2016-10-08T07:00:00Z\",\n        \"2016-10-09T07:00:00Z\",\n        \"2016-10-10T07:00:00Z\",\n        \"2016-10-11T07:00:00Z\",\n        \"2016-10-12T07:00:00Z\",\n        \"2016-10-13T07:00:00Z\",\n        \"2016-10-14T07:00:00Z\",\n        \"2016-10-15T07:00:00Z\",\n        \"2016-10-16T07:00:00Z\",\n        \"2016-10-17T07:00:00Z\",\n        \"2016-10-18T07:00:00Z\",\n        \"2016-10-19T07:00:00Z\",\n        \"2016-10-20T07:00:00Z\",\n        \"2016-10-21T07:00:00Z\",\n        \"2016-10-22T07:00:00Z\",\n        \"2016-10-23T07:00:00Z\",\n        \"2016-10-24T07:00:00Z\",\n        \"2016-10-25T07:00:00Z\",\n        \"2016-10-26T07:00:00Z\",\n        \"2016-10-27T07:00:00Z\",\n        \"2016-10-28T07:00:00Z\",\n        \"2016-10-29T07:00:00Z\",\n        \"2016-10-30T07:00:00Z\",\n        \"2016-10-31T07:00:00Z\",\n        \"2016-11-01T07:00:00Z\",\n        \"2016-11-02T07:00:00Z\",\n        \"2016-11-03T07:00:00Z\"\n      ],\n      \"type\": \"blacklist\"\n    },\n    \"sections\": [\n      {\n        \"endDate\": \"2016-04-03T06:59:59Z\",\n        \"entries\": null,\n        \"label\": \"Preseason\",\n        \"startDate\": \"2016-02-21T08:00:00Z\"\n      },\n      {\n        \"endDate\": \"2016-10-03T06:59:59Z\",\n        \"entries\": null,\n        \"label\": \"Regular Season\",\n        \"startDate\": \"2016-04-03T07:00:00Z\"\n      },\n      {\n        \"endDate\": \"2016-11-04T06:59:59Z\",\n        \"entries\": null,\n        \"label\": \"Post Season\",\n        \"startDate\": \"2016-10-03T07:00:00Z\"\n      }\n    ],\n    \"startDate\": \"2016-02-21T08:00:00Z\",\n    \"type\": \"day\"\n  },\n  \"emptyResponse\": false\n}\n. I am using 1.2.0.\n. Thanks, I will update the version and Ill see its working or not.\n. As per your suggestions I updated library version to newest.\nBut now I am getiing : error: unknown field: emptyResponse\n. I doubled check Schema I am not seeing any fields are missing :\nnamespace SportsCalendar;\ntable SportsCalendarInfo {\n    calendar : Calendar;\n    timestamp : string;\n    status : string;\n}\ntable Calendar {\n    type : string;\n    startDate : string;\n    endDate : string;\n    eventDates : EventDates;\n    sections : [Section];\n}\ntable EventDates {\n    type : string;\n    dates : [string];\n}\ntable Section {\n    label : string;\n    startDate : string;\n    endDate : string;\n    entries : [Entry];\n}\ntable Entry {\n    label : string;\n    detail : string;\n    startDate : string;\n    endDate : string;\n    links : Links;\n}\ntable Links {\n    api : API;\n}\ntable API {\n    self : Self;\n    events : Events;\n}\ntable Self {\n    href : string;\n}\ntable Events {\n    href : string;\n}\nroot_type SportsCalendarInfo;\n. Thanks Ill try that.\n. ",
    "wattamwar": "Problem is solved I am able to see parsed data now and You are right some unexpected filed was there in response. Thanks for your time and help. \n. ",
    "tylorr": "My use case is to read a buffer from file, edit the contents and write back to file. I'm wondering if protocol buffers would be better suited for this need. I may need to convert from protobuffers to flatbuffers though.\n. ",
    "verokarhu": "Actually, using files does sound simpler, I hadn't even considered that. I was afraid that hand-crafting buffers or building the messages with flatbuffers would be the only way, which sounded more tedious than implementing the accessors. Chalk it up to a lack of RTFM or maybe interface fever.\nThanks!\n. ",
    "kfirufk": "exactly what I was looking for. i guess it could be better documented. thank you\n. hi. thank you for your comment. \nsomething is not quite clear for me. \n10.21 in binary is 01000001001000110101110000101001 which is 32 bit. why would it be truncated ? anyhow i'm gonna try with double and report back here with results.\n. ok thank you :)\n. ",
    "sahiljain": "This looks to have been fixed with #3976.\n. I think if you assume that no vtables are shared, and no bytes are added for alignment, it's possible to analyze the serialized size of tables/structs within a flatbuffer. \nEach table uses:\n- 4 bytes for the vtable pointer\n- 2 bytes for the vtable's size\n- 2 bytes for content's size\n- 2 bytes for each field pointer (00 00 if field is not set)\n- bytes for the fields themselves\nEach vector uses:\n- 4 bytes for the pointer to the vector\n- 4 bytes for the number of items in the vector\n- bytes for each element in the vector\nEach struct uses:\n- Each field is stored inline, so sum the bytes of the fields\nFor example, consider the schema:\n```\nstruct Vec3 {\n  x:float;\n  y:float;\n  z:float;\n}\ntable Monster {\n  pos:Vec3;\n  mana:short = 150;\n  hp:short = 100;\n  name:string;\n}\nroot_type Monster;\n```\nThe Monster flatbuffer would use (4 + 2 + 2) + 2*numFields + posLength + manaLength + hpLength + nameLength.\nnumFields = 4\nposLength = 4 + 4 + 4\nmanaLength = 2\nhpLength = 2\nnameLength = 4 + 4 + 2*(numChars + 1)\nThe total length is 42 + 2*numChars.\n. @aardappel I've reopened the pull-request here: #4179. The checks have passed and it is ready to merge!. \ud83d\udc4d will do\n. strtoll will modify endptr to point to 1 past the last parsed character, so it can't be const.\n. @gwvo My mistake! Fixed.\n. ",
    "ngbrown": "On a messaging protocol that doesn't allow null and any default value isn't sent/set, thus wouldn't be present, I think this is something to avoid.  I made the same mistake on a system using Protocol Buffers, where, if changed, the value was set.  However, now, in Protocol Buffers 3, those isStatePresent fields were removed.\nI would suggest having another field or set of boolean flags used to indicate what fields you are sendind in this update.  That way you will be able to receive a value that was the \"default\" value.\n. ",
    "rgilles": "Hi,\nIn the java reflection #4019 PR I introduce the Reflection#hasValue(...) method. If this method needs to be rework let me know?\nThanks,\nRomain.\n. Hi Wouter,\nI was just pointing the method I have created just to know if the semantic of this method must comply to the one you propose here and therefore rename the Reflection one to Reflection#isFieldPresent.\nDoes it make sens?\nRegards.\n. Hi Wooter,\nFirst sorry for the delay...\nI see your points. And I understand that you don't want to make to much structural modification in your project. For the JUnit point I just do a dummy test to show you the code style but I didn't want to modify all the tests before your acknowledge.\nI propose you 3 options that you can choose or not (this at least the concept of options):\n1. Move the java pom.xml to the root in order to separate the configuration / build directory from the sources folder and it is more natural in maven. \n2. Make the produced jar OSGi compatible. Not a big deal and I would like to use it in OSGi environment.\n3. Migrate the test code to JUnit framework:\n   - pros: better integration with the ide. Supported by maven gradle... out of the box within the build process.\n   - cons: will make more complex the sh / bat file where we may have to dowload or save JUnit jar and its dependency (hamcrest) has the latest version (4.12) does not provide the uber jar version.\n     So do you want to close this pull request and open a new one to apply the options you would like select?\nRegards,\nRomain\n. Hi Wooter,\nIf you want we can close this pull request and create a new one to apply the options I mention above?\nAs you want.\nRegards,\nRomain\n. Hi Wouter,\n2: This only thing that osgi introduce in the jar is dedicated manifest entries to specify the visibility of the package provided by flatbuffers lib. It has nothing to do with service but allow you to link generated code from flatc to the flatbuffers lib into osgi env. But if you don't want, do you authorized me to fork your flatbuffers java lib and publish it as an osgi compatible library under my domain on maven central?\n3: What JUnit will buy you (us) is an automatic report into maven build as it is a the default test framework for maven. And also a integration in major the java IDE as eclipse and intellij with code coverage and plenty of way to run the tests. If you have a look to other google java library like guava, guice, dagger you will see that they are using JUnit as a test framework.\nRegards,\nRomain\nPS: sorry for the typo in your first name.\n. Hi Wouter,\nSorry I didn't test it with JDK other than 8. But I give it a try and I get not error:\nCompile then run the Java test.\njava version \"1.6.0_45\"\nJava(TM) SE Runtime Environment (build 1.6.0_45-b06)\nJava HotSpot(TM) 64-Bit Server VM (build 20.45-b01, mixed mode)\n./JavaTest.sh: 26: ./JavaTest.sh: [[: not found\nclean target\nFlatBuffers test: completed successfully\nExcept the issue with the if statement [[ ]]\nRomain\n. I did a test with the 6,7 and 8. It works for me. Can you point me to the error?\nJava 7\nCompile then run the Java test.\njava version \"1.7.0_75\"\nJava(TM) SE Runtime Environment (build 1.7.0_75-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 24.75-b04, mixed mode)\n./JavaTest.sh: 26: ./JavaTest.sh: [[: not found\nclean target\nFlatBuffers test: completed successfully\nJava 8\nCompile then run the Java test.\njava version \"1.8.0_102\"\nJava(TM) SE Runtime Environment (build 1.8.0_102-b14)\nJava HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode)\n./JavaTest.sh: 26: ./JavaTest.sh: [[: not found\nclean target\nFlatBuffers test: completed successfully\nRomain\n. I will create a new pull request ok?\n. Hi Wouter,\nIf you get this error:\nJavaTest.java:22: cannot access com.google.flatbuffers.FlatBufferBuilder\nbad class file: /home/rogilles/sandbox/google/my-flatbuffers/tests/../java/com/google/flatbuffers/FlatBufferBuilder.class\nclass file has wrong version 52.0, should be 50.0\nPlease remove or make sure it appears in the correct subdirectory of the classpath.\nimport com.google.flatbuffers.FlatBufferBuilder;\n                             ^\nException in thread \"main\" java.lang.NoClassDefFoundError: JavaTest\nCaused by: java.lang.ClassNotFoundException: JavaTest\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)\nCould not find the main class: JavaTest.  Program will exit.\nThen you must first run the following command:\nfind ../ -name \"*.class\" | xargs rm\nBy doing this you will remove the remaining class from the source folder and then you can run again the JavaTest.sh as follow:\n```\nflatbuffers/tests > ./JavaTest.sh \nCompile then run the Java test.\n./JavaTest.sh: 22: ./JavaTest.sh: [[: not found\nFlatBuffers test: completed successfully\nflatbuffers/tests > java -version\njava version \"1.8.0_102\"\nJava(TM) SE Runtime Environment (build 1.8.0_102-b14)\nJava HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode)\nflatbuffers/tests > jdk6\nRemove previous JAVA_HOME link: /usr/local/lib/jvm/jdk\nPoint settings to /usr/local/lib/jvm/jdk1.6\nflatbuffers/tests > ./JavaTest.sh \nCompile then run the Java test.\njava version \"1.6.0_45\"\nJava(TM) SE Runtime Environment (build 1.6.0_45-b06)\nJava HotSpot(TM) 64-Bit Server VM (build 20.45-b01, mixed mode)\n./JavaTest.sh: 26: ./JavaTest.sh: [[: not found\nclean target\nJavaTest.java:22: cannot access com.google.flatbuffers.FlatBufferBuilder\nbad class file: /home/rogilles/sandbox/google/my-flatbuffers/tests/../java/com/google/flatbuffers/FlatBufferBuilder.class\nclass file has wrong version 52.0, should be 50.0\nPlease remove or make sure it appears in the correct subdirectory of the classpath.\nimport com.google.flatbuffers.FlatBufferBuilder;\n                             ^\nException in thread \"main\" java.lang.NoClassDefFoundError: JavaTest\nCaused by: java.lang.ClassNotFoundException: JavaTest\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)\nCould not find the main class: JavaTest.  Program will exit.\nflatbuffers/tests > find ../ -name \"*.class\" | xargs rm\nflatbuffers/tests > ./JavaTest.sh \nCompile then run the Java test.\njava version \"1.6.0_45\"\nJava(TM) SE Runtime Environment (build 1.6.0_45-b06)\nJava HotSpot(TM) 64-Bit Server VM (build 20.45-b01, mixed mode)\n./JavaTest.sh: 26: ./JavaTest.sh: [[: not found\nclean target\nFlatBuffers test: completed successfully\n```\nRegards,\nRomain\n. I can add this clean into the script but it is relevant just for the first migration step\n. Ok\n. Hi,\nBut by construction my flatbuffers.Object#fields attribute is a vector of flatbuffers.Field see the declaration of the field here in the reflection schema: https://github.com/google/flatbuffers/blob/master/reflection/reflection.fbs#L71.\nDoes it make sens?\nMaybe what you mean is it must be a sorted vector of Field?\nThanks,\nRomain.\n. Hi,\nFirst because I cannot use the pre define constant due to the point 1 above in my first comment. Then because the current implementation of the Field#lookupByKey(...) I had to apply -4 in order to compensate the +4 here: https://github.com/google/flatbuffers/pull/4019/files#diff-be50a31b6d33da1f5b8e4a19fc1674baR85\nThanks,\nRomain\n. Hi Wouter,\nI have started to add more tests. I would you to have a look to the naming of the methods (see the TODO). I would like to have your approval on the naming in order to start to implements more.\nThanks in advance,\nRomain\n. Hi Wouter,\nThe idea was about discussing the overall design through a simple use case i.e. short and then apply it 'massively' for the other ones. But any way I'm doing it for the other ones.\n. Hi,\nWe are working on the Java reflection PR and we encounter the same issue. In this PR we did an evolution of this method in order to fix this issue. For now we discuss with Wouter to extract this evolution into a separate PR there for this bug should be fixed by the coming soon evolution.\nRegards,\nRomain\n. Hi Wouter,\nThis is the plan to add the same set of tests than the C++ part. But I'm doing it one step at a time ;). For now we (with Baptiste) are trying to solve the first issue we encounter: being able to generate java code in a prefixed package (Step 1). Then as soon as your are satified with this we will move forward on the testing and implementation part. \n. Yes this is the plan not time to do it we would like to be sure first that what we did on the C++ generation part was not stupid. Thanks for your review and sorry for not respecting the Google C++ code style.\n. Fixed\n. Hi Wouter,\nI would like to make this method part of the parent class (i.e. Table) instead of generate it for all the subclasses. Does it make sens for you?\nI'm using it in the Reflection.java at: https://github.com/google/flatbuffers/pull/4019/files#diff-fe28cbc55417d497c520cbc9797dbc6eR27\n. Done. Sorry just want to have answer on the name of the method for now I didn't check the code enough.\nSorry for that.\n. Stupid things. I was sure it was something existing in the cpp version but now as I'm checking it... It is not the case. Therefore I remove it.\n. There was no .bat but I will add it ;)\n. Hi Wouter,\nThe current version of the field access by lookup was not usable to make progress on this PR. We will extract this evolution into a different PR in order to apply separation of concern as you request.\nBut we would like to merge those external PR into this one in order to avoid to block this one.\nDoes it make sens for you?\nLet us know. How to you see it?\n. Why are you not using a float for the value here?\n. Same point than for the float value why not a double value?\n. Again :P why not a string value?\n. Hi Wouter,\nI'm agree with you using inheritance for technical concern is a bad design. But IMHO using generation to avoid it is not really better because for me generated code should be written as if I was writing it manually. Therefore what I propose is to extract those methods in a technical class and use delegation instead of inheritance.\nWhat do you think about that?\nRomain.\n. Hi Baptiste,\nLets do it as Wouter want: move the code from Table.java to the code generator.\n. ",
    "mzaks": "Will be glad to create a PR, when most of the Issues I defined in my repo are resolved.\nOne of them is generating API which let user generate binary array without creating objects beforehand.\n. Sorry for being under the radar for that long. Moved the project to support Swift3. Didn't battle tested it yet, but the unit tests are green :)\nI still have some features which has to be implemented to reach the feature parity though.\nLike:\n- https://github.com/mzaks/FlatBuffersSwift/issues/9\n- https://github.com/mzaks/FlatBuffersSwift/issues/10\n- https://github.com/mzaks/FlatBuffersSwift/issues/11\n- https://github.com/mzaks/FlatBuffersSwift/issues/15\n- https://github.com/mzaks/FlatBuffersSwift/issues/48\nThan FlatBuffersSwift can generate invalid buffers as I implemented support for cycles.\nI also provide users with the possibility to not nil terminate the strings. \nThis is however a small thing. I could remove it from the BuildConfig and force null termination.\nTo be honest I am also puzzled, how it would be merged as the only thing you need is the code generator fbsCG.jar. I guess it is similar to C port which has a separat repository as well.\n. The main reason why I implemented cyclical graph support, was because it conforms with object graph mapping. \nGiven following fbs\ntable Person {\n  name : string;\n  friends : [Person]\n}\nThe graph mapping strategy let's user encode following graph.\nlet p1 = Person()\np1.name = \"Maxim\"\np1.freinds = [p1]\nlet data = try?p1.toData()\nThe last statement produces the buffer. Which will be valid iff Object offsets are allowed to be signed 4 bytes integers. As for example vTable offsets are. But we already had this discussion :)\nWhat I could do is following:\nThe toData method receives a config parameter, which has a default definition.\npublic struct FBBuildConfig {\n    public let initialCapacity : Int\n    public let uniqueStrings : Bool\n    public let uniqueTables : Bool\n    public let uniqueVTables : Bool\n    public let forceDefaults : Bool\n    public let nullTerminatedUTF8 : Bool\n    public init(initialCapacity : Int = 1, uniqueStrings : Bool = true, uniqueTables : Bool = true, uniqueVTables : Bool = true, forceDefaults : Bool = false, nullTerminatedUTF8 : Bool = false) {\n        self.initialCapacity = initialCapacity\n        self.uniqueStrings = uniqueStrings\n        self.uniqueTables = uniqueTables\n        self.uniqueVTables = uniqueVTables\n        self.forceDefaults = forceDefaults\n        self.nullTerminatedUTF8 = nullTerminatedUTF8\n    }\n}\nI could flip nullTerminatedUTF8from false to true and add an allowCycles property which will default to false\nThis way: p1.toData() will throw an exception. The user will have to explicitly call:\np1.toData(config: FBBuildConfig(\n  initialCapacity = 1024, uniqueStrings = true, uniqueTables = true, uniqueVTables = true, forceDefaults = false, nullTerminatedUTF8 = true, allowCycles = true\n))\nIn order to build buffer which contains cycles and there for is incompatible with standard FlatBuffers implementations. I understand that it is not ideal, but it will be users conscious decision to go with a solution which is incompatible with other implementations.\nIn my current iOS project (http://resiapp.io) I use FlatBuffers for app state persistence. By enabling cyclic graph I get a persistence strategy which is better than any other technology out there. It also implies that I don't really care about compatibility with other languages, because the data does not leave the device. Removing the cycle support completely feels like putting a tempo limit on an engine which can go faster because of some public regulations :)\nI understand the reasons behind the \"public regulation\" but it still hurts\n. @m339606 Which incompatibility problem do you have in mind?\nGenerally it would be cool if we had a formalised test of suits. \n1. Multiple schemas to verify IDL support for different code generators.\n2. Schemas and binaries to verify if the platform specific implementation can encode and decode data represented with such binary.\n3. A few use cases for bench mark tests, where every platform implementation performance characteristics is listed.\nI am willing to contribute in creation of such test suit.. @eduardbosch it should be safe to read.\nhttps://github.com/mzaks/FlatBuffersSwift/blob/master/FlatBuffersSwift/FlatBuffersReader.swift Contains a file and memory reader. So you can read from Data or from FileHandle.\nYou can use fbsCG.jar to generate Swift classes. However there could be some warning now, as it was built for Swift3, I worked on a pure Swift parser and code generator on following branch. https://github.com/mzaks/FlatBuffersSwift/tree/code_gen it suppose to be a console application written in Swift which can read fbs files and generate swift classes. It is currently on hold because I don't use FlatBuffers in current projects and there were not much request help from the community. I also think I saw other ports. Haven't tried them though.. I signed it!. Yeah I already figured that out :)\nI can fix the padding by rewriting test with partCheck as in:\nhttps://github.com/mzaks/flatbuffers/blob/flexbuffers-unit-tests/tests/flexbuffers_test.cpp#L392\nI think the main goal of those tests is to be a visual aid for people who want to understand format and port it to another language. I know it helped me with Swift. I am also thinking about writing another port for C# and there I would just start with the same unit tests to make sure that the buffer is correct.\nHaving a round trip test is cool but, it does not help understanding the layout. It is more of an integration test.\nTo fix the padding problem and to keep the layout still readable. I can do following:\ncheck(\"3\", \"_\", \"_\", \"_\", \"1\", \"0\", \"0\", \"0\", \"204\", \"216\", \"0\", \"0\", \"3\", \"0\", \"0\", \"0\", \"12\", \"50\", \"1\")\nIf we would use strings in the check instead of int, we could describe the padded bytes with _\nIt's a bit more work to implement, but gives the best picture for the internals. Also will directly show where we waste space through padding.\nOr less nice but simpler to implement:\ncheck(3, -1, -1, -1, 1, 0, 0, 0, 204, 216, 0, 0, 3, 0, 0, 0, 12, 50, 1)\ncheck receives int16_t which we ignore if <0 or else cast to uint8_t and compare with value in buffer.. Deleting code / tests is always fine with me :)\ncheck macro (or actually TestEq) already outputs the index which lead to the problem.\nhttps://github.com/mzaks/flatbuffers/blob/flexbuffers-unit-tests/tests/flexbuffers_test.cpp#L29\nI am not particularly proud of how I pass the index, but well \ud83d\ude43.\nPadding bytes are actually not always 0. The test failed before I introduced partCheck. But now I think I will rewrite it as I described in my last comment - marking padded regions with negative numbers.\nAnd I will also add extra .cpp to the Android.mk and refactor redundant code to test.h\nWill write a comment when I am done.. Now I made one test fail on appveyor.\nthe output is strange on this thing but if you look in this log\nhttps://ci.appveyor.com/project/gwvo/flatbuffers/build/1.0.604/job/q664942cme19xwue\nYou can see starting from line 96 that index 4, 5 and 6 are not 0 even though it is the padded region. In the next build I added a dump call so we can see the full byte array.. OK now the build with the dump is also done:\nhttps://ci.appveyor.com/project/gwvo/flatbuffers/build/1.0.605/job/7sauw7mfjebq7mk8\nOn Line 97 you can see that 3 was memory aligned to 8 bytes and bytes 4, 5, 6 are random garbage.. Well this in not what I meant :)\nTYPE_VECTOR_BOOL would be great but I would expect it to be a bitfield than.\nWhat I mean is a typed vector which contains bool values which are internally stored as uint.\nA bool is now stored as 1byte uint value. \nIf you would like to add a scalar vector of bools. Like with this method:\nhttps://github.com/google/flatbuffers/blob/master/include/flatbuffers/flexbuffers.h#L1343\nBefore you would create a typed vector which contains a vector of uint8 values which you could also access with asBool.\nNow this is impossible because of IsTypedVectorElementType & ToTypedVector and maybe a few more places.\nHere is my commit where I introduced Bool type in FlexBuffers, maybe it helps:\nhttps://github.com/mzaks/FlexBuffersSwift/commit/509db06e49aeaaef347a9db46e2f5da40495d527\nSo I would expect that I can still save a typed bool vector which is internally stored as uint8. And Have a complete different more bit packed version for TYPE_VECTOR_BOOL. \nWell actually if TYPE_VECTOR_BOOL can be introduced than we can skip what I did in FlexBuffersSwift, but I guess a proper TYPE_VECTOR_BOOL is much more complex. So I went with an intermediate solution.\nI guess for now you can just check if you can create a vector of bools like this: fbb.Vector(true, false, true); I could not in Swift and I needed to because I already use such construct in production.\n. Yeah sorry my answer was not really structured. I would suggest following:\nCheck if it was possible to create a scalar bool vector through an API like fbb.Vector(true, false); or fbb.Add({true, false}); here I am referring to \nhttps://github.com/google/flatbuffers/blob/master/include/flatbuffers/flexbuffers.h#L1135\nNot sure if I got the syntax correct, sorry.\nIf those API calls worked before they should work now because otherwise you are breaking the API for people who might used it. \nThis API broke for me in FlexBuffersSwift, maybe because I enforce the vector to be typed if user passes in array of booleans in Swift [Bool]. It might be that in the C++ implementation you will just fall back to untyped vector which than would be not ideal but good enough solution.\nAs it is defined here \nhttps://github.com/google/flatbuffers/blob/master/include/flatbuffers/flexbuffers.h#L1260\nValue holding boolean is a Value holding uint64_t meaning that TYPE_VECTOR_BOOL is equivalent to TYPE_VECTOR_UINT. \nSo I change isTypedVectorElement to make sure that a I get a typed vector if all elements in the vector are bools.\nNow here is the part which confuses me my self when I am looking at it again :) \nI did not introduce TYPE_VECTOR_BOOL I changed isTypedVectorElement to return true if self type is bool and I changed toTypedVector to return .vector_uintif self is bool. It works as all my tests are passing, but now I am a bit paranoid that I broke / am missing something.. Looking at FlexBuffersTest just add a vector of bools to the example, like following\n{ vec: [ -100, \"Fred\", 4.0, false ], bar: [ 1, 2, 3 ], bar3: [ 1, 2, 3 ], foo: 100, bool: true, bools: [true, false, true], mymap: { foo: \"Fred\" } }\nSee if it decodes from JSON and than encodes to proper JSON again.\nI guess it should be fine.. Cool. Will introduce TYPE_VECTOR_BOOL in FlexBuffersSwift, when the PR (https://github.com/google/flatbuffers/pull/4410) is through.. I think base64 is a valid representation for BLOB in JSON. It is regularly used in JSON and HTML.\nI also use it in FlexBuffersSwift for FlexBuffers to JSON converter when FlexBuffers holds BLOB. ",
    "m339606": "We are a team that would like to use this library, but we are still afraid of the incompatible problem. Therefore, we would like to know if the official would support Swift, or if it is already in your schedule. Thank you.. ",
    "jnordberg": "I would love to see official Swift support for FlatBuffers!\n@aardappel are you saying that cycles needs to be completely removed from the implementation for it to be compatible, it's not enough to have a configuration option that defaults to off like @mzaks suggests?. ",
    "Dev1an": "What is the current status on this topic?. ",
    "eduardbosch": "@aardappel @mzaks Is this swift lib secure if it's used only to read Flatbuffers files generated with the official Flatbuffers library?\nI generate my files with the official JS and I just want to read those files from an iOS application. I think that if I don't generate any file with this swift library, nothing can break as I won't introduce any incompatible thing in my Flatbuffers files.. Thanks @mzaks \nI finally used this library https://github.com/TonyStark106/SwiftFlatBuffers\nMaybe this library could be used to add support to swift if it follows all the same official FlatBuffers specifications.\nIt's working good and let me use it with Carthage.. I've moved to https://github.com/mzaks/FlatBuffersSwift only to read FlatBuffers files.\nThe lib https://github.com/TonyStark106/SwiftFlatBuffers has this bug https://github.com/TonyStark106/SwiftFlatBuffers/issues/2 that causes nearly all my FlatBuffers files unreadable. Seems that it does not handle correctly optional values.. I'm also interested in using Flatbuffers on iOS.\nAs you describe @aardappel, this is to use Flatbuffers in C++ or ObjC code but not in swift, right?\nIs there any easy way to wrap this C++ code to use it as a Swift class?\nThanks. I'm currently using this library to use Flatbuffers with swift https://github.com/TonyStark106/SwiftFlatBuffers. Thanks @aardappel,\nI just need a Swift lib to read flatbuffers files, which looks that https://github.com/TonyStark106/SwiftFlatBuffers works well.\nIf I should need to writes files, I'll definitely take a deep look to those libraries or better migrate to the flatcc.\nThanks!. ",
    "hassila": "One reflection is that @mzaks Have spent fair amount of time on optimization and benchmarking - couldn\u2019t find any such numbers for the other implementation. . ",
    "darklinden": "Thanks for reply. I'm sorry I was sticked by some other things in several days.\n@louisremi Thanks, now I 'm trying to make a code generator to do this. The only cost is every root js object may need a class name string property.\n@gwvo I think there may not so many unexpected js objects. Imagining the logic like below.\n```\nlogicMgr = { ... };\nMyWorkingObj = { ... };\nflatHelper = { ... };\nflatHelper.js2flat = function(obj) {\n    var builder = new flatbuffers.Builder(0);\nsth.startSth(builder);\n...\n\n}\nsth.onMsgFromServer = function() {\n    logicMgr.workObj = flatHelper.flat2js(buff);\n};\nsth.workingOn = function() {\n    ... // do sth   \n    funcA(logicMgr.workObj.x);\n    funcB(logicMgr.workObj.y);\n    logicMgr.workObj.n = funC( ... );\n    ...\nvar buff = flatHelper.js2flat(jsDataModuleObj);\nlogicMgr.sendBack(buff)\n\n}\n```\nI thought a lot. Everything that js object is used for their logic uses. The js object would been created even if you didn't use flatbuffers. Is me correct? And if I do so, may be RequireJs do help for memory costs?\nPS: \nMy work mates says the js data object may be too large because there 's some buider function calls inside. He is about to create a new simple data object with the data copy to pass and deal with logic. Is he correct?\nThanks.\n. @evanw Great, that's maybe the thing I expect for.\nI 'd do some research on this several days ago and I thought there may a lot of problems. \nI'm new for javascript, so I just change the cpp code to work.\nThere's a function to export the default json, and functions to convert from json to flat and back.\nI thought I could store the parser to make the parse faster.\nHere the code, it works for me, but has not been tested much.\nhttps://github.com/darklinden/flat2js\n@gwvo It's so pleasure to get the news. Thanks for your attention.\nThanks.\n. ",
    "cloutiertyler": "Evan, it looks like your Kiwi link no longer works.\n. ",
    "kevzettler": "I am in the process of deciding on a Serialization library and bummed to discover this discussion thread. Glad to see this issue is still open because I feel it is a huge pain point. I had committed to flatbuffers and started instrumenting when I was very disappointed to see:\n\nAt https://google.github.io/flatbuffers/flatbuffers_support.html\nNo JSON parsing support in Javascript 'the JSON language'?\nAt this time I am considering using @evanw 's old package at https://github.com/evanw/node-flatbuffers/ because the API is significantly less work to integrate into my project (a js webrtc game). Would this be a bad idea? \nAdditionally, after reading this thread I'm now confused and debating whether I should re-evaluate Protobufs, consider Evan's Kiwi project, or maybe roll my own serialization as it may be less overhead. \n. > this thread is about converting a JS object to FlatBuffers, akin to the \"object API\" that now exists in C++. This would make a lot of sense for JS.\nYes. Sorry to conflate JSON parsing with JS object-to-flatbuffer  \"object API \" idea.\nWhat I'm really looking for is an API like flattBuffer.fromJS(JsObject)\nIs this type of \"object API\" currently available? \nI conflated the two ideas because In my mind \"JSON parsing\" is: flattBuffer.fromJS(JSON.parse(JsObjectString));\n. ",
    "cretz": "Can the golang label be added here or is it worth creating a new issue?. ",
    "zchee": "@rw Currently, can't compile because byteSliceToString needs import unsafe package.\nCould you split that function such as https://github.com/google/flatbuffers/issues/3991?. @rw Also, why not use +build !appengine tag? If that change for only GAE, I think use appengine tag is better.. @EricLagergren Got it, agreed. Thanks.. I saw rw is Go maintainer, /cc @rw . @aardappel @rw ping, or close is best?. @aardappel Is the meaning is below?\n\n@JRonak made the original Go implementation.\nYes, Feel free to add support for language specific indentation to the printer class. You should be able to pass it the current language flags (from idl.h) and decide indent based on that.\n\nIf so, I understand. I'll fix it.\n\nthis file is shared with the GRPC project, so formatting changes here are not desirable, they will make future updates/merges much harder.\n\nIIRC, The typical generated gRPC header comment(use protoc --go_out=plugins=grpc:. *.proto) is\ngo\n// Code generated by protoc-gen-go.\n// source: ***.proto\n// DO NOT EDIT!\nDo you think change better is //If you make any local changes, they will be lost header comment to gRPC comment?\n\nThis file is auto generated, so any changes here should be because of code generation, not gofmt.\n\nI don't know what you mean. that's diff just patched flatc --go results. not use gofmt.\n@JRonak \n\nIs it really needed to add is_func, as it limits the use of signature in future updates.\n\ngot it. It was necessary to judge whether to put a space or {, but I'll consider another way.. @aardappel happy new year.\n\nSorry. my comment was not about this line, it is about the whole file. I'm saying it is better to change go_generator.cc in upstream project.\n\nI saw https://github.com/google/flatbuffers/pull/4123#issuecomment-269530765 comment, and I believe I almost understood.\nIn C++ case, above \"upstream project\" means https://github.com/grpc/grpc/blob/master/src/compiler/cpp_generator.cc, right?\nIf so, Yes, I agreed that also change the grpc/grpc source.\nBut in Go case, grpc/grpc haven't go_generator.cc. There is no \"upstream project\" for go_generator.cc file.\nI do not know the circumstances, but it is divided as grpc-go.\nIIRC actual gRPC code generator's source is https://github.com/golang/protobuf/blob/master/protoc-gen-go/grpc/grpc.go. That is only for protobuf(.proto file).\nIn other words, Current Go's gRPC generator is not managed by grpc-go(and grpc/grpc) project, provide gRPC API only. It's maintained by protobuf project.\nSo, still I don't know what you said \"this file is shared with the GRPC project\" and \"I'm saying it is better to change go_generator.cc in upstream project.\"\nOr, you means adding go_generator.cc source to grpc/grpc/src/compiler ?\n\nFinally, Sorry I'm not English native. I'm Japanese.\nI'm sorry to cause you trouble. If my interpretation is incorrect, please point it out.\nI will make additional code modifications after that.. @aardappel Thanks for the polite reply. I totally understand and glad :)\nI'll try to fix the is_func problem(consider another way?) and Streams(or etc) sections format problem.\n\nAnd no worries, my Japanese is much worse than your English ;)\n\nThanks :D\nBut I'll study English harder.. @aardappel Sorry for a delay.\nI'll rebase(for conflicted tests/MyGame/Example/MonsterStorage_grpc.go) and fix some point later.. @gwvo Okay, will do.. @aardappel Well... actually this fix is not implementation change, definite bug. (Although it's a test) \nMaybe @rw is busy(?)... I understand his situation.\nflatbuffers seem to be used with Chromium project, so I know that C++ is the top priority in flatbuffers.\nHowever, I planning implements C++ features that now lacking for Go, such as reflection.\nBut, I'm worried about it will be merged as the code review is a little slow(I saw other PR threads). Unfortunately...\nIs there any solution?\nForgive me for being so rude.. What meaning it? I'm not use gofmt for this file.\nThat's just re-generate use fixed flatc.. so, in other word, I use code generation. did not use gofmt. I'm not changes the this line.\nbut I agreed. we should following the original gRPC header comment.. ",
    "yurivict": "https://github.com/google/flatbuffers/pull/3793\n. So, it is truly a headers-only library? I need to disable libs then, because I didn't realize that.. Ok, thanks!. ",
    "akerfoot": "As I mentioned in https://github.com/google/flatbuffers/issues/3811, this has only been tested on Python 2.7.3. I also tested the isinstance cases that CreateString would use on 2.7.3, 2.7.10, and 3.4.3 to make sure they behaved as expected.\n```\nPY3:\nstring_types = (str,)\nbinary_types = (bytes,bytearray)\n                            unicode     str  bytearray    bytes\nisinstance(t ,string_types)    True    True      False    False\nisinstance(t ,binary_types)   False   False       True     True\nPY2:\nstring_types = (unicode,)\nbinary_types = (str,bytearray)\n                            unicode     str  bytearray    bytes\nisinstance(t ,string_types)    True   False      False    False\nisinstance(t ,binary_types)   False    True       True     True\nnotes:\n- bytearray only defined in PY26 onwards\n- bytes in PY26 and PY27 is only an alias for str\n```\nI didn't have a pre-2.6 interpreter handy, and my knowledge of how python handled encoded vs decoded strings in prior versions is a bit spotty, so I don't know if there are cases where a str type could be unencoded, or if it would always be encoded.\n. I've submitted https://github.com/google/flatbuffers/pull/3806, which has worked for us, but it has only been tested on Python 2.7.3. I did test the isinstance cases that CreateString would use on 2.7.3, 2.7.10, and 3.4.3 to make sure they behaved as expected. \nI didn't have a pre-2.6 interpreter handy, and my knowledge of how python handled encoded vs decoded strings in prior versions is a bit spotty, so I don't know if there are cases where a str type could be unencoded, or if it would always be encoded.\n. ",
    "253056965": "\u8fd9\u4e2aapi \u7684\u8bbe\u8ba1\u6211\u786e\u5b9e\u4e0d\u559c\u6b22\u3002\u53ea\u8981\u4e0d\u662f\u57fa\u672c\u7684\u6570\u636e\u7c7b\u578b \u90fd\u9700\u8981\u5148\u521b\u5efa\u51fa offset  \uff0cprotobuf  \u7684api \u53ef\u4ee5\u4f5c\u4e3a\u53c2\u8003\u3002\u6211\u611f\u89c9\u90a3\u6837\u7684\u8bbe\u8ba1\u662f\u6bd4\u8f83\u597d\u7684\u3002\n. \u4f60\u8bf4\u7684\u786e\u5b9e\u662f\u8fd9\u6837\u3002\u8c22\u8c22\u7684\u7b54\u590d\n. \u5f53 flatbuffers.js \u5f15\u5165\u5230typescript   \u9879\u76ee\u7684\u65f6\u5019 \u4ed6\u4e0d\u662f\u6807\u51c6\u7684typescript \u8bed\u6cd5\uff0c\u5bfc\u81f4\u667a\u80fd\u63d0\u793a\u90fd\u6ca1\u6709\uff0c\u8fd8\u6709\u5c31\u662f \u9700\u8981\u914d\u7f6e \u8ba9typescript  \u652f\u6301 \u548cjs \u7684\u6df7\u7f16. \u6211\u7684\u610f\u601d\u662f\u5b98\u65b9\u662f\u5426\u53ef\u4ee5\u63d0\u4f9b flatbuffers.js \u4ed6\u7684ts \u7248\u672c\u3002\u6216\u8005\u5b98\u65b9\u662f\u5426\u6709\u8ba1\u5212 \u5728\u540e\u7eed\u7248\u672c\u4e2d\u52a0\u5165\u8fd9\u4e2a. ",
    "huyi": "\u697c\u4e3b\u597d\u725b\u903c,\u8fd9\u4e48\u6de1\u5b9a\u7684\u4f7f\u7528\u4e2d\u6587.\n. ",
    "LeesinYasuo": "\u6211\u51b3\u5b9a\u4e5f\u50cf\u697c\u4e3b\u4e00\u6837\u7528\u4e2d\u6587\u53d1\u95ee\u4e86\n. \n\nSession is the field of ReportItem\n\n. Thanks a lot ,i got that  why the return value was 0,it was my code error .\nbecause it is a binary file,so i think the extension won't  influence decoding file.is it right?\nso i use the extension .text casually.\nby the way ,will flatbuffers  surport increasing the ByteBuffer's size dynamiclly ,in the future?once i create\nthe object ,i can't add new field,i hava to create new object,it is Inconvenient.\n. can u give me a example for calling to startSession / startPageInfo / startVector or similar that hasn't yet been finished with the proper \"end\" call.\ni can't understand.depth first,Session.createSession method include  startSession and the proper \"end\" call,why there is a call to startSession / startPageInfo / startVector or similar that hasn't yet been finished with the proper \"end\" call.\n. Here is the code,when the data is become big,it will throw that exceptions,normally ,it is ok.I got this exception by monkey test.\nfor (PageItem pageItem : mPageItems){\n            LogUtils.D(\"pageName\uff1a\"+pageItem.getPageName());\n            if (pageItem.getEndTime() == -8099484176161439744L){\n                if (mIsExit){\n                    pageItem.setEndTIme(mExitTime);\n                    LogUtils.I(pageItem.getPageName()+\":app aborts,ready to save data\");\n                } else {\n                LogUtils.E(pageItem.getPageName()+\":didn't call onPageEnd\");\n                }\n            }\n            int pageNameOffset = mBuilder.createString(pageItem.getPageName());\n            int pageInfoOffset = PageInfo.createPageInfo(mBuilder, pageNameOffset, pageItem.getStartTime(),\n                    pageItem.getEndTime());\n            mCustomPageInfoList.add(pageInfoOffset);\n        }\n        mPageItems.clear();\n        if (mIsNewSession) {\n            mSessionIDoffset = mBuilder.createString(UUID.randomUUID().toString());\n            mPageInfoList.clear();\n                mPageInfoList.addAll(mCustomPageInfoList);\n                mCustomPageInfoList.clear(); \n            int pageInfosOffset = Session.createPageInfosVector(mBuilder, list2intArray(mPageInfoList));\n            int sessionOffset = Session.createSession(mBuilder,mSessionIDoffset,mStartTime, mExitTime, pageInfosOffset);\n. reusing a FlatBufferBuildr from a previous buffer? i can't understand,can u give me a  example?\nwhere i create mBuilder are as follows\uff1a\nprivate static FlatBufferBuilder mBuilder = new FlatBufferBuilder(0);\nanother piece of code\n  if (mBuilder.dataBuffer().capacity()>=DATA_MAX){\n            mIsDataMax = true;\n            mBuilder = new FlatBufferBuilder(0);\n        }\n. Thanks your help,the issue can be closed\n. I have no ways ,normally everything is ok ,but when the data saved by flatbuffers is big,it will throw this exception.After crash,the next time i start my program,it can work well.I hope u can give me some advices.the compelte codes are as follws:\nconstruct Session:\nmSessionIDoffset = mBuilder.createString(UUID.randomUUID().toString());\nint pageInfosOffset = Session.createPageInfosVector(mBuilder, list2intArray(mPageInfoList));\nint sessionOffset = Session.createSession(mBuilder,mSessionIDoffset,mStartTime, mExitTime, pageInfosOffset);\n mSessions.add(sessionOffset);\nconstruct ReportItem code include sessions vector:\nprivate void buildReportItem(){\n        int sessionsOffset = ReportItem.createSessionsVector(mBuilder, list2intArray(mSessions));\n        int eventsOffset = ReportItem.createEventsVector(mBuilder, list2intArray(mEvents));\n        int exceptionOffset = ReportItem.createExceptionsVector(mBuilder, list2intArray(mExceptions));\nReportItem.startReportItem(mBuilder);\n    ReportItem.addSessions(mBuilder, sessionsOffset);\n    ReportItem.addEvents(mBuilder, eventsOffset);\n    ReportItem.addExceptions(mBuilder, exceptionOffset);\n    ReportItem.addDate(mBuilder,System.currentTimeMillis());\n    int reportItemOffset = ReportItem.endReportItem(mBuilder);\n    mBuilder.finish(reportItemOffset);\n}\nSaveReportItem code:\npublic void saveReportItem2File(FlatBufferBuilder builder, Context context){\n        File filesDir = context.getFilesDir();\n        File logFileDir = new File(filesDir, LOG_DIR);\n        if (!(logFileDir.exists())) logFileDir.mkdirs();\n        File logFile = new File(logFileDir, DateUtils.getCurrentDay() + \"_\" + STATISTICS_LOG_NAME);\n        if (!logFile.exists()){\n            try {\n                logFile.createNewFile();\n            } catch (IOException e) {\n                e.printStackTrace();\n            }\n        }\n        try {\n            BufferedOutputStream os = new BufferedOutputStream(new DataOutputStream(new          FileOutputStream(logFile)));\n            os.write(builder.dataBuffer().array(), builder.dataBuffer().position(), builder.offset());\n            os.close();\n        } catch(IOException e) {\n            LogUtils.E(\"FlatBuffers test: couldn't write file\");\n        }\nGetReportItem code:\npublic ReportItem getReportItemFromFile(Context context){\n        File filesDir = context.getFilesDir();\n        File logFileDir = new File(filesDir, LOG_DIR);\n        File logFile = new File(logFileDir, DateUtils.getCurrentDay() + \"_\" + STATISTICS_LOG_NAME);\n        if (!logFile.exists()){\n            LogUtils.I(\"file not exists\");\n            return null;\n        }\n        byte[] data = null;\n        RandomAccessFile f = null;\n        try {\n            f = new RandomAccessFile(logFile, \"r\");\n            data = new byte[(int)f.length()];\n            f.readFully(data);\n            f.close();\n            if (data.length == 0){\n                LogUtils.E(\"length is zero\");\n                return null;\n            }\n            ByteBuffer byteBuffer = ByteBuffer.wrap(data);\n            ReportItem reportItem = ReportItem.getRootAsReportItem(byteBuffer);\n            LogUtils.D(\"flat decode test\uff1a\"+reportItem.sessions(0).startTime());\n            return reportItem;\n        } catch(Exception e) {\n            LogUtils.E(\"FlatBuffers test: couldn't read file\");\n            e.printStackTrace();\n        }\n       return null;\n    }\n. the first one,and the crash is accidental,some time it won't throw this exception by monkey test\n. Precisely speaking,the crash is happening when read the field.\nDeserialize is ok.\nsession.put(\"id\",reportItem.sessions(i).id()); //throw exception\n. is there any possible when writing  and reading the same File is not  Thread Safe can cause this exception?\nor FlatBuilder's method such as createString  is not Thread Safe?\n. so one thread read FlatBuffer another one write FlatBuffer may cause some unexpected exception,right?\n. it is the schemas and version control promblem.i guess i'd  better try-catch every where i use flatbuffer.\ni can't control it perfectly ~~. thanks,it works,i should read it more carefully.. ",
    "yuangu": "\u6211\u5df2\u7ecf\u5728github\u5f00\u59cb\u7528\u4e2d\u6587\u53d1\u95ee\u4e86\u3002\u633a\u65b9\u4fbf\u7684\u3002\n. ",
    "avfounder": "\u597d\u50cfgithub\u53ef\u4ee5\u4f7f\u7528\u8c37\u6b4c\u7ffb\u8bd1\uff0c\u7528\u4e2d\u6587\u95ee\u9898\u4e0d\u5927\u3002. ",
    "cmidt-veasna": "Sorry for taking too long. I am off from work this few days.\nFirst I want to explain about the parser. The parser is the generated Code base on the schema that we define. First we read the schema and write the code of parser (You can see the code here https://gist.github.com/cmidt-veasna/e9a93d003b842aec79a6). The parser is using the generated code to loop through the JSON object and build the FlatbBuffer base on Schema attribute. This is how our parser work.\nSecond \"wrong result\", \nWe saw the different byte array by compare the output from C++ and from \"FlatBufferBuilder.java\". From the comparison we found that:\n1. ByteBuffer return from \"FlatBufferBuilder.java\" has a space padding from the head and tail.\n2. Even without spacing from head, each byte in the array is not identical. Note that we convert C++ byte array to java byte array via JNI before compare this two byte arrays.\nAfter that, I decide to deep dive into Java FlatBuffer Library, I found a method call \"sizedByteArray()\" from \"FlatBufferBuilder.java\" which omit padding from head. In this case, we can save this byte array into file and reuse it later. However we still see the different at the tail of output array between C++ and Java.\nThe byte array of FlatBuffer are mean to be the same ? as it is compatible for any platform or language.\nOne thing I have notice that, the space at the head and tail of the array is grown when the large JSON data.\n. Sorry for taking too long. I am off from work this few days.\nFirst I want to explain about the parser. The parser is the generated Code base on the schema that we define. First we read the schema and write the code of parser (You can see the code here https://gist.github.com/cmidt-veasna/e9a93d003b842aec79a6). The parser is using the generated code to loop through the JSON object and build the FlatbBuffer base on Schema attribute. This is how our parser work.\nSecond \"wrong result\", \nWe saw the different byte array by compare the output from C++ and from \"FlatBufferBuilder.java\". From the comparison we found that:\n1. ByteBuffer return from \"FlatBufferBuilder.java\" has a space padding from the head and tail.\n2. Even without spacing from head, each byte in the array is not identical. Note that we convert C++ byte array to java byte array via JNI before compare this two byte arrays.\nAfter that, I decide to deep dive into Java FlatBuffer Library, I found a method call \"sizedByteArray()\" from \"FlatBufferBuilder.java\" which omit padding from head. In this case, we can save this byte array into file and reuse it later. However we still see the different at the tail of output array between C++ and Java.\nThe byte array of FlatBuffer are mean to be the same ? as it is compatible for any platform or language.\nOne thing I have notice that, the space at the head and tail of the array is grown when the large JSON data.\n. Note that the parser code is not intent to support all feature of FlatBuffer. All those code is build match our needed.\nAlso The parser code is intent to run on Android. However we might be able to use it on any platform by switch JSON Library for Java.\n. We try to improve performance on Android Apps and the backend are ready so we decide to parse it on Android so that the second time, it will be faster as we don't need to parse the data again. Unfortunately, C++ embedded in Android cause crash on some device due missing library. So we decide to implement it on Java so that we are sure the crash will not happening and We understand that we implement only what we need and all flatbuffer will be support.\nNow we decide to push the change to the next CR so that backend will support us.\nAs for your suggestion the converting back to JSON seem not working. I must missing something.\nI try to add file identifier and file extension but did not success. Both binary from Java and binary from C++ is failed to convert to JSON. Below is the error I got from flatc command:\n\nAssertion failed: (finished), function Finished, file /tmp/flatbuffers20151029-31300-17v3k5o/flatbuffers-1.2.0/include/flatbuffers/flatbuffers.h, line 573.\n\n@gwvo Did I do something wrong ? Do we have any document about how Flatbuffer are store as an byte array ? I would like to help building Java Parser and make it exactly the same C++ output.\n. @gwvo Here is my command line for generate binary and json\n1. Command to generate binary flatc -b page.fbs page.json\n2. Command to generate Json  flatc -t page.fbs -- page.bin\nYou can see my FlatBuffer schema and the json file with link below\nhttps://gist.github.com/cmidt-veasna/170df3181972400695d51cac32b5bedf\nLet me know if something wrong with the command above and my FlatBuffer schema.\nIn this case, I only verify with C++ parser via command line. As for java code that I generate it seem like I miss the file extension and file identifier. First I try with option --raw-binary but after I look into the C++ parser code, it seem that it required file identifier to be present in the binary buffer.\nAs for your suggestion, I'm not really sure. I haven't work on C++ much. I know Java more than C and C++. My understanding is that, option a and b is required to debug through C++ code during process of parsing and it mind take me sometime as I don't really work on C++. Please correct me if I am wrong.\nFor option c, I do use manually Java code to construct a buffer. The generated code is only generate a Java code where it call to the Java code which is generated by flatc -j.\nThanks\n. @gwvo Here is my command line for generate binary and json\n1. Command to generate binary flatc -b page.fbs page.json\n2. Command to generate Json  flatc -t page.fbs -- page.bin\nYou can see my FlatBuffer schema and the json file with link below\nhttps://gist.github.com/cmidt-veasna/170df3181972400695d51cac32b5bedf\nLet me know if something wrong with the command above and my FlatBuffer schema.\nIn this case, I only verify with C++ parser via command line. As for java code that I generate it seem like I miss the file extension and file identifier. First I try with option --raw-binary but after I look into the C++ parser code, it seem that it required file identifier to be present in the binary buffer.\nAs for your suggestion, I'm not really sure. I haven't work on C++ much. I know Java more than C and C++. My understanding is that, option a and b is required to debug through C++ code during process of parsing and it mind take me sometime as I don't really work on C++. Please correct me if I am wrong.\nFor option c, I do use manually Java code to construct a buffer. The generated code is only generate a Java code where it call to the Java code which is generated by flatc -j.\nThanks\n. @gwvo Yes, you are right. I am running FlatBuffer 1.2.\nWith the latest code from Github, the convertion from binary to Json is working fine and flatc can convert both binary from C++ and Java output.\nI enable the code Java parser on Android. Now with FlatBufferBuilder.sizedByteArray(), the byte array's length is identical with C++ parser. I could convert this binary array back into Json and it produce the same result.\nI think the latest FlatBuffer is fixing the this issue.\n@gwvo Thanks for your support\n. ",
    "villintehaspam": "Thanks! It seems the function is called GetRoot and not GetRootAs though, but something tells me you already know.\n. ",
    "jvidziunas": "I have run into a similar issue in my own project. It would be nice if the generated parser file just replicated the path into the produced C++, but the path seems to be explicitly stripped in the generator.\n. I'm content with the path behavior simply reflecting what's in the directive, with no (direct) bearing on the namespace. As an example, I have an include directive\ninclude \"../PipelineTypes.fbc\";\ninside a schema. The current behavior will add\n#include \"PipelineTypes_generated.h\"\nto the C++ file. My desired behavior would be adding\n#include \"../PipelineTypes_generated.h\"\ninstead. If the user, for example, has a flat project directory structure then there would be no need to prepend anything to the path in the schema to begin with and thus stuff Just Works.\nAlternately, adopt C/C++-style angle brackets and quotes?\n. ",
    "jenvannier": "A command flag for this would be great. At the moment I'm going in with a script and editing the file after it's been produced. This is not optimal....\n. ",
    "lukepg": "Ok, thank you.\n. ",
    "cpayne-tesla": "Never mind, I found the following can accomplish what I'd like to do in Python. In the example, I assume I have a struct in my schema called Value (with data members x and y) and a vector of these called Values somewhere in a table\nfb.StartValuesVector(buffer, len(vec_of_values))\n            for value in vec_of_values:\n               fb.Value.CreateValue(buffer, value.x, value.y)\n            fb_vec = tile_buf.EndVector(len(vec_of_values))\n            fb.AddValues(buffer, fb_vec)\n. ",
    "quabqi": "update\n. update\n. ",
    "Nnamdi": "Thanks for getting back to me.\nI think I could do what I had planed quite neatly with just the GetId (or GetFullyQualifiedName?) function - would you be willing to accept a patch allowing the generation of those functions on tables/structs, given a flatc switch?\nThanks.\n. Might it be a good idea to use const char * rather than std::string, so we don't have to pay the cost of constructing a std::string if we aren't using one?\n. Alternatively, you could take it one step further, and use a type similar to this:\n``` cpp\nstruct StringRef\n{\n    template \n    static auto GetSize(const StringType& str) -> decltype(str.size()) { return str.size(); }\ntemplate <std::size_t N>\nstatic std::size_t GetSize(const char (&)[N]) { return N; }\n\nstatic std::size_t GetSize(const char *str) { return std::strlen(str); }\n\ntemplate <typename StringType>\nstatic const char *GetData(const StringType& str) { return str.data(); }\n\ntemplate <std::size_t N>\nstatic const char *GetData(const char (&str)[N]) { return str; }\n\nstatic const char *GetData(const char *str) { return str; }\n\ntemplate <std::size_t N>\nStringRef(const char (&str)[N]) : str_(str), len_(N - 1) { }\n\ntemplate <typename StringType>\nStringRef(const StringType& str) : str_(GetData(str)), len_(GetSize(str)) { }\n\nconst char *str_;\nstd::size_t len_;\n\n};\n```\nThen the length can be passed to the CreateString call, but strlen doesn't need to be called if the input is a string literal, or any std::string-like type (i.e. has .data() and .size()).\n``` cpp\nflatbufffers::Offset CreateFoo(flatbuffers::FlatBufferBuilder& fbb, flatbuffers::StringRef value)\n{\n    auto valueOffset = fbb.CreateString(value.str_, value.len_);\n// ... construct Foo as before ...\n\n}\n```\n. ",
    "notsimon": "Would it be useful to have something similar when a table is included in a union ? I could make a PR.\nFor instance, multiplayer.fbs from pienoon contains union Data { PlayerAssignment, PlayerCommand, StartTurn, EndGame, PlayerStatus }, which would generate the following static methods (assuming scoped enums):\nc++\nData PlayerAssignment::GetDataUnionId() { return Data::PlayerAssignment; }\nData PlayerCommand::GetDataUnionId() { return Data::PlayerCommand; }\nData StartTurn::GetDataUnionId() { return Data::StartTurn; }\n...\nThis would avoid a switch when reading unknown data.\n. ",
    "tkelman": "CLA signed, I think (the CLA site logged me in with a gmail account that I don't really use)\n. ",
    "jaszczw": "\nI wonder why the original code was written this way...\n\nAs for cause why the code was written this way I think it was made like this because of thinking about process flow, maybe before writing it down those two conditions similarity wasn't so obvious.\n\n...maybe to protect against integer wrap-around?\n\nI think integer wrap-around cannot happen in this case as:\nOffset cannot be lower than 0 -\n _buffer.Length- > returns positive integer\nlength -> passed arguments are always some variations of sizeof(TYPE) - but doesnt matter in fact, as length would have to be greater than int32.MaxValue (impossible) as a substractor to cause wrap-around.\n\nDoesn't _buffer already throw some kind of out or range exception? If so, what's the value of this check at all?\n\nFrom what I have been able to tell the unsafe part of code wouldn't throw hence could access some data not meant for it etc.\n. ",
    "lyxm": "Thanks.   C++ standard seems to say \"0f\", for example, is not a valid float literal.   So it would be for all compilers.\n. I want to sort by scores.  It's natural to have them from high to low.\nOn Mon, Apr 25, 2016 at 1:24 PM, Wouter van Oortmerssen \nnotifications@github.com wrote:\n\nNot sure I see why this is needed.\nYes, we don't enforce unique keys, this is something left to the caller.\nWhat application do you have for sorting in decreasing order?\nWhat is the problem with the current use of a table comparator?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/3858#issuecomment-214509454\n. That would work, since you said key doesn't imply uniqueness anyways.\n\nOn Mon, Apr 25, 2016 at 2:04 PM, Wouter van Oortmerssen \nnotifications@github.com wrote:\n\nSo.. maybe we can make that an optional value to the key attribute? not\nspecified / 0 means increasing, 1 means decreasing?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/3858#issuecomment-214521274\n. I don't see a quick fix for it.   It's related to IndirectHelper and going\nthrough hoops.\n\nOn Mon, Apr 25, 2016 at 1:20 PM, Wouter van Oortmerssen \nnotifications@github.com wrote:\n\nThat is indeed an oversight. There need to be non-const versions of\nVector::Get(), Vector::operator[] and possibly VectorIterator. Are you\nable to make a PR for this?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/flatbuffers/issues/3862#issuecomment-214508093\n. Got it.  Thanks for the clarification.   Error on non parsable (or not fully parsable) string might be better to catch some client side errors early on.\n. \n",
    "lighter-cd": "sure and thanks.I'll make a PR Within a day or two.@gwvo \n. ",
    "dushibaiyu": "Thanks\uff01I update the code\u3002please comments to\u3002\n. @WebFreak001  Tank you very much.\nin the Monday, I will write the test for D buffer construction.\nIt is sorry , my english is bad. I used the goolgle translate, but I can not quite understand Some translators .\n. update the document\n. flatbuffer is used he endianness of the current machine? in x86_64 we test is little endian. \n. fixd it\n. The style not be only, It should be controled by user.\n. ",
    "wilzbach": "\nWhat's the status of this PR? Still see no documentation, for example.\n\n@zoujiaqing have you already tried to use ddoc or ddox with the FlatBuffer theme?\n. > @zoujiaqing have you already tried to use ddoc or ddox with the FlatBuffer theme?\nping @zoujiaqing ;-)\n. unused imports?\n. 2016?\n. ",
    "diegovisentin": "My request is about to get the single bit of a byte. \nE.g. with the enum:\nenum DigitalInputs: byte (bit_flags) {\n    DI_CLK1,DI_CLK2,DI_S1,DI_S2\n}\nthe generated code could include something like this:\npublic boolean digitalInputs_DI_CLK1() { return ((bb.get(bb_pos + 0) & 1) == 1) ? true : false; }\npublic boolean digitalInputs_DI_CLK2() { return ((bb.get(bb_pos + 0) & 2) == 2) ? true : false; }\npublic boolean digitalInputs_DI_S1() { return ((bb.get(bb_pos + 0) & 4) == 4) ? true : false; }\npublic boolean digitalInputs_DI_S2() { return ((bb.get(bb_pos + 0) & 8) == 8) ? true : false; }\n. ",
    "BlueHotDog": "Update\ni've tried to do this via:\n```\ntable a {\n  item: ubyte;;\n}\ntable b {\n  item2:string;\n}\n```\nbut it seems like the generated file(JS) treats a.item as an array of ubytes, and not as a rootelement\n. Seems like the relevant functions are missing from the JS generator though :(\n. Yeah, i managed to add it, the problem is that there is no helper method to get a chunk of the buffer. Just read it one item at a time\n. yeah, or, i see that the Java generated file contains helper method to actually get the object, ready.\n. ",
    "bhamiltoncx": "I signed it!\n(Facebook signed the Google corporate CLA.)\n. I just found out Google has a new Corporate CLA. The open-source folks at Facebook are working on signing it now. I'll update as soon as that's done.\n. Looks like the new CLA is now in place. I should now be listed as a CLA-approved contributor.\n. Hi Google folks! Any comments on this PR?\n. You're welcome! I updated the tests as you recommended.\n. Does anyone have any other comments?\n. Thanks! Yeah, I can see how this grew over time.\nBy the way, I didn't check any of the generators other than idl_gen_text.cpp; hopefully they all handle emitting literals for values > U+FFFF correctly.\nMy guess is at least one generator needs to be fixed. :) Many languages use a different syntax (\\uXXXX vs. \\U0010XXXX) for values > U+FFFF.\n. By the way, I wonder if we should just change the signature of GenerateText() to take a bool * parameter to intentionally break API compatibility here. There are probably other clients which need to know this operation can fail.\n. > Indeed, can you add a check for this bool to the call coming from flatc.cpp ? flatc can be used to turn any binary into JSON, so it be good to report an error here.\nI made flatbuffers::GenerateTextFile() fail in this case. Thankfully, flatc checks if GenerateTextFile() fails and reports a (fairly generic) error:\nhttps://github.com/google/flatbuffers/blob/master/src/flatc.cpp#L339\nI'll leave it up to you if you want to break the API or not.. :)\n. Makes sense. I was working completely in the dark there. Will fix.\n. ",
    "martinus": "Ok, I've done the renaming!\n. ",
    "battre": "Thanks!\n. ",
    "steffenGit": "Nevermind... Just found the --gen-all compiler flag, that fixes this problem.\n. ",
    "ildarsharafutdinov": "thanks \n. ",
    "Dexteroo7": "I found some similar discussions on other issues and google group.\nI think a developer can make the required changes himself according to his needs, which depend on the use-case entirely.\n. ",
    "bog-dan-ro": "The problem is that some (most ?) of the GPL v2 project owners don't agree, and personally I'll not try to understand their reasons or FSF's :).\nThe point is that it will be unfortunate for some projects to choose another data serialization instead of flatbuffers just because the license doesn't allow them.\nIf the project mission is the give to the people the best way to encode/decode binary data and not to fight some religious licence war, than, IMHO will not hurt anyone if this project will use some BSD/MIT/BOOST license for headers :), just as the other competitor projects do  e.g. Protocol Buffers, Cap'n proto, etc. \nThe only thing that will happen is that more and more project will start to use it ;-)\n. It's an old BSD-3-Clause license (https://opensource.org/licenses/BSD-3-Clause), BSD-2-Clause is the most common/used one (https://opensource.org/licenses/BSD-2-Clause). For more information about BSD licenses please check https://en.wikipedia.org/wiki/BSD_licenses\n. Thanks a lot!\nThis change is not only for me :), it should be for everyone. Personally I'd like to use it to create a new old games (ZX Spectrum) collection format, which I'd like to add it to some existing FOSS projects, some of them are GPL2 (e.g. fuse-emulator.sf.net).\n. I signed it!\n. I know, that's why it is defined only when the user uses --scoped-enums :)\n. You're welcome ! ;-)\n. I thought the Verifier knows (or at least can \"guess\") the size of the message...\nI can try to implement and make a PR (PR stands for Pull Request, right), but probably it will take much more than for you :), I'm still not very familiar with flatbuffers internals.\n. Well, in principle people will (should) use the verifier in such cases, because first and foremost you need to know if there is garbage in the stream which will crash your application, at the end of the verification if you'll get also the size of the stream, that is the bonus ;-).\nI'll try to implement it myself, any more tips about how to do it will be highly appreciated ;-) !\n. Can you please check if I got it right https://github.com/google/flatbuffers/pull/3905 ? :)\nIt seems way too easy :)\n. @promethe42  if Verifier::GetComputedSize returns 0, it means you need to read more data into the buffer.\nThis is one way to use it:\n```\nstd::string buff;\nwhile(canReadMoreData) {\n buff.append(moreDataFromSockOrFromDisk);\n flatbuffers::Verifier verifier(buff.c_str(), buff.size());\n if (!VerifyMonsterBuffer(verifier)) // replace VerifyMonsterBuffer with your VerifyXXXXXBuffer function\n   continue;\n // Now is safe to create and use the monster from buffer \n auto monster = GetMonster(buff.c_str());\n // do something with monster\n// Drop the current message and continue reading the next one\n auto msgSize = verifier.GetComputedSize();\n assert(msgSize); // because VerifyMonsterBuffer succeed, msgSize is always > 0\n buff = buff.substr(msgSize);\n}\n``\n. @promethe42 , nope the important part is to call it afterVerifyMonsterBuffer` which is the function that triggers the computation of the size.\n. Well, first and foremost adding this support doesn't mean you need to drop the current one.\nCurrent one is still extremely useful for read-only operations (which can be done without any allocations) ! \nBut when it comes to change the content and to serialize it back, it's a different story. Most of the time the multate facility is not enough and there are cases when the reflection is slower than creating a full tree of objects and allowing the user to change it easily (by using std::vector, std::string, etc.) and at the end, serialize it once. Let me give you an example: supposing that the user need to change 4 things in the message that each of them will change the size of the message. Using reflections, for every change it will re-serialize it. This means you'll have 4 serializations instead of just one :).\nAnyway, based on your suggestions I'll try to come with a proposal (maybe even with an PR).\nP.S. Is there any ML for flatbuffers where we can discussed this publicly with other people? Maybe they will have other (better) ideas ;-).\n. That's great! Actually this week I wanted to start working on it myself :). I had some problems to find a solution for \"unions\", but because std::any will be available in C++17, I think the best solution is to use a vector of std::unique_ptr/shared_prt instead...\nI'm very interested in this topic, is it possible to push your wip somewhere (i.e. create a pr or wip branch)?\nThanks!\n. Thanks a lot! I have a few small observations though:\n- instead to generate this kind of destructors for unions:\ninline EquipmentUnion::~EquipmentUnion() {\n  switch (type) {\n    case Equipment_Weapon: delete reinterpret_cast<WeaponT *>(table); break;\n    default:;\n  }\n}\nisn't easier to add a default virtual destructor to \"struct NativeTable\" \nstruct NativeTable {\n  vrtual ~NativeTable() {}\n}\nand then the union destructor will be much simplified:\ninline EquipmentUnion::~EquipmentUnion() { delete table; }\n- I can't find the implementation for unions copy constructor & operator=  (EquipmentUnion(const EquipmentUnion &); and  EquipmentUnion &operator=(const EquipmentUnion &)).\nI need to wait to see unions copy constructor & operator=, but my feeling is that without using smart pointers it won't be easy to copy them like:\nEquipmentUnion a;\nEquipmentUnion b = a;\nor like\nstd::vecttor<EquipmentUnion> va{....};\nstd:vector<EquipmentUnion> vb = va;\n. Why are you using std::unique_ptr for Tables and not just TableT ? :)\n. One last thing for today :)\nI can't find Pack method for tables, in CppUsage.md you have monsterobj->Pack(fbb); but flatc doesn't generate it anymore. I also checked the code and it's not there ...\n. I created a pull request here https://github.com/google/flatbuffers/pull/3961 which allows us to have deep-copy of the structs ;-) without any smart pointers which it should be a little bit faster. \nThe non-smart pointers version is also needed by upcoming Qt & QML --gen-object-api support, which I hope to be able to implement it in the near future.\n. I'm going to address your comments after I'll comeback from my vacation ( ~14 from now). \n. As requested I added the FLATBUFFERS_TRACK_VERIFIER_BUFFER_SIZE define.\nI also tested how much it slowdown and I have some numbers:\nI checked 10 000 000 times a message:\n- without FLATBUFFERS_TRACK_VERIFIER_BUFFER_SIZE it took 3171ms\n- with FLATBUFFERS_TRACK_VERIFIER_BUFFER_SIZE it took 3478ms\nthe difference is ~300ms for 10M iterations, which IMHO is a fair trade as long as VerifyXXXXX checks also the real size. Having said that let me know if you still want to keep FLATBUFFERS_TRACK_VERIFIER_BUFFER_SIZE define.\n. Why std::vector on Ci doesn't have emplace_back ?\n. I think we'll need a new flag which can used to know what kind of C++ standard/framework to generate code for:\n- c++0x - will use a subset of C++11, but not all goodies e.g. emplace_back, {}, etc.\n- c++11 (default?) - will use full C++11 standard, including usage of {} for constuctors, emplace_back, etc.\n- c++17 ? - I think we can use std::any for unions ...\n- qt5 - will generate code that can be used by Qt/QML apps (e.g. will add Q_OBJECT/Q_GADGET, Q_PROPERTIES, Q_ENUM, Q_DECLARE_METATYPE, etc. which is needed by Qt Meta stuff).\nLet me know if my approach is ok for you.\n. Well, at least for qt5 we'll need some kind of flag to generate code for it.\nAFAIK compilers can't generate code that is closed to emplace_back for push_back ... last time I checked push_back was 2x times slower than emplace_back, because it does an extra copy of the pushed element.\nRegarding the testing, the CI already uses new compilers that seems to have full C++11, so it should not be a problem for testing.\nLocally I have the vector.reserve fix ;-) I'm waiting for your feedback to know if or how to continue ...\n. @gwvo  I added only C++x0 and C++11 variants\n. What's wrong with googlebot ?\n. Ah, it seems the crappy compilers can't do range for ;-(\n. Ah, it seems the crappy compilers can't do range for ;-(\n. Folks I have no idea how to fix the last window error ;-( any help will be very appreciated.\nIn the last patch cmake will test if the gcc is new enough to handle -std=c++11 and if yes it will enable by default c++11 support in flatbuffers, otherwise it will use c++0x\n. @gwvo Ah I forgot optional fields :( In this case, next week, I'm going to write a std::optional like class which does the deep-copy, and I'll use it instead of unique_ptr (as @Bklyn pointed unique_ptr is not suitable for this job). deep-copy is really needed when you want to copy (parts of) a struct to another (i.e. monser2.testarrayoftables = monser1.testarrayoftables without deep-copy it will never work properly). If the users doesn't want to deep-copy they can use pointers or references.\nRegarding vectors, I think std::vector<MonsterT> is the right way, because an item of the vector can't be optional, if you don't want an item in the vector then you just remove it :) .\n. Table & structure fields are flatbuffers::optional now, I hope I got it right this time :)\n. I'm pretty happy with current patch, the only thing is missing is the initialization of the table fields with their default values in its default constructor. I hope to have time tonight to implement it and update this patch, or I can do it in another patch.\n. I added the scalar fields init too.\n. Renamed flatbuffers::optional[Table] to flatbuffers::Optional[Table]\n. Optional class uses vanilla C++ :). Without it it will be vary hard to have deep copy, which so needed mostly when you create a message from scratch or when you want to merge parts from one message to another.\nI'll address the remaining issues tomorrow (though the virtual destructor makes the binary smaller and is at least as fast as the switch/case destructor).\n. @Bklyn it's less confusing and more beautiful ;-) \nI implemented it because in the future compiles might warn about using the old way and I thought it doesn't hurt to use the new way, anyway if you all think it doesn't worth, I'm ok to remove it.\n. @gwvo c'mon range for is so cool and it makes the code nicer and smaller. It's one of the most appreciated features of C++11!\n. removed all but emplace_back, C++11 goodies\n. The access to the Optional and unique_ptr things is the same! Actually Optional and unique_ptr APIs are very similar, the only difference is that Optional is designed for flatbuffers needs (same as you already added all those classes for read-only access) and unique_ptr is designed only to retain the ownership of an object via a pointer and destroys that object when it goes out of scope.\nIMHO you underestimate the need for deep-copy. I think deep-copy is the most important part of the generated objects, without it, it will be very hard to create & merge objects with lots of optional tables/structs.\nThe Clone method will make the things uglier & much harder... check the following use cases (using the monster_test.fbs):\n```\nMonsterT monster1{monsterPtr};\nMonsterT monster2;\n// Now I want to copy enemy fields from monster1 to monster 2\n// Using Optional you just do:\nmonster2.enemy = monster1.enemy;\n/// Using Clone & unique method, you'll do:\nmonster2.enemy.reset();\nif (monster1.enemy)\n  monster2.enemy.reset(monster1.enemy->Clone());\n//Another more complex example is to copy a vector of optionals, let's copy test4:\n// Optional way:\nmonster2.test4 = moster1.test4;\n/// Clone & unique method, way:\nmonster2.test4.clear();\nfor (const auto &val : moster1.test4)\n  monster2.test4.push_back(std::move(std::unique_ptr(val->Clone()));\n// Of course if the user will dare to do:\nmonster2.test4 = moster1.test4;\n// In a Clone & unique_ptr world, it will lead to a crash ;)\n```\nAs you can see from the developer point of view, using Optional makes the usage much more easy and the code much more elegant. I hope these examples to convinced you to accept the Optional :)\nAnyway, without Optional and deep-copy all my work on this patch is useless :), the previous version is simpler ....\n. I think flatbuffers::Optional can subclass std::unique_ptr to allow users to have have them both, but IMHO it will just unnecessary complicate the code...\nAnyway, please fell free to cherry-pick any parts of this patch you think are useful. For all my use cases (easy) deep-copy (flatbuffers::Optional) is mandatory, therefore for my own needs I'm going to keep my own flatbuffers version :)\n. I truly believe your comment is not fair. I signal my intention to get rid of std::unique_ptr to enable deep-copy from very beginning. I created this PR because I'm a long time FOSS developer and I don't like permanent forks and I believe that all the patches should be discussed and upstreamed (if possible).\nAs I said in my previous comment, no Optional type means no deep-copy which means no go for me which also means that I'm forced to make a permanent fork ... \nAgain if you don't want deep copy, then IMHO this patch is pretty useless for you, because all the things I added: Optional, Union's  copy operator&constructor, etc.  were only to get deep copy...\n. Well, I added Optional after I realized that I can't just use StructT variable; because it's not optional anymore and the Pack method will use it, so, I had to find a way to make it optional but still have deep copy. I thought a lot how to use STL for this job, and I could not find anything available in c++0x, therefore I ended up with Optional... Optional is not more complicated than the existing classes from flatbuffers.h, and if the developers can use those, I don't think they will have any difficulties with the new class.\n. Dear lp35,\nI'll be more than happy to see it merged, but sadly there is nothing I can do ... :(\n. @gwvo just out of curiosity, what are the unrelated changes in this patch? I was pretty sure (I'm still) that all the changes I made where just to enable deep-copy.\n. The last patch has only the emplace_back afik ...\n. @gwvo will you agree to add a new argument as @lp35 proposed, which make use of Optional instead of uniqe_ptr and enable deep-copy?\nSadly this month I don't think I'll have time to rebase my changes on top of flatbuffers (I need to prepare my presentation and to go to QtWS).\n. @gwvo I think your suggestion makes sense only if there are cases when you want to mix the pointer types e.g.\ntable Type {\n string: bla_bla;\n}\ntable Test {\n  val1:Type (std::unique_ptr);\n  val2:Type (std::shared_ptr);\n  val3:Type (Optional);\n}\nbut personally I don't see any use case for it...\nWhat about a flatc argument where you specify the optional type and let the user to put there anything he wants: flatc --cpp_optional_type=Optional|std::unique_ptr|std::shared_ptr|std::optional|boots::optional|QVariant|etc. --... ? \nI think this approach is more flexible than a field attributes because if the user's build system detects that the compiler can do C++17 it can automatically switch from flatbuffers's internal Optional to std::optional. Or if the user uses boost he can use boost::optional instead...\nEven more, we can add one more annotation to .fbs (e.g. similar to root_type Monster; we'll have cpp_optional_type flatbuffers::Optional;) which sets the default optional type, but of course we still need the flatc argument which can be used to override it.\n. Yesterday I tried to rebase my patch on top of current HEAD. There are tons of conflicts and it seems I need to rewrite it again from scratch :(. Sadly I'm pretty buys at work these days and I'm afraid I cna't do anything this month.\nI also found a two problems:\n- there is no way to know if the cpp-ptr-type supports assignment operator (and copy constructor), so I don't know if I can emit \"operator = (...)\" code or not. If I emit this code no matter what and the user will try to use it with naked ptrs it will crash. For std::unique_ptr you'll get a compilation error ... I have no idea how to fix this problem without adding a new parameter to flatc. @gwvo: do you have any ideas?\n- @gwvo if you want to move Optional to it's own file, then we need a way to include the needed header file(s) in the generated code. My suggestion is to create a c++ section in the .fbs e.g.:\n```\nc++ {\n gen-object-api = true; // Generates the object api\n cpp-ptr-type = flatbuffers::Optional; // global cpp-ptr-type for this file\n #include \n // here we can also enable the assignment operator (and copy constructor) ? e.g.:\n deep-copy-objects=true; // ?\n// All this options can be overwritten by command line params ?\n// Allow defines as well ?\ndefine FLATBUFFERS_TRACK_VERIFIER_BUFFER_SIZE\ndefine OTHER_COOL_MACRO\n// Choose another target framework\nframework=qt5; // default is stl\n}\n// Example IDL file for our monster's schema.\nnamespace MyGame.Sample;\nenum Color:byte { Red = 0, Green, Blue = 2 }\n....\n```\nDefault C++ framework is STL, but my flatbuffers fork can generate for Qt5 as well check https://github.com/bog-dan-ro/flatbuffers/commit/b49b1d3c25b9b005ea65a7c747c7f3f627f4f756 which I'd like to upstream it as well.. Well, if it fails I want size_t GetSize() const to return 0 .\n. Done\n. Done\n. It is, because bool Verify(const void *elem, size_t elem_len) const it is const :)\n. Ah, I add it by mistake to this patch :D, anyway I'll update it as you suggested.\n. Done. \nThough I think adding a number is close to 0 when it comes to speed penalties. \n. GetSize() just become much important than I thought :)\nWithout this check flatbuffers returns true even the size of the buffer is not right e.g.:\nVerifier verifier(flatbuf, length - 1);\nVerifyMonsterBuffer(verifier) == true !!!\n. bool Verify(...) is called from other const methods. Same bool Check(bool ok) const which is also const.\nShall I remove const everywhere? Or we can leave with one mutable :)?\n. If we don't test for entire buffer you'll get contradictorios results:\nVerifyMonsterBuffer will return true but Verifier::GetSize returns 0 ... which is not ok, that said, IMHO VerifyMonsterBuffer should return false in this case.\n. Well, I tried to follow the other methods ... shall I remove all the comments?\n. Done, I also renamed GetBuffer -> GetInternalBuffer and GetBufferSize -> GetInternalBufferSize\n. I can't as long as VerifyMonsterBuffer is using it.\n. Ah, I didn't know about generate_code.sh script, I'll run it and push all the changes.\n. Done.\n. This is exactly what I'm doing, first two tests are checking two flatbuffer one after another, but I also wanted to cover non-aligned data, that's why the 3rd and 4th tests are shifted 1 and 2  bytes,  which are resulting into unaligned data, these are needed to cover cases when the flatbuffer data is not aligned in memory.\n. auto str = builde.createString(flatBufferObject->someString());\nif flatBufferObject->someString() is null the above line will crash.\n. Ah sorry I thought you're asking for a use case :). I'll use ?: :)\n. Done\n. It will remain unused ... and I don't think that's very orthodox \n. Removed as requested. \nUnaligned (starting address) seems to work well now, I hope nobody will break it in the future, because there is no specification in the docs that they must be aligned, and people might keep them into an unaligned address.\n. done\n. Shall I rename it back to GetSize() ? \n. Ok, I'll revert this change, though most of the modern (after 2010) compilers should know how to devirtualize (where possible) any functions\nI choose this approach because it make the code simpler, smaller & nicer ...\n. done\n. done\n. Done\n. Done\n. I ran the generate_code.sh script and it generated all these changes. I'll remove them from patch.\n. Nice catch! I can't remove it, because the C++11 variant uses range for with val variable. I'll use a reference as you suggested.\n. According to https://gcc.gnu.org/gcc-4.8/cxx0x_status.html is the first gcc with full C++11 support, but I checked https://gcc.gnu.org/gcc-4.7/cxx0x_status.html and it seems it has all the goodies too.\nAnyway I'll choose 4.8 for safeness.\n. It is, but only for table fields not also for vectors, this is the reason why is not here anymore.\n. here enemy should be MyGame.Example2.Monster, right?\n. ",
    "heynemann": "I thought this could be an issue with the way Iris implements Websockets, so I tried with Gorilla Websockets as well. Same error:\ngoroutine 36 [running]:\nnet/http.(*conn).serve.func1(0xc82024e800)\n/usr/lib/go-1.6/src/net/http/server.go:1389 +0xc1\npanic(0xb7f6a0, 0xc8200140d0)\n/usr/lib/go-1.6/src/runtime/panic.go:426 +0x4e9\ngithub.com/google/flatbuffers/go.(*Table).Offset(0xc8202bbab0, 0x7f788b450004, 0xc820034010)\n/home/heynemann/dev/go/src/github.com/google/flatbuffers/go/table.go:15 +0x1b5\ngithub.com/heynemann/egs/channel.(*Message).MessageType(0xc8202bbab0, 0x1)\n/home/heynemann/dev/go/src/github.com/heynemann/egs/channel/Message.go:25 +0x33\ngithub.com/heynemann/egs/channel.echo(0x7f788b419a18, 0xc820252f70, 0xc82027a540)\n/home/heynemann/dev/go/src/github.com/heynemann/egs/channel/ws.go:31 +0x683\nnet/http.HandlerFunc.ServeHTTP(0xdc9dc0, 0x7f788b419a18, 0xc820252f70, 0xc82027a540)\n/usr/lib/go-1.6/src/net/http/server.go:1618 +0x3a\nnet/http.(*ServeMux).ServeHTTP(0xc820010d20, 0x7f788b419a18, 0xc820252f70, 0xc82027a540)\n/usr/lib/go-1.6/src/net/http/server.go:1910 +0x17d\nnet/http.serverHandler.ServeHTTP(0xc82024e600, 0x7f788b419a18, 0xc820252f70, 0xc82027a540)\n/usr/lib/go-1.6/src/net/http/server.go:2081 +0x19e\nnet/http.(*conn).serve(0xc82024e800)\n/usr/lib/go-1.6/src/net/http/server.go:1472 +0xf2e\ncreated by net/http.(*Server).Serve\n/usr/lib/go-1.6/src/net/http/server.go:2137 +0x44e\nHope the extra info helps!\n. I kept trying and figured that in the JS code I was not calling finishMessageBuffer. After fixing it, the serve can receive it properly:\nchannel.Message.finishMessageBuffer(b, msg)\nconst buff = b.asUint8Array()\nthis.socket.send(buff.buffer)\nBut now when I try to read the union table Ping, I get an error runtime error: index out of range. The code I'm using to read the table is:\n```\n    var table flatbuffers.Table\n    if ok := message.Payload(&table); !ok {\n        log.Println(\"Loading source ping failed.\")\n        return\n    }\nvar sourcePing Ping\nsourcePing.Init(table.Bytes, table.Pos)\nfmt.Printf(\"RECEIVED %d\", sourcePing.ClientSent())\n\n```\nIt always fails, if I do not guard against it by returning. The ClientSent() method fails with:\n2016/05/28 23:02:28 Loading source ping failed.\n2016/05/28 23:02:28 panic on hijacked conn: runtime error: index out of range\nStack trace:\ngoroutine 14 [running]:\nruntime/debug.Stack(0x0, 0x0, 0x0)\n    /usr/lib/go-1.6/src/runtime/debug/stack.go:24 +0x80\ngithub.com/valyala/fasthttp.hijackConnHandler.func1(0x7efcb986a060, 0xc820034308, 0xc82023e500, 0x7efcb986a000, 0xc820034308, 0xc82024c940)\n    /home/heynemann/dev/go/src/github.com/valyala/fasthttp/server.go:1612 +0x7d\npanic(0xb6e7a0, 0xc820012100)\n    /usr/lib/go-1.6/src/runtime/panic.go:426 +0x4e9\ngithub.com/google/flatbuffers/go.(*Table).Offset(0xc820040ad8, 0xc820250004, 0x1c)\n    /home/heynemann/dev/go/src/github.com/google/flatbuffers/go/table.go:15 +0x1ae\ngithub.com/heynemann/egs/channel.(*Ping).ClientSent(0xc820040ad8, 0x1)\n    /home/heynemann/dev/go/src/github.com/heynemann/egs/channel/Ping.go:18 +0x37\ngithub.com/heynemann/egs/channel.(*App).handleMessage(0xc82022b200, 0xc820040c40, 0xc8574a4db4, 0x7efcb98b4630, 0xc82001f7a0)\n    /home/heynemann/dev/go/src/github.com/heynemann/egs/channel/app.go:68 +0x20d\ngithub.com/heynemann/egs/channel.SocketSupport.func1.1(0xc8202ca000, 0x400, 0x600)\n    /home/heynemann/dev/go/src/github.com/heynemann/egs/channel/socket_handler.go:29 +0x390\ngithub.com/kataras/iris/websocket.(*connection).messageReceived(0xc82001f7a0, 0xc8202ca000, 0x400, 0x600)\n    /home/heynemann/dev/go/src/github.com/kataras/iris/websocket/connection.go:173 +0x6ad\ngithub.com/kataras/iris/websocket.(*connection).reader(0xc82001f7a0)\n    /home/heynemann/dev/go/src/github.com/kataras/iris/websocket/connection.go:133 +0x2c0\ngithub.com/kataras/iris/websocket.(*server).handleConnection(0xc820177130, 0xc8201afb00)\n    /home/heynemann/dev/go/src/github.com/kataras/iris/websocket/server.go:87 +0x96\ngithub.com/kataras/iris/websocket.(*server).(github.com/kataras/iris/websocket.handleConnection)-fm(0xc8201afb00)\n    /home/heynemann/dev/go/src/github.com/kataras/iris/websocket/server.go:73 +0x2a\ngithub.com/iris-contrib/websocket.(*Upgrader).Upgrade.func1(0x7efcb98b4578, 0xc82024c940)\n    /home/heynemann/dev/go/src/github.com/iris-contrib/websocket/server.go:309 +0xbf\ngithub.com/valyala/fasthttp.hijackConnHandler(0x7efcb986a060, 0xc820034308, 0x7efcb986a000, 0xc820034308, 0xc82023e500, 0xc82022be90)\n    /home/heynemann/dev/go/src/github.com/valyala/fasthttp/server.go:1622 +0xf4\ncreated by github.com/valyala/fasthttp.(*Server).serveConn\n    /home/heynemann/dev/go/src/github.com/valyala/fasthttp/server.go:1535 +0xede\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal 0xb code=0x1 addr=0x50 pc=0x6d6535]\nI'll keep trying and I'll update here and close if I find out what's going on!\n. Sorry if that came out wrong @gwvo. I didn't mean it to be rude. I think flatbuffers is awesome. An incredible advance in the right direction. I'm just new to it and thus may have newbie questions.\nWhat I meant with no docs is that I could not find my use case in the docs. I did find it in the tests, though. The use case of deserializing a Table in Golang after serializing it in Javascript.\nI'll try again tonight and thank you very much for the pointers.\nOnce more sorry if it came out wrong.\n. ",
    "Isameru": "I think this is not the case of type promotion. On second thought, there are no good ways to make it work, as the type of enum constants have nothing to do with specified values. Thus, the following .fbs sample is a perfectly fine enum definition:\nenum Number : ubyte {\n    Zero = 0,\n    ...\n    Thousand = 1000,\n}\nIs such a case singalling an error would be probably more appriopriate than extending the size of the type specified.\n. Not a flatbuffers bug.. ",
    "chenjian221": "I have same issue. I had something like ubyte foo = 209;  But I got was public static final byte  = 209; It is really sucks that Java doesn't support unsigned byte.\n. ",
    "josephDunne": "I signed it!\n. Sure I will do that. If its going to be self contained should I just implement it entirely in Rust...in a separate repo perhaps?\nI would much rather use Rust macros for this kind of thing then hand writing cpp\n. Thanks for reviewing. I will do the cleanup and refactor and then @rw can take a look\n. Makes sense. I will add it. Do you mind if its quickcheck tests (assuming quickcheck works on rust stable)? Or do you prefer I copy the approach already in use?\nAlso how do you feel about replacing the generated boiler plate with macros...something like this:\nrust\ntable!(Monster, [{hp, 8, 150},\n                 {mana, 10, 100},\n                 {test,, 12, Vec3}\n                 ...\n                ]);\nThese macros are then expanded at compile time. In general I dont like adding extra layers of indirection but it would make the CPP generator a lot easier to maintain as most of the work will be done in the rust library.\n. @rw \ud83d\udc4d \n. I have done some initial refactoring and cleanup. Fuzzing tests are still outstanding. I may only get to this next weekend. I am however using this code as part of a project so its getting some field testing in the mean time :-)\nsome discussion points (@rw?)\n- Using macros has made the cpp code a lot smaller and cleaner. However its a bit tricker to implement the builders as macros. Will revisits this when I do the Fuzz testing.\n- Switching to Trait extensions for the builders works nicely. Users will need a strategy to deal with name conflicts if the generated builder objects have similar function names - using Rusts scoped imports should work.\n- I dont like the iterator implementation. It does not implement object reuse so is allocating more pointers on the stack then it really needs to. Also for scalar types it should be possible to return slices rather than full Iterators; A slice of objects that deference to a scalar type (so the little endian conversion can happen in the deref operation) something like this:\n``` Rust\nfn get_scalar_vector(buff: &[u8], offset: UOffset) -> &[T] where T: Deref { .. }\nfor x in table.get_scalar_vector(..) {\n   println!(\"This is a u32 number: {}\", *x)\n   println!(\"This is a pointer into the buffer transmuted as some type T: {p}\", x);\n}\n```\nNot sure..\n- Table and Struct objects wrap and own a Table Object. The alternative is they only borrow a table. The owned implementation is simpler but allocates more on the stack when an object is created from a buffer. Since a table is just one pointer and one usize on the stack I feel that this is acceptable. Either way I need to revisit this to make sure that we can properly reuse existing objects.\n- need to implement more traits and utility functions. Suggestions welcome.\n- Are Union types only allowed to be tables? Or cant a Union have a struct, scalar or table option?\n. The Vec is the only heap allocated object.\nI will port the go fuzzing code.\n. Just an update: I have an issue logged with the Rust core team about macro expansion inside traits. I would like to use macros to build traits (used by the individual object builder classes to extend the standard library builder). This would make the code consistent and allow me to delete about half the cpp gen_rust code. So if you don't mind I would like to keep this PR open for a while. Let me know - I can always close and reopen\n. @rw Yeah I dont have to use macros for the builder objects. We can keep that code in gen_rust.cpp for now.\n(I have submitted a pull request to the Rust team to fix the issue so might be in for v1.10)\n. Hi sorry @rw I will focus on finishing it this week. Just been busy.\n. Hi @rw @gwvo I am struggling to find time to finish this but it is still on my radar. I have just pushed my latest refactor which I believe addresses all the concerns raised. The following is still missing:\n- fuzz testing\n- more documentation with doc examples\n- integrating the documentation with this projects other documentation \n. @hollinwilkins  has started a triage of tickets that need to be completed to bring this to completion.\nplease feel free to add any issues you can think of here: https://github.com/josephDunne/flatbuffers/issues. I have added you as admin on crates.io for the flatbuffers crate. Let me know if you get it. Yes I will put that in the table object.\n. I was trying to avoid using Trait extensions. I will switch to using trait extensions then these wont be repeated. \n. ",
    "blt": "Hi, @josephDunne and @gwvo. I caught this PR up with the most recent master, 69dc71b5ed0382e325f96a9bdef057fca94b1c77. The work is here: https://github.com/blt/flatbuffers/tree/rust_support\nI haven't done anything to address the fuzzing or documentation work needed. Here in the next few weeks I aim to. . I'm unsure about the nightly requirement but I'll aim for stable. I need\nto implement the tests.\nOn Sun, Jan 1, 2017, at 06:28 PM, Robert wrote:\n\n@blt[1] Does this still require nightly? Does it have thorough\ntests now?\n\u2014 You are receiving this because you were mentioned. Reply to this\nemail directly, view it on GitHub[2], or mute the thread[3].\n\n\nLinks:\n\nhttps://github.com/blt\nhttps://github.com/google/flatbuffers/pull/3894#issuecomment-269930206\nhttps://github.com/notifications/unsubscribe-auth/AAAN-xnvH8dpufMTJbVLB0b16MTmQ54_ks5rOGDZgaJpZM4Ir42e\n. @rw Will do. I think porting the existing Go tests to Rust ought to be a pretty reasonable undertaking. I may regret having said that 3/4 of the way through. :) . @photex oh hey, woah, somehow I entirely missed that PR. Dang. \n\nIt looks like that was a merge up, yeah. . @binary132 I can't speak for anyone else that was plugging away at this, but for me I ran out of work-sponsored time for this project. All that's needed is someone with time and inclination to run the project through to the end. . ",
    "Manishearth": "You can get rid of the nightly requirement via a build script that uses syn, see https://github.com/dtolnay/syn#custom-derives-on-stable-rust (the instructions are for custom derives, but this is solved a similar way).\n. ",
    "wesm": "Since we're using Flatbuffers in Apache Arrow (https://github.com/apache/arrow) and would like to have a native Rust implementation at some point, if there's anything we can do to help raise awareness around this PR let me know. @rw that's great news. Since our use of Flatbuffers in Arrow is limited to IPC metadata, we could always work around the lack thereof by creating a C library for FFI use in Rust (or anywhere else), but native bindings will be :100: . We're seeing some of these -Wimplicit-fallthrough warnings on gcc 7.3.0, it seems that the // FALL-THRU comments are inadequate to suppress \nhttps://github.com/apache/arrow/issues/3004. I haven't been able to reproduce the issue myself; it seems to be somehow related to ccache per discussion with the issue reporter. . Could ccache be mangling the source in such a way that gcc is not able to match the \"FALL-THRU\" comment properly?. ",
    "olanod": "Any updates on this subject? it seemed that it was about to happen in August and now it looks like it was forgotten \ud83d\ude1f it would be a nice Christmas present! \ud83c\udf85  . It's text based because the creator considered it a good idea? hehe I don't know his reasons but it probably was because of simplicity, the protocol is similar to the one redis uses and the payload of a message is just a slice of bytes so a flatbuffer fits perfectly, http is text based and grpc+flatbuffers doesn't have issues with that \ud83d\ude1c \nI haven't check how code gets generated, I bet it's not hard but it's probably a bad ida to let me write  C++ code \ud83d\ude05 \nAs for pub/sub or similar addition to the syntax I'm not sure yet, maybe being able to define procedures that accept callbacks? are callbacks a thing already? . ",
    "Posnet": "I have made an attempt to fix the merge conflicts here https://github.com/Posnet/flatbuffers/tree/rust_support\nHowever I haven't had a chance to review all the rust tests yet.\nJust posting this here in case @rw or someone else is planning to make progress.. ",
    "photex": "I'm curious, does anyone know if https://github.com/blt/flatbuffers/pull/1 was based on any of this work here?\nIf there is some way for me to contribute I'd like to help out getting this landed.\n. I think the most widely used approach in rust tooling is to generate code in whatever way is convenient and if rustfmt is on PATH just run it on the output to ensure code is formatted according to individual project conventions. . Sorry, I could have said \"generate or write\". A lot of folks seem to rely on editor tools when writing it to automatically format on save. That allows projects to place their own rustfmt configurations in repos and expect contributors to have run rustfmt when submitting PRs.\n. ",
    "binary132": "I'm sure I'm not alone when I say this would be nice to see shipped.\nMaybe a reduced feature set could get out the door earlier than the whole fully-baked great everything?\nMaybe I could help?. @rw please don't hesitate to post your WIP branch, even if it's not clean yet.  Weeks are precious :). @zhangminglei, can you publish your benchmark code and data?. @aardappel one challenge I've gotten when suggesting the use of flatbuffers is that they're not optimized for size, and therefore (since IO, not CPU, typically dominates resources) \"why would we want to do that?\"  I have to admit I'm not sure how to approach that.  Is there a way to tune it for size?. I am really interested in this!  I have two questions:\n1. can flatc support a plugin model for language extension, like protoc?  Otherwise, I feel the reach will be limited to only support languages which flatc chooses to support.\n2. can we please take care to avoid the platform lock-in pitfall which the Dart protoc plugin has fallen into where it only supports Unix-y platforms?  mingw / cygwin are not a solution for me.. I'm also keen on Flutter, but good options in the browser are limited.  Dart makes a good case to me as a multi-platform client runtime, which I'm looking to use flatbuffers for.  I'll try to find time to take a look at the browser side of this.  I am entirely new to flatbuffers, but it looks intriguing (especially since capnproto has weak Javascript options.)\nThere's already a Javascript target (right?), so maybe in the short-term this could be done by proxying to that plus generating a Dart-native wrapper. \ud83e\udd14 \nBut then you're not dealing with a single dart target, you're looking at a dart-native and a dart-js target.. Not urgent, more curious.  Adding panic handlers as goroutine wrappers everywhere I might read a flatbuffer makes for extremely un-idiomatic code, though.. How do you know that these bounds checks will not be elided by the compiler, since they have no side-effect?  Additionally, this appears to incur one operation more than is necessary.\nI'm not an optimization expert, so it's entirely possible the following has a negative consequence I'm not aware of, but my suggestion is to rearrange the order of the |= to use the highest offset first.  This will make it unambiguous w.r.t. elision, and not dependent on compiler internals, as well as eliminating a potential extra instruction.\n\nExample:\n\ngo\nfunc GetUint16(buf []byte) (n uint16) {\n    n |= uint16(buf[1]) << 8\n    n |= uint16(buf[0])\n    return\n }\n. \ud83d\udc4f \ud83d\udc4f \ud83d\udc4f . there are no well-documented rules for escape analysis by the Go compiler imo.  It is also an area of compiler optimization, so exposing the rules as a public interface would be counterproductive....\nhttps://github.com/golang/go/wiki/CompilerOptimizations#escape-analysis-and-inlining. I would suggest embedding the sequence of packed binaries in some other protocol, perhaps simply by prefixing them with their length encoded as a little-endian uint32, instead of attempting to use a delimiter.\nAnother approach is to prefix with a Flatbuffer message containing only the size, for better platform portability.  Then your reader will first consume a Size fb, then a Message fb, then another Size fb, etc....\nIf you know your indices ahead of time, you could of course write them all as a header, too, since you're using a static file approach.  Then the reader could hop to the indexed message.. ",
    "eberkund": "What's left to be done? Is it just the unit testing stuff?. ",
    "andygrove": "@rw This is great news! I am wanting to use flatbuffers for the Apache Arrow rust library (we need it for IPC). I would be happy to be an early adopter / tester!. @rw Is there any update on this? I'm keen to use this with the Rust version of Apache Arrow so that we can implement the IPC mechanism. I'm happy to be an early adopter/tester.. @rw What is a good way to report issues / discuss this? I'm hitting compilation errors building the project. Do you have a gitter address?. The issue I'm having is described well in https://github.com/google/flatbuffers/issues/342 but the summary is that the Java-style namespace specified in the schema file is not friendly for Rust. The flatc generator has namespace overrides for some languages but not Rust. I guess I am requesting that feature e.g. --rust-namespace. ",
    "ChristopherRabotin": "@rw is there a repo where you're making this commits? It seems like the branch associated to this pull request hasn't been updated since August 2017. I'd be keen to help in the development of this feature, if my help is needed. Thanks. ",
    "abreis": "@ChristopherRabotin I believe it's this branch:\nhttps://github.com/rw/flatbuffers/commits/2018-02--rust. ",
    "ry": "@rw Thanks for your work! I'm closely tracking your progress and have already made attempts to use it. I wasn't able to get it working last week, but I will try again soon.\nhttps://github.com/ry/deno/blob/ad4f335847daffcc5556008aebebaf2d1eecac67/gclient_config.py#L31-L36\n. Awesome work @rw - thank you! We're already using your patch in our project and it's working great.. @krojew I don't understand? The reason for this change is to keep them in sync. If the types are maintained where module is developed they won't get out of sync. Also negates the need to install separate @types/flatbuffers package.\n(They are out of sync currently btw.). @krojew Yes - many projects distribute their own types file in their NPM packages. The types file would just be there when \"npm install flatbuffers\" is run. There's no downside AFAIK.\nFor example, see TensorFlow.js \nhttps://github.com/tensorflow/tfjs-core/blob/e128f936aa7f92e5fcc53e54daa3b8b3ef989da9/package.json#L9. @krojew Actually my project is in that camp. We would like to live on flatbuffer's master branch and not install a possibly out-of-date package from npm. Having the types file maintained here helps this.\nhttps://github.com/denoland/deno/blob/941e27d8c1c2748e62972510f8059838fdc84dad/gclient_config.py#L37-L40\nThe only downside I can think of is that the flatbuffers npm package will grow by 10kb unnecessarily for people who don't use TS. I think that's a reasonable trade off tho.. @krojew Agree - there shouldn't be two. The @types package should be closed if this is landed.. ",
    "michalmalecki": "Thanks a lot, next time I will do PR!\n. ",
    "promethe42": "I was trying to send/receive FlatBuffer streams on a socket. Looks like a major feature to me...\nAny hope to have this in an actual release anytime soon ?\n. > So #3905 was merged, and allows the verifier to compute the size of a FlatBuffer in a larger buffer.\nI've built master and it should work.\nBut Verifier::GetComputedSize always returns 0...\nCould you provide a sample to show how it works?\n. @bog-dan-ro thank you for the sample! So the important part is to call GetComputedSize() after the call to GetXXXXX?\n. @bog-dan-ro OK thanks it works!\nI've successfully used FlatBuffers to create a networking protocol between Blender (python) and Minko applications (C++11, https://github.com/aerys/minko). That's pretty cool! Here are some (old) videos of how it's used to do live edition/configuration of a 3D app built/scripted in Blender and developper with the Minko SDK:\nhttp://blogs.aerys.in/jeanmarc-leroux/wp-content/uploads/2015/08/23/Desktop-08.23.2015-18.34.25.07.mp4\nhttp://blogs.aerys.in/jeanmarc-leroux/wp-content/uploads/2015/08/23/Desktop-08.23.2015-18.22.41.06.mp4\nhttp://blogs.aerys.in/jeanmarc-leroux/wp-content/uploads/2015/08/23/Desktop-08.23.2015-14.46.37.04.mp4\nThanks for all the great work/help!\n. Composition ?\ntable Monster { .. }\ntable XxxMonster {\n  base:Monster;\n  ...\n}\n. table Monster { .. }\ntable XxxMonster {\n  base:Monster;\n  ...\n}\ntable YyyMonster {\n  base:Monster;\n  ...\n}\nunion AnyMonster {\n  XxxMonster,\n  YyyMonster\n}\nYou can use AnyMonster to store an XxxMonster or an YyyMonster.\nFlatBuffers will also store the actual type of the union field so you can cast it to the actual type (cf \"Union\" in https://google.github.io/flatbuffers/flatbuffers_guide_writing_schema.html).\n. ",
    "zoon": "Not at the moment, unfortunately.\n. ",
    "jinq0123": "I think using fully qualified name will be OK.\nmonster_test.fbs\nnamespace MyGame.Example2;\ntable Monster {}  // Test having same name as below, but in different namespace.\nnamespace MyGame.Example;\n...\ntable Monster {\n  ...\n  color:Color = Blue (id: 6);\n...\nConvert monster_test.bfbs to json:\ntests>flatc.exe --json --raw-binary ..\\reflection\\reflection.fbs -- monster_test.bfbs\nmonster_test.json:\nobjects: [\n    {\n      name: \"Monster\",\n      fields: [\n      ],\n      minalign: 1\n    },\n    {\n      name: \"Monster\",\n      fields: [\n        {\n          name: \"color\",\n    ...\nShould use qualified name:\nobjects: [\n    {\n      name: \"MyGame.Example2.Monster\",\n      fields: [\n      ],\n      minalign: 1\n    },\n    {\n      name: \"MyGame.Example.Monster\",\n      fields: [\n        {\n          name: \"color\",\n    ...\n. I think using fully qualified name will be OK.\nmonster_test.fbs\nnamespace MyGame.Example2;\ntable Monster {}  // Test having same name as below, but in different namespace.\nnamespace MyGame.Example;\n...\ntable Monster {\n  ...\n  color:Color = Blue (id: 6);\n...\nConvert monster_test.bfbs to json:\ntests>flatc.exe --json --raw-binary ..\\reflection\\reflection.fbs -- monster_test.bfbs\nmonster_test.json:\nobjects: [\n    {\n      name: \"Monster\",\n      fields: [\n      ],\n      minalign: 1\n    },\n    {\n      name: \"Monster\",\n      fields: [\n        {\n          name: \"color\",\n    ...\nShould use qualified name:\nobjects: [\n    {\n      name: \"MyGame.Example2.Monster\",\n      fields: [\n      ],\n      minalign: 1\n    },\n    {\n      name: \"MyGame.Example.Monster\",\n      fields: [\n        {\n          name: \"color\",\n    ...\n. String is utf8 and binary data uses [byte]. So this is not an issue.\n. ",
    "twiggler": "Sure.\nOp 7 jun. 2016 1:23 a.m. schreef \"Wouter van Oortmerssen\" \nnotifications@github.com:\n\nAh indeed. Are you able to make a PR? I think you could just simplify it\nto return VectorIterator(data_, 1)\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/3900#issuecomment-224118469,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AMNRuyiLOtZ6RH-ovvlOcB6006lo3xpkks5qJKvugaJpZM4IuYFW\n.\n. \n",
    "senior7515": "I signed it!\n. @gwvo  fixed. Thanks. \n. # Comments for reviewers:\n1) Modifying the test/generate_code.sh to include a string type like this:\n../flatc --cpp --cpp-str-type \"sstring\" --gen-mutable --gen-object-api monster.fbs\nThis is the output changes of the generated code. \n``` \n@@ -148,7 +148,7 @@ struct MonsterT : public flatbuffers::NativeTable {\n   std::unique_ptr pos;\n   int16_t mana;\n   int16_t hp;\n-  std::string name;\n+  sstring name;\n   std::vector inventory;\n   Color color;\n   std::vector> weapons;\n@@ -344,7 +344,7 @@ flatbuffers::Offset CreateMonster(flatbuffers::FlatBufferBuilder &_fbb,\nstruct WeaponT : public flatbuffers::NativeTable {\n   typedef Weapon TableType;\n-  std::string name;\n+  sstring name;\n   int16_t damage;\n   WeaponT()\n       : damage(0) {\n```\nIn addition, note that the last master without my changes do not include inline changes, but these are unrelated. \n```\n@@ -224,6 +224,10 @@ struct Monster FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {\n   const void equipped() const {\n     return GetPointer(VT_EQUIPPED);\n   }\n+  template const T equipped_as() const;\n+  const Weapon equipped_as_Weapon() const {\n+    return (equipped_type() == Equipment_Weapon)? static_cast(equipped()) : nullptr;\n+  }\n   void mutable_equipped() {\n     return GetPointer(VT_EQUIPPED);\n   }\n@@ -250,6 +254,10 @@ struct Monster FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {\n   static flatbuffers::Offset Pack(flatbuffers::FlatBufferBuilder &_fbb, const MonsterT _o, const flatbuffers::rehasher_function_t _rehasher = nullptr);\n };\n+template<> inline const Weapon *Monster::equipped_as() const {\n+  return equipped_as_Weapon();\n+}\n+\n```. @gwvo . I do flatbuffers in flatbuffers. \nFor it, I usually check just the top level fields  say you have a \n```\nuint8_t *ptr\nauto t = GetRoot(ptr)\nt->field()  -------------> sometimes throws.\n```\nhaving this is nice to have, don't see major issues. . @gwvo  \nI had a file that was like this: \n```\nostream &\noperator<<(ostream &o, const ::smf::wal::tx_put_fragment &f) {\n  o << \"tx_put_fragment=\"\n    << flatbuffers::FlatBufferToString(reinterpret_cast(&f),\n                                       tx_put_fragmentTypeTable());\n  return o;\n}\nostream &\noperator<<(ostream &o, const tx_get_request &r) {\n  o << \"tx_get_request=\"\n    << flatbuffers::FlatBufferToString(reinterpret_cast(&r),\n                                       tx_get_requestTypeTable());\n  return o;\n}\nostream &\noperator<<(ostream &o, const tx_get_reply &r) {\n  o << \"tx_get_reply=\"\n    << flatbuffers::FlatBufferToString(reinterpret_cast(&r),\n                                       tx_get_replyTypeTable());\n  return o;\n}\n...\n...\n```\nI had a few core_dumps and then realize it was a typo on the name. \nI can write a type safe quick wrapper with this change. . All the unit tests are passing now\ntest 1\n    Start 1: flattests\n1: Test command: /home/agallego/workspace/flatbuffers/build/flattests\n1: Test timeout computed to be: 9.99988e+06\n1: ALL TESTS PASSED\n1/1 Test #1: flattests ........................   Passed    0.06 sec\n100% tests passed, 0 tests failed out of 1\nTotal Test time (real) =   0.06 sec\n. one can then create a simple helper like this \n```\ntemplate< typename T>\nstruct fbs_print{\n   fbs_print(const uint8_t *x):data(x){}\ninline flatbuffers::TypeTable* print() { return T::DefaultTypeTable(); }\n   const uint8_t * data;\n};\nstd::ostream operator<<(std::ostream &o, const fbs_print &any{\n   return o << any.print();\n}\n....\nthen you can it it on your code as \nchar* buf = ... \nstd::cout << fbs_print(buf) << std::endl;\n```. Sounds good, I fixed it, let me know if you want me to do any other edits. . @evolutional  I'm interested in testing :D in case you want to put something up for WIP-review. . Technically the only challenge is to expose a new method that allows me to do this:\n```\nexplicit vector_downward(size_t initial_size,\n                           Allocator *allocator,\n                           bool own_allocator,\n                           size_t buffer_minalign)\n      : allocator_(allocator ? allocator : &DefaultAllocator::instance()),\n        own_allocator_(own_allocator),\n        initial_size_(initial_size),\n        buffer_minalign_(buffer_minalign),\n        reserved_(0),\n        buf_(nullptr),\n        cur_(nullptr),\n        scratch_(nullptr) {\n    FLATBUFFERS_ASSERT(allocator_);\n  }\n```. thanks @aardappel  - Sounds good will follow up w/ a patch. \nThe graphs are latency measures between multiple runs of the same application where the only difference of the test is the strategy of flatbuffers described above.  - measured in microsecs. \n. The perf difference was so small, that i don't think is worth the code complexity. . @zhimingxie  - it works - it picks up the project settings, i can confirm. \nThis is what I use for example:\nhttps://github.com/senior7515/smf/blob/master/CMakeLists.txt#L6-L11\n. Awesome, feel free to assign to me, I'll work on it this weekend. . * w.r.t first benchmark - agreed - i took it from the top hit of google when you search for flatbuffers allocator cost (https://github.com/thekvs/cpp-serializers) - no need for extra alloc\n\n\nw.r.t cap 'n proto - yup. \n\n\nw.r.t large buffers - maybe. Say you are integrating w/ an existing API - say reading from disk, - someone says read 4 records, you don't know apriori how big the records are, so while yes, in the ideal case you would, in practice it would require a ton of work to pre-read all headers, compute the size and allocate a large buffer. Also, turns out that is not actually faster.\n\n\nI measured a large thread-local buffer say 16K - and teh cost of memcpy after the flatbufferBuilder::Finish() is called - it's not that significant of a difference to warrant the extra logic. \n\n\nw.r.t object-api - i disagree. Specially for objects that are constructed slowly, over time and a series of calls. Say you have a key=value store - something simple. The user asks you to read all records with a filter of a key so say get(key=\"prefix*\"). So you return all records that start w/ \"prefix\" up to a maximum. if the object API didn't exist you would effectively have to have your own object api for doing lazy object construction. No doubt. \n\nThe benefits are largely still there. Evolution, fast access to serialize data (no deserialization), x-lang support, etc, etc.\n\nHow do you suggest supporting this use case which is AFAIK the most common one for the teams I know using flatbuffers internally. \n\n\nw.r.t generated code of estimating the size of the struct.  -> I don't see the correlation between being worried about reallocs and the object-api. \n\nYou have to, one way or another, get data into the flatbuffersbuilder, no alternative. The object-api is a nice way to abstract some of the lazy object construction to interact w/ existing API's.    \nreallocs really matter on the edge case. However, even in the small case, it forces a use of the flatbuffersbuilder to keep a local cache around. It is to say, you build and then you memcpy the FlatbuffersBuilder::Size() number of bytes into an output buffer of. \nHaving this method allows you use the nice ::Detach() api you built to alloc exactly the number of bytes the builder will ever need. You can guarantee no reallocs, but more strongly, you can guarantee that you will need exactly that number of bytes and no more.\n\n. w.r.t code generation, not sure how complex it would be tho - in my head, it doesn't have to be complex, specially if the rules for encoding are well defined. We want to guarantee one ordering is optimal since likely the other orderings of encoding won't necessarily yield wildly different results, just different within some small number of bytes - my initial thought at least. \nI'll spend sometime to see if I can think of something simpler.\nDo you have some pointers to think through the vtable sharing ?\n i think this is the last piece i don't fully grok. \nThanks!. @aardappel  - was looking at flamegraph for the usecase i listed above and the minireflect doesn't work in that it expects already a buffer. The use case is \nturn a flatbuffers::NativeTable -> FlatbuffersBuilder::Buffer basically with hopefully one call to allocate_downward (via some sort of vector::reserve()-type of call)\nJust commenting here for book keeping. . @aardappel  - so i think i found a way to get predictable building/etc.\nhttps://github.com/google/flatbuffers/blob/master/src/idl_gen_cpp.cpp#L2270-L2429\nI use these methods for converting from a typename T::NativeTableType -> flatbuffers::FlatbuffersBuilder which means that at least there is some guaranteed ordering. \nas long as the data is created via the ::Pack() and ::Unpack() methods then it has an implicit ordering. \nSo as long as we do the same it should be exactly one allocation of the underlying buffer no? . for example this is my handy util to take any type and turn it into a buffer. \nhttps://github.com/smfrpc/smf/blob/master/src/include/smf/native_type_utils.h#L24\n. @aardappel thanks for the tip on force-defaults good point. \nI have a test on an app for which one allocation - by passing the right size to the flatbuffersbuilder - decreased the tail latency by 20ms - at the same time, it increased the throughput by 200MB. \nI am hacking the code generator next week. \nstay tunned for a detail histogram of the app latency measurements pre-post change.\n. @gwvo  ok cleaning and resubmitting\n. don't we want this tho? i.e.: a type that has T::c_str() and T::length() which includes:\n\nbasic_string i.e.: https://gcc.gnu.org/onlinedocs/gcc-6.2.0/libstdc++/api/a01076_source.html\n\nand most other string types that simplify this interface?\nHow do you propose we bound the user supplied string so that we can convert it into a flatbuffers::String\n. We need this otherwise we have to change the generated code too no?\nIf we were to provide a CreateStringFrom we have to change the code generator to know that there is a opts.cpp_object_api_string_type \nif you look at the converters from nativeT -> fbb  via Create##Type() it uses CreateString() i also don't think it will create ambiguities given the currect implementations. i.e: no other types passed work on member functions, they are just plain pointers. \nHappy to add the other CreateStringFrom i'm just not sure what the benefit is. \ni.e.: from a template level i don't see any errors. \n. reading the template argument deduction rules, don't see any conflicts... \nhttp://en.cppreference.com/w/cpp/language/function_template\nin fact, we want this kind of type duck typing that's how we expose this to the world. \nmaybe I'm missing something. . i guess maybe it should be\ntemplate<class T> . @aardappel . @aardappel  Yeah, here is a test for you: \nhttp://cpp.sh/77moa\nGiven this test, I don't think we need a std::string overload. \nI'll add it, but it seems like dead code to me. \nLet me know.  . In addition, here is one that highlights this case: \nhttp://cpp.sh/33v4\n. I checked the assembly on https://godbolt.org/ \nand there is no difference at -O2 . @aardappel  done! I rebased too so it looks clean. . Sounds good, will submit fix. Thanks!. ",
    "devantoine": "I signed it!\n. I'll fix the rest asap!\n. I've updated the PR. Test is not here at the moment, it's kinda a mess in it. I've seen there's a \"testUnicode()\" function in it, which is commented. It may be a good starting point.\n. @gwvo I agree, but this handwritten code scares me, I know nothing about character encoding.\n. Because it's a non-default extension: http://php.net/manual/en/mbstring.installation.php\n. You're right, it was bad code. Now it's fixed using a single if.\n. Yes, you're right, I really did a bad job there, just editing the file on github directly. Sorry for your time.\nI'll checkout the project, do that change properly and add a new test, but I'm not sure how to test both behaviors, one using mb_detect_encoding and the other using the hexadecimal algorithm.\n. ",
    "ChrisCates": "@gwvo The use case would primarily be used for on the fly uploads.\nOne can simply upload a file and/or copy and paste a binary.\nThe flatbuffer could be converted directly in Javascript and a REST service to work with C can be used as well.\n. @gwvo understood. Would you be comfortable me refactoring the unit tests to be more readable and or potentially having better logging of commands and actions done in the tests?\n. @gwvo I'll provide a deliverable monday. Thank you :)\n. ",
    "mpouttuclarke": "Basically, for the node.js app it would enable visual editing of flatbuffer schemas locally as well as on a server.  The basic features would be as follows:\n1. Complete visual editing of multi-layered flatbuffer schemas with pull down lists for supported data types, support for nested types, etc..\n2. Ability to export a schema file from a visual definition.\n3. Ability to import a schema file and have a visualization of that schema you can use to edit.\n4. Ability to copy and paste schema definitions in and out of the tool as a substitute for using files.\n. mod_flatbuff would provide a HTTP web service to stream JSON to flatbuffers and back using the C APIs.  Apache mods are typically written in C.  By combining this with the node.js app developers will be able to verify that the schema produces valid results against the C API.  Also, the mod_flatbuff would enable fast and efficient conversions of JSON to flatbuff and vice versa. \n. Ok, thanks for the feedback!  We will reference the original project and give you guys a link to our docs\n\nOn Jun 20, 2016, at 3:04 PM, Wouter van Oortmerssen notifications@github.com wrote:\nA visual editor for any FlatBuffer data would be awesome to have! However it does sound like a larger project, so maybe it would be more appropriate in its own project, with the FlatBuffer project referring to it?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. We have added the repo below.  Feel free to watch, star, and give feedback as our efforts progress...\nhttps://github.com/LamdaFu/flatbuffers-visual\n. We have added the repo below.  Feel free to watch, star, and give feedback as our efforts progress...\nhttps://github.com/LamdaFu/flatbuffers-visual\n. We have added the repo below.  Feel free to watch, star, and give feedback as our efforts progress...\nhttps://github.com/LamdaFu/flatbuffers-visual\n. \n",
    "johngalt131": "Looking more closely, C++ would pretend to work, but internally it's using int8_t as well.\nHALP!\n. > Also users can still have (unofficial) unions with 256+ types by doing what we should have been doing for them : in a Table, put an uint field for their unionType and a [ubyte] field for the union data, init the [ubyte] with the right type, cast and return. If they have to write this code and we refuse to do it for them, they are really going to hate us.\nThis is how I rewrote my IDL to get around this limitation\n. I'm not calling that function directly. I am using a generated \"add\" method (addTimestamp)\nIt's implied, but timestamp is of type \"long\"\nvar fbb = new flatbuffers.Builder(4096);\nresponseId = fbb.createString(responseId);\nrequestId = fbb.createString(requestId);\nvar sessionToken = fbb.createString(pData[\"token\"] || \"Empty\");\nIOSServer.IOSCore.Messages.Message.startMessage(fbb);\n// This needs to be set. It doesn't matter what it's set to because IOSServer will\n// overwrite it. It can't mutate a field which isn't set though.\nIOSServer.IOSCore.Messages.Message.addTimestamp(fbb, 999); // *** Called here ***. This is fixed in https://github.com/google/flatbuffers/pull/5151. Though this (somehow?) broke a build..\n",
    "gregschlom": "@gwvo also worth noting that the current behavior of hardcoding the union type to a byte goes against Fatbuffers'  own grammar: \nenum_decl = ( enum | union ) ident [ : type ] metadata { commasep( enumval_decl ) }\nSource: https://google.github.io/flatbuffers/flatbuffers_grammar.html\nThe grammar clearly states that unions can have an optional type, however the compiler rejects that syntax.. By the way, if anyone is wondering, Flatbuffers do work well on Arduino. We just had to add the StandardCplusplus library to the project and #define FLATBUFFERS_CPP98_STL.  We also had to comment out #include <cstdint> and #include <utility> from flatbuffers.h, but that was it.. ",
    "YuryBandarchuk16": "I signed it!\n. @Lakedaemon thanks.\nOoops, I'm sorry\nFixed.\n. ",
    "albertofem": "I signed it!\n. ",
    "hshankar": "Yeah that's what I ended up doing. But that will cause an additional vtable lookup which I wanted to avoid. Can't use structs in unions either.\n. Ok, here's my use case:\nunion Bar {\n  bar1,\n  bar2\n}\ntable foo {\n  barType:\n\n}\nBasically I need to use the type of the union in foo for some other purpose. I can access the enum in code as Bar.bar1 but there is no way to say that a field represent the enum type corresponding to a union. I have to now create a separate enum for this, and keeping the enum and the union in sync is a pain. I am not sure how common is this use case though. I am also trying to work around this through better designing my application API so that this isn't needed.\n. Ok, here is a contrived example:\nSay there is a union called VehicleData, which is\nunion VehicleData {\n  CarData,\n  BusData\n}\ntable Vehicles {\n  vehicleId: int,\n  vehicleData: VehicleData\n}\nLet's say we store Vehicles in some file, and I want to define schema for a query language which can query to get all Vehicles that are cars. So the schema for the query might be something like:\ntable Query {\n  vehicleType: VehicleDataType\n}\nVehicleDataType and VehicleData have a one-to-one mapping, so if the enum was exposed, I could have used it directly.\n. There are several use cases for this. My particular use case is to convert from another serialization format to flatbuffers. Currently, the only way seems to be to use reflection to iterate over the static bytes which is ugly and seems unnecessary.\n. Making names public will be a change we won't be able to undo without breaking compatibility. I don't think it is a good idea. Why not have an internal Map? Is it because the codegen is shared between languages and hard to extract out for Java? Or some other reason?\n. It won't break anything today, but it will be impossible to reduce access in future. If we expose the String array, it is not safe in the sense someone could mess around with the values in the array. So it is better to not expose it directly. Once it is exposed, it cannot be made private.\nIn Java we can do fancier things. Like have an actual enum instead of static bytes. Enums are very efficient in Java. Inside enums, you could have mapping from enum -> byte (or int or whatever is specified in the schema) and validation. But I guess that is too specific to Java and we can't change it now anyway.\nString->enum is a functionality which is very widely used in Java at least in my experience and it would be good to have it implemented efficiently. Map will use some memory, but it will be just once per enum type so I guess it's not a big deal. If you are concerned about memory, it can be implemented as a linear search in the array for now. That way, the possibility of a future Map implementation remains open.\n. Interesting! I did not know that it can bloat it so much. To be fair though, the example in the app is probably an extreme case, and Enums in java really are much more powerful than a fancy way to refer to integers.\nHow about the other suggestion of introducing a valueOf() method with linear search?\n. Coming back to this discussion, I am trying to convert an application that used avro to use flatbuffers, and I have already run into a bunch of issues because of lack of type safety while using bytes. I feel like optimizing for Android is like optimizing for an single use case. \nThe type safety and general usefulness of java enums is great. We could add a flag to the code generator to generate java enums for use cases that don't care about the slight static memory increase due to use of enums. (e.g, server side use cases like mine). What do you think about that?\n. The FlatBar approach is what I ended up doing. Adds the cost of an additional vtable lookup though. And unnecessary [byte]-only tables.\n. ",
    "love-ginger": "OK, I already signed it.\n. ",
    "jonathantullett": "PR created: https://github.com/google/flatbuffers/pull/3931\n. ",
    "llchan": "I'd rather not have to do something like this lol:\npython\nVT_FOO = int(re.search(r'self._tab.Offset\\((\\d+)\\)', inspect.getsource(Test.Foo)).group(1))\n. I agree that the byte array accessor should be a generated piece of code (maybe a PR later).  I was just doing some quick and dirty testing outside of the flatbuffers source (generated or otherwise) and found that I needed those offsets to experiment with the data.\n. Yeah, constants per generated table.  I'd probably put it inside the class rather than global in the module.  I'd probably also call them VT_FOO to match the C++ naming convention?\nFwiw I wound up using C++ with Cython bindings, so I dont really need this anymore, but I still think it'd be good to add.\n. I should add that the behavior implied with the proposed change is that if the value goes from a non-default value back to the default, it should modify the existing field rather than invalidate the operation. The assumption is that mutation is used when you want to maximize reuse, so it should invalidate as infrequently as possible.\n. I signed it!. Update: I saw that flatbuffers.h had IsScalar/IsInteger/IsFloat helpers, so I added the equivalents for the reflection types, and I updated the code to use them.. My current workaround is to do it directly with this helper:\nc++\ntemplate <class T>\nstatic inline void forceAddElement(fb::FlatBufferBuilder& fbb, fb::voffset_t field, T e)\n{\n  auto off = fbb.PushElement(e);\n  fbb.TrackField(field, off);\n}\nThis was lifted straight from flatbuffers.h, just with the default check removed.\nUsage looks like this, which is fine, but I'd rather not have to know about the vtable constants:\nc++\nforceAddElement(buf, Foo::VT_XYZ, 123);. I'm aware of that, but that applies at the buffer level. I'd like a little more granularity, because maybe certain fields should force defaults but not others. Seems pretty harmless to expose through the add functions unless I'm missing something? With a default argument of false, it would be a backwards-compatible change. If youre opposed to the change that's okay, I'm just curious why.\nSidenote: yes, some of this type of behavior can be achieved by converting some of these fields to structs and restructuring the messages, but that breaks backwards compatibility and I'm looking for a more incremental builder-side change that requires no client changes.. Maybe I'm missing something, but I think the generated code would be pretty minimally-different:\n```c++\n// before\nvoid add_id(uint32_t id) {\n  fbb_.AddElement(StringId::VT_ID, id, 0);\n}\n// after\nvoid add_id(uint32_t id, bool force=false) {\n  fbb_.AddElement(StringId::VT_ID, id, 0, force);\n}\n```\nFurthermore, I suspect that because these are all inline-able the compiler would optimize that flag away entirely in the default case (I havent verified this), so the performance cost should be small/zero.\nModifying the buffer-wide force defaults is a possible solution, and I do like that better than my current workaround because it doesnt involve vtable constants, but it involves unnecessary writes to the flag in memory rather than a flag in the call args that's likely to be optimized out anyways.. Fair enough, your call. There are straightforward workarounds so nbd :). Just a note about this PR: I don't think these are changes for 1.3+, they look like they are based on gRPC master (presumably will land in 1.4.x).\nFor example, the Create static func doesn't exist yet in the latest gRPC release (1.3.2):\nhttps://github.com/grpc/grpc/blob/v1.3.2/include/grpc%2B%2B/impl/codegen/async_unary_call.h\nHowever, it is present in master:\nhttps://github.com/grpc/grpc/blob/master/include/grpc++/impl/codegen/async_unary_call.h\nNot a big deal, just hit some unexpected errors after rebasing. I'll upgrade my gRPC to a dev build.. @per-gron Don't worry about it! I was just mentioning it so that maintainers are aware. We should be sure not to cut a release until the corresponding gRPC release is out. Down the road we may want some sort of flatc command line flag to specify gRPC version, but that's prob overkill for now.. The ideas described here have been merged into master :). Thanks for the comments. I'll respond inline.\nNote that I had some issues with the allocator as it's implemented right now, and sent over another PR #4312 to address those limitations/bugs. I'll rebase this PR once that stuff has been reviewed.. Disregard the many commits above, I rebased off my other PR to get some changes tested. Once that's merged this will be a single-commit PR.. Regarding the comment about inheriting from FlatBufferBuilder: I generally do agree with the composition over inheritance mantra, but in this case I wanted the existing generated code to work with it without changes, and the generated builders operate on FlatBufferBuilder references. I also didn't want to have to forward every method, so instead I let the inheritance take care of that.\nYou also mentioned templates, and in fact my original implementation did exactly that. I templated vector_downward and FlatBufferBuilder, very similar to what you proposed. However, @aardappel felt like this would be too large of a change, so as requested I modified the changes to rely on inheritance instead.. It's always a tradeoff with templates, and I understand the desire for simplicity. I don't feel strongly enough to push for it, especially since it's not a blocking issue and the inheritance approach is also working (albeit with slightly less type enforcing at compile-time).\nI've rebased this branch off master and updated things as mentioned above.. I've also added a vector_downward buffer accessor so that we don't need FlatBufferBuilder members to be protected for MessageBuilder to see them. This addresses @per-gron's comment about the added maintenance cost of protected members.. @per-gron yeah that might be handy to save the user a check in every rpc call. How would it look? Would we want flatbuffers::grpc to have its own serialization traits? We could do something like this in the flatbuffers::grpc namespace, though maybe a different name might be better:\n```c++\n// default is to verify\ntemplate \nclass SerializationTraits {\n  static constexpr bool verify = true;\n};\n// user can turn it off if they know they are dealing with trusted sources\ntemplate <>\nclass SerializationTraits {\n  static constexpr bool verify = false;\n};\n```. Actually maybe something better would be something like this:\nc++\ntemplate <class T>\nclass MessageVerifier {\n  static inline bool Verify(const Message<T>& msg) const {\n    // the usual verify code\n  }\n};\nThe user can then choose to stub it out with a no-op, they could use the default verifier, and they can even add additional checks (e.g. if fields have constraints not expressed in the flatbuffers schema).. @per-gron I've added message-verification-by-default in the latest commit. There is a fundamental limitation with the current traits-based toggling: for a given type, you can only really specify a single type of verification behavior. That is, you can't have two different rpc methods accepting a Monster, and have one verify the message but not the other. This is not really a show-stopper, since it should be possible to work around this with some wrapper types, but just something to be aware of.\nI'm open to other implementation ideas if you have any.. Interesting idea to put it in the schema for safer guarantees, but I'm not too familiar with that part of the code so I'll defer to another contributor/maintainer.. @aardappel any comments on the current state of this PR? I'm happy to iterate on this branch, but it might benefit from extra time/visibility on master before we cut the next release. We should also add some docs, but I think that can be a separate issue/PR.. I don't have a preference regarding the default verification behavior, since it's easy to override with MessageVerifier<T> specializations.\nThe latest commit disables it by default, adds a preprocessor flag that can enable it by default, and adds a helper function to Message<T> so specializations can easily enable it on a type-by-type basis.. Based on the discussion in the issue and comments here, I think the consensus is that the gRPC verifier should at least run the basic verification by default to ensure that message access will be safe.\nI'd like to do docs in separate PR since this one is already getting fairly lengthy, but I will def include this in the docs.. I've made the requested changes, currently working on merging some base.h changes that were introduced in master.. Okay merge is done. Most of the changes in the last iteration were fairly minor/trivial. The only one that changes behavior materially is the change to make vector_downward uniformly lazy for all code paths: ctor, clear, and reset. It will allocate on the first write i.e. first make_space(..) call, making builders dynamic-allocation-free to instantiate.. I added some example code + simple docs (and fixed a builder reuse bug I found while doing so). My priority is to get this PR merged in, so if the docs are not up to par and you don't want them on master, I can rip them out and we can deal with them in a separate PR.\nJust thought I'd include a first pass at it since people sometimes like changes to be accompanied with the corresponding doc changes in the same PR.. Yep, it needs master or the 1.4.0 pre-release. For what it's worth, that requirement was already introduced into master before this PR, but doesn't hurt to mention it here too.. Something that's fundamentally different about vector_downard vs std::vector is that it's not copy constructible, which means there's less stuff to worry about with respect to the allocator (i.e. stateful ones can just prohibit copy construction).\nSo the interface that's exposed will be slightly different from the std::vector one, taking in a rvalue reference rather than const reference for the default allocator.. Ah derp, I should have searched before I opened this.\nYeah I suspected you may not want a templated FlatBufferBuilder, but figured I'd go for the long shot first ;). We can probably achieve a similar thing with interfaces, I just proposed templates first because it feels more natural coming from stdlib. The important nugget in this PR is that vector_downward owns the allocator and can move it into the deleter, which is doable with both approaches.\nLet me rework and update this. I hope to keep this active and pushed unlike the previous PRs that went stale.... I'm going to squash commit and re-push so that the commits are cleaner. Let me know if we need the history for any reason.. PR updated. I think I've addressed the above points. There are no longer templates nor any std::allocators, it's all flatbuffers-specific. Most existing allocator overrides in the wild should work with fairly minimal changes:\n- simple_allocator base -> Allocator interface\n- FlatBufferBuilder ctor takes in a std::unique_ptr<Allocator> to express the fact that the underlying vector/buffer owns the allocator. Leaving it as a raw pointer could be misleading and cause that pointer to get pushed into more than one unique_ptr, leading to bad things.. I've updated this PR with an inheritance-based approach. The important idea behind this remains the same: that the vector should own the allocator and be able to move it into the deleter when the buffer is released. The implementation is a little more verbose with the std::unique_ptr<Allocator>s, but without templates I think it's necessary to store a pointer to an external allocator. It's probably possible to do it with raw pointers too if anyone cares, but seems safer to use unique_ptr.. Does anyone have access to a windows box with MSVC 16 to help me debug some remaining issues? There's probably a simple fix, but using commit+push+appveyer is a terrible workflow and I am calling it quits for now :). @aardappel appreciate the feedback on this. I hope it's not too much noise seeing all my work-in-progress commits, but I think we are making good progress and I hope the end result will be a more flexible + correct allocator interface.. Also in general, if we are committed to supporting c++98, can we add it to the CI configs? Last I saw from skimming issues it seemed like there still an open discussion and there hadn't been a final say about whether we wanted to support it, or if people should use flatcc for older compilers.\nFwiw my vote is for c++11 and best-effort support for older standards (e.g. dropping the unique_ptr stuff for c++98), rather than coding everything to the oldest standard.. Ah got it. So the compiler/std is not 98, it's just that the STL implementation might not support the newer language features.\nWith the latest commit, we are finally passing the MSVC 16 tests now. A few notable changes since you last reviewed:\n- Since unique_ptr_t has a custom deleter that is stateful and unsafe to mix with anything outside of flatbuffers, I actually opted to make a DetachedBuffer class to serve that role of transfering a buffer outside of a FlatBufferBuilder/vector_downward. This means everything shares the same implementation without the compiler-specific overrides, and I think some of the CPP98_STL-deprecated sections are no longer necessary. This fixes both VS2010 and CPP98_STL builds, I believe.\n- I also removed std::unique_ptr<Allocator> uses, and replaced them with raw pointers with a steal flag. By default, the allocator will be borrowed, matching the previous behavior.\n- Introduced a new AllocatorProxy, which is just a non-owning proxy to an allocator. This is useful if the user wants to share some externally managed allocator.\n- I add some deprecated typedefs and stubs (unique_ptr_t and simple_allocator), so that users can transition a little more gradually if they would like.. @aardappel see what you think of the latest state of the PR. I think I've addressed most of the above points in a way that's  friendly to older compilers/stls.. Let me take a step back... what's the use case for release()? Are users intended to actually use the data pointer there? They don't have a size, so seems like the use case for it is pretty limited, unless I'm missing something. Do we actually just want move ctor/assignment for FlatBufferBuilder so they can move the buffer around?. How would they use the buffer though, without an associated size? It seems that the DetachedBuffer class does what we want, and using unique_ptr for it is kind of incorrect, because it doesnt act like a normal unique_ptr (e.g. no reset to another pointer).. Sorry couldnt work on this yesterday, just made some changes now. Some notes:\n- Deprecations have been removed, and we are aggressively breaking the old simple_allocator API on purpose.\n- There is now a static DefaultAllocator that everyone can reference without a dynamic memory allocation.\n- There is now a new bool boolean flag indicating whether the builder/vector/buffer owns the allocator and should free it, and this is transferred as necessary. This eliminates the AllocatorProxy class and eliminates heap allocations for the common (i.e. non-owning) case.\n- The more I look at it, the more I think we should be precise about what release() or ReleaseBufferPointer() returns. For example, in the test.cpp, the tests get the pointer from the returned flatbuf, but since it doesnt have a size, it gets the size from a string from a different test. With this change, the returned flatbuf is self-contained and has enough information to safely use the buffer, and usage should be cleaner. The tests have been updated to illustrate this.\n- The old DetachedBuffer is now called FlatBuffer. The chain of thought is that a FlatBufferBuilder builds the message, and when you call Release(), it gives you a finished FlatBuffer that now owns the memory.\n- I've removed the reset(...) method that isn't used here locally. Will include with the gRPC PR as necessary.. Also, I'd like for the FlatBuffer class to have a Verify<T>() and GetRoot<T>(), so that a released FlatBuffer can be used easily. However, this would require some fairly aggressive reordering in the header file, so I will defer that for later.\nAs a general question, is there a reason to put everything in one header, as opposed to smaller ones that flatbuffers.h includes? It generates a huge diff here if I move anything around, and we could avoid that if things were split up.. Great! :)\nDo you intend to bump the major version number, or is this fringe enough to stay on 1.x.x? It isn't really backwards compatible, so technically it should be a 2.x.x release. If we are going that route, I'd say let's hold off a bit and get the breaking gRPC changes in there as well.. Sorry for not noticing this earlier, I only really subscribe to my own issues/PRs.\nI had the same issues with the BufferRef<T>, and agree that there are some unsafe assumptions about its usage. I already have a PR #4310 going that will give the gRPC generator/API a facelift. It's currently pending another allocator-related PR #4312 that I'm iterating with @aardappel to polish and merge, but I think we can get that squared away soon.\nThe main points of the gRPC PR are as follows:\n- We create a custom allocator that's backed by grpc_slice objects (refcounted/thread-safe memory buffers that gRPC knows how to handle). In addition to fixing the memory ownership issues you pointed out, it also allows us to transfer ownership to the gRPC ecosystem without copying. The previous model would copy when serializing into gRPC buffers, and if there were an N-way broadcast it would do N copies. By using gRPC buffers under the hood, we can avoid all of these copies and write directly into gRPC-aware buffers. We've had someone from the gRPC team skim the code and it seems to be correct, but if you are familiar I'd love to get more eyes on it.\n- BufferRef<T> is basically removed, in favor of a flatbuffers::grpc::Message<T> which has more book-keeping surrounding the underlying grpc_slice. I still need to document some of the changes, but I tried to make it fairly intuitive and it should be easy to figure out.\n- There will be a flatbuffers::grpc::MessageBuilder which is nothing more than a FlatBufferBuilder with the gRPC allocator baked in, and a few Message<T> helpers.\nAnyways, that PR will be a breaking API change for gRPC code, so if you have opinions on the API, I would certainly welcome feedback. It's not in its final state because I periodically rebase it on the other PR that will be merged in soon, but the gist of it is there.. One potential consideration is random access of a large, trusted, mmap'd file, where verification would require touching the whole file. The cost there could be quite high compared to the direct access.\nI don't think we should change the existing behavior, because people have already written their code with the current assumptions. Would it be sufficient to add a GetVerifiedRoot<T>(...) that basically does GetRoot<T>(...) + Verify<T>(...)?. Hah, sorry @per-gron, I must have been tired when I replied, totally misread and thought you meant both gRPC + non-gRPC. If we're talking gRPC only, I agree that the cost will be fairly small since it should be hot in cache/memory.\nI also lean towards enabling verification by default, with opt-out via a preproessor flag or type-specific specialization. Even on a trusted network, someone could make a mistake and send the wrong message type and cause the server process to blow up.. I like the operator overloads for convenience :)  Not sure why I didn't put them there in the first place; now that I look at it, the GetRoot() calls are just unnecessary noise. I'm happy with that change if the maintainers agree.\nAs for the copy/assignment, I originally had those in there pretty much exactly the same way, but removed them to audit my implementation and make sure there weren't any unnecessary refcount changes happening in the core library code (they have nonzero cost since the refcounts are threadsafe, but yeah probably not a big deal). I left it deleted in the end because I hadn't confirmed some gRPC behavior yet.\n\nFor \"outbound\" response messages, having the copy/assign are helpful in simplifying message reuse for e.g. broadcasts or static responses, and I like the idea of having them for that. Fwiw, you can currently sort of do it with BorrowSlice(), but it's pretty ugly.\nFor \"inbound\" request messages though, I wasn't sure if gRPC is allowed to reuse the memory buffers, in which case saving a shared ref to that memory past the lifetime of the rpc function isn't safe. If they allocate fresh each time, it would be totally fine to save the reference. We just need to look at the code and check, though it makes me slightly uneasy that it's tied to their implementation and not statically checked. I kind of wish they had a distinction between a \"regular mutable slice\" and \"slice with mutable refcount but const data\".. Regarding the operator->(), I think we should reconsider, as it helps remove needless noise from the rpc implementations. As it is right now, you need to have both a Message<T> *request_msg and a T *request, and then access your fields, which isn't very clear to a newcomer and just adds boilerplate everywhere.\n\nIf runtime cost is a concern, could we save the T * to a member of Message<T> at deserialization time? That way it's basically just a pointer access and the shared_ptr<T> analogy is closer. In that case the compiler will almost certainly find the common subexpressions and merge them (I've tested this for some example code by inspecting the generated assembly).. It's important to note that a Message<T> does not grow in size or get reallocated willy-nilly, so it's not really a buffer in the way that the builders/vectors are. It's just an owned piece of memory + a pointer to an address within that memory.\nA shared_ptr<T> and a T* are fundamentally different concepts too (also memory owner + pointer), but users understand that the operators exist and that they translate between the two types. By your argument, shared_ptr<T> should also remove the operator overrides and require access via shared_ptr<T>::get(). If you think those operators should exist, I think this discussion about Message<T> is just a matter of familiarity. If you disagree with the shared_ptr<T> operators, then I can at least understand the motivation (there is an argument for explicit calls vs implicit magic). I just think the two views should be consistent.. Btw, I just want to be clear that I'm not trying to be difficult or argumentative, just trying to help make the interface as good as possible :)\nIf you feel very strongly about it we can def drop it.. To the end user though, a Message<T> is a shared_ptr<T> or T *. The fact that it resides in a memory block starting at an earlier address isn't really relevant to a message consumer, and in fact it could be argued that all the grpc_slice/refcounting details should be hidden from the public. The way I'm thinking of the associated grpc_slice is analogous to the malloc chunk headers. The payload is just the T *. There's extra metadata outside the region to manage memory lifetime, but this is invisible to the user.\nI'm not saying that we should change FlatBufferBuilder or anything like that. The fact that Message<T> is a finished T * with refcounts to deallocate automatically makes it sound just like a shared_ptr<T>.. Fair enough :)  Let's drop it then, not a big deal.. Good catch, thanks!. I am not aware of any bugs, but I haven't run it through address sanitizer or memory check tools, so there very well may be things I overlooked. My testing only consisted of logging the ref/unref calls and checking that they paired up, but this was a manual process and did not have full code coverage (e.g. I didnt use move assignment on a non-empty message). I'll give it a more in-depth look. Would you mind pasting your example in a gist so I can reproduce?. Yeah, the initial_size var was originally a uoffset_t, and we updated it to a size_t to eliminate some casts. Must have missed this. I agree with making capacity() return a size_t rather than casting.. I'll take a look, but my first pass is hitting some speed bumps: no docker on my box, and gcc-4.8 means i cant use -fsanitize=leak, but I see some probably-related leaks using valgrind memcheck. Have you tried the same example using protobufs instead of flatbuffers?. Yeah, I have it building with the Makefile + local mods, but I'm on gcc-4.8 so I have to use valgrind instead of the gcc one.. Cross referencing the original serialization traits in grpc, looks like we may be transferred ownership of the grpc_byte_buffer *buffer, rather than receiving it as a borrowed ref. This means we need to destroy it at the end of the deserialize func. Let me make some changes and see if that fixes things.. Was able to get docker set up on my box and could reproduce the memory leak in your example. I think the PR above fixes it, could you give it a go?. Btw, not sure how close this code is to your actual benchmark, but a few things to note:\n\nIf you have a good guess of the size of the message, you can save on some allocations by setting the MessageBuilder initial size. Protobufs (I think) can write to segmented buffers, so appends are relatively cheap, whereas flatbuffers require a single contiguous buffer, so appends can cause fairly large reallocs/memcpys. I tried a MessageBuilder builder(sizeof(float) * NUM_VALUES + 1024), but you can probably figure out a closer bound.\nYou can also builder.CreateVector(data->values()->data(), NUM_VALUES) and skip the copy to parameters, though maybe you actually need to copy and this is just an artifact of the stripped down example.\n\nWith those two changes I see a ~2.2x increase in throughput.. Nice find. I agree we should pay the readability cost to make this as optimal as possible on little-endian platforms.\nBtw, I remember seeing this a while back:\nhttps://github.com/thekvs/cpp-serializers\nWhen I saw it, I was a little confused why capnproto was so different from flatbuffers, but I didn't dig into it at all.  I suspect this exact ~bug~ overhead is contributing to the performance difference, since the benchmark is creating a vector of longs and a vector of strings over and over.. I see, so if there's a static that's created before the allocator singleton is created, and that static holds on to some FlatbufferBuilders, the order of destruction can be messed up on teardown. I think we can fix this, will have to see what the best way would be. I don't have time this week but can get to it next week.\n@exhau As a workaround for now, you can construct a static DefaultAllocator instance before the builderPtr, and pass that in to the FlatBufferBuilder ctor, that way you have more control over the destruction order. Alternatively, I think you can explicitly call DefaultAllocator::instance() before the builderPtr is constructed, and the unwinding at the end will do it in the right order.. There are a number of ways to resolve this, does anyone have opinions? Probably the simplest would be to allocate the default allocator on the heap and never delete it.. Yeah that's my guess too, that the vtable is getting clobbered.\n@aardappel just to be clear, the heap-allocated default allocator I mentioned above would still be a singleton, so there would be no additional allocation at runtime, other than the first call. As a quick skeleton:\nc++\nstatic DefaultAllocator &instance() {\n  static DefaultAllocator *inst = nullptr;\n  if (UNLIKELY(inst == nullptr)) {\n    inst = new DefaultAllocator();\n  }\n  return *inst;\n}\nSeems like a simpler diff, no FlatBufferBuilder memory layout change, and the nullptr branch should be perfectly predicted so the runtime cost should be pretty negligible.\nWhat do you think?. @aardappel I had suspected you'd respond with a \"leaks are no go, even if tiny and intentional\" response but thought I'd test the waters :)\nGiven what I've heard so far, the simplest options are:\n1. Add a DefaultAllocator member to every vector_downward+DetachedBuffer. This changes memory layout and wastes a bit of memory, but probably takes the least amount of effort.\n2. Have a nullptr allocator indicate that it should use the default allocator behavior. This doesnt change memory layout or APIs, but requires code changes to a number of places in the vector/buffer code.\n3. Use std::shared_ptr<Allocator>. This changes memory layout, and possibly adds some additional runtime cost for refcounts, but is semantically the most correct.  IIRC, we didn't do this in the first place due to some STL port issues, but I am not familar enough to remember off hand.. Hm I haven't looked at this in a while, looks like the PR with the fix is still sitting there. Let me dust it off and see if we can get it merged in.. I don't have a strong preference w.r.t. this change (I didn't write this particular block, for what it's worth). Technically it's valid standards-wise to just swap (or even copy, for that matter), but I see your point and don't see any harm in destroying the old one immediately on the move. I think this is a good change overall, minor comments on the PR.. That would work yeah, but are we worried about that being brittle if the enum values change? If that's something we don't care about, I'm okay with either implementation. Imho the switch makes the intent more clear, but nbd either way.. Nevermind, just realized those enum values are generated from the reflection.fbs, and if we're following the normal schema upgrade conventions those shouldnt be changed.\nI'll update PR.. I put that in there in case we ever add any scalars that aren't ints/floats, but if we're good with the the assumption that scalar types arent going to be added without changes here I can turn it into an assert.. Yes, I agree that we should beef up documentation on this. I'll contribute what I can with this PR, but we'll probably have to keep expanding over time. I hope to see gRPC + flatbuffers as well documented as the default protobuf usage.. Yep, the ownership is a little incorrect in this PR, actually. Turns out the previous deleter was copying the allocator via copy ctor (notice it was binding to allocator_ and not &allocator_), so the old code was safe. SliceAllocator in this PR is not copy-constructible because it's stateful, so the old code wasn't working and I slapped the lambda on it, but that's not really right either.\nThe other PR I just sent #4312 fixes this for both scenarios. Once that's addressed I'll rebase this PR.. Do you know someone who you could pull in to the review this? I'd like to make sure it's correct, and I don't trust my \"skim the grpc code for an afternoon\" level of understanding.. Noted. I'll update these.. Got it, I'll assert and return a nullptr.. Yeah, I can document this inline with the code. Essentially this extracts a subslice of a grpc_slice that shares the refcount, so that we don't need to make a copy to reference the message we are sending out.. It's only intended to be used by flatbuffer's internal vector_downward, and there there is one-to-one correspondence between them (i.e. a single vector owner per allocator) due to the copy ctors being disabled. It needs two because reallocate has one in flight while it does the memcpy. \nIn theory the allocator should only need one grpc_slice, and if we are up for changing the implementation a bit, it would simplify things to shift that into a reallocate func on the allocator that takes care of the memcpy and such. Let me get some of the other PR's sorted out and I'll update this one.\nIf we are worried about people seeing/using the allocator without knowing its specific limitations, we could plop it into a flatbuffers::grpc::details namespace or something, or disable the ctor outside of predefined friend classes.. Yep, this is just doing the private declare for older compilers/stds. I think erring on the side of making things too private is better than pushing up to public by accident (in case the macro is used in a private/protected section), so let's leave it as is.. reallocate_downward is actually overridden in the gRPC allocator, which is why I added this method. By letting the allocator handle the memcpy, the allocator no longer needs to have up to 2 allocate regions outstanding, and can just do the temp work in the function scope while it's reallocating. If you compare that PR's old/new impl you'll see that it's simplified significantly.. As for why we don't merge Allocator and DefaultAllocator... I suppose that's just a personal preference to keep interfaces fairly abstract and implementation-less. If you have a strong preference I'm fine with either.. Noted, will make sure these are disabled by that flag.. Good catch. I've been using clang-format and I am trying to match convention, prob just missed that line.. Yeah I wasn't really intending for this to get merged, I just have no way of compiling MSVC other than pushing to this branch, so you're just seeing my experiments to see what the heck is wrong with MSVC.\nOn a more general note, I agree that we could probably refactor some of the compiler config checks into a standalone header (stuff like the final/override macros).. Yeah, I considered doing that, but it seems fairly dangerous. Maybe we could do something like what gRPC does with their slice wrappers, where they require the user to specify the ownerhsip transfer explicitly. Mocked off the top of my head here:\n```c++ \n// The unique_ptr based implementation would be enabled where available\nifndef FLATBUFFERS_CPP98_STL\nFlatBufferBuilder(uoffset_t, std::unique_ptr&);\nendif\n// Raw pointer-based implementations would require the user to be explicit about\n// how ownership is transferred\nenum StealAllocator { STEAL_ALLOCATOR };\nFlatBufferBuilder(uoffset_t, Allocator*, StealAllocator);\nenum BorrowAllocator { BORROW_ALLOCATOR };\nFlatBufferBuilder(uoffset_t, Allocator*, BorrowAllocator);\n```\nThis hints at something else I was going to add to this allocator PR, which is the ability to use shared allocators that are explictly not owned by the vector. The plan was to add a proxy object that relays calls to the shared allocator. The use case might be memory pools and such, and I want to make sure we can support things like that.. Okay, I just thought it'd make upgrades easier if there was a compiler warning/error telling you what changed. But we can certainly axe things aggressively instead.. Same as above, I was hoping to make upgrades more gradual by giving a deprecation rather than removing it entirely, but I'm fine with nixing it if we don't need that.. The proxy is a non-owning reference to an allocator. An example would be a memory pool allocator that should be shared across many buffer builders, which means that the vectors should not free that allocator. Seemed like a simpler solution than a refcounted solution, but maybe there's something in between. I'll see what alternatives there are.. This wasn't just for CPP98_STL, it also fixed MSVC 16, which appears to have a wonky unique_ptr implementation. If we're fine with dropping release() for that I don't mind using unique_ptr. However, for what it's worth, the deleter involved with the unique_ptr is effectively the same thing as this, so it wouldn't save that much code. Plus it's not truly a unique_ptr anyways (you can't swap in another pointer and expect the allocator not to blow up), so I kind of feel like we should have a dedicated class for it.. It's used in the other gRPC-related PR. When a message is popped out of a builder, we want to reset the vector with an allocator so it remains valid. Otherwise as soon as we pop out the message, the vector/builder is a landmine with nullptrs inside of it. I can take this out and rework that if necessary.. This was introduced when we decided we didn't want templates and went to inheritance, which means an extra level of indirection, and we can't store the allocator as a member. I can modify the default case to point to a static DefaultAllocator since it's stateless.. Templates would really improve a lot of this as far as efficiency goes, and with the right typedefs it'd be invisible to users who don't define custom allocators... might be worth reconsidering :). Renamed back to DetachedBuffer. What's the appropriate ifdef to use for it? I'm happy to block it out if you want. I can also add a swap() method and default ctor so people on older compilers can at least do a manual move it they need to.\nI thought it was best to omit the unique_ptr_t typedef since it doesn't really behave like a std::unique_ptr<T> and might be confusing for newcomers trying to use it as such.. Got it. Updated my clang-format rules too.. Updated my clang-format rules for this. Not sure why you guys like that, the diffs are cleaner and it's easier to read with them split onto separate lines, but I'll follow project conventions :). I just checked, and the interwebs seems to be telling me that rvalue refs were added in VS2010/MSVC16 and GCC 4.3. Since this a subset of the C++11 preprocessor check at the top of the file, I think we can leave the move ctor as is.. Yeah, you can get to a mutable buffer via BorrowSlice(), so Message<T> isn't really a bulletproof immutable object. I'll have to poke around to see if there's a way to make a grpc_slice have a const buffer but a non-const refcount. If we're worried about BorrowSlice() being exposed, I could probably do something with private + friend class/func where it's used internally, but generally I prefer to let the users shoot themselves in the foot rather than having them frustrated that they can't.\nThere is a use case for a non-const accessors, since flatbuffers support mutations (and I have used it in my code before, so it's not just an unused feature). But for something like that I'd probably add a GetMutableRoot() func to make it explicit.. Will do, thanks for the suggestion. Fixed. Unfortunately the structs in the union have different layouts (i.e. length is at different places). I'm guessing they did this deliberately either for optimization or to prevent people from making the assumption that they can rely on a consistent layout.\nOr, maybe I'm misunderstanding your suggestion?. I'd personally prefer to avoid the extra verbosity, because it will just be an extra line of boilerplate in every single RPC func. No one really sees this implementation detail, so maybe it's not so bad?. Sure, I can remove them, just a habit of mine to spell it out when it's in a header :). Can do. Was only included for readability.. Is there a rule for when to use auto? Everything inside a function definition?. Yeah, I wasn't totally certain what code is appropriate for a deserialization error, if you could ask gRPC folks that might be helpful to get the best practice code to return.\nI don't think the gRPC server needs to do anything with the error code. I'm not positive on this, but I think when gRPC goes to deserialize and detects an error return, it won't even call the RPC func at all, so the user shouldn't need to worry about it.\nClient code might have to handle it, but if they're sending over malformed messages there's something wrong anyways.. It may be a bit overkill, but if someone wants to be safe, the preprocessor flag is a much better guarantee than making sure they specialize every type (not to mention less code).  I agree we will want this documented.. I'm taking a look at this now, it might be a vestige of some older impl details with an older iteration of the allocator API.. This was added primarily as a response to @per-gron's concern that protected inheritance can lead to implementation coupling that isn't necessarily obvious from the public interface. I don't have a preference either way, so I'll change this to the original protected access mode impl.. I based this on a grpc_slice wrapper I saw in the gRPC code. One benefit that I could see is that it makes the calls more readable (i.e. you see ADD_REF rather than true), but could also be done with a single enum type. I suppose making it two distinct types means they can have to completely different impls for the two ctors.\nIn any case, the ctors here are trivial so I'm happy to make it a bool.. Probably because I wrote the message wrapper before I discovered those macros :). Okay I took a look at this and it was indeed an artifact of some older code. It wasn't actually performing a dynamic allocation so it wasn't that bad, but it was setting some allocator pointers that didn't need to be set (they don't change).\nI've updated this to just be a plain reset() func that deallocates the old buffer and reallocates a new one with a capacity based on the original initial_size.. Maybe it could also be INVALID_ARGUMENT. When I read through the descriptions originally, I interpreted that code as a way for the rpc call itself to indicate that one of the arguments inside the request was invalid, not that the request was corrupted/unparseable.\nDATA_LOSS's description was \"Unrecoverable data loss or corruption.\", so I originally used that.\nI'm fine with any suggestion, just lmk.. Do we care about allowing per-type specialization? I left that as an option for flexibility, but maybe that's an unlikely use case, and we can make the assumption that each translation unit either wants verification globally or not.. Can do. Static inline func sound good?. Ah, you were talking about buffer allocation, not allocator allocation (at some point we had heap-allocated allocators, and i thought you were referring to those). I can make that lazy, yes.. None of the code as written should be utilizing those, and I disabled them to audit my own code and make sure we're not needlessly modifying the refcount. I think there are some memory fences or other atomics around those for thread safety, so it's not totally free to modify the refcount.\nI don't think a general use case should be making copies of the messages, and I'd like for it to be an conscious decision to enable the opt-in feature.. As for merging the two defines, I'm okay with that, doesnt matter to me.. roger. Thanks for looking that up, I agree we should try to behave like protobuf, so unless there's a strong opinion otherwise I'll update to use that.. Unless there's specifically a code added for \"parsing error\" or somesuch, I think the best we can do is check the error string, which might be a little brittle. But I can certainly add that.\nBtw, in case you didn't see, I'll be updating the error code to INTERNAL to match the protobuf gRPC behavior when the message is unparseable.. While I'm looking at this, is there a reason for clear() or the ctor to allocate? Could we also make that lazy and rely on make_space(...) for all allocations? That way an unused or cleared fbb doesnt cost any heap allocations until it's used.. I should be more precise: clear() when there is a nullptr buffer reallocates, and I think we can remove that and let make_space handle the reallocation. If the buffer already exists, we can leave it alone and reset the cur_ pointer.. Might have missed my previous response, pasting i below:\nNone of the code as written should be utilizing those, and I disabled them to audit my own code and make sure we're not needlessly modifying the refcount. I think there are some memory fences or other atomics around those for thread safety, so it's not totally free to modify the refcount.\nI don't think a general use case should be making copies of the messages, and I'd like for it to be an conscious decision to enable the opt-in feature.. sure thing. yep, i'll put it back inline. fine by me, making this change. Actually now that I look at this, we should just put the reserved_ == 0 case in growth_policy(...).\ninitial_size is a uoffset_t because that's how it was in the original implementation. I think it could be a size_t across the board, so I'll go ahead and do that if we don't care about the change in size/alignment.. So clang-format wants to put them back at column 0... is there a flag I need to add? More generally, could we include a .clang-format so I don't need to think about these things?. Actually if your point is that we should remove them entirely, I'm okay with that.\nI don't know for sure if gRPC buffers are reused. If they are, then messages are not safe to keep outside of the rpc function, and there's pretty much no reason to copy/assign a flatbuffers::grpc::Message<T>. @ctiller, could you comment on the gRPC buffer reuse policy just so I know?\nI think in either case I'll just remove these, since the semantics of a copy might be misleading if it's still tied to the same buffer.. Not sure why I overlooked the samples directory, maybe just a reflex since examples is more common. I'll move them into grpc/samples.. Yeah, I don't have much experience with cmake, and I don't have non-Linux platforms to test on, so I don't feel qualified to write the CMakeFile.txt for the example. I'll leave it as-is for now and someone else can update it later.. I saw a mixture of inlined and separate-lined templates in flatbuffers.h so I thought it was a \"whichever fits better on the line\" rule, but now that I look I think it's a \"inline for methods and separate-line for regular funcs\". Is that correct?\nIs there any way to express that rule in clang-format? Or do I need to manually format these? Or do you guys use a different formatter? I'm trying my best to infer conventions, but requiring contributors to get formatting details right without automated tooling is kind of asking a lot.. Sure, it doesnt matter to me. I mainly wanted it to be symmetric with the SayHello code, where the \"thing to do\" was defined in the main func and not in the GreeterClient.. Btw, this example is based off the gRPC cpp example.. It's not entirely the same when there's an existing buffer. Clear reuses, and Reset deallocates (with a lazy allocate when it's next used). This is an important distinction with the gRPC slices, where it's not safe to reuse the buffers.. The ReleaseMessage<T>() function actually detaches the message/buffer from the builder, so the builder is already cleared after each request is sent out. I can make it a local var if you prefer, I only did it this way because the grpctest implementation kept the builder in a member var.. This is used in the reallocate func, where the alignment is marked as a constexpr. I saw that there were already preprocessor checks for a constexpr fallback to const, so I thought we'd already handled portability for older compilers?. Sure, but what's the harm? We already did the preprocessor dance to make it work with older compilers?. Yep, this was a clang-format misconfiguration. Fixed.. Yep, this was a clang-format misconfiguration, fixed.. Well, this is somewhat irrelevant now that I've switched it to a std::function based on the comment below. But I still think we should provide formatting configs if we expect very specific formatting.. We could reset the vector after we clear, which would have the same end result. Un-optimized, there'd be an extra store to the cur_ pointer, but I think the compiler should be able to eliminate that first store (correct me if I'm wrong). I'll update it to do so, since either way the cost is minimal/zero.. Yeah, it should unref slice_ before the assignment, in case it's already referencing something non-empty.. Suggestions for this method:\n- Can we just make it operate on this and change the calls to other.empty_assign()?\n- If we want to keep the naming similar to STL, the closest thing would probably be std::unique_ptr<T>::release(), so my vote would be to name it release.\n- Can we make it private/protected, since it's rather unsafe for the outside world to use?. Since this may now be run outside of the dtor, I would assign nullptrs to the vars above to guarantee that stuff isn't double freed.. Oops, sorry, misread the diff in the GitHub UI, thought it was immediately under data().\nThe semantics of reset are different in my mind though: that destroys the held object vs release merely detaches it. At the end of the day I don't really care what it's called, but maybe @gwvo has an opinion.\nEDIT: removed an outdated comment. We could minimize the diff by keeping the swaps and just calling other.destroy() afterwards. For me that reads a little more directly. What do you think?. I'll re-post my comment here in case it gets lost on the outdated diff: at least for std::unique_ptr<T>, reset destroys the previous object, vs release just detaches it so someone else can manage it. I don't have a strong preference since reset can also be taken to mean \"reset my own internal state\". For what it's worth, there are other funcs in flatbuffers with release-related names.. ",
    "Rattelsnake": "Thank you for the quick reply.\nAnd... here is my version information:\ncmake version 2.8.12.2\nGNU Make 3.81\nseems pretty old!\n(insert linux newbee question here): whats the best way (or quickest) to update these tools ? \n. just fyi: i tried the following to update, but no success:\n$ sudo apt-get install cmake\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\ncmake is already the newest version.\n$ sudo apt-get install make\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\nmake is already the newest version.\n. And one other question: What version of gcc are you running?\n$ gcc --version\ngcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3\n. ",
    "atamgp": "Hi friends, this is not yet fixed with latest software of today...\nI am using entities (fb table's) in domain namespace and grpc functions in process namespace. grpc functions are using the entities from domain namespace.\nLike in the above example everything is generated but cannot compile because of the reference problem. The extra command line option --go-namespace is now available but just overrides the whole eventual go namespace generated.\nIf I use that to let both be generated in same namespace I could have just put everything in the same namespace to begin with.\nI appreciate your effort, can someone please look at this? I think it is an important must have. Thanks. I migrated my code to one namespace, \"contracts\". This works ok.\nBut I moved to proto3 because of integration needs with (any) orm. For that I need a normal generated struct with accessible fields (which proto3 generates) and custom tags (for orm) for which I'm trying the  retag plugin.\nI agree that flatbuffers is way faster and efficient but other aspects for my use case are also important such as maintainability and ease of use (in order message composition in not always easy).\n@everyone, thank you for your work here!. ",
    "AmandaRiu": "Thank you for the response. So if I'm reading our response right I'm thinking my schema file would look something like this:\n```\nenum FileType:byte {\n    IMAGE = 1,\n    HTML = 2,\n    PDF = 3,\n    VIDEO = 4\n}\nunion File { VideoFile, HtmlFile, ImageFile, PdfFile }\ntable Project {\n    id : string;\n    name : string;\n    files : [File]\n}\ntable VideoFile {\n    fileId : string;\n    fileTypeId : FileType = VIDEO;\n    path : string;\n    originalFileName : string;\n    contentBytes : long;\n    mD5Hash : string;\n    isPublished : bool;\n    createdBy : long;\n    createdDate : string;\n    modifiedBy : long;\n    modifiedDate : string;\n    maxModifiedDate : string;\n    height : short;\n    width : short;\n    durationMilliseconds : int;\n    overallBitsPerSecond : int;\n    videoFormat : string;\n    videoProfile : string;\n    videoBitsPerSecond : int;\n    audioFormat : string;\n    audioBitsPerSecond : int;\n}\ntable HtmlFile {\n    fileId : string;\n    fileTypeId : FileType = HTML;\n    path : string;\n    originalFileName : string;\n    contentBytes : long;\n    mD5Hash : string;\n    isPublished : bool;\n    createdBy : long;\n    createdDate : string;\n    modifiedBy : long;\n    modifiedDate : string;\n    maxModifiedDate : string;\n}\ntable ImageFile {\n    fileId : string;\n    fileTypeId : FileType = IMAGE;\n    path : string;\n    originalFileName : string;\n    contentBytes : long;\n    mD5Hash : string;\n    isPublished : bool;\n    createdBy : long;\n    createdDate : string;\n    modifiedBy : long;\n    modifiedDate : string;\n    maxModifiedDate : string;\n    height : short;\n    width : short;\n    durationMilliseconds : int;\n    overallBitsPerSecond : int;\n    videoFormat : string;\n    videoProfile : string;\n    videoBitsPerSecond : int;\n    audioFormat : string;\n    audioBitsPerSecond : int;\n    height : short;\n    width : short;\n}\ntable PdfFile {\n    fileId : string;\n    fileTypeId : FileType = PDF;\n    path : string;\n    originalFileName : string;\n    contentBytes : long;\n    mD5Hash : string;\n    isPublished : bool;\n    createdBy : long;\n    createdDate : string;\n    modifiedBy : long;\n    modifiedDate : string;\n    maxModifiedDate : string;\n    height : short;\n    width : short;\n    durationMilliseconds : int;\n    overallBitsPerSecond : int;\n    videoFormat : string;\n    videoProfile : string;\n    videoBitsPerSecond : int;\n    audioFormat : string;\n    audioBitsPerSecond : int;\n}\nroot_type Project\n```\nWhen I run the json file (from the original post) and the schema through flatc, how will it know to create an HtmlFile if the fileTypeId = 2. I don't see a way of adding a rule. Maybe it's just not possible and that's fine. I'm just incredibly impressed by this tool and it's ability to significantly speed up the process so I don't want to rule it out until I know for sure. \nThanks,\nAmanda \n. ",
    "rushabhy": "I signed it!\n. ",
    "wasppdotorg": "version 1.3.0\n. I see, thank you.\n. Even better, thanks. :)\n. ",
    "mobilegameserver": "pass\nAssertion failed: parser.root_struct_def_, file idl_gen_text.cpp, line 278\n. Thank you.\n. ",
    "sdeleuze": "Maven artifacts seems to be available via https://github.com/davidmoten/flatbuffers.. ",
    "dopsun": "No plan for official maven repository?. Thanks @aardappel \nAfter #3902, maven local build is already working. For the publish part, I assume this is not something volunteers can help, as this requires access to some private keys which should only be accessible by owner of group id (com.google.*)?. @aardappel This is great. Thanks!. ",
    "jchensc": "Hi @aardappel , our team has the same problem on this issue. As you know, flatbuffers is being used in a lot of projects. However, due to the lack of official maven repo for common classes (FlatBufferBuilder.java/Table.java ...), developers are copying those files into source package and packing them internally inside their jar. The problem we met is that those classes are put under the same package name. For example, in ObjectBox https://github.com/objectbox/objectbox-java/tree/master/objectbox-java/src/main/java/com/google/flatbuffers , GreenRobot copied and pasted those classes under com.google.flatbuffers package. In another case, another developer did the same thing\nhttps://github.com/velvia/filo/blob/master/flatbuffers/src/main/java/com/google/flatbuffers/FlatBufferBuilder.java. So if we want to import these two library into our project, the only way to solve the dup entries is probably using jarjar to unpack/repackage those dependencies, which is complicated and hard to maintain. Also, when we want to import a third library with the same issue, it could even be more complex. \nTherefore, it would be nice to have a maven repo so that other projects only need to take it as a transitive dependency and we could solve the conflicts easily by using gradle.. @aardappel That's amazing! Thanks a lot!. @aardappel That has been resolved in objectBox. https://github.com/objectbox/objectbox-java/commit/57de49ffefd311a96ffd9e8d058fed44ba9d7f94. ",
    "forrestthewoods": "I'm out of town until next week. Will submit a PR when I get back if you don't get to it.\n. ",
    "yakiro": "I signed it!\n. ",
    "eburkitt": "I signed Google CLA\n. My changes are to another contributor's file, but I am the author of the changes I made\n. Now that I've got past compiling flatc and the samples I see that monster_generated.h is a generated file and so my pull request is not very useful. I also see that rebuilding monster_generated.h with the current flatc also solves the problem, so I'll just leave quietly now.... :)\n. ",
    "WolfgangKluge": "flatc version 1.4.0 (Dec  2 2016)\nI ran into the same problem. Maybe it's only a misinterpretation?\nI've two files.\ne1.fbs\n```\ninclude \"e2.fbs\";\ntable Border {\n    style:BorderStyle;\n}\n```\ne2.fbs\nenum BorderStyle:short {\n    SOLID,\n    DOTTED,\n    DASHED\n}\nIf I run flatc --cpp --gen-includes e1.fbs I only get one file (e1_generated.h), the BorderStyle is referenced in there, but never declared. There's a #include \"e2_generated.h\" line, though.\nDo I miss something?. That's what I've done so far. Thanks for pointing out.\nHowever, since the order of the files is not irrelevant and already given by the include-statements it would be nice to have such an option (to generate code of all included user files).. But yes, it works well without, too  ;-). ",
    "Bklyn": "This is a really nice change (modulo the stuff that might not work for some users / older compilers).  +1\n. I do see one issue with the generated assignment operators for unions.  You must check for assignment-from-this, else you'll hit UB.\n. I think what may be going on is that since Vec3 has MANUALLY_ALIGNED_STRUCT(16), so MonsterT similarly requires 16-byte-alignment because it starts with a Vec3?\n. As to the nested tables being held by-value, I agree that this is may not be the best approach, but is holding everything by unique_ptr really any different when you factor in C++11 compilers with move semantics?  Actually, unless you have a C++11 compiler and move semantics, your vector<unique_ptr> is going to be filled with empty values if you ever resize!\nFrankly I believe I'd use the FooT objects in my applications more than the Foo derived from flatbuffer::Table.  It seems more natural to me to work with these value-types and Pack them when I'm ready to ship data out-of-process.\n. Looks really good now.  In terms of existing flatbuffers coding conventions, should these new templates be named flatbuffers::Optional and flatbuffers::OptionalTable?\n. I get that emplace_back is a performance win over push_back for the C++11 mode, but does the {} form of the constructor change anything?  In other words why bother with the added complexity of having code to generate two different forms of the constructor if there is no difference between them?\n. @bog-dan-ro range-for is a huge win for us humans, but humans aren't modifying the generated code (which @gwvo points out is not there to be pretty).  So is it worth the added complexity in the implementation to have the two different code-paths in places where there's little benefit (like quite possibly none in the compiler's output).  I'd say emplace_back could qualify as a big win, so that one makes a lot of sense.  But are all of the other output changes worth maintaining old/new C++ code branches?\n. @lp35: I think anyone who works with \"normal\" C++ objects would like to use an object interface more like @bog-dan-ro 's branch than the one on master.  But the maintainer has voiced concerns about making the code-generation tools overly complex by having a lot of different output flags.  These concerns have merit, and I think sadly this approach will not get merged.\nIts a shame, though, because the approach taken on the master branch is more-or-less unusable in real code IMHO.\n. @gwvo thanks for the recap.\nI believe ultimately an optional type (whether a custom one or std::optional if a c++17 code generator were OK) may be better than any choice of smart pointer.  Otherwise, you end up with potentially confusing semantics.  Like should operator= do a deep copy?\n- With unique_ptr it must always\n- With shared_ptr, copies may be deep or shallow; how can the user select?\nNow that I write this, unique_ptr seems less bad to me than it did just before, but deep copying requires generating the code for the copy methods, which strikes me as a bit of a code smell.  Optional gives the same sort of semantics as a pointer, and I believe all of the compiler-generated copy operations would be fine.  But the down side is sizeof(optional<T>) will typically be greater than sizeof(T*), so your objects will be much larger.\n. @gwvo I'm not sure I am parsing your question correctly.  Are you asking what would be the point?  If the objects contain unique_ptr they are not copyable.  This may make using them in application code difficult.\nThe way I think about this is that the MonsterT (i.e. NativeTable) types would be the ones I'd typically use as an application developer, and I would only build a Monster flatbuffer when I was ready to send something on the wire or persist it.  If I cannot copy MonsterTs, its a lot less useful as a type.\nThis mostly stems from the difficulty of actually creating and working with the flatbuffers::Table objects (e.g. Monster) when the objects become large or contain many or deeply-nested sub-objects.\n. @bog-dan-ro I believe it was the extra flags for C++0x/C++11 code-generation (enabling emplace_back and uniform initializers) and possibly the addition of a vtable somewhere?\n. @gwvo I see.  Yes, the code generator could emit copy constructors, assignment operators and the like.  Also, given the now-wide use of C++14 you'd want move-from and swap as well.  If all of this is necessary, this is the \"code smell\" I was referring to above.\nSo you'd be making something that acts like a value type, but that needs a lot of help to do so; this makes me think the approach is flawed.\n. @bog-dan-ro I thnik Optional or some other codegen changes are required to enable any type of NativeTable copying, not just deep copy! :-)\n. @lp35 what is the use case for dynamic_cast here?  Are you suggesting using pointer-to-NativeTable instead of pointer-to-MonsterT?  What would that accomplish?\n. Sorry was having issues with the CSL and a wrong email\nOn Wed, Jul 27, 2016, 4:31 PM Wouter van Oortmerssen\nnotifications-at-github.com |github-*| ip43tq4h7t@sneakemail.com\nwrote:\n\nThis seems like a useful change, why close it?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/3964#issuecomment-235711180,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6gK5wBotFq3tHGZDr7U1dvnX09Ftoks5qZ8AtgaJpZM4JWkp1\n.\n. Argh.  Can't get the hang of this thing!\n. Jeepers I cannot work this git thing.  Trying to make 2 separate branches for the documentaiton change and this fix to UnPack but I keep pushing to master.  Sigh.\n. I've started to add support for this to the C++ code generation, but doing this right is a bit of a heavy lift.  Needs special handling in every code generator.  I do not envy you guys!\n. I think it needs to be both places, sadly.  NaN is a funny beast which will require special handling.  For example the if (e == def && !force_defaults_) return check in AddElement.  Also the syntax for initializing a value to NaN/Inf will be language-specific.  For example in Python you'll need to use float(\"nan\"), while in C++ you'll need to use one of std::numeric_limits<float>::quiet_NaN() or possibly std::nan(nullptr) or NAN (in C++11).  Similarly (but not as hard) with Infinity.\n\nIt may be more trouble than it is worth.\n. While special values like \"nan\" and \"inf\" (basically anything strtod will\naccept I think) are supported by the parser, the generated C++ code adds an\nextra \"f\" to the value if the data type is \"float\" and not \"double\".  This\ncauses a compile error.  The generated code is OK for \"double\" fields (no\ntrailing \"f\").\nOn Tue, Dec 25, 2018 at 12:40 PM Vladimir Glavnyy\nnotifications-at-github.com |github-*| ip43tq4h7t@sneakemail.com\nwrote:\n\nProbably, both are supported in C++ parser. Could you check?\nI will be able to check it at the beginning of the next week.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/3972#issuecomment-449864937,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD6gJj5xWhbWn21nKOBOSiOlYZb1VV5ks5u8mL5gaJpZM4JYJNS\n.\n\n\n-- \nCaleb Epstein\n. Misssing \"inline\" here\n. Need \"inline\" here as well.\n. Needs \"inline\"\n. Needs \"inline\"\n. ",
    "jpivarski": "nan is a funny beast, but inf and -inf are reasonable numbers (obeying the normal rules of ==, <, >, etc.). They're just not available for integers.\nFloating point infinities are useful as an identity value for min and max operations: e.g. min(inf, x) == x just as add(0, x) == x, which allows us to apply all of these reducers (add, multiply, min, max, etc.) on the same footing. In particular, we don't have to have special cases for taking the min or max of an empty list. Just as the sum of an empty list is 0, the min of an empty list is inf.\nI wanted to make a data structure representing min and max values be insensitive to whether this is the first element to be added, so the default values for min and max should be inf and -inf, respectively. As the IDL is currently defined, I shouldn't be able to do that, but I managed it with\nc++\ntable StatisticFilter {\n  min: double = \"-Inf\";\n  max: double = \"+Inf\";\n}\nand then post-processing the Python output with replace(\"inf.0\", \"float('inf')\"). Of course, this is a hack and another hack would be needed if I want to use my data structure in another language.\nWould it be possible to officially support \"-Inf\" and \"+Inf\" (and not nan) as floating-point defaults?\n(JSON does not support these special floating point values, but I think it's common for binary formats to. I used to use Avro a lot and I'm pretty use it supported infinities. Non-standard JSON parsers like Python's built-in module and Jackson with an option enabled support infinities\u2014 there's enough desire for this feature that developers are willing to work with not-strictly-compliant JSON to do it.). Just as the generated Python has a trailing .0 that causes SyntaxErrors.\nWhile handling nan might be onerous (to put in special cases for its strange equality semantics), I think that handling inf and -inf might just be a matter of generating the right code. (In every language target.). ",
    "vglavnyy": "Probably, both are supported in C++ parser. Could you check?\nI will be able to check it at the beginning of the next week.\nhttps://github.com/google/flatbuffers/blob/87704e987eb51dbd9852d7cae4e8013d47ad6eef/tests/test.cpp#L1552-L1566. The extra \"f\" easy to fix with std::isfinite.. I have resolved this issue for the C++ generated code (#5102).\nIf need can update Python and C# generators.\nAs already noted by @Bklyn, float-point NaN values require additional attention.\nThe main problem is NaN is not equal to NaN.\nThis NaN behavior is known properties of float-point arithmetics. But sometimes people forget about it.\nFor example, a naive operator== for table/struct will fail even if wired buffers and JSON descriptors of both tables are identical.\n. Is there a plan for this issue?\nHow to test a program with flatbuffers under Release?\nGlobal overload of the assert macro is a possible but unstable solution because some files in a program don't expect it.\nA total number of \"assert(cond)\" in the flatbuffers code is near 200.\nMaybe should rename assert to FBCOND or FB_CHK or similar?\nThis rename will preserve code formatting.. The term \"to test\" was incorrect. I meant a checking in runtime both for Debug and Release.\nI think that the assert method should be overridable, this is useful for the Fail-fast error detection and for logging. As the example, I had a few foolish errors in the application code. The one of which had in Release only.\n1) Indirect call of Finished() method with invalid state of the builder.\nThis method called from GetBufferPointer and Release.\nhttps://github.com/google/flatbuffers/blob/86153fd7405ad3bae28aee7f5affdbeb2d77e533/include/flatbuffers/flatbuffers.h#L812-L819\n2) Finish() method call with an invalid file_identifier string.\nhttps://github.com/google/flatbuffers/blob/86153fd7405ad3bae28aee7f5affdbeb2d77e533/include/flatbuffers/flatbuffers.h#L1562\nIf this issue is actual I will prepare PR.. I have a newbie question about build flatbuffers for Android.\nHow to prepare VM for Android build?\nThe Android build is more restrictive than MSVC2010 while preparing PR.\nBetter to check this build at a local machine before push a new commit to PR.\nFrom Travis/CI I know that I need Ubuntu 14.04 (Linux \"trusty\").\nFrom pre-build section:\n- git clone https://github.com/urho3d/android-ndk.git\n- install gcc 4.9\n- Python 3.6 for conan build\nWhat does mean \"Android\" section?\njson\n  \"android\": {\n    \"components\": [\n      \"tools\",\n      \"platform-tools\",\n      \"build-tools-25.0.2\",\n      \"android-25\",\n      \"extra-android-m2repository\"\n    ]\n  }\nWhich packages I should install using apt-get to meet this section?. @AntonYudintsev, @aardappel \n#pragma push without pop.\nIs it error or not?\nWhere pop was expected?\n. I have deserialization code from base64 array:\nattribute \"base64\";\ntable Data\n{\n playload : [ubyte] (base64);\n}\njson:\n{\nplayload : \"MTIxMg==\"\n}\nSerialisation not implemented (not necessary for me).\nIf it is actual, I can add Serialisation to base64 with tests and make PR.\n. Attribute base64 like hash attribute for string, it doesn't have any meaning outside of JSON  parser.\nI\u2019m guided by Newtonsoft.Json(C#) and Python base64 implementation for encoding. I\u2019m guided by Newtonsoft.Json(C#) and Python base64 implementation for encoding. I decode the base64 strings created by them. And both implements RFC 3548 with characters \u2018+\u2019, \u2019/\u2019 and padding with \u2018=\u2019.\nIt is possible to add a new attribute like base64url, with proposed replacements.\nI don\u2019t understand where \\r\\n\\t and whitespace may be useful. Base64 generated by machine for other machines. A human can use text viewer with line wrapping or block folding.\nDecoding a string, which contains those symbols, is not a problem itself.\nProblem is a test coverage and Encoding procedure.\nI think that base64 string must be common single line string like \u201c1+/34w==\u201d or as proposed \u201c1-_34w\u201d.\n. From my point of view the json-base64 attribute is ambiguous.\nCommonly base64 is implicit equal to RFC 4648/3548.\nBut we have JB64 proposal, which mentioned before:\n\"The JSON-Base64 file format, aka JB64\" (https://jb64.org/).\nMaybe suitable will be to use attributes:\nexport_base64, import_base64, export_base64url, import_base64url, \nIf we return to the meaning of problem:\n1) Base64 applicable for a [ubyte] only.\n2) Base64 of [ubyte] is ASCII-sequence compatible with RFC 4648/3548;\n3) Encoded Base64 sequence is a quoted string, compatible with JSON format.\n4) Base64url (https://tools.ietf.org/html/rfc4648#page-5):\n4.1) '+' replaced by '-'\n4.2) '/' replaced by '_'\n4.3) pad character \"=\" can be omitted.\n5) Optionally: ignore(skip) characters from set: {0x9,0xA,0xD,0x20}\n. I examined the current state of JSON rules for multiline strings.\nIf one needs to split a long base64 string to multiline, then an array of substrings can be a right solution without any escapes inside.\n{\n \"multiline_b64\" :  [\n    \"120\",\n    \"1212==\"\n  ]\n}\nIf we select this solution:\n1) It will be fully compliant with JSON;\n2) Acceptable for human view;\n3) One can easy split own base64 while building JSON record;\n4) Easy decoding. At least in C++ we can concatenate all substrings to a big one, before decoding.\n5) After concatenation, it will be compliant with RFC.\nAs a consequence, we do not need \"\\t\\r\\n\\0x20\" and extra code for support them.\nWhat do you think about this solution?. Ok.\nThis weekend I will prepare a draft for single line decoder.\nLater I will add encoder.. I prepared a draft  of base64 decoder: base64\nCan you review this commit?\nUsage code can be found in tests/tests.cpp@JsonBase64Test.\nAttributes are \"base64\" and \"base64u\". Names of attributes can be easily replaced later.\nSorry for my English in comments for the code.\n. Ok, I will make PR as soon as I add base64 an encoder from [ubyte] to base64-string. \nIn the current draft, only the decoder is implemented.. @gwvo , @mikkelfj \nI need help to determine a connection point of base64-encoder.\nBase64-encoder replace common presentation of a json-array [a1,a2,..,an] by base64-string.\nBase64-encoder take place if and only [ubyte] array declared with \"base64\" attribute.\nThe entry point for export any non-scalar type is function idl_gen_text.cpp/GenFieldOffset.\nReal work is delegated to template<> bool Print<const void *>.\nIn turn, Print function passes a work to the function template<typename T> bool PrintVector.\nI think that the encoding to base64 should be located inside PrintVector. \nAt first, a [ubyte] array is encoded to a base64 string (if \"base64\" attribute is set).\nSecond, call Print for resulting base64 string.\nBut I can't find a way to pass a knowledge about the \"base64\" attribute to Print and PrintVector functions.\nFirst input argument of GenFieldOffset is const FieldDef &fd and fd.attributes.Lookup is accessible.\nBut Print function doesn't have access to FieldDef &fd, as result PrintVector doesn't have it also.\nCan you advise a suitable solution for this problem?. I added base64 to GenerateText.\nSupported both base64 and base64url.\nAt weekend I will prepare a pull request.\n. My implementation is fully compatible with your documentation.\nPadding symbols are mandatory both for encode and decode if (base64) attribute is set.\nPadding symbols are optional for decode and mandatory for encode if the (base64url) attribute is set.\nMy test cases are not good enough for PR. \nI will try to rewrite them using your tests. I hope to prepare my PR for this week.\nCurrent interface and implementation of base64 here:\nhttps://github.com/vglavnyy/flatbuffers/blob/base64/src/idl_gen_text.cpp#L88-L112\nhttps://github.com/vglavnyy/flatbuffers/blob/base64/src/idl_parser.cpp#L859-L891\nhttps://github.com/vglavnyy/flatbuffers/blob/base64/src/idl_parser.cpp#L859-L891\nhttps://github.com/vglavnyy/flatbuffers/blob/base64/include/flatbuffers/util_base64.h\nhttps://github.com/vglavnyy/flatbuffers/blob/base64/src/util_base64.cpp\n. I already tried approach with padding control, code was ugly.\nI can extend Base64Mode with additional flag like kBase64OptionalPadding/kBase64MandatoryPadding  and use it by ORing. But test-cases and use-cases was very complicated and I deleted this solution.\n\nIn conclusion, I think the optional padding (by default) in parsing and mandatory padding (by default) in printing is the best behavior. RFC 4648 does permit omitting padding when this is documented so there is no violation.\n\nI agree with you and I fix decode routine to optional padding (by default) both for Strict and Url mode.. \"base64\" and \"base64url\" already added:\nhttps://github.com/vglavnyy/flatbuffers/blob/base64/include/flatbuffers/idl.h#L507-L543\nI have finished base64, at first round:\nAdded padding control to base64 encoder and decoder and written test coverage for this:\nhttps://github.com/vglavnyy/flatbuffers/blob/base64/include/flatbuffers/util_base64.h\nhttps://github.com/vglavnyy/flatbuffers/blob/base64/src/util_base64_test.cpp\nAdded unit-test for base64 to the Monster schema:\nhttps://github.com/vglavnyy/flatbuffers/blob/base64/tests/test.cpp#L1864-L1939\nI going to refine the code and prepare PR for this week.. @spmckenney, have you progress with PR for this issue?\nIt will be very helpful if FlatBufferBuilder has these methods.. I sign CLA.\nI would be happy if proposed solution would help the project.\nI missed the discussion about #PR4517 and #4509.\nRepeated tests from #4509 give:\ntest_rgb.json:\n{\n  r: Red,\n  g: Green,\n  b: Blue\n}\nRun: flatc -t --defaults-json --raw-binary test.fbs -- test_rgb.bin\nResult for test_rgb.json:\n{\n  r: Red,\n  g: Green,\n  b: Blue\n}\nNow for empty Test table:\nempty.json:\n{}\nRun: flatc -t --defaults-json --raw-binary test.fbs -- empty.bin\nResult for empty.json:\n{\n  r: Red,\n  g: Red,\n  b: Red\n}\nRed is default value in enum Color: byte { Red, Green, Blue }.\nI think that result for empty.json is right.\nBut #4509 also can be solved without commit if in C++ code set ForceDefault(true) for a builder.\nIt will force insert r field into a generated buffer even if it explicitly set to Red.\n. I exam the code and your comment #4509\nMaybe I don't exactly understand, but we can't avoid or remove \n\nif(is_present)+else\n\nor replace it by \"if (is_present || 1)\"\nbecause non-scalar values processed by GenFieldOffset or by UNION-code will fail if\n\ntable->CheckField(fd.value.offset)\n\nreturn False.\nIn this case, the result will be undefined.\nAt least assert(enum_val) will be activated.\n\nif (fd.value.type.base_type == BASE_TYPE_UTYPE) {\n          auto enum_val = fd.value.type.enum_def->ReverseLookup(\n                                  table->GetField(fd.value.offset, 0));\n          assert(enum_val);\n          union_type = &enum_val->union_type;\n}\n\nThat's why the code was added:\n\nEnumDef* const pen = fd.value.type.enum_def;\nif(opts.output_enum_identifiers && pen && !(pen->is_union || pen->uses_type_aliases) )\n. I tried to fix as you said. Can you check it?\nver2\nUpdated test-case:\nver3\nI know about git-rebase, but I have not experience with it.. Thank you for review.  I fixed all remarks.. I did rebase for this PR.\nNow enum values are strictly quoted with (\"\") as proposed by @yfinkelstein.\nCan you review?\n\nP.S. Travis checks failed, and I don't know how to resolve this.\n  . At commit 79b80f84 the method ReserveElements was removed from FlatBufferBuilder class.\nReserveElements\nMy code uses this public method for memory preallocation.\nHow to preallocate a big Vector without using temporary objects like std::vector<>?. Resolved. \nProblem with ReserveElements can be solved with method:\nuoffset_t CreateUninitializedVector(size_t len, size_t elemsize,  uint8_t **buf). Can you present examples of these cases?\nI'm finishing requested changes within a few days.\nAt the first stage (this PR), only [ubyte] arrays are supported.\nAt the second, nested buffers.. @mikkelfj,\nThank you for review and you are right.\nThis commit with a default base64 decode/encode behaviour was not good. This was experiment with code. As we discussed earlier encoder/decoder behavior must satisfy:\nbase64/base64url decoder: padding is optional,\nbase64 output: with padding,\nbase64url output: with padding and have IDL-option JsonBase64CancelPadding.\n. The (base64url) decoder implicitly can decode sequence form (base64) encoder, but versa does not.\nI can add compatibility (base64url)=>(base64).\nBut this will be further away from the RFC specification.\nEven (base64)=>(base64url) compatibility violates RFC.. @aardappel ,\nI done almost all requested changes and did rebase to master.\nNow I have two unresolved problem.\nFirst:\nTo the file \"util_base64.h\" aded templated function field_base64_mode.\nIt used in two places now. But if add nested buffers support it will be used in additional two places.\nShould I delete this function or move it to another file?\nSecond:\n\nChange requets for implementation of PrintVectorBase64 function:\nwhy does this function encode into a vector and not a string? If it took a string, we could pass it text, and it could append straight where it is needed. See similarly how EscapeString is called.\n\nPrintVectorBase64\nUse appeding of characters to a string (like EscapeString) is impossible for current implementation of base64 encoder. Base64 encoder use direct pointer for writing to destination memory.\nThe vector copy can be avoided if use direct write to the string internal memory after extending it append(0, req_len). Address of memory can be obtained using &test[x] operator.\nC++11 gives guarantee that internal memory of a std::string use a contiguous block of memory. But I'm not sure with correctness of this solution.\n. @aardappel , could you review latest changes in the base-PR code?. @aardappel, Thank you for your review.\nYou are completely right about the implementation of the PrintVectorBase64 function and I fixed it.\nCan we split review of this PR into two parts?\nFirst part is an interface between the Flatbuffers and a base64 routines. The current interface declared in the \u201cbase64_utils.h\u201d and used in the \"idl_gen_text.cpp\", \"idl_parse.cpp\" and \"test.cpp\". The enum \u201cBase64Mode\u201d define working modes for decode/encode routines.\nThe encoder has a simple interface. The decoder has two routines: Base64DecodedSize and Base64Decode. The current implementation of the decoder can return the position of a first invalid symbol inside a base64 string. This restricts a possible implementation of a base64 decoder. It can be removed from the interface and from the tests.\nThe second part is an implementation of base64 routines. There are a lot of implementations of base64 encode/decode routines and anyone can find or write own.\nThe proposed implementation located in the base64_utils.cpp. But for the first commit, it can be replaced by stub which does not violate current behavior of the Flatbuffers.\n. Ok. I understand your proposal.\nAnd I propose closing this PR and return to the issue stage.\nWe can take the already defined attributes \"base64\" and \"base64url\" as a basis for the issue (with or without the IDL option \"base64_cancel_padding\").\nThen once again discussing the behavior of the encoder and decoder for these attributes if that behavior is not clear. After that, someone can propose a new solution for this issue.\n. I don't know how to implement your suggestions and I need advice.\nLet's start from simple case of printing.\nCurrent implementation\n```\ntemplate\nbool PrintVector(const Vector &v, Type type, int indent,\n                 const IDLOptions &opts, std::string _text,\n                 const FieldDef fd = nullptr) {\n  std::string &text = *_text;\n// Try to print UCHAR array as base64 string.\n  if (type.base_type == BASE_TYPE_UCHAR) {\n    auto b64mode = FieldGetBase64Mode(fd);\n    if (b64mode) {\n      text += \"\\\"\";\n      Base64Encode(\n          opts.base64_cancel_padding ? Base64CancelPadding(b64mode) : b64mode,\n          reinterpret_cast(v.data()), v.size(), &text);\n      text += \"\\\"\";\n      return true;\n    }\n  }\n  .....\n}\n```\nProposed:\n// Interface declaration from future version of the util_base64.h\n// Try to print the vector of UCHAR as base64 string.\n// Returns false if the field fd doesn't have base64 attribute.\nbool PrintBase64Vector(const Vector<UCHAR> &v,  // data source\n                       Type type, int indent, const IDLOptions &opts,\n                       std::string *_text,  // destination\n                       const FieldDef *fd);\nFuture implementation of PrintVector from idl_gen_text.cpp\ntemplate<typename T>\nbool PrintVector(const Vector<T> &v, Type type, int indent,\n                 const IDLOptions &opts, std::string *_text,\n                 const FieldDef *fd = nullptr) {\n  std::string &text = *_text;\n  if (type.base_type == BASE_TYPE_UCHAR) {\n    // very ugly reinterpret cast\n    PrintBase64Vector(*(reinterpret_cast<const Vector<UCHAR> *>(&v)), type,\n                      ident, opts, _text, fd)\n  }\n  ....\n  ....\n}\nHow to add other types not only UCHAR?\nTemplated version of the PrintBase64Vector will break encapsulation and will lead to the current version of PrintVector.\n. @aardappel, could you review this PR again?\nI rewrote interface to the base64 routines and removed low-level core routines from the header file.\nIs this the correct way?\n. I apologize for wasting your time.\nThis problem is harder for me than I thought.\nI hope this issue will be solved by someone with more acceptable way.\nIf a one needs a temporary solution, I have made it as the addon with source code under CC0.. The problem was with me and my hidden goal.\nMy project requires high-performance base64 encoder and decoder with AVX2 and NEON optimizations.\nSuggested approach with the symbol by symbol appending to the string will block the optimization.\nCould we once again return to the beginning of PR?\nHere is the updated proposal of the base64 interface base64-PR2.\nThis interface has only two unimplemented routines:\nvoid util_base64_encode(int b64mode, const uint8_t *src, size_t src_size, std::string *_text);\nsize_t util_base64_decode(int b64mode, const std::string &src, uint8_t *dst, size_t dst_size);\nI believe that these routines should be implemented inside util_base64.cpp without dependencies from flatbuffers. In this case, one can easily replace the encoder/decoder if it is needed.\nIs this interface acceptable?\n. Now I agree with Wouter. The code should be simple and clean and optimization doesn't help to make the code simple.\nIf util_base64_encode and util_base64_decode are pure functions this will keep simplicity and flexibility.\nIn the proposed solution, only CreateUninitializedVector can't be avoided.\nMemory for decoded array should be reserved before decode.\nCalculation of required size might be very simple as R=text.size()3/4 if to trim preallocated vector to written size after decoding.\nThis \"3/4\" guess will differ from the real written number no more than 2 bytes. It depends on trailing padding in the source base64 string.\nUpdate:\nI rewrote the ParseBase64Vector: base64-PR2.. #4895 adds hexadecimal floats to input of the parser only.\nSerialization of floats to hexadecimal possible if add it as part of #4605.\nProbably, better way to add an attribute for float and double, like printf:\ntable X{\n Y : float (print_format : \"%.9g\");\n Z : double (print_format : \"%.17g\");\n}\nIt should accept at least:\ne | Scientific notation (mantissa/exponent), lowercase | 3.9265e+2\ng | Use the shortest representation: %e or %f | 392.65\na | Hexadecimal floating point, lowercase | -0xc.90fep-2. The hexadecimal print should be faster and simpler both for the little and big-endian platforms.\nThis like hexadecimal integer print if a position of the dot calculated.\nCalculation of a dot position is easy (exponent has the base of 2).\n. made the request (#4606). Yes, this was tricky, but I did it. Almost all \"cursor_\" logic remains unchanged.\nNevertheless, you are right it may be hard to guarantee the code is correct.\nDue to interface restrictions, I can't read any memory address outside of the range [ptr, ptr+size) even if the zero '\\0' really exist in the memory at address [prt+size] after c_str() call.\nThis is why it is necessary to use code lib_process(json_request.c_str(), json_request.size()+1).\nFor C# a caller creates a copy of a string as byte[] array for each call like this:\n\nSystem.Text.UTF8Encoding g_utf8_ = new System.Text.UTF8Encoding();\nbyte[] out_str = g_utf8_.GetBytes(json_str + \"\\0\");\n\nThan the out_str passed to a dll. A similar way for python.\nProbably it's better to wait until this problem becomes actual for someone else, not only for me.\n. Thank you for your help. I close this issue.. The problem is in attribute usage:\nitem:uint8 (ui.label: \"Foo\");\nThe ui.label here is not a string constant like \"ui.label\" and dot symbol '.' breaks parser.\nFrom flatbuffers grammar\nstring_constant = \\\".*?\\\\\"\nattribute_decl = attribute string_constant ;\nmetadata = [ ( commasep( ident [ : single_value ] ) ) ]\nident = [a-zA-Z_][a-zA-Z0-9_]*\nAn ident is not a string_constant.\nThis is ambiguous to the definition of attribute_decl.\nAs a solution - remove dot symbol form the attribute name.. Thank you for advice. I have modified commit.\nNow an user-defined attribute can be either a bare identifier or an identifier inside string.. ```\nattribute \"1\";\nattribute \" \";\ntable test{\n  q : int (required, \"1\": \"1\"); // valid\n  x : int (required, \"1\": 2);   // valid\n  y : int (required,  1 : 3);   // invalid\n  w : int (required,  \" \" : 5); // valid\n  u : int (required,   \\b : 6); // invalid\n}\n``\nThis \"truth\" table is what you meant?\n. Done.\nIs an unit-test needed for this case?. I have executed the script/tests/generate_code.sh.\nThree files updated.\n. Ok.. The test moved to the end ofParseAndGenerateTextTestand it reuse Monster parser.\nName of the flag andflatc` command-line option now are the same.\nUnresolved moment: auto-reset of human-readable mode.\nI think that natural_utf8 should be canceled by the first invalid uft-8 character in the string.\nif (allow_non_utf8) {\n              text += \"\\\\x\";\n              text += IntToStringHex(static_cast<uint8_t>(c), 2);\n              // Cancel natural printing for current string and switch to escape\n              // mode for further utf-8 characters till the end of the string.\n              natural_utf8 = false;\n            } else {\n. Yes, you are right.\nI tried to find arguments, but I could not.\nFor correct strings this is useless. Whereas for incorrect strings natural printing should not be used.\n. I have plan to return to this PR in late June, but if you take and do that it will be very helpful.\nIf you find anything helpful in my previous attempts, you can use it freely.\nYou can count on my help.\nBest regards.. @KageKirin, Do you have progress on this issue?\nHave you started to do it?. Thank you,\nGetBufferStartFromRootPointer exactly what I need now.\nI will make PR with GenerateTextFromTable in several days.. This change was checked by removing lines:\ntypedef ptrdiff_t difference_type; and operator-.\nThere was only one place where compilation was broken.\nTEST_EQ(*it, inv_data[it - inventory->begin()]);\nNow this place looks:\nfor (auto it = inventory->begin(); it != inventory->end(); ++it) {\n    auto indx = it - inventory->begin();\n    TEST_EQ(*it, inv_vec.at(indx));  // Use bounds-check.\n    TEST_EQ(*it, inv_data[indx]);\n  }\nAlthough this does not guarantee full compatibility.\n. I have printed a Vector of unions without any problem.\n1) Can you post the schema?\n2) What would you expect when you pushed an invalid offset to Vector?. You are right about a vector of unions, but in part only.\nI have written simple test code:\ncpp\n  const char* test_2_schema =\n    \"table A{x: int;}\\n\"\n    \"table B{y: int;}\\n\"\n    \"union AB { A, B }\\n\"\n    \"table UV{uv: [AB];}\\n\"\n    \"root_type UV;\\n\"\n    \"file_identifier \\\"_UV2\\\"\\n\";\n    \"file_extension \\\"bin\\\"\\n\";\nflatbuffers::Parser parser;\nauto parser_done = parser.Parse(test_2_schema);\nTEST_OUTPUT_LINE(\"%s\", parser.error_.c_str());\nAnd I have got parser_done==false and the message:\nerror: Vectors of unions are not yet supported in all the specified programming languages.\nThe flatcc returns the same message, I checked it.\nIt is impossibe to use flatbuffers::GenerateText without the valid parser.\nOk, next I pass this invalid parser to the programm:\nauto text_done = flatbuffers::GenerateText(parser, fbb.GetBufferPointer(), &text);\nAnd have got assert at the idl_gen_text.cpp:273:\nFLATBUFFERS_ASSERT(parser.root_struct_def_);  // call SetRootType()\nYou can predefine FLATBUFFERS_ASSERT globally for each source code file (all compilers have this feature) and catch an exception at latter stage.\nIn my project, I use an envelope:\ntable A { x : int; }\ntable B { y : int; }\nunion AB{ A, B }\ntable EnvelopeAB { ab : AB; }\ntable UV{ uv : [EnvelopeAB];}\nroot_type UV;\nfile_identifier \"_UV_\";\nfile_extension \"bin\";\nJSON for this schema looks like:\njson\n{\n  \"uv\": [\n    {\n      \"ab_type\": \"A\",\n      \"ab\": { \"x\": 1 }\n    },\n    {\n      \"ab_type\": \"B\",\n      \"ab\": { \"y\": 2 }\n    }\n  ]\n}\nThis solution gives one homogenous array of objects in JSON.\n. > But the question is why can't be this or something similar be there in flatbuffers library itself?\nA user predefined FLATBUFFERS_ASSERT macro is a flexible and lightweight solution.\nSomeone can use own logging system, not only C++ exception or .. Can you fix it and prepare PR?\nProbably, the problem is located here:\nhttps://github.com/google/flatbuffers/blob/ea06768ad1e1522e9ecdfa9559bfb73dc3e5227c/src/idl_gen_cpp.cpp#L1745\nI think that possible solution can be like this:\ncpp\ncode_ += \"    const auto {{FIELD_NAME}}_key = {{FIELD_NAME}}();\";\nand so on, considering the new name of the local variable key.\n. I'm sorry for mistake in code format. Now this fixed and code was updated with sh generate_code.sh.\nThe Dart code was updated significantly by sh generate_code.sh.\nProbably the field identifiers e, _e, o, _o should be reserved words of grammar. These identifiers (except e) are widely used inside generated code.\nBut problem with mutable class members stay unresolved:\ntable X { test : int; test_ : int; }. PR updated slightly.\nFor simplicity, the extra check of CMake version removed.\nIt is assumed that the version is not less than 2.8.11 (I have never seen less than 3.0).\nThe building documentation has updated (need to check the spelling and style).\nI have used GoogleTest readme  file as a template.\nIs this \"copy-past\" approach acceptable for merge?\n. Ok.\nThe stroll function interprets first '0' as octal prefix if and only base set to 0 or 8:\nprefix (0) indicating octal base (applies only when the base is 8 or 0).\nThis isn't our case, the parser uses base=10.\nI will update both the grammar for float and documentation about differences to JSON.\n. A new problem with a floating-point number has been detected.\nThe -.1  literal is accepted by the parser but the .1 is rejected with the error \"floating point constant can't start with '.'\".  The parser has two processing points for the dot symbol .:\nhttps://github.com/google/flatbuffers/blob/c721009491dc8275052cf33f7334e015ed737927/src/idl_parser.cpp#L295-L301\nhttps://github.com/google/flatbuffers/blob/c721009491dc8275052cf33f7334e015ed737927/src/idl_parser.cpp#L458-L464\nWe have two choices:\n1. Accept both .1 and -.1\n2. Reject both\nShould we reject [-1., 1., 1.e1] also if select (2)? \nProbably, a float-point literal like [0-9]+\\.[0-9]+  is more clear than C/C++ compatible literals.. @gwvo,\nI have new questions about the grammar.\nThe commit fbc8af40 introduced: Allow JSON numeric fields to be specified by a numeric data in a string.. Starting from this, an integer or float can be initialized in two ways:\n{\n  x1 :  1,\n  x2 : \"2\" // also integer constant\n}\nor\ntable X {\n x1 : int = 1;\n x2 : int = \"2\";\n}\nThe second \"in-string\" method isn't mentioned in the grammar.\nFirst: Where is this initialization method useful?\nA boolean field can be initialized by identifiers or integer: true, false, integer.\nBut not with in-string: \"true\", \"false\", \"integer\".\nSecond: Why don't allow in-string initializers for booleans?\nThird: Is it right that a boolean field accepts an integer number as initialization constant?. The release mode fixed.. @aardappel, thank you for code review.\nI'm sorry. I'm going away for two weeks.\nI added fuzzer test (libFuzzer) for scalars and found two unexpected behaviors of parser.\nUnfortunately, I don't have time to prepare clean commit with this test.\nWhen I return I will answer and resolve all review notes.. I did the refactoring of this PR. Latest commit adds:\n1) Fuzzer for scalar values with cross-checking by regex.\n2) Fix bug with recurse_protection_counter.\n3) Reject any objects after parsed json.\n4) Change the behavior of null value for scalars. Is it right?\n5) Missed checks defined(clang) for conditional code in base.h.\n6) Locale-independent code and test for it.\nThe locale-independent code is located in idl_parser.cpp.\nThe util.h is a most obvious place I have tried it, but I'm not sure.\nIsolated cpp module like idel_parser.cpp looks better for me.\nI need advice, where it should be located.\n. Thank you for the review, this is really big PR.\nI can split this PR to a series of PR.\nI can remove the fuzzer code from this PR and make standalone PR for it.\nFuzzer has been very helpful for testing but it not the main part of the library.\n. Removed assert  inside TestFail was useful for debug.\nNow need to use step-by-step debugging to see the call-stack on first fall\nThe test engine should have flag like --gtest_break_on_failure, like google-test.\nThis flag could be passed over main() arguments or as define over CMake.\nA fuzzer test also require  break on failure mode.\n. I found that glibc implements strtod  as the call to strtod_l with a pointer to the current locale as the argument. This means that we do not lose in performance (at least on Linux).\nWe can use strtod_l by default (glibc 2.3, MSVC2005).\nFunctions strtod_l and strtof_l are not part of C/C++ standard (but de-facto exist) therefore some platforms (embedded or android) may not have these functions.\nFor this case add flag FLATBUFFERS_LOCALE_LEGACY which switch code to strtod.. Yes, this is reasonable. These changes need to be tested on volunteers.\nI will prepare a commit with FLATBUFFERS_LOCALE_LEGACY.\nI can split this commit to split into two PR.\nOne with parser refactoring and second with locales.. The locals already removed from this PR.\nI will prepare PR with locals at the end of this week.\nAll conflicts with the master I will resolve after 1.10.. @aardappel \nPR updated. Could you review this update?\n- PR without locales for strod/strof.\n- PR was checked with fuzzer, no errors was found.\n. I rewrote the numbers parser a bit\nTo short the source code I used the trick:\nhttps://github.com/google/flatbuffers/blob/819598d32a97bebd5a31bb5df8ca1509d091fe3a/src/idl_parser.cpp#L429-L436\nThe pointer cursor _ and dot_lvl updated inside while() expression if first condition ('.') is true.\nIs this trick acceptable?\nThis code:\nhttps://github.com/google/flatbuffers/blob/819598d32a97bebd5a31bb5df8ca1509d091fe3a/src/idl_parser.cpp#L425-L428\ncan be rewritten as single line expression:\ncpp\nif (use_hex)  start_digits = ++cursor_;  // '0x' is the prefix, skip it\n. Probably, this is the final version.\nAll checks have passed.. @aardappel Thank you for the reviews made on this PR code.\nNext, I will prepare PR with fuzzer for this #4948, after that local-independent strtod.. \u0421oincidence :)\nI added it too (#4952).\n. A compiler will ignore assert(0) in Release if NDEBUG definition is not canceled.\nBy default, NDEBUG is active in Release.\nTherefore the test suite will continue after failures, until completion of all tests in the suite.\nQuestion/proposal:\nMaybe better to define TEST_EQ and TEST_ASSERT as aliases of TEST_ASSERT_FUNC and TEST_EQ_FUNC?\nAfter the check, I have found the problem in Release:\nIf a test fails in the middle and continues execution, then Segmentation fault (core dumped) error occurs. Like access to an incomplete constructed object.\nFor example template<typename T> T TestValue arise the segmentation fault if return code from parser ignored.\nThe error can be injected:\nTEST_EQ(TestValue<int8_t>(\"{ Y:w127 }\", \"byte\"), 127);. I don't know, both are right. The first time I wrote >=, but then changed it.\nif ((1 + recurse_protection_counter) > const) is like a prohibition to increment.\nif (recurse_protection_counter >= const) is a fact-check.\nReplace it to >= ?. Ok, fixed.. Thank you for the explanation.\nThe BeforeHash will be a good option for formatting.\nThe problem is not in the proposed cmake target, one can use git clang-format instead.\nI found the script /src/clang-format.sh with strange checkout command.\nProbably this script was launched a long time ago.\nIf run it all source files will be modified.\nThese multiple changes mean that source code not formatted properly.\nCan someone use git clang-format if this command affects on lines of unchanged source code?\nOne more annoying bug (feature) of clang-format.\nThis code is changed on each clang-format call:\ncpp\nvoid foo() {\n    // clang-format off\n    #define X\n}\nto\ncpp\nvoid foo() {\n// clang-format off\n    #define X\n}\nand back to the initial formatted code.\nTo fix it, need insert one blank line after // clang-format off:\n```cpp\nvoid foo() {\n    // clang-format off\n#define X\n\n}\n``\n. For example, the current version oftests.cpp` has the mistake:\nhttps://github.com/google/flatbuffers/blob/ebb410062b066982ab28f6e03017bd84edcfaf3a/tests/test.cpp#L38-L43\nThe directive  clang-format off is missed (or #include <random> can be moved 15 lines up).\nIf add missed clang-format on and run git clang-format more than 20 lines of code will be changed.\n. I sent the bug report to bugs.llvm.org.\nMy suggestion about a source of this bug in clang-format:\nThe clang-format interprets a comment line before PPD as the comment for this PPD.\nThe // clang-format off directive is a comment line.\nThe analyzer ignores this off directive and applies the standard rule for this comment line.\nOn the second run, analyzer returns this comment line to the right place because it not aligned with PPD and assumes that it is not associated with PPD.\nIf I right, we need to add a blank line between // clang-format off and a preprocessor directive to freeze a formatting of source code or we should stop to use clang-format off.\nProbably, BeforeHash option should solve this problem without a blank line.\nComment line // clang-format off will be moved to the right place automatically.\nOther open-source projects:\n- opencv - doesn't use clang-format\n- google-tests - doesn't use clang-format off\n- microsoft-GSL - doesn't use clang-format off\n- llvm-mirror/libcxx - use with and without blank lines\n- protobuf - use in one place only \n. Probably, better will be stick with the current status quo: don't touch what already works.\nIf somebody applies clang-format to a whole file, he should apply it the second time after git stage to ensure that code not changed by the second pass.\nAs an option, he can insert blank lines to resolve clang-format off/on problem.. @fbenkstein \nI see, thank you.. @aardappel, I need advice.\nI have examined the clang-format source code. Now I can prepare a patch for LLVM 36020.\nDo we really need this feature for the Flatbuffers project?\nAccording to google style rules\nThe hash mark that starts a preprocessor directive should always be at the beginning of the line.\nEven when preprocessor directives are within the body of indented code, the directives should start at the beginning of the line.\n0) If we don't need indent for PPD in accordance with GCS, we don't need extra indent for C ++ code inside the conditional PPD?\n1) If I'm right the BeforeHash also not necessary for Flatbuffers (https://bugs.llvm.org/show_bug.cgi?id=36019).\n2) Should we update the format of all files according to GCS?\nP.S. LLVM 36020 issue.\nSimple indentation of PPD is easy (have done it).\nHowever, this issue is ambiguous if indentation of conditional PPD should affect the indentation of nested C++ code. It is hard to cover all cases in C++ code, but possible if needed.\n. Likely PR #4948 will fix this issue.\nThis PR adds a strong range checking while parsing a schema.\nYou can get this PR and check it with your EnumOutOfRangeTest().\nIn any case, the function EnumOutOfRangeTest should be added to the tests.\n. Probably I was wrong.\nFolowing code isn't redurant:\nif (is_union) {\n        if (ev.value < 0 || ev.value >= 256)\n          return Error(\"union enum value must fit in a ubyte\");\n      }\nIf someone create an union with 256 or more tables:\nunion tables_0_256 {\n t0, .. , t255, t256\n}\nthe parser will accept it as a valid declaration.\nThis is the normal case without = assignment.\nGenerated values for this union aren't checked to be in range.\nIt will be better to check that all values are in range regardless of using direct assignment = and state of is_union flag.. @fbenkstein\nWhy not use a 'duck' approach?\n- Cast an implicit assigned int64 value to a string;\n- Check that '=' case passed and modify the string if needed;\n- Check string with ParseSingleValue(check_now=true), next apply atot<int64>.\nWith implicit assignment (without =) we have three extra steps:\n1) assigned ev.value to string with NumToString from utils.h;\n2) String to a number by ParseSingleValue::atot;\n3) String to a number by atot<int64>.\nWith explicit assignment using = we have:\n2) String to a number by ParseSingleValue::atot;\n3) String to a number by atot<int64>.\nSteps (2) and (3) are common.\nThe code is ugly but short (minimal code duplication) and covers all cases.\nWhat do you think?\nAnother way: Extract check_now code from ParseSingleValue::atot to standalone static or templated function and try to reuse it.. @sutambe, @aardappel \nProbably, at line 54 we have memory leak.\nLog from #5009: leak.log.\nhttps://github.com/google/flatbuffers/blob/bd20a60d6a1d4d0511687122fd3dea71a8165ea1/tests/test_builder.h#L49-L65\nThis is guess only:\nb1.ReleaseRaw(size, offset); b1 is released without cleanup of released memory.\nReturn value is not used, arguments size and offset are not used. This is non-safe semantic.\n. @fbenkstein \nThis code is UB since C++17, it may lead to terminate():\nC++\n  TEST_EQ_STR(\"\", EnumNameColor(static_cast<Color>(-1)));\n  TEST_EQ_STR(\"\", EnumNameColor(static_cast<Color>(1000)));\n\nInteger, floating-point, or enumeration type can be converted to any complete enumeration type (the result is unspecified (until C++17) undefined behavior (since C++17) if the value of expression,\nconverted to the enumeration's underlying type, is not one of the target enumeration values)\n\nhttps://en.cppreference.com/w/cpp/language/static_cast\nThis code only masks unsafe and undefined behavior:\nC++\ninline const char *EnumNameColor(Color e) {\n  // If Color is a strict enum then the compiler may ignore this check as redundant.\n  // An enum is always is a valid enum. Recast to underlying also may be ignored as redundant.\n  if (e < Color_Red || e > Color_Blue) return \"\";\n  // Warning: here <int> type may be wrong type. if <int> is <int32> but underlying type is <int64>\n  const size_t index = static_cast<int>(e) - static_cast<int>(Color_Red);\n  return EnumNamesColor()[index];\n}\n. First\nC++17 final draft\n\u00a7 8.2.9.10 (Static cast)\n\n\nA value of integral or enumeration type can be explicitly converted to a complete enumeration type. The value is unchanged if the original value is within the range of the enumeration values (10.2). Otherwise, the behavior is unde\ufb01ned. A value of \ufb02oating-point type can also be explicitly converted to an enumeration type. The resulting value is the same as converting the original value to the underlying type of the enumeration (7.10), and subsequently to the enumeration type.\n\n\nBy my opinion, this code is UB according to \u00a7 8.2.9.10:\nC++\nTEST_EQ_STR(\"\", EnumNameColor(static_cast<Color>(-1)));\nIf I right and this test is necessary it is possible to add folowing code to the generator:\n```C++\nstatic inline Color EnumColor(Color e) { return e; } // non-templated\ntemplate\nColor EnumColor(T e) {\n  for (auto v : EnumValuesColor()) { // binary searh is possible if sorted\n    auto tv = static_cast::type>(v);\n    if (tv == e) return v;\n  }\n  return Color_NONE;\n}\nTEST_EQ_STR(\"\", EnumNameColor(EnumColor(-1)));\n``\n**Second**\nI was wrong about(e < Color_Red || e > Color_Blue).\nThe interval [Color_Red;Color_Blue`] doesn't cover full enum range, therefore, this check can't be omitted by a compiler.\n. @fbenkstein \nYou are right, this is not UB:\n\nNot necessarily. The very next sentence says:\n\u00a710.2(8)\nFor an enumeration whose underlying type is fixed, the values of the enumeration are the values of the underlying type. Otherwise, \u2026\n\nI was stupid.\nVery good explanation of the problem on StackOverflow: What happens if you static_cast invalid value to enum class?\n. What can be stronger than a scoped enum: enum class or enum struct?\nHas MSVC 2010 support of scoped enum?\nScoped enums without underlying type specification have int.\nAnother way: https://en.wikibooks.org/wiki/More_C%2B%2B_Idioms/Type_Safe_Enum\nHand-made:\n```C++\ntypedef struct{\n    enum{red=1, green, blue};\n}enum_t1;\ntypedef struct{\n    enum{red=2, green, blue};\n}enum_t2;\nint main()\n{\n    enum_t1 e1;\n    auto e2 = enum_t2::green;\n    cout << \"E1: \" << e1.red << \", \" << enum_t1::red << '\\n';\n    cout << \"E2: \" << e2 << \", \" << enum_t2::red << '\\n';\n    return 0;\n}\n`. What can be stronger than a scoped enum: `enum class` or `enum struct`?\nHas MSVC 2010 support of scoped enum?\nScoped enums without underlying type specification have `int`.\nAnother way: https://en.wikibooks.org/wiki/More_C%2B%2B_Idioms/Type_Safe_Enum\n[Hand-made](https://onlinegdb.com/SkDsy5wsX):C++\ntypedef struct{\n    enum{red=1, green, blue};\n}enum_t1;\ntypedef struct{\n    enum{red=2, green, blue};\n}enum_t2;\nint main()\n{\n    enum_t1 e1;\n    auto e2 = enum_t2::green;\n    cout << \"E1: \" << e1.red << \", \" << enum_t1::red << '\\n';\n    cout << \"E2: \" << e2 << \", \" << enum_t2::red << '\\n';\n    return 0;\n}\n`. Could you explain the issue?\n[Example](https://godbolt.org/z/-8uwrH) of implementation for MSVC2010:C++\nstruct table1{\n    typedef struct {\n        enum _type { VT_NAME = 4, VT_VALUE = 4, VT_TYPE = 6 };\n    } type;\n    type::_type ofs;\n    table1() : ofs(type::VT_NAME) {}\n};\nstruct table2{\n    typedef struct {\n        enum _type { VT_NAME = 4, VT_VALUE = 4, VT_TYPE = 6 };\n    } type;\n    type::_type ofs;\n    table2() : ofs(type::VT_NAME) {}\n};\n// have defferent signatures\nvoid ofs_check(table1::type ofs) {}\nvoid ofs_check(table2::type ofs) {}\nvoid t1_ofs_check(table1::type ofs) {}\nvoid check() {\n    table1 t1;\n    table2 t2;\n    t1_ofs_check(t2.ofs); // compile-time error\n    t1.ofs = t2.ofs; // compile-time error\n    t1.ofs == t2.ofs; // compile-time warning\n    t1.ofs == table2::type::VT_NAME; // compile-time warning -Wenum-compare\n}\n. Are json objects large?\nIs it possible to rename field name in json before pass it to the parser?cpp\nstd::string json = load(...);\nauto p = json.find(\"\\\"E\\\" :\" );  // find first '\"E\" :' \nif (std::string::npos != p) json.at(p+1) = 'R';\nparser.Parse(json.c_str());\n```\nDon't use field names e, _e, o, _o.\nIf you use a mutable class, don't use field <name> with field <name>_ in one table (test with test_ for example).\nFor details: #4802. Great, it looks like magic.\nThe last argument \"void* state\" of ParseVectorDelimiters is \"this\" of the parser in all calls, therefore, this argument is redundant.. @aardappel Updated. Could you post flatbuffers schema for this JSON and an example of JSON (or JSON generator)?\nHave you tested this fault using other compilers?\nHave you tested this fault under debug build?. Ok. I will check this dataset over the next two days.\nHave you debugged?\nWhere exactly fault has arisen if you know the line number?. @crogrezen \nCould you post flatc command line arguments have used for C++ code generation?\nI can't check it without serialization:: code.. Why Eigen::Vector3d is interoperable with expected serialization::Matrix3 type?\nWhat is geometry namespace?\n```C++\n        Eigen::Vector3d rotation;\n        rotation << buffer->ground_truth_pose()->rotation()->x(),\n            buffer->ground_truth_pose()->rotation()->y(),\n            buffer->ground_truth_pose()->rotation()->z();\n    Eigen::Vector3d translation;\n    translation << buffer->ground_truth_pose()->translation()->x(),\n        buffer->ground_truth_pose()->translation()->y(),\n        buffer->ground_truth_pose()->translation()->z();\n\n    frame_data.ground_truth_pose = geometry::Se3(rotation, translation);\n\n```\nThis is my test code.\nI have used flatbuffers/tests/tests.cpp file as a carrier for the test code.\nThis code uses types and functions from std, flatbuffers and generated zeus::serialisation namespaces only.\nFor simplicity, I merged common_schema.fbs and dataset_schema.fbs to one dataset_schema.fbs file.\n```C++\ninclude \"serialization/dataset_schema_generated.h\"\nvoid check() {\n  std::string schema_str;\n  TEST_EQ(flatbuffers::LoadFile((test_data_path + \"dataset_schema.fbs\").c_str(),\n                                false, &schema_str), true);\n  std::string dataset_str;\n  TEST_EQ(flatbuffers::LoadFile((test_data_path + \"dataset.json\").c_str(),\n                                false, &dataset_str),  true);\n  flatbuffers::Parser parser;\n  auto schema_ok = parser.Parse(schema_str.c_str());\n  TEST_ASSERT_FUNC(schema_ok);\n  auto dataset_ok = parser.Parse(dataset_str.c_str());\n  TEST_ASSERT_FUNC(dataset_ok);\n  auto root = parser.builder_.GetBufferPointer();\n  TEST_ASSERT_FUNC(root);\n  std::string json_back;\n  parser.opts.strict_json = true;\n  parser.opts.output_default_scalars_in_json = true;\n  auto json_ok = GenerateText(parser, root, &json_back);\n  TEST_ASSERT_FUNC(json_ok);\n  TEST_ASSERT_FUNC(json_back.c_str());\n  std::ofstream out(\"json_back.json\");\n  out << json_back;\n  // do traverse over 'root' object\n  using namespace zeus;\n  auto dataset = serialization::GetDataset(root);\n  auto data_type = dataset->data_type();\n  TEST_ASSERT_FUNC(data_type == serialization::DatasetData::JsonDataset);\nauto jsdataset = dataset->data_as_JsonDataset();\n  auto frame_buffers = jsdataset->frame_data();\n  TEST_OUTPUT_LINE(\"Number of frames: %d\", frame_buffers->Length());\n  auto i = 0;\n  for (auto buffer = frame_buffers->begin(); buffer != frame_buffers->end(); ++buffer, ++i) {\n    TEST_OUTPUT_LINE(\"TS(%d). %f\", i, buffer->time_stamp());\n  }\n}\n```\nAll have passed.\nThe \"json_back.json\" has the same structure as initial \"dataset.json\" file.\nProbably, Flatbuffers doesn't have an error.\nCan you check this code?\nIs it possible to check the program with undefined behavior (UBSan) and/or address (ASan) sanitizers to get more info?\ndataset_schema.zip. @aardappel \nIs it possible to add GCC 7 or newer and/or LLVM 7 or newer to CI as a test build by request?\nFor example 1 (#5005):\n\nI've only tried with gcc 5.4. I did try my binary with gcc 8.2, but flatbuffers won't compile with that version (a bunch of errors due to fall-through in some switch statements).\n\nExample 2: Conversation about UB in #4982\nBackward compatibility is a good idea.\nBut we are moving into the future with every time moment.\nCompatibility with new C++ standard (at least C++17) is mandatory.\nCode with unspecified or compiler dependent behavior in C++11 becomes UB in the newer standards (C++14, C++17, C++2a).. I assume that the possible way to add \"cutting edge\" is:\n- add a new branch like \"cutting edge\".\n- add new jobs to travis.yml with \"cutting edge\" branch conditions.\nAfter that forward-merge from the master branch of pull-requests will start the jobs.\nAny PR to this branch should be rejected. One can use this branch as a test before merging to the master.\nThis only assumption, I have never setup CI before.\nHave you looked this PR?\nProposed fsanitize flags work now without latest compilers.\nThese flags will prevent errors in code and will simplify a code debugging in a situation like #5005.\nThe unresolved problem with this PR (https://travis-ci.org/google/flatbuffers/jobs/443988991#L2461-2466):\n\n$ ./flattests\n/Users/travis/build/google/flatbuffers/tests/monster_test_generated.h:125:7: runtime error: load of value 4294967295, which is not a valid value for type 'MyGame::Example::Color'\nSUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /Users/travis/build/google/flatbuffers/tests/monster_test_generated.h:125:7 in \n/Users/travis/build/google/flatbuffers/tests/monster_test_generated.h:125:24: runtime error: load of value 1000, which is not a valid value for type 'MyGame::Example::Color'\nSUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /Users/travis/build/google/flatbuffers/tests/monster_test_generated.h:125:24 in \nALL TESTS PASSED\n\nThe conversation about this problem in 4982.\nIt is possible to supress these warnings but this dont supress the problem.\nSecond (minor) problem: CI job execution time longer than without fsanitize flags.. I think it is possible to pass -Dsome_option to cmake form Travis.\nThe unconditional fsanitize was added to flatc because it is the part of test due to the dependency of tests from flatc. The second was: 'flatcalso need to be tested. Variants:\n1) Addfsanitizetoflatcin debug build both in local and Travis;\n2) Add flagFLATBUFFERS_SANITIZE_COVERAGEand activate it in Travis;\n3) Don't testflatcwithfsanitize, use this option fortests` only.\nAdd FLATBUFFERS_SANITIZE_COVERAGE?\nAbout 5005: If tests code has fsanitive we can advise to reproduce an error inside tests.cpp  with fsanitize activated. As result, a developer will find an error in own code or can bring a new test case to flatbuffers test suite. Both cases are good.. I was wrong in #4982.\nVery good explanation of the problem on StackOverflow: What happens if you static_cast invalid value to enum class?. @aardappel PR updated.\nBy default sanitizer disabled both for tests and flatc.\nCMake flag -DFLATBUFFERS_CODE_SANITIZE=ON activates -fsanitize=address,undefined.\nA developer can control type of sanitizers with:\n-DFLATBUFFERS_CODE_SANITIZE=\"=thread\"\nTravis configuration script activates flag -DFLATBUFFERS_CODE_SANITIZE=ON for all builds.\nProbably, the problem with #4982 resolved.\nBut I have got a new issue which isn't surfaced in Travis build.\nUnder LLVM7.0 and GCC7.3 ASAN found memory leaks while run flattest:\nleak.log\nCould you look at this log file? Is it a real problem or false positive?\nP.S. It may be better to add GCC7 (or LLVM7) to Travis and set -DFLATBUFFERS_CODE_SANITIZE=ON only for this build.. Isn't better to use here unque_ptr<uint8_t> with custom deleter?\nThe alternative, ReleaseRaw may return some kind of raii_owner_t with move semantics.\nThis owner should assert in destructor if memory not released explicitly by a program with Release or Reset methods.. A smart pointer or manual deallocation is sufficient to resolve this problem, as already done.\nI have exam #4848 and #4885. A raii_owner_t is not needed.\nReleaseRaw is an interesting function. Perhaps, it allows implementing a memory arena with one deallocation or reset point.\n. I think better replace the hard-coded FLATBUFFERS_TEST_LOCALE by an environment variable:\nEnable test of locale independent code.\n-DFLATBUFFERS_TEST_LOCALE=\"\" - test with default C-locale\n-DFLATBUFFERS_TEST_LOCALE=\"ru_RU.CP1251\" - test with ru_RU.CP1251\nto\n1) std::getenv(\"FBS_TEST_LOCALE\"); - in a code (since C++11)\n2) FBS_TEST_LOCALE=\"ru_RU.CP1251\" ./flattests - in a console\n3) FBS_TEST_OPTIONS=LOCALE=\"ru_RU.CP1251\" ./flattests - like ASAN/UBSAN \n. Have done.\nstd::getenv passed all builds without #ifdef.\nAdditionally:\n- some code have been formatted (test_assert.[cpp && h], util.h);\n- the idl_parser.cpp file in the master branch has format UTF-8 with BOM while others files have UTF-8 without BOM. Current commit saved the file as simple UTF-8 without BOM;\nQuestion:\nShould the functions in util.h be inline: SaveFile, StripExtension, GetExtension, StripPath, StripFileName, ConCatPathFileName and etc?\nSome #include directives may be removed from util.h if move these functions to util.cpp.. VS2013-Release-x86 Rust failed  with message memory allocation of 2147483648 bytes failederror: test failed, to rerun pass '--test integration_test'.\nI have no idea why this error occurred.. @aardappel \nI'm sorry, can you explain this note?\n\ncan't link to cards??\n\nWhat I should link?. @rw \nI'm sorry for the spam to Appveyor-CI from this PR.\nProbably Rolling builds option is inactive.\nCan you check Appveyor settings?\nhttps://www.appveyor.com/docs/build-configuration/\n\n(Note that \u201cRolling builds\u201d can only be enabled in the settings UI of a project and not via appveyor.yml and that the presence of appveyor.yml does not disable this UI setting.)\n. This PR divided into parts:\n\n5105, #5106. @aardappel\nI see this issue (was too busy before),\nProbably this issue linked with #3972 problem.. > It be nice if we could avoid having to special case such values for all languages, by using.. hex floats? I am guessing there may be languages that don't even have a way to produce a NaN.\nThe hex-float isn't a bullet for this case.\nNot all languages have hexadecimal floats support as part of the language or standard library.\nProbably, all languages have IEEE-754 NaN and Inf (signle and/or double precision):\n- General approach:\n  - 0/0 => NAN\n  - 1/0 => INF\n- C++:\n  - std::numeric_limits::quiet_NaN()\n  - std::numeric_limits::infinity()\n- C#:\n  - (Single|Double).NaN\n  - (Single|Double).PositiveInfinity\n- Dart:\n  - DOUBLE.NAN\n  - DOUBLE.INFINITY\n- GO:\n  - func NaN() float64\n  - func Inf(sign int) float64\n- JS:\n  - NaN\n  - Infinity\n- Java:\n  - (Float|Double).NaN\n  - (Float|Double).POSITIVE_INFINITY\n- Lobster:\n  - NaN\n  - INF\n- Lua:\n  - math.nan\n  - math.huge\n- PHP:\n  - NAN\n  - INF\n- Python:\n  - float('nan'), np.nan\n  - float('inf'), np.inf\n- Rust:\n  - std::f64::NAN\n  - std::f64::INFINITY\n- TS:\n  - NaN\n  - Infinity\nIn the FBS-schema and the JSON we already have the support of \"nan\" and \"inf\" keywords.. @schoetbi \nC#?\nhttps://github.com/google/flatbuffers/blob/63d51afd1196336a7d1f56a988091ef05deb1c62/tests/MyGame/MonsterExtra.cs#L50-L56\n. @acthp,\nthank you for the report.\nI will fix it at the weekend.. @acthp,\nCould you test with gcc 4.8 before PR?\nhttps://github.com/vglavnyy/flatbuffers/tree/fix_issue_5079\nhttps://github.com/vglavnyy/flatbuffers/commit/304d4c092b908319e47b91f7c87d083348a33bcb\n. >We could conditionally compile support for this, but that is messy too. Simpler, I'd prefer to define it such that a NaN as a default value does whatever IEEE == does, meaning you probably don't want it.\nYou are right.\nBetter will make default NaN as a documented exceptional default value which always is written to the binary stream (or JSON stream).\nThis tradeoff saves performance and gives the ability to use NaN.\nI will revert AddElement back and we take this.\nAfter feedback will add conditionally compile support of default NaN to the AddElement.\n. There are two functions which require isnan checking:\n- template<typename T> void AddElement(voffset_t field, T e, T def)\n- template<typename T> bool SetField(voffset_t field, T val, T def)\nThe AddElement adds a new scalar to a table.\nThe SetField mutate existing scalar.\nBoth are used in auto-generated code only.\nIt is possible to use these functions in application code but it is hard.\nIf assume that application code never call these functions directly there are two ways:\n1) Add AddElementEx and SetFieldEx. Are used only with default NaN and make full checking.\n2) Add two special NaN-constant wrapped to the struct and add specialization of AddElement and SetField.\nBoth cases exclude second checking of std::isnan(def) because the def already known.\nSketch of special NaN-constant: https://godbolt.org/z/h_PX2V\nBut the first approach with AddElementEx is more robust against a compiler pessimization.\n. Yes, ready.. I have tested this PR on the C#/Java code generator.\nTest code is based on the head of the current PR.\nhttps://github.com/vglavnyy/flatbuffers/compare/generated_float_nan_inf...vglavnyy:def_nan_csharp_java\nArguments of several functions changed from const Value &value to const FielDef &filed.\nMost generators (except C#/Java, JS, PHP) use FieldDef instead of Value when generate default values.\nIf this test code is acceptable will be easy to add NaN to all langs with the table from #5069.\nI can merge the new code to this PR or make a new PR after merging.. The idea with MonsterExtra as a draft of new features is wrong.\nNow MonsterExtra added to C++, C#, Java, Python test suites to test NaN/Inf.\nAdding a new feature to MonsterExtra will break all tests.\nPossible workarounds:\n1) add a feature test suite to each supported language\n2) disable MonsterExtra in each language when an unsupported feature added\n3) don't test with MyGame.Monster\nWhat is the right way to introduce a new feature to the MyGame?\nUpdate:\nProbably, MonsterExtra should have name MonsterNaN and work-flow should be:\n1) Add import of monster_nan.fbs/MonsterNaN to all langs ready for a new feature.\n2) Wait until all languages import or include MonsterNaN and all tests passed. \n3) Move MonsterNaN table to monster_test.fbs\n4) Remove all references to \"monster_nan_generated*\" files from the tests.\nStep (2) requires knowledge about all supported languages which depend on monster_test.fbs.. @aardappel \nIs there anything else to complete in this PR?\nMSVC CRT detector is already active with appveyor-ci.\nHowever, tests will complete successfully even if memory leaks are detected.\nIt can be fixed in a future with a post-test script by analyzing the stdout.\n. @rw Added. \nI'm not familiar with the requirements of python tests.\nCode formatting and style may be broken.\nNeed to fix something?. @rw \u0421++, C#, Java are tested at compile time with include/import directives.\nIt is easy to add explicit checking for NaN/Inf in the C++.\nUnfortunately, I have not an environment to compile C# and Java tests.. I have reproduced this error on the HEAD of the master.\n@aardappel \nProbably this error was introduced by (https://github.com/google/flatbuffers/commit/f431a96523520617358b571911362cca2cb7abec).\nBefore  f431a965:\ncpp\n  if (type.enum_def && IsScalar(type.base_type) && !struct_def.fixed &&\n      !type.enum_def->attributes.Lookup(\"bit_flags\") &&\n      !type.enum_def->ReverseLookup(\n          static_cast<int>(StringToInt(field->value.constant.c_str()))))\n    Warning(\"enum \" + type.enum_def->name +\n            \" does not have a declaration for this field\\'s default of \" +\n            field->value.constant);\nSince f431a965 (idl_parser.cpp 656-663):\ncpp\n  if (type.enum_def &&\n      !type.enum_def->is_union &&\n      !type.enum_def->attributes.Lookup(\"bit_flags\") &&\n      !type.enum_def->ReverseLookup(static_cast<int>(\n                                 StringToInt(field->value.constant.c_str())))) {\n    return Error(\"default value of \" + field->value.constant + \" for field \" +\n                 name + \" is not part of enum \" + type.enum_def->name);\n  }\nChecking && IsScalar(type.base_type) && !struct_def.fixed check was removed by this commit.. @zplzpl Can you fix it and add a test for this case?. The Monster already has Test in Vec3 in Monster.\nhttps://github.com/google/flatbuffers/blob/b99332efd732e6faf60bb7ce1ce5902ed65d5ba3/tests/monster_test.fbs#L24-L38\nMay it be better to use these already existing structures?\nCreate for Test and Vec3:\n```C#\npublic static Offset CreateTest(FlatBufferBuilder builder, short A, sbyte B){}\npublic static Offset CreateVec3(FlatBufferBuilder builder,\n                                                              float X,\n                                                              float Y,\n                                                              float Z,\n                                                              double Test1,\n                                                              Color Test2,\n                                                              short test3_A,\n                                                              sbyte test3_B){}\n```\nA new Create... method will be added to each struct (like a copy constructor):\nC#\npublic static Offset<Vec3> CreateVec3(FlatBufferBuilder builder, Vec3 v3){}\npublic static Offset<Test> CreateTest(FlatBufferBuilder builder, Test t){}\nright?. You are right default constructor should be implemented.\nThe new version doesn't use default constructor: #5107\nCan you try to compile this code?\nI have tested it on the GCC 7.3 and Clang 7.0.. Thank you for help. I have built \"def_nan_csharp_java\" branch with your locker file.\nThis tool will be helpful for me.. @pyottamus \nThe C++, C#, JAVA, PHP Monsters return simple ASCII string \"MONS\".\nHowever, you may be right with the escaping.\nDeclared grammar allows \\ symbols in the file_identifier, as mentioned by @rw.\nIs it possible to use the raw-string like r'\\u01' or r'MON\\' instead of hex-string? (https://docs.python.org/3/reference/lexical_analysis.html)\npython\n //in the event any of file_identifier characters are special(NULL, \\, etc), problems occur.\n//to prevent this, convert all chars to there hex escaped equivilent\nreturn flatbuffers.util.BufferHasIdentifier(buf, offset, b\"\\x4D\\x4F\\x4E\\x53\", size_prefixed)\nGrammar: https://github.com/google/flatbuffers/blob/master/docs/source/Grammar.md\n\\\".*?\\\" from https://regex101.com/\n/\n\\\" matches the character \" literally (case sensitive)\n.*? matches any character (except for line terminators)\n*? Quantifier \u2014 Matches between zero and unlimited times, as few times as possible, expanding as needed (lazy)\n\\\" matches the character \" literally (case sensitive)\n@aardappel \nident\nIt is possible to add a trivial_string = \\\"[^\\\\]*?\\\" to the grammar (any characters except \\).\nThe idl_parser already has attr_is_trivial_ascii_string_ flag to signal that a string is a string without escape sequences.\nOr enable ident-like strings only:\nfile_extension_decl = file_extension \\\"[a-zA-Z_][a-zA-Z0-9_]*\\\" ;\nfile_identifier_decl = file_identifier \\\"[a-zA-Z_][a-zA-Z0-9_]*\\\" ;\nAnother way: add new tests with various file identifiers, including surrogate pairs.\n. This should be fixed by #5131 if you build with /std:c++17.\nThe master(HEAD):idl_parcer.cpp already has the attribute fallthrough to notify a compiler:\nhttps://github.com/google/flatbuffers/blob/63d51afd1196336a7d1f56a988091ef05deb1c62/src/idl_parser.cpp#L414-L419. I'm sorry, this my bug. I checked this only with clang 7.0 and MSVC2017.\nI will fix this as soon as possible.\nhttps://github.com/google/flatbuffers/blob/347dba8501a6c4deacc63afd0554fbfe5048104f/src/idl_parser.cpp#L414-L419\nWorkaround:\n- delete or comment line 417\nor\n- replace FLATBUFFERS_ATTRIBUTE(fallthrough); by [[fallthrough]];\n. @neomantra \nIs it possible to reuse travis.yml as the template (or create a new template with GPRC, sanitizers and ctest run)?\nhttps://github.com/google/flatbuffers/blob/63d51afd1196336a7d1f56a988091ef05deb1c62/.travis.yml#L91-L103\nWith test environment FLATBUFFERS_BUILD_GRPCTEST=ON, DFLATBUFFERS_CODE_SANITIZE=ON and\nhttps://github.com/google/flatbuffers/blob/63d51afd1196336a7d1f56a988091ef05deb1c62/.travis.yml#L5-L8\n. Probably, the idea to test grpc with all compilers was not good.\nThe build and test time is 2 hours. This is too long.\nhttps://travis-ci.org/google/flatbuffers/jobs/483157950\n$ docker build -f tests/docker/Dockerfile.testing.cpp.clang_3_8_1 .\n-- The C compiler identification is Clang 3.8.1\n-- The CXX compiler identification is GNU 6.3.0\nGNU 6.3.0 is GCC 6.3.0.\nIs it right that CXX compiler identification is GNU 6.3.0 with ENV CCX /usr/bin/clang++?\ntests/docker/Dockerfile.testing.cpp.clang_3_8_1 looks correct:\nENV UBSAN_OPTIONS \"halt_on_error=1\"\nENV ASAN_OPTIONS \"halt_on_error=1\"\nENV CC /usr/bin/clang\nENV CCX /usr/bin/clang++. @aardappel \nDo you have a doubt about this pull-request?\nThis PR will enable testing Flatbuffers with the latest versions of GCC and Clang.\nI checked this PR with d8210d5a83faa345046648e520d82b54ea724e35, all CI passed.. Thank you for the fix.\nThese typos was added by 4ed6fafdfa5689d72e821baec522a9d03ee6d652.\nThe MSVC2017 and VSCode open this file as UTF-8 without BOM, therefore, they are invisible in editors and during compilation.. 1) FlatBuffers::Clear()\nThe Clear is like std::vector::clear().\nThe flatbuffers::FlatBufferBuilder reuse previously allocated memory until Release().\nThe test.cpp and message_builder_test.cpp have many examples.\n2) It is impossible to implement a fully static memory layout.\nFor the Parser it is impossible to avoid memory allocation due to intensive std::string usage.\nFor the Builder you can minimize calls to the new, it depends on the Release() usage.\nBy default, the flatbuffers::FlatBufferBuilder::ctor preallocate 1024 bytes.\nYou can change this value manually or use a trick:\nc++\n  flatbuffers::Parser parser;\n  uint8_t* tmp;\n  parser->builder_.CreateUninitializedVector(prealloc, 1, &tmp);\n  parser->builder_.Clear();\nIn C++, you can trace all calls to the new/delete.\nSpecial memory allocators for the std and flatbuffers::FlatBufferBuilder can help you.\nHow small your embedded platform?\nSimplest way - build test.cpp with a target compiler and run it.\nI have never seen an exception from the Flatbuffers.\nYou can try to disable exception support if you have own new/delete operators.. If you are only getting and reading binary Flatbuffers messages:\n- needn't the Parser\n- needn't Builder\nThe flatbuffers::GetRoot<T> + generated code gives access to data:\nhttps://github.com/google/flatbuffers/blob/347dba8501a6c4deacc63afd0554fbfe5048104f/tests/test.cpp#L2369-L2378\n. Have you tried to compile the code?\nWhat part of Flatbuffers library do you plan to use on the embedded side?\nAlmost all compiler specific code located in one file /flatbuffers/base.h.\nThis discussion may help you: https://github.com/google/flatbuffers/issues/5135\nI think it is possible to run Flatbuffers on the SMT32F4 (the F407 / 417 has enough flash memory).\nYou can try GCC for a prototype. The GCC is comparable to IAR or Keil for a soft real-time. \nUpd:\nThis can also may be useful: https://github.com/google/flatbuffers/pull/4700. flatbuffers::numeric_limits<CTYPE>::max()) returns 0xFFFFFFFF interpreted as -1.\nThe 0 is greater than -1.\nThis issue has a simple solution, but there is a hidden problem.\nInternally, the underlying type of any enums is int64_t (see struct EnumVal).\nTherefore it is impossible to use enum values large then max(long) even if the fbs-enum has ulong underlying type.\nstatic_cast<int64_t> from uint64_t is implementation-defined (platform dependent) since C++03:\n\nIf the destination type is signed, the value is unchanged if it can be represented in the destination type > (and bit-field width); otherwise, the value is implementation-defined.. @iceb0y do you already have a solution?\n\nI think it is acceptable to use simple static_cast<int64_t>(u64_val) to store a new enum value to the vector without type punning.\nImplicit signed to unsigned cast is modulo 2n cast by the C++ standard. \nBut unsigned to signed cast is implementation-defined by the C++ standard if a u64 value doesn't fit int64.\nDe facto the unsigned to signed cast is modulo 2n too.\nI hope, the C++20 standard will make this cast well-defined:\nhttps://en.cppreference.com/w/cpp/language/implicit_conversion\nProposal:\nhttp://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0907r3.html. @aardappel \nI have got a draft for this issue.\nThe solution will be ready in one or two weeks.\nProbably #5194 may depend on this issue.. Embedded *.FBFS files is good idea.\nIn my view, is C++ not the best choice for code generation (working with strings, regex, files).\nThis can be implemented on bash or batch level by calling a python script, for example. \nSimple generator without abstract factory:\n```python\ndef example(fbfs_fname, cpp_file, root_type):\n  with open(fbfs_fname, mode='rb') as file:\n    bin_fbfs = bytearray(file.read())\n  data_len = len(bin_fbfs)\n  n_chunk = 64\nwith open(cpp_file, 'w+',encoding='ascii') as out_file: # append if exist\n    # templated getter, used by abstract factory\n    out_file.write(\"template<> struct fbfs_getter<{}> {{\".format(root_type))\n    out_file.write(\" static const unsigned char data[{}+1]; }}\\r\\n\".format(data_len))\n    # the array can be placed into separated cpp file\n    out_file.write(\"const unsigned char fbfs_getter<{}>::data[{}+1] = {{\\r\\n\".format(root_type, data_len))\n    for i in range(0, data_len, n_chunk):\n      chunk_data = bin_fbfs[i:i+n_chunk]\n      out_file.write(\"\".join(\"{},\".format(b) for b in chunk_data) + \"\\r\\n\")\n    # append zero-termination\n    out_file.write(\"0 };\\r\\n\")\n  pass\nif name == \"main\":\n    example('monster_test.bfbs', 'file_to_append.cpp', 'example::Monster')\nThis script can generate embedded data and factory method used by an abstract factory:cpp\ntemplate<> struct fbfs_getter { static const unsigned char* data[11624+1]; }\nconst unsigned char fbfs_getter::data[11624+1] = {\n28,0,0,0,66,70,66,83,0,0,0,0,16,0,28,0,4,0,8,0,12,0,16,0,20,0,24,0,16,0,0,0,76,0,...\n```\nI implemented embedded *.FBS-library in a similar way using C++ raw-strings uR\"(fbs-body)\".\nEasy to test, modify, maintain. \n@sjoblom65 what do you think about this?\n. @sjoblom65 \n- If the flatc will generate embedded fbfs-data  it should generate a standalone cpp file with this data.\n- The generated header can declare an interface to this data.\nThe binary representation of fbs isn't invariant of the original fbs-schema, therefore, embedding binary data into a generated header file isn't good solution.\nIn the future, a newer version of flatc may change or optimize fbfs binary presentation (in a compatible way or with versioning). It is possible a situation when neither a fbs-schema nor before generated C++ code changed but a lot of lines in the generated file are updated. As consequences: full rebuild of application (data in header), changes in a source code file without changes in the source code,  a code review, repository update.\nThis will be a nightmare for a long life project (5-10-20 years).\nWhat will a new developer think in 2030, when the embedding context is lost?\nThat is why I preferred to embed original FBS text files into application code using raw-string.\nPython or not:\nAll modern desktop Linux distros have at least Python-2.7 and Perl. If you have a development environment with installed gcc, clang or msvc then you have Python with high probability.. Ok, * .cpp or * .h is just a style or habit. I try to avoid placing data in header files. The binary scheme is data. It is neither type nor interface.\nBinary schema file has all data for introspection. One can extract a fully qualified name of root table from a standalone bfbs-file then append an access method and schema into selected cpp, h, java or cs file. There are at least two ways to do this without changes in flatc.\nIn my opinion, flatc is like a swiss-knife (see IDLOptions struct). Perhaps it should be divided into a set of utilities with pipeline capabilities.\nAbility to embed a bfbs-scheme in the source code will be a useful feature, regardless of implementation (flatc flag or a separate utility).. @aardappel what do you think?\n. @krojew \nCan you update the description for the request?\nHow the issue related to clang-7?\nDuplication?\nhttps://github.com/google/flatbuffers/pull/5164. Isn't better to add fake read or write access instead of preprocessor macro, for example to ctor body?\n(void)upper_bound_\n. https://github.com/google/flatbuffers/blob/cc7f9b89f3795b44bb7ec38ac96c15f95e1c7d24/include/flatbuffers/flatbuffers.h#L2173-L2188\n(void)upper_bound_; before return at 2184 may resolve the issue?. #5171. Can you explain the context of the issue?\nIs it possible to reproduce the error as part of tests.cpp with help of Monster schema?. The void InvalidNestedFlatbufferTest() fom #5179 is good start point.\nCan you add this test to the tests.cpp with short comments?\nThe testnestedflatbuffer filed has table Monster type.\nThe Monster has several required fields which are ignored by the test.\nAs a consequence, the parser should return false, right?\nYour test doesn't check a \"true\"-case with a correct testnestedflatbuffer string.\nThis second case may be added to the monsterdata_test.json file if all languages have the support of nested buffers.. Yes, this looks good.\n@ll-antn can you re-save test.cpp as UTF-8 without BOM?\nThe BOM was accidentally added by me at 12c4c223. This is the only file in the repository with BOM.\n. @bwelling \nYes, this strange behavior could have been introduced by this commit (4ed6fafd).\nIn the Parser::ParseSingleValue under some condition the string is converted to number then back to  string using NumToString<CTYPE>.\nhttps://github.com/google/flatbuffers/blob/a1f14005ab823adc1300754fd37c01e9842ed4bc/src/idl_parser.cpp#L1513-L1524\nI think this code is correct.\n. The specialization NumToString<char> is not good idea.\n\nWhen things unexpectedly end up as char, the culprit is usually typedef char int8_t;\n\nThe typedef char int8_t; is always the culprit by definition.\nFundamental types (by the standard):\n\nPlain char, signed char, and unsigned char are three distinct types, collectively called narrow character types.\nIn any particular implementation, a plain char object can take on either the same values as a signed char or an unsigned char; which one is implementation-de\ufb01ned.\n\nThe 'int8_t' and 'uint8__t' types for  <cstdint> are optional types by the standard.\n\nTheoretically neither int8_t nor uint8_t have to be char types but practically they are implemented that way.\nWhat is the Strict Aliasing Rule and Why do we care?\n\n@aardappel If the code depends on the behavior of these types may add static asserts for them?\n```cpp\nstatic_assert(std::is_signed, \"text\");\nstatic_assert(std::is_unsigned, \"text\");\n// check that reinterpret_cast is same as reinterpret_cast\nstatic_assert(std::is_same, \"text\"); \n// optional\nstatic_assert(std::is_same, \"text\"); \nstatic_assert(std::is_same, \"text\");\n```\nAnother solution, replace int8/uint8 to explicit signed/unsigned char in the type definition table?\nhttps://github.com/google/flatbuffers/blob/a1f14005ab823adc1300754fd37c01e9842ed4bc/include/flatbuffers/idl.h#L49-L55\nUpdate:\n\nThe specialization NumToString<char> is not good idea.\n\nBetter specialization:\ncpp\ntemplate<> NumToString<char>(T t); // disable T=char. @bwelling How you build your project? Have you tried -fsigned-char flag?\n# Certain platforms such as ARM do not use signed chars by default\n  # which causes issues with certain bounds checks.\n  set(CMAKE_CXX_FLAGS  \"${CMAKE_CXX_FLAGS} -fsigned-char\"). @Quuxplusone  You are right, \nThe char as is distinct type no matter how the compiler interprets it (signed or unsigned).\nThe problem is in the expectation of int8_t/uint8_t, and not in char type.\nPerhaps, NumToString<> should be extended to 'int8_t/uint8_t'.\nThe -fsigned-char flag only resolve the ambiguity:\n\nIn any particular implementation, a plain char object can take on either the same values as a signed char or an unsigned char; which one is implementation-de\ufb01ned.\n\nBut this flag doesn't cancel:\n\nPlain char, signed char, and unsigned char are three distinct types, collectively called narrow character types.. Agree.\n\nIs sparc64 allow unaligned memory access?\nIf no, with some probability you may catch a fault.\nThe directives __supress_ubsan __ (\" alignment \") have been added to ignore errors from UBSAN.\nUBSAN detected at least one unaligned read in the test.cpp.\n. #5209 is the right decision, but it can't resolve UA only localizes it in the ReadScalar.\nUbuntu 18.0, clang 7.0.1 with ubsan:\n```cpp\n// Suppress Undefined Behavior Sanitizer (recoverable only). Usage:\n// - supress_ubsan(\"undefined\")\n// - supress_ubsan(\"signed-integer-overflow\")\ndefine supress_ubsan(type)\n// #if defined(clang)\n//   #define supress_ubsan(type) attribute((no_sanitize(type)))\n// #elif defined(GNUC) && (GNUC * 100 + GNUC_MINOR >= 409)\n//   #define supress_ubsan(type) attribute((no_sanitize_undefined))\n// #else\n//   #define supress_ubsan(type)\n// #endif\n```\nubsan.log. The reported problem located in the Flexbuffers code (see attached screenshot)\nubsan\nTest at the line 2262 not passed:\ncpp\nTEST_EQ(tvec.size(), 3);. Thank you for the report.\nThis can be false positive due to hidden mutually exclusive conditions.\nCan you reproduce this conditional flow in run-time as an isolated unit-test in the test.cpp?\nFlatbuffers cmake script has the ability to enable ASAN and UBSAN for the flattests target using -DFLATBUFFERS_CODE_SANITIZE=ON flag.. You are right. Initial PR was wrong.\nThe issue:\n- The  flag core.autocrlf is true (Windows LF->CRLF, CRLF->LF);\n- MSVC loads .editorconfig with the end_of_line = LF rule;\n- New source code lines will be saved with LF while an initial file has CRLF according to autocrlf=true;\n- git-commit generates the warning about the mixing CRLF and LF line endings.\nLatest commit removes  end_of_line = LF from .editorconfig and reverts .gitattributes file.. How one can reproduce this issue?\nI do not quite understand your workflow.\nLet, I have Flatbuffers repository at \"D:\\Source\\flatbuffers\".\nCan you create the smallest MSVC solution (with short main.cpp)?\n. Comparison with zero is fixed.\n. > opts.generate_hexfloat_in_json && IsFloat(type.base_type). Could it be better to check both representations of a pi-number in one test, without the external argument \"useHexfloat\"?\nFirst test as is:\n\nTEST_EQ(std::string::npos != jsongen.find(\"testf: 3.14159\"), true);\n\nNext, regenerate json output:\n\nparser.opts.generate_hexfloat_in_json = true;\nTEST_EQ(GenerateText(parser, builder.GetBufferPointer(), &jsongen), true);\nTEST_EQ(std::string::npos != jsongen.find(\"testf: 0x1.921fa0p+1\"), true);\n. I assume that this is portable: https://en.cppreference.com/w/c/io/setvbuf\nProblem was with MSVC2010 when test failed. Most part of stdout has been lost.\nAnother solution it to use sprintf to stderr, The stderr is unbuffered by default.. I too never understood global C-locale and errno.\n\n\"but that involves writing our own strtod? Are there any better solutions?\"\nThe proposed approach with strtod_l and strtof_l is acceptable?\n. With help of CMake we can detect that strtod_l exist in system (Linux) during build and automatically link it.\nBut should 99.9% of users pay for this? Most of them dont know about locales.\nThis could be actual if Flatbuffers library is part of another library or SDK (my case) and a developer isn't end-user.\nWorst case - if Flatbuffers library is installed to /usr/lib.\nI can write small performance test strtod vs strtod_l and check under Ubuntu and Windows to estimate cost of strtod_l.\n. I found that glibc implements strtod as the call to strtod_l with a pointer to the current locale as the argument.\nThis means that we do not lose in performance (at least on Linux).\n. I don't understand this test.\nThis declaration: union Any { Monster, Weapon, Pickup } is correct.\nBut union X { Y = 256 } is incorrect. Here we refer to undefined type 256, not number.. Here we have code duplication. The function atot already checks a range of parsed value.\nBut internal underlying type for enums is int_64.\nMoreover, we have ParseSingleValue which checks a base type and calls atot if check_now=true.\nThis function is used for checking of metadata and scalar default values.\nCan we reuse it here?. AInUnionAgain:A = 10 is not equal to AInUnionAgain = 10 from a sematic point of view.\nIn any case, this is a minor issue, I think this will be resolved automatically.\nExtra check if (ev.value < 0 || ev.value >= 256) will be redurant.. If @aardappel agree with bool-base enums it will be very simple:\n- comment or remove one line of code\n- update grammar documentation\nBoolean values as a base for enum was disabled before refactoring, I only save this behavior.\n. @aardappel \nIs following declaration acceptable?\nenum X:ubyte { A = true, B = false }\nOr simply: Can boolean constants true and false are used for implicit initialization (1, 0) of an integer-type values?\nNow true or false can be applied only to a bool-type value.. If someone does not use CMake to build?\nHowever, if someone has installed LLVM, he can install and run CMake.. @fbenkstein\nCould you review this code?\nAre you agree with these changes and comments?. This directive is not needed since the commit 2e7806e by Harsh Vardhan at 08/19/2016.\nThis commit has removed the dependency on std::unique_ptr<>.. Yes, I missed this.\nI will check with FLATBUFFERS_CONSTEXPR definition under Clang.\nShould this code be removed as obsolete?\n\nBackwards compatability for g++ 4.4, and 4.5 which don't have the nullptr and constexpr keywords.\n\nhttps://github.com/google/flatbuffers/blob/751a3d0f7804b97a423117c56926180c92125134/include/flatbuffers/base.h#L72-L90. I agree. MSVC2017 full release without debug info: CMake Release x64 build (little endian).\n1) T ReadScalar - flatbuffers.lib size is 3794904\n2) add inline T ReadScalar - 3790900\n3) add inline T EndianScalar - 3785982\n4) add inline T EndianSwap - 3785982\nHave checked this behavior under Linux (GCC 7.3 and Clang 7.0) with Release x64.\nSize of flatbuffers.a is independent of inline keyword.\nThe C++ standard allows inline specifier for template functions, even without specialization.\nA template isn't inline by default.\nThe same in the book (if I right): C++ Templates: The Complete Guide. @aardappel \nI have checked (https://godbolt.org/z/e0vros).\nThe -std=c++11 is available since gcc 4.7.1.\nIt is impossible to use C++11 without this specifier.\nGCC 4.7.1 and higher have nulptr_t. Moreover type nulptr_t not used in code.\nThe constexpr not used directly also. Instead of it has FLATBUFFERS_CONSTEXPR.\nI think this code is redundant.. Checked and fixed in #5105. You are right, This is a good idea.\nThe only public method of FloatConstantGenerator is const by declaration.. Updated.. What C++ should do?\nLooks like a generator not as the statement Has a property.\nDo you want to generate esapedID string based on the parser_.file_identifier_?\nWhy not use the std::string and std::stringstream for this without sprintf?\nLike this:\nhttps://github.com/google/flatbuffers/blob/63d51afd1196336a7d1f56a988091ef05deb1c62/include/flatbuffers/util.h#L183-L200. Remove (comment) protobuf compilation while GRPC doesn't work (#5099)?. ",
    "bbczeuz": "Hi Wouter,\nThanks for the reply. My problem is, that the JSON scheme cannot be changed (interfacing to InfluxDB). tags keys can be pre-defined in a limited application, but in general they can change even at runtime.\nvalues format: How about anonymous vectors of unions (values is an array or rows; a row has 1+ fields; format is the same for every row, but in very difficult to predict)?\nYou can close the issue if you think flatbuffers is probably not useful in my application.\n. Hi Wouter,\nThanks for the reply. My problem is, that the JSON scheme cannot be changed (interfacing to InfluxDB). tags keys can be pre-defined in a limited application, but in general they can change even at runtime.\nvalues format: How about anonymous vectors of unions (values is an array or rows; a row has 1+ fields; format is the same for every row, but in very difficult to predict)?\nYou can close the issue if you think flatbuffers is probably not useful in my application.\n. ",
    "Guillaume227": "\nIf you can't change the JSON, then yes, there's no way to parse it with FlatBuffers at the moment.\n\nDoes 'at the moment' imply that there is a plan to support parsing nested vectors at some point? What is the problem from a fbs perspective?\nTaking a simple example, I would think the following json data:\n```\n{\n      \"polygon\": [[1,2], [3,4], [5,6]]\n}\ncould work with a schema like\nstruct Point{ \n    x: int;\n    y: int;\n}\ntable Polygon{\n    points: [Point];\n}\ntable MyRoot\n{\n    polygon: Polygon;\n}\nroot_type MyRoot;\n```\nAre there complicated/edge cases that cannot be handled nicely?\n. @gwvo thanks for the detailed answer.\nI have started work on that and I added a new test:\nin monster_test.fbs:\nfbs\ntestnestedlist:[Test] (id: 30); // Test is declared above as: struct Test { a:short; b:byte; }\nin monster_test.golden:\nfbs\n  testnestedlist: [\n    [10, 20],\n    [30, 40]\n  ]\nOne test fails when comparing generated json to golden as we would expect since the generated json shows as before:\ntestnestedlist: [\n    {\n      a: 10,\n      b: 20\n    },\n    {\n      a: 30,\n      b: 40\n    }\n  ]\n}\nI would feel bad not including that basic test in the PR. Do you have a suggestion about how to handle that test case?\n. Done thanks. PR #4338 sent.. I signed it!\n2017-06-03 9:38 GMT+02:00 googlebot notifications@github.com:\n\nThanks for your pull request. It looks like this may be your first\ncontribution to a Google open source project. Before we can look at your\npull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://cla.developers.google.com/\nhttps://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll\nverify. Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your\n   GitHub username or you're using a different email address. Check your\n   existing CLA data https://cla.developers.google.com/clas and verify\n   that your email is set on your git commits\n   https://help.github.com/articles/setting-your-email-in-git/.\nIf you signed the CLA as a corporation, please let us know the\n   company's name.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/4338#issuecomment-305958529,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AJAgcIEGzxMBxUAkXFcEm8-FyNuoqKJAks5sAQ1ugaJpZM4NvAyg\n.\n. Big thank you for all the hard work that goes into maintaining this super useful project.. In the top level list case (illustrated in my comment just above), json effectively omits the name of the vector field from the root type object we are trying to map it to. \nThat checks makes sure there is no ambiguity introduced by skipping the field name: it enforces there is a single field in the root type object.\n. That '[' there is to support the case where the top level element in the json file is a vector, which is valid json.\nExample taken from the TopLevelListTest in the PR:\n[ {a:10, b:20}, {...} ]\n\nCorresponding FBS could be:\nstruct Test { a:short; b:byte; } table T { F:[Test]; } root_type T;\nI could not find a way to only change ParseTable for that case, as ParseTable relies on being called down the callstack from a ParseVector() call.. That statement was actually factored out as a consequence of introducing an if/else statement just above and so is part of the changes. Point taken thanks! The PR should now be stylistically correct.. Ok done (sorry was misled by other parts of that file which have more blanklines to think it was acceptable, like Parser::SkipJsonObject). Done.. OK I took it out. I want Flatbuffer to be my one-stop parsing solution for all the json I have to deal with in that project I am working. Some json files I have no control over are in that straight top level array format so I will just keep the change around in a fork.. After removing that top level list parsing, the change is now all contained in idl_parser.cpp, no need for the include/idl.h change.. ",
    "awaizman1": "No sorry\n. ",
    "benstadin": "Thanks @sahiljain. I think it should be possible to implement such analyzer. Or rather, it should be possible to generate analyzer code next to the generated code for a given schema. \n. Thanks for the quick reply. \n. ",
    "americanjeff": "That return signature looks good.\nAlso, not that it matters for generated code, but the indentation of the offending block looks kind of off.\n. Leaving it out sounds like the best option.  I doubt I would use it.\n. I'm afraid I don't know what a PR is.\n. If there was an example in the tutorial I would have figured it out.\nOn Sun, Aug 14, 2016, 11:11 PM Robert notifications@github.com wrote:\n\n@americanjeff https://github.com/americanjeff I'd like to improve the\ndocs or error handling to make this easier to debug. Do you have any\nsuggestions for how to do that?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/3988#issuecomment-239735566,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGQqs_MaKrGyUd-k-SUqD2jAyHBW-l56ks5qgAMmgaJpZM4JkIIa\n.\n. \n",
    "namusyaka": "Any progress on this?. @rw @aardappel  FYI, this change works smoothly on GAE/Standard Environment.. ",
    "yome": "This works well on GAE/Standard, it would be great if it could be merged as otherwise flatbuffers isn't usable out of the box with Go on App Engine.. ",
    "akazakov": "I signed it!\n. ",
    "hvardhanx": "updated the PR @gwvo\n. --python --grpc and --cpp --grpc works now. Added the generated python file. \nThere was some unique_ptr bug. Does this look good ? @gwvo \n. I made a new one here: https://github.com/grpc/grpc/pull/8063\nI thought that PR got outdated. So, I updated and made a new PR.\n. updated flatc.cpp\n. Should we print an error message like \"No language specified\" if only --grpc flag is specified and no language is specified?\n. Updated PR\n. Sorry for the delay. Updated PR.\n. @ekarlso There's some work left on the grpc side (https://github.com/grpc/grpc/pull/8063) which needs to be merged too.\n. updated.\n. Oh. I can fix them if you point them out.\n. I have added comments on the things I have fixed. Have I missed something ?\n. There was some significant change to the python's code generator. So, I think I might need to update them here as well.\n. I was working on https://github.com/grpc/grpc/pull/8063 and I was facing some problems passing the tests. It would be great if you could review it.. I'll fix the merge conflicts.. I'll rework on it and send another patch.. Yep, planning to work on it this week.. I have been running into few errors while running make : \nbash\n[  2%] Building CXX object CMakeFiles/flatc.dir/src/idl_gen_grpc.cpp.o\nIn file included from /home/hvardhan/flatbuffers/src/idl_gen_grpc.cpp:27:0:\n/home/hvardhan/flatbuffers/grpc/src/compiler/python_private_generator.h:58:16: error: \u2018grpc::string grpc_python_generator::{anonymous}::PrivateGenerator::GetGrpcServices()\u2019 used but never defined [-Werror]\n   grpc::string GetGrpcServices();\n                ^~~~~~~~~~~~~~~\n/home/hvardhan/flatbuffers/grpc/src/compiler/python_private_generator.h:55:3: error: \u2018grpc_python_generator::{anonymous}::PrivateGenerator::PrivateGenerator(const grpc_python_generator::GeneratorConfiguration&, const grpc_generator::File*)\u2019 used but never defined [-Werror]\n   PrivateGenerator(const GeneratorConfiguration& config,\n   ^~~~~~~~~~~~~~~~\ncc1plus: all warnings being treated as errors\nAny ideas what could be the cause?. Does the grpc generator for c++ work? I tried:\nflatc -c --grpc  monster.fbs. But only  monster_generated.h was generated and no monster.grpc.fb.cc or monster.grpc.fb.h was generated.. No. :sweat_smile: Sorry, will take a closer look next time. I will try to write rpc_service and check.. Does this look good?\nmonster_grpc_fb.py\nbuilder.py\ngrpctest.py\nThis works, just need to make some changes so that desired monster_grpc_fb.py is generated.. Added the changes. Just need to fix the python generator.. Ready for review.. How is the data structured in the Bytes array?. So, should we send the whole Bytes array over an RPC or only a slice of an array which is required?. How do we determine which slice to send?. I'm okay :). Friendly ping @aardappel  :). gdb shows error occurs in GetServices.\n. You mean this file should be saved in Mygame/Example directory whenever we generate it ?\n. oops didn't notice that\n. I think we'd need this. If we remove has_grpc_lang_set check then even if we provide the language with the grpc flag, this condition would run and will show an error. \n. Fixed. \n. Fixed.\n. Fixed.\n. Fixed.\n. Fixed.\n. This gave me an error saying that Monster_fb is not a module. Same for Weapon_fb.. I tried removing the import Monster_fb and import Weapon_fb while testing, after that, this line gave an error saying that, name Monster is not defined.. I think we need to change get_input_type_name and get_output_type_name in idl_gen_grpc to something more appropriate?. I guess, I will have to write my own SerializeToString and FromString methods for python generator? Since those methods are protobuf specific. Are there any serializers or deserializers already written for flatbuffers in python? I saw that C++ grpc has Serialize and Deserialize methods.. It should happen here.. We are returning a tuple from the Store function in the MonsterStorage class above. So, the response variable here is a tuple.. I'll add some comments too. . When we make a UnaryUnary type of call. operation_send_message is called. In this line operation_send_message is called and it has parameters serialized_request which is a byte string. Here is the implementation of operation_send_message where it expects the data variable to be of bytes type. An exception is thrown if the serialized_request isn't a byte string 'Exception serializing request!'. \nThe pickle module is good for serializing any python object to bytes. pickle.dumps returns the serialized representation of the object.. To access the SerializeToString and FromString function from flatbuffers. Just like we do in CreateString.. Should I just upload the monster_grpc_fb.py in the grpc/tests/python folder or include it with the tests/MyGame folder?. What do you mean by \"directly uses the serialized Flatbuffer data\"? How would I use the Flatbuffer data without converting it into bytes?. request_serializer in monster_grpc_fb.py requires a function which converts any object to bytes.. Looks like @bsoniam isn't using SerializeToString and FromString in the generated file (monster_test_grpc_fb.py). So this can be safely removed.. I don't think we need the *_pb2_grpc.py file. It is generated by the proto plugin. :). Since we are not using SerializeToString and FromString we can remove this too.. ",
    "mi6crazyheart": "@gwvo  I'm following the same example which has given in the documentation. Using the same SCHEMA file with PHP. Then from SAMPLES directory I'm executing ./php_sample.sh command to execute the SampleBinary.php file.  Here is terminal output after executing the command\n\nThen in that file (SampleBinary.php) I'm using following piece of code to generate a binary file with all buffer content.\n```\n$file = 'buffer_content.bin';\n$output = serialize($builder->dataBuffer());\n$fp = fopen($file, \"wb\");\nfwrite($fp, $output);\nfclose($fp);\n```\nThen, in from a simple HTML file I'm making an AJAX call to a SERVER side PHP file which is extracting data from this BINARY file & return back in response. I'm following the example of extracting BINARY file data from Client side code. Following is the HTML file code\n```\n\n\n\n<script src=\"js/flatbuffers.js\"></script>\n<script src=\"js/monster_generated.js\"></script>\n\n<script>\n$( document ).ready(function() {\n    console.log( \"document loaded\" );\n\n    $.ajax({\n        url: \"php/getBufferData.php\",\n        cache: false\n    })\n    .done(function( bufferData ) {\n        console.log('Buffer Data : '+bufferData);\n        //console.log('Data after parsing : '+bufferData.toString());\n\n        var buf     = new flatbuffers.ByteBuffer(bufferData);\n        var monster = MyGame.Sample.Monster.getRootAsMonster(buf);\n\n        var hp = monster.hp();\n        var pos = monster.pos();\n        var name = monster.name();\n\n        console.log(\"hp : \"+hp);\n        console.log(\"pos : \"+pos);\n        console.log(\"name : \"+name);\n    });\n\n});\n\n\nfunction readFile() {\n        var reader = new FileReader(); // This example uses the HTML5 FileReader.\n        var file   = document.getElementById('file_input').files[0]; // \"monster.dat\" from the HTML <input> field.\n\n        reader.onload = function() { // Executes after the file is read.\n            var data    = new Uint8Array(reader.result);\n            var buf     = new flatbuffers.ByteBuffer(data);\n            var monster = MyGame.Sample.Monster.getRootAsMonster(buf);\n\n            var hp = monster.hp();\n            var pos = monster.pos();\n            var name = monster.name();\n\n            console.log(\"hp : \"+hp);\n            console.log(\"pos : \"+pos);\n            console.log(\"name : \"+name);\n        }\n\n        reader.readAsArrayBuffer(file);\n    }\n\n\n\n\n\n\n\n```\nMy getBufferData.php file content\n$buffData = file_get_contents( \"buffer_content.bin\" );\necho $buffData;\nHere is my Chrome console response screenshot.\n\nI don't know what I'm doing wrong ? \n. Hi @gwvo If I don't use serialize I'm getting following error...\nPHP Warning:  fwrite() expects parameter 2 to be string, object given in /home/opsol/Desktop/upload/flatbuffers/samples/SampleBinary.php on line 76\nPHP Stack trace:\nPHP   1. {main}() /home/opsol/Desktop/upload/flatbuffers/samples/SampleBinary.php:0\nPHP   2. main() /home/opsol/Desktop/upload/flatbuffers/samples/SampleBinary.php:123\nPHP   3. fwrite() /home/opsol/Desktop/upload/flatbuffers/samples/SampleBinary.php:76\nThe FlatBuffer was successfully created and verified!\n. Hey, I followed your stackoverflow answer but didn't get any reference towards how to extract the bytes contained in the buffer object. Can you point me towards any sample code? \nBy the way, I've fixed my JavaScript part. It's working fine now. But, I did this testing by making a BINARY file by using command ./../flatc -b monster.fbs monsterdata.json & access that through AJAX request. Now, I want to do this by getting the direct binary response from flatbuffer data object.\n. I think I got the thing now. I'm calling following method from FlatbufferBuilder.php & returning the response data to the AJAX request. It's working now.\n```\npublic function sizedByteArray()\n{\n    $start = $this->space;\n    $length = $this->bb->capacity() - $this->space;\n$result = str_repeat(\"\\0\", $length);\n$this->bb->setPosition($start);\n$this->bb->getX($result);\n\nreturn $result;\n\n}\n```\n. ",
    "jojohello": "I meet a problem here\nI create a c# window Console project\uff0cusing the framework .net 3.5, to learn how to use flatbuffers\nBut it get a error that:\nCannot inherit from seaeled struct  'Struct'\nPlease tell me what happen and how to resolve it. \nThanks!\n. ",
    "sampaioletti": "Awesome thanks for being so responsive. I'll test it out.\n. Verified, thanks for the fix!\n. Thanks @gwvo I didn't want to cloud the post with to much more info (was already too long) but at first I ran the test including the 'parsing' step and even at 10 items the test showed fb slower overall\nfbAve:0.9600000000000364ms this:0.9600000000000364ms\njsAve:0.035000000000763976ms this:0.035000000000763976ms\nAnd was slower at different levels of items in this scenario (100, 10000, etc)\nI've updated the plunkr to reflect that.\nI was merely trying to isolate out that the problem wasn't in the 'parsing' step.\nI would only like to bring it up as we love the library and wanted to see if people far better with js than us had any thoughts on optimizing (or pointed out our tests were bogus). We will look at it and have done some CPU profiling and if we can reduce the number of function calls (maybe some arrays of offsets so we don't have to call functions to get offsets every time) it might help. But not trying to be critical just trying to open a discussion. The reduction in overhead on our server for the serialization step makes the reduced performance on the client more than worth it (since they process for free haha), and I guess thats a core point of the library.\nI'll close this out so you don't have to think about it since you are aware that the access is less performant than native and its not really an issue, and if we come up with any performance increases in our work we'll shoot you a pull.\nThanks again!\n. Yeah i looked at the the Object API posts you mention, but when I went to @evanw s github all I could find was the normal flatbuffers.js file. If we can get ahold of his api I'd be happy to take a look at it and run the same tests to compare the differences, and look at doing a fork with the alternate api so we can see how it goes (since as he mentioned they are using a different library now).\nThanks\n. @gwvo had a thought while looking at some other stuff we were working on and based on your c++ comment. I might try next week if I have some time to run your c++ code through emscripten and see how efficient the generated js is given this scenario. I did a project a while back where we had a small utility library that we converted and the performance really surprised us. API was ugly from a js perspective, but it worked quite well.  I don't think that that is a good long term strategy but if it proves more efficient it might give us a starting point for optimization in the existing code for 'free'. Now that I say that we could even try something similar given the golang generated code and gopherjs project. Both might just provide good test data as a starting point, Just a thought.\n. Curiosity got the best of me so I did a quick project with gopherJS (added it to plunkr), it wasn't faster in any case. However, an interesting thing happened I was having trouble with the gopherJS returning a string since the GO version returns a []byte...so I decided to just do numbers and eliminate looking up the SensorId string value, Instant difference in the 'official' implementation. Now for very small collections (say 10) it is barely slower and on some iterations actually is faster than the json (it is a little slow on first run as you would expect, but optimizes quickly). I think at that level I still have some problems with the way i'm running the test and that probably throws in some GC that makes the results inconclusive, but its night and day vs running with a string. Once you get over 100 items in the collection it becomes faster than the json version, go with 1000-100000 and fb its substantially faster and stays that way (meaning combined time, access is still slower but the tradeoff works now as you would expect). (updated plunkr with this)   \nSo I still want to run the emscrpiten if I get time next week, but this is excellent news for the application I'm working on as it consists of mostly sending thousands of numeric values for realtime graphing purposes. We will just avoid Strings as much as possible or break them out on first pass into objects.\nAnother test we could run would be looking up multiple numeric values to make sure it scales...but that is another day.\nSo I would say from a JS optimization path we should look at how we are handling strings, the cost might be in the UTF8/16 decision tree or elsewhere..but I think it gives us a starting point. I'll continue to look into it, but I'm pretty pleased with the results. Again just my 2 cents.\nSorry to continue to post to a closed issue..but at them moment I think its a safe place to house my ramblings.\n. Not my wheelhouse, but I'll run some tests..quick google gives a couple ideas..perhaps http://ecmanaut.blogspot.com/2006/07/encoding-decoding-utf8-in-javascript.html?m=1 is worth a try. Glad to be of help. We try and give back if we are using a project. Between the performance and header only c++ this project fits us perfectly (tons of real time data, cross platform and embedded).\n. I signed it!\n. Tested the flatbuffers.ts and generated.ts files in our project...worked like a champ\n. Changed the example to be compatible with the one in the flatbuffers/tests area. Both the flatc->generated js file and the flatc->generatedts->tsc->generatedjs files pass the flatbuffers/tests/JavaScriptTest.js suite good first start.\n. Thanks yeah it was a late night hackathon style was meant to just see how\nit would go and then it worked haha. I'll go through and have a look at\nyour comments. I'll delete the duplicate files I just copied so I'd have\nTemps to work with until ready.\nOn Oct 21, 2016 5:07 PM, \"Wouter van Oortmerssen\" notifications@github.com\nwrote:\n\n@gwvo commented on this pull request.\ngenerally looks like the right direction, just needs a lot of cleanup :)\nIn include/flatbuffers/code_generators.h\nhttps://github.com/google/flatbuffers/pull/4065#pullrequestreview-5332360\n:\n\n@@ -112,6 +112,22 @@ class BaseGenerator {\n     return WrapInNameSpace(def.defined_namespace, def.name);\n   }\n-  std::string GetNameSpace(const Definition &def) {\n\nplease use spacing conform https://google.github.io/\nstyleguide/cppguide.html\nIn include/flatbuffers/code_generators.h\nhttps://github.com/google/flatbuffers/pull/4065#pullrequestreview-5332360\n:\n\n@@ -112,6 +112,22 @@ class BaseGenerator {\n     return WrapInNameSpace(def.defined_namespace, def.name);\n   }\n-  std::string GetNameSpace(const Definition &def) {\n-  const Namespace *ns = def.defined_namespace;\n-  const std::string &name = def.name;\n\nWhy was this needed? There's already the function FullNamespace\nIn js/tsTest/JavaScriptTest.js\nhttps://github.com/google/flatbuffers/pull/4065#pullrequestreview-5332360\n:\n\n@@ -0,0 +1,350 @@\n+// Run this using JavaScriptTest.sh\n\ndon't see any types in this file, why is there a copy?\ntsTest -> ts_test\nIn js/tsTest/monster_test.fbs\nhttps://github.com/google/flatbuffers/pull/4065#pullrequestreview-5332360\n:\n\n@@ -0,0 +1,80 @@\n+// test schema file\n\nplease just use all the existing files rather than making copies.\nIn src/idl_gen_js.cpp\nhttps://github.com/google/flatbuffers/pull/4065#pullrequestreview-5332360\n:\n\n@@ -59,7 +65,14 @@ class JsGenerator : public BaseGenerator {\n       code += exports_code;\n     }\n-    return SaveFile(GeneratedFileName(path_, file_name_).c_str(), code, false);\n-    std::string filename = \"\";\n-    if (ts) {\n\nyes, all these if-thens can use some cleanup. prefer to stick them in a\ntable (see LanguageParameters in idl_gen_general.cpp for an example)\nwhere possible, and otherwise think how to represent the code with as few\nif-thens as possible by sticking them into functions where needed.\nIn src/idl_gen_js.cpp\nhttps://github.com/google/flatbuffers/pull/4065#pullrequestreview-5332360\n:\n\ncode += \"  var offset = this.bb.__offset(this.bb_pos, \" + NumToString(field.value.offset) + \")\\n\\n\";\n   code += \"  if (offset === 0) {\\n\";\n   code += \"    return false;\\n\";\n   code += \"  }\\n\\n\";\n-      code += \"  this.bb.write\" + MakeCamel(GenType(field.value.type)) + \"(this.bb_pos + offset, value);\\n\";\n-      std::string value = \"value\";\n-      if (GenTypeName(field.value.type, true)==\"boolean\") {\n\nif (field.value.type.base_type == BASE_TYPE_BOOL)\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/4065#pullrequestreview-5332360,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AIZod67DYiQahjqA5XocvcebdQpygPUhks5q2UWTgaJpZM4Kb6mW\n.\n. Used the ugly version in two projects that have been running in controlled\nproduction with no issues. Haven't had time to go back and do a final\nrewrite. On my list (: but it works very well.\n\nOn Jan 4, 2017 5:00 PM, \"Wouter van Oortmerssen\" notifications@github.com\nwrote:\n\nStatus?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/4065#issuecomment-270523123,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AIZod3KdVfN6T6B7mmPNaxA2A72rZh1jks5rPDKWgaJpZM4Kb6mW\n.\n. Yes unless someone else has time to twke it over ive just been slammed\n\nOn Mar 3, 2017 12:02 AM, \"gwest7\" notifications@github.com wrote:\n\n@sampaioletti https://github.com/sampaioletti is this still on your\nlist?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/4065#issuecomment-283881764,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AIZod1BlKJD-AVuc252nWstlnP8UGyW9ks5rh7sggaJpZM4Kb6mW\n.\n. Ok sounds good in my fork I just redirected them to the writeInt methods\nlike you suggest  ...so I'll do a PR when I have a chance...I'll look into\nhow they were passing in the first place but since js doesn't have number\ntypes I wonder if it gets implicitly cast so it misses it altogether.\n\nOn Oct 21, 2016 4:36 PM, \"Wouter van Oortmerssen\" notifications@github.com\nwrote:\n\nWe definitely have unsigned ints in our test schema, so I wonder how that\ncan have been missing?\nEither way they should be equivalent to the signed version.. It looks like\nit may work just calling the signed version instead, since all it does is:\nflatbuffers.ByteBuffer.prototype.writeInt32 = function(offset, value) {\n  this.bytes_[offset] = value;\n  this.bytes_[offset + 1] = value >> 8;\n  this.bytes_[offset + 2] = value >> 16;\n  this.bytes_[offset + 3] = value >> 24;\n};\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/4069#issuecomment-255481450,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AIZodwjMP_0F39vKK3q7NGp1f-YUf3pjks5q2T5rgaJpZM4KddMO\n.\n. Funny thing is I didn't catch it the TS compiler did... wouldn't have\nnoticed if I wasn't working on the TS fork\n\nOn Oct 21, 2016 4:44 PM, \"Sam Paioletti\" sampaioletti@gmail.com wrote:\n\nOk sounds good in my fork I just redirected them to the writeInt methods\nlike you suggest  ...so I'll do a PR when I have a chance...I'll look into\nhow they were passing in the first place but since js doesn't have number\ntypes I wonder if it gets implicitly cast so it misses it altogether.\nOn Oct 21, 2016 4:36 PM, \"Wouter van Oortmerssen\" \nnotifications@github.com wrote:\n\nWe definitely have unsigned ints in our test schema, so I wonder how that\ncan have been missing?\nEither way they should be equivalent to the signed version.. It looks\nlike it may work just calling the signed version instead, since all it does\nis:\nflatbuffers.ByteBuffer.prototype.writeInt32 = function(offset, value) {\n  this.bytes_[offset] = value;\n  this.bytes_[offset + 1] = value >> 8;\n  this.bytes_[offset + 2] = value >> 16;\n  this.bytes_[offset + 3] = value >> 24;\n};\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/4069#issuecomment-255481450,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AIZodwjMP_0F39vKK3q7NGp1f-YUf3pjks5q2T5rgaJpZM4KddMO\n.\n. Of course, I'm all for making this work for the project. My original work was a late night hack that I just put together so I could use it quickly in a project I needed it for, sort of a proof of concept. I have been meaning to get back to it but have not had the time. I'll try and contribute where I can by reviewing and testing the updated code. After using TS with FB it really works quite well and I think is a perfect match for the project.. \n\n",
    "exhau": "I think it can be a feature. One can choose to use or not to use. In my case, I plan to use flatbuffers internally, like in-memory data storage (can be temporarily stored on hard drive).\nIn current design without inheritance, what is the best practice to represent this relationship?\n. Thank you for your reply. This is an option, but disadvantage is polymorphism.\nLet say there is a buffer from somewhere. I know it is a kind of Monster but don't know the exact type(XxxMonster).\nUsing the inheritance way, the buffer can be read as either a Monster or a XxxMonster with no problem.\nIf we don't know the number of inheritance layers, there is no way to read a buffer of derived type as a base type unless inserting type information into buffer.\n. Thank you guys.\nWhat I am planning to do is to wrap a buffer as a normal class. When a buffer is read from disk or in-memory storage and binded to a certain type, getters can directly access the buf without copying anything. If we wish to edit it, we can unpack it to an object type(copy/deserialize), do changes and then save the changes. Saving is actually packing the object to byte array again. \nHowever, on the other hand, I also wish the classes can be used as normal ones, with runtime polymorphism.\nInheritance can be complex and unpredictable, for example, B:A, C:B, D:B,E:D... (fortunately no multiple inheritance like C:B,A). In the example, class A and E may belong to different modules developed by two teams. E's module is a standalone addon. The solution by \"Union\" is not very reasonable in my case. The module containing A can't know E in advance.\nOur application has a lot of data managed by Key-Value database, and there are frequent derializations (which are expensive) right now. In most time, they are immutable.\nAnd we don't plan to use flatbuffer for network communication or persisted storage. All buffers are temporary running data. Compared to the concern of fragile base class, it is more important for us that inheritances from different team do not touch framework. In other words, we stick to OO but wish to change the memory layout of an (readonly)object. In our case, if framework changes, everything re-generates the code.\nSorry if I didn't make it clear. I just want to utilize flatbuffer to avoid unnecessary copy and deserialization within the library. Maybe it's a misuse or overkill. Forgive me for my amateurish.\n. @gwvo I understand the situation. Do you think it is doable if I fork the repo and do the change myself?\n\nassuming you need to pass part of a \"derived class\" to a function that expects a \"base class\".\n\nI need to pass the entire derived class instead of a part of to a function in base module. And then cast it to a base type. Virtual functions should work.\n. Virtual methods are implemented in the wrapper class. As long as the buffer has the type information to construct correct wrapper class.\nAssuming there exists \"Inheritance\" by appending, one can cast a buffer of derived buffer to any base type buffer without any cost.\nTo make it clear, the design could be like the following code.\n```\nBaseBuffer { int typehash; }   // general base with only type information\nMonsterBuffer : BaseBuffer{...}\nXxxMonsterBuffer : MonsterBuffer{...}\nWrapperBase\n{\nprotected:\nBaseBuffer* buf\n}\nWrapperMonster : WrapperBase\n{\npublic:\n   getters;\nprivate:\n    MonsterBuffer* TypeCast() { return static_cast(buf);}\n}\nWrapperXxxMonster : WrapperMonster\n{\npublic:\n   getters;\nprivate:\n    XxxMonsterBuffer* TypeCast() { return static_cast(buf);}\n}\n```\nGiven a raw buffer, we can retrieve type information from the very beginning, then use correct constructor (here it is WrapperXxxMonster ). Wrapper classes now can have virtual methods\n. Thank you. The method that finishes flatbufferbuilder twice works\n.. Sample code that may cause crash could be.\n`\nint main()\n{\nstatic thread_specific_ptr builderPtr;\n// ... other code ... //\nbuilderPtr.reset(new FlatbufferBuilder());\n// In the construct of FlatbufferBuilder, DefaultAllocator::instance() is first called.\n// ... code using builderPtr ... //\n}\nbuilderPtr is created before DefaultAllocator::instance(), I think DefaultAllocator should be destructed before builderPtr.\n. We shouldn't rely on the initialization. But the above code does. In some versions of visual studio(like 2012 2013), it throws error when the program is terminating.\nI am not sure if I get your point. DefaultAllocator::instance() creates a functional-scope static allocator when first called. In certain circumstances, the allocator may be destructed before the builder.\n. @aardappel \nHolding a FlatBufferBuilder in a static is not exactly a common use case\nCould you advise a best practice if I want to reuse the allocated memory? Using a pool allocator? What I did is just a thread_local static.. ",
    "xyczero": "@promethe42 This solution may be the best. Otherwise , we have to  generate code for your inheritance by manually when deserializing from the hard drive.\n. I see , thank you.\n. ",
    "grandprixgp": "Are there any full CPP examples of this type of pseudo-inheritance shown by @promethe42 here?. ",
    "josch": "If you add support for a shared library, please do not forget also manage their SONAME. In cmake that would be:\nset_target_properties(flatbuffers_shared PROPERTIES VERSION xxx SOVERSION yyy)\nYou can also avoid recompiling the sources again to produce a shared library by using something like:\nadd_library(objlib OBJECT ${FlatBuffers_Library_SRCS})\nset_property(TARGET ${objlib} PROPERTY POSITION_INDEPENDENT_CODE 1)\nadd_library(flatbuffers_shared SHARED $<TARGET_OBJECTS:objlib>)\nadd_library(flatbuffers_static STATIC $<TARGET_OBJECTS:objlib>)\nDistributions like Debian (and many of its derivatives) favour dynamic linking over static linking https://wiki.debian.org/StaticLinking. Though Debian policy allows packages to only ship the static library if (among other reasons) the upstream project (you) \"explicitly intended [them] to be available only in static form\".\nI can see how it can be a maintenance burden for you to provide proper SONAME management in case you add support for a dynamic library.\n. ",
    "iamthebot": "Wow, that worked perfectly. Actually, the /// comments in the schemas directly translate into '///' in the generated C++ headers. So I simply used ///<comments and it worked perfectly. Closing.\n. @alvinsay just to clarify the above response, any schemas in golang that have a field whose name corresponds with a reserved keyword (not only \"type\") can cause problems. The recommended response by @gwvo (adding a _ in front in the generated code so you have _type) works perfectly as a workaround. Don't add the underscore in the schema itself, just in the compiled code (the error resulting from \"go build\" will point you to the correct line). \n. @gwvo just curious what the reasoning is for adding the _ at the end? flatc doesn't do this automatically last time I tried it (unless something changed recently to that effect).\n. @gwvo the rationale for this being part of flatc is that if a user updates a schema and recompiles, it doesn't require updating functions to do this. Though I can understand on embedded systems not wanting the extra memory usage. Being able to convert freely between enums and strings allows flatbuffers to serve as an intermediate exchange format with minimal boilerplate on the user's part. The fact another issue (for java) now exists for something similar indicates the demand exists.\nSo, having a void PopulateEnumTypeMap(std::map<EnumType, const char*> user_map_ptr) function could work, but it makes it hard to create a lookup map with static storage. Returning the map instead would work much better. Something like: std::map<const char*,MyEnumType> EnumMapMyEnumType() A std::map is probably a better choice than std::unordered_map given the small (<10k elements) size of most enums since it's more cache-friendly and allows sequential iteration.\nIf we just add this \"return a map\" function, then there's no need for a flatc flag and we can use a similar solution on golang and java.\n. @gwvo suppose we wanted to declare a static string->enum map in a file \"test.c\" that makes up libmytest.so with a flatbuffer table called \"MyTable\"\n``` c++\ninclude \"mytable_generated.h\"\nstatic std::map StrLookups = mynamespace::EnumMapMyTable();\n...\n```\nSomething like this enables a library to have a shared str->enum lookup map ready at runtime without the user having to call a function to first initialize the map (in the case of a library, we need this map ready before main() is called in the linked application). Sure, if you use a void PopulateEnumTypeMap(std::map<EnumType, const char*> user_map_ptr) function then you can have something like:\n``` c++\ninclude \"mytable_generated.h\"\nstatic std::map StrLookups;\nvoid initLookupMap (void) attribute ((constructor)); //only works on g++\nvoid initLookupMap(void) { mynamespace::PopulateEnumTypeMap(&StrLookups);}\n...\n```\nBut this is rather ugly and non-portable (only working on gcc). Maybe there's a better way to do this?\n. @gwvo any modern stdlib implementation of std::map should not have issues with a statically allocated std::map. Especially if the values are primitive types (eg; a C-string). That said, providing the map via the return is going to be the most flexible option.\nAfter developing an application that makes heavy use of string:enum conversions (with thousands of values), this feature would definitely be nice to have. The semantics should transfer easily to Go as well.. Yes, @gwvo it's still not a great idea to have any sort of static allocation forced upon the user. To be clear, I am not advocating static storage for the std::map be implemented.\nThe solution you proposed in #4104 of keeping a table of enums and strings and an optional function that takes that table produces a std::map seems like a good way to go for tackling both of these problems at once (sparse enums and string->enum lookup) with very little forced upon the user. If that sounds fine, I will close this issue and redirect further discussion to #4104.. Closing since this has been answered.\n. @gwvo any chance the unsafe import line can be re-added? This latest commit is breaking automated builds across the board. Not sure how this got merged given the Travis CI test shows a failure. #4112 fixes this until #4111 is ready to go. . Current issue with the default allocator case (where flatbufferbuilder goes out of scope causing a segfault). Will fix this tomorrow.. @aardappel didn't get a chance to work on this over the holidays. But I should be able to take a look this weekend or early next week. It's not as trivial a change as one might expect.. ",
    "dreifachstein": "4045 provides a simpler solution for my problem. Closing.\n. ",
    "ekarlso": "Any news on when this will land ? \n. ",
    "kapilsh": "Any updates on this? Is grpc + flatbuffers still not supported for Python. I am happy to help to move this along. . @aardappel Should I work off of PR #4705 ?. @aardappel I have resolved the conflicts and removed the import pickle line the builder.py file. What else needs to be done for this PR? More tests?.  I meant that I have removed it in my local fork and was also able to fix the merge conflicts. Seems like from above comments that @rw wants more properties of example monster tested. I will work on that and push the changes. . That was it! Thanks you!\nI was combining the -b and -j tags. It works without a warning now. \nHave a great weekend!. I think the problem is here - line 254 of flatbuffers/src/idl_gen_grpc.cpp\np.package_name = LastNamespacePart(*(def->defined_namespace));\nWhich is fine to use for the package name for the go bindings but not for generating grpc method names since go package names differ from Java and C++ package names\nI guess grpc_go_generator::Parameters should have another parameter that is eventually used for getting the method name instead of using go package name directly for generating the method name. \nLet me know if that makes sense.\n. I signed it!. No problem, Thanks for merging it in. . ",
    "YasuoStanley": "sorry!\nIt was my misunderstood.\nThank you for your comment!\n. ",
    "mogemimi": "I signed it!\n. ",
    "paszea": "How do you run tests? I ran \"make test\" in root dir and it passed.\nOn Wed, Sep 14, 2016 at 11:06 AM, Wouter van Oortmerssen \nnotifications@github.com wrote:\n\ndoes this pass the tests? E.g. there is in test.cpp:\nTEST_EQ_STR(root_table->name()->c_str(), \"Monster\");\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/4024#issuecomment-247103736,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADVeu_saAlW05OcqyUWaXTKpGIC3weTKks5qqDfCgaJpZM4J9D5p\n.\n. I signed it\n. ok. I regenerated monster_test.bfbs and fixed the test. please have another look.\n. abandon this pr. see https://github.com/google/flatbuffers/pull/4025 instead\n. I signed it again\n. i authored all the commits\n. I have no idea how to make it happy at this point. Do you know anyone from googlebot that can help?\n. i am Xun Liu\n. add @xunl who is paszea\n. I signed it!\n. \n",
    "xunl": "i am paszea and i approve the pr\n. now what?\n. i signed it\n. r+\n. i gave up\n. I signed it!\n. ",
    "krgn": "webpack-tests.zip\nI created a small node app that demonstrates the problem. Hope it helps..\n. Indeed, well spotted. It was a really late work day, which explains the id case :)\nThe second one is a real gotcha though. Thanks for spotting! \n. Ah, and for possible future reference: \nto obtain the ArrayBuffer correctly sliced for sending across the wire one just needs to do the following:\n``` javascript\nbuilder.finish(offset); \nvar ints = builder.asUint8Array();                                                  \nvar buf  = ints.buffer.slice(ints.byteOffset, ints.byteOffset + ints.byteLength);   \nsocket.send(buf);\n```\n. Sorry, I overlooked this :) \nI think it was necessary in my case, because I was doing in-Browser testing and re-using the underlying the buffers. It might be that send does that actually. Need to test that at some point.\n. ",
    "seantcanavan": "I'm seeing this issue as well but I'm initializing a new FlatBufferBuilder each iteration and calling start, end, and finish:\n// Now broadcast exactly 10 updates with pause\n    for (int i = 0; i < 100000; i++) {\n        FlatBufferBuilder fbb = new FlatBufferBuilder(16);\n        Car nextCar = Car.getRandomCar();\n        CarBuffer.startCar(fbb);\n        CarBuffer.addBrand(fbb, fbb.createString(nextCar.getBrand()));\n        CarBuffer.addModel(fbb, fbb.createString(nextCar.getModel()));\n        CarBuffer.addSold(fbb, nextCar.isSold());\n        CarBuffer.addWeight(fbb, nextCar.getWeight());\n        int carOffset = CarBuffer.endCar(fbb);\n        CarBuffer.finishCarBuffer(fbb, carOffset);\n        fbb.endObject();\n        publisherSocket.send(fbb.sizedByteArray(), 0);\n        Thread.sleep(1000);\n    }\nEDIT: Fixed my code. Not sure why it's fixed but it is:\n// Now broadcast exactly 10 updates with pause\n    for (int i = 0; i < 100000; i++) {\n        FlatBufferBuilder fbb = new FlatBufferBuilder(16);\n        Car nextCar = Car.getRandomCar();\n        int brand = fbb.createString(nextCar.getBrand());\n        int model = fbb.createString(nextCar.getModel());\n        CarBuffer.startCar(fbb);\n        CarBuffer.addBrand(fbb, brand);\n        CarBuffer.addModel(fbb, model);\n        CarBuffer.addSold(fbb, nextCar.isSold());\n        CarBuffer.addWeight(fbb, nextCar.getWeight());\n        int carOffset = CarBuffer.endCar(fbb);\n        fbb.finish(carOffset);\n        byte[] result = fbb.sizedByteArray();\n        publisherSocket.send(result, 0);\n        Thread.sleep(1000);\n    }\n. ",
    "jbrads": "Not a problem, will get those changes made\n. You're welcome :-)\n. ",
    "nickjmeyer": "Finally figured it out.  Need to add root_type Test; to the end of the schema.\nThen the compiler provides the function GetTest which takes a pointer (e.g. from vec.data() where vec is of type std::vector<uint8_t>.).\n. ",
    "ruslo": "\nThis pull request integrates optional support for it\n\nIt's not quite true. Since flatbuffers don't have dependencies you don't need to have any Hunter specific code (like HunterGate). Release can be used as is.\nSo just to clarify this patch is only about installing automatically generated flatbuffersConfig.cmake which allow to use find_package(flatbuffers REQUIRED CONFIG) on user's side.\n. ",
    "popko": "Thank you, closing\n. ",
    "thongbkvn": "Yes, I try this with the version that was commited the day before.\n. ",
    "blep": "I spotted two other issues while writing tests:\n- <field>ByKey()  should take key as the \"user facing\" type (int instead of byte)\n- lookupByKey() should also take key as the \"user facing\" type (int instead of byte)\n. I reworked the implementation of lookUpByKey to minimize duplication of the binary search algorithm. There is now only 3 remaining implementations: two for 32 bits and 64 bits integers, and one for string.\nWe could only keep the 64 bits integers version (removing the 32 bits) but I think this would be bad for performance on 32 bits processor.\nThe advantage of having those functions is that look-up in reflection can be implemented just by dispatching to the correct lookup function based on the key field type.. I've moved the files to the keysearch_test/ sub-dir where files are now generated.\nAlso found another Java generator bug: incorrect code is generated for bool key (bool are not comparable in Java).\n. Can you provide the list of language I (or not) should generate the code for ?\n. I've restricted code generation to the languages above.\n. Yes.\n. Hmm, indeed I suspect you are correct. The \"signed\" conversion should only be needed for ulong. Now that generated code compile, I can write some tests. I'll get back to you once I've done that.\n. Fixed too many commit issues (I had merged romain's master by error instead of upstream).\n. Yes, this has been well tested on my side with keysearch_test #4048 generated code.\n. fixed\n. fixed\n. fixed\n. fixed\n. fixed.\n. ns.components and the value returned by PrefixedNamespaceComponents() are equals for all language other than Java.\nTo add support for namespace prefix in other languages, I see the following tasks for each language:\n- Add new prefix to known_attributes_\n- Update Language enum if the language is missing\n- Update PrefixedNamespaceComponents() function to retrieve the namespace prefix attribute corresponding to the language being generated\n- Replace use of Namespace::component in those generators by PrefixedNamespaceComponents()\n. Changes current address package statement generation, and creating the necessary sub-directories for the namespace prefix.\nThinking about it, namespace prefix be accounted for in import statement generation. I'll tweak some namespace examples and check if other changes are needed...\n. Hmm, I think it's better to stay consistent with idl_gen_cpp.\n. fixed\n. Indeed, it seems the example I picked up for generation is wrong. \nI'll fix the check for missing field case, it is indeed buggy. \nI must say I don't understand lookupByKey() by key implementation:\nint vectorLocation = bb.array().length - vectorOffset; // why ??? \n    int span = bb.getInt(vectorLocation);\n    int start = 0;\n    vectorLocation += 4; // The reason for the -4, lookupByKey expect offset on data containing size of the array\nAFAIK, __vector already provided a ready to use offset on the first item of the array. I don't understand why the byte buffer size is involved in computing vectorLocation...\nprotected int __vector(int offset) {\n    offset += bb_pos;\n    return offset + bb.getInt(offset) + SIZEOF_INT;  // data starts after the length\n  }\nIf I understood correctly, you want me to:\n- remove the odd int vectorLocation = bb.array().length - vectorOffset;\n- change it so that it can directly work with the offset provided by __vector() (do the -4 inside to get the vector size)\nThis change is going to break all existing code on this public API. Is that ok?\n. As you asked, the code that generate the default value do it using user facing type. The problem is that some part of the builder API don't use user facing type, hence the cast.\n. Good catch, should be fixed now.\n. being developped in a branch where I have keysearch_test data. I'll report the change on this branch soon.\n. The changes are too intrusive for me to change C# code blindly (I don't even know how to run the C# tests). I would recommend pointing C# maintainers/contributors to this PR & the related keysearch_test.  I'll add another summary comment with the issues I found and corrected.\n. The only purpose of value is checking the default value code generation. Since string don't have default value, I kept an int as it is easier to write the test for.\n. I managed to modify the code to always use comparison directly. This class was renamed unsigned and now only perform the conversion required to compared unsigned number using signed operators.\n. In practice there is a lot of little differences between the different types. From my point of view, it clearly took less time to a similar changes 10 times than having to tweak the generator (already happen a few time: \"missing\" key bug & comparison optimization). It also avoid the code bloat caused by having that much code in each generated class.\n. By code bloat I meant execution time code bloat: if you have 5 object types with string key, then with generated code, the JIT has to work 5 times at optimizing those 5 lookup functions and generating the corresponding machine code. This put unwanted pressure on the processor instruction cache. Those issues are avoided by having a centralized implementation for a common type.\nOn another issue: How do you implement look-up by key using reflection if the look-up algorithm is only in the generated class code?\n. ",
    "gyanadeep": "I think iam facing a similar issue in flatbuffer.js\nflatbuffers.ByteBuffer.prototype._string = function(offset, opt_encoding)  //{offset =8  } while debugging\noffset += this.readInt32(offset)  //(offset = 1065353252)\n. ",
    "krishjlk": "@aardappel I am facing a similar kind of issue while parsing a large JSON file of size 39MB. Please check my issue here https://github.com/google/flatbuffers/issues/4239 and please let me know your suggestions.. @aardappel Please help me. Does Flatbuffers library has any limitations in size of JSON file ??. Hello @aardappel Thanks for your reply. As per my understanding to parse a json data using Flatbuffers we need to first convert that into binary format and it cannot parse the json data directly. Am I correct ? Please confirm. Thanks a ton. I will check out another way\nOn 27-Mar-2017 11:22 PM, \"Wouter van Oortmerssen\" notifications@github.com\nwrote:\n\nConfirmed.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/4239#issuecomment-289509214,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACIOW_sKDUPsEW97UN3nZg2GxTE-_uJYks5rp-UigaJpZM4Mk2Ym\n.\n. \n",
    "andrewlu1": "models made by flatbuffers is unfriendly , i override it  to support javabean operations by Java lang. \n\n\nhttps://github.com/andrewlu1/FlatBuffers\n. \n\n",
    "0x1997": "@gwvo Yes, it's not necessary, just a personal preference. I've updated the PR to only include the missing <map> header.\n. @gwvo Yes, it's not necessary, just a personal preference. I've updated the PR to only include the missing <map> header.\n. ",
    "Fingerbob2": "Thanks Wouter. I thought I'd grabbed the release, I must have pulled from master instead, probably running with a different version of flatc. I'm now up and running using the 1.4 release. FWIW, it's not obvious where the releases are on the landing page, although searching for flatbuffers releases takes you there.\n. ",
    "redboltz": "I wrote a comment on msgpack-c wiki:\nhttps://github.com/msgpack/msgpack-c/wiki#06302015\n. I wrote a comment on msgpack-c wiki:\nhttps://github.com/msgpack/msgpack-c/wiki#06302015\n. ",
    "bocajspear1": "That appears to work, thanks!\n. ",
    "moriarty": "I had the same issue yesterday and opened pull request #4059\nThe master branch is currently failing Travis.\n. I signed it!\n. Sorry - I guess Travis still fails.\nFor what it's worth, it's working on my 14.04 system, and earlier today I made similar changes to get it to build on 16.04\n$ gcc --version\ngcc (Ubuntu 4.9.4-2ubuntu1~14.04.1) 4.9.4\n$ cat /etc/lsb-release \nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=14.04\nDISTRIB_CODENAME=trusty\nDISTRIB_DESCRIPTION=\"Ubuntu 14.04.5 LTS\"\n$ uname -rvp\n4.4.0-42-generic #62~14.04.1-Ubuntu SMP Fri Oct 7 23:15:48 UTC 2016 x86_64\n. Finally all all checks pass. I will squash my commits and push again.\nI still don't think it's the most elegant solution.\n. No problem, as long as its fixed. Thanks\n. ",
    "MrSampson": "I just had the same error, and tentatiely removed -Werror from CMakeLists.txt and it seemed to build successfully.\nDid -Werror make it back in since this was closed?. Yes, I am (pretty) sure.\n[flatbuffers]$ git show-branch\n[master] Re-applied reverted fix, adding missing codegen files.\n. ",
    "jronak": "Thanks!\n. @TechnotronicOz  yes thanks!\n@gwvo @rw fixed the syntax issues and refactored the generator\nPlease review\n. Updated PR\n. @gwvo any feedback?. Please review, looks good to me now. Issue is with the old flatbuffers go package, just update the package.\ngo get -u github.com/google/flatbuffers/go\n But wait until #3791 is resolved as it fails the build as of now. C++ already supports GRPC.. SampleGoGrpc was left only for this PR, I will create tests based on monster fbs. This folder will be removed \n. Ok will do that. \n. Yes\n. Use Grpc-go file as a reference, but few things were changed. Seems fine?\n. Didn't clean it, will be done. \n. Sublime reformator  issues. Will be fixed\n. Same as above \n. Yes, same as above\n. As of now all the services are generated in the same file grpc.go, should generator each service in a separate file just like how the each table has its own file?\n. *should we generate \n. Yes \n. GenerateGRPC was pushed to the bottom\n. Didn't need much of a change other than Method class, can be improved ?\n. You mean keep the above code under  same namespace  of grpcGenerator, just the edit the name with go? \n. I didn't understand, could you please elaborate?\nI have written it similar to the one here https://github.com/grpc/grpc-go/blob/master/rpc_util.go#L66\n. Yes, it has been completely compatible with grpc-go. The compiler is not maintained by grpc-go though, it's taken care by the proto-go -  https://github.com/golang/protobuf/tree/master/protoc-gen-go\nAnd there are no protobuf specific things, so this should be fine. \n. Wouter, C++ and other languages are maintaining the grpc code in the same file(idl_gen_grpc.cpp) ?\nAs of now I have left both of them in the same file with different namespace for each language i.e grpc_go and grpc_cpp. So the above function should be passed the list of languages and should be able to generate the service in those languages. \nI left it to generate the go code only as of now. \n. There are difference for go and cpp with the FlatbufMehtod and FlatbufFile classes. FlatbufService and FlatbufPrinter is the same. So is there a way to imporve it?\n. I feel happy-path test is reasonable, I can't really think of adding fuzz testing to it\n. yes, it'll be moved to test\n. mathClient is returned as an interface by the method NewMathClient, more over it is private to avoid unnecessary public names as it would never be accessed directly. Same convention is followed by the protobuf grpc-go plugin \n. output is passed on to Invoke method of grpc and data is unmarshalled into out. Out is returned as a pointer too.\nHow could alloc be avoided in this case ?\n. How can it be avoided?\nIt was exactly similar in the protobuf grpc-go generator, \n. Reusing the code from c++ version, I have overridden only the methods which had to be modified.\nThe  enum Streaming { kNone, kClient, kServer, kBiDi }; is a typo, will be removed\n. Won't GenerateGRPC generate grpc for all the languages?, although as of now it calls go grpc generator for testing only. As there is only one flag --grpc for grpc generator I thought we could pass a vector of languages to generate the grpc code to GenerateGRPC. I can definitely rename it, but can you explain how are we going to handle grpc generation for multiple languages?\nPlease advise\n. Again that's taken care by the GenerateGRPC once we call the C++ generator from the GenerateGRPC method\n. My bad, will try to avoid it in future. I had done the same thing as seen in the python PR but was sceptical about, anyway I'll quickly fix this and get back to you.\n. Alright fixed it\nThanks @TechnotronicOz . @rw Can we avoid generating flatbuffers codec in every grpc generated package by leaving it in the flatbuffers/go package?. Is it really needed to add is_func, as it limits the use of signature in future updates.. ",
    "CyborgRahil": "Is there any other way to convert string json to byte buffer by using flatBuffer? I using the above link you mention in your comment but there is a crash while executing my application. I attach the log to that link.\n. @gwvo \nIs there any source code for flat c which we use inside our application as a .so file?\n. ",
    "gwest7": "@sampaioletti is this still on your list?. ",
    "msb-at-yahoo": "why a template instead of just creating a Create overload in the type's namespace.\n``` c++\n\ufffc // generic-code friendly name\ninline flatbuffers::Offset Create(flatbuffers::FlatBufferBuilder &_fbb, const WeaponT _o, const flatbuffers::rehasher_function_t rehasher) {\n    (void)rehasher;\n    return Create(_fbb,\n            _o->name.size() ? _fbb.CreateString(_o->name) : 0,\n            _o->damage);\n}\n// backwards compatibility\ninline flatbuffers::Offset CreateWeapon(flatbuffers::FlatBufferBuilder &_fbb, const WeaponT _o, const flatbuffers::rehasher_function_t rehasher) {\n    return Create(_fbb, _o, rehasher);\n}\n. this shouldn't just be use.const uint8_t Indireect(const uint8_t , uint8_t)``` should be marked inline or static.\npersonally, i think all of the static functions in flexbuffers.h should be inline.. There are a few (reasonable!) hoops I have to jump through before I can submit a PR.  @njh0602 since this is affecting you, I suggest looking at flexbuffers.h and marking all free functions inline and remove static where present, and then test.\nIs the intent to keep the C++ library header only?  While none of the functions I looked at would be bad to mark inline, that might not always be true.\n. this change seems unrelated to the other bits of the PR.  i don't know how the maintainers approach this, but in general, i believe a PR should have one logical change.\nyou can also avoid the overhead of the implicit strlen: return std::string(c_str(), length());\n. ",
    "louisbob": "Hi,\nAlthough I have a working version with template, I would love to investigate your suggestion for the PR. Unfortunately I'm in the rush for my project. Upvote from community may motive me to dedicate some time to implement it.\nI will try to throw an eye on it in the next couple of weeks. . ",
    "benssson": "Signed!\nChu Joe Hoang\nSent from\nhttps://polymail.io/\nOn Fri, 21 Oct 2016 at 10:14 googlebot\n<\nmailto:googlebot notifications@github.com\n\nwrote:\n\na, pre, code, a:link, body { word-wrap: break-word !important; }\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd\nPlease visit\nhttps://cla.developers.google.com/\nto sign.\nOnce you've signed, please reply here (e.g.\nI signed it!\n) and we'll verify. Thanks.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address. Check\nhttps://cla.developers.google.com/clas\nand verify that your\nhttps://help.github.com/articles/setting-your-email-in-git/\n.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly,\nhttps://github.com/google/flatbuffers/pull/4068#issuecomment-255333285\n, or\nhttps://github.com/notifications/unsubscribe-auth/AIaPeWU7ERK6h_D8jD09o3kTp6prtyMtks5q2IKGgaJpZM4KdBVZ\n.\n. > Would it be better to stick the adding of the byteOffset to inside __vector, since every use of __vector would have this problem?\nI don't think so (see below).\n\nDo any other accessors have this problem? None of the offsets add byteOffset, so that may generally not be supported?\n\nNo, the other accessors don't use the TypedArray buffer property directly and instead use the [] operator (and so the byteOffset will already taken into account in those cases). \nAs far as I can tell, this is the only place where the buffer property is used to create a new view on the data and so should also take into consideration the byteOffset.\n\nAlso if you make changes to generated code, make sure to add the generated code to the commit (run sh generated_code.sh).\n\nThanks - done.\n. ",
    "Gompangs": "I changed from fbb.DataBuffer.Data to fbb.SizedByteArray() and it's perpectly working now !\nThanks for your reply.\n. ",
    "lunker": "I attached my scripts. \n[schema]\nenum SrcDstType :int\n{\n    MATCHING_SERVER = 0,\n    MATCHING_CLIENT,\n    ROOM_MANAGER,\n    PACKET_GENERATOR,\n    MONITORING_SERVER\n}\nenum Command : int \n{\n    MATCH_REQUEST,\n    MATCH_COMPLET,\n    LATENCY,\n    HEALTH_CHECK,\n    MSLIST_REQUEST,\n    PG_START,\n    PG_END\n}\ntable Header{\n    length: int;\n    srcType: SrcDstType;\n    srcCode: int;\n    dstType: SrcDstType;\n    dstCode: int;\n}\ntable Body{\n    cmd: Command;\n    data: string;\n}\n[generated file (partial)]\npacket.Header.prototype.length = function() {\n  var offset = this.bb.__offset(this.bb_pos, 4);\n  return offset ? this.bb.readInt32(this.bb_pos + offset) : 0;\n};\npacket.Header.prototype.srcType = function() {\n  var offset = this.bb.__offset(this.bb_pos, 6);\n  return offset ? /* @type {packet.SrcDstType} / (this.bb.readInt32(this.bb_pos + offset)) : packet.SrcDstType.MATCHING_SERVER;\n};\npacket.Header.prototype.srcCode = function() {\n  var offset = this.bb.__offset(this.bb_pos, 8);\n  return offset ? this.bb.readInt32(this.bb_pos + offset) : 0;\n};\npacket.Header.prototype.dstType = function() {\n  var offset = this.bb.__offset(this.bb_pos, 10);\n  return offset ? /* @type {packet.SrcDstType} / (this.bb.readInt32(this.bb_pos + offset)) : packet.SrcDstType.MATCHING_SERVER;\n};\n. then why does generated file can't deserialize bytes to defined object ? \ni manually modified interval 2bytes to 4bytes and get right objects. \n. Ok. These are my codes.\nIn modified_generated js file, i just modified header field's getter method for test. \nFor example, \nreturn offset ? this.bb.readInt32(this.bb_pos + offset) : 0;\n=> \nreturn this.bb.readInt32(16); ( 4bytes interval)\nplz check my file. \n. ",
    "jchiu0": "Sorry, I think I fixed the compiler error. Will check again. The use case is that sometimes we pass the flatbuffers in the deserialized form, because of its convenience. Otherwise, we have to keep writing GetRootAs... everywhere in the code. However, we still need to serialize it sometimes. @gwvo \n. ",
    "adsharma": "I signed it.\n. The UPWARD code hasn't been tested yet. I'm still working on the tests that would exercise the code via KVStoreBuilder. Will send an update here when ready.\nThe purpose of the pull request was to see if such templatization would be acceptable, to minimize the delta between the byteorder2 branch and master.\n. https://github.com/adsharma/flatbuffers/commit/29a3b0128e0c123276263638fa58eb96724512cd\nhas a KVStoreBuilder class that uses FlatBufferBuilderImpl<UPWARD> and passes KeyComparisonTest(). I haven't made the codgen changes to use KVStoreBuilder instead of FlatBufferBuilder yet (manually edited the *_generated.h file). Should give you a sense of how this code is likely to be used.\nHere's another data point. After this change:\n-typedef FlatBufferBuilderImpl<DOWNWARD> FlatBufferBuilder;\n+typedef FlatBufferBuilderImpl<UPWARD> FlatBufferBuilder;\nI see some tests break. Would passing all of the existing tests in this mode sufficient? \nAnother possibility is to move all UPWARD template specialization code to KVStoreBuilder.h where it's used.\n. > Can you explain the need for upward building in more detail?\nFor a table that looks like this:\ntable test1 (original_order) {\n  obj_id:long (key);\n  age:long (key);\n  country:string (key);\n  score:double (key);\n  name:string;\n  mydata:string;\n}\nThe desired layout is in the attached image.\n\nOn second thoughts, it may be possible to get this layout without changing the direction of buffer growth. We need to consider at least two cases:\n- A client is constructing the buffer to send to the server\n- Server read this buffer from storage, needs to construct the vtable based on the schema and then does a zero copy transmit\nread-modify-write is interesting, but infrequent enough that we'll probably accept a copy there.\n. > And you can only enforce them being contiguous if you can enforce how the buffer is being constructed.\nYes, I realized this as well. We'll support zero-copy to storage and back only when the client uses KVStoreBuilder. This builder can verify that add_xxx() methods are called in the right order and throw an exception if they're not.\n\"original_order\" hopefully reduces the possibility of some reorderings.\nAll great points. Thanks for calling them out.\n. This is not needed. A different approach worked fine.. https://github.com/adsharma/flexpy has some python3 compatible code to read flexbuffers.\nWarning: hot off the press, needs more review.\nCould use numpy to avoid copies.. ",
    "anatekar": "lambdas are also being generated for object-based API's (specifically while generating methods that works with an unpacked C++ object :  idl_gen_cpp.cpp : 1155)\nAlso \"grpc/src/compiler/cpp_generator.cc\" uses std::string back() which is available in C++11, can be safely replaced with at() or [] using len-1 as subscript (any other alternative?). Doesn't seem to be a compiler issue. \n/bin/sh: $: command not found\nsuggests that flatc was not found even though it was previously generated successfully(at 52%)\nCan you kindly provide the build steps that were followed? Also do \"ls -l\" after the failed compilation and provide the results.\n. what does  $ls -l give. wrong information in sense?\nYou can try manually generating the samples/monster_generated.h using\n./flatc -c --no-includes --gen-mutable --gen-object-api -o samples ./samples/monster.fbs\nIf this works need to check the \n\"CMakeFiles/flatsampletext.dir/build.make\" file \n. wrong information in sense?\nYou can try manually generating the samples/monster_generated.h using\n./flatc -c --no-includes --gen-mutable --gen-object-api -o samples ./samples/monster.fbs\nIf this works need to check the \n\"CMakeFiles/flatsampletext.dir/build.make\" file \n. Great. Did make work after this command?\nCan you post \"CMakeFiles/flatsampletext.dir/build.make\" file if it didn't work. This seems to be the actual error. Unfortunately cc version 4.4.x doesn't support lambdas (https://gcc.gnu.org/projects/cxx-status.html#cxx11) you will need at least gcc/g++ version 4.5\n  . @gwvo seems to be fixing this for #4076 . flatbuffers needs gnu cxx version 4.5 or higher to build. Refer #4093 for more details. ",
    "haiiro": "Ah, whoops. Apparently 1.1.0. Sorry, thought my project had the latest version. Confirmed working with 1.4.0. Thanks, and sorry for the bother!\n. ",
    "TechnotronicOz": "@JRonak you should consider running go fmt on these files to fix the formatting issues.\n. @gwvo Effective Go\n\nGo has no line length limit. Don't worry about overflowing a punched card. If a line feels too long, wrap it and indent with an extra tab.. \n",
    "garretmcgraw": "I signed it!\n. ",
    "tamulj": "Thanks for the response.\nI definitely mistook the fact that there are 4 bytes overhead before the first vtable offset as taking 4 bytes per offset, that is indeed only 2 bytes per field.\nI'm not sure I follow how the vtable fields are shared among similar objects unless you are taking about arrays. The code I see generated has a vtable written out for every Table object start/end.\nI don't think field Ids would be needed as you can use a fixed order (which does mean all fields would be required).\nMaybe an example would clear things up. Image a definition something like:\n```\ntable Reference {\n  path: string;\n  id: string;\n  lastModifiedUserId: uint;\n  lastModifiedTimestamp: double;\n}\ntable Node {\n  edges:[Reference];\n}\n```\nEvery Reference ends up having a vtable which can be a significant percent of its total memory footprint. I'm happy that the Node keeps an array of offsets for each Reference so I can find that quickly, but I would trade the speed to reading each field out of the Reference for the savings in buffer size by dropping the vtable in the Reference.\nEach Reference buffer would be packed as:\n| Path Len | Path String | Id Len | Id String | 4 byte lastModifiedUserID | 8 byte lastModifiedTimestamp |\nThis is exacerbated by the fact that my real use case would really have an array of a union of slightly different types. If I didn't have that union striping each field individually in its own array would be reasonable middle ground. For Example:\ntable Node {\n  paths: [string];\n  ids: [string];\n  lastModifiedUserIds: [uint];\n  lastModifiedTimestamps: [double];\nUnfortunately the multiple variable length types uses a lot of space to encode multiple sets of offsets (one for paths and one for ids), and we don't get to take advantage of the fact that each array is of equal length.\nIn our proto typing we saved ~20% of buffer size by using the fixed schema encoding, which matters when there are O(Millions) of buffer objects. For now I'm manually packing a byte buffer into string and using an encoding like:\ntable Node {\n  reference: [string];\n  type: [ENUM];\n}\nThis works but I lose all of the nice accessors and builders that Flatbuffers generators for me.\n. I did point out in my last post that all fields would have to be required, which makes the representation a bit of a hybrid between structs & tables.\n\nI don't think field Ids would be needed as you can use a fixed order (which does mean all fields would be required).\n\nThe internal documentation doesn't do a great job of explaining what the definition of similar objects means, but looking at the code I was able to infer that it means identical vtable (same object len and same offsets for fields).  Seems like saying that explicitly would be a good addition to the docs.\nEdit: My original message was mistaken about amount of vtable deduplication, as objects written out in the same order with the same set of fields populated should dedupe nicely.\nGiven that context your comments make more sense. While this is a nice optimization our data doesn't get the full benefit because we have a large number of Nodes, each with a relatively small number of References. The data is split up this way to allow some mutability and limit the size of data needed to be re-encoded.\nThanks for the reference to FlexBuffers I'll have to read up on that more. I'm also going to think about if there are changes we could make to our data to make them more vtable friendly.\nHopefully now that things are a bit cleared up, my proposal makes enough sense that you can comment on if a feature like this could/would be compatible with flatbuffers?\nThanks again!\n. ",
    "artt81": "I think I Found the answer from: https://github.com/google/flatbuffers/issues/3973.\nSo don't seem to be possible.\n. ",
    "mmt": "I'm new to Flatbuffers and trying to integrate Python generated files into a Bazel project.  I think adding this feature would greatly simplify the build rules.  Thanks!. ",
    "SkyToGround": "In the current implementation, I have to copy the data byte by byte:\npython\nnr_of_bytes = fb_arr.PDataLength()\nnp_buffer = np.empty([nr_of_bytes, ], dtype = np.uint8)\nfor i in range(nr_of_bytes):\n    np_buffer[i] = fb_arr.PData(i)\ndata_arr = np.fromstring(np_buffer.tobytes(), dtype = np.int32)\nThus any improvement to this would be welcome.. I might look into solving this problem myself eventually, if no one has implemented this feature in a few months time.. ",
    "maxw": "Piggybacking on this issue, would it by possible to get byte vectors as a whole object in python? The C# API does this beautifully. I understand C# makes it slightly easier. However, at the moment it looks like I'll need to create an empty bytearray and append each byte to it by looping through the vector.. This is what I'm doing at the moment:\ndataBytes  = bytearray()\ndataLength = flatbufferMessage.DataLength()\nfor i in range(dataLength):\ndataBytes.append(flatbufferMessage.Data(i))\nI'm fairly new to python so this might not be ideal, but this seems to be the best approach I can take in python 2.. +1. The C# API supports this nicely, but the python API does not.\nI am currently using flatbuffer with app engine, and my app engine requests get killed for using up the 60 seconds of allotted time to process the request :O!!! It's always caught looping through a byte vector while I turn it into a bytearray =P. I actually ended up hacking the CreateString / table._String code to create a faster byte array implementation. The slowness just comes from having to iterate through each byte in a vector one by one. On top of that the retrieve / prepend byte methods do some bounds checking on the value and the prepend method increases the backing byte buffer by only a small amount each time.\nMy changes will get wiped out the next time I compile my flatbuffer schema, but when I free up on this project in a few weeks, I can look into implementing it properly in the flatbuffer compiler.. ",
    "joireman": "Using the following schema storing int16 data like so:\nunion MyDataUnion {\n  Int8Data,\n  Int16Data,\n  ...\n}\n\ntable MyData {\n  id: ubyte;\n  readings: MyDataUnion;\n}\n\nwhere the typed data is defined similar to the following.\ntable Int16Data {\n  data:[short];\n}\n\nIf I have a flatbuffer reference to the MyData table my_data_fb as an object:  Looking into the generated flatbuffers code for the Int16Channel class Data method I'm able to define a method\nthat gets the offset to the data which I can then use along with the data length and Bytes method to send to np.frombuffer\nimport flatbuffers\nimport numpy as np\n\ndef get_data_byte_offset(my_data):\n    o = flatbuffers.number_types.UOffsetTFlags.py_type(my_data._tab.Offset(4))\n    offset = my_data._tab.Vector(o) if o != 0 else 0\n    return offset\n\nmy_int_data = Int16Data.Int16Data()\nmy_int_data.Init(my_data_fb.Readings().Bytes, my_data_fb.Readings().Pos)\nnum_pts = my_data.DataLength()\n\n# Get offset to start of data \noffset = get_data_byte_offset(my_int_data)\n\n# Unpack data to a numpy array\ndata_np = np.frombuffer(my_data_fb.Readings().Bytes,\n                        offset=offset,\n                        dtype=np.dtype('i2'),\n                        count=num_pts)\n\nThis works and is very fast, relative to calling:\ndata = np.array([my_data.Data(i) for i in range(num_pts)])\n\nbut is this universally applicable?\n. ",
    "ialmetwally": "@gwvo yes I'm able to read the other data correctly, only the child lists \"sub\" and \"top\" are always empty.\nis there anyway to validate the binary file by converting it back to JSON using the complier?. @gwvo by the way, I used flatbuffers before on the same dataset, with only one child list and it worked correctly.. UPDATE\nIt seems the problem is having two children with the same data type List, I removed top from the JSON file and now I can read \"sub\" again.\nAnyway to workaround this?. UPDATE\nI was able to convert the generated binary file back to JSON and it seems valid. And then I discovered that I was using a cached version of the binary file on the server.\nI'll close the issue.. ",
    "tansuangai2008": "Ok  gwvo,Thank you for your answer and solution!. $ cmake -G \"Unix Makefiles\"\n$ make. drwxr-xr-x  4 root root    4096 11\u6708 21 17:55 android\n-rw-r--r--  1 root root     311 11\u6708 21 17:55 appveyor.yml\ndrwxr-xr-x  4 root root    4096 11\u6708 21 17:55 biicode\n-rw-r--r--  1 root root      83 11\u6708 21 17:55 biicode.conf\ndrwxr-xr-x  2 root root    4096 11\u6708 21 17:55 CMake\n-rw-r--r--  1 root root   11290 11\u6708 21 18:05 CMakeCache.txt\ndrwxr-xr-x 12 root root    4096 11\u6708 22 21:31 CMakeFiles\n-rw-r--r--  1 root root    3066 11\u6708 21 18:05 cmake_install.cmake\n-rw-r--r--  1 root root    7677 11\u6708 21 17:55 CMakeLists.txt\n-rw-r--r--  1 root root     377 11\u6708 21 17:55 composer.json\n-rw-r--r--  1 root root    2256 11\u6708 21 17:55 CONTRIBUTING.md\n-rw-r--r--  1 root root     330 11\u6708 21 18:05 CTestTestfile.cmake\ndrwxr-xr-x  4 root root    4096 11\u6708 21 17:55 docs\n-rwxr-xr-x  1 root root 2009757 11\u6708 22 21:31 flatc\n-rwxr-xr-x  1 root root   19298 11\u6708 22 21:31 flathash\ndrwxr-xr-x  2 root root    4096 11\u6708 21 17:55 go\ndrwxr-xr-x  4 root root    4096 11\u6708 21 17:55 grpc\ndrwxr-xr-x  3 root root    4096 11\u6708 21 17:55 include\ndrwxr-xr-x  3 root root    4096 11\u6708 21 17:55 java\ndrwxr-xr-x  2 root root    4096 11\u6708 21 17:55 js\n-rw-r--r--  1 root root 2619964 11\u6708 22 21:31 libflatbuffers.a\n-rw-r--r--  1 root root   11342 11\u6708 21 17:55 LICENSE.txt\n-rw-r--r--  1 root root   27444 11\u6708 22 21:31 Makefile\ndrwxr-xr-x  3 root root    4096 11\u6708 21 17:55 net\ndrwxr-xr-x  2 root root    4096 11\u6708 21 17:55 php\n-rw-r--r--  1 root root    2645 11\u6708 21 17:55 pom.xml\ndrwxr-xr-x  3 root root    4096 11\u6708 21 17:55 python\n-rw-r--r--  1 root root    2596 11\u6708 21 17:55 readme.md\ndrwxr-xr-x  2 root root    4096 11\u6708 21 17:55 reflection\ndrwxr-xr-x  4 root root    4096 11\u6708 22 21:30 samples\ndrwxr-xr-x  2 root root    4096 11\u6708 21 17:55 src\ndrwxr-xr-x  7 root root    4096 11\u6708 21 18:09 tests\nI want to know why there are wrong information!Can solve?. drwxr-xr-x  4 root root    4096 11\u6708 21 17:55 android\n-rw-r--r--  1 root root     311 11\u6708 21 17:55 appveyor.yml\ndrwxr-xr-x  4 root root    4096 11\u6708 21 17:55 biicode\n-rw-r--r--  1 root root      83 11\u6708 21 17:55 biicode.conf\ndrwxr-xr-x  2 root root    4096 11\u6708 21 17:55 CMake\n-rw-r--r--  1 root root   11290 11\u6708 21 18:05 CMakeCache.txt\ndrwxr-xr-x 12 root root    4096 11\u6708 22 21:31 CMakeFiles\n-rw-r--r--  1 root root    3066 11\u6708 21 18:05 cmake_install.cmake\n-rw-r--r--  1 root root    7677 11\u6708 21 17:55 CMakeLists.txt\n-rw-r--r--  1 root root     377 11\u6708 21 17:55 composer.json\n-rw-r--r--  1 root root    2256 11\u6708 21 17:55 CONTRIBUTING.md\n-rw-r--r--  1 root root     330 11\u6708 21 18:05 CTestTestfile.cmake\ndrwxr-xr-x  4 root root    4096 11\u6708 21 17:55 docs\n-rwxr-xr-x  1 root root 2009757 11\u6708 22 21:31 flatc\n-rwxr-xr-x  1 root root   19298 11\u6708 22 21:31 flathash\ndrwxr-xr-x  2 root root    4096 11\u6708 21 17:55 go\ndrwxr-xr-x  4 root root    4096 11\u6708 21 17:55 grpc\ndrwxr-xr-x  3 root root    4096 11\u6708 21 17:55 include\ndrwxr-xr-x  3 root root    4096 11\u6708 21 17:55 java\ndrwxr-xr-x  2 root root    4096 11\u6708 21 17:55 js\n-rw-r--r--  1 root root 2619964 11\u6708 22 21:31 libflatbuffers.a\n-rw-r--r--  1 root root   11342 11\u6708 21 17:55 LICENSE.txt\n-rw-r--r--  1 root root   27444 11\u6708 22 21:31 Makefile\ndrwxr-xr-x  3 root root    4096 11\u6708 21 17:55 net\ndrwxr-xr-x  2 root root    4096 11\u6708 21 17:55 php\n-rw-r--r--  1 root root    2645 11\u6708 21 17:55 pom.xml\ndrwxr-xr-x  3 root root    4096 11\u6708 21 17:55 python\n-rw-r--r--  1 root root    2596 11\u6708 21 17:55 readme.md\ndrwxr-xr-x  2 root root    4096 11\u6708 21 17:55 reflection\ndrwxr-xr-x  4 root root    4096 11\u6708 22 21:30 samples\ndrwxr-xr-x  2 root root    4096 11\u6708 21 17:55 src\ndrwxr-xr-x  7 root root    4096 11\u6708 21 18:09 tests\nI want to know why there are wrong information!Can solve?. OK ,flatc work properly   ,correct build\n-rw-r--r-- 1 root root   503 11\u6708 21 17:55 monster.fbs\n-rw-r--r-- 1 root root 15342 11\u6708 23 09:46 monster_generated.h. Yes ,I use this command \nBut when  I use \"make install\" of command appear error \nbelow for error log \nroot@mail flatbuffers] make install\n[ 11%] Built target flatbuffers\n[ 52%] Built target flatc\n[ 55%] Built target flathash\n[ 58%] Building CXX object CMakeFiles/flatsamplebinary.dir/samples/sample_binary.cpp.o\nIn file included from /home/flatbuffer/flatbuffers/samples/sample_binary.cpp:17:\n/home/flatbuffer/flatbuffers/samples/monster_generated.h: In function \u2018flatbuffers::Offset MyGame::Sample::CreateMonster(flatbuffers::FlatBufferBuilder&, const MyGame::Sample::MonsterT, const flatbuffers::rehasher_function_t)\u2019:\n/home/flatbuffer/flatbuffers/samples/monster_generated.h:292: error\uff1aexpected primary-expression before \u2018[\u2019 token\n/home/flatbuffer/flatbuffers/samples/monster_generated.h:292: error\uff1aexpected primary-expression before \u2018]\u2019 token\n/home/flatbuffer/flatbuffers/samples/monster_generated.h:292: error\uff1aexpected primary-expression before \u2018i\u2019\nmake[2]:  [CMakeFiles/flatsamplebinary.dir/samples/sample_binary.cpp.o] error 1\nmake[1]:  [CMakeFiles/flatsamplebinary.dir/all] error 2\nmake: *** [all] \u9519\u8bef 2\n. cc version 4.4.7 20120313 (Red Hat 4.4.7-11) (GCC) . ",
    "fbenkstein": "Shouldn't this be closed by #4626?  Is something else needed?. Also see https://github.com/google/flatbuffers/pull/4977 for reporting out-of-range enum values for enums so\nenum X : byte { Y = 1000 }\nwill be flagged by the parser, not just by the C++ compiler.. > Would this also work for u64 enums, or is there further work needed for that?\nAs far as I can see due to the way integer literals are parsed enum values between INT64_MAX and UINT64_MAX are not supported at the moment.  It seems like adding support for them would require some work in the parser.  Also, I think there is at least one integer overflow here issue which can lead the C++ generator trying to iterate from INT64_MIN to INT64_MAX \u2014 essentially an infinite loop.  I'll try to investigate and come up with a report (and maybe even a solution).. Why implement the formatting logic twice?  Simply make flatc; rustfmt the canonical way to generate the rust code.. Btw. in test.cpp, Line 38 there is an unpaired // clang-format off which disables formatting for the whole file.. @vglavnyy should go in first.  I'll rebase my stuff on top of his.. @vglavnyy @aardappel I've rebased and found that #4948 didn't address the out of range enums.  I've adjusted the error message to be consistent.  It will also print the valid interval as that's pretty useful.  There's still the question of very large ulong enum values (between INT64_MAX and UINT64_MAX).  I haven't found an issue referencing these, so maybe we should create one?. @vglavnyy @aardappel Is there something still to be done here or can this go in?. @vglavnyy Good catch.  I will try to add tests for that as well.. @vglavnyy @aardappel Please have another look.. @vglavnyy \n\nAnother way: Extract check_now code from ParseSingleValue::atot to standalone static or templated function and try to reuse it.\n\nI like that approach but I'd rather do that later rather than in this PR.. I signed it!. @aardappel I addressed your review comments.. @aardappel Rebased.. Not necessarily.  The very next sentence says:\n\u00a710.2(8)\n\nFor an enumeration whose underlying type is fixed, the values of the enumeration are the values of the underlying type. Otherwise, \u2026\n\nIf I understand correctly \"\u2026\" essentially states (if I understand correctly) that the values are in some interval [b_min, b_max] which can be smaller than the smallest declared value or larger than the largest declared value respectively.\nIn my opinion this makes the checks in EnumName... perfectly reasonable.  Admittedly, the test might not be well chosen since it might lead to undefined behavior in C++17.  Feel free to change it!\nLet's see if any compilers will actually generate nasty code instead of just truncating.  If they do I guess we'd have to add special enum verification code to Table.\nC++17 final draft:  http://open-std.org/JTC1/SC22/WG21/docs/papers/2017/n4659.pdf. @aardappel I think I found a much simpler way to achieve type-safety for IsFieldPresent.  I chose the name FlatBuffersVTableOffset since it's very unlikely to cause conflicts with existing fields.  Since a C++11 compiler is already required for flatbuffers I also gave the base type of flatbuffers::voffset_t, mostly for documentation purposes.  I don't have a strong opinion on either of these so feel free to request changes.. I had to introduce FLATBUFFERS_VTABLE_UNDERLYING_TYPE since apparently VS2010 doesn't support underlying types for enums.  What about generalizing this into the following?\n```\nif \ndefine FLATBUFFERS_ENUM_BASE_TYPE(T) : T\nelse\ndefine FLATBUFFERS_ENUM_BASE_TYPE(T)\nendif\n```\nThen we could use this to always give the base type to enums, scoped and unscoped.  I don't think there\nshould be any downsides to that.\n@vglavnyy do you have an opinion on that?. @vglavnyy I don't think you can use type-safe enums in MSVC2010.. I think this is a good idea but beyond the scope of this PR.\nOn Sun 21. Oct 2018 at 10:32 Vladimir Glavnyy notifications@github.com\nwrote:\n\nCould you explain the issue?\nExample https://godbolt.org/z/-8uwrH of implementation for MSVC201:\nstruct table1{\n    typedef struct {\n        enum _type { VT_NAME = 4, VT_VALUE = 4, VT_TYPE = 6 };\n    } type;\n    type::_type ofs;\n    table1() : ofs(type::VT_NAME) {}\n};\nstruct table2{\n    typedef struct {\n        enum _type { VT_NAME = 4, VT_VALUE = 4, VT_TYPE = 6 };\n    } type;\n    type::_type ofs;\n    table2() : ofs(type::VT_NAME) {}\n};\n// have defferent signaturesvoid ofs_check(table1::type ofs) {}void ofs_check(table2::type ofs) {}void t1_ofs_check(table1::type ofs) {}\nvoid check() {\n    table1 t1;\n    table2 t2;\n    t1_ofs_check(t2.ofs); // compile-time error\n    t1.ofs = t2.ofs; // compile-time error\n    t1.ofs == t2.ofs; // compile-time warning\n    t1.ofs == table2::type::VT_NAME; // compile-time warning -Wenum-compare\n}\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/4988#issuecomment-431649570,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AHXSI8Z3va8Bc-rB8-TJYfc6Q1Oq_sAWks5unDEngaJpZM4XTTYu\n.\n. @aardappel could you look at the failures, please?  Is it expected that binaries can look different on different platforms (or build types on Windows)?  If so I'd just exclude the binaries from the diff if you think this change is still useful.. I've added the messages and also created #5008 to track the binary differences.. I think in cases where Rust takes values than can come from third parties and converts to something else Result is usually returned.  get_root_as_... is more like from_utf8 which does return Result.  To keep the behavior between languages consistent, maybe the contract of the flatbuffers Rust library could be that panics do not occur if verify_ has been called?. I don't I'll be able to finish this PR \u2014 at least not any time soon.  I can leave it open if anybody else wants to pick it up.. Here's the files I could produce on Windows.  Maybe it helps:\n\ndifferent-files-vs2017.zip\n. > Ok, so this is ready to be merged? Any trait changes can be proposed in a different PR..\nYes.  I think this is ready.. Yes. Makes sense.\nOn Mon 10. Dec 2018 at 21:46 Wouter van Oortmerssen \nnotifications@github.com wrote:\n\n@aardappel commented on this pull request.\nIn src/flatc.cpp\nhttps://github.com/google/flatbuffers/pull/5077#discussion_r240373492:\n\n@@ -106,6 +125,8 @@ std::string FlatCompiler::GetUsageString(const char *program_name) const {\n     \"  --raw-binary       Allow binaries without file_indentifier to be read.\\n\"\n     \"                     This may crash flatc given a mismatched schema.\\n\"\n     \"  --size-prefixed    Input binaries are size prefixed buffers.\\n\"\n+    \"  --schema-size-prefixed\\n\"\n\nUsing the verifier to decide doesn't sound great, better would be to use\nreflection::SchemaBufferHasIdentifier, and when that fails, try again\nassuming the first 4 bytes are a size.\nI guess to me --size-prefixed is a niche feature, so\n--schema-size-prefixed is a niche of niche, and would prefer to not\npolute the options with it if we can reasonably either demand only regular\nFlatBuffers are used for binary schemas, or silently detect both with the\nabove option.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/5077#discussion_r240373492,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AHXSIyCdWrjQ5TmSF7cP436UOjd_geSdks5u3sgogaJpZM4Y98Nt\n.\n. The problem is that you can't have optional arguments in macros without __VA_ARGS__.  I could provide two macros but I didn't want to touch all callers of TestError to prevent conflicts with other PRs.. That seems reasonable although I'm unsure about the naming.  The hierarchy will then be MyTable -> public BaseTable -> private Table.  Would it be okay to rename Table to BaseTable or something like InnerTable so we'll get MyTable -> public Table -> private BaseTable?. I didn't run reflection/generate_code.sh for #4982.  I think the PR template should be amended to mention that also.  Additionally, what about a travis CI check that fails if either generate_code.sh creates a difference?. No, the union actually looks like a mixture of enum and struct:\n```\nnamespace weird_union;\n\ntable A {}\ntable B {}\ntable C {}\nunion U {\n  AInUnion:A,\n  BInUnion:B,\n  CInUnion:C,\n  AInUnionAgain:A = 10\n}\n```\nThis gives you the following C++:\n```cpp\nnamespace weird_union {\nstruct A;\nstruct B;\nstruct C;\nenum U {\n  U_NONE = 0,\n  U_AInUnion = 1,\n  U_BInUnion = 2,\n  U_CInUnion = 3,\n  U_AInUnionAgain = 10,\n  U_MIN = U_NONE,\n  U_MAX = U_AInUnionAgain\n};\n...\n```\nThe underlying enum is fixed to type ubyte which is what the test tests.. Yes, you are right.  Using ParseSingleValue makes the code much shorter now.  I've kept denying boolean enums even though you could define enum X:bool { A = true, B = false } which could make sense because it doesn't currently forbid enum X:bool { A = 100 } which doesn't make sense in my opinion.  It would also need special handling in the code generators, I guess.. The reason is that with flatc --schema --binary --size-prefixed a size prefixed schema is created.  Now if you want to read or create a plain buffer with a size prefixed schema or a size prefixed buffer with a plain schema you need flags to tell which is which.  I imagine that the size-prefixed schema case itself is rare so you rarely ever need to give this flag.. Looks good to me.. > While this is a more lenient system, it does introduce the problem that the moment you add a second instance of a type in your union, all your code generation changes. Before, your code generation would change the moment you switch to using aliases, which is a little more predictable.\n\nNot saying your approach is wrong, but we should be aware of this. It shouldn't affect too many people, but I wonder if it is worth it, being less predictable.\n\nWhat do you think of removing the definition of template<typename T> struct {{ENUM_NAME}}Traits but always providing the declaration (regardless of uses_multiple_type_instances?  If uses_multiple_type_instances we can generate a comment or static_assert that explains why it's missing.  As a side effect it would also also make {{ENUM_NAME}}Union::Set stricter since it would report a compiler error instead of just setting NONE when given a type that is not in the union.  I'd do that in a separate PR, though.. This affects Rust and C++ so I'd like to have this in moster_test.fbs.  I'll try to fix C# build.. > Other people may use those traits to map from type to enum? What do you mean by \"always providing the declaration\" (example)?\nI don't think the default definition\ncpp\ntemplate<typename T> struct AnyTraits {\n  static const Any enum_value = Any_NONE;\n};\nserves any purpose.  Rather it will hide errors since it allows me to call AnyUnion::Set with any type, silently discarding the value of any type that is not in the Any union.\nI think we can just always declare it like this:\ncpp\ntemplate<typename T> struct AnyTraits;\nSo you'll get something like\nerror: incomplete type 'AnyTraits<int>' used in nested name specifier\nThere might be other ways (static_assert or such) to customize the error message but the main point highlight instead of swallowing what is most likely unintended behavior.\nIf possible we might want to customize the error message so in case the trait cannot be generated because of ambiguities you'll get a message telling you that.  Maybe that is too complex though and a comment in the generated code is sufficient.. I think it'll still be possible.  I'll try to produce some code soonish.  That'll make it easier to talk.. Problem is, you already have two kinds of schema files, depending whether you call  flatc --binary --schema with the  --size-prefixed option or not.  In #5001 I added the flag so you can serialize and deserialize with or without size prefix independently of whether your schema is size prefixed.. ",
    "krupnov": "Yes.\nIf there is no problem with it I will try to make it next week.. Hi. I created patch and tried to push it. But I got 403 HTTP error.\nCould you please help me?\nI am doing:\ngit checkout -b vector_random_access_iterator\ngit push origin vector_random_access_iterator\nAnd geting error:\nerror: The requested URL returned error: 403 while accessing https://github.com/google/flatbuffers.git/info/refs\nfatal: HTTP request failed\nMy git version:\ngit --version\ngit version 1.7.9.5\nSorry. It is my first pull request. I think I need to make fork. :). OK.\nI have done it with fork. Hope everything is fine now :). I signed it!. ",
    "unice2345": "Thank you for your reply!\nYes, I use the unmodified benchmark in the  benchmark branch, but using VS2015. \n. ",
    "cdkamat": "Thanks for fixing this.. ",
    "TamToucan": "Thanks. Any thoughts on how I can code up generating a json representation of the schema from just the .fbs? I guess reflection is no use since that only works on existing data? (which if I had that I could just use --json).\nSeems strange that json is supported as an input, but if I have a large schema I have to start with an empty file to create input (or write code to populate data which negates most of the benefit). Would be handy to have the complete json to start with.. Brilliant, I hadn't thought about exactly what the --schema option did. Need to think about how best to show union.\nThanks.. FYI I added a -flag, pretty hacky, but suitable for my needs\nhttps://github.com/TamToucan/flatbuffers/tree/schema-to-json. The union being incompatible was deliberate to force a choice of which union member to use. Maybe not the best, but I prefer being able to see all the members easily. For recursion it just prints \"NESTED->name\" since I don't see myself using recursive data (a chest containing a chest).. The parser isn't static, the object that contains the parser is static. e.g. It's not hard to imagine a function that has a static object which is init'ed with a FBS at the top of function and that function then uses the object on whatever JSON file has been given.\nI can easily work around the problem, but at the very least there needs to be a clear warning that you can't use the library this way. Especially since in theory the problem could appear/disappear with different versions of the compiler.. ",
    "zplzpl": "grpc need c++11\nso i use gcc 4.8.2~. I updated the flatbuffers project, and now he's working on it.. getValues is through flat.exe get the method, not my own preparation. 50 millisecond\u3002 not microseconds.. ",
    "linwukang": "hi @aardappel, when will the python-supported feature be released? :-). Is the generator for java support flexbuffer as well?. ",
    "manolama": "Ahhh, duped #4110. As long as one of em makes it in that'd be great.. ",
    "ricardoquesada": "I signed it!. @aardappel fixed. thanks.. ",
    "leeygang": "I signed it!\n. @aardappel , I signed CLA and fix the email config in my git config, how can I ask cla bot to recheck? or how can I get the reason failed for cla?. replied the comment and update the comment.. Thanks Wouter!\nOn Mon, Oct 9, 2017 at 8:40 AM, Wouter van Oortmerssen \nnotifications@github.com wrote:\n\nMerged #4451 https://github.com/google/flatbuffers/pull/4451.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/4451#event-1284375400, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAbPzJJmO6B7D6Izo0PHSBsnOlv62BmNks5sqj59gaJpZM4PvhVZ\n.\n. update to generate the import when gen_nullable flag is on.. Thanks!. xcode treat clang extension as error, so this will stop report _Nullable or not use _Nullable as error.. yes, currently --clang-nullable is off by default, I can revert the generate script.. Thanks aardappel! I tried: return flatbuffers::GetRoot(&empty_table[4]); but the test will crash.. I was looking for internal document, did not find where the memory layout for table :-(. oh, this should work, I should not use GetRoot<>, is this co-incident again?\n  static const uint8_t empty_table[] = { 0, 0, 0, 0, / table pointer points here / 4, 0, 0, 0};\n  return reinterpret_cast(&empty_table[4]);. \n",
    "WiSaGaN": "@aardappel , could you point to the commit/pull request that fixes this? This still happens in 1.9.0.. @aardappel it's the same error:\n```\n-sh-4.2$ flatc --version\nflatc version 1.9.0 (Jun 21 2018 10:18:25)\n-sh-4.2$ cat foo_generated.h\n// automatically generated by the FlatBuffers compiler, do not modify\nifndef FLATBUFFERS_GENERATED_FOO_H_\ndefine FLATBUFFERS_GENERATED_FOO_H_\ninclude \"flatbuffers/flatbuffers.h\"\nstruct Empty;\nMANUALLY_ALIGNED_STRUCT(1) Empty FLATBUFFERS_FINAL_CLASS {\n private:\npublic:\n  Empty() {\n    memset(this, 0, sizeof(Empty));\n  }\n  Empty()\n      :  {\n  }\n};\nSTRUCT_END(Empty, 0);\nendif  // FLATBUFFERS_GENERATED_FOO_H_\n``\nNote the duplication of default constructor inpublic.\nBy the way, I usedgcc 7.1to compile theflatc`.. Thank you @aardappel . I assume this will go into 1.10.0?. ",
    "nicholasbishop": "I'm still seeing an error when compiling an empty struct:\n```\n$ g++ -I flatbuffers/include empty_generated.h \nIn file included from empty_generated.h:7:\nflatbuffers/include/flatbuffers/flatbuffers.h:2356:32: error: static assertion failed: compiler breaks packing rules\n     static_assert(sizeof(name) == size, \"compiler breaks packing rules\")\nempty_generated.h:19:1: note: in expansion of macro \u2018FLATBUFFERS_STRUCT_END\u2019\n FLATBUFFERS_STRUCT_END(Empty, 0);\n ^~~~~~~~~~~~~~~~~~~~~~\n$ g++ --version\ng++ (GCC) 8.1.1 20180712 (Red Hat 8.1.1-5)\nCopyright (C) 2018 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n$ build/flatbuffers/flatc --version\nflatc version 1.10.0 (Oct 13 2018 10:06:04)\n```. My use case involves auto-generating a lot of structs from a pre-existing format. So it's certainly possible for my code to insert a dummy struct member, or perhaps to represent empty structs as just names in an enum.\nPerhaps for now it would be best to explicitly error out on empty structs, and revisit the problem if someone provides a more compelling use case than mine.. ",
    "CarsarSky": "@aardappel grpctest.cpp failed to compile when streaming methods are enabled. This change simply moved the#include \"flatbuffers/grpc.h\"line from monster_test.grpc.fb.cc to monster_test.grpc.fb.h.. ",
    "ankurdave": "Signed CLA. Looks like the Windows build is broken in an unrelated way.. ",
    "gbeili": "I signed it! (as a Googler). as this refactoring is already partially done by other PRs, so i'm going to close this one.. Done. Thanks!. closed the PR by accident. Trying to squash the commits by using \"\ngit reset $(git commit-tree HEAD^{tree} -m \"commit message\")\", then \"git push -f origin vector_unions\". The PR is then closed. Now trying to recover.. Restored the PR at https://github.com/google/flatbuffers/pull/4148. actually the sample schema is not changed. the code here generated is due to the change of the GenUnionPost, which is invoked on the existing union.. i'd love to return a error code / message so it's easier to debug. for now i'd just return false as you suggested. any plan/demand to add such code/msg?. Done.. Done.. Done.. ACK.. Done.. Done.. Done.. Done for flatbuffers::Offset types. I personally find buf concrete type to be pretty useful.. Done.. Done. make sense.. Discussed offline. Added a TODO. . Done.. ",
    "cbsuh": "I'm busy with other stuffs so, I've changed my fbs from ulong to long.\nI'll try to make a patch later, if no one fixes it. :). ",
    "mpusz": "I signed it!. For my traffic (finance domain) it was nearly 20% improvement on flatbuffer serialization. Note that we took care that no reallocation is being done on handling data path.. For my traffic (finance domain) it was about 18% improvement on flatbuffer serialization. You should measure it on your workloads to verify how it proves for you. However just with a simple check where push() is used in flatbuffers.h you will see those are probably not as small data ranges as it was mentioned in the comment.. As I wrote I already used that patch with memcpy() in my project and got significant performance increase. Wanted to share that with others but it is up to you to decide if you want to merge it or not. Probably you should measure it by yourself on your benchmarks.. ",
    "ctiller": "There should be one '.'.. LGTM.. Note that gRPC does need C++11, so I expect a full CI with VS2010 is going to barf anyway :). Is this somehow guaranteed to be called only twice?. why not just use the GRPC_SLICE_START_PTR(slice_) macro?. ",
    "pascaldekloe": "Maybe run go fmt after generation?. Of course you do pref that. It's too much work for me tough. Especially the little details like field value aligns such as https://github.com/google/flatbuffers/pull/4134/files#diff-841c858bd318a96a7b116201e772d453. ",
    "tzutalin": "Agree. By the way, if google can provide more examples, template methods, or handy tool to serialize or deserialize flatbuffer, it will be better. \nE.g: adding some snippet of sample code to demonstrate how to serialize or deserialize flatbuffer, in  samples/sample_binary.cpp \nE.g: Add a class and template methods to serialize or deserialize\nstd::ofstream os(\"out.bin\", std::ios::binary);\n  flatbuffer::BinaryOutputArchive archive(os);\n  archive(builder);. When I started to replace my cereal or protobuf with flatbuffer, I cannot find a handy method to save/load serialized file to/from disk. . ",
    "EdwardPrentice17-21": "I believe I have asked many questions over the past couple of days, on\nvarious sites and have had them answered by yourself. Thank you, it is much\nappreciated.\nI have now got it working now and it does seem trivial now I appreciate\nthat each individually created table is stored inside my arrays of\nflatbuffers::Offset\nFlatbuffers will be employed within my integrated masters project in\nelectronic engineering at York university, part of which involves sending\nlarge amounts of enviromental data over a wireless network. Just incase\nthis is of interest!\nThanks again!\nOn 11 January 2017 at 19:01, Wouter van Oortmerssen \nnotifications@github.com wrote:\n\nYes, there's no way around the lack of nesting, this is required for the\nencoding to work properly.\nUsually it is is not much more code though, i.e. it is a question of\ncollecting offsets (in e.g. a vector) and then using those offsets when\nconstructing the parent.\nOne thing would be to make sure to use vectors where possible, i.e. the\nabove schema should be:\ntable FB_Subsubdata {\n  dataContainer:[FB_DataContainer];\n}\ntable FB_Subdata {\n  section:[FB_Subsubdata];\n}\ntable FB_DataPacket {\n  db:[FB_Subdata];\n}\nThat should simplify your code a lot.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/4136#issuecomment-271961962,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/APPRgqM0OoJMpvQJwvds2C2p5EPBwvZSks5rRScmgaJpZM4LfoKc\n.\n\n\n-- \nEdward Prentice\nElectronic Engineering Student, University of York\nMob: 07938129307\nMail: epp502@york.ac.uk\n. ",
    "tatsuyuki25": "@aardappel \nHi, I have tried the second solution you provided, but still could not make it.\nWould you please provide sample code of this solution -> One alternative is to store all of bytes in the new builder using a [ubyte].\nThank you so much.. @aardappel \nthis is my testing code\nfbs\n```\ntable A {\n  i:int;\n  b:[B];\n  s:string;\n}\ntable B {\n  i:int;\n}\nroot_type A;\n```\nFlatBufferBuilder\njava\n...\n    public void putB(byte[] b) {\n        bb.position(space -= b.length);\n        bb.put(b);\n    }\n...\n```java\n        FlatBufferBuilder builder2 = new FlatBufferBuilder();\n        FlatBufferBuilder builder = new FlatBufferBuilder();\n        int offset = B.createB(builder2, 784);\n        builder2.finish(offset);\n        byte[] bytes = builder2.sizedByteArray();\n        builder.putB(bytes);\n        int o1 = builder.offset();\n    builder2 = new FlatBufferBuilder();\n    offset = B.createB(builder2, 77);\n    builder2.finish(offset);\n    bytes = builder2.sizedByteArray();\n    builder.putB(bytes);\n    int o2 = builder.offset();\n    builder.startVector(4, 2 , 4);\n    builder.addOffset(o1);\n    builder.addOffset(o2);\n    offset = builder.endVector();\n\n    A.startA(builder);\n    A.addB(builder, offset);\n    A.addI(builder, 2);\n    A.finishABuffer(builder, A.endA(builder));\n    A a = A.getRootAsA(ByteBuffer.wrap(builder.sizedByteArray()));\n    System.out.println(a.i());\n    System.out.println(a.bLength());\n    System.out.println(a.b(0).i());\n    System.out.println(a.b(1).i());\n\n```\noutput\n2\n2\n0\n524294\nI think it may be wrong in my revised FlatBufferBuilder. ",
    "KageKirin": "Thanks for the clarification.\nIn my use-case, it is indeed intended to run on little-endian, so no problems here.\nRegards.. Addendum: I just tried the above repro-steps with flatc v1.60, and the the file pair hello.grpc.fb.h/cc gets generated, albeit in the folder flatc is run from.. https://github.com/google/flatbuffers/pull/4207\nHow do we proceed from here? I signed the CLA.. You're welcome. Thanks for the integration.. I have a potential fix, but it requires more changes in the generator and is C++17 only:\ntemplate<typename T, typename S = T> Offset<Vector<const T *>> CreateVectorOfStructs(\n      const std::vector<S> &v) {\n    return CreateVectorOfStructs<T,S>(data(v), v.size());\n  }\nand \n```\ntemplate Offset> CreateVectorOfStructs(\n      const S *v, size_t len) {\n      if constexpr(std::is_same()) {\n        StartVector(len * sizeof(T) / AlignOf(), AlignOf());\n        PushBytes(reinterpret_cast(v), sizeof(T) * len);\n        return Offset>(EndVector(len));\n      }\n      else {\n        auto del = {delete [] p;};\n        std::unique_ptr vv(new T[len]);\n        std::transform(v, v+len, vv.get(), { return Pack(_v);});\n    StartVector(len * sizeof(T) / AlignOf<T>(), AlignOf<T>());\n    PushBytes(reinterpret_cast<const uint8_t *>(vv.get()), sizeof(T) * len);\n    return Offset<Vector<const T *>>(EndVector(len));\n  }\n\n}\n```\nIt's C++17-only because of the if-constexpr which greatly simplifies the template logic.\nAnd the further changes it requires are to generate the calls to CreateVectorOfStructs() with both template params.\nWould this approach actually be acceptable for a PR?\nRegards.. Here's a more generic C++14-compatible solution, that does not modify the existing functions.\n```\ntemplate Offset> CreateVectorOfStructsPack(const S *v, size_t len) {\n    extern T Pack(const S&);\n    std::vector vv(len);\n    std::transform(v, v+len, vv.begin(), { return Pack(_v);});\n    return CreateVectorOfStructs(vv.data(), vv.size());\n  }\ntemplate Offset> CreateVectorOfStructsPack(\n      const std::vector &v) {\n    return CreateVectorOfStructsPack(data(v), v.size());\n  }\n```\nThe calling code in the _generated.h would have to be like this example:\nauto _vertices  = _o->vertices.size() ? _fbb.CreateVectorOfStructsPack<flatmath::Vector3_f64, glm::dvec3>(_o->vertices) : 0;\nWould this be an acceptable solution?\nRegards.\n. Hello again,\nI just submitted a PR to fix this issue. ~~#4274~~ #4276\nBest regards.. You're welcome.. Oh, looks like I didn't run my changes over the tests. Sorry for the hassle, but I'll be closing this PR and re-submit when everything is fixed.. Hello again,\nAfter a few trials, I found that using \"embedded\" schemas (strings) is possible, even if these schemas include further ones. \nBelow solves the first part of my question (in the example I stated above):\nflatbuffers::Parser parser\nbool parseOk = parser.Parse(math_schema_source, nullptr, \"math.fbs\")\n  && parser.Parse(monster_schema_source, nullptr, \"monster.fbs\");\nmonster.fbs includes math.fbs, but since math.fbs is parsed before monster.fbs, it is already known at that point and the include statement does not produce a parser error.\nIt would be great if this information could be added to documentation.\nI'll be submitting a PR for a nicer Parse() function later.\nAs for the second issue I mentioned, I would still like to know whether it's possible to initialize a parser from a binary schema.\nBest regards.\n. Fair point about it being C++11+ only and the headers getting included.\nI can surround it by a #if __cplusplus >= 201103L check to have it only available in C++11 and beyond, if needed.\nThe main idea behind this function is to avoid calling and checking Parse() many times, especially when having a longer list of dependent schemas (8, in my case).\nIf the C++11 usage of intializer_list and tuple is still an issue, we can replace them by an array/pointer to either a tuple or a structure, although I don't think introducing a new type for the sake of being passed as function argument is worth the work. (IMHO, this is why there is tuple for this sort of cases).\nIn the end, this PR is a proposal and it's your decision whether to accept it or not, or whether you would accept in a different form.\nI have a wrapper function on my side that does exactly the same (in fact, the implementation prototype of the new Parse()), so this wouldn't be much an issue on my side.\nRegards.. I think, the question was supposed to be \"are flatbuffers compatible with web assembly, i.e. does the lib compile with Emscripten?\"\nWell, yes, that's the case. The C++ code can compile to WASM/ASM.js via Emscripten, as it's just classical C++11 source, so entirely compatible with Emscripten's Clang compiler.\n. Thanks.\nOk, so far, I've found the place in where to set the bool flag for nested_flatbuffers https://github.com/google/flatbuffers/blob/master/src/idl_parser.cpp#L714\nAdding the else-if for this flag below the flexbuffer check (https://github.com/google/flatbuffers/blob/master/src/idl_gen_text.cpp#L178) is trivial, but I don't see how to call GenStruct on the nested flatbuffer to recursively generate its Json output.\nMore in detail: current else-if looks like this\nelse if (fd.nested_flatbuffer) {\n      auto vec = table->GetPointer<const Vector<uint8_t> *>(fd.value.offset);\n      auto root = GetRoot<Table>(vec->data());\n      return GenStruct(*fd.value.type.struct_def,\n                       root,\n                       indent, opts, _text);\n  }\nMy main issue, is that fd.value.type.struct_def is NULL, hence calling GenStruct fails on a nullptr access. Is there another way to access the struct_def, or should I be set when parsing the schema?\nCould you please provide a bit of help?\nBest regards.\n. I like how I always manage to find an anwser 5 minutes after asking for help.\nI found that at https://github.com/google/flatbuffers/blob/master/src/idl_parser.cpp#L725\nthe return value of LookupCreateStruct(nested->constant) is left unassigned.\nIf I assign it to field->value.type.struct_def like field->value.type.struct_def = LookupCreateStruct(nested->constant);, I get perfectly well generated Json for the nested_flatbuffer.\nAre there any side-effects to this assign?\nRegards.. Also, since we have to support Json parsing as well,\nSeeing there's a special case handling flexbuffers at https://github.com/google/flatbuffers/blob/master/src/idl_parser.cpp#L929, I assume the Json parsing for nested_flatbuffers should get its own case below.\nAre there better ways than instancing a new Parser and having it parse the JSON substring into a buffer to assign back to the container?\n. Thanks.\nI made a few changes following your suggestion about the type's StructDef: setting it in the 'nested_flatbuffer' attribute now. (I'll be submitting a first pull request soon).\nAs for the JSON parsing, I have a working 'workaround' that relies on Flexbuffer, and which could incidentally be a good use case for https://github.com/google/flatbuffers/issues/4380\n```\n} else if (field->nested_flatbuffer) {\n          auto nested = field->attributes.Lookup(\"nested_flatbuffer\");\n          flexbuffers::Builder builder(1024,\n                                         flexbuffers::BUILDER_FLAG_SHARE_ALL);\n          ECHECK(ParseFlexBufferValue(&builder));\n          builder.Finish();\n          auto fxbuf = builder.GetBuffer();\n          auto fxroot = flexbuffers::GetRoot(fxbuf.data(), fxbuf.size());\n          std::string substring;\n          fxroot.ToString(true, false, substring);\n      Parser nestedParser;\n      nestedParser.root_struct_def_ = nested->type.struct_def;\n\n      if (!nestedParser.Parse(substring.c_str(), nullptr, nullptr))\n      {\n        ECHECK(Error(nestedParser.error_));\n      }\n      auto off = builder_.CreateVector(nestedParser.builder_.GetBufferPointer(), nestedParser.builder_.GetSize());\n      val.constant = NumToString(off.o);\n    } else {\n\n```\nIt's really just a workaround I needed as proof-of-concept, but it works.. Ah, thanks. Indeed, using the ParserState members would have been easier.\nAnyway, I have an improved code version ready and am currently preparing a pull request for the whole modification.\nCheers.. Alright for the changes. I will update the PR tomorrow at work.. Ok, so I pushed the changes (after rebasing master into it, which forced me to push --force-with-lease the full PR again).\nPlease let me know if there are more changes required.. Alright. I pushed the requested changes.. I pushed the last requested change.\nBest regards.. Well, it's more a very specific 'feature' of the glTF standard, that allows to embed binary data buffers as base64-encoded strings inside its JSON format.\nFull disclaimer: I'm currently writing a GLTF library based on flatbuffers and its Object-API. I already successfully ported the glTF JSON schema to a flatbuffer schema and I'm almost done writing an object creation API. The plan is to release this later this year to github, once it's stable.\nI agree with the alternatives you proposed, but having the option to use a base64 string instead of an u8array, and having flatbuffer's JSON parser automatically convert between them, could make sense in other use cases as well.\n. \"embedded\" glTF JSON files that contain all the binary data in base64-strings are 1 possible way to deliver self-contained assets.\nThe other one is glTF-binary (glb) which contains 1 large blob after the JSON metadata.. glTF is just one, albeit personal, use case (see https://github.com/KageKirin/flatgltf_2_0 for reference) where out-of-the-box base64 encoding would allow for less code on my side.\nI agree with you that it's kind of hacky and not the best solution for performance.\nIdeally, a [ubyte] could take an optional attribute to set a flag for base64 (with mimeType) in the schema, and only those buffers would get base64-encoded when serializing to JSON, i.e. the flatbuffer binary version would keep the bytes as is.\nWhat do you think of this idea?\nAnd where would be the best place to start adding this?\n. Oh, sorry. It figures I had modified the project's build settings to compile with -std=c++14 instead of -std=c++11, so this change actually compiled.\nI'm closing this pull request for now, as support for for-loops in constexpr functions is not possible with C++11.\nRegards.. Ok, so far I added the parsing support (see pull request above).\nSo parsing works, but... due to the \"cpp_type\"\nThe former std::vector<uint64_t> children; gets generated as NodeT* children;.\nObviously, I'd like this to be a std::vector<NodeT*> children;.\nI see that the type change occurs in CppGenerator::GenMember() but are there other areas to take into account?\nPlease advise.\nRegards.\nEDIT: Already added the generation part. Please follow the pull request for updates and advice/code review. Thanks.. added so far:\n- parser support for fields : [ulong](hash: \"fnv1a\", cpp_type: \"FieldT\");\n- C++ code generation of above definition as std::vector<FieldT*> fields;\n- C++ code generation of initializer of above field as : fields()\nmissing yet:\n- rehasher generation\n- resolver generation. Follow up: \nThe Pack-ing part of this works in my small test case, provided a custom rehasher_function_t.\nThe UnPack-ing part of this works as well, given a custom resolver_function_t.\nSo, I think the functionality can be merged now.\nWhat do you think?\nRegards.. Oh, I see merge conflicts. Do you want me to rebase the changes first and then update this pull request?. Ok, so I rebased and tested the code on my little example from #4613 as well as on my full codebase.\nLooks good so far.\n. no problem.\nI'll also condense the changelists into fewer ones if you're fine with it.. Alright.\nI rebased from master, condensed the commits into fewer and fixed the whitespace issues.\nWould be great to have this merged pretty soon: I have another WIP with a fix for #4615 that I'd like to pull-request this week.\nCheers.. You're welcome!. Any ETA on when this PR is finalized and merged? I'd like to use it for some projects of mine (among which, https://github.com/KageKirin/flatGLTF). I see.\nSo, I made this short example schema:\n```\nnamespace test;\ntable Target {\n  id:ulong(key, hash:\"fnv1a_64\");\n  name:string;\n  foobar:[uint];\n}\ntable Reference {\n  hoge:int;\n  fuba:float;\nref:ulong(hash:\"fnv1a_64\", cpp_type:\"TargetT\");\n}\ntable Root {\n  targets:[Target];\n  refs:[Reference];\n  ref2:ulong;\n}\n/// file meta\nfile_identifier \"rslv\";\nfile_extension \"resolvetest\";\nroot_type Root;\n```\nGiven the code this schema generates, I have this part for Json serialization:\n```\nauto root = std::unique_ptr(new test::RootT());\nauto rehasher = flatbuffers::rehasher_function_t(\n        &root -> flatbuffers::hash_value_t {\n            for(auto& tgt : root->targets)\n            {\n                if(pointer == tgt.get())\n                {\n                    return tgt->id;\n                }\n            }\n            (void)pointer;\n            return 0;\n        });\n    test::FinishRootBuffer(parser.builder_, test::RootT::TableType::Pack(parser.builder_, root.get(), &rehasher));\n```\nSo, my questions:\n1. Am I right assuming that, the more referrable types (like Target) I have, the more collection must be looped through in the rehasher?\n2. Passing the container through lambda context ([]) is the only way to access the elements, as nothing gets passed into the rehasher function?\n\nFor the resolver function: is there a way to get the context of which hash is being resolved? I.e. what type of element is referring or being referred to?\nWhat do I need to pass as context to the resolver function to look up the correct pointers?. After testing a bit more, I managed to write a resolver function:\nauto root = std::unique_ptr<test::RootT>(new test::RootT());\nauto resolver = flatbuffers::resolver_function_t(\n        [&root](void **pointer_adr, flatbuffers::hash_value_t hash) {\n            for(auto& tgt : root->targets)\n            {\n                if(hash == tgt->id)\n                {\n                    *pointer_adr = tgt.get();\n                    return;\n                }\n            }\n        });\ntest::GetRoot(parser.builder_.GetBufferPointer())->UnPackTo(root.get(), &resolver);\n\nThis works, but a few things were unexpected:\n- the short version, auto root = test::UnPackRoot(parser.builder_.GetBufferPointer(), &resolver); cannot work, as the returned instance is created inside the UnPack function and thus cannot be bound to the resolver. (I think there's a design issue here).\n- the resolving part is not postponed until all elements are created: it works in this example case because the Targets are created before the References and thus can be resolved, but if the schema has the elements in a different order, they cannot be properly resolved.\n- the latter makes circular references impossible.\nCheers.\n. Thanks for the feedback. (Also for pointing out the obvious solution for resolving, i.e. \"store hash and pointer and resolve after UnPack() returned\").\nWhat I called the \"short version\" was this:\nauto root = test::UnPackRoot(parser.builder_.GetBufferPointer(), &resolver);\nwith resolver defined as this:\nauto resolver = flatbuffers::resolver_function_t(\n        [&root](void **pointer_adr, flatbuffers::hash_value_t hash) {\n            for(auto& tgt : root->targets)\n            {\n                if(hash == tgt->id)\n                {\n                    *pointer_adr = tgt.get();\n                    return;\n                }\n            }\n        });\nwhich would have led to chicken & egg problem, as the root bound into the lambda does not exist yet when the lambda is created.\nAnyway, I'll be playing around with the different possibilities for rehasher/resolver and try to figure out a way to generate these functions automagically.\n(And with regards to a pending solution #4615, I'll try to figure out how to structure the resolver function for non-raw pointer types).. Short update: resolving hashes as post-processing step works pretty good.\nHere's a resolver example:\n// store map{ pointers -> hash } when creating elements\n    std::map<void**, flatbuffers::hash_value_t> ptr_to_hash;\n    auto resolver = flatbuffers::resolver_function_t(\n        [&ptr_to_hash](void **pointer_adr, flatbuffers::hash_value_t hash) {\n            ptr_to_hash[pointer_adr] = hash;\n        });\n    auto root = test::UnPackRoot(parser.builder_.GetBufferPointer(), &resolver);\n    // use map to resolve pointers as a post-process step\n    for(auto& p_h : ptr_to_hash)\n    {\n        auto [pointer_adr, hash] = p_h; // C++17 decomposition, but using pair::first()/pair::second() works as well\n        for(auto& tgt : root->targets)\n        {\n            if(hash == tgt->id)\n            {\n                *pointer_adr = tgt.get();\n                continue;\n            }\n        }\n    }. Hi,\nif you use the C++ object API (generated with the arguments --cpp --gen-object-api), you can unpack the flatbuffer to a ZamPacketListT that contains a regular std::vector<std::unique_ptr<ZamPacketT>> to which you can append. (You will need to pack this  back to a flatbuffer for writing out).\nI might be mistaken, but from looking at the generated code and the implementation of flatbuffers::Vector<>, I don't see any function for appending data directly.\nCheers.. Alright, I rebased (and fixed the warning/compiler error in flathash.cpp along the way).\nCheers.. You're welcome.. As always, you're welcome. And thanks for the merge.. Interestingly, the build failure comes from the Android build with GCC 4.9 which does not provide std::shared_ptr and std::weak_ptr in the <memory> header, or so it seems.\n(I use shared/weak pointers to illustrate the feature in monster_test.fbs).\nI'm honestly a bit puzzled as to why this Android/GCC 4.9 has unique_ptrs but not shared/weak ones...\nSetting environment variables from .travis.yml\n$ export GCC_VERSION=\"4.9\"\nerror: no member named 'shared_ptr' in namespace 'std'\n  std::vector<std::shared_ptr<ReferrableT>> vector_of_strong_referrables;\n              ~~~~~^\n/home/travis/build/google/flatbuffers/tests/monster_test_generated.h:723:31\nand many other related errors follow. Oh, I overlooked this:\n31% EXECUTING [3m 19s]> Task :externalNativeBuildStlportDebug\nso it's the STLport build that fails due to the lack of shared_ptr/weak_ptr?\nI think there is a flatbuffers:: alias, should I use this one instead?. Ok, I'd like this pull request to be mergeable, which implies that the build passes.\nThe culprit is the usage of non-supported std::shared_ptr and std::weak_ptr in the example code of monster_test.fbs, i.e. these lines:\nvector_of_strong_referrables:[Referrable](id:38, cpp_ptr_type:\"std::shared_ptr\");\nco_owning_reference:ulong(id:39, hash:\"fnv1a_64\", cpp_type:\"ReferrableT\", cpp_ptr_type:\"std::shared_ptr\");              //probably not a good idea for real usage\nvector_of_co_owning_references:[ulong](id:40, hash:\"fnv1a_64\", cpp_type:\"ReferrableT\", cpp_ptr_type:\"std::shared_ptr\");\nnon_owning_reference:ulong(id:41, hash:\"fnv1a_64\", cpp_type:\"ReferrableT\", cpp_ptr_type:\"std::weak_ptr\", cpp_ptr_type_get:\".lock().get()\");             //prefer this instead\nvector_of_non_owning_references:[ulong](id:42, hash:\"fnv1a_64\", cpp_type:\"ReferrableT\", cpp_ptr_type:\"std::weak_ptr\", cpp_ptr_type_get:\".lock().get()\");\nShould I rather comment out these lines with a mention of the reason, or just change them to something different (pretty much only std::unique_ptr or naked at this point), or just remove them altogether?\nPlease advice.. Well, I guess, I'll go ahead and replace the shared_ptrs with unique_ptr ones, and the weak_ptrs with naked ones for now.\nAnd then, in a later pull request, I'll send you the sample programs for rehasher/resolvers that I used to create this code. I wanted to add them anyway, as documentation and to solve #4613.\nRegards.. Thanks for the merge.. Ok, I see. Then it makes sense to leave it as it is.\nIt might make sense to change the codegen once C11/C++20's \"designated initializers\" have become more ubiquitous, though.\n. Hello,\nwe got bitten by this as well, so I submitted the pull request #4761 listed above.\nBest regards.. Ok, nevermind. I missed the obvious, namely marking the fields as (required) in the schema.. err, Travis is failing with the following, totally unrelated error:\nThe command \"if [ \"$TRAVIS_OS_NAME\" == \"linux\" ]; then sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test; fi\" failed and exited with 1 during .\nI don't think that's normal, is it?. Thanks for the review and the merge.. Happy new year!\nI haven't done much on flatbuffer features for a majority of last year, sorry.\nI plan to return to it this month (finally!) though.\n(Long story short, I had to \"optimize\" our in-house engine and that took more time than it was successful).\nIt would be great if we could coordinate our efforts, or at least test our use-cases against each others.\nMy plan is to be fully schema-compatible with flatcc's attributes and to allow my use-case (a glTF 2.0 subset) to work.. What do further layers of nested_flatbuffers require to be copied i.o. to be parseable?\n. Ok. In my opinion it made sense to keep the type inside the attribute, since the schema declaration states it.\nnested: [ubyte](nested_flatbuffer: \"my_nested_type\");\n. Fair enough. Also, storing it in nested_flatbuffer requires less lookups in the later run, no need to actually fetch the attribute. Change has been committed and pushed to the pull request.. I guess I'll go and write a unit test for multiple layers of nested flatbuffers, just to see how far this can be pushed.\nIn the end, I agree that it does not make much sense, unless someone plans to make an asset database out of nested flatbuffer assets.. Ok. I removed that comment (it was a remainder from inlining the initializing function, sorry for forgetting to remove it).\nI also applied the correct formatting style to the other comments in the same function.. True. != nullptr would be more correct than an implicit bool cast. Fixed.. Sorry for code-blindness hitting in when switching between coding styles (I'm using a modified 'Allman' in my codebase). Fixed.. Ah, sorry. Now I understand what you mean. You were referring to line 1069, and not the highlighted line above.\nWith more context: the nested type already hold by field.nested_flatbuffer is looked up again.\nif (field.nested_flatbuffer != nullptr) {\n    auto nested = field.attributes.Lookup(\"nested_flatbuffer\");\n    auto nested_type = nested->type.struct_def;\n    auto nested_type_name = WrapInNameSpace(*nested_type);\nObviously, this can be shortened to:\nif (field.nested_flatbuffer) {\n    auto nested_type_name = WrapInNameSpace(*field.nested_flatbuffer);\nOk, I will test this before updating this pull request.\nBtw, just below, I see some variables that are camelCase and not snake_case, namely\nnestedMethodName and getNestedMethodName. They seem to be in this form on purpose. Should I just rename them while I'm at it?\nauto nestedMethodName = MakeCamel(field.name, lang_.first_camel_upper)\n      + \"As\" + nested_type_name;\n    auto getNestedMethodName = nestedMethodName;\n    if (lang_.language == IDLOptions::kCSharp) {\n      getNestedMethodName = \"Get\" + nestedMethodName;. Alright. The renamer just got committed as well.. fair point.\nI reversed the logic to check for BASE_TYPE_VECTOR instead of IsScalar.. Actually, I don't think this part is needed, as vectors are initialised just fine without a call to their empty ctor.\nAlso, this makes a class' initializer list longer, which is not intended.. oh! looks like a merge error. will fix this in the next rebase.. This is because C++11 does not allow for-loops in constexpr, whereas C++14 does.\nBy making the definition separate, we can assure the functions become inline for C++11, which is what Flatbuffers is targeting, but can be used as constexpr when the compile unit is compiled as C++14.\nThis also why I added the feature check for __cpp_constexpr >= 201304L according to the ISO standard, for compilers that might not be fully C++14 compatible, but already expose this feature.. ",
    "CodeFortress": "What a turnaround time!  Thank you so much :). ",
    "jsanmiya": "Ugh. It's too bad we can't reopen this PR. We'll submit the new PR though (linked above), which is the same change.. Reopening, since we can't reopen the original PR (linked above). . ",
    "cmcneil": "*I'm looking through the different language APIs, and realizing that some of them DO implement my option 2 request. Javascript provides a fieldNameArray() method. So maybe this is a request for better coverage across the different language APIs.. ",
    "wrigby": "Hey guys! I wanted to chime in because I'm using Flatbuffers for similar use cases (moving large amounts of scientific data around). In my case, I'm transporting large vectors of uint16 sample data from a sensor.\nDepending on the language binding you're using - I've been working in Python and Go - you can find out the offset of the beginning a vector (of any data type), and use that.\nSince I'm moving the data into a NumPy array, the task is simplified by using numpy's fromstring function, but in  Be sure you're using raw mode for reading from files (open(filename, \"rb\")). Here's an example from a Jupyter notebook that I threw together (pardon the improper error handling):\npython\ndef array_from_flatbuffer(msg, offset, dtype=np.float64):\n    \"\"\" Creates a numpy array from a flatbuffer vector at vtable offset `offset` \"\"\"\n    o = flatbuffers.number_types.UOffsetTFlags.py_type(msg._tab.Offset(offset))\n    if o == 0:\n        return None\n    count = msg._tab.VectorLen(o)\n    start = msg._tab.Vector(o)\n    end = start + count * np.dtype(dtype).itemsize\n    return np.fromstring(msg._tab.Bytes[start:end], dtype=dtype)\nYou can look at your generated source code to find the vtable offset to pass into that function.\nCaveats:\n\nPotential endianness problems\nAny others?. @aardappel I agree - I acknowledge that this is a dirty hack. I may switch up our schema to use a byte array, and specify that application code should be responsible for knowing how that data is packed.. @rw I'm by no means a numpy expert, but I'd be happy to take a stab at it. It turns out the dtype argument to numpy.fromstring accepts an endianness specifier [1]. I just tested it out and it works perfectly for this. I can try to throw a quick PR together for review this weekend. I also noticed that #4090 is out there; sounds like numpy.frombuffer is a better approach. I'll do some reading up on it and see if I can take a stab at the codegen.\n\n1: https://docs.scipy.org/doc/numpy/user/basics.byteswapping.html. @googlebot I signed it!. @rw sure thing! It'll probably be a few days before I can write appropriate tests, but I'll take a look and see what I can do.. @aardappel @rw \nThis relies entirely on numpy's ability to specify the underlying format of the data you pass to it, which provides endian-safeness. The numpy array provides a view into the data in the flatbuffer, which makes this zero-copy. This PR doesn't have any usefulness for folks that aren't using numpy, but I think it's relatively ubiquitous in statistical and scientific computing applications that use Python.\nThis unfortunately doesn't help anyone that is storing other BLOB formats inside a flatbuffer. I think the right approach there is to have a method to get a python buffer or memoryview from a char or uchar vector in a flatbuffer. Providing this for vectors of other types would be possible, but would provide no safety for the client code.\nI do have reservations about putting so much code in the code generator just to support numpy, and I'm working up an alternative plan that puts more of the heavy lifting (in LOC terms, not computational cost) in the flatbuffers Python package.. I haven't forgotten about this, but have gotten a bit swamped with work. I have some POC Python-library code that adds a VectorAccessor class, implementing __get__, __len__, and __iter__, which allows accessing items from a vector with code like msg.MyVector()[0], or loop through with for item in msg.MyVector().. @ahundt Ooh thanks for the reminder! I transitioned jobs and dropped this on the floor in the process. I'll have to check around internally to make sure I'm okay to submit patches, but maybe I'll hack on this a little over the weekend.. ",
    "byron-hawkins": "For very large files that contain just a few large arrays, performance can be improved up to 2x by inflating the NumPyArray directly from the file, instead of copying the whole file into a bytearray (as we are instructed in the flatbuffers python tutorial). This does require a seek() which is slow, so small fields need to be sliced out and read via the buffer interface (i.e., what we currently have in the flatbuffers implementation). But for a large enough block of homogeneous records, the turnaround time of seek() plus numpy.fromfile() can be much better than the time of bytearray() alone. \nI would suggest a second set of numpy array accessors that use the file directly, with comments explaining the scenario where it may perform better. The messy part is that it may affect every call to encode.Get(), of which there are many... it won't help performance to put another branch or indirection step in that function (or worse, all of its call sites). A workaround would be to duplicate all the code necessary for loading a numpy array directly from the file/offset (including encode.Get()), and instruct the user to put smaller fields in some other file that can be reasonably loaded via bytearray() (an advanced user could slice them out of a large file, but that's tricky and probably requires tweaking offsets).\nHere is a little test program to demonstrate the potential speedup, where the entire file is just one giant block of homogeneous records (no seek required):\nimport numpy as np\n\ndef loadViaByteArray():\n    buf = open(\"largeFile.dat\", \"rb\").read()\n    buf = bytearray(buf)\n    array = np.frombuffer(buf, dtype=np.dtype(\"<u1\"))\n\n    print(\"Loaded a numpy array of length %d\" % len(array))\n\ndef loadViaFile():\n    largeFile = open(\"largeFile.dat\", \"rb\")\n    array = np.fromfile(largeFile, dtype=np.dtype(\"<u1\"))\n\n    print(\"Loaded a numpy array of length %d\" % len(array))\n\nComparison via timeit, starting with 726ms for a load via bytearray:\n> python -m timeit -n 10 -r 1 -- \"import test; test.loadViaByteArray();\"\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\n10 loops, best of 1: 726 msec per loop\n\nNow a load directly from the file in 353ms:\n> python -m timeit -n 10 -r 1 -- \"import test; test.loadViaFile();\"\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\nLoaded a numpy array of length 756270677\n10 loops, best of 1: 353 msec per loop\n\n.    The error was introduced in commit 3b23ff18ea27c6cac33c8211d405d57d4595db1a, and I get the same error on this gcc 8:\n    Using built-in specs.\n    COLLECT_GCC=/usr/bin/gcc\n    COLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/8/lto-wrapper\n    OFFLOAD_TARGET_NAMES=nvptx-none\n    OFFLOAD_TARGET_DEFAULT=1\n    Target: x86_64-redhat-linux\n    Configured with: ../configure --enable-bootstrap --enable-languages=c,c++,fortran,objc,obj-c++,ada,go,lto --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --with-linker-hash-style=gnu --enable-plugin --enable-initfini-array --with-isl --enable-libmpx --enable-offload-targets=nvptx-none --without-cuda-driver --enable-gnu-indirect-function --enable-cet --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux\n    Thread model: posix\n    gcc version 8.1.1 20180712 (Red Hat 8.1.1-5) (GCC)\n\nHere the error looks like this:\n    In file included from /home/sureau/lab/lib/flatbuffers/src/tests/test.cpp:19:\n    /home/sureau/lab/lib/flatbuffers/src/include/flatbuffers/minireflect.h: In constructor \u2018flatbuffers::ToStringVisitor::ToStringVisitor(std::__cxx11::string, bool, std::__cxx11::string, bool)\u2019:\n    /home/sureau/lab/lib/flatbuffers/src/include/flatbuffers/minireflect.h:293:7: error: declaration of \u2018vector_delimited\u2019 shadows a member of \u2018flatbuffers::ToStringVisitor\u2019 [-Werror=shadow]\n           : d(delimiter),\n           ^\n    /home/sureau/lab/lib/flatbuffers/src/include/flatbuffers/minireflect.h:290:8: note: shadowed declaration is here\n       bool vector_delimited;\n            ^~~~~~~~~~~~~~~~\n    cc1plus: all warnings being treated as errors\n\n. Here is a two-liner workaround:\n3b23ff18ea27c6cac33c8211d405d57d4595db1a.patch.tar.gz\n. The author of the commit says CI is using gcc 4.9, which does not have this warning. Apparently he is unaware of this issue and has not looked into it yet.. ",
    "sjoblom65": "I can create a PR if you tell me where to file it. I thought this issue list was where to file post it.. I signed it!. I have attached the output from generate_test. I have also attached the .fbs and generated code of service example that had the issue. \nThe change is that it uses the tables namespace (if any) to call \"CreateXXX\" from \"CreateXXXDirect\"\n```\ninline flatbuffers::Offset CreateStatDirect(\n    flatbuffers::FlatBufferBuilder &_fbb,\n    const char *id = nullptr,\n    int64_t val = 0,\n    uint16_t count = 0) {\n  return MyGame::Example::CreateStat(\n      _fbb,\n      id ? _fbb.CreateString(id) : 0,\n      val,\n      count);\n}\n```\nmonster_test_generated.h.txt\nservice.fbs.txt\nservice_generated.h.txt\n. I have updated the PR with some data...\nOn Mon, Feb 6, 2017 at 1:05 PM, Wouter van Oortmerssen \nnotifications@github.com wrote:\n\nCan you please run generate_code.sh in tests/ (and add any changes to\nthis PR) so we can see how this affects the generated code?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/4161#issuecomment-277763055,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AYEGoXmWFSJCfgqVE4ZZGXQ7sVig-bquks5rZ2DhgaJpZM4L1ev0\n.\n. Don't know what else to do. I have 3 commits pending to google : master and the code change can be seen here:\nhttps://github.com/google/flatbuffers/pull/4161/files\n. The generated file has been uploaded. Don't know why the diff is showing all the \"deletes\" and \"adds\" when just a few lines has been changed. The split view shows the changes Ok. Yes, I'm mostly a Windows guy... Wouldn't think Github would show the line ending as a file change to a text file. It got to be a setting to the diff tool or it would be good to auto strip the line feeds if this is the case. ... I tried to delete the commit and fix it but could find a way to do it. \n\nAt least the split view shows the actual change in darker \"Green\" see for example line 442.... Fixed the newline issue. Also added a test to monster for the \"ambiguous call to overloaded function\".. Done...\nOn Wed, Feb 8, 2017 at 3:22 PM, Wouter van Oortmerssen \nnotifications@github.com wrote:\n\n@aardappel commented on this pull request.\nIn tests/monster_test.fbs\nhttps://github.com/google/flatbuffers/pull/4161:\n\n@@ -6,6 +6,11 @@ namespace MyGame.Example2;\n\ntable Monster {}  // Test having same name as below, but in different namespace.\n+/// This will test a potential \"ambiguous call to overloaded function\" using the same data from different namespace.\n+table MonsterStatus {\nYou'll likely have to make the changes on your local branch, then push\nagain.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/4161, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AYEGoduiv1PfUtoLWuNtSU6nYOmQsHHrks5raiP7gaJpZM4L1ev0\n.\n. This seems all messed up... Mix of old files from 2007. Yes, is you have some handy unix style tools it helps a lot. I had a need to run flatc with many input files instead of executing it for each file. I seems like a handy option to have and it help me out a lot on a plain vanilla windows box.. Yes, I agree that it's fringe. It's also such a simple implementation and got really handy (and fast) on my windows box.\n\nExample:\ndir /b/s schemas/*.fbs > fbslist.txt\nflatc --cpp -file-names fbslist.txt\nI have another pull request (#5162) open and I'll clean this up once that's closed. \n. I'll open a new pull request with this cleaned up.. I understand what you mean with using tools to package the binaries. I started embedding the bfbs schema files as a resource on windows. This worked fairly good on windows but was a more challenging on the unix platforms. \nWhat I wanted to solve is the following:\n- Ensure the schemas are not lost (not be external in another file/resource).\n- Optionally add the schemas to the generated code so it can easily be added to the binaries.  \nI simply wanted a way to add the bfbs data as an optional resource to the binaries. Can you think of a different practical way of handling this? I started to generate a post process tool to do the same think but I felt this fit best inside of flatc. This is a spin-off from #5077 and #4178 to deliver the bfbs with the app.\n. Yep, I have looked at some of them and felt like it was not clean solution. I think the bfbs format is great, just missing some data handling...\nThe intent with adding this option to flatc was to have more common way of getting to the bfbs data on other languages too.  Like Java could implement the GetMonsterBinarySchema() by getting the underlying resource though the class loader.  \nRemember it's and optional extra parameter to flatc (use it if it makes sense). It might come in really handy working more and more with bfbs instead of using xxd, etc. :smirk:\n. I think we are on the same page that we want a way of embedding the bfbs so it can be easily access though code .\nI started also with creating a utility to generate the code for the bfbs. I realized that it was not going to clean and it made it more complicated having to have extra post processing operations. I don't see the format of the generated source be changed. It's just the raw data we already have in the flatc parser that needs to be encoded. Also having another script doing this work, that also mean you need python (if the utility is written in that language) on the machine you are building on and that add yet another requirement.\nWe just needed a \"encoding\" function in flatc and we get the code generated with a consistent naming convention (using the root_type). There was also no extra post processing of the generated files and not extra to be generated. \nAdding this command line option, there is no need to generate the bfbs file using \"flatc -b -schema\". The -gen-bfbs-embed simply does the whole job by serializing the schema to the parser buffer and we just encode that into code (no external files generated). \nThis to me was the most elegant and contained solution. . @vglavnyy \nBack from vacation so here is my delayed response!\n\nWhy do you say the flatc should generate a separate cpp file for the embedded data? I don't see any benefit for that.\nHaving a utility to generate cpp code will be hard to fit different projects. Most users will probably like to tweak it to generate code they are happy with. \nYes, an interface to the bfbs is ideal and can be easily created without using a utility. See the examples below how flexible it makes it pulling the data from the auto-generated headers. \n\nNot sure what you mean with:\n\nThe binary representation of fbs isn't invariant of the original fbs-schema\n\nThe schema used to generate the code goes hand in hand and so does the flatbuffer compiler. This it why it can be really important to have the schema as part of the generated code and not have it external and risk of loose it. For any project that is released, the flatbuffer compiler should also be saved so the auto-generated the code can be recreated. If a project is rebuilt 5 years later and uses new tools, you now have a new product. A practical example I encountered was switching from Solaris 12.4 to 12.5 (C++11) compiler where it produced a bad product (bug in the compiler). As soon as anything is replaced in the build system, things will change. So if the project is rebuilt in 2030 using the newest version of all the build tools, you might be in for a lot work!\nHere are a few different example implementation how to get access to the embed schema data:\nUsing templates:\n```\nifndef FLATBUFFER_SCHEMAS_H\ndefine FLATBUFFER_SCHEMAS_H\nnamespace FlatBufferSchemas\n{\n    template struct fbfs_getter;\ntemplate<> struct fbfs_getter<example::Monster>\n{ \n    static const unsigned char *data; \n    static const unsigned char *endPtr;\n};\n\n}\nendif // FLATBUFFER_SCHEMAS_H\nCompilation unit (cpp file):\nnamespace FlatBufferSchemas\n{\n    const unsigned char * fbfs_getter::data = MyGame::Sample::GetMonsterBinarySchema();\n    const unsigned char * fbfs_getter::endPtr = MyGame::Sample::GetMonsterBinarySchema(true);\n}\n```\nA C-Style implementation:\n```\nifndef FLATBUFFER_SCHEMAS_H\ndefine FLATBUFFER_SCHEMAS_H\nnamespace FlatBufferSchemas\n{\n    struct SchemaInfo\n    {\n        const char fbNamespace;\n        const char fbTypename;\n        const unsigned char bfbs;\n        const unsigned char bfbsEndPtr;\n    };\nsize_t GetInfo(const SchemaInfo **pSchemaInfo);\n\n}\nendif // FLATBUFFER_SCHEMAS_H\nCompilation unit (cpp file):\ninclude \"FlatBufferSchemas.h\"\ninclude \ninclude \nnamespace FlatBufferSchemas\n{\n    static const SchemaInfo schemaInfo[] = {\n        { \"MyGame::Sample\", \"Monster\", MyGame::Sample::GetMonsterBinarySchema(), MyGame::Sample::GetMonsterBinarySchema(true) } ,\n        { \"MyGame::Example2\", \"Monster\", MyGame::Example2::GetMonsterBinarySchema(), MyGame::Example2::GetMonsterBinarySchema(true) }\n    };\nsize_t GetInfo(const SchemaInfo **ppSchemaInfo)\n{\n    if (ppSchemaInfo) {\n        *ppSchemaInfo = schemaInfo;\n    }\n\n    return sizeof(schemaInfo) / sizeof(schemaInfo[0]);\n}\n\n}\n```\nExample using STL:\n```\nifndef FLATBUFFER_SCHEMAS_H\ndefine FLATBUFFER_SCHEMAS_H\ninclude \ninclude \nnamespace FlatBufferSchemas\n{\n    struct BFBSData\n    {\n        const unsigned char *bfbs;\n        size_t size;\n    };\ntypedef std::map<std::string, BFBSData> SchemaInfoMap;\nconst SchemaInfoMap &GetInfo();\n\n}\nendif // FLATBUFFER_SCHEMAS_H\nCompilation unit (cpp file):\nnamespace FlatBufferSchemas\n{\n    static const SchemaInfoMap schemaInfoMap = {\n        { \"MyGame::Sample::Monster\", \n            { MyGame::Sample::GetMonsterBinarySchema(), \n              MyGame::Sample::GetMonsterBinarySchema(true) - MyGame::Sample::GetMonsterBinarySchema() \n            } \n        } ,\n        { \"MyGame::Example2::Monster\", \n            { MyGame::Example2::GetMonsterBinarySchema(), \n              MyGame::Example2::GetMonsterBinarySchema(true) - MyGame::Example2::GetMonsterBinarySchema() \n            }\n        }\n    };\nconst SchemaInfoMap &GetInfo() {\n    return schemaInfoMap;\n}\n\n}\n```\n. Yes, I think we are all in agreement that embedding the bfbs schema into the code can be really useful! \nNow to the outstanding question, how to do it most cleanly?\n\nI found generating into the auto-generated code was going to fit most cases the best. It made it flexible to control where to instantiate the data. Having this in the header doesn't bloat any code and it should only be used if you want to embed it. \nThis could also fit well if implementation of a reflection API is made on other languages (java, .net, etc)  and need the schema data. Makes it really easy to embed the data. Not extra utility has to be made.\n\nSo what shall we do here, does this have a future? It needs to be a simple solution that doesn't have to be tweaked.... @aardappel (and @vglavnyy)\nYou can see that we are in agreement how useful embedding the bfbs can be. We are just not on the same page where to put this. I agree with you that having the encoded data in the header make this really flexible. It doesn't add the complexity of generating a compilation unit (cpp file) and the user is free to decide where the code should be instantiated. I think it also follows the pattern how code is generated from flatc. \nIt's not much code added to flatc (not to dirty). One encoding function and a new command line option. It give the user another option how to work with the bfbs and the management a little more flexible.\nFor my use case, it solves a huge amount of work managing the bfbs as an external resource. There is no change required to my build system and there is no post processing needed. . Having a extra header file for just the data is a great idea. \nWhat naming convention should we use for the new header?  How about something like this to follow the existing pattern: \nsamples/monster_bfbs_generated.h\nDo you want me to update this pull request once we are can agree on the header filename?. @aardappel When you say you don't want this main stream and want to test it out, how do you envision this? Shall we make it a hidden command line option (not showing up in the usage help)? \nAlso let me know about the header filename to produce so I can start making the changes.... Are we trying to add the \"ifdef\" back? Wasn't it removed by id=929847?\nWhy don't we remove all the rest of the \"ifdef FLATBUFFERS_TRACK_VERIFIER_BUFFER_SIZE\" like in #5162 . Makes sense, I just wanted you to have the option to commit this if you wanted since there isn't a test case in covering this issue. . Don't know how from this clunky web interface! Why is it so hard to find and manage your commits?. ",
    "voznesenskym": "I found the issue :) I was calling fbb.Finish() in my serialize step. The solution is:\nflatbuffers::Offset<models::Node> Cache::Node::serialize() {\n    models::NodeBuilder builder = models::NodeBuilder({fbb});\n    builder.add_key(flatBufferKey);\n    if (next != nullptr) {\n        builder.add_next(next->flatBufferKey);\n    }\n    builder.add_size(size);\n    flatbuffers::Offset<models::Node> node = builder.Finish();\n    return node;\n}\nCalled from:\n```\n    std::vector> serializedNodes;\nfor (auto it = nodeMap.begin(); it != nodeMap.end(); ++it ) {\n    flatbuffers::Offset<models::Node> node = it->second->serialize();\n    serializedNodes.push_back(node);\n}\n\nauto vectorOffset = fbb.CreateVector(serializedNodes);\n\nmodels::NodesBuilder builder = models::NodesBuilder(fbb);\nbuilder.add_nodes(vectorOffset);\nflatbuffers::Offset<models::Nodes> finishedOffset = builder.Finish();\n\nfbb.Finish(finishedOffset);\n\n```\n. ",
    "ZacLiveEarth": "The existing C# source already has the generated structs implementing an interface, and using a generic constraint to access the properties of that interface. See for example Table.cs line 98 I need to verify, but the .net jitter would have enough information here to forego using a vtable.\nAny additional consequences would be opt-in. A partial struct is as good as a struct when not extended. If the programmer adds an interface, and calls generic methods with type constraints it should still be as good as a struct. A common mistake might be for a programmer to accept the interface as an argument to a function, rather than accepting a generic parameter with the type constraint. In that situation there would be a performance penalty because the struct would be boxed. I would hope that the Venn diagram of programmers who understand how to extend partial generated code would have a lot of overlap with programmers who would avoid the performance penalty associated with boxing.. I started on the PR and made an interesting discovery. There's already an undocumented attribute \"csharp_partial\" that when applied to a table or struct will make the resulting generated C# struct partial. This functionality already exists.\nTo close this issue, I see two roads forward:\nOption A is to simply document the attribute so that it will show up in the list of attributes on this page.\nOption B is to remove this attribute and make all generated structs partial like we had originally decided. I prefer this option because partial is strictly dominant to non-partial, and is idiomatic for generated code. Partial just means that the definition of the struct can span multiple files and has zero effect on the runtime. (Sorry if you already know this, I'm being pedantic because I'm not sure how familiar you are with C#).\nWith option B there is another decision to be made. If the attribute is removed, there is a backward compatibility concern that schemas already using the attribute will complain when compiled that a user defined attribute must be declared before usage. Because the attribute was undocumented, this should effect only a small percentage of users. The other option is to leave it in place, but deprecate and document the attribute, ignoring it's value and always generating partial structs.\nPlease advise on how you would like me to proceed.. I'll give some time for @evolutional to say why making C# structs partial could be a breaking change. I can't conceive of any reason, but I'm often wrong.. It should be efficient and allocation free when used appropriately.\nI'll get started as soon as my employer gives the thumbs up to contribute.. As for the streaming partial  FlatBuffer, the new equivalent to the ByteBuffer class would check both Stream.CanRead and Stream.CanSeek in the constructor, if either is false an InvalidArgumentException would be thrown. As long as the underlying stream supports those (both RecyclableMemoryStream and FileStream do, and streams from a network should not) then this is a non-issue.\nRecyclableMemoryStream does not expose it's byte[]s. Even if it did, that would add a lot of unnecessary complexity to the flatbuffer C# code. (Consider for example that a read may span multiple byte[]s) Another downside is that the implementation would be tied to RecyclableMemoryStream, which would not support other scenarios like the file read scenario or scenarios that we have not yet thought of.\nGoing through the Stream interface would definitely have overhead, I agree. It would be up to the programmer to decide/profile which kind of overhead is the dominating factor. Sometimes the overhead of requiring that a large sequential byte[] be allocated in memory may dominate the overhead of the virtual calls. Sometimes the overhead of reading more of the file than necessary may dominate the overhead of the virtual calls. In most cases, the overhead of the virtual calls is significant, and using ByteBuffer and a single byte[] is best. That's why ByteBuffer should be default, and using a Stream should be a compiler option that the programmer needs to carefully consider the tradeoffs in their scenario before enabling.\nTo be fair, I haven't profiled either yet. I know that using RecyclableMemoryStream was a big win for my scenario before using Flatbuffers. After converting my serialization format to Flatbuffers I'll be able to test whether this is still win for me or not, and based on the result make a recommendation. I do want to test how receptive you are to the idea here though before going through the pain of implementing this, because it's a fairly large change and I won't know the results until after doing it.. A stream which does not support seeking (eg: NetworkStream) will throw an InvalidArgumentException when the flatbuffer code is being set up, even before GetAsRootObject can be called. So the scenario where flatbuffers tries to read a byte that hasn't been streamed in yet is not possible.\nMy particular use case looks like this:\nStream bytes in from a socket\nWrite bytes to RecyclableMemoryStream as they come in\nWhen all bytes are finished streaming in, read data using flatbuffers.\nThe advantage of RecyclableMemoryStream in this case is that it's a pooled memory allocator. I'm about to repeat the process of streaming in bytes to process so the memory pool saves GC time over many runs.. #4159 I'll tackle this as well.\nSorry for creating a bunch of yet unresolved issues at once. My employer wants to review things before agreeing to the Contributor Agreement.. If there were an official Nuget package, I'd like to see some way to handle the existing #define symbols. Right now the C# compiler can define BYTEBUFFER_NO_BOUNDS_CHECK or UNSAFE_BYTEBUFFER as performance optimizations. The official Nuget package should have a way to enable these optimizations as well.\nI'm not familiar enough with Nuget to know what the idiomatic way to approach this would be. I just want to raise it as a concern that I'm interested in.. I generally agree with everything you said. Though, the proposed [ThreadStatic] attribute is a more lightweight alternative to TLS which I believe will yield better performance. (Indeed, TLS uses [ThreadStatic] under the hood).\nI have a working PR using [ThreadStatic] for the helper variables ready (again, waiting for my employer to develop an open source policy at this point before I can make any of  the PR's publicly available). The only concern I have is the same as yours, it comes down to whether the introduction of [ThreadStatic] would hinder single-threaded performance.\nTo answer that question, do we have a set of benchmarks that we can run? I couldn't find any.. ",
    "SimonCropp": "+1 for having an official nuget package. ",
    "petertiedemann": "Libraries that are not on www.nuget.org, basically do not exist from the point of view of your typical C# developer. Having to copy code from a subfolder of a repository in order to get the necessary dependencies to compile the auto-generated code is really weird.\nThe natural way to handle this with is first of to include a csproj file with the C# code in this repo (targeting netstandard preferably). As part of your normal CI ( i can see that you are using appveyor?), you then need to do dotnet build+pack to get a nuget you can then publish to nuget.org. Normally that would be done as part of normal CI on a release build. It would of course also be nice to include flatc.exe in the nuget.\nDoesn't google already have a nuget.org account that can be used to publish?\n. As an example of what i mean, i updated the csproj here:\nhttps://github.com/petertiedemann/flatbuffers/pull/1/files\nI am not familiar with your build system however. Would you be doing the building and publishing on AppVeyor? And how do you normally handle incrementing versions? It looks like its manually updated in for example pom.xml and package.json?. ",
    "kwaclaw": "FYI - I just forked FlatBuffers and converted the C# project to the new .NET core format, adding support for netstandard 1.1. This is published as a Nuget package: https://www.nuget.org/packages/KdSoft.FlatBuffers/\nNote: I changed the C# namespace from FlatBuffers to Google.FlatBuffers (similar to how it is with protocol buffers). This means you have to use the modified flatc.exe code generator included in the package.. ",
    "ettoretorti": "Is there a way to explicitly pass a type that isn't declared as the root type within a file? Maybe through a command line argument? My use case involves more than one \"root\" depending on the circumstances.. ",
    "MizukiSonoko": "Ok. 1. Sorry, update grpc and compile successful !\n\nI wrote \n// auto sampleRootT = std::move(*request->GetRoot()->UnPack()); \nauto sampleRootT = UnPackSampleRoot(request);\nBut\n==18640==\n==18640==\n==18640== Process terminating with default action of signal 11 (SIGSEGV)\n==18640==  Access not within mapped region at address 0x101A4FE0\n==18640==    at 0x410F1E: int flatbuffers::ReadScalar<int>(void const*) (in /tmp/sandbox/grpctest)\n==18640==    by 0x40D4AC: flatbuffers::Table::GetVTable() const (in /tmp/sandbox/grpctest)\n==18640==    by 0x40D4D9: flatbuffers::Table::GetOptionalFieldOffset(unsigned short) const (in /tmp/sandbox/grpctest)\n==18640==    by 0x413701: flatbuffers::Vector<flatbuffers::Offset<sample::BaseObject> > const* flatbuffers::Table::GetPointer<flatbuffers::Vector<flatbuffers::Offset<sample::BaseObject> > const*>(unsigned short) (in /tmp/sandbox/grpctest)\n==18640==    by 0x411613: flatbuffers::Vector<flatbuffers::Offset<sample::BaseObject> > const* flatbuffers::Table::GetPointer<flatbuffers::Vector<flatbuffers::Offset<sample::BaseObject> > const*>(unsigned short) const (in /tmp/sandbox/grpctest)\n==18640==    by 0x40E568: sample::SampleRoot::baseObjects() const (in /tmp/sandbox/grpctest)\n==18640==    by 0x40F507: sample::SampleRoot::UnPackTo(sample::SampleRootT*, std::function<void (void**, unsigned long)> const*) const (in /tmp/sandbox/grpctest)\n==18640==    by 0x40F4CC: sample::SampleRoot::UnPack(std::function<void (void**, unsigned long)> const*) const (in /tmp/sandbox/grpctest)\n==18640==    by 0x40FF2E: sample::UnPackSampleRoot(void const*, std::function<void (void**, unsigned long)> const*) (in /tmp/sandbox/grpctest)\n==18640==    by 0x40FFEB: ServiceImpl::Endpoint(grpc::ServerContext*, flatbuffers::BufferRef<sample::SampleRoot> const*, flatbuffers::BufferRef<sample::Response>*) (in /tmp/sandbox/grpctest)\n==18640==    by 0x40896E: grpc::Status std::_Mem_fn_base<grpc::Status (sample::SampleService::Service::*)(grpc::ServerContext*, flatbuffers::BufferRef<sample::SampleRoot> const*, flatbuffers::BufferRef<sample::Response>*), true>::operator()<grpc::ServerContext*, flatbuffers::BufferRef<sample::SampleRoot> const*, flatbuffers::BufferRef<sample::Response>*, void>(sample::SampleService::Service*, grpc::ServerContext*&&, flatbuffers::BufferRef<sample::SampleRoot> const*&&, flatbuffers::BufferRef<sample::Response>*&&) const (in /tmp/sandbox/grpctest)\n==18640==    by 0x407ED6: std::_Function_handler<grpc::Status (sample::SampleService::Service*, grpc::ServerContext*, flatbuffers::BufferRef<sample::SampleRoot> const*, flatbuffers::BufferRef<sample::Response>*), std::_Mem_fn<grpc::Status (sample::SampleService::Service::*)(grpc::ServerContext*, flatbuffers::BufferRef<sample::SampleRoot> const*, flatbuffers::BufferRef<sample::Response>*)> >::_M_invoke(std::_Any_data const&, sample::SampleService::Service*&&, grpc::ServerContext*&&, flatbuffers::BufferRef<sample::SampleRoot> const*&&, flatbuffers::BufferRef<sample::Response>*&&) (in /tmp/sandbox/grpctest)\n==18640==  If you believe this happened as a result of a stack\n==18640==  overflow in your program's main thread (unlikely but\n==18640==  possible), you can try to increase the size of the\n==18640==  main thread stack using the --main-stacksize= flag.\n==18640==  The main thread stack size used in this run was 8388608.\n==18640==\n==18640== HEAP SUMMARY:\n==18640==     in use at exit: 4,597,615 bytes in 262 blocks\n==18640==   total heap usage: 582 allocs, 320 frees, 4,641,389 bytes allocated\n==18640==\n==18640== LEAK SUMMARY:\n==18640==    definitely lost: 0 bytes in 0 blocks\n==18640==    indirectly lost: 0 bytes in 0 blocks\n==18640==      possibly lost: 788,133 bytes in 9 blocks\n==18640==    still reachable: 3,809,482 bytes in 253 blocks\n==18640==         suppressed: 0 bytes in 0 blocks\n==18640== Rerun with --leak-check=full to see details of leaked memory\nSorry... \n\nI think some approach.\nFirst, I have question, Why ***Union is not extend \"flatbuffers::NativeTable\"?\nI think table's type is flatbuffers::NativeTable, so I should write this.\nstruct ObjectUnion: public flatbuffers::NativeTable\nIf above approach is no problem, I will rewrite this  https://github.com/google/flatbuffers/blob/2df3d1c9654ab0ddd8c58be51609dbad86109b15/src/idl_gen_cpp.cpp#L649 and Pull request.\nAnd \n==15195==\n==15195== Invalid free() / delete / delete[] / realloc()\n==15195==    at 0x4C2F24B: operator delete(void*) (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==15195==    by 0x40F226: sample::ObjectUnion::Reset() (in /tmp/sandbox/grpctest)\n==15195==    by 0x40D4B9: sample::ObjectUnion::~ObjectUnion() (in /tmp/sandbox/gr\nIt's cause by double free.\nSo I think I should write it\ncpp\ninline void ObjectUnion::Reset() {\n  if(table != nullptr){ // Add check \n    switch (type) {\n      case Object_Object1: {\n        auto ptr = reinterpret_cast<Object1T *>(table);\n        delete ptr;\n        break;\n      }\n      case Object_Object2: {\n        auto ptr = reinterpret_cast<Object2T *>(table);\n        delete ptr;\n        break;\n      }\n      case Object_Object3: {\n        auto ptr = reinterpret_cast<Object3T *>(table);\n        delete ptr;\n        break;\n      }\n      default: break;\n    }\n  }\n  table = nullptr;\n  type = Object_NONE;\n}\nIn now, there is memory leak. \n==18532== 56 bytes in 1 blocks are definitely lost in loss record 3 of 8\n==18532==    at 0x4C2E0EF: operator new(unsigned long) (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==18532==    by 0x412532: void sample::WrapperUnion::Set<sample::Wrapper1T>(sample::Wrapper1T&&) (in /tmp/sandbox/grpctest)\n==18532==    by 0x40BB1D: main (in /tmp/sandbox/grpctest)\n==18532== LEAK SUMMARY:\n==18532==    definitely lost: 324 bytes in 4 blocks\n==18532==    indirectly lost: 128 bytes in 2 blocks\n==18532==      possibly lost: 0 bytes in 0 blocks\n==18532==    still reachable: 72,728 bytes in 2 blocks\nI'm fixing now.\nthanks.. I refactored. but I cannot use English well, so Could you comment for comments in code?.. Ok, we can reuse FlatBufferBuilder, but ClientContext is not.\nIt happens assert like this\nE0508 17:56:15.072436359     296 client_context.cc:97]       assertion failed: call_ == nullptr\nfrom https://github.com/grpc/grpc/blob/master/src/cpp/client/client_context.cc#L98.. ",
    "YorkShen": "Got it.\nWell, I am trying to switch from JSON to flatbuffer. In my json file, there is a field, like flexDirection:{\"row-reverse\"} . This field is actually an enum, but the value in the enum may contain -. Is there any way that I can define this field as an enum in flatbuffer.. Let me clear, the json data format can be tricky, and I can't find a way to represent the corresponding format in flatbuffer for the data format below.\n\nwidth:70\nwidth: \"70\"\nwidth: \"70px\"\n\nThough width: \"70px\" is the same as other formats in my program, it has to be supported for back-forward compatibility reason.\n\nThat define width as int in the fbs file will cause last case fail.\nThat define width as string in the fbs file will cause the first case fail.\n\nWhat's more, there will be width: \"70dp\" in the future, and I have to do some math to convert dp to px according to 1dp=3px or similar equation.\nAny suggestion to solve the problems above?. My mistake, the third format should bestyle :{\"width\":70}.\nI am still trying to go through the whole process and find out what I missed. Actually, it's more complicated than I think that switching from json to flatbuffer, especially considering ISVs and backforward compatibility. \nI will create some PRs when I figure out what the big picture is.. I am trying to find a way to solve the problem and got two possible solutions.\n extra : {\n   foo: \"Hello World\"\n   bar: \"extra\"\n }\n\n\n\nDefine extra as string in .fbs file, then modifying the idl_parser.cpp to parse data as a string even if it looks like an jsonObject. Finally, parse the string representation of  extra to jsonObjcet runtime.\n\n\nSupport FlexBuffers in flexBuffer json parser.\n\n\nIt seems to me the first approach is easy, but the performance may be bad. While The second solution is complicated, the performance might be better.\nAny advice?. Got it, thanks.. ",
    "lebdron": "\nUnPackSampleRoot should not be used on request, since it requires buffer as argument.\nMoving an object pointed by request->GetRoot()->UnPack() also does not seem like a good solution to me, since it introduces a memory leak by not freeing memory allocated in UnPack.\nThe solution that worked for me:\n```\nauto sampleRootT = request->GetRoot()->UnPack();\n\nassert(sampleRootT->wrapper.AsWrapper1() != nullptr);\nassert(sampleRootT->wrapper.AsWrapper1()->object.AsObject1() != nullptr);\nassert(sampleRootT->wrapper.AsWrapper1()->object.AsObject1()->text == \"object1_text\");\nassert(sampleRootT->wrapper.AsWrapper1()->object.AsObject1()->boolean);\ndelete sampleRootT;\n- Double free problem described in https://github.com/MizukiSonoko/test_flatbuffers/blob/master/README.md is caused by move constructor of union leaving the pointer in the old object. Hopefully this gdb log will describe it.\n108   wrapper1T.object.Set(std::move(object1T));\n(gdb) \n110   auto sampleRootT = SampleRootT();\n(gdb) p object1T\n$1 = { = {}, text = \"\", boolean = true}\n(gdb) p (Object1T)wrapper1T.object.table\n$3 = { = {}, text = \"object1_text\", boolean = true}\n(gdb)\n111   sampleRootT.wrapper.Set(std::move(wrapper1T));\n(gdb) \n113   flatbuffers::FlatBufferBuilder fbb;\n(gdb) p wrapper1T\n$4 = { = {}, object = {type = sample::Object_Object1, table = 0x636310}}\n(gdb) p (Wrapper1T)sampleRootT.wrapper.table\n$6 = { = {}, object = {type = sample::Object_Object1, table = 0x636310}}\n```\nAs you can see, object1T is moved correctly, while wrapper1T data is still present in old object.. I signed it!. ",
    "lexthang": "I see. \nI thought that the union itself can be sorta like an \"envelope\" type if what we want is a message with just the union, but it looks like we need to wrap it in a table first. \nThank you @aardappel . ",
    "tavi-cacina": "sorry, I'm not comfortable with the internals, yet. . I confused a bit the cause/effect connections. I have tested the JSON parsing/generation using custom root types. The namespace pollution causes the Parser::SetRootType to fail when the type name is not fully qualified. Calling GenerateText afterwards causes the assert(parser.root_struct_def_);  // call SetRootType(). Here is the complete test function, adapted from sample_text.cpp:\n```\nint namespace_pollution()\n{\n  std::string schemafile;\n  std::string jsonfile;\n  std::string jsongen;\nbool ok = flatbuffers::LoadFile(\"samples/monster.fbs\", false, &schemafile) &&\n    flatbuffers::LoadFile(\"samples/monsterdata.json\", false, &jsonfile);\n  if (!ok) {\n    printf(\"couldn't load files!\\n\");\n    return 1;\n  }\nflatbuffers::Parser parser;\n  const char *include_directories[] = { \"samples\", nullptr };\n  ok = parser.Parse(schemafile.c_str(), include_directories);\n  assert(ok);\n  size_t namespace_count = parser.namespaces_.size();\nok = parser.SetRootType(\"Monster\");\n  assert(ok);\nparser.Parse(jsonfile.c_str(), include_directories);\n  assert(ok);\n  assert(parser.namespaces_.size() == namespace_count + 1); // a new empty namespace was added\nok = parser.SetRootType(\"MyGame.Sample.Monster\"); // fully qualified works\n  assert(ok);\nok = parser.SetRootType(\"Monster\");\n  // Fails to find the type, because it searches in the last namespace. \n  // Parser::root_struct_def_ was now reset to null.\n  assert(!ok); \n// This would assert, because the Parser::root_struct_def_ is now null\n  //ok = GenerateText(parser, parser.builder_.GetBufferPointer(), &jsongen);\n  //assert(ok && jsongen == jsonfile);\nparser.Parse(jsonfile.c_str(), include_directories);\n  parser.Parse(jsonfile.c_str(), include_directories);\n  parser.Parse(jsonfile.c_str(), include_directories);\n// Now you have one more empty namespace for each Parse call.\n  assert(parser.namespaces_.size() == namespace_count + 4);\nreturn 0;\n}\n```\nI think the more robust way would be to not mix the schema parsing with data parsing. For ex. the Parser should have a method ParseJSON that would accept only JSON, would not add any namespace and ideally would write the output not in the same Parser::builder_, but would accept a parameter \"FlatBufferBuilder& dest_builder\".. ok. But how do you see the fact from the title, that calling parser.Parse repeatedly with json files will cause parser.namespaces_ to grow indefinitely?. ",
    "aaronhudon": "This was a significant fix for me - thanks!. I signed it!\nAaron\nOn Tue, Mar 14, 2017 at 9:13 AM, googlebot notifications@github.com wrote:\n\nThanks for your pull request. It looks like this may be your first\ncontribution to a Google open source project. Before we can look at your\npull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://cla.developers.google.com/\nhttps://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll\nverify. Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your\n   GitHub username or you're using a different email address. Check your\n   existing CLA data https://cla.developers.google.com/clas and verify\n   that your email is set on your git commits\n   https://help.github.com/articles/setting-your-email-in-git/.\nIf you signed the CLA as a corporation, please let us know the\n   company's name.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/4223#issuecomment-286472611,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AEM-G7nb8Haa34S_Feo7ktvMyBnOH8cYks5rlryggaJpZM4MczOi\n.\n. \n",
    "ekousp": "I signed it!. ",
    "Peterfly": "Sorry for the false alarm. Since the underlying type is fixed, the behavior is defined as long as the input is not over the range of the underlying type (the byte): http://en.cppreference.com/w/cpp/language/enum\nClosing this issue for now.. Actually, it'd be nice if we could generate \"scoped\" enums instead of the \"unscoped\" ones: http://en.cppreference.com/w/cpp/language/enum\nFor unscoped enums,\nenum Type {\n  Foo = 0, Bar = 1\n};\n...\n// Undefined.\nstatic_cast<Type>(2);\nBecause \"If the underlying type is not fixed, the range is all values possible for the smallest bit field large enough to hold all enumerators of the target enumeration\"\nFor scoped enums:\nenum class Type {\n  Foo = 0, Bar = 1\n};\n...\n// Integer value 2. Not equal to Foo or Bar.\nstatic_cast<Type>(2);\nBecause \"If the underlying type is fixed, the range is the range of the underlying type\", which is an int.\n. Nice. That is what I want. Why not make it a default option?. ",
    "pam2014": "Thanks. I did resolve my problem. I also realized that while the tutorial states that during nesting, the offsets should be taken into a temporary array, this step is not required for structs.. ",
    "tianyapiaozi": "I will give it a try.. @aardappel \nI manage to get the cpp code work. \nHowever, running sh generate_code.sh fails with Segmentation fault.\nThe reason is that I add a struct with attribute \"key\" to monster_test.fbs and Java/CSharp generator does not support struct with attribute \"key\". There is the same problem with HEAD.\nIt will work if I remove --java --csharp from generate_code.sh.\nShould I continue the PR or what?. @aardappel \nI create a PR #4245 , please have a look at it.\nThe key_field here is nullptr when it is a struct. That's why it segfaults.. ok. ok. Genius!. ",
    "dalevine": "I quite agree about the benefits of UTF-8. \nTreating wchar_t as short and casting between wchar_t and short ought to work.\nThanks. ",
    "stemann": "This has been tested (experimentally) on an Arduino Mega using a simple schema.\nCompilation issues were experienced with a schema in which the root table included a union of two tables.. ",
    "chronoxor": "Yes, I have very pedantic compilation rules (/W4 /WX for Visual Studio). I created a pull request with a fix for this issue - https://github.com/google/flatbuffers/pull/4210. Also increased warning level for Visual Studio build (/W4 /WX) - https://github.com/google/flatbuffers/pull/4214. Moved the warning disable pragma at the begin and at the end of the file!. Fixed this pedantic warning with a static_cast. Please check an updated fix!. This does not work. VS warns with the following message:\nwarning C4334: '<<': result of 32-bit shift implicitly converted to 64 bits (was 64-bit shift intended?)\nhttps://msdn.microsoft.com/en-us/library/ke55d167.aspx\nWe can change 1U to 1u64, but I believe this will cause problems with 32bit builds.  . Fixed. Fixed. Fixed. ",
    "motxx": "Oh, sorry. I missed the issue #4190. And now same PR has been created at #4207, so I'll close my PR.. Thank you for reviewing. I applied it to the codes.. ",
    "SlavSlavov": "The same behavior is observed if you add more than 254 union elements without initializing them. The bug will be observed for element 255 and above.. When I execute the following, the header_generated.h file is created with no errors and warnings.\nflatc -c --gen-object-api header.fbs\nI've created an IDL file with 260 union elements (see attached). flatc generates 1meg file, 32k lines but again with no errors and warnings. \nheader_large.txt\nforgot to mention the version I am using - 1.6.0. ",
    "zejal": "Quick question: is the uint8_t type for storing a union type a \"hard\" requirement please ? For example is changing it to uint16_t (and rebuild) safe ? It is likely I'd have cases where I'd have unions with more than 256 types (esp as member of a root type). Thanks.. Thanks, would a table containing 1 member per type be an alternative (only 1 is set) ? Is there also a 256 limit for members in a table ? Thanks a lot. Thanks a lot for replying. In what aspect will it be inefficient ? Performance or space (or ...) ? If I understood correctly the encoding: spacewise hundreds of fields with only 1 set should only make the vtable large, and given the vtable only contain uint16_t IIRC of 2 bytes each even with 1000 fields we have roughly 2K, not a big deal (and there is only 1 in the whole message). Data itself is only 1 offset (assuming all types are tables). Performancewise I could store the id of the field that's set (a \"which\" field). After it should be only a matter to lookup the vtable directly to that id (and check it is not 0 of course). This is a bit an adhoc use of encoding I must admit.\nThanks a lot.. IMO, 2K of unused data is not a big deal if the total size of FB buffer is in order of magnitude of 10's of megabytes or even 100's or gigabyte. If smaller yes I agree it's more of a problem. . Never mind, I just realised that \"root_type\" is somewhat reserved name. So I guess naming an union \"root\" is not valid. Perhaps worth adding a check or relax this ? Thanks a lot.. Would it be difficult to add ? I could maybe try to implement support for this in JSON, but what would be your guidelines for this please ? This is really a feature we'd like to use (for data visualisation).\nThanks. I'll have a look then. Just to confirm: this is not something you plan to add any time in the coming months ? Thanks. I agree, but if we rename a table how we can check that new schema conforms with older one. I see there is a handy --conform option in flatc that seems to perform such schema checks. I guess I can build my own checker using an attribute and tagging tables with it, and use schema reflection to read information.. My suggestion would be to add a new predefined attribute (I think there are already some existing) and modify the conformance algorithm to use this attribute on given type (if present only in that type) to match 2 types with different names. Default is matching by names as of now, and if attribute id is present matching by ids (if there are types with same id in both schemas, otherwise id is not used). This would make the conformance flexible and backward compatible with existing.. Yes that's possible but how the conformance algorithm would know how to pair types ? I mean trying all possible pairs of types would not be efficient I think.. Makes sense, I'll have a try. Thanks a lot.. Playing a bit with --conform flatc option I see that it ignores completely tables if they have different names.\nFor example: schema1.fbs\ntable T2\n{\n   t: double;\n   y: double;\n   y0: double;\n   z: double;\n}\nschema2.fbs\ntable T\n{\n   x: double;\n   y: double;\n   z: double;\n}\nThese sch\u00e9mas clearly not compatible but: ./flatc.exe --conform schema1.fbs schema2.fbs outputs no error. BTW when it does it would probably be nicer to not print out usage/help message IMO.. Not in the dummy examples I used, but in real FBS yes the tables would be root_type of schema.. Yes of course, with a schema, forgot to mention that. . Hello, sorry for jumping in this conversation, but I am not clear why we need the deprecated field. I mean if we have for example: table { c:int (id: 2); b:int (id: 1); } where a field has been removed entirely, should not we generate a vtable with entriies 0, 1 and 2 and entry 0 is always 0 offset (always empty). Is not this what does \"deprecated\" property ?\nOf course it's entirely possible I missed something.\nThanks a lot. I see thanks. Would it be safe then to simply put some \"placeholder\" with arbitrary type ? ie go from\ntable { a:int; b:int; c:int } to: table { deprecated0 : empty (deprecated); b:int; c:int } where \"empty\" is some type (an empty table for example). The question comes from the fact we might generate FlatBuffers sch\u00e9mas from our own idl format and we'd like to be able to remove fields complelely, at the cost of having explicit ids. So we'd lose name and type.. If I remember correctly there is already a facility to convert back and forth from JSON and FlatBuffer binary format ? Certainly from JSON to FB (use it routinely). The only thing that JSON output does not handle as far as I know is the eventual multiple references to same table in the structure.\nBest.\nDe : Hugo Maingonnatnotifications@github.com\nEnvoy\u00e9 le :jeudi 28 septembre 2017 22:33\n\u00c0 : google/flatbuffersflatbuffers@noreply.github.com\nCc : zejaljosephcanedo@hotmail.com; Mentionmention@noreply.github.com\nObjet :Re: [google/flatbuffers] Existential questions and insights (#4385)\nAlright I was just asking for an export to json as in some cases we need to give our user an human readable result (from a buffer) which would be easier to parse for them if it was strict json.\nJust giving you my use cases, don't feel forced to implement those features only because of me ;)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/google/flatbuffers/issues/4385#issuecomment-332955547, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AZSv1mlnSMjBgeYYMJJFkiwI-8yVx48mks5snAJ6gaJpZM4OZa2h.\n. Thanks for your quick reply. No, I am not using latest. I've downloaded the latest flatc binary from releases page. I will get the latest source from master and build. I've searched in issues but could not find anything related to that, that's why I've raised this one.\n. I confirm same issue with building the C++ files. If I do \ninclude \"C1_generated.hpp\"\ninclude \"C2_generated.hpp\"\nI get an error in C2_generated.hpp telling that C1 is unknown.\nI guess we might need to do a fwd declaration of C1 before #include \"C2_generated.hpp\" in \"C1_generated.hpp\" (and vice versa). Or manually do that fwd declaration before #including Cn_generated.hpp. The following builds for me\nstruct C1;\nstruct C2;\ninclude \"C1_generated.h\"\ninclude \"C2_generated.h\"\nint main()\n{\n    return 0;\n}\nAnother question is, does FlatBuffers encoding supports offsets in both ways (as there is a cycle possibly).\n. A question related to this use case is: does FlatBuffers encoding support cycles in data ? If I remember correctly the offset type is uoffset which is an unsigned type so it sounds diffcult to me that cycles are supported in FlatBuffers, but can be wrong.\nThe document says this:\nUnsigned means they can only point in one direction, which typically is forward (towards a higher memory location). Any backwards offsets will be explicitly marked as such.\nI am not clear what is meant by \"Any backwards offsets will be explicitly marked as such.\" ? How this is marked in the encoding please.\nThanks\n. I have a similar case with a union definition involved and this breaks flatc.\n// File foo.fbs:\ninclude \"union.fbs\";\ntable foo\n{\n   U: u;\n   T: t;\n}\n// File union.fbs\ninclude \"foo.fbs\";\ntable t\n{\n   I: int;\n}\nunion u\n{\n   foo: foo\n}\nflatc.exe -c foo.fbs runs all fine, but flatc.exe -c union.fbs complains with:\nerror: union.fbs(12, 0): error: type referenced but not defined (check namespace): u, originally at: foo.fbs:5\nIt seems that union u must be defined before it's used in table foo as member. BTW I see same error if I merge the 2 fbs files and I put union u definition after table foo definition.\nI need to run flatc.exe -c on both FBS files to get all generated code AFAIK. Is there any workaround for this issue? For example a way to generate all code only running flatc -c on foo.fbs ?\n. By the way, on a unrelated matter, I noticed that \"root_type\" seems no longer required in FB schema ?\nThanks. Perhaps related issue, but for unions (and probably enums, but have not tried) we need to declare them before they are used in schema. This is not the case for table types.\nFor example:\nstruct TestUnion { f: int; }\ntable test_object { object : object_classes; }\nunion object_classes { TestUnion }\nFails with ./flatc -c \nBut if object_classes was a table, for example:\nstruct TestUnion { f: int; }\ntable test_object { object : object_classes; }\ntable object_classes { u: TestUnion; }\nThis succeeds with ./flatc -c \nThanks. Thanks, I see. This is in context of automatic code generation of FlatBuffer schemas from some in house IDL format. We generate 1 FBS file per union, so fix is simply to leave this FBS file without any include and include required tables definitions FBS files after including the FBS file for the union in all other FBSes, not an issue I think. \n. I put a counter inside the FileExistsRaw function and it gets called 220K times, which is well above the number of FBS files I have (roughly 1,000). Would it be worth caching this check or reduce the calls number ?  I see that in case of N include files we actually have a N^2 / 2 algorithm because of DoParse recursivity (if I understood that function correctly).. Is \"name' the string value appearing in the include statement ? If so I wonder if there is not another problem with say:\ninclude \"foo.fbs\";\ninclude \"path/foo.fbs\"; \nand paths being \"path\". \nI think (not sure) in such case same foo.fbs file will have 2 entries in the included_files_ map and hence I think all definitions will be seen as duplicated.. Thanks for the explanation. I guess given the first reason you won't implement changes to remove this limitation ?\nRegarding smaller reason, does this mean the if I have:\nstruct s { i: int; } union u { s } the struct is accessed via an offset ? In such a case I think we have the same inefficiency as with scalar in this case. BTW, if that's case I think it contradicts a bit this statement from doc (http://google.github.io/flatbuffers/flatbuffers_internals.html):\nThey are always stored inline in their parent (a struct, table, or vector) for maximum compactness.\n. The scalar directly in unions limitation is not that problematic I agree, only a couple of 1 member helper structs need to be defined and used.\nThe other question was related to struct types. They can only contain scalars. Why is this restriction in place ? In some cases I have little helper tables grouping non scalar fields (like a string for example) but I know their structure won't change (at least very unlikely :-)) and IMO it would be nice to avoid the V-Table indirection for these.\n. The intention behind this question is small and simple helper tables for which it's no need to really have a V-Table. For example to implement an array of arrays or a matrix type. AFAIK we need a table with 1 member for such structures.\ntable int_array { data: [int]; } // will never have anything else than this member\ntable other { arr2 : [int_array]; }\n. Not a huge problem, but was wondering if things like array 2D or other helpers could be defined simpler. For example having a string in a union requires a table as the following raises a strange error:\nunion u { string }\nerror: type referenced but not defined (check namespace): string, originally at: ...\n. Did not know we could give a name to union members. I'll use that then. \nThanks a lot, this will help to remove some \"helpers\" I wrote to workaround the error with union u { string }. AFAIK as of version 1.7.0 structs can be member of unions but only in a subset of languages (C++ for example). But if I try to transform a JSON to binary using flatc -b I get the error: \nc:\\Archives\\FlatBuffers\\OmegaFBS\\test_obj_persistence.fbs(63): error: only tables can be union elements in the generated language: u\nI see this function in idl_parser.cpp:\nbool Parser::SupportsVectorOfUnions() const {\n  return opts.lang_to_generate != 0 &&\n         (opts.lang_to_generate & ~(IDLOptions::kCpp | IDLOptions::kJs |\n                                    IDLOptions::kTs | IDLOptions::kPhp)) == 0;\n}\nShould not kBinary added as well in the or list ? Or are there issues with encoding ?\nBTW, same remark might apply to kJson (for the --json option). \nI made the change in my local copy and tried a round trip JSON -> fb -> JSON and it looks fine, but do not know for sure if it's safe to add kBinary and kJson to the list above.\nThanks a lot.. I see. struct in unions are implemented in JSON parser (I guess it is because adding the kJSON and kBinary is fine with struct in union case from what I see) and also in binary encoding, so one issue is that currently the same SupportsVectorOfUnions() function is used for 2 cases which have different language support. I guess that's what you mean by \"the error is at least partially correct\" ?\n. How difficult would it be to implement support for vector of unions in JSON parser ? I am trying to understand the code logic for parser to see if I can maybe contribute changes for this but no much idea on where to have a look unfortunately. I also unsure what would be the JSON would be for vector of unions.\nThanks\n. Thanks a lot for this info.. I confirm that I get precision issues when doing round trips between JSON and binary FBs using flatc -b and --json. Would it make sense maybe to add a --precision option to make this configurable ? Normally for double we might need 17 as std::setprecision. . The option was a suggestion, I agree that most important is correctness for sure.. Please ignore, missed the \"fork\" step.. If I correctly understood when converting from FB to JSON the language is set to kJson and when doing inverse conversion language is kBinary. At least when invoking flatc with t/--json and -b/--binary command lines options. I'd expect both of these \"languages\" support struct as union member. Maybe the fix would be to add another function and replace SupportsVectorOfUnions in this check as in the end this has even nothing to do with vector of unions.\n. Which languages do not support structs as union members? I do not know that list unfortunately. Also what's the use case lang_to_generate == 0 ?. Sorry but it is not clear at all what the supposedly SupportsStructInUnion() (or other more ad\u00e9quate name) function might do I am afraid.. The current code is:\nC++\n    bool Parser::SupportsVectorOfUnions() const {\n      return opts.lang_to_generate != 0 &&\n             (opts.lang_to_generate & ~(IDLOptions::kCpp | IDLOptions::kJs |\n                                        IDLOptions::kTs | IDLOptions::kPhp |\n                                        IDLOptions::kJava | IDLOptions::kCSharp)) == 0;\n   }\nWhat is not clear is what's the list of languages which do not support structs in unions? What is not clear too is the case where opts.lang_to_generate == 0.. My knowledge in FlatBuffer is not sufficient to verify this. I am pretty sure that kBinary supports structs Inside unions because FlatBuffers supports that. For kJson it should also be the case, but not 100% sure. Where in the code this can checked please?. ns.top exists, or is the \"ns.top\" value incorrect in nested_flatbuffers attribute? Also in case it helps, using flatc -c seems to generate correct c++ code with a proper accessor to embedded table iwth correct C++ type.. Debugging inside flatc code it seems that logic is to consider \"ns.top\" (or \"tip\") in nested_flatbuffers attribute value as part of current namespace. In idl_parser.cpp line 785:\nauto nested_qualified_name =\n        current_namespace_->GetFullyQualifiedName(nested->constant);\n    field->nested_flatbuffer = LookupStruct(nested_qualified_name);\nnested_qualified_name equals to ns.header.ns.top which of course does not exists.\nWhen writing for example in schema:\nnamespace ns;\ntable top2 {\n    T: ns.top;\n}\nApparently this works fine. I see there is a call to Parser::LookupEnum() function to resolve name in parent namespaces. Would it make sense to also call Parser::LookupEnum() in the context of resolving nested_flatbuffer name too?\n. Adding the following at idl_parser.cpp line 785:\nfield->nested_flatbuffer = LookupStruct(nested_qualified_name);\n    // Search thru parent namespaces.\n    for (size_t components = current_namespace_->components.size();\n         components && !field->nested_flatbuffer; components--) {\n      field->nested_flatbuffer = LookupStruct(\n          current_namespace_->GetFullyQualifiedName(nested->constant, components - 1));\n    }\nseems to fix my problem. If that sounds correct to you I'll try to create a PR.. ",
    "hazelnusse": "Thanks for the rapid response!\nThe reason I noticed this was because I am trying to get bazel to build flatc.  Having a symlink in the zip archive that points to a file in a parent directory is causing issues with bazel's new_http_archive() function, I reported an issue here:\nhttps://github.com/bazelbuild/bazel/issues/2656\n. ",
    "athei": "Yep: https://github.com/google/flatbuffers/pull/4217. I signed it!. ",
    "flier": "In fact, the patch only changed the argument in the getter/setter function.\ngo\nfunc FieldAddType(builder *flatbuffers.Builder, type_ flatbuffers.UOffsetT) {\n    builder.PrependUOffsetTSlot(3, flatbuffers.UOffsetT(type_), 0)\n}\nThe field name will not be impacted, because it always use the Camel format.\nIf you really concern the collisions in future, we could trace the identities in structure.\n```go\nstatic std::string GoIdentity(const std::string& name, std::set& identities) {\n  std::string id = name;\nfor (size_t i=0; i<sizeof(g_golang_keywords)/sizeof(g_golang_keywords[0]); i++) {\n    if (id == g_golang_keywords[i]) {\n      id += \"_\";\n  while (identities.find(id) != identities.end()) {\n    id += \"_\";\n  }\n\n  break;\n}\n\n}\nidentities.insert(id);\nreturn MakeCamel(id, false);\n}\n``\nBut I doubt it is necessary :). Yes,Parserwill extract theknown_attributes_in constructor, but we need theIDLOptionsfor generator, it is a little strange to add a generating option to parser's known attributes. So, I prefer to pass theIDLOptions` to Generator itself.\nIf you think use Parser::known_attributes_ will be better, I can update the patch later.. got it, changed to use IDLOptions in Parser. . ",
    "njh0602": "I signed it!. I'm not sure, but the ToString() function seems to be a bit more specific. But I will try PR again.. Attaching inline keywords works correctly. Then I will remove the static to the global function, fix it with inline, and make a PR. I am very happy to be able to write my program flexbuffer again.. thanks review!\ni will follow the manager's decision about this PR.\nmy own test shows that the performance of std::string(c_str()) and std::string(c_str(), length()) were not significantly different. based on the following test results, performance was rather poor at 3%\nauto start = chrono::high_resolution_clock::now();\nfor ( int i = 0 ; i < 100000000 ; ++ i)\n{\n    auto str = \"abcd\";\n    string s(str);\n    // string s(str, strlen(str));\n}\nauto end = chrono::high_resolution_clock::now();\nauto interval = chrono::duration_cast<chrono::milliseconds>(end - start);\ncout << interval.count() << \" msec\" << endl;\ni tested 10 times using release mode. (the unit is msec.)\nstring s(str) : 895, 887, 893, 927, 906, 894, 887, 903, 903, 901 (avg: 899.6)\nstring s(str, strlen(str)) : 946, 928, 933, 927, 928, 911, 936, 910, 947, 922 (avg: 928.8)\ndo you have any problem with the test method?\n. ",
    "BAADGames": "This is indeed true as the buffer was incorrectly composed on client side and sent malformed. But shouldn't it be detected by Verify method? In this case it returned true, so I assumed that everything is fine and in worst case I would get just empty string (or garbage), but not nullptr. Now it requires me to always do double checking:\n\nCall Verify method and check return value\nDon't truly believe in Verify return code, check all possible variables for nullptr where needed\n\nThanks,. ",
    "jasonstubbs": "I've fixed the byte, short and *int bounds checking with jasonstubbs/flatbuffers@2adf24d58586e4bb2c3dbe947c4ded7ad9f1e669\nI'm not completely happy with the almost-duplicate CheckBitsFitUnsigned, but I couldn't see a better way to do it at a glance and aren't able to spend too much time on it. Suggestions welcome.\nAlso, is there any problem with using std::enable_if<> or std::is_unsigned<> ? I couldn't find any minimal compiler requirements anywhere.... std::numeric_limits was a better fit.\nhttps://github.com/google/flatbuffers/compare/master...jasonstubbs:work/parser_limits_checking\nIf the changes look okay, i'll clean up the change set and create a PR.. > < may have to be <= ? Same for >.\nThese are correct. It's a tad confusing because the test is for out-of-range rather than in-range. I've flattened the code a little further to hopefully make it clearer. Ensuring correctness here is actually the reason why I added the extra tests. :)\n\nKeep to the Google C++ coding standard, e.g. camelCase -> snake_case for variables.\n\nYep, sorry about that. Last minute addition where I was just coding by habit.\n\nThere was a check in there for 64 bits which is now gone. Does the new code work correctly with (un)signed 64bit? worth adding a check.\n\nI've added checks to make sure that the valid extremes are still accepted. For the int64_t case, the template expands to (always false) int64_t checks against int64_t extremes so I'd expect compilers to just optimize them out. Should I add an int64_t specialization to make it completely clear?. Mostly, but enough for me to be happy. The string->(u)int64_t conversions are still unchecked, but I guess I should probably have made that a separate issue.. I did have that when I first worked on it, but I didn't like the repeated val < min in order to have separate error messages.\nSee https://github.com/jasonstubbs/flatbuffers/blob/0789a9c2bb39a33bcf1b3259825391956ffeaa04/src/idl_parser.cpp#L91-L97\nChanging the error message to work in both cases would be fine to.\nSee https://github.com/jasonstubbs/flatbuffers/blob/fix/4227-alternate/src/idl_parser.cpp#L91-L93\nI'd really prefer to show the type (eg, ulong) rather than the numeric limit that is being exceeded, but I can't see a way to do that without making it even more convoluted.\nI'm happy to change it to whichever way you prefer.. Yeah, I was a bit surprised that there wasn't an \"expression is always false\" sort of warning.\nFor the 64bit types, I was leaving that for a separate PR - perhaps by somebody else. I noted on the #4227 that I'm not looking to fix that part at this stage.\nThe uint64_t doesn't hit this code at all, as there's a atot specialisation that essentially calls strtoull instead. Still the same issue with lack of error checking on the result, though.. ",
    "krojew": "I signed it!. I based this PR on previous work in https://github.com/google/flatbuffers/pull/4065 where TS and JS were merged in one file. This resulted in a lot of if (ts) statements, not to mention some other problems (exports for js were unneeded for ts; missing imports for ts, while they're unneeded for js; missing namespaced external dependencies for ts, etc.). That's why I decided to separate those two.. Ok, so should I merge them?. Thanks for all your comments. I'll add more changes as soon as possible.. I'm actually using generated TS files for my project (almost 100 .ts message files, with lots of includes/imports) and it works fine. TS is used with noImplicitThis, noUnusedParameters, noUnusedLocals, noImplicitAny and noImplicitReturns.. TS typings just landed in Definitely Typed (so they will be available in @types soon).. JavaScript tests seem broken - they use writeUint* methods, which do not exist in flatbuffers,js file, yet the test passes. Probably they are never used. TS shares the same code with JS and tries to call those methods, but the TS test fails since it's being caught by the compiler. Should those methods exist or is JS/TS generation broken?. @aardappel any suggestions how to approach this problem?. Fixed JS runtime. Note - TS tests will pass when https://github.com/DefinitelyTyped/DefinitelyTyped/pull/15615 gets merged (verified locally).. The difference in generated js is that it's a result of compiling generated ts to js. I think I shouldn't commit it, but it's not a problem, since it gets regenerated on each test invocation.\nAs for the special boolean case - mutable bool is being passed as a number to the buffer, and tsc complains about implicit conversion. The solution is to use an explicit one by prepending +.. You're right - Long is problematic. In my project, I solved it by simply not using numbers, where Long is expected, and passing Longs all along. And there's this hack:\nexport function numberToLong(value: Number): flatbuffers.Long {\n    let padded = value.toString(2).padStart(64, '0');\n    return flatbuffers.Long.create(parseInt(padded.substring(32), 2), parseInt(padded.substring(0, 32), 2));\n}\nNevertheless, this is a topic for another PR.. ok, done. Unfortunately, you need to be able to run npm since the test needs flatbuffers typings (and the latest version, which doesn't seem to be merged yet). Otherwise the compilation will fail.\nThe js file is backed up and restored by the script, so overwriting shouldn't be a problem. It was the fastest way to utilize existing JS test suite.. I can refactor the JS and TS tests to be more independent. Note that not using npm requires us to include flatbuffers typings in the source and manually update them every time those change. Is that really what we want?. When dealing with external JS libraries, TS doesn't know anything about what's inside. That's why it needs a helper declaration file, usually with a .d.ts extension, which tells the TS compiler what is really contained in a given JS lib. For flatbuffers, I've made such declaration, and it's available here: https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/types/flatbuffers/index.d.ts\nRetrieving those declarations is done via npm. Without them, the compiler will just throw an error each time it sees flatbuffers usage.. The standard place for typings is the @types repository which fetches files from DefinitelyTyped. Storing one locally here would mean synchronizing it with DT, so other people could get it the standard way. I think it's more practical to update it once in DT and make the test fetch it from @types. That's the typical workflow for TS projects.. It might be inconvenient to library authors, but it's quite convenient to the users - all that's needed to use flatbuffers in TS is: npm install flatbuffers @types/flatbuffers\nAs for the other points - I'll make another PR.. @liukun most of JS libs I've seen have typing in @types, which is really convenient for the users - both the lib and typings are kept in sync via npm, and there's no need to ping-pong between npm and library repo. In fact, this is the second time I actually see something not publishing its typings to @types.. This import is a leftover from a time where there was literally nothing TS-related for flatbuffers (from the first PR). You can disable it with a --no-fb-import flag.. --no-fb-import only has effect on generated TS, so JS is unchanged. JS tests pass successfully.. Now it breaks any strict TS usage. Can I somehow resubmit a fixed version after the revert? I'm not that familiar with git to try such thing.. Just took a look at your partial revert, and I see it only affects JS code. So no need for TS changes.. What is your flatc command?. True - nulls are only possible when creating vectors.. We can add some kind of helper which would take an array of structs and simply create them inline in a vector. Could be handy, like the corresponding cpp createDirect helpers.. This was already present in the generated JS code and I only added missing \";\" at the end :) Nevertheless the question remains open - do we allow null to be passed to vector constructors? If not, it seems there was a bug in the original JS. The answer also impacts https://github.com/google/flatbuffers/issues/4307 so I'll wait with any fixes for now.. I see a copied this check from an older TS PR. I say it can be removed.. If you have no objections, I'll removed it in the near future and it will also fix https://github.com/google/flatbuffers/issues/4307. Can you share the source .fbs?. Added PHP support.. Latest commit accidentally fixes #4302 and the corresponding bug for php (not sure if reported). Also fixes some php class access issues.. I got lost at the first sentence :). That's why he should use typings from @types/flatbuffers as with any other js library.. Hmm this looks strange. I'll need to investigate.. Yes, changing it will break stuff, that's why there's a flag not to add this import. Can't remember why it was added, tho.. From the TS point of view, I don't see any problems. TS code only assumes flatbuffers are available (via typings) and the resulting JS code will have it available using the user-chosen import method, whatever that will be.. Absolutely yes. Will take a look as soon as I have some time.. I figure gcc implemented the new aligned \"new\" operators, which flatbuffers doesn't use. Regarding suggested options:\n\nCompiler-specific flags, in my opinion, are not the best option. It would be great to have a generic solution.\nExplicitly calling the appropriate operator new, where available, looks the best.\nType-specific operators would take precedence and likely solve the issue, but this is not really needed, since we can make this happen in a generic way (point 2).\n\nI advise going with the second option, where we have c++17 with new operators, with fallback when no support is available. As for the platform which caused the error - it's a 32bit ARM with Debian.. Yes, but after the weekend.. I looked at the issue in detail and it turns out the compiler flag is needed after all, unless we want to make class-specific operators.. Can you provide a concrete example? I actually use imports in fbs files, and the resulting TS code has no errors (although I'm using a slightly outdated version).. I wasn't planning to, but I can when I get some free time.. Will the site update in the near future?. I don't think that's the case - if you trace the verification code of vectors of tables, you can see null elements being treated as valid. . Although I like the decision, I fear this might be a breaking change, if someone used this feature. I can't really think of any use of null elements, so maybe this is not a problem. . Ok then, let's make the verifier safer. I'm also not sure it verifies out of bounds element offsets. . Just checked and it's true - the verifier doesn't check element offset bounds. That's actually a security issue.. Added a PR. Can you take a look?. Ok, if it's safe, no point in merging.. Yes, I've been planning to fix that, but haven't had any time.. Because the docs have not been regenerated yet.. Looks like Travis died before the build even started. . Unfortunately, I can't. I just added the generated files to the existing c# tests to see if it builds correctly. Other than that it looks exactly as in other languages, so I suspect it works, but I have neither the knowledge nor the tools to write any real tests.. There seems to be a problem with Travis: \nAndroid NDK: INTERNAL ERROR: The armeabi ABI should have exactly one architecture definitions. Found: ''. Also AppVeyor faile because of lobster: src\\idl_gen_lobster.cpp(152): error C2143: syntax error : missing ',' before ':' . Anyone using conan will have the ability to run flatc from the built package, rather than poking in the build directory. . It's a schema error - you are using a TS keyword \"constructor\" as one of your fields. flatc currently doesn't filter out such keywords.. Such improvement should be done for every supported language, so maybe @aardappel knows if such thing is on the horizon.. Someone will then need to be responsible for keeping the two in sync. Experience shows this sooner or later fails. . So in effect you want to move it out of @types to here? Ok, is there a benefit to this? @types is the standard place for definitions and moving them out will force people to search and manually install them into each project. This is a big downside from using npm to simply install the file. . The downside would be for the projects which do not install flatbuffers from npm, yet want typings. If we're ok with that then we can move it here. This should then be synchronized with deprecating @types.. Therefore the question becomes - is it better to have it here (and simply state flatbuffers should be installed via npm) or to keep @types up to date. I'm fine either way, as long as we don't end up with two declarations. . appveyor fails on memory allocation: memory allocation of 2147483648 bytes failed. You mean .mk file? It doesn't contain any generators. . Any chance for merge?. Something is wrong on appveyor: memory allocation of 2147483648 bytes failed. Sure, I should have some time this week.. Those were based on the C++ implementation. I agree, \"short names\" flag looks like a good idea.. https://github.com/google/flatbuffers/blob/master/src/idl_gen_js_ts.cpp#L1103. Hmm, strangely I've never encountered this problem. Need to look into. . No explicit reason - I just added those to TS only.. Please don't write \"if\" bodies in the same line as the condition itself. It's impossible to insert a breakpoint there.. Description added.. No, since the whole feature is surrounded by ifdefs and those places were simply missing it. For consistency, only one method should be used. . It may, but the explanation why this approach isn't used is in my reply above. . This is a question to the original author. Use ifdefs, build always, or use known patterns to enable at runtime - let's choose one and stick with it, instead of introducing a mess. . That is possible, since this support in c# was experimental. As I said in the original PR - I didn't have a way to test it, so I don't consider it stable until proven otherwise.. @Aranir can you check if current master still has this issue?. Can anyone confirm c# version is fixed?. Java and c# share the same generator, so it might be fixed for both languages. . Yes, it would be ignored, but then there's no point in generating it in the first place. As the old saying goes - some work is more work than no work :). The issue here is that flatbuffers has a .js import but no .d.ts file. I have one locally, and there are more floating around on the web, but I don't know if I should include it in this PR. Probably so. Nevertheless, that's why the flatbuffers import is optional.. Yeah, having one of them is really a must. .d.ts seems more practical as you said. If nobody has any objections, I'll add the one I'm using.. On the other hand, it would be better to submit it to Definitely Typed.. This looks like a breaking change for everyone using local flatbuffers module.. Probably because of the differences in file paths and/or hashing algorithms in various std libraries.. Yes, that's why there's a condition for gcc above. . Technically no, but it's handy when someone wants to add flatbuffers git as a gradle dependency.. Without it, jitpack uses jdk 1.6 which is too old to build the code. Sure, we can remove it, but it doesn't hurt to keep it, and we would loose jitpack dependency possibility. . Hmm, I can try doing it on Linux and see what happens.. Actually, it might not be necessary if we change the version in pom.xml, although I don't know if we want to do that. We can try 1.8 and see what happens. . Those parameter comments in TS are quite useless and only add noise.. Please move \"continue\" to a new line.. Please don't put condition body in the same line as the condition itself, since it's hard to debug such code.. As above.. As above.. Still in the same line.. ",
    "fasterthanlime": "I just discovered this PR, when I was cleaning up #4065 myself!\nSince you got further than me, I submitted mine anyway for reference (you'll see I made some slightly different choices), I'm explaning my rationale here: #4244 . @aardappel would it be possible for you to take a look at this PR again? Still a bit if-happy but I believe it addresses most of the points in the original review of #4065 . > This PR looks pretty reasonable to me now. It appears to already have absorbed some changes from #4244. @fasterthanlime are you happy with the state of this PR?\nYep, I reviewed the code and @krojew was reactive in incorporating that feedback into the PR!\nIfs apart, maybe the last remaining point is testing - it'd be nice to at least check that the generated .ts validates with the typescript compiler, but that would require a full-blown node.js runtime on Travis & AppVeyor - does the current CI setup already support that?. > JavaScript tests seem broken - they use writeUint* methods, which do not exist in flatbuffers,js file, yet the test passes. Probably they are never used. TS shares the same code with JS and tries to call those methods, but the TS test fails since it's being caught by the compiler. Should those methods exist or is JS/TS generation broken?\nI think it's a bug in the flatbuffers JS runtime (it's lacking writeUint16 and writeUint32), here's a failing test case:\n```javascript\nvar flatbuffers = require('../js/flatbuffers').flatbuffers;\nvar MyGame = require('./monster_test_generated').MyGame;\nfunction main () {\n  var fbb = new flatbuffers.Builder(1);\n  MyGame.Example.Stat.startStat(fbb);\n  MyGame.Example.Stat.addCount(fbb, 10);\n  var ab = MyGame.Example.Stat.endStat(fbb);\ntestMutation(fbb.dataBuffer());\n}\nfunction testMutation (bb) {\n  var stat = MyGame.Example.Stat.getRootAsStat(bb);\n  // this line crashes\n  stat.mutate_count(20);\n}\nmain();\n```\n(drop in tests/, run with node failing.js).\nTest output:\n```\n$ node failing.js\nC:\\msys64\\home\\amwenger\\Dev\\flatbuffers\\tests\\monster_test_generated.js:521\n  this.bb.writeUint16(this.bb_pos + offset, value);\n          ^\nTypeError: this.bb.writeUint16 is not a function\n    at MyGame.Example.Stat.mutate_count (C:\\msys64\\home\\amwenger\\Dev\\flatbuffers\\tests\\monster_test_generated.js:521:11)\n    at testMutation (C:\\msys64\\home\\amwenger\\Dev\\flatbuffers\\tests\\failing.js:17:8)\n    at main (C:\\msys64\\home\\amwenger\\Dev\\flatbuffers\\tests\\failing.js:11:3)\n    at Object. (C:\\msys64\\home\\amwenger\\Dev\\flatbuffers\\tests\\failing.js:20:1)\n    at Module._compile (module.js:573:32)\n    at Object.Module._extensions..js (module.js:582:10)\n    at Module.load (module.js:490:32)\n    at tryModuleLoad (module.js:449:12)\n    at Function.Module._load (module.js:441:3)\n    at Module.runMain (module.js:607:10)\n```\nThere doesn't seem to be any significant difference between writeInt8 and writeUint8, presumably because numbers in JavaScript are actually floats, should the writeUint16 and writeUint32 methods look like their signed counterparts?. > This also makes some changes to the JS generated code, can you explain what that is for, and the consequences?\nAre you referring to this commit https://github.com/google/flatbuffers/pull/4232/commits/08697cf6fd01a5ae203ee9f215b65e99cc002a7f ? I don't know either.\nLooks good to me too :+1:. While we're in javascript territory: is there any way flatbuffers.Long could get a more convenient constructor?\nJavaScript numbers have 53 bits of integer precision, but bitwise operations (like &, <<, >>) convert the number to 32-bit first. I don't see a good way to split a 53-bit value into 32-bit low and high values - can anyone else?\nAt the moment I'm just passing 0 for high, which means I'm limited to 32-bit of precision.\nedit: This SO thread has some interesting info - since it's non-trivial, it might be good to ship directly within flatbuffers.{js,ts} instead of expecting users to come up with their own working version?. The problem with having it just be 'flatbuffers' is that it forces users to be mindful of which version their package.json points to.\nIf they generate flatbuffers glue with a flatc version that's more recent than whatever their package.json resolves to, they'll have errors. Same for older flatc version. As long as there isn't a clear version compatibility contract between flatbuffers.js and the generated code, I think the TypeScript generate should generate both flatbuffers.js and flatbuffers.d.ts so that it's always in sync with the flatc version that generated the bindings.. (Turns out I had already signed the Google CLA - I guess all that remains is asking @sampaioletti if he's okay with my PR being based on his?). Closing this PR since it was absorbed into #4232 (the fork+branch remains for any future reference). as per the google C++ styleguide, importedFiles => imported_files. Instead of passing a ts boolean, I recommend using the same approach as idl_gen_general and use an array of structs: here's one way to do it. If you opt for the JsLangParams array of structs, it's easier to have the third parameter be a const JsLangParams & and use lang.file_extension here. for non-typescript, wouldn't import_code be the empty string anyway? it seems like the surrounding if could be removed, making the += unconditional. This seems like a good candidate for a boolean in the JsLangParameters struct (explicit_exports for example, true for js, false for ts). since it's used here again, _generated has become a magic string - if changed in one place it'll break in the other, so it's a good candidate for a constant instead. importedFiles => imported_files. the original PR also has this import emit in a loop, but I wasn't able to figure out why: it seems to be that it only needs to be emitted once?\nalso, to make the code self-documenting, it might be worth it to assign it->find('.') == std::string::npos to a local bool variable that indicates what that condition is in fact testing. With JsLangParameters, these can be deduplicated, here's one way it can be done.. speaking of flatbuffers.ts, I don't see it anywhere in the PR, is that normal? am I missing something? is it also supposed to be generated by flatc?. I see. I suppose either shipping flatbuffers.ts, or a flatbuffers.d.ts is acceptable. The latter is probably superior since it would avoid code duplication between the javascript & the typescript versions?. I'm pretty sure appending an empty string in C++ is very close to \"no work\" :) Besides it's not in a loop or anything (happens at most once per module) so it's not performance critical (not that flatc is in the first place). I still suggest removing the condition to remove \"branching on type\" (see anti-if campaign).\nThat way, if a third javascript-like language is added, the condition doesn't have to change, import_code will contain what's needed!. > DefinitelyTyped\nor just a @types/flatbuffers package on npm, see https://www.npmjs.com/~types. ",
    "liukun": "@krojew Thanks for you work!\nHowever, IMHO @types is mainly for js libraries whose origin repo not easy to modify. A ts library typically includes type definitions itself. For example, if you try npm install @types/moment, you will get\nnpm WARN deprecated @types/moment@2.13.0: This is a stub types definition for Moment (https://github.com/moment/moment). Moment provides its own type definitions, so you don't need @types/moment installed!. ",
    "AGoblinKing": "Why does it attempt to import flatbuffers from './flatbuffers' which assumes a relative location, rather than just 'flatbuffers' which would allow us to module resolution?\nEx: npm install --save flatbuffers => import { flatbuffers } from 'flatbuffers';. I want it but just with 'flatbuffers' since I'm using node and webpack for module resolution.  I made a fork with the change but yeah I saw that it gets reverted when people attempt that pull request. I can dig into the reasons and get it working w/ the errant environments. :sadness:\nThe namespacing is super weird when using them as modules too since everything would be exported on whatever namespace you provide (and broken if you don't provide a namespace). \n```ts\nimport { Namespace } from './transform-generated';\nNamespace.Transform.etc..\nvsts\nimport { Transform } from './transform-generated';\n```\nEdit:\nAn alternative approach to generating TS would be generating the .d.ts files for the javascript version. This would give type information to javascript users in modern IDEs and allow typescript users to use them directly.. That'd definitely meets my needs. The only thing better would be bundling flatc with the npm module like http://casperjs.org/. It attempts to build the binary and falls back on grabbing it if that fails. \nEdit:\nWhat happens when you want to generate hierarchy of flatbuffers? Would there be a flatbuffers.js in each level then?. ",
    "maingoh": "It seems to have been fixed since last release on the master branch. Is there a release planned soon ?\n. Yes I am, no worries !. Thank you for your detailed answer ! Yes, we needed GRPC support for Python and C++ (which I saw has been reworked few weeks ago). \nHere are my answers/new questions:\nHigh level : \n\nThank you for the clarification, hope it will get adopted more and more internally :)\nI was wondering if there was a way to \"cast\" the buffer into a class/struct, but apparently the constructors of the STL types always copy the data (probably the same problem for other languages). Is it why it is inefficient ? If those constructors existed we would probably have something easy to use and efficient (like a wrapper around the buffer).\nI guess I was expecting other languages to improve as fast as C++, but I understand it is a lot of work :) \nGood news !\nThe DebugString method in protobuf doesn't required anything like loading the .proto (and the field names are printed). For debug purpose it is quite frustrating to have to load a .fbs file (you have to know were they are). I don't know if something more straightforward could be possible. What about generating the needed information inside the .h so that the DebugString function knows the schema ? And eventually make it optional.\nThank you for adding it to the documentation ! I saw in another issue that the reverse order was more an historical decision, probably a support for the normal order could be nice ? To avoid new users to focus on this.\n\nVersionning: \n\nSo If an old program that does not contain the required field in the .fbs and does not use the verifier, receives the new required field in the buffer it will just ignore it ? \nSame question, if an old program doesn't have the \"deprecated\" attribute on the required field and does not receive it because it has been deprecated in the new version ? I guess it will crash no ?\nI was expecting to completely remove the field from the .fbs file, not just renaming it. Or i guess we need to first deprecate it everywhere and then remove it ? What would be the best strategy ?\n\nPerformance:\n\nI was just looking for comparison, I didn't check much the benchmarks, and we don't use java so it is fine. I was just wondering why it is a slower than protobuf in some languages. Could it be more efficient with C/C++ binding library ? Like Cython/JNI\nIt seems negligible. What takes more memory ? The header for each field ? Aproximately how many bytes per fields ?\n\nThank you for taking notes of my insights, hope this get materialized soon :)\n. Thanks, everything is clear for me now.\nHope this issue gave you some areas of improvement ! I let you close it :). Wow, I didn't expect it to arrive as fast. Thanks for your great work! Also does it handle serialization to json? . Alright I was just asking for an export to json as in some cases we need to give our user an human readable result (from a buffer) which would be easier to parse for them if it was strict json.\nJust giving you my use cases, don't feel forced to implement those features only because of me ;)\n. I was following the previous PR. Note that bytes() makes a copy of the bytearray https://stackoverflow.com/questions/35880065/does-converting-from-bytearray-to-bytes-incur-a-copy\nIn order to avoid this, it would need a change on GRPC side which only allow bytes type (not bytearray) :/. ",
    "sgbench": "Good question! C# using directives outside of namespace blocks are global only in the context of that source file. So since the files generated by flatc don't contain any custom code written by the user, there should never be any clashes like the ones you described.. I found a better solution: Instead of moving the using directives outside of the namespace block, just change them from\nusing System;\nusing FlatBuffers;\nto\nusing global::System;\nusing global::FlatBuffers;\nThe global:: qualifiers cause the C# compiler to correctly resolve the directives to the System and FlatBuffers namespaces instead of possibly confusing them for part of the enclosing namespace (the user-provided namespace from the FlatBuffers schema).\nThis solution is better than the first one because:\n1) It's simpler - it only requires changing a single string in idl_gen_general.cpp.\n2) It minimizes the chances of the user unintentionally hiding types belonging to the System or FlatBuffers namespaces. (Under the first solution, the C# compiler searches every level of the enclosing namespace before searching the System and FlatBuffers namespaces. So if the user happens to have a class called Offset in any level of the enclosing namespace, the compiler will attempt to use that instead of FlatBuffers.Offset, which is not good.)\nI'll see if I can submit a pull request later tonight.. I signed the CLA.. Sorry, I forgot I needed to do that :). ",
    "Warchant": "I signed it!. Okay, it was stupid question.\nuint8_t* blob = (uint8_t*) asset;\nBut then, I have to write blob into file. For this I have to know its size. I would appreciate if somebody can point me to good solution.. ",
    "siebeneicher": "Thanks for your feedback, I dont have any other schemas. So most likely I wont try it :(. ",
    "ElliotRobot20": "aardappel, did you ever end up making that dumper? We have a lot of clients switching from protobuf to Flatbuffers and that would be extremely helpful even if it wasn\u2019t exact.. ",
    "Casamatechs": "I signed it!. I just have done a rebase including the missed instance variables. . ",
    "schoetbi": "I signed it!. Thank you for the hint. \nWhen I included the full auto-generated tag with both opening and closing braces I encountered that Visual Studio added the full namespace qualifier and this lead to StylecopAnalyzers or better Roslyn did not detect this file as being autogenerated.. I volunteer for  this PR. I should have something in the next few hours.. Ok, then I'll remove the extra clutter. It will help with the stylecopAnalyzers. What do you think about the GeneratedCodeAttribute?. I just tried only to put in the <autogenerated> without the closing </autogenerated>.\nResult:\n- Without closing: Resharper and also stylecopanalyzers report errors during editing and during compilation\n- With closing: Resharper and StylecopAnalyzers do not report any errors\n-> We need proper XML in the header!\nThe minimal comment that leads SA to recognize the file as being autogenerated is:\n// <autogenerated>\n// automatically generated by the FlatBuffers compiler, do not modify\n// </autogenerated>\nHaving this correct comment I do not need the extra attribute.\nTo generate the correct comment  I had to remove the extra newlines after \"automatically generated by the FlatBuffers compiler, do not modify\". In this step I put this string in a compile time constant to avoid the function call at every occurrence.. The function call with copying the string around seemed to me as a waste in contrast to an inlinable static string. Maybe you could explain a bit more why you think its worse? Performance, codesize, memory? I could remove it yes, but it will make the code in the generators worse because of the additional function call.\nThe reason for the change was that the additional newlines lead to an invalid comment so that the Stylecop rules don't get disabled. At least the version that Resharper is using.. Thanks for your explanation. I did not know this. I checked with godbolt but there is only one compile module. I'll revert that.. How do we continue here? Should I change the gen generator so that the java output stays the same? Or do we say \u201cIt won't hurt anybody\u201c?. For testing I tried this website:\nhttp://www.jsonschemavalidator.net/\nExample json-schema:\n```\n{\n  \"$schema\": \"http://json-schema.org/draft-04/schema#\",\n  \"definitions\": {\n    \"product\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"Id\": {\n          \"type\": \"integer\"\n        },\n        \"Name\": {\n          \"type\": \"string\"\n        },\n        \"Price\": {\n          \"type\": \"number\"\n        }\n      }\n    },\n    \"root\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"Products\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"$ref\": \"#/definitions/product\"\n          }\n        }\n      }\n    }\n  },\n\"$ref\" : \"#/definitions/root\"\n}\n```\nSample Data:\n{\n  \"Products\": [\n    {\n      \"Id\": 0,\n      \"Name\": \"Pen\",\n      \"Price\": 1.30\n    },\n    {\n      \"Id\": 1,\n      \"Name\": \"Rubber\",\n      \"Price\": 4.23\n    },\n    {\n      \"Id\": 2,\n      \"Name\": \"Ruler\",\n      \"Price\": 56.21\n    }\n  ]\n}. @aardappel I started a branch with a new generator. But now I need some input. \nIs the basic concept ok?\nHow should I test this? I could include a full fledged flatbuffer schema and generate a json schema out of it and verify the output.\nShould I create an PR right now while improving it or should I PR only as soon as the quality is ok?. This is merged -> Done. Was a pleasure.. Just found --unknown-json. The $schema keyword is used by all schemas to reference their schema. For example this schema card.json references http://json-schema.org/draft-04/schema#. I have not found other hints that any json document can reference their schema using this in the same way.\nSkipping only $schema would be good because with --unkown-json one would ignore all additional fields not just $schema.. yes, it changes the generated json schema file. The reason that there are no changes in the tests directory is, that no type in the monsters example references an enum type. I tried to generate the test-code again but saw only changes related to the change of the vector namespace.. Just as a sidenote to this. I was also looking for something similar for the usecase of a \"version\" inside the flatbuffer data. A Version is like a default value but when read via another build of the schema (on the other side) it has to be present inside the buffer. Otherwise a check of the default value would have no sense.\nI like the idea of a \"force\" attribute on a scalar field. It should behave so that the value is serialized into the buffer no matter if it is the default value or not.. @aardappel. Thank you for this hint. The documentation didn't mention this attribute. https://google.github.io/flatbuffers/md__schemas.html. Yes, that was my intend. Thank you for clarifying this. I initally thougt I could use __assign for this.. Thats true, I had two problem domains that had different output paths for the generated headers. Now I merged the two.. @KevinSomers The next couple of weeks I do not have time to look into it. Maybe later.. Close issue: Probably wrong compilation unit used.. @aardappel My usecase is a double value that might have a real value but sometimes the value might be not present. In these cases I use double/float.Nan to denote that the value is not present. I could wrap this value inside a table to get the \"nullability\" but having a NaN is nice.\nAnother question is related here: What happens if in C# someone creates a flatbuffer with double.NaN inside and converts this to a json document? I guess inside the json document is \"-nan(ind)\". What happens when I convert this json document back to a flatbuffer? . Should we also care about Double/Float.NegativeInfinity?. I took the output of xsd.exe as a sample see here.\nhttps://github.com/samadhicsec/fuzzware/blob/4b516b3d2a9be5000baffee4f3efbdaf886e3c04/Schemas/AutoGenerated/Configuration.cs. Right in case of the Rosly project it might be correct. But there are other tools out there.\nSo I was thinking that the output of another Microsoft code generator (xsd.exe) will lead to the highest compatibility.\nThe main difference is that in the xsd output there is valid xml in the output.. Should we also add the GeneratedCodeAttribute to the generated classes?\nBut then the problem would be what version to put in. Does Flatbuffers store a version somewhere?. I Just found how the xsd.exe comment is generated. It is in system.dll\nSo it will be used by all tools that generate source with the class CSharpCodeGenerator. I thought it would but you are right it gives no benefits. See here: http://quick-bench.com/Xh4YAery7pHuO_AIg_ElnqTc3RU\nI will remove them. I planned to remove as soon as everything is done. Any thoughts on the enum with bit_flag problem? What is expected to be in the json. The name of the enum (=string) or a number? String is only possilbe when one flag is true so I assume that a number should be the right thing?. Was auto in vs2010? I do not have a copy installed.. It should never go there. So I removed the cerr part and return \"\". I have only found a specification on how to reference the schema for the actual schema from the schema file (what a sencence). Here Understanding-json. So it seems that only $schema is used. \nI have not found a specification how ordinary json documents reference its schema.\nBut you are  right normal compare should be fine.. Should this be float in the comment?. ",
    "korman": "ok,thx. I signed. kormanwork@gmail.com. no,I use same email address signed the CLA,Please check your system again.. ",
    "Lucifer11111": "gogogo. ",
    "dim-an": "About pull request:\nflatbuffers compilation fails when trying to compile with C++ stdlib that is not part of linux distro (actually I'm not sure why it works with default stdlib).\nAbout signature:\nI'm a little bit lazy to read and sign all that stuff, but I don't mind if someone from core team will commit this simple diff from their name, I just want this change to be commited :)\nSo please tell me if you will commit it or you ask me to read and sign agreement (or if you don't want to accept such change).\nThank you! . ",
    "amodj": "Used the incorrect email address for the git commit. Sending a PR with the correct email.. Thanks for taking a look at the PR. \nJust to add some context, I'm migrating a service from using protobufs to using flatbuffers. Having these functions essentially makes it easy to migrate the clients since their interface to look into the messages doesn't change. We also have a requirement where it is possible we get either protobuf messages or flatbuffer messages (during upgrades). I could just templatize those functions in the client, instead of maintaining two separate copies.\nI could write wrapper objects that wrap around the flatbuffer and provide these convenience methods. However, I would need to write wrappers by hand for each of the messages. \nI agree with the first point you make; although we always force defaults in the messages. But, I did not quite understand the point about name clashes. \nIf you feel this is not worth getting into the mainline, please feel free to close the PR.\nThanks,\nAmod\n. ",
    "cempayzin": "It looks like since FlatBuffer 1.4.0 release there is a bug on flatbuffers.h for C++ causing raw buffer check using Type specific Verify function (which calls VerifyBufferFromStart) to fail. since flatbuffers version 1.4.0 this is the function defined as:\ntemplate bool VerifyBufferFromStart(const char identifier,\n                                                  const uint8_t start) {\n    if (identifier &&\n        (size_t(end_ - start) < 2 * sizeof(flatbuffers::uoffset_t) ||\n         !BufferHasIdentifier(start, identifier))) {\n      return false;\n    }\n// Call T::Verify, which must be in the generated code for this type.\nreturn Verify<uoffset_t>(start) &&\n  reinterpret_cast<const T *>(start + ReadScalar<uoffset_t>(start))->\n    Verify(*this)\n    #ifdef FLATBUFFERS_TRACK_VERIFIER_BUFFER_SIZE\n      && GetComputedSize()\n    #endif\n        ;\n\n}\nit looks like verify function expects Bufferidentifier to be inserted infront of the raw buffer if it is defined. however it has not been implemented anywhere to encode raw buffer with bufferidentifier. \nIf Buffer has an identifier not defined all works fine. however if a buffer identifier defined verifying raw data fails. \nI have checked flatbuffers 1.3.0 version which does not have the same code on verify function.\nPlease let me know if I am missing something. . This is not really true. I have created another simple buffer with only one field (int) with value 1 and I don't see identifier inserted anywhere it is 20 bytes long. i use flatbufferbuilder and special build function created by flatc.exe per schema. Also I have debugged source code of flatbuffers with my project and I don't catch a break point on static function which returns identifier when buffer is being created at all. \nAt this point i am pretty sure somewhere in the code inserting identifier is being skipped. If you can point me the correct function to check I will make a test on my project to confirm.. I understand what you are saying however there is no way to know if identifier is inserted or not if you are on the receiving end of the buffer either as a binary file or over the socket. It looks like you have the option to insert the identifier or not depending on your function call on C++. However there is no way to optionally define verify buffer to check the identifier. Also in our implementation, it happens that buffer is being created from an application running on java and using java. Only information I have is the fbs file which buffer is based on. With this information verify fails for all incoming buffer since fbs defines identifier and cpp structure which flatc creates also have it. In this case Verify function ALWAYS checks identifier assuming that if fbs defines it shoud; be there. However based on your recommendation it is totally optional. So I am not sure which part must be changed to keep implementations consistent. Either finish function must have include if one defined (which makes sense) or verify function must also have an option to not check identifier (which is not a good option since it can still fail if implementer has way to define this requirement on fbs).\nWhat do you think ?\n. I totally understand how the buffer should be created. I understand if FBS defines an identifier it should be inserted. Also we already modified Java portion to insert identifier for every single buffer creation. Everything works now. However I still believe there is a conflict on buffer creation and verification process. \nFBS defines a buffer. It is the way of telling flatc to create correct implementation of schema on giving language. Identifier is a part of FBS. Once it is defined it should be added to buffer by the created object since it is defined. We can always add additional requirement over fbs asking implementors to call correct function to add identifier. All is good however not ideal. I think it is preferred to add this functionality to created object so it adds functionality if identifier is defined, so there should be no confusion. \nNow the real conflict happens on verification. As I explained verification always checks identifier if it is defined on fbs. this is where all problem starts. \nI am not sure why this is an issue being discussed. I already understand the solution and implemented. However what I did not understand that why we have field which should be added not being added unless implementor takes action. As far as I understood; schema defines the buffer not implementation.  \nAnyways we have our solution. I hope somebody else does not have to spent time on this again.\n . ",
    "saeidfarivar": "thanks @aardappel , would you be able to give an ETA on the fix too?. ",
    "lvc": "The tool can filter out symbols from the analysis by a regexp. I can rerun the analysis if you'll provide one.\nThank you.. ",
    "arnetheduck": "As a corollary to this feature request, it would be even more convenient if the generators were able to include the schema in the generated .h file as a constant - such that not even the .bfbs needs to be distributed with the application! Sure - this can be done by the build system and some conversion tool, but... My use case is actually the other way around - I want to log flatbuffer messages that I receive in a custom, non-JSON format, but the means would be the same ;)\nIf the .bfbs were available as constants, I would likely not use the Parser class at all, but rather go to the reflection helpers - these seem more natural - but even here, depending on the use case, it's likely that one will want to build some special indices or maps for the specific task at hand, so if I'd had a choice, I wouldn't make whatever flatc generates too \"smart\".\nThe nice thing, if the same format as .bfbs is used (reflection.fbs-format really), is that I can then reuse the same facilities when sending the data over the wire, if I want self-describing messages - it all comes together in a very reusable and broad way.. @aardappel Sure! Any such function will need the relection bfbs, and I'm merely pointing out that I prefer working with the reflection API rather than the Parser class, as I find it more generic and suited to this purpose. GetAnyValueS is a great place to start, and the output could be JSON (without instantiating Parser and friends) or maybe protobuf TextFormat (we use protobuf currently and want to transition some parts to flatbuffers instead - having the same text format is a small help in that process).\nThe reflection API works with reflection-style descriptors, and I'd like to be able to reuse the same logging (or other reflection-based stuff, like CopyTable and friends!) code for both these cases:\n\nI receive descriptor over wire together with message (self-describing messages)\nI have the descriptor in code, generated by the flatc compiler\n\nAnyway, I digress from this issue, which is about the Parser - should I open a new one?\n4178 looks related too.. uuh, it's just a bool init, and I've seen that I have to read and sign a CLA for the project to accept PR's. If I run across something substantial that makes it worth it, or if I can skip the CLA, I will for sure.\nThis one, I just randomly ran across while browsing the code in Eclipse whose code analyzer complained - they should have the credit, not me ;). Oops, you're right of course, missed the & when typing it in. I'm pretty sure the compiler can figure the unused return value out and drop it.\nI'll see if I find time to dig in enough to make a PR - I guess just changing the generator is trivial but there's tests and docs and other things that someone familiar with the code base could change in a breeze... > Can you make a PR\nNot right now, I'm afraid. \n\nno point in modifying any of the data\n\nThe table that contains the descriptor is not one that gets modified but it sits in the same file as some others - could of course split it out to a separate file but that's not too neat either.. ",
    "akamat-ibm": "BIG_ENDIAN is not defined for s390x and seems to be specific for MAC platform.\nFor s390x, we have BIG_ENDIAN defined. \nThe platform define has been added as travis failures were noticed. Also as the changes are verified for s390x & x86, the platform define has been retained.. Closing this PR. Will create a new request with changes to support s390x.. With flag __BIG_ENDIAN defined, build failure issues were seen on x86 & Travis build. \nHence, the platform define(__s390x) has been added.. By using #if defined(__BIG_ENDIAN__) || defined(__BIG_ENDIAN) code change, the following failure is seen on x86. \nflatc: /flatbuffers/include/flatbuffers/flatbuffers.h:171: void flatbuffers::EndianCheck(): Assertion `*reinterpret_cast(&endiantest) == FLATBUFFERS_LITTLEENDIAN' failed.\nIt seems on x86 __BIG_ENDIAN flag is recognised (even after setting __BYTE_ORDER to __LITTLE_ENDIAN) & sets define FLATBUFFERS_LITTLEENDIAN to '0'. As a result,  we see a failure in EndianCheck() function.\nHence, introduced the platform define to support s390x. Have added the flag above the existing ifdef so as to avoid breakage in the coding format.. ",
    "sharwell": "The minimal recommended form of this comment would be the following:\ncsharp\n// <auto-generated/>\nMost other C# code generators already include a comment with this form in the header comments. For greatest usability, the comment should always be included in the generated output without the user needed to manually configure it to be included.\nA quick search suggests the following may be the preferred way for this project:\ncsharp\n// <auto-generated>\n//   automatically generated by the FlatBuffers compiler, do not modify\n// </auto-generated>. > ... if this is the way the C# world works ...\nIt is. All Microsoft-produced code generating components during a build either already behave this way or have active bugs to correct deviations. Third party projects are increasingly adopting the approach as more projects are incorporating analyzers (part of the C# compiler's static analysis API), since analyzers are generally written to avoid reporting things like code style issues inside of what the compiler detects as generated code.\nI don't have time to implement the PR right now, but if someone asks me again in a couple days I'll figure out a time to.. @schoetbi I have no real opinion one way or the other for [GeneratedCode].. @aardappel Did you have a specific question?. There is no need for ---- in the header. The compiler is just looking for <autogenerated or <auto-generated in the first line comment (which can span multiple lines). The fact that code generators typically use a proper XML tag with a matching closing tag is just a convention.. See here and here for the compiler implementation, if you are curious.. > From the discussion elsewhere I was expecting the closing tag to either be at the end of the file (enclosing all generated code), or omitted entirely.\nThe \"XML\" is a file header. The content of the XML element is the message describing what/why the code is generated.\n:memo: I say \"XML\", because unlike /// comments these comments are not actually treated as XML by the language. The form seen here is just a de-facto convention that stuck over time and now everyone just uses it for consistency.. ",
    "xedrac": "I signed it!. ",
    "Zer0bee": "What if i convert both keys and values into separate arrays, because they have \"one to one\" relationship. \ni.e.\n{\n    \"keys\": [\"1\", \"2\",.......],\n    \"values\": [\"value1\", \"value2\",......]\n}\nNow, I can write schema for it. Can i take advantages of FlatBuffer now and Is it a good solution?. Yes, you are right, we can convert our json in key/value pair to take advantage of in-place binary search, as you shown above.  \nSo. I used a json something like this:\n[\n{\n    \"key\": 1,\n    \"value\": \"value1\"\n},\n........\n........\n]\nBut, after writing schema for this, i got an error \"expecting: table instead got: [\".\nSchema is as follows:\ntable DataArray {\n    dict : [KeyValue];\n}\ntable KeyValue {\n    key : int (key);\n    value : string;\n}\nroot_type DataArray;. Ya, did that, thanks. . ",
    "aleemstreak": "I signed it!. ",
    "Gozala": "\nThere already is a 1.6.0 release.. what exactly are you asking for?\n\nSorry, I don't know how I end up writing such a bad description. I was just was asking to publish latest version to npm. Currently version there is 1.5.0\nThanks! . @aardappel I see makes sense, thanks for pointing out. Would it make sense to have builder.asArrayBuffer() to take care of this ?. @aardappel You are correct! I was assuming use of ArrayBuffer.transfer without realizing it was not standardized. \nThanks for your help!. ",
    "travisfw": "@aardappel signed. It's saying it doesn't like the email address on my commits. I don't use the gmail address, and the CLA page didn't give me an option.. ",
    "mindori": "Move to the issue below\nhttps://github.com/google/flatbuffers/issues/4304. After debugging, the server code has no problem. \nLength bytes(20th ~ 23th index) of buffer was contaminated(e.g. 239, 191, 189, 0) in client code.\nClient code as follows.\nflatbuffers::FlatBufferBuilder Builder(1024);\nstd::string SengMsgStr(TCHAR_TO_UTF8(*Message)); // Message is correctly.\nauto SendMsg = Builder.CreateString(SengMsgStr); \nauto SendObj = CreateClientToChatMsg(Builder, SendMsg);\nBuilder.Finish(SendObj);\nWhich code can make invalid length bug? How can I bug fix?. OK. Key information is as follows.\nwrite the buffer in C++\n```\nFString Message = \"hellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohellohello\"; // 240 bytes\nflatbuffers::FlatBufferBuilder Builder(1024);\nstd::string SengMsgStr(TCHAR_TO_UTF8(*Message));\nauto SendMsg = Builder.CreateString(SengMsgStr); \nauto SendObj = CreateClientToChatMsg(Builder, SendMsg);\nBuilder.Finish(SendObj);\n```\nsend the buffer in C++\nFSocket *Client;\nint32 Sent = 0;\nClient->Send(Builder.GetBufferPointer(), Builder.GetSize(), Sent);\ncapture the SendMsgStr (C++ Client)\n\ncapture the data, recvBuffer (Node.js Server)\n\ncapture the data, recvBuffer (send to \"hello\" only) (Node.js Server)\n\n\"data\" is string and data[20] is 65335. I think the length byte(20th index) is already misinformed from C++ client. Maybe this problem caused data 268 bytes => recvBuffer 270 length.. @aardappel OK. :) It was a very small mistake. \nI specified to socket's encoding(UTF8) explicitly in Node.js. But, C++ Client send to binary data(not UTF8). so, I removed socket's encoding option and It was solved!. Let me ask you one more question :)\nHow can I represent value greater than INTEGER_MAX? \nconst bigValue = 4147483648;\nconst fooBigValue = builder.createLong(-bigValue, bigValue);\nIs it correct?. Thank you! :). ",
    "per-gron": "@aardappel Thanks for the review! I have pushed fixes now for your second and third review comment. For the first one, I'll see if I can make a PR against gRPC that should make it possible for Flatbuffers in the future to not have to modify cpp_generator.cc, but I'm not sure I can commit to actually getting those changes in and doing the adjustments later on in flatbuffers.git. There are now no changes to the generated .go files so I suppose they should be ok now.. @llchan Oh, sorry about that! I did check for this exact thing but must have misinterpreted what I saw somehow.\nIt might make sense to revert this then and then re-apply it once gRPC master has been released? It's up to you.. I think this is great! I love that you are taking it all the way to actual zero-copy, that is super useful.\nThat this is an API break is not a problem for me at all since I'm still just in the experimentation phase of what I want to do with gRPC/Flatbuffers. We both seem to agree that an API break is necessary to make this safe.\nFrom what I can tell, I would not have any issues using this API as is, but if I put on my project maintainer hat on, I would not like that MessageBuilder inherits from FlatBufferBuilder, and especially not that member variables of FlatBufferBuilder are made protected. Doing so is against the Google code style by the way, and that is a rule that I think is probably more important than stuff that clang-format can fix. That these are made protected means that almost any change that is done to FlatBufferBuilder's implementation risks becoming an API break. If you must keep the inheritance, I would prefer to make FlatBufferBuilder and MessageBuilder friends instead, to limit API break exposure.\nPersonally, I would like more if FlatBufferBuilder would be made a template that takes the allocator type as a template parameter (with a default value or a different name for the template class + typedef to not break the normal FlatBufferBuilder use case) (this would make the virtual methods on the allocator redundant but that's not the point), because then the allocator type could expose something that has the bulk of MessageBuilder::GetMessage's functionality. Then, FlatBufferBuilder could have a method that calls the allocator to do MessageBuilder::GetMessage<T> does here, and MessageBuilder could be a typedef of FlatBufferBuilder<SliceAllocator>. Then there would be no need for protected member variables or inheritance.\nI haven't thought this through super carefully so it might not work out but I think it could be worth spending a little bit of time trying to refine it. I have a feeling that current inheritance + protected members scheme could become increasingly messy over time.\n. Thanks for addressing some of my comments :-) Just to it clear: I wrote some review comments with some of the things I thought about when I read the PR, but I don't at all want that to block merging this PR. I think this is a great improvement and I wouldn't have any problems to use the library if it were like this. Your work is very much appreciated @llchan and @aardappel.. @llchan Another thing that might be interesting to think about:\nTo me, it would be very useful if flatbuffer validation was performed automatically for me, by the SerializationTraits. I think not validating input that is likely from network is not a good default setting. Would this be possible? It looks like it, given the return type of Serialize and Deserialize I guess this could be done in a separate PR though.. @llchan Thanks for adding verification, that will be very helpful for me.\nI like the approach that you came up with that allows further type-specific verification, that sounds like a useful additional feature (although the user will have to be careful about actually including that template specialization always).. A somewhat safer approach could be to allow the user to specify if validation should happen or not (or custom via trait) in the .fbs file. But that seems a bit over-engineery to me. Just doing verification by default seems sane to me and anything else is ok if it's a bit tricky to get at imo.. @aardappel For my use case, a preprocessor flag is workable, but I still think it's a mistake to not do validation. Since this is off-topic for this PR I opened a separate issue: #4336. Nice!. @llchan This is great! I'll close this issue then and make some comments on your PR instead :-). @llchan I agree with you: The main, non-gRPC APIs have uses where verification makes no sense. This is why I suggest having it on by default for gRPC, where the normal use case is messaging across machines, but leave it as is for the main Flatbuffer APIs.. Besides, the current GetRoot function could not do verification even if we wanted it to because it takes a void * without a size. \ud83d\ude04 . @aardappel Thanks for listening to my opinion, I really appreciate it!\nUnfortunately, I don't have much useful to share from the profiling that I have done. What I know is that validation has never showed up in a profiler trace for me except when validating a flatbuffer freshly mmapped from disk (in that case it's really measuring disk read speed). Given my understanding of how the verification code works, it really doesn't do much, which is great.. @llchan good feedback on the copy/assignment operators! I am a bit busy right now but will investigate soon.. @aardappel personally I don't think GetRoot() communicates runtime cost better than operator-> and I really like the convenience and shared_ptr-likeness but I do see your point and since it's just for convenience I'm not going to fight about it \ud83d\ude04 \nRegarding error prone-ness: Yes I totally agree: if it's risky in any way to have a copy constructor/assignment op then it shouldn't be there. It seems like it is safe though and if that's true then I think they are worth having. As I said, I will check this.. Okay so I have done a little bit of research on this. To be honest, I find it a little bit hard to follow the gRPC code (not because it's bad, it's just a lot of layers to take in) but from what I gather, Flatbuffer's SerializationTraits::Deserialize is called from here, with a grpc_byte_buffer* whose address is given to a grpc_op* in the same file. The only places that grpc_byte_buffer* is actually assigned are here and here (same procedure). So from what I can tell, the grpc_byte_buffer* set to the return value of grpc_raw_compressed_byte_buffer_create or grpc_raw_byte_buffer_create.\nSo it seems to me like gRPC does not re-use buffer like @llchan feared.\nI have also written a bunch of tests with code that uses the copy constructor and assignment operators in this PR and run them with address sanitizer, and it did not complain. So I think it seems safe to have them.\nI'll update the PR to remove the smart pointer operators.. Sorry about the delay, I've been travelling. I'll fix the missing deref call in the assignment op (oops) then I think this is mergable.. I don't think so, closing.. oops. this won't work will it. I really like that you are exposing only const access to Message's underlying buffer. Exposing it mutable has no good use case that I can see and it makes it a lot more scary to pass around and use.. I think it would help to expand this doc comment and say that Message, like grpc_slice, has shared reference counted ownership of the underlying buffer. This is obvious if you know grpc_slice and read the code but it would still help I think.. Actually, I was reading this wrong. Because of BorrowSlice I think you are exposing mutable access to the buffer.. nit: typo \"workign\". From what I can tell, the protobuf version of SerializationTraits::Deserialize returns StatusCode::INTERNAL: https://github.com/grpc/grpc/blob/master/include/grpc++/impl/codegen/proto_utils.h#L228\nThat seems a little bit odd to me. DATA_LOSS makes more sense than INTERNAL to me. On the other hand, it might still be better to behave like Protobuf does, for consistency.. ",
    "mattiasbrand": "Any chance we will see another upgrade of gRPC for flatbuffers? Unfortunately it seems like they have diverged more since this commit was made. . ",
    "cberner": "This looks great! What version of gRPC do I need to use this with? Just tried compiling a simple service, but end up with a compiler error:\nparameter_server_fb.grpc.fb.cc:38:10: error: 'Create' is not a member of 'grpc::ClientAsyncResponseReader<flatbuffers::grpc::Message<PS::Parameters> >'\n   return ::grpc::ClientAsyncResponseReader< flatbuffers::grpc::Message<Parameters>>::Create(channel_.get(), cq, rpcmethod_SyncParameters_, context, request);\nedit seems like 1.4.0-pre1 works!. Thanks :) Are you aware of any other memory leaks in the GRPC integration? Even with this fix my server & client are crashing, and address sanitizer shows a memory leak from the grpc_malloc code path. The server and client are extremely simple, and just send an array of 1 million floats back and forth. @llchan, @aardappel  thanks! I have a small test case that reproduces the leak I'm seeing: https://github.com/google/flatbuffers/issues/4348. @llchan, yep my service was originally protobufs and I'm trying to port it to flatbuffers to improve throughput. The original protobufs version works fine. Cool, thanks for the tips! Ya, that second one is just an artifact of removing a lot of code, but sizing the MessageBuilder would probably give a good speed up in my real test. @aardappel so far it looks like ~2x, although I'm still investigating performance bottlenecks. It seems like I should be able to get another 5x, as I'm still no where close to saturating the NIC on my machines. @aardappel found the first of my issues, and have filed a ticket here: https://github.com/google/flatbuffers/issues/4354. Tested this PR against our internal code base and it resolves the memory leak I was seeing. Thanks for the quick fix!. @aardappel yep, here you go. Hope I did it right!\nhttps://github.com/google/flatbuffers/pull/4355. Updated to address comments. ",
    "7anner": "How do I generate the code for the Protobuf case?. How does this look?\nhttps://github.com/grpc/grpc/pull/11221. ",
    "JoeUX": "You're right. Reading my post again and the relevant FlatBuffers page, I'm not sure why I thought I was looking at JSON. I must have been tired or something. It looks a lot cleaner than JSON.\nOn the IDL issue, the page links to the Wikipedia page about IDLs as a category of languages, which lists about 20 examples of IDLs, including protobuf, Thrift, etc. One of those examples is called IDL specification language. (Link to its Wikipedia page.) If that is in fact the language FlatBuffers is using, then you might want to update the link to point to it specifically. Or maybe even a reference site for it, if there is one. (It will be hard for people to find resources otherwise, since IDL is hard to Google \u2013 there's the concept/category of IDLs, the IDL programming language, the IDL specification language, et al.). ",
    "sfariv": "I signed it!. done.. hopefully I got it right this time. done.. done.. done.. Could you help me understand how this can happen?\nI tried schema\nnamespace sample;\ntable NestedObject1 {\n    count: int;\n    nestedObject: NestedObject (required);\n}\ntable NestedObject {\n    count: int;\n}\ntable ParentObject {\n    nestedObject: NestedObject (required);\n    nestedObject1: NestedObject1 (required);\n}\nroot_type ParentObject;\nwith the following json, but it detected that nestedObject was missing eventhough the parent object contains a similar field.\n{\n    nestedObject: {\n        count : 5\n    },\n    nestedObject1: {\n    }   \n}. ",
    "dfroze": "As arguments, it was passed only the first argument containing the schema, I assure you, there is no null. The strange part is that it worked in some cases, such as in a release mode, or on linux there was no problems a all.\nWhen I tried to link statically (the main lib containing a feature that uses flatbuffers), I got something close to memory corruption (got a segfault when trying to call the method of a variable recently initialzied). I know, something is wrong somewhere, but all started when I added the flatbuffers and can't figure it out where to look.\nOn linux, it works, except for static case (not sure if it's related)\nThe valgrind only said something when it segfaulted with the static mode (something like jumping to invalid function).\nAny ideas?. The problem was that I was using some old headers for a newer lib. . ",
    "pavelkalinnikov": "I agree that by default the client should not care about the allocator, and I hope the mentioned PR will improve the interface. But my PR is not intended to fix the interface, it fixes the incorrect usage of the current interface (the returned buffer points to a destructed allocator).. Added a comment to make this a little bit clearer.. Sounds good, thanks!. Agree. I also fixed the same issue in src/idl_gen_fbs.cpp. ",
    "emilio1234": "Tested \"./csharp_sample.sh\" in other releases:\nv1.4.0 - Working.\nv1.5.0/v1.6.0 - Not working. \nseems this functionality was lost at v1.5.0 onward. . ",
    "Grimeh": "I have a partial fix at Grimeh/flatbuffers@abe903d.\nOnce I sort out how to generate lookup by key functions (taking a string) for vector fields I will submit a PR.\n(note the referenced commit is missing the changed generated files, that will be fixed before it's submitted). Ah simple mistake, submitting PR now.. Found an issue with generation for required unions, have updated with the fix.. Yeah you're right, an assert would be much better than silently returning garbage data. I'll get to work on it.\nTo be absolutely clear, everything that is implicitly nullable we should assert on? So for C# this means reference types, and Java anything that isn't a primitive?. I haven't had a chance to touch this in a while, but have some time off coming up over the holidays where I will be looking into this again.\nRe: assert vs exception, afaik all C# Debug.* calls are not included in release, not sure what the equivalent in Java is. However I do see the need to gracefully handle exceptional circumstances in production if the loaded data may not always be in the correct state.\nDoes throwing an exception in the case that any required field with a non-nullable type has a 0 offset work for you?. ",
    "AntonYudintsev": "I sign it!. It is \"Unreasonable\" if someone wants to use ONLY FlexBuffers.\nThey need still need two headers, but not onw header is base one (small) and not whole flatbuffers one.. I am sorry for multiple typos :(. ",
    "BAndiT1983": "Sorry for posting a duplicate, haven't found the old issue. Maybe this threads can be merged. \nAs it's more or less urgent for our GSoC project, to help our student to get his task done in a clean way, i would try my luck and take a look into the PR.\nIf you have some pointers to make the task simpler, you're welcome. I suppose that besides inserting the type into the file there is also generating of right method declarations involved. Currently they show just byte or int, depending on enum type as argument.. No problem, i hope he will reply in the next days. Already started debugging through the Go generator to get the idea and also looking through the PR. Can't be a big problem, just the difference between generated code for C/C++ which i'm used to and the Go one is rather striking. Especially methods for creating/filling structs.. Thanks for directions. Will try to get it working as it seems like not many people are currently using this part of FB, but we need it for GSoC. Available PRs are good, just have to understand which parts were inserted for which processing, as i have just a basic overview of Go generator.\nAlso the Add methods have to be updated with right type, that's what i'm currently looking at.. I've done small adjustments, would anyone care to take a look as i'm not sure if it's sufficient or something important is missing? Currently this works for my schema, but eventually there should be a switch/option in flatc, so the old code does not break.\nhttps://github.com/BAndiT1983/flatbuffers/commit/e6ea43c85da7a5edf1bcbe94869465949c86caaa. Thanks, will look through your comments. I've taken some code from the PR which you've pointed me to, but to be honest my C++ style is different, just tried to stay in the way the existing code is styled.. Adjusted the style and added nullptr handling, but reflection_generated.h does not show any change after running the command. Does my change even affect it?\nhttps://github.com/BAndiT1983/flatbuffers/commit/65ec439995bbb5c2dd41de1d2ac3bf9c21a4f57e. ",
    "vabr-g": "Good catch!\nIndeed, the current two constructors do the same thing. Judging from the name of the class, this is an iterator, so basically a wrapper around the pointer. In that case, copying the underlying data pointer seems like a copy action to me (as opposed to a container object, where copying the pointer without the data would be moving), so I will delete the moving constructor.. ",
    "emilm": "It sure does! Yes I would love to make a PR, I also looked into Span which creates a view of a section of a big byte buffer. But that seems to be a .NET core thing.\nThe generated code is like this:\npublic static VectorOffset CreateRVector(FlatBufferBuilder builder, byte[] data) { builder.StartVector(1, data.Length, 1); for (int i = data.Length - 1; i >= 0; i--) builder.AddByte(data[i]); return builder.EndVector(); }\nNow, it does loop but it seems to do it in reverse. Would it harm to do a block copy or are the bytes written backwards?\nI guess for other types like ints etc I need to reverse endians from network order?\nThanks!. OK! I will make a PR!\nWhat's the reason that it's built backwards, just so I understand what's happening here. :). I was and still am on a time constraint so I haven't been able to implement it unfortunally...\nThe codegen needs to be changed basically, it's a class that builds a huge string both for Java and C#.. ",
    "cnheider": "Any update on this PR?\nCopying large byte arrays is quite a bottleneck in my application. ",
    "dbaileychess": "I am working on a PR for this, as I have needs in C# to access vector of structs quickly.\nI have implemented the getter side first since it is simpler to work with the existing buffers. But I will work on the serialization side as well.\n. Thanks for the feedback, I'll work on those and aggregate the changes into another commit. As for the benchmark speeds, I did spend a good half day working with a profiler to optimize them, but yes, I agree that speed is important for this. \nThe biggest hurdle is the lack of native array support in Lua. I could implement a Lua userdata to abstract the binary buffer away for faster access, but then it adds more complication for building the library for different environments. . I am getting a segmentation fault when running flatc.exe in Release mode (Windows), but not Debug or RelWithDebInfo configurations. I also tried it with the python generation and got the same seg fault. Want me to open a general issue for this?\n\n. @aardappel  There was a buffer overflow in inserting the keywords into the unordered_set that was causing the segfault in Release mode, but not Debug mode. I took this code from the python implementation, so I also fixed the python generation file to match the fix I did with Lua. \nI also merged from upstream/master. I can rebase if it makes things easier to view.. @aardappel Working on Docs now. I noticed the Tutorial section uses a schema that contains:\npath:[Vec3];        // Vector of structs.\nBut that is no longer in the samples/monster.fbs file.  Should the monster.fbs file be updated to match the docs?. Sure. I am just profiling it a bit more to speed it up. But that can be a new PR as that probably will take some time to get it working well.\nThanks for your quick review!. I only added new functions, so all the original functions should be as is. The only original implementation I changed was the the ToStringArray() method which calls the generic ToArray<T>() method internally. I feel this is a small change.. Ok, i'll take a look. Which version of Mono were you using / what should we target? Looks like the mono isn't handling the generic parameters correctly.. It was just a Preprocessor issue. When running the tests with UNSAFE_BYTEBUFFERS, one of the new functions I added was commented out. \nI push to my fork, but only the 'revert' commit is showing up, I have my b930e with the actual fix. Do i need another pull request?\n. I am actually not sure how the features flags work. I can alternative change the check to just look at the gcc version as an indicate, and assume if it is a non-gcc compiler, that it supports template aliasing. . Gotcha, fixed.. That was the location of the generated lua files from the monster schema. I can remove this ignore and commit the generated files so we can compare in the future.. Will do.. Generally locals are faster than the heap, but I guess these are stored as up-values to all the following functions, and might not be much quicker. However, it definitely doesn't slow down access.. That was primarily for debugging the byte array, I could just remove it as it is not needed by the implementation. . Ah, I looked this up. Good info, i'll fix it.. Gotcha. I don't follow this comment? what needs to be changed at this location?. I questioned it myself. I just ran the generate_code.bat from windows and those files were changed, which I too thought was odd, but didn't investigate. . Ok. @aardappel When reading the buffer, I do store the buffer as a binary string and use the ;sub() function to extract parts of it.. This added some complications to the std::string code += lines. If the lines just concat const char * it throws a compiler error since the rvalue is just adding two pointers and is not automatically cast to std::string.\n```\nstatic const char * Indent = \"    \";\n///\nstd::string code;\ncode += Indent + Indent; // compiler warning\n````\nso the code needs to be changed to:\n```\nstatic const char * Indent = \"    \";\n///\nstd::string code;\ncode += std::string(Indent) + Indent;\n````\nor a more major refactoring to fix the issue.. Sure. Fixed. I just followed the rest of the Add<type>() signatures, which just are void returns. . Fixed. Yes, I did test against the accessor method. The test code is basically:\n```csharp\nvar sum1 = 0\nfor(int j=0; j<sp.Array1Length; j++)\n{\n sum += sp.Array1(j);\n}\n// versus\nvar sum2 = sp.GetArray1Array().Sum();\n````\nAnd the latter performed 20x faster for when length = 3000. \nI think there is a lot of overhead in the accessor method. You have to do a v-table lookup each time (which makes me think that it should memorize the value), and you have to get the vector index each time as well. . Scratch the memorization comment, that won't work well, as it may be different depending on the buffer provided.. ",
    "WhoBrokeTheBuild": "Fixed with Pull Request #4347. Thank you! And that's good to know!. ",
    "concatime": "Sorry, but I will not sign it because It requests personal information, but you're 100% free/libre with this pull request.. ",
    "ztkmkoo": "Thanks you~\nI Understand Java cannot work like c++.\nIf I have the better way, i'll try to make a branch~. thx. it's nice of you. it is much faster now!. ",
    "sebix": "Where do I need to add it? I'm not experienced with cmake.. Adding it there and running mkdir build && cd build && cmake -DCMAKE_BUILD_TYPE=Release .. && make V=1 does not add it to the Makefile and does not raise the error. (yes, I clean everything with git clean -ffd). That seems to work, partly:\n\nmake V=1\n\nScanning dependencies of target flatc\n[  2%] Building CXX object CMakeFiles/flatc.dir/src/code_generators.cpp.o\ncc1plus: error: missing argument to \u2018-Wimplicit-fallthrough=\u2019\nmake[2]:  [CMakeFiles/flatc.dir/build.make:63:\nCMakeFiles/flatc.dir/src/code_generators.cpp.o] Error 1\nmake[1]:  [CMakeFiles/Makefile2:68: CMakeFiles/flatc.dir/all] Error 2\nmake: *** [Makefile:141: all] Error 2\nShould we use 2?\nFrom gccs manpage:\n           <-Wimplicit-fallthrough=2 case insensitively matches>\n               \".falls?[ \\t-]thr(ough|u).\" regular expression.\n. > But from which GCC is this warning available?\n7, according to https://gcc.gnu.org/gcc-7/changes.html\nIt seems the existing comments did not match the regex used by gcc. Patch is on the way.. bbb72f0b737e84553e19352d940f7e367f111c8e fixes this bug and it's part of the recently released 1.7.x\nBut it does not add this to CMakeLists.txt\nBut as I said, I don't know how to achieve that.. ",
    "rstoica": "The conditional check on ARDUINO at the moment fails to build with ArduinoSTL when flatbuffers try to include utility.h instead of utility. One potential fix, is to conditionally look for the include guard of ArduinoSTL lib, i.e. ARDUINOSTL_M_H, which needs to be included anyway before the flatbuffers to pull in the STL headers. \nFor the cstdint, is a matter of user preference I guess whether it is included or not in the statically generated binary for the Arduino.\nIf you think this fix is good enough I can open a PR on it.. I signed it!. ",
    "alexames": "Definitely seems like a bug. Care to make PR?. I'll give it a look if I can figure out how to load up clang-5. . Does this sound like a workable idea? What if instead of adding functions like flatbuffers::vector_back(), we added a flatbuffers::vector class - under everything except STLPort this is just:\ntemplate <typename T> using vector = std::vector<T>;\n\nbut under STLPort it's a class that extends std::vector and implements the functions we care about (data, back, emplace_back, all the things stewart already wrote wrappers for). Then the only things that need to change are the types of the data. The code itself can be written in idiomatic C++11. (It's not a perfect solution of course, if we decide to use other C++11 features we have to add those to our customized version of the classes, but the number of new functions on the container classes is relatively small I think.)\nAs for the things new to C++11 like unique_ptr, I feel like we should be able cheat a little bit and just drop those in namespace std even though that's technically UB. (And only under STLPort builds of course). \nIf the above ideas are not feasible solutions, I'm not sure whether it's worse to increase the hurdles to contributing or lose out on potential users who are married to STLPort. I know there's a surprising number of people using STLPort but I don't know how many of them are itching to use flatbuffers.. I tried reproducing this using flatc on the sample file samples/monster.fbs and it seemed to work fine.\nflatc -n  samples/monster.fbs --gen-onefile\n\nCan you test that file and let me know what happens? I'll also try running on windows (currently testing on linux) and seeing how that goes. . Yeah I can have a look.. Yeah, that looks like a bug. Care to make a PR?. I'm a bit busy at the moment, but I'll leave this issue open to remind myself to come back to it later. If someone else wants to make the PR in the meantime I can merge it in. . PRs are always welcome :) I'm not an expert on the TS implementation, but I believe it occurs as part of the Javascript code generation. You can have a look at src/idl_gen_js.cpp as a starting point. The --keep-prefix flag is stored in the parser options in a field called keep_include_path. Here is where this is used in the C++ generator. That might give you an idea of what you need to do to get it working. . Since he said it looks good to him, I'll go ahead and merge it in.. Since @gwvo is out right now, @rw or @evolutional, do either of you have an idea of what could be going on here? . That's definitely what it looks like to me, but I'm not familiar with how the javascript implementation handles parsing. If you're willing to dive in and investigate, by all means do so :). Sorry, I was out the last two days and didn't get a chance to look very deeply into this. I'll give it another look and see what I can find. . I spent some time looking at this and I can't reproduce the error locally. Can you provide a minimal example that demonstrates the error you're seeing? What's the expected output compared to the output you're actually getting. . Should &data be &&data, and then be forwarded into the emplace_back call? e.g. std::forward<T>(data). I believe it is guaranteed that these two operations are identical, so it seems like adding unnecessary complexity here to have it change behaviors when not using stlport. . nit:spaces. optional: Worth writing a StringBack() function?. ",
    "robbiemc": "Yeah, it's not too bad. The other change I needed to make was to clear the \"generated\" flag from everything in the files specified on the command line. Without that, only the contents of the last file on the command line will actually be generated, due to how MarkGenerated() is called. EDIT: I guess it's not the last file on the command line that would be generated if you don't clear the \"generated\" flag, but all files on the command line that aren't included by other files, as well as the first file in an include cycle.\nLet me know if you can think of a cleaner way around the \"generated\" flag issue.. You're right, I misunderstood the error I was seeing - it actually has to do with predeclarations. Consider two schemas, A.fbs and B.fbs, that contain an A and B table respectively that circularly depend on each other. When you try to compile A.fbs, it will parse B.fbs because it's included, during which it will predeclare table A. Immediately after parsing B.fbs, it will call MarkGenerated(), which will mark the predeclared table A as generated, so it gets skipped in the code generation stage. To fix this, I changed MarkGenerated() to only mark defined structs as generated, not predeclared ones.\nI updated the pull request.. ",
    "TheNeuralBit": "I'm using *.fbs in arrow/format. On a somewhat related note, I'm wondering if there's a better way to deal with the namespaces in the arrow schemas. All four of the arrow fb schemas use the org.apache.arrow.flatbuf namespace, which seems to work well in the generated C++ and Java, but in TypeScript I can't merge the namespaces across files. So far the best I've figured out is these ugly imports: https://github.com/ccri/arrow/blob/flatbuf-typescript/js/src/arrow.ts#L19. ",
    "ssbanerje": "@aardappel:\n Updated to use CMake 2.8\n Reverted to old travis build. @aardappel Looks like the header only INTERFACE library feature was added only CMake 3.0. I don't think this is going to work without that bump.\nI think going forward I can make one of two changes:\n Install CMake 3 from sources on the travis build\n Install CMake 3 from PPA\nI think the home-brew build should come with a newer CMake version, so that shouldn't be a problem.\nAdditionally, since the library version needs to be included in generated the CMake package files, I will either have to hard-code the version information in the CMake file, or get it from Git. This will imply that git will have to be installed on the Travis instance as well.\n. I guess this is going to be impossible without a bump in the CMake version. Which would mean updating the TravisCI script to use a PPA for a newer CMake version. I can do that if you would like.. This needs CMake 3+. The default Travis container does not have this.... The main reason for the CMake version bump is for the generator script that creates <install_prefix>/lib/cmake/flatbuffers/FlatBuffersConfigVersion.cmake. This script also needs the package version, which is why it is extracted out of Git. This is pretty much based on how it is done in the deb creator that was already in the codebase . This is not unrelated to the CMake package config change. The advantage of the generated config files is that compile flags for the original CMake flatbuffers or flatbuffers_shared libraries can be inherited by any other project that links to it. I think it is undesirable to have the warning flags propagated through to any other projects that links to flatbuffers. Hence the warnings have been updated to fix this. I have maintained the original warning flags for all the system configurations.. ",
    "antmerlino": "What is the status of this PR? How can I be of help to get it finished?  These CMake changes are very good improvements from a build system perspective. These changes prevent pollution of global compiler flags/settings that can be problematic for downstream usage. Furthermore, this library adds additional targets like flatbuffers_headers which are useful when downstream only needs header-only library or perhaps doesn't have the requirements to compiler the flat library (I have this use-case).\nCMake is now on version 3.12, I think taking advantage of the new features make sense. However, I understand the concern of forcing users to upgrade. Perhaps we can use CMAKE_MAJOR_VERSION\nin the very beginning of the top-level CMakeLists.txt.  to include(flatbuffers-cmake2.cmake) or include(flatbuffers-cmake3.cmake) based on that version? The downside of this is that there needs to be 2 build tests and 2 build systems maintained. So maybe that is not a great solution, but at least you aren't forcing users to upgrade. However, I'm not convinced that it should be a requirement for flatbuffers to always stay at cmake 2.8. I'd think there has to be some point where the improvement to flatbuffers outweighs the downside of switching to cmake 3. Perhaps downstreams just have to stay at a previous release of flatbuffers if they don't want to upgrade to cmake 3?\nAs I said, please let me know the status and how I can be of help getting it integrated.\n. I don't have a good answer for statistics on usage per version.  Here is the only reference I could find with any hint of info:\nhttps://gitlab.kitware.com/cmake/community/wikis/doc/cmake/Life-Cycle-Considerations\nLooking at popular Linux distros in the table, excluding the super long-term support versions, most seem to be on > 3.5. Not sure if that's sufficient data.\nI will try and figure out the minimum required version for the suggested changes.\n. After looking at this more, there are less improvements than I expected. The PR is really out of date, so many of the CMake improvements such as installation have been done in upstream since.  The one thing that would still be helpful is having a header-only cmake target. I'll submit a fresh PR to a support header-only library target.\nAs for this PR, there may still be some useful changes in the mix here, but I think the OP would be best to merge and suggest what is still relevant. . ",
    "bobobo1618": "The signature of CreateVector from the docs is Offset<Vector<T> > flatbuffers::FlatBufferBuilder::CreateVector   ( const std::vector< T > &      v), and that indicates that it would return Offset<Vector<Thing>>, without an offset. clang concurs:\n```\nstd::vector things;\nthings.push_back(*GetThing(data));\nOffset>> things_offset = mb_.CreateVector(things);\nerror: no viable conversion from 'Offset>' to 'Offset>>'\n```\nIt's not possible to use the array constructor either since it's not possible to declare an array without a pointer (which results in Offset<Vector<Thing*>>):\n```\nstd::unique_ptr things(new Thing[num_things]);\nerror: call to implicitly-deleted default constructor of 'Thing'\n```\nAnd it's not possible to use CreateVectorOfStructs since it returns Offset<Vector<const Thing *>>.\n```\nstd::vector things;\nthings.push_back(*GetThing(data));\nOffset>> things_offset = mb_.CreateVectorOfStructs(things);\nerror: no viable conversion from 'Offset>' to 'Offset>>'\n```\nIt doesn't seem to be possible to use CreateVector or its ilk as I need to pass Offset<Thing> to them and there's no way for me to produce Offset<Thing> without using CreateThing, and perhaps using reflection to copy from the read Thing to the Offset<Thing>.. > Creating and reading a FlatBuffer are two entirely seperate APIs, and you can't just mix them. \nThat's exactly the problem. Perhaps I should create a separate issue for that?. I see. Why is that? Is it a tradeoff inherent in the way flatbuffers work? Could it be documented somewhere? As a user of similar libraries (Protocol Buffers and Cap'n'Proto), I expect to be able to write something I've written unless I'm told otherwise.. > Unlike Cap'n Proto, FlatBuffers doesn't store fields that are not set or are not default, so being able to modify them after creation doesn't make much sense (it would require a very complicated resizing operation).\nIsn't that what GetMutableThing does? Or is it limited to fields that are already set?\n\nYou were also trying to point to objects of different buffers, which would never work, not even in Cap'n Proto. Luckily, as you find out, the API is strongly typed enough that it doesn't allow you to do those kinds of things.\n\nWith Cap'n'Proto I can do:\ncpp\ncapnp::FlatArrayMessageReader reader(data);\ncapnp::List<Thing>::Builder things = container.initThings(num_things);\nThing::Reader thing = reader.getRoot<Thing>();\nthings.setWithCaveats(0, thing);\nthings.setWithCaveats(1, thing);\n[...]\nInternally, it copies the entire struct, which is basically what I was expecting would be done here (and is the same as what I'd do with protobuf I believe).. ",
    "zhangminglei": "Hmm, That is why time the deserialization in my results for the FlatBuffers is basically always return almost to 0 milliseconds.  Makes sense better now to me. With respect to above code what you said is wrong, I would also say \"It is literally wrong.\" What I did these things as I would like to know FlatBuffers to write data to the disk that occupy how much space compared to Protobuf though.  The conclusion is that the cost of the disk space of the Flatbuffers is relatively large than that of PB, which should be a way to take advantage of space to exchange time savings. \nHere, I give a set of test results for how much disk space for serializing text file by means of Flatbuffers and protobuf.\n\nText files from Kafka size: 36, 879 KB ( input data )\nWith PB, write the bytes to output stream: 17, 878 KB ( wire format ) \nWith Flatbuffers, write the bytes to output stream: 38, 589 KB (wire format) ( > 36, 879 KB belongs to text file size)\n\nI will test for accessing 1 field, 2 fileds, 14 fields, and all fields [totally 27 filelds within an object] under different cases.\nLast but not least, Would be better if Flatbuffers can provide a compressed API interface, and then send the compressed data to network for transmission, What do you think ? And Flink  on the way to use compression (e.g. snappy) for full check/savepoints. https://issues.apache.org/jira/browse/FLINK-6773. FYI. I put a report testing here from my work recently as following as a reference. And the input data comes from Kafka production environments. I have two sets of data, one's size is 36,879 KB, another size is 145,157 KB respectively.  I parsed those input data into totally 27 fields and below are the results under different cases which including access 1, 2, 14 and 27 fields. Time is timed by millisecond.\n\n\n. ",
    "geeknik": "Well, the binary itself compiles fine and I can go on fuzzing with it( which I have and sent 2 bugs to the Google Security Team), but this seems to just be failing during one of the built-in tests that happens during compilation.. This is where I got my clang-5:\ngit clone https://chromium.googlesource.com/chromium/src/tools/clang\nclang/scripts/update.py\nAnd then just update your $PATH to reflect the location of clang.. ",
    "PSanetra": "I think a Docker image with flatc is a good parent image to compile a schema as part of a multi-stage build. But I would also appreciate a pre-built flatc executable for Linux.. Yes, I agree a pe-built flatc would cover most use cases.. ",
    "dt665m": "+1 on maintaining a CI stage that builds a docker image with each flatc release.  Container based CI systems such as Drone(https://github.com/drone/drone) are gaining some momentum because of ease of customizing each pipline stage with docker images.  Google Cloud even has a flavor of this in Container Registry Build Triggers.. ",
    "neomantra": "In case people are searching for this, I've made a separate repo (vs pull request) which contains FlatBuffers build artifacts.  It was intended to be used either as part of one's own Docker tooling via COPY or directly in a build pipeline via docker run and volume bind mounting.\nImage builds are performed by Travis-CI.  Tagged FlatBuffer releases are available (starting at v1.8) and the master branch of FlatBuffers is built weekly.\n\nDocker Registry: https://hub.docker.com/r/neomantra/flatbuffers/\nGitHub Repo: https://github.com/neomantra/docker-flatbuffers\nTravis Builds: https://travis-ci.org/neomantra/docker-flatbuffers\n. I just built that successfully with my Dockerfile with both clang and gcc.\n\nHere's some self-service instructions if you want to iterate:\ngit clone https://github.com/neomantra/docker-flatbuffers\ncd docker-flatbuffers\ndocker build \\\n  --build-arg FLATBUFFERS_USE_CLANG=true \\\n  --build-arg FLATBUFFERS_ARCHIVE_BASE_URL=\"https://github.com/vglavnyy/flatbuffers/archive/\" \\\n  --build-arg FLATBUFFERS_ARCHIVE_TAG=\"def_nan_csharp_java\" \\\n  .\n. I also just tested with Debian Buster (unreleased but official Docker images exist), which uses GCC 8.2.0 and Clang 7.0.1.\n[I just pushed changes to docker-flatbuffers to support changing the base image.]\ngit clone https://github.com/neomantra/docker-flatbuffers\ncd docker-flatbuffers\ndocker build \\\n  --build-arg FLATBUFFERS_IMAGE_TAG=\"buster-slim\" \\\n  --build-arg FLATBUFFERS_USE_CLANG=true \\\n  --build-arg FLATBUFFERS_ARCHIVE_BASE_URL=\"https://github.com/vglavnyy/flatbuffers/archive/\" \\\n  --build-arg FLATBUFFERS_ARCHIVE_TAG=\"def_nan_csharp_java\" \\\n  .. I've updated these per your suggestions.  I also added a GCC Dockerfile for better coverage.  I think the ctest run does all the tests you want -- if not let me know.. This failed on Travis because the build took too long:\nThe job exceeded the maximum time limit for jobs, and has been terminated.\nIt looks like this build stage normally take less than 10 minutes.  \nWith these full C++ / GRPC builds, it is taking much longer and Travis has a limit of 50 minutes.   Each C++ build (there are two, GCC and Clang) takes about 30 minutes.\nI will put these into separate Travis stages, rather than being under the \"languages\" stage... . Noting that the Appveyor build failure there is unrelated to this commit.  It was a failure to find a Rust crate repository.    Note the Travis build passed and it built the Dockerfiles successfully.. Good catch... typo there in the Dockerfile ENV statement : CCX vs CXX.   I am running a test build again locally and will update once it is straight.\nWith the same commit, I will make it faster by adding parallel build of 2.  This should be OK for Travis, according to this. \n Maybe that will shorten it by half.\nWith respect to whether this is part of the build or not, is up to the maintainers.   If not accepted, there's still some coverage from my weekly Docker build.\n. I have added parallel builds with JOBS=2.  I also restructured the Dockerfiles a built so that the build caching mechanisms can speed things up a bit (mostly building GRPC).  Hopefully this cuts down on the build time.\nI also fixed the CCX -> CXX typo.. I removed the Docker GCC build from the .travis.yml (but left the corresponding Dockerfile) because it largely does the same thing as the co-existing Travis builds (the stanza below what I just removed).  So in the end this changeset just adds the clang build and 20-30 minutes of build time.. I don't think the Appveyor build failure is related to any of my changes.  It looks like the build was hung during compilation and was terminated by Appveyor after an hour.. @rw I took it out of that directory because Travis build steps are limited to one hour and that whole languages subdir build is one step.  Adding the ~30 minute C++ build to that step caused the timeout to happen.\nThus I separated it into its own Travis step (in .travis.yml) with the comment:\n```\nrun separately from build-and-run-docker-test-containers.sh because of build timeouts\n```\n. To clarify, because of build time and that the build already does a non-Docker GCC, the final squashed commit of this adds the Dockerfile for GCC but does not run it. \nIt does run the Clang one, but not via the languages subdir but rather as an explicit build step.    \nThere were a bunch of updates to this PR to get it going but the squashed diff is pretty simple. . I rebased this to master and squashed this PR.\nThe CI build is failing on clang because of https://github.com/google/flatbuffers/commit/9bf9b18f0a705dfd6d50b98056463a55de6a1bf9. Clean build \ud83c\udf89 . Oh, sorry, I looked through the issues for it but not the PRs.. This build failure also came up in my weekly Docker builds.   I'm confused why the existing CI system didn't catch it, as these Docker builds usually only catch clang problems.  Still looking.... ",
    "ghorbanzade": "Hi @neomantra, @aardappel and @mikkelfj ,\nI came across this thread today while working to refactor build process of a project to use multi-stage build instead of the builder pattern. What @neomantra has done certainly solves the issue, but I think many would prefer if flatbuffers project provided official docker image of flatc. For company policy reasons, I could not use neomantra/flatbuffers so I created a somewhat simplified version of it which provides the most recently released version of flatc in ubuntu:disco. see here if interested. Would you see value in integrating this or something similar into the build process of flatbuffers or flatcc repo? I would be happy to help with that.. I completely agree providing binaries would be a more generic solution. But I believe it requires providing binaries for each platform and architecture; Isn't that a bit hairy and harder to maintain? For a certain group of users looking to generate code from a given schema as part of their build, using a docker image as a build stage may just suffice. They won't have to worry about the platform and architecture the flatc is built on. Granted, building flatc from source is fast and smooth but it can be avoided after all. In any way, I think I understand your reservations, specially if you are already thinking about publishing binaries. :). That is true, I admit a statically linked 64-bit executable can go a long way and is sufficient for most of the use cases. :). ",
    "christianvoigt": "I can't cast it to IFlatBufferObject in the generic method as a \"non-nullable value type\" is required for TTable.\nI would like to have a system in which I can add or remove table structs that implement my interface without having to add or remove cases to a central switch statement.. Sorry I forgot to add an example. Let's say I have an entityData table with a vector called components. This contains instances of a table that wraps a union field. Normally I would do something like this:\nc#\nfor (int j = 0; j < entityData.ComponentsLength; j++){\n    var componentWrapper = entityData.Components(j).Value;\n    switch(componentWrapper.componentType){\n        case MyUnion.TypeA:\n            var typeAStruct = componentWrapper.Component<TypeA>();\n            typeAStruct.myInterfaceMethod();\n            break;\n        case MyUnion.TypeB:\n            var typeBStruct = componentWrapper.Component<TypeB>();\n            typeBStruct.myInterfaceMethod();\n            break\n        ...\n    }\n}\nWhat I would like to do is this:\nc#\nfor (int j = 0; j < entityData.ComponentsLength; j++){\n    var componentWrapper = entityData.Components(j).Value;\n    var component = componentWrapper.Component<IMyInterface>();\n    component.myInterfaceMethod();\n}. No, it is simply a method whose signature is defined in IMyInterface. I implement this interface for each table of the union in partial structs. This allows me to do different things for different tables, but still use the same interface.. As far as I know, partial classes (or partial structs) will be combined into one class (or one struct) and behave as if they were defined in one file. So if I cast typeAStruct to the interface and call the method, the method implementation of TypeA can access any fields of its generated table struct.. ",
    "pawel-kaminski-krk": "yes I call finish on FlatBufferBuilder, all other values are correct only byte[] is invalid, out of range.\nOK I understand that without code and data you can tell very little about what is going on, I ll try to make a small example that replicate this issue and post back.. thanks @gwvo for comments. I just wanted to share my results as use case and data are real so benchmarks and test also show real differences. I though it might help improve java lib. To share code I probably should ask for permission. Maybe compaction mode would be nice addon - yes yes, cap'n proto provides such feature so maybe you dont want.\nobviously 3 topic was my subjective perspective - and you are right I could do that, just I was not sure what will happen, it was nowhere documented, or at least visible enough and I was quite tired fighting with code - you could change the example https://github.com/google/flatbuffers/blob/master/samples/SampleBinary.java.\nas for 2nd topic. I tried to test various methods to serialize our value object to string so I can send it as GET parameter. this operation requires also to encrypt that binary array and process result with base64. Same operation applies to protobufs. \nSo maybe flatbuffers win when you actually dont read all values from buffer - here I had to read them all and provide defaults where needed.\nOn average FB are twice as slow as PB. maybe it is due to length of resulting byte array. . you didn't understand. \nanyway stringifing is not an issue here.. ",
    "alexrwasserman": "Thank you for the feedback, I believe all the changes you requested should be there now. We decided to implement this as an analogous function to sizedByteArray(int start, int length), which is also public. I created TestBuilderBasics() which contains the copied code and I call that instead now. Can you verify that this change does not sufficiently address your request?. The file is now updated to include this change. ",
    "gabyx": "Is there any work going on on this issue?\nI am wondering because I am thinking about using either a Union of tables or a flexbuffer field which to encode additional data. In the flexbuffer case it is not important for the schema files what kind of data resides there and the user can then load the appropriate schema...\nin the union case, the schema files need to be already available...\nfor the flexbuffer case: we can already store another flatbuffer as a flexbuffer, just make another builder make the flatbuffer, get the pointer and size -> put it into the flexbuffer field.... I think I can wrap around this: https://google.github.io/flatbuffers/classflatbuffers_1_1_flat_buffer_builder.html#ae4ab74f53187e6c3e6041d34e01904c4. If I understood correctly you are suggesting something along:\ncpp\nuint8_t* buffer;\nstd::size size;\nstd::size offset;\nstd::tie(buffer,size,offset) = builder.ReleaseRaw()\nIs that correct?. Is a PR welcome? so meaning to implement the function\nin the builder:\nstd::size size;\nstd::size offset;\nstd::tie(buffer,size,offset) = builder.ReleaseRaw()\nwith a proper documentation that buffer points to the start of the memory block , and buffer + offset is the start of the flatbuffer itself (with size: size-offset).\nor on the detached buffer:\nstd::size size;\nstd::size offset;\nstd::tie(buffer,size,offset) = detachedBuffer.ReleaseRaw()\nWould be handy, especially if we use own allocators. . Cool, the release_raw functionality I think is still usefull if you want to decouple flatbuffers stuff from other binary stuff =). This issue is closed due to PR #4885 . I would strongly suggest, that you use something along the line:\ncpp\nFlatBufferBuilder builder(image_size);\nstd::size size;\nstd::size offset;\nstd::tie(buffer,size,offset) = builder.ReleaseRaw();\nIn that way you are now in charge of the real buffer (owned by you) and you should delete this by \nDefaultAllocator::deallocate(buffer, size) (in this case!) and not doing something else (like you suggesting by deleting the DetachedBuffer object (which underneath holds the buffer)).\nIf you hand in a special allocator to FlatBufferBuilder builder(image_size, myAllocator, false /*dont own the allocator*/); You should of course delete then with myAllocator.deallocate(buffer, size).\n. But since, we have probably already have move semantics #4897. You could move arround the FlatBufferBuilder (if I am not mistaken :). The constructor of NodeConstructionInfo (which depends on a simple ConstructorKV table with a union Schema) is:\ntypescript\n/**\n     * @param {number} index\n     * @param {executionGraphGUI.serialization.ConstructorKV=} obj\n     * @returns {executionGraphGUI.serialization.ConstructorKV}\n     */\n    constructor(index: number, obj?: NS3319911885040634447.executionGraphGUI.serialization.ConstructorKV): NS3319911885040634447.executionGraphGUI.serialization.ConstructorKV | null {\n      var offset = this.bb!.__offset(this.bb_pos, 8);\n      return offset ? (obj || new NS3319911885040634447.executionGraphGUI.serialization.ConstructorKV).__init(this.bb!.__indirect(this.bb!.__vector(this.bb_pos + offset) + index * 4), this.bb!) : null;\n    };\nI really dont get this constructor (it returns an object which is not allowed apparently, what does the index: number mean, its not even used???...\nSomekind of error in the generator I think.... Ah you mean the word constructor:\ntable NodeConstructionInfo {\n    type:string (id: 0, required);          //!< The rtti of the logic node.\n    name:string (id: 1, required);          //!< The name of the logic node.\n    constructor:[ConstructorKV] (id: 2);    //!< Key-value pairs for the cosntructor. (not yet used)\n}\nHaha, lol, weird, didnt got to this solution, thanks :-)!\nIs there any improvement towards an error message for the user, if he uses keywords he should not :-)?. Is a PR welcome? Could somebody guide me where this TS generating happens as a starting point :-), to see if I can adapt the include statements in case --keep-prefix is given =)?. See PR #5018. OK problem solved, the bug is due to my hacky workaround which could be solved with #4848 (that the buffer cannot be released properly)...\nSorry :-). cpp\nauto dBufferRaw = builder.ReleaseRaw();\nin #4885 : I tried to have this release functionality discused in #4848 : the problem I stumbled upon is:\nFlatBufferBuilder::ReleaseRaw's  semantic  is kind of strange. \nFirst, if the user relies on the DefaultAllocator, he is responsible of deleting the memory also over this allocator, so dBufferRaw.allocator() == nullptr means deleting with flatbuffers:DefaultAllocator.\nSecond, there is this bool, own_allocator_ which is kind of troublesome..., \nWhat should dBufferRaw do? I decided to do exactly nothing:\n- DetachedBufferRaw does not at all manage the memory (exactly the behavior for rewrapping somewhere else..)\n- no memory management for the allocated memory and also not for theallocator() itself.\n- it should only be movable, since we should not keep copies lying around (move should nullify the class, not yet done...)\nMaybe we could in the future remove the DetachedBuffer class which relies on memory management  behavior, or simply leave it in, since it might be useful for some users...\nThanks for your help.. do you rather prefer a return struct or a return by input arguments? or a std::pair<std::pair<uint8_t*, size_t>, size_t>.\nI would define a return struct.... :) ok. will adapt to this intention. super simple is ok. super ugly not. will make arg references :). thx. Requesting review, for my changes :-) Thanks =). good point. There is no member size ? Except the member function size(). Is it that? Strange...\nwhy should there be a shadowing?. Hm... I had similar doubts/thought about the semantics of deallocation with the allocator when release_raw. Your first point seems perfectly fine, and is just a nice feature to have (?).\nThe thing is, \"ownership\" is transferred from FlatBufferBuilder down into the vector_downward member. \nSo you're kind of right, after\ncpp\nFlatBufferBuilder builder(alloc, true,...)\n...\nbuilder.ReleaseRaw(...);\nThe allocator is destroyed which is not really a good behavior. I think (2) and (3) are both perfectly valid and should be implemented as you did :-).\n. I will have a look at the detached related things, what I ve seen so far is better, yes! maybe tomorrow. @sutambe I think, you did some good work here. =). wrong branch.... Input:\n```typescript\ninclude \"/executionGraph/serialization/schemas/DataTypes.fbs\";\ninclude \"executionGraph/serialization/schemas/SocketLink.fbs\";\nnamespace executionGraph.serialization;\n**Output:**typescript\n// automatically generated by the FlatBuffers compiler, do not modify\nimport * as NS578127928888604879 from \"/executionGraph/serialization/schemas/SocketLink_generated\";\nimport * as NS14965100849554803941 from \"./executionGraph/serialization/schemas/DataTypes_generated\";\n/*\n * @constructor\n /\nexport namespace executionGraph.serialization{\nexport class WorkspaceVisualization {\n  bb: flatbuffers.ByteBuffer|null = null;\n```. The change should now reflect what we want with this PR :-)\nMeaning, we only support relative paths, anything else is senseless (and anyway a no-go)\n```typescript\ninclude \"/executionGraph/serialization/schemas/DataTypes.fbs\";\ninclude \"executionGraph/serialization/schemas/SocketLink.fbs\";\nnamespace executionGraph.serialization;\ntypescript\n// automatically generated by the FlatBuffers compiler, do not modify\nimport * as NS578127928888604879 from \".//executionGraph/serialization/schemas/SocketLink_generated\";\nimport * as NS14965100849554803941 from \"./executionGraph/serialization/schemas/DataTypes_generated\";\n/*\n * @constructor\n /\nexport namespace executionGraph.serialization{\nexport class WorkspaceVisualization {\n  bb: flatbuffers.ByteBuffer|null = null;\n``. a flag, can be opt-in, and maybe made deprecated in future releases, why not?. Or spitting out a list of regexes, the user can use to clean up his code, for backward compatibility... ;-)\n. Can you point me to the location where these names are generated, should be a simple matter for a PR and an additional flag. No files, are formatted with.clang-formatis this actually used?? . Ah nice :), didnt know about clang-format. Made the Review changes.. Should we run clang-format on the file? Would clean up quite good.. Also happy with such an approach. without std::tuple. will change it.... thx, I couldnt somehow use clang-format -> kind of messes up everything, when I apply it... (i use clang 6.0 maybe thats why.... Do I understand correctly, thatslice` is owning the memory (refcount) and grpc_slice_unref destructs it...?. Ok, relative paths should be prepended with \"./\". I will revert this! Maybe directly a legacy requirement from JS.... The if here is to test if its an absolute path so we do not prefix with \"./\" as that was the old behavior.\nOnly relative paths should be prefixed, in my opinion... (thats why this AbsPath, test)\n. Not that I ve seen, I ve seen some error on travis when I did not use \"./\" for relative paths.\nBut how can we make this work for windows?\n. Hm... I thought (if think I ve read that somewhere) internally the paths are posix? So they are not.\n. Should we only support relative paths in include statements in .fbs files?. Thats my hope to, so we leave the behavior as it was! I revert it \ud83d\udc4d . I dont know for sure! :) But we can skip this by just leaving it as it was and only support relative paths =) any thing else as absolut paths is senseless\n. Sure, I also include missing flags whcih are not in the doc. 3min. Oh, yes of course.. ",
    "mfarrugi": "Just wanted to chime in and say we choose protobuf for a project that wasn't performance sensitive because --gen-object-api isn't supported for java/python (ie. https://github.com/google/flatbuffers/issues/4769). \nIn general, I think having the simpler API would make flatbuffers very close to being strictly better than protobuf.. ",
    "rouzier": "Thanks for considering my pull request.\nThe reason for the pull request is to have better compatibility with JSON and not lose \"true\" and \"false\".. Updated everything let me know if there is anything else needed.. Done!. That would also require to add a new type TYPE_VECTOR_BOOL.\nIs that something that you think should be done for completeness.\nIf so I will create a pull request over the weekend.. Since there is no TYPE_VECTOR_BOOL there is no need to change IsTypedVectorElementType.\nSince IsTypedVectorElementType(flexbuffers::TYPE_BOOL) is false at the moment.\nUnless I misunderstand the purpose of the function.\nWhich I assume is to determine if a type can be a turned into a typed vector.\n. The only issue I see with that you can lose semantic meaning.\nA very contrived example.\njson\n{ \n  \"question\" : \"Is flex awesome\", \n  \"answer\"   : \"true\", \n  \"choices\"  : [\"true\", \"false\", \"unsure\"]\n}\nSomeone would expect the type of answer to always be a string by converting a string during parsing to a boolean you that meaning.\nSo which direction would you want me to go?. ",
    "MaximilienRedemption": "Just tried it, it successfully created the monster.cs.\nLooks like it doesn't like the .txt extension, it runs fine with my schema if I use .fbs extension. There was no mention of this change on the release.\n@alexames Thanks you :). ",
    "jlafon": "@rw I would be happy to.. ",
    "seifertm": "Empty string fields return None as of #4733.\nHowever, the OP also mentioned that string fields return a bytes object in Python. This basically forces the user to call `.decode(')' for every string field.\nTake the following schema as an example:\ntable MyObject {\n  id:string;\n  name:string;\n}\nThe resulting generated code would be used like this:\no = MyObject.MyObject.GetRootAsExample(buffer, 0)\nprint(o.Id().decode('utf-8'))\nprint(o.Name().decode('utf-8'))\nThis is cumbersome and was one of the reasons I started writing wrapper types around generated Python classes.\nThe documentation states that the Flatbuffers type string should hold either UTF-8 or ASCII encoded data and that other encodings should use [byte] or [ubyte]. The serialization helper flatbuffers.builder.CreateString already encodes strings as UTF-8 by default.\nI suggest that string fields should return an object of type str in Python3 as opposed to bytes by calling .decode('utf-8') somewhere under the hood. This way, generated Python classes behaved more predictable with regards to string fields:\no = MyObject.MyObject.GetRootAsExample(buffer, 0)\nprint(o.Id())\nprint(o.Name())\nHowever, I don't know whether this can cause issues for Python2.. Generally, this is a good idea. For Python specifically I feel that a typed getter would not be an improvement.\nI had another thought, but please correct me if understanding is wrong:\n1. The difference between a string field and a [ubyte] field is only the semantics (utf-8/ascii characters vs arbitrary binary).  (Due to this, I expected string fields to return a Python3 str, instead of bytes)\n2. They both generate the same byte sequence on the wire (a vector of bytes). Only the generated code differs.\nIf so, the receiver could specify the type of the field differently, depending on whether he wants to treat the value as bytes or as a string. Would this solve the performance concerns?. I signed it!. Thanks for the input!\nidl_gen_python defined a lot of static functions. I moved these into the PythonGenerator class. This allowed me to make the keywords list local to PythonGenerator instead of polluting the namespace.\nAll definition names are now escaped, if they match a keyword, not only enum values.\nPR was rebased to latest master.\nThe solution is now pretty much the same as the one in idl_gen_cpp. Any idea what I can do about the failed build on TravisCI?. ",
    "kbrose": "@aardappel It could probably be made optional, although it's not as easy as just adding a flag to the generator since I've put import numpy statements in the flatbuffers python package as well. The import statement could be moved into the method so that the ImportError: No module named numpy would be delayed until the user tried to call the obviously numpy-dependent method <vector name>AsNumpy(), but that would have performance implications as well. I'll think on it more.. Yes, that could be done. It would reduce readability a little as related methods wouldn't be grouped together as much. Another possibility is to find if numpy exists using the imp module and use that to drive the logic.. Also, it looks like python tests aren't being run in the CI, or maybe I'm just not understanding things correctly. Do you have any insight into that?. I'm rebasing this into something more manageable right now. Using this gist as profile_python.py, I find that building up a full array using numpy is much faster than building up a full array element by element as expected. In fact, building the full vector using numpy is O(1), since there is no copying going on, whereas building the list by copying is of course O(n).\n<removed>:~/flatbuffers/tests$ python profile_python.py 10\n  secs to retrieve full inventory 100 times, numpy: 0.00116419792175\n  secs to retrieve full inventory 100 times, list : 0.00986695289612\n  list time / numpy time : 8.47532254761\n<removed>:~/flatbuffers/tests$ python profile_python.py 100\n  secs to retrieve full inventory 100 times, numpy: 0.00113606452942\n  secs to retrieve full inventory 100 times, list : 0.0906109809875\n  list time / numpy time : 79.758656873\n<removed>:~/flatbuffers/tests$ python profile_python.py 1000\n  secs to retrieve full inventory 100 times, numpy: 0.00117087364197\n  secs to retrieve full inventory 100 times, list : 0.915771961212\n  list time / numpy time : 782.127061698\n<removed>:~/flatbuffers/tests$ python profile_python.py 10000\n  secs to retrieve full inventory 100 times, numpy: 0.00115704536438\n  secs to retrieve full inventory 100 times, list : 9.00237393379\n  list time / numpy time : 7780.48485473\n<removed>:~/flatbuffers/tests$ python profile_python.py 100000\n  secs to retrieve full inventory 100 times, numpy: 0.00131392478943\n  secs to retrieve full inventory 100 times, list : 90.3762190342\n  list time / numpy time : 68783.4035565\nI also updated the profiling code to access only a single element out of the vector, and compare Monster.Inventory(5) to Monster.InventoryAsNumpy()[5]. These results found that the numpy method is about 1.4x slower. So if you're only looking to use a single element out of a vector then the existing method is still faster. However, in the important use case of needing to unpack nested flatbuffers, the numpy method will be much, much faster.\nIt's also convenient to be able to do all the nice slicing Python programmers know and love, things like Monster.InventoryAsNumpy()[5:], Monster.InventoryAsNumpy()[::-1], Monster.InventoryAsNumpy()[0:-10:4], etc.. Thanks @aardappel!\nI wanted to also note that since this is a view into the data, modifying the numpy array will modify the data in place. This should help speed up some writing processes as well (#4144):\n```python\n\n\n\ninv = monster.InventoryAsNumpy()\ninv[10]\n114\ninv[10] = 234\nmonster.Inventory(10)\n234\ninv[:10]\narray([124, 123, 122, 121, 120, 160, 118, 117, 116, 115], dtype=uint8)\ninv[:10] = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[monster.Inventory(i) for i in range(10)]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n````. @rw I'll work on some more tests. I was hoping I'd be able to avoid changing any schemas so it wouldn't effect all the generated code but it looks like that might be necessary?\n\n\n\n@ahundt At least in the current implementation of this PR, there is no extra flatc flag, the *AsNumpy() methods are always generated, but if you try to use them without having numpy installed you get an exception. (If you do have numpy installed, then it will work as expected.). I don't understand. There is no way to load an entire vector in the currently generated API. You have to do something like\npython\n[fb_object.VectorName(i) for i in range(fb_object.VectorNameLength())]\nThere is no API like fb_object.VectorNameAsList(), or at least I don't think so. Can you clarify what current method you would like to be modified to returning the numpy array?. People have discussed Cython before. I've never used it. If you want to look at jumping off points to start contributing that then you could start at:\n284 Python: Support Cython build\n304 Python: Speedup with cython extension\n(Also linked to in the first comment of this PR.). @rw added test for a vector of longs containing the values\npython\n    b.PrependInt64(100000000)\n    b.PrependInt64(1000000)\n    b.PrependInt64(10000)\n    b.PrependInt64(100)\n    b.PrependInt64(1)\nwhich have passed on appveyor.. Sounds reasonable enough, but I will admit that my experience with the testing infrastructure has been frustrating.\n\nmaking changes to the (global) testing schema modifies a whole bunch of files that I have no business checking for correctness\nbut there are definitely drawbacks to having a schema for each individual language!\n\n\nthe python code was not even being tested in the CI before this PR\nit is unclear where the gold standard for example data is, you would think it's in monsterdata_test.golden but that has different data from monsterdata_test.json (inventory is different). The Python code rebuilds the data in the testing code to match the JSON file, and appears to test it for correctness against yet another file, monsterdata_test.mon.\n\nThis has been my biggest pain point while contributing. I think improving the quality of life around test writing in the different languages could really help people be more \"aggressive when writing tests\". I will try to reflect more and submit these issues formally when I'm done with the PR.. @rw I added tests for a vector of type float64. I actually think it makes sense for the test schema to have multi-byte vectors -- I'm sure other languages could benefit from tests with those types as well. It does make the diff of this PR a bit hard to follow, but I could always rebase to improve readability. Something like break into 3 commits: python flatbuffers library changes, code generator changes, and tests (I initially had this, but then added a bunch of additional testing commits).\nThoughts?. @rw if you want to add python2.7 support it should be fairly straightforward to do using conda, but it would probably ~double the appveyor runtime. Not sure if the tradeoff is worth it.. Relevant: https://docs.travis-ci.com/user/deployment/pypi/. @aardappel Sorry to see that happen. It doesn't feel good when the CI fails and it's the fault of the tests and not the package code. Unfortunately, I don't see a good way to test the numpy methods without pulling numpy from the internet, but relying on the internet during tests will make them more undeterminstic by necessity. Considering the python code was not being tested by the CI servers at all prior to this PR, you may be comfortable with not testing the numpy functions during CI tests until a more robust testing is implemented.\nI'll summarize the paths forward I see:\n\nDo not test the numpy methods in the CI (remove lines https://github.com/google/flatbuffers/blob/master/appveyor.yml#L65-L67)\nMake the numpy testing a separate appveyor run, and mark those runs as allowed failures.\nReduce the number of dependencies installed by running conda install numpy --yes --no-deps, but beware that there's usually a reason the dependencies are installed.... I would guess conda says that numpy depends on MKL because that usually speeds things up significantly, and MKL may have been looser with their deps than numpy. But that's just conjecture.\n\nI can try and create a PR when I get home in the evening, but it is equivalent to just deleting (or commenting) those three lines in appveyor.yml I highlighted above.. Hello @aardappel I hate to be that person pestering the over-worked OSS maintainer, but do you have an idea of when the next release of flatbuffers will be? I'd like to be able to target an actual release number when managing dependencies but also want to use this feature.. Greetings! Thank you so much @aardappel for releasing the new version and congrats on 1.8!\n@rw Any way we could publish the new version of the python code to pypi? I can try to help out with any pain points if you let me know what issues exist with that publication process.. Hi @tokotchd! My work was solely about the reading of flatbuffers (I had no need to write in my use case), so unfortunately I will be unable to provide support on this work, but I wish you luck!\nI will say that there may be a way to add the bytes in bulk with something like\nbuilder.Bytes[builder.head : (builder.head + my_array.nbytes)] = my_array.tobytes()\nmaking sure everything is in the correct byte order (little endian) and the builder has been correctly initialized (with StartVector I think?).\nIt would be great if this works out and that could be incorporated directly into the python API!. Glad to hear it @tokotchd !\nDo you think you could add an example of that to the documentation? @aardappel is that something you think would be useful?. @Amit072 Do you think you could add an example of that to the documentation?. cc @mikeholler who wrote pypi publishing PR\nI believe the only problem was making sure those who have the necessary pypi credentials are able to put them into the travis configuration (encrypted of course). I'm not familiar with that process, though, so can't add much.\nEDIT: This is the better link for tracking disucssion: #4516 . Yep - just force pushed change that should assert the correct error is raised if numpy does not exist, and I run the python tests twice (once w/o numpy, and once after numpy is installed).. This makes appveyor stop running as soon as it encounters an error. Useful as I'm developing to make appveyor churn through failed tests faster. I can remove this before merging if you want.. Sorry, I don't understand. Do you want error in a function to reduce duplicated code in number_types.py and encode.py?. Whoops nevermind it does nothing :P I'll remove it.. Certainly not impossible, I could add it to the compat.py file and import it in both places. I'll go ahead and do that.. I updated to have a custom exception NumpyRequiredForThisFeature that is defined in compat.py, but I decided I didn't actually like putting a function there that throws the error. It's only repeated twice, and would make debugging slightly harder (one extra layer of indirection in the stack trace) if the error was raised.. Is this docstring correct?. ",
    "mikeholler": "@aardappel how often do you publish new versions to pypi? Our engineering team here is interested in using this functionality but would prefer to do so at on official release. We'd love to know what the timeline to next release is.. @aardappel we'd love to use the code provided by this PR specifically. What about it prevents it from being included in 1.7 instead of 1.8?. Yeah, I was looking into a Travis solution. The one thing I'm having a hard time with is figuring out how Travis knows where to find the setup.py file or the sdist tar.gz. All the projects I've seen have setup.py in the directory root, and I don't see a way to configure Travis to look in a subdirectory like this project has.\nI'll keep looking into it, but if anyone has guidance on doing this that'd be very appreciated.\n\nFrom: Andrew Hundt notifications@github.com\nSent: Thursday, November 23, 2017 12:07:51 PM\nTo: google/flatbuffers\nCc: Mike Holler; Mention\nSubject: Re: [google/flatbuffers] [Python] (scalar) vector reading speedup via numpy (#4390)\nLooks like travis might be able to do that:\nhttps://docs.travis-ci.com/user/deployment/pypi/\nhttp://www.robinandeer.com/blog/2016/09/01/automated-pypi-releases/\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/google/flatbuffers/pull/4390#issuecomment-346677074, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ABHDULU3Ckmi4pBpcZy7OY_6iJgyePNOks5s5bR3gaJpZM4Oelv7.\n. Yeah, I think keeping setup.py out of the root is a good idea (although I do find it odd that the Java pom is in the root). I'm not so sure about having a separate repo. I could see a few disadvantages that could (for example) make testing harder.\nI still have hope for a solution that uses Travis but doesn't change the structure of this repo much. There has got to be a way to support projects with setup.py not in the root. If anyone finds something let me know. I fiddled with Travis for about an hour on Wednesday and I should be able to find more time soon.\nFrom: Robert\nSent: Saturday, November 25, 02:57\nSubject: Re: [google/flatbuffers] [Python] (scalar) vector reading speedup via numpy (#4390)\nTo: google/flatbuffers\nCc: Mike Holler, Mention\nIdeally we would not put setup.py into the root, because the semantics don't make sense: we only want the contents of the python directory to be part of the python package.\nWe could have a separate tracking repo for this purpose.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/google/flatbuffers/pull/4390#issuecomment-346927983, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ABHDUPSRrudnrUlJ3lq4nZf_rQvdSSXtks5s59aEgaJpZM4Oelv7.\n. I provided more info on the subdirectory issue in #4507. We might consider moving further discussion there for PyPI publication. . Hey @rw thanks for responding! I'm playing around with Travis on my fork and am making some progress towards publication when a new tag is pushed. I found a blocker for an issue I logged here: https://github.com/travis-ci/travis-ci/issues/8801\nI'll give them a chance to respond, and if they don't get back in a reasonable amount of time I'll try to hack at it myself. . Resolves #4507.\nNote, this build will likely succeed here, but fail when Travis notices master has changed (unless the changes I made in the description above have been made).. I signed it!. I had to add a few tweak commits since I decided to PyPI with my credentials (I changed the package name to test). There were a few issues that I've fixed but I think everything's 100% working and ready for review now. I'm interested to see what you think of it @rw . Awesome, glad you like it. Let me know if you have any troubles and I'll try to assist.. Hey Robert, this is already happening essentially. Deployment only happens on git tag or when a merge into master is made. You already only make got tags from commits in master, and things only get in master if you and the other maintainers approve PRs, and you only approve PRs when CI passes. \nHow does that square with you? Of course it's possible to add, but the way tests are being done here makes it a bit difficult and personally, I don't think it's necessary. . *git tags\nSorry for the spelling, I'm on mobile and can't edit and apparently can't proofread before I post ;) . @rw I made the requested change. Just wanted to stress this part one more time:\n\nIt's also recommended that the owner of flatbuffers use a separate account\nin the unlikely event that the environment variable somehow becomes\ncompromised. Again, this is very unlikely, since the environment\nvariable is only set for \"safe\" builds approved by maintainers (not on\nrandom pull requests).. Thanks for merging, everyone! Any chance we can retroactively publish v1.8.0? The next master build would be good, too. Do you have to manually approve after publication in PyPI? I don't see it on PyPI.. Looks like the master build merging this was supposed to produced had an error on deploy. Are your settings correct?\n\nhttps://travis-ci.org/google/flatbuffers/jobs/316632701. I had passing Travis jobs with this MR on my Fork. The most likely culprit here is configuration. Travis is finicky about passwords having certain special characters. Maybe consider changing your password to a longer (so similar entropy) password without symbols.. @rw maybe the thing to do here is to play with this code on a separate fork, like I did. Once you're able to have it working and configured, you could merge it in. If it's anything like my experience, you should be able to get things right within 30 minutes or so.\nI wish I could assist more, but with secrets and environment variables in play that I'm not privvy to it makes it difficult for me to troubleshoot.. Is there a reason why sizedByteArray(int, int) is public but maybe this should not be? I personally can't see any use to sizedByteArray(int, int) or sizedInputStream(int, int) that wouldn't be covered by sizedByteArray() and sizedInputStream() respectively. Perhaps I'm missing a use case that I haven't stumbled on yet. Was it intentional to make sizedByteArray(int, int) public?. ",
    "Amit072": "Is there any equivalent API for writing the large byte-arrays into builder without using PrependByte() ?. @tokotchd  I think direct copy of data to Bytes buffer using below approach is not safe.\n[1] builder.Bytes[builder.head : (builder.head + len(bytesOfImage))] = bytesOfImage\nPrependbyte() internally calls Prep() which may take decision to enlarge the Bytes buffer with padding etc, and will update head if it decide to regrow the Byte buffer.\nI think it is problematic to directly use approach [1] if we have followed up write operation  to builder using Prependbyte. \nAnd also, what if your image size is large than allotted Bytes array, you may overwrite the allotted byte buffer in builder without taking care of alignment, head etc.\nI'm also looking for this optimization, PrependByte is too slow for large buffers. . Thanks @tokotchd, I am able to make it working with below change. We just need to seek the header to correct location before writing to Bytes array.\n bytesOfImage = testImage.tobytes()\n Image.ImageStartDataVector(builder, len(bytesOfImage))\n #We need to seek the header to correct place before writing into Bytes array.\n builder.head = builder.head - len(bytesOfImage)\n\n builder.Bytes[builder.head : (builder.head + len(bytesOfImage))] = bytesOfImage\n data = builder.EndVector(len(bytesOfImage))\n\nYou can check my answer @stackoverflow \nhttps://stackoverflow.com/questions/49239293/fast-python-serialize-to-flatbuffer?answertab=active#tab-top . ",
    "jitheshtr": "Thanks @aardappel & @stewartmiles \ud83d\udc4d \n\nAny reason you use the lambda version btw, instead of just passing the vector as-is?\n\nI was converting from protobuf's RepeatedPtrField<T> to flatbuffers [T'] using lambda and hit this issue. Just created a sample using std::vector for this submission.\n. ",
    "anpol": "This should be closed because of 5d9930a and 89711c9. Well, you already did the reformatting and now you could do it again without being afraid of messing the history up.\nSome of the possible future refactorings could include:\n\nuse range-based for loops\nadd override modifiers\nchange formatting of __#if into #__if and remove clang-format comments.\n. \n",
    "vrachels": "signed. sorry, we've moved on and I don't really have time to support this push at the moment.  Let me know if you need anything clarified, or I can just kill the pull request if you're not interested.  I had to add a few additional local features, so we'll be supporting our own fork for a while, anyway.   Thanks!. hm.  I believe this actually made it work with flatbuffers as an npm module, rather than as a local file dependency.  This is otherwise irrelevant to the main changes, so take it how you like.. I agree.  the original diff was basically a -file +file.  I had to go line by line to to get it resolved to this point.  (original code source was from pre-google-formatting, used more legible for loops, strstream, etc)\nFeel free to apply your own linter/formatter.. ",
    "41204486b": "In sample_binary.cpp, I wanna create \"monster.json\". So i do this:\nstd::string jsongen;\nflatbuffers::Parser parser;\nGenerateText(parser, builder.GetBufferPointer(), &jsongen);\nprintf(\"%s\", jsongen.c_str());\nBut it's errror. What's wrong? \ni'm super newbie. Maybe I got a stupid question.\n. Thanks very much, I got it. . ",
    "shivendra14": "I am still facing a similar issue.\nI don't understand how forward_declare works with flat buffer.\n//in c1.fbs\ninclude \"c2.fbs\"\ntable C1\n{\nc2 : C2;\n}\n//in c2.fbs \ninclude \"c1.fbs\"\ntable C2\n{\nc1: C1;\n}\ncompiles fine with flatcc, but causes a cyclic dependency in generated cpp headers!\nhow to forward declare C2 in c1.fbs, and remove the call: include \"c2.fbs\". @aardappel: do we have any true solution for this now?   the workaround of writing a single schema is making the schema huge and difficult to maintain.\nregarding forward declaring everything, that doesn't seem to work as currently only structs in a file are forward declared in it, and structs of other included files don't get declared!\nwhat  i propose as a temp. solution is to have forward declarations before header import restriction, so that even in case of cycles, all forward declarations are there in a file.\nthis will solve the problem where we are just referring classes from different header.. eg.:\nCurrently we have:\nA.h\npragma once\ninclude B.h;\nclass AC; // forward declarations\nBC obj;\nclass AC {};\nB.h\npragma once  // include restrictions\ninclude A.h;\nclass BC;   // forward declarations\nAC obj;\nclass BC {};\nnow when A.h includes are expanded:\nits pragma is defined and thus when it expands #include B.h;  compilation fails to find AC.obj.\nwith proposed solution, we will have:\nA.h\nclass AC; // forward declarations moved at top\npragma once\ninclude B.h;\nBC obj;\nclass AC {};\nB.h\nclass BC;   // forward declarations\npragma once  // include restrictions\ninclude A.h;\nAC obj;\nclass BC {};\nnow when A.h includes are expanded:\nAC is already forward declared, and when #include B.h is is expanded, even though pragma once restricts the implementation, AC obj is created using forward declaration..\nwhole jist of solution is that we don't have problem when we forward declare multiple times and that we can leverage to solve this problem partially.. or atleast can we have --gen-onefile option for cpp also.. @aardappel : I am on the latest release i.e. 1.8.0. But will be glad to know if the issue is fixed after 1.8.0.\nWith --no-includes, the problem is still there, and it fails even early at compile time. Due to lack of information about classes referred from other header.\nWhat I observed --no-includes does is simply avoid including other headers, but how will the classes in one header know about classes defined in other headers? I currently don't see any neat solution to this problem myself, and that is why was requesting the hack to partially fix the issue. (A better solution is always welcomed. Maybe forward-declarations in schema?)\nOtherwise I will end up writing all my code in a single multi-thousand-line .fbs file :( . I understand with --no-includes client is expected to manage the includes, but the problem of cyclic referencing will still be there. Are you suggesting client needs to forward-declare all the structures that are defined in generated-headers too for fixing the cross reference?\n//file:   A.fbs\ninclude \"ABC.fbs\";\ntable LayerTest\n{\n    layer:GroupTest;\n}\n// file: ABC.fbs\ninclude \"A.fbs\";\ntable GroupTest\n{\n    group:LayerTest;\n}\n//use_case.cpp\nstruct LayerTest; // I forward declared every table needed before including the needed header.\nstruct GroupTest; // because without these lines no matter in which order I include A_generated.h or ABC_generated.h the cylcle will still be there.\ninclude \"A_generated.h\"\nAbove one is the solution we are thinking for getting away with cyclic reference problem. To remove this manual need of forward-declaring all the structs we are now thinking of a pre-build script that goes through all schema files, parse and find all table names.. and then dump them in Forward_declarations.h which can be included before any generated include.\nDoes the above hack seem reasonable or you see any other problem with this .. or a better solution that can be built in flatbuffers itself.\n. I am too facing issues transforming fb binary into json. Unions having structs are not allowed!\nPlease add this support.. @kageKirin why was it required, when. You already chose output_default_scalars? . Its good to hear about the upcoming release :)  Hope to see it soon. \n3rd point was for just in case an info is out :P \nLong live flatbuffers \ud83d\udc4d  . I tried using boost::memoryMap and it worked fine for reading buffer. \nFor write I am planning to try Custom allocator as you suggested, but the problem is I don't know how much to reserve for mmap, as file created by clients can be from few bytes to 100s of MBs.  \nI don't think I can add more memory to the pool contiguously as the binary requirement grows. Is there any suggestion on what could be the best to reserve by allocator for mmap? Some heuristic?\nI am also doubtful about use of mmap in general on iOS. iOS doesn't Purge unused pages, and for a full file read kind of scenario I think mmap will cause a crash rather than benefiting by incremental memory read.  Any suggestions on using mmap to read fb on iOS?. @aardappel Thanks for inof on verifier. I will look how to use that, and maybe it will help me in basic sanity of document.\nfor comparison of document can this issue be considered as afeature request and that code be added?\nOr is  there any other forum where I can add a feature request?. I see only in memory comparison  is possible with flatbuffer generated binaries. Comparing each object one by one.\nVerifier is good to solve my 1st problem to some extent by providing a safe read.\nBut having something similar for 2nd case is what I am still looking for in the flatbuffer library. An in-memory Comparator support is what I am looking for from this issue. . Okay this worked for me. :)\nI moved a step further to use GenertateText API for generating json at runtime. There is a new problem I face now.\nGenerateText expects path of the folder where all the imports are present. This is not possible for me, as the application where I am using flatbuffers cannot keep schema with application binaries.\nIs there a solution to this problem, where I don't need to expose schema files with app binaries?. ```\nunion Foo\n{\n    X, Y, Z\n}\ntable FooWrapper // unnecessary table I need to create to wrap this union because otherwise binary won't convert to JSON! but that is a separate issue needed to be fixed.\n{\n    object: Foo;\n}\ntable FooWrapperArray\n{\n    elements:[FooWrapper];// need to store a vector of table which can't be a struct! Also I know before hand how many Foo needs to be created so just want to reserve using unitialized vector and memcopy later, rather than creating a std::vector of offsets first and do 1-by-1 copy of offsets in flatbuffer's vector.\n}\n```\n. thanks for detailed info. \nDidn't finalize on vector of offsets yet. Was evaluating a generic guideline, as there are couple of vectors to be made.\nvector of structs with CreateUnitializedVector came out to be better than vector of table offsets with CreateVector.. \ncan try per-allocating std::vector to save new/ delete cost.. but copy cost still needs to be paid.. . I kept type to save from going through all fields. \nyou mean same indirection if i consider case-1 and case-3..  case-2 has an extra indirection due to that intermediate table,, isn't it.\nagreed on 3 vtables we have in XYZwrapper of case-3, for each combination..\nbut in case-2, XYZwrapper has 1 vtable.. and then the union I suppose again has 1-vtable to tell where X/Y or Z is.. so there is 1 extra vtable we are talking about in case-3..\n(since vtables are shared, I think for large amount of data an extra vtable should not be problem.. but if data is small and no. of elements in union are large, I can see how no. of vtables may increase)\nor the unions are stored some other way with no indirection/ vtable?\nPS: want to contribute, but want to have the basics first :). thanks.\nhttps://groups.google.com/forum/#!topic/flatbuffers/JtDGnBPx9is. Is the limit of 1 million on max tables and 64 on max depth just random or there was some logic while choosing these defaults.\nPS: I have passed them UINT_MAX for both now. (seems a better default). I signed it. \nand no clue how to fix that appveyor build. Any suggestions?. \n. \n. Last PR automatically got closed because I reset --hard my branch to an earlier commit. This was needed because I was not able to correct the wrong user-name in the pushed commit. (Now I see we can do stuff like --amend and force push)!\nRegarding comments:\nDon't add another struct to the schema, there's already a vector of structs in the monster tests.\nThat vector of structs was already used to test CreateVectorOfStructs API.. didn't wanted to overwrite that testcase. Without schema change for testing the new API another monster had to be created, with just this vector of structs set with new API.. seems complex.\nAlso please use the same style of code as the surrounding style (google C++ guidelines)\ntried to match.. plz mark in review if you find any specific discrepancy.. Any update on this PR.. does it seem good now?. - --defaults-json Output fields whose value is the default when\\n\" \" writing JSON\\n\" \nThis option enables to write default fields in JSON which are already there in flatbuffer binary. \nBut the issue raised is for an option which enables writing fields which are not there in JSON, but defaults for which are provided anyway by flatbuffers.\nRegarding JSON being converted back to flatbuffer binary, this should still work since defaults are not serialized and will be dropped again while json to flatbuffer conversion. \nThe problem I am facing here without the above change is in implementing a JSON parser over the flatbuffer generated json. Here while reading JSON I don't know if a  field holds any default value. But as said while reading flatbuffer I will get defaults.\nI may assume some defaults in JSON parser, for fields which are not present. But the problem is even severe if there are custom defaults in flatbuffer schema which a JSON parser may not know.\nHence a need of correct defaults in generated JSON is important, for a similar read experience from either flat buffer or json. . \"No, it also outputs fields not present in the binary.\"\nOkay.. then it will solve my problem.\nBut the comment mentioned in code is misleading then. It can be a bit more informative to convey the message.. For the last point, it is a tedious task to do at client end. What my schema design is:\nA flatbuffer holding a flexbuffer, which in turn may again hold a flatbuffer from same schema. Thus this can be nesting chain.\nRefernce:ToString having an optional param for parser, so that it can call generatetext itself would be good for my need. Does this solution looks good, or any other easy solution you would recommend.\n . Flexbuffers is good and suggested for implementing a map/ dictionary. This map may hold various basic types but may also contain complex types ( which also need a map) for which flat buffers is more suitable. Hope this clears a bit more on requirement.\nWithout a good way of serializing this nested chain in json, I will unfortunately have to keep my entire design including a map to be implemented in flat buffers only :( \n. @vglavnyy vector of unions is supported in flatbuffers, but its conversion to json is not yet present. There'e is no problem I am facing in writing the flatbuffers binary with such schema. Problem is when I try to convert a flatbuffer binary from such schema to JSON using GenerateText api. Here a crash will be seen. \n1. Here's the schema:\nunion AB { A, B}\ntable Holder { ab : [AB] } \n\nI would expect an error code or an exception for the evil I have done. This way I can do some error handling, like maybe skip the read or show some better message to my app users.\n. @vglavnyy : Thanks. I missed that Parser returns a status bool! Following two lines will partially solve my problem :)\nauto parser_done = parser.Parse(test_2_schema);\nTEST_OUTPUT_LINE(\"%s\", parser.error_.c_str());\n\n\nAnd have got assert at the idl_gen_text.cpp:273:\n\nI don't get these since in our release builds we have asserts disabled.\n\nYou can predefine FLATBUFFERS_ASSERT globally for each source code file (all compilers have this feature) and catch an exception at latter stage.\n\nThis seem a workaround but should be fine, I can try this in my client code:\n#define FLATBUFFERS_ASSERT(Cond) if (!Cond) throw;\nBut the question is why can't be this or something similar be there in flatbuffers library itself? . and if use typed=false, fixed=true,  there is no issue while writing, but while reading I get an invalid type for the reference = 41. If using the specialized constructor is faster, lets not add the template api with lambda then. It will unnecessarily promote using the CreateVector route.\nwill add a PR for check on typed=flase, fixed=true.\nI would even prefer, if you permit to add an assert for typed=true, fixed = true.  Let the library provide only one route of doing things and the one which is most performant.. Reading more of code, it seems this is as designed.\nBut I don't see how and when to use this fixedTypedVector. for such a small size vector, I could simply store the elements independently!\nlike for a 2-D data, i.e. x and y coordinate, I can save 2 doubles..  why create a vector having x,y..\nor you suggest this for strong types, without the overhead of typed vector.. \nwould be curious on what exactly we are saving when using fixedTyped instead of typed vector for creating this 2-D point).. @aardappel: changed the order again.. https://github.com/google/flatbuffers/issues/4816. As I understand, non-scalars if not written, will require to be nullptr checked before reading. These checks are overhead during reading. A simple solution can be to mandate non-scalars fields using Required during write.\nScalars on the other hand already provides a default value during read in case they are not written. This removes the need of mandating scalars during write.\nStill if you want everything to be required in your schema, just wrap the basic scalars in struct, and use these structs with Required attribute. (There won't be overhead of structs as they are inlined).\nFor e.g. \n```\nstruct Int\n{\ndata: int;\n}\ntable MyTable\n{\nintTest: Int (Required);\n}\n```\n. @aardappel :  does this following change looks fine?\nelse if (IsBlob()) {\n      auto blob = AsBlob();\n      std::string str(reinterpret_cast<const char*>(blob.data()), blob.size());\n      s +=str;\n    }\n. Is this Java specific? Anything similar for cpp?. okay.. just wanted to be consistent. but good to know about template use case. We should rename the api Add taking multiple arguments then, to something AddPair or something else.. i got into the problem because the blob i have in my application is of const char type and not const void type.. \nso while blindly using this add api for writing everything, i fell into this pitfall of writing blob wrongly..\nstruct Blob\n{\nconst char* data{};  size_t size{};\n}\nAdd(2);\nAdd(2.3);\nAdd(blob.data, blob.size); // and Bamm.. here i fell into issues!\nThis multi input Add api is creating confusion at client end \n. Ok will do that.. but IsAnyVector will still be at the end as it is using other apis.\nSo new order.\nIsVector\nIsTypedVector\nIsFixedTypedVector\nIsAnyVector\nSeems fine?. don't know.. not useful for me at this time. removing it completely. can add that separately if clients need it.. Thanks.. updated the new changes.\nJust forgot about this PR, causing such a delay!. @aardappel fixed the formatting case. I actually didn't correctly understood your comment that's why had this alternate fix.. oh I see now. We can do either way. Will submit a patch with suggested changes. Thanks :). ",
    "Arthur-qi": "OK\uff0cThank you~!,\nLua is a very popular language in game development\uff0ctrust me~\uff01. He said flatbuffers not support typescript\u3002Do you want to support typescript\uff1f. 253056965 said  flatbuffers.js is not typescript format. So, flatbuffers.js no have intellisense in VS.\n. @253056965 \u4f60\u88c5typings\u4e86\u5417\uff1f\u6ca1\u88c5\u88c5\u4e00\u4e2a\uff0c\u8fd9\u4e1c\u897f\u53ef\u4ee5\u63d0\u4f9b\u667a\u80fd\u611f\u77e5\u3002\nhttps://github.com/typings/typings \u8fd9\u91cc\u3002\nthanks krojew \u3002\nthanks aardappel\u3002. Ok\uff0cthanks~\uff01. ",
    "ll-antn": "Hi guys,\nI see this issue is still open... Did you decided not to fix it? Solution is still one that @llchan suggested?. Yes, here is test case that crashes on ~Parser():\nvoid InvalidNestedFlatbufferTest() {\n  std::string schemafile;\n  TEST_EQ(flatbuffers::LoadFile(\n              (test_data_path + \"monster_test.fbs\")\n                  .c_str(),\n              false, &schemafile),\n          true);\n  auto include_test_path =\n      flatbuffers::ConCatPathFileName(test_data_path, \"include_test\");\n  const char *include_directories[] = { test_data_path.c_str(),\n                                        include_test_path.c_str(), nullptr };\n  flatbuffers::Parser parser1;\n  TEST_EQ(parser1.Parse(schemafile.c_str(), include_directories), true);\n  TEST_EQ(parser1.Parse(\"{ testnestedflatbuffer: { color: \\\"NONEXISTS\\\"}}\"),\n          false);\n}. Fixed in PR #5184 . > As a consequence, the parser should return false, right?\nCorrect. But I added required fields to emphasize that it's enum value that makes flatbuffer invalid\n\nThis second case may be added to the monsterdata_test.json file if all languages have the support of nested buffers.\n\nI tried to add it to monster_test.golden (.json seems not to be used it test.cpp). But it won't pass in case when we use binary schema (.bfbs)\n. @vglavnyy Done. Sorry for the delay. ",
    "norman784": "Cool, didn't test it yet, but found a voxel engine that uses one of the proposed solution, so it must work\n```c\nBlockType Chunk::GetBlockType(int x, int y, int z)\n{\n    return m_blockType[x + y * CHUNK_SIZE + z * CHUNK_SIZE_SQUARED];\n}\nvoid Chunk::SetBlockType(int x, int y, int z, BlockType blockType)\n{\n    m_blockType[x + y * CHUNK_SIZE + z * CHUNK_SIZE_SQUARED] = blockType;\n}\n```\nfor further references check Chunk.cpp. ",
    "endorph-soft": "I signed it!. The .sh and .bat files produce different output, because they use different parameters to flatc (e.g. --cpp-ptr-type). Should they be the same?\nI'm happy to update one, but I need to know which one is correct. I assume the .sh file?. I have added the changed files. There were also changes to tests/namespace_test/namespace_test2_generated.ts, which are unrelated to my pull request. I omitted this file. It appears that some sort of generated ID changed.. It seems the build is failing because I updated generate_code.bat to reflect the shell script. As a result, it changes directory, which seems to break the subsequent build steps.\nShall I update the script to change back to the correct directory after it has finished, or is this a build system problem?. ",
    "shanshanpt": "I have solved the problem. But it is not what i want i think.\nFor example, I have a big vector, i dont want the vector space held by buf_,\nI just want buf_ offer a pointer to the memory. So, can flatbuffer do that ? Thanks ! . thanks .. ",
    "Velart": "Thank you for answering.\nI managed to get nested flatbuffer as flatbuffers::Table instance using:\nauto child_table_ptr = flatbuffers::GetAnyFieldAddressOf< flatbuffers::Table >(parent_table_ptr, field_ptr);\nUnfortunately, I could not get nested flexbuffer :(\nwhat i tried:\n1.\nauto vec_of_ubytes = parent_table->GetPointer< const flatbuffers::Vector< uint8_t > *>(field_ptr->offset());\nauto flex_ref = flexbuffers::GetRoot(vec_of_ubytes->Data(), vec_of_ubytes->size());\n2.\nauto vec_of_ubytes = flatbuffers::GetFieldV< uint8_t >(parent_table_ptr, field_ptr);\nauto flex_addr = parent_table_ptr->GetAddressOf(field_ptr->offset());\nauto flex_ref = flexbuffers::GetRoot(flex_addr, vec_of_ubytes->size());\n3.\nauto flex_ref_ptr = flatbuffers::GetAnyFieldAddressOf< flexbuffers::Reference >(root_table, field_ptr);\nPlease, can u be more specific about how to get nested flexbuffer as flexbuffers::Reference?\n. btw, I could not extract \"nested_flatbuffer\" or \"flex_buffer\" attributes using reflection api from monster_test.bfbs.\nThis code block is not working:\n    auto schema = reflection::GetSchema(bfbsfile);\n    auto schema_root_obj = schema->root_table();\n    auto fields = schema_root_obj->fields();\n    auto field_ptr = fields->LookupByKey(\"testnestedflatbuffer\");\n    //auto field_ptr = fields->LookupByKey(\"flex\");\n    std::pair nested_flatbuffer(false, \"\");\n    bool flexbuffer = false;\nauto attributes = field_ptr->attributes();\nif(attributes) {\n    for(flatbuffers::uoffset_t a = 0; a < attributes->size(); a++) {\n        auto attr_ptr = (*attributes)[a];\n        if(attr_ptr->key()->str() == \"nested_flatbuffer\")\n            nested_flatbuffer = std::make_pair(true, attr_ptr->value()->str());\n        else if(attr_ptr->key()->str() == \"flex_buffer\")\n            flexbuffer = true;\n    }\n}. Thank you for answering.\n\nI am talking about binary schema which i cloned from original rep from here:\nhttps://github.com/google/flatbuffers/blob/master/tests/monster_test.bfbs\nwhich is created using fbs from here:\nhttps://github.com/google/flatbuffers/blob/master/tests/monster_test.fbs\nI could not read any attributes for \"testnestedflatbuffer\" and \"flex\" fields using reflection api.\nFor flatbuffer and binary schema which are used in ReflectionTest:\nThis line is working for \"testnestedflatbuffer\" field, although u are saying it is not correct(extracted nested table contains readable fields):\nflatbuffers::GetAnyFieldAddressOf< flatbuffers::Table >(parent_table_ptr, field_ptr);\nAnd this code returns unusable flexbuffers::Reference object instance(although it can be read as simple vector of ubytes):\nauto vec_of_ubytes = flatbuffers::GetFieldV(parent_table, field_ptr);\nauto flex_ref = flexbuffers::GetRoot(vec_of_ubytes->Data(), vec_of_ubytes->size());\nTry it out for yourself, please.\n. ",
    "chrgrd": "1) Sure\n2) Done :) . I was finally able to use TypeScript flatbuffers in Angular 5 (CLI), here are the high level steps (code snippets in bold) :  \n\ncreate a \"hello world\" angular CLI application\ninstall the flatbuffer npm package (npm install flatbuffers). This will create a flatbuffer module under node_modules directory with the precious flatbuffer.js file (I didn't check how this version relates to the github flatbuffers one)  \n\ngenerate the Typescript from the .fbs file without the flatbuffer import (flatc --ts --no-fb-import)\nIn this specific example, I did put the *_generated.ts files under src/fbs\nHere is the specific fbs schema: \nnamespace NSfbs;\ntable TestStatus\n{\n    status : ushort;\n}\nroot_type TestStatus;\n\n\nin app.component.ts: \n\nimport the flatbuffer module (installed using npm), In this example below, 'flatbuffers' refers to node_modules/flatbuffers \nimport { flatbuffers } from 'flatbuffers'; // located in node_modules \n\nimport the generated TypeScript object: \n    import * as testStatus from '../fbs/TestStatus_generated';\n\n\nGenerate the flatbuffer \n  const builder = new flatbuffers.Builder();\n  testStatus.NSfbs.TestStatus.startTestStatus(builder);\n  testStatus.NSfbs.TestStatus.addStatus(builder, 7);\n  const serialized = testStatus.NSfbs.TestStatus.endTestStatus(builder);\n  // builder.finish(serialized); // without the file identifier\n  testStatus.NSfbs.TestStatus.finishTestStatusBuffer(builder, serialized); // To include the File identifier\n  const data = builder.asUint8Array();\n\n\n\n\nPain is temporary, pride is forever :-) . ",
    "Voultapher": "That is true if both variables live in the same scope.\nLet's assume std::array would behave this way.\nEdit better example:\n```cpp\nusing buf_t = std::array;\nbuf_t long_live_var;\nvoid on_user_request()\n{\n  // assume we reset long_live_var somewhere else\n  const auto user_input = std::move(long_live_var);\n}\n```\nAssume this is a long runnig task in a server.\nNotice how long_live_var does not go out of scope.\nThe destructor of user_input could now be triggered by a seemingly unrelated event. Making debugging harder.. Quoting the standard std::vectormove assignment:\n\nIn any case, all elements originally present in *this are either destroyed or replaced by elementwise move-assignment.. Just updated the example.. Read more about it here. Ok, my bad, not a memory leak per se. Still not great as it diverges from common behavior.. Even more about it. Ok, will do. Any reason, there is no move constructor and assignment for FlatBufferBuilder?. Fixed by https://github.com/google/flatbuffers/pull/4437.. It is protected.\nWhy call it release, it does not return any kind of detached buffer?\nreset is much closer to it's functionality.. reset as name isn't ideal. It technically releases any existing buffer. However it does not return it.\nSo it's really just empty assigning with assumed destruction.. Personally not fond of std::swap in move assignments.. \n",
    "amjabb": "Thanks for the response, is that function that looks up from an enum to string necessary to core functionality (building/reading)?\nI guess an alternative would be to make everything a table, could you shed some light on the tradeoffs here? As far as I can tell its just the difference between inline memory and vtable offsets.. There seems to be no such function X_nest_root() is X supposed to be the name of the table or the nested flatbuffer? I've tried both and neither seem to work. Just to make sure in the .fbs file the structure is table Y { X: [ubyte] (nested_flatbuffer: \"Z\") } where Z is some other table. . I have searched all the generated code, I haven't found it is there anywhere specific I should look?. ",
    "IlldianX": "@aardappel Thanks for the quick response! I understand that (2) doesn't cost more space, but we've deployed a large number of FlatBuffer schemas into prod, it's nontrivial to convert them into struct wrappers.\nLet me rephrase my proposal. I'd like to have a way to identify whether a scalar field has an explicitly declared default value. And I think it's doable because the FlatBuffer binary schema that stores the field default value is also a FlatBuffer object, which means force_defaults option can be used to prevent explicitly declared default values of 0 from being omitted. With this, scalar fields' default values will be wired into the binary schema iff it's explicitly declared. I can then achieve the semantic that a field is present iff there's data wired into the buffer, or the field has an explicitly declared default value. The overhead will be a couple more 0s in binary schema file. Do you see there any issue in this proposal? Appreciate the feedbacks!. ",
    "robert-schmidtke": "I signed it!. Both the Java and C# tests succeed as well now, the Java part of which also tests the C++ written, binary size prefixed monster file.. I have removed the explicit tests for the size prefixed version, and added inline tests. This brought to my attention, that it might be useful to introduce SizePrefixedMonsterBufferHasIdentifier methods that do not rely on the fact, that the user has advanced the provided buffer by 4 bytes themselves?. Sorry, with the last push I added the Python code as well, because I had merged the things on my end already. Guess I'll close the Python PR and extend this one to include the Python changes as well.. Hi @aardappel, thanks for coming back to me on this. I'm using my version of the branch successfully (mostly with Java though) already. I'd like to see the changes merged if the project can benefit from that. Please let me know how I can help with this PR!. I have merged the current master and regenerated the source files. The Java, C# any Python tests all succeed on my Mac.. Okay so I've added quite a few changes to ByteBuffer: it now hides the internal byte array (which I think is an appropriate design choice, similar to Java), and it keeps track of the offset into a parent's buffer, in case it was created via Slice(). Furthermore I've added more tests that check the slices, as well as size prefix operations. The implementation is fairly close to what Java does internally, I don't think there is much of a performance drawback by carrying a parent's offset around. I'm not a C# expert, but my guess is that the JIT will probably be smart and figure out that the offset is zero in most of the cases, right?\nSo anyway, let me know what you think. I realize there is some potential API breaking in here.. I understand. Let me try and rework the additional code into a subclass of ByteBuffer that capsules the slicing behavior, essentially leaving the original ByteBuffer alone.. So I managed to encapsulate the slicing behavior in a subclass of ByteBuffer, such that 'pure' ByteBuffers do not suffer from the overhead of carrying the _off. Is that more acceptable for you?. Should I squash the commits into one, or will you do that upon merging eventually?. Unless you have any more comments/requested changes, I don't think I have anything to add.. moved into #4445 . @desqaz I'm looking into incorporating your changes into my PR, good idea. I may struggle a little with when the option is relevant, so any insight is greatly appreciated:\n- during reading of size prefixed binaries (--read-size-prefixed?)\n- during writing of size prefixed binaries (--write-size-prefixed?)\n- ~~handling of nested buffers (are they ever size prefixed? I'm trying to figure that out)~~ they're not.\nAlso for the API design: there is a seperate GetRoot method for size prefixed buffers, GetSizePrefixedRoot, but for handling the buffer identifiers, BufferHasIdentifier and GetBufferIdentifier, a default parameter should be introduced? Should we maybe do one thing or the other to indicate size prefixed buffers? Maybe @aardappel has a preference?. I suggest @desqaz PR be accepted and I merge the updated master into my PR. This would be a cleaner separation of new features, plus the naming is already proper in this PR.. I have added a constant for Java and C# each. The Java one had to be explicitly public though, because the other ones are package-private.. There is a new buffer created, however slice() uses the data of the other buffer, there is no copying involved, only position, limit etc. are reset. The same happens with the Slice() version I have added to the C# ByteBuffer. My reasoning behind this was that I wanted to avoid advancing the passed-in _psbb to avoid side-effects for the user. One possibility would be to increment the position by 4, do the regular getRootAsMonster, and then decrement it again. However I was worried about concurrent access to the buffer. Any thoughts on this? Would the +4, getRootAsMonster, -4 version be okay with you?. I could see something like I've done in lines 1335+. However the method bodies would depend on i, sort of forcing the for-switch anti-pattern.. I've pushed a version with less overlap in e84452f.. I understand that, I have thought about it similarly. However, I'm not quite sure how else I may check the interoperability between C++ and Java (in this case), if not for a generated file. One could add another dedicated test that checks for interoperability in all supported languages? This test would first generate the regular and the size-prefixed version, and then invoke a small test for each language. This way, the new schema would not need to be versioned in git.. Will do!. I know it's not optimal. However the only way of reusing another ByteBuffer's Data is through an additional offset. The alternative would be to create a new subarray for each slice, with the first couple of entries removed. However this would cause memory overhead.\nAlternatively, slicing could be abandoned altogether and the same buffer is reused, and only it's position is changed. However I'm not sure this will always work -- e.g. when 'Reset'ting a buffer, it's position will be at the size prefix again.\nYou're right with the usage of _off, it's not consistent and should probably be factored in Length too.. The method with your suggested signature is protected though, and I won't be able to call it here this way (if I interpret my compiler's errors properly).. There is probably a way to implement this a little more efficiently using unsafe C# if that does not violate your guidelines. Note that the Java ByteBuffer implementation does something similar for each get, e.g. in HeapByteBuffer:\n```Java\nprotected int ix(int i) {\n    return i + offset;\n}\npublic byte get(int i) {\n    return hb[ix(checkIndex(i))];\n}\nfinal int checkIndex(int i) {\n    if ((i < 0) || (i >= limit))\n        throw new IndexOutOfBoundsException();\n    return i;\n}\n```. My final note on this will be that I'll vote for a similar abstraction in C# as in Java:\n```csharp\nusing System;\nnamespace FlatBuffers\n{\n    public class ByteBuffer\n    {\n        private readonly byte[] _buffer;\n        private readonly int _cap; // calculated capacity\n        private int _pos;          // Must track start of the buffer.\n        private readonly int _off; // parent's position if sliced\n    public int Length { get { return _cap; } }\n\n    public byte Get(int pos) { return _buffer[_off + pos]; }\n\n    public void Put(int pos, byte b) { _buffer[_off + pos] = b; }\n\n    public ByteBuffer(byte[] buffer) : this(buffer, 0) { }\n\n    public ByteBuffer(byte[] buffer, int pos) : this(buffer, pos, 0) { }\n\n    private ByteBuffer(byte[] buffer, int pos, int off)\n    {\n        _buffer = buffer;\n        _pos = pos;\n        _off = off;\n        _cap = buffer.Length - _off;\n    }\n\n    public int Position {\n        get { return _pos; }\n        set { _pos = value; }\n    }\n\n    public void Reset()\n    {\n        _pos = 0;\n    }\n\n    // Create a new ByteBuffer on the same underlying data.\n    // The new ByteBuffer's position will be 0, but starting\n    // from this ByteBuffer's current position.\n    public ByteBuffer Slice()\n    {\n        return new ByteBuffer(_buffer, 0, Position);\n    }\n\n    // ...\n\n}\n```\nEDIT: this will obviously be propagated throughout the code, as well as the unsafe versions in ByteBuffer.cs.\nEDIT2: I'll later push a revised version that incorporates the changes so you can have a proper look.. it is applicable for parsed JSON and schema serialization, right?. That makes sense, I'll add that.. My interpretation was that basically, there shouldn't be too many additional functions for size prefixed buffers. Instead, the size prefix should be removed from the size prefixed buffer (GetSizePrefixedRoot), and the resulting buffer should be used with the existing functions.. In my PR I have missed this part. Then it should also probably be included in GenFieldOffset, right?. So nested flatbuffers never have their size prefixed?. @aardappel @desqaz should there be two distinct options, one for reading size prefixed binaries, and one for writing them? or should reading size prefixed input always also entail writing size prefixed output?. Data was used, mainly for length checks and memory copies. I thought that fully hiding the underlying buffer was slightly better design, especially when dealing with slices. Thus, the buffer's content is modified/read using the Put* and Get* methods.. It's mainly for testing to check that the padding is correct. This was previously done by checking against Data directly.. By hiding Data, I moved growing the buffer from FlatBufferBuilder to ByteBuffer. This is a 1:1 copy of that method. It's called from FlatBufferBuilder::GrowBuffer.. Then I frankly do not see another way to support slices, which imho is the cleanest way of representing size-prefixed buffers. Note that using ByteBuffer in Java has exactly the same design, even for non-slices. The last resort would be do create a new ByteBuffer with a separate underlying byte[] array and copy all the previous contents to it, minus the size prefix. That would be a one-time overhead for creating the slice, and about 100% memory overhead, unless the caller discards all references to the size-prefixed buffer, and lets the GC handle that.. Just FYI, I have added a small benchmark:\n```csharp\n[FlatBuffersTestMethod]\npublic void BenchmarkFlatBuffer()\n{\n    Random rnd = new Random(0);\n    ByteBuffer bb = new ByteBuffer(File.ReadAllBytes(@\"Resources/monsterdata_test.mon\"));\n    // bb = bb.Slice(); // toggle for ByteBufferSlice performance\n    int maxIndex = bb.Length - 4;\n    int sum = 0;\nStopwatch sw = Stopwatch.StartNew();\nfor (int i = 0; i < 1000000; ++i)\n{\n    sum += bb.GetInt(rnd.Next(maxIndex));\n}\nsw.Stop();\n\n// write sum to avoid loop optimization\nConsole.WriteLine(\"Regular: {0}, {1} GetInt ops/ms\", sum, 1000000.0 / sw.Elapsed.TotalMilliseconds);\n\n}\n```\nI ran it on my Mac ten times with the regular ByteBuffer and with the ByteBufferSlice and averaged the results. Numbers are GetInt operations per millisecond.\nVersion | Safe | Unsafe\n--- | --- | ---\noriginal ByteBuffer (current master) | 33,876 | 44,436\nByteBuffer using + _off | 33,790 (-0.3%) | 43,837 (-1.3%)\nByteBuffer using virtual | 33,017 (-2.5%) | 44,017 (-1.0%)\nByteBufferSlice using override | 32,400 (-4.4%) | 42,877 (-3.5%)\nThis is an extremely dense testing scenario, and should thus represent the worst case performance degradation.\nGiven these results I'd vote for the slicing behavior implementation from 5c682598f17fd016f409bf232e92f80c20de6d45, as I think a 0.3% to 1.0% performance decrease would be acceptable.. Hi, the timings are from Mono:\nbash\nMono JIT compiler version 5.4.1.7 (2017-06/e66d9abbb27 Wed Oct 25 12:10:41 EDT 2017)\nCopyright (C) 2002-2014 Novell, Inc, Xamarin Inc and Contributors. www.mono-project.com\nCompiled with mcs:\nbash\nMono C# compiler version 5.4.1.0\nI'm not sure there is an alternative on Mac? Should I get my hands on a Windows machine and benchmark it there as well?\nI was thinking to separate _pos and _off, because _pos can be set externally, and therefore the size prefix could be restored (accidentally, e.g. by Reset()). For the use case where bb_pos is used as offset (when interacting with the ByteBuffer through Monster) you're correct, but since the ByteBuffer may be used rather freely I figured it'd be better to logically separate them.\nI have added your suggestion, and these are the results:\nVersion | Safe | Unsafe\n--- | --- | ---\noriginal ByteBuffer (current master) | 33,876 | 44,436\nByteBuffer using + _off | 33,790 (-0.3%) | 43,837 (-1.3%)\nByteBuffer using virtual | 33,017 (-2.5%) | 44,017 (-1.0%)\nByteBufferSlice using override | 32,400 (-4.4%) | 42,877 (-3.5%)\nByteBuffer using advanced _pos | 32,263 (-4.8%) | 45,593 (+2.6%)\nI don't really know why this performs so poorly, I guess it's simply not the exact same system load today than it was last time. Your approach would certainly save a lot of code.. The GetInt in the ByteBufferSlice subclass would add _off in the call to base.GetInt, whereas the GetInt in ByteBuffer does not, correct. Hence the regular vs. Slice comparison.\nHowever the current state implements your suggestion of simply passing an advanced ByteBuffer to GetRootAsMonster. For clarity I changed the terminology from 'slice' to 'duplicate', also for the Java version.. ",
    "aaronovz1": "It seems to be the *Builder structs that pass in one arg which is the FBB:\nAdapterBuilder(flatbuffers::FlatBufferBuilder &_fbb)\n        : fbb_(_fbb) {\n    start_ = fbb_.StartTable();\n  }. Done. See #4452. I signed it!\n\nFrom: googlebot notifications@github.com\nSent: Thursday, 5 October 2017 1:15:22 PM\nTo: google/flatbuffers\nCc: Aaron Zinghini; Author\nSubject: Re: [google/flatbuffers] [C++] Add explicit keyword on generated constructors for Builder structs (#4452)\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address. Check your existing CLA datahttps://cla.developers.google.com/clas and verify that your email is set on your git commitshttps://help.github.com/articles/setting-your-email-in-git/.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/google/flatbuffers/pull/4452#issuecomment-334579380, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AT0vbv_XNT0v7fSzMPzrvLvjx9QlmYviks5spTjagaJpZM4PvotO.\nAaron Zinghini\nGuardian Software Architect\nT       +1 917 346 2993\nM       +1 917 346 2993\nE       aaron.zinghini@seeingmachines.com\nW       www.seeingmachines.comhttp://www.seeingmachines.com\n[Seeing Machines]https://www.seeingmachines.com/\nThis email is confidential. If you are not the intended recipient, destroy all copies and do not disclose or use the information within. No warranties are given that this email does not contain viruses or harmful code.\n. Which generator script? \"tests/generate_code.sh\"? If so I have attached the generated output for \"monster_test_generated.h\" which shows the explicit keyword added.. I understand this issue is closed but I recently found the need to be able to control the lifetime of the memory inside the FlatBufferBuilder object. I thought I could use DetatchedBuffer but it doesn't appear to allow copying due to:\nFLATBUFFERS_DELETE_FUNC(DetachedBuffer(const DetachedBuffer &other))\n  FLATBUFFERS_DELETE_FUNC(\n      DetachedBuffer &operator=(const DetachedBuffer &other))\nI am using FB in conjunction with ZeroMQ, and to achieve zero-copy I need to provide a custom free/deallocator function to ZMQ so that it can free the memory when it is finished sending the message. I thought I could do something like:\n```\nvoid freeFrame(void data, void hint)\n{\n    DetachedBuffer detached_buf_ptr = (DetachedBuffer)hint;\n    delete detached_buf_ptr;\n}\n...\nDetachedBuffer detached_buf = fbb.Release();\nDetachedBuffer* hint_ptr = new DetachedBuffer(detached_buf);\npublisher->sendNoCopy(detached_buf.data(), detached_buf.size(), &freeFrame, fbb);\n```\nCan someone point me in the right direction on how I can do this using the current FB release?. @sutambe I ended up getting something working by allocating FlatBufferBuilder on the heap and using that as the hint pointer.\nFlatBufferBuilder* fbb = new FlatBufferBuilder(image_size);\n...\npublisher->sendNoCopy(detached_buf.data(), detached_buf.size(), &freeFrame, fbb);\nDo you know which method might be more efficient?. I'm not calling Release anymore with this way.. Yep. I was just curious if there would be any performance difference between the two methods.. ReleaseRaw is not in the 1.9.0 release though?. I guess I was trying to find a way to avoid the memcpy but I'm not really seeing how that would be possible. The only way I can achieve it in our setup is to the pull the image blob out of FB and send it separately as-is.. This is pretty much what I ended up doing. ZMQ has a concept of message parts, so I store the FB in one part, and the blob in a second part.. ",
    "doug": "Ah great that is exactly what I was looking for. Thank you. Sadly I'm depending on other protos that I can't change so I would need to get --proto to do that translation automatically. Is that a reasonable feature to add?. Great, will update this bug if I start work on it to prevent duplication.. ",
    "jfinken": "@rw, @aardappel \nWonderful.  Many thanks for the swift response and the PR.  It does indeed meet my needs.  On my setup the serialization time for data that included the byte vector went from ~120.40ms down to ~0.27ms.\nThank you again.. ",
    "bchavez": "I signed it!. ",
    "CMajeri": "Hi,\nYes this happens in go, we forgot to mention that!\nInitially it happened during a small test, where we forgot to base64 decode a very short flatbuffer, which caused it to be 3 bytes long.\nThere should be no real case where this happens, unless in case of a major disaster (one component just goes bonkers) but some input validation from the flatbuffers side would be nice to have!. ",
    "braoru": "Hello, I'm @CMajeri and @harture colleague and as I work on the same topic, I think a panic and a good error message would be fine :-) .. ",
    "DariuszOstolski": "I was trying to reproduce your issue here:\nhttps://github.com/DariuszOstolski/flatbuffers/tree/issue4457\nAdded:\nenum Position:byte { ABSOLUTE = 0, RELATIVE = 1}\nin \nhttps://github.com/DariuszOstolski/flatbuffers/blob/issue4457/tests/monster_test.fbs\nand \n\nifdef _WIN32\nifndef WIN32_LEAN_AND_MEAN\n#define WIN32_LEAN_AND_MEAN\nendif\nifndef NOMINMAX\n#define NOMINMAX\nendif\ninclude \ninclude \ninclude \nendif\ninclude \"monster_test_generated.h\"\nin https://github.com/DariuszOstolski/flatbuffers/blob/issue4457/tests/test.cpp\nBut apparently I couldn't reproduce it:\nhttps://ci.appveyor.com/project/DariuszOstolski/flatbuffers/build/1.0.6 . As far as I can see in help --scoped-enums implies --no-prefix, that's why I was not able to reproduce this issue. In my opinion windows headers contain a lot of preprocessor traps, lets see if this can be solved. . As you can see in:\nhttps://ci.appveyor.com/project/DariuszOstolski/flatbuffers/build/job/3n2k19smjp9bj5as\n\nbuild was successful even with --scoped-enums. You can download artifacts:\nhttps://ci.appveyor.com/project/DariuszOstolski/flatbuffers/build/job/3n2k19smjp9bj5as/artifacts\nand compare generated sources with yours. \nMy suspicion is that there are some defines/configuration in your project that cause to include some parts of windows headers.. As you can see below:\n```\nifdef _WIN32\nifndef WIN32_LEAN_AND_MEAN\ndefine WIN32_LEAN_AND_MEAN\nendif\nifndef NOMINMAX\ndefine NOMINMAX\nendif\ninclude \ninclude \ninclude \nendif\ninclude \"monster_test_generated.h\"\n```\nI intentionally included windows headers before generated file. I suspect that there is some define that disables this part of header file in flatbuffers and in your project those are enabled.\n. Please close this PR, I'll create new one when I correctly rebase latest changes. \nBR,\nDarek. ",
    "slippery-surface": "@DariuszOstolski great.. ",
    "DanEble": "Perhaps the following observation is related to this issue.  The tutorial says,\n\nImportant: Unlike structs, you should not nest tables or other objects, which is why we created all the strings/vectors/tables that this monster refers to before start. If you try to create any of them between start and end, you will get an assert/exception/panic depending on your language.\n\nI could not make sense of this warning because I do not see start or end anywhere in the example.. I doubt a rotten orange has any bad intent either, but it doesn't change the fact.  :-). > The problem is in attribute usage\nThat's a problem.  Consider: if an attribute can not be used, why accept its declaration?  Flatc should anticipate the inability to use the attribute and complain at the point of declaration.\n\nAs a solution - remove dot symbol form the attribute name.\n\nExactly.  This is the approximate level of detail that flatc could and should provide in its diagnostic message.. ",
    "spmckenney": "I created some helper functions that look something like this for a project I am working on:\n```c++\n/\n * Returns a pointer to an object given the Offset\n/\ntemplate \nType GetMutablePointer(flatbuffers::FlatBufferBuilder& builder, const flatbuffers::Offset& object) {\n  return (reinterpret_cast(builder.GetCurrentBufferPointer() + builder.GetSize() - object.o));\n}\ntemplate \nType* GetPointer(flatbuffers::FlatBufferBuilder& builder, flatbuffers::Offset& object) {\n  return (reinterpret_cast(builder.GetCurrentBufferPointer() + builder.GetSize() - object.o));\n}\ntemplate \nconst Type* GetPointer(flatbuffers::FlatBufferBuilder& builder, const flatbuffers::Offset& object) {\n  return (reinterpret_cast(builder.GetCurrentBufferPointer() + builder.GetSize() - object.o));\n}\n```\nI'd be happy to contribute if you want to give me some feedback and let me know where you'd like them.. ",
    "rajukp": "Is there simple way for accessing scalar data members before calling finish . When we add data in the beginning we do not get any offset similar to strings .\nExample\n.fbs:\nnamespace sPData;\ntable StreamPersistData{\n  version:uint;\n  seqNum:uint;\n  agentId:uint;\n  fileName:string;\n  offset:ulong;\n  custom1:string;\n}\nroot_type StreamPersistData;\nflatbuffers::FlatBufferBuilder builder;\nbuilder.ForceDefaults(true);\nflatbuffers::Offset<flatbuffers::String> name;\nname = builder.CreateString(fileName);\n\nStreamPersistDataBuilder sPBuilder(builder);\nsPBuilder.add_version(version);\n//sPBuilder.add_agentId(agentId);\nsPBuilder.add_seqNum(seqNum);\nsPBuilder.add_fileName(name);\nsPBuilder.add_offset(offset);\n\nauto str1 =   reinterpret_cast<flatbuffers::String *>(builder.GetCurrentBufferPointer() + builder.GetSize() - name.o); // String works since I have an offset\n\nHow do we get scalar variable since we dont have offset ...\n//auto sq =  reinterpret_cast<uint32_t *> (builder.GetCurrentBufferPointer() + builder.GetSize() - StreamPersistData::VT_SEQNUM);\n\n. Thanks for the reply !\nI am trying to address a use case where I need to re-use data of already serialized variable to derive  few more fields. I have the option of storing them in temporary variable but was looking for an option to avoid it.\nEx: \nbuilder.add(f1)\nbuilder.add(f2)\n........\nbuilder.add(field100)\n// At this point I have to derive new \n//Fields using some of the earlier serialized data\nfieldN= custome operation(field5,field6)\nbuilder.add(fieldN)\nbuilder.Finish();\n. ",
    "stefan301": "|I signed it!|\n. ",
    "fire": "Sorry for not being clear. I want to generate a wrapper for \n```\ntable HelloReply {\n  message:string;\n}\ntable HelloRequest {\n  name:string;\n}\ntable ManyHellosRequest {\n  name:string;\n  num_greetings:int;\n}\nrpc_service Greeter {\n  SayHello(HelloRequest):HelloReply;\n  SayManyHellos(ManyHellosRequest):HelloReply (streaming: \"server\");\n}\n```\nand convert it to something like:\n```\nDECLARE_DYNAMIC_MULTICAST_DELEGATE_OneParam( FHelloReplyDelegate, FHelloReply );\nUSTRUCT()\nstruct FHelloReply : public UObject\n{\n    GENERATED_UCLASS_BODY()\n        UPROPERTY()\n        UString message;\n}\nUSTRUCT()\nstruct FHelloRequest : public UObject\n{\n    GENERATED_UCLASS_BODY()\n        UPROPERTY()\n        UString name;\n}\nUSTRUCT()\nstruct FManyHellosRequest : public UObject\n{\n    GENERATED_UCLASS_BODY()\n        UPROPERTY()\n        UString name;\n        UPROPERTY()\n        uint8 num_greetings;\n}\nUCLASS()\nclass UGreeterBlueprintLibrary:\n    public UBlueprintFunctionLibrary\n{\n    GENERATED_UCLASS_BODY()\n    UFUNCTION(BlueprintCallable, Category=\"Greeter\")\n    static HelloReply SayHello(HelloRequest Hello);\nUFUNCTION(BlueprintCallable, Category=\"Greeter\")\n// Requests the user to provide a callback  delegate\n// Returns 0 if Callback is not bound\nstatic int32 SayManyHellos(ManyHellosRequest Hello, FHelloReplyDelegate Callback);\n\n}\n// Implementation ...\n```\nThis should be possible to do within the flatc executable.. Where should I place this code to generate the wrappers?. https://github.com/google/flatbuffers/blob/master/grpc/src/compiler/cpp_generator.cc is pretty ugly with its strings.. ",
    "linuxaged": "OK, there's something wrong with my Print function.. ",
    "tamasgal": "Sorry, classic. 5min after writing the issue I found the solution. I had to update the pip installation of the flatbuffers Python library from master too.\nSo as I see, this feature is not released yet.... ",
    "avsej": "@aardappel, but keep in mind that shared object version has some rules, and you have to update major version on breaking ABI changes, even when the project version is not going to change first number. This is why I started from 1.0.0, because in this case you won't confuse it with package version.\nBut anyway I have changed it to 1.7.0 as you asked, and dropped small note about it. Feel free to update it if you want.. > The core of FlatBuffers is headers-only, and was generally made with static linking in mind.. any reason why you need this versioned shared library?\nStatic libraries discouraged in Linux distributions, and they will track ABI anyway.\n\nI cannot guarantee that the major version will go up on an ABI change. Is there any point in having the version follow along with FlatBuffers releases then?\n\nRight, maybe lets just make it 1.0.0. . Why do you against distributing this library?. This is for Fedora and probably CentOS. Static libraries are discouraged there. Also flatbuffers are not headers-only, so it requires shared object to link.. And also as far as I can see Gentoo also deploys dynamic library: https://gitweb.gentoo.org/repo/gentoo.git/tree/dev-libs/flatbuffers/flatbuffers-1.7.1.ebuild?id=0c24b5e4bd0c84f038d6fd2b94e65b46afd82947#n28. This is default compiler on Fedora linux, while it building the package.\nhttps://kojipkgs.fedoraproject.org//work/tasks/451/24660451/build.log\n/usr/bin/c++   -I/builddir/build/BUILD/flatbuffers-1.8.0/include -I/builddir/build/BUILD/flatbuffers-1.8.0/grpc -I/builddir/build/BUILD/flatbuffers-1.8.0/tests -I/builddir/build/BUILD/flatbuffers-1.8.0/samples  -O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -fexceptions -fstack-protector-strong -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1 -m64 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -mcet -fcf-protection -std=c++0x -Wall -pedantic -Werror -Wextra -Werror=shadow -fsigned-char -DNDEBUG   -o CMakeFiles/flatc.dir/src/idl_parser.cpp.o -c /builddir/build/BUILD/flatbuffers-1.8.0/src/idl_parser.cpp\nmake[2]: Leaving directory '/builddir/build/BUILD/flatbuffers-1.8.0'\n/builddir/build/BUILD/flatbuffers-1.8.0/src/idl_parser.cpp: In member function 'flatbuffers::CheckedError flatbuffers::Parser::ParseHexNum(int, uint64_t*)':\n/builddir/build/BUILD/flatbuffers-1.8.0/src/idl_parser.cpp:220:62: error: type qualifiers ignored on cast result type [-Werror=ignored-qualifiers]\n     if (!isxdigit(static_cast<const unsigned char>(cursor_[i])))\n                                                              ^\n/builddir/build/BUILD/flatbuffers-1.8.0/src/idl_parser.cpp: In member function 'flatbuffers::CheckedError flatbuffers::Parser::Next()':\n/builddir/build/BUILD/flatbuffers-1.8.0/src/idl_parser.cpp:260:62: error: type qualifiers ignored on cast result type [-Werror=ignored-qualifiers]\n         if(!isdigit(static_cast<const unsigned char>(*cursor_))) return NoError();\n                                                              ^\ncc1plus: all warnings being treated as errors. Yes, it could be 1.7.1, I will update PR. ",
    "icexelloss": "Hi Folks!\nI wonder if there are any updates on this? We are working on Apache Arrow and are thinking about publishing our unofficial flatc maven artifact just to build the Java code, but we would much rather to use an official flatc artifact. . Thanks for the reply!\nI don't think Maven is absolutely necessary here. Something like a download link, e.g.:\nhttps://github.com/google/flatbuffers/releases/download/v1.9.0/flatc_windows_exe.zip\nWould also work because people can download that as a part of maven build. If you guys are already building flatc executables for different platforms in packaging process, it would great to release the flatc executables for platform other than Windows.\n. ",
    "Aerin41": "Ah, understood. Thanks!. ",
    "angelasheu": "Ah awesome, I will make a PR. Btw I think we will need to set the clause to be num_fields < 127 because a long can count as 2 units https://stackoverflow.com/questions/30581531/maximum-number-of-parameters-in-java-method-declaration. I've signed the CLA :). ",
    "dnfield": "It looks to me like the Dart SDK implements at least some of it here:\nhttps://github.com/dart-lang/sdk/blob/master/pkg/front_end/lib/src/base/flat_buffers.dart \n@scheglov any good reason to not just split this out into its own package?  I think this would be interesting to use particularly in Flutter projects. I'm working on dart codegen.. Yes, I'm working on implementing the code generation for flatc.  However, it'd make sense for there to be a flat_buffers dart package (which would basically reuse the code implemented already) - similar to the flatbuffers npm package for JavaScript.. I don't see why not.. How should we handle the publication of flat_buffers.dart?  I've had to make a few additions from what was in the front_end package (it was missing helpers for adding some typed data, like Int16/UInt16 and Float32s).  I'm adding the up to date file to my brach of google/flatbuffers which has my start on Dart code gen.\nWho owns, for example, publishing flatbuffers.js to NPM?. @binary132 - flatc compiles with Visual Studio and it looks like there are windows based binary releases.  That should address your second point. \nHowever, to be fair, I'm more concerned with using this for Flutter than for dart2js - my testing is much more around that and I can't guarantee I haven't broken anything for dart2js (particularly around int64 handling).\nAs far as its reach, it's entirely possible to implement the spec in another language - in fact, the Dart SDK team did something close to that.  However, it makes it more challenging to keep up with new features/changes.. It loads for me.  Either way do you want me to just create a new pr?. @aardappel I created a script here: https://github.com/google/flatbuffers/blob/master/dart/publish.sh to help with publishing.\nIf you want to change the version, it needs to be changed here: https://github.com/google/flatbuffers/blob/master/dart/pubspec.yaml (Currently at version: 1.9.0). You would need to have pub and dart installed.. Discovered a bug with vectors of enums - need to work that out for now. @aardappel  I'll try to work through some of the Dart questions as I have time.  However, much of that code was written by @scheglov (I only added code for reading/writing U/Int16 and Float32s, which was absent).\nThe bool logic should probably be removed - it's a special case that they appear to use internally but wouldn't be compatible with other platforms.\nIf @scheglov can comment on any of this it would be helpful, but like I said I'll work through it as I have time as well.. I think I'm actually wrong about the way the bools is implemented as I look into it more closely.  It's not packing multiple bools into a single Uint8, it's just converting bools to Uint8 and vice versa. I think this should interop fine with other languages.. I've made the string interning opt-in behavior (defaults to not keeping a map of the strings).  I could work on refactoring the Offset<T> class and the _VTable usage by the Builder, but it might be better to try to come up with some profiling around it first to see.  My suspicion is that the difference will be negligible - I'll see if I can come up with some benchmarks around that.. I was waiting for things to settle on the implementation and code gen.  If that's in good shape now I'll work on those.. I'm working through the docs updates.  I notice that the examples for several of the languages directly use the flatbuffers builder class for that language directly - but in Dart, there's generated builders that are much easier to use.  \nI would prefer to focus on that and just note that you could look at how to directly interact with the Builder by looking at the generated Builder classes, but it's a little bit inconsistent with the other languages.  Are you good with that approach for the general overview doc (Tutorial.md)?. Nevermind my last comment.  Dart isn't quite as different as I thought.  I'm still working through this, found what I believe would cause a bug in the way vectors of structs are stored and working on that.. I didn't have much time for this over the last week - as I'm getting back to it, it seems like the original implementation of this only envisioned supporting tables (not structs).  My effort here so far has been based on that implementation, and all of the logic is wrong when handling a struct (it writes and attempts to read it as a vtable).\nSo basically, I haven't forgotten about this but it's going to take me a bit more time to rework it to support structs properly.  @aardappel, if you'd like me to close this in the mean time let me know (or just close it yourself I suppose?).  . I have the reading part reworked and seems good for a C++ generated flatbuffer.\nStill working through the writing portion.. At this point I should just be down to finishing up the tests.  Note that I have made some substantial additions to the dart code since the last review - probably warrants another look over.\nI also want to look at restructing the /dart folder a bit to make it match the expected format for Dart pub.  . I think this should be pretty close.  I've restructed the Dart folder a bit to make it match what dartpub will expect to see.  It can be published using pub publish from that folder.\nI've added tests in there and the sample in there (with pointers to that folder from the tests and samples folder shell scripts) - that's a typical setup for pub packages.\nI can go ahead and publish it and add you as a publisher as well if you like.. Thanks for that @isoos . This should be good to go as far as I'm concerned.  @aardappel - would you like me to publish it with you as an author/publisher as well?. I've separated out the previous generated Builders into what I'm now calling \"ObjectBuilders\" - I believe that's in line with the Object model in other langauges, roughly.  I think these classes will be more accessible for most Dart users, and probably meet many performance/memory requirements.\nI've added a new set of Builder classes that are more in line with the existing implementations, as well as some tests for those and a test that ensures we can read a C++ generated file.  This did lead to a couple changes in how enums are handled - I had been assuming enums would not allow gaps, but that's not the case for bit_flag enums or for otherwise overridden enums.  \nThe test cases are still under the dart/ folder, and the script in tests/ references that - but I'm not sure there's a nice/clean way for Dart and Pub to get necessary dependencies if we put it all under the tests/ folder.  I'd welcome input on that...\nOne thing I haven't done yet is to update the documentation.  I'll try to work on that over the next few days, but wanted to get the code up here for review in the meantime.. I did another pass over the Dart and C++ code, fixed a few missing/minor issues.\nI've updated the LICENSE in Dart to try to reflect that the flat_buffers.dart code is based on code from chromium - I'm not married to that formulation but perhaps it works for what you're looking for for this repo.\nI added a bash script to help with pub publication - it will copy a couple files, do the publish, and then remove them afterwards to avoid file duplication.  Maybe someone who's more adept with pub and dart packaging than I am can come up with a better way to organize these files at some point, but I haven't been able to figure out a way to keep the files the way you have them and still make Dart find the correct packages to run the tests.. One last push here - the sample file schema was actually slightly different (had one extra field), updated the dart example to not use that field anymore.  Also removed what I thought was going to be an optional parameter in the C++ but isn't needed.. Should the version be 1.9.0 now?  Or will it be 1.10.0 when Dart is support is released?. Thanks for all the time and feedback on this!  Looking forward to being able to use it \ud83d\ude04 . A couple notes on this that I'd like clarification on if I'm wrong...\n\nThe verifier is primarily useful in languages/environments where it is possible to unintentionally read/write memory you shouldn't have access to.  So, for example, in C/C++  you want to make sure that none of the offets point you to memory a potentially malicious FlatBuffer file would point you to live outside the buffer.  In higher level languages you shouldn't run into this issue - a bounds checking error will occur when you try to read data not in your byte array.\nBinary comparison is not enough.  It would be entirely possible for two buffers to be binary identical but intended for use with different schemas.  It's also possible for two buffers with identical data per schema and internal offset structure to have different contents, for example if data was written in a different order to the buffer and padding/alignment was handled slightly differently.  I've definitely seen this happen in my testing between, for example, C++, JavaScript, and Dart.  Each one read the same data from a message, but had slightly different layouts/sizes.\n\nSo the bottom line, I think, is that for testing purposes you should be mostly concerned with your ability to successfully and consistently read (and, if applicable, verify) the buffer in whatever langauges/platforms you'd need to read it.  I do not believe doing a binary comparison of the files is the right way to go, unless you're certain that the order of writing the fields and the alignment calculations haven't been affected - but that seems like a bad thing to test/assume.. This makes FlatBuffers a bit different from other serialization formats - in many cases, serializing an object and doing a byte comparison works as a \"deep equals\" against objects.  It might be good to document that this is not the case somewhere in the FlatBuffers docs.\nThere certainly could be parallels to this though - if you use two different JSON serialization libraries for example that handle \"meta\" attributes differently, or that are based on native object model classes that write the fields in different order (or custom serialization rules that write the fields in different order).\nEven so, in FlatBuffers the order isn't controlled by the order of property declaration in the class(es).  This could lead to some confusion and apparently inconsistent results for people who are trying to do \"document\" or object comparisons using FlatBuffers.. @WoodyGuo  - it looks like this commit broke the tests (or, that the tests were never updated to reflect these changes).  Just for future reference, please be sure to run the test script (in tests/DartTests.sh - or the equivalent commands on your platform) and make adjustments needed to the tests :). In particular, https://github.com/google/flatbuffers/blob/2361dfb66a0b6f1791e0a27d5d4fa285c2bbc913/dart/test/flat_buffers_test.dart#L62 needed to be updated. Something seems off with this too trying to run tests, I'll open an issue to discuss.. Looks like some of those deprecated names have finally been removed/renamed. \nIt's not hard to fix, but unfortunately would mean removing support for Dart 1 - which is probably ok - or creating two packages (one for Dart 1, one for Dart 2), which is annoying.. Yes, they were removed 8 days ago now - https://github.com/dart-lang/sdk/commit/f2402b3c08b849062707ec06e6f43abb182e2196\nMy preference would be to add an environment constraint (say, sdk: '>=2.0.0-dev.28.0 <3.0.0', which follows what a lot of the official Flutter plugins currently have), and update the constants.  I can create a PR for that and maybe collect feedback there.\n@aardappel  this will require publishing a new Dart package to dart pub, but users could in theory refer directly to the git repository for now.. Also, this would be a great candidate for https://github.com/dart-lang/sdk/issues/33249 - could conditionally wrap code with these references rather than needing to worry about how to support users who might still want to use Dart 1.x\nThat said, there is a working version of this lib that would still support Dart 1.x.... @aardappel  it still needs to be published on pub - there's a script in the package to assist with that https://github.com/google/flatbuffers/blob/master/dart/publish.sh \n@6643 in the meantime you can add a reference directly to Git, with something like this:\nyaml\ndependencies:\n  flat_buffers:\n    git:\n      url: git://github.com/google/flatbuffers.git\n      path: dart. @6643  I should add I haven't tested that but the documentation says it should work...  If it doesn't, you can just pull it down locally somewhere and set it up as a local package.. I'll take a look.  \nI will say that both the Dart and the JS implementations would sometimes produce different binaries than the C++ implementation, but I hadn't run into issues verifying/decoding them.  That said, I agree that this sounds like an alignment issue.. The C++ code is able to correctly get the full user ID -\ncpp\n  auto user = Repro::GetUser(repro_fb.data());\n  printf(\"%s %s\\n\", user->id()->c_str(), user->name()->c_str());\ngives the expected result.  But the verification fails as reported. \nC++ code to write the same data gives four additional null bytes of padding at the end of the file for \"Foo Bar1\" (so a 52 byte file vs a 48 byte file from Dart).  This is something missing in the Dart code, although it doesn't seem to impact readability (just verifiability).  I'll look into it some more.. The C++ code ensures the String's null terminator will be contained within the buffer.  Dart code was not doing that.\nThis wasn't causing any problems in reading (although I suppose it could have?), but it did cause problems in verifying (which was trying to make sure the string is null terminated properly).\nShould have a PR for this shortly, but I'm noticing the tests don't seem to be running correctly right now and want to see what's going on with that.. Also, it wasn't causing any problems if the String happened to be the right length to leave some space in the buffer (e.g. \"Foo Bar\" in your example) - but your example (\"Foo Bar1\") just had the String ending exactly at the end of the allocated space.. @aardappel just has to run the publish script. I can't remember why of the top of my head, but I'm pretty sure there was a reason some of these weren't done that way.  I'll try to look into it some more.. I'll try to look at why it's segfaulting.. This wasn't intentional, I had meant to revert this.  Will remove.. Yes. it is not optional here.  This map is used by writeString to achieve string interning in resultant file (and it is only used in write operations obviously).  I'm not sure it's much more expensive than writing duplicates of the string if a string is duplicated. It would not be difficult to allow consumers of writeString to opt out of this (or opt in), but I'd think it'd make more sense to opt out.. (There is no such thing as a non-nullable type in Dart). This is a special case where multiple bools are stored as a single uint8.  It's nice for space savings if you have a lot of bools, but it's not interoperable with other langauges and so should probably be removed.. This would only make sense as a generic if the methods it called and setup it did could be done generically.  Each function has to do setup that is aware of the size in bytes of the type it's writing, and has to call a concrete non-generic implementation of a writer function.. Dart will not treat this as by-val.  Offsets could be made into naked ints, but that seems like a lot of loss in clarity/safety.  Was the Java choice made due to benchmarks demonstrating its perf increases?. There's no allocation involved for these - they're used in ListReaders in the generated code, but only through const references.  A const class in Dart is a canonicalized/interned instance that requires no allocation beyond the initial static allocation.. I'm pretty sure it's cheaper to allocate new ones than to attempt to reuse and update the object.  Garbage collectors usually do better with shorter lived objects than with having to keep track of/move/reallocate space for long lived objects. I've made it opt-in to match C++ API. Done.. So looking at the Go implementation, there's this note:\n// Search backwards through existing vtables, because similar vtables\n    // are likely to have been recently appended. See\n    // BenchmarkVtableDeduplication for a case in which this heuristic\n    // saves about 30% of the time used in writing objects with duplicate\n    // tables.\nAlthough it looks like it searches the byte buffer for VTables and only stores offsets rather than the whole objects.  C# and Java do that too.  I'll modify the Dart code. Fixed this. I like the way they implemented that, unfortunately it's non-standard.. This is the content that would show up on pub.dartlang.org.  I can link it to the specific Dart documentation, but it shouldn't be the exact content of DartUsage.md because that has some doxygen specific stuff that wouldn't render properly on pub.dartlang.org. I initially had this in sample/, but moved it here.  I'd like it to be included in what's distributed on pub.dartlang.org, and this is a common convention for doing so (and will be more immediately helpful to people who install the package).  Do you want a copy of it also included in sample/?  . Sorry for being dense but can you point me to the file?  I'm not finding another file where individual .cpp files are referenced besides the BUILD file and this one (which I updated both).. Making this null would require a check on each call of writeString to create it if it's null and intern is true.  The trade-off is allocating one of these per instance of the Builder, or making that check every time someone wants to intern a string.  Neither one seems especially expensive, but it feels more complicated to me to have to check if a previous call to writeString already newed up the map or not.  If you really like I could move it though.. You can't nest classes in dart, and I'd rather avoid adding even more fields/methods to the already large Builder class.. Do you want the package version for dartpub of this to mirror the flatc version?  It seems to me like they could develop independently and be versioned independently.. Moving this file makes it more complicated to properly include the package files.  It also means that the test suite would not get distributed through dart pub, which expects these files to be in the test folder here.\nCould we add a symlink from tests/ to this?. Raw enums in dart exists but must be 0 indexed, which is not the case in FlatBuffers.  If we used raw integer constants we'd lose type-safety which doesn't seem like a good tradeoff in this case, but could be possible.  My preference would be to keep the enum-like classes here.. 1. I did this mainly because it followed the design in the SDK code.  I'm not necessarily committed to this design, but I suppose it could allow for other implementations of the same model class that aren't FlatBuffer driven more easily (e.g. a MonsterJsonImpl or some such thing).\n2. double is the only floating point type available in Dart and is defined by the spec to be 64 bits (int, per the spec, can be as many bits as you have memory).  There is some notion of lists/vectors of Float32 or Float64s, but not individual numbers that I'm aware of.\n3. The declaration double _x; is equivalent to double _x = null;.  Null is a constant in dart, and as far as I understand does not involve any allocation.  However, I'm pretty sure that internally there'd be a pointer generated for the reference to null.\n4. ??= is an operator that assigns the variable if it is null, or leaves it as is if it is not null.\nBasically, for 3 and 4 - it's caching access to the underlying buffer so that subsequent calls to the getter will not have to touch the underlying buffer again.  I don't have any tests around this, but I'd imagine there are cases where it's faster one way or the other.  This is how it was developed in the SDK code and I kept it.. This is if you want to create a Vec3 object from a list of bytes (the List<int> - Dart does not have a byte type, int storage is dynamic per the spec, similar to JavaScript).  That factory constructor should not re-allocate the bytes passed in if the list is actually a Uint8List from the typed_data library - otherwise it will create a copy of the data into a Uint8List for the user.. This flag gets passed through to that Builder.  This way, particular tables/objects can choose to intern strings or not.. I'll do some work on that.  These were helpful to validate some changes I made from the SDK code, and are largely based on that work.. The user could still use the Builder class more directly, the way the generated code does.\nThe \"builder helpers\" do allocate references to the data that will be held until the helper goes out of scope/is garbage collected.\nIn a lower level language, this would definitely be a concern and more important to avoid - I'm just not as convinced it makes as much sense to avoid in Dart.  The scale at which this would become a problem in Dart will likely already be a problem for other reasons (in other words, I don't see Dart being the best candidate for processing very large FlatBuffers data, even if it wasn't making any unnecessary allocations).  I also am not convinced that the runtime wouldn't decide to make allocations behind our backs anyway (I've certainly seen that in other garbage collected environments), even though we've made the code more difficult to use by trying to avoid any unnecessary allocations. \nI've basically followed the design that was done in the original SDK work, although in this case I think that it's worth keeping both for ease of use and low likelihood of performance or usability improvements.. This could end up storing additional references if the consuming code is holding a reference to the data (but shouldn't create an additional reference if the consumer is passing in a literal).  See my comments above on example.dart. Moved these down. . So right now only one gets created per buffer.  I believe I'd be meeting your request here by not initializing this by default and adding something like _strings ??= <String, int>{}; after checking if intern is set to true in writeString, which is fine but adds a few more instructions for each call to writeString to save a cheap allocation.. I'll create a script to move things around and prep the package.. I think it'd make sense to refactor this then to use raw ints and have an enum helper class available with some kind of toInt/fromInt methods/constructors.  I'll work on that.. Ok.  I think this should be plenty fast either way.  I'll get rid of the inherited impl classes too. If you wanted to create a Vec3 from a raw array of bytes, this would do it.  But we're not creating BufferContexts for each Vec3 in a monster - we're using the constructor on the Impl class, which takes the existing BufferContext and an offset. In other words, internally, the factory constructor for Vec3 is never used.  It would only be used by an external end user who wanted to have a flatbuffer of just Vec3s.. I think this makes sense to rework as a simple method then on the Vec3 class.  This will be a substantial change though.. I'd like to avoid global variables, and I like the idea of being able to intern strings for some objects but not others...  This allows for that.. Actually, reexamining the current implementation - it isn't doing all the allocating it seems to at first.  The use of const on these means they're all only allocated once: \nstatic const Color Red = const Color._(0);\n  static const Color Green = const Color._(1);\n  static const Color Blue = const Color._(2);\nThese only require the initial allocations for each value - so three allocations.  Those three allocations are statically stored in the program and reused - there's no 'new' allocation used anywhere related to these (it's always const).  A change to this to use ints would be fairly simple, but it would also be less clear and not really save a whole lot in the end.  Are you still opposed to having it done this way?. I'm almost set on this - I've renamed the existing builders to \"ObjectBuilder\", because I think the syntax/usage will feel more natural and is easier to use for people who don't mind the additional allocations.  But I'm also generating builders closer to what you have in the other languages. Got it now.  Have the builder constructor take the parameter, and from there determine whether to allocate the map for it or not.  That makes more sense.. I suppose I could drop these for structs, but in this implementation, it's possible to build a \"flat buffer\" with just Vec3s in it (and no root table).  I believe the C implementation supports that as well.  My preference would be to document this as non-standard behavior rather than remove the ability to do it if that works for you.. Coding is done, working on upating the docs/tests.. Got it now, this should be an easy change.. So should this only appear for the root_type or can it appear on any table and just not structs?. One challenge with this - the test script requires a few packages.  I'm not really sure if or how I could resolve those dependencies without a pubspec.yaml.  I suppose I could add one to the tests folder, and run pub get as part of the test running script, but that feels a bit messy.  Are you sure you'd prefer that to having a script in the tests folder that can run the tests in the dart/test folder and keep all the dart/pub stuff contained to that (and not potentially cause confusion for future users/IDEs that will try to helpfully modify those files)?. I have no idea how this is really supposed to work.  I have no objection to it all being licensed under Apache 2, but I don't know what that means for the original work that flat_buffers.dart was based on and whether that can just be relicensed.  Either way, a LICENSE file should be included in this folder as that will show up in Pub and pub requires it to be there.  If you want me to replace this with the Apache 2 one I suppose I can, as long as Google won't be coming after me for any of this ;). Done. Perhaps have the Apache 2 license for all new code, and include the Chromium license specifically for the flat_buffers.dart file?. For some reason, some views are missing changes to this file, but I did add a \"CheckOtherLangaugesData\" that compares a C++ generated buffer, and a BuilderTest that is similar to a lot of tests the Go tests run.  . They wouldn't be able to execute properly in samples/ - they wouldn't be able to find the package files (e.g. flat_buffers.dart) correctly without some kind of special logic to rewrite them.. (again, at least as far as my knowledge of pub and packaging goes). ",
    "asaarnak": "@dnfield Thanks for a comment, Google Search got me here. :)\nCurrently the official flatc schema compiler is not supporting dart code generation.\nSeems only a format.fbs and format.dart files generated from idl.dart are using the flat_buffers.dart\nGeneration script:\n1. format.fbs generate.dart#L377\n2. format.dart generate.dart#L432. Oh :) Then i was just investigateing interesting  code. . ",
    "scheglov": "Yes, we use flat buffers in Dart, and like it.\nIdeally yes, it would be a separate package.\nUnfortunately we don't have time to do this and support it later :-(. You're most welcome to use any existing flat buffers code for Dart. I honestly don't remember exact difference between the spec and implementation. I know we added one or two new things, like list of booleans.. Yes, we tried to pack information as much as possible, because we wanted at some point to put it into Dart SDK, but now we don't. Mostly it is for indexes, see https://github.com/dart-lang/sdk/blob/master/pkg/analyzer/lib/src/summary/idl.dart#L280\nIt will not be the end of the world (at least not worse than migrating to a slightly different flat buffers implementation) is this goes away.. I don't have opinion on whether making canonicalization opt-in or opt-out, we generate code that interacts with flat buffers, so we can change it easily.\nWe store information about Dart packages in flat buffers, and names of classes and methods often duplicates across files in the same package, so we decided to implement this canonicalization.. ACK, these functions are similar, but different enough to make it easier and probably faster to have specialized implementations.. We were told by Dart VM team that allocation is cheap. I can imagine that VM might optimize this to integers. But we have not actually performed benchmarking.\nNew offset object is created for every object (table), list and string. Is this a lot? Maybe. But probably will not make a lot of performance difference.. I don't have objections against replacing Offset with plain int.. ",
    "isoos": "@scheglov: How much work is needed to cover the spec and separate it into a package? Is the spec otherwise covered? I'd volunteer for some of that work.. @aardappel: anything to be done before merging #4676?. @aardappel: did somebody publish the Dart package to pub.dartlang.org?\nAlso related: the README.md and the homepage could also include the information about support. Shall I file separate issues for them?\nAlmost forget: huge thanks for everybody involved!. Yay: https://pub.dartlang.org/packages/flat_buffers. > I've added tests in there and the sample in there (with pointers to that folder from the tests and samples folder shell scripts) - that's a typical setup for pub packages.\npub.dartlang.org will display the example file if it matches one of the many recognized patterns, e.g. example/<package_name>.dart, or example/example.dart among others. If you could rename the file it will be picked up automatically.. AFAICT this is actually a dart-y way of creating an efficient FlatBuffer vector.. (sidenote: can we call it interning instead of canonicalization)\nWe use String interning in our Dart codebase a lot, and I was planning to publish it as a library for a while. If you are interested I can do it soon-ish.. It is better to specify version ranges, e.g. here\ntest: ^0.12.33 (will be auto-upgraded when 0.12.34 will come out, but won't do it for 0.13.0).\ntest_reflective_loader : ^0.1.4. if there are no dependencies, this can be removed. For published libraries Dart usually recommends to not publish the pubspec.lock (and it is usually put in .gitignore to keep it out of the repository). IIRC this won't be an issue if you publish it, but you may get a warning about it.. @aardappel: we are encouraging certain patters, and we have settled on the example/ directory so far. You could script the publishing, and create a temp copy it from sample/ just before calling pub publish, and remove it afterwards.. ",
    "gedw99": "I am keen on using this with Flutter in 2 ways:\n\nAs a Flutter plugin on the mobiles. Flutter client <--> golang plugin \ngolang client <--> golang server . Is anyone using flat buffers with dart yet ?\n\nWould be great to have their opinion.\n. ",
    "dakom": "Okay cool- it's a little tough to go into details since I'm still kinda hacking away at the architecture, but I appreciate your insight!\nI think you've cued me onto another idea... there's a point right before my state data is sent to the main thread where I walk the \"tree\" and apply all the matrix transforms (e.g. to build a scene graph sort of hierarchy). I think instead of doing that - I'll take that opportunity to serialize everything, so instead of writing the transform data to the state, it gets written only to the render buffer. Since the render buffer can be easily read, I can still inspect that state-side for hit detection and things... \nJust thinking out loud, feel free to close this issue since you've totally answered it - by design, flatbuffers are intended for serialization really, not as a drop-in replacement for plain mutable objects.. done - https://github.com/google/flatbuffers/pull/449. hiya @googlebot - I signed it!. OK, thanks.\nIt's an easy replacement via a post-build script though, isn't actually affecting me now.... Unfortunately I already wrote around the bug... I can't remember if it was when serializing or deserializing - but it was specifically where I had something like the following:\nFoo.fbs\n```\ninclude \"./Bar.fbs\";\nnamespace Foo;\ntable Props {\n    width: double;\n    height: double;\n    color: [double];\n    matrix: [double];\n    bar:Bar.Props;\n}\nroot_type Props;\n```\nBar.fbs\n```\nnamespace Bar;\ntable Props {\n    name:string\n    geometry: [double];\n    matrix: [double];\n}\nroot_type Props;\n```\nAgain - this isn't exactly the code that caused my problem, but it's illustrative. Hope it helps!\nIt turns out that my actual use case was storing vectors of specific length, e.g. for a 4x4 matrix, so I just created a Matrix table with each variable - i.e. a:double; b:double; c:double. Slightly annoying but not a big deal at all, I'd guess it's even a bit more efficient :)\nThanks!. ",
    "red1939": "Signed.\nOn 13 November 2017 at 21:33, googlebot notifications@github.com wrote:\n\nThanks for your pull request. It looks like this may be your first\ncontribution to a Google open source project. Before we can look at your\npull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://cla.developers.google.com/\nhttps://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll\nverify. Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your\n   GitHub username or you're using a different email address. Check your\n   existing CLA data https://cla.developers.google.com/clas and verify\n   that your email is set on your git commits\n   https://help.github.com/articles/setting-your-email-in-git/.\nIf your company signed a CLA, they designated a Point of Contact who\n   decides which employees are authorized to participate. You may need to\n   contact the Point of Contact for your company and ask to be added to the\n   group of authorized contributors. If you don't know who your Point of\n   Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have\n   the pull request author add another comment and the bot will run again.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/4491#issuecomment-344048677,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ANrCaCOTCNuBUMeuVjyd6kTJZvYK99M2ks5s2KeIgaJpZM4QcXmS\n.\n. \n",
    "wittyameta": "The generated method could re-use the accessor object internally. Only the accessor for array is extra. Something like this:\npublic Weapon[] weapons() {\n  return IntStream.range(0, weaponsLength()).mapToObj(this::weapons).toArray(Weapon[]::new);\n}\nI hope I had understood you correctly.\n. ",
    "Meoo": "This can also be done using the \"INCLUDES DESTINATION\" keyword in install commands.\nSee https://github.com/Meoo/flatbuffers/commit/2b0200c1de1e32f9306797c0fa48c4b0d6430d88\nIt doesn't seem to be available before CMake 3.0 either though.. I signed it. PR: https://github.com/google/flatbuffers/pull/4528. ",
    "bluekyu": "I didn't know INCLUDES DESTINATION keyword.\nI think https://github.com/Meoo/flatbuffers/commit/2b0200c1de1e32f9306797c0fa48c4b0d6430d88 is more clear in this case because Line 177 and Line 201 is duplicated with include_directories(include).. @aardappel I also agree. This looks better.. The module is not related with OS. It is used to avoid hard-corded path name (e.g. include, lib, bin).\nAlso, the CMake installation already works well on Windows.. ",
    "trxcllnt": "I signed the CLA with the email paul@graphistry.com.. > @aardappel This makes use of some scripting to generate flatbuffers.js which makes me uneasy, can you explain how this supports all possible platforms?\nAbsolutely! You can preview changes to the npm package proposed in this PR at trxcllnt/flatbuffers-esm. The Arrow project is presently depending on this instead of the mainline flatbuffers npm package.\nExcepting those pulling the file from source-control, this PR won't affect anyone currently relying on the existing flatbuffers npm module. If it's believed that people are pulling js/flatbuffers.js straight from source-control, we can remove the entry for js/flatbuffers.js in the .gitignore.\nThis PR adds support for importing flatbuffers in newer versions of node with native ESModules support enabled. Without this change, compilers (like TS) will happily compile TypeScript source to ES6+ with ESModule imports, but users will see the above exception trying to execute the code in node with the --experimental-modules flag. Additionally, the new mjs file is compatible with newer versions of Google Closure Compiler without modification.\nI'm not necessarily a fan of the scripts that create the flatbuffers.js and flatbuffers.mjs CommonJS and ESModule files, but the approach avoids both committing duplicate files to source control, and adding additional dependencies for build tooling.\nThe JS lib seems mature enough that it doesn't require frequent updates or complex build tools. It's reasonable to revisit this assumption if the project does one day resume extensive active development.\nBuilding on Windows:\nIt's my understanding the cp command in the append-cjs-export and append-esm-export scripts aren't compatible with Windows. If you don't mind adding a dev-dependency, it's common to import and use the shx module in npm scripts instead (shx cp instead of cp).\nA bit about me:\nI'm @reactivex core team member and Apache contributor on Arrow. I coauthored rxjs (which ships in Angular2+ core) and am responsible for build tooling and compatibility for a number of other projects.\nA bit about Arrow:\nThe Arrow project is written in TypeScript. We compile the TS source to a 3x3 matrix of supported JS versions and module formats, as well as ES6 + ESModule \".mjs\" files for use in node with the --experimental-modules flag. We compile with the most restrictive TypeScript compilation settings available, and use closure compiler, webpack, and uglifyjs-es at different points in the build.\nAt the moment we have a battery of 117,420 unit and integration tests that are executed against each of compilation targets to ensure compatibility and correctness across the entire matrix.. @aardappel I don't expect it to break anybody using Closure compiler. The full diff of changes to the  flatbuffers.js file that's downloaded from npm is:\ndiff\n- // Exports for Node.js and RequireJS\n- this.flatbuffers = flatbuffers;\n/// @endcond\n/// @}\n+ \n+ // Exports for Node.js and RequireJS\n+ this.flatbuffers = flatbuffers;\nTo any JS interpreter, we just moved the /// @endcond comments up a few lines. As best as I can tell from the Closure compiler docs + testing with the online API, CC doesn't care whether the export is inside or outside the @cond annotation comment brackets.\nI can't say conclusively whether this would matter to older versions of the Closure compiler, but since I can't find any references to @cond in the docs, and a google search for closure compiler \"@cond\" yields no results, I think we're safe.\nWe compile ES6 to ES5 in the Arrow build via Closure compiler with advanced optimizations. This is a relatively new feature in Closure compiler, and they don't allow you to mix JS module systems, so adding the .mjs file (with the ES6-style export) will actually enable people to use newer versions of Closure compiler going forward.. > @aardappel I'd prefer it if this keeps working for people who work directly with our repo, I am sure there are some :)\nAh, then feel free to commit the flatbuffers.js and flatbuffers.mjs files to git as well :-). @aardappel oh, yeah! sorry, I wasn't sure if you wanted to defer that decision until after this PR is merged or not. Will commit that here in a bit.. @aardappel added the files and rebased from master \ud83d\udc4d . @aardappel after this PR, flatbuffers.src.js is the source of truth, and flatbuffers.js and flatbuffers.mjs are the derived files.\n\nnpm run append-cjs-export creates the flatbuffers.js file, with the legacy export appended\nnpm run append-esm-export creates the flatbuffers.mjs file, with the ES6 export appended\n\nNode needs the .js and .mjs files to share the same name (with different extensions) to work, and the .mjs file can't have both the legacy export and ES6 export because that breaks Closure compiler.\n\nThat way we don't even need to have the copies in the repo?\n\nYes that would be my preference (and why I'd gitignore'd the generated files initially), but you raised concerns people may be linking to the files directly via git instead of using the published packages, and I'm happy to defer to you here.. @aardappel we can do that, but removing the legacy commonJS export from flatbuffers.js will break backwards compatibility for builds that have the path to js/flabuffers.js hard-coded.\nIf you're fine with that, then we'd need to rename the derived files to something like flatbuffers.lib.js and flatbuffers.lib.mjs, and set the \"main\" field in the package.json to js/flatbuffers.lib (without the file extension).\nDo you want to schedule a quick hangout or slack call to discuss options? You can reach me at the email in my github profile, or via ptaylor@apache.org. @aardappel alternatively I could use awk to find and replace the legacy commonJS export with an ES6 export to synthesize the mjs file on publish, but I figured a PR with an awk script might be even less well-received ;-j. @aardappel awesome, thanks! I'm available now in hangouts. @aardappel alright, good to go! I'd mixed up sed and awk, meant to say sed before. So now the js/flatbuffers.js source file remains unchanged/the source of truth, and npm publish synthesizes the js/flatbuffers.mjs file like this:\n$ sed \"s/this.flatbuffers = flatbuffers;/export { flatbuffers };/\" js/flatbuffers.js >> js/flatbuffers.mjs. @aardappel shouldn't be. If you're using npm >= 4.0.0, npm will execute the \"prepublishOnly\" script just before pushing the files. If you're using a version of npm < 4.0.0, you can do npm run append-esm-export to create the mjs file first, then do the normal npm publish.. ",
    "mitjap": "Like you concluded, we changed uoffset_t  and soffset_t to uint64_t and int64_t respectively, due to large amount of data we need to save. However this limits us to use only 64bit offsets. I was hoping we would be able to accomplish the same in javascript but it does not seem to be as straightforward as it was in c++.. That's true. However, there should be no problems just serializing 32bit ints (JS) to 64bit offsets (flatbuffer) and vice versa.. ",
    "vishvananda": "What happened to this. Looks like there is potential for pypi ci/cd but it isn't enabled?. did you see the comment about running pip install wheel? I think you need to run that to make the bdist_wheel command available. ",
    "Tinche": "Please do this, it's not that straightforward to depend on git and pip will yell at you anyway.. Here are the steps, generally:\n\nin the python directory, run python setup.py sdist. The current setup.py is set up to get the version from the environment variable 'VERSION', so set that beforehand. If you don't set it it will fallback to using the date. This will produce a 'dist/flatbuffers-1.9.tar.gz' source archive.\nUpload this archive to PyPI. The best tool for this currently is Twine. (https://pypi.org/project/twine/) Install Twine into a virtualenv (or globally, if you're brave) and just run twine upload dist/flatbuffers-1.9.tar.gz.\n\nYou can do a simple smoke test into a virtualenv before uploading (and use this virtualenv to install twine).\nIn the python directory (or anywhere, but let's do it here), run python3 -m venv .venv. This creates a virtualenv in the .venv directory. Activate the virtualenv: . .venv/bin/activate (there are multiple activate scripts for different shells). Once activated, run pip install twine to install twine into the virtualenv, and pip install dist/flatbuffers-1.9.tar.gz to install the package you've just made. Then jump into the interactive interpreter and try importing the package:\n```\n$ python\n\n\n\nimport flatbuffers\n(no error)\n```\nOnce that's done, do the twine upload command, provide your credentials and you're done.. You can hide the old versions I think. It's probably parsing the version string as a tuple of integers, and 2015 > 1.. No, the problem is the setup.py is set up to try to fish out the version from an environment variable (and from the date when that fails) when it's run, and it gets run when you install the default type of Python package (a source distribution, what we're uploading). That's simply how Python packages work.\n\n\n\nThere is another kind of Python package called a wheel. They are a little different in that no code gets run when you install from a wheel, just files get copied. (They can also be platform-specific and contain binary code for their platforms, that's how you can for example install numpy on Windows/macOS/Linux without a compiler. But that's beside the point.)\nThe downside of wheels is you need a slightly newer pip version to install them. But they have existed for a long time now so I don't think that's an issue any more. Would you like instructions on how to build a wheel (it's very similar)? We can have both wheels and sdists (.tar.gz) up on PyPI at the same time.. Sorry, I didn't have the time to respond until now.\nAdding to what haroal wrote,\nyou will need to pip install wheel before using python setup.py bdist_wheel. (To install the actual library for making wheels.)\nIf you create a setup.cfg file next to setup.py and put this in it:\n[bdist_wheel]\nuniversal=1\nyou will create a universal wheel (both Py2 and Py3). Then you can upload just that.\nI think uploading wheels will actually solve the version thing, but we should test it out beforehand. The current problem is that executing setup.py reads or generates the version when it's run, and source distributions work by running the setup.py both when building the distribution and when installing it. Wheels gets their version during building and put it metadata.json inside the archive.\nHere's a way to test. Generate both a sdist and a wheel. Then remove the VERSION environment variable and install the actual package into a virtual environment. Then look at what the environment thinks what the version is.\n```\n\npython3 -m venv .venv\n. .venv/bin/activate.fish\n(.venv) > pip install dist/flatbuffers-1.9.tar.gz\n(.venv) > pip list\nDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\n--- flatbuffers (20180622121247) ---\npip (9.0.3)\nsetuptools (39.0.1)\n```\n\nNow for the wheel:\n```\n\npython3 -m venv .venv\n. .venv/bin/activate.fish\n(.venv) > pip install dist/flatbuffers-1.9-py2.py3-none-any.whl\n(.venv) > pip list\nDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\n--- flatbuffers (1.9) ---\npip (9.0.3)\nsetuptools (39.0.1)\n```. The generated import statement is incorrect and will result in an ImportError when you try reading a Conversation.. I can take a look at fixing bool too.. Thanks for merging this in!. Pushed a small DRY refactor.. Addressed :). Damn, you're right. I'll look into writing a fix.. @aardappel Fixed.. I think serializing is fine, don't know what to improve.. \n",
    "tarehart": "I'm a bit new to pip, so just to make sure I understand--the current guidance is to use a github url in my setup.py dependency_links, and then tell all my clients to use --process-dependency-links when installing my package?\nAnd the aforementioned pip yelling is because that flag is deprecated, so this strategy may break at some point? https://github.com/pypa/pip/issues/4187\nThat puts me in such a compromising position that I might consider copying the flatbuffers source into my own repo and that's pretty gross.\nWould you consider doing a manual publication for each tagged release until you figure out your automation? Considering that you've only had one release so far in 2018, that doesn't strike me as unduly burdensome.. Thanks, that would be great! I believe the instructions here are accurate and appropriate for your code: https://packaging.python.org/guides/distributing-packages-using-setuptools/#packaging-your-project. Yeah, it's probably because of the naming. I guess people who just do pip install flatbuffers won't get the latest version, but personally I don't mind because any sane client is going to specify the version anyway. Thanks for doing the release!. I'm getting some odd behavior when trying to install 1.9 via pip:\n```\n\npip install flatbuffers==1.9 --no-cache-dir\nCollecting flatbuffers==1.9\n  Downloading https://files.pythonhosted.org/packages/13/21/68e38fddb9271ae4f070bdd9315b44efbb37f46c3ec11bcbe56240578865/flatbuffers-1.9.tar.gz\n  Requested flatbuffers==1.9 from https://files.pythonhosted.org/packages/13/21/68e38fddb9271ae4f070bdd9315b44efbb37f46c3ec11bcbe56240578865/flatbuffers-1.9.tar.gz#sha256=5f9f88ecac3d44909eada8614920b3765ff4315a1f6f0fec7898eb337902eae2, but installing version 20180617224558\nInstalling collected packages: flatbuffers\n  Running setup.py install for flatbuffers ... done\nSuccessfully installed flatbuffers-20180617224558\n```\n\nWhen using flatbuffers==1.9 as an install_requires in my own package, I get this error when installing: my-package 0.0.5 has requirement flatbuffers==1.9, but you'll have flatbuffers 20180617224558 which is incompatible.\nThis does not seem to be fatal, but I'm worried about hidden consequences in the future.\nIs it possible something went awry with the versioning?. ",
    "haroal": "To build a wheel, it is almost the same thing as building a source package like you did a few days ago: you just have to run python setup.py bdist_wheel instead of python setup.py sdist. It will create a .whl package in the dist/ folder. Then you just have to upload it like you did before. Be sure to have a VERSION=\"1.9\" environment variable to create the wheel with the right version.\nBe aware that wheels are dependent on the python version you use: if you create it using python 3, it will works on python 3 only (same thing for python 2). So if you want flatbuffers to be compatible with python 2 and python 3, you have to create a wheel using python 3 and then do the same thing using python 2.\nYou can release only wheels, but it is recommended by pypi to also release a source package to be compatible with old pip versions (that don't handle wheels).\n\nYou should always upload a source archive and provide built archives for the platforms your project is compatible with. In this case, our example package is compatible with Python on any platform so only one built distribution is needed.\nSource: Python doc\n\nPublishing a wheel won't do anything to correct the current mess because it will be added to the \"1.9\" version on the Pypi repo. I think the best way to clear it, is to download the old versions, remove them from pypi and reupload them with a 0.x version, in order to be sure that Pypi considers them as old packages. You should also add a message to indicate that they are equivalent to the 2015.x versions. This way, people who use them will have an error (only if they reinstall their dependencies, meaning that prod should continue working) but they will just have to set the version number to the new one to make their code working again, without forcing them to upgrade it.\n. #4736 and #4773 fixe the bool case.\nThanks! I close this issue.. I don't know if it is possible to easily fix it with the current codebase, but there is a bug with this commit: if you set a default value to a bool variable in your schema, and that its value is equal to this default value, the returned value is always False, even if the default value was set as true (here). Actually, I splitted my schemas into multiple .fbs files and I just want to import all into a single .fbs file (here parent_file.fbs) before compiling with flatc. I just noticed that the include path is not relative to the current file (as I expected) but to the file which includes the current file. \nI thought it would be clearer to know where are included files into the folders just by reading the children_file*.fbs file... But I understand that I can define include path without ./ so I'm ok with that as it looks like C/C++ includes :smile: \nThanks!. ",
    "amiralia": "Are flatbuffer releases cross version compatible? E.g., can a flatbuffer written by version 1.10 be read by version 2015.*?. ",
    "Olipro": "\"installing\" with CMake might just mean outputting to an intermediate directory for your own project's consumption. I don't see any reason why installing should be considered a release-only action.\nThis also applies in cases where you might want to package the debug builds to ship elsewhere for testing.. The meaning of \"install\" is entirely up to the end-user (or builder)\nIf I specify my own prefix to CMake, it will install there.\nOn the other hand, if I specify absolutely nothing at all, never run say, make install but instead run CPack, I can get a nice ZIP/RPM/DEB/MSI/etc out of it.\nIf it really bothers you that much, it can be given its own separate install() when building debug which passes OPTIONAL and/or EXCLUDE_FROM_ALL\nI'm happy to do this myself - I'd just like to get some agreement on what's a reasonable solution before writing CMake code that's never going to get merged.. Because my build process likes to ship the packaged stuff elsewhere.\nOr perhaps because I don't want to have to be aware of flatbuffers internal build process directory structure in order to use it in case it changes in future.\nI can go on with reasons, but in any case, I'm more interested in the justification for opposing adding this option which will impact absolutely nobody unless they explicitly opt to utilise it.. ",
    "lfjones": "At the very least the \"how to build\" docs need to be updated to reflect what is now required to get flatc installed. (I just got burned by this.) Prior to 1.8.0, cmake -G \"Unix Makefiles\", make, and make install would put flatc in /usr/local/bin. But with 1.8.0 you have to know more about cmake, and add the -DCMAKE_BUILD_TYPE=Release, since Debug is the default. Or edit the CMakeLists.txt file, which is beyond what should be expected for someone that just wants to use the package.\nIf you are upgrading you can easily not know this and end up with new header files but the same old flatc in bin.\n. One suggestion would be to alter the CMakeLists.txt and related files to make Release the default. One way to do it is described at https://blog.kitware.com/cmake-and-the-default-build-type/ .\nBut then not have \"make install\" depend on build type, just let the user install whatever they built.\nTo end up with debug in /usr/local/bin, which you seem to want to avoid, the builder would have to intentionally call cmake with -Dyada..yada to make that happen.\nMuch like calling autotool's ./configure --enable-debug.\nIdeally, either plain cmake -G \"Unix Makefiles\" or plain ./configure followed by make, make install should yield the same result. (if both cmake and autotools are set up). I think that's the expectation most installers would have.\nRegardless, I think the \"Building with CMake\" section on the Build page could use some more details to avoid the situation I ran into.. ",
    "lenerd": "Hello everyone,\nafter an update of my project's submodules I noticed my build is breaking because of the missing flatc binary. I have a similar setup as @Olipro and \"install\" my dependencies under some prefix in my project's build directory.\nMy other components can get their dependencies from this prefix and do not have to know where those were build. So I would rather stay with ${DEPENDENCY_PREFIX}/bin/flatc than using something like complicated/path/to/flatbuffers-prefix/src/flatbuffers-build/flatc.\nI do not know if that is the best setup, but so far it has worked out for me. Therefore, I would like to have at least the option to install flatc also in Debug mode.\n. ",
    "jaredcasper": "I have no stake in what flatbuffers does going forward since I've now updated my own scripts, but count me as another user that was bit by this when upgrading to 1.8.0 and was annoyed to spend a few minutes wondering where in the world the flatc binary went.  Just because I forgot to put -DCMAKE_BUILD_TYPE=Release shouldn't mean I get a broken install (flatc isn't that performance critical).. ",
    "jvanstraten": "Like I mentioned in the issue thread, I'm not at all familiar with the project, especially not its source code, so no offense taken. This was kind of what I was expecting, which is exactly why I tried to touch as little code as possible.\nUnfortunately, flatbuffers are only a very small fraction of the project I'm currently supposed to be working on. I'm not even sure if I'll need to use them directly in the end, or if I can leave that to the existing API. So I can't really justify spending more time on this right now. Maybe someday.\n(I have no idea if it's customary at this point for me to leave the PR open for future reference or close it?). ",
    "yfinkelstein": "I run into in this issue as well. There is definitely a problem with json produced by flatc when --strict-json is not enabled. The output is simply not a well formed json. While it's ok to leave property names unquoted - node.js and js in general use this form. But it's absolutely unacceptable to output unquoted string tokens like below:\n {                                                                                                                                                       \n  red: Red,                                                                                                                                             \n  green: Green,                                                                                                                                         \n  blue: Blue,                                                                                                                                           \n  true: true,                                                                                                                                           \n  false: false,                                                                                                                                         \n  zero: 0,                                                                                                                                              \n  one: 1                                                                                                                                                \n}\nIt should be \n{                                                                                                                                                       \n  red: 'Red',                                                                                                                                             \n  green: 'Green',                                                                                                                                         \n  blue: 'Blue',                                                                                                                                           \n  true: true,                                                                                                                                           \n  false: false,                                                                                                                                         \n  zero: 0,                                                                                                                                              \n  one: 1                                                                                                                                                \n}\nMost of schema-based json parsers will map quoted enum tokens to enum names defined in their languages such as with most json Java parsers.\n. I signed it!. I signed it!\n. I signed it!. I'm unclear on why CLA test is not passing. The email address on by github account matches by gmail account with which I signed CLA that was found per above, and my additional email on google account matches secondary on github account. At this point there is nothing else I can think of doing and need somebody at google to take a look at.. I resolved the issues that you flagged. Here is the summary:\n\n\nRestored compilation with msvc 2010. This required additional syntax fixing in both c++ and java grpc generators. Both of these files came from grpc and grpc-java projects respectively. Moving forward, if there are additional changes the GPRC APIs somebody wold have to follow my path and merge original code with the modified versions here.\n\n\nAdded a whole new top level folder called java-grpc. It's a multi-module maven project which simplifies maintenance of 3 separate java maven artifacts: flatbuffers-java, flatbuffer-java-grpc, and flatbuffers-compiler. The compiler project publishes flatc binaries to maven in tar,gz form (zip on windows). There is a separate archive for each of the 3 platforms: Linux, osx, and Windows. The 2 java modules a test-dependent on flatbuffers-compiler and if the test fail the publishing would fail too. See https://github.com/yfinkelstein/flatbuffers/blob/master/java-grpc/flatbuffers-java-grpc/pom.xml#L57 for how the compiler is used at test time and https://github.com/yfinkelstein/flatbuffers/blob/master/java-grpc/flatbuffers-java-grpc/src/test/java/example/MonsterGrpcTest.java#L78 for an example of GRPC client and service based on FB. \n\n\nNote that a duplicated com.google.flatbuffers.*.java files under java-grpc for now. I did that because I believe flatbuffers-java needs to move there and removed from top-level java folder. Also, manual Java tests should be removed as the proper way to test Java code is inside of maven build which is what I did. However, I'm not sure how you will take this. If you disagree, just remove java-grpc/flatbuffers-java folder and remove this line: https://github.com/yfinkelstein/flatbuffers/blob/master/java-grpc/pom.xml#L203\nEverything is working, however the real challenge is ahead: you need to find the place to stick maven publishing of the new artifact into the release process somewhere. Basically, all you need to do is to invoke mvn deploy inside of grpc-java folder. However, the problem is that in order to publish flatc binary for the 3 platforms you need to have the binaries stored locally at the time of maven deploy. How that can be accomplished - I have no idea. For now, I made the assumption that flatc binary that needs to be published is located two levels higher than java-grpc/flatbuffers-compiler folder i.e. at the root folder of the project. That assumption and related considerations is documented here: https://github.com/yfinkelstein/flatbuffers/blob/master/java-grpc/flatbuffers-compiler/pom.xml#L20\nI spent quite a bit of time on this PR and would love to see it go through. Later we can fix any build-related nuances - almost certainly we may need to add a bit of automation to publish maven artifact s every time you have an official release.\nJust to emphasize: existence of current flatc binary in maven is a very big deal for adoption of flatbuffers in Java where developers are accustomed to having code generation at the time of the build rather than before the build. Doing this before the build requires them to install flatc on their build machines through some ad hoc mechanisms which is much harder and leads to various compatibility/versioning issues. . One more thing. I added .clang-format and .editorconfig at the top of the project as they were missing. Without these files I simply could not merge my changes and maintain original formatting. I copied these files from gprc so they should follow google style. But do make sure my assumptions about tab length etc are correct.  src/idl_gen_grpc.cpp was reformatted by clang-format and hence quite a few lines have changed - I simply had no other way.. Ok, I will be submitting multiple smaller PRs:\n1. For gprc/cpp_generator.cc to become compatible with grpc 1.7/1.8\n2. Small bug fix to idl_gen_json_schema.cpp \n3. java grpc code generator (flatc cpp code changes)\n4. flatbuffers-java-grpc maven jar\n5. flatbuffers-compiler for deploying fratc binaries to maven\n6. .clang-formatter, etc\nQuestion: items 3 and 4 above are sufficiently different in nature (c++ vs Java) however to test item 4 item 3 needs to be in place. Do you want to keep them in separate PRs or combine? Personally, I'd prefer to keep them separate since there may be more questions about item test and I want to get the flatc changes in and resolve them.\nI will keep this PR open simply to keep the context open but once the smaller PRs go in I will close this PR . With regards to publishing flatc compiler binaries to maven and your comment\n\nIf this means it is going to require me to build these on 3 platforms, then pull them back to Linux to be able to run Maven, then that is very inconvenient, and not sure that is something I want to commit to.\n\nas a reference point consider that protobuf/protoc is doing exactly that:\nhttp://repo1.maven.org/maven2/com/google/protobuf/protoc/3.5.0/\nThey must have found a sustainable method of building executables on multiple platforms and publishing them to maven central. I fond this page that provides a good hint:\nhttps://github.com/google/protobuf/tree/master/protoc-artifacts\nIf I get it right, they simply cross compile using gcc:\n for windows they build on Linux with MinGW-w64 (x86_32, x86_64) \n for osx, it says nothing specific but I guess they run the build and publish from a VM with osx (https://github.com/geerlingguy/macos-virtualbox-vm)  Or perhaps google has a form of mac servers .... So, now that all my smaller PRs are merged I'd like to revisit the only topic left: publishing flatc binaries to maven. Per earlier discussion, protobuf is doing this and they have detailed info/scripts on how they are doing it. The only part that's not documented is how they are spinning the VMs with osx and windows.. I was looking closer at the protobuf maven plugin code and discovered that they also support generation of code for all languages supported by protobuf and beyond: they support custom plugins written in Java. The protobuf maven plugin can wrap the Java-based protc plugin code with a native binary that protobuf complier can pick up if it's placed in the right directory and corresponding command line args are supplied. Furthermore, to simplify maven distribution they bundle the plugin with a wrapper script mvnw https://github.com/xolstice/protobuf-maven-plugin/blob/master/mvnw which can fetch maven distro on the fly at the time of code generation. \nWhat all this means is that this protobuf maven plugin effectively complements protoc to make it easy to download, run, and extend on all platforms  where Java is supported.  But this maven plugin does not come with protoc binary itself and instead relies on maven repo to download it from: http://repo1.maven.org/maven2/com/google/protobuf/protoc/3.5.0/. Protobuf project systematically publishes all new releases to maven central repo.\nI'm convinced that a minimum-sized one time effort to set up a automatic release-time flatc publishing to maven for 3 popular platforms will pay off in a form of wider adoption of flatbuffers not just for Java but for other languages as well. . One more thing which is somewhat separate from the above. I also updated the grpc generator for c++ to match the latest version https://github.com/grpc/grpc/blob/master/src/compiler/cpp_generator.cc.\nThe previous version of this generator checked in in this project is no longer compatible with the latest gprc so I pulled in the latest. However, there is one wrinkle here. The grpc generator is no longer compatible with pre C++11 compilers. For one, it uses for range loop (ex: https://github.com/grpc/grpc/blob/master/src/compiler/cpp_generator.cc#L180) which is not compiling with msvc 10 and was breaking appveyor build. For that reason I had to comment out msvc 10 build: https://github.com/yfinkelstein/flatbuffers/blob/master/appveyor.yml#L15\nI don't see an easy way to solve this problem. We can edit cpp_generator.cc to compile with old compilers but this will make the act of merging all future versions of this generator harder. Today it's a matter of changing several trivial lines and can probably be auto merged. \nThe question is - how badly to do you want to maintain pre c++11 compatibility? \n. >We recently started pushing jars to maven central for each version release, what does the work by @davidmoten add that we don't have yet? flatc?. yes the flatc binary for each platform. He packages binaries for Mac, linux, and Windows into a targ.gz and publishes it to maven repo. Then the client uses maven download plugin to fetch and unzip tar.gz at build time and invoke the compiler. There is a specialized plugin for grpc/protobuf for java that does something similar. . To summarize, there is a need for 2 new maven artifacts. . 1. flarbuffers-java-grpc which contains a single utility class FlatBuffersUtil.java which is referenced by flatc generated code when run with --java --grpc.\n2. The tar.gz assembled and published every tune there a change in flatc code affecting Java or Java/Grpc code generation . Given your objection to commenting out msvc 10 build I will cancel this PR and submit 2 separate PRs: one for java/grpc code generation, and another for c++/grpc code generation. I imagine the second will take longer to resolve since grpc folks may object to your suggestion to replace for range loop with iterator loop. Both grpc and and protobuf have anounced that they are deprecating pre-C++11 builds and C++11 is required for anything new.  . For the first maven artifact - flarbuffers-java-grpc, I can submit a maven pom file but I can not test is fully because it involves publishing google code to maven which requires account that I don't have. So, it will need your help. \nFor the second artifact - tar.gz - this is were it really needs your help with setting up a script to pull flatc executables for all 3 platforms from CI builds or whatever. The logistics of this is the hardest problem to solve.\n. I've made the changes and left  long comment in the PR here : https://github.com/google/flatbuffers/pull/4539\nTo test, simply \ncd FB_ROOT/java-grpc\nmvn install\nbut make sure the flatc binary is already built inside of FB_ROOT as it is normally done with cmake .. The maven assembly expects to find flatc in FB_ROOT/flatc\nThe above command builds and publishes tag.gz to your local maven repo (on local file system) and then flatbuffers-java and flatbuffers-java-grpc download (from local repo) and use flatc to generate code on the fly during the build. Each of the projects has a test which uses generated code and should pass.. updated PR #4539, waiting for some more feedback. Ok, thanks. \n\nWhat do you need it for?\n\nI needed an optimization to copy a child element with a simple byte copy in order to store it in some other FB-unaware store and later retrieve and decode it as FB-formatted. The knowledge of the type/schema of this child element is contextually implied when parsing it from a byte array after retrieval.\nJust one question before I close this issue. Is it true that flatbuffers spec makes no assumption about placement of a parent and its child relative to each other? I.e. a child may be placed above the parent in the byte array? My understanding is that \"backward\" placement algorithm used by all generators in this project effectively ensures that child is never placed above parent, but other placement strategies are also legit. Is this true?. >If you want to be able to copy children, the best way is to store in a nested FlatBuffer.\nOk, that's exactly what I was looking for. \nhttps://github.com/dvidelabs/flatcc/blob/master/doc/binary-format.md#nested-flatbuffers\nspells out the pitfalls of doing this. As I understand it, flatcc has support for this in the form of nested_flatbuffer attribute: https://github.com/dvidelabs/flatcc/blob/babbc8ced2138409c1e9d63509afa3585bf915aa/src/compiler/semantics.c#L549\nThat's a neat extension over flatc. Any considerations for supporting this in the future?. He also has a description of Stream Buffers https://github.com/dvidelabs/flatcc/blob/master/doc/binary-format.md#streambuffers\nwhere the placement of the child is before the parent: \nStreamBuffers treat uoffset_t the same as soffset_t \nIn that context, re your answer:\n\nNo, parent-child offsets are unsigned, so a child is guaranteed to always be later in the buffer. \n\nmy only thought is that relying on language types such as uoffset_t as a means of specification is not a robust method of producing a specification. It would be nice to have a true formal flatbuffers spec one day.\n. Do you agree that .root_struct_def_  at parser scope has to be removed or may be replaced with a vector (set really) of pointers to root types seen by the parser? This seems to be simply a fallout from the fact that multiple roots are allowed. However in those generators that today rely on this single root type pointer it will cause significant consequences. For instance- Json schema generator will have to output anyOf declaration listing all root types.. In that case I'm not clear on what code changes would you want to make. \n1. What should be the effect of multiple root_type declarations on the internal parser structures? Today it's the last one wins: if 2 or more root_types are found only the last one is remembered in root_struct_def_. \n2. What should the parser do with regards to root types when is discoveres rpc_service?\nI can work on this PR but I need to have a clear understanding of what you expect.\n. Sure, no problem. Do yo want me to submit a PR?. >And yes, we are currently not using clang format on the code\nWhat ARE you using actually? I'm simply trying hard to figure out how can I merge my changes and ensure that my editor does not change formatting accidentally. Why NOT constitute the rule that .clang-format definition is the standard for this project?. Of course if nobody enforces the formatting according to files I'm checking in here it will be useless. But it's a common practice to help committers avoid situations like I'm having with src/idl_gen_grpc.cpp in PR https://github.com/google/flatbuffers/pull/4553 where I reformatted the file according to .clang-format and it turned out that the source was not formatted with this style before. Perhaps there is some other way you know about? . >Does your editor clang-format things outside of your control? I don't understand what the problem is to not clang-format your PRs.\nNo, it does not format automatically. The problem is apparently I invoked format-all early at some point and then made a ton of edits. Now, without any formal formatting tool I'm looking at manually undoing and redoing all the changes. . >clang-format has some undesirable side-effects on e.g. #ifdef indentation.\nAnd that is unsolvable and a show-stopper problem? :). Anyway, I restored formatting in src/idl_gen_grpc.cpp in PR #4553 - please review again. But it would be nice to address the issue in this PR somehow more formally. I would accept this PR for now if only because of tab size 2 and later at the right moment reformat all code in this project based on a chosen formatting method - whatever you chose.. Very happy, you made my day :). >directory-wise, here main/java also seems redundant. Since this is a runtime file, it would make sense for me to be stored in the root java folder instead, much like grpc.h in C++ is stored with the non-grpc headers.\nI can make all these changes, but I feel you are unaware of the principles of maven. The whole idea there is that \"convention reduces the amount of markup\". All these redundant-looking directories are by convention. \nPlease take a look at the structure here https://github.com/google/protobuf/tree/master/java \nand if you stil stick with your suggestions I will make the changes.\n\nthis should probably be in `grpc/tests'\n\nI can move it there, but I thought you want all tests in one place. Where do you want the invocation of java grpc test to be in that case? Right now I placed it in standard location: tests/TestAll.sh. In contrast, the cpp grpc test is invoked from cmake. Java grpc should not be in cmake, right?\n\ndoes every dir containing Java need one of these files? this cannot be centralized?\n\nI created a parent-pom setup in my original PR for convenience of managing multiple maven jars and preserved it here. It won't hurt and chances are over time you will decide to combine publishing of flatbuffers-java and flatbuffers-java-grpc to maven in a single process. Again, for reference, the above example with protobuf/java shows similar structure: https://github.com/google/protobuf/blob/master/java/pom.xml#L210. Also, regarding moving java grpc test to /grpc/test, \nthere is generated code  for Java produced in /tests by generate_code.sh It will be really awkward to reference generated files located in /tests from within /grpc/tests. >The gRPC tests should typically be independent of the regular FlatBuffer tests since we don't want the main project to depend on gRPC\nOk, I will move it. Please answer my earlier question though:\n\nWhere do you want the invocation of java grpc test to be in that case? Right now I placed it in standard location: tests/TestAll.sh. In contrast, the cpp grpc test is invoked from cmake. Java grpc should not be in cmake, right?. I moved the java grpc test files out of /tests into /grpc/tests but kept \"maven convention\" business. \nRenaming flatbuffers-java-grpc to java would not make sense if one day you do decided to publish multiple maven jars as I mentioned. \n\nYou do need to publish flatbuffers-java-grpc to maven as I cannot do it myself. As such, you may encounter some issues (basic rule - what's not tests - does not work). Let's work together to resolve them.. I created a test script to test java grpc:\ngrpc/tests/java-grpc-test.sh\nBefore invoking the script you need to build and install (into your local maven repo) the utility jar flatbuffers-java-grpc by doing this:\ncd grpc\nmvn install\n. To publish the new jar flatbuffers-java-grpc to maven you need to go through the process similiar to the one you already use when publishing flatbuffers-java to maven. I think it's a matter of just \ncd grpc\nmvn deploy\nbut I don't know exactly how you do it.. As I mentioned earlier - I'm not surprised that maven deploy failes because it's a new artifact and I have not tested the process. I will try to fix the error above (missing project name) just by guessing where it is because when I do mvn deploy I get:\n[ERROR] Failed to execute goal org.sonatype.plugins:nexus-staging-maven-plugin:1.6.7:deploy (injected-nexus-deploy) on project flatbuffers-parent: Execution injected-nexus-deploy of goal org.sonatype.plugins:nexus-staging-maven-plugin:1.6.7:deploy failed: Server credentials with ID \"ossrh\" not found! -> [Help 1]\nbecause I don't have sonatype local configuration . Please give it another try. I assume grpc/tests/java-grpc-test.sh is passing successfully?. yes, everything is in, please merge. I think I see the problem:\nhttps://github.com/google/flatbuffers/blob/master/src/idl_gen_general.cpp#L622\ndest_cast contains the type that is not namespace-qualified in case of Java.  Inside DestinationCast\nhttps://github.com/google/flatbuffers/blob/master/src/idl_gen_general.cpp#L383\nthere is a line https://github.com/google/flatbuffers/blob/master/src/idl_gen_general.cpp#L395\nif (IsEnum(type)) return \"(\" + WrapInNameSpace(*type.enum_def) + \")\";\nbut only for CSharp - not for Java. It's also needed for Java.\n. I can clearly see that reformatting checkin is causing windows build failure by looking at appveyor build history. First red after green shows the failure https://ci.appveyor.com/project/gwvo/flatbuffers/history:\n\n. My intent was to consolidate all Java client codec in a single directory and build and test with a single maven project. In that case you would delete the existing Java folder and there would be no duplication.\nIf you don\u2019t like it, where do you suggest to put pom.xml of flatbuufers-java-grpc? Also, once you have 2 or more jars where one depends on another you really need a parent pom file to keep things consistent. That\u2019s what I achieved with my multi-module maven project.\nAs for your comment about publishing flatc for multiple platforms - that is the hardest problem and I expected the difficulties. But we really need to figure this out. Otherwise we are back in the woods with developer needing to get flatc on his own\n. ",
    "rmawatson": "when using with a table that contains a struct, due to the unique_ptr that holds the struct, it must be moved.. When I made this change I was just trying out flatbuffers and had a very simplistic Table definition. As soon as I added anything that generates unique_ptrs (like a nested Table), then an lvalue is not acceptable due to the unique ownership. So it is only appropriate for this function to accept by rvalue reference.. had a crack at custom allocation for tables/structs here. It appears to work for me. Would be great to get this or something like it added!\nThanks. latest commit appears to have no changes when generate_code.sh has been run.. This was moved due to the sizeof() in the operator new, so that the full definition is available.. this has been done and the results commited.. ",
    "ZaMaZaN4iK": "As show my experience, will be better to store Conan package in different repo like 'conan-flatbuffers'.\nAnother question is that we should update conan recipe for every release (if you haven't huge changes, just change version number). Will you be able to update it?. ",
    "yazevnul": "I signed it!. ",
    "Bharathvarsha": "Do any one have a flexbuffers example using Golang, for serializing the Maps. ohh ohhh :( , Thank you @aardappel . So, how can i write a schema.fbs for \" map[string]interface{} \" since interface{} is a dynamic dataType?. Thanks a lot!! @aardappel . ",
    "grhasgf": "Is there a plan to implement this feature for Python as well or is anyone working on it?. ",
    "hhpop": "bb!: flatbuffers.ByteBuffer;. ",
    "ylubibi": "Thanks Wouter,\nBut following API seems return different type.\nOffset > flatbuffers::FlatBufferBuilder::CreateUninitializedVector\nOffset > flatbuffers::FlatBufferBuilder::CreateVectorOfStructs\nNo sure how to use CreateUninitializedVector to create struct.\nThanks\n. Thanks ! this issue is fixed.. ",
    "salamanders": "\nThere was an effort to write a native Kotlin FlatBuffers implementation at some point, but that stalled.\n\nbummer!  I got as far in the instructions as \"write a schema file\" instead of class annotations, and realized it took some setup and configuration to use.  Before I dove in,  I was hoping a Kotlin implementation would be a beginner-friendly onramp to using flatbuffers without the cognitive overhead - the FlatBufferStream(FileOutputStream(file)).use{it.writeObject(myAnnotatedObjectInstance)} hello-world version.. In theory could a Kotlin Data Class represent the schema, allowing users to completely skip the entire \"compile\" step?. ",
    "Nickersoft": "@aardappel The SearchResult FlatBuffer includes a Vector of Entry buffers, and this->dictionary refers to a Dictionary FlatBuffer, which includes a Vector of Entries as well. Here is my schema:\n```\ntable Entry {\n    id:string;\n    term:string (key);\n}\ntable Dictionary {\n    id:string;\n    name:string;\n    entries:[Entry];\n}\ntable SearchResult {\n    query:string;\n    results:[Entry];\n}\nroot_type Dictionary;\n```\nThis code would work if entry was in-fact a new Entry (built using CreateEntry()), that had an Offset<Entry> type, however, LookupByKey() returns the return_type type, which apparently is incompatible with an Offset.\nI hope I'm making sense here. It's just the fact that I'm trying to retrieve an existing entry by its key that complicates things. I could always take the values of the return_type and create a new Entry buffer with it, but that seems unnecessary and expensive. . ",
    "RobertBColton": "Seems to work:\ncpp\n    auto &fields = *root_table->fields();\n    for (auto field : fields) {\n        std::cout << field->name()->c_str() << std::endl;\n    }\nThanks!. Further, we would like type safety because GetRoot does not allow you to call SetField or GetField because it has private inheritance of Table.. We can do it algorithmically, that's what I'm hoping for, but it's not a simple matter of cases, the gmx attribute will be a full XPath relative to the root XML node in the equivalent XML file.\nA little more background may be helpful. Our project is an open source augmentation of GameMaker. GameMaker has 3 different project formats, one of these is GMX which is XML. We do not actually intend to support any of them in a new IDE, the idea here is to just map the project as closely to our format as possible and leave the rest unconverted (lossy).\nHere is a small example (shortened for brevity):\nxml\n<!--This Document is generated by GameMaker, if you edit it by hand then you do so at your own risk!-->\n<object>\n  <spriteName>spr_door_hor</spriteName>\n  <solid>0</solid>\n  <visible>-1</visible>\n  <depth>0</depth>\n  ...\n  <events>\n    <event eventtype=\"0\" enumb=\"0\">\nOur IDE for our engine used to be Java and it would use JNA to map the game resources to plain old c-style structs when \"speaking\" to our C++-based compiler about how to build the game. Whenever we would want to update these structs it would result in catastrophic failure (I think you are familiar with this since this is why you invented FlatBuffers).\nOld backend:\nhttps://github.com/enigma-dev/enigma-dev/blob/bc5dffefdfb2f754d35f636af16310118cb5fec4/CompilerSource/backend/resources/GmObject.h#L15\nMy proposed new backend I am building with FlatBuffers:\n```flatbuffers\ntable Object {\n  name:string;\n  id:int;\nsprite_id:int (gmx: \"spriteName\");\n  solid:bool (gmx: \"solid\");\n  visible:bool (gmx: \"visible\");\n  depth:int (gmx: \"depth\");\n```\nSo basically my idea here is that I am not only going to be solving the IDE<->compiler communication, I will also be killing the project conversion bird with the same stone. We are also writing a C++ IDE to replace the Java one and FlatBuffers will allow us to put it out of process.. Ok, so after some more digging I realized I can get this by making the binary bfbs conform to the reflection schema using flatc --json reflection.fbs -- test.bfbs however, the output looks redundant and I am not sure if this is correct?\nIs there a better way to do this in one step instead of two? Also, why does it seem to segfault when I attempt to get the root (this works with the bfbs) ?\ncpp\nflatbuffers::LoadFile(\"test.json\", false, &jsonFile);\n//std::cout << jsonFile << std::endl;\nauto &schema = *reflection::GetSchema(jsonFile.c_str());\nauto root_table = schema.root_table(); // segfaults\n```flatbuffers\nattribute \"gmx\";\ntable Test {\n    smooth_edges:int (gmx: \"smoothEdges\");\n}\nroot_type Test;\n```\n```JSON\n{\n  objects: [\n    {\n      name: \"Test\",\n      fields: [\n        {\n          name: \"smooth_edges\",\n          type: {\n            base_type: Int\n          },\n          offset: 4,\n          attributes: [\n            {\n              key: \"gmx\",\n              value: \"smoothEdges\"\n            }\n          ]\n        }\n      ],\n      minalign: 1\n    }\n  ],\n  enums: [\n],\n  file_ident: \"\",\n  file_ext: \"\",\n  root_table: {\n    name: \"Test\",\n    fields: [\n      {\n        name: \"smooth_edges\",\n        type: {\n          base_type: Int\n        },\n        offset: 4,\n        attributes: [\n          {\n            key: \"gmx\",\n            value: \"smoothEdges\"\n          }\n        ]\n      }\n    ],\n    minalign: 1\n  }\n}\n```. Thanks, that basically confirms what I found out. We'll have to make a decision on whether we want to include the full parser or not. My assumption is that using the binaries for schema reflection, and just putting them in the repository, is going to be faster than using the full parser to map these project files. Would you say that assumption is correct?\nI am closing because this is no longer an issue, it is in fact possible to get the JSON schema to have the attributes as I described above.. Possibly, it's not a pressing matter, for now, I've simply reversed my make lines to get around it.. It seems the decision for these to be incremental is arbitrary and not out of necessity. Every possible use case people will have for reflection is hard to define and thus you can't really deduce how much reflection data they are going to need. In my particular use case, it just happens that I do in fact need all 3, but I believe there are potentially many use cases where you may only need the attributes and nothing else.\nI truly believe that the most used option is going to be --reflect-attrs because people want to use them easily and there's only two other ways (full reflection or linking the parser), both of which are cumbersome. This is why I do not really want to see it conflated with the other command line options.\nI see two potential solutions here that I think you should consider:\n Add more short options to make it easier to combine them.\n Add an extra option that does combine all three i.e. --reflect-all.. ",
    "JoshDreamland": "To be very specific, we want an Attributes table in the TypeTable. It'd be similar to the names list (and probably appear directly after), but would offer the attributes used by each field.. Thinking I don't need to sign that. Added myself to the Google org.. To start off this discussion a little bluntly, the new codegen is kind of ugly. I could have made it somewhat prettier using vectors instead of raw arrays, but I don't understand the constraints of that table (it seems to be carefully avoiding STL containers).\nOther thoughts:\n- I did not modify the visitor API at all to make use of these changes. There didn't seem to be a backward-compatible way of doing so, and the information is technically already obtainable since the complete TypeTable is available.\n- I'm vaguely concerned about the flag implementation (--reflect-names and --reflect-attrs now imply --reflect-types, but with very little reason). I could have made a further enum constant, but that's sort of inferring a natural order, which will fade when someone goes to add, eg, documentation to the reflection API.\n- This seems philosophically at odds with the intention of \"MiniReflect,\" but I am not certain of that and don't understand why it would be. What is the advantage to keeping the reflection data small? It's one-time overhead, is it not?\nLet me know if you have any bones to pick with this implementation.\nThanks!. Yeah, my big concern is that I'm going to bloat up an API that works really well on embedded systems, just to squeeze a little extra reflective mileage out of it. It's a big direction to take this project for the tentative interest of one team (we would still have needed features after this one), but if others have a use, they should feel free to bump this thread, and I will get it cleaned up and checked in.. Technically speaking, that's not necessary, although I agree it would be odd to want attributes and not names. I have some concerns on that, though (see above).. I think doing so here would hurt readability. It saves a lot of backtracking when reading this code to know immediately that this variable is a SymbolTable. It may make a refactor more difficult, later, but I believe we should optimize the code for the reader, rather than the refactorer.. Agreed; this information is (or should be) available in a more readily-consumable, less verbose format.. This code will be useful for us, but based on your other comments, I agree that it does not belong in this project. The intention, here, was to provide users with a familiar view of the simpler data stored in the generated headers. I've replaced the methods with linear search equivalents.. I think I understand what you're going for, now. I have redone the table format to be more compact. It now only uses one extra pointer per attribute list, and I have interned the lists to prevent needless bloat. I added a macro to tidy up the codegen, too. Thoughts?. Resolved. I believe we'll run into problems very soon, trying to maintain a natural hierarchy, here. For example, we'll probably want access to offsets after we have access to attributes. Would --reflect-offsets come before or after --reflect-attrs? What if another user wanted access to documentation comments?\nFor now, I've made the change. I think we should talk about other reflection options a bit more before we finalize this, though.. Good catch; carelessness on my part (replaced the wrong boolean to enable this feature). Fixed.. All pointers, now.. It's important, actually\u2014it determines the size of the array. Otherwise, I need another pointer, as I had before. Because this struct is parameterized, it appears in memory as [\u00a0length,\u00a0name1,\u00a0value1,\u00a0name2,\u00a0value2...\u00a0] instead of [length,\u00a0keys,\u00a0values] (or, at best, [length,\u00a0pairs]).. I thought you were trying to save pointers. This array has the size of the given template parameter. I pass it around as a pointer that assumes it has length 1, but that is incorrect (the length is probably more than that, and is available in the first field of this class).. ",
    "Kostofey": "No I'm not able to create PR for now, sorry.\nI see, Thanks. \n. Thanks a lot . ",
    "florianthonig": "okay, thats it in include/flatbuffers/util.h:\n@@ -29,13 +29,13 @@\n #    define WIN32_LEAN_AND_MEAN\n #  endif\n #  ifndef NOMINMAX\n #    define NOMINMAX\n #  endif\n-#  include <direct.h>\n-#  include <winbase.h>\n #  include <windows.h>\n+#  include <winbase.h>\n+#  include <direct.h>\n #else\n #  include <limits.h>\n #endif\n #include <sys/stat.h>\n #include <sys/types.h>. ",
    "lutzroeder": "brew seems to be downloading the latest source archive and running a Unix build on it.\nThe more general question: is there a straightforward and platform-neutral way to get a copy of flatc onto a machine without installing it globally. Right now cloning the repo and running make seems to be the best option and that requires custom input for the make file generator?. ",
    "jschaf": "Yep, same username at Google.  I'm happy to give it a go.  For a high-level\nplan, I'm thinking:\n\nAdd all the Bazel stuff (WORKSPACE, BUILD, etc) via GitHub PRs.\nImport into Google making sure to only expose the internal rule.\n\nOn Tue, Jan 9, 2018 at 11:10 AM, Stewart Miles notifications@github.com\nwrote:\n\n@aardappel https://github.com/aardappel seems possible though I can\nimagine some of what we maintain in our internal BUILD files may not work\nwith Bazel and may refer to projects we can't expose. In short it sounds\nlike work, @jschaf https://github.com/jschaf (if you're the Googler of\nthe same name) seems like we may need to maintain a couple of BUILD files,\none that is open sourced and another that remains internal, you game for it?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/4574#issuecomment-356383500,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AABXcdGn_RLTRoBd59Av7dEavsbEWYVgks5tI7magaJpZM4ROq1q\n.\n. The high level plan is to mirror how tensorflow is doing it:\n\n\nDeps under //third_party/, e.g. tensorflow/third_party/flatbuffers\nEach third_party dependency defines:\n<dep_name>.BUILD - a BUILD rule referenced by new_http_archive or similar command to actually build the dependency.\nBUILD - re-exports the build rules from <dep_name>.BUILD to enable referencing rules as //third_party/<dep_name>:target instead of @com_github_<user>_<repo>//:target\nbuild_defs.bzl - Any useful skylark macros go here.\n\nThe repos will be defined in third_party/repo.bzl which will be imported by the root WORKSPACE.\n. Yep, I found out last night, that makes things a lot easier.  I ran into some issues with cc_library includes.  You mind taking a look at the PR #4608.  I'm not quite sure how to fix the error.. WIP at #4861, currently blocked by a double include error and me having no experience writing C++ test code.. I'm okay with a new PR.  . I think so you can reference the files for codelabs.  Normall, bazel doesn't let you reference files from other packages unless you export the files.. Probably not needed for this case, so I'll remove.. Done.. Done.. Done.. ",
    "tmandry": "I think this can be closed now?. ",
    "shakeel": "Unfortunately, this is incomplete without implementation of flatbuffer_cc_library() rule. \nTake a look at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/schema/BUILD to see how it is used to wrap and invoke flatc from a Bazel Build file.\nHere is how tensorflow implemented the flatbuffer_cc_library() rule, https://github.com/tensorflow/tensorflow/blob/master/third_party/flatbuffers/build_defs.bzl \n. ",
    "fabiensanglard": "Any progress on this issue, it seems we will need flatbuffer_cc_library and bazel to ship with flatc binary.. ",
    "AustinSchuh": "I rebased #4861 and made all the tests pass.  https://github.com/AustinSchuh/flatbuffers/tree/flatbuffer_cc  I can send a pull request, (maybe?) take over #4861, or you can grab the changes I did and merge them into #4861 . @jschaf, any preference?. #5061 is the new PR.  I'd love feedback :)  I left @jschaf's commit alone after rebasing and cleaned it up in follow-on commits to track ownership better.. https://github.com/AustinSchuh/flatbuffers/tree/flatbuffer_cc has the changes required to make all the tests pass.  Is there any way I can help finish this?  We use bazel and I'm looking at using flatbuffers and would rather start with no local modifications :). I need to figure out why the android build is failing, but other than that, it should be ready.  I'm going to do that later tonight.. I'm still struggling on the android build.  Any hints would be welcome.  It feels like there's a missing dependency link, but for the life of me, I don't know how to add it or find out which one is missing.. Everything passes now.  I can clean up the git history if that would be helpful.  It was hard to test locally.. @aardappel , anything else to fix, or can this be merged?. Which bazel version are you running?. Ah, got it.  I'll see if I can reproduce it maybe later today.  Likely it's just updating the WORKSPACE file to the new golang rules.. Looks pretty easy to fix, done.\n@aardappel , Is there any interest in setting up a Bazel CI to catch stuff like this in the future?  I can look into it.  Bazel has CI resources for open source projects.. Thanks @fweikert !  I was thinking of getting the basics working first and then trying out a RBE based build.  (I think I saw a description for how to run the tests against RBE before going live with it). https://buildkite.com/bazel/flatbuffers/builds/68 !. I think so?  Sounds like from conversations with the bazel folks a change needs to be pressed somewhere on the flatbuffers side to make it work.. There are a bunch of ways of doing this, but it boils down to it's not going to change going forwards unless pretty braking changes are made.\naustin[108343] aschuh-debian-x270 (flatbuffer_cc) ~/local/flatbuffers/bazel-bin/flatbuffers_test.runfiles\n$ find .\n.\n./com_github_google_flatbuffers\n./com_github_google_flatbuffers/flatbuffers_test\n./com_github_google_flatbuffers/tests\n./com_github_google_flatbuffers/tests/include_test\n./com_github_google_flatbuffers/tests/include_test/include_test1.fbs\n./com_github_google_flatbuffers/tests/include_test/sub\n./com_github_google_flatbuffers/tests/include_test/sub/include_test2.fbs\n./com_github_google_flatbuffers/tests/monster_test.bfbs\n./com_github_google_flatbuffers/tests/monster_test.fbs\n./com_github_google_flatbuffers/tests/monsterdata_test.golden\n./com_github_google_flatbuffers/tests/prototest\n./com_github_google_flatbuffers/tests/prototest/imported.proto\n./com_github_google_flatbuffers/tests/prototest/test.golden\n./com_github_google_flatbuffers/tests/prototest/test.proto\n./com_github_google_flatbuffers/tests/prototest/test_union.golden\n./com_github_google_flatbuffers/tests/union_vector\n./com_github_google_flatbuffers/tests/union_vector/union_vector.fbs\n./com_github_google_flatbuffers/tests/unicode_test.json\n./MANIFEST\nThe tests are run from the com_github_google_flatbuffers folder.  This constant comes from WORKSPACE name, which shouldn't be changing going forwards.\nIf you build the tests as part of another repo, all the test data will be in com_github_google_flatbuffers anyways.  The test will be run from the folder of the main repo (frc971_org for us).  So, in both cases, ../com_github_google_flatbuffers gets you back to the right folder.\nI can pull it out into the BUILD file if you think that would be better.. I'm fine with either, but the former is a lot easier for me to do.  I'm way better at bazel than cmake.  The challenge here is that test.cpp already includes monster_test_generated.h.  So, I can't just include a second version of everything.  I would need to delete monster_test_generated.h and modify the cmake rules to generate it as well.. I spoke too soon.  I think I've figured out what's going on with cmake.  I'll do the latter.. Done. I put it in //third_party/flatbuffers in our repo.  The trick is to then write:\nlocal_repository(\n    name = \"com_github_google_flatbuffers\",\n    path = \"third_party/flatbuffers\",\n)\nAnd then everything will just work.\nFrom what I've been seeing for the last couple of years, the trend is to use local_repository (or one of the various other ways to pull in a repository) rather than //external and bind.. I can't use test.cpp for testing the starlark and have this file checked in.  You are asking me to do 2 conflicting things.  Which of the two is more important to you?  How you want me to resolve this?  The starlark rules now work.  I am struggling to do what you want to finish the rest of the review.. ",
    "chenxiaolong": "Sure, no problem! I created a PR here: https://github.com/google/flatbuffers/pull/4583. Looks like this was implemented in https://github.com/google/flatbuffers/pull/4730. ",
    "WallyCZ": "In C/C++ all files could not have to be recompiled too and it gets broken too. And in Java too. And in other compilable languages as well. There is no guarantee on compiler level that everything will be recompiled even in most modern compilable languages. With your approach, C++ would never have inheritance, because programmer can forgot to recompile some of dependent modules. I mean your argument why not have any form of inheritance is wrong, but I can live with it. This project needs new features or it will stagnate soon (if this has not happened already).. Strange decision. I would like to contribute to another part, such as Java generated classes are much dangerous (in meaning type safety) than in C#, but with your false arguments I lost all my motivations.. ",
    "brianhall77": "CLA should be valid now.. ",
    "brendan-w": "I signed it!. ",
    "Nokia12": "Hi. ",
    "basdl": "Thanks for the answer!\nYes, will do. Though, it will take until Monday or so.... Sorry for the late reply. I was too busy to do a proper fix and to create a pull request. For the specific number above, I agree this would fix it for me.\nBut still, I fear that this is not correct as it could be. With 12 decimal places we just shifted the problem. Say, we have 4.94066e-15 it would render as 0. So I guess when we want to cover more cases, I would try and use scientific notation again. What do you think?. @zejal According to this IEEE paper the table \"Span and Precision of IEEE 754 Floating-Point Formats\" says that 17 significant decimals should suffice to display a double correctly. So if we should keep the fixed format in JSON and don't go back to scientific notation a default precision of 17 for doubles should be enough.\nHowever, I don't see any reason why we should make it an option. IMHO the most important thing when serializing/deserializing is correctness. Introducing this option one could easily break this correctness.\nBut I would still opt for going back to scientific notation (for floats and doubles) and use the default behavior when serializing/deserializing doubles. \n\nThere was a reason for us using fixed, though I don't recall what exactly. Maybe some of consumers of that data didn't support scientific notation, or it was merely an attempt to get readable floating point numbers (if you have 0.0 with some rounding error, it may show up as scientific notation that looks nothing like 0.0).\n\n\nIf some of consumers didn't support scientific notation: I would still drop that support as the flatbuffers library should first of all be correct and the consumer could still introduce support for scientific notation. If not accepted, let's introduce a flag \"--fixed-floatingpoints\" to tell the JSON serializer to use a fixed notation, which uses then 9 significant decimals for floats and 17 for doubles. Then the serializer should be correct.\nIf it was merely an attempt to get readable floating point numbers: Let's drop that feature in order to be correct and stick to default behaviour of the std libraries.\n. I will come up with a pull request. But I see it will at least come near to that what @aardappel proposed, as the default behavior of std::stringstream is not good enough.. @aardappel here is my PR: 4605. I went back to scientific notation if required. I have not included an option that allows for fixed notation output, as I still had isses to do perfect roundtrips with doubles.. > I think at least by default we'd want to keep fixed, as we likely have people depending on this.\nYes, I'd agree. Being backwards compatible is always good. But be aware: When using the fixed notation, the unit tests won't work and 17 decimal places do NOT suffice.\nTry it yourself:\n\n```\ninclude \ninclude \ninclude \ninclude \ntemplate std::string FloatToString(T t, int precision)\n{\n  std::stringstream ss;\n  ss << std::fixed;\n  ss << std::setprecision(precision);\n  ss << t;\n  return ss.str();\n}\nbool FloatToStringRoundtripTest(int precision)\n{\n  auto d = 0.01234567891234567;\n  for (int i = 0; i < 50; ++i)\n  {\n    auto s = FloatToString(d, precision);\n    auto parsed = strtod(s.c_str(), nullptr);\n    if(d != parsed)\n        return false;\n    d /= 4.3;\n  }\nd = 0.01234567891234567;\n  for (int i = 0; i < 50; ++i)\n  {\n    auto s = FloatToString(d, precision);\n    auto parsed = strtod(s.c_str(), nullptr);\n    if(d != parsed)\n        return false;\n    d *= 4.3;\n  }\nreturn true;\n}\nint main()\n{\n  std::cout << (FloatToStringRoundtripTest(17)?\"passed\":\"failed\") << std::endl;\n  std::cout << (FloatToStringRoundtripTest(16)?\"passed\":\"failed\") << std::endl;\n}\n```\n\nAs for the default precision, which is 6 in C++, may be because if you use 7, it is possible to get undesirable digits in some cases, i.e. it printing .9999999 when you'd expect .0.\nUnfortunately true and ugly! The same holds for 16 and 17 with doubles. However, to let the test above  pass, one needs to use 17 + scientific notation.\n\nSo to wrap it up: Scientific notation + precision 7/17 is the only way to be lossless on de/serialization round-trips. \nEdit: How about I implement  6/16 for precision with fixed notation and with an option hexfloat format which definitely will not loose any information? So for a lot cases the default double/float serialization should be precise enough,and for all others we use hexfloat via option?. Currently working on it. I hope I can manage to update the PR tomorrow ;-). Thanks so far for the feedback. I hope I can come up with the next commit tomorrow, or the day after tomorrow.... Sorry, that I didn't finish this pull request until now... This pull request https://github.com/google/flatbuffers/pull/4895 however, seems to have a more elaborate and complete solution to the problem.... ",
    "sachnk": "Any updates on a release for this? This is unfortunately a deal breaker for us for using FlatBuffers.... ",
    "alanchiao": "Bump on this. Having the same issue, but with float needing precision 9.. ",
    "uilianries": "I signed it!. I agree with you Injection is ugly. In general we use a wrapper to call conan_setup_tools and include the original Cmake:\n```\ncmake_minimum_required(VERSION 2.8)\ninclude(${CMAKE_BINARY_DIR}/conanbuildinfo.cmake)\nconan_basic_setup()\ninclude(CMakeLists.txt)\n```\nIn fact, we need to load Conan setup by such method. This method fills env vars, directory paths ...\nAnyway, when Conan builds the package, it copies all listed files on exports_sources to a build directory, this include your cmake. But as a copy, your original cmake won't be changed.\nAlso, we can't change your original cmake directly, because this will force to run Conan by default.\nI could create a directory with the cmake wrapper to avoid the injection. This can be done on next PR.\nAbout Bintray, my suggestion is build at least using gcc (linux), clang (linux and mac) and msvc (windows). Flatbuffers take some minutes to be built and I agree that build on each commit will increase so much your CI time.\nIt's possible to build only when a new release is created by branch name or CI vars. I'll provide this feature on next PR.\nAlso will be necessary that you or some maintainer create a Bintray account to push the packages.\nThere is a good documentation about this here\nShortly, you need:\n Create a new account\n Create a Conan repository\n* Set CI environment (Travis and Appveyor)\n   - CONAN_LOGIN_USERNAME: \n   - CONAN_PASSWORD: \nYou can obtain the API key by your Bintray profile\nAny question, please feel free to ask!\nRegards!\n. Of course! I'll check ASAP! Thanks for report!\n. The error started during apt-get update, the remotes were not achieved:\nW: Failed to fetch http://ppa.launchpad.net/openjdk-r/ppa/ubuntu/dists/trusty/InRelease  Could not connect to ppa.launchpad.net:80 (91.189.95.83), connection timed out\nAfter that, was not possible to run gcc: [gcc]: command not found.\nIndeed, the error occurred before to build anything:\nThe command \"if [ \"$TRAVIS_OS_NAME\" == \"linux\" ]; then sudo apt-get install -qq g++-$GCC_VERSION; fi\" failed and exited with 100 during .\nI just re-ran the master branch and I didn't see any server error: https://travis-ci.org/uilianries/flatbuffers/builds/337763849\nCould you re-run the build? It looks like some temporary server error.\nRegards!. Travis supports some cache option, but AFAIK it's a paid feature. So, no free lunch.\nRegards.. Hi @aardappel !\nActually there is a problem with pypi service: https://status.python.org/\nIt's global, so we have no power to fix :(. Hi! \nI just updated this PR to use less code as necessary. There is no different behavior, only less code.\nAlso, I updated to upload all packages on bintray.com/aardappel/flatbuffers, but could be customized by CONAN_UPLOAD.\nRegards!. Hi again!\nI just updated this PR to use new compiler versions.\nYou could check the CI status here:\nhttps://travis-ci.org/uilianries/flatbuffers/builds/415252608\nhttps://ci.appveyor.com/project/uilianries/flatbuffers/build/1.0.62\nAlso, I created a tag to start full Travis build:\nhttps://travis-ci.org/uilianries/flatbuffers/builds/415252831\nWhat is needed to continue this PR?. Adding @lasote and @danimtb. They are official Conan members.\n@memsharded is on vacation :tropical_drink: :beach_umbrella: . @aardappel you need to set CONAN_PASSWORD on CI env vars (Travis and AppVeyor). During the CI execution, Conan will upload all packages using your Bintray account. Your username and repo address are specified in Conan scripts.\nThe password is provided on your Bintray account, the API key.\nTo get your API key, you will need to access https://bintray.com/profile/edit and click on \"API Key\"\nFor example:\n\nReferences:\nhttps://docs.conan.io/en/latest/uploading_packages/bintray/uploading_bintray.html. Nope. I just forgot to read the comments above :)\nOn Thu, Aug 16, 2018, 7:37 PM Wouter van Oortmerssen \nnotifications@github.com wrote:\n\n@uilianries https://github.com/uilianries I had done this already a\nwhile ago (see above), and checking in travis shows that it is set. Do\nthese get invalidated every so often?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/4594#issuecomment-413705112,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AEpQHfZb50EYUB6z7PpTeyjNz-UU1KScks5uRfQUgaJpZM4ReNlQ\n.\n. Awesome! \ud83c\udf7b \n\nBintray's page there are fields related to Conan package. Unfortunately those field are not filled by Conan, they need manually update.\nBelow I filled all requested fields in red color. Also you could use Flatbuffers' logo and some tags. To edit those fields, just click on \"Edit\" button on the top level.\nCatch2 could be a nice example if you are interested.\n\n. Seems FindFlatBuffers.cmake is not been copied to the package ...\nI'll provide a PR including that file.. because it's exported flatbuffersConfig.cmake also. In your case, you will need to import FlatBuffers otherwise cmake will look for Findflabuffers.cmake or flatbuffersConfig.cmake\nfind_package(FlatBuffers REQUIRED). Just wait for https://ci.appveyor.com/project/uilianries/flatbuffers/builds/20186281. Thanks for reviewing @Croydon :smile: \n@aardappel It's good to go. All jobs worked properly.\nAfter this PR, you no longer need to update the Conan recipe version for each new release.. sure!. Hi everyone! \nPlease, apply the flatbuffers-patch.txt attached to fix that error related to Conan.\nI don't why, but I'm not able to open a PR to oss-qm\nRegards!\n. @metux sorry, I only sent git diff result, my bad.\nYes, I created a fork, but when I open a new PR, Github pointed me only google/flatbuffers as target, your fork is not listed, but I have no idea about which settings are related. Maybe I should deleted my fork and re-create it based on your fork.\n. > Did you fork from my repo or the google one ?\nMy fork is from Google, but I created my fork much time ago.. seems reasonable. I'll adopt your solution.. The point here is that conan-package-tools doesn't solve x86_64 for Visual Studio 10. Would be nice to solve all this stuff directly from package tools.. My fault! I just revised package tools and I found vs10_x86_64_enabled. I'll remove this block.. how could take both values to set CONAN_REFERENCE? Is there some trick using the API?. thanks to @solvingj. I could merge into conan on a next PR. flatbuffers installs runtime libraries on lib folder. After to install all libraries, I moved from lib to bin. Otherwise, will be necessary a workaround on package_info.\nI could run self.copy, but there will be duplicate files on package.. Done!. no, I just copied from zlib recipe. I'll update. Done!. suggestion\n    exports_sources = \"*\". ",
    "memsharded": "Thanks for the feedback!\n\nNot super happy with the large amounts of Python being introduced, because when any of this breaks, I am not sure I'll have time to figure out how to fix it. Is this how you normally build things with Conan? \n\nYes, I agree. I think an effort to reduce its complexity has to be done. Many times it is not necessary such code, for example, I am using just this script: https://github.com/memsharded/conan-zmq/blob/4.2.0/build.py to build libzmq for tenths of the most typical configurations in all OSs. I will try to collaborate to improve this\n\nAs for binary build matrix, what is most useful is that people can rely on a downloadable flatc executable, which can be just x64 release mode, and does't need all other combinations. Pre-built binaries for libs is not as useful, since FlatBuffers is small and mostly intended to be statically linked, and in most use-cases is actually header-only. But I suppose it doesn't hurt to have them, if you feel strongly about it.\n\nYes, I want to do the same for a protobuf package, to split the protoc into its own package, that will use static linkage, and just the OS and arch to be defined. I'd say it is an identical case. Pre-built binaries for libs might be well skipped with build_policy=\"missing\" in the recipe, but IMHO it is a good validation to have the binaries there, it is a proof that such platform/configuration is supported, so you don't need to find out it doesn't build when building from sources. \n\nIn particular, it would be great if this causes flatc for the 3 platforms to be available at a predictable url\nOnce uploaded to bintray, the file URL can be obtained, and it is predictable given the configuration ID, but this is a sha1 hash, so it is not human-friendly in the sense that you type some url/path/to/release/2.3/flatc and you can get it. . I am just checking and this would be an example: https://bintray.com/conan-community/conan/7z_installer%3Aconan/1.0%3Astable#files/conan%2F7z_installer%2F1.0%2Fstable%2Fpackage%2F3475bd55b91ae904ac96fde0f106a136ab951a5e. So it might be doable to automate, but the inconvenient would be that conan actually creates a .tgz file with all the package contents there.\n\nI think for distributing flatc it might be better to just upload the files to a generic repo to get something shorter and easier to automate. Should be relatively easy to automate in CI.. Not additional CI job, just an extra upload at the end of the process, uploading flatc to bintray too, but to an easier URL. In any case, I am not sure, I need to investigate, maybe bintray has some other mechanism to simplify URLs, I will have a look.\nDo you think that distributing flatc inside a .tgz file would be an issue for users or maybe not?. IMHO, this is too lengthy. I am not sure I get all the abstractions as useful, I would do just:\npython\nimport os\nif os.getenv(\"APPVEYOR_REPO_TAG\") != \"true\":\n    print(\"Skip...\")\nelse:\n    os.system(\"pip install conan conan-package-tools\")\n. I am not sure if I get why it is necessary to get the package name & version from the recipe, they are already there, no need to extract them to pass them back.. I have already seen this in another package. @uilianries it seems it could be nice to have it automated in some conan tool, to simplify the test() method. Why these extra shutil.move? Why not standard self.copy()?\nAlso, why necessary if there is already a cmake.install()?. Runtime might be simplified if doing only MD/MDd, as now conan can dynamically adjust it to release/debug according to build_type. Oh, I see, thanks for the explanation, it is ok then.. But I'd say in most cases you don't need the reference, if name/version are defined in the recipe, and user/channel are env-vars.. ",
    "solvingj": "For what it's worth, there's an alternate URL format you can use with Bintray for any repo type at the dl.bintray.com domain: \nAn example for FMT: \nhttps://dl.bintray.com/bincrafters/public-conan/bincrafters/fmt/4.0.0/stable/package/e0a02d496bbb652b6295152dfce0d3937acc0b56/conan_package.tgz\nThis still requires you construct the URL from a bunch of relevant variables, and in the case of Conan packages this includes the SHA.  It's possible to query and grep the SHA for a local binary using conan info command, but maybe in this case there's an easier way @memsharded can think of (if doing so is even necessary). \n. Indeed, I would also like clarification.  Would certainly love to eliminate this code. . ",
    "danimtb": "@uilianries @aardappel LGTM as well. Great job!. ",
    "Croydon": "Nice work anyone.\nNow it would be nice if we can get within some reasonable time frame a new release of flatbuffers, so we have actually packages on Bintray\nplus as a follow up the inclusion in conan-center . Depending on how soon a new release would happen anyway it might not be worth the effort I guess. It would require to create a new branch, back port the Conan recipe and the CI additions and adjusting them for an one-time run. (since we don't want and can't have a second 1.9 git tag which would normally trigger the package building and uploading).\nHowever, if anyone wants to take the time for this then it would be fine of course.. @aardappel Awesome! \nCould you now please request the inclusion in conan-center?\nGo to https://bintray.com/aardappel/flatbuffers/flatbuffers%3Agoogle and click the \"Add to Conan Center\" button.. @aardappel If you are logged in you should see in the flatbuffer package overview something like this:\n\n. It is now available on conan-center \ud83c\udf89 \nhttps://bintray.com/conan/conan-center?filterByPkgName=flatbuffers%3Agoogle. Wouldn't it better to use google as the username? This would create some consistency across Google libraries. E.g. I have suggested to use @google for the benchmark library as well https://github.com/google/benchmark/pull/647. Is there a specific reason for beta?. ",
    "wpalfi": "Please trigger one for 1.9!. ",
    "danielin917": "I signed it!. Done!. ",
    "lubkoll": "Thx a lot for the fast feedback! Your fix looks good. I will try it out tomorrow and then let you know about the result. Thx again, also in the name of my colleagues!. Works, thank you!. ",
    "Li-Deng": "I signed it!. ",
    "chradcliffe": "Looks like the --gen-mutable flag (which I missed in the flatc compiler) has solved this issue already.. ",
    "mjscosta": "Thanks a lot for your reply.\nWhat does the VerifySizePrefixedBuffer, do exactly?\nJust to make sure I'm using it properly.\nWhen I use size prefixed buffers, the first 4 bytes of the buffer will have a uoffset_t, with the size excuding the sizeof(uoffset_t)\nuint8_t *buf = builder.GetBufferPointer();\nint size = builder.GetSize();\nI get the size using: ?\nprefixed_size = flatbuffers::ReadScalar(buf);\nI'm getting a 4 byte difference from prefixed_size and the builder.Size(), being the prefixed_size = builder.Size() -4 that what it is expected right?\n. ",
    "Cluster444": "One is meant for human consumption and the other for tool consumption. The documentation tag is just that, documentation, the attribute is something that can be inferred using reflection. So essentially it should have both.\nThe big difference in my case is that the presence of the documentation tag does not have any effect when the flag in VS is set to \"Suppress results from generated code\". That flag only responds to the presence of the GeneratedCode attribute as that is essentially what it is for.. Sure, I'll have a look into it, I think it should be fairly simple. Two things I'll need for it to fill in the attribute required values are the tool name and version info. I'm assuming for the name \"flatbuffers\" would be ok? Is there a version string accessible from a header somewhere?. ",
    "thanhlv93": "Thanks KageKirin,\nHow can I do it with java ?\n. ",
    "robatussum": "@aardappel would I be correct in assuming that you would be recreating a linked list type structure using your proposed method?\nie. each buffer would require a reference to the previous buffer (starting with NULL). Thanks @aardappel, I'm just trying to wrap my head around it. \nI was thinking that the buffers would need some sort of an ID in order to be picked up again from disk, in the case where the vector holding the reference to each of the buffers goes out of scope and is garbage collected/deallocated. :). ",
    "ronakypatel": "Is this issue still open? Is the feature implemented already? It is going to be extremely useful feature, if one can append to the already created file. I was assuming if I get a pointer at the end of file, I should be able to append more data, which I think I am wrong. \nThis is extremely useful library for tiny but enormous amount (a few billions) of data. I look forward for append feature, through (assuming it will still support key attribute and lookupbykey method for sorted vectors).. ",
    "alexnixon": "Everything I wrote there is public domain.\nI'm not a contributor to this project - is there any chance you could PR that one line on my behalf to save me the hassle of going through legal?. Sure, no worries. It's easy enough for me to work around this in the meantime. Thanks in advance.. Thanks for fixing this.. ",
    "dvolosnykh": "@aardappel I've looked into that patch, and, yes, while it introduces .deb generator, it makes CPack bundle what is there at the moment. So, I believe that absence of FlatbuffersConfigVersion.cmake is orthogonal to that PR. This file is part of installation-related setup which is more about general CMake stuff.. Opened #4625 for this case.. @aardappel I've completed the CLA requirement, but corresponding check fails anyway. Does it take some time to complete?. ",
    "masatonagai": "I signed it!. @aardappel Thanks for your feedback. I tweaked them.. ",
    "philipsdoctor": "I signed it!. @aardappel not to lobby against my own change but for posterity, Error CAN be caught, it just shouldn't, but there's an anti-pattern of catch(Throwable ex) which would catch an Error, it'll just also catch ... other ... bad things..., an Error still implements Throwable interface it just don't inherit from Exception.  /me crosses fingers that the commit isn't reverted.  . ",
    "yazdan": "I have seen this PR which is not merged.\nhttps://github.com/google/flatbuffers/pull/4445\nHow can I make it easier for this PR to merge. ",
    "MitchelLabonte": "Thanks for the fast merge @aardappel, when can I expect the maven artifact to be updated? https://mvnrepository.com/artifact/com.google.flatbuffers/flatbuffers-java. Oh no, I'm very sorry. I had to write the code in an air-gapped environment and I tested it there.  Then, I copied the code back character by character, and I should've tested it again, this was very irresponsible of me. . @aardappel any plan to upload the new release on Maven Central?\nhttps://mvnrepository.com/artifact/com.google.flatbuffers/flatbuffers-java. I see the version of flatbuffers-parent was not updated from 1.8.0 (already released) to 1.9.0. Also, flatbuffers-java is not in the modules, do you normally deploy this artifact separately?\nThe actual error is this:\n[ERROR] No SCM URL was provided to perform the release from\nbut I can see that the SCM is specified in the pom, so I'm not sure what's going on.. \"Server credentials with ID \"ossrh\" not found!\"\nLooks like you need an account or something. . The calls are in the same order, with the addition of bb.position(0); to prevent an IllegalArgumentException when setting the limit while the position is higher. I should probably change it to bb.rewind(); instead to be clearer and to save some cycles from the .position(..) checks. What do you think?. I didn't do it this way because the method __lookup_by_key(..) is public, changing its signature would make it non backward-compatible. If you don't think that's an issue, I will implement the change.\nI also thought about just calling the overloaded method from the original __lookup_by_key(..), but since this new method could create new garbage in a case that didn't before (i.e. it would always creates a new Monster() even when the method returns null), I didn't want to make a decision on it.\nWhat's your preference on how I should handle this?. Ok, implemented with the null check.. ",
    "Zentendo": "@krojew Here is an isolated example:\nCurrent command: (Of which various combinations of flags have been tried / removed.)\nflatc  -o out/ts --ts --gen-all --no-fb-import ./main.fbs\nSchemas\nmain.fbs\n```c++\ninclude \"example/example.fbs\";\nnamespace schemas;\ntable Main {\n  example:Example;\n}\nroot_type Main;\n```\nexample/example.fbs\n```c++\ninclude \"/nested/nested.fbs\";\nnamespace schemas;\ntable Example {\n  nested:Nested;\n  foo:int;\n}\nroot_type Example;\n```\nnested/nested.fbs\n```c++\nnamespace schemas;\ntable Nested {\n  foo:bool;\n  bar:byte;\n}\nroot_type Nested;\n```\nOutput file: main_generated.ts\nImporting non-existent files which aren't generated, but instead exist within the namespaces of the single file. If the import, aliases, and every reference to the alias in the file are removed, it works and compiles in TypeScript. Another workaround is generating each file individually without --gen-all which causes flatc errors for the include paths due to absolute/relative path issues. Either way, both are very tedious workarounds. These issues only occur when compiling to TypeScript. Otherwise, the Javascript, Go, ect. targets are fine. \n```typescript\n// automatically generated by the FlatBuffers compiler, do not modify\nimport * as NS2288246575 from \"./example_generated\";\nimport * as NS2707897223 from \"./nested_generated\";\n/\n * @constructor\n */\nexport namespace schemas{\nexport class Nested {\n  /\n   * @type {flatbuffers.ByteBuffer}\n   */\n  bb: flatbuffers.ByteBuffer;\n/\n   * @type {number}\n   */\n  bb_pos:number = 0;\n/\n * @param {number} i\n * @param {flatbuffers.ByteBuffer} bb\n * @returns {Nested}\n */\n__init(i:number, bb:flatbuffers.ByteBuffer):Nested {\n  this.bb_pos = i;\n  this.bb = bb;\n  return this;\n};\n/*\n * @param {flatbuffers.ByteBuffer} bb\n * @param {Nested=} obj\n * @returns {Nested}\n /\nstatic getRootAsNested(bb:flatbuffers.ByteBuffer, obj?:Nested):Nested {\n  return (obj || new Nested).__init(bb.readInt32(bb.position()) + bb.position(), bb);\n};\n/*\n * @returns {boolean}\n /\nfoo():boolean {\n  var offset = this.bb.__offset(this.bb_pos, 4);\n  return offset ? !!this.bb.readInt8(this.bb_pos + offset) : false;\n};\n/*\n * @returns {number}\n /\nbar():number {\n  var offset = this.bb.__offset(this.bb_pos, 6);\n  return offset ? this.bb.readInt8(this.bb_pos + offset) : 0;\n};\n/*\n * @param {flatbuffers.Builder} builder\n /\nstatic startNested(builder:flatbuffers.Builder) {\n  builder.startObject(2);\n};\n/*\n * @param {flatbuffers.Builder} builder\n * @param {boolean} foo\n /\nstatic addFoo(builder:flatbuffers.Builder, foo:boolean) {\n  builder.addFieldInt8(0, +foo, +false);\n};\n/*\n * @param {flatbuffers.Builder} builder\n * @param {number} bar\n /\nstatic addBar(builder:flatbuffers.Builder, bar:number) {\n  builder.addFieldInt8(1, bar, 0);\n};\n/*\n * @param {flatbuffers.Builder} builder\n * @returns {flatbuffers.Offset}\n /\nstatic endNested(builder:flatbuffers.Builder):flatbuffers.Offset {\n  var offset = builder.endObject();\n  return offset;\n};\n}\n}\n/\n * @constructor\n */\nexport namespace schemas{\nexport class Example {\n  /\n   * @type {flatbuffers.ByteBuffer}\n   */\n  bb: flatbuffers.ByteBuffer;\n/\n   * @type {number}\n   */\n  bb_pos:number = 0;\n/\n * @param {number} i\n * @param {flatbuffers.ByteBuffer} bb\n * @returns {Example}\n */\n__init(i:number, bb:flatbuffers.ByteBuffer):Example {\n  this.bb_pos = i;\n  this.bb = bb;\n  return this;\n};\n/*\n * @param {flatbuffers.ByteBuffer} bb\n * @param {Example=} obj\n * @returns {Example}\n /\nstatic getRootAsExample(bb:flatbuffers.ByteBuffer, obj?:Example):Example {\n  return (obj || new Example).__init(bb.readInt32(bb.position()) + bb.position(), bb);\n};\n/*\n * @param {schemas.Nested=} obj\n * @returns {schemas.Nested|null}\n /\nnested(obj?:NS2707897223.schemas.Nested):NS2707897223.schemas.Nested|null {\n  var offset = this.bb.__offset(this.bb_pos, 4);\n  return offset ? (obj || new NS2707897223.schemas.Nested).__init(this.bb.__indirect(this.bb_pos + offset), this.bb) : null;\n};\n/*\n * @returns {number}\n /\nfoo():number {\n  var offset = this.bb.__offset(this.bb_pos, 6);\n  return offset ? this.bb.readInt32(this.bb_pos + offset) : 0;\n};\n/*\n * @param {flatbuffers.Builder} builder\n /\nstatic startExample(builder:flatbuffers.Builder) {\n  builder.startObject(2);\n};\n/*\n * @param {flatbuffers.Builder} builder\n * @param {flatbuffers.Offset} nestedOffset\n /\nstatic addNested(builder:flatbuffers.Builder, nestedOffset:flatbuffers.Offset) {\n  builder.addFieldOffset(0, nestedOffset, 0);\n};\n/*\n * @param {flatbuffers.Builder} builder\n * @param {number} foo\n /\nstatic addFoo(builder:flatbuffers.Builder, foo:number) {\n  builder.addFieldInt32(1, foo, 0);\n};\n/*\n * @param {flatbuffers.Builder} builder\n * @returns {flatbuffers.Offset}\n /\nstatic endExample(builder:flatbuffers.Builder):flatbuffers.Offset {\n  var offset = builder.endObject();\n  return offset;\n};\n}\n}\n/\n * @constructor\n */\nexport namespace schemas{\nexport class Main {\n  /\n   * @type {flatbuffers.ByteBuffer}\n   */\n  bb: flatbuffers.ByteBuffer;\n/\n   * @type {number}\n   */\n  bb_pos:number = 0;\n/\n * @param {number} i\n * @param {flatbuffers.ByteBuffer} bb\n * @returns {Main}\n */\n__init(i:number, bb:flatbuffers.ByteBuffer):Main {\n  this.bb_pos = i;\n  this.bb = bb;\n  return this;\n};\n/*\n * @param {flatbuffers.ByteBuffer} bb\n * @param {Main=} obj\n * @returns {Main}\n /\nstatic getRootAsMain(bb:flatbuffers.ByteBuffer, obj?:Main):Main {\n  return (obj || new Main).__init(bb.readInt32(bb.position()) + bb.position(), bb);\n};\n/*\n * @param {schemas.Example=} obj\n * @returns {schemas.Example|null}\n /\nexample(obj?:NS2288246575.schemas.Example):NS2288246575.schemas.Example|null {\n  var offset = this.bb.__offset(this.bb_pos, 4);\n  return offset ? (obj || new NS2288246575.schemas.Example).__init(this.bb.__indirect(this.bb_pos + offset), this.bb) : null;\n};\n/*\n * @param {flatbuffers.Builder} builder\n /\nstatic startMain(builder:flatbuffers.Builder) {\n  builder.startObject(1);\n};\n/*\n * @param {flatbuffers.Builder} builder\n * @param {flatbuffers.Offset} exampleOffset\n /\nstatic addExample(builder:flatbuffers.Builder, exampleOffset:flatbuffers.Offset) {\n  builder.addFieldOffset(0, exampleOffset, 0);\n};\n/*\n * @param {flatbuffers.Builder} builder\n * @returns {flatbuffers.Offset}\n /\nstatic endMain(builder:flatbuffers.Builder):flatbuffers.Offset {\n  var offset = builder.endObject();\n  return offset;\n};\n/*\n * @param {flatbuffers.Builder} builder\n * @param {flatbuffers.Offset} offset\n /\nstatic finishMainBuffer(builder:flatbuffers.Builder, offset:flatbuffers.Offset) {\n  builder.finish(offset);\n};\n}\n}\n```\nfbs-test.zip\n. I'm not familiar with how the FlatBuffers C++ side works but I imagine you would just have to include flatbuffers in an intermediate C++ file as well as the node.h header file and create a node module which instantiates FlatBuffersBuilder and binds to its functions (and binding using the correct function names generated by the flatc javascript/typescript target). \nThen flatc would just have to generate files that import the new module instead of flatbuffers.js. It could be called flatbuffers-native.js or something  (ie: import 'flatbuffers' from 'flatbuffers-native' / var flatbuffers = require 'flatbuffers-native'), and then the generated file would only have to change one line in what it normally generates for javascript.\nProtobuf C++ bindings example:\nhttps://github.com/fuwaneko/node-protobuf\nMsgpack C++ bindings example:\nhttps://github.com/msgpack/msgpack-node\nSimple binding example:\n```c++\n// hello.cc\ninclude \nnamespace demo {\nusing v8::FunctionCallbackInfo;\nusing v8::Isolate;\nusing v8::Local;\nusing v8::Object;\nusing v8::String;\nusing v8::Value;\nvoid Method(const FunctionCallbackInfo& args) {\n  Isolate* isolate = args.GetIsolate();\n  args.GetReturnValue().Set(String::NewFromUtf8(isolate, \"world\"));\n}\nvoid Initialize(Local exports) {\n  NODE_SET_METHOD(exports, \"hello\", Method);\n}\nNODE_MODULE(NODE_GYP_MODULE_NAME, Initialize)\n}  // namespace demo\n```\nhttps://nodejs.org/api/addons.html\nhttps://nodejs.org/api/n-api.html\nhttps://medium.com/@marcinbaraniecki/extending-node-js-with-native-c-modules-63294a91ce4\nhttps://medium.com/@tarkus/how-to-call-c-c-code-from-node-js-86a773033892. ",
    "englercj": "I'm currently writing my own application that uses idl_parser and generates my own service stub file. Unfortunately since the CppGenerator is not exposed for me to easily extend it, I have a copy-paste of it in my custom application :(\nI can look into writing a PR for the reflection schema changes, but since I am not currently using that feature it might take me some time to get around to it.. That absolutely would work, I will do that for sure. For some reason in my head I wasn't running flatc and my generator on the same file, but there is no reason not to.. I like the idea of it being an index into objects(). Otherwise this would duplicate some data serialized into objects, correct?. ",
    "oberstet": "@aardappel \n\nGoing via the reflection data would be another option, and we definitely should have service data in there. \n\nhttps://github.com/google/flatbuffers/pull/4713. I signed it!. superseded by https://github.com/google/flatbuffers/pull/4713. CI passes now: forgot to regenerate the test data. @aardappel ping: anything else I should do rgd the PR? thx!. => attributes removed (note: the use of doc_comment from the base class instead of the dedicated and now removed rpc_comment triggered small changes elsewhere). I added executable flag (as with most other scripts in the repo). I don't understand what you mean here. I tried to follow closely the rest of the model - and the model works (I am using the new stuff, a bfbs file generated, reading from Python). Could you expand? What is the concern, or how would I write the reflection schema differently?. uups. moved services to the end. no, this was caused by me not reverting my random reformats as I hacked on the code;) => reverted. renamed to --bfbs-builtins (there is a tradeoff here;). ",
    "sssilver": "This is a major show-stopper for us as we need to build our project for many different platforms, including macOS/iOS, and use clang extensively even outside of Apple's platforms.\nWe'd be happy to submit a PR that fixes this, however it's tricky to ensure that the change doesn't break otherwise sane checks on compilers/platforms that misbehave, as you've pointed out.\nWhat would be the optimal way forward?. ",
    "desqaz": "Beyond the name of the options (@robert-schmidtke named it --prefix-size, and me --size-prefixed), our works seem complementary. \n@robert-schmidtke , maybe you can merge my modification into your branch and have single pull request about that, @aardappel can decide which option name he prefers ;-).\nAlso, I have another modification already coded but I don't know how it should be integrated : \nIn my use case, I have a stream of flatbuffers (size prefixed of course) and I am interested in jsonify my binary stream for debug purpose among other things. But flatc will only jsonify the first flatbuffers in my stream. So, I was thinking about adding an option like --stream, to indicates that the binary can contains more than one flatbuffers and the parsing should continue. \nI don't know if you have already thought about this usecase. For now, with the modifications I've made, the output json is a list of object instead of a single object.. Ok, I've removed this, as it is completely useless :-). I've simplified the code as you mentioned.\nConcerning code beauty, I've tried to run src/clang-format.sh, but it makes too many modifications (I don't know it is used anymore). . I've added the code to write also.\n@robert-schmidtke, I don't think it applies to GenFieldOffset as the size prefix is only for  the top root table, if I had understood correctly.. ",
    "asmallfurrywombat": "Are you referring to javascript_sample.sh? If so, I didn't miss it. It doesn't use babel, but many node projects do so i thought, perhaps, someone might have run into this. Not a blocker, in any case.. It doesn't seem to be ringing any bells and it's not blocking so i'll close it.. ",
    "emileindik": "Yes, I fixed it for myself by removing the line break between lines 129-130 of CMakeLists.txt\nI changed: \n\"${CMAKE_CXX_FLAGS} -Wunused-result -Werror=unused-result \\\n    -Wunused-parameter -Werror=unused-parameter\"\nto\n\"${CMAKE_CXX_FLAGS} -Wunused-result -Werror=unused-result -Wunused-parameter -Werror=unused-parameter\"\nThe way to line break a string in cmake 2.8.11 is like this: \n\"${CMAKE_CXX_FLAGS} -Wunused-result -Werror=unused-result\"\n    \" -Wunused-parameter -Werror=unused-parameter\"\nIt might also work in cmake 3, though I'm not entirely sure. If it does, perhaps it might be more appropriate to use that.. Thanks for the reply.\nSo the most efficient way to preserve the speed of structs is to wrap Monster struct in a table like this?\ntable MonsterTable {\n  monster:Monster;\n}. ",
    "ebdadsetan": "It would seem that protobuf faced the same issue on ios [1]. It is quite an unfortunate macro name the ios developers chose to define. Renaming the enum seems to be the cleanest way to move forward here. #ifdef/#undef feels quite hacky, no?\n[1] https://github.com/google/protobuf/issues/140\nAnd as mentioned in that protobuf issue: https://stackoverflow.com/questions/15759559/variable-named-type-boolc-code-is-conflicted-with-ios-macro. ",
    "artooro": "Yeah honestly I'm confused about the hanging as well. What I mean by that is if I try to access a field for example: name := exampleRoot.Name() nothing after that call will run.\nIf I build a minimum example program to demonstrate the issue the program panics.\nBut in our actual program it doesn't panic, and we don't use the recover function anywhere which you'd use to recover from a panic.\nI'm going to try to figure out how it \"hangs\" as that indeed is pretty odd.\nIn this project I'm working on, we are using a flatbuffer in the authentication process, which is why I'm testing with bogus data to see what happens and how to protect against it. So far it seems protection against it isn't really that easy based on the limitations you mentioned.. ",
    "ianXian": "I understand the code behaves correctly with & 0x80. \nThe ubsan getting reported is 'UndefinedBehaviorSanitizer: invalid-shift-base'. because **in can be negative, << of a negative value is undefined behavior.\nI agree changing the function signature to unsigned char is the right approach. But that's a much bigger change, requiring all of the other caller place changing this signature. considering the calling chain, that's a lot.\nWhen I running this locally, I have to put a temp fix forcing it to be unsigned char to bypass ubsan. quite annoying actually.. I signed it!. ",
    "mpirvu": "I'm afraid I don't have enough experience to do it.\nOn the surface it's just copying the src/compiler/cpp_generator.cc file from grpc into  grpc/src/compiler/cpp_generator.cc from flatbuffers and replace occurrences of \n    kCppGeneratorMessageHeaderExt/kCppGeneratorServiceHeaderExt with message_header_ext()/service_header_ext()\n. An update: flatbuffers 1.9 works fine with gRPC 1.10, so I will close this issue.. ",
    "Montoli": "Singed up for CLA.. ",
    "ianXian2": "i signed the CLA using my gmail account. my github account is from a different email. But when signing the CLA, I have specified my github account.. ",
    "cflaviu": "The changes in generated files are the results of changes in idl_gen_cpp.cpp. They are synchronized. Usually the generated code is not versioned but I guess there is a good reason for versioning. So I committed also the changes in the generated code.\nOverwriting cpp_generator.cc it's ok for me if it will happen. After this integration I will try changing the file in the upstream project.. Hi, I made the changes.. Welcome! Ok. Ok. ",
    "tokotchd": "I haven't had much luck directly accessing builder.Bytes, when doing so I end up only writing len(my_array.tobytes()) bytes to the builder.\nWhen I write with prependByte() and CreateByteVector() I end up writing about 30% more than expected bytes (8388608 instead of 6220804).  I'm not sure where or why this overhead is coming from, even looking directly at source.\nCan anyone elaborate?. The above overhead problem persists, but I have a working workaround thanks to @kbrose \nbytesOfImage = testImage.tobytes()\nImage.ImageStartDataVector(builder, len(bytesOfImage))\nbuilder.Bytes[builder.head : (builder.head + len(bytesOfImage))] = bytesOfImage\ndata = builder.EndVector(len(bytesOfImage)). @Amit072 Your points are valid, but I believe as long as you ensure that you prepare the data vector with the proper length using <object>StartDataVector()... it should be okay.\n\nThat being said, you could always change your library to expose the Prep() function and call it yourself before writing directly to the bytes for additional safety.  I'm unfortunately going to be not of much help besides the above solution as I have lost familiarity with the code and switched to using protobuffers for performance reasons. . ",
    "gyenesvi": "This addition to the builder seems useful, however, one thing is not clear. If I have a data member in a table defined with forced alignment as:\ntable Buffer {\n  data:[ubyte] (force_align: 16);\n}\nand I want to create the data array from numpy using data = builder.CreateNumpyVector(numpy_array), and then add the data with the generated method BufferAddData(builder, data) does it properly align the data? Isn't it a problem that we are bypassing the generated method BufferStartDataVector because of the use of builder.CreateNumpyVector? I guess it depends on where the forced alignment is handled actually.. It's been a while since I worked on this, but as far as I remember, the point of the question was that if the schema contains force_align (which I have seen that way I put in my simplified example), and I write out the data using the above workaround (which does not seem to handle alignment), will that be a valid document, i.e. will it match the schema? \nI was assuming that in the above case, the force_align is for the data member of the table, and not for the array itself (not between array members, just the beginning of the array, which seemed reasonable to me). From the documentation it seems you are right @felixfrank, that force_align is defined for structs (so the point is that it is ignored for tables, and not that it cannot be applied for arrays), so it is ignored in this case anyway, so the above workaround produces proper buffers, right?. I believe that the alignment that builder.CreateNumpyVector takes care of is fine for me. I was only confused whether that produces valid documents, but it seems now that yes, since force_align does not matter here.. ",
    "felixfrank": "@gyenesvi, correct me if I'm wrong, but the way I read the flatbuffer docs and interpret the underlying idea of force alignment, is that it is an attribute of structs and cannot be used for arrays. As far as I know, if you add the attribute to a table field (like in your example) it is just ignored.\nAlso I'm curious, why would you actually want to add additional padding between individual array members of the same type?. I see. Let me elaborate on my previous statement that force_align cannot be used for arrays. This basically follows from:\n1. force_align is ignored for tables\n2. flatbuffer structs cannot hold arrays (because structs are compile-time fixed size objects, whereas arrays in flatbuffer schemas are variable length). \nThat being said, you are absolutely right in that the builder.CreateNumpyVector only takes care of the alignment according to the datatype of the numpy.array members and the additional uint32 field for the array size (which is what the Start...Vector function in the generated code does as well). In that regard it does produce proper buffers. \nIf you require a different alignment it get's tricky. I guess you can use the builder.StartVector function and pass the required alignment there directly.. I signed it!. @rw can we merge this or do you have additional comments? . @rw Please review again. @rw ping. So any update @rw @aardappel ?. Yes,\nthe compat version of import_numpy checks first if numpy is an installed module and then assigns np to be either the numpy module or None. So my intuition for byteswap was to avoid looping over a long vector in python. numpy uses the C for-loop and the cpu BSWAP instructions (if available), which I assumed to be much quicker than anything available in python. Additionally byteswap deals with the datatype specifics (different swapping instructions depending on itemsize). The additional cost of a memory allocation should be small compared to the overhead of the python for loop.\nI would definitely try to avoid modification of the input data. The in_place default is False, which should prevent that, however, I think enforcing it, just in case they change it in future versions, should be done.\nTo verify my assumptions I did a quick benchmark for three different variants of the function.\n\ncurrent implementation\nPython for loop with the correct builder.Prepend method\nPython for loop with a direct call to struct.pack_into\n\nI serialize vectors of different sizes 1000 times and compute average and maximum time needed for serialization.\nCode\nResults:\n\nExcept for very small vectors byteswap significantly outperforms the other two methods. Note that additionally the other two methods would need big if clauses to modify the behavior according to the correct datatype.. I was wrong on this one :)\nFound a way to test it. done. Done. I allow bool, int, unsigned and float elements.. ",
    "bencyoung": "Thanks! That's what I assumed, just didn't see it spelled out. ",
    "renato2099": "Thanks for getting back to me @aardappel !\nI am getting warnings like the following on the generated code:\narrow/java/format/target/generated-sources/flatc/org/apache/arrow/flatbuf/Block.java:17: warning: no @ return\n[ERROR] public long offset() { return bb.getLong(bb_pos + 0); }\nSure! I would be happy to get rid off these warnings. Thanks!. ",
    "jasonpfi": "FBS Schema containing enumerations to be used. Named Aenums.fbs:\nnamespace A;\nenum Example1: ubyte\n{\n    Unknown = 0,\n    Logic,\n    Signal\n}\nenum Example2: ubyte\n{\n    None = 0,\n    Open,\n    Close,\n    On,\n    Off\n}\nenum Example3: ubyte\n{\n    Reset=0,\n    Running,\n    Stopped\n}\n. The File that requires the Enumerations:\ninclude \"Aenums.fbs\";\nnamespace A;\ntable NewTable\n{\n    FieldNumberOne  : Example1 = Unknown;\n    FieldNumberTwo  : Example2 = None;\n}\nroot_type NewTable;. Resulting Typescript function from flatc 1.7\nThe NS118... file is created by flatc to represent the enumeration schema\nBut is not included at the top of the generated file, resulting in an \"unknown namespace\" error\n/*\n  @returns {A.Example1}\n */\ntransitionExample1():NS11858840996067445030.A.Example1 {\n  var offset = this.bb.__offset(this.bb_pos, 6);\n  return offset ? /** @type {A.Example1} */ (this.bb.readUint8(this.bb_pos + offset)) : NS11858840996067445030.A.Example1.Unknown;\n};\nSo I have to add this Line:\nimport * as NS11858840996067445030 from './Aenums_generated';\n. I don't know if it's important, but I'm building my application using \"ng serve\", which is the angular command that fails.. ",
    "SupJoeeeeeey": "@aardappel Hi, I am still running the issue with the latest flatc compiler, has the issue really been fixed already?\nMy case is exactly the same with the description provided by @jasonpfi . @aardappel Well, I just checked again, and I was using 1.10.0 from https://github.com/google/flatbuffers/releases\nActually, I think this can be easily reproduced. If you define some enums in a fbs file, and include it in other schemas, while compiling the fbs files, the enum fbs is actually not imported into the other schemas.\nHowever, if defining a table type data in this fbs file, and using it in other schemas, it can be correctly imported.. ",
    "rjkat": "I have opened a PR on FlatBuffers.jl for this and am almost ready to open a PR on this project, pending final legal approval.. Looks like only VS2010 failed - all the other ones passed. I'll attempt to hunt down a machine with VS2010 and reproduce...\nEdit: fixed!. I don't have any objections to bringing the Julia code into the flatbuffers project. One thing to consider would be the license for this code, @dmbates is the original author and @quinnj has also made substantial contributions. I also don't know how we could then release it as a Julia package - Julia expects packages to be git repos in their own right with a certain structure. Would adding it as a git submodule be an acceptable option?. I just brought in the FlatBuffers.jl package. I haven't preserved the git commit history from that repository. My thinking was that this is something we can do later if the need arises. . @rw thanks for your enthusiasm :) and for sure, happy to add a Docker test for Julia. I've got some time over the next couple of weeks to work on this.\n. Sorry for the delay on this. The remaining tasks to be done are:\n- Incremental building/reading\n- Docker tests for Julia\nOne question in my mind is whether having a patch for the generated code as part of the test is acceptable. Currently the generated code for the Monster test needs to be tweaked - since Julia does not have forward declaration of types, the types which reference each other need to be parametrised.\nhttps://github.com/JuliaLang/julia/issues/269\nIf this is not acceptable as a solution, then there will be significant effort required to make the generator spit out parametric struct definitions. \nI'm on holiday for 2 weeks from today, but I am keen to keep working on this when I'm back at the end of the month.. @ExpandingMan the only thing left to do here is make the generator support tables that reference each other.\nThis would mean it would have to spit out type variables for types that haven't been defined yet, based on the include order. See JuliaLang/julia#269 for some context. It's a fair amount of work which I don't have time for at the moment, but I'm hopeful I may be able to get to it eventually. . suggestion\n  * rk = rjkat. suggestion\n                       RTYPE, JLTYPE) \\. suggestion\n~~~{.jl}. suggestion\n~~~{.jl}. suggestion\n~~~{.jl}. suggestion\n~~~{.jl}. I feel like this is a question for @quinnj. There is a lower-level API in Julia for reading/writing flatbuffers directly however it relies on metaprogramming to determine the size of struct fields, meaning you have to construct a language object in order to be able to serialize one. Just so I'm clear, is performance the reason you want to avoid creating language objects? If so, we could run some tests.. Unfortunately, if FBS files depend on other ones, then yes this is true until Julia has forward declaration of types. https://github.com/JuliaLang/julia/issues/269\n. I'm just looking at updating the documentation for this and it seems a bit unclear. The default seems to be that code will be generated from included files unless the --no-includes option is specified. However in Schemas.md it says \nWhen using the `flatc` compiler to generate code for schema definitions,\nonly definitions in the current file will be generated, not those from the\nincluded files (those you still generate separately).\nWhat should I write where?. This is basically because I've gone and added the whole Julia package in here. I already have an external link to these documents in the docs/ folder, so you can get to them from the website. Until we are actually releasing the Julia package out of the flatbuffers project, it might be best to keep the documentation for that package along with it and link to it externally for now? . Again, artefact of adding the julia package in here. Might be best to keep the tests for that package along with it rather than duplicating them in here until we are releasing the Julia package from this repository?. Okay here is my hokey performance benchmark.\n```\njulia> include(\"MyGame.jl\")\njulia> using FlatBuffers, .MyGame\njulia> mutable struct HpTable\n          hp::Int\n       end\njulia> struct HpStruct\n          hp::Int\n       end\njulia> io = IOBuffer()\njulia> @time [FlatBuffers.serialize(io, MyGame.Example.Monster(;hp=42)) for _ = 1:1e5];\n 14.450891 seconds (16.88 M allocations: 2.280 GiB, 1.39% gc time)\njulia> @time [FlatBuffers.serialize(io, HpTable(42)) for _ = 1:1e5];\n  1.284482 seconds (3.55 M allocations: 220.096 MiB, 1.91% gc time)\njulia> @time [FlatBuffers.serialize(io, HpStruct(42)) for _ = 1:1e5];\n  0.497959 seconds (1.28 M allocations: 103.010 MiB, 4.31% gc time)\njulia> @time [FlatBuffers.serialize(io, 42) for _ = 1:1e5];\n  0.429897 seconds (1.26 M allocations: 79.441 MiB, 1.92% gc time)\n```\nSo it seems constructing a whole object (at least in the case of a Monster) consumes an order of magnitude more time and memory than constructing a one-element table, which is about 3 times slower than constructing a one-element struct. Sounds like the ability to create serialised data directly would definitely be a big performance win. However, it would be a significant API breaking change to the FlatBuffers julia package (which is already in use by a non-zero amount of people). \nI'd like the addition of Julia to be true to the original intent of FlatBuffers, and for the generated code to be as performant as possible. I'm prepared to put in some effort to make this happen. It would be a significant increase in scope on the current changes, however. Could we consider integrating Julia support as-is (i.e. suboptimal), and later adding the API for direct serialization? @quinnj do you think this could be a \"1.0\" breaking release of the FlatBuffers package?. Changed to Google Inc.. Done!. Done!. Done!. Done!. Done!. 100% agree, this is the point of code reviews - making sure new stuff fits as closely as possible :)\nJust so I\u2019m clear on the desired behaviour, let\u2019s take namespace_test as an example. There are two .fbs files - namespace_test1.fbs and namespace_test2.fbs. namespace_test1.fbs defines a namespace NamespaceA.NamespaceB. namespace_test2.fbs includes namespace_test1.fbs and defines some other things in NamespaceA. With the singleton class, the Julia code generated looks like this\njulia\nmodule NamespaceA\n    include(\u201cNamespaceB/NamespaceB.jl\u201d)\n    include(\u201cTableInFirstNS.jl\u201d)\n    include(\u201cSecondTableInA.jl\u201d)\nend\nWithout the singleton class, the include(\u201cNamespaceB/NamespaceB.jl\u201d) is lost and the TableInFirstNS type is missing definititions for the types of its fields. \nTo be explicit, the definitions for NamespaceB are not generated here, but we need to keep track of which files to include, hence the need for the ModuleTable. This issue could potentially be resolved by doing away with the object API altogether as discussed elsewhere in this PR. . I'm with you. I think I may have a solution to this and the inplace reading/writing. I'm on holiday this week but keen to get this sorted early in the new year. . Spent a bit of time looking at this today. It doesn't appear that Julia modules can be re-opened in the same way that C++ namespaces can. Also, Julia does not use prefixing - files must be explicitly included. Since there are no forward declarations, the generated files must also be included in the right order. It does seem like there should be a way to do this, but the solution is not obvious to me and I suspect it is quite hairy and involved. . I haven't, but I don't feel well acquainted enough with any of the Julia package experts to bug them. @quinnj might have better contacts than me. \nThe only solution I can think of is to save the module hierarchy in a structured format. When you need to add something into the hierarchy, merge it in and then eval() everything into existence. Something like this:\n```julia\nNamespaceTest1.jl\nmodule NamespaceTest1\nfunction getmoduletree()\n(\n    name=\"NamespaceA\",\n    includes=[\n        (name=\"NamespaceB\",\n         includes=\"NamespaceB/NamespaceB.jl\"),\n         \"TableInFirstNS.jl\"\n    ]\n)\nend\nend\nnamespace_test1_generated.jl\nfunction loadmoduletree(root)\n    quote \n    module $(root.name)\n        $([isa(String, n) ? :(include(\"$n\")) : loadmoduletree(n) for n in root.includes]...)\n    end\n    end\nend\ninclude(\"NamespaceTest1.jl\")\nimport .NamespaceTest1\nroot = NamespaceTest1.getmoduletree()\neval(loadmoduletree(root))\nnamespace_test2_generated.jl\nfunction mergemoduletrees(tree1, tree2)\n    # merge two module hierarchies together\nend\ninclude(\"NamespaceTest1.jl\")\nimport .NamespaceTest1\nroot = NamespaceTest1.getmoduletree()\ninclude(\"NamespaceTest2.jl\")\nimport .NamespaceTest2\nroot = mergemoduletrees(root, NamespaceTest2.getmoduletree())\neval(loadmoduletree(root))\n```\nThis is not pretty, and means we are circumventing Julia's module system, which makes it tricky to work with the generated code. That said, if you think the benefits outweigh these costs I'm happy to give this a go.\n. thanks to a slack session with @quinnj yesterday, have come up with a solution to this. Singleton class is gone. Final things left to do are the API for incremental building and adding Docker tests for Julia. . ",
    "xiaogaozi": "@aardappel Any plan to release v1.9.0 flatbuffers-java to Maven Central?. ",
    "greenmonn": "I signed it!\n__\n\uc724\uc8fc\uc5f0 \ub4dc\ub9bc.\nJuyeon Yoon\n\ubcf4\ub0b8 \uc0ac\ub78c: googlebot\n\ubcf4\ub0b8 \ub0a0\uc9dc: 2018\ub144 4\uc6d4 3\uc77c \ud654\uc694\uc77c \uc624\ud6c4 6:38\n\ubc1b\ub294 \uc0ac\ub78c: google/flatbuffers\n\ucc38\uc870: Juyeon Yoon; Author\n\uc81c\ubaa9: Re: [google/flatbuffers] Remove line break of CMakeLists.txt (#4691)\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\nWhat to do if you already signed the CLA\nIndividual signers\n\u2022 It's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\nCorporate signers\n\u2022 Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\n\u2022 The email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\n\u2022 The email used to register you as an authorized contributor must also be attached to your GitHub account.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. ",
    "paulreimer": "I signed it!!. Good bot. Not quite perfect, I forgot to commit the generated test code! New PR is 4696. Sorry!. Yes, definitely it needs some changes -- I think template specialization on floating-point types seems best. But some of the existing behaviour is unclear (C++ has defaults but sprintf needs it specified), so I wanted to be sure of the spec first before refactoring.\nSpecifically:\n- expected precision of NumToString<float>, NumToString<double> ?\n- expected behaviour of e.g. FloatToString<int>(42, 5)?. Alright, it is DRY-er now, although longer. I'll keep using it, I have no strong urge to have it merged unless it is helping others too.. I have made the requested changes (and rebased on master), except for removing the whole IntToDigitCount stuff. I agree it's verbose, but on my target I have RTOS tasks which don't have 387 stack bytes to use as char buffer for this. \nI've written routines like that several times, I can't believe it isn't easier, or some simple standard lib provided, but oh well.. d63acf9 is the part about the std::string trailing byte that feels weird to me, but it does work.. I wonder if there is a way to static_assert whether that works? I can't see how else c_str() would work in a sane way, but I can't be sure it holds everywhere.. This does not provide the behaviour I was expecting from ForceDefaults, although it does extend the existing behaviour correctly to a flatc option.\nThe existing ForceDefaults behaviour is more of a AllowDefaults, since it will emit the default values if they are present, but e.g. it will not emit the default value for a field which is not present (which is the behaviour I want).\nSpecific use case is being able to mutate all the values from a JSON->flatbuffer conversion from flatc. (so, in this case I should not be required to specify the default value for every field in the JSON. Any field that is omitted, should be supplied a default value).\nIs my assessment of the current ForceDefaults behaviour correct? That it cannot provide defaults for a field which is not present?. Added to docs in ea5b2bb. Although IMO this feature should be called --allow-defaults/AllowDefaults(), or (ideally) be able to emit default values when not present.. Actually, my use case is that I would like only specific fields to be emitted such like this. So it seems closest to an \"attribute\" that I could use on a per-field basis. But, this PR is still useful to me with the current behaviour (I can include the specific fields with a known-default value.) Also, quick question that may also work (without needing this PR), but are all scalar values within a struct included no matter what the values? So I could wrap the fields that will be mutated in a struct?. Rebased in ea233153. I don't think --allow-defaults is the right name, either. Perhaps --emit-defaults?. Or, your thoughts on the attribute approach?. OK, I didn't fully understand the struct behaviour described above -- after testing, I don't think it will solve my problem. It seems the struct will be present when all fields are supplied (defaults or not), but there is no default struct provided if the entire struct field is absent (or using JSON {}, or null). So e.g. for a mutable vec3 I'd have to write out all fields in the struct, providing (any) values, {\"pos\": {\"x\": 0, \"y\": 0, \"z\": 0}}.. Yup, wasn't sure if it would be possible to get the behaviour mentioned in the docs. And, I was suggesting a name change of ForceDefaults to EmitDefaults as well, so it would all be consistently Emit* instead of Force* (and Force* being marked deprecated).\nIf it is \"no\" to both of the above, then I can change the docs to reflect the current behaviour, and then this can be merged (it is still helpful for me. Although I've since found that I can use ForceDefaults on/off at will during building, which gets the per-field behaviour I'm hoping for. I just wanted it in flatc, too).. Updated docs in 706a545\nStill suggest the name change for clarity, but that could be a separate PR now.. Sorry to let this go stale. I updated the wording as you suggested, it fits in a single line now.\nI do think the name should be changed, but that should be another PR.. It could be const string_view, but is currently not const. Sometimes in-place substrings on the string_view are useful, wasn't sure what is best here.. I think there is no CI build configuration which will currently enable this. At least -std=c++14 would be needed.. OK, I updated to use a typedef.\nWhat I mean about the CI configuration not catching this, is that because of the #ifdef __has_include and __cplusplus version guards, the code inside this PR is not actually getting exercised/compiled by the C++11 or lower configurations (unless there is a lot more in AppVeyor). However it is being successfully ignored for compilers which do not support it (and... well... it works on my machine!).. Is it possible to issue a one-off build in the case of PRs where C++14 / C++17 code is introduced? (I can say it does work in my embedded gcc C++14 use case). Ah, it was my understanding that <experimental/string_view> MAY be present for C++14 (not C++11), and <string_view> SHOULD be there for C++17 (but not C++14).\nI have never seen that table of C++ feature testing macros. That's part of the standard and is portable? That's awesome! Do you even need __has_include then? Or is it that the macros describe the standard, and __has_include verifies the toolchain does provide it?\nI'm not sure what the best behaviour is (on my embedded platforms they have kind of crippled  vs \"headers\"), but if there are C++14 users with the right  in place then there should be a way for them to get it.. Oh, I see now that the <version> header that would provide them for the purposes of checking, is only in C++20. So -- it will be awesome.\nI can make a PR to change them to the hard-coded versions from the feature testing macros page, I suppose that is the best that can be done for now.. https://github.com/google/flatbuffers/pull/4841. OK wait a minute, so the net effect of this in #4841 was actually to make the version checks even stricter, despite your original request that out-of-bounds C++14 versions still having string_view should be considered.\nAs well, this change broke one of my embedded toolchains which was a C++11/14-ish that still provides an experimental/string_view. So... I'm not sure whether I like the \"official\" values very much.. I'm not sure what is the right thing to do here (just make up some generous values? provide a FLATBUFFERS_OVERRIDE_ macro? remove the checks and trust has_include? something else?), but it is definitely going to work for even fewer people now, the way it is.. Rebased on latest master, updated to implement review suggestions.. Hey this doesn't work with --scoped-enums. I get the following error(s) now:\n./display_generated.h: In function 'const char* Display::EnumNameIcon(Display::Icon)':\n./display_generated.h:66:10: error: 'Icon_None' was not declared in this scope\n     case Icon_None: return \"None\";\n          ^\n./display_generated.h:67:10: error: 'Icon_Healthy' was not declared in this scope\n     case Icon_Healthy: return \"Healthy\";\n          ^\n./display_generated.h:68:10: error: 'Icon_Warning' was not declared in this scope\n     case Icon_Warning: return \"Warning\";\n          ^\n./display_generated.h:69:10: error: 'Icon_Error' was not declared in this scope\n     case Icon_Error: return \"Error\";\n          ^\n./display_generated.h:70:10: error: 'Icon_HeavyCheckmark' was not declared in this scope\n     case Icon_HeavyCheckmark: return \"HeavyCheckmark\";\n          ^\nAnd this (invalid) from the codegen:\ninline const char *EnumNameIcon(Icon e) {\n  switch (e) {\n    case Icon_None: return \"None\";\n    case Icon_Healthy: return \"Healthy\";\n    case Icon_Warning: return \"Warning\";\n    case Icon_Error: return \"Error\";\n    case Icon_HeavyCheckmark: return \"HeavyCheckmark\";\n    default: return \"\";\n  }\n}\nIt should use the scoped operator :: instead of _ in the codegen (I think?).. @aardappel Just to confirm but are you suggesting that wherever we are using DetachedBuffers in APIs, we might as well be using const FlatBufferBuilders in those cases? I do sometimes re-use the same fbb to produce multiple DetachedBuffers and for that reason use DetachedBuffer in some function signatures. But if I could(should?) switch to const FlatBufferBuilder consistently and never worry about having to release() then that would be nice.. Oh sorry I should have mentioned that the fields do read OK when I receive a message (but it is from a service over the network so I would like to verify).\nAlso, good call on those extra 4 bytes. I tried hexedit'ing 4 00s onto the end of one of the failing repro.fb buffer files and it did verify now!\nI'll note that even beyond the padding, the buffers are not binary identical as generated from C++ / flatc, the contents of the set of bytes (before what appears to be the string data from the fields)  is slightly different. But if C++ verifies that's good enough for me!. Awesome! Can't wait to check it out. Does publishing packages to pub happen when flatbuffers makes an official release, or could this specific issue maybe trigger a patch release for Dart only?. Yes! Just confirmed that buffers generated from Dart in 1.9.2 verify successfully in C++ now!. Agreed, if that is an OK alternative. (might get SBO on all of them, but IOStreams is also good here).. I agree it's a bit more verbose. This does allocate less memory (only what is required, plus 1 for the extra null byte). I can remove this calculation and allocate the maximum amount for a double (387 chars, IIRC) then resize down, but that seemed like a lot of overhead which would rarely be used. Whatever is preferred, though, I agree it could be a much smaller change without this.. (there is a portable way to get that 387 number BTW, that's just my recollection for the approximate worst-case char count). Sure, done in a6b1f73. I could maybe inject it into namespace flatbuffers { using string_view = ...; } or some such, to avoid the macro.. (would still need a boolean FLATBUFFERS_HAS_STRING_VIEW, it just wouldn't store the type in the macro this way.) I'm also not sure whether those usings are C++03-compliant (so it would have to be a typedef)?. Done in bccbfe3. Done in 7f8200b0. Done in bccbfe3, char now.. Done in a99f5ce. Done in eed85752, aea5d8a8. Done in 43394a6. Well that sounds great, I was vaguely aware of that but I thought it was an implementation detail and couldn't be relied on or used for other purposes. Removed (and removed the resize -1 later) in e04a8463. Done in 2b41739. Done in 51e2098. How does a user specify 0 precision then? Right now the precision = 0 serves for the integer types as well.. Should it indent the whole block (as in this case) even if the #ifdef is at 0 indentation? I got all the other cases where it was #ifdef / #endif was clearly within braces.. Done in 426759d.. Do these need clang-format? I'm not sure how to find where it is needed or not.. So then... it won't format the rest of the block? Is there some sort of \"disable-next-line\" flag, rather than disabling the whole block?. OK, done in 57390e63. ",
    "ChrisTrenkamp": "I would like to store the data as UTF-8, but the problem is two-fold: we're a Java only shop (as many organizations are), and there aren't any REAL UTF-8 libraries out there for Java.  The closest thing I could find was this, and even it ends up storing a UTF-16 string under the hood for caching purposes.  \nI think the reason why no one has developed a UTF-8 library for Java is because not having an unsigned byte primitive makes things awkward when attempting to write the library, and then the developer decides it's less hassle to just convert it to UTF-16.\nEDIT: I would also make the argument that UTF-16 is potentially more widely used than UTF-8 purely because of JavaScript, even though  they don't explicitly say it's a UTF-16 string. ",
    "bsoniam": "Hello,\nI have signed the CLA.\nKind regards,\n  Sonia\nOn Tue, Apr 17, 2018 at 4:13 PM, googlebot notifications@github.com wrote:\n\nThanks for your pull request. It looks like this may be your first\ncontribution to a Google open source project (if not, look below for help).\nBefore we can look at your pull request, you'll need to sign a Contributor\nLicense Agreement (CLA).\n\ud83d\udcdd Please visit https://cla.developers.google.com/\nhttps://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I\nsigned it!) and we'll verify it.\n\nWhat to do if you already signed the CLA Individual signers\n\nIt's possible we don't have your GitHub username or you're using a\n   different email address on your commit. Check your existing CLA data\n   https://cla.developers.google.com/clas and verify that your email is\n   set on your git commits\n   https://help.github.com/articles/setting-your-email-in-git/.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are\n   authorized to participate. Ask your POC to be added to the group of\n   authorized contributors. If you don't know who your Point of Contact is,\n   direct the Google project maintainer to go/cla#troubleshoot (Public\n   version https://opensource.google.com/docs/cla/#troubleshoot).\nThe email used to register you as an authorized contributor must be\n   the email used for the Git commit. Check your existing CLA data\n   https://cla.developers.google.com/clas and verify that your email is\n   set on your git commits\n   https://help.github.com/articles/setting-your-email-in-git/.\nThe email used to register you as an authorized contributor must\n   also be attached to your GitHub account\n   https://github.com/settings/emails.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/pull/4705#issuecomment-382007201,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AeApz99UByzJVZK3YTSiyIePNaQXSPlAks5tpfh2gaJpZM4TYY8V\n.\n. Hello, \nI would say we can start the code review @aardappel . . Hello @aardappel , \nYes, I did see your review. This does not exclude that some things might not be done well. As you have more expertise, your feedback is very much appreciated. Thanks. \n. Hello @aardappel  and @rw , \n@royalharsh  was right. I wasn't using SerializeToString and FromString.  It was my mistake that I did not remove that code. It is done now. . I have updated my tests. Is this what you had in mind @rw?. \n",
    "wfrisby": "Is the pickle module import blocking this PR from being merged?. ",
    "dennybritz": "Hello,\nAre there any updates on this? Is the test improvements still the only thing that prevents this from being merged or have there been other changes/plans in the meantime? @rw . ",
    "iceb0y": "I digged in a little - what makes this interesting is that the parser creates two fields union_type and union in idl_parser.cpp:642, where the parser knows nothing about language specific keyword escaping. The usage was at idl_gen_cpp.cpp:1641.\nIn this case I think we might want to escape the generated _type field. However currently Name() only supports Definition or EnumVal. Currently one way to escape the _type fields is:\nc++\nEscapeKeyword(field.name + UnionTypeFieldSuffix())\nWhat do you think?. I signed it!. bazel supported windows at least since 2015: https://github.com/bazelbuild/bazel/commits/master/site/docs/windows.md\nFor FLATBUFFERS_COPTS, we need to understand why -Wno-implicit-fallthrough and -linclude is used. If they are not needed, we should remove them from linux build; if they are needed, we should find whether it's default in MSVC, or to add corresponding flags. The branching can be implemented by using the (select)[https://docs.bazel.build/versions/master/be/functions.html#select] function in bazel. I tried removing the copts and it works fine in both linux and windows. Together with #4861, bazel is able to do the thing that first builds flatc and then use flatc to generate header files blazingly fast and cross-platform.. For native_inline for tables, we can use std::optional (port as flatbuffers::optional) if we want to keep the optional semantics.. There are two other approaches:\n\nAbseil convergence: Does flatbuffers minimum requirement met abseil's requirement? if so we can depend on abseil.\nUnpack null as empty in object API and pack empty to null. We have actually done this for string, so they are string and not optional<string>.\n\nI met this problem because currently we are generating unique_ptr for nested tables, which makes the object non-copyable.\nAlso when we drop VC2010 support (see #4646), we can support this:\n```flatbuffers\ntable Foo {\n    a: string;\n}\ntable Bar {\n    b: int;\n    c: Foo;\n}\n```\nIn C++:\nc++\nBarT bar{1, {\"a\"}};\nFor longer term we may also want to add view-based object API, so that we can perform zero-copy deserialization and one-copy serialization in object API.. Do we also want CreateVectorOfSharedStrings (or CreateSharedVectorOfStrings)?. I feel like this functionality should belong somewhere else, either a script, a build system or a fork. In unix-like systems you can also use cat files_to_process.txt | xargs flatc -cpp.. At the same time we should also allow empty enums.. I think we can split the current EnumVal into two parts:\n```c++\nstruct EnumValBase {\n  std::string name;\n  std::vector doc_comment;\n  Type union_type;\n};\ntemplate \nstruct EnumVal {\n  EnumValBase base;\n  T value;\n};\n```\nNot sure on the reflection side. I feel we can use union-like type there.. In this way, every code that needs to access EnumVal<T>::value needs to know T at compile time (by template param). We still need a runtime dispatch at some level, but I think that might already partially exist, like the current CTYPE.. Looking at code I agree that it's better to dispatch at value level than higher level. We can continue to use int64_t to store the value, while casting from T to int64_t, we can branch on value > max(int64_t) to use defined behavior. For longer term we can introduce a type utility that makes this easier.. I can reproduce this with bazel 0.22.0 .. Looks like we can create a derived class (without vtable) from Verifier which additionally computes the upper_bound_.. Is it possible to make this compatible when flatbuffers is put in //third_party/flatbuffers instead of registered as a bazel repository? Maybe //external:flatc, but bind is being deprecated.. Thanks for the suggestion. I tried to avoid using sub-repo whenever a repo supports direct vendoring (used as //third_party/... instead of @...). Looks like the trend is not going that way :face_with_head_bandage:. Also it looks like sub-repo and //third_party are different but I don't find a canonical path for these sub-repos. We currently use //vendor.. The code_ is not a string but a CodeWriter. Each += will implicitly append a line break at the end. In order to lift out CreateString, we need to either use a {{tag}}, or create a temporary string. I will switch to using a tag soon.. Made verbose by adding \"object based API\" into description to allow text searching.. Renamed to shared and made comment verbose.. ",
    "Shielsy": "I've tested this with an even simpler fbs file (below) and see the same error message 'bus error\" when I try and read the float value on the client side:\n```\ntable TestFloat {\n  test_float:float;\n}\nrpc_service test_service {\n  GetTestFloat(float_request):TestFloat;\n}\n```. In my test app I'm passing a hard-coded float (1.2345f). Below is a snippet:\nauto msg_offset = CreateTestFloat(mb_, 1.2345f);\n  mb_.Finish(msg_offset);\n    *response_msg = mb_.ReleaseMessage<TestFloat>();\nCreateTestFloat is generated by the flatc compiler\ninline flatbuffers::Offset<TestFloat> CreateTestFloat(\n    flatbuffers::FlatBufferBuilder &_fbb,\n    float test_float = 0.0f) {\n    TestFloatBuilder builder_(_fbb);\n    builder_.add_test_float(test_float);\n    return builder_.Finish();\n}\nThis error only seems to occurs when streaming/reading a float from the server. I've since written another app using client side streaming and I don't see the crash which is not what I expected.\n   . I've managed to get QTCreator debugger running today on the Raspberry PI. When the crash (bus error) occurs the debugger stops in the function ReadScalar (base.h). I cast'ed the *p to a float and the value looks correct (1.2345f). I've attached a screenshot. The Raspberry PI is little endian so EndianScalar just returns the passed in 't' parameter. \n\n. Yep, its looks that way. I thought I would add more details on how I'm calling the function client side (where the crash occurs). The code I'm using to retrieve the float is as below:\nflatbuffers::grpc::Message<TestFloat> response_msg;\n        auto status = stub_->GetTestFloat(&context, request_msg, &response_msg);\n        if (status.ok()) {\n            const TestFloat *response = response_msg.GetRoot();\n            const float testFloat = response->test_float();   <-- crash\n            std::cout << \"Success. =  test_float=\" << response->test_float() <<  '\\n';\n        } else {\n            std::cerr << \"Failed. Code=\" << status.error_code() << \" | \" << status.error_message() << '\\n';\n            return;\n}\nTestFloat is generated by flatc compiler:\nstruct TestFloat FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {\n                enum {\n                        VT_TEST_FLOAT = 4\n                };\n                float test_float() const {\n                return GetField<float>(VT_TEST_FLOAT, 0.0f);\n                }\n                bool Verify(flatbuffers::Verifier &verifier) const {\n                    return VerifyTableStart(verifier) &&\n                   VerifyField<float>(verifier, VT_TEST_FLOAT) &&\n                   verifier.EndTable();\n                  }\n        };. Sorry for dragging this one up again but I've been working on another project. I'm guessing there is no workaround/solution  - is the a way to force this (internal flatbuffers) buffer to be allocated in a region of memory which is correctly aligned (apologies if this is a stupid question)? \n. ",
    "nwbruce": "I've seen the same problem. It occurs when a field's type exists in another namespace. The generated python code tries a relative import in the current directory, but the actual class is defined in a totally different directory hierarchy.. ",
    "husobee": "I signed it!. @aardappel: updated as requested, added a panic on the failed file identifier size validation.  Also updated call to prep to perform padding which aligns with the js and java implementations.. updated. It seems that the other implementations do this as well.  For example: \nhttps://github.com/google/flatbuffers/blob/master/java/com/google/flatbuffers/FlatBufferBuilder.java#L858\nhttps://github.com/google/flatbuffers/blob/master/js/flatbuffers.js#L670-L671\nAlso, flatcc generated verification fails without the alignment a multiple of 8. . thank you for the clarification.  updated accordingly.. ",
    "diamondq": "I've added PR #4726 \nThere is a Nonnull attribute, but it's generally not used. In 99% of the cases I've seen (although my opinion), the default is Nonnull. \nIn any case, I've just add a PR that removes the @Nullable for required fields.. Yes, and as far as I can see, there was no impact.. ",
    "PublicParadise": "This bug report has expired. No change after a month. Closing.. Says my slackbot:\n\n. @aardappel But seriously, a month is a pretty long time in this industry and knowingly leaving a site run into a 404 for that period of time leaves a bit of a negative impression. Snippet from my inner dialog: \"How serious are those guys with their Typescript support? I guess, now I know.\"\nIn general I close issues that are just sitting around after about a month. I think, that's fair. If somebody responds, or provides a brief update - even if it's just a \"hang in there, we are working on it\" - then I'll extend the waiting period. I've done this with other issues and PRs in the past. I think Glide uses a bot that closes \"stale\" issues automatically. I'd rather see issues fixed. But at one point I have to move them from my balance sheets. \nGood luck with your project!. \nI am still getting a 404:\n\n. Works!\nThanks.. ",
    "TTSMike": "I'm not having much luck with this. Given this schema.\n```c++\nnamespace game;\nstruct vec2 {\n    x: float;\n    y: float;\n}\nstruct vec3 {\n    x: float;\n    y: float;\n    z: float;\n}\ntable Entity {\n    pos: vec3;\n    velocity: vec3;\n    name: string;\n}\ntable Flexy {\n    pos: vec2;\n    slug: string;\n    meta:[ubyte] (flexbuffer);\n}\nroot_type Entity;\n//root_type Flexy;\n```\nAnd this dummy data file:\njson\n{\n    \"pos\": {\n        \"x\": 5,\n        \"y\": 4\n    },\n    \"slug\": \"robot\",\n    \"meta\": {\n        \"age\": 10,\n        \"name\": \"Billy\",\n        \"items\": [ 1, 3, 17 ],\n        \"beverages\": [\n            {\n                \"name\": \"coffee\"\n            },\n            {\n                \"name\": \"milk\"\n            }\n        ]\n    }\n}\nInvoking flatc like so:\nbash\nflatc --root-type Flexy -o gen/ --binary data.fbs flex.json\nGives me this error:\nbash\nflatc: error: unknown root type: Flexy\nIf I omit the --root-type argument and manually comment/uncomment the root type in the schema, it works fine.. Ah, actually it looks like fundamentally there's more work needed here (worth its own issue).\nOff hand:\n it works for code generation, but not parsing json files\n though I say it works, the GetType/VerifyTypeBuffer/etc functions are omitted for the tables other than the root specified in the file. So if you set a root-type using the argument other than the one in the schema file, you don't get any GetType'ish functions generated. ",
    "yumeyao": "The reason why I suggest this built into flatc is, it's native for flatc to find the dependencies.\nOf course for simple include \"SomeFileInTheSameDir\" cases we can easily write a helper script for this purpose. but a generic way which support include \"SomeFileWeNeedToSearchInIncludeDirs\" would always be using the native flatc.. ",
    "VincentJ1989": "Oh,I get it, just use the 4 classes in java folder~. ",
    "ramzes2": "ctype.h included from cctype header file. And cctype included from string (by some another includes, not directly). Another problem, ctype.h has a lot of other problematic defines:\n```\ndefine _U  01\ndefine _L  02\ndefine _N  04\ndefine _S  010\ndefine _P  020\ndefine _C  040\ndefine _X  0100\ndefine _B  0200\n```\nSo only possible workaournd is to add undefs in flatbuffers/base.h after including string.\nCurrently i'm using lowercase x, y, z letters and all works fine :). ",
    "victorstewart": "when I clone the git repository, everything is good! Just the last release doesn't include the const tweaks.. close this whenever you like, just thought it should be up here in case it saves anyone time in the future lol.. Thanks for the clarification. I only work in C-based languages so was unaware of that with utf-8, but could sense something was afoot.\nC++ only, so strings it is!. whoops. accomplished by reflections, my bad!. Actually. Reflection is a bit heavy weight, and requires a multi-KB binary file to be loaded. If a string were updated, always within a fixed length... direct memory access updating would be much more efficient no?. It's really only in the case of resizing that reflection is necessary.. just saw the \"// TODO: support strings.\" in void SetAnyValueS(reflection::BaseType type, uint8_t *data, const char *val)\nI guess my point is that objective?. Bypassing reflection on fixed sized string mutations would definitely be a huge efficiency jump for me, covering about 90% of my mutation cases. \nI'll give it a look and see if any PRs are warranted to accomplish / formalize.. @jpathak unfinished?? that sounds like heresy lol.\nI should add it's technically possible to change scalar values by just using the generated header file (aka without a binary schema and reflection) with a few simple tweaks (as long as the value was written to the buffer, which( for example) sometimes requires changing default values for bool fields to true, and explicitly setting the value to false).. @jpathak so you want to change the value of the \"field\" field in \"Row\" tables that have already been created. Ya you should be able to do this. You have to go into your generated header file and add a field like this...\n```\nvoid changeFieldValue(int newValue) {\nSetField(VT_FIELD, newValue, 0);\n\n}\n```\nwhere 0 is default value. @rw believe it or not the module is almost trivial to write... maybe 150 lines of C/C++. You just declare your new command functions and then implement them. So, gathering the flatbuffer at a key, editing it, overwriting the old. Then you compile it as a shared library, and feed the module in via command line or config file. Super easy.\nCould also use flexbuffers to build in native support for nested tables in Redis (I wrote this last night actually).\nOnly problem is, as flatbuffers/flexbuffers currently stand, every update to the buffer requires the reallocation of the entire buffer size, plus new buffer space as elements are added/changed. Which makes the most process incredibly wasteful and potentially really slow.\nI think a great solution would be to create a mode that added lots of padding to all vector types, maybe 150% of the containing data. So a client pushes a flatbuffer to Redis, we receive it and add all this padding. Then when updates start rolling in, most likely they can be handled via direct memory access (for all data types), and worst case we just allocate the container + 50% over again.\nThen when the flatbuffer is requested, we remove all this padding, and return a tight one. And the same idea with flexbuffers and nested tables. \nTo me, it only makes sense to do it this way. Otherwise might as well just store the raw data, and create the flatbuffer on demand. \nIf there's interest I'd love to help making the necessary adjustments to create this \"+50% padding mode\". Maybe one of you guys with innate knowledge of the internals could point out where to get started... instead of brute force figuring it out.. @rw great idea actually! The std::vector byte array used during reflection, can simply be a file-global variable, allocate once + grow, and memcpy the flatbuffer to be edited onto its data() location. \nDefinitely makes this practical now. \nAlso this is a much better solution overall, not only is it much simpler but way less allocations executed over time than the way I originally suggested.\nI\u2019ll link here after I write it.. @rw @gwvo \nhttps://github.com/victorstewart/flatbuffersxredis\n(tentatively) works for strings and (all but bytes and shorts) scalars. As you can see, this is pretty easy to accomplish.\nUnsure how to handle the inputs to bytes and shorts? I don't use those either, but maybe someone can give guidance there.\nNeed to add support for nested structures.. \nI see a reflection function to resize a vector... but no way to remove elements from a vector, or push new elements into a vector. Am I correct?\nAnyone who wants to be added as a contributor and help me build it out, more than welcome.. @aardappel\nWhat about creating a table of offsets for data/field locations? So instead of the inline offset for field A being 30 (the exact address), it\u2019s simply an index, \u201c1\u201d. Which then maps to 30.\nMaybe make the offset table optional when constructing the builder? And if not turned on, they\u2019re stored inline as normal.\nWhen resizing the context it would make that process as seamless as possible, update the offset map (every offset address greater than say 30, add 10), and shift over the bytes.\nThat plus reusing a preallocated memory block to work on, I\u2019d have to imagine it\u2019s quick?\n\nI\u2019m storing users as flatbuffers in my cache, so about 95% of the cases just involves getting and sending the user\u2019s prepackaged buffer. In the cases where data does need to be updated, a minor cost is fine.. tried zstandard on max. got 381 bytes and 7897 bytes.\nmy use cases are restricted to sending flatbuffers back and forth between my servers and cellular devices, so I was just experimenting with whether it would be advantageous to compress them before transmission. \nbut since they are SO tiny (leaving aside the ones with image data)... even with a 50% compression ratio... in light of 500 bytes vs 10-50 Mb/s bandwidth even on mobile... and 100ms latencies on LTE... I feel the cpu + time cost far outweigh the nanosecond or two of transmission savings lol. . @rw wow on second look. zstandard is incredible-- the ability to \"train\" / generate dictionaries! the data in my flatbuffers definitely breaks up well along family lines.\nEven if I could get 500 bytes to 100 bytes though (say, in an extreme case)... I question whether they'd be any material benefit over LTE given the latencies. Maybe on wifi.\nreducing the number of packets the flatbuffer fits into is also a topic (I assume all MTUs between any client and an ethernet-ed server are always 1500 bytes?), though maybe irrelevant.. I just poked @ google and it seems an MTU of 1500 was negotiated..\n```\nping -D -g 1400 -G 1900 -h 10 www.google.com\nPING www.google.com (xx.xxx.xxx.105): (1400 ... 1900) data bytes\n72 bytes from xx.xxx.xxx.105: icmp_seq=0 ttl=44 time=18.016 ms\nwrong total length 92 instead of 1428\n72 bytes from xx.xxx.xxx.105: icmp_seq=1 ttl=44 time=18.989 ms\nwrong total length 92 instead of 1438\n72 bytes from xx.xxx.xxx.105: icmp_seq=2 ttl=44 time=17.852 ms\nwrong total length 92 instead of 1448\n72 bytes from xx.xxx.xxx.105: icmp_seq=3 ttl=44 time=18.819 ms\nwrong total length 92 instead of 1458\n72 bytes from xx.xxx.xxx.105: icmp_seq=4 ttl=44 time=18.803 ms\nwrong total length 92 instead of 1468\n72 bytes from xx.xxx.xxx.105: icmp_seq=5 ttl=44 time=18.728 ms\nwrong total length 92 instead of 1478\n72 bytes from xx.xxx.xxx.105: icmp_seq=6 ttl=44 time=18.982 ms\nwrong total length 92 instead of 1488\n556 bytes from www.routerlogin.com (192.168.0.1): frag needed and DF set (MTU 1488)\nVr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst\n 4  5  00 da05 0f1f   0 0000  40  01 5964 192.168.0.17  xx.xxx.xxx.105 \nping: sendto: Message too long\nRequest timeout for icmp_seq 7\nping: sendto: Message too long\nRequest timeout for icmp_seq 8\nping: sendto: Message too long\nRequest timeout for icmp_seq 9\netc...\n```. ",
    "glensc": "4698 fixes this..",
    "johngull": "You was very close :)\nPlease check PR with the fix.. Signed CLA.. ",
    "GStones": "thanks reply, now we plan to  use golang's recover to catch panics in each  fuction. ## one of our schema files:\n```\ninclude \"Common.fbs\";\nnamespace msg_fbs;\nenum HallMessage:int32 {\n    CGEnterHall= 20000,\n    GCEnterHall,\nCGMatchBattle,\nGCMatchBattle,\n\nGCMatchOver,\n\nCGCancelMatchBattle,\nGCCancelMatchBattle,\n\nGCHallAddPlayer,\nGCHallRemovePlayer,\n\nCGLeaveHall,\nGCLeaveHall,\n\nCGHallPlayerReady,\nGCHallPlayerReady,\n\nCGHallPlayerCancelReady,\nGCHallPlayerCancelReady,\n\nCGHallTransferTeamOwner,\nGCHallUpdateTeamOwner,\n\n}\nenum HallErrCode : byte { \n         Success = 0,\n         NotLogin, \n         NotExist,\n         NotFound, \n         AlreadyIn,\n         OverMax,\n         LogicErr,\n         DataErr,\n         NotReady\n }\ntable HallRole{\n    UID:string;\n    SeatID:int32;\n    Name:string;\n    Gender:RoleGender;\n    Avatar:int32;\n    StarLevel:int32;\n    Team:BattleTeam;//delete\n    RankStage:int32;          \n    RankLevel:int32;          \n}\nenum BattleLevel:byte{Esay = 0,Normal,Hard}\ntable CGEnterHall{\n    UID:string;//delete\n    MatchMode:MatchMode;   \n    TargetUID:string;     \n}\ntable GCEnterHall{\n    Code:HallErrCode;\n}\ntable CGCancelMatchBattle{}\ntable GCCancelMatchBattle{\n    Code:HallErrCode; \n}\ntable CGMatchBattle {} \ntable GCMatchBattle {\n    Code:HallErrCode; \n    Duration:int64;    //second\n}\ntable GCMatchOver{\n    RoomID:int64; \n    Roles:[HallRole];// delete\n}\n//------------Team-------------------\ntable GCHallAddPlayer{\n    Roles:[HallRole];\n}\ntable GCHallRemovePlayer{\n    UID:string;\n}\ntable GCHallUpdateTeamOwner{\n    UID:string;\n}\ntable CGHallTransferTeamOwner{\n    UID:string;\n}\ntable CGLeaveHall{\n    TeamID:string;//new\n}\ntable GCLeaveHall{\n    Code:HallErrCode;\n}\ntable CGHallPlayerReady{}   \ntable GCHallPlayerReady{\n    UID:string;\n}\ntable CGHallPlayerCancelReady{}\ntable GCHallPlayerCancelReady{\n    UID:string;\n}\n```. ### server must keep stable\nwhen client send error data,the server will panic with  runtime error: slice bounds out of range\nSorry, I didn\u2019t respond in time.\n. ",
    "joligarson": "I signed it!. ",
    "GauthamBanasandra": "Hi, I've signed the Google CLA. Kindly verify.. The pleasure is mine. \ud83d\udc4d . ",
    "jpcima": "Thank you for fixing this!. ",
    "monktastic": "For posterity: installing flatbuffers==1.9 fixed this particular error for me. It seems that the naming convention in PyPI changed from YYYYmmdd to semver, pushing 1.9 behind all the 2015xxxx.. ",
    "Mortoc": "@evolutional Also interested if you've got anything ready to test. ",
    "roussosalex": "The Vector type unfortunately does not handle large blobs of data well. CreateUninitializedVector may help when packing the buffer, but unpacking it later causes the Vector to copy each individual ubyte out to a std::vector. That is not necessary for the bytes use-case. \nExample in monster_test_generated.h:\nc++\nfor (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->inventory[_i] = _e->Get(_i); }\nThe most similar type to the bytes use case would be string, which is why we use String offsets in the bytes case as well. Even if we use String, we can't avoid unnecessary copies of our buffer. If we take a large, existing buffer as our first case, using String would require us to create at least one copy of the buffer before packing it in the flatbuffer. \nIn order to solve these problems, we would need to combine the advantages of String and Vector, which is what Bytes does.. Using the base API has solved the issue, thanks.. Thanks! I missed that part of the documentation.. ",
    "WoodyGuo": "I signed it!. The CI failure doesn't appear to be related to the commit in question.\nThe job log says:\nThe command \"if [ \"$TRAVIS_OS_NAME\" == \"linux\" ]; then sudo apt-get install -qq g++-$GCC_VERSION; fi\" failed and exited with 100 during .. Thanks  Wouter! Made the change accordingly.. Sorry for the late response. The test fails because:\n monster_test.fbs includes include_test1.fbs.\n Therefore, the code generated for monster_test.fbs also includes the one generated for include_test1.fbs. \n* But the code for include_test1.fbs is not there as it's not told to generate it .\nI tried to generate it by running ../flatc --dart -I include_test -o ../dart/test include_test/include_test1.fbs but it failed with error Segmentation fault: 11.\nI removed include \"include_test1.fbs\"; from monster_test.fbs and the test results are the same as before.\nWhat #4803 actually does is to add import xxx; as per the includes in a schema file.\nIf I revert #4803 , then the code generated for the schema files below is failing to compile because import 'foo_com.company.fbs_generated.dart';  is missing from bar_com.company.fbs_generated.dart:\nfoo.fbs\n```\nnamespace com.company.fbs;\ntable Foo {\n   // ...\n}\n```\nbar.fbs\n```\ninclude \"foo.fbs\";\nnamespace com.company.fbs;\ntable Bar {\n   // ...\n}\n```. ",
    "crackcomm": "\nIf you wish to copy a table out of a larger buffer, the easiest is to create a nested_flatbuffer (this works in all languages). Alternatively, there is CopyTable in C++ reflection.\n\nHow can I do that in Go?. Thanks, that answers my questions.. ",
    "BarryBVL": "Strange , I made a clone of the master this weekend, build flatc myself and then It seemed to work . (at least it generated a .cs file without any error message)\nworksforme.zip\n. ",
    "OMPMDG": "This indeed seems to work in the master branch, while in release 1.9.0 and older, this is not the case.\nIf I would use this latest version, is it still compatible with applications using older releases of FlatBuffers?\nWhat I mean is, whether FlatBuffers are binary compatible between releases (in my case between 1.7.1 and the master branch)?. ",
    "b1nhm1nh": "Flatbuffers Lua can support Lua 5.1, 5.2 and Luajit with 2 additional modules: bit32 and lua-compat-5.3. Take a look at my commit\nhttps://github.com/b1nhm1nh/flatbuffers/commit/1ba3b896b99ca04983f820f50ff44fda3c13980b . ",
    "6643": "Can it be designed as follows? Will be very convenient to use;\n```\n$ cat person.fbs \nnamespace game;\ntable Person {\n    name:string;\n    age:int;\n}\n$ flatc -d person.fbs\n$ cat test.dart \nimport 'dart:typed_data' show Uint8List;\nimport 'person_game_generated.dart' as game;\nmain(List args) {\n  var p = new Person();\np.name = 'sam';\n  p.age = 20;\nprint(new Person.fromBytes(p.toBytes()));\n}\nclass Person {\n  String name;\n  int age;\nPerson();\n  Person.fromBytes(List bytes) {\n    final p = new game.Person(bytes);\n    name = p.name;\n    age = p.age;\n  }\nUint8List toBytes() {\n    return new game.PersonObjectBuilder(name: name, age: age).toBytes();\n  }\nString toString() {\n    return 'Person{name: $name, age: $age}';\n  }\n}\n$ dart test.dart \nPerson{name: sam, age: 20}\n``\n. [1.9.1] Import failed\n```\n$ cat pubspec.yaml \nname: zhubao.bin\ndependencies:\n  flat_buffers: ^1.9.1\n$ pub upgrade\nResolving dependencies... (3.3s)\nBecause zhubao.bin depends on flat_buffers ^1.9.1 which doesn't match any versions, version solving failed.\n```. @dnfield @aardappel thanks. ",
    "damienpontifex": "I am basing this change off my experience doing similar changes to my projects in the past. Saying that, I've still been able to reference projects migrated to this csproj format and targets with mono and full framework applications. \nI can't comment on older VS setups though. Maybe limited to VS15 and later (guessing as this is the oldest I've used with this format).\nSaying all this, it does enable more for current targets basing off the implementation support table\nIf he has time, maybe @davidfowl could provide any insights or best practices?. @aardappel I thought the tutorial said to include them manually anyway...i.e. there isn't a published nuget package by google for this so instructions were build flatc and copy these files.\n@davidfowl no nuget package published I don't believe but would be nice if there was. Main question/guidance is probably best practices for TargetFrameworks property to give both best compatibility and latest features?\nOnly reference may be past csproj file https://github.com/google/flatbuffers/blob/master/net/FlatBuffers/FlatBuffers.csproj. ",
    "davidfowl": "What does the existing flatbuffers package target? I couldn't find the nuget package. This library needs some nuget love. We\u2019d be happy to help \ud83d\ude04. That's pretty odd. Do you know how they are consuming this project? Is there a nuget package?. No problem. Can we open up an issue to follow up? I\u2019ll try to take a look when i get back from vacation.. ",
    "HoLLy-HaCKeR": "Visual Studio 2017 and onward use a new XML schema for project files (.csproj). Loading this in older versions of Visual Studio (or some other IDEs) will throw that error.\nAs far as I know, the new format is the default for projects that are created targeting .NET Core or .NET Standard. I'm not sure if the old format supports multi-targeting, but with the new one you can target eg. .NET Framework 4.0, .NET Standard 1.0 and .NET Standard 2.0 at the same time.. ",
    "tkmru": "Thanks, but I think it is better to treat strict json by default.. ",
    "ElliotHYLee": "First, I'm no one.  Just in case if there's any person who needs it ASAP. No proper test was done. Also, I didn't need hasFileIdentifier since another side is C#.\nIt would be nice if there's an actual update coming soon.\ndef FinishWithFileIdentifier(self, rootTable, fid):\n    if (fid is None or len(fid) !=4 ):\n        raise Exception('fid must be 4 chars')\n\n    flags = N.Uint8Flags\n    prepSize = 4\n    self.Prep(self.minalign, prepSize+len(fid))\n    for i in range(3, -1, -1):\n        print(i)\n        self.head = self.head - flags.bytewidth\n        encode.Write(flags.packer_type, self.Bytes, self.Head(), fid[i])\n\n    return self.Finish(rootTable)\n\n\nI found that I need  hasIdentifier on the Python side. Finally ended up creating the methods.. I signed it!. @rw I added the tests.. @rw  You can choose either #4853 or #4892\nI think the basic difference of #4853 #4892 is that the FileIdentifier function arguments. The function in #4853 expects the byte array and the function #4892 expects the string and converts the string to byte array .\nSince #4853 already iterated your comment, what I can do optionally is to add this line if you like \"auto\" python2-3. \nif sys.version_info < (3, 0):\n            fileID = []\n            for i in range(0, len(string)):\n                fileID.append(ord(string[i]))\n        else:\n            fileID = \"CLPR\".encode()\n. Hi @rw , I can give it a shot. I once made a PR #4853 and it doesn't look went through. If you mean this, how to pass the tests, yes please let me know the PR process.\nAlso, if the feature is made on your side, how long would it take? I'm considering that myself making contribution might take some time if it would require the detailed knowledge of FB protocol. But, I can give it a shot - do you have any recommendation which file(s) are generating the script files based on the flags?. @rw  Oh there was an unsuccessful check. Could I know what makes the failed pass?. @rw please never mind the previous reply.. ",
    "evanmoran": "Thanks @aardappel! . ",
    "NN---": "That's nice solution but I would prefer if it was built-in\nLet's say standard type required_int32.\nOr special syntax (scalar_required) which will do essentially the same.. I want to ensure that when FlatBuffers is created all fields are set.\nIn Protobuf if I forgot to set any field, I get an exception. I want the similar behavior in FB.. ",
    "jpathak": "Is it possible to do direct memory updates on an unfinished buffer ?. Thanks for your quick reply! That's exactly my usecase: I have a table in which I want to change scalar values previously written but I haven't finished building the rest of the table yet. (I stored the offsets but there doesn't seem to be a way to use it to access the already written scalars, I likely just haven't looked enough). \nEg: \ntable Row {\n  field:int;\n}\ntable MyTable {\n  rows:[Row];\n}\nAs I add Row's to MyTable, sometimes I want to change the values written to the field for Rows already written. I'll check the header for what I can use. . ",
    "sutambe": "FYI. FlatBufferBuilder now has move constructor and move assignment.. @gabyx @aardappel  I've an observation regarding release and release_raw functions in vector_downward added in PR #4885. They are a little too aggressive about setting the allocator to nullptr. They break the is-a relationship between flatbuffers::grpc::MessageBuilder and FlatBufferBuilder. I wrote GrpcLikeMessageBuilder to show it. Can you please take a look? PR #4902. \n. @aaronovz1, try \nDetachedBuffer* hint_ptr = new DetachedBuffer(std::move(detached_buf));\npublisher->sendNoCopy(detached_buf.data(), detached_buf.size(), &freeFrame, hint_ptr);. The fbb object is not going to be relevant after calling Release. DetachedBuffer takes over the memory managed by fbb earlier. So I'm not sure how it's working for you. It should not, IMO.. ok, so you don't have detached_buf anymore. In that case, you are using FlatBufferBuilder as a buffer manager. That could work.. No difference. In both cases, it's gonna be DefaultAllocator().deallocate, which is basically delete[].. I signed the Google Individual Contributor License Agreement.. @aardappel Thx for your quick response. I use hunter to download and install grpc++ dependencies. They are typically installed in ~/.hunter. Is there a way to point the CMakeLists.txt  in flatbuffers to pickup grpc includes and libs setup by hunter?. I've grpc built and installed on Linux. Do you have the diffs for adding grpc dependency in flatbuffer cmake files? . Hi @aardappel, I think I've something now that's ready for review. I've\n1. Added move semantics to MessageBuilder, FlatBufferBuilder, SliceAllocator, and vector_downward\n2. Added unit tests\n3. Updated flatbuffers + gRPC build instructions\n4. Updated CMakeLists.txt with simplified build instructions.\nThe general pattern is default/move construct and swap idiom. I think it works well for these classes because some of them have up to 6 members and swap helps reduce verbosity. More importantly,  no conditional delete calls for string_pool and allocator are needed in FlatBufferBuilder and vector_downward, respectively. The \"right\" things happen as a side-effect of temp going out of scope.\nThe unit tests check the same invariants for MessageBuilder, FlatBufferBuilder, and a test-only message builder that uses heap allocation (HeapMessageBuilder). \n@aardappel Who else do you think should review this patch?. @aardappel I've simplified the build instructions to minimum necessary. I tested with Ubuntu 18 and RHEL7. Both distributions need all the steps mentioned earlier. . @llchan Do you have some cycles to look at this patch? If not, @aardappel do you think it's good to go in? I'll squash the PR to a single commit.. Checkout my personal travis build testing the changes in the CMakeLists.txt. Otherwise, this build is completely unrelated to this PR.. @aardappel Thanks much! \nfixes #4897 . Such generic interface allows overload set resolution on the member operator() method. For instance, I now have a test for the following code.\n```cpp\nint overload_set_test() {\n  ::grpc::ServerContext context = nullptr;\n  flatbuffers::grpc::Message request1 = nullptr;\n  ::grpc::ServerAsyncResponseWriter< flatbuffers::grpc::Message> response = nullptr;\n  ::grpc::CompletionQueue new_call_cq = nullptr;\n  ::grpc::ServerCompletionQueue notification_cq = nullptr;\n  void tag = nullptr;\nflatbuffers::grpc::Message request2 = nullptr;\n  ::grpc::ServerAsyncWriter< flatbuffers::grpc::Message> writer = nullptr;\nOverloadSet set;\n  // call WithAsyncMethod_Store::RequestStore;\n  set(context, request1, response, new_call_cq, notification_cq, tag); \n  // call WithAsyncMethod_Store::RequestRetrieve;\n  set(context, request2, writer, new_call_cq, notification_cq, tag);\n  return 0;\n}\n``. After some more experimentation, I think the method name could be simplyWithAsyncMethod_Foo::Request(without any suffix).flatccompiler could have an option like--no_suffix_async_methodand it would simply drop the suffix while generating code.. The PR adds--no_suffix_async_cpp` option that either keeps the method name as suffix or uses a generic name. But never both. \nI'm starting to think that keeping both in the generated code may be valuable. The reason is that referencing a specific .Request when generic code is generated is excruciatingly painful. For instance, see MyGame::Example::MonsterStorage::AsyncService.\ncpp\n    async.MonsterStorage::WithAsyncMethod_Store<MonsterStorage::WithAsyncMethod_Retrieve<MonsterStorage::Service>>::Request(0, 0, 0, 0, 0, 0);\n    async.MonsterStorage::WithAsyncMethod_Retrieve<MonsterStorage::Service>::Request(0, 0, 0, 0, 0, 0);\nThis is simply ridiculous. See test. \nSo, I propose adding a different flatc option --gen_generic_async_cpp that generates both methods. \n. @gabyx @aardappel  I've an observation/suggestion regarding release and release_raw functions in vector_downward. I think both release functions are a little too aggressive regarding setting the allocator to nullptr. \nThe reason I'm suggesting this is that grpc::MessageBuilder has a funky member allocator and aggressively setting the allocator to nullptr in release and release_raw functions makes the this object unusable in case of MessageBuilder. \nI think release functions should only relinquish the ownership of the buf_ pointer and leave the ownership of allocator to the destructor. \nMore specifically, I propose\n1. release should transfer the ownership of buf_ and allocator_  to DetachedBuffer and not reset the allocator_ to nullptr. It should only disown the allocator.\n2. release_raw should only return a pointer and not eagerly delete the allocator_. Nor should it set it to nullptr. In this case, destructor of FlatBufferBuilder will take care of deleting the allocator if/when needed. \n3. If clearing allocator is actually desired, there's .clear_allocator for that.\nIn essence, the current code breaks MessageBuilder is-a FlatBufferBuilder relationship, which is very surprising. I've a test that proves that. See builder_move_assign_after_release_test and builder_move_assign_after_releaseraw_test in this PR. I created GrpcLikeMessageBuilder to test. Without the changes in vector_downward the tests fail. With this PR, the is-a relationship is restored.\nThoughts?\n. I think a small correction is needed in my proposal. That's because DetachedBuffer::Release and vector_downward::release have been around since May' 17. I don't think it makes sense to change that behavior at this point. So, point 1 I mentioned earlier needs tweaks. Here're my revised thoughts.\n1. If FlatBufferBuilder owns the allocator, release should transfer the ownership of buf_ and allocator_ to DetachedBuffer and set the allocator_ to nullptr. This is the behavior since May'17. \n2. If FlatBufferBuilder owns the allocator, release_raw should not transfer the ownership of buf_ and allocator_ (it's not possible anyway) and not deallocate the allocator_ nor should it be set to nullptr. This is a suggested change since @gabyx patch last week.\n3. If FlatBufferBuilder does not own the allocator, both Release and ReleaseRaw should not reset the allocator_ to nullptr. I think this is a safe change as something external is responsible for deleting the allocator anyway.\nThe latest commit in this PR incorporates the above change. It has better, larger set of tests and they work fine. However, grpc::MessageBuilder has some issues with this test suite (BuilderTests). I have a hunch of why the tests are not good enough for grpc::MessageBuilder.. After much more testing, I think I've something that looks reasonable to me.\nIn the current state, if grpc::MessageBuilder calls ReleaseRaw, grpc::MessageBuilder basically silently degenerates to  default-constructed FlatBufferBuilder as the allocator_ is reset to nullptr. If the MessageBuilder object is used after ReleaseRaw, it allocates using the internal DefaultAllocator. It appears to work. But in reality, you can no longer call MessageBuilder::ReleaseMessage() as grpc_slice_ is completely bypassed. This behavior breaks the expectation of the user of MessageBuilder. \nSpecifically, the following test code should fail but does not. \ncpp\nflatbuffers::grpc::MessageBuilder b1;\nfor (int i = 0; i < 5; ++i) {\n      auto root_offset1 = populate1(b1); // fill in m1_name and m1_color\n      b1.Finish(root_offset1);\n      size_t size, offset;\n      const uint8_t *buf = b1.ReleaseRaw(size, offset);\n      TEST_ASSERT(verify(buf, offset, m1_name, m1_color));\n      // b1 now indistinguishable from FlatBufferBuilder.\n}\nWith this PR, the new behavior is that calling grpc::MessageBuilder::ReleaseRaw has no hidden surprises and the MessageBuilder object is no longer usable (destructor and move-assign are allowed). No silent degeneration takes place. If someone tries to populate the MessageBuilder after ReleaseRaw is called, the program crashes with assertion failures (thanks to asserts in SliceAllocator). \nThis PR includes a negative test same as above. \nThe MessageBuilder can be brought back to life if a default-constructed MessageBuilder is moved into it. . @aardappel @rw I feel I'm pretty much done with this PR. I want to avoid making test.cpp longer that it is already. I like the separation of test code between test_builder files and previous test.cpp. There's nice cohesion between tests/test_builder.* and grpc/tests/message_builder_test.cpp . Although I mentioned earlier that I'm done modifying the PR, I made some important additions last night. These last set of changes fix what I thought was the sticking point in this PR--failing MessageBuilder test in REUSABLE_AFTER_RELEASE_RAW case. See message_builder_test.cpp. It no longer fails. To fix it, here's what I did.\nI added MessageBuilder::ReleaseRaw function. The signature is \nuint8_t *ReleaseRaw(size_t &size, size_t &offset, grpc_slice &slice);\nAs you can see, this function is similar in spirit to the one @gabyx added in FlatBufferBuilder. The similarity is that in both classes, calling this function relinquishes the ownership of the buffer pointer  to the caller. That means in MessageBuilder, grpc_slice must be returned so that the caller can free the raw pointer later by calling grpc_slice_unref(). After this function, MessageBuilder is as if it's default constructed. \nWithout this new function, calling base class's ReleaseRaw has really surprising side-effects. The released pointer is deleted after MessageBuilder goes out of scope. This weird behavior caused one test to fail, which is now fixed.. @aardappel @rw What are the next steps for this PR now? \n@gabyx Do you have some cycles to take a look?. @gabyx Thx much! Your comment made my day.. @aardappel @rw What are the next steps for this PR now? . @aardappel I did some changes in response to your comment. I left a reply comment last week. Reposting it here.\nI understand your comment about assert(0). I prefer counting test failures rather than bailing out on the first error. This seems to be a norm in the gradle world (Java). But I could be persuaded otherwise.\nI updated the TEST macros to include full function name as there are template instantiations. I tested without assert(0) and caused some tests to fail deliberately. I see output like this\n```\n/Users/sutambe/projects/myoss/myflatbuffers/cmake-build-debug/flattests\nVALUE: \"0\"\nEXPECTED: \"1\"\nTEST FAILED: /Users/sutambe/projects/myoss/myflatbuffers/tests/test_builder.h:166, release_n_verify(b1, m2_name, m1_color) in static void BuilderTests::builder_move_assign_after_releaseraw_test() [Builder = flatbuffers::FlatBufferBuilder]\nVALUE: \"0\"\nEXPECTED: \"1\"\nTEST FAILED: /Users/sutambe/projects/myoss/myflatbuffers/tests/test_builder.h:166, release_n_verify(b1, m2_name, m1_color) in static void BuilderTests::builder_move_assign_after_releaseraw_test() [Builder = TestHeapBuilder]\nVALUE: \"0\"\nEXPECTED: \"1\"\nTEST FAILED: /Users/sutambe/projects/myoss/myflatbuffers/tests/test_builder.h:166, release_n_verify(b1, m2_name, m1_color) in static void BuilderTests::builder_move_assign_after_releaseraw_test() [Builder = GrpcLikeMessageBuilder]\n3 FAILED TESTS\nProcess finished with exit code 1\n```\nOutput on Windows is not gonna be so nice. It is still better as it tells me that 3 tests failed in all as opposed to the first one only. I'll rebase if you are ok with it or revert and drop the counter.\nWhat say?. @aardappel Thx much! Closes #4897 (with improvements). @aardappel @vglavnyy Created PR #4951. . Closing as #4952 appears to be more general.. @vglavnyy You have some windows specific changes that my PR #4951 does not. So I'll pull back. It would be great if you keep int testing_fails so that multiple test failure reporting in Release builds is retained.. @aardappel The reason I would like to convert FlatBufferBuilder to a MessageBuilder efficiently is to two fold: \n\nAvoid dependency on grpc in non-networking related components in my system\nAvoid a copy of a flatbuffer to a MessageBuilder on the way out\n\nI've a system where there's a strong separation of concerns between the network layer and everything else. The network layer depends on flatbuffers::grpc namespace because it deals with MessageBuilder. However, the rest of the modules do not depend on grpc but only flatbuffers. So, they also don't want dependency on MessageBuilder. This separation of concerns enables pluggable transports in the future, which do not need or use MessageBuiler. \nIf efficient conversion of FlatBufferBuilder to MessageBuilder is not available, the networking layer will have to copy every flatbuffer produced internally into a MessageBuilder because that's what grpc understands. This copy could be prohibitively expensive in a high-throughput server.\nThis PR appears big only because the additions are exhaustively tested. The PR does not change the existing functionality. It only adds move-conversion functions, which are reusing previously available Swap and grpc_slice_new. As a matter of fact, I was a bit surprised how little new things are needed  (outside tests) to support this functionality. . @stewartmiles The Android gcc build is running into some gradle issues. I see a \njava.io.IOException: Could not parse XML from android/accounts/annotations.xml \nCan you please help me out here?. Actually, I realized now that the build does not like std::move for some reason. Weird. I will have to macro out new code containing std::move. . @stewartmiles Thx for the link. Yeah, it was an issue with std::move, std::is_same not being available.. @aardappel More thoughts on this PR?. I can't find the comment I posted before my last update so reposting here. \n@aardappel I zapped the unused static alloc function from DefaultAllocator. Do you think the PR can be merged soonish?. Awesome! Thanks much @aardappel . Good catch. It's a memory leak. I'm wondering how my valgrind tests did no catch it previously. This time around it found it right-away. I probably had that part of the code disabled. Opened PR #5021. . Updated to use std::shared_ptr as flatbuffers::DefaultAllocator has no operator(). \n@gabyx worked on the api of ReleaseRaw. Perhaps there's already code out there that expects raw pointers from ReleaseRaw. I thought that's kind of the point of the function. . The objective of this PR is to fix the leak in the test. I'm fine with manual deallocation or a convenient scoped deleter. IMO, this PR is not the right place to reconsider ReleaseRaw signature. . @aardappel Thx for looking into it. It's still missing in MVNRepository though.. These fixes are not from upstream. Both grpc or grpc-java don't look like an upstream to me.\nI looked at both third_party/.../java_generator from grpc and  java_plugin/.../java_generator from grpc-java. They look quite different from what's in the flatbuffers repository. They seem to have evolved a lot in different direction. grpc-java seems closer. For instance, there's method->client_streaming() as opposed to method->ClientStreaming(). I got no hits for FlatbuffersUtils in grpc-java, which seem to be everywhere in the code generator in the flatbuffers repo (obviously).\nIs maintaining ties with either grpc or grpc-java code generators still a goal? . I see a lot of value in staying up-to-date with \"upstream\". For instance, I hit JDK 8 vs. 11  javax.annotation.Generated issue today with the flatbuffers Java code generator. Basically, the generated code does not build on JDK 11. It appears to have been solved upstream. Cherry-picking fixes from upstream may not be very effective as it is probably labor intensive. I'm unclear on the long-term path forward. \nFor now, @aardappel do you have more thought on this PR?. You may be right. I did not make install gRPC in the system directory. C++ gRPC docs warn against that saying uninstalling will not be easy. So I did make prefix= $(pwd)/install install. I'm using includes and libs off of that. Hence the paths.\nJust compiling and installing grpc was not enough. While compile grpc_test.cpp I got a compiler error that common.h is missing\nScanning dependencies of target grpctest\n[ 71%] Building CXX object CMakeFiles/grpctest.dir/tests/monster_test.grpc.fb.cc.o\nIn file included from /home/sutambe/projects/my-oss/grpc/install/include/grpcpp/impl/codegen/proto_utils.h:28:0,\n                 from /home/sutambe/projects/my-oss/grpc/install/include/grpc++/impl/codegen/proto_utils.h:26,\n                 from /home/sutambe/projects/my-oss/myflatbuffers/tests/monster_test.grpc.fb.h:13,\n                 from /home/sutambe/projects/my-oss/myflatbuffers/tests/monster_test.grpc.fb.cc:6:\n/home/sutambe/projects/my-oss/grpc/install/include/grpcpp/impl/codegen/config_protobuf.h:25:42: fatal error: google/protobuf/stubs/common.h: No such file or directory\n #include <google/protobuf/stubs/common.h>\n                                          ^\ncompilation terminated.\nmake[2]: *** [CMakeFiles/grpctest.dir/tests/monster_test.grpc.fb.cc.o] Error 1\nmake[1]: *** [CMakeFiles/grpctest.dir/all] Error 2\nmake: *** [all] Error 2\nIt's kind of weird that protobuf headers are needed while compiling flatbuffer tests. The fix could be in a separate PR though.\nThe INCLUDE_DIRECTORIES and LINK_DIRECTORIES serve dual purpose: CMake code and comments. I'm fine with proper cmake variables. I was wondering about them myself.. No, it was not automatic. I had to do two separate build and install. I'll give it a shot on a different Linux distro.. There are three cases\n1. FlatBufferBuilder with a null allocator--FlatBufferBuilder defaults to DefaultAllocator internally.\n2. MessageBuilder with a funky allocator that's a member--FlatBufferBuilder uses the passed allocator in non-owned manner.\n3. TestHeapMessageBuilder with a custom allocator (owned) that's neither option 1 nor 2. I used DefaultAllocator as it was handy. It could be anything that is-a flatbuffers::Allocator.\nMay be this is clearer.\n```cpp\nstruct OwnedAllocator : public flatbuffers::DefaultAllocator {};\nstruct TestHeapMessageBuilder : public flatbuffers::FlatBufferBuilder {\n  TestHeapMessageBuilder()\n    : flatbuffers::FlatBufferBuilder(2048, new OwnedAllocator(), true) {}\n};. Yes. I'll add if to bail out if the vars are not defined. . It's not possible to get that far if the includes are missing. Compilations fails in make with the following error on Ubuntu 18. Same story on RHEL7. \nBuilding protobuf is not necessary. I've updated the instructions to reflect that. Include path, however is necessary. \n$ cmake -DFLATBUFFERS_BUILD_GRPCTEST=ON ..\n$ make\n...\n[ 45%] Building CXX object CMakeFiles/grpctest.dir/tests/monster_test.grpc.fb.cc.o\nIn file included from /home/sutambe/projects/myoss/myflatbuffers/tests/monster_test.grpc.fb.h:8:0,\n                 from /home/sutambe/projects/myoss/myflatbuffers/tests/monster_test.grpc.fb.cc:6:\n/home/sutambe/projects/myoss/myflatbuffers/include/flatbuffers/grpc.h:23:10: fatal error: grpc++/support/byte_buffer.h: No such file or directory\n #include \"grpc++/support/byte_buffer.h\"\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nCMakeFiles/grpctest.dir/build.make:66: recipe for target 'CMakeFiles/grpctest.dir/tests/monster_test.grpc.fb.cc.o' failed\nmake[2]: *** [CMakeFiles/grpctest.dir/tests/monster_test.grpc.fb.cc.o] Error 1\nCMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/grpctest.dir/all' failed\nmake[1]: *** [CMakeFiles/grpctest.dir/all] Error 2\nMakefile:140: recipe for target 'all' failed\nmake: *** [all] Error 2. I'm no longer proposing this. I think we should have both methods. I commented earlier explaining why.. @aardappel A simple way is to always generate two methods: one template friendly and another  human-friendly. In that case, there's no need for an command-line argument at all. I really like this approach as the change can be made in gRPC repo directly. \nSo the followup PR should be opened against gRPC and not flatbuffers?. Does this comment mean, you now prefer separation of test code in .h and .cpp?. It's not clear what the issue is with assert(0). There are globals like testing_fails. I'm separating declaration and definition.  The test.cpp file is kind of too big.. @aardappel I opened a PR in gRPC. Do you know who the best person(s) is to review it? Also, I'm a somewhat unclear about the labels thingy in the gRPC repo. I believe I need some help from someone with write access to the repo. I understand this is probably not be the right place to talk about it though.. yes, that's my understanding.. More accurately, grpc_slice_unref destroys the data and refcount when the refcount hits zero. Otherwise, slice only refers to refcount. . I understand now. I prefer counting test failures rather than bailing out on the first error. This seems to be a norm in the gradle world (Java). But I could be persuaded otherwise.\nI updated the TEST macros to include full function name as there are template instantiations. I tested without assert(0) and caused some tests to fail deliberately. I see output like this\n```\n/Users/sutambe/projects/myoss/myflatbuffers/cmake-build-debug/flattests\nVALUE: \"0\"\nEXPECTED: \"1\"\nTEST FAILED: /Users/sutambe/projects/myoss/myflatbuffers/tests/test_builder.h:166, release_n_verify(b1, m2_name, m1_color) in static void BuilderTests::builder_move_assign_after_releaseraw_test() [Builder = flatbuffers::FlatBufferBuilder]\nVALUE: \"0\"\nEXPECTED: \"1\"\nTEST FAILED: /Users/sutambe/projects/myoss/myflatbuffers/tests/test_builder.h:166, release_n_verify(b1, m2_name, m1_color) in static void BuilderTests::builder_move_assign_after_releaseraw_test() [Builder = TestHeapBuilder]\nVALUE: \"0\"\nEXPECTED: \"1\"\nTEST FAILED: /Users/sutambe/projects/myoss/myflatbuffers/tests/test_builder.h:166, release_n_verify(b1, m2_name, m1_color) in static void BuilderTests::builder_move_assign_after_releaseraw_test() [Builder = GrpcLikeMessageBuilder]\n3 FAILED TESTS\nProcess finished with exit code 1\n```\nOutput on Windows is not gonna be so nice. It is still better as it tells me that 3 tests failed in all as opposed to the first one only. I'll rebase if you are ok with it or revert and drop the counter.\nWhat say?. Removed.. Good catch. I added it for symmetry. Later on never needed it. If it's really bothersome, I would drop it.. A pointer to DefaultAllocator::dealloc is passed to grpc_slice_new in MessageBuilder(FlatBufferBuilder &&). DefaultAllocator::alloc is not used anywhere.. I generalized the MessageBuilder move constructor to accept a deallocator. The default is DefaultAllocator::dealloc. This change enables conversion of FlatBufferBuilder that reserves memory out of a custom allocator with a global state. . @aardappel  I zapped the unused alloc function from DefaultAllocator. Do you think the PR can be merged soonish?. ",
    "csharrison": "Oops I missed that. I'll make sure to fix it upstream if this regresses next time.. ",
    "ianfhunter": "@rw does this need a re-review? looks like @ElliotHYLee addressed your comments. ",
    "Esvandiary": "I'm happy to have a go; I did already have a look at idl_gen_general.cpp, and it didn't look too tricky other than ensuring I didn't accidentally mark things as internal on the Java generation too!\nI'll read up about the specifics of the metadata/attribute system and give it a go.. ",
    "toddlipcon": "I guess in theory you could use a cached ByteBuffer and swap the byte[] in and out using private reflection, but that gets into some pretty sketchy areas, reflecting the internals of JDK library classes. Plus, using the cached buffer involves the thread local which isn't so good for performance as shown in the JMH benchmarks I ran.\nThe instanceof approach is the 'FileDesc4' class here: https://gist.github.com/a9c1fa1e9e46a558a8a2d123e5633d25\nEssentially we store an Object in the Table class, and then do 'instanceof' checks. The JIT will likely optimize these checks out in the case that the underlying type is monomorphic (ie always the same) - and the JMH benchmarks prove this out as the 'instanceof'-based approach performs basically the same as directly storing a byte[] in the class.. That's a good point. My suspicion is that any VM in the last several years will do type profiling and specialization, but who knows.\nIs it possible to install the Android VM (is it dalvik?) on a Linux server and run it? I'm not an Android dev and don't have any setup to run benchmarks.. Do you guys have a standardized set of benchmarks you can run on Android for Flatbuffers to validate changes like this? Worst case we can make a local fork for our use case but I'd prefer to contribute back.. Hmm.. so it sounds like we may be at an impasse. There is a concern about performance on Android but no way to verify.\nOne more trick we could do: I'm pretty sure Android would optimize out static final constants. Maybe we could add a static final boolean indicating Android OS, and only enable the instanceof checks on that platform. We would still need the downcast, though, but I'd be surprised if an unchecked downcast had any overhead.\nWorst case, we will just fork for our project, but it would be a shame.. Right, the suggestion would be:\n```\nclass Table {\n  private static final IS_ANDROID = System.getProperty(\"os.version\").equals(\"android\"); // or whatever\n  private Object bb;\nprotected int getShort(int off) {\n    if (IS_ANDROID) { return ((ByteBuffer)bb).getShort(off); }\n    else { return fancyGetShort(off); }\n  }\n...\n}\n```\nAt least in all version of HotSpot that I've worked with (since Java 5, maybe earlier), \"static final\" fields are treated as constants, and branches on constants are optimized out, even at the first tier of compilation. I'd be really surprised if ART JIT/AOT doesn't handle that. Then the question is whether the ByteBuffer cast is a checked cast or optimized out in the case that the class has never been instantiated with a different type. Downcasting from Object is exceedingly common given type erasure (any collection access is doing a downcast) so it seems quite likely this has zero runtime cost.\nAnyway, without any benchmark, I guess this discussion is mostly academic, and I may as well just fork flatbuffers in order to get the memory savings and performance we need.. Guess I'll close this as wontfix.. ",
    "EscApp2": "\nCould it be some non-standard libc?\n\nI dont know. But I dont think so. \nI reinstall  libstdc, like this.\n\nsudo yum reinstall libstdc++.so.6\n\nAnd get this:\n\n[root@some-list flatbuffers]# /sbin/ldconfig -p | grep stdc++\n        libstdc++.so.6 (libc6,x86-64) => /lib64/libstdc++.so.6\n        libstdc++.so.6 (libc6) => /lib/libstdc++.so.6\n[root@some-list flatbuffers]#  strings /usr/lib/libstdc++.so.6 | grep LIBCXX\nGLIBCXX_3.4\nGLIBCXX_3.4.1\nGLIBCXX_3.4.2\nGLIBCXX_3.4.3\nGLIBCXX_3.4.4\nGLIBCXX_3.4.5\nGLIBCXX_3.4.6\nGLIBCXX_3.4.7\nGLIBCXX_3.4.8\nGLIBCXX_3.4.9\nGLIBCXX_3.4.10\nGLIBCXX_3.4.11\nGLIBCXX_3.4.12\nGLIBCXX_3.4.13\nGLIBCXX_3.4.14\nGLIBCXX_3.4.15\nGLIBCXX_3.4.16\nGLIBCXX_3.4.17\nGLIBCXX_3.4.18\nGLIBCXX_3.4.19\nGLIBCXX_DEBUG_MESSAGE_LENGTH\n\nAnd still geting error\n\n[root@some-list flatbuffers]# cmake3 -G \"Unix Makefiles\" -DCMAKE_BUILD_TYPE=Release\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /root/flatbuffers\n[root@some-list flatbuffers]# make\n[ 11%] Built target flatbuffers\n[ 13%] Linking CXX executable flatc\nCMakeFiles/flatc.dir/src/util.cpp.o: In function flatbuffers::LoadFileRaw(char const*, bool, std::string*)':\nutil.cpp:(.text+0x42c): undefined reference tostd::istream::seekg(long long, std::_Ios_Seekdir)'\nutil.cpp:(.text+0x457): undefined reference to `std::istream::seekg(long long, std::_Ios_Seekdir)'\ncollect2: error: ld returned 1 exit status\nmake[2]:  [flatc] Error 1\nmake[1]:  [CMakeFiles/flatc.dir/all] Error 2\nmake: *** [all] Error 2. I'm not familiar with C++ or C \nAnd I do not have access to other Linux.\n\nThank you for your time. ",
    "kulswanand": "I signed it!. Yes, changed 'sizeof(data)' to 'length'. \nTested code changes. \nUpdated code and committed diff. . Good catch! \nAdded c_str() and committed diff. \n. ",
    "emilekm2142": "I think visual micro uses microsoft compiler. However, I got the same error even with a regular Arduino IDE that uses GCC.. I might be doing something wrong since I am not familiar with C++ build system, but it does not work even on a clean arduino installation with only ArduinoSTL installed.\nHere is what I did:\nDownloaded Arduino IDE and installed it.\nDownloaded ArduinoSTL from the manager\nCloned the repository and moved the /src folder into my sketch so I have this structure:\n\nsketch.ino\nflatbuffers.h\n/flatbuffers/base.h\n/flatbuffers/*.h\n\nNext, I had to change the line #include \"flatbuffers/stl_emulation.h\" in base.h to #include \"stl_emulation.h\" to get rid of the error saying that there is no such file\nSketch with the following content:\n```\ninclude \ninclude \"flatbuffers.h\"\nvoid setup() {\n}\nvoid loop() {\n}\ndoes not compile. Is it the fault of the ArduinoSTL missing std::emplace_back and others?\nIn file included from sketch/flatbuffers/base.h:52:0,\n                 from sketch/flatbuffers.h:20,\n                 from /home/emile/Desktop/test_flatbuffers/test_flatbuffers.ino:2:\nsketch/flatbuffers/stl_emulation.h: In function 'void flatbuffers::vector_emplace_back(std::vector, V&&)':\nsketch/flatbuffers/stl_emulation.h:77:26: error: 'forward' is not a member of 'std'\n     vector->emplace_back(std::forward(data));\n                          ^\nsketch/flatbuffers/stl_emulation.h:77:40: error: expected primary-expression before '>' token\n     vector->emplace_back(std::forward(data));\n                                        ^\nsketch/flatbuffers/stl_emulation.h: At global scope:\nsketch/flatbuffers/stl_emulation.h:113:50: error: 'is_scalar' in namespace 'std' does not name a template type\n     template  using is_scalar = std::is_scalar;\n                                                  ^\nsketch/flatbuffers/stl_emulation.h:114:60: error: 'is_same' in namespace 'std' does not name a template type\n     template  using is_same = std::is_same;\n                                                            ^\nsketch/flatbuffers/stl_emulation.h:115:58: error: 'is_floating_point' in namespace 'std' does not name a template type\n     template  using is_floating_point = std::is_floating_point;\n                                                          ^\nsketch/flatbuffers/stl_emulation.h:116:52: error: 'is_unsigned' in namespace 'std' does not name a template type\n     template  using is_unsigned = std::is_unsigned;\n                                                    ^\nsketch/flatbuffers/stl_emulation.h:136:48: error: 'unique_ptr' in namespace 'std' does not name a template type\n     template  using unique_ptr = std::unique_ptr;\n                                                ^\nIn file included from /home/emile/Desktop/test_flatbuffers/test_flatbuffers.ino:2:0:\nsketch/flatbuffers.h: In function 'flatbuffers::Vector > flatbuffers::VectorCast(flatbuffers::Vector >)':\nsketch/flatbuffers.h:326:17: error: 'is_base_of' is not a member of 'std'\n   static_assert(std::is_base_of::value, \"Unrelated types\");\n                 ^\nsketch/flatbuffers.h:326:34: error: expected primary-expression before ',' token\n   static_assert(std::is_base_of::value, \"Unrelated types\");\n                                  ^\nsketch/flatbuffers.h:326:36: error: expected string-literal before 'U'\n   static_assert(std::is_base_of::value, \"Unrelated types\");\n                                    ^\nsketch/flatbuffers.h:326:36: error: expected ')' before 'U'\nsketch/flatbuffers.h: In function 'const flatbuffers::Vector > flatbuffers::VectorCast(const flatbuffers::Vector >*)':\nsketch/flatbuffers.h:332:17: error: 'is_base_of' is not a member of 'std'\n   static_assert(std::is_base_of::value, \"Unrelated types\");\n                 ^\nsketch/flatbuffers.h:332:34: error: expected primary-expression before ',' token\n   static_assert(std::is_base_of::value, \"Unrelated types\");\n                                  ^\nsketch/flatbuffers.h:332:36: error: expected string-literal before 'U'\n   static_assert(std::is_base_of::value, \"Unrelated types\");\n                                    ^\nsketch/flatbuffers.h:332:36: error: expected ')' before 'U'\nsketch/flatbuffers.h: In member function 'flatbuffers::vector_downward& flatbuffers::vector_downward::operator=(flatbuffers::vector_downward&&)':\nsketch/flatbuffers.h:596:26: error: 'move' is not a member of 'std'\n     vector_downward temp(std::move(other));\n                          ^\nsketch/flatbuffers.h: At global scope:\nsketch/flatbuffers.h:1391:18: error: 'function' in namespace 'std' does not name a template type\n       const std::function &f) {\n                  ^\nsketch/flatbuffers.h:1391:26: error: expected ',' or '...' before '<' token\n       const std::function &f) {\n                          ^\nsketch/flatbuffers.h:1471:38: error: 'function' in namespace 'std' does not name a template type\n       size_t vector_size, const std::function &filler) {\n                                      ^\nsketch/flatbuffers.h:1471:46: error: expected ',' or '...' before '<' token\n       size_t vector_size, const std::function &filler) {\n                                              ^\nsketch/flatbuffers.h: In member function 'flatbuffers::FlatBufferBuilder& flatbuffers::FlatBufferBuilder::operator=(flatbuffers::FlatBufferBuilder&&)':\nsketch/flatbuffers.h:861:28: error: 'move' is not a member of 'std'\n     FlatBufferBuilder temp(std::move(other));\n                            ^\nsketch/flatbuffers.h: In member function 'void flatbuffers::FlatBufferBuilder::AssertScalarT()':\nsketch/flatbuffers.h:999:19: error: 'is_scalar' is not a member of 'flatbuffers'\n     static_assert(flatbuffers::is_scalar::value, \"T must be a scalar type\");\n                   ^\nsketch/flatbuffers.h:999:43: error: expected primary-expression before '>' token\n     static_assert(flatbuffers::is_scalar::value, \"T must be a scalar type\");\n                                           ^\nsketch/flatbuffers.h:999:44: error: '::value' has not been declared\n     static_assert(flatbuffers::is_scalar::value, \"T must be a scalar type\");\n                                            ^\nsketch/flatbuffers.h: At global scope:\nsketch/flatbuffers.h:2301:16: error: 'function' in namespace 'std' does not name a template type\n   typedef std::function\n                ^\nsketch/flatbuffers.h:2303:16: error: 'function' in namespace 'std' does not name a template type\n   typedef std::function rehasher_function_t;\n                ^\nexit status 1\nError compiling for board Arduino/Genuino Uno.\n```\n. ",
    "slnovak": "Hi @ccifra. Thanks for looking into this! I have a use-case that would also benefit from this. Have you considered also including support for memory-mapped files? I poked around a little bit and came across dotnet/corefx#26603 (and this comment in particular) suggests that it might be possible.. ",
    "et1975": "@ccifra a \ud83d\udc4d from me... I'd like to move my Storm interop to a lower overhead protocol and if zero-copy is the name of the game, I think the .net flatbuffers would benefit from all the zero-copy we can get.  Span as well as PipeWriter/PipeReader for IO instead of Stream would be a huge win imho.\nHaving said that I think it would be valuable to see the difference , like this proposal in protobuf: https://github.com/google/protobuf/issues/3530#issuecomment-327960466. >the user has to explicitly enable ENABLE_SPAN_T\nGiven that this is a preprocessor directive, how would this surface to the user?. ",
    "ccifra": "Memory mapped files / shared memory should work fine with this change.  \nI did an initial implementation that used Span instead of directly using the byte[] in ByteBuffer.  Unfortunately this was considerable slower.  Now I have another implementation where byte* is used in much the same way it is used when UNSAFE_BYTEBUFFER is defined and I managed to get about a 10% performance improvement over the current implementation.  My change allows using a byte[] as the backing store for the ByteBuffer by default.  The byte[] implementation can be swapped out with another implementation like one that uses a memory mapped file instead.\nI also added support for writing to the buffer with a Span instead of just byte[].  This allows writing blocks of data from many sources without first copying to a byte[]. \nI should have a prototype implementation ready in the next day or so.\nI will have to look into pipe reader / writer.  I am not too familiar with those.. I pushed my changes to enable direct access to shared memory to my fork:\nhttps://github.com/ccifra/flatbuffers\nI should be ready for a PR soon.  Maintaining compatibility is tricky so I put the support behind a compile symbol ENABLE_SPAN_OF_T. I have created PR 4886 to implement this support.\nhttps://github.com/google/flatbuffers/pull/4886\n. #4886\n. I think this is my fault.  I will try and do a PR with a fix today.  We don't need to use expression bodied members for this code.. All set.. When I added the Span support I wanted to do something similar to your proposal.  But when I profiled the implementation it was slower than keeping a byte* around.  \nIf I remember correctly having the code access a property which returned a Span to do writes was 10-15% slower (about the same difference you get when using UNSAFE_BYTEBUFFER or not.  I tested on both .NET Core and .NET Framework. The framework version was a little bit slower than the core version when using Span.\nMaybe this performance difference is not enough to matter?  I think the implementation would be cleaner without pointers.. I will have to dig up the benchmark apps I was using, but it basically doing something like this several times in a row and report the times.\n``` csharp\n            FlatBufferBuilder builder = new FlatBufferBuilder(1024 * 1024 * 32);\n            var start = System.DateTime.UtcNow;\n        // Create tables with 3 doubles and a small string.\n        for (int x = 0; x < 1000000; ++x)\n        {\n            var offset = builder.CreateString(\"T\");\n            builder.StartObject(4);\n            builder.AddDouble(3.2);\n            builder.AddDouble(4.2);\n            builder.AddDouble(5.2);\n            builder.AddOffset(offset.Value);\n            var o = new Offset<DataValue>(builder.EndObject());\n            c[x] = o;\n            ++x;\n        }\n\n        // Build an array of the tables\n        var arrayElements = builder.CreateVectorOfTables(c);\n        var a = ArrayValue.CreateArrayValue(builder, arrayElements);\n\n        var end = System.DateTime.UtcNow;\n\n```\nI tested using a ByteBufferAllocator with an API which returned a byte* vs. Span.  No matter what I tried, the Span version was slower.. I added another benchmark to your tests.\nIt looks like this:\ncsharp\n[Benchmark]\npublic void TestTables()\n{\n    FlatBufferBuilder builder = new FlatBufferBuilder(1024 * 1024 * 32);\n    for (int x = 0; x < 500000; ++x)\n    {\n        var offset = builder.CreateString(\"T\");\n        builder.StartObject(4);\n        builder.AddDouble(3.2);\n        builder.AddDouble(4.2);\n        builder.AddDouble(5.2);\n        builder.AddOffset(offset.Value);\n        builder.EndObject();\n    }\n}\nHere is my branch with that test added.\nhttps://github.com/ccifra/flatbuffers/tree/AddBenchmarks\nWhen I run the tests I get these results:\nMy AddBenchmarks\nMethod |      Mean |     Error |   StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n-------------- |----------:|----------:|---------:|------------:|------------:|------------:|--------------------:|\nBuildMonsters |  7.057 ms | 0.3868 ms | 1.141 ms |    484.3750 |    234.3750 |   - |             1.88 MB |\nTestTables | 79.477 ms | 1.5359 ms | 1.437 ms |    200.0000 |    200.0000 |    200.0000 |               32 MB |\nI pulled in the changes from your UseMemory branch into a branch on my fork:\nhttps://github.com/ccifra/flatbuffers/tree/UseMemory\nI ran the tests again and got these results:\nMethod |       Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n-------------- |-----------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\nBuildMonsters |   1.857 ms | 0.0371 ms | 0.0938 ms |    468.7500 |           - |  - |             1.88 MB |\nTestTables | 124.384 ms | 2.4251 ms | 2.0250 ms |    750.0000 |    750.0000 |    750.0000 |               32 MB |\nIt's interesting that my first set of numbers were a lot different than yours for BuildMonsters. My numbers for BuildMonsters in the UseMemory branch are close.\nI cloned your Repo directly and ran the tests and got closer numbers:\nAddBenchMarks branch\nMethod |       Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n-------------- |-----------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\nBuildMonsters |   2.292 ms | 0.0441 ms | 0.0453 ms |    843.7500 |           - |           - |             3.38 MB |\nTestTables | 117.429 ms | 1.9028 ms | 1.6868 ms |    800.0000 |    800.0000 |    800.0000 |               32 MB |\nUseMemory Branch\nMethod |       Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n-------------- |-----------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\nBuildMonsters |   1.746 ms | 0.0414 ms | 0.0917 ms |    468.7500 |           - |  - |             1.88 MB |\nTestTables | 135.762 ms | 2.9911 ms | 8.6777 ms |    750.0000 |    750.0000 |    750.0000 |               32 MB |\nThis is close to your results, but in all cases the UseMemory branch is slower for my test than the current implementation.  :frowning:\n.NET Framework\nI also created a test app for .NET Framework and tested it against both implementations.\nhttps://github.com/ccifra/flatbuffers/tree/UseMemory/tests/FlatBuffers.FrameworkBenchmark\nHere the story is even worse.\nThis app runs the same test several times and writes the times:\nThe current implementation results are:\n62.0017 ms\n55.9995 ms\n51.9854 ms\n56.0341 ms\n49.9761 ms\n53.0163 ms\nAnd the UseMemoryBranch results are:\n191.0013 ms\n167.998 ms\n172.9987 ms\n163.9987 ms\n166.9985 ms\n166.9984 ms\nThis difference is a lot more than I would have expected.  Hopefully there is a flaw in my testing. I do like your implementation better.\nI did not get a chance to profile yet or look at why the GC is so high for my test.  Maybe it is just a GC issue that we can cleanup?. I think option 3 is interesting.\nTestTables is really doing the same operations that the generated code does under the hood, it just didn't start with a schema.\nIf I understand it correctly there would have to be several changes:\n\n\nMake a ref struct version of FlatBufferBuilder, call it StructBuilder, that some API, BeginOperation() or similar, on FlatBufferBuilder would return.  FlatBufferBuilder would still own the vtable management and StructBuilder would cache the Span or whatever it needs to be fast.\n\n\nFlatBufferBuilder's current API would forward to a local instance of StructBuilder.\n\n\nChange the generated code to take a StructBuilder instead of FlatBufferBuilder as a parameter so that the generated code can be used just as efficiently as the low level API.  Maybe there could be an implict conversion between the FlatBufferBuilder and StructBuilder?\n\n\nMy projects tend to build buffers with a large number of tables which is why I was testing this use case.  We mostly use the generated API to build the tables, but sometimes we use the low level APIs.  I think it is important to ensure good performance with the generated APIs. \nEfficient usage would look like this:\n``` csharp\nvar builder = new FlatBufferBuilder(16);\n// Use StructBuilder to get fastest performance\nvar fastBuilder = builder.BeginOperation();\nvar str1 = fastBuilder.CreateString(nestedMonsterName);\nMonster.StartMonster(fastBuilder);\nMonster.AddName(fastBuilder, str1);\nMonster.AddHp(fastBuilder, nestedMonsterHp);\nMonster.AddMana(fastBuilder, nestedMonsterMana);\nvar monster1 = Monster.EndMonster(fastBuilder);\nMonster.FinishMonsterBuffer(fastBuilder, monster1);\nWith implicit conversion this would also work but would be slower: csharp\nvar builder = new FlatBufferBuilder(16);\nvar str1 = builder.CreateString(nestedMonsterName);\nMonster.StartMonster(builder);\nMonster.AddName(builder, str1);\nMonster.AddHp(builder, nestedMonsterHp);\nMonster.AddMana(builder, nestedMonsterMana);\nvar monster1 = Monster.EndMonster(builder);\nMonster.FinishMonsterBuffer(builder, monster1);\n```\nI don't mind having to know to create a local StructBuilder to get better performance but maybe others would disagree?\nside note:\nFor me the current unsafe implementation is faster than basic array case (no #IFDEF).\nWith my .NET Framework test:\n\n115ms - no #ifdef\n163ms - UseMemory implementation with UNSAFE_BYTEBUFFER;ENABLE_SPAN_T;BYTEBUFFER_NO_BOUNDS_CHECK\n55ms - Current implmenetation with UNSAFE_BYTEBUFFER;ENABLE_SPAN_T;BYTEBUFFER_NO_BOUNDS_CHECK\n\nMaybe this is because you are not setting BYTEBUFFER_NO_BOUNDS_CHECK?. I was thinking a bit more about you proposal and had a different thought.\nI've only thought about the building side side, but I think we can do something similar when accessing.\nif instead of switching everything away from using a byte we can have the builder better inform the allocated about the needed lifetime of the byte.  Right now the allocator always have the byte ready.  In practice we only need the byte around while we are building and we already know the needed scope of the byte*.  We could pin of first access and then have the builder tell the allocator to unpin when\nFinish() is called.\nWe can have the default ByteArrayAllocator support returning the underlying buffer as a Memory  and users can always create a custom Allocator for any specific needs.\nI think this would let you have Memory support via the allocator, and we could improve the lifetime of the pinned memory.\nI don't completely know what trouble you are having with the current implementation so I may be off base here.  Let me know if this sounds interesting and I can elaborate it some more.\n. I have an idea of another implementation that, I think, should get everyone what they want.  Specifically being able to nicely use a Memory for the backing memory of a buffer and being able to get a Memory of a built buffer while maintaining good performance for a wide variety of usages.\nWe can change the ByteBufferAllocator to have a Pin() method which returns a MemoryHandle to the underlying memory:\ncsharp\npublic abstract MemoryHandle Pin();\nThis would replace the current buffer property\ncsharp\npublic unsafe byte* Buffer\nThis mirrors the API of Memory Memory.Pin().  Reading and writing a flat buffer would use the Pin method to access the raw memory via the MemoryHandle's Pointer property.  The memory handle would be disposed inside of each method.  \nThis, i believe, would have all of the advantages we are looking for:\n Very compatible with a Memory as backing store\n Short lifetime of pinned memory\n A way to get optimized performance by either creating a custom allocator which is always pinned or by manually pinning the allocator while building or reading from a buffer.\n No additional allocations since MemoryHandle is a struct\n Avoid's the cost of constantly constructing a Span\n Minor changes to the existing API of Flatbuffers and only affects the newly introduced ByteBufferAllocator API.\nWhat do you think?\n. If you are just wanting to get a Memory from the byte buffer can we not just add a ToMemory() method to ByteBuffer / ByteBufferAllocator?  The default allocator can certainly return a Memory since it is byte[] back.\nThe other concern you seem to have is around the pinned lifetime of the underlying array, and using Dispose() solves that.\nAdding a ToMemory seems like a better solution that adding yet another want to compile the library.. I was able to incorporate your UseMemoryPlayground version into our actual application and give it a try.\nMy tests involved using our custom allocators with the new Span / Memory based API and profiling the \"inner loop\" creation of a flat buffer.  The tests were against .NET Framework, which is likely the slower than .NET Core.\nThe tests showed that this new approach is just slightly slower than our current builds.  Times went from about 35ms to about 40ms for the build operation.  \nI think this is an acceptable difference in performance for the gains in API improvements.  I say we should move forward with this new approach.  \nI was not the one who added the original UNSAFE_BYTE_BUFFER configuration so there may be a use case out there which needs the extra perf?. You will have to build the library with ENABLE_SPAN_T defined in much the same way to need to build the library with UNSAFE_BYTEBUFFER defined to get the performance benefits.. Done.. I can indent them, but it seems like this would be inconsistent with the rest of the #ifdef's in the project.  Do you want me to indent every #ifdef?. I removed the check.. Remove the comments.  The generated TypeScript is now unchanged.. Updated.. Updated.  Sorry about all of these formatting issues, my editor found a formatting rule set and keeps applying the rules to this file.. fixed.. fixed. fixed. Should we remove IDisposable since the default implementation no longer needs it?  Custom allocators could still be disposable if needed.\n. I don't think that scenario would be a problem.  It is not for any of mine.\n. I think it is worth noting that with this change you would no longer be able to use a custom allocator without also enabling Span support.  This would really only be an issue for older versions of .net that do not support Span.  Custom allocators was only recently added and I don't expect anyone will run into this, but I thought it should be \nnoted.\nSorry, for not mentioning it earlier but I just realized it while reviewing the code.\n. Maybe we should make this non virtual to avoid one additional virtual call?  Not sure when the base implementation would not be enough.. If you can get a perf gain by overriding then I would be inclined to leave it as is.. Looks good to me!. ",
    "rressi": "I have just signed CLA as requested.. OK I'll try.. ",
    "MasonRemaley": "I don't have time to figure out what part of the code needs to be changed to fix this + add the appropriate tests and stuff right now, I just figured that since I saw the issue I'd let you know it existed.\nAs for what the fix should be, I'd recommend looking into why C# opted to use signed integers here and then either switch to using an unsigned type or add a bounds check. IMO wrapping the integer is just trading one kind of unspecified behavior for another, I'm not sure that saving a single bounds check is worth incorrect semantics--anyone using C# has already accepted that array accesses have this overhead. At the very least if you were gonna wrap it you'd wanna document super clearly that that's being done or that negative indices produce unspecified results or such but currently there isn't any C# specific documentation for flatbuffers that I can find so I'm not sure where those docs would go.. I agree, I was very surprised as well to find out that C# used signed integers for indices, I just think it would be doubly surprising to then also wrap them. Thanks for the link!. ",
    "lcxywfe": "I signed it!. I have closed #4891\nAnd i think this PR include the functions of #4853?\nMerge #4853 first and solve the conflict?. I rebased the master and solved the conflict of generated files (i don't know the use of the files). Closed it.. if the namespace is \"A::B::C\", only create A/B/C/init.py, because i think A/B/init.py should be created when the namespace is \"A::B\". (But maybe there is no Struct in A/B, so the A/B/init.py would not be created, i don't know if it is correct)\nThe old logic will repeating SaveFile at every iter, and overwriting it,  so i add this check to reduce repeating, should i add a set to record and avoid repeating SaveFile?. done. done. ",
    "phayes": "Should the fie tests/rust_usage_test/test_bench_output.txt have been included in this PR?. ",
    "DrYSG": "OUCH!!. This is something I have known for over 35 years. I must be getting senile.\nIt is working fine now.\n@aardappel \nWhat a wonderfully fast response time. Thank you for not sneering at me.\n. I am willing to put some sweat and time into investigation, but as you can see it is a description field, so it is not on my critical path, if you can wait @alexames my hunch is it has something to do with null termination. No?. I think without clues from your team on where to look in the FB source, I have gotten as far as I can, which is why I put it as issue. As I said, the exact same packets are being looked at by C++ code and there is no problem with deserializing.\nA brief overview of what is going on. I have a C++ windows exe that is using NanoMessage (NNG) with Google FB to send packets from one windows node to another.\nI have two listeners on the bus. One is in node using the JS version of flatbuffers (and NNG) and the other is in C++. No issues with deserializing the FB with the C++ exe. Only with the JS version. All three are using the exact same schema. Any suggestions here?\nSent with GitHawk. So it is odd. If you look at the FBS schema, I have two string fields in  the table.\nsystemTimeString: string;\n  description: string;\nThe time is coming through just fine. But the description that was sent was:\n\"AFSIM sending: Venom12\"\nI thought it was perhaps chopping the string short at 11 characgters, But if I just sent Venom11 with no AFSIM sending: then I get the \\u0000\\u0000\" right away.\n\"systemTimeString\": \"Thu Sep 20 10:10:55 2018\",\n\"description\": \"AFSIM sending: V\\u0000\\u0000\",\n\nIF I add on some extra text after the Venom12 then I get more of the text, but it is definitely truncating early.\n     string desc = \"AFSIM sending: \" + callSign + \" From YSG4206\";\n\nThey I get the following:\n\"systemTimeString\": \"Thu Sep 20 10:20:33 2018\",\n    \"description\": \"AFSIM sending: Venom12 From \\u0000\\u0000\",\nThe small demo I have requires NanoMSG and a network connection. So I don't think that is what you want. It looks like it is always about 7 characters short.. I am reading it from a network socket. Using NanoMSG: https://github.com/nickdesaulniers/node-nanomsg\nDo you think the description is at the end of the FB payload? It is not defined there in the schema, nor added last?\nI wrap the FB payload in a small struct, Here is how I unpack:\n```\n    receive(packet) {\n        let view = new DataView(packet.buffer)\n        let topic = view.getUint32(0, true)\n        let size = view.getUint32(4, true)\n        let payload = packet.slice(8, packet.length - 8)\n        global.Dispatcher.process(topic, size, payload)\n    }\n```\nand here is how I pack the packet:\n```\nbool PacketTelemetry::build(double lat, double lon, double alt, std::string desc, Topics topic) {\n    flatbuffers::FlatBufferBuilder builder(PayloadMax);\n    char *timeStamp = this->date();\n    long long clock = this->wallClock();\n    auto position = PNT::PosVector(lat, lon, alt);\n    auto description = builder.CreateString(desc);\n    auto source = builder.CreateString(nodeName);\n    auto dateTime = builder.CreateString(timeStamp);\n    auto msg = PNT::CreateTelemetry(builder, source, clock, dateTime, description, &position);\n    builder.Finish(msg);\n    return this->finish(&builder, topic);\n}\nbool Packet::finish(flatbuffers::FlatBufferBuilder builder, Topics topic) {\n    uint8_t payload = builder->GetBufferPointer();\n    int payloadSize = builder->GetSize();\n    if (payloadSize > PayloadMax) return false;\n    memcpy(thePacket->payload, payload, payloadSize);\n    thePacket->payloadSize = payloadSize;\n    thePacket->topic = topic;\n    return true;\n}\nconst int PayloadMax = 256;\nstruct packetStruct {\n    Topics topic;\n    unsigned int payloadSize;\n    uint8_t payload[PayloadMax];\n};\n```\nI am sure I am not hitting the 256 limit, since the build would complain.\n. @aardappel  Yea, I got myself confused. I thought the size was the packet size, not the payload size.. I apologize, I wrote this poorly. Busy tomorrow, but I will get the simple example to you soon.\n@vglavnyy I hope I described the issue better. Still working on a trivial example for the error.. line 368: of util.h\n  *val = flatbuffers::numeric_limits<uint64_t>::max();\n\npackage.json says:\n\"name\": \"flatbuffers\",\n  \"version\": \"1.10.2\",\n. Thank you Dr. Oortmerssen\nChanging line 368 worked for me. \nI have a preference for that approach. Having been programming since before the times of Ritchie and  Kernighan, (and 80+ other languages and OS since then) I try not to get into who to blame but look for practical work arounds.\n(If anyone I sometimes through blame at Stroustrup, for such a hodge-podge language, but he cannot be faulted here). \nFunny thing, we are also using FB for serious gaming R&D work. And quite happy with it.. We are hacking DIS/HLA (e.g VBS3 https://bisimulations.com/products/virtual-battlespace , and others)  to simulate new sensors and platforms via peer remote high detail models of new sensors, and doing this monte-carlo with hundreds if not thousands of sim engines and then comparing performance.\nI am not sure if our firewall will allow the PR to go through.. ",
    "felix": "I signed it!. It depends on how long we have to wait. Can't this be merged here so FB's tooling keeps pace and then it will be moot once upstream changes?. ",
    "cholcombe973": "I signed it!. Interesting.  For whatever reason my mod came out differently. @rw I'm working on a PR for my filesystem project.  Here's the simple schema I created to test your work.  https://github.com/cholcombe973/rusix/blob/client_server/api/protos/service.fbs and it generated this: https://gist.github.com/cholcombe973/0c5407b5158114da88dbc7f9372684d1.  So when I cargo check against that it produces a ton of errors. @rw so do i just run the sh generate_tests?  I get a bunch of error messages about:\n../flatc: warning: GRPC interface generator not implemented for Rust along with all the other types. Leave it to users to use your code in unexpected ways ;) lol.  Because I never namespaced my schema it never imported the flatbuffers crate.. I think that should do it. No problem.  I've merged them all together.  Thanks for jumping on this so fast!. @rw oh no you're right!  I used the wrong function to calculate the fractional part of the time.  Sorry about that. Yes it's quite fast once I used the right function lol.  . I think rustfmt generally wraps at around 80 characters. ",
    "ehsanul": "cc @rw . @rw Your comment implies that only a single root_type should be declared in a schema. It turns out that this schema (not of my design) has multiple declared root types, unless I'm missing something. Is that not considered valid?\nGiven there is another way to do it without the convenience function, it's not a big deal to me, but it seems strange that the last declaration \"wins\", which is what appears to be happening to me.. Does work fine with a regular // instead of the ///.. Works great!. @rw Great, nested tables now work. But a struct nested within a table still has the same issue. Minimal example:\n```\nstruct Foo {\n    bar:float;\n}\ntable Baz {\n    bux:Foo;\n}\n```\nThis generates:\npub struct BazArgs {\n    pub bux: Option<&'a  Foo>,\n}\nimpl<'a> Default for BazArgs {\n    #[inline]\n    fn default() -> Self {\n        BazArgs {\n            bux: None,\n        }\n    }\n}. ",
    "mattfs": "This works! I grabbed that commit and it compiled out of the box. Thanks @rw . I'm cool with turning off clippy/rustfmt on the generated files. The big issue here is that we get massive diffs when people here forget to rustfmt before committing. As-is, the rust code is perfectly readable.. ",
    "eolivelli": "An alternative approach would be to make FlatBufferBuilder an interface or an abstract class, that an user could implement a Netty ByteBuf backed builder. @aardappel thanks for your quick response !\nI am building a prototype, I will share it as soon as I have something really useful.\nHaving a ByteBufferFactory#release(ByteBuffer) method will enable some sort of pooling of ByteBuffers.\nUnfortunately it does enable an user to leverage Netty ByteBufs, which are convenient because there is built in support in the outbound pipeline. The only way is to rewrite FlatBufferBuilder with ByteBuf instead of ByteBuffer.\nCurrently (with 1.9.0) it is possible to emulate the improvement we are discussing just by subclassing FlatBufferBuilder and redefining 'prep' this way:\n```\npublic class EnhancedFlatBufferBuilder extends FlatBufferBuilder {\nprivate Consumer<ByteBuffer> releaser;\n\npublic EnhancedFlatBufferBuilder(ByteBuffer existing_bb,\n        FlatBufferBuilder.ByteBufferFactory bb_factory, Consumer<ByteBuffer> releaser) {\n    super(existing_bb, bb_factory);\n    this.releaser = releaser;\n}\n\n@Override\npublic void prep(int size, int additional_bytes) {\n    // Track the biggest thing we've ever aligned to.\n    if (size > minalign) {\n        minalign = size;\n    }\n    // Find the amount of alignment needed such that `size` is properly\n    // aligned after `additional_bytes`\n    int align_size = ((~(bb.capacity() - space + additional_bytes)) + 1) & (size - 1);\n    // Reallocate the buffer if needed.\n    while (space < align_size + size + additional_bytes) {\n        int old_buf_size = bb.capacity();\n        ByteBuffer prev = bb;\n        bb = growByteBuffer(prev, bb_factory);\n        if (bb != prev) {\n            // RETURN PREVIOUS BUFFER TO THE POOL\n            releaser.accept(prev);\n        }\n        space += bb.capacity() - old_buf_size;\n    }\n    pad(align_size);\n}\n\n}\n```\nThis solution lets me build a working prototype. I will try to use a pool of Direct Buffers.\nMy real issue is about reducing memory allocations, it is not a ideal to allocate a new ByteBuffer for each serialization. . This is my proposal #4914. @aardappel great ! thank you very much.\nUsual questions, will you commit this to master ?\nIs there any schedule for the next release ?\nAt the moment I have this fully working FlatBufferBuilder which is able to leverage Netty ByteBuf Pool\nhttps://github.com/diennea/herddb/blob/master/herddb-utils/src/main/java/com/google/flatbuffers/ByteBufFlatBufferBuilder.java\n. Sure.\nDefault methods are not supported.\nThere is no way to make this change without breaking the API and keeping compatibility with jdk7.\nNot sure how I can deal with this problem.\nJdk8 is mostly EOL as well, do you really need to support java7 ?. The best option would be to make ByteBufferFactory a class instead of an interface.\nThis will break binary compatibility, applications will have to be recompiled.\n. If you want I can send a new patch. Sure, I will send the patch soon.\nDo I have to create an issue on some bug tracker?. @aardappel Here it is the PR #5155 \nLet's follow up the discussion on that patch. I have to fix the tests.\nI did not find that one.\nFill fix today. @aardappel \nI missed that test because I had found a pom.xml and I thought that \"mvn test\" would have run all of the tests.\nI have also tried to fix that pom in order to make it run Java tests, but the fix is not straightforward.\nIf you want I can send another patch for a fix on pom. @aardappel \nyou are welcome.\n\nAnd yes, a fix for the pom would be good, if it can be done elegantly.\n\nsure. I will try to fix the pom when I have cycles. I think this kind of change will speed up expecially the case with Direct Buffers.\nBecause that lookup ends up in several Unsafe#getXxx calls. ",
    "onurkaraman": "I signed the Google Individual Contributor License Agreement.. ",
    "pwrdwnsys": "As an interim mitigation, are there any examples of creating and using a Rust FFI wrapper for the C++ buffer verifier?. ",
    "tymcauley": "@rw I might be interested in taking a look at that at some point (work's busy right now, so it might not be for a week or two). I'll let you know if I've got any questions about the implementation you had in mind.. Now that #5229 is merged, the Rust crate is now being built/tested with a big-endian target (mips-unknown-linux-gnu) in CI. See this file for more information:\nhttps://github.com/google/flatbuffers/blob/9e82ee25275e07e35f489f6feac543611f81fb83/tests/docker/languages/Dockerfile.testing.rust.big_endian.1_30_1. Resolved by #5229.. Hey @rw, I'm guessing this pull request is in relation to #5091. I see that you're getting a linking error during the cargo MIPS build, and I've run into this problem in the past. You need to specify that Rust use a MIPS linker for the compilation. I think that adding this install command is needed:\napt install gcc-mips-linux-gnu\nAnd then you'll have to tell cargo to use mips-linux-gnu-gcc as the linker. You can do this either by setting the CARGO_TARGET_MIPS_UNKNOWN_LINUX_GNU_LINKER environment variable to mips-linux-gnu-gcc, or by creating a cargo config file with contents like this:\n[target.mips-unknown-linux-gnu]\nlinker = \"mips-linux-gnu-gcc\". Alright, just signed it. Looks like you have to make another comment to make the bot run again.. So, I just tried running the Rust tests like this:\n% apt install qemu-user\n% cd tests/rust_usage_test/\n% CARGO_TARGET_MIPS_UNKNOWN_LINUX_GNU_LINKER=mips-linux-gnu-gcc CARGO_TARGET_MIPS_UNKNOWN_LINUX_GNU_RUNNER=\"qemu-mips -L /usr/mips-linux-gnu\" cargo test --target mips-unknown-linux-gnu\nAnd I got these errors:\n``\nerror[E0599]: no function or associated item namedfollowfound for typeflatbuffers::ForwardsUOffset<&[T]>in the current scope\n   --> tests/integration_test.rs:796:23\n    |\n796 |             let got = <flatbuffers::ForwardsUOffset<&[T]>>::follow(buf, 0);\n    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ function or associated item not found inflatbuffers::ForwardsUOffset<&[T]>|\n    = note: the methodfollowexists but the following trait bounds were not satisfied:flatbuffers::ForwardsUOffset<&[T]> : flatbuffers::Follow`\nerror[E0277]: the trait bound &[u8]: flatbuffers::Follow<'_> is not satisfied\n    --> tests/integration_test.rs:1142:29\n     |\n1142 |                 let v = tab.get::>(fi2fo(i as flatbuffers::VOffsetT), None);\n     |                             ^^^ the trait flatbuffers::Follow<'_> is not implemented for &[u8]\n     |\n     = note: required because of the requirements on the impl of flatbuffers::Follow<'_> for flatbuffers::ForwardsUOffset<&[u8]>\nerror[E0277]: the trait bound &[T]: flatbuffers::Follow<'_> is not satisfied\n    --> tests/integration_test.rs:1221:31\n     |\n1221 |                 let got = tab.get::>(fi2fo(i as flatbuffers::VOffsetT), None);\n     |                               ^^^ the trait flatbuffers::Follow<'_> is not implemented for &[T]\n     |\n     = note: required because of the requirements on the impl of flatbuffers::Follow<'_> for flatbuffers::ForwardsUOffset<&[T]>\nerror[E0277]: the trait bound &[u8]: flatbuffers::Follow<'_> is not satisfied\n    --> tests/integration_test.rs:1665:52\n     |\n1665 |         let off: flatbuffers::FollowStart<&[u8]> = flatbuffers::FollowStart::new();\n     |                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ the trait flatbuffers::Follow<'_> is not implemented for &[u8]\n     |\n     = note: required by <flatbuffers::FollowStart<T>>::new\nerror[E0599]: no method named self_follow found for type flatbuffers::FollowStart<&[u8]> in the current scope\n    --> tests/integration_test.rs:1666:24\n     |\n1666 |         assert_eq!(off.self_follow(&vec[..], 4), &[1, 2, 3, 4][..]);\n     |                        ^^^^^^^^^^^\n     |\n     = note: the method self_follow exists but the following trait bounds were not satisfied:\n             &[u8] : flatbuffers::Follow\nerror[E0277]: the trait bound &[u8]: flatbuffers::Follow<'_> is not satisfied\n    --> tests/integration_test.rs:1679:52\n     |\n1679 |         let off: flatbuffers::FollowStart<&[u8]> = flatbuffers::FollowStart::new();\n     |                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ the trait flatbuffers::Follow<'_> is not implemented for &[u8]\n     |\n     = note: required by <flatbuffers::FollowStart<T>>::new\nerror[E0599]: no method named self_follow found for type flatbuffers::FollowStart<&[u8]> in the current scope\n    --> tests/integration_test.rs:1680:24\n     |\n1680 |         assert_eq!(off.self_follow(&vec[..], 4), &[1, 2, 3][..]);\n     |                        ^^^^^^^^^^^\n     |\n     = note: the method self_follow exists but the following trait bounds were not satisfied:\n             &[u8] : flatbuffers::Follow\nerror[E0277]: the trait bound &[u8]: flatbuffers::Follow<'_> is not satisfied\n    --> tests/integration_test.rs:1861:24\n     |\n1861 |         assert_eq!(tab.get::>(fi2fo(0), None), Some(&vec![109, 111, 111][..]));\n     |                        ^^^ the trait flatbuffers::Follow<'_> is not implemented for &[u8]\n     |\n     = note: required because of the requirements on the impl of flatbuffers::Follow<'_> for flatbuffers::ForwardsUOffset<&[u8]>\nerror[E0277]: the trait bound &[u8]: flatbuffers::Follow<'_> is not satisfied\n    --> tests/integration_test.rs:1882:24\n     |\n1882 |         assert_eq!(tab.get::>(fi2fo(0), Some(&vec![70, 71, 72][..])), Some(&vec![70, 71, 72][..]));\n     |                        ^^^ the trait flatbuffers::Follow<'_> is not implemented for &[u8]\n     |\n     = note: required because of the requirements on the impl of flatbuffers::Follow<'_> for flatbuffers::ForwardsUOffset<&[u8]>\nerror[E0277]: the trait bound &[u8]: flatbuffers::Follow<'_> is not satisfied\n    --> tests/integration_test.rs:1907:24\n     |\n1907 |         assert_eq!(tab.get::>(fi2fo(0), Some(&vec![70, 71, 72][..])), Some(&vec![70, 71, 72][..]));\n     |                        ^^^ the trait flatbuffers::Follow<'_> is not implemented for &[u8]\n     |\n     = note: required because of the requirements on the impl of flatbuffers::Follow<'_> for flatbuffers::ForwardsUOffset<&[u8]>\nerror: aborting due to 10 previous errors\n```\nIt appears the problem is here. The Follow trait is only implemented for slices of EndianScalars on little-endian targets, not big-endian ones. Honestly, I'm not familiar enough with the codebase to know how to implement the fix for this. Any ideas?\nFor the record, if you just comment out this line, then the test will compile and run, it will just have a number of failures (presumably because this code assumes the wrong endianness). These are the results when you execute that run:\n```\nfailures:\n    follow_impls::to_slice_of_struct_elements\n    follow_impls::to_struct\n    follow_impls::to_vector_of_struct_elements\n    roundtrip_table::table_of_vectors_of_scalars::fuzz_f32\n    roundtrip_table::table_of_vectors_of_scalars::fuzz_f64\n    roundtrip_table::table_of_vectors_of_scalars::fuzz_i16\n    roundtrip_table::table_of_vectors_of_scalars::fuzz_i32\n    roundtrip_table::table_of_vectors_of_scalars::fuzz_i64\n    roundtrip_table::table_of_vectors_of_scalars::fuzz_u16\n    roundtrip_table::table_of_vectors_of_scalars::fuzz_u32\n    roundtrip_table::table_of_vectors_of_scalars::fuzz_u64\n    roundtrip_vectors::scalar::fuzz_f32\n    roundtrip_vectors::scalar::fuzz_f64\n    roundtrip_vectors::scalar::fuzz_i16\n    roundtrip_vectors::scalar::fuzz_i32\n    roundtrip_vectors::scalar::fuzz_i64\n    roundtrip_vectors::scalar::fuzz_u16\n    roundtrip_vectors::scalar::fuzz_u32\n    roundtrip_vectors::scalar::fuzz_u64\ntest result: FAILED. 162 passed; 19 failed; 0 ignored; 0 measured; 0 filtered out\n``. So, I could be completely off base here since I'm still trying to understand the internals of this library, but let's assume that we comment out [this line](https://github.com/rw/flatbuffers/blob/9000ee364e2280204ceb36c193158693dc4ab131/rust/flatbuffers/src/vector.rs#L119) so that the test runs. In that situation,follow_slice_helperis being used for both big- and little-endian targets. It looks like [this line](https://github.com/rw/flatbuffers/blob/9000ee364e2280204ceb36c193158693dc4ab131/rust/flatbuffers/src/vector.rs#L111) is reading the size of the inputbuf, and theEndianScalar::read_scalar()function performs proper endianness conversion. This would suggest that the inputbuftofollow_slice_helper` is stored in little-endian format, regardless of the target's endianness. So, the following line would seem to be a problem, since it implicitly assumes the rest of the data in the buffer is stored in little-endian format (I think?). If that's the case, then it seems to me that we need to implement the proper byte-swapping on the elements in this array.\nAm I on the wrong track there? If so, I can start reading through the library a bit more, but I'm not sure if I'm going to have time to do that in the near future.. Alright, I'm still working on reading through the library, but I decided to take a look at one of the test failures, and found something interesting. The first test that fails is follow_impls::to_slice_of_struct_elements, which can be found here. This is the failure signature:\n---- follow_impls::to_slice_of_struct_elements stdout ----\nthread 'follow_impls::to_slice_of_struct_elements' panicked at 'assertion failed: `(left == right)`\n  left: `[FooStruct { a: 1, b: 2, c: 772 }]`,\n right: `[FooStruct { a: 1, b: 2, c: 1027 }]`', tests/integration_test.rs:1750:9\nNow, the test takes a vector of bytes ([1, 2, 3, 4]) and stores them in the following struct:\nrust\nstruct FooStruct {\n    a: i8,\n    b: u8,\n    c: i16,\n}\nOn a little-endian machine, [3, 4] will cast into an i16 as (4 << 8) + 3 = 1027 (the lowest address in memory is the least-significant byte). On a big-endian machine, [3, 4] will cast into an i16 as (3 << 8) + 4 = 772 (the lowest address in memory is the most-significant byte). So it seems that the MIPS build did the right thing here, but the test's answer is hard-coded for a little-endian machine. Let me know if you agree with that analysis.\nNow, while reading some of the library's documentation (such as the \"Direct memory access\" section of this page), I realized that some features are intentionally only implemented for little-endian targets, since the wire format is always little-endian. Then I read the docstring for the Follow trait implementation on slices: \"Implement direct slice access if the host is little-endian.\" This made me think that perhaps all of the compilation errors were just tests that will only ever work on little-endian machines. So I added #[cfg(target_endian = \"little\")] tags to a minimum subset of code (which is only in the integration_test.rs file) that fixes those compilation errors, and got this test result:\n```\nfailures:\n---- follow_impls::to_slice_of_struct_elements stdout ----\nthread 'follow_impls::to_slice_of_struct_elements' panicked at 'assertion failed: (left == right)\n  left: [FooStruct { a: 1, b: 2, c: 772 }],\n right: [FooStruct { a: 1, b: 2, c: 1027 }]', tests/integration_test.rs:1755:9\n---- follow_impls::to_struct stdout ----\nthread 'follow_impls::to_struct' panicked at 'assertion failed: (left == right)\n  left: FooStruct { a: 1, b: 2, c: 772 },\n right: FooStruct { a: 1, b: 2, c: 1027 }', tests/integration_test.rs:1724:9\n---- follow_impls::to_vector_of_struct_elements stdout ----\nthread 'follow_impls::to_vector_of_struct_elements' panicked at 'assertion failed: (left == right)\n  left: FooStruct { a: 1, b: 2, c: 772 },\n right: FooStruct { a: 1, b: 2, c: 1027 }', tests/integration_test.rs:1778:9\nfailures:\n    follow_impls::to_slice_of_struct_elements\n    follow_impls::to_struct\n    follow_impls::to_vector_of_struct_elements\ntest result: FAILED. 152 passed; 3 failed; 0 ignored; 0 measured; 0 filtered out\n```\nIt appears that the only remaining test failures are due to hardcoded assumptions of only running on little-endian targets. Additionally, I was able to run the two test binaries, and they functioned properly:\n% CARGO_TARGET_MIPS_UNKNOWN_LINUX_GNU_LINKER=mips-linux-gnu-gcc CARGO_TARGET_MIPS_UNKNOWN_LINUX_GNU_RUNNER=\"qemu-mips -L /usr/mips-linux-gnu\" cargo run --bin monster_example --target mips-unknown-linux-gnu\n    Finished dev [unoptimized + debuginfo] target(s) in 0.03s\n     Running `qemu-mips -L /usr/mips-linux-gnu target/mips-unknown-linux-gnu/debug/monster_example`\n80\n150\n\"MyMonster\"\n% CARGO_TARGET_MIPS_UNKNOWN_LINUX_GNU_LINKER=mips-linux-gnu-gcc CARGO_TARGET_MIPS_UNKNOWN_LINUX_GNU_RUNNER=\"qemu-mips -L /usr/mips-linux-gnu\" cargo run --bin alloc_check --target mips-unknown-linux-gnu\n    Finished dev [unoptimized + debuginfo] target(s) in 0.03s\n     Running `qemu-mips -L /usr/mips-linux-gnu target/mips-unknown-linux-gnu/debug/alloc_check`\nRust: Heap alloc checks completed successfully\nNow, I don't know how exhaustively these tests/binaries exercise the features that end users will use, but if an end user will never try to follow a slice on a big-endian machine (and Rust code will never be generated to follow a slice), then maybe there's no missing implementations for big-endian targets, and the only problems here are test-related.\nIn case it's useful, here's the diff you can use to get all the tests passing for the MIPS build:\n```diff\ndiff --git a/tests/rust_usage_test/tests/integration_test.rs b/tests/rust_usage_test/tests/integration_test.rs\nindex b9e44d85..8252ddeb 100644\n--- a/tests/rust_usage_test/tests/integration_test.rs\n+++ b/tests/rust_usage_test/tests/integration_test.rs\n@@ -774,6 +774,7 @@ mod roundtrip_byteswap {\n mod roundtrip_vectors {\n #[cfg(test)]\n\n\n\n[cfg(target_endian = \"little\")]\nmod scalar {\n     extern crate quickcheck;\n     extern crate flatbuffers;\n@@ -1116,6 +1117,7 @@ mod roundtrip_table {\n }\n#[test]\n+    #[cfg(target_endian = \"little\")]\n fn table_of_byte_strings_fuzz() {\n     fn prop(vec: Vec>) {\n         use flatbuffers::field_index_to_field_offset as fi2fo;\n@@ -1181,6 +1183,7 @@ mod roundtrip_table {\n     quickcheck::QuickCheck::new().max_tests(n).quickcheck(prop as fn(Vec));\n }\n\n\n[cfg(target_endian = \"little\")]\n mod table_of_vectors_of_scalars {\n     extern crate flatbuffers;\n     extern crate quickcheck;\n\n@@ -1660,6 +1663,7 @@ mod follow_impls {\n     }\n#[test]\n+    #[cfg(target_endian = \"little\")]\n fn to_byte_slice() {\n     let vec: Vec = vec![255, 255, 255, 255, 4, 0, 0, 0, 1, 2, 3, 4];\n     let off: flatbuffers::FollowStart<&[u8]> = flatbuffers::FollowStart::new();\n@@ -1674,6 +1678,7 @@ mod follow_impls {\n }\n#[test]\n+    #[cfg(target_endian = \"little\")]\n fn to_byte_string_zero_teriminated() {\n     let vec: Vec = vec![255, 255, 255, 255, 3, 0, 0, 0, 1, 2, 3, 0];\n     let off: flatbuffers::FollowStart<&[u8]> = flatbuffers::FollowStart::new();\n@@ -1716,7 +1721,14 @@ mod follow_impls {\n let vec: Vec<u8> = vec![255, 255, 255, 255, 1, 2, 3, 4];\n let off: flatbuffers::FollowStart<&FooStruct> = flatbuffers::FollowStart::new();\n\n\nassert_eq!(*off.self_follow(&vec[..], 4), FooStruct{a: 1, b: 2, c: 1027});\n\n[cfg(target_endian = \"little\")]\n\n{\nassert_eq!(*off.self_follow(&vec[..], 4), FooStruct{a: 1, b: 2, c: 1027});\n}\n\n[cfg(not(target_endian = \"little\"))]\n\n{\nassert_eq!(*off.self_follow(&vec[..], 4), FooStruct{a: 1, b: 2, c: 772});\n}\n }\n\n#[test]\n@@ -1747,7 +1759,14 @@ mod follow_impls {\n let buf: Vec<u8> = vec![1, 0, 0, 0, /* struct data */ 1, 2, 3, 4];\n let fs: flatbuffers::FollowStart<flatbuffers::Vector<FooStruct>> = flatbuffers::FollowStart::new();\n\n\nassert_eq!(fs.self_follow(&buf[..], 0).safe_slice(), &vec![FooStruct{a: 1, b: 2, c: 1027}][..]);\n\n[cfg(target_endian = \"little\")]\n\n{\nassert_eq!(fs.self_follow(&buf[..], 0).safe_slice(), &vec![FooStruct{a: 1, b: 2, c: 1027}][..]);\n}\n\n[cfg(not(target_endian = \"little\"))]\n\n{\nassert_eq!(fs.self_follow(&buf[..], 0).safe_slice(), &vec![FooStruct{a: 1, b: 2, c: 772}][..]);\n}\n }\n\n#[test]\n@@ -1770,7 +1789,14 @@ mod follow_impls {\n     let buf: Vec = vec![1, 0, 0, 0, / struct data / 1, 2, 3, 4];\n     let fs: flatbuffers::FollowStart> = flatbuffers::FollowStart::new();\n     assert_eq!(fs.self_follow(&buf[..], 0).len(), 1);\n-        assert_eq!(fs.self_follow(&buf[..], 0).get(0), &FooStruct{a: 1, b: 2, c: 1027});\n+        #[cfg(target_endian = \"little\")]\n+        {\n+            assert_eq!(fs.self_follow(&buf[..], 0).get(0), &FooStruct{a: 1, b: 2, c: 1027});\n+        }\n+        #[cfg(not(target_endian = \"little\"))]\n+        {\n+            assert_eq!(fs.self_follow(&buf[..], 0).get(0), &FooStruct{a: 1, b: 2, c: 772});\n+        }\n }\n#[test]\n@@ -1858,7 +1884,10 @@ mod follow_impls {\n     ];\n     let tab = >::follow(&buf[..], 0);\n     assert_eq!(tab.get::>(fi2fo(0), None), Some(\"moo\"));\n-        assert_eq!(tab.get::>(fi2fo(0), None), Some(&vec![109, 111, 111][..]));\n+        #[cfg(target_endian = \"little\")]\n+        {\n+            assert_eq!(tab.get::>(fi2fo(0), None), Some(&vec![109, 111, 111][..]));\n+        }\n     let v = tab.get::>>(fi2fo(0), None).unwrap();\n     assert_eq!(v.len(), 3);\n     assert_eq!(v.get(0), 109);\n@@ -1879,7 +1908,10 @@ mod follow_impls {\n     ];\n     let tab = >::follow(&buf[..], 0);\n     assert_eq!(tab.get::>(fi2fo(0), Some(\"abc\")), Some(\"abc\"));\n-        assert_eq!(tab.get::>(fi2fo(0), Some(&vec![70, 71, 72][..])), Some(&vec![70, 71, 72][..]));\n+        #[cfg(target_endian = \"little\")]\n+        {\n+            assert_eq!(tab.get::>(fi2fo(0), Some(&vec![70, 71, 72][..])), Some(&vec![70, 71, 72][..]));\n+        }\n let default_vec_buf: Vec<u8> = vec![3, 0, 0, 0, 70, 71, 72, 0];\n let default_vec = flatbuffers::Vector::new(&default_vec_buf[..], 0);\n\n@@ -1904,7 +1936,10 @@ mod follow_impls {\n     ];\n     let tab = >::follow(&buf[..], 0);\n     assert_eq!(tab.get::>(fi2fo(0), Some(\"abc\")), Some(\"abc\"));\n-        assert_eq!(tab.get::>(fi2fo(0), Some(&vec![70, 71, 72][..])), Some(&vec![70, 71, 72][..]));\n+        #[cfg(target_endian = \"little\")]\n+        {\n+            assert_eq!(tab.get::>(fi2fo(0), Some(&vec![70, 71, 72][..])), Some(&vec![70, 71, 72][..]));\n+        }\n let default_vec_buf: Vec<u8> = vec![3, 0, 0, 0, 70, 71, 72, 0];\n let default_vec = flatbuffers::Vector::new(&default_vec_buf[..], 0);\n\n``. Sorry for the long absence. I understand that making these errors only run on little-endian targets isn't a good solution. However, we have a limited number of options. The direct-slice-access methods will not function at all on big endian targets, and that's by design. We can however change the tests to instead useflatbuffers::Vector`s in place of slices. A question on that though: would we want to make the little endian targets to still run tests using the direct-slice-access methods, or should we just have both big and little endian targets run tests using vectors?\n\n\nThe following diff shows a method for fixing all the broken tests by making all targets use flatbuffers::Vectors, but this means that there isn't a lot of testing of the direct-slice-access methods. Also notice that the two tests follow_impls::to_root_to_table_get_slot_string_multiple_types_default_via_vtable_len and follow_impls::to_root_to_table_get_slot_string_multiple_types_default_via_vtable_zero (these are the last two changes in the following diff) have their direct-slice-access methods maintained (and only running on little endian targets). I did this because both tests already test the get() method using a flatbuffers::Vector right after they test the direct-slice-access get() method. Let me know if you think this is the wrong solution.\n```diff\ndiff --git a/tests/rust_usage_test/tests/integration_test.rs b/tests/rust_usage_test/tests/integration_test.rs\nindex b9e44d85..66d8602c 100644\n--- a/tests/rust_usage_test/tests/integration_test.rs\n+++ b/tests/rust_usage_test/tests/integration_test.rs\n@@ -780,7 +780,13 @@ mod roundtrip_vectors {\n     const N: u64 = 20;\n\n\nfn prop(xs: Vec) {\nfn prop(xs: Vec)\nwhere\nT: for<'a> flatbuffers::Follow<'a, Inner = T>\n\n\nflatbuffers::EndianScalar\n\n\n\n\nflatbuffers::Push\n\n\n\n\n::std::fmt::Debug,\n\n\n\n{\n             use flatbuffers::Follow;\n     let mut b = flatbuffers::FlatBufferBuilder::new();\n\n@@ -793,8 +799,12 @@ mod roundtrip_vectors {\n     let buf = b.finished_data();\n\n\n\nlet got = >::follow(buf, 0);\n\nassert_eq!(got, &xs[..]);\nlet got = >>::follow(&buf[..], 0);\nlet mut result_vec: Vec = Vec::with_capacity(got.len());\nfor i in 0..got.len() {\nresult_vec.push(got.get(i));\n}\n\nassert_eq!(result_vec, xs);\n         }\n #[test]\n\n@@ -1139,8 +1149,10 @@ mod roundtrip_table {\n         let tab = >::follow(buf, 0);\n     for i in 0..xs.len() {\n\n\nlet v = tab.get::>(fi2fo(i as flatbuffers::VOffsetT), None);\nassert_eq!(v, Some(&xs[i][..]));\nlet v = tab.get::>>(fi2fo(i as flatbuffers::VOffsetT), None);\nassert!(v.is_some());\nlet v2 = v.unwrap().safe_slice();\n\nassert_eq!(v2, &xs[i][..]);\n         }\n     }\n     prop(vec![vec![1,2,3]]);\n@@ -1187,7 +1199,13 @@ mod roundtrip_table {\nconst N: u64 = 20;\n\n\n\n\nfn prop<'a, T: flatbuffers::Follow<'a> + 'a + flatbuffers::EndianScalar + flatbuffers::Push + ::std::fmt::Debug>(vecs: Vec>) {\n\nfn prop(vecs: Vec>)\nwhere\nT: for<'a> flatbuffers::Follow<'a, Inner = T>\n\n\nflatbuffers::EndianScalar\n\n\n\n\nflatbuffers::Push\n\n\n\n\n::std::fmt::Debug,\n\n\n{\n             use flatbuffers::field_index_to_field_offset as fi2fo;\n             use flatbuffers::Follow;\n\n@@ -1218,10 +1236,14 @@ mod roundtrip_table {\n             let tab = >::follow(buf, 0);\n         for i in 0..vecs.len() {\n\n\nlet got = tab.get::>(fi2fo(i as flatbuffers::VOffsetT), None);\nlet got = tab.get::>>(fi2fo(i as flatbuffers::VOffsetT), None);\n                 assert!(got.is_some());\n                 let got2 = got.unwrap();\nassert_eq!(&vecs[i][..], got2);\nlet mut got3: Vec = Vec::with_capacity(got2.len());\nfor i in 0..got2.len() {\ngot3.push(got2.get(i));\n}\nassert_eq!(vecs[i], got3);\n             }\n         }\n\n@@ -1662,8 +1684,8 @@ mod follow_impls {\n     #[test]\n     fn to_byte_slice() {\n         let vec: Vec = vec![255, 255, 255, 255, 4, 0, 0, 0, 1, 2, 3, 4];\n-        let off: flatbuffers::FollowStart<&[u8]> = flatbuffers::FollowStart::new();\n-        assert_eq!(off.self_follow(&vec[..], 4), &[1, 2, 3, 4][..]);\n+        let off: flatbuffers::FollowStart> = flatbuffers::FollowStart::new();\n+        assert_eq!(off.self_follow(&vec[..], 4).safe_slice(), &[1, 2, 3, 4][..]);\n     }\n #[test]\n\n@@ -1676,8 +1698,8 @@ mod follow_impls {\n     #[test]\n     fn to_byte_string_zero_teriminated() {\n         let vec: Vec = vec![255, 255, 255, 255, 3, 0, 0, 0, 1, 2, 3, 0];\n-        let off: flatbuffers::FollowStart<&[u8]> = flatbuffers::FollowStart::new();\n-        assert_eq!(off.self_follow(&vec[..], 4), &[1, 2, 3][..]);\n+        let off: flatbuffers::FollowStart> = flatbuffers::FollowStart::new();\n+        assert_eq!(off.self_follow(&vec[..], 4).safe_slice(), &[1, 2, 3][..]);\n     }\n #[cfg(target_endian = \"little\")]\n\n@@ -1716,7 +1738,14 @@ mod follow_impls {\n     let vec: Vec<u8> = vec![255, 255, 255, 255, 1, 2, 3, 4];\n     let off: flatbuffers::FollowStart<&FooStruct> = flatbuffers::FollowStart::new();\n\n\nassert_eq!(*off.self_follow(&vec[..], 4), FooStruct{a: 1, b: 2, c: 1027});\n\n[cfg(target_endian = \"little\")]\n\n{\nassert_eq!(*off.self_follow(&vec[..], 4), FooStruct{a: 1, b: 2, c: 1027});\n}\n\n[cfg(not(target_endian = \"little\"))]\n\n{\nassert_eq!(*off.self_follow(&vec[..], 4), FooStruct{a: 1, b: 2, c: 772});\n\n}\n     }\n#[test]\n@@ -1747,7 +1776,14 @@ mod follow_impls {\n let buf: Vec<u8> = vec![1, 0, 0, 0, /* struct data */ 1, 2, 3, 4];\n let fs: flatbuffers::FollowStart<flatbuffers::Vector<FooStruct>> = flatbuffers::FollowStart::new();\n\n\nassert_eq!(fs.self_follow(&buf[..], 0).safe_slice(), &vec![FooStruct{a: 1, b: 2, c: 1027}][..]);\n\n[cfg(target_endian = \"little\")]\n\n{\nassert_eq!(fs.self_follow(&buf[..], 0).safe_slice(), &vec![FooStruct{a: 1, b: 2, c: 1027}][..]);\n}\n\n[cfg(not(target_endian = \"little\"))]\n\n{\nassert_eq!(fs.self_follow(&buf[..], 0).safe_slice(), &vec![FooStruct{a: 1, b: 2, c: 772}][..]);\n}\n }\n\n#[test]\n@@ -1770,7 +1806,14 @@ mod follow_impls {\n     let buf: Vec = vec![1, 0, 0, 0, / struct data / 1, 2, 3, 4];\n     let fs: flatbuffers::FollowStart> = flatbuffers::FollowStart::new();\n     assert_eq!(fs.self_follow(&buf[..], 0).len(), 1);\n-        assert_eq!(fs.self_follow(&buf[..], 0).get(0), &FooStruct{a: 1, b: 2, c: 1027});\n+        #[cfg(target_endian = \"little\")]\n+        {\n+            assert_eq!(fs.self_follow(&buf[..], 0).get(0), &FooStruct{a: 1, b: 2, c: 1027});\n+        }\n+        #[cfg(not(target_endian = \"little\"))]\n+        {\n+            assert_eq!(fs.self_follow(&buf[..], 0).get(0), &FooStruct{a: 1, b: 2, c: 772});\n+        }\n }\n#[test]\n@@ -1858,7 +1901,8 @@ mod follow_impls {\n     ];\n     let tab = >::follow(&buf[..], 0);\n     assert_eq!(tab.get::>(fi2fo(0), None), Some(\"moo\"));\n-        assert_eq!(tab.get::>(fi2fo(0), None), Some(&vec![109, 111, 111][..]));\n+        let byte_vec = tab.get::>>(fi2fo(0), None).unwrap().safe_slice();\n+        assert_eq!(byte_vec, &vec![109, 111, 111][..]);\n     let v = tab.get::>>(fi2fo(0), None).unwrap();\n     assert_eq!(v.len(), 3);\n     assert_eq!(v.get(0), 109);\n@@ -1879,7 +1923,10 @@ mod follow_impls {\n     ];\n     let tab = >::follow(&buf[..], 0);\n     assert_eq!(tab.get::>(fi2fo(0), Some(\"abc\")), Some(\"abc\"));\n-        assert_eq!(tab.get::>(fi2fo(0), Some(&vec![70, 71, 72][..])), Some(&vec![70, 71, 72][..]));\n+        #[cfg(target_endian = \"little\")]\n+        {\n+            assert_eq!(tab.get::>(fi2fo(0), Some(&vec![70, 71, 72][..])), Some(&vec![70, 71, 72][..]));\n+        }\n let default_vec_buf: Vec<u8> = vec![3, 0, 0, 0, 70, 71, 72, 0];\n let default_vec = flatbuffers::Vector::new(&default_vec_buf[..], 0);\n\n@@ -1904,7 +1951,10 @@ mod follow_impls {\n     ];\n     let tab = >::follow(&buf[..], 0);\n     assert_eq!(tab.get::>(fi2fo(0), Some(\"abc\")), Some(\"abc\"));\n-        assert_eq!(tab.get::>(fi2fo(0), Some(&vec![70, 71, 72][..])), Some(&vec![70, 71, 72][..]));\n+        #[cfg(target_endian = \"little\")]\n+        {\n+           assert_eq!(tab.get::>(fi2fo(0), Some(&vec![70, 71, 72][..])), Some(&vec![70, 71, 72][..]));\n+        }\n let default_vec_buf: Vec<u8> = vec![3, 0, 0, 0, 70, 71, 72, 0];\n let default_vec = flatbuffers::Vector::new(&default_vec_buf[..], 0);\n\n```. This change should resolve warnings that come up during the big-endian build:\n\n\n```diff\ndiff --git a/rust/flatbuffers/src/vector.rs b/rust/flatbuffers/src/vector.rs\nindex 64815b2e..397089a6 100644\n--- a/rust/flatbuffers/src/vector.rs\n+++ b/rust/flatbuffers/src/vector.rs\n@@ -19,7 +19,9 @@ use std::mem::size_of;\n use std::slice::from_raw_parts;\n use std::str::from_utf8_unchecked;\n-use endian_scalar::{EndianScalar, read_scalar};\n+#[cfg(target_endian = \"little\")]\n+use endian_scalar::EndianScalar;\n+use endian_scalar::read_scalar;\n use follow::Follow;\n use primitives::*;\n@@ -105,6 +107,7 @@ impl<'a> Follow<'a> for &'a str {\n     }\n }\n+#[cfg(target_endian = \"little\")]\n fn follow_slice_helper(buf: &[u8], loc: usize) -> &[T] {\n     let sz = size_of::();\n     debug_assert!(sz > 0);\n```\nAlso, these changes should unify the x86 and MIPS Rust test suites:\n```diff\ndiff --git a/tests/RustTest.sh b/tests/RustTest.sh\nindex ac050c42..0a3974b9 100755\n--- a/tests/RustTest.sh\n+++ b/tests/RustTest.sh\n@@ -15,8 +15,14 @@ set -e\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+if [[ \"$1\" == \"mips-unknown-linux-gnu\" ]]; then\n+    TARGET_FLAG=\"--target mips-unknown-linux-gnu\"\n+    export CARGO_TARGET_MIPS_UNKNOWN_LINUX_GNU_LINKER=mips-linux-gnu-gcc\n+    export CARGO_TARGET_MIPS_UNKNOWN_LINUX_GNU_RUNNER=\"qemu-mips -L /usr/mips-linux-gnu\"\n+fi\n+\n cd ./rust_usage_test\n-cargo test -- --quiet\n+cargo test $TARGET_FLAG -- --quiet\n TEST_RESULT=$?\n if [[ $TEST_RESULT  == 0 ]]; then\n     echo \"OK: Rust tests passed.\"\n@@ -25,7 +31,7 @@ else\n     exit 1\n fi\n-cargo run --bin=alloc_check\n+cargo run $TARGET_FLAG --bin=alloc_check\n TEST_RESULT=$?\n if [[ $TEST_RESULT  == 0 ]]; then\n     echo \"OK: Rust heap alloc test passed.\"\n@@ -34,4 +40,4 @@ else\n     exit 1\n fi\n-cargo bench\n+cargo bench $TARGET_FLAG\ndiff --git a/tests/docker/languages/Dockerfile.testing.rust.big_endian.1_30_1 b/tests/docker/languages/Dockerfile.testing.rust.big_endian.1_30_1\nindex d467cc31..f2e93f4e 100644\n--- a/tests/docker/languages/Dockerfile.testing.rust.big_endian.1_30_1\n+++ b/tests/docker/languages/Dockerfile.testing.rust.big_endian.1_30_1\n@@ -1,16 +1,15 @@\n FROM rust:1.30.1-slim-stretch as base\n-\n-WORKDIR /setup\n-RUN apt -qq update >/dev/null\n-RUN apt -qq install -y curl gcc-mips-linux-gnu qemu-user\n-RUN curl https://sh.rustup.rs > rustup.sh\n-RUN cat rustup.sh | sh -s -- -y --default-toolchain none\n+RUN apt -qq update -y && apt -qq install -y \\\n+    gcc-mips-linux-gnu \\\n+    libexpat1 \\\n+    libmagic1 \\\n+    libmpdec2 \\\n+    libreadline7 \\\n+    qemu-user\n RUN rustup target add mips-unknown-linux-gnu\n-\n-\n WORKDIR /code\n ADD . .\n-WORKDIR /code/tests/rust_usage_test\n-ENV CARGO_TARGET_MIPS_UNKNOWN_LINUX_GNU_LINKER=mips-linux-gnu-gcc\n-ENV CARGO_TARGET_MIPS_UNKNOWN_LINUX_GNU_RUNNER=\"qemu-mips -L /usr/mips-linux-gnu\"\n-RUN cargo test --target mips-unknown-linux-gnu\n+RUN cp flatc_debian_stretch flatc\n+WORKDIR /code/tests\n+RUN rustc --version\n+RUN ./RustTest.sh mips-unknown-linux-gnu\n```\nNotice that I added four new packages to the Docker setup for the MIPS test: libexpat1, libmagic1, libmpdec2, and libreadline7. For some strange reason, when running tests using this Docker setup (without these 4 additional packages), even the simplest Rust MIPS binaries would fail with a segfault. So, I installed gdb-multiarch to debug the segfault, I reran the Rust MIPS binary, and it worked (no more segfault). So I pared down the packages that gdb-multiarch installs, and found that these 4 packages were the minimum necessary subset to get Rust MIPS binaries to run in this container.\nIf you'd like to try and recreate the segfault, then try out the following:\n\nRemove those 4 extra packages from tests/docker/languages/Dockerfile.testing.rust.big_endian.1_30_1\nRun the following commands from the root directory of this repo:\ndocker build -t build_flatc_debian_stretch -f tests/docker/Dockerfile.testing.build_flatc_debian_stretch .\nBUILD_CONTAINER_ID=$(docker create --read-only build_flatc_debian_stretch)\ndocker cp ${BUILD_CONTAINER_ID}:/code/flatc flatc_debian_stretch\ndocker build -t rust.big_endian.1_30_1 -f tests/docker/languages/Dockerfile.testing.rust.big_endian.1_30_1 .. Sure thing @rw! Just to confirm, you want me to open a PR to merge into your 2018-12--rust-big-endian branch, right?. This can be closed now that #5229 is merged.. This would be a great change! In this commit that was just merged, I had to add some code in a few spots to tests/rust_usage_test/tests/integration_test.rs that looks like this:\n\nlet mut rust_vec: Vec<T> = Vec::with_capacity(flatbuffer_vector.len());\nfor i in flatbuffer_vector.len() {\n    rust_vec.push(flatbuffer_vector.get(i));\n}\nThose might be good candidates to replace with code that tests out this new functionality. Not exactly sure, but maybe something like this:\nlet rust_vec = flatbuffer_vector.iter().collect::Vec<T>();. Great, thanks!\nHonestly, nothing has stuck out to me as clear ways to clean up the traits involved. At the very least, it's nice that all of the traits are completely hidden from library users. And really the only reason that we have to deal with all of these complex traits is that we're circumventing the API of the generated code for a lot of these tests. However, I'll keep this on my mind, and I'll file an issue if I can think of any ways to simplify the trait system.. So, while the Debian package name for the MIPS GCC toolchain is gcc-mips-linux-gnu, the actual executable that gets installed is called mips-linux-gnu-gcc, and it should be located at /usr/bin/mips-linux-gnu-gcc so it'll be in your path.\nsuggestion\nENV CARGO_TARGET_MIPS_UNKNOWN_LINUX_GNU_LINKER=mips-linux-gnu-gcc. Need to install the qemu-user package so we can run MIPS binaries.\nsuggestion\nRUN apt -qq install -y curl gcc-mips-linux-gnu qemu-user. Need to tell cargo how to run the MIPS binaries.\nsuggestion\nENV CARGO_TARGET_MIPS_UNKNOWN_LINUX_GNU_RUNNER=\"qemu-mips -L /usr/mips-linux-gnu\"\nRUN cargo test --target mips-unknown-linux-gnu. This is just an endianness issue. This test (as well as follow_impls::to_slice_of_struct_elements and follow_impls::to_vector_of_struct_elements) just manually creates a FooStruct from a vector of bytes, and the FooStruct.c field has a different representation on different platforms since it's a two-byte field. On a little-endian machine, [3, 4] will cast into an i16 as (4 << 8) + 3 = 1027 (the lowest address in memory being the least-significant byte). On a big-endian machine, [3, 4] will cast into an i16 as (3 << 8) + 4 = 772 (the lowest address in memory being the most-significant byte).. So, are you thinking that we should write a Push implementation for FooStruct in these 3 tests (follow_impls::to_struct, follow_impls::to_slice_of_struct_elements, and follow_impls::to_vector_of_struct_elements), and then construct this as a FlatBuffers struct?. Sounds great, see if you think 6c1d51f resolves this.. A comment like that sounds like a good idea. Also, any reason not to make FooStruct common to the whole follow_impls module, since we now have 3 identical implementations?. See 828db84 for possible resolution.. I was thinking about that, but don't the assertions on the next few lines accomplish the same thing?. I was thinking about that, but don't the assertions on the next few lines accomplish the same thing?. ",
    "jean-airoldie": "@rw I'm afraid I'm quite busy at the moment and I can't really commit to anything too time consuming.. @rw sure thing, i'll have a PR ready by the end of the day.. Here it is.. @rw in your Rust tests, you expect at this line that the generated function named test_as_monster() for the namespace MyGame.Example2 be named test_as_my_game___example_2___monster(). Don't we want instead the generated function to be called test_as_my_game__example_2__monster() (namely 2 '_' instead of 3)? I understand using substituting . into __ to avoid generated namespace collision, but the namespaces foo__bar and foo.bar would collide anyway.. @rw I looked at the cpp generated code and they translate the namespace foo.bar into a nested namespace foo::bar which removes any possibility of collisions.. @rw I'm talking about this cpp code generated from this schema in the appveyor build when testing. The same behavior occurs for code directly generated by flatc --cpp. For instance this schema file would generate this code. We can note that nested namespaces are used in both cases when the schema namespace contains a . character.. @rw If we decide to use two underscores instead of nested namespaces, we should probably create a issue documenting the potential namespace collision issue (albeit minor) of doing so.. @rw \n1. Specifically cpp generates MyGame::Example2::Monster *test_as_MyGame_Example2_Monster().\nBy using a nested namespace, namely MyGame::Example2:: there is no need to replace\na . by two _ since there is no possibility for collision.\nSo the rust equivalent would be my_game::example_2::test_as_my_game_example_2_monster()\nas opposed to my_game::example_2::test_as_my_game___example_2___monster().\n2. I'm sorry I tought I did. I edited my diffs.\n3. That seems reasonable. I'm gonna take a stab at that.\n4. My previous point about potential namespace collision was misinformed, I looked back at the code.. @rw Superceded by #4936. I also noticed that is currently no unit test case for a module name generated by a schema with a namespace contains at least one underscore. I would add that case but that would involve modifying this file since it does not contain such a namespace.. @rw Yes this does fix #4932.. @rw Done.. 1. Exactly.\n2. (Sorry, I am on my phone so I have no code at the moment) I am receiving &[u8] messages which I know are buffers containing individual Value objects. I want to be able to temporarily store these buffers then dump them to a file when I have enough. Currently I am de-serializing every Value buffers I receive into a struct that I push to into vector. Then I create my Values flatbuffer object by iterating over the vector. Is there a more efficient way to do so?. @aardappel Excellent! Thank you.. @rw Sure I'll look into it tomorrow.. @rw What do you think of simply adding the required imports to the global namespace right here? This will lead to unused imports thought. I don't think it can be done in SetNameSpace as you suggested because it might import the same thing multiple times in the global namespace (not unless we add another parameter).. @rw I was giving the same lifetime to the FlatBufferBuilder and to its mutable reference. This solves it.\nrust\n    pub fn to_bytes_delegated<'a, 'b>(\n        &'b self,\n        builder: &'a mut FlatBufferBuilder<'b>\n    ) -> &'a [u8] {\n        // ...\n    }\nThanks for the quick response nonetheless!. @rw Actually, I just though about this further and my solution doesn't work. Furthermore it seems to reveal a significant problem when reusing a FlatBufferBuilder like an arena. I simplified my example and added additional comments to illustrate the problem. But here is the general concept.\nThe current problem is that the lifetime of WIPOffset<_> returned from FlatBufferBuilder::create_vector() must be bound to the lifetime of the builder. This is a problem when your application reuses the same builder by periodically calling reset on it because its lifetime will necessarily outlast those of objects it serializes. The only current viable way (that i could think of) would be to do the entire serialization in one function so that no WIPOffset<_> escapes the anonymous lifetime. Otherwise, any function calling this this serialization function would need the lifetime of its parameters (that will be serialized) to be bound to the builder's lifetime, which effectively renders it unusable. But doing it this way prevents all forms of code reuse and forces to needlessly clone bytes within the builder to prevent returning a byte slice bound to the builder.\nConceptually I think the WIPOffset / slice to bytes within the builder should be bound to something other than the builder and that calling FlatBufferBuilder::reset() (or the builder going out of scope) invalidates the references. This way we could bind the lifetime of a function that does the serialization to the lifetime of the bytes in the builder.. @rw I reread my explanation and I agree I could be clearer and more concise.\nBasically the WIPOffsets that escape the builder are bound to the lifetime of the builder. This works fine when we create a builder each time we intend to serialize something because the serialized bytes will be dropped from scope at the same time as the builder.\nThe problem arises when we want to reuse the builder by creating it once, keeping it in a server (some struct that lives for the entire program) and then call reset on the builder after each serialization. Such scenario is appealing in a long lived program from a performance standpoint because we don't need to create a builder (its underlying vector) each time we want to serialize something. The problem is that, in this scenario, the builder will necessarily outlive the things it serializes. Since the WIPOffsets returned from pushing a struct are bound to the lifetime of the builder, the objects we push must also have to be bound to the lifetime of the builder, which is impossible for most usage.\nMy usage would be store the builder in a server struct and call builder.reset() after each serialization. This would require that the lifetime of the WIPOffsets be bound somehow to the anonymous lifetime of the function in which the serialization is done.. @rw \n1. My use case is not about deserializing since we can simply call get_root without any builder. I don't understand how I could construct a flatbuffers representation using get_root_as.\n2. I use get_root directly since there can only be a unique root_type per schema file (from my understanding).\n3. \nrust\nfn foo(&mut self) {\n  // The builder is empty here.\n  let mut builder = self.builder;\n  {\n    // Somehow the WIPOffset must be bound to this anonymous lifetime\n    let offset = serialize_a_vector_of_things(thing, &mut builder);\n    // use the offset when constructing a more complex flatbuffer object\n    ...\n    // send the serialized bytes\n  }\n  // The WIPOffset is out of scope so we can safely call reset. \n  builder.reset()\n  // The builder is empty once again.\n}. @rw Regarding the API I was thinking of something like this:\nrust\nfn send_things(&mut self, things: Vec<Thing>) {\n  // This is a contiguous vector that we keep to reduce allocations\n  // which is currently empty but has reserved storage.\n  let mut vec = &mut self.vec;\n  {\n    // This creates a new builder that captures the anonymous lifetime\n    // and uses our vector as its storage\n    let mut builder = FlatBufferBuilder::from_vec(&mut vec);\n    let offset = serialize_a_vector_of_things(things, &mut builder);\n    // use the offset when constructing a more complex flatbuffer object\n    ...\n    // write the serialized bytes to a `Sink`\n    // builder.reset(); might be an issue if we forget so we rather have `Drop` call it\n  }\n  // The builder goes out of scope and so do its WIPOffsets\n  // We get back our vector empty with at least as much reserved storage.\n}\nOr probably cleaner:\nrust\nfn send_thing(&mut self, things: Vec<Thing>) {\n  // This is a a `FlatBufferArena` that is currently empty but has reserved storage.\n  let mut arena = &mut self.arena;\n  {\n    // This creates a `FlatBufferBuilder` that captures the anonymous lifetime\n    // and uses storage from the arena (so no allocations required).\n    let mut builder = arena.create_builder();\n    let offset = serialize_a_vector_of_things(things, &mut builder);\n    // use the offset when constructing a more complex flatbuffer object\n    ...\n    // write the serialized bytes to a `Sink`\n    // builder.reset(); might be an issue if we forget so we rather have `Drop` call it\n  }\n  // The builder goes out of scope and so do its WIPOffsets.\n  // We get back our arena which is empty.\n}. @rw I'm not sure what you mean by that. If you mean replacing &mut self by &mut FlatBufferBuilder in send_things, then we have the same problem because the builder's lifetime is still bound to the lifetime of the struct that contains it. Therefore borrowck will require that the anonymous lifetime of any function that calls send_things (to which the &mut borrow of the builder would be bound) last as long as the lifetime of the builder (and thus the struct who owns the builder), which does not make any practical sense. By definition a serialization function is meant to be used more then once, which means that the lifetime of each function will be strictly smaller than the one of the builder.\nHere is a short example:\n```rust\n// Lets say its basically the same from my previous example, albeit\n// with a different function signature.\nfn send_things(&mut builder, things: Vec) {\n  / ... /\n}\nfn gen_rand_things() -> Vec {\n  unimplemented!();\n}\nfn main() {\n  // The lifetime of the builder is bound to the lifetime of the main function.\n  let mut builder = FlatBufferBuilder::new();\n  let things = gen_rand_things();\n  // The lifetime of the borrow must be the same as the lifetime\n  // of the builder which isn't the case. Here the anonymous lifetime\n  // of the send_things function call is clearly shorter than the\n  // one of the builder.\n  send_things(&mut builder, things);\n  let more_things = gen_rand_things();\n  // Same here.\n  send_things(&mut builder, more_things);\n}\n```\nedit: fixed typo in main. @rw I made a typo, I edited my previous answer. The same situation occurs, but the builder is bound to the lifetime of the main function instead of a struct.. @rw Damn, it never occurred to me that I did not need to store the builder slice. This makes perfect sense. Thanks a lot for your time!. @rw Sure! I think some common usage examples would be interesting as well. There are a couple of concepts that required me to look at the source code to understand better.. I recommend providing your flatbuffers schema file for clarity when asking questions.\nYou can create the WIPOffset<&'a str> (which is an work in progress offset to a string) via this builder method.\nHere is an example using the generated PlasmaContainsRequest::create constructor:\n(This might not compile as I am writing if from the top of my head)\n```rust\nlet mut builder = FlatBufferBuilder::new();\nlet object_id = \"your_string\";\n// This is how we get our WIPOffset<&'a str>\nlet str_offset = builder.create_string(&object_id);\nlet args = PlasmaContainsRequestArgs {\n  Some(str_offset),\n};\n// This is a WIPOffset<&'a PlasmaContainsRequest>\nlet offset = PlasmaContainsRequest::create(&mut builder, &args);\n// This marks our WIPOffset<&'a PlasmaContainsRequest> as finished.\n// here builder.finish(offset, None); could also be used.\nbuilder.finish_minimal(offset);\n// This yields the serialized PlasmaContainsRequest.\nlet bytes = builder.finished_data();\n```\nAlternatively the generated PlasmaContainsRequestBuilder can also be used to serialize your object.. Turns out that it is the table's generated enum offset type that is documented instead of the generated struct. I will submit a PR.. Done. @rw The indentation seemed wrong as it not consistent with the rest of the file. See this from line 9 to 16.. @rw Alright.. @aardappel This would not work against a string which is already in snake case or a string containing a numeric character. But I will make it simpler.. @rw That should do it.. @rw Yes, I forgot to ask you about that.. ",
    "SF-Zhou": "OK I find that the buffer is changed when monster mutate values... Just save it.. ",
    "kzvi": "@rw I don't know what a child lifetime bound is, but I'll open a PR to show what I mean.. @rw I don't think that it is necessary to specify any relationship between the lifetime of self and the lifetime of self.buf (i.e. 'a) in the case of this function.. @rw @mmastrac's issue isn't fixed by my PR. Fixing it would require a change to the code generator.. I signed it!. Test added.. I ran make test and it said the tests passed, but that command also completed very quickly so I'm not sure if it actually did anything. I also ran cd tests && sh generate_code.sh since it said to do that in the pull request template. Is there another command that I should use to run tests?\nAlso, what do you mean by reduce the code size? Should I remove the changes to tests/monster_test.bfbs and tests/namespace_test/namespace_test2_generated.ts in the commit?. Oh, I found it... it's cd tests && ./RustTest.sh. Sorry for not running it -- I'll do that :). Ok, I made it so that those tests pass.. Fixed.. ",
    "medwards": "Sure, the hardcoding in idl_gen_cpp.cpp is easy but I'm not sure what to do about the definition of TypeTable itself in flatbuffers.h which has:\ncpp\nconst int32_t *values;  // Only set for non-consecutive enum/union or structs.. Ironically the second version I published had it all in the constructor instead but I didn't like it that way. Old compilers are killing me here XD\nI'll publish that version again before next week.. Ready to go now.. ",
    "buster3": "I signed it!. Thanks for the review and merging. First of all, yes, breaking existing code is not a good idea. Thanks for the hint with native_inline. I had a deeper look on the code and just tried to understand the intended behavior. \nNot encoded scalar types are replaced by their default values on read. For the user it is an an implementation detail that default values of scalar types (int, enum) are not encoded and thus not transmitted. I think this is a good strategy. It is a bit weird to me that on the other hand, structs or tables have a state \"not transmitted\". Why not applying the default value strategy again? Is there any reason or is it simply the way it is?\nMaybe I found a bug in the current implementation?\n```\nstruct Foo (native_type:\"MyFoo\") {\n  x:float;\n}\ntable FooBar {\n  f:Foo (native_inline);\n  ff:Foo;\n}\nresults in\nstruct FooBarT : public flatbuffers::NativeTable {\n  MyFoo f;\n  std::unique_ptr ff;\n}\ninline void FooBar::UnPackTo(FooBarT _o, const flatbuffers::resolver_function_t _resolver) const {\n  { auto _e = f(); if (_e) _o->f = flatbuffers::UnPack(_e); };\n  { auto _e = ff(); if (_e) _o->ff = flatbuffers::UnPack(_e); };\n```\nThe current struct unpack is not suitable for std::unique_ptr case. I would appreciate if there is a solution without a heap allocation of new MyFoo in unpack function.\n. It seems to me that the native_type feature is completely broken.\nI cannot define a Pack function without a fully defined type FlatbufferStruct.\nFlatbufferStruct Pack(const native_type& obj);\nIt is not possible to include the generated header in the pack and unpack functions. \nCan anybody show me an example code which uses the native_type feature?. - Ok, got it. Table values can be null.\n- You are right. If i predefine prototypes Pack works. Additionally, I have to make a forward declaration for the generated classes. All in all it looks quite messy. Example:\n```\npragma once\ninclude \n// forward declaration of generated classes\nstruct Complex;\nnamespace flatbuffers {\n    Complex Pack(const std::complex& obj);\n    std::complex UnPack(const Complex& obj);\n}\ninclude \"test_generated.h\"\nnamespace flatbuffers {\n    inline Complex Pack(const std::complex& obj)\n    {\n        return Complex(obj.real(), obj.imag());\n    }\ninline std::complex<double> UnPack(const Complex& obj)\n{\n    return std::complex<double>(obj.i(), obj.q());\n}\n\n}\n```\nIt might have been a good idea to split the object base API in a separate header.\n\nI think the following definition does not produce compilable code. std::complex<double> can not be converted to std::unique_ptr<std::complex<double>>.\n```\nstruct Complex (native_type:\"std::complex\") {\n  i:double;\n  q:double;\n}\n\ntable FooConfig {\n  native_inline:Complex (native_inline);\n  default:Complex;\n}\n```\nNative_type for tables\nI make the following suggestion:\n- Automatically native_inline all structs/tables with a native_type. Users who wants to have a std::unique_ptr<User_t> can define (native_type:\"std::unique_ptr<User_t>\"). \n- inline flatbuffers::Offset<Signal> Pack(flatbuffers::FlatBufferBuilder &_fbb, const native_type &_o, const flatbuffers::rehasher_function_t *_rehasher);\n- native_type UnPack(const FlatbufferTable& obj, const flatbuffers::resolver_function_t *_resolver = nullptr);. I do not understand which code would be broken if we default native_inline. Currently, the signature of UnPack is defined as native_type UnPack(const FlatbufferStruct& obj);. This will not work without native_inline. UnPack is not defined to return 'std::unique_ptr'.\nI would work on a native_type for tables feature, but currently I have no idea which kind of Pack/Unpack interfaces to define.. Thanks for the feedback. I will try to work on this the next weeks. This is the error:\n1>------ Build started: Project: dummy, Configuration: Debug Win32 ------\n1>Microsoft (R) C/C++ Optimizing Compiler Version 19.14.26431 for x86\n1>Copyright (C) Microsoft Corporation.  All rights reserved.\n1>\n1>cl /c /IC:\\git\\evaluation_flatbuffers\\dummy\\..\\flatbuffers\\include /Zi /W3 /WX- /diagnostics:classic /Od /Ob0 /Oy- /D WIN32 /D _WINDOWS /D \"CMAKE_INTDIR=\\\"Debug\\\"\" /D _MBCS /Gm- /EHsc /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /GR /Fo\"dummy.dir\\Debug\\\\\" /Fd\"dummy.dir\\Debug\\vc141.pdb\" /Gd /TP /analyze- /FC /errorReport:prompt C:\\git\\evaluation_flatbuffers\\dummy\\main.cpp\n1>\n1>main.cpp\n1>c:\\git\\evaluation_flatbuffers\\flatbuffers\\include\\flatbuffers\\flatbuffers.h(1510): error C2440: 'static_cast': cannot convert from 'Complex (__cdecl *)(const std::complex<double> &)' to 'Pack_t &'\n1>c:\\git\\evaluation_flatbuffers\\flatbuffers\\include\\flatbuffers\\flatbuffers.h(1510): note: static_cast and safe_cast to reference can only be used for valid initializations or for lvalue casts between related classes\n1>c:\\git\\evaluation_flatbuffers\\flatbuffers\\include\\flatbuffers\\flatbuffers.h(1579): note: see reference to function template instantiation 'flatbuffers::Offset<flatbuffers::Vector<const Complex *>> flatbuffers::FlatBufferBuilder::CreateVectorOfNativeStructs<T,S>(const S *,size_t)' being compiled\n1>        with\n1>        [\n1>            T=Complex,\n1>            S=std::complex<double>\n1>        ]\n1>c:\\git\\evaluation_flatbuffers\\dummy\\test_generated.h(345): note: see reference to function template instantiation 'flatbuffers::Offset<flatbuffers::Vector<const Complex *>> flatbuffers::FlatBufferBuilder::CreateVectorOfNativeStructs<Complex,std::complex<double>>(const std::vector<std::complex<double>,std::allocator<_Ty>> &)' being compiled\n1>        with\n1>        [\n1>            _Ty=std::complex<double>\n1>        ]\n1>c:\\git\\evaluation_flatbuffers\\flatbuffers\\include\\flatbuffers\\flatbuffers.h(1510): error C2672: 'std::transform': no matching overloaded function found\n1>c:\\git\\evaluation_flatbuffers\\flatbuffers\\include\\flatbuffers\\flatbuffers.h(1510): error C2780: '_DestTy *std::transform(const _InIt1,const _InIt1,_RightTy (&)[_RightSize],_DestTy (&)[_DestSize],_Fn)': expects 5 arguments - 3 provided\n1>c:\\program files (x86)\\microsoft visual studio\\2017\\professional\\vc\\tools\\msvc\\14.14.26428\\include\\algorithm(1501): note: see declaration of 'std::transform'\n1>c:\\git\\evaluation_flatbuffers\\flatbuffers\\include\\flatbuffers\\flatbuffers.h(1510): error C2780: '_DestTy *std::transform(const _InIt1,const _InIt1,_InIt2,_DestTy (&)[_DestSize],_Fn)': expects 5 arguments - 3 provided\n1>c:\\program files (x86)\\microsoft visual studio\\2017\\professional\\vc\\tools\\msvc\\14.14.26428\\include\\algorithm(1487): note: see declaration of 'std::transform'\n1>c:\\git\\evaluation_flatbuffers\\flatbuffers\\include\\flatbuffers\\flatbuffers.h(1510): error C2780: '_OutIt std::transform(const _InIt1,const _InIt1,_RightTy (&)[_RightSize],const _OutIt,_Fn)': expects 5 arguments - 3 provided\n1>c:\\program files (x86)\\microsoft visual studio\\2017\\professional\\vc\\tools\\msvc\\14.14.26428\\include\\algorithm(1475): note: see declaration of 'std::transform'\n1>c:\\git\\evaluation_flatbuffers\\flatbuffers\\include\\flatbuffers\\flatbuffers.h(1510): error C2780: '_OutIt std::transform(const _InIt1,const _InIt1,const _InIt2,_OutIt,_Fn)': expects 5 arguments - 3 provided\n1>c:\\program files (x86)\\microsoft visual studio\\2017\\professional\\vc\\tools\\msvc\\14.14.26428\\include\\algorithm(1449): note: see declaration of 'std::transform'\n1>c:\\git\\evaluation_flatbuffers\\flatbuffers\\include\\flatbuffers\\flatbuffers.h(1510): error C2780: '_DestTy *std::transform(const _InIt,const _InIt,_DestTy (&)[_DestSize],_Fn)': expects 4 arguments - 3 provided\n1>c:\\program files (x86)\\microsoft visual studio\\2017\\professional\\vc\\tools\\msvc\\14.14.26428\\include\\algorithm(1415): note: see declaration of 'std::transform'\n1>c:\\git\\evaluation_flatbuffers\\flatbuffers\\include\\flatbuffers\\flatbuffers.h(1510): error C2780: '_OutIt std::transform(const _InIt,const _InIt,_OutIt,_Fn)': expects 4 arguments - 3 provided\n1>c:\\program files (x86)\\microsoft visual studio\\2017\\professional\\vc\\tools\\msvc\\14.14.26428\\include\\algorithm(1395): note: see declaration of 'std::transform'\n1>Done building project \"dummy.vcxproj\" -- FAILED.. \"compile_flatbuffers_schema_to_cpp\" is used to compile the schemas for the tests. It is not used anywhere else. I have to enable --gen-compare in order to successfully compile the tests. I have written a very basic tests. See EqualOperatorTest(). Is this sufficient?. ",
    "eerhardt": "\nIs there no way to put this in a central location in C#? Or better yet, outside the source-controlled code?\n\n@aardappel - Are you referring to the comment, or the define itself?\nIn C#, you can add project-level defines in the .csproj:\n<PropertyGroup>\n <DefineConstants>$(DefineConstants);UNSAFE_BYTEBUFFER;BYTEBUFFER_NO_BOUNDS_CHECK;ENABLE_SPAN_T</DefineConstants>\n</PropertyGroup>\nThis will define those constants for all the C# files being compiled as part of that project.. > If I remember correctly having the code access a property which returned a Span to do writes was 10-15% slower\nCan you point me to the benchmarks you were using?  I've also seen a slow down when using Memory<T> vs. directly using a byte[]. These can sometimes be fixed by changing how the code is written.\n\nMaybe this performance difference is not enough to matter? I think the implementation would be cleaner without pointers.\n\nI think it is a trade-off. Pinning the byte[] for the duration of the lifetime of ByteBuffer may cause heap fragmentation if the objects are long-lived, like the documentation mentions.. (Sorry this got so long - but I was having fun today \ud83d\ude09)\n@ccifra - I wrote a little benchmark (stealing the code from the tests project)\nhttps://github.com/eerhardt/flatbuffers/tree/AddBenchmarks\nWhen I run this benchmark with the current code in master I get the following results:\n\nWith Defining UNSAFE_BYTEBUFFER and ENABLE_SPAN_T:\n\n|        Method |     Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|-------------- |---------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| BuildMonsters | 10.90 ms | 0.1864 ms | 0.1652 ms |    484.3750 |    234.3750 |           - |             1.88 MB |\n\nWith Defining just UNSAFE_BYTEBUFFER:\n\n|        Method |     Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|-------------- |---------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| BuildMonsters | 10.93 ms | 0.2114 ms | 0.2076 ms |    484.3750 |    234.3750 |           - |             1.88 MB |\n\nWith no defines at all (so not using the byte*, but instead using byte[]):\n\n|       Method |     Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|-------------- |---------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| BuildMonsters | 1.546 ms | 0.0302 ms | 0.0513 ms |    541.0156 |           - |           - |             2.17 MB |\nNotice that the first 2 are basically identical. But they are significantly slower than the no defines way. What's even more a little alarming is if you notice the Gen 1 GC operations for the first 2 scenarios. With this small of a benchmark, we shouldn't be pushing memory to Gen 1 - it typically means that something isn't getting disposed properly (and the finalizers are running).\nSo I added a call to FlatBufferBuilder.DataBuffer.Dispose() in the benchmark (note that FlatBufferBuilder isn't IDisposable itself). After making that change, I ran the first scenario again (define  UNSAFE_BYTEBUFFER and ENABLE_SPAN_T):\n|       Method |     Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|-------------- |---------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| BuildMonsters | 1.964 ms | 0.0407 ms | 0.0557 ms |    468.7500 |           - |           - |             1.88 MB |\nMuch better.  Still slower than byte[] directly, but at least it isn't 5x slower. And no Gen 1 GCs, and even less Gen 0 GCs - so we are consuming less memory.\nNow if I make my proposed change above (use Memory<byte> instead of byte*):\nhttps://github.com/eerhardt/flatbuffers/tree/UseMemory\nI run with UNSAFE_BYTEBUFFER and ENABLE_SPAN_T defined:\n|       Method |     Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|-------------- |---------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| BuildMonsters | 2.055 ms | 0.0381 ms | 0.0338 ms |    468.7500 |           - |           - |             1.88 MB |\nThis is very close (<5%) to the current version using byte* in master. But it still wasn't equivalent.\nSo then I decided to use the [EtwProfiler] feature of BenchmarkDotNet to see why my implementation was slower. (See Adam Sitnik's awesome blogs for more info.)\nDoing a little investigation there, I noticed that ~36% of the time was being spent in Monster.CreateTestnestedflatbufferVector, and half of that was in ByteBuffer.PutByte. So I looked at that method:\nhttps://github.com/google/flatbuffers/blob/980a6d66d38e3267c601dfa2d33901dfcca73feb/tests/MyGame/Example/Monster.cs#L215\nIts adding one byte at a time to the buffer. Since we are grabbing a Span every time the PutByte method is called, that is causing a performance bottleneck.  So I changed that line of code to call FlatBufferBuilder.Put<T>(T[] x) instead. And I re-ran the 3 scenarios again:\n|        Method |     Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|-------------- |---------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| master UNSAFE_BYTEBUFFER and ENABLE_SPAN_T| 1.770 ms | 0.0280 ms | 0.0262 ms |    468.7500 |           - |           - |             1.88 MB |\n|  master no defines | 1.394 ms | 0.0280 ms | 0.0468 ms |    541.0156 |           - |           - |             2.17 MB |\n| UseMemory UNSAFE_BYTEBUFFER and ENABLE_SPAN_T| 1.572 ms | 0.0306 ms | 0.0430 ms |    468.7500 |           - |           - |             1.88 MB |\nThey all got faster, but the UseMemory path is now beating the byte* path.\nLooking a bit more at the Etw trace, there are a few places where we are writing to a ByteBuffer in a loop:\nhttps://github.com/google/flatbuffers/blob/d0321df8cfa24b86c15fdca029aadc2c232029cc/net/FlatBuffers/FlatBufferBuilder.cs#L607-L612\nhttps://github.com/google/flatbuffers/blob/d0321df8cfa24b86c15fdca029aadc2c232029cc/net/FlatBuffers/FlatBufferBuilder.cs#L772-L776\nWe could try to squeeze those methods out as well, so we aren't calling PutXX in a loop, but I didn't go that far.\nSo overall, I think we could make this approach work, perf-wise. It cleans up the implementation, can be made faster than the current byte* implementation, and the callers don't need to remember to call Dispose() on the FlatBufferBuilder.DataBuffer when just using managed byte[] as the backend storage.\nNOTE: If there are better benchmarks that could be analyzed, please let me know.. Unfortunately, the scenario you illustrated is a \"worst case scenario\" for using Memory<T> and Span<T> around a byte[]. What is happening is that in a tight-loop you are calling a method that is getting a Span from a Memory. Doing this is relatively cheap, but if you do it so often, these \"relatively cheap\" time segments add up. The recommended pattern is to cache the Span whenever you are going to be doing a bunch of operations in a tight-loop.\nOne question I have (because I'm so new to the flatbuffers project) is: Is your scenario a common use case? Or would customers typically be using the generated code to build their data, like in the BuildMonsters case?\nIf it is a common use case, and we want to be able to use either managed byte[] or native memory, as the backing storage, maybe introducing a new ref struct API to build the flat buffers would be an option. Making it a ref struct will allow you to cache a Span inside of it. This is the same API pattern that is being followed in the new Utf8JsonReader/Writer APIs in corefx. It is not easy code to get right, and it doesn't make for an easy to consume API (since you are using ref struct), but it can be really fast, and it is safer than using pointers.\nYes, I imagine it is slower on .NET Framework because Span<T> isn't built into the framework. Thus a lot of perf optimizations in the runtime aren't there.\nOne note is that .NET Core 3.0 is making the UseMemory code a little faster. Running your test on my machine:\n| .NET Core Version |     Method |     Mean |    Error |   StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|--------|----------- |---------:|---------:|---------:|------------:|------------:|------------:|--------------------:|\n| 2.1 | TestTables | 122.4 ms | 1.395 ms | 1.305 ms |    800.0000 |    800.0000 |    800.0000 |               32 MB |\n| 3.0 | TestTables | 102.6 ms | 0.7190 ms | 0.6725 ms |    800.0000 |    800.0000 |    800.0000 |               32 MB |\nSo there are gains being made in this space in the runtime.\nThe other thing of note is that the UseMemory implementation on your TestTables test is roughly equivalent to not using #define UNSAFE_BYTEBUFFER on my machine. Here are the results with no #define:\n| .NET Core Version |     Method |     Mean |    Error |   StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|--------|----------- |---------:|---------:|---------:|------------:|------------:|------------:|--------------------:|\n| 2.1 | TestTables | 111.5 ms | 0.8030 ms | 0.7511 ms |    800.0000 |    800.0000 |    800.0000 |               32 MB |\n| 3.0 | TestTables | 113.8 ms | 0.8153 ms | 0.7228 ms |    800.0000 |    800.0000 |    800.0000 |               32 MB |\nSo UseMemory is a little slower on 2.1, but is a little faster on 3.0.\nSo what should we do here?\n\nContinue using byte*. Which forces callers to call .Dispose() on the ByteBuffers, or else they get even worse performance in the BuildMonsters scenario.\nAdd another #define to switch the implementation from using byte* to use Memory instead. Then users can choose which one they want.\nSwitch to using Memory, and optionally create a new ref struct API for the performance critical cases that are similar to the TestTables scenario.\n\nThoughts?. > The addByte loop may also be good to clean up.\nI'll see if I can submit a separate PR for that. I think it would be a nice clean up as well.\n\nNot sure if I'd be a fan of splitting up FlatBufferBuilder, especially if that requires API changes or different API usage to be efficient.\n\nI prototyped that approach a little today, and I wasn't impressed how the API/code was turning out, and basically gave up. I don't think I will pursue that approach anymore.\n\n\nI'm certainly for cleaning this up and giving the user options, as long as the default way stays the fastest possible.\n\nAny thoughts on my approach (2) above?\n\n2. Add another #define to switch the implementation from using byte* to use Memory instead. Then users can choose which one they want.\n\nThis approach doesn't change any user API or code gen. It does add a couple more #ifs to the ByteBuffer class, but that's about it. I've been leaning this way more and more, as I think it gives the most flexibility.. Returning a MemoryHandle is not going to meet my needs. I need to get a Memory<T> out of a ByteBuffer in order allow zero-copy reads in the C# Apache Arrow implementation.\nSee my current changes here: https://github.com/eerhardt/arrow/commit/bb98085f86a00fee8c9e4fd91d96642966be16fb#diff-ac9f7f90c2e6b8d576ae7d5051cd3839R109\nYou can still accomplish what you are talking about above (\"Reading and writing a flat buffer would use the Pin method to access the raw memory via the MemoryHandle's Pointer property.\") with a Memory<T>. You can just call .Pin() on the Memory<T>, if you want access to the raw pointer for a period of time.\nI've put up a PR #5191 for my approach (2) above - add a new #define for ENABLE_MEMORY_T. I think this is the only route that is going to meet all the requirements listed above - to allow me to get a Memory<T> from a ByteBuffer, and to allow you to meet your low-level performance requirements.. > a better solution that adding yet another want to compile the library.\nI would have preferred to just use public Memory<byte> Buffer when UNSAFE_BYTEBUFFER is defined and not use pointers and pinning at all. But as discussed above, if you are going from byte[] => Memory<T> => Span<T> multiple times in a very tight loop, the code will be slower. This isn't a concern for my scenarios because it gets washed out with all the other operations (like in the BuildMonsters benchmark example above). But if it is a major concern for other consumers, then it can be preserved by keeping the pointer and pinning code.\nI can't see how using a MemoryHandle is going to change the performance characteristics. Have you perf tested the approach you've listed above?\nC#\npublic abstract MemoryHandle Pin();\nMy initial assumption is that you won't be able to get the same performance as pinning the buffer for the lifetime of the ByteBuffer in the manner the code is doing now.\n\nThe other concern you seem to have is around the pinned lifetime of the underlying array, \n\nYes, that is a major concern, and why I opened the issue.\n\nand using Dispose() solves that.\n\nYou mean forcing the consumer to always remember to call Dispose() on the ByteBuffer (like we have now)? Or do you mean pinning and then disposing inside of every call in FlatBufferBuilder?\nIn my opinion, it feels unnatural to force the callers to call Dispose() at all when they are just dealing with managed objects. For example, byte[] and List<T> aren't IDisposable.. @ccifra - your thoughts above inspired me to reconsider how to implement this. I changed ByteBufferAllocator to instead have GetSpan and GetMemory abstract methods, and I'm seeing some pleasant results (better than I was getting with always going through Memory<T> Buffer).\nI've pushed my changes (and the 3 benchmarks) here: https://github.com/eerhardt/flatbuffers/tree/UseMemoryPlayground\nHere are the results I'm seeing:\nCurrent code\nmaster branch with UNSAFE_BYTEBUFFER;ENABLE_SPAN_T;BYTEBUFFER_NO_BOUNDS_CHECK\n|             Method |      Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|------------------- |----------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| BuildNestedMonster | 18.833 ms | 0.3729 ms | 0.7361 ms |   4687.5000 |           - |           - |            18.77 MB |\n|       BuildMonster |  9.028 ms | 0.1998 ms | 0.1869 ms |   2093.7500 |           - |           - |             8.39 MB |\n|         TestTables | 60.940 ms | 0.6307 ms | 0.5899 ms |    222.2222 |    222.2222 |    222.2222 |               32 MB |\nUseMemory\nMy original implementation using Memory<byte> Buffer property as currently proposed in #5191 \n|             Method |       Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|------------------- |-----------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| BuildNestedMonster |  20.084 ms | 0.3994 ms | 0.8160 ms |   4687.5000 |           - |           - |            18.77 MB |\n|       BuildMonster |   9.308 ms | 0.1758 ms | 0.1645 ms |   2093.7500 |           - |           - |             8.39 MB |\n|         TestTables | 132.399 ms | 1.4828 ms | 1.3145 ms |    750.0000 |    750.0000 |    750.0000 |               32 MB |\nUseMemoryPlayground\nNew implementation using GetSpan and GetMemory abstract methods.\n|             Method |      Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|------------------- |----------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| BuildNestedMonster | 14.618 ms | 0.2708 ms | 0.2660 ms |   4609.3750 |           - |           - |            18.46 MB |\n|       BuildMonster |  7.429 ms | 0.2642 ms | 0.2595 ms |   2054.6875 |           - |           - |             8.24 MB |\n|         TestTables | 99.970 ms | 1.2580 ms | 1.1767 ms |    833.3333 |    833.3333 |    833.3333 |               32 MB |\nAs you can see, the BuildNestedMonster and BuildMonster scenarios are actually faster than master with this new implementation. However, the TestTables is still slower (for the same reasons above - constantly getting a Span in a tight loop).\nWould you mind perf testing a real-world scenario in your projects that use a large number of tables with this implementation? I'd like to see if it would be possible to collapse the two implementations into one, and get rid of the byte* Buffer property, and pinning usage all together. That would lead to much more maintainable code than having the extra #defines that I am currently proposing.. I signed it!. Is the appveyor CI broken? I don't think I changed anything that would have broken it.. > I am going to assume it works for now :)\nIt works on my box. \ud83d\ude09 . I've updated the code based on the discussion in #5181. Please take a look.\n\nAny ideas on how we could add to our AppVeyor CI to maybe run with these #defines on and off?\n\n@aardappel - Do you mean the new Benchmarks? Or do you mean the existing unit tests? For the benchmarks, it makes the most sense to run them in a perf regression environment where you are on a consistent machine and if you log the output (how much time was taken for each benchmark).\nFor now, my thought was to add them to the repo at least so others could compare before and after their local changes, and so new interesting scenarios can be added over time.. @aardappel - I added a command to appveyor to run the tests with ENABLE_SPAN_T defined.\nFrom the error on the appveyor check, it looks like I need to sync up with the master. Does this repo typically like to merge or rebase PRs to get them updated with master?. @aardappel - I've rebased my changes on the latest master.. I see that the appveyor is using VS 2015.\nI've tried to make the ENABLE_SPAN_T support work on VS 2015, but I just don't think it is possible. (At least not without some hacks.)\nI was able to get one more #define under test - UNSAFE_BYTEBUFFER. So I did make some progress.\nI think if we add a VS 2017 leg to the CI, we could easily put ENABLE_SPAN_T in regression as well. But I don't think I'd be able to do that myself.\n@aardappel @ccifra - this is ready for review now.. > but that can also happen in a next PR.\nI'd like to at least resolve this comment: https://github.com/google/flatbuffers/pull/5191#discussion_r259037932 before merging. That way I can have a solid ByteBufferAllocator public API.. @aardappel, all feedback should be addressed. Unless you or @stephentoub objects, I think this is ready.. I think that could make sense... The caller that is creating the custom allocator can be responsible for disposing it.\nNote that this means ByteBuffer would also not be IDisposable anymore.\nSince the allocator isn't exposed on ByteBuffer, there would be no way you could create a custom/disposable allocator, pass it into a ByteBuffer and then hand that ByteBuffer off to some other code to dispose of it when they are done with it.\nDo you think that scenario would be a problem?. OK, I can remove IDisposable from both then. Any more feedback? Or will it be good after that change?. I bounced back and forth on this... But I ended up here \"in the name of performance\".\nThe default ByteArrayAllocator class overrides these methods because I saw an extremely small performance gain in doing so. I never verified exactly why, but my assumption is the performance gain is caused by not having to do bounds checks on the array. If you just call GetSpan() or GetSpan(start), we directly return array and array.AsSpan(start), thus no bounds checks for the first call, and only one bounds check for the second call.\nI can go back to making these non-virtual, as I think it cleans up the API. But that was the reasoning I had these virtual.. Is there somewhere we should make a note of it?. Note that the documentation at the top of the file already indicates this, so I think we're OK?\nhttps://github.com/google/flatbuffers/blob/4f32cbf268a8ad469974b9453924a3582dbf2f30/net/FlatBuffers/ByteBuffer.cs#L19-L31. Yes, if we want to enable custom ByteBufferAllocator implementations to have read-only stores. (It is slightly a misnomer to call it an \"allocator\". In reality, I think it would make sense to call it a \"store\" or \"storage\".)\nRead-only stores is a scenario for the Apache Arrow zero-copy reads task I'm working on.\nIn that case, when reading an Arrow stream, we don't need to write to the ByteBuffer, we only need to read. So calling GetSpan is going to throw a NotSupportedException. This enables the Arrow stream reader to take in a ReadOnlyMemory<byte> object and read a flatbuffer ByteBuffer over top of it.\nIf we only had a writable GetSpan API - that scenario wouldn't be possible. I don't think you can get a writable Span<byte> from a ReadOnlyMemory<byte>.. Will add.. The three benchmarks I'm adding to the repo showed a small improvement doing it the way I currently have it. Let me get some numbers again to make sure.. I would think the same code that is creating the ByteBuffer is the one that is using it. If they create a read-only ByteBuffer, they probably shouldn't be using it to write.\nThis is sort of the same discussion about removing IDisposable. If you created a disposable allocator, you should dispose of it yourself.\nI think we can add a CanWrite API in the future, if it is necessary.. ByteBuffer is a pretty core type in Flat Buffers, from what I can tell. You basically need it in order to do anything with the API.\nFor example, here's how Apache Arrow is reading an instance of its Message table:\nhttps://github.com/apache/arrow/blob/9600d8b85d44af8d59f1f680341021717738a7b1/csharp/src/Apache.Arrow/Ipc/ArrowStreamReader.cs#L70\nYou need to pass a ByteBuffer into Flatbuf.XXX.GetRootAsXXX(buffer) method in order to read a table.. It's actually a bit more complicated than that. The pos parameter appears to be in terms of bytes, the len parameter appears to be in terms of T.  See the \"non-SPAN_T\" code below, and the original code.\nSo the above should be:\nC#\nreturn MemoryMarshal.Cast<byte, T>(_buffer.ReadOnlySpan.Slice(pos)).Slice(0, len).ToArray();. It's not anymore. Removing.. Good call.. I ran the benchmarks 3 times with each way, and I didn't see any noticeable perf differences.\nCurrent code:\n|             Method |      Mean |     Error |    StdDev |    Median | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|------------------- |----------:|----------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| BuildNestedMonster | 11.964 ms | 0.1665 ms | 0.1476 ms | 11.963 ms |   4609.3750 |           - |           - |            18.46 MB |\n|       BuildMonster |  5.849 ms | 0.1155 ms | 0.2678 ms |  5.724 ms |   2054.6875 |           - |           - |             8.24 MB |\n|         TestTables | 86.696 ms | 1.6580 ms | 1.4697 ms | 86.228 ms |    833.3333 |    833.3333 |    833.3333 |               32 MB |\n|             Method |      Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|------------------- |----------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| BuildNestedMonster | 11.886 ms | 0.2122 ms | 0.1985 ms |   4609.3750 |           - |           - |            18.46 MB |\n|       BuildMonster |  5.652 ms | 0.0884 ms | 0.0784 ms |   2054.6875 |           - |           - |             8.24 MB |\n|         TestTables | 90.610 ms | 2.6365 ms | 3.1385 ms |    833.3333 |    833.3333 |    833.3333 |               32 MB |\n|             Method |      Mean |     Error |    StdDev |    Median | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|------------------- |----------:|----------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| BuildNestedMonster | 11.805 ms | 0.1513 ms | 0.1415 ms | 11.843 ms |   4609.3750 |           - |           - |            18.46 MB |\n|       BuildMonster |  5.880 ms | 0.1545 ms | 0.3122 ms |  5.751 ms |   2054.6875 |           - |           - |             8.24 MB |\n|         TestTables | 88.449 ms | 0.6658 ms | 0.6228 ms | 88.661 ms |    833.3333 |    833.3333 |    833.3333 |               32 MB |\nSuggested\n|             Method |      Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|------------------- |----------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| BuildNestedMonster | 11.836 ms | 0.1115 ms | 0.0988 ms |   4609.3750 |           - |           - |            18.46 MB |\n|       BuildMonster |  5.585 ms | 0.1064 ms | 0.0995 ms |   2054.6875 |           - |           - |             8.24 MB |\n|         TestTables | 91.392 ms | 0.8878 ms | 0.7870 ms |    833.3333 |    833.3333 |    833.3333 |               32 MB |\n|             Method |      Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|------------------- |----------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| BuildNestedMonster | 12.159 ms | 0.1445 ms | 0.1281 ms |   4609.3750 |           - |           - |            18.46 MB |\n|       BuildMonster |  5.668 ms | 0.0386 ms | 0.0361 ms |   2054.6875 |           - |           - |             8.24 MB |\n|         TestTables | 89.481 ms | 0.6771 ms | 0.6333 ms |    833.3333 |    833.3333 |    833.3333 |               32 MB |\n|             Method |      Mean |     Error |    StdDev |    Median | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|------------------- |----------:|----------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| BuildNestedMonster | 12.448 ms | 0.2476 ms | 0.6695 ms | 12.131 ms |   4609.3750 |           - |           - |            18.46 MB |\n|       BuildMonster |  5.578 ms | 0.1005 ms | 0.0940 ms |  5.550 ms |   2054.6875 |           - |           - |             8.24 MB |\n|         TestTables | 89.953 ms | 1.4435 ms | 1.2796 ms | 89.771 ms |    833.3333 |    833.3333 |    833.3333 |               32 MB |\nSo I refactored to the suggested 4 abstract properties. (Honestly, I kind of forgot you can have a Span property on a class ... just not a field.)\nThen I went and implemented some of the other suggestions in this PR, and the new numbers I have look even better (I assume it is mostly due to the bounds-check eliding in the Put method):\n|             Method |      Mean |     Error |    StdDev | Gen 0/1k Op | Gen 1/1k Op | Gen 2/1k Op | Allocated Memory/Op |\n|------------------- |----------:|----------:|----------:|------------:|------------:|------------:|--------------------:|\n| BuildNestedMonster | 11.531 ms | 0.1855 ms | 0.1644 ms |   4609.3750 |           - |           - |            18.46 MB |\n|       BuildMonster |  5.567 ms | 0.0707 ms | 0.0661 ms |   2054.6875 |           - |           - |             8.24 MB |\n|         TestTables | 85.469 ms | 1.0523 ms | 0.9843 ms |    833.3333 |    833.3333 |    833.3333 |               32 MB |. It probably can, but I think that can be a separate PR. This was existing code that just got auto-formatted by VS.. Agreed.. Unfortunately no.  :(\nSeverity    Code    Description Project File    Line    Suppression State\nError   CS0117  'BitConverter' does not contain a definition for 'SingleToInt32Bits'    Apache.Arrow(netstandard1.3)    F:\\git\\arrow\\csharp\\src\\Apache.Arrow\\Flatbuf\\FlatBuffers\\ByteBuffer.cs  514 Active\nError   CS0117  'BitConverter' does not contain a definition for 'Int32BitsToSingle'    Apache.Arrow(netstandard1.3)    F:\\git\\arrow\\csharp\\src\\Apache.Arrow\\Flatbuf\\FlatBuffers\\ByteBuffer.cs  717 Active\nThe Int32 and float APIs are .NET Core 2.0 only. https://apisof.net/catalog/System.BitConverter.Int32BitsToSingle(Int32)\nThe Int64 and double APIs have been there since .NET Framework 1.0. https://apisof.net/catalog/System.BitConverter.Int64BitsToDouble(Int64). For double, I'd prefer to keep the implementation the same as float.. > Does raise the question of whether (eventually, obviously not part of this PR) there should be a netcoreapp build of the library in addition to netstandard2.0.\nThe way the Flat Buffers code \"ships\" (as far as I can tell) is that users copy this code into their projects. So there isn't really a \"build of the library\". So to make netcoreapp vs. netstandard code here would involve more #if defines, and then the users would have to set the appropriate defines.. There are two \"better\" ways of doing what you are doing here.\n\n(preferred for now) Is to add <GenerateAssemblyInfo>false</GenerateAssemblyInfo> to your PropertyGroup above. This will disable the automatic generation of the AssemblyInfo.cs file that the Microsoft.NET.Sdk does automatically. Then you can remove all these ItemGroups here. The advantage to this approach is that the assemblies produced by both FlatBuffers.Core.csproj and FlatBuffers.csproj will have the same assembly level attributes.  As it stands now, because you aren't setting things like <Copyright> in this project, you aren't getting the same assembly attribute in the compiled assembly as the other project does. See here.\nLong term, a better way would be to delete the Properties/AssemblyInfo.cs file all together and add any specific properties (like Copyright, Version, and AssemblyFileVersion) to this .csproj. Then the SDK can just generate the AssemblyInfo.cs file with the properties from this .csproj.\n\nCheck out this blog and the public docs for more info.. ",
    "jiawernlim": "https://google.github.io/flatbuffers/md__internals.html \n(End of page): \"Note that this not the only possible encoding, since the writer has some flexibility in which of the children of root object to write first (though in this case there's only one string), and what order to write the fields in. Different orders may also cause different alignments to happen.\"\nDid I perhaps misunderstand the meaning of this paragraph?. Closing as non-issue. The seemingly non-deterministic part of the output was due to the order in which map entries were being written into the buffer.. ",
    "VeaaC": "Would this also work for u64 enums, or is there further work needed for that?. ",
    "barweiss": "@rw I understand that you want the changes in this PR to be added moderately, but I wouldn't like to revert the changes I made as they fix many of the issues I found with the current generator. I'd appreciate it if you'll be able to review the changes, and I will gladly continue working on this PR, but I also understand that this PR suggests major changes in the code which might not match your vision for flatbuffers.. @rw Thanks! There's a sample of the generated code here: https://github.com/barweiss/flatbuffers/blob/53b16928cd8d8ac3c56af03b1ecf9ab7f9bbbcf2/samples/monster_generated.py\nOr did you mean something else?. @rw I would like to run it.\nHowever, my changes require the schema files to define only a single package. Since it is probably more efficient I started reverting my changes regarding file structure. Unfortunately I can only make progress on weekends, so my progress here is slow, but I'm still working on it :). It made sense to put all the naming conventions related functions in the same place.\nI don't mind really where it is, but if I move it MakeUpper should probably be moved as well.. I would like this feature, but it makes it harder to generate code for testing and isn't in line with the behavior of generators of other languages which base on directory structure, I'll add a flag to turn this feature on.. ",
    "akhilman": "I would like to contribute python generator.\nFor example I would like to implement direct buffer read/write trough numpy.frombuffer() with support of array of structs as numpy structured arrays or recarray's.\nAlso I would like to add enum.IntFlag  for enums with bit_flags attribute.\nAnd add to generated class more information about schema: field types and attributes\nAnd may be add PEP484 type hints.. ",
    "steveklabnik": "It does not have as much weight as gofmt does because gofmt existed before go 1.0.\nThat said, most, but not all, people have adopted it. It is configurable. But it\u2019s defaults are the official style that all official projects are intended to use, and the book and all examples use code in that style.\nMany people run it with \u2014check in CI to fail the build if formatting is off.\n\nOn Sep 28, 2018, at 3:09 PM, Matt Mastracci notifications@github.com wrote:\nI can't say for sure that the Rust ecosystem is backing rustfmt, but I believe it is a Rust ecosystem component (ie: https://github.com/rust-lang-nursery/fmt-rfcs) and as such is basically the \"canonical\" format for Rust code. I assume that go fmt has the same weight in that ecosystem.\n@steveklabnik might be able to weigh in with more authority.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Oh, I should also mention: nobody ever likes every style. The advantage of these formatters is that you no longer have to argue about style. To me, this is a significant advantage that outweighs any particular dislike of a given style.\n\n(I should also mention that i was on the team that decided the style. I didn\u2019t always get my way :smile:)\n\nOn Sep 28, 2018, at 3:09 PM, Matt Mastracci notifications@github.com wrote:\nI can't say for sure that the Rust ecosystem is backing rustfmt, but I believe it is a Rust ecosystem component (ie: https://github.com/rust-lang-nursery/fmt-rfcs) and as such is basically the \"canonical\" format for Rust code. I assume that go fmt has the same weight in that ecosystem.\n@steveklabnik might be able to weigh in with more authority.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. I think that approach makes sense too.\nOn Oct 9, 2018, at 4:24 AM, zhangsoledad notifications@github.com wrote:\n![cfg_attr(feature = \"cargo-clippy\", allow(clippy))]\n![cfg_attr(rustfmt, rustfmt_skip)]\nI recommend turned off rustfmt and clippy by default\nprevent modify generated code\nrustfmt and clippy are configurable, see rustfmt clippy, different team has different way, there is no consensus.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "zhangsoledad": "```\n![cfg_attr(feature = \"cargo-clippy\", allow(clippy))]\n![cfg_attr(rustfmt, rustfmt_skip)]\n```\nI recommend turned off rustfmt and clippy by default \n- prevent modify generated code\n- rustfmt and clippy both are configurable, see rustfmt clippy, different team has different way, there is no consensus.. ",
    "whatisaphone": "If you emit those attributes in the generated code, wouldn't it mean people can't use rustfmt even if they want to? I would prefer if we still had the choice to do it either way. It's possible to put module-level attributes on the mod statement as well as inside the module itself.\nIf you want to skip rustfmt, you can import like this:\n```rust\n[cfg_attr(rustfmt, rustfmt_skip)]\nmod myschema_generated;\n```\nOr if you are a rustfmt user, you can import without the skip attribute:\nrust\nmod myschema_generated;. I hit this problem as well. I did not precisely bisect the offending commit, but I suspect #4949 was the cause. I rolled back to the commit before that PR was merged and it solved the issue. In case it's helpful, our schema is public and can be seen here.. I'll put together some repro steps this weekend.. It looks like I was using incompatible versions of flatc vs. the flatbuffers crate. Upgrading to the latest flatbuffers fixed my issue. Sorry for the false alarm!. ",
    "tajmorton": "I signed it, robot! :robot: . Thanks @aardappel!. Thank you @aardappel!. Fixed, thanks.. That's an interesting question.\nI did a quick benchmark here of objects with mixed key types which showed that integer lookups were slow, but it didn't seem to make a huge difference if the object had mixed types or not. http://jsben.ch/dynSO\nIt's also possible this isn't a representative test case. :)\nThe Typescript Enum documentation talks about one way their compiler \"could\" generate the reverse mappings, but doesn't offer guarantees:\n\nTypeScript might compile this down to something like the the following JavaScript:\njs\nvar Enum;\n(function (Enum) {\n    Enum[Enum[\"A\"] = 0] = \"A\";\n})(Enum || (Enum = {}));\nvar a = Enum.A;\nvar nameOfA = Enum[a]; // \"A\"\nIn this generated code, an enum is compiled into an object that stores both forward (name -> value) and reverse (value -> name) mappings. References to other enum members are always emitted as property accesses and never inlined.\n. I don't think I know enough about Javascript to have an opinion. The fact that Typescript includes both mappings in a single object makes me think that it is hopefully not a huge issue.\n\nThe benchmark I did (for what it's worth) makes it look like accessing values by integer index was slow, regardless of if the object had keys of mixed types or not.\nAre there some people more familiar with Javascript who we could tag here to have them weigh in?. ",
    "laur89": "It does make sense, but I now realize my question was phrased poorly - didn't mean new fields being added to the structs, but to vector itself.\nWas basically asking about something like delta compression (if say an array of objects were serialized with FB and sent over the wire, followed by pushing another object in to the array and re-sending), but that's not really the purpose nor duty of serialization.. Yes, realized later my question statement made little sense.\nEven easier method would be diffing serialized messages on byte-array level with whatever diff algo one chooses.\n. ",
    "kostya-sh": "I have attempted to add a test for MutateTestbool. Since false is a default value for a bool field I had to change the initial value of testbool to true. However tests for other languages assume that its value should be false. I am not sure what is the best way to fix this? Fix tests for other languages? Or do not test MutateTestbool for Go (just check that it returns false and doesn't change the value, similar to the test for mana field)? . @rw can you please elaborate? Not sure if I explained it clearly in my previous comment but the problem is that it is not possible to mutate a property that has the default value because it is not stored in the buffer.. I have updated tests for other languages. Please have a look.. ",
    "mariocao": "Yesterday I was changing some configurations and today I don't get the same errors.\nRight now, the cargo clippy complains (only warnings) about 3 different cases as listed below.\nDoes it make sense to report this?\nRight now, we are excluding cargo clippy from the generated code of flatbuffers, but perhaps in the future it would be nice to have that code in compliance with clippy/\nRedundant field names:\n``rust\n    |\n170 |             _tab: flatbuffers::Table { buf: buf, loc: loc },\n    |                                        ^^^^^^^^ help: replace it with:buf`\n    |\n    = note: #[warn(clippy::redundant_field_names)] on by default\n    = help: for further information visit https://rust-lang-nursery.github.io/rust-clippy/v0.0.212/index.html#redundant_field_names\nwarning: redundant field names in struct initialization\n   --> src/./message_generated.rs:170:50\n    |\n170 |             _tab: flatbuffers::Table { buf: buf, loc: loc },\n    |                                                  ^^^^^^^^ help: replace it with: loc\n    |\n    = help: for further information visit https://rust-lang-nursery.github.io/rust-clippy/v0.0.212/index.html#redundant_field_names\n```\nStatic lifetimes:\nrust\n   |\n78 | const ENUM_NAMES_COMMAND:[&'static str; 7] = [\n   |                           -^^^^^^^---- help: consider removing `'static`: `&str`\n   |\n   = note: #[warn(clippy::const_static_lifetime)] on by default\n   = help: for further information visit https://rust-lang-nursery.github.io/rust-clippy/v0.0.212/index.html#const_static_lifetime\nRedundant closures:\nrust\nwarning: redundant closure found\n   --> src/./message_generated.rs:572:21\n    |\n572 |       self.ip().map(|u| Ipv6::init_from_table(u))\n    |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ help: remove closure as shown: `Ipv6::init_from_table`\n    |\n    = help: for further information visit https://rust-lang-nursery.github.io/rust-clippy/v0.0.212/index.html#redundant_closure. I will do my best once I get some free time. :). ",
    "MelvinNau": "Hi,\nAll right , more information ! \nMy schema for ingredients:\nenum Ingredient : ubyte {\n    betteraveRouge = 0,\n    carotte,\n    potimarron,\n  ....\n}\nIn my main table, I wish use this enum like that:\ntable reciH {\n  id:uint32;\n  recipeIngredients:[Ingredient];\n}\nIn my reciH_generated.h I have this add function:\nvoid add_recipeIngredients(flatbuffers::Offset<flatbuffers::Vector<uint8_t>> recipeIngredients)\nSo, when I want to use like that for exemple:\nstd::vector<uint8_t> types; \n    types.push_back(EnumValuesIngredient()[1]);\n    types.push_back(EnumValuesIngredient()[2]);\n    recipe.add_recipeIngredients(builder.CreateVector(types));\nThe compilation is ok. But when I execute my program, I have this error : \n\nAssertion failed: !nested (/usr/local/include/flatbuffers/flatbuffers.h: NotNested: 924)\n\nIt's a \"types\" problem I think\nThanks for your reply\n. I use EnumValuesIngredient()[1] because the number \"1\" here is an ID in my database. I just want to bind thoses values like EnumValuesIngredient()[1] was equals to Ingredient_carotte.\nThanks for you reply, I will look =). ",
    "PaulFake": "Ah, thanks :)\nFor the record, allocating with new worked, and\nreturn builder.ReleaseBufferPointer().data();\ndid not.  I'm fine with using new, but I'm curious what about the above is incorrect.. ",
    "rumatoest": "Yes it was latest master version. > In what situation does it panic?\nInvalid input buf: [u8] that can not be read\n\nI believe a verifier is the reasonable alternative when corrupt data has to be dealt with gracefully.\n\nBut in Rust graceful way to deal with error is to return Result.\nI is better to make get_root_as_...  function to always return Result\nAnd for other cases create another method like get_root_as_..._unwraped which name should give a hint about possible panic, because this is what you can get if you call unwrap() functions.. > It would make a lot of sense for Rust to follow the same model\nWhich is incompatible with Rust approach for handling errors.\n\nsince it is also performance oriented.\nI doubt that. \nBTW in my pet project I can't see any performance in Rust implementation at all.\n. @rw \n1 - Yes it is definitely slow I'm expecting to have about 1 or 2ms\n2 - will do it and recheck\n3 - I initialize it every time because I expect to execute such logic only once per second.\n4 - I can't right now. First I'll try to use reset instead of re initializing. Well it is not about FB initialization time.\n\nThis is the code that takes about 97 ms\nrust\n    let limg_content = Some(fb.create_vector(left));\n// OR\n   let rimg_content = Some(fb.create_vector(right));\nWhere left and right are 300kb &Vec<u8> \nGenerally this two lines computed about 190ms.\nReading & converting same data to Vec from usb cam takes about 30ms (time mostly depends of camera FPS)\nTo write all this data to output stream only 1-2ms required. Well right now I have about 45ms speed improvement.\nIn totally two Vec (300kb each) created for about 80-90ms\n. But I think it is still very slow, because there is no need to copy data.\nIn my particular case it is possible to access Vec as slice &[u8] without wasting any time.. ",
    "Tommytrg": "I'm getting this error in the latest master version of flatbuffers.\ncargo 1.31.0-nightly \nrustc 1.31.0-nightly\nMy schema can be found here.\nExample errors are: \nE0495\nerror[E0495]: cannot infer an appropriate lifetime for autoref due to conflicting requirements\n   --> data_structures/src/flatbuffers/protocol_generated.rs:206:15\n    |\n206 |     self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Table<'a>>>(Message::VT_COMMAND, None).unwrap()\n    |               ^^^\n    |\nnote: first, the lifetime cannot outlive the anonymous lifetime #1 defined on the method body at 205:3...\n   --> data_structures/src/flatbuffers/protocol_generated.rs:205:3\n    |\n205 | /   pub fn command(&self) -> flatbuffers::Table<'a> {\n206 | |     self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Table<'a>>>(Message::VT_COMMAND, None).unwrap()\n207 | |   }\n    | |___^\nnote: ...so that reference does not outlive borrowed content\n   --> data_structures/src/flatbuffers/protocol_generated.rs:206:5\n    |\n206 |     self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Table<'a>>>(Message::VT_COMMAND, None).unwrap()\n    |     ^^^^^^^^^\nnote: but, the lifetime must be valid for the lifetime 'a as defined on the impl at 174:6...\n   --> data_structures/src/flatbuffers/protocol_generated.rs:174:6\n    |\n174 | impl<'a> Message<'a> {\n    |      ^^\n    = note: ...so that the types are compatible:\n            expected serializers::flatbuffers::Follow<'_>\n               found serializers::flatbuffers::Follow<'_>\nE0599\n``\nerror[E0599]: no method namedmapfound for typeserializers::flatbuffers::Table<'>in the current scope\n   --> data_structures/src/flatbuffers/protocol_generated.rs:212:22\n    |\n212 |       self.command().map(|u| Version::init_from_table(u))\n    |                      ^^^\n    |\n    = note: the methodmapexists but the following trait bounds were not satisfied:&mut serializers::flatbuffers::Table<'> : std::iter::Iterator`\nerror[E0599]: no method named map found for type serializers::flatbuffers::Table<'_> in the current scope\n   --> data_structures/src/flatbuffers/protocol_generated.rs:222:22\n    |\n222 |       self.command().map(|u| Verack::init_from_table(u))\n    |                      ^^^\n    |\n    = note: the method map exists but the following trait bounds were not satisfied:\n            &mut serializers::flatbuffers::Table<'_> : std::iter::Iterator\n. I've avoided **E0599**, `.map` error one, removing the required field from our table Message:\nunion Command (required) { Version, Verack, GetPeers, Peers, Ping, Pong }\ntable Message {\n    magic: uint16;\n    command: Command;\n}\nThen the generated code is:\n  #[inline]\n  pub fn command(&self) -> Option> {\n    self._tab.get::>>(Message::VT_COMMAND, None)\n  }\nInstead, when I compile table Message with required field:\nunion Command (required) { Version, Verack, GetPeers, Peers, Ping, Pong }\ntable Message {\n    magic: uint16;\n    command: Command (required);\n}\nI get the code that produces mentioned **E0599** error:\n  #[inline]\n  pub fn command(&self) -> flatbuffers::Table<'a> {\n    self._tab.get::>>(Message::VT_COMMAND, None).unwrap()\n  }\n``\nCould be a problem with required union fields? We want to use required modifiers as it is needed to specify our protocol.  \nBy the other hand, I still get the **E0495** which this issue is about.. Upgrading to latest  flatbuffers crate fixed **E0495**. By the other hand, I'm still getting **E0599**.  I've opened #5056 to track this issue and I've made a PR to solve it.\n. > @Tommytrg also please regenerate the code, if applicable, using./generate_code.sh`\nRunning ./generate_code.sh:\n../flatc: warning: GRPC interface generator not implemented for binary\n../flatc: warning: GRPC interface generator not implemented for JavaScript\n../flatc: warning: GRPC interface generator not implemented for Dart\n../flatc: warning: GRPC interface generator not implemented for TypeScript\n../flatc: warning: GRPC interface generator not implemented for C#\n../flatc: warning: GRPC interface generator not implemented for Python\n../flatc: warning: GRPC interface generator not implemented for Lobster\n../flatc: warning: GRPC interface generator not implemented for Lua\n../flatc: warning: GRPC interface generator not implemented for Rust\n../flatc: warning: GRPC interface generator not implemented for PHP\n../flatc: warning: GRPC interface generator not implemented for binary\n../flatc: warning: GRPC interface generator not implemented for JavaScript\n../flatc: warning: GRPC interface generator not implemented for Dart\n../flatc: warning: GRPC interface generator not implemented for TypeScript\n../flatc: warning: GRPC interface generator not implemented for C#\n../flatc: warning: GRPC interface generator not implemented for Python\n../flatc: warning: GRPC interface generator not implemented for Lobster\n../flatc: warning: GRPC interface generator not implemented for Lua\n../flatc: warning: GRPC interface generator not implemented for Rust\n../flatc: warning: GRPC interface generator not implemented for PHP\nAny ideas?. Now, If the field is required, it will skip the .map and return Some(U_ELEMENT_TABLE_TYPE). @rw I would like to update the tests. To do that, I have to add any_required:Any (id:48, required); and update tests in consequence. Isn't it?. I've added a required_fields mod and I've fixed almost all the tests that have broken with the new any_required field. I couldn't fix the tests which use monsterdata_test.mon file. How is this file generated?. @rw?. Sorry, it was editor's autotrim plugin. ",
    "csmoe": "@rw okay, I'll have a try.\n\nthis doesn't cause anyone problems, because of the allow directives.\n``rust\nerror: an inner attribute is not permitted in this context\n --> target/debug/build/witnet_data_structures-1da2aa5c92bdff1c/out/protocol_generated.rs:4:3\n  |\n4 | #![allow(dead_code)]\n  |   ^\n  |\n  = note: inner attributes, like#![no_std], annotate the item enclosing them, and are usually found at the beginning of source files. Outer attributes, like#[test]`, annotate the item following them.\n\nerror: an inner attribute is not permitted following an outer attribute\n --> target/debug/build/witnet_data_structures-1da2aa5c92bdff1c/out/protocol_generated.rs:5:3\n  |\n5 | #![allow(unused_imports)]\n  |   ^\n  |\n  = note: inner attributes, like #![no_std], annotate the item enclosing t\n``\nNoop, while the user wanna generate protocol file with [build.rs](https://doc.rust-lang.org/cargo/reference/build-scripts.html) and include that into codebase withinclude!(concat!(env!(\"OUT_DIR\"), \"/protocol_generated.rs\"));`, rustc will complains about the unnecessary attributes in outer snippet: inner attributes is not permitted,. I signed it!. ",
    "stewart-ritchie": "In support of leaving things the way they are - consider Primitive Obsession code smell.. ",
    "travistrue2008": "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n\nI signed it!. Looks like the tests fail because of the namespace name convention change I made. I was hoping that this would be a simple change that wouldn't require me to run the tests locally (shame on me, seriously), and I don't have Java setup on my Mac at this time. Plus, it looks like ensuring that the directory structure matches the package path is also important, which is configured in BaseGenerator's constructor. I'd have to add a language-specific hack to fix this, which could obscure things...\nThat said, I'm going to close this for now.. ",
    "cor3ntin": "I signed it!. ",
    "C4N4D4M4N": "Closing this since I found an alternative that suits my needs. However if someone does know the answer to this I would still like to know since maybe this is still better than the alternative I found.. ",
    "itsMeBrice": "Oh that's very interesting and explains the behavior. It's just confusing, that the monster example checks the buffer directly assert monster.Name() == 'Orc'. The cpp example correctly converts to string but the python one doesn't. This should probably be changed as to not confuse future noobs like me.\nI'm not sure what the correct way to cast the buffer to string should be but it probably: assert monster.Name().Str() == 'Orc. ",
    "riklund": "Done. Attached updated goldens.. ",
    "akshayz": "Thanks @aardappel. Will try the first solution.\nFor the second problem,\nWe want to make the Java object mutable. Say for example I have set an id as 1 and 'name' as A and the user wants to change the name to ABC, can I set the new value of the field 'name' on the same java object which was created from flatbuffers?. ",
    "wangsu-xflag": "You're right.I just forgot to update the library files in my project.Thanks a lot!. ",
    "harsh-telepathy": "Size prefixing appears to be missing from Python altogether . ",
    "dalnew": "Is there already a PR for supporting vectors of unions in JSON? It seems 1.10 still doesn't support this but there was a reference for adding a change for that back in PR 4143 in Jan 2017.. ",
    "jensreck": "First at all, thank you for your fast response!\nRegarding enum and exceptions.... sorry for the unaccurate naming, exception means here I got an assertion if I do not provide the required field \"enum\" (see reflection.fbs, \"Schema\").\nFurthermore the enum was also the cause of my issue in general because I didn't compose a valid enum.  With the help of your hint regarding \"Verify()\" (and the covered 'okay' for the code) I found that I have to add an (useless) EnumVal object in additional. But... it's also valid to add an empty enum vector :-) Finally I have comment the (valid) enum part in the example...\nThere are still some open points...\n\nThe Builder.Finish() runs w/ and w/o SchemaIdentifier() which differs in results, but only with an SchemaIdentifier() the stream is valid, right?\nThe Verifier succedded with \"FlatBufferBuilder.GetBufferPointer()\" and also with \"FlatBufferBuilder.GetCurrentBufferPointer()\" but which adress and size (FlatBufferBuilder.GetSize()) is the right one e.g. for saving to *.bfbs file (or for forwarding the stream to another one)?\nThe stream (from current pointer) differs strongly to the *.bfbs generated by flatc.exe (e.g. two times 'BFBS' in my example) but is also a valid bfbs if Verify() succeeded? Is there any 'compress' or 'clean up'?\n\nfollowing the adapted code-verified example (no changes at .fbs)...\n`\nusing namespace flatbuffers;\nusing namespace reflection;\nint test()\n{\n  FlatBufferBuilder ilBuilder;\n///\n  ///\n  ///\n  auto ilTypeUByte = CreateType(ilBuilder, UByte);\nauto ilFieldTestByte = CreateFieldDirect(ilBuilder, \"testByte\", ilTypeUByte);\nstd::vector> ilFields;\n  ilFields.push_back(ilFieldTestByte);\n  auto ilObjectTestByte = CreateObjectDirect(ilBuilder, \"TestByte\", &ilFields);\nstd::vector> ilObjects;\n  ilObjects.push_back(ilObjectTestByte);\n///\n  ///\n  ///\n  //auto ilEnumValDummy = CreateEnumValDirect(ilBuilder, \"dummy\");\n  //std::vector> ilEnumVals;\n  //ilEnumVals.push_back(ilEnumValDummy);\n//auto ilEnumsDummy = CreateEnumDirect(ilBuilder, \"Dummy\", &ilEnumVals, false, ilTypeUByte);\nstd::vector> ilEnums;\n  //ilEnums.push_back(ilEnumsDummy);\n///\n  ///\n  ///\n  auto ilSchema = CreateSchemaDirect(ilBuilder, &ilObjects, &ilEnums, \"BFBS\", \"bfbs\", ilObjectTestByte);\nilBuilder.Finish(ilSchema, SchemaIdentifier());\n///\n  ///\n  ///\n  Verifier verifierBfbs(ilBuilder.GetCurrentBufferPointer(), ilBuilder.GetSize());\n  if (VerifySchemaBuffer(verifierBfbs)) {\n    std::cout << \"jippie :-)\" << std::endl;\n    return 0;\n  }\n  else {\n    std::cout << \"oh no... \" << std::endl;\n    return 1;\n  }\n}\n`. ",
    "crogrezen": "I've only tried with gcc 5.4. I did try my binary with gcc 8.2, but flatbuffers won't compile with that version (a bunch of errors due to fall-through in some switch statements).\nAttached is schema files and a sample json that fails.\ndataset.tar.gz\nI'm parsing with (appx.): \n``` c++\n    std::ifstream schema_file_handle(schema_file_path.string());\n    std::ostringstream schema_stream;\n    schema_stream << schema_file_handle.rdbuf();\n    std::string schema_str = schema_stream.str();\nflatbuffers::Parser parser;\n\nconst char* include_directories[] = {schema_directory.string().c_str(), nullptr};\nbool ok = parser.Parse(schema_str.c_str(), include_directories);\nok = ok && parser.Parse(dataset_str.c_str(), include_directories);\nauto dataset = serialization::GetDataset(parser.builder_.GetBufferPointer());\nname_ = dataset->name()->str();\nmaster_image_stream_ = dataset->master_image_stream()->str();\nauto data_type = dataset->data_type();\nassert(data_type == serialization::DatasetData::JsonDataset);\n\nconst serialization::JsonDataset* jsdataset = dataset->data_as_JsonDataset();\nauto image_streams = jsdataset->image_streams();\nfor (auto image_stream = image_streams->begin(); image_stream != image_streams->end();\n     ++image_stream)\n{\n    std::string stream = image_stream->image_stream()->str();\n    ImageStreamMetaData metadata;\n    metadata.depth = image_stream->depth();\n    metadata.size.width = image_stream->width();\n    metadata.size.height = image_stream->height();\n    metadata.channels = image_stream->channels();\n    metadata.camera_height_over_ground = image_stream->camera_height_over_ground();\n    auto matrix = image_stream->calibration_matrix();\n    metadata.calibration_matrix << matrix->m00(), matrix->m01(), matrix->m02(), matrix->m10(),\n        matrix->m11(), matrix->m12(), matrix->m20(), matrix->m21(), matrix->m22();\n    image_stream_data_[stream] = metadata;\n}\n\nauto frame_buffers = jsdataset->frame_data();\nstd::cerr << \"Number of frames: \" << frame_buffers->Length() << std::endl;\nsize_t i = 0;\nfor (auto buffer = frame_buffers->begin(); buffer != frame_buffers->end(); ++buffer, ++i)\n{\n    FrameDataJson frame_data;\n    std::cerr << \"TS(\" << i << \") \" << *buffer << std::endl;\n    std::cerr << \"  \" << buffer->time_stamp() << std::endl; // CRASHES HERE at i == 257\n    frame_data.timestamp = (Timestamp)buffer->time_stamp();\n    std::cerr << \" <- APA \" << std::endl;\n\n    Eigen::Vector3d rotation;\n    rotation << buffer->ground_truth_pose()->rotation()->x(),\n        buffer->ground_truth_pose()->rotation()->y(),\n        buffer->ground_truth_pose()->rotation()->z();\n\n    Eigen::Vector3d translation;\n    translation << buffer->ground_truth_pose()->translation()->x(),\n        buffer->ground_truth_pose()->translation()->y(),\n        buffer->ground_truth_pose()->translation()->z();\n\n    frame_data.ground_truth_pose = geometry::Se3(rotation, translation);\n\n    auto images = buffer->images();\n    for (size_t j = 0; j < images->Length(); j++)\n    {\n        auto image = images->Get(j);\n        frame_data.image_paths[image->image_stream()->str()] = image->path()->str();\n    }\n    frame_data_.push_back(frame_data);\n}\n\n``\nHere,dataset_stris anstd::stringthat's already been loaded.. I've debugged on my side, I noted the crashing line in a comment. In the second loop (overframe_buffers), at a certain point (i==257) the iterator (same as usingGet(i)) returns a pointer that points way out of the current process memory. Up until then, all is dandy..flatc --scoped-enums -c -o serialization dataset_schema.fbs`. I found it. Thanks for all your help!\nThe problem was that the flatbuffers::Parser held all the memory, and I allocated the instance on the stack in a helper function. When the function exited, the pointer to the builder_ was of course not valid anymore. Good pointer to adding the -fsanitize flags, it caught the problem immediately, and I should have thought of that before bothering you.\nThanks again, I'm closing this issue!. ",
    "jakirkham": "It's also reported to CPython as issue 9229 and is marked won't fix. The bug is isolated to Python 2 only (Python 3 works correctly).\nCoercing to buffer would be a feasible workaround for Python 2 only (Python 3 does not have buffer). It's worth noting buffer returns a read-only array. So won't work if writing to a mmap is needed, but should be fine for reading (e.g. FWICT this case).. ",
    "Pumpuli": "It seems that the whole file has double content: https://unpkg.com/flatbuffers@1.10.0/js/flatbuffers.mjs\nTheory: the npm publish command was run twice. It didn't go through the first time, but still executed the prepublishOnly script. The second publish command executed the same script again, but since it only appends to flatbuffers.mjs, the file content was effectively duplicated.. ",
    "coetry": "I signed it!. good point :). ",
    "chaplin89": "Ok but why find_package does not complain about missing findflatbuffers.cmake? . Ok, thank you!. ",
    "nxrighthere": "The idea is to generate fields that FlatBuffers will ignore and which is not intended for serialization since data like message ID/channel ID/sequence ID and so on is usually encoded at low-level in networking protocols rather than at serialization level.. Right now I have one .cs file that is generated by FlatC, let's say SpawnMessage for example. I store there object ID, position, rotation, and so on. I want conveniently identify that message at low-level before deserialization, directly using a constant field with predefined unique ID that was generated from a scheme. So I don't need manually write fields to the generated code or make separated .cs file for this stuff.. > That doesn't make a lot of sense in the case of FlatBuffers, since something like SpawnMessage is just a handle to buffer of serialized data. It is not an object that you can use for storage of variables unrelated to serialization.\nIt doesn't matter FlatBuffers, Protobuf, or MessagePack. The primary goal is to bind a particular serializable object to a unique identifier or to something else which related to this object but not serialized by the library itself. I can have a struct SpawnMessage. and access to its ID directly rather than make a SpawnMessage-Protocol-Data for this purpose.\nAsk yourself why almost every other popular serialization solution provide this feature out of the box, and you will find a reason to implement it in FlatBuffers.. I understand this, and this is the reason why I'm trying to migrate from MessagePack.\nWhen ignore attribute is used any extra field is become invisible for serialization/deserialization not just to pack/unpack only a target data, but also to conveniently create logic that related to a particular object.\nA quick example:\n```csharp\nprivate void OnMessage(byte channel, Peer peer, ushort id, byte[] data, int length) {\n    switch (id) {\n        case SpawnMessage.id: {\n            var spawnMessage = SpawnMessage.GetRootAsSpawnMessage(data);\n        ...\n\n``\nThis is the callback that triggered by the networking system which is identifying message before starting to work with received FlatBuffers data. Here I have a single struct for message ID and data for the game. Clean and convenient way to create any logic that related to this object based on message ID. The same way I can useSpawnMessage.channelorSpawnMessage.sequence` to keep this message in sequence when packets may come out of order and so on. There are many use cases.. ",
    "Mygao": "\nI signed it!\n\n. ",
    "omalley": "I signed it!. I haven't looked at the flatbuffer C++ implementation and how it is converting strings to & from UTF-8, so I don't know. Java stores strings in UTF-16, so you always have to convert. In C++, the encoding for std::string isn't defined by the library, so there probably isn't matching functionality.\nThe Java code was using the standard Java library for such encoding and creating a new byte buffer with the UTF-8 string and then copying it to the final buffer. By using the UTF-8 encoder from Protobuf, the new code can calculate the size of the UTF-8 bytes ahead of time and directly write them into the final buffer. Both the size calculation and translation have a fast path for ASCII characters.. > The comments mention this implements a relatively strict decoder. I'd be worried that someone's use of FlatBuffers in the field suddenly breaks because they have sloppy UTF-8 data stored. What is the likelyhood of this?\nThe chance is very very low. Many of us have used protobuf for years and I've never talked to anyone who has had a problem with their UTF-8 encoder. I don't think the Java encoder generates such sequences.\n\nAlso uneasy about the use of sun.misc.Unsafe, there are rumors it will be removed (https://www.javaworld.com/article/2952869/java-platform/understanding-sun-misc-unsafe.html)? Is it even supported on all platforms (e.g. all version of Android?, https://gitter.im/scala-android/sbt-android/archives/2018/01/09) Would it simply fall back on the slower path?\n\nsigh Yes, Java is trying to remove Unsafe, but as yet haven't proposed a workable solution for projects that get significant speed boosts by using it.\nThe code detects whether unsafe is available and automatically falls back to the safe path.\n\nThere's also logging statements in the code, would prefer to not have those.\n\nOk.\n\nWhy is unsafe even needed? UTF-8 is byte based parsing, surely it can't be that slow to just read bytes out of a byte[] ?\n\nI'll test out the performance of the safe vs unsafe code paths.\n\nGenerally, while I understand that there's an advantage to copying these files as-is from Protobuf just in case we'd want to update them in the future, it is also a LOT of code, quite a bit of which doesn't seem necessary for the FlatBuffers use case. If possible, I'd prefer reduces special case code with less dependencies.\n\nTaking the entire pair of classes is a trade off:\n more code, which is bad\n less chance of introducing bugs, which is good\n* easier to apply future fixes from protobuf, which is good\nIt all depends on which you value more highly.. Ok, I've factored out the faster encoders into an API and three implementations:\n\nOne that uses the built-in Java encoder\nOne that uses the protobuf \"safe\" encoder\nOne that uses the protobuf \"unsafe\" encoder\n\nThis code is a first pass and really should have some unit tests. Note that the safe encoder has two variants for array-backed ByteBuffers or non-array ones. The unsafe encoder has three variants the array-backed ByteBuffers, direct ByteBuffers, and neither. The neither case uses the same code as the safe non-array.\nOn my benchmark, the results I get are:\ngenerated     direct             orig  avgt   10  32.133 \u00b1 1.755  ms/op\ngenerated     direct             safe  avgt   10  12.973 \u00b1 0.189  ms/op\ngenerated     direct           unsafe  avgt   10  11.745 \u00b1 0.231  ms/op\ngenerated      array             orig  avgt   10  45.376 \u00b1 1.546  ms/op\ngenerated      array             safe  avgt   10  14.136 \u00b1 0.286  ms/op\ngenerated      array           unsafe  avgt   10  12.482 \u00b1 0.158  ms/op\nSo direct ByteBuffer with the unsafe encoder is the fastest. Do you care about the extra 10% to have the unsafe encoder?. Ok, I've removed the unsafe code and the numbers look good. I left the java utf8 encoder as an option in case someone wants to revert to the original encoder.. You do need to change the version string before the next release, but I changed it here mostly for my own upstream testing to make sure that I got the new artifact with my change and not the cached 1.10.0.\nIn my projects, I change the version string just after a release. In your case, I would have changed it to \"1.11.0-SNAPSHOT\". Just before the release, I would take off the \"-SNAPSHOT\". . ",
    "simonking200": "Sorry for late reply, I'd like to make a PR.. I signed it! CLA\n\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\nPlease visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n\nI signed it!. You are welcome, and also thank you for your amazing flat-buffers!. ",
    "FrancisRussell": "I see now that presence of scalar fields seems to be fuzzily defined so closing this issue.. ",
    "Performador": "Indeed, it would be very useful to have helpers for copy especially for writing processors / filters that operate on flatbuffer files.\nThanks.. ",
    "metux": "\nCan you describe what exactly this fixes? Would this install debug binaries into the system folders (which I believe is undesirable)?\n\nNo idea about your definition of 'debug binaries', but I expect/need the package's build script to install all libraries + binaries without unstripped, so the distro packaging toolchain (eg. debhelper) can automatically put the debug data into the proper packages.\n. > With debug I mean an un-optimized executable (no -O2 or whatever).\n\nI would claim you would not want such binaries in your system directories, especially not when speed \ncan matter (which can be the case for flatc, e.g. when it needs to parse large amounts of JSON).\n\nActually, I do want this.\nI'm working strictly via distro toolchains, which implies eg.:\n the whole build process is completely controlled by the distro's build/packaging toolchain (including compiler flags, actuall calls to tools like cmake, make, etc)\n stripping etc shall not be done by the package itself, but the packaging toolchain (eg. for automatically moving debug info into separate packages)\n* compiler flags (eg. optimizations, target configuration, ...) are controlled by the packaging toolchain - distro decides what kind of code to generate.\nAnd yes: if I do want an debug installation (eg. w/o any optimizations), I do wanna have this built/packaged exactly like the production packages, so I can test/debug in exactly the same environment, w/o any extra manual setups.\n--mtx. > Please, apply the flatbuffers-patch.txt attached to fix that error related to Conan.\nthx, applied to my oss-qm branch.\n\nI don't why, but I'm not able to open a PR to oss-qm\n\nhmm, I just tried pull requests - you'd have to create a fork and then open a pull request.\nNo idea whether threre's another way. Maybe some repo settings I could change ?\nOTOH, I'm fine w/ textual patches (but please add author and description headers).\n. > I'm still somewhat worried that people not paying attention (that are not your use case) will accidentally end up with slow binaries in their system.. \nIMHO, only developers/maintainers would build on their own, and they should know what they're doing.\nPlain users should only install from their distro.\n\nwhat do you think about forcing the build to Release if not specified?\n\nFine for me.\n. > Yes, I created a fork, but when I open a new PR, Github pointed me only google/flatbuffers as target, your fork is not listed, but I have no idea about which settings are related. \nDid you fork from my repo or the google one ?\nIf you forked from google's, github's behaviour seems correct, as pull requests (IMHO) always go to the repo's upstream.\nBy the way: I've meanwhile rebased onto 1.10.0 as well as master.\n. > Not sure how realistic that is. Distro's are often several years behind, \nThat's what backports repos are for. I happen to be maintainer of such backports.\n\nand for flatc it is important that it matches the runtime code. \n\nAdd proper version dependencies - the package manager then handles that on it's own.\nFor debian it looks like that:\nDepends: libflatbuffers (= 1.9.0~ascii0), libc6 (>= 2.3), libgcc1 (>= 1:4.2), libstdc++6 (>= 5.2)\n\nSo if you have a project that relies on FlatBuffers via git submodule or whatever, and thus takes the \n\nThis is a VERY VERY BAD practise, that makes packaging extremly hard. \nJust don't do that.\n\nFrankly, rather than making release and debug function the same, I'd argue that flatc (and lib/include) \nshould NEVER be installed in system dirs, since you may have multiple projects relying on multiple \nversions of FlatBuffers, and you'll create a mess trying to compile them on the same system.\n\nActually, it should ALWAYS be installed according FHS.\nIF you want MVCC, then add the API name into the command file name (eg. 'flatc-1.9') and \nlet the distro packages take care of the alternative configuration for 'flatc' command.\nAs upstream you could also add the symlink /usr/bin/flat => /usr/bin/flatc-$version yourself,\nand distros just skip it from the final package.\n\nYou cannot compare FlatBuffers with libraries installed into Linux system dirs (that have super stable \nAPIs/ABIs) that way. Not only does FlatBuffers evolve more, there is an API boundary between \ngenerated code and runtime code that changes on occasion, all in headers.\n\nIn that case, you want MVCC build. At that point, also make sure, the so name carries the version number.. > > Did you fork from my repo or the google one ?\n\nMy fork is from Google, but I created my fork much time ago.\n\nAh, that might be the problem why you can't create a pull request against my repo.\nI've just contacted github whether they can make that possible - let's see what they're saying.. > It sounds like you live in a perfect world where there is only Linux. When building portable apps that rely on 3 desktop OSes (and maybe 2 mobile ones), relying on a distro package manager is impossible, and relying on the github repo is the only way to go.\nWhy not just using the package managers of the target platforms ?\nEven Windows has one (sort of).\n. > Windows has a package manager? What, Chocolatey? The Linux subsystem for Windows? vcpkg? \nActually I thought about nuget, which is opensource and pretty famous.\n(yes, not a real package manager like we're used from GNU/Linux world, but at least a good start)\n\nAnd you think you can depend on developers all using those, and getting your dependencies thru them?\n\nYes. I'm doing so myself for about 25 years (I once wrote my own package manager and packaging build toolchain, aeons ago). I've even implemented package management several for commercial applications (eg. Zimbra). And I'm always designing my software so it's very modular and easy to package.\nAnd no: I don't do any production deployment w/o package management, because I need high reliability and reproducability, easy and cheap operating.\nCertainly, the ignorance of the Windows world (and most of the commercial sw vendors) against such essential technologies - for 25 years now - is one of the major reasons why development as well as operating is so extremly expensive here. \n\nNot even on Mac is usage of package managers universal. No package manager for iOS/Android either. It is a non-starter.\n\nAs said, the commercial world is ignorant (sometimes even hostile) against such simple and essential technologies. Not surprising: they're creating problems so they can sell their solutions for absurd prices. All these expensive \"Enterprise\" depolyment tools just would be completely obsolete, if people would just use package managers (and sw vendors would get up their lazy asses and provide packages). This isn't a technical issue, it's their business model. (one that I'll never buy into).\nAnd for Apple and Google: their business model is based on NOT having any package management, but instead their ridiculous \"apps\" (for each task hundreds of different ones, usually badly maintained) and depolyment through mechanisms that they can control and charge for. Their business model is just based on signling instead of collaboration, foreign rule instead of freedom.\nI just can't count these commercial toy platforms as anywhere near anything like decent software engineering.\n\nEven on Linux, there is no apt-get install flatbuffers, and FlatBuffers is not a small project.\n\nActually, it is packaged for many distros: https://repology.org/metapackage/flatbuffers/versions\n(Debian folks also did package it, but not yet in any official deb repo).\nI've packaged the recent release for Devuan myself.\n\nI have no idea how anyone can say Why not just using the package managers of the target platforms \n? unless they only use very popular libraries and think there is no world outside of Linux.\n\nActually, I'm using many libraries that aren't in the official repos of the Distros I'm using. Therefore I'm just packaging what I'm missing. In the past I've also written my own package managers (even for win32).\nIn the end, if one just things through the problem carefully, it's rather trivial. I really wonder what's so hard to grasp those simple concepts - I'm wondering about that for 25 years now.\nBy the way: even if you - for whatever reason - want a bundled build, there are so many tools for rolling your own micro-distro, and those things are rather simple to implement on your own. All it takes is just a modular approach and a clean separation between the realm of an indivual package and the distro's realm. Most times, a Makefile then does the job.\n(oh, flatbuffers isn't quite modular ...)\n--mtx. How exactly ?\nThe .pc template is hand-written on purpose (so we specific additions/changes can easily be done anythime when necessary) - it just needs to fill in some built-time defined data (eg. pathes).. I assumed that would be really obvious.\npkg-config is very helpful for detecting libraries (and corresponding compiler/linker flags) in a very simple, portable and canonical way. actually it's the standard mechanism since aeons. Actually, I never would like to do that manually again.. Okay, for a explaination:\nPkg-config is a tool for easily retrieving compiler/linker flags that are needed to link against some library. The idea is: each library package also ships a .pc file that contains the necessary metadata (eg. cflags, ldflags, ...), including dependencies. These can be queried eg. from a makefile (or some genarator like autoconf, etc), so individual applications don't need any special per-library logic/knowledge - just the name and minimal version.\nIsolating out those things into a generic tool is also VERY helpful for distros that might need some tweaks (eg. processing pathes for crosscompiling -> sysroot, etc), w/o having to patch each individual package.\nFor example in a makefile you could add things like:\nPKG_CONFIG ?= pkg-config\nCFLAGS += $(PKG_CONFIG) --cflags foo\nLIBS += $(PKG_CONFIG) --libs foo\nThe patch for CMakeLists.txt tells cmake to generate the file flatbuffers.pc from template flatbuffers.pc.in, replacing variables w/ the actual build-time values (eg. pathes, etc), and also install that file on the target.. ",
    "spacejumpunreal": "\nIf it has to be even smaller, just store all properties as a vector of ints, with the int being either an int directly, a float bit representation, or index into a vector of strings etc.\n\nthank! This is exact what I am doing. Yet there will be extra cpu cost for accessing strings(get the index for string, then index the vector of strings), and extra memory cost of storing the vector of strings. So things can be better if we have typeless offset(void*) to \"somethings that I just know its type\" in struct, or to allow cast an interger to \"an offset which points to something that I know it's type\"?. the test code\n```\n    public class TryFlatbuffer\n    {\n        static void Main(string[] argv)\n        {\n            int N = 1024;\n            byte[] ar;\n            {\n                var bb = new ByteBuffer(N * 40);\n                var builder = new FlatBufferBuilder(bb);\n                {\n                    var table2ArrayOffsets = new Offset[N];\n                    var stringArrayOffsets = new Offset[N];\n                    for (int i = 0; i < N; ++i)\n                    {\n                        int ii = -i - 0;\n                        table2ArrayOffsets[i] = Table2.CreateTable2(builder, ii, ii);\n                    var so = builder.CreateString(i.ToString());\n                    stringArrayOffsets[i] = StringWrapper.CreateStringWrapper(builder, so, so);\n                }\n                var o1 = Table2.CreateSortedVectorOfTable2(builder, table2ArrayOffsets);  //exception on this line\n                var o2 = StringWrapper.CreateSortedVectorOfStringWrapper(builder, stringArrayOffsets);\n                var r = Root.CreateRoot(builder, o1, o2);\n                builder.Finish(r.Value);\n            }\n\n            ar = bb.ToSizedArray();\n        }\n        {\n            var bb = new ByteBuffer(ar);\n            var r = Root.GetRootAsRoot(bb);\n            var ii = r.Table2ArrayByKey(-1000).Value.V;\n            var ss = r.StringArrayByKey(\"34\").Value.V;\n            var none = r.StringArrayByKey(\"?\").HasValue;\n            var nonev = r.StringArrayByKey(\"?\").Value.V;\n            Console.WriteLine(\"aaaaaaaa\");\n        }\n    }\n}\n\n```\nthe schema\n```\ntable StringWrapper\n{\n    k:string(key);\n    v:string;\n}\ntable Table2\n{\n    k:int(key);\n    v:int;\n}\nstruct uStruct\n{\n    hash:int;\n    value:int;\n}\ntable Root\n{\n    table2Array:[Table2];\n    stringArray:[StringWrapper];\n}\nroot_type Root;\n```\nand this is the code generated for Table2, the sort code is where the exception came from. I think it is not checking the default value case.\n```\npublic struct Table2 : IFlatbufferObject\n{\n  private Table __p;\n  public ByteBuffer ByteBuffer { get { return __p.bb; } }\n  public static Table2 GetRootAsTable2(ByteBuffer _bb) { return GetRootAsTable2(_bb, new Table2()); }\n  public static Table2 GetRootAsTable2(ByteBuffer _bb, Table2 obj) { return (obj.__assign(_bb.GetInt(_bb.Position) + _bb.Position, _bb)); }\n  public void __init(int _i, ByteBuffer _bb) { __p.bb_pos = _i; __p.bb = _bb; }\n  public Table2 __assign(int _i, ByteBuffer _bb) { __init(_i, _bb); return this; }\npublic int K { get { int o = __p.__offset(4); return o != 0 ? __p.bb.GetInt(o + __p.bb_pos) : (int)0; } }\n  public int V { get { int o = __p.__offset(6); return o != 0 ? __p.bb.GetInt(o + __p.bb_pos) : (int)0; } }\n........\npublic static VectorOffset CreateSortedVectorOfTable2(FlatBufferBuilder builder, Offset[] offsets) {\n    Array.Sort(offsets, (Offset o1, Offset o2) => builder.DataBuffer.GetInt(Table.__offset(4, o1.Value, builder.DataBuffer)).CompareTo(builder.DataBuffer.GetInt(Table.__offset(4, o2.Value, builder.DataBuffer))));\n    return builder.CreateVectorOfTables(offsets);\n  }\n```. ",
    "gurumeditating": "CLA signed.. ",
    "mhfrantz": "Let me know if you prefer using std::lexicographical_compare instead of the DIY StringLessThan.. All comments addressed, commit amended.. This is to make sure that we compare the embedded null and what follows.. Finished style fixes for tests.. ",
    "rockmenjack": "yeah...I've already switched to multi-file version.... ",
    "oakad": "More in general the state of Go support is rather disappointing at the moment:\n\n\nThe go builder needs more utility methods - at the very least \"CreateUoffsetVector\" taking slice and producer functions.\n\n\n\"--gen-object-api\" should be supported by flatc, generating plain Go structs and pack/unpack methods\n\n\nLong standing \"import\" bug should be finally addressed (#3927).\n. So there are basically no active developers for the Go FBs at the moment? Because he promised to fix #3927 half a year ago but it is not fixed yet.. I signed it. I will write a description when I get a minute or two. Basically, instead of using free constructor functions I have introduced a builder object (in line with most other languages) and implemented the native object marshaling code generation.. https://github.com/oakad/flatbuffers/blob/master/docs/source/GoApi_new.md. @aardappel \n\n\n\nwhich causes an object to be allocated for each object constructed\n\nThis is incorrect. Most of the time those objects are simply pushed to stack (it's not Java here, Go has a well defined concept of \"automatic\" variables as well as pointers to stack held vars).\nI would also like to point out that \"reducing the number of allocations\" while desirable, should not become a fetish in memory managed languages (in early Go versions memory allocator was very so-so, but it has improved dramatically since then). For example, an allocation on average may take less time than converting a byte range to a float (and it may even take less time than converting a byte range to an int) - because most of the time an allocation has an amortized constant time complexity comparable to a stack push. Consider protobufs for example - that library allocates a massive amount of short lived objects and still manages to pull of a rather decent performance in Go.\nAs to the keeping the old API - it confuses the IDE autocomplete enough to make it very annoying to work (at least with my schema). But this can be addressed by a flag, I suppose.\nBecause the new compiler implementation is more or less a complete rewrite I will have to explicitly add some code for it.\n. It's not an optimization, it's language model. In Go, neither creating a struct nor passing structs by value causes any allocations. Consider the following program (https://godbolt.org/z/PW4WwJ):\n```\npackage main\nimport (\n    \"fmt\"\n)\ntype Hello struct {\n    Value string\n}\nfunc print(a Hello) {\n    fmt.Println(a)\n}\nfunc main() {\n    a := Hello {\n        Value: \"Yeah, baby!\",\n    }\n    print(a)\n}\n``\nNo allocations will happen here apart fromruntime.convT2Ebefore the call tofmt.Println`.\n. I have brought back the original builder API, even though I think it is way to cumbersome for any practical use.\nThe flatbuffer strings are still treated as Go strings unconditionally, because pretending that \"we can avoid that allocation by using a byte slice instead\" is merely a fig leave, hiding no naked truth. :-). @aardappel  @binary123's remark has nothing to do with the issue at hand, because escape analysis is not applicable to structs passed by value. In Go, not every value is a reference - it's not Java by any measure.\nByte arrays are already allowed: people can just use [ubyte] in schema if they feel like it. Adding a switch to a compiler result in a horrible mess of code for no good reason (I've actually tried it already).\nAs to the benchmark (the only applicable one is BenchmarkBuildGold-4) it ends up exactly the same (and I imagine the generated machine code is also quite similar).\nOriginal (after tweaking for parent name space issue):\nBenchmarkBuildGold-4                     5000000               957 ns/op         233.91 MB/s            0 B/op          0 allocs/op\nMine:\nBenchmarkBuildGold-4                     5000000               947 ns/op         236.48 MB/s            0 B/op          0 allocs/op\n````\n// BenchmarkBuildGold uses generated code to build the example Monster.\nfunc BenchmarkBuildGold(b *testing.B) {\n        buf, offset := CheckGeneratedBuild(b.Fatalf)\n        bytes_length := int64(len(buf[offset:]))\n    reuse_str := \"MyMonster\"\n    reuse_test1 := \"test1\"\n    reuse_test2 := \"test2\"\n    reuse_fred := \"Fred\"\n\n    b.SetBytes(bytes_length)\n    bldr := example.BuildMonster(flatbuffers.NewBuilder(0))\n    b.ResetTimer()\n    b.ReportAllocs()\n    for i := 0; i < b.N; i++ {\n            bldr.Reset()\n\n            str := bldr.CreateString(reuse_str)\n            test1 := bldr.CreateString(reuse_test1)\n            test2 := bldr.CreateString(reuse_test2)\n            fred := bldr.CreateString(reuse_fred)\n\n            bldr.StartInventoryVector(5)\n            bldr.PrependByte(4)\n            bldr.PrependByte(3)\n            bldr.PrependByte(2)\n            bldr.PrependByte(1)\n            bldr.PrependByte(0)\n            inv := bldr.EndVector(5)\n\n            bldr.Start()\n            bldr.AddName(fred)\n            mon2 := bldr.End()\n\n            bldr.StartTest4Vector(2)\n            example.CreateTest(bldr.Builder, 10, 20)\n            example.CreateTest(bldr.Builder, 30, 40)\n            test4 := bldr.EndVector(2)\n\n            bldr.StartTestarrayofstringVector(2)\n            bldr.PrependUOffsetT(test2)\n            bldr.PrependUOffsetT(test1)\n            testArrayOfString := bldr.EndVector(2)\n\n            bldr.Start()\n\n            pos := example.CreateVec3(bldr.Builder, 1.0, 2.0, 3.0, 3.0, 2, 5, 6)\n            bldr.AddPos(pos)\n\n            bldr.AddHp(80)\n            bldr.AddName(str)\n            bldr.AddInventory(inv)\n            bldr.AddTestType(1)\n            bldr.AddTest(mon2)\n            bldr.AddTest4(test4)\n            bldr.AddTestarrayofstring(testArrayOfString)\n            bldr.Finish(bldr.End())\n    }\n\n}\n````. @rw\n\n\nRight\n\n\nI have not introduced any new allocations, apart from the object api\n\n\nYou can not construct a valid go application without paying for string or slice field value allocation. Because, either you reuse the main buffer for the message -  this forces allocation for a field value copy (no matter if string or slice) or you don't reuse the main buffer, so you need to allocate it anew for each message (consider your own test code for what it takes).\n\n\nAt any rate, I shall not pursue any further discussions on this topic. Feel free to drop this PR.\n. This is one of those things where a slight loss of efficiency is way outweighed by the increase in usability. I mean, breaking your fingers casting all the strings around to byte slices is hardly worth the few nanoseconds  of efficiency gain, if at all (I'd imagine this is very dependent on the actual schema and it's use in the particular program).. I am breaking the API, that's the whole idea. The present API is bordering on unusable outright (and my schema is not even that complex). \nThe same applies to the generated code, which is also quite different.\nMy problem is, I want to use (for now, at least) flatbuffers to interface with preexisting c++ stuff. That's why I can't take \"baby steps\" on this.\nI do appreciate that the interim result will probably be non-mergeable from your perspective. However, if there's an interest in FBs on Go (there was not so much until now), this PR may be a useful place to discuss the more appropriate changes to the main FB library.\n. Having multiple implementations of whatever language is not the most desirable outcome, yes, but it is definitely not unheard of. After all, IDL libraries are just a glue for the much larger applications, and those application may exhibit a rather different trade off profiles, especially when managed memory languages are concerned, where direct casting of byte ranges to application values is not available.\n. @rw Of course they require it. However, if we look at the application level, byte slices are usually a not a substitute for strings anywhere apart from C/C++. So yes, the IDL library can pretend to save a bit of effort not doing that allocation, but the library user will have to construct the string first thing upon returning from the call, with that allocation and all.\nThe performance/allocation savvy users can always use [byte] in critical places, but considering that string is probably the most used (and by a big margin) data type in Go API's they are not going to get too far with it.. ",
    "tira-misu": "I signed it!. I will pick up @fbenkstein implementation and merge it into mine. It seams that he has no time for it. I wasn't aware that someone had already taken on the subject.. Using existing bfbs in example now.\nUsed/fixed existing test of @fbenkstein. (And found/fixed some issues in deserialization). fixed.. Code is now Google C++ Style Guide conform.. Damn IDE. It has changed some points of its own. I hoped that I reverted all, but somehow i missed something. I will look through all changes again an fix it.. Compromise: removing this option and allow both file formats. Binary will be verified against both verifier and right one will be choosen. What's your opinion?. ",
    "acthp": "just removing the attribute from base.h, for now, since it seems to be a debug option. @tingxingdong like so:\ndiff --git a/include/flatbuffers/base.h b/include/flatbuffers/base.h\nindex 3166602..87e56d3 100644\n--- a/include/flatbuffers/base.h\n+++ b/include/flatbuffers/base.h\n@@ -213,7 +213,7 @@\n #if defined(__clang__)\n   #define __supress_ubsan__(type) __attribute__((no_sanitize(type)))\n #elif defined(__GNUC__) && (__GNUC__ * 100 + __GNUC_MINOR__ >= 408)\n-  #define __supress_ubsan__(type) __attribute__((no_sanitize_undefined))\n+  #define __supress_ubsan__(type)\n #else\n   #define __supress_ubsan__(type)\n #endif\nmakes the preprocessor expand __supress_ubsan__ to empty string, instead of expanding it to __attribute__((no_sanitized_undefined)).. ",
    "tingxingdong": "I am blocked by this bug, too. Can you fix it in the github ? or can you tell the specific steps of fixing it? I do not understand how to \"remove the attribute\" from bash.h. \nThank you . ",
    "pdillinger": "Working through the CLA 12 step program. ;). I signed it!. ",
    "cklsoft": "I found this assert is casued by the fllowing:\nauto data_vec = fbb.CreateVector(data_buf, byte_len);\nCan I change to the following?\nconst std::vector data_vec(data_buf, data_buf + byte_len);. Thanks for your reply:D\nMy solution works. I will try your suggestion.. ",
    "barreyra": "I am sorry, false alarm, my build was picking up an older version of the generated header file from before I had added the (key) attribute. Works perfectly.. ",
    "drewqb": "You can close out issue it was pilot error on my part, my lua code was wrong it is working perfectly.. ",
    "robagar": "CLA signed. ",
    "langchr86": "I signed it!. ",
    "quinnj": "The new Julia package manager available in 1.0 will soon support \"packages\" that don't necessarily follow the one-package-per-repository setup, so I think it's fine to put the code here and we can just maintain both FlatBuffers.jl and this until we feel comfortable removing the FlatBuffers.jl repository. Also, I don't think any of @dmbates code is still around; the repository has been licensed as MIT since the beginning, so I don't think there should be licensing concerns.. We can certainly go forward w/ a better design here; even if it means making breaking changes. The path forward would be to implement the changes, and put package version limits on downstream packages; then to go the extra mile, do pull requests for downstream packages to switch to the new APIs and then they can do new releases requiring the new FlatBuffers version. Happy to help navigate all that.. @rjkat, I'm happy to help how I can here; I'm not quite following the details of the issue however. In particular, I'm not sure what exactly you mean when you mention \"singleton class\" in Julia; happy to correspond over email if that's easier, or on the JuliaLang slack for quick back and forth if that would also be easy. It would probably be best to maybe share how the \"ideal\" code layout would look and then point out what exactly Julia can't handle and then we can brainstorm the best approach (and as mentioned, I can definitely ask around as well, I'm close with a number of core devs).. ",
    "ExpandingMan": "Hello all, is there any hope of this being resurrected?. ",
    "niicojs": "I think it's caused by a bad implementation on the server as it shouldn't be a problem from what I understand.\nThe real app is doing serialisation in C++, apparently my modification generates something closer to what's beeing done in the c++ lib.\nI didn't investigate much further, but this allow me to fix my issue so it might benefits other.\n. ",
    "raulchen": "@aardappel thank you for replying!\nIn my case, bar is actually a 2-d array of ubyte. Since flatbuffers doesn't support multi-dimensional arrays, it seems that I'll have to define a separate Table. E.g.,\ntable Bar {\n   data: [ubyte];\n}\ntable MyTable {\n   bar: [Bar];\n}\nIt could work, but would be a little too clumsy.\nI think since we already have the asByteBuffer api for single-string fields. It should make sense to also provide the same api for string lists as well?\nThanks!. Thanks. In my case, all the strings do have the same size. However, the table is now shared between C++ and Java. For C++, it's pretty natural to define such a data structure with [string]. Also, there is a bunch of existing C++ code that depends on this definition. If I convert the field to a flat 1d array, I'll have to change all the code and such usage is less intuitive for C++. That's why I prefer providing a asByteBuffer API for Java. \n. ",
    "tujian": "@binary132 Very helpful, thank you! What you proposed is a simple yet practical protocol for my case.. ",
    "henrywoo": "I just submitted a PR, can you please take a look? Thanks.\nhttps://github.com/google/flatbuffers/pull/5100\n. This PR should be able to fix it:  https://github.com/google/flatbuffers/pull/5100. Without this change, I got compile errors at grpc cpp generator code:\n/tmp/flatbuffers/grpc/src/compiler/cpp_generator.cc: In function \u2018void grpc_cpp_generator::PrintHeaderClientMethodCallbackInterfacesEnd(grpc_generator::Printer*, std::map<std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char> >*)\u2019:\n/tmp/flatbuffers/grpc/src/compiler/cpp_generator.cc:606:43: error: unused parameter \u2018vars\u2019 [-Werror=unused-parameter]\n     std::map<grpc::string, grpc::string>* vars) {\n                                           ^~~~\n/tmp/flatbuffers/grpc/src/compiler/cpp_generator.cc: In function \u2018void grpc_cpp_generator::PrintHeaderClientMethodCallbackStart(grpc_generator::Printer*, std::map<std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char> >*)\u2019:\n/tmp/flatbuffers/grpc/src/compiler/cpp_generator.cc:622:43: error: unused parameter \u2018vars\u2019 [-Werror=unused-parameter]\n     std::map<grpc::string, grpc::string>* vars) {\n                                           ^~~~\n/tmp/flatbuffers/grpc/src/compiler/cpp_generator.cc: In function \u2018void grpc_cpp_generator::PrintHeaderClientMethodCallbackEnd(grpc_generator::Printer*, std::map<std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char> >*)\u2019:\n/tmp/flatbuffers/grpc/src/compiler/cpp_generator.cc:669:43: error: unused parameter \u2018vars\u2019 [-Werror=unused-parameter]\n     std::map<grpc::string, grpc::string>* vars) {\n                                           ^~~~. I just realized this need support from gRPC code so that I submitted two PRs: https://github.com/grpc/grpc/pull/17603 and https://github.com/grpc/grpc/pull/17605. The problem is ByteBuffer doesn't provide a way to get its underlying buffer. To make things working in an efficient way, either of the two PRs need to be approved.... ",
    "cltsang": "Turns out it's my bad.\nI have to read it like\nouter.inner(new Test.TypeA()).valueA(). ",
    "shevek": "If we have length-prefixed bytebuffers, calling  ByteBufferUtil.getWithoutSizePrefix() allocates a new ByteBuffer wrapper (yes, I know it's just a pointer, but it's still a 80-ish byte allocation) and we can't afford this. We need to allocate exactly one flatbuffer wrapper and make it point at an arbitrary size-prefixed ByteBuffer WITHOUT doing any allocation.\nRight now, we have the following objects:\nByteBuffer (data, points to entire region)\nByteBuffer (pointer, points to sub-region of data containing our object, size-prefixed)\nFlatBufferObject (initialized to data in pointer).\nWe COULD use one extra statically allocated ByteBuffer to point to a subset of the pointer buffer, but right now, we'd rather call __init with the table offset because it seems neater and uses less memory total.. For getByteBufferPosition(), it's to do with implementing something like max() when the FlatBufferObject is a flyweight, as presumably intended. To clone a flyweight, we need to clone the internal state, and that means we need access to it.\nAssume we have an underlying ByteBuffer (data), and some caller routine has a statically allocated FlatBufferObject. A callee subroutine wants to keep a pointer to FlatBufferObject, but it cannot do so because the caller is going to mutate that object after the return. So the callee needs to clone the internal state of the given FlatBufferObject into its OWN statically allocated FlatBufferObject, but it can't, because it can't get the offset-pointer.\nSuch a callee is, for instance a min() routine, which wants to keep the minimum of a set of objects it's being called with, but the argument is always a flyweight. . clone() would require an allocation. We already have the object wrapper allocated. Consider:\nByteBuffer data = ...\nByteBuffer slice = data.duplicate();\nFoo foo = new Foo();\nint offset = 0;\nwhile (offset < data.size()) {\n   int length = data.getInt();\n   slice.offset(offset);\n   slice.limit(offset + length());\n   foo.__init(slice, 0);\n   process(foo);\nOK, so we can iterate a ByteBuffer and read Foo objects out of it without allocation. So far, so good. Now, let's have:\nclass MaxOfFoo {\n    private final Foo max = new Foo();\n    private final Comparator<T> comparator;\n    void process(Foo value) {\n        if (comparator.compare(value, max) > 0)\n              max.__init(value.getByteBuffer(), value.getByteBufferPosition());  // <-------- THIS METHOD\n    }\n}\nThat method does not exist. We can't get the buffer out of an existing object without reflection. We don't know which buffer a given Foo came from.. Also: Defining __init in the super-interface would make this be MaxOf<T> not a special MaxOfFoo per type.. ",
    "maxpaynebupt": "\nIt would make the library much easier to use if:\n\n__init(int, ByteBuffer) were defined in Table and Struct, not the specific subclass. This would actually make the codebase smaller overall.\nThere was a getByteBufferPosition() { return bb_pos; } method - this would make it possible to clone an object-wrapper.\n\nOther stuff which would be \"nice:\n\nA superinterface of Table and Struct for the purpose of common methods (mostly ByteBuffer stuff)\n\nUse case:\n\nWriting an iterator over a large ByteBuffer of an unknown flatbuf type. I can do it, but my iterator has to be parameterized with a ton of lambdas, and I need an extra weird hack to get bb_pos.\n\n\nhi, could u please give some demo in your snmp4j project ? thanks.. ",
    "tobifx": "I signed it!. Hi,\nthanks for the fast response.\nMy Test.fbs is only a example. I know that is no useful object. That should only demonstrate what the generator do.\nThe problem is that no 'Create...' methode for tables that include one or more struct types in the schema are created.\nSo it is very difficult create such flatbuffers objects. If you look in the C++ generated code the 'Create...' methods will be generated.. Yes, that is right. The Create... method is like a copy constructor.. And you are right. I should use the monster_test.fbs for test this functionality.. The reason was that I would reuse the WrapInNameSpace(...) code.\nBut I could also do it locally.. Ok you are right. I do that in the idl_gen_general.cpp. ",
    "dertom95": "Ok, thx for the fast reply. I will think of some solution and might send a PR some day.\nThe  'structs and garbage' was an assumption of mine so I will need to dive a bit more into c# and memory. Thanks for pointing that out ;)\n. ",
    "alirazeen": "I signed it!. @aardappel : A second accessor makes sense. I'll add that. As for nil and \"\", an alternative is for the accessor to have a return type of *string and return nil if the string is not present. \n. Actually, having tried it out, I don't see the value of a separate accessor with a *string return type. I was going for making the generated code nicer to use. However, if one has to check for nil anyway, one might as well just use the original getter. I will close this PR.. ",
    "pyottamus": "@rw I have added the tests.. @rw I made all the requested changes. @rw, @aardappel The problem is\n\nTo allow special charectors in file_identifiers, said special charectors must be escaped.\nThe faster means to acomplish this is sprintf and hex escape.\nMSVC warns sprintf unsafe and the tests fail\nMSVC does not have snprintf in versions prior to 2015\nI cant get _snprintf_s or snprintf_s to be found\n. @rw  I have removed the scanf code and replaed it with IntToStringHex as per @vglavnyy  suggestion. @vglavnyy That doesn't handle other edge cases like quotes or hex escaped strings, such as \\x00, which are allowed in accordance with the grammar. Furthermore, if hex escaped strings are passed, they are received by the idl generators as raw std::strings in parser_.file_identifier_, with all escaping removed. It is simply much easier, and more efficient, to escape all characters. \n\nThe only alternative would be either a conditional encoding using a std::set or something, or a jump table, which would be 256*5 bytes large(with null terminated strings and a fixed size offset of five). > \n\nthe schema compiler will currently accept any bytes for file_identifier (since it is a string constant where escape characters are permitted), but the intent was for it to only be printable ascii characters (32..126).\nThe reason being that when such an identifier is missing, the memory being tested may contain the start of a vtable (a 16bit vtable size & object size). With the smallest ascii characters, you'd need a vtable size >= 0x2020 (> 4000 fields) before you run the risk that BufferHasIdentifier would accidentally return true for the wrong kind of buffer. This is still an issue, I guess we could say that the use of the file_identifier feature is not safe for users of really big tables. This is bad design, caused by adding this feature to the format retroactively. It should not bite anyone in practice though.\nSo in summary, I'd be in favor to restricting file_identifier to printable ascii where possible. I'd even be in favor in retro-actively adding this to flatc, maybe at first as a deprecation warning, just in case anyone actually uses this.\n\n@aardappel \nIn new versions of flatc, you could add a default file_identifier of \\0\\0\\0\\0, which, by also only allowing printable ASCII characters, would be completely unambiguous.. @aardappel It would be compatible, since file_identifiers are already compatible, since the root offset simple increments by 4. Furthermore, it would only be an additional 4 bytes, and it could be fewer, for example, by making the default file_identifier a single \\0, which would still be a valid flatbuffer as long as the root offset was incremented by 1, excluding any padding issues. Although it would not be a file_identifier, it would still be unambiguous, since file_identifiers can't contain \\0, and the first 4 bytes would.. @rw Are there any changes I have missed?. @rw @aardappel He said it could be left in. As far as i am concerned, there is consensus.. Thanks for the suggestion. ive implemented it.\nhttps://github.com/pyottamus/flatbuffers/blob/618c9bf64d275781288fd06b62a8299ac964cb39/src/idl_gen_python.cpp#L555-L576. I made the change. @rw I fixed this. @rw I added this test. @rw i found a couple more occureces of this in different files, including the code generated by idl_gen_python.cpp. I have fixed them.. number_types.*Flags.bytewidth defines the size, in bytes, of that type. Acording to the standard, the size prefix and root table offset are uoffset_t. The standard says that currently this is uint32_t, implying this could change.. @rw I fixed this. @rw Are you referring to\n offset+=(number_types.UOffsetTFlags.bytewidth if size_prefixed else 0) + number_types.UOffsetTFlags.bytewidth.\n",
    "frol": "@aardappel PRed #5127. ",
    "agearhart": "I included the grpc headers because https://github.com/google/flatbuffers/blob/master/include/flatbuffers/grpc.h appears to rely on them.  Do you have any ideas on how I could integrate FlatBuffers over gRPC into Unreal?. I found another project that gets me closer, https://github.com/vizor-games/InfraworldRuntime.  Thanks for the help!. ",
    "JasonJiang91": "Hi, I got the same eror information when I tried to make install the flatc\ngcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\n[  1%] Building CXX object CMakeFiles/flatc.dir/src/idl_parser.cpp.o\n/home/jjsjiang/project/flatbuffers/src/idl_parser.cpp: In member function 'flatbuffers::CheckedError flatbuffers::Parser::Next()':\n/home/jjsjiang/project/flatbuffers/src/idl_parser.cpp:416:9: error: this statement may fall through [-Werror=implicit-fallthrough=]\n         }\n         ^\n/home/jjsjiang/project/flatbuffers/src/idl_parser.cpp:418:7: note: here\n       default:\n       ^~~~~~~\ncc1plus: all warnings being treated as errors\nCMakeFiles/flatc.dir/build.make:75: recipe for target 'CMakeFiles/flatc.dir/src/idl_parser.cpp.o' failed\nmake[2]: *** [CMakeFiles/flatc.dir/src/idl_parser.cpp.o] Error 1\nCMakeFiles/Makefile2:72: recipe for target 'CMakeFiles/flatc.dir/all' failed\nmake[1]: *** [CMakeFiles/flatc.dir/all] Error 2\nMakefile:140: recipe for target 'all' failed\nmake: *** [all] Error 2\n. ",
    "dwaynebradley": "Thanks @vglavnyy! I manually changed line 417 to [[fallthrough]]; and it worked perfectly on Fedora 29.\n. ",
    "find": "Signed it. ",
    "thpotier": "Hi \nThanks for your response, but i'm a bit lost.\nMy auto generated c# class has only a one method to finish the serialization : \nFinishMYCLASSNAME(FlatBufferBuilder builder, Offset<MYCLASSNAME> offset) { builder.Finish(offset.Value); }\nI can see here that this should exist  :\n/// <summary>\n        /// Finalize a buffer, pointing to the givenrootTable.\n        /// </summary>\n        /// <param name=\"rootTable\">\n        /// An offset to be added to the buffer.\n        /// </param>\n        /// <param name=\"fileIdentifier\">\n        /// A FlatBuffer file identifier to be added to the buffer before\n        ///root_table.\n        /// </param>\n        public void Finish(int rootTable, string fileIdentifier)\n        {\n            Finish(rootTable, fileIdentifier, false);\n        }\nI guess that it is what you told me about. \nIf i understand well i should be able to call \"Finish()\" and give it a fileIdentifier, than later test this file identifier with a MYCLASSNAMEHasIdentifer method right ?\nBut my generated c# class doesn't have a MYCLASSNAMEHasIdentifer method neither...\nShould those method appear in my generated c# file ? Or am i completely missunderstanding the process ?\n. ERRATUM : I didn't understand that i should specified a fileIdentifier in in the shema. Since it wasn't used in the example we found, my colleague and i decided to not add it. Now we understand the situation better ^^ \nI added it, than i regenerated my c# file and now i have access to those method.\nThanks for your help :). ",
    "g-arjones": "Ok... This is good to hear, I guess it is acceptable to have the the initial memory allocated for the builder in the heap if that will be reused between Clear() calls.. \nNow that you mentioned the extensive use of std::string, it made wonder if I should be worrying about the receiving side as well. I mean, the whole point of FlatBuffers is to access the data without having to copy/decode the message beforehand. Now, how does that work for std:string's and other non-primitive fields? Is the data copied from the receiving buffer so that the object is constructed before I can actually use the data?\nMy platform is an ARM MCU with 512kb flash (for the application) + 192kb RAM.  . Ok.. Thanks for the insights. Pretty helpful! \ud83d\udc4d . ",
    "HenryRLee": "@aardappel I thought the original author was trying to use the word specification instead of specialization, so that when the type argument is a signed number but is none of char or int, the error message would show. But anyway, I can keep the original message if I understood incorrectly.. I see. I've unchanged the message, fixing the typo only.. I think you're right, I should be using *(iter_ - 1). But I also need to add a const keyword on the VectorIterator::operator-(const uoffset_t &) function to make it work.\nI'll make the changes asap.. I've updated my PR. Thanks for the comment.. ",
    "westfly": "solved as blew\nif (elemobjectdef->is_struct()) {\n        int tok_num = tok.get_tok_num();\n        auto element_size = elemobjectdef->bytesize();\n        builder->StartVector(tok_num, element_size);\n        for (int i = tok_num - 1; i >= 0; --i) {\n          flatbuffers::uoffset_t element_offset;\n          ProcessFieldTable(builder, schema, elemobjectdef, tok[i],\n                              &element_offset);\n        }\n        *offset = builder->EndVector(tok_num);\n}. ",
    "KidyovrS": "Thanks!. ",
    "naure": "For anyone bumping into this issue, here a quick fix to add to your build.rs.\n```rust\n            // Fix an issue in generated code.\n            // The lifetime 'a should be on the return value, not on &self.\n            // See https://github.com/google/flatbuffers/pull/5140\n            {\n                let file = &Path::new(\"src\").join(\"gadget_generated.rs\");\n                let code = std::fs::read_to_string(file).expect(\"could not read file\");\n            let re = regex::Regex::new(\n                r\"pub fn (\\w+)_as_(\\w+)\\(&'a self\\) -> Option<(\\w+)> \\{\"\n            ).unwrap();\n            let fixed = re.replace_all(\n                &code,\n                r\"pub fn ${1}_as_${2}(&self) -> Option<${3}<'a>> {\",\n            ).to_string();\n\n            let re2 = regex::Regex::new(\n                r\"\\(&self\\) -> Option<flatbuffers::Vector<flatbuffers::ForwardsUOffset<\"\n            ).unwrap();\n            let fixed2 = re2.replace_all(\n                &fixed,\n                r\"(&self) -> Option<flatbuffers::Vector<'a, flatbuffers::ForwardsUOffset<\",\n            ).to_string();\n\n            std::fs::write(file, fixed2).expect(\"could not write file\");\n        }\n\ntoml\n[build-dependencies]\nregex = \"1\"\n```. Can you help deciphering the CI logs? This is what I ran on my side:\n```\n\u279c make test\nRunning tests...\nTest project /home/a/dev/flatbuffers\n    Start 1: flattests\n1/1 Test #1: flattests ........................   Passed    0.06 sec\n100% tests passed, 0 tests failed out of 1\nTotal Test time (real) =   0.06 sec\n\u279c cd tests && sh generate_code.sh\n\u2026\n../flatc: warning: GRPC interface generator not implemented for Rust\n\u2026\n```. I believe the same problem exists here:\nhttps://github.com/naure/flatbuffers/blob/cfd24be7c1f0c3985f4fa7f2d8b6896439a14f4a/src/idl_gen_rust.cpp#L1030\nIt should be:\n-> Option<flatbuffers::Vector<'a, flatbuffers::ForwardsUOffset<\nBut I'm not familiar with the code at all so I can't tell why this case is different than the others that do have the lifetime at the right place.. @rw In CI:\ncould not download file from 'https://static.rust-lang.org/dist/channel-rust-stable.toml.sha256'. ",
    "harshshah903": "https://stackoverflow.com/questions/54446371/systemaccessoutofbound-exception-while-trying-to-access-lengthoftable-from-fla \nThanks. @aardappel .  . I'm sorry. I was worried that community might not respond and I was working on deadline. Thank you for your support.  \ncheers! . ",
    "randomascii": "This change looks good and is needed for ARM64 Chromium on Win32. ",
    "AlexDenisov": "Thank you for clarification, @aardappel!. ",
    "sheinbergon": "I encountered a similar behavior (empty field objects instead of thrown exception)\nI can confirm upgrading to the latest master with this fix had resolved the issue.\nAs I'm reliant on this feature, and as there are no snapshots published, I'll build the jar and host it in my own nexus repository. \n@aardappel any chance for a new stable release anytime soon?. ",
    "jaynus": "Done and passed.. As per the commit note, added a small test addition to roundtrip_vectors, which is implemented in the prop function which creates and tests a vector of a given type. this means the iterator is tested for every prop generation test cycle (every time used in roundtrip vectors). Closing these in favor of a single rust overhaul commit.. Closing these in favor of a single rust overhaul commit.. ",
    "TethysSvensson": "I don't think this works as expected with regards to lifetimes.\nSpecifically, the items returned by the iterator has a lifetime limited by the borrowing of the vector, instead of by the lifetime argument in the vector itself.\nI would expect all four of these functions to compile (and give the same result):\nhttps://gist.github.com/TethysSvensson/2615a8a1aff7f243fda78c3c1e1ced51\nIn the current implementation, only the first and last one compiles.\nI think part of it is that Vector does not implement Copy and Clone even though it easily could.. I think this PR should include something along the lines of this to work as expected:\nhttps://github.com/TethysSvensson/flatbuffers/commit/cb7d088f346dd5341b72a89daba2156465a20070\n(This commit comes from a branch we use at my work. It contains a cherry-pick of your commit, but also adds additional derives to the generated enums that we don't expect to get upstreamed.). ",
    "mockbutler": "I signed the CLA.. @vitalyisaev2  The switch FLATBUFFERS_PACKAGE_DEBIAN must be set to true. This was the preexisting behavior.\nmarc@heavymetal:~/snowflake/flatbuffers$ cmake -DFLATBUFFERS_PACKAGE_DEBIAN=TRUE\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for strtof_l\n-- Looking for strtof_l - found\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/marc/snowflake/flatbuffers\nmarc@heavymetal:~/snowflake/flatbuffers$ cpack -G DEB\nCPack: Create package using DEB\nCPack: Install projects\nCPack: - Run preinstall target for: FlatBuffers\nCPack: - Install project: FlatBuffers\nCPack: Create package\nCPack: - package: /home/marc/snowflake/flatbuffers/flatbuffers_1.10.0-92_amd64.deb generated.\n. @vitalyisaev2 I'm happy to withdraw the merge request deferring to the Fedora packaging. I may well need to release from master, but to a tiny audience.. I don't believe there are any negative consequences. I have a windows packager configuration that needs to be polished up for a merge request after I have fed back all the changes from this request.. This was cribbed from the DEB package .cmake. Is there a suitable catchall email? I'm unclear how to refer to the Google Group in this entry.. I've taken the summary from the README.md stripped of Markdown. The full readme would be excessive for description. In this case less is more.. ",
    "aleixpol": "I signed. #4753 got fixed because make install now installs it. That's how I understood it at least.\nTo me it was a predictability issue, I had to look into the code to see that if it wasn't CMAKE_BUILD_TYPE=Release it wouldn't install, which I haven't seen in any other project.\nAbout #5050 I think it makes sense although I'm not using pkgconfig or conan on my projects so it doesn't really make a difference to us.. ",
    "tannergooding": "I had the same idea as @eerhardt for #3. That is, expose and use Memory<T>. Additionally, you can expose a ref struct Name { private Span<T> _data; }. The ref struct would then expose methods like PutDouble.\nThis ensures that:\n The code plays nicely with the GC when the backing data is managed\n The code works for native backed memory\n* Users who need the perf can avoid the Memory<T> to Span<T> calls by just caching it.\nFor convenience (but at the cost of \"exploding the surface area\" a bit), you could also expose the methods (e.g. PutDouble) on the main type (rather than just the ref struct). It could then just forward to the ref struct implementation so you don't have code duplication.. ",
    "Tsiddiqui123": "ok thanks for the response ..\n. ",
    "stefansjs": "Yeah. I'll take a look. It might take me a few days since I'm under a\ndeadline right now.\nOn Thu, Feb 14, 2019, 16:00 Wouter van Oortmerssen <notifications@github.com\nwrote:\n\nLooks like the C++ generator isn't checking the deprecated flag for these\nfields.. can you make a PR with a fix?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/5186#issuecomment-463852104,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AFnny3vIc6ETczPihv6oPrVTL00QWwhzks5vNfiCgaJpZM4a8d-2\n.\n. \n",
    "kewur": "I signed it. you're right, the .DotSettings file was coming from Jetbrains, I removed the file.\nThe csproj is empty because .net core projects are inclusive rather than exclusive like the .Net Framework projects. So it has all the files already included in the project folder.\nIf you'd like to test it, you can install .net core sdk from here https://dotnet.microsoft.com/download\nand run \ndotnet build FlatBuffers.Core.csproj in the folder.. Thank you Eric, will check it out. ",
    "fweikert": "Btw, we have a platform called \"rbe_ubuntu1604\" for testing the integration with remote execution (e.g.https://raw.githubusercontent.com/bazelbuild/continuous-integration/master/buildkite/pipelines/bazel-remote-execution-postsubmit.yml). ",
    "incubus370": "Offset of each field depends on vtable position, vtable size and field position inside vtable.\nvtable position and vtable size depends only on Table#bb_pos.\nI could find only three places where Table # bb_pos changes its value:\n- Table#__union.\n- Table#__reset.\n- __init method of generated Table class.\nThese values can be read once with change of Table#bb_pos and used for any field lookup. Doing this with every call of Table # _offset is redundant.\nCurrent implementation:\n```\npublic class Table {\n  protected int bb_pos;\n  protected ByteBuffer bb;\n...\n  protected int __offset(int vtable_offset) {\n    int vtable = bb_pos - bb.getInt(bb_pos);\n    return vtable_offset < bb.getShort(vtable) ? bb.getShort(vtable + vtable_offset) : 0;\n  }\n...\n}\npublic class Foo extends Table {\n...\n  public void __init(int _i, ByteBuffer _bb) { \n    bb_pos = _i;\n    bb = _bb;\n  }\n  public String bar() { int o = __offset(4); return o != 0 ? __string(o + bb_pos) : null; }\n...\n}\nProposal:\npublic class Table {\n  protected int bb_pos;\n  protected ByteBuffer bb;\n  protected int vtable_pos\n  protected int vtable_size\n...\n  protected int __offset(int vtable_offset) {\n    return vtable_offset < vtable_size ? bb.getShort(vtable_pos + vtable_offset) : 0;\n  }\n...\n}\npublic class Foo extends Table {\n...\n public void __init(int _i, ByteBuffer _bb) { \n  bb_pos = _i;\n  bb = _bb;\n  vtable_pos = bb_pos - bb.getInt(bb_pos);\n  vtable_size = bb.getShort(vtable);\n }\n public String bar() { int o = __offset(4); return o != 0 ? __string(o + bb_pos) : null; }\n...\n}\n```\n. I signed it!. I signed it!. There are a number of conceptual issues. The current API for scalar and string vectors is different from the API for structure and table vectors in access objects:\n1) All of these types are built-in and we need to manually create a vector object for each type of scalar (integer, fractional, byte, etc.) in com.google.flatbuffers, but not in the generated code.\n2) The generator must add mutability methods to access objects for scalar type vectors if the corresponding flag is set. This conflicts with clause 1. The vectors of tables and structures are immutable.\n. Or we will have to generate code for the com.google.flatbuffers package along with the code from the schema. This will be similar to the work of the C ++ compiler with templates. The file structure of the generated code will look something like this:\nfoo/bar/ - classes from schema\ncom/google/flatbuffers/ - classes for vectors of built-in types. Unfortunately, I am not an expert in C # and I am not familiar with the subtleties of a language. First I propose to add an alternative API for Java. For C#, do it separately.. @aardappel The method with the generated classes will be conflicted when several schemes are used in the same project with different values of the --gen-mutable flag.. How is this problem solved in C ++ code?. ",
    "llongi": "What a coincidence, I was benchmarking Flatbuffers C++ -> Java for an internal app, it's just a simple table containing an array of structs (struct size 24 bytes). The C++ part generates such a message with 1000 such structs, sends it via network to Java, where it's read into a DirectByteBuffer and iterated over. If the Java code just discards the data it's receiving (effectively benchmarking the C++ generation, serialization and network output, as well as the Java network input) I get about 112M (millions of structs processed per second). If I add the parsing and iterating of the struct in Java performance drops to 51M. With the simple change detailed above it goes up to 62M. That's a measurable ~ 20% increase, at least for my one test case, which is heavy in iteration and thus calls to __offset().\nMy two cents: this seems worth it.. That's the other solution and is fine by me too. I'll update the PR tomorrow to use and document the empty requirement properly. I'll also document that the custom class does need to have a constructor or assignment operator taking std::string, given that FB->str() is always used to construct it in Unpack(). That is somewhat unfortunate, as it means you cannot make a zero-copy-construction version of your own string class, and you always incur at least two copies (once to build the std::string, and once to copy the memory to your own implementation, since you cannot safely move the underlying storage from std::string). I was thinking of proposing to change that ->str() call to be ->string_view() (if HAS_STRING_VIEW), as that would, at least for recent C++, allow all problems above to be nicely solved. But that would break the current API, as any custom string class would, right now, implement a constructor from std::string, but not necessarily from string_view, and since there's no automatic string_view to std::string conversion, implementors would have to take action.\nThoughts on this? It's not a crucial optimization or use case for me, but I did take note of the above possible limitations.. I could add an option to do that, but what would it be concretely?\nA) custom_str(e->string_view()) is the cleanest approach, but only C++14/17 and up. Could be ifdef HAS_STRING_VIEW, but still needs an alternative for older language standards.\nB) custom_str(e->c_str(), e->length()) is basically the portable version.\nC) custom_str(e), where e is a flatbuffers::String directly, the most flexible approach.\nPersonally I would go for C, as that gives the maximum flexibility: the implementor can then decide which way to access the flatbuffers::String is appropriate for his use case.. True, didn't think of the dependency on flatbuffers.h. Let's go for B then.\nSuggestions for the switch/attribute name? --cpp-str-flex-ctor, --cpp-str-no-string-ctor, .... There it is.\n- Updated all the documentation to mention empty() and construct from std::string to be a requirement.\n- Added --cpp-str-flex-ctor switch and cpp_str_flex_ctor option attribute to use custom_string_constructor(e->c_str(), e->size()) instead.\n- Fixed bug with vector of custom string types (present in current code). CreateVectorOfStrings() was always used with string types, but required std::string, which fails to compile if the type is a custom string type instead. Switching to CreateVector() with function variant does the same operation with the same performance, but works with all types of strings.. Also just a quick note: the first commit is just a clang-format run on idl_gen_cpp.cpp., to ensure any subsequent changes were properly formatted. I hope that's not a problem, at least it helps in having consistent formatting in the file again.. Thank you! And I didn't know about git clang-format, looks like a useful tool.\nBut somehow I can't find the code showing up in google/flatbuffers:master, even if here it says you merged it to it? Is there some delay or weird git at work?. That's perfectly fine. Thank you!. PR #5242 should fix this. Same issue as #5240.. PR #5242 should fix this. Same issue as #5238.. Fixed.. True, FB always has a uint32_t for length(), so that's why I suggested it. Using size_t also works and is probably more natural and what people will have implemented already.. Done.. Okay, now if type is std::string the normal CreateVectorOfStrings() is used, in all other cases the function.. In the first clang-format run I did, I disabled the enforcement of 80 chars lines, mostly because as you can see from the new commit 4791923806965637d5b13f7003329bfbb2fdf18b it formats a lot of code. But now it's fully compliant with the provided clang-format file, which it wasn't before. It's a good thing, I just wasn't sure if you'd accept this kind of changes in a PR.. Yeah, clang-format seems to be confused about how to properly align the comments themselves while inside a block. Just putting the on/off comments outside the block makes everything work consistently, fixed in 117e3b0679209f2aa55cbee18c4036e7da4bd4b3.. Done. VectorAccessor() takes care of this now.. Yeah true, the lookup code for ptr, string and vector type is the same, I've abstracted that away into FieldStringLookup().. In theory it's never null, I just took the check over from the other similar functions.\nI don't think it's a problem to have it there, so I've kept it once in FieldStringLookup(), just to be safe.. ",
    "dstoiko": "If anyone has the same issue: actually just using\njs\nimport 'flatbuffers'\nin the destination file worked as it takes the ESM version (.mjs) so no need for require in modern nodeJS apps.\nMaybe it should be added to the JS tutorial on the website, as for now it only mentions requireJS and browser <script/> imports.\nAlso, if you use StandardJS, this import statement will make ESLint complain about the fact that the flatbuffers variable is not declared. Disabling the rule inline or making flatbuffers a global fixes it:\njs\nlet buffer = new flatbuffers.ByteBuffer(data) // eslint-disable-line no-undef\nor:\njs\nimport 'flatbuffers'\n/* global flatbuffers */. ",
    "rida914-4": "I have debugged the code and displayed the value of self.loc at each point for a single field extraction. It seems that the self.loc parameter is not set correctly.\n\nHere vtable is given self.loc 50\nORC WIPOffset(8, PhantomData)\nBUF [12, 0, 0, 0, 0, 0, 6, 0, 8, 0, 4, 0, 240, 240, 240, 240, 0, 0, 0, 65]\nMONSTER Monster { _tab: Table { buf: [12, 0, 0, 0, 0, 0, 6, 0, 8, 0, 4, 0, 240, 240, 240, 240, 0, 0, 0, 65], loc: 12 } }\nslot_byte 4, self.loc 12\nself. loc 12\nHere vtable is given loc 252645148\nv table get byte_loc 4, self.loc 252645148,  self.loc+byte_loc 252645152\nindex 252645148..252645150\nthread 'main' panicked at 'index 252645150 out of range for slice of length 20', /checkout/src/libcore/slice.rs:580\nnote: Run with RUST_BACKTRACE=1 for a backtrace.\n. I found the issue.\n\nThe vtable.rs file has to be modified.\n>>::follow(self.buf, self.loc)\nneeds to be \n>::follow(self.buf, self.loc). It was a flat buffers lib issue. I Have notified the devs. Please close the\nbug.\nThanks for the prompt response.\nOn Fri, 22 Feb 2019, 1:17 AM Robert, notifications@github.com wrote:\n\n@rida914-4 https://github.com/rida914-4 Do you think this is an issue\nin the Rust library or the Rust generated code? If you do, please upload\nyour code to a repository so that I can take a look!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/flatbuffers/issues/5199#issuecomment-466150111,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ASWFPIGRn1cNxCcW9OofHkQ8uBHcTWmGks5vPv6-gaJpZM4bE9B-\n.\n. \n",
    "bwelling": "I believe that would work for me, but I think it would cause problems when using versions of gcc that predate the definitions of __BYTE_ORDER__ and __ORDER_BIG_ENDIAN__.. More specifically, if neither are defined, they'd both evaluate to 0, which would cause any such platform without __BIG__ENDIAN__ defined to incorrectly be treated as big endian.. Something like #if defined(__BIG__ENDIAN__) || (defined(__BYTE_ORDER__) && __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__) might work, if that looks reasonable.. That doesn't seem to work.  If I add that code to <flatbuffers.base.h>, the build fails because Offset is undefined.  If I add it to <flatbuffers/flatbuffers.h>, it fails with:\n/u0/home/bwelling/work/flatbuffers/head/source/flatbuffers-1.10.0/include/flatbuffers/flatbuffers.h:33:70: error: non-type partial specialization 'WriteScalar<flatbuffers::Offset<T> >' is not allowed\n template<typename T> void WriteScalar<Offset<T>>(void *p, Offset<T> t) {\n                                                                      ^. That's still failing with the \"error: non-type partial specialization\" error.. Disabling the test works, although I think your conditional is backwards, and the next few lines also need to be removed, as they're accessing a variable (string_ptr) that no longer exists.\nI also tried the previous change (adding the specialization) with a newer compiler (gcc 8.2.0), and it failed in the same way.. I just realized that I'd badly applied the second specialization patch; I copied the forward declaration, but didn't notice that the prototype had changed.  This does build for me.\n+template<typename T> struct Offset;\n+template<typename T> void WriteScalar(void *p, Offset<T> t) {\n+  *reinterpret_cast<uoffset_t *>(p) = EndianScalar(t.o);\n+}\n+. Interesting.  When I applied your patch to 1.10 (and removed the ubsan stuff), it builds.  If I apply the patch to -head, it fails, with strange errors like:\nIn file included from /u0/home/bwelling/flatbuffers/tests/test.cpp:32:0:\n/u0/home/bwelling/flatbuffers/tests/monster_test_generated.h:773:34: error: stray '\\2' in program\n       : color(static_cast<Color>()) {\n                                  ^\n/u0/home/bwelling/flatbuffers/tests/monster_test_generated.h:791:58: error: stray '\\2' in program\n     return static_cast<Color>(GetField<int8_t>(VT_COLOR, ));. It looks like it's related to some other change between the 1.10 release and -head.  I believe I've bisected it to 4ed6fafdfa5689d72e821baec522a9d03ee6d652, but that's a pretty large change.. I'm not seeing anything that looks obviously suspicious in that code, but it's pretty dense.  I'm happy to help debug this, but am not sure where to start.. I might as well file a PR, even if additional changes are needed.. I've done some more debugging, and I think the problem is somehow related to code calling the NumToString<> templated method; for some reason, it ends up being called as NumToString<char>(2) on Solaris Sparc, and NumToString<int>(2) on MacOS (and presumably other places).  It's then writing the value to a stream, which would do the integer to string conversion in the latter case, but not in the former case.\nI haven't been able to figure out why something is ending up as a char instead of an int on Solaris Sparc (this doesn't appear to be happening on Solaris x86-64, for the record).  It also looks like there are NumToString specializations for signed char and unsigned char, and those aren't being called.  This is also confusing, but it looks like char doesn't match either of those.\nAdding an additional specialization for char is enough to get the build to complete, but I'm not sure if there are any other problems lurking.  I can submit a PR for that if it's the right solution, but it might be better for someone more familiar with the code to figure out why the types are different.. I can't imagine that Oracle would consider the behavior a bug, as it's probably how Solaris has behaved since 1985.  It sounds like adding an additional specialization for char will still work, so I'll file a PR for that.. It sounds like everyone agrees that #5208 is a reasonable fix, so could that be applied?\n@vglavnyy - I believe that sparc64 does not allow unaligned memory address.  The build and test completed, so it's possible that code isn't being executed, or isn't being executed with an address that's unaligned when executed from the test.  In any case, I think that's a separate issue.. The unaligned access may also be fixed by #5209.  That looks potentially related.. For the record, I don't think that the char issue is specific to the fact that Sparc64 a big-endian architecture; it's specific to Solaris on Sparc64.  I agree that having big-endian testing is a good thing, but I'm not sure if it would have caught that.. If that's the case, it sounds like the right fix is to conditionalize ReadScalar/WriteScalar so that the current version is used if alignment is not required, and a slower version with temporaries is used if alignment is required (and if the machine is big endian, since there's no conversion done otherwise).. Theoretically, yes.  If they were correctly handling alignment, they wouldn't need ubsan directives to disable alignment checking.  Using reinterpret_cast<> to cast between pointer types is not safe.\nIn reality, some architectures (like x86-64) don't require aligned memory accesses, and ReadScalar/WriteScalar should effectively be compiled out on all little-endian platforms.  There could be problems on big-endian platforms that do not support unaligned memory access (like sparc64), but I haven't encountered any yet.. If that is the case, then there shouldn't be any ubsan warnings related to alignment, and @vglavnyy said that there were.\nIt's probably not worth worrying about this too much unless it's causing problems.  Those problems would be crashes, so would be pretty easy to notice.. I signed the CLA.. Sigh.  Hopefully that works.. I signed the CLA again.. One more try.. And another one.. ",
    "mryan1539": "A bit more information: it seems that some numbers are not being properly rendered in the\nmonster_test_generated.h file; e.g., instead of the string '8', the literal byte with the value 0x08\nappears.. A diff between the form generated on sparc and that checked in.\nmonster.txt\n. ",
    "Quuxplusone": "@bwelling: When things unexpectedly end up as char, the culprit is usually typedef char int8_t;. And indeed I've verified in this case that is_same<int8_t, char> using the g++ on Solaris, but is_same<int8_t, signed char> on Linux.\n```\ninclude \ninclude \ntemplate\nvoid f() { puts(PRETTY_FUNCTION); }\nint main() {\n    f();\n    f();\n    f();\n}\n```\nThe only solution I'm aware of is \"file a bug with the Solaris library vendor\" and meanwhile \"stop using int8_t in your source code at all.\" To do the latter, you could introduce a flatbuffer_int8_t that's guaranteed to be a typedef for signed char; or you could just use int everywhere instead; or you could carefully wrap each int8_t value in a cast to int at its point of use.. @vglavnyy: char is already signed on Solaris; that's how it's able to be used as int8_t in the first place. The problem is that \"the char type, which is signed\" is not identical with \"the signed char type.\" On sane platforms int8_t is a typedef for signed char; but on Solaris apparently it's a typedef for \"char, which is signed.\"\nI do think it would be an excellent idea to remove the one use of the int8_t typedef in \"flatbuffers/include/flatbuffers/idl.h\" and replace it explicitly with signed char given that that's the type actually expected by NumToString. That would help. It might not fix every int8_t-related issue in the world, but it would definitely help.\n\nYou also suggested adding static_assert(std::is_same<int8_t, signed char>, \"text\");. This would indeed catch the problem on Solaris... but only by making the project fail to build. Which it already doesn't! If you want it to build on Solaris you have to make it work even when int8_t is plain char.. > Perhaps, NumToString<> should be extended to 'int8_t/uint8_t'.\nThat's essentially what adding the NumToString<char> specialization would do, yes.\nSince you don't know which of int8_t/signed char/char and uint8_t/unsigned char/char might be synonyms on any given platform, the only portable thing to do is to specialize all two of int8_t/uint8_t, or to specialize all three of signed char/char/unsigned char. You've already got specializations for signed char/unsigned char, so I think the fix is obvious:\ndiff --git a/include/flatbuffers/util.h b/include/flatbuffers/util.h\nindex 71e1973..de650e0 100644\n--- a/include/flatbuffers/util.h\n+++ b/include/flatbuffers/util.h\n@@ -124,6 +124,9 @@ template<typename T> std::string NumToString(T t) {\n   // clang-format on\n }\n // Avoid char types used as character data.\n+template<> inline std::string NumToString<char>(char t) {\n+  return NumToString(static_cast<int>(t));\n+}\n template<> inline std::string NumToString<signed char>(signed char t) {\n   return NumToString(static_cast<int>(t));\n }. ",
    "dnr": "The normal way to declare a new integral type would be\ntype MyEnum int32\nI'm no expert on Go internals, but I can't imagine any way this would have different performance characteristics. It should only affect the type checker.\nThe new type would be observable by reflection (I'm not sure if type aliases are observable), so it wouldn't produce identical binaries. And because of the stricter type checking, it could cause code that previously compiled to not compile until appropriate casts are added. But I suspect most Go programmers would prefer enums to create new types.. .2. The latency of sync.Once should be very low once it's initialized, so I wouldn't worry about that.\n.3. The reason I brought it up now is that if a map is the public api for this feature, then it has to be a map forever. If the public api is a function instead, then the implementation can freely change later.\n.5. Good idea, and I don't think you even need unsafe: you can slice strings without allocation.. 2. Right. It's hard to imagine it would be a problem in practice (and it has an easy fix), but given the philosophy of flatbuffers, I agree that sort of behavior is to be avoided.\n3. Sounds good to me.. Also, just to note: Since we know all our lengths and offsets fit in 32\nbits, we could actually do it in 16 bytes with one pointer and two 32-bit\nfields for length and position. But that would require using unsafe (as\nfar as I can tell) and would probably be harder to maintain. So this is\nsort of a middle-ground.\n\n. It would be possible to add a function like MutateInventoryBySlice(offset int, values []byte) bool that sets a sub-vector to given slice with less overhead. Actually I am using it in a loop in my application so I would use that version if it existed, though I'm dealing with small vectors so it doesn't make much difference.. It does return string values where they exist in the map. The fallback path for unknown values is based on the equivalent in stringer. It provides more information than a fixed string, but if you'd prefer a fixed string that's fine with me too (after all, it's a situation that \"shouldn't\" happen).. Any thoughts on the other comments/questions in the pr?. Sure, it's called a \"full slice expression\" in the spec. (https://golang.org/ref/spec#Slice_expressions). The third argument becomes the cap of the new slice. Table never modifies the slice it's given or reads beyond its len, so there's no need to retain the original cap.. The cap of _buf is the len of the slice that Init was originally given, so it basically returns the same slice. The only difference is that we throw out the value of the original cap, but Table doesn't need that for anything. If a caller wants to reuse the buffer with the original cap, they can just hold on to a reference to it themselves.. This is responsible for this change in generated code:\n-func (rcv *Monster) TestType() byte {\n+func (rcv *Monster) TestType() Any {\nEnums with is_union set are the autogenerated \"_type\" fields for unions, as I understand it. Following other enum fields, it seems like it should return the actual enum type instead of byte, for better type checking, etc. It works with the change on line 407, to use GenTypePointer in GetUnionField. (Since obj is an out-argument, GenTypePointer makes more sense to me there anyway.)\n\nActually, this part of the change could have been made earlier, since the enum type was an alias for byte. Was there a reason for using byte explicitly?. I might be missing something, but Builder doesn't use Table at all, right? Table is just for the read path.\nThis cap is not the cap of the buffer given to Init. I agree that would be bad, because it would expose data past the end of that buffer. But Init sets the cap of _buf to be the len of the buffer it's given. And this reads that value out and creates a slice with the correct len. So nothing ever sees bogus data, no matter how buffers are reused. (Except obviously you can't reuse a buffer for building and also be reading the old structure in it at the same time.). I think I'm not explaining the idea clearly. Let me try again:\nA Table needs to store three things, a buffer start, a buffer length, and a position within that buffer. The obvious way to do that is with a []byte and an extra int field for the position. Because of alignment, that takes four words. But we only need three, can we drop one? Note that a []byte has three words itself: a start, a length, and a capacity. We're only using two of them, the start and the length. The capacity is not used at all by Table. So what if we put the position there? Well, that doesn't work, because cap has to be greater than len. But we can swap them and put the position in the length field, and the length in the cap field. That means that the byte slice doesn't have its usual meaning, so we should keep clients from accessing it to avoid confusion. But it has all the information we need to make use of the Table, as long as we use cap as the end of each slice, instead of the default (len).\nSo, users don't have to think about it at all. They continue passing whatever []byte they like to GetRootAsX or any Init function. The trick is encapsulated in Table, and Table never accesses anything beyond the length of the given slice.. Right, that's a good way to put it! It's using the fields of the slice header for something other than their intended meanings. So in a sense, the surprising thing is that it works without using unsafe. I think that's because the meanings are \"close enough\" to the intended meanings.. ",
    "bspeice": "Also appears to be the case that the Appveyor build had a one-off internet issue, all the other tests appear to pass.. Apologies for the delay, CI is passing now.. Looking better now: https://travis-ci.org/google/flatbuffers/jobs/504053627#L1018. ",
    "iskandarov-egor": "I signed it!. ",
    "rjsberry": "I've just seen #5124 - is there any reason this isn't merged/wasn't considered for part of the main FlatBuffers repository?. I think a builder pattern that wraps shelling out to the schema compiler is the best way to go. If you check out the link to my initial implementation in my original post you should get the gist of how I would personally go about implementing the tool.\nIf you have used the cc crate before for compiling C/C++ code as part of a build script that should also give you an idea of what we should try to achieve with a FlatBuffers implementation.\nIt seems that a lot of the command line options with the schema compiler aren't available when targetting Rust output. This simplifies the wrapper quite a bit, and means we can make sure users can only ever build valid commands for Rust output using the wrapper.\nPotential Usage\nIf all options were available, valid usage would look like:\nrust\nfn main() {\n    ::flatc::Build::new()\n        .schema(\"flatbuffers/foo.fbs\")\n        .schema(\"flatbuffers/bar.fbs\")\n        .include_prefix(\"flatbuffers/extra\")\n        .gen_mutable()\n        .gen_onefile()\n        .compile();\n}\nThis would automatically run from CARGO_MANIFEST_DIRECTORY and expand to the shell command:\n$ flatc --rust -O \"<OUT_DIR>\" -I \"flatbuffers/foo.fbs\" -I \"flatbuffers/bar.fbs\" --include-prefix \"flatbuffers/extra\" --gen-mutable --gen-onefile\nWhere <OUT_DIR> would be expanded from the build script.\nSourcing Generated Code\nOutput Location\nOUT_DIR is the correct place to put the generated source files. Build scripts should not write to CARGO_MANIFEST_DIR. For this reason we shouldn't expose the -o flag, and always set this to OUT_DIR.\nFor each schema file added to the builder, we place the generated file in OUT_DIR with the standard conventions: filename postfixed with _generated and the extension set to .rs.\nExample\nAs mentioned in my initial post, this will require modification of the generated Rust source files by flatc. This is due to how users will need to include generated code placed in OUT_DIR.\nrust\nmod monster_generated {\n    include!(concat!(env!(\"OUT_DIR\"), \"/monster_generated.rs\"));\n}\nWith the current version of flatc this expands to code with invalid inner attributes:\n```rust\nmod monster_generated {\n    // automatically generated by the FlatBuffers compiler, do not modify\n#![allow(dead_code)] // <-- invalid in this context\n#![allow(unused_imports)] // <-- invalid in this context\n\nuse std::mem;\nuse std::cmp::Ordering;\n\nextern crate flatbuffers;\nuse self::flatbuffers::EndianScalar;\n\n// etc\n\n}\n```\nGenerating something like the following should resolve this:\n```rust\nmod monster_generated {\n    // automatically generated by the FlatBuffers compiler, do not modify\n#[allow(dead_code)]\n#[allow(unused_imports)]\nmod __flatc_codegen {\n    use std::mem;\n    use std::cmp::Ordering;\n\n    extern crate flatbuffers;\n    use self::flatbuffers::EndianScalar;\n\n    // etc\n}\n\npub use self::__flatc_codegen::*;\n\n}\n```\nUsers won't be able to include source files by modifying the path location with the path attribute:\n```rust\n[path = \"\"]\nmod monster_generated;\n```\nconcat and env cannot be used in place of a literal in this context.\nError Reporting\nGenerally I think it's safe to assume that if a user is generating code from FlatBuffers schema as part of their crate build and the generation is unsuccessful then we want to explicitly panic and kill the build.\nOut of the box cargo produces nice output that the build script was the cause of the failure:\n``\nerror: failed to run custom build command forcrate v0.1.0 (/home/user/crate)process didn't exit successfully:/home/user/crate/target/debug/build/crate-b372802e65fea7a1/build-script-build` (exit code: 101)\n--- stderr\nthread 'main' panicked at '\nAn I/O error occurred during FlatBuffer compilation: No such file or directory (os error 2)\n',\nnote: Run with RUST_BACKTRACE=1 for a backtrace.\nwarning: build failed, waiting for other jobs to finish...\nerror: build failed\n```\nOther than I/O error (can't find flatc on PATH) and errors produced by flatc itself I can't think of anything else we might need to report through this interface.\nQuestions\n\n\nDo you agree it's right we ship an official wrapper, or should we leave it to crates like flatc-rust?\n\n\nDo you agree that a builder pattern is the best option to go with for wrapping flatc?\n\n\nHow much of flatc would we need to support in the build script wrapper? Do we want to enforce the -r flag at all times?\n\n\n. @aardappel Yeah, code generation in build scripts is a common pattern seen throughout the Rust community:\n\nProtobuf: prost-build, protoc-rust\nCap'n Proto: capnpc.\n\nThese generally follow builder patterns as proposed in my original issue. There's also great documentation in the cargo book.\n\ndo note that bad things can happen if your flatc is not in sync with the rest of your Rust FlatBuffers code or generated code.. so you'd want to somehow enforce that.\n\nI'm not sure I follow. The idea is for the flatc crate to use std::proccess::Command to call the FlatBuffers schema compiler which must supplied by the user - this isn't distributed via Cargo. Code generation then becomes part of the build process, and it is up to the user to make sure the correct compiler is available to the crate. Do you mean keeping flatc in sync with previously generated code?\nPerhaps we could allow an override using an environment variable in addition to searching PATH.\nWe will also need to ensure that the version of the schema compiler being used is generating code that can work using the include! macro.\nContrary to my original post there is actually an ongoing Rust issue that prevents this macro working with inner attributes. Regardless of the cause we still need to work around this by modifying the schema compiler to remove inner attributes on generated Rust code for the time being.. Thanks for the clarification. So you're saying that the build script wrapper should pull flatc source, build with CMake, then invoke its own copy of the flatc executable rather than relying on the user to have a pre-compiled copy lying around somewhere? Definitely do-able. Would you suggest we couple the version of the crate on crates.io with the flatc release number? What about building from master?\nIs an officially distributed crate for managing compilation of FlatBuffers schema in Rust build scripts something you would be willing to introduce to the project once we iron out the implementation kinks? I'm certainly keen to see a way to idiomatically automate compilation of schema as part of my Rust build processes, but understand if it's something you don't think is right for the FlatBufers project itself to distribute.. Right, so we could tie releases of the crate to releases of flatc.\nUsing the future release of 1.11.0 as an example, in the Cargo.toml file users would need to put:\ntoml\n[build-dependencies]\nflatc = \"1.11.0\"\nIf we wanted to go the route of including pre-built binaries things would get complex when it comes to cross compilation. I'm not sure whether tools like cross execute build scripts on the host architecture before executing Rust builds inside its various Docker containers to compile for the different Rust targets, or if it would require a native version of flatc to be built. I think it would be best if we include the flatc source with the crate - then we'll be able to use the cmake crate to build a copy of flatc in OUT_DIR which the build script wrapper would then call to compile schemas.\n\nIf you want to be able to update crates.io more frequently then.. don't know. Does crates.io store the commit id it was derived from?\n\nHm. Yeah that's a tricky one. You can specify \"git\" dependencies in Cargo.toml files:\ntoml\n[build-dependencies]\nflatc = { git = \"https://github.com/google/flatbuffers.git\", rev = \"<SHA>\" }\nTo do this we will need to add a Cargo.toml to the root of the FlatBuffers repository with the following content:\ntoml\n[workspace]\nmembers = [\n    \"rust/flatbuffers\",\n    \"rust/flatc\",\n]\nNot sure what the best way for us to distribute the source code with the crate will be though. For context, typically with crates that bind to C libraries I would include the library source code as a submodule of the crate and lock the commit to the version I wanted to call from Rust. If the build script wrapper crate is a subdirectory of the FlatBuffers repository itself we will only be able to support versions of flatc after the initial release of the build script wrapper crate because we won't be able to checkout commits before the build script wrapper crate existed. This might also complicate matters if we wanted to add commits to the build script wrapper but keep using a locked version of the schema compiler source code.\nWe could of course put the crate in a separate repository, but from the looks of things everything is currently in the one repository.\n\n@rw Do you have any additional comments? I'm happy to draft up an initial implementation based on my discussion with Wouter unless you have anything to add.. ",
    "Sebdevar": "@rw Sure, I'll give it a try when I have some free time ;).. ",
    "stac47": "Hello,\nThis warning is raised when the destination is an object of class type and when writing into such an object bypass the non-trivial constructor.\nOf course, we agree that the current implementation of the default constructor is harmless.\nThis said, the GCC documentation (see page 52 of https://gcc.gnu.org/onlinedocs/gcc-8.3.0/gcc.pdf) states that:\n\nExplicitly casting the pointer to the class object to void * or to a type that can be safely accessed by the raw memory function suppresses the warning.\n\nOk for the second remark, I will update.\nStac. ",
    "JerryJoyce": "I suspect you are correct about this being a false positive.  I've used NOLINT comments to calm down the noise.. ",
    "chuanlei": "@rw \ncound you have a look?. @jean-airoldie \nthank you\nit works. ",
    "dcastro": "@aardappel Ah that makes sense, thanks for clarifying so promptly! :). @aardappel, Would you say that these two specs look correct? Just to make sure I'm understanding things correctly.\n```\nnamespace ;       enum E1 : short{x}   enum E2 : short{x}   enum E3 : short{x}\nnamespace A;      enum E1 : short{x}   enum E2 : short{x}\nnamespace A.B;    enum E1 : short{x}\nnamespace A.B.C;  enum E1 : short{x}   enum E2 : short{x}   enum E3 : short{x}\nnamespace A.B;\nstruct S {\n  x: E1; // should be A.B.E1\n  y: E2; // should be A.E2\n  z: E3; // should be E3\n```\n```\nnamespace ;         enum E1 : short{x}   enum E2 : short{x}   enum E3 : short{x}\nnamespace A;        enum E1 : short{x}   enum E2 : short{x}   enum E3 : short{x}\nnamespace A.B;      enum E1 : short{x}   enum E2 : short{x}   enum E3 : short{x}\nnamespace A.A;      enum E1 : short{x}   enum E2 : short{x}\nnamespace A.B.A;    enum E1 : short{x}\nnamespace A.B.C.A;  enum E1 : short{x}   enum E2 : short{x}   enum E3 : short{x}\nnamespace A.B;\nstruct S {\n  x: A.E1; // should be A.B.A.E1\n  y: A.E2; // should be A.A.E2\n  z: A.E3; // should be A.E3\n}\n```. ",
    "T-chuangxin": "@aardappel  Thank you for your reply, I have realized the problem. However, the current version of the maven repository is 1.10.0, which I thought was the latest version. By the way, how to parse JSON directly in Android, is it supported now?Or is there a sample for Android developers to refer to? . @aardappel Thanks a lot . I just wrote an Android demo about FlatBuffers with C++,like  : http://frogermcs.github.io/json-parsing-with-flatbuffers-in-android/\nBut when I parsed JSON with FlatBuffers, I found that it took longer than Gson parsing. Can you help me see where I am wrong?\nHere is my code:https://github.com/T-chuangxin/JniDemo. ",
    "asiderop": "The above if-guard seems to be backwards, no? That is, this appears to add -stdlib=libc++ to the flags only if USE_CXX03_STDLIB is not defined.\nAlso, what is the behaviour of CXX when using both -std=c++11 and -stdlib=libc++?\n--ap\n. Okay, I think I see what we're doing here now.\n1. this only affects Apple platforms\n2. this only affects clang (since the -stdlib=libc++ flag is clang-specific)\n3. we remove -stdlib=libc++ as a default flag unless USE_CXX03_STDLIB is given\nStill, I question not changing the -std=c++11 flag here, if the entire reason of this PR is to support c++03. So, either I'm still confused about how your change is accomplishing the task, or the change is not complete?\n--ap\n. ",
    "chood8": "ya don't. it's sugar.\n. ",
    "dcarp": "Define the class a template of InputRandomRange \n. Why derive from Error and not from Exception?\n. This should build an inputRange, so that you don't need to take care of the memory management\n. ",
    "jacob-carlborg": "I think this should be the license key.\n. I recommend using four spaces for indentation.\n. ",
    "daksenik": "Could you write an example?\n. Can we just print the error message and then use exit(1) to finish the program?\n. Function for struct generation is out of generator class in go and python generators, so this function has not access to the error_ field. I see two solutions here: to pass error_ as additional parameter to GenStruct function or wrap GenStruct and GenEnum into generator class.\n. ",
    "manukranthk": "vector_directed\n. ",
    "qsr": "Done.. ",
    "rufeooo": "Done. It reads better when formatted as you suggested.. std::array is more readable, but this has less dependencies. Returning a reference to an array requires no additional dependencies for users that do not care about this feature.. My first implementation was std::vector, but this generated template instantiations for each enumeration class when using the scoped_enums parser option. A debug binary increased in size by 40KB for 15 enumeration types when comparing std::vector to an array refererence.. Color* would lose the size information. The current implementation also provides access to the number of enumeration values in a type by using:\ndefine countof(arr) sizeof(arr) / sizeof(arr[0])\nsize_t colors = countof(EnumValuesColor());. Color* would also cease to function in this c++11 syntax, which requires a valid range:\nfor (auto c : EnumValuesColor()) {\n  std::cout << EnumNameCOLOR_TYPE(c) << std::endl;\n}\n. Both usages are applied in cases where an enumeration is Sparse, as you mentioned at the start.\nThere is some assurance provided to a developer that the above for each syntax will continue to operate, even if another developer adds an enumeration value, regardless of that value.\nOur game uses this to output the valid inputs in an console client.. ",
    "nate-meta": "Yup, Bazel requires a WORKSPACE file at the root directory:\nhttps://docs.bazel.build/versions/master/tutorial/cpp.html#build-with-bazel. ",
    "NikHGGH": "Ah I see. This was pulled in only for MakeCamel(), happy if I move that into util.cpp?. ",
    "thecsapprentice": "Done! I didn't see that file previously. Thanks for pointing it out to me.. ",
    "Ophirr33": "https://doc.rust-lang.org/std/mem/fn.size_of.html. ",
    "unintellisense": "Nothing, forgot to remove it after I thought I needed it to recreate the ByteBuffer. Removed.. Good catch, thanks.. I guess I wasn't scientific about adding that, thought it was necessary to reuse the ByteBuffer, but I can see now it isn't.  Removed.. ",
    "stephentoub": "Do we need both the read-only and writable variants?  There shouldn't be a penalty associated with the implicit cast from Span to ReadOnlySpan.. >  If you just call GetSpan() or GetSpan(start), we directly return array and array.AsSpan(start), thus no bounds checks for the first call, and only one bounds check for the second call\nI'd have thought you could get the equivalent of that by having:\nC#\npublic abstract Span<byte> GetSpan();\npublic Span<byte> GetSpan(int start) => GetSpan().Slice(start);\npublic Span<byte> GetSpan(int start, int length) => GetSpan().Slice(start, length);\nDoes that not perform as well?. This should be sealed unless we expect folks to derive from it.  That'll allow for devirtualization in some cases by the JIT.. > I don't think you can get a writable Span from a ReadOnlyMemory.\nYou can, but it's unsafe and obviously dangerous.  We shouldn't use it here.\n\nSo calling GetSpan is going to throw a NotSupportedException.\n\nShouldn't there be some property then that states whether you can use the writable members? e.g. CanWrite? CanGetWritableMemory? etc.?. Seems like this could be simplified to just:\nC#\nreturn MemoryMarshal.Cast<byte, T>(_buffer.GetReadOnlySpan()).Slice(pos, len).ToArray();\n?. Why is unsafe needed?. Rather than reading byte by byte, couldn't this (and all the other methods like it) just use, e.g. BinaryPrimitives.ReadUInt64LittleEndian?. These PutByte and PutSbyte methods are going to be relatively expensive (compared to the cost of storing a byte).  Hopefully they're not used on hot paths.. This will likely incur a bounds check on each write.  That would be eliminated if this were instead:\nC#\nSpan<byte> span = _buffer.GetSpan(offset, count);\nfor (int i = 0; i < span.Length; i++)\n    span[i] = value;. Can this instead be:\nC#\nSpan<byte> span = _buffer.GetSpan(offset);\nBinaryPrimitives.WriteInt32LittleEndian(span, BitConverter.SingleToInt32Bits(value));\n?. Same question.. > I would think the same code that is creating the ByteBuffer is the one that is using it.\nIf that's true, it'd be nice to avoid the ByteBuffer abstraction entirely :)\n\nI think we can add a CanWrite API in the future, if it is necessary.\n\nOk. (And if you did do that, then the API could even just expose GetSpan(), parameterless, and let the caller just do their own slicing.). (And it could then just be a public abstract Span<byte> Span { get; } property, since you wouldn't need to worry about overloads :smile:). Yup, you're right.. Could also be:\nC#\n_buffer.GetSpan(offset, count).Fill(value);\nWhether that's beneficial would depend on how big the span is.. > The Int32 and float APIs are .NET Core 2.0 only. \nAh \ud83d\ude26 Ok. Does raise the question of whether (eventually, obviously not part of this PR) there should be a netcoreapp build of the library in addition to netstandard2.0.\nThat said, the implementation is just *((int*)&value), so it can be done without the API.. "
}
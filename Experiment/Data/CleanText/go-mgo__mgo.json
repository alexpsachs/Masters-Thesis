{
    "fatih": "I don't believe that would work. You need to implement the String() method for every type you want to pass to make use of the Stringer interface. That means now it's not possible to pass a simple string type anymore :) http://play.golang.org/p/myVcRtnc_A\nI suggest to revisit this PR or even close it.\n. I don't believe that would work. You need to implement the String() method for every type you want to pass to make use of the Stringer interface. That means now it's not possible to pass a simple string type anymore :) http://play.golang.org/p/myVcRtnc_A\nI suggest to revisit this PR or even close it.\n. ",
    "niemeyer": "@fatih is spot on.\n. Thanks!\n. Thanks. You can run the test suite by first calling \"make startdb\" on the main source directory. This will start several mongod instances to run the tests against.\n. This is looking good. I haven't merged yet as I've been in a sprint, but I'll surely test locally and get it in over the next few days.\nThanks again for the change.\n. That's merged in the v2-dev branch (6af013a1). Will be in the next release.\n. I recommend subscribing to the mailing list. It always gets important news, and allows you to participate in relevant design decisions in time to change them:\nhttps://groups.google.com/d/forum/mgo-users\n. Thanks again. If that's okay, I'd prefer to keep project details at the project page to avoid having to maintain multiple sources. I've added a trivial README.md, though, and also updated labix.org/mgo to include these testing instructions.\n. Thanks. That's merged in the v2-dev branch (b4733f6f) and will be in the next release.\n. Thanks! This was merged into v2-dev (72ad3358) and will be in the next release.\n. That's great, thanks Valeri!\nIt looks generally good. I'll add a few short notes inline.\n. Please run tests. :-)\n. Sorry for bothering, but spacing is still messed up. See lines 307 and 606.\nI'll post a couple of git hooks to make sure we don't need to fight those trivial details on reviews.\n. No, this seems ready to go in.\n. I suppose you need a new release to unblock that, rather than just a merge?\nWe can do either.\n. Merged on v2-dev on f047e079.\n. Cool, it's in.\n. Yeah, thanks for getting us to know golint's preferences, but even without debating which of these should actually be done, we simply cannot do these breaking changes without a fantastic reason.\n. It looks mostly good, but there are a few points to address:\nWhy is Stop not supported on Windows?  This is just stopping and restarting the process via supervisord\nDuplicating the whole configuration is not great. There must be a nice way to avoid it.\nWhat's the deal with the DIRECTORY environment variable?\nThe very specific `uname == 'CYGWIN_NT-6.1'` check doesn't provide a lot of confidence that other people will be able to reproduce the results.\n. We can swap supervisord for something better and that works in Windows. Any suggestions? Any idea if runit works well on Windows?\n. Yes, I'm definitely happy to merge something simpler that solves your current problem.\nTo get this merged, can you please revert the changes to Thaw/Freeze, and instead just isolate the real offending call (syscall.Kill) in a similar fashion?\n. It's looking good otherwise.\n. Sorry it's causing you some trouble. I do appreciate your effort, though. Much better to understand and polish this in isolation.\nI'll get it merged in a moment.\n. Merged into v2-dev as bfb5e400.\n. Per mailing thread:\nKyle, I have reviewed the proof of concept over the weekend, and apparently it's just mimicking the behavior of Iter.Next, but copying the byte slice out of the internal buffer.\nIf I'm right about that, a couple of questions:\n1. Have you tried to use a bson.Raw? If so, what was the impact? If you use one for the whole document, it's very close to being a raw copy of the byte slice, and we can definitely improve the performance further by fine tuning that code path. Which leads to the second point:\n2. Do you have a small self-contained benchmark we could share, preferably one that might be part of the test suite permanently? I believe I might easily improve the performance of the important code path to be satisfactory, but I must be able to tell whether we've reached satisfaction or not, and hopefully we can keep that in place so it remains the case in the future.\n. As discussed over email, commit 00004f72 turns unmarshaling into a bson.Raw into a trivial operation (~150ns locally), so we should be able to use the standard Next method and achieve the same benefits of the described proof of concept.\n. The original plan was to have something simpler as you suggest, and then came the idea of using a more flexible syntax to prevent potential future compatibility issues. That said, since I wrote this my feeling has changed back that this is not necessary, and a simpler model conforming to the current convention should work for a long time.\nThat said, there's a bug in the current proposal. The special metadata name is \"textScore\", not \"score\". The latter is the field name.\n. As I mentioned, a simpler model conforming to the current convention should work.\nSomething along the lines of:\n$textScore:score\n. There are several changes unrelated to the conversation above in the current pull request. It'd be good to trim it down to adding just that alone, and then adding a test. Other changes, if you still want to push them, may be proposed in a separate request.\n. Thanks. I'll provide a few comments inline.\n. That's it for now. Thanks again for pushing this forward.\n. Okay, this is looking good. I suspect the comments above are the last iteration.\nThanks for keeping it up.\n. Any news on this?\n. This was merged in v2-dev with b3fc1f86a and will be in the next release.\nThanks for keeping it up!\n. That's not quite right. The logic in the default cause includes the handling of some slices.\nI'm still looking at the use case, but the fix is not so straightforward if we do it.\n. Brilliant, thank you!\nWould be nice to have the benchmarks integrated in gocheck as well. The support is exactly equivalent. They just need to be made part of the suite, and the traditional (c *C) argument used instead.\n. This was merged in v2-dev in 10662809. I'll follow up on the comments via email so we can keep the conversation in a single place.\n. Thanks for the work, Valeri.  A few initial comments above.\n. Sorry, wrong button.. didn't mean to close it.\n. I'm all for having less code that we need to maintain, but the proper way to do that is to use an external library, which is the process I have followed to implement the current support in Linux, and which hopefully actually works in Windows too (I never got a proper response to whether it does work there or not, and why not, given that the library does mention support for Windows). I'd be quite surprised if there's no proper library that abstract that exact API for multiple operating systems.\nThat said, if you prefer implementing it that way, and supporting it too, that's okay too. But if we are going down the rabbit role of importing external code, we'll need to maintain that code sane and working for us. That includes file headers, fixing things such as the loose error handling, adding tests, and on the bright side getting rid of code that we're in fact not depending upon. Looks like we can get rid of quite a bit of it.\n. Having experience with other lands, I don't find Go's package management to be in a sorry state, but let's have that conversation in a different forum.\nI'm happy to support you in merging this work nevertheless. Let's just try to polish it to a state where we're both happy to maintain it in-tree.\n. Going over the comments above, I still see a number of uncovered points.\nOne easy way to ensure we're on the same page is by responding to the actual comments, even if it's just a \"Done.\"  This gives a chance for both of us to tell whether things were completed, or whether there was a disagreement, and what was it.\n. Also, can we please drop the \"_\" prefix from all of the C function names? These are just normal functions in our own namespace.\n. There are comments in #20 that were not addressed. Even some of the ones marked as \"Done\".\n. Okay, please let me know once this is actually ready for review.\n. Ok, just did a quick scan with a few comments, but I'll need to go back to it later.\nMeanwhile, where are the tests?\n. I might, but where are the tests?\n. I don't understand what kind of significant work might take place for that. Registering a new test suite requires three or four lines of code.\nEither way, it doesn't make sense to merge this without tests. Please also make sure that the tests that are being submitted with the logic actually exercise it well, and are passing for real.\n. Also, what do you mean by \"break Kerberos tests\"?  The test that exists today are trivial in their implementation and design. We shouldn't be breaking them out or removing them from where they are.\n. It might be best to submit that new logic you're talking about in a separate branch before polluting this PR, actually, so I can at least have an idea about what's being done.\n. Okay, that looks reasonable, but let's name it what it is: KerberosSuite, and have the suite defined right above kerberosFlag instead of having a new file for it. We can reorganize it after it lands if necessary.\nIf that's what it is, we can merge it in this PR so you can make sure tests pass before we merge this.\n. Thanks! Can you please run the tests on Linux as well, to make sure we're still good there after the changes?\n. Thanks for this, Gabriel. Looks quite nice. I'll add a comment inline, but the main thing this is missing is a test. This logic can be exercised by default, right? If so, we need a clean test that always run with it.\n. Thanks!\n. Thanks for the build fix, happy to apply it.\nThat said, we cannot apply the suggested rename as-is. People out there are likely depending on this name, and it was modified in the server right before the 2.4 release:\nhttps://jira.mongodb.org/browse/SERVER-8501\nWe should support the two names instead.\nTo avoid another roundtrip, I have applied the two fixes to v2-dev. Thanks again for the changes.\n. Sorry, it was late and I forgot to push it. Should be there now.\n. Thanks Mike.\n. I have merged this to v2 by mistake, and quickly reverted back.\nThis is now properly merged in v2-dev with 4d7c7241 and will be in the next release.\n. Thanks for the changes.\n. That's improving the previous approach with caching, but the underlying concept is still not good. There's no reason to pay the cost of marshaling of these structs simply for obtaining a well defined order. It's also pretending to work for things that actually do not work. Marshal will marshal anything, including things with an undefined order. These should continue to break.\nWe should be able to easily have an ordering mechanism for the structs we care about without resorting to marshaling.\n. Thanks for that. I don't have energy right now to review this, but I'll try to get over it in the next couple of days, or over the weekend in the worst case.\n. The change looks good overall, thank you.\nThere are trivial tweaks to be made, such as ensuring that the tests run whenever the server at hand is SSL-capable, instead of requiring manual procedures that imply the test would be rarely run.\nI'll merge this right now, though, and will do these tweaks myself afterwards. Thanks again.\n. Okay, I performed the mentioned tweaks, and also fixed the test so that it works if run multiple times (second time would fail to add the Admin user because it had no auth). That was done by integrating the new server in the common init and cleanup workflow, whenever SSL is available.\nYou can see all those tweaks in c1b9c9f28. Then, the auth test was moved to the common auth test file in dc6fe4aaa.\nPlease let me know if you have any questions.\n. Hey Gabriel,\nI wonder if we should have these in. In my experience the error codes are quite unreliable, changed over time, and there are often multiple codes for the same error (see IsDup for an idea). Being just partial also hints of suboptimal value.\nIt feels like it might be worth continuing with the pattern we have at the moment: important errors are either represented by first time constants (ErrNotFound) or have comprehensive checkers (IsDup). Other errors are easy for people to have their own verifiers or code constants in the local code.\n. Would you mind to describe in more detail what the problem being solved is?\nThere are still questions about how the internal implementation of bulk handling will look like, and what the meaning of the count method would be. There is the potential for grouping operations, for example, so would that mean \"count the number of requests\" or \"count the number of individual changes\".\nFor those reasons, it would probably be best to wait until the Bulk API is fully fleshed out before we add such introspection methods.\n. Thanks, I'll consider this further once the bulk API is completed. For now, unfortunately I'll need to delete this pull request because I need to rename the v2-dev branch to enable us to make it visible via gopkg.in (it needs to be -unstable), and unfortunately GitHub doesn't offer any way to rename a branch politely.\n. Hey Mike,\nSorry for the feedback. This is looking good, and was already in my agenda for merging today. I'm going to merge it directly to v2 since it feels independent enough to not cause any harm to people that are not depending on the feature yet. Hopefully this will give you a hand there.\n. Yes, I'll have a look at this for the next release. Thank you.\n. The change looks great. There are only a few minors which I will comment inline.\n. That's about it. A few entries, but nothing fundamental. Thanks for these changes.\n. While thinking about that change further, there is a more fundamental issue. The cluster.err variable is propagated on any error, irrespective of whether the failure happened with that specific server. Now imagine there are 5 servers in a replica set. If one of the servers turns out to be misconfigured, it's nice to reject communication with it, but it would be bogus to fail against any other correct server with that error.\nMakes sense?\n. > Once set, the error is only propagated insofar as spawnSync for that server is short-circuited (so communication with that server is rejected). The other goroutines for possibly properly configured servers proceed as usual.\nThat's not quite true. Consider which code paths can lead to the return of cluster.err, and what is the relationship of the call site with the bad server.\n. Thanks!\n. The change is looking good, and I'm happy to merge it. There's one detail which would be good to address before we do that: some of these tests are now quite ineffective at testing anything at all, by just dialing and making sure there are no errors. There are good ways to verify that the offending server is not being communicated with:\n1. You can get the set of current live servers from session.LiveServers()\n2. You can inspect the log to verify that the expect error was reported, via c.GetTestLog() in the test method\nWould you mind to improve the tests a little so we can be sure they're exercising what they seem to be?\n. This was merged into v2-dev, with the trivial changes from e0d7e593b, and will be in the next release.\nThanks for the contribution.\n. Thanks!\n. Thank you!\n. Sorry, but this does not look okay. It's changing the Insert API from being a local operation into running remote commands arbitrarily. We can fix this issue in better ways by being smart at execution time, but I'd prefer to wait to integrate such smartness once the bulk API is completed, integrating recent bulk features, so that we do the logic correctly for both cases.\nCan you please open an issue about it so we can track the work?\n. We shouldn't have an option that turns the documentation wrong. The proper way to do this is to have methods that do not do the caching instead. We'll introduce those soon.\n. I'm still on the fence about this one. It feels surprisingly costly for such a trivial operation.\n. The trade-off doesn't seem so clear to me as it is to you. You see it as a bug, and I see it as a feature explicitly left out for lack of a reasonable way to handle it. Requiring 3000 allocations to encode 1000 values which contain 3 arrays with a couple of integers each sounds bad, even ignoring the fact that they need to be copied. The impact is disproportional to the expectation of what such an operation would do.\nI'm not rulling it out yet, but it's not so straightforward a choice to me.\n. Okay, I'd be happier about that implementation as well.\n. We very much follow go fmt rules, as you can observe by how small the patch set is. Sometimes things do go in without following the rules, though (merges, etc).\nI'm happy to apply your change set, either way, but not on v2. Any changes should go into v2-unstable.\n. There are a few conditions that we use in the json unmarshaler logic which seem appropriate for the text marshaler as well. Can you please have a look and let me know what you think?\nAlso, I believe the marshaler should take the receiver by value, and not as a pointer. It is a string, so there's no benefit in receiving it as a pointer in this case.\n. Thanks for these changes. The only thing to fix here that I could notice is that strange special casing of the a string which is a single empty space. If we want to handle empty spaces as an unset id, any number of spaces should have the same effect.\nThe other bit is that this is missing tests. Otherwise, it looks good.\n. As a hint, the check you want there is likely something along these lines:\nif len(bytes.TrimSpace(data)) == 0 { ... }\n. One more detail: can you please rebase this against v2-unstable?\n. Thanks, I'll have look.\n. Next version is coming in the next couple of days. Something along these lines will be in. Will probably take your patch and tweak for usual conventions.\n. Thanks!\n. Oops.. this patch sneaks in a serious breakage in the cluster synchronization logic which is completely unrelated to fixing the statistics. I'll let you guess which line does that. :-) \n. Also note that existent tests prevented this from being released, which probably means they were not run.\n. Sounds good. It takes slightly more than capitalizing the function, but the idea is sound.\n. Thanks, this is definitely an often-requested feature, and I'll make sure to get it in place for you. #97 was a bit closer to the end result, but still a bit off in that we need the result to be a pointer, which is what the public API expects. I'll get something in and let you know via this PR.\n. We can include stock support for the ssl flag somewhere in the driver, but not in the main package like this. This would force the inclusion of crypto/tls and all its non-trivial dependencies on every application building the driver. Instead, I'd prefer to have an auxiliary package that may be explicitly imported to introduce the SSL support.\n. By the way, thanks for the contribution. I appreciate the interest, and would be glad to figure out with you a proper way to do this.\n. The txn package works in MongoDB 2.4 and 2.6, and probably earlier ones. The error you describe doesn't make sense to me, as it reports an attempt to store a $ field, which $pullAll should of course not be as this is an update operator.  The pull request also does not make sense, as it is turning such an update operator into a normal field, thus breaking the package (there is a test suite you can run to test your changes).\nPlease follow up in issue #87.\n. Thanks for the change. The field name is required because the default is to fully lowercase the field name, or \"bucketsize\" in this case, which isn't correct.\n. By the way, shouldn't the bucket size be an integer value?  Does it make sense to provide fractional bucket sizes?\n. Nevermind.. confirmed in the C++ code that it is a double.\n. This patch would prevent people from using YAML's null value to initialize Go values, for no benefit apparently. As you point out in the proposal, it's currently supported and trivial to choose to have a zero value instead of the initialized value, if that's what you want.\n. Can you provide more details about exactly what you are asking for and why?  Will be happy to provide you with an example once you explain what you are trying to achieve and are being unable to. \n. This is a dup of #85.\n. Just a couple of trivials. Please ping me when done and I'll merge it.\n. We discussed this concept out of band.\n. It doesn't look like this test is doing much to test that issue. We have quite a lot of tests in cluster_test.go verifying that the values are basically right.\n. Sounds good. The test as it is is hugely dependent on subtle internal details either way. A regression that breaks this logic could trivially turn the test irrelevant in one go.\n. Thanks!\n. Thanks for the contribution. Let's please avoid adding a new method, as it's unrelated to the bug being fixed and we don't need to agree on it right now (LastError.Found is misleading, for example). Then, we do need tests for it. You can start servers in your machine by running \"make startdb\" before running the test suite. If you don't want to do this, please just open an issue including a snippet that demonstrates the bug and I'll fix it locally.\n. Also, please mention which server versions you are testing this under.\n. Sorry, this cannot wait. I've fixed it and will tag a new release shortly.\n. This is out.\n. Supporting such a wildcard option would mean preventing extending that structure to support further options in the future in a backwards compatible way, as we've done multiple times over the history of the driver (and have just done so again days ago). It might be fine to support actual options, though.\n. This is introducing the use of FieldByName twice in the fast path of a loop. We don't have a single call of that method so far, and I'm not quite convinced doing so is a good idea. If you have a benchmark showing this has a positive impact rather than a negative one in relevant cases, please feel free to reopen the ticket.\n. Thanks, but I believe this is grammatically sound, and also irrelevevant if you understood the intended meaning.\n. Hi Diana,\nI'm afraid your use case is beyond what mgo is meant to offer. For emulating a proxy, you'll find many shortcomings in this driver, because its goal is to abstract away the wire protocol and command mechanism of the server into something that is convenient for developers to use when building their applications, not to be a transparent mechanism that a proxy server can use to fully introspect actions and dumbly replay incoming requests into the server.\n. We can't merge this unless we find a non-artificial reason why a normal application would use these details. As far as I can see, it makes no sense to change batch size during iteration, and nobody ever asked me to expose the cursor ID, since the driver is abstracting iterations in a very convenient way, precisely so that they don't need to care about these internals.\n. The fundamental question is why do you want to create a new iterator? This is the sort of task the driver itself should be doing for you. There's a very special reason why NewIter exists, and one that took some significant thinking time. I wasn't keen on exposing it at all to begin with, and it's not an excuse to open the gates to functionality that is added without justification.\nRegarding creating a new driver, if your use case is incompatible with the goals of this driver, I see no problem with creating another independent driver that is tailored for you own use case. This driver is not meant to be a proxy, and I'm not keen on adding functionality that is completely irrelevant for every other use case but the proxy one.\n. I've never had someone mention that use case, and we can easily add cursor id to the information we log if that becomes necessary. That's what I meant by non-artificial. I'm interested in what is your use case for this, not the potential use case of a theoretical user. Once more, what is your use case for creating iterators manually, and why do you need a cursor id? \n. Closing for lack of feedback.\n. Thank you!\n. Please note I've tweaked the logic so the file isn't removed if index creation fails.\n. Thanks! Looks like it's not properly indented, but I'll fix that locally.\n. Thanks, sounds reasonable.\nCan we please have a test for this?\n. Okay, that sounds fine. There's a small question here, though. What is the semantics of IsDup when one of the errors is a dup but the other is not?\n. The reason bulkError is not exposed is precisely because it is not yet representing everything it needs to for bulk scenarios. If the idea was to have a single error always, it wouldn't make sense to wrap it in such a way.\n. Maybe we need something along the lines of BulkError.Has or BulkError.Contains, which takes a predicate function of the IsDup style. Another option would be to say that such standard predicates return true if any one of the errors match, which doesn't smell so great, but maybe it's the useful thing to do. We need to think through what would be the correct way to handle such multi-error cases.. what will we want to do when we have one of these at hand?\n. After sleeping on this, we should make IsDup return true when all errors in a bulk update are dup errors, which means it would be fine to make IsDup return true when there's on a single error and that error is a dup error. That seems like the most useful and safe approach.\n. Why? I was tempted to adopt your suggestion, for the reason exposed above.\n. Different packages really have different fields with different potential attributes and meaning. We really can't promise full compatibility with json there, and it's also questionable whether this is always what's desired.\n. Raymond, what was pushed doesn't reflect our agreement. What happened?\n. Thanks for the follow up, Raymond. I'll add a few initial comments here.\n1. The +/-infinity approach sounds fine.\n2. The precision details are not clear to me. Why would 2E-50 have more precision than supported by MongoDB, when both the significand and the exponent are well within the range of the decimal128 type? Rationals are arbitrary-precision, and MongoDB is not, but both should be able to represent 2E-50 fine.\n3. Rather than storing a precision within the Decimal, we should instead compute the precision based on the actual rational value (the divisor, more precisely). This should make it not only roundtrip correctly, but work appropriately once operations are performed with multiple decimal values as well. It also removes the need for manual precision methods and parameters.\n4. As we talked, the String method is a strong convention in Go, and needs to have the specific prototype we discussed, without arguments. Otherwise it's not implementing the proper interface and will not work properly in contexts where it should.\n5. As we agreed, none of the extra methods and types should be exposed in the bson package. The external interface should remain untouched. For testing, you can create public functions within export_test.go that just call the private ones, and then use these in tests. That will prevent these functions from actually shipping in the package outside of testing.\n6. Let's please not create dummy types such as uint32x4 and Uint64x2... Instead, we can use the real underlying types ([4]uint32 and [2]uint64).\nPlease let me know if you have any questions.\n. Alternative implementation introduced in https://github.com/go-mgo/mgo/commit/14a4475d4d32049500ac89e1499c46be62bf7bdf.\n. Looks good, thanks.\n. Please see conversation in #103. I'd suggest agreeing on something before spending time on code.\n. Thanks!\n. This is useful for me to understand what sort of feature you need, but the API is not quite there. It's too close to the internal wire protocol implementation instead of the user-oriented semantics that the driver offers today.\n. Sounds reasonable, thanks.\n. Yeah, sounds reasonable. Thanks!\n. I'm not entirely sure we want to change the semantics here. Consider the fact we need to allocate an extra buffer for each single binary field we find, instead of using a single byte slice which in the case of the mgo driver was already allocated and will be thrown off if unused.\nAlso, if we want to make guarantees about not using the source buffer, there's more to be done. Consider Raw, for example. Will we pay the cost of double-allocation for every Raw unmarshalled?\n. >  This change makes the binary data handled more similarly to how the string data is handled.\nStrings are very inefficient to handle in general because they are immutable, so making it more similar to them isn't a great reason to change it.\n\nIf you read in some BSON documents, with binary data fields, into individually allocated buffers, and then marshal them to bson.M variables, then each of the buffers will not be garbage collected until the bson.M variables can be. You're binding the lifecycle of the BSON document buffer to the lifecycle of the unmarshaled data structure. With this change the buffer can be collected as soon as its last regular reference is. If one is creating single use I/O buffers, then my change will reduce memory use, and if one is reusing a single I/O buffer, then my change will make it possible.\n\nIndeed. The counter side is that your change will make every single binary field become an extra allocation and an extra copy of the data, which means decoding a GridFS file and its 256k chunks will allocate twice as much memory, create twice as much garbage, and become a copying algorithm rather than a zero-copy one.\nWithout proper analysis I'm not convinced this is a win at all.\n. Thanks!\n. Hey Gabriel,\nThis doesn't look quite right, and running tests confirms it is indeed introducing a bug. The new logic should be in default, shouldn't it?  Have you tried to run tests there?\nAlso, we need a test demonstrating why the current logic is broken, and to prevent regressions.\nThanks for the fix.\n. Thanks Gabriel.\n. I appreciate the contribution, but unfortunately that's not something we should be doing. When we return an error, we indicate to developers that this operation can fail, and that the error should be checked. Neither is the case here.\nI suggest doing something like ioutil.NopCloser in the affected logic.\n. Thanks for the contribution. A couple of comments above.\n. Thank you!\n. This is missing a better description of the problem, and some tests, but perhaps there's a more fundamental problem with the proposal. BSON indeed only supports maps with string keys.. the way we represent that in Go is by enforcing the use of maps with key elements typed as strings, which are also more convenient to use in general. I'm not sure there's much benefit in loosening these rules.\nI'm closing down the issue on that basis, but please feel free to complement the report.\n. Sorry, but I still don't understand the issue. Who will introduce the map[interface{}] type?  Do you have sample code?\n. Actual code, please?\n. Sorry for bothering, but can you please provide something I can actually run locally? It should be self-contained and runnable, without using anything that is not relative to the offending package (mgo, bson) or the standard library. Without that there's no way I can tell what the code is in fact doing, or what the types involved are.\n. As I mentioned, it should be self-contained and runnable, without using anything that is not relative to the offending package (mgo, bson) or the standard library. Without that there's no way I can tell what the code is in fact doing, or what the types involved are.\n. Hint: you are using third-party code that does decoding. I bet this is actually what introduces the map[interface{}]interface{} type. If that's the case, then there's nothing wrong with the fact the bson package is strict about accepting map[string] only. That's indeed what's supported by BSON (the spec).\n. Okay, so mgo/bson is doing exactly the same as encoding/json by telling you that in fact encoded maps must use string keys. I think both packages are correct. If you want to map interface{} keys into string keys, that's easy to do with conversion code. \n. Thanks, merging it. I'll add a test for it as well.\n. Thank you! The package path is fine. gopkg.in/mgo.v2 vs. mgo.v2-unstable are different branches of the same repository. You're targeting the unstable one, so the package path should be unstable as well. \n. Please see conversation in the issue itself.\n. This is a bit of a leap, but perhaps it is indeed the most useful thing to do.\nCan you please add some tests covering that?\n. No, it's definitely going in. I've just been giving the test harness and CI some love last week.\n. Thanks @liviosoares\n. That's nice. I've been meaning to look into buffer reuse, but always thought about the harder case which is unmarshaling. This is a much easier win.\nWill need a few tweaks since we should expose that via an interface such as bson.NewEncoder instead of MarshalBuffer, but I can tweak that on my end.\n. No, that looks fine. I'll get it in early this week.\nThanks for the fix!\n. Thank you!\n. That's not quite right, as you can observe by the broken tests.\nPlease open an issue with a small self-contained reproducer exposig the problem clearly and I'll be happy to help.\n. Thanks!\n. Before anything else, thanks for reaching out.\nCan you provide some details about the use case?  It feels like the proper way to handle this would be to simply call Next, as you indicate. If we implement HasNext, we need it to be true. In other words, if HasNext returns true, Next should never return false. This conflicts with the proposed implementation.\n. Fair enough.\nCan we please invert the logic and call the method Done. This reflects better the fact we have certainty about it being done, but we have no certainty about really having more data or not (tailable or not).\nThen, can we please have one or more tests specific for that functionality that verify that each of these branches is actually doing the right thing, rather than spreading the logic out through other unrelated tests?\nThanks again.\n. Thank you!\n. There's a bug here. iter.err may be set while there's still data available for consumption. I'll need to tweak the implementation.\n. I've considered a similar idea before, but I'm still slightly uncomfortable with merging something like this alone. It feels too simplistic to be generally advisable. For example, if I have a map of a thousand items and want to insert a new one, this will simply marshal the the whole 1001 items to update the map. I'd prefer people to think about what they are doing rather than believe the driver to be doing the right thing for them when it's not in such cases.\nAlso, despite having access to the internals, the suggested implementation seems to be doing its logic in a pretty high-level way, by marshaling structs into BSON and then back into a map. If you're willing to go there, the implementation of AsUpdate is somewhat simple: marshal in as bson, unmarshal into a map, and parse the struct looking for omitempty tags to separate what you want in set vs. unset.\nFinally, that last assumption is worth consideration by itself. The logic interprets an empty field per omitempty logic as meaning \"unset it\". Very frequently people use structs with omitempty fields precisely because they don't want the field to be touched. This would be the first time the driver is transforming \"omitempty means do not marshal it\" into \"omitempty means the field should be unset\".\nFor all of those reasons, I would like to implement this logic as bson.Delta(obj1, obj2), so the update can then be much richer and more precise.\n. > We're actually only marshaling to BSON and back when the object isn't\n\na struct (in practice that's only for Raw), which is when we can't fetch its\nfields directly.\n\nI see. That sounds okay.\n\nDelta sounds attractive but doesn't quite address our requirements, because I think that in order to get any \"unset\" operations, the fields in the first object would need be non-zero, and that means that one would need to keep some object in sync with the type containing arbitrary non-zero values for all fields.\n\nWe might support a mode with Delta(nil, obj1), which is closer to your requirements.\n\nThat is, bson.Delta(T{}, x) (where x is of type T) could never return an unset operation AFAICS because all fields in T would be zero.\n\nWell, of course, because obj1 has every field unset, so there's nothing to unset. Along similar lines, Delta(x, x) will always be empty because the value is the same. These are well understood semantics for delta operations, which is most of the reason why it's a good API. The rest of the reason is that this can much more efficiently represent the delta for streaming.\n. I really doubt you haven't.\nVery frequently we have an object at hand that we want to change a field for in the database:\n// ... arbitrary changes in obj ...\n... := collection.Update(bson.M{\"$set\": ...obj.field...})\nThe proposed idea allows transforming that logic into:\nold := obj\n// ... arbitrary changes in obj ...\n... := collection.Update(bson.Delta(old, obj)))\n. Delta would return a rich update statement transforming the first object into the second. The first object would represent the result of a query in general, rather than being hand cooked. If the first object is nil, it would interpret the intention to be to blast whatever is in place and make it equal to inserting the second object, which is what you want.\n. It seems somewhat self-evident that updates to database documents frequently consider a prior state that is in memory. And yes, it's racy if there are concurrent changes in progress. Every single update has the potential to be racy, inherently. If that's a problem then the interaction design is broken.\nBut please don't worry about those details. It's not a problem you have and so there's no need to be concerned with it. We just cannot merge your proposal because it has the issues described above, sorry.\n. You basically agreed with me in a very wordy way. Update operations have the potential to be racy. Delta simply computes the delta automatically.. it doesn't make it more racy or less racy. \n. I know you are a very smart person, Roger, easily capable of understanding that a $set that is generated by a delta and a $set that is generated by hand has exactly the same potential for race conditions.\nKnowing you personally, I also understand you love to argue about simple points. We don't have to do that.\n. Sorry, but I consider the reasoning above enough for a good understanding of the problem.\nThanks for the details you provided and the code you contributed. It may end up being used.\n. I mean it. :-)\n. Thank you!\n. Thanks for the contribution, but I'm not quite sure this is the right thing to do. Timestamps in the database do not have a timezone, and people often work with timestamps in their own timezones.\nWhich documentation suggests it is UTC?\n. Right, that documentation describes stamps should be stored in UTC, and mgo does store times as UTC per the specification. It also does not corrupt the stored times pretending they are in the local timezone either. Instead, it recognizes the fact the the stored timestamp is UTC, and loads that timestamp under the local timezone per normal Go rules. In practice, that's exactly the same you get when you run time.Now, or time.Unix: a stamp that is timezone aware, and may be easily set to UTC if you call the respective method.\nSo it sounds like the best possible outcome with the rules available: applications can use timestamps as known locally per default Go behavior, and these can roundtrip through the database as UTC using both the proper conventions of MongoDB and Go.\nIt's not clear to me what exactly you're struggling with or trying to fix.\n. Thanks, I'll fix the doc. It's actually always 255kb, because it's a client-side defined setting.\n. Thanks, Gabriel.\n. Any chance we can get a test?\nAlso, wonder if we should be retrying on mgo itself. Any reason not to?\n. Merging, will fix that afterwards.\n. Thanks!\n. Thanks!\nJust restarted the failing test.\nSpeaking of tests, can we have a test that breaks so we can be sure these changes are doing what we want them to do in the future?\n. Thanks!\n. This was fixed and tested via #316.\n. Thanks!\n. This pulls in too many unrelated changes. Will merged later by doing v2 => v2-unstable.\n. The documentation is right. The < 0 is an implementation detail and can change, breaking the code if one doesn't follow the documentation.. Thanks!. @fatih is spot on.\n. Thanks!\n. Thanks. You can run the test suite by first calling \"make startdb\" on the main source directory. This will start several mongod instances to run the tests against.\n. This is looking good. I haven't merged yet as I've been in a sprint, but I'll surely test locally and get it in over the next few days.\nThanks again for the change.\n. That's merged in the v2-dev branch (6af013a1). Will be in the next release.\n. I recommend subscribing to the mailing list. It always gets important news, and allows you to participate in relevant design decisions in time to change them:\nhttps://groups.google.com/d/forum/mgo-users\n. Thanks again. If that's okay, I'd prefer to keep project details at the project page to avoid having to maintain multiple sources. I've added a trivial README.md, though, and also updated labix.org/mgo to include these testing instructions.\n. Thanks. That's merged in the v2-dev branch (b4733f6f) and will be in the next release.\n. Thanks! This was merged into v2-dev (72ad3358) and will be in the next release.\n. That's great, thanks Valeri!\nIt looks generally good. I'll add a few short notes inline.\n. Please run tests. :-)\n. Sorry for bothering, but spacing is still messed up. See lines 307 and 606.\nI'll post a couple of git hooks to make sure we don't need to fight those trivial details on reviews.\n. No, this seems ready to go in.\n. I suppose you need a new release to unblock that, rather than just a merge?\nWe can do either.\n. Merged on v2-dev on f047e079.\n. Cool, it's in.\n. Yeah, thanks for getting us to know golint's preferences, but even without debating which of these should actually be done, we simply cannot do these breaking changes without a fantastic reason.\n. It looks mostly good, but there are a few points to address:\nWhy is Stop not supported on Windows?  This is just stopping and restarting the process via supervisord\nDuplicating the whole configuration is not great. There must be a nice way to avoid it.\nWhat's the deal with the DIRECTORY environment variable?\nThe very specific `uname == 'CYGWIN_NT-6.1'` check doesn't provide a lot of confidence that other people will be able to reproduce the results.\n. We can swap supervisord for something better and that works in Windows. Any suggestions? Any idea if runit works well on Windows?\n. Yes, I'm definitely happy to merge something simpler that solves your current problem.\nTo get this merged, can you please revert the changes to Thaw/Freeze, and instead just isolate the real offending call (syscall.Kill) in a similar fashion?\n. It's looking good otherwise.\n. Sorry it's causing you some trouble. I do appreciate your effort, though. Much better to understand and polish this in isolation.\nI'll get it merged in a moment.\n. Merged into v2-dev as bfb5e400.\n. Per mailing thread:\nKyle, I have reviewed the proof of concept over the weekend, and apparently it's just mimicking the behavior of Iter.Next, but copying the byte slice out of the internal buffer.\nIf I'm right about that, a couple of questions:\n1. Have you tried to use a bson.Raw? If so, what was the impact? If you use one for the whole document, it's very close to being a raw copy of the byte slice, and we can definitely improve the performance further by fine tuning that code path. Which leads to the second point:\n2. Do you have a small self-contained benchmark we could share, preferably one that might be part of the test suite permanently? I believe I might easily improve the performance of the important code path to be satisfactory, but I must be able to tell whether we've reached satisfaction or not, and hopefully we can keep that in place so it remains the case in the future.\n. As discussed over email, commit 00004f72 turns unmarshaling into a bson.Raw into a trivial operation (~150ns locally), so we should be able to use the standard Next method and achieve the same benefits of the described proof of concept.\n. The original plan was to have something simpler as you suggest, and then came the idea of using a more flexible syntax to prevent potential future compatibility issues. That said, since I wrote this my feeling has changed back that this is not necessary, and a simpler model conforming to the current convention should work for a long time.\nThat said, there's a bug in the current proposal. The special metadata name is \"textScore\", not \"score\". The latter is the field name.\n. As I mentioned, a simpler model conforming to the current convention should work.\nSomething along the lines of:\n$textScore:score\n. There are several changes unrelated to the conversation above in the current pull request. It'd be good to trim it down to adding just that alone, and then adding a test. Other changes, if you still want to push them, may be proposed in a separate request.\n. Thanks. I'll provide a few comments inline.\n. That's it for now. Thanks again for pushing this forward.\n. Okay, this is looking good. I suspect the comments above are the last iteration.\nThanks for keeping it up.\n. Any news on this?\n. This was merged in v2-dev with b3fc1f86a and will be in the next release.\nThanks for keeping it up!\n. That's not quite right. The logic in the default cause includes the handling of some slices.\nI'm still looking at the use case, but the fix is not so straightforward if we do it.\n. Brilliant, thank you!\nWould be nice to have the benchmarks integrated in gocheck as well. The support is exactly equivalent. They just need to be made part of the suite, and the traditional (c *C) argument used instead.\n. This was merged in v2-dev in 10662809. I'll follow up on the comments via email so we can keep the conversation in a single place.\n. Thanks for the work, Valeri.  A few initial comments above.\n. Sorry, wrong button.. didn't mean to close it.\n. I'm all for having less code that we need to maintain, but the proper way to do that is to use an external library, which is the process I have followed to implement the current support in Linux, and which hopefully actually works in Windows too (I never got a proper response to whether it does work there or not, and why not, given that the library does mention support for Windows). I'd be quite surprised if there's no proper library that abstract that exact API for multiple operating systems.\nThat said, if you prefer implementing it that way, and supporting it too, that's okay too. But if we are going down the rabbit role of importing external code, we'll need to maintain that code sane and working for us. That includes file headers, fixing things such as the loose error handling, adding tests, and on the bright side getting rid of code that we're in fact not depending upon. Looks like we can get rid of quite a bit of it.\n. Having experience with other lands, I don't find Go's package management to be in a sorry state, but let's have that conversation in a different forum.\nI'm happy to support you in merging this work nevertheless. Let's just try to polish it to a state where we're both happy to maintain it in-tree.\n. Going over the comments above, I still see a number of uncovered points.\nOne easy way to ensure we're on the same page is by responding to the actual comments, even if it's just a \"Done.\"  This gives a chance for both of us to tell whether things were completed, or whether there was a disagreement, and what was it.\n. Also, can we please drop the \"_\" prefix from all of the C function names? These are just normal functions in our own namespace.\n. There are comments in #20 that were not addressed. Even some of the ones marked as \"Done\".\n. Okay, please let me know once this is actually ready for review.\n. Ok, just did a quick scan with a few comments, but I'll need to go back to it later.\nMeanwhile, where are the tests?\n. I might, but where are the tests?\n. I don't understand what kind of significant work might take place for that. Registering a new test suite requires three or four lines of code.\nEither way, it doesn't make sense to merge this without tests. Please also make sure that the tests that are being submitted with the logic actually exercise it well, and are passing for real.\n. Also, what do you mean by \"break Kerberos tests\"?  The test that exists today are trivial in their implementation and design. We shouldn't be breaking them out or removing them from where they are.\n. It might be best to submit that new logic you're talking about in a separate branch before polluting this PR, actually, so I can at least have an idea about what's being done.\n. Okay, that looks reasonable, but let's name it what it is: KerberosSuite, and have the suite defined right above kerberosFlag instead of having a new file for it. We can reorganize it after it lands if necessary.\nIf that's what it is, we can merge it in this PR so you can make sure tests pass before we merge this.\n. Thanks! Can you please run the tests on Linux as well, to make sure we're still good there after the changes?\n. Thanks for this, Gabriel. Looks quite nice. I'll add a comment inline, but the main thing this is missing is a test. This logic can be exercised by default, right? If so, we need a clean test that always run with it.\n. Thanks!\n. Thanks for the build fix, happy to apply it.\nThat said, we cannot apply the suggested rename as-is. People out there are likely depending on this name, and it was modified in the server right before the 2.4 release:\nhttps://jira.mongodb.org/browse/SERVER-8501\nWe should support the two names instead.\nTo avoid another roundtrip, I have applied the two fixes to v2-dev. Thanks again for the changes.\n. Sorry, it was late and I forgot to push it. Should be there now.\n. Thanks Mike.\n. I have merged this to v2 by mistake, and quickly reverted back.\nThis is now properly merged in v2-dev with 4d7c7241 and will be in the next release.\n. Thanks for the changes.\n. That's improving the previous approach with caching, but the underlying concept is still not good. There's no reason to pay the cost of marshaling of these structs simply for obtaining a well defined order. It's also pretending to work for things that actually do not work. Marshal will marshal anything, including things with an undefined order. These should continue to break.\nWe should be able to easily have an ordering mechanism for the structs we care about without resorting to marshaling.\n. Thanks for that. I don't have energy right now to review this, but I'll try to get over it in the next couple of days, or over the weekend in the worst case.\n. The change looks good overall, thank you.\nThere are trivial tweaks to be made, such as ensuring that the tests run whenever the server at hand is SSL-capable, instead of requiring manual procedures that imply the test would be rarely run.\nI'll merge this right now, though, and will do these tweaks myself afterwards. Thanks again.\n. Okay, I performed the mentioned tweaks, and also fixed the test so that it works if run multiple times (second time would fail to add the Admin user because it had no auth). That was done by integrating the new server in the common init and cleanup workflow, whenever SSL is available.\nYou can see all those tweaks in c1b9c9f28. Then, the auth test was moved to the common auth test file in dc6fe4aaa.\nPlease let me know if you have any questions.\n. Hey Gabriel,\nI wonder if we should have these in. In my experience the error codes are quite unreliable, changed over time, and there are often multiple codes for the same error (see IsDup for an idea). Being just partial also hints of suboptimal value.\nIt feels like it might be worth continuing with the pattern we have at the moment: important errors are either represented by first time constants (ErrNotFound) or have comprehensive checkers (IsDup). Other errors are easy for people to have their own verifiers or code constants in the local code.\n. Would you mind to describe in more detail what the problem being solved is?\nThere are still questions about how the internal implementation of bulk handling will look like, and what the meaning of the count method would be. There is the potential for grouping operations, for example, so would that mean \"count the number of requests\" or \"count the number of individual changes\".\nFor those reasons, it would probably be best to wait until the Bulk API is fully fleshed out before we add such introspection methods.\n. Thanks, I'll consider this further once the bulk API is completed. For now, unfortunately I'll need to delete this pull request because I need to rename the v2-dev branch to enable us to make it visible via gopkg.in (it needs to be -unstable), and unfortunately GitHub doesn't offer any way to rename a branch politely.\n. Hey Mike,\nSorry for the feedback. This is looking good, and was already in my agenda for merging today. I'm going to merge it directly to v2 since it feels independent enough to not cause any harm to people that are not depending on the feature yet. Hopefully this will give you a hand there.\n. Yes, I'll have a look at this for the next release. Thank you.\n. The change looks great. There are only a few minors which I will comment inline.\n. That's about it. A few entries, but nothing fundamental. Thanks for these changes.\n. While thinking about that change further, there is a more fundamental issue. The cluster.err variable is propagated on any error, irrespective of whether the failure happened with that specific server. Now imagine there are 5 servers in a replica set. If one of the servers turns out to be misconfigured, it's nice to reject communication with it, but it would be bogus to fail against any other correct server with that error.\nMakes sense?\n. > Once set, the error is only propagated insofar as spawnSync for that server is short-circuited (so communication with that server is rejected). The other goroutines for possibly properly configured servers proceed as usual.\nThat's not quite true. Consider which code paths can lead to the return of cluster.err, and what is the relationship of the call site with the bad server.\n. Thanks!\n. The change is looking good, and I'm happy to merge it. There's one detail which would be good to address before we do that: some of these tests are now quite ineffective at testing anything at all, by just dialing and making sure there are no errors. There are good ways to verify that the offending server is not being communicated with:\n1. You can get the set of current live servers from session.LiveServers()\n2. You can inspect the log to verify that the expect error was reported, via c.GetTestLog() in the test method\nWould you mind to improve the tests a little so we can be sure they're exercising what they seem to be?\n. This was merged into v2-dev, with the trivial changes from e0d7e593b, and will be in the next release.\nThanks for the contribution.\n. Thanks!\n. Thank you!\n. Sorry, but this does not look okay. It's changing the Insert API from being a local operation into running remote commands arbitrarily. We can fix this issue in better ways by being smart at execution time, but I'd prefer to wait to integrate such smartness once the bulk API is completed, integrating recent bulk features, so that we do the logic correctly for both cases.\nCan you please open an issue about it so we can track the work?\n. We shouldn't have an option that turns the documentation wrong. The proper way to do this is to have methods that do not do the caching instead. We'll introduce those soon.\n. I'm still on the fence about this one. It feels surprisingly costly for such a trivial operation.\n. The trade-off doesn't seem so clear to me as it is to you. You see it as a bug, and I see it as a feature explicitly left out for lack of a reasonable way to handle it. Requiring 3000 allocations to encode 1000 values which contain 3 arrays with a couple of integers each sounds bad, even ignoring the fact that they need to be copied. The impact is disproportional to the expectation of what such an operation would do.\nI'm not rulling it out yet, but it's not so straightforward a choice to me.\n. Okay, I'd be happier about that implementation as well.\n. We very much follow go fmt rules, as you can observe by how small the patch set is. Sometimes things do go in without following the rules, though (merges, etc).\nI'm happy to apply your change set, either way, but not on v2. Any changes should go into v2-unstable.\n. There are a few conditions that we use in the json unmarshaler logic which seem appropriate for the text marshaler as well. Can you please have a look and let me know what you think?\nAlso, I believe the marshaler should take the receiver by value, and not as a pointer. It is a string, so there's no benefit in receiving it as a pointer in this case.\n. Thanks for these changes. The only thing to fix here that I could notice is that strange special casing of the a string which is a single empty space. If we want to handle empty spaces as an unset id, any number of spaces should have the same effect.\nThe other bit is that this is missing tests. Otherwise, it looks good.\n. As a hint, the check you want there is likely something along these lines:\nif len(bytes.TrimSpace(data)) == 0 { ... }\n. One more detail: can you please rebase this against v2-unstable?\n. Thanks, I'll have look.\n. Next version is coming in the next couple of days. Something along these lines will be in. Will probably take your patch and tweak for usual conventions.\n. Thanks!\n. Oops.. this patch sneaks in a serious breakage in the cluster synchronization logic which is completely unrelated to fixing the statistics. I'll let you guess which line does that. :-) \n. Also note that existent tests prevented this from being released, which probably means they were not run.\n. Sounds good. It takes slightly more than capitalizing the function, but the idea is sound.\n. Thanks, this is definitely an often-requested feature, and I'll make sure to get it in place for you. #97 was a bit closer to the end result, but still a bit off in that we need the result to be a pointer, which is what the public API expects. I'll get something in and let you know via this PR.\n. We can include stock support for the ssl flag somewhere in the driver, but not in the main package like this. This would force the inclusion of crypto/tls and all its non-trivial dependencies on every application building the driver. Instead, I'd prefer to have an auxiliary package that may be explicitly imported to introduce the SSL support.\n. By the way, thanks for the contribution. I appreciate the interest, and would be glad to figure out with you a proper way to do this.\n. The txn package works in MongoDB 2.4 and 2.6, and probably earlier ones. The error you describe doesn't make sense to me, as it reports an attempt to store a $ field, which $pullAll should of course not be as this is an update operator.  The pull request also does not make sense, as it is turning such an update operator into a normal field, thus breaking the package (there is a test suite you can run to test your changes).\nPlease follow up in issue #87.\n. Thanks for the change. The field name is required because the default is to fully lowercase the field name, or \"bucketsize\" in this case, which isn't correct.\n. By the way, shouldn't the bucket size be an integer value?  Does it make sense to provide fractional bucket sizes?\n. Nevermind.. confirmed in the C++ code that it is a double.\n. This patch would prevent people from using YAML's null value to initialize Go values, for no benefit apparently. As you point out in the proposal, it's currently supported and trivial to choose to have a zero value instead of the initialized value, if that's what you want.\n. Can you provide more details about exactly what you are asking for and why?  Will be happy to provide you with an example once you explain what you are trying to achieve and are being unable to. \n. This is a dup of #85.\n. Just a couple of trivials. Please ping me when done and I'll merge it.\n. We discussed this concept out of band.\n. It doesn't look like this test is doing much to test that issue. We have quite a lot of tests in cluster_test.go verifying that the values are basically right.\n. Sounds good. The test as it is is hugely dependent on subtle internal details either way. A regression that breaks this logic could trivially turn the test irrelevant in one go.\n. Thanks!\n. Thanks for the contribution. Let's please avoid adding a new method, as it's unrelated to the bug being fixed and we don't need to agree on it right now (LastError.Found is misleading, for example). Then, we do need tests for it. You can start servers in your machine by running \"make startdb\" before running the test suite. If you don't want to do this, please just open an issue including a snippet that demonstrates the bug and I'll fix it locally.\n. Also, please mention which server versions you are testing this under.\n. Sorry, this cannot wait. I've fixed it and will tag a new release shortly.\n. This is out.\n. Supporting such a wildcard option would mean preventing extending that structure to support further options in the future in a backwards compatible way, as we've done multiple times over the history of the driver (and have just done so again days ago). It might be fine to support actual options, though.\n. This is introducing the use of FieldByName twice in the fast path of a loop. We don't have a single call of that method so far, and I'm not quite convinced doing so is a good idea. If you have a benchmark showing this has a positive impact rather than a negative one in relevant cases, please feel free to reopen the ticket.\n. Thanks, but I believe this is grammatically sound, and also irrelevevant if you understood the intended meaning.\n. Hi Diana,\nI'm afraid your use case is beyond what mgo is meant to offer. For emulating a proxy, you'll find many shortcomings in this driver, because its goal is to abstract away the wire protocol and command mechanism of the server into something that is convenient for developers to use when building their applications, not to be a transparent mechanism that a proxy server can use to fully introspect actions and dumbly replay incoming requests into the server.\n. We can't merge this unless we find a non-artificial reason why a normal application would use these details. As far as I can see, it makes no sense to change batch size during iteration, and nobody ever asked me to expose the cursor ID, since the driver is abstracting iterations in a very convenient way, precisely so that they don't need to care about these internals.\n. The fundamental question is why do you want to create a new iterator? This is the sort of task the driver itself should be doing for you. There's a very special reason why NewIter exists, and one that took some significant thinking time. I wasn't keen on exposing it at all to begin with, and it's not an excuse to open the gates to functionality that is added without justification.\nRegarding creating a new driver, if your use case is incompatible with the goals of this driver, I see no problem with creating another independent driver that is tailored for you own use case. This driver is not meant to be a proxy, and I'm not keen on adding functionality that is completely irrelevant for every other use case but the proxy one.\n. I've never had someone mention that use case, and we can easily add cursor id to the information we log if that becomes necessary. That's what I meant by non-artificial. I'm interested in what is your use case for this, not the potential use case of a theoretical user. Once more, what is your use case for creating iterators manually, and why do you need a cursor id? \n. Closing for lack of feedback.\n. Thank you!\n. Please note I've tweaked the logic so the file isn't removed if index creation fails.\n. Thanks! Looks like it's not properly indented, but I'll fix that locally.\n. Thanks, sounds reasonable.\nCan we please have a test for this?\n. Okay, that sounds fine. There's a small question here, though. What is the semantics of IsDup when one of the errors is a dup but the other is not?\n. The reason bulkError is not exposed is precisely because it is not yet representing everything it needs to for bulk scenarios. If the idea was to have a single error always, it wouldn't make sense to wrap it in such a way.\n. Maybe we need something along the lines of BulkError.Has or BulkError.Contains, which takes a predicate function of the IsDup style. Another option would be to say that such standard predicates return true if any one of the errors match, which doesn't smell so great, but maybe it's the useful thing to do. We need to think through what would be the correct way to handle such multi-error cases.. what will we want to do when we have one of these at hand?\n. After sleeping on this, we should make IsDup return true when all errors in a bulk update are dup errors, which means it would be fine to make IsDup return true when there's on a single error and that error is a dup error. That seems like the most useful and safe approach.\n. Why? I was tempted to adopt your suggestion, for the reason exposed above.\n. Different packages really have different fields with different potential attributes and meaning. We really can't promise full compatibility with json there, and it's also questionable whether this is always what's desired.\n. Raymond, what was pushed doesn't reflect our agreement. What happened?\n. Thanks for the follow up, Raymond. I'll add a few initial comments here.\n1. The +/-infinity approach sounds fine.\n2. The precision details are not clear to me. Why would 2E-50 have more precision than supported by MongoDB, when both the significand and the exponent are well within the range of the decimal128 type? Rationals are arbitrary-precision, and MongoDB is not, but both should be able to represent 2E-50 fine.\n3. Rather than storing a precision within the Decimal, we should instead compute the precision based on the actual rational value (the divisor, more precisely). This should make it not only roundtrip correctly, but work appropriately once operations are performed with multiple decimal values as well. It also removes the need for manual precision methods and parameters.\n4. As we talked, the String method is a strong convention in Go, and needs to have the specific prototype we discussed, without arguments. Otherwise it's not implementing the proper interface and will not work properly in contexts where it should.\n5. As we agreed, none of the extra methods and types should be exposed in the bson package. The external interface should remain untouched. For testing, you can create public functions within export_test.go that just call the private ones, and then use these in tests. That will prevent these functions from actually shipping in the package outside of testing.\n6. Let's please not create dummy types such as uint32x4 and Uint64x2... Instead, we can use the real underlying types ([4]uint32 and [2]uint64).\nPlease let me know if you have any questions.\n. Alternative implementation introduced in https://github.com/go-mgo/mgo/commit/14a4475d4d32049500ac89e1499c46be62bf7bdf.\n. Looks good, thanks.\n. Please see conversation in #103. I'd suggest agreeing on something before spending time on code.\n. Thanks!\n. This is useful for me to understand what sort of feature you need, but the API is not quite there. It's too close to the internal wire protocol implementation instead of the user-oriented semantics that the driver offers today.\n. Sounds reasonable, thanks.\n. Yeah, sounds reasonable. Thanks!\n. I'm not entirely sure we want to change the semantics here. Consider the fact we need to allocate an extra buffer for each single binary field we find, instead of using a single byte slice which in the case of the mgo driver was already allocated and will be thrown off if unused.\nAlso, if we want to make guarantees about not using the source buffer, there's more to be done. Consider Raw, for example. Will we pay the cost of double-allocation for every Raw unmarshalled?\n. >  This change makes the binary data handled more similarly to how the string data is handled.\nStrings are very inefficient to handle in general because they are immutable, so making it more similar to them isn't a great reason to change it.\n\nIf you read in some BSON documents, with binary data fields, into individually allocated buffers, and then marshal them to bson.M variables, then each of the buffers will not be garbage collected until the bson.M variables can be. You're binding the lifecycle of the BSON document buffer to the lifecycle of the unmarshaled data structure. With this change the buffer can be collected as soon as its last regular reference is. If one is creating single use I/O buffers, then my change will reduce memory use, and if one is reusing a single I/O buffer, then my change will make it possible.\n\nIndeed. The counter side is that your change will make every single binary field become an extra allocation and an extra copy of the data, which means decoding a GridFS file and its 256k chunks will allocate twice as much memory, create twice as much garbage, and become a copying algorithm rather than a zero-copy one.\nWithout proper analysis I'm not convinced this is a win at all.\n. Thanks!\n. Hey Gabriel,\nThis doesn't look quite right, and running tests confirms it is indeed introducing a bug. The new logic should be in default, shouldn't it?  Have you tried to run tests there?\nAlso, we need a test demonstrating why the current logic is broken, and to prevent regressions.\nThanks for the fix.\n. Thanks Gabriel.\n. I appreciate the contribution, but unfortunately that's not something we should be doing. When we return an error, we indicate to developers that this operation can fail, and that the error should be checked. Neither is the case here.\nI suggest doing something like ioutil.NopCloser in the affected logic.\n. Thanks for the contribution. A couple of comments above.\n. Thank you!\n. This is missing a better description of the problem, and some tests, but perhaps there's a more fundamental problem with the proposal. BSON indeed only supports maps with string keys.. the way we represent that in Go is by enforcing the use of maps with key elements typed as strings, which are also more convenient to use in general. I'm not sure there's much benefit in loosening these rules.\nI'm closing down the issue on that basis, but please feel free to complement the report.\n. Sorry, but I still don't understand the issue. Who will introduce the map[interface{}] type?  Do you have sample code?\n. Actual code, please?\n. Sorry for bothering, but can you please provide something I can actually run locally? It should be self-contained and runnable, without using anything that is not relative to the offending package (mgo, bson) or the standard library. Without that there's no way I can tell what the code is in fact doing, or what the types involved are.\n. As I mentioned, it should be self-contained and runnable, without using anything that is not relative to the offending package (mgo, bson) or the standard library. Without that there's no way I can tell what the code is in fact doing, or what the types involved are.\n. Hint: you are using third-party code that does decoding. I bet this is actually what introduces the map[interface{}]interface{} type. If that's the case, then there's nothing wrong with the fact the bson package is strict about accepting map[string] only. That's indeed what's supported by BSON (the spec).\n. Okay, so mgo/bson is doing exactly the same as encoding/json by telling you that in fact encoded maps must use string keys. I think both packages are correct. If you want to map interface{} keys into string keys, that's easy to do with conversion code. \n. Thanks, merging it. I'll add a test for it as well.\n. Thank you! The package path is fine. gopkg.in/mgo.v2 vs. mgo.v2-unstable are different branches of the same repository. You're targeting the unstable one, so the package path should be unstable as well. \n. Please see conversation in the issue itself.\n. This is a bit of a leap, but perhaps it is indeed the most useful thing to do.\nCan you please add some tests covering that?\n. No, it's definitely going in. I've just been giving the test harness and CI some love last week.\n. Thanks @liviosoares\n. That's nice. I've been meaning to look into buffer reuse, but always thought about the harder case which is unmarshaling. This is a much easier win.\nWill need a few tweaks since we should expose that via an interface such as bson.NewEncoder instead of MarshalBuffer, but I can tweak that on my end.\n. No, that looks fine. I'll get it in early this week.\nThanks for the fix!\n. Thank you!\n. That's not quite right, as you can observe by the broken tests.\nPlease open an issue with a small self-contained reproducer exposig the problem clearly and I'll be happy to help.\n. Thanks!\n. Before anything else, thanks for reaching out.\nCan you provide some details about the use case?  It feels like the proper way to handle this would be to simply call Next, as you indicate. If we implement HasNext, we need it to be true. In other words, if HasNext returns true, Next should never return false. This conflicts with the proposed implementation.\n. Fair enough.\nCan we please invert the logic and call the method Done. This reflects better the fact we have certainty about it being done, but we have no certainty about really having more data or not (tailable or not).\nThen, can we please have one or more tests specific for that functionality that verify that each of these branches is actually doing the right thing, rather than spreading the logic out through other unrelated tests?\nThanks again.\n. Thank you!\n. There's a bug here. iter.err may be set while there's still data available for consumption. I'll need to tweak the implementation.\n. I've considered a similar idea before, but I'm still slightly uncomfortable with merging something like this alone. It feels too simplistic to be generally advisable. For example, if I have a map of a thousand items and want to insert a new one, this will simply marshal the the whole 1001 items to update the map. I'd prefer people to think about what they are doing rather than believe the driver to be doing the right thing for them when it's not in such cases.\nAlso, despite having access to the internals, the suggested implementation seems to be doing its logic in a pretty high-level way, by marshaling structs into BSON and then back into a map. If you're willing to go there, the implementation of AsUpdate is somewhat simple: marshal in as bson, unmarshal into a map, and parse the struct looking for omitempty tags to separate what you want in set vs. unset.\nFinally, that last assumption is worth consideration by itself. The logic interprets an empty field per omitempty logic as meaning \"unset it\". Very frequently people use structs with omitempty fields precisely because they don't want the field to be touched. This would be the first time the driver is transforming \"omitempty means do not marshal it\" into \"omitempty means the field should be unset\".\nFor all of those reasons, I would like to implement this logic as bson.Delta(obj1, obj2), so the update can then be much richer and more precise.\n. > We're actually only marshaling to BSON and back when the object isn't\n\na struct (in practice that's only for Raw), which is when we can't fetch its\nfields directly.\n\nI see. That sounds okay.\n\nDelta sounds attractive but doesn't quite address our requirements, because I think that in order to get any \"unset\" operations, the fields in the first object would need be non-zero, and that means that one would need to keep some object in sync with the type containing arbitrary non-zero values for all fields.\n\nWe might support a mode with Delta(nil, obj1), which is closer to your requirements.\n\nThat is, bson.Delta(T{}, x) (where x is of type T) could never return an unset operation AFAICS because all fields in T would be zero.\n\nWell, of course, because obj1 has every field unset, so there's nothing to unset. Along similar lines, Delta(x, x) will always be empty because the value is the same. These are well understood semantics for delta operations, which is most of the reason why it's a good API. The rest of the reason is that this can much more efficiently represent the delta for streaming.\n. I really doubt you haven't.\nVery frequently we have an object at hand that we want to change a field for in the database:\n// ... arbitrary changes in obj ...\n... := collection.Update(bson.M{\"$set\": ...obj.field...})\nThe proposed idea allows transforming that logic into:\nold := obj\n// ... arbitrary changes in obj ...\n... := collection.Update(bson.Delta(old, obj)))\n. Delta would return a rich update statement transforming the first object into the second. The first object would represent the result of a query in general, rather than being hand cooked. If the first object is nil, it would interpret the intention to be to blast whatever is in place and make it equal to inserting the second object, which is what you want.\n. It seems somewhat self-evident that updates to database documents frequently consider a prior state that is in memory. And yes, it's racy if there are concurrent changes in progress. Every single update has the potential to be racy, inherently. If that's a problem then the interaction design is broken.\nBut please don't worry about those details. It's not a problem you have and so there's no need to be concerned with it. We just cannot merge your proposal because it has the issues described above, sorry.\n. You basically agreed with me in a very wordy way. Update operations have the potential to be racy. Delta simply computes the delta automatically.. it doesn't make it more racy or less racy. \n. I know you are a very smart person, Roger, easily capable of understanding that a $set that is generated by a delta and a $set that is generated by hand has exactly the same potential for race conditions.\nKnowing you personally, I also understand you love to argue about simple points. We don't have to do that.\n. Sorry, but I consider the reasoning above enough for a good understanding of the problem.\nThanks for the details you provided and the code you contributed. It may end up being used.\n. I mean it. :-)\n. Thank you!\n. Thanks for the contribution, but I'm not quite sure this is the right thing to do. Timestamps in the database do not have a timezone, and people often work with timestamps in their own timezones.\nWhich documentation suggests it is UTC?\n. Right, that documentation describes stamps should be stored in UTC, and mgo does store times as UTC per the specification. It also does not corrupt the stored times pretending they are in the local timezone either. Instead, it recognizes the fact the the stored timestamp is UTC, and loads that timestamp under the local timezone per normal Go rules. In practice, that's exactly the same you get when you run time.Now, or time.Unix: a stamp that is timezone aware, and may be easily set to UTC if you call the respective method.\nSo it sounds like the best possible outcome with the rules available: applications can use timestamps as known locally per default Go behavior, and these can roundtrip through the database as UTC using both the proper conventions of MongoDB and Go.\nIt's not clear to me what exactly you're struggling with or trying to fix.\n. Thanks, I'll fix the doc. It's actually always 255kb, because it's a client-side defined setting.\n. Thanks, Gabriel.\n. Any chance we can get a test?\nAlso, wonder if we should be retrying on mgo itself. Any reason not to?\n. Merging, will fix that afterwards.\n. Thanks!\n. Thanks!\nJust restarted the failing test.\nSpeaking of tests, can we have a test that breaks so we can be sure these changes are doing what we want them to do in the future?\n. Thanks!\n. This was fixed and tested via #316.\n. Thanks!\n. This pulls in too many unrelated changes. Will merged later by doing v2 => v2-unstable.\n. The documentation is right. The < 0 is an implementation detail and can change, breaking the code if one doesn't follow the documentation.. Thanks!. ",
    "sheki": "Added a test case \n. A small ping :)\n. Thanks a ton. Any place I can wait for Releases?\n. Added a test case \n. A small ping :)\n. Thanks a ton. Any place I can wait for Releases?\n. ",
    "vkarpov15": "Comments addressed. Sorry about the whitespace issues, vim's copy/paste seems to maul tabs\n. Yep good call\n. Yeah that would be good. And I really need to get myself a real Linux machine instead of just SSH-ing into EC2 boxes :)\n. Hey @niemeyer any more comments re: this?\n. Any idea when? I got a code review for mongo-tools blocking on this :)\n. I think merging should be sufficient for us, we just need the gitspec in the git repo so gpm can do its thing\n. Hi @niemeyer, any feedback on this? I'm going to put in a PR for Windows kerberos soon that depends on these changes.\n. 1) Supervisor doesn't work on native windows, and cmd.Exec() runs commands against the native windows shell, so supervisor will blow up.\n2/3) The reasoning for the DIRECTORY variable and config duplication is the usual cygwin vs native mess: even if you run supervisor on cygwin, mongod still needs the native windows paths because the mongod windows executable isn't built with cygwin. However, when you run supervisor on cygwin, %(here)s gives you the cygwin path. In addition, mongod on windows doesn't support --fork.\nI agree the configuration isn't very general, but I don't have any better ideas for getting the tests to run on MCI EC2 windows boxes without swapping out supervisor entirely.\n. Not terribly familiar with runit. I know the node driver has its own windows-specific cluster manager using the windows taskkill command. It wouldn't be too difficult to adapt such a codebase for Go (all you really need is a clean API for killing and starting a process, which should be easy enough with build tags), and would give us the added benefit of not having a dependency on python for testing.\nHowever, I think that's outside the scope of this particular PR, since I just need the Kerberos tests and those don't require make startdb. If I just got rid of the changes to setup.sh and testdb/supervisord-cygwin.conf would this PR be mergeable?\n. Ok I removed the supervisor changes and isolated syscall.Kill as per your suggestion\n. Ok see how this one strikes you. Also keep in mind that this is blocking my work on #20, because without this change my workflow is a mess. Not my best idea to put this as a separate PR.\n. In general, I'd rather not make too many changes to kerberos_sspi.*, because those are copied directly from Christian's code, and I'd like to be able to import future bug fixes without too much work. I'd appreciate it if you have a better way of pulling in that code though, I agree the style of kerberos_sspi.* doesn't match the rest of the code and that's unpleasant.\n. I don't know the exact reasoning why Cyrus SASL doesn't work for Kerberos on Windows. Probably due to the fact that Windows-flavored Kerberos enables you to get a TGT ticket in the process itself without going through kinit. As far as I can tell, that's the most obvious difference between these two APIs. Regardless, using SSPI's API on Windows is the established standard for mongodb clients, both the shell and the handful of drivers that implement this use it.\nGiven the sorry state of go's package management, I think its a better idea to keep the code in there. I'll go in and make kerberos_sspi conform to some reasonable stylistic standards.\n. Thanks. Let me know if you have any more comments on the most recent iteration, I did what I could to clean up the kerberos_sspi files.\n. Yeah I hadn't pushed them yet, sorry. Subsequent commit contains them. I just wanted to open up a new PR because I merged v2-dev into this branch and so the previous PR had a lot of noise.\n. Go ahead and review, I'm reasonably certain I've addressed all your comments.\n. Thanks for comments, I'll fix them in a bit. Re: tests, working on that, that will require moving kerb tests to a separate suite for now. My current testing setup overwrites existing tests and so isn't mergable.\n. Hey Gustavo, just a gentle reminder to see if you have any more comments.\n. In a separate branch and ready to merge into this one. Do you want them in\nthis branch? There's some significant work there to break Kerberos tests\noff into their own suite so they don't go through the normal setup and\nteardown.\nOn Mon, Sep 22, 2014 at 12:30 PM, Gustavo Niemeyer <notifications@github.com\n\nwrote:\nI might, but where are the tests?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/go-mgo/mgo/pull/25#issuecomment-56400243.\n. See here for diff. The general idea is:\n\n1) have a separate test suite, RemoteSuite, for tests that don't require a local mongod setup. With this, the Kerberos tests no longer have to use the Setup() and TearDown, so the tests can run on Windows.\n2) I switched to using the official drivers Kerberos testing host instead of the mms one, because its the standard practice, and SSPI doesn't really do keytabs as far as I know.\n3) Because SSPI requires you to acquire your kerberos ticket in-process, I added an environment variable for setting the password that gets passed to SSPI\n. Ok I merged in the test suite. Here's a screen cap of the windows tests passing:\n\n. Tests now pass on ubuntu 12.04 as well:\n\n. Hey Gustavo, just a ping for any more comments\n. Also fixed an incorrect authentication mechanism name\n. @niemeyer I unfortunately don't see the MONGODB-CR change in v2-dev. Would go a long way to getting our tools back on the main driver and off of our fork :)\n. Comments addressed. Sorry about the whitespace issues, vim's copy/paste seems to maul tabs\n. Yep good call\n. Yeah that would be good. And I really need to get myself a real Linux machine instead of just SSH-ing into EC2 boxes :)\n. Hey @niemeyer any more comments re: this?\n. Any idea when? I got a code review for mongo-tools blocking on this :)\n. I think merging should be sufficient for us, we just need the gitspec in the git repo so gpm can do its thing\n. Hi @niemeyer, any feedback on this? I'm going to put in a PR for Windows kerberos soon that depends on these changes.\n. 1) Supervisor doesn't work on native windows, and cmd.Exec() runs commands against the native windows shell, so supervisor will blow up.\n2/3) The reasoning for the DIRECTORY variable and config duplication is the usual cygwin vs native mess: even if you run supervisor on cygwin, mongod still needs the native windows paths because the mongod windows executable isn't built with cygwin. However, when you run supervisor on cygwin, %(here)s gives you the cygwin path. In addition, mongod on windows doesn't support --fork.\nI agree the configuration isn't very general, but I don't have any better ideas for getting the tests to run on MCI EC2 windows boxes without swapping out supervisor entirely.\n. Not terribly familiar with runit. I know the node driver has its own windows-specific cluster manager using the windows taskkill command. It wouldn't be too difficult to adapt such a codebase for Go (all you really need is a clean API for killing and starting a process, which should be easy enough with build tags), and would give us the added benefit of not having a dependency on python for testing.\nHowever, I think that's outside the scope of this particular PR, since I just need the Kerberos tests and those don't require make startdb. If I just got rid of the changes to setup.sh and testdb/supervisord-cygwin.conf would this PR be mergeable?\n. Ok I removed the supervisor changes and isolated syscall.Kill as per your suggestion\n. Ok see how this one strikes you. Also keep in mind that this is blocking my work on #20, because without this change my workflow is a mess. Not my best idea to put this as a separate PR.\n. In general, I'd rather not make too many changes to kerberos_sspi.*, because those are copied directly from Christian's code, and I'd like to be able to import future bug fixes without too much work. I'd appreciate it if you have a better way of pulling in that code though, I agree the style of kerberos_sspi.* doesn't match the rest of the code and that's unpleasant.\n. I don't know the exact reasoning why Cyrus SASL doesn't work for Kerberos on Windows. Probably due to the fact that Windows-flavored Kerberos enables you to get a TGT ticket in the process itself without going through kinit. As far as I can tell, that's the most obvious difference between these two APIs. Regardless, using SSPI's API on Windows is the established standard for mongodb clients, both the shell and the handful of drivers that implement this use it.\nGiven the sorry state of go's package management, I think its a better idea to keep the code in there. I'll go in and make kerberos_sspi conform to some reasonable stylistic standards.\n. Thanks. Let me know if you have any more comments on the most recent iteration, I did what I could to clean up the kerberos_sspi files.\n. Yeah I hadn't pushed them yet, sorry. Subsequent commit contains them. I just wanted to open up a new PR because I merged v2-dev into this branch and so the previous PR had a lot of noise.\n. Go ahead and review, I'm reasonably certain I've addressed all your comments.\n. Thanks for comments, I'll fix them in a bit. Re: tests, working on that, that will require moving kerb tests to a separate suite for now. My current testing setup overwrites existing tests and so isn't mergable.\n. Hey Gustavo, just a gentle reminder to see if you have any more comments.\n. In a separate branch and ready to merge into this one. Do you want them in\nthis branch? There's some significant work there to break Kerberos tests\noff into their own suite so they don't go through the normal setup and\nteardown.\nOn Mon, Sep 22, 2014 at 12:30 PM, Gustavo Niemeyer <notifications@github.com\n\nwrote:\nI might, but where are the tests?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/go-mgo/mgo/pull/25#issuecomment-56400243.\n. See here for diff. The general idea is:\n\n1) have a separate test suite, RemoteSuite, for tests that don't require a local mongod setup. With this, the Kerberos tests no longer have to use the Setup() and TearDown, so the tests can run on Windows.\n2) I switched to using the official drivers Kerberos testing host instead of the mms one, because its the standard practice, and SSPI doesn't really do keytabs as far as I know.\n3) Because SSPI requires you to acquire your kerberos ticket in-process, I added an environment variable for setting the password that gets passed to SSPI\n. Ok I merged in the test suite. Here's a screen cap of the windows tests passing:\n\n. Tests now pass on ubuntu 12.04 as well:\n\n. Hey Gustavo, just a ping for any more comments\n. Also fixed an incorrect authentication mechanism name\n. @niemeyer I unfortunately don't see the MONGODB-CR change in v2-dev. Would go a long way to getting our tools back on the main driver and off of our fork :)\n. ",
    "ghost": "Then do I understand correctly that you prefer the following syntax?\ngo\nquery4 := collection.Find(nil).Sort(\"score:{$meta:textScore}\")\nSo the select will be automagically created from the Sort function, isn't it?\nI can give a try to this idea and pull-req again.\nAnd yes, you are right that my code right now is wrong.\n. I miss understood your comment. I fixed the issue. Does it look ok now? Anything to improve?\n. There is two commits that add two features:\n- text indexes\n- score sort\nCan you point me to other unrelated changes?\nI'll add some tests.\n. I just added a test.\nDoes it look ok now? Does my implementation of the text indexes makes sense?\n. I think I addressed all your comments.\nI can not remove all the weight support, it's needed by the index. I use indexSpec as return value of parseIndexKey to reduce the amount of return values. Is this ok?\n. Another iteration :)\n. I was waiting for you to answer to my comments on your comments.\n. Updated, now I understood your point on the text indexes, and I hope I fixed it.\n. Great!!\nThank you for keeping with it.\n. After creating this, I noticed that a similar PR already exists: https://github.com/go-mgo/mgo/pull/281\n. Then do I understand correctly that you prefer the following syntax?\ngo\nquery4 := collection.Find(nil).Sort(\"score:{$meta:textScore}\")\nSo the select will be automagically created from the Sort function, isn't it?\nI can give a try to this idea and pull-req again.\nAnd yes, you are right that my code right now is wrong.\n. I miss understood your comment. I fixed the issue. Does it look ok now? Anything to improve?\n. There is two commits that add two features:\n- text indexes\n- score sort\nCan you point me to other unrelated changes?\nI'll add some tests.\n. I just added a test.\nDoes it look ok now? Does my implementation of the text indexes makes sense?\n. I think I addressed all your comments.\nI can not remove all the weight support, it's needed by the index. I use indexSpec as return value of parseIndexKey to reduce the amount of return values. Is this ok?\n. Another iteration :)\n. I was waiting for you to answer to my comments on your comments.\n. Updated, now I understood your point on the text indexes, and I hope I fixed it.\n. Great!!\nThank you for keeping with it.\n. After creating this, I noticed that a similar PR already exists: https://github.com/go-mgo/mgo/pull/281\n. ",
    "3rf": "Ported the tests to gocheck. One potential issue: when running the original BenchmarkFindIterRaw benchmark, on my laptop I get\n BenchmarkFindIterRaw        2000      99252834 ns/op\nbut after porting the code to gocheck I get\nPASS: session_test.go:3261: S.BenchmarkFindIterRaw        200     559879123 ns/op\nwhich is a whole order of magnitude longer ns/op. Any idea what could be causing this? The benchmarks in the bson package didn't increase in time at all\n. Ported the tests to gocheck. One potential issue: when running the original BenchmarkFindIterRaw benchmark, on my laptop I get\n BenchmarkFindIterRaw        2000      99252834 ns/op\nbut after porting the code to gocheck I get\nPASS: session_test.go:3261: S.BenchmarkFindIterRaw        200     559879123 ns/op\nwhich is a whole order of magnitude longer ns/op. Any idea what could be causing this? The benchmarks in the bson package didn't increase in time at all\n. ",
    "gabrielrussell": "Hey Gustavo. Here is a test, along with a couple more changes to the driver to get the tests to work. Let me know what you think.\n. I agree that Raw should contain slices of the input buffer. It clearly makes the most sense. If you were to copy the data when creating a Raw you might recurse and then copy the data again, and so on, completely defeating the usefulness of the Raw.\nNone of the other simple types wrap slices of the input buffer, and many of them are ref-counted objects that require as much memory as the bindata buffer. This change makes the binary data handled more similarly to how the string data is handled.\nIf you read in some BSON documents, with binary data fields, into individually allocated buffers, and then marshal them to bson.M variables, then each of the buffers will not be garbage collected until the bson.M variables can be. You're binding the lifecycle of the BSON document buffer to the lifecycle of the unmarshaled data structure. With this change the buffer can be collected as soon as its last regular reference is. If one is creating single use I/O buffers, then my change will reduce memory use, and if one is reusing a single I/O buffer, then my change will make it possible.\nWithout this change the only reason you can't reuse an I/O buffer to unmarshal in to a bson.M or bson.D is because bson.Binary's reference the I/O buffer. With this change you can reuse an I/O buffer while unmarshaling in any non Raw data structure.\n. Yes, absolutely, in the default. I've also added a test.\n. Hey Gustavo. Here is a test, along with a couple more changes to the driver to get the tests to work. Let me know what you think.\n. I agree that Raw should contain slices of the input buffer. It clearly makes the most sense. If you were to copy the data when creating a Raw you might recurse and then copy the data again, and so on, completely defeating the usefulness of the Raw.\nNone of the other simple types wrap slices of the input buffer, and many of them are ref-counted objects that require as much memory as the bindata buffer. This change makes the binary data handled more similarly to how the string data is handled.\nIf you read in some BSON documents, with binary data fields, into individually allocated buffers, and then marshal them to bson.M variables, then each of the buffers will not be garbage collected until the bson.M variables can be. You're binding the lifecycle of the BSON document buffer to the lifecycle of the unmarshaled data structure. With this change the buffer can be collected as soon as its last regular reference is. If one is creating single use I/O buffers, then my change will reduce memory use, and if one is reusing a single I/O buffer, then my change will make it possible.\nWithout this change the only reason you can't reuse an I/O buffer to unmarshal in to a bson.M or bson.D is because bson.Binary's reference the I/O buffer. With this change you can reuse an I/O buffer while unmarshaling in any non Raw data structure.\n. Yes, absolutely, in the default. I've also added a test.\n. ",
    "waigani": "PTAL\n. PTAL\n. PTAL, keeping dockey_test.go as an internal test because docKeys is internal to txn.\n. PTAL, Added failing test which now passes.\n. PTAL\n. PTAL\n. PTAL, keeping dockey_test.go as an internal test because docKeys is internal to txn.\n. PTAL, Added failing test which now passes.\n. ",
    "klauspost": "Count() would be nice, because sending a bulk request with 0 documents gives an error from Mongo.\nSo for flushing purposes it would be nice to be able to check for 0 documents, or otherwise Run() should just return if the count is 0.\n. Count() would be nice, because sending a bulk request with 0 documents gives an error from Mongo.\nSo for flushing purposes it would be nice to be able to check for 0 documents, or otherwise Run() should just return if the count is 0.\n. ",
    "julien-c": "I think this is the reason I needed it myself.\n. No problem. I'll keep an eye open for the Bulk API.\n. I think this is the reason I needed it myself.\n. No problem. I'll keep an eye open for the Bulk API.\n. ",
    "mpobrien": "Hey Gustavo, sorry but just wanted to check in on this - any other changes you'd like to see here?\n. Thanks Gustavo!\n. Hey Gustavo - any further thoughts on this one or https://github.com/go-mgo/mgo/issues/50? It would be really useful for us to fix some rough edges in the new mongo tools.\n. The other PR making the corresponding change to v2-unstable was already merged (https://github.com/go-mgo/mgo/pull/285) which was then merged back to v2, so I think this PR can also just get closed as a no-op.\n. Hey Gustavo, sorry but just wanted to check in on this - any other changes you'd like to see here?\n. Thanks Gustavo!\n. Hey Gustavo - any further thoughts on this one or https://github.com/go-mgo/mgo/issues/50? It would be really useful for us to fix some rough edges in the new mongo tools.\n. The other PR making the corresponding change to v2-unstable was already merged (https://github.com/go-mgo/mgo/pull/285) which was then merged back to v2, so I think this PR can also just get closed as a no-op.\n. ",
    "akrennmair": "For the record, I ran into the same problem that some indexes were created with the NumberLong type, and was about to prepare essentially the same patch when I discovered this pull request.\n. For the record, I ran into the same problem that some indexes were created with the NumberLong type, and was about to prepare essentially the same patch when I discovered this pull request.\n. ",
    "keitap": "@niemeyer Could you please consider to merge this pull request?\n. @niemeyer Could you please consider to merge this pull request?\n. ",
    "timezstyle": "When will it be merged?\n. When will it be merged?\n. ",
    "ctrlrsf": "I had to apply this patch manually to the vendor'ed version of mgo so I could get https://github.com/compose/transporter working correctly. It was not migrating certain Mongo collections due to this error.\n```\n...\nINFO[0000] done iterating collections                    db=development\npanic: Got unknown index key type for field modified\ngoroutine 46 [running]:\ngithub.com/compose/transporter/vendor/gopkg.in/mgo%2ev2.simpleIndexKey(0xc420272900, 0x1, 0x8, 0x0, 0x1, 0xa0)\n        /Users/r/gocode/src/github.com/compose/transporter/vendor/gopkg.in/mgo.v2/session.go:1555 +0x38c\ngithub.com/compose/transporter/vendor/gopkg.in/mgo%2ev2.indexFromSpec(0xc420297865, 0xb, 0xc42028f120, 0x12, 0xc420272900, 0x1, 0x8, 0x0, 0x0, 0x0, ...)\n        /Users/r/gocode/src/github.com/compose/transporter/vendor/gopkg.in/mgo.v2/session.go:1498 +0x8b\n```. I had to apply this patch manually to the vendor'ed version of mgo so I could get https://github.com/compose/transporter working correctly. It was not migrating certain Mongo collections due to this error.\n```\n...\nINFO[0000] done iterating collections                    db=development\npanic: Got unknown index key type for field modified\ngoroutine 46 [running]:\ngithub.com/compose/transporter/vendor/gopkg.in/mgo%2ev2.simpleIndexKey(0xc420272900, 0x1, 0x8, 0x0, 0x1, 0xa0)\n        /Users/r/gocode/src/github.com/compose/transporter/vendor/gopkg.in/mgo.v2/session.go:1555 +0x38c\ngithub.com/compose/transporter/vendor/gopkg.in/mgo%2ev2.indexFromSpec(0xc420297865, 0xb, 0xc42028f120, 0x12, 0xc420272900, 0x1, 0x8, 0x0, 0x0, 0x0, ...)\n        /Users/r/gocode/src/github.com/compose/transporter/vendor/gopkg.in/mgo.v2/session.go:1498 +0x8b\n```. ",
    "deafgoat": "Hi Gustavo,\nThanks for taking a look. I'm holding off on updating the code to address the first set of issues you raised since I don't completely understand your last comment. The cluster.err variable is only set when a server in the seed list - or during the course of discovery - is found not to be part of the requested replica set. Once set, the error is only propagated insofar as spawnSync for that server is short-circuited (so communication with that server is rejected). The other goroutines for possibly properly configured servers proceed as usual.\nBy not adding it to the list of known servers, there can be no communication with it. While cluster.err is still set at that point, it is only used if the code gets here. At this point, we only use cluster.err - if it's set - to indicate a possible cause of the issue (note that at this point, we're already returning \"no reachable servers\"). Do you mean we should not assume cluster.err at that point? Would it suffice to just remove the check for cluster.err here?\n. OK, I'll completely remove cluster.err.\n. Hey Gustavo, removed the use of cluster.err. Please let me know if there are any other issues I need to address, thanks.\n. Done.\n. Removed the superfluous check.\n. Thanks for taking the time to review!\n. Hi @sundarv85,\nYou can view the v2 documentation of the mgo driver at http://godoc.org/gopkg.in/mgo.v2 - which also contains documentation on the ReplicaSetName feature.\nTo communicate with just the primary (or seed server(s)) of a replica set), it suffices to set the Direct field of the DialInfo struct - so no server discovery is performed. See here for an example.\nThe ReplicaSetName name feature simply ensures that if Direct is unset and you dial MongoDB with seed servers from different replica sets, only those servers that are part of the the specified replica set are communicated with. This can be achieved either by using the DialInfo struct or a connection URI.\n. Hi Gustavo,\nThanks for taking a look. I'm holding off on updating the code to address the first set of issues you raised since I don't completely understand your last comment. The cluster.err variable is only set when a server in the seed list - or during the course of discovery - is found not to be part of the requested replica set. Once set, the error is only propagated insofar as spawnSync for that server is short-circuited (so communication with that server is rejected). The other goroutines for possibly properly configured servers proceed as usual.\nBy not adding it to the list of known servers, there can be no communication with it. While cluster.err is still set at that point, it is only used if the code gets here. At this point, we only use cluster.err - if it's set - to indicate a possible cause of the issue (note that at this point, we're already returning \"no reachable servers\"). Do you mean we should not assume cluster.err at that point? Would it suffice to just remove the check for cluster.err here?\n. OK, I'll completely remove cluster.err.\n. Hey Gustavo, removed the use of cluster.err. Please let me know if there are any other issues I need to address, thanks.\n. Done.\n. Removed the superfluous check.\n. Thanks for taking the time to review!\n. Hi @sundarv85,\nYou can view the v2 documentation of the mgo driver at http://godoc.org/gopkg.in/mgo.v2 - which also contains documentation on the ReplicaSetName feature.\nTo communicate with just the primary (or seed server(s)) of a replica set), it suffices to set the Direct field of the DialInfo struct - so no server discovery is performed. See here for an example.\nThe ReplicaSetName name feature simply ensures that if Direct is unset and you dial MongoDB with seed servers from different replica sets, only those servers that are part of the the specified replica set are communicated with. This can be achieved either by using the DialInfo struct or a connection URI.\n. ",
    "sundarv85": "@deafgoat @niemeyer I was looking for the ReplicaSetName in the latest documentation - but I\u2019m unable to find it. Am I looking at the correct location? Is this ReplicaSetName feature now removed..\nWe have a replica set and only want to communicate to the primary and not every server that is discovered. How can I achieve that from the current mgo package.\n. @deafgoat @niemeyer I was looking for the ReplicaSetName in the latest documentation - but I\u2019m unable to find it. Am I looking at the correct location? Is this ReplicaSetName feature now removed..\nWe have a replica set and only want to communicate to the primary and not every server that is discovered. How can I achieve that from the current mgo package.\n. ",
    "jtsylve": "I came across this issue on a project that I'm working on.  If you try to bulk insert a large amount of data at once, the insertion will fail because Mongo has a maximum message size limit.  In this pull request, if the maximum message size is exceeded, mgo will split up the insert into multiple smaller inserts.\n. This makes sense.  It actually occurred to me after submitting this that I\nwas adding a side effect to the Insert functionality. I'll open an issue.\nOn Fri, Jan 9, 2015 at 10:58 AM, Gustavo Niemeyer notifications@github.com\nwrote:\n\nClosed #60 https://github.com/go-mgo/mgo/pull/60.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/go-mgo/mgo/pull/60#event-216615480.\n. I came across this issue on a project that I'm working on.  If you try to bulk insert a large amount of data at once, the insertion will fail because Mongo has a maximum message size limit.  In this pull request, if the maximum message size is exceeded, mgo will split up the insert into multiple smaller inserts.\n. This makes sense.  It actually occurred to me after submitting this that I\nwas adding a side effect to the Insert functionality. I'll open an issue.\n\nOn Fri, Jan 9, 2015 at 10:58 AM, Gustavo Niemeyer notifications@github.com\nwrote:\n\nClosed #60 https://github.com/go-mgo/mgo/pull/60.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/go-mgo/mgo/pull/60#event-216615480.\n. \n",
    "MaerF0x0": "I was failing to read the sentence correct. my bad.\n. I was failing to read the sentence correct. my bad.\n. ",
    "tdubourg": "Hum, up?\n. Hum, up?\n. ",
    "tchajed": "The problem is that if CanAddr() returns false, you can't address the array's memory. This makes it tricky to pass the bytes in the array to the encoder; since Go doesn't have generics, you can't write a method that takes an arbitrary array, it has to take a slice, and there's no way to get a zero-copy slice in this case.\nOne way around the copy is to instead pass the bytes individually to the encoder. This will have a high reflection overhead, since you'll have to call (v Value) Index() and convert each reflect.Value to a byte.\nI agree that a copy seems unnecessary, but it's better than one reflection conversion per byte or leaving the bug there.\n. I checked what encoding/json does and it's actually the other option I suggested (access each element as a reflect.Value and then get the element). This will be slower but won't cause another memory allocation.\n. Cool, implemented that in pull #110.\n. The problem is that if CanAddr() returns false, you can't address the array's memory. This makes it tricky to pass the bytes in the array to the encoder; since Go doesn't have generics, you can't write a method that takes an arbitrary array, it has to take a slice, and there's no way to get a zero-copy slice in this case.\nOne way around the copy is to instead pass the bytes individually to the encoder. This will have a high reflection overhead, since you'll have to call (v Value) Index() and convert each reflect.Value to a byte.\nI agree that a copy seems unnecessary, but it's better than one reflection conversion per byte or leaving the bug there.\n. I checked what encoding/json does and it's actually the other option I suggested (access each element as a reflect.Value and then get the element). This will be slower but won't cause another memory allocation.\n. Cool, implemented that in pull #110.\n. ",
    "jackspirou": "Both points make sense.  I attempted to addressed them below.\n1. I took a look at the UnmarshalJSON logic and tried to apply it where it made sense.  Please take a look and let me know if I missed a particular case you would like to include, I won't be offended :)\n2. I have updated the marshaler to take the receiver by value and not as a pointer.  It is also worth pointing out that the MarshalJSON method takes the receiver by value as well.\n. closing this pull request to start fresh\n. @niemeyer I think I am ready to open a new pull request with these changes: https://github.com/go-mgo/mgo/compare/v2...jackspirou:v2\nDo you want me to target the v2 branch or the v2-unstable branch?  Sorry about closing this pull request, I wasn't sure if you wanted me to git rebase v2-unstable or target the v2-unstable branch to submit the changes against.  I thought I would start clean since git rebase v2-unstable caused conflicts for me.\nLet me know how you would like me to submit this pull request.  Thanks for your patience.\n. @rodriguise this was never merged, but I did just submit the changes to the unstable codebase at https://github.com/go-mgo/mgo/pull/212. cc @niemeyer.\n. @niemeyer I just noticed the bson_test package is using the package gopkg.in/mgo.v2-unstable/bson which seems to be a different repo than bson.  Is this current PR good or do the changes made in the bson package also need to be added to gopkg.in/mgo.v2-unstable/bson? \n. Both points make sense.  I attempted to addressed them below.\n1. I took a look at the UnmarshalJSON logic and tried to apply it where it made sense.  Please take a look and let me know if I missed a particular case you would like to include, I won't be offended :)\n2. I have updated the marshaler to take the receiver by value and not as a pointer.  It is also worth pointing out that the MarshalJSON method takes the receiver by value as well.\n. closing this pull request to start fresh\n. @niemeyer I think I am ready to open a new pull request with these changes: https://github.com/go-mgo/mgo/compare/v2...jackspirou:v2\nDo you want me to target the v2 branch or the v2-unstable branch?  Sorry about closing this pull request, I wasn't sure if you wanted me to git rebase v2-unstable or target the v2-unstable branch to submit the changes against.  I thought I would start clean since git rebase v2-unstable caused conflicts for me.\nLet me know how you would like me to submit this pull request.  Thanks for your patience.\n. @rodriguise this was never merged, but I did just submit the changes to the unstable codebase at https://github.com/go-mgo/mgo/pull/212. cc @niemeyer.\n. @niemeyer I just noticed the bson_test package is using the package gopkg.in/mgo.v2-unstable/bson which seems to be a different repo than bson.  Is this current PR good or do the changes made in the bson package also need to be added to gopkg.in/mgo.v2-unstable/bson? \n. ",
    "rodriguise": "Was this ever merged? Will it make it into the stable codebase?\n. Was this ever merged? Will it make it into the stable codebase?\n. ",
    "minyoung": "Any comments/feedback?\n. Any comments/feedback?\n. ",
    "dynamike": "Would love to see this merged. Anything holding it up?\n. Would love to see this merged. Anything holding it up?\n. ",
    "charity": "Yes please!!  We are waiting on this.  :)\n. Yes please!!  We are waiting on this.  :)\n. ",
    "dmage": "Sorry, this is true, I didn't notice tests. I added a test for this case in #109.\n. Ok, let's wait for regression.\n. Sorry, this is true, I didn't notice tests. I added a test for this case in #109.\n. Ok, let's wait for regression.\n. ",
    "rgbkrk": "Apparently others have wanted this, because there's a fork of just the parseURL function. It doesn't have tests and I'd rather see it as part of this package since you update it regularly.\n. Consider this my humble issue if you prefer, I'm happy if there's more you'd like to see. :wink:\n. Fixes #84.\n. Great, thanks for the response @niemeyer.\n. Apparently others have wanted this, because there's a fork of just the parseURL function. It doesn't have tests and I'd rather see it as part of this package since you update it regularly.\n. Consider this my humble issue if you prefer, I'm happy if there's more you'd like to see. :wink:\n. Fixes #84.\n. Great, thanks for the response @niemeyer.\n. ",
    "roman-mazur": "@niemeyer Could you show a small example how this feature is used? Out of curiosity...\n. What I want to achieve is omitting an allocation of my type when nil is deserialized.\nI do not have strong arguments that it impacts performance in any way. I just do not understand why this should happen.\nIn Go structure\ngo\ntype Container struct {\n  value *MyCustomTypeThatImplementsSetter\n}\nI define value as a pointer. This means that value can be nil. And if Container is unmarshaled from bson {\"value\":nil}, I expect the value field in Container to become nil. And it does not make sense to allocate a new instance of MyCustomTypeThatImplementsSetter to set value to nil.\nActually marshalling works in the same way. If you define a method\ngo\nfunc (p *SomeType) SomeMethod() {\n}\nyou usually do not expect p to be nil. And it's what happens when nil is serialized with this bson package.\nAFAIK neither json.Unmarshaler nor json.Marshaler method will not be invoked in case of nil. At least it's what I experience implementing both SetBSON and UnmarshalJSON (GetBSON and MarshalJSON) on my type.\nFinally, even if we think that allocating Setter to unmarshal nil is ok, SetBSON implementation becomes dirty (IMO).\nOne of intents to use bson package is to avoid dealing with bson format directly. But the only way to understand that underlying value is nil (please correct me if I do not see something) is to write\ngo\nif raw.Kind == 0x0A { \n  return bson.SetZero\n}\nAnd to write this code you have to dive into bson spec and see that nil is encoded as 0x0A.\nJust sharing my experience and feelings :)\n. @niemeyer Could you show a small example how this feature is used? Out of curiosity...\n. What I want to achieve is omitting an allocation of my type when nil is deserialized.\nI do not have strong arguments that it impacts performance in any way. I just do not understand why this should happen.\nIn Go structure\ngo\ntype Container struct {\n  value *MyCustomTypeThatImplementsSetter\n}\nI define value as a pointer. This means that value can be nil. And if Container is unmarshaled from bson {\"value\":nil}, I expect the value field in Container to become nil. And it does not make sense to allocate a new instance of MyCustomTypeThatImplementsSetter to set value to nil.\nActually marshalling works in the same way. If you define a method\ngo\nfunc (p *SomeType) SomeMethod() {\n}\nyou usually do not expect p to be nil. And it's what happens when nil is serialized with this bson package.\nAFAIK neither json.Unmarshaler nor json.Marshaler method will not be invoked in case of nil. At least it's what I experience implementing both SetBSON and UnmarshalJSON (GetBSON and MarshalJSON) on my type.\nFinally, even if we think that allocating Setter to unmarshal nil is ok, SetBSON implementation becomes dirty (IMO).\nOne of intents to use bson package is to avoid dealing with bson format directly. But the only way to understand that underlying value is nil (please correct me if I do not see something) is to write\ngo\nif raw.Kind == 0x0A { \n  return bson.SetZero\n}\nAnd to write this code you have to dive into bson spec and see that nil is encoded as 0x0A.\nJust sharing my experience and feelings :)\n. ",
    "tcard": "I wasn't able to make the changes in time. Thanks for taking care of it.\n. I wasn't able to make the changes in time. Thanks for taking care of it.\n. ",
    "dmliao": "Hi Gustavo,\nThank you for your insight, and I appreciate your concerns with using the driver in this way. I'm not building a dumb proxy (one that copies bytes from client to server), but rather one that also abstracts away parts of the wire protocol and provides extra functionality. I can explain the project in more detail, but I don't think it's necessary for these changes.\nCan we include these changes and discuss future changes as needed?\n. Mgo is also currently the only well-maintained MongoDB driver for Go, so without exposing some of those internals the alternative I (and other developers who do care about those details) have would be to write an entirely new driver, which seems redundant when mgo already has most of the feature set we'd be looking for.\nI agree with you that batch size shouldn't change during iteration; the original intention was for there should be a way to set the batch size when creating a new Iterator. It should probably be set as an argument for the NewIter function, though that would introduce a breaking change for any application using the NewIter function currently. I can modify the pull request as needed if that is preferred.\nI still think that it's important that batch size be settable when creating a new Iterator with NewIter. If developers are using the NewIter function, then they're already working with the internals of MongoDB by providing a firstBatch and a cursorID, and not allowing them to set a batch size makes the implementation seem incomplete.\nAlthough the driver's focus is on abstracting away internals for simplifying development, I think the internals can be exposed as long as it doesn't detract from the design or complicate the development process, and we already do so a little bit via the NewIter function. Exposing CursorID or being able to set a batch size for new iterators won't make working with mgo any more difficult for app developers, as it doesn't change the existing abstractions.\n. The functionality does have other use cases, though. Having access to the cursorID is important to correlate logs between the client application and the MongoDB server, which would be useful for diagnostics. The MongoDB server's logs use the cursorID as an identifier for different queries, and have additional information that isn't sent to the client, including the amount of time it took the server to process that request (useful for diagnosing slow queries), among other things.\n. Hi Gustavo,\nThank you for your insight, and I appreciate your concerns with using the driver in this way. I'm not building a dumb proxy (one that copies bytes from client to server), but rather one that also abstracts away parts of the wire protocol and provides extra functionality. I can explain the project in more detail, but I don't think it's necessary for these changes.\nCan we include these changes and discuss future changes as needed?\n. Mgo is also currently the only well-maintained MongoDB driver for Go, so without exposing some of those internals the alternative I (and other developers who do care about those details) have would be to write an entirely new driver, which seems redundant when mgo already has most of the feature set we'd be looking for.\nI agree with you that batch size shouldn't change during iteration; the original intention was for there should be a way to set the batch size when creating a new Iterator. It should probably be set as an argument for the NewIter function, though that would introduce a breaking change for any application using the NewIter function currently. I can modify the pull request as needed if that is preferred.\nI still think that it's important that batch size be settable when creating a new Iterator with NewIter. If developers are using the NewIter function, then they're already working with the internals of MongoDB by providing a firstBatch and a cursorID, and not allowing them to set a batch size makes the implementation seem incomplete.\nAlthough the driver's focus is on abstracting away internals for simplifying development, I think the internals can be exposed as long as it doesn't detract from the design or complicate the development process, and we already do so a little bit via the NewIter function. Exposing CursorID or being able to set a batch size for new iterators won't make working with mgo any more difficult for app developers, as it doesn't change the existing abstractions.\n. The functionality does have other use cases, though. Having access to the cursorID is important to correlate logs between the client application and the MongoDB server, which would be useful for diagnostics. The MongoDB server's logs use the cursorID as an identifier for different queries, and have additional information that isn't sent to the client, including the amount of time it took the server to process that request (useful for diagnosing slow queries), among other things.\n. ",
    "eklementev": "Added asserts to bulk tests\nTesting the IsDup directly is impossible due to bulkError is private\n-- Eugeny\nOn Sun, Jul 26, 2015 at 9:45 PM Gustavo Niemeyer notifications@github.com\nwrote:\n\nThanks, sounds reasonable.\nCan we please have a test for this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/go-mgo/mgo/pull/138#issuecomment-125026687.\n. At the moment bulkError is container for single error.\nFor single error IsDup is applicable. \n\nThe possible solution to provide array contains an error document for each write operation that errors.\nhttp://docs.mongodb.org/manual/reference/method/BulkWriteResult/#BulkWriteResult\n. Need more investigations\n. Added asserts to bulk tests\nTesting the IsDup directly is impossible due to bulkError is private\n-- Eugeny\nOn Sun, Jul 26, 2015 at 9:45 PM Gustavo Niemeyer notifications@github.com\nwrote:\n\nThanks, sounds reasonable.\nCan we please have a test for this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/go-mgo/mgo/pull/138#issuecomment-125026687.\n. At the moment bulkError is container for single error.\nFor single error IsDup is applicable. \n\nThe possible solution to provide array contains an error document for each write operation that errors.\nhttp://docs.mongodb.org/manual/reference/method/BulkWriteResult/#BulkWriteResult\n. Need more investigations\n. ",
    "raymondjacobson": "1, 4, 5, 6 -- cool (fixed).\n2,3 -- Wow. yeah. Cool.\n. 1, 4, 5, 6 -- cool (fixed).\n2,3 -- Wow. yeah. Cool.\n. ",
    "mwmahlberg": "Sorry, made a mistake including the other commits.\n. Thank you \u2013 and of course you are right. Bitshifting is kind of new to me, coming from Java Web Applications.\nI'll update\n. Sorry, made a mistake including the other commits.\n. Thank you \u2013 and of course you are right. Bitshifting is kind of new to me, coming from Java Web Applications.\nI'll update\n. ",
    "zph": "@mwmahlberg This PR solved an issue I had with needing to create an oplog style bson.MongoTimestamp, thank you. The use case I have is generating a bson.MongoTimestamp from time.Time in order to start replaying oplog from point in time calculated using time.Duration.\n@niemeyer If this is updated to merge cleanly w/ current master, would it be accepted and merged? If so, I'm happy to do the git work/cleanup.\n. @mwmahlberg This PR solved an issue I had with needing to create an oplog style bson.MongoTimestamp, thank you. The use case I have is generating a bson.MongoTimestamp from time.Time in order to start replaying oplog from point in time calculated using time.Duration.\n@niemeyer If this is updated to merge cleanly w/ current master, would it be accepted and merged? If so, I'm happy to do the git work/cleanup.\n. ",
    "RyouZhang": "I understand your concern. But more time use mongodb like this:\n{\"name\":\"foo\",\"items\":[{\"count\":1,\"price\":1.00},{\"count\":2,\"price\":2.00}], \"total\":5.00}\nif use map[string]interface{} to convert bson object,\nfirst  golang will judge the slice like []interface{},\nsecond judge interface{} like map[interface{}]interface{},  will trigger the issue\n. please, try to write this json object to mongodb\n{\n    \"device_model\": \"iPhone6,2\",\n    \"origin_time\": 1453042991466.9,\n    \"app_build\": \"20151231\",\n    \"channel\": \"YOULIN\",\n    \"uid\": \"8cab9179926546f8a208d5343f851498\",\n    \"local_time\": 1453131230831.6,\n    \"server_time\": 1453131100000,\n    \"country\": \"\u4e2d\u56fd\",\n    \"city\": \"\u676d\u5dde\",\n    \"carrier\": \"\u4e2d\u56fd\u79fb\u52a8\",\n    \"_id\": \"10b721fc8dd42b9cd3afcd4434edc485\",\n    \"app_id\": \"123456\",\n    \"os_ver\": \"7.1.2\",\n    \"device_id\": \"8819d7e3cbed8dd6a22ff9629d2f6aa808d1bf2a\",\n    \"os\": \"iOS\",\n    \"args\": {\n      \"duration\": 23950.764894485,\n      \"last_page_nick\": \"feed/detail_70db371e040e4647b0f8e166a19bacf7\",\n      \"page_name\": \"feed/home\",\n      \"page_nick\": \"feed/home\",\n      \"last_page_name\": \"feed/detail\"\n    },\n    \"community_id\": \"f2c3572bdca74347b8f1dc00461334d4\",\n    \"imei\": \"827c939fdca42c6f2550a4843e5fe43c\",\n    \"network\": \"wifi\",\n    \"app_ver\": \"2.3.0\",\n    \"screen\": \"1136x640\",\n    \"event_id\": \"page_event\"\n  }\nif you define the struct, it works well. but if use map[string]interface{}, it's wrong\n. func (m MongoInsert) run() {\n    var (\n        session mgo.Session\n        mh      codec.MsgpackHandle\n        item    map[string]interface{}\n        name    interface{}\n        ok      bool\n        err     error\n    )\n    mh.RawToString = true\n```\nm.Wg.Add(1)\nfor raw := range m.Input {\n    item = nil\n    err = codec.NewDecoderBytes(raw, &mh).Decode(&item)\n    if err != nil {\n        logrus.WithField(\"raw\", raw).Error(err)\n        continue\n    }\n    name = nil\n    for _, key := range m.keys {\n        name, ok = item[key]\n        if ok {\n            name = item[key]\n            break\n        }\n    }\n    if name == nil {\n        logrus.WithFields(item).Error(\"invalid collection name\")\n        continue\n    }\nsession, err = mongopool.GetMongodbPoolInstance().Get(m.url)\nif err == nil {\n    db := session.DB(m.database)\n    col := db.C(name.(string))\n    err = col.Insert(item)\n    if err != nil && false == mgo.IsDup(err) {\n        logrus.Error(err.Error())\n    }\n} else {\n    logrus.Error(err)\n}\n\n}\nm.Wg.Done()\n```\n}\nthe Actual code\n. package main\nimport (\n    \"github.com/Sirupsen/logrus\"\n    \"github.com/ugorji/go/codec\"\n  \"gopkg.in/mgo.v2\"\n)\ntype JsonByteHook struct {\n}\nfunc (j JsonByteHook) ConvertExt(v interface{}) interface{} {\n    return string(v.([]byte))\n}\nfunc (j JsonByteHook) UpdateExt(dest interface{}, v interface{}) {\n}\nfunc main() {\n    var (\n        session *mgo.Session\n        jh      codec.JsonHandle\n        raw     []byte\n        keys    []string\n        item    map[string]interface{}\n        name    interface{}\n        ok      bool\n        err     error\n    )\n    keys = []string{\"event\", \"event_id\"}\n  raw = []byte(\"{\\\"device_model\\\":\\\"iPhone6,2\\\",\\\"origin_time\\\":1453042991466.9,\\\"app_build\\\":\\\"20151231\\\",\\\"channel\\\":\\\"YOULIN\\\",\\\"uid\\\":\\\"8cab9179926546f8a208d5343f851498\\\",\\\"local_time\\\":1453131230831.6,\\\"server_time\\\":1453131100000,\\\"country\\\":\\\"\u4e2d\u56fd\\\",\\\"city\\\":\\\"\u676d\u5dde\\\",\\\"carrier\\\":\\\"\u4e2d\u56fd\u79fb\u52a8\\\",\\\"_id\\\":\\\"10b721fc8dd42b9cd3afcd4434edc485\\\",\\\"app_id\\\":\\\"123456\\\",\\\"os_ver\\\":\\\"7.1.2\\\",\\\"device_id\\\":\\\"8819d7e3cbed8dd6a22ff9629d2f6aa808d1bf2a\\\",\\\"os\\\":\\\"iOS\\\",\\\"args\\\":{\\\"duration\\\":23950.764894485,\\\"last_page_nick\\\":\\\"feed/detail_70db371e040e4647b0f8e166a19bacf7\\\",\\\"page_name\\\":\\\"feed/home\\\",\\\"page_nick\\\":\\\"feed/home\\\",\\\"last_page_name\\\":\\\"feed/detail\\\"},\\\"community_id\\\":\\\"f2c3572bdca74347b8f1dc00461334d4\\\",\\\"imei\\\":\\\"827c939fdca42c6f2550a4843e5fe43c\\\",\\\"network\\\":\\\"wifi\\\",\\\"app_ver\\\":\\\"2.3.0\\\",\\\"screen\\\":\\\"1136x640\\\",\\\"event_id\\\":\\\"page_event\\\"}\")\n```\njh.RawBytesExt = &JsonByteHook{}\nerr = codec.NewDecoderBytes(raw, &jh).Decode(&item)\nif err != nil {\n    logrus.WithField(\"raw\", raw).Error(err)\n    return\n}\nfor _, key := range keys {\n    name, ok = item[key]\n    if ok {\n        name = item[key]\n        break\n    }\n}\nif name == nil {\n    logrus.WithFields(item).Error(\"invalid collection name\")\n    return\n}\nsession, err = mgo.Dail(\"mongodb://your mongodb url\")\ndefer session.Close()\nif err == nil {\n    db := session.DB(\"your db\")\n    col := db.C(name.(string))\n    err = col.Insert(item)\n    if err != nil && false == mgo.IsDup(err) {\n        logrus.Error(err.Error())\n    }\n} else {\n    logrus.Error(err)\n}\n```\n}\n. ha, it's another interest issue, you can try \"encoding/json\"\uff0cit throw exception can\u2018t encode map[interface{}]interface{}. the \"golang fastjson\" do the same, but codec json can encode it. \nthe golang judge map[string]interface{}  to map[interface{}]interface{}\n. sorry\uff0c I don't understand how to do the  conversion code?? Now, when I insert data to mongodb, the mgo/bson throw nothing, but insert error data. \nYou can run my code, and check the data in mongodb collection.\n. I understand your concern. But more time use mongodb like this:\n{\"name\":\"foo\",\"items\":[{\"count\":1,\"price\":1.00},{\"count\":2,\"price\":2.00}], \"total\":5.00}\nif use map[string]interface{} to convert bson object,\nfirst  golang will judge the slice like []interface{},\nsecond judge interface{} like map[interface{}]interface{},  will trigger the issue\n. please, try to write this json object to mongodb\n{\n    \"device_model\": \"iPhone6,2\",\n    \"origin_time\": 1453042991466.9,\n    \"app_build\": \"20151231\",\n    \"channel\": \"YOULIN\",\n    \"uid\": \"8cab9179926546f8a208d5343f851498\",\n    \"local_time\": 1453131230831.6,\n    \"server_time\": 1453131100000,\n    \"country\": \"\u4e2d\u56fd\",\n    \"city\": \"\u676d\u5dde\",\n    \"carrier\": \"\u4e2d\u56fd\u79fb\u52a8\",\n    \"_id\": \"10b721fc8dd42b9cd3afcd4434edc485\",\n    \"app_id\": \"123456\",\n    \"os_ver\": \"7.1.2\",\n    \"device_id\": \"8819d7e3cbed8dd6a22ff9629d2f6aa808d1bf2a\",\n    \"os\": \"iOS\",\n    \"args\": {\n      \"duration\": 23950.764894485,\n      \"last_page_nick\": \"feed/detail_70db371e040e4647b0f8e166a19bacf7\",\n      \"page_name\": \"feed/home\",\n      \"page_nick\": \"feed/home\",\n      \"last_page_name\": \"feed/detail\"\n    },\n    \"community_id\": \"f2c3572bdca74347b8f1dc00461334d4\",\n    \"imei\": \"827c939fdca42c6f2550a4843e5fe43c\",\n    \"network\": \"wifi\",\n    \"app_ver\": \"2.3.0\",\n    \"screen\": \"1136x640\",\n    \"event_id\": \"page_event\"\n  }\nif you define the struct, it works well. but if use map[string]interface{}, it's wrong\n. func (m MongoInsert) run() {\n    var (\n        session mgo.Session\n        mh      codec.MsgpackHandle\n        item    map[string]interface{}\n        name    interface{}\n        ok      bool\n        err     error\n    )\n    mh.RawToString = true\n```\nm.Wg.Add(1)\nfor raw := range m.Input {\n    item = nil\n    err = codec.NewDecoderBytes(raw, &mh).Decode(&item)\n    if err != nil {\n        logrus.WithField(\"raw\", raw).Error(err)\n        continue\n    }\n    name = nil\n    for _, key := range m.keys {\n        name, ok = item[key]\n        if ok {\n            name = item[key]\n            break\n        }\n    }\n    if name == nil {\n        logrus.WithFields(item).Error(\"invalid collection name\")\n        continue\n    }\nsession, err = mongopool.GetMongodbPoolInstance().Get(m.url)\nif err == nil {\n    db := session.DB(m.database)\n    col := db.C(name.(string))\n    err = col.Insert(item)\n    if err != nil && false == mgo.IsDup(err) {\n        logrus.Error(err.Error())\n    }\n} else {\n    logrus.Error(err)\n}\n\n}\nm.Wg.Done()\n```\n}\nthe Actual code\n. package main\nimport (\n    \"github.com/Sirupsen/logrus\"\n    \"github.com/ugorji/go/codec\"\n  \"gopkg.in/mgo.v2\"\n)\ntype JsonByteHook struct {\n}\nfunc (j JsonByteHook) ConvertExt(v interface{}) interface{} {\n    return string(v.([]byte))\n}\nfunc (j JsonByteHook) UpdateExt(dest interface{}, v interface{}) {\n}\nfunc main() {\n    var (\n        session *mgo.Session\n        jh      codec.JsonHandle\n        raw     []byte\n        keys    []string\n        item    map[string]interface{}\n        name    interface{}\n        ok      bool\n        err     error\n    )\n    keys = []string{\"event\", \"event_id\"}\n  raw = []byte(\"{\\\"device_model\\\":\\\"iPhone6,2\\\",\\\"origin_time\\\":1453042991466.9,\\\"app_build\\\":\\\"20151231\\\",\\\"channel\\\":\\\"YOULIN\\\",\\\"uid\\\":\\\"8cab9179926546f8a208d5343f851498\\\",\\\"local_time\\\":1453131230831.6,\\\"server_time\\\":1453131100000,\\\"country\\\":\\\"\u4e2d\u56fd\\\",\\\"city\\\":\\\"\u676d\u5dde\\\",\\\"carrier\\\":\\\"\u4e2d\u56fd\u79fb\u52a8\\\",\\\"_id\\\":\\\"10b721fc8dd42b9cd3afcd4434edc485\\\",\\\"app_id\\\":\\\"123456\\\",\\\"os_ver\\\":\\\"7.1.2\\\",\\\"device_id\\\":\\\"8819d7e3cbed8dd6a22ff9629d2f6aa808d1bf2a\\\",\\\"os\\\":\\\"iOS\\\",\\\"args\\\":{\\\"duration\\\":23950.764894485,\\\"last_page_nick\\\":\\\"feed/detail_70db371e040e4647b0f8e166a19bacf7\\\",\\\"page_name\\\":\\\"feed/home\\\",\\\"page_nick\\\":\\\"feed/home\\\",\\\"last_page_name\\\":\\\"feed/detail\\\"},\\\"community_id\\\":\\\"f2c3572bdca74347b8f1dc00461334d4\\\",\\\"imei\\\":\\\"827c939fdca42c6f2550a4843e5fe43c\\\",\\\"network\\\":\\\"wifi\\\",\\\"app_ver\\\":\\\"2.3.0\\\",\\\"screen\\\":\\\"1136x640\\\",\\\"event_id\\\":\\\"page_event\\\"}\")\n```\njh.RawBytesExt = &JsonByteHook{}\nerr = codec.NewDecoderBytes(raw, &jh).Decode(&item)\nif err != nil {\n    logrus.WithField(\"raw\", raw).Error(err)\n    return\n}\nfor _, key := range keys {\n    name, ok = item[key]\n    if ok {\n        name = item[key]\n        break\n    }\n}\nif name == nil {\n    logrus.WithFields(item).Error(\"invalid collection name\")\n    return\n}\nsession, err = mgo.Dail(\"mongodb://your mongodb url\")\ndefer session.Close()\nif err == nil {\n    db := session.DB(\"your db\")\n    col := db.C(name.(string))\n    err = col.Insert(item)\n    if err != nil && false == mgo.IsDup(err) {\n        logrus.Error(err.Error())\n    }\n} else {\n    logrus.Error(err)\n}\n```\n}\n. ha, it's another interest issue, you can try \"encoding/json\"\uff0cit throw exception can\u2018t encode map[interface{}]interface{}. the \"golang fastjson\" do the same, but codec json can encode it. \nthe golang judge map[string]interface{}  to map[interface{}]interface{}\n. sorry\uff0c I don't understand how to do the  conversion code?? Now, when I insert data to mongodb, the mgo/bson throw nothing, but insert error data. \nYou can run my code, and check the data in mongodb collection.\n. ",
    "rdallman": "+1, thanks!\n. +1, thanks!\n. ",
    "NickCSardo": "Anything blocking this? +1\n. Anything blocking this? +1\n. ",
    "cezarsa": "Today I figured a way to simplify the code and improve even more the benchmarks. I've already updated the commits and the benchmark result comparison on the original comment.\n. Oops, I'm sorry. I just found out that this has already been fixed on v2 branch but not merged back to v2-unstable yet.\n. Today I figured a way to simplify the code and improve even more the benchmarks. I've already updated the commits and the benchmark result comparison on the original comment.\n. Oops, I'm sorry. I just found out that this has already been fixed on v2 branch but not merged back to v2-unstable yet.\n. ",
    "wpjunior": "@niemeyer: any questions about my pull request ?\n. @niemeyer, Thanks!\n. @niemeyer: any questions about my pull request ?\n. @niemeyer, Thanks!\n. ",
    "asm-jaime": "@niemeyer maybe i'ts my mistake. So, \n\nWe don't support that because Go ... Note you can set the value to 1 and have the same behavior.\n\nThat's mean, a first number what will be correct is 1000000000 time.Duration, or 1 * time.Second. Any lesser will be equal 0. Maybe best decision: notify about incorrect value.\n. @niemeyer maybe i'ts my mistake. So, \n\nWe don't support that because Go ... Note you can set the value to 1 and have the same behavior.\n\nThat's mean, a first number what will be correct is 1000000000 time.Duration, or 1 * time.Second. Any lesser will be equal 0. Maybe best decision: notify about incorrect value.\n. ",
    "wgallagher": "candidate fix for:  https://github.com/go-mgo/mgo/issues/254\n. We have been running this in production for months.  It resolved the issue.\n. I intended to upstream but coming up with a testcase but the issue is hard to repro.\n. candidate fix for:  https://github.com/go-mgo/mgo/issues/254\n. We have been running this in production for months.  It resolved the issue.\n. I intended to upstream but coming up with a testcase but the issue is hard to repro.\n. ",
    "nikhilm": "@wgallagher Have you had any experience with using your suggested fix in production? See my updates on #254 . Thank you.\n. Thanks. We are going to try it too.\n. @wgallagher Have you had any experience with using your suggested fix in production? See my updates on #254 . Thank you.\n. Thanks. We are going to try it too.\n. ",
    "jyoon17": "Any updates on this? A single timeout causes excessive CPU usage due to isMaster calls.. Any updates on this? A single timeout causes excessive CPU usage due to isMaster calls.. ",
    "alesstimec": "\ud83d\udc4d \n. Mongo documentation states all timestamps are stored in UTC (https://docs.mongodb.com/manual/tutorial/model-time-data/). So it is very unexpected when i retrieve objects from mongo and times are in Local. It also makes testing a lot harder since we put in times in UTC, but have to do jc.DeepEquals with timestamps in Local..\n\nOn 24 May 2016, at 14:12, Gustavo Niemeyer notifications@github.com wrote:\nThanks for the contribution, but I'm not quite sure this is the right thing to do. Timestamps in the database do not have a timezone, and people often work with timestamps in their own timezones.\nWhich documentation suggests it is UTC?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n. \ud83d\udc4d \n. Mongo documentation states all timestamps are stored in UTC (https://docs.mongodb.com/manual/tutorial/model-time-data/). So it is very unexpected when i retrieve objects from mongo and times are in Local. It also makes testing a lot harder since we put in times in UTC, but have to do jc.DeepEquals with timestamps in Local..\nOn 24 May 2016, at 14:12, Gustavo Niemeyer notifications@github.com wrote:\nThanks for the contribution, but I'm not quite sure this is the right thing to do. Timestamps in the database do not have a timezone, and people often work with timestamps in their own timezones.\nWhich documentation suggests it is UTC?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n. \n",
    "ebroder": "My use case is fairly similar to those outlined #170. I have an internal API that's backed by a Mongo query. If the call doesn't return the full result set, I want to set a flag (\"there are more results if you ask for them\") on the response, hold onto the Iter, and giving the client an identifier to reference it.\nIt basically amounts to a reimplementation of cursors, but for applications where I don't want them to directly connect to (or know about) the Mongo cluster in question. I'd prefer a method like this over calling Next - if I do that, I have to store the results and the state tracking generally becomes more complicated. This seemed like a fairly straightforward API change that would make the implementation of my code much simpler.\nIt looks like several of the other drivers have an \"alive\" or \"dead\" method (c.f. Python and PHP) which has similar semantics to my implementation - maybe Alive() would be a better name for the function?\n. Awesome, that makes sense to me. And sorry for not saying it earlier, but thanks so much for taking a look so quickly!\nI've pushed an update that makes the changes you requested to the Iter code. I'll work on adding the tests tomorrow (the initial implementation was mostly to convince me the code worked at all).\nOne thing I'm not sure about is how to test the docsToReceive condition - in order for that test to be reliable, I effectively need to prevent the result of an in-flight operation from getting processed. Do you have any pointers on how to test that particular condition?\n. My use case is fairly similar to those outlined #170. I have an internal API that's backed by a Mongo query. If the call doesn't return the full result set, I want to set a flag (\"there are more results if you ask for them\") on the response, hold onto the Iter, and giving the client an identifier to reference it.\nIt basically amounts to a reimplementation of cursors, but for applications where I don't want them to directly connect to (or know about) the Mongo cluster in question. I'd prefer a method like this over calling Next - if I do that, I have to store the results and the state tracking generally becomes more complicated. This seemed like a fairly straightforward API change that would make the implementation of my code much simpler.\nIt looks like several of the other drivers have an \"alive\" or \"dead\" method (c.f. Python and PHP) which has similar semantics to my implementation - maybe Alive() would be a better name for the function?\n. Awesome, that makes sense to me. And sorry for not saying it earlier, but thanks so much for taking a look so quickly!\nI've pushed an update that makes the changes you requested to the Iter code. I'll work on adding the tests tomorrow (the initial implementation was mostly to convince me the code worked at all).\nOne thing I'm not sure about is how to test the docsToReceive condition - in order for that test to be reliable, I effectively need to prevent the result of an in-flight operation from getting processed. Do you have any pointers on how to test that particular condition?\n. ",
    "rogpeppe": "\nAlso, despite having access to the internals, the suggested implementation seems to be doing its logic in a pretty high-level way, by marshaling structs into BSON and then back into a map\n\nWe're actually only marshaling to BSON and back when the object isn't\na struct (in practice that's only for Raw), which is when we can't fetch its\nfields directly.\nTo be honest, the map and Raw cases are only there so that the semantics\nare easier to explain - AsUpdate isn't really useful for those cases.\nThe struct case is the important one (for us, at any rate) and I'd be\nquite happy if the logic was restricted to structs only.\n\nVery frequently people use structs with omitempty fields precisely because they don't want the field to be touched.\n\nInterestingly, one place where the field is touched is when the document\nis overwritten, because only the fields that are included will be in\nthe resulting document.  That's the case we're interested here - we want\nto maintain the invariant that fields with the omitempty qualifier\nare never in the database with their zero value.\n\nFor all of those reasons, I would like to implement this logic as bson.Delta(obj1, obj1), so the update can then be much richer and more precise.\n\nDelta sounds attractive but doesn't quite address our requirements, because\nI think that in order to get any \"unset\" operations, the fields in the\nfirst object would need be non-zero, and that means that\none would need to keep some object in sync with the type\ncontaining arbitrary non-zero values for all fields.\nThat is, bson.Delta(T{}, x) (where x is of type T) could never return\nan unset operation AFAICS because all fields in T would be zero.\nWe've been through a few iterations of function name and semantics.\nOther suggestions appreciated.\n. > parse the struct looking for omitempty tags to separate what you want in set vs. unset.\nThat's some of the logic that's in mgo/bson that we'd prefer to avoid second-guessing\nin third party code. It's not totally trivial to do that in the presence of inline and it would\nbe nice not to duplicate the type cache too, simple though it is.\n. > We might support a mode with Delta(nil, obj1), which is closer to your requirements.\nSo Delta(nil, obj) would be equivalent to AsUpdate(obj) ?\nThat would seem reasonable.\nImplementing Delta itself efficiently without doing any marshaling/unmarshaling round trippage\nseems hard, because deep checking equality is going to be hard.\nI can imagine a version of Delta implemented in terms of (an unexported) AsUpdate, something\nlike this:\n```\n// Delta returns the updates that will result if the object\n// obj0 is updated to be obj1. It does not always return\n// the most efficient update - when the values are hard\n// to compare, it might result in redundant Set members.\n//\n// If obj1 is nil, the result will be as if obj0 was a value\n// of the same type as obj0 with all fields non-zero and\n// different from those in obj1.\nfunc Delta(obj0, obj1 interface{}) (Update, error) {\n    u1 := asUpdate(obj1)\n    if obj0 == nil {\n        return u1, nil\n    }\n    u0 := asUpdate(obj0)\nfor attr := range u1.Unset {\n    if _, ok := u0.Unset[attr]; ok {\n        delete(u1.Unset, attr)\n    }\n}\nfor attr, val1 := range u1.Set {\n    if val0, ok := u0.Set[attr]; ok && obviouslyEqual(val0, val1) {\n        delete(u1.Set, attr)\n    }\n}\nreturn u1\n\n}\n// obviouslyEqual returns true if x and y are easily comparable\n// and will be equal when marshaled as BSON, and false\n// otherwise.\nfunc obviouslyEqual(x, y reflect.Value) bool {\n    ...\n}\n```\nI guess the obviouslyEqual function could fall back to marshaling and comparing\nbytes for the harder cases.\nI'm interested in your envisaged use cases for Delta - I haven't yet come across a situation\nwhere something like that might be useful.\n. I like that idea, but I'm not sure how it can work decently. As a really simple example,  say I have the type:\ntype T struct {\n    A int\n    // other fields\n}\nand I have a method that wants to set A in the database. \nfunc SetA(a int) {\n    old := T{ ??? }\n    obj := T {A: a}\n    ... := collection.Update(bson.Delta(old, obj)))\n}\nWhat should we set the value of old to if a can be any int? ISTM that the only way is to set old.A to any value other than a (a+1 for example) because otherwise we are not guaranteed that a will be set (or unset if A is marked as omitempty).\nIt feels to me like it would be more awkward in practice than just writing out the update doc.\n. Another possibility might be to go with AsUpdate (or other name with similar semantics) but add arguments to explicitly include and/or exclude a set of fields from the result.\n. > The first object would represent the result of a query in general, rather than being hand cooked.\nI don't understand how that could work without being racy. First, you probably don't want to do a query before doing an update as it's an unnecessary extra round trip to MongoDB. Second, the value you just retrieved might not be anything like the value that's in the database when you actually apply the update.\nBut I'm sure I misunderstand you somehow. Perhaps you could show how an existing piece of code could be changed to use Delta?\n. > Every single update has the potential to be racy, inherently.\nNot that racy.  Consider a document where the fields must be\nself-consistent and updated together.\ntype doc struct {\n    X, Y int\n}\nWe have two concurrent actors that are both concurrently trying to set\ndifferent (self-consistent) values for X and Y and they're using Delta\nto do it.  The document has the initial value {X: 1, Y: 2}.\nAlice: get -> {X: 1, Y: 2}\nBob: get -> {X: 1, Y: 2}\nAlice: set delta({X: 1, Y: 2}, {X: 3, Y: 2}) -> $set {x: 3}\nBob: set delt({X: 1, Y: 2}, {X: 1, Y: 4}) -> $set {y: 4}\nThe final value is {X: 3, Y: 4}, which is not self-consistent\nand doesn't represent the value required by either of the\nactors.\nIf both had done $set with values for both x and y, then\nalthough there's a race, the final value will at least be internally\nconsistent with one of the updates.\nThat's an inherent problem with Delta as you've suggested it\nAFAICS.\n. > Update operations have the potential to be racy. Delta simply computes the delta automatically.\nIt depends what you mean by \"racy\". There's the inherent race involved\nin two updates running concurrently - they might execute in either order\nand that's fine because the end result is either one or the other. There's\nalso the race that results from two updaters reading a value, modifying\nit and writing it back, which is not fine - we can end up with a value\nthat's neither of the two intended values. We wouldn't try to increment a value\nin the database that way - why should Delta be any different?\n. > a $set that is generated by a delta and a $set that is generated by hand has exactly the same potential for race conditions.\nNo. Assuming no conditional logic, a $set generated by hand will always set the same number of fields. A $set generated by a delta will set some number of fields depending on the value presented as the \"before\" value of the delta. The former will set all the required fields consistently. The latter may not.\nI am honestly trying to understand how I might use Delta in a useful way in real code. A example of how you might change some real existing code to use it would be helpful. You may see this as a simple point, but I don't, sorry.\n. FWIW I've discussed with some others, and they can't understand the reasoning either. Sometimes a fragment of code can provide greater understanding than prose.\nA parrticular phrase of your explanation that eludes me is \"the result of a query in general\". If I understood what you meant by that,  perhaps I'd understand your position (which I'd like to, BTW - I'd much prefer to contribute to the code base rather than forking parts of it).\n. LGTM FWIW. What an unfortunate semantic.\n. BTW it would be great if this could land soon, as it fixes a current critical bug in juju.\n. Please add a test case for this issue and verify that it fails without this fix.\n. I don't understand this fully, but my immediate suspicion is that the code looks racy. I'd at least expect a doc comment explaining how/why it's not.. > Also, despite having access to the internals, the suggested implementation seems to be doing its logic in a pretty high-level way, by marshaling structs into BSON and then back into a map\nWe're actually only marshaling to BSON and back when the object isn't\na struct (in practice that's only for Raw), which is when we can't fetch its\nfields directly.\nTo be honest, the map and Raw cases are only there so that the semantics\nare easier to explain - AsUpdate isn't really useful for those cases.\nThe struct case is the important one (for us, at any rate) and I'd be\nquite happy if the logic was restricted to structs only.\n\nVery frequently people use structs with omitempty fields precisely because they don't want the field to be touched.\n\nInterestingly, one place where the field is touched is when the document\nis overwritten, because only the fields that are included will be in\nthe resulting document.  That's the case we're interested here - we want\nto maintain the invariant that fields with the omitempty qualifier\nare never in the database with their zero value.\n\nFor all of those reasons, I would like to implement this logic as bson.Delta(obj1, obj1), so the update can then be much richer and more precise.\n\nDelta sounds attractive but doesn't quite address our requirements, because\nI think that in order to get any \"unset\" operations, the fields in the\nfirst object would need be non-zero, and that means that\none would need to keep some object in sync with the type\ncontaining arbitrary non-zero values for all fields.\nThat is, bson.Delta(T{}, x) (where x is of type T) could never return\nan unset operation AFAICS because all fields in T would be zero.\nWe've been through a few iterations of function name and semantics.\nOther suggestions appreciated.\n. > parse the struct looking for omitempty tags to separate what you want in set vs. unset.\nThat's some of the logic that's in mgo/bson that we'd prefer to avoid second-guessing\nin third party code. It's not totally trivial to do that in the presence of inline and it would\nbe nice not to duplicate the type cache too, simple though it is.\n. > We might support a mode with Delta(nil, obj1), which is closer to your requirements.\nSo Delta(nil, obj) would be equivalent to AsUpdate(obj) ?\nThat would seem reasonable.\nImplementing Delta itself efficiently without doing any marshaling/unmarshaling round trippage\nseems hard, because deep checking equality is going to be hard.\nI can imagine a version of Delta implemented in terms of (an unexported) AsUpdate, something\nlike this:\n```\n// Delta returns the updates that will result if the object\n// obj0 is updated to be obj1. It does not always return\n// the most efficient update - when the values are hard\n// to compare, it might result in redundant Set members.\n//\n// If obj1 is nil, the result will be as if obj0 was a value\n// of the same type as obj0 with all fields non-zero and\n// different from those in obj1.\nfunc Delta(obj0, obj1 interface{}) (Update, error) {\n    u1 := asUpdate(obj1)\n    if obj0 == nil {\n        return u1, nil\n    }\n    u0 := asUpdate(obj0)\nfor attr := range u1.Unset {\n    if _, ok := u0.Unset[attr]; ok {\n        delete(u1.Unset, attr)\n    }\n}\nfor attr, val1 := range u1.Set {\n    if val0, ok := u0.Set[attr]; ok && obviouslyEqual(val0, val1) {\n        delete(u1.Set, attr)\n    }\n}\nreturn u1\n\n}\n// obviouslyEqual returns true if x and y are easily comparable\n// and will be equal when marshaled as BSON, and false\n// otherwise.\nfunc obviouslyEqual(x, y reflect.Value) bool {\n    ...\n}\n```\nI guess the obviouslyEqual function could fall back to marshaling and comparing\nbytes for the harder cases.\nI'm interested in your envisaged use cases for Delta - I haven't yet come across a situation\nwhere something like that might be useful.\n. I like that idea, but I'm not sure how it can work decently. As a really simple example,  say I have the type:\ntype T struct {\n    A int\n    // other fields\n}\nand I have a method that wants to set A in the database. \nfunc SetA(a int) {\n    old := T{ ??? }\n    obj := T {A: a}\n    ... := collection.Update(bson.Delta(old, obj)))\n}\nWhat should we set the value of old to if a can be any int? ISTM that the only way is to set old.A to any value other than a (a+1 for example) because otherwise we are not guaranteed that a will be set (or unset if A is marked as omitempty).\nIt feels to me like it would be more awkward in practice than just writing out the update doc.\n. Another possibility might be to go with AsUpdate (or other name with similar semantics) but add arguments to explicitly include and/or exclude a set of fields from the result.\n. > The first object would represent the result of a query in general, rather than being hand cooked.\nI don't understand how that could work without being racy. First, you probably don't want to do a query before doing an update as it's an unnecessary extra round trip to MongoDB. Second, the value you just retrieved might not be anything like the value that's in the database when you actually apply the update.\nBut I'm sure I misunderstand you somehow. Perhaps you could show how an existing piece of code could be changed to use Delta?\n. > Every single update has the potential to be racy, inherently.\nNot that racy.  Consider a document where the fields must be\nself-consistent and updated together.\ntype doc struct {\n    X, Y int\n}\nWe have two concurrent actors that are both concurrently trying to set\ndifferent (self-consistent) values for X and Y and they're using Delta\nto do it.  The document has the initial value {X: 1, Y: 2}.\nAlice: get -> {X: 1, Y: 2}\nBob: get -> {X: 1, Y: 2}\nAlice: set delta({X: 1, Y: 2}, {X: 3, Y: 2}) -> $set {x: 3}\nBob: set delt({X: 1, Y: 2}, {X: 1, Y: 4}) -> $set {y: 4}\nThe final value is {X: 3, Y: 4}, which is not self-consistent\nand doesn't represent the value required by either of the\nactors.\nIf both had done $set with values for both x and y, then\nalthough there's a race, the final value will at least be internally\nconsistent with one of the updates.\nThat's an inherent problem with Delta as you've suggested it\nAFAICS.\n. > Update operations have the potential to be racy. Delta simply computes the delta automatically.\nIt depends what you mean by \"racy\". There's the inherent race involved\nin two updates running concurrently - they might execute in either order\nand that's fine because the end result is either one or the other. There's\nalso the race that results from two updaters reading a value, modifying\nit and writing it back, which is not fine - we can end up with a value\nthat's neither of the two intended values. We wouldn't try to increment a value\nin the database that way - why should Delta be any different?\n. > a $set that is generated by a delta and a $set that is generated by hand has exactly the same potential for race conditions.\nNo. Assuming no conditional logic, a $set generated by hand will always set the same number of fields. A $set generated by a delta will set some number of fields depending on the value presented as the \"before\" value of the delta. The former will set all the required fields consistently. The latter may not.\nI am honestly trying to understand how I might use Delta in a useful way in real code. A example of how you might change some real existing code to use it would be helpful. You may see this as a simple point, but I don't, sorry.\n. FWIW I've discussed with some others, and they can't understand the reasoning either. Sometimes a fragment of code can provide greater understanding than prose.\nA parrticular phrase of your explanation that eludes me is \"the result of a query in general\". If I understood what you meant by that,  perhaps I'd understand your position (which I'd like to, BTW - I'd much prefer to contribute to the code base rather than forking parts of it).\n. LGTM FWIW. What an unfortunate semantic.\n. BTW it would be great if this could land soon, as it fixes a current critical bug in juju.\n. Please add a test case for this issue and verify that it fails without this fix.\n. I don't understand this fully, but my immediate suspicion is that the code looks racy. I'd at least expect a doc comment explaining how/why it's not.. ",
    "ajpen": "Can see this as unnecessary. Closing...\n. Can see this as unnecessary. Closing...\n. ",
    "babbageclunk": "Good point, the retry should definitely be in mgo, rather than mgo.txn. I'll move it. Are you alright with the infinite loop to do the retry? Or should I be doing something a bit cleverer?\nSorry, I should have added a test - it's fiddly to do, but it looks like there are already some other tests trying to reproduce racy things.\n. Hi, sorry for the delay on this - I've moved the retrying into mgo, in Query.Apply and Collection.Upsert. Unfortunately I've had real trouble reproducing the problem in a test at that level - no matter how I try I can't get the upserts to fail with an 11000 error. \nHowever when I run the txn tests TestTxnQueueStashStressTest was reliably failing and now passes consistently with the change. (I don't really understand why the txn tests aren't run by go test ./... from the mgo directory.)\n. It seems like the -check.v in .travis.yml means the mgo/txn tests aren't being run - is that deliberate? I've added a line to run the txn tests as well.\n. Hmm - that's unfortunate. It looks like TestTxnQueueStressTest fails intermittently on the parent branch as well. I'm going to pull the change to turn on the mgo/txn tests out of this PR into a new one and add a skip for that test until someone can work out what's going wrong.\n. Sigh - no, of course, that doesn't work because this PR fixes TestTxnQueueStashStressTest. So I'm including the skip here.\n. To show that this change does fix the duplicate key error, here's a build of a branch that has the txn tests turned on without the upsert retrying - TestTxnQueueStashStressTest fails in the mongo 3.2 test run.\n. Thanks for going through this!\n. This is a spurious failure - the script to start the various mongo processes and set up users failed with couldn't add user: not master\nThe same commit passed on Travis here: https://travis-ci.org/babbageclunk/mgo/builds/143769207\n. Thanks!\n. I've got a branch with these same changes against the v2 branch. Do you want me to make another PR for that, or will you just merge this across to there?\n. Good point, the retry should definitely be in mgo, rather than mgo.txn. I'll move it. Are you alright with the infinite loop to do the retry? Or should I be doing something a bit cleverer?\nSorry, I should have added a test - it's fiddly to do, but it looks like there are already some other tests trying to reproduce racy things.\n. Hi, sorry for the delay on this - I've moved the retrying into mgo, in Query.Apply and Collection.Upsert. Unfortunately I've had real trouble reproducing the problem in a test at that level - no matter how I try I can't get the upserts to fail with an 11000 error. \nHowever when I run the txn tests TestTxnQueueStashStressTest was reliably failing and now passes consistently with the change. (I don't really understand why the txn tests aren't run by go test ./... from the mgo directory.)\n. It seems like the -check.v in .travis.yml means the mgo/txn tests aren't being run - is that deliberate? I've added a line to run the txn tests as well.\n. Hmm - that's unfortunate. It looks like TestTxnQueueStressTest fails intermittently on the parent branch as well. I'm going to pull the change to turn on the mgo/txn tests out of this PR into a new one and add a skip for that test until someone can work out what's going wrong.\n. Sigh - no, of course, that doesn't work because this PR fixes TestTxnQueueStashStressTest. So I'm including the skip here.\n. To show that this change does fix the duplicate key error, here's a build of a branch that has the txn tests turned on without the upsert retrying - TestTxnQueueStashStressTest fails in the mongo 3.2 test run.\n. Thanks for going through this!\n. This is a spurious failure - the script to start the various mongo processes and set up users failed with couldn't add user: not master\nThe same commit passed on Travis here: https://travis-ci.org/babbageclunk/mgo/builds/143769207\n. Thanks!\n. I've got a branch with these same changes against the v2 branch. Do you want me to make another PR for that, or will you just merge this across to there?\n. ",
    "256dpi": "Thanks! How long will it take until this change is merged into v2 (stable)?\n. Thanks! How long will it take until this change is merged into v2 (stable)?\n. ",
    "BenLubar": "@niemeyer it looks like one of the Travis builds failed because a port was already in use.\n. You had a test case for cursors that are supposed to time out, but no test case for cursors that aren't supposed to time out.\n. @niemeyer it looks like one of the Travis builds failed because a port was already in use.\n. You had a test case for cursors that are supposed to time out, but no test case for cursors that aren't supposed to time out.\n. ",
    "niklaskorz": "Any reason this hasn't been merged yet? The behavior for no timeout can't get any more broken than it currently is, no matter if there's a test for no timeout.\n. Any reason this hasn't been merged yet? The behavior for no timeout can't get any more broken than it currently is, no matter if there's a test for no timeout.\n. ",
    "bigodines": "Is there anything missing to this PR to get it merged?. Is there anything missing to this PR to get it merged?. ",
    "kat-co": "Just to clearly state the race:\n- On socket.go:329 we dereference the stats pointer to call socketsAlive. We do this without locking the mutex on stats.go:34\n- On stats.go:59\nScenario 1\n- We dereference the stats pointer before ResetStats has a chance to point it to a new instance.\n- The stats we are trying to set are lost.\nEnd result: we're trying to set stats on something we're resetting and we don't care.\nScenario 2\n- We dereference the stats pointer after ResetStats points it to a new instance.\n- We enter the critical section by locking the mutex and {inc,dec}rement the correct memory.\nEnd result: we're settings stats on something we thought had been reset and we receive an incorrect result.\n. Just to clearly state the race:\n- On socket.go:329 we dereference the stats pointer to call socketsAlive. We do this without locking the mutex on stats.go:34\n- On stats.go:59\nScenario 1\n- We dereference the stats pointer before ResetStats has a chance to point it to a new instance.\n- The stats we are trying to set are lost.\nEnd result: we're trying to set stats on something we're resetting and we don't care.\nScenario 2\n- We dereference the stats pointer after ResetStats points it to a new instance.\n- We enter the critical section by locking the mutex and {inc,dec}rement the correct memory.\nEnd result: we're settings stats on something we thought had been reset and we receive an incorrect result.\n. ",
    "reedobrien": "Hello, @niemeyer any thoughts on this or when it might make it to a stable release? It is triggering farily regularly. Thanks!\ncc @mjs @AlexisBruemmer\n. Hello, @niemeyer any thoughts on this or when it might make it to a stable release? It is triggering farily regularly. Thanks!\ncc @mjs @AlexisBruemmer\n. ",
    "mjs": "Note that #302 also fixes this issue and in an arguably more elegant way.\n. Note that #302 also fixes this issue and in an arguably more elegant way.\n. Note that #302 also fixes this issue and in an arguably more elegant way.\n. Note that #302 also fixes this issue and in an arguably more elegant way.\n. ",
    "evan-stripe": "@niemeyer Err, sorry to bug, but any thoughts on this? This would be pretty helpful for one of our use cases.\n. @niemeyer Err, sorry to bug, but any thoughts on this? This would be pretty helpful for one of our use cases.\n. ",
    "adamjace": "Please can this be merged?. +1. Please can this be merged?. +1. ",
    "luca-moser": "+1. +1. ",
    "swathysubhash": "+1. +1. ",
    "momchil-mitev": "+1. +1. ",
    "uschen": "is mgo still under active maintain?. is mgo still under active maintain?. ",
    "thao6626": "so, mgo still does not support partial index now??  \n2017-08-19. so, mgo still does not support partial index now??  \n2017-08-19. ",
    "mzimmerman": "Patch works for me. Patch works for me. ",
    "omani": "+1. can we merge this please?. +1. can we merge this please?. ",
    "domodwyer": "Closing and re-targeting the v2 branch.. Hi @carter2000 \nThis looks very interesting, and it's something we're interested in merging (if that's OK!) - do you have any tests for this?\nDom. Closing and re-targeting the v2 branch.. Hi @carter2000 \nThis looks very interesting, and it's something we're interested in merging (if that's OK!) - do you have any tests for this?\nDom. ",
    "h12w": "This PR handles update one record at at time and makes the bulk update useless.. This PR handles update one record at at time and makes the bulk update useless.. ",
    "eaglerayp": "Hi, @h12w thanks for your comment. I've fix the problem. Btw, I think that CI should be passed since I don't see any test failed, do I miss something?. Hi, @h12w thanks for your comment. I've fix the problem. Btw, I think that CI should be passed since I don't see any test failed, do I miss something?. ",
    "asaf": "@niemeyer Anything blocks from merging this PR? it is a big issue,\nThanks . @niemeyer Anything blocks from merging this PR? it is a big issue,\nThanks . ",
    "quii": "Just to add this is effecting the project I am working on too. Just to add this is effecting the project I am working on too. ",
    "mxplusb": "Is there an update to this PR? It's very much needed for #288 for multiple people and companies.. Is there an update to this PR? It's very much needed for #288 for multiple people and companies.. ",
    "Oppodelldog": "Sorry for that pr. Coming from Bitbucket I'm not used to github PRs. I tried to create this PR against the fork-repository.\nNevertheless the changes in this PR are not just a test, but a solution we need to apply to the driver.\nSo if someone sees this PR I'd like to get an idea if my changes are a good enough to find their way into the driver repo.\n. Sorry for that pr. Coming from Bitbucket I'm not used to github PRs. I tried to create this PR against the fork-repository.\nNevertheless the changes in this PR are not just a test, but a solution we need to apply to the driver.\nSo if someone sees this PR I'd like to get an idea if my changes are a good enough to find their way into the driver repo.\n. ",
    "gonzaloserrano": "\ud83d\udc4d . \ud83d\udc4d . ",
    "hexadecy": "In travis it lacks test for mongo 3.2, 3.4\nBut evergreen looks cool. I suggest EnsureIndexFromStruct.. In travis it lacks test for mongo 3.2, 3.4\nBut evergreen looks cool. I suggest EnsureIndexFromStruct.. ",
    "fmpwizard": "If you close and reopen the PR, travis will rerun this build and thanks to  https://github.com/go-mgo/mgo/pull/462 , your PR should pass all tests on all mongodb versions . If you close and reopen the PR, travis will rerun this build and thanks to  https://github.com/go-mgo/mgo/pull/462 , your PR should pass all tests on all mongodb versions (even though this is just a doc fix that didn't touch any code). If you close and reopen the PR, travis will rerun this build and thanks to  https://github.com/go-mgo/mgo/pull/462 , your PR should pass all tests on all mongodb versions . If you close and reopen the PR, travis will rerun this build and thanks to  https://github.com/go-mgo/mgo/pull/462 , your PR should pass all tests on all mongodb versions . If you close and reopen the PR, travis will rerun this build and thanks to  https://github.com/go-mgo/mgo/pull/462 , your PR should pass all tests on all mongodb versions . If you close and reopen the PR, travis will rerun this build and thanks to  https://github.com/go-mgo/mgo/pull/462 , your PR should pass all tests on all mongodb versions . If you close and reopen the PR, travis will rerun this build and thanks to  https://github.com/go-mgo/mgo/pull/462 , your PR should pass all tests on all mongodb versions (even though this is just a doc fix that didn't touch any code). If you close and reopen the PR, travis will rerun this build and thanks to  https://github.com/go-mgo/mgo/pull/462 , your PR should pass all tests on all mongodb versions . If you close and reopen the PR, travis will rerun this build and thanks to  https://github.com/go-mgo/mgo/pull/462 , your PR should pass all tests on all mongodb versions . If you close and reopen the PR, travis will rerun this build and thanks to  https://github.com/go-mgo/mgo/pull/462 , your PR should pass all tests on all mongodb versions . ",
    "craiggwilson": "oh, probably need to rebase.... oh, probably need to rebase.... ",
    "0x457": "omit. omit. ",
    "nghialv": "Hi @niemeyer.\nCould you take a look at this PR?. Hi @niemeyer.\nCould you take a look at this PR?. ",
    "nogoegst": "ok?. Is is not an implementation detail because it defines behavior of the exported function.  What happens if timeout is set to -2?. ok?. Is is not an implementation detail because it defines behavior of the exported function.  What happens if timeout is set to -2?. ",
    "carter2000": "@domodwyer \nI use the patch two years ago in our production environment, and it works as expected all the time.\n. Hi @domodwyer \nunit test: https://github.com/go-mgo/mgo/pull/437/commits/15704195518898b49708b480e14f609addb5a6f5#diff-a0c7225d6bebd1194ed175feb4da9d2eR2093. https://github.com/go-mgo/mgo/pull/437. @domodwyer \nI use the patch two years ago in our production environment, and it works as expected all the time.\n. Hi @domodwyer \nunit test: https://github.com/go-mgo/mgo/pull/437/commits/15704195518898b49708b480e14f609addb5a6f5#diff-a0c7225d6bebd1194ed175feb4da9d2eR2093. https://github.com/go-mgo/mgo/pull/437. ",
    "vivace-io": "Duplicate of #446 - shall we close this and move talking to there?. Naming feels rather confusing, doesn't seem to accurately describe what the function does. Might want to consider another name?. Duplicate of #446 - shall we close this and move talking to there?. Naming feels rather confusing, doesn't seem to accurately describe what the function does. Might want to consider another name?. ",
    "mapete94": "I made the requested changes. I did not add a test. I didn't see a way to create a 64bit int for your index type within the documentation. I may have missed it in the documentation, but if there is a way and I'm missing it I can write one.. I made the requested changes. I did not add a test. I didn't see a way to create a 64bit int for your index type within the documentation. I may have missed it in the documentation, but if there is a way and I'm missing it I can write one.. ",
    "jacksontj": "+1. +1. ",
    "zensh": "@niemeyer https://travis-ci.org/go-mgo/mgo/jobs/289072737 this job failed, it should be restart~. After this PR\uff0cwe can use:\ns := session.Clone()\ndefer s.Close()\ninstead of \ns := session.Copy()\ndefer s.Close(). We can try the issue with code:\n```go\nfunc main() {\n    session, err := mgo.DialWithInfo(someRemoteDailInfo)\n    if err != nil {\n        panic(err)\n    }\nfor {\n    fmt.Println(\"Ping:\", session.Ping())\n    time.Sleep(time.Second)\n}\n\n}\n```\nRun it, then close the wifi, and then open the wifi, some result like this:\nsh\nPing: <nil>\nPing: <nil>\nPing: <nil>\nPing: <nil>\nPing: <nil>\nPing: <nil>\nPing: <nil>\nPing: read tcp 192.168.0.72:58883->192.168.0.21:27017: i/o timeout\nPing: no reachable servers\nPing: <nil>\nPing: <nil>\nPing: <nil>\nIt means that the session will reconnect automatic.. @niemeyer Could you review this PR? We have used the fork version in our productions. @niemeyer https://travis-ci.org/go-mgo/mgo/jobs/289072737 this job failed, it should be restart~. After this PR\uff0cwe can use:\ns := session.Clone()\ndefer s.Close()\ninstead of \ns := session.Copy()\ndefer s.Close(). We can try the issue with code:\n```go\nfunc main() {\n    session, err := mgo.DialWithInfo(someRemoteDailInfo)\n    if err != nil {\n        panic(err)\n    }\nfor {\n    fmt.Println(\"Ping:\", session.Ping())\n    time.Sleep(time.Second)\n}\n\n}\n```\nRun it, then close the wifi, and then open the wifi, some result like this:\nsh\nPing: <nil>\nPing: <nil>\nPing: <nil>\nPing: <nil>\nPing: <nil>\nPing: <nil>\nPing: <nil>\nPing: read tcp 192.168.0.72:58883->192.168.0.21:27017: i/o timeout\nPing: no reachable servers\nPing: <nil>\nPing: <nil>\nPing: <nil>\nIt means that the session will reconnect automatic.. @niemeyer Could you review this PR? We have used the fork version in our productions. ",
    "luby": "um.  um. "
}
{
    "hayate": "sorry for delay :arrow_right: LGTM\n. thank you @mattn LGTM\n. LGTM\n. :+1: LGTM\n. :eyes: :+1: :v: :m: :e-mail: \n. LGTM\n. LGTM\n. there is already a f.Flush() before the loop (https://github.com/gengo/grpc-gateway/pull/30/files#diff-c90e18d9d959604ec95823fb307c9f65R31) \n. LGTM\n. LGTM\n. :+1:  LGTM \n. LGTM\n. LGTM\n. :+1: LGTM\n. LGTM\n. LGTM\n. :+1: LGTM\n. ",
    "yugui": "Thank you for your contribution.  Could you fix the minor issues before merge?\n. LGTM\n. Originally I thought of terminating OAuth2 authorization in the gateway.\nBut I have started to think that it will work fine enough and is more extensible and more consistent to google.api.http to \njust forward some HTTP headers as CallOptions.\n. @IceMan81 good point.  Since the API of oneof in go protobuf has changed, we need to care of that too.\n. Thank you!\n. thanks\n. @shurcooL Not yet. But it was enough to replace the old implementation.\nI am working on query parameters and other updates from Wolfgang in another branch #14.\n. No, I don't have any plan to do.\nI wonder why you need C++ version.  Since it generates a reverse-proxy, it does not restrict the language  in which you implement your gRPC server.\nAlso I don't think C++ version will significantly improve the performance of the proxy.\nThey only benefit which comes to my mind is that C++ version would be able to support arbitrary message without rebuilding the proxy because C++ has a support of DynamicMessage.\nWhat do you think?\n. The question seems to be resolved.\nhttps://groups.google.com/d/msg/grpc-io/JnjCYGPMUms/2TSnRPm7E7cJ\n. > Is there any C/C++ api gateway developed as on today\nAFAIK, no.\n\nare there any examples I can refer to understand how I can link this reverse-proxy with my gRPC server written in C++\n\nIt is probably possible to link the generated go code with C++ server since CGO can generate a shared library. \nBut can I confirm what you really need in case?  For what purpose are you trying to link the reverse-proxy to your gRPC server? For reducing network latency? For simplicity of deployment and monitoring?\nIf it is for network latency, I don't think linking solves the problem because the reverse proxy still communicates with the gRPC server over TCP or a  unix domain socket.. \nThis is how grpc-gateway usually works. So you don't need dive into the complicated world of error handlings, concurrency and garbage collection in CGO. \nWith CGO, in theory it is still possible to run the reverse-proxy server in the same process as the gRPC server. But usually it is not very beneficial.\nhttps://github.com/yugui/grpc-gateway/tree/example/embed/examples/cmd/example-cxx-server.. Thank you for letting me know that.  But it looks to be an issue in protoc or googleapis.\nLet me confirm to the upstream.\nAlthough the extension google.api.http in MethodOptions is an optional field, protoc emits as if it is a repeated field for the syntax (2).  I am not sure what is the expected behavior.\nSome possibilities come to my mind.\n1. protoc should have combined those lines into one extension if the two variations of syntax mean the same thing?\n2. goprotobuf should have done it?\n3. google.api.http should have been repeated field and grpc-gateway should have combine the repeated values into one?\nHere is the dump of the code generation request.\n...\n    name: \"EchoBody\"\n    input_type: \".gengo.grpc.gateway.example.SimpleMessage\"\n    output_type: \".gengo.grpc.gateway.example.SimpleMessage\"\n    options: <\n      /* 38 unknown bytes */\n      72295728: \"\\\"\\x15/v1/example/echo_body\"\n      72295728: \":\\x01*\"\n    >\n. LGTM.  Again, thanks a lot for your contribution.\n. LGTM.  Thank you for your contribution.\n. Actually that part of the behavior of protoc-gen-go is inconsistent and insane.  Let me think about how to follow the behavior.\n. @shurcooL Your question is whether the multi-segment wildcard ** is greedy or not.\nI believe it is and I implemented the match mechanism as greedy by intention.  But it is not clearly described in the spec.\nWe might need to ask the spec owner.\n. +1\n. Interesting.\nBut at this moment, compatibility to Swagger is more important for us.  Let me check the compatibility before decision.\n. > We're actually interested in doing this ourselves. @yugui, would you accept PRs for this?\nYes, I will.\n. I have read the spec of the server-sent events.\nIIUC, the spec defines something different from the current line-based streaming. So I decided not to simply replace the current way of streaming with server-sent events.\nBut it might be useful if we can support server-sent events as a part of pluggable marshaling mechanism.\n. lgtm\n. Sorry for my late reply.  I have just merged #34.\nOn Tue, Aug 11, 2015 at 4:36 PM Michal Witkowski notifications@github.com\nwrote:\n\n@yugui https://github.com/yugui, do you need anything from me in\nrelation to #34 https://github.com/gengo/grpc-gateway/pull/34 to get\nthis fixed? :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/gengo/grpc-gateway/issues/32#issuecomment-129740634.\n. Ah, I understand.\n\nAre you talking about the same one as this design doc?\nhttps://docs.google.com/document/d/1e8kOo3r51b2BWtTs_1uADIA5djfXhPT36s6eHVRIvaU/edit\n. The change itself LGTM, but I want to know what was the error #36 exactly.\n. LGTM\nc.f. https://docs.google.com/document/d/1e8kOo3r51b2BWtTs_1uADIA5djfXhPT36s6eHVRIvaU/edit\n. LGTM, but IIUC it depends on #38 thus it is not ready to merge?\n. LGTM\n. I guess both proto3 and grpc-gateway are working fine, but let me confirm.\nIIUC, you defined a service which takes an Alerts message as a request, and tried to pass exactly [{\"2\":\"3\"}] as an HTTP request body.\nYour generated gateway rejected this request because it expects a JSON object as a request body but received an array of JSON objects.\nIn this case, an example of the request format which your gateway expects is something like\njson\n{\n    \"alert\": [\n        {\n            \"m\": {\n                \"1\": \"2\",\n                \"3\": \"4\"\n            }\n        },\n        {\n            \"m\": {\n                \"5\": \"6\"\n            }\n        }\n    ]\n}\nFYI,\n- the gateway accepted {\"1\": \"2\"} because JSON unmarshaller just ignores unknown keys.\n- proto3 does not allow repeated map fields but allows repeated message fields which contain maps.\n. Probably we should have a cool diagram in the README page to help people to catch the idea at a glance.\n. @tmc sure.\n. LGTM.  Thank you!\n. I guess the idea requires the gateway to run in the same process as the wrapped gRPC service.\nIt can be useful and patch is welcome if it is compatible enough to ordinal gRPC services.\nBut I have no plan to work on that because of two reasons.\n1. it is not a bottleneck for us\n2. Needs changes to grpc-go itself to keep \"google.golang.org/grpc\".SendHeader compatible to the ordinal call flow.\n. Thank you for your contribution. LGTM.\nJust for recording:  It follows golang/protobuf@535a10468679b4cf155f6a7afdf53b554633fc09\n. I have restarted the tests.  I expect it will success since I have merged #51.\n. LGTM\n. @peter-edge sorry for being late.  I'll take a look.\n. Could you resolve the merge conflict?\n. Although I'm a fan of glog, I like the overall direction of this PR.\nI have some comments on implementation details.\n- I don't want you to add dependency on go-dlog for now.\n  It is because I don't want to add much dependency on things which neither grpc-go or protoc-gen-go depends on.\n- Some glog.V(1) logs are too verbose for Info level.  How about adding Debug level?  Is it too far from grpclog?\n. Closing this PR due to this reason:\n\nI don't want you to add dependency on go-dlog for now. It is because I don't want to add much dependency on things which neither grpc-go or protoc-gen-go depends on.\n\nand because https://github.com/grpc/grpc-go/pull/341 was closed.\n. LGTM\n. Interesting.  But can I consider this PR after #53?  I guess this feature gets easier to implement without adding hacky flag once we complete #53.\n. Now we can indent JSON outputs with JSONPb marshaler.\n. Personally I don't think it's worth to have a docker image because it is not so hard to build protoc-gen-grpc-gateway if the client has a dev environment of golang, and the client must have the dev environment in any way to build generated go files.\nAlso, we have no plan to switch from Travis CI to CircleCI for consistency to other gengo repositories.\n. I like the direction, but please fix some errors.\n- Two golint errors.\n- Build error.  This error was overlooked because .travis.yaml of the repository was wrong.  So I have just fixed .travis.yaml in the master branch.\n. LGTM.  Thank you for your contribution.\n. LGTM\n. Thanks.  LGTM for Authorization header.\nIt would be even better if you split this PR into two; one for each header.\nFor Host header, I am wondering if it should be forwarded as a forwarded metadata according to RFC 7239.  What do you think?\n. > the change is in accordance to Section 5.3 of that RFC. It says that the original Host header of the proxy request is passed into the Host header of the request to the backend.\nIIUC, the section is talking about host parameter of forwarded header but not forwarding host header itself.\n. Sorry for being late.\nAlmost LGTM except several things\n- The comment I posted above\n- Merge conflict\n- What do you think about Forwarded header in RFC 7239?\n. @achew22 No, I don't have a good example since I am just trying Bazel in a small project which does not use grpc-gateway.\nBut I agree that it would be useful if we have a Skylark macro like grpc_gateway_library or grpc_gateway_binary\n. @ivucica Thank you for sharing that.\nAlthough I'm not going to write the skylark rule by myself very soon, but it is definitely on my to-do list.\nAlso I'm happy to accept PRs for the rule. In both cases, your genrule will be quite helpful.\n. Amazing!\nI'm afraid, but it takes a bit long to take a look at this PR since it is large.\nMaybe I have some time to do that this weekend.\n. @achew22 \nThank you.  I really like the overall direction, e.g.\n1. Map proto message to predefined message schema\n2. Construct Swagger definition in process and marshal it in JSON afterward.\nThere are some comments.\n1. You don't have to copy entire the protoc plugin. Code generator part of protoc-gen-grpc-gateway is pluggable by intention.  So you can reuse the existing message/option parser. Feel free to propose some extension of the parser if is functionality is not sufficient for your purpose.\n2. I'm not sure, but sometimes it might be better to define parameters of paths with parameterObject rather than referenceObject because it is allowed to place some fields in request paths.\n. :+1: \n. https://travis-ci.org/gengo/grpc-gateway/builds/97933023\nThe test is broken because master branch was broken.\nMaybe you just need to rebase onto the current master to fix the test.\n. I see. I'm looking forward to seeing it.\n. @achew22 \nI want to hear your opinion about https://github.com/gengo/grpc-gateway/pull/68/files#r50193073.\nBesides the comment and other small coding-level issues, I like the overall direction.\n. The design looks good to me.\nI'll add some coding-level comments.\n. LGTM.\nThank you for your excellent work!\n. I'm trying to investigate the issue.\nThere are some quick updates:\n- I am not sure why make test works fine. But protoc-gen-grpc-gateway fails to load message descriptor correctly.  When it loads your message descriptor, the descriptor actually lacks http path mapping even though you specified it in .proto file.\n- Another possibly related thing is that proto.MarshalText and proto.UnmarshalText don't roundtrip for oneof fields. \n- protoc warns [libprotobuf WARNING google/protobuf/compiler/parser.cc:492] No syntax specified for the proto file. Please use 'syntax = \"proto2\";' or 'syntax = \"proto3\";' to specify a syntax version. (Defaulted to proto2 syntax.) when I see this issue.\n. The root cause was that generated third_party/googleapis/google/api/*.pb.go were incompatible to recent github.com/golang/protobuf.\n71 will fix this issue.\n. merged.\n. #93 helps us to forward headers (and trailers).\nAlso it would be better to let the gateway forward them by default.\n. Solved by #93 \n. I agree that there are such use cases.\nBut I am wondering if validation just shows an example of more generic interception mechanism.\nWhat do you think about having a more generic middleware (interceptor) system?\ne.g.\n``` go\ntype Handler interface {\n  Handle(context.Context, http.ResponseWriter, http.Request, map[string]string)\n}\ntype HandlerFunc func(context.Context, http.ResponseWriter, http.Request, map[string]string)\nfunc (f HandlerFunc) Handle(ctx context.Context, w http.ResponseWriter, r *http.Request, pathParams map[string]string) {\n  f(ctx, w, r, pathParams)\n}\ntype Interceptor interface {\n  Intercept(next Handler) Handler\n}\ntype InterceptorFunc func(ctx context.Context, w http.ResponseWriter, r *http.Request, pathParams map[string]string, next Handler)\nfunc (f InterceptorFunc) Intercept(next Handler) Handler {\n  return HandlerFunc(func(ctx context.Context, w http.ResponseWriter, r *http.Request, pathParams map[string]string) {\n    f(ctx, w, r, pathParams, next)\n  })\n}\n``\n. @gdm85 I'd appreciate it if you update the PR. But let me thing about if the interfaceHandleris good enough to expose.\nI am suspecting ifpathParams` should not be there.\n. @gdm85 I'm sorry but let me change my opinion.\nI don't think it is useful to add the interceptor mechanism to grpc-gateway itself because ServeMux is just an ordinal implementation of http.Handler.  So you can wrap ServeMux in the standard way of golang, like https://godoc.org/github.com/goji/httpauth.\nThis PR or my proposal about interceptor can be still useful in one of the following cases, but none of them is enough to justify the idea of interceptor.\n1. Your interceptor needs to depend on pathParams; or\n2. Your interceptor needs to depend on gRPC method name as your PR does\n3. You need to apply different set of interceptors per url pattern.\nFor 1 and 2, I don't want to make the interceptor aware of pathParams nor method names because it is too intrusive -- gateways should focus on HTTP-level and anything in gRPC-level should happen in the actual gRPC services behind the gateways.\nFor 3, I am not sure if there is such a real use-case. But I guess you don't have such a use case because your PR does not cover the functionality of 3. If so, I'd like to revisit this idea when we see an actual use-case.\n. LGTM. Thank you.\n. @peter-edge Thank you for your information.  I didn't know that it is that slower.\nThen, it sounds better to have an option to keep using encoding/json, although jsonpb is mandatory for users who wants Any, Timestamp types, oneof fields and others.\nI am thinking of a mechanism which allows users to register/overwrite marshaler per MIME type.\nWith this mechanism, jsonpb would be a good default marshaler for application/json but you could keep using encoding/json if you don't need new proto3 types of fields.\n. I am not sure, but maybe go-protobuf does not yet support fields of Any type?\ngolang/protobuf#44\n. Thank you\n. @JohanSJA Thank you.  Yes, it is #79.\n. Now the default marshaller uses 'jsonpb'.  So I think it is supported.\n- https://github.com/gengo/grpc-gateway/blob/master/examples/browser/a_bit_of_everything_service.spec.js#L81\n- https://travis-ci.org/gengo/grpc-gateway/jobs/137727616#L1049\n@gyuho Could you re-confirm with the HEAD of the master branch?\nLet me know if you still have the issue.\nI'll close this issue if otherwise.\n. @gyuho I see. I'll take a look.\n. @gyuho \nDoes this work fine?\nshell\ncurl -L http://localhost:2379/v3alpha/watch -X POST -d '{\"create_request\": {\"key\": \"Zm9v\"}}'\nYou need to specify literally one of the fields grouped by request_union.\n. @gyuho \nIt does not look to be an issue of grpc-gateway.  I could reproduce the issue without grpc-gateway.\n``` go\npackage main\nimport (\n        \"log\"\n    \"github.com/coreos/etcd/etcdserver/etcdserverpb\"\n    \"github.com/golang/protobuf/jsonpb\"\n\n)\nfunc main() {\n        const input = {\"create_request\": {\"key\": \"Zm9v\"}}\n        var protoReq etcdserverpb.WatchRequest\n        if err := jsonpb.UnmarshalString(input, &protoReq); err != nil {\n                log.Fatal(err)\n        }\n}\n```\nconsole\n$ go run main.go\n2016/07/12 12:15:28 unknown field \"create_request\" in etcdserverpb.WatchRequest\nexit status 1\nI don't know what the root cause actually is, but I suspect incompatibility between github.com/golang/protobuf/jsonpb and gogoproto.\n. @gyuho Confirmed in https://gist.github.com/yugui/4425e705680fb4f6c99423b99b8a83d2\nconsole\n$ go get gist.github.com/4425e705680fb4f6c99423b99b8a83d2.git\n$ go build -o test gist.github.com/4425e705680fb4f6c99423b99b8a83d2.git\n$ ./test\n$ echo $?\n0\n$ go build --tags=gogo -o test gist.github.com/4425e705680fb4f6c99423b99b8a83d2.git\n$ ./test\n2016/07/12 12:53:05 unknown field \"create_request\" in main.WatchRequest\n$ echo $?\n1\n. @gyuho \nI am not yet confident but I suspect if this proto in (*WatchRequest).XXX_OneofFuncs,\n\nfunc (*WatchRequest) XXX_OneofFuncs() (func(msg proto.Message, b *proto.Buffer) error, func(msg proto.Message, tag, wire int, b *proto.Buffer) (bool, error), func(msg proto.Message) (n int), []interface{}) {\n\nat https://github.com/gyuho/etcd/blob/3108859828f851e9a46772fe8e16d257b40f3639/etcdserver/etcdserverpb/rpc.pb.go#L1084\n should have been github.com/golang/protobuf/proto but not github.com/gogo/protobuf/proto so that it is compatible to https://github.com/golang/protobuf/blob/ba6f978a1a6606adf3ccb6987f15c64262bfdbc2/proto/properties.go#L729.\n. Just %-encoding is not a safe representation of an arbitrary byte sequence within path due to golang/go#10887.\nWe need to confirm the expected spec at first.\n. Thank you for your contribution.\nLGTM as a short-term solution except the license issue I commented above.\nFor mid-term, it might be better to generate static mapping between proto field names and go field names because the only runtime usage of PascalFromSnake is in runtime/query.go. So once we have the static mapping for runtime usage, we can safely depend on \"github.com/golang/protobuf/protoc-gen-go/generator\".CamelCase.\nThe test failure in Travis CI is caused because generated files in the repository are out-of-date. So you can ignore the failure for now.\n. LGTM\n. Thank you!\n. #96 fixed this.\nThe struct tags generated by protoc-gen-go are a kind of such static mappings.\n. I don't think it is better to lose error code in the message.\nSo I still slightly prefer err.Error(). Why do you think grpc.ErrorDesc(err) is better?\n. @floridoo It is not too bad.  But I still don't understand what's wrong in the current format of the message.\n. OK.  I will accept it if you give me a pull request with the style:\njson\n{\n  \"error\": \"Unauthenticated\",\n  \"code\": 16\n}\n. LGTM\n. Good. Could you re-generate example files with make example?\n. LGTM\n. What do you want to do with NodeJS?\nI think grpc-gateway works fine even if you write your gRPC server with NodeJS.  But I guess you are talking about something different.\n. It is a kind of intentional because proto3 does not distinguish default values and missing values.\nBut I know that this behavior sometimes causes problems in client side.\nIt will be fixed if you use jsonpb marshaller as discussed in #79.\n. I agree on its benefit. But I prefer simply using grpclog package.\nWhat do you think?\n. I don't remember why I closed #54.  But maybe I confused it with adding dependency to dlog?\nI did not want to add extra dependency for that small improvement.  But grpclog is free from that concern.\n. @crast You are right.  I am fine with grpclog.\n. > the caller of request_xxx set the metdata to context \nYes.\nAlso I am thinking of forwarding headers and trailers in the forwarder functions.  What do you think about it?\n. @kazegusuri \n\nI have fixed metadata handling with some limitations; Dropping support for header metadata in streaming RPC because it's a bit difficult, is it acceptable?\n\nYes. Let me examine it by myself later. But it is acceptable for now.\n\nAnd added support to return metadata to client by default.\n1.Header metadata is returned to client by HTTP header with Grpc-metadata- prefix in both normal response and error response, but only on Unary RPC.\n\nOK.\n\n2.Trailer metadata is returned to client by response body with top-level trailer key and json value only on error response, but both Unary RPC and Streaming RPC are supported.\n\nIt looks weird to put it in response body because it was metadata, and the application-level error does not belong to a specific chunk of body.\nI understand that it is hard to implement for streaming RPC, but is it possible to forward trailer metadata as HTTP 1.1 trailers?\n. @achew22 Good point.\ngetResponseHeader looks to be expected to contain trailers, although Safari doesn't implement it.\nhttps://github.com/w3c/web-platform-tests/blob/master/XMLHttpRequest/getresponseheader-chunked-trailer.htm\n@kazegusuri Although the most straightforward mapping of gRPC trailer metadata is HTTP trailers, there are several things to consider.\n1. As @achew22 pointed, it might not be compatible to browser implementations.\n2. We can list up trailer fields before sending response body for unary RPC. But we cannot for streaming RPC. So we cannot send Trailer header in such a case -- it violates a recommendation in RFC 2616.\nhttps://tools.ietf.org/html/rfc2616#section-14.40\n\nAn HTTP/1.1 message SHOULD include a Trailer header field in a\nmessage using chunked transfer-coding with a non-empty trailer\n. @kazegusuri \nAs described above, yes we cannot use HTTP 1.1 trailer for streaming RPC.\nSo I used HTTP 1.1 trialer only for unary RPC, and for streaming RPC grpc trailer is returned as independent chunk like error message. In both case grpc trailer is always returned to client if set regardless of the response is error or not. Does this make sense? @yugui\n\nYes, it makes sense.\nStrictly speaking, we can still send HTTP 1.1 trailers in server streaming RPCs without the corresponding Trailer header if we hijack the HTTP connection. But I don't think it is worth to try because it makes the implementation much complicated and it violates the recommendation in RFC 2616.\nSummarizing, I expect the following spec.  Is this correct?\n1. Header metadata are mapped to HTTP headers which have Grpc-Metadata- prefix. It is supported in all of unary, client-streaming, server-streaming and both-streaming RPCs.\n2. Trailer metadata are mapped to HTTP trailers which have Grpc-Trailer- prefix. But it is supported only in unary RPC and client-streaming RPC.\n   - We can reconsider trailers in server-streaming RPCs when we have a concrete use case.\n. > Not supported for server-streaming and both-streaming RPCs (at least this PR) because header timing issue.\nYou need to call stream.CloseSend. Then you can safely call Header() before CloseAndRecv.\n\nYes. But for server-streaming and both-streaming, Trailer metadata are returned as an extra chunk of message body.\n\nI am not confident if it is the right solution. So it looks better to conservatively avoid supporting trailer metadata in server-streaming and both-streaming.  Is it OK for you?\n. Thank you very much.  Now the design looks good to me.\nLet me add some more coding-level comments.\n. LGTM.\nThank you for your excellent contribution.\n. test failing.\n. LGTM\n. > Will this have any affect on the field names in the swagger documentation?\nI don't think so because swagger generator does not directly use name conversion, and the currently gogen.CamelCase and utilities.PascalFromSnake are exactly same.\n. LGTM\n. lgtm\n. LGTM\n. np. Thank you for your contribution.\n. LGTM.\nThank you for your suggestion.\n. LGTM\n. Let me confirm.  Why is the shorter name better than the current name?\nI am worrying about if it causes name conflicts because request/response types for a service can come from several packages.\n. @t-yuki Thank you.  But I don't think it justifies the change if the change causes name conflicts.\n. Sounds reasonable.  I love to see the PR.\n. Done with #155\n. Almost LGTM.\n- Could you re-generate .pb.go files with the latest protoc-gen-go because the test is failing due to the difference?\n. LGTM\n. LGTM\n. I guess the revision of google.golang.org/grpc you have is inconsistent to the example.\nCould you try running go get -u google.golang.org/grpc to update the revision?\n. Can't reproduce.\nAnother possible guess is that your protoc-gen-go is out-of-date. What happens if you run go get -u github.com/golang/protobuf/protoc-gen-go then try it again?\n. @cuongdo Thank you.\nI have added some comments on logs that can happen in normal flow.  Could you remove the logs because they are too verbose to show in normal flow?\n. LGTM.\nThank you for your contribution.\n. LGTM\n. Maybe fixed by #120 \n. mux is just an implementation of http.Handler.  So you can wrap it with anything you want as you can do in generic http.Handlers.\n. LGTM\n. @willtrking, thank you for your job.\nI like the overall direction. I have several comments.\n- I was thinking of implementing this feature as a generic abstraction of marshaling mechanism.\n  It doesn't need to be extensible for other MIME types for now. But what do you think about naming JSONAdapter -> Marshaler or Codec?\n- Unfortunately there is an edge case. Even when you use jsonpb, you still need to marshal/unmarshal primitive types or JSON arrays when google.api.HttpRule.body is not *. You would need to combine it with encoding/json in this case.\n. > Name change makes a lot of sense to me, I think I'll probably go with Marshaler.\n\nGood to hear.\nHow are you thinking of doing the MIME type abstraction? Based on the Content-Type header from the client? If so it seems like that would be pretty easy for me to take care of.\n\nIt'd be better if you kindly do it. But we can do it later.\n\nI was considering some sort of system to determine if the protocol buffers being used required jsonpb as well, but this would of course take longer. It would also hide potential performance impacts of using jsonpb.\n\nYeah, you are right.  So I prefer to leaving it with the users.  They can use encoding/json version if they need more performance and their protobuf is simple enough.  And we use jsonpb unless they explicitly configure.\n\nIn what cases can the google.api.HttpRule.body not be a protocol buffers? Looking through the examples it looked liked the body sections of the options still deserialized into a protocol buffer in the generated code, although I'm sure I'm missing something given how new I am to this project. Any particular source files I should look through for this?\n\nAt examples/examplepb/a_bit_of_everything.proto#L97, the request body points a primitive leaf field gengo.grpc.gateway.examples.sub.StringMessage.value, whose type is string.\nSo the marshaler must be able to unmarshal a JSON string in the generated code a_bit_of_everything.pb.gw.go#L354.\n. Merged in #144.\nThank you for your contribution.\n. Yeah.\nCurrently grpc-gateway does not support push notification very well.  I don't have a plan to support it because it looks to be too much for typical use cases.\n. Thank you for the pointers.\nThe purpose of pRPC and the one of grpc-gateway are different. They are trying to transport something similar to gRPC over HTTP/1.1. In particular, it allows users to run their gRPC application in classic Go runtime of AppEngine. Also they allow users to reuse existing service implementation in process, but the implementation must be in Go.\ngrpc-gateway intend to provide traditional JSON-based RESTful APIs on top of other gRPC services. It cannot run in the same process as the services, but it does not require the services to be in Go. Also its handers are compatible to net/http.  So you can use the generated proxies as building blocks in your application server.\n. It is designed to be compatible to standard golang interfaces.  So you can combine any middleware for standard libraries.\nIn particular, Request multiplexer implements the standard HTTP handler interface.\nSo you can easily combine it with such middleware in Gorilla. https://godoc.org/github.com/gorilla/handlers\n. @achew22, @ivucica \nThank you for your contributions. I've fixed the build issue in ivucica/grpc-gateway#1. Could you take a look?\n. @achew22 \nI made a PR #136.  It contains only Travis configuration change.\nYou still need to update .pb.go with protoc-gen-go after golang/protobuf@001690d39bd620847bb265d93a7c5e1bd3737308.\nDo you want me to create another PR which contains the .pb.go changes?\n. Thank you for your contributions.\n. LGTM.\nI can merge this PR as soon as I or you resolve merge conflicts.\n. It might be a bug because I didn't consider import public when I implemented the code generator.\nBut I am not sure if I understand the problem you saw for the same reason.\nCould you give me a more concrete example, what you expected and what you got?\n. I guess a scenario something like the followings.\n- You had a.proto, b.proto and c.proto in a package.  a.proto contained something like\n  protobuf\n  message A{ }\n- You also had another file x.proto in another package, which contained\n  protobuf\n  import public \"path/to/a.proto\";\n- You expected that you get a gateway imports imports the package of message A because protoc-gen-go does so.  But actually the gateway generator didn't.\nBut as far as I understand, import public does not transitively forward services unlike messages.\nSo I don't think it caused compilation error or something even if the generated .gw.go lacks a reference to the other package.  What exactly problem did you have?\n. You won't see message definition unless it is actually used in services.\nYou never see message definition at all in .gw.go because it is intended to be compiled together with .pb.go.\n. LGTM.\n. Thank you, @willtrking, @tmc .\nIt looks good to me as an initial implementation.  I'll add some small fixes later.\n. I wonder if we can use custom options to annotate patches.\nWhat do you think?\n. @tmc, @ivucica \nIn my personal opinion, it is not too bad even if we only have a single string custom option which stores schema patch in JSON because at least we could avoid parsing comments.\nI am worrying about parsing comments because it is not very common in protobuf convention and it has less power than custom options.\nWith custom option, we could deprecate the simple string option and introduce a well-organized messages to describe schema in future.\n. FYI: @tmc, @willtrking \n. It is unlikely that I will merge this PR because\n- I have a similar cleanup #151.\n- I don't think this is a \"proper\" handling of Accept header since it ignores media range and quality factor of the header.  https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.1\n- Accept header is not the right way to identify MIME type of request body.\nIt would be even better if we can property handle Accept header, however, I don't think it is mandatory.\n- c.f. http://programmers.stackexchange.com/a/170409\nIf we handle the header, it would require:\n- interpret media range and quality factor\n- split Marshaler into Marshaler and Unmarshaler.\n  - Independent registry for each\n. @tamird \nThank you.  I confused something in the original commits.\nI'll add some comments.\n. LGTM. Thank you for your contribution.\n. Thank you.  Merged with some modification.\n. I never add the feature unless it is \"standardized\" in googleapis repo or I realize that the use case is common enough.\nFor now, I'm not sure if the use case is really common.\nIn my understanding, such custom dispatch tends to be implemented as a custom VERB.\nhttps://github.com/googleapis/googleapis/blob/master/google/api/http.proto#L210\nAlso if we really need this kind of \"pattern matching\", we would also need to discuss what kind of inputs should be involved into the pattern match. Why is it applied to only headers?  Why not match with query parameters?\nFor uncommon use cases, you can combine any custom Go http handler with top of the one grpc-gateway generates.\ne.g.\ngo\nfunc TagVerbFilter(h http.Handler) http.Handler {\n  return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request){\n    if r.Header.Get(\"foo\") == \"bar\" && r.Header.Get(\"other\") == \"val\" {\n      r2 := new(http.Request)\n      *r2 = *r\n      u, err := url.Parse(r.URL.String() + \":myVerb\"\n      if err != nil {\n        http.Error(w, err.Error(), http.StatusInternalServerError)\n        return\n      }\n      r2.URL = u\n      r = r2\n    }\n    h.ServeHTTP(w, r)\n  })\n}\n. LGTM\n. I don't have much idea.\nBut the number of the well-known types is limited. So we could extend the mapping table to the swagger types.\n. LGTM\n. I'm happy to accept streaming support with websockets, but I expect two things in the support.\n1. Concrete use cases\n   - The main reason why I have not yet implemented it was that I didn't have a concrete use case.\n2. Good support in client-code generation.\n   - At least, we need to be able to expect that we can have a good support in future.\n. @tmc Wow. That's excellent.\nIt does not force users to depend on WebSockets but brings even more functionality.\n. > a streaming handler did not start a goroutine and ran all Send()s before returning and having the caller call Recv().\nI don't think it was changed with this PR because <-sendErrs waits for an error from the goroutine or the termination of the goroutine. Also on error from <-sendErrs, the caller returns wihtout calling Recv.  So Send and Recv never interleave, IIUC.\nSo in my understanding, the change made by this PR was\n- Read req.Body until its end even if there was an error\n- Send streams to the upstream grpc server even if there was an error\nI think your intention\n\nto block until the message is sent by the client as a sort of middle ground between just ignoring errors and interrupting the stream on any error.\n\nmakes sense. But if my understanding above was correct, you didn't need the new goroutine. You could do the same thing in the same goroutine as caller.\n. > Did you miss that the first Send if successful will send nil and stop blocking?\nYes, I did. Sorry.\nThen, why do you treat the first item in the stream specially?\nIt returns 400 Bad Request if it fails to decode the first item but it keeps going when it fails to decode later items.\nAlso I'm not sure if it is safe to start writing response body before reading the request body until the end.\nhttps://www.w3.org/Protocols/rfc2616/rfc2616-sec6.html#sec6\n\nAfter receiving and interpreting a request message, a server responds with an HTTP response message.\n. @tmc Thank you. That's much clearer.\nI still have small coding-level comments and I am worrying about compatibility to the current HTTP/1.1 version of response forwarder.\n\nhttps://www.w3.org/Protocols/rfc2616/rfc2616-sec6.html#sec6\n\nAfter receiving and interpreting a request message, a server responds with an HTTP response message.\n\nIn my understanding, it requires us to receive the request before sending any response in the TCP session.  So #168 looks to be a requirement of this PR.  What do you think?\n. @tmc.  Thank you.  LGTM.\n. Thank you.\nI merged it, but I did not choose the metadata name \"RemoteAddr\" because there was a similar PR #155. \n. You are right.  We need to fix that part.\n. @mattn it is intended to be the same CLA as GRPC itself. I think it should be your state/location.\nhttps://github.com/grpc/grpc-go/blob/master/CONTRIBUTING.md\n. @achew22 Good question.\n\nWill work on these things be done in a public way? On github?\nWhat does governance look like?\nWhat license will they be releasing future code under?\nWhat will this organization do with patentable materials?\nIs there a code of conduct for this organization? What behavior is considered acceptable or unacceptable?\n\nIt keeps to be public on github, and there's no license change.\nFor other questions, @zinuga has better answers.\n. To summarize,\n\nWill work on these things be done in a public way? On github?\n\nIt will be done in public in the github organization, grpc-ecosystem.\n\nWhat does governance look like?\n- Contributors are expected to follow the contributor guideline of gRPC, https://github.com/grpc/grpc-contrib/blob/master/CONTRIBUTING.md.\n- Decision will be made in github issues.\nWhat license will they be releasing future code under?\n- No license change when we distribute grpc-gateway in github.\n- There's a section about license in CLA\nWhat will this organization do with patentable materials?\n- patent will be covered by CLA\nIs there a code of conduct for this organization? What behavior is considered acceptable or unacceptable?\n- No explicit code of conduct to apply.\n\n@zinuga Could you correct me if something is wrong?\n@achew22 Is everything clear for you?\n. Done.\nThank you for your contributions and cooperation!\n. LGTM, but could you update the generated files as well?\n. You can use a custom instance of JSONPb marshaler and pass it as an option when you instantiate Mux.\nm := &runtime.JSONPb{\n  OrigName: true,\n  EnumsAsInts: true,\n}\nmux := runtime.NewServeMux(runtime.WithMarshalerOption(\"application/json\", m))\n...\n. I am trying to keep grpc-gateway as consistent to Google's internal similar solution as possible.\nSo I am a bit worrying about if it breaks compatibility to the spec described in google/api/http.proto.\n. @shurcooL Sorry for being late.\nI thought about that and I concluded that nothing technically prevents us from implementing that.\nOn the other hand, personally I still don't have much enthusiasm for this idea because I am not sure how common it is to combine gogoproto with grpc.  This is also because the only benefit from this change is to make the representation in querystring clearer.\nHowever, I could change my mind if I had more concrete use cases.\n. Yes. I'll close this issue.\n. LGTM\n. Sounds good.\nAnother thing I want to consider is a compatibility to gopkg.in.  So I'll tag the very initial release as \"v1.0.0\".\n. cc: @tmc \nI think it is intentional, so that HTTP response headers are available.\n\nI cannot seem to get same code generated locally as in Travis CI...\n\nIt uses a relatively new functionality of protoc-gen-go.  Hows does it look like if you upgrade protoc-gen-go?\n. @shaxbee Sure.\nThank you for your interest in improving grpc-gateway.\n. @tmc Sounds reasonable.  I'll merge this PR.\n. Question to reviewers.\nWhat is the recommended protobuf package name for grpc-gateway?\nIt used to be gengo.grpc.gateway and I tentatively replaced it with grpc.gateway.  Is there any other recommended option?\n- https://github.com/grpc-ecosystem/grpc-gateway/pull/192/commits/2a6dbec2493b8e6ab504481cf0fdb0bd43afa9b2#diff-c810b38ee43b494b6b43d05835936c8cR3\n- https://github.com/grpc-ecosystem/grpc-gateway/pull/192/commits/2a6dbec2493b8e6ab504481cf0fdb0bd43afa9b2#diff-15333c7f3c03bc3e7993d7e1afe5dea4R2\n. Thank you for your reviews. Merged.\n. Thank you for your contribution.\nBut I'm closing this PR because it was covered as a part of #192.\n. Fixed in #191, but we'll discuss how we should handle zero-length stream.\n. @gyuho Right. The second suggestion is what I suggested in #191.\nI'll take a look at this issue by myself on Monday.\n. > why we were sending zero-length input in the first place\nI don't think we are sending zero-length input. #200 just fixes an edge-case bug which can happen when the stream is empty.\nAlso the very first invocation of handleSend is different from the later ones just because let the caller know errors earlier.\n\nThe print statement returns {\"create_request\": {\"key\": \"Zm9v\"}} , so the request body was not empty, but dec.Decode(&protoReq) returns io.EOF, which doesn't make sense. Is this expected? Or some kind of grpc-specific behavior?\n\nI can't reproduce. Could you give me a small reproducible case?\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"strings\"\n\"github.com/grpc-ecosystem/grpc-gateway/runtime\"\n\n)\nfunc main() {\n    var m runtime.JSONPb\n    dec := m.NewDecoder(strings.NewReader({\"create_request\": {\"key\": \"Zm9v\"}}))\nmsg := make(map[string]interface{})\nif err := dec.Decode(&msg); err != nil {\n    fmt.Println(msg, err)\n    return\n}\nfmt.Println(msg)\n\n}\n``\n. done.\n. grpc-gateway has moved togithub.com/grpc-ecosystem/grpc-ecosystem.\nCould you regenerate your gatewaymy-grpc-gatewaywith the latest revision ofgithub.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway?\n. @tmc Would you mind taking a look at this PR?\n. @tmc Does it mean we can expect that compiled version ofannotations.proto` will be available in google/go-genproto?\n. https://github.com/grpc-ecosystem/grpc-gateway/tree/cleanup/use-go-genproto\nI tried to move them, but it turned out that there's an compatibility issue.\nBoth google.golang.org/genproto/googleapis/api/serviceconfig and github.com/golang/protobuf/protoc-gen-go/plugin have references to their own version of descriptor.proto.  They cannot coexist because the two instances of descriptor.pb.go collide with each other.\n```\npanic: proto: duplicate enum registered: google.protobuf.FieldDescriptorProto_Type\ngoroutine 1 [running]:\npanic(0x1c89c0, 0xc42000c7e0)\n        /usr/local/Cellar/go/1.7/libexec/src/runtime/panic.go:500 +0x1a1\ngithub.com/golang/protobuf/proto.RegisterEnum(0x228685, 0x29, 0xc4200106f0, 0xc420010720)\n        /Users/yugui/gengo/go/src/github.com/golang/protobuf/proto/properties.go:815 +0x13a\ngoogle.golang.org/genproto/protobuf.init.3()\n        /Users/yugui/gengo/go/src/google.golang.org/genproto/protobuf/descriptor.pb.go:1888 +0x4d0\ngoogle.golang.org/genproto/protobuf.init()\n        /Users/yugui/gengo/go/src/google.golang.org/genproto/protobuf/type.pb.go:394 +0xa67\ngoogle.golang.org/genproto/googleapis/api/serviceconfig.init()\n        /Users/yugui/gengo/go/src/google.golang.org/genproto/googleapis/api/serviceconfig/usage.pb.go:189 +0x58\ngithub.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway/descriptor.init()\n        /Users/yugui/gengo/go/src/github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway/descriptor/types.go:323 +0x76\nmain.init()\n        /Users/yugui/gengo/go/src/github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway/main.go:119 +0x71\n``\n. https://github.com/google/go-genproto/issues/9\n. @igateno Something is wrong because there are no active reference to \"gengo.grpc.gateway\" in the source of grpc-gateway.\n. @sriniven \nThank you for fixing the issue.\n- Could you also fixjsonpb.UnmarshalNextindecodeNonProtoField`?\n- github is failing to check if you have signed a CLA.\n  I have asked grpc team about this issue. But could you sign the CLA if you have not yet?\n. LGTM.\n@sriniven Thank you for your contribution.\n@zinuga Thank you for your help.\n. github is failing to check if Eran has signed a CLA.\nI have asked grpc team about this issue.\n@EranAvidor please sign the CLA if you have not yet\n. Thank you for your feedback. We should definitely have had such documentation.\n. @achew22, @christianvozar \nI've started to realize that there are use cases which require HTTP headers forwarded, unlike our past assumption.\nSo I like the overall direction of this PR.\nBut I wonder if we can have better handling of several HTTP headers in the whitelist.\n- Content-Type, User-Agent, TE: They are a part of grpc wire protocol.. So they are not allowed as custom metadata names.\n- Connection: It does not make much sense to forward this field and it is prohibited by RFC 2616 14.10.\n- Upgrade, Accept, Accept-Encoding, Content-Length: It is possible to forward them but it would cause an issue if the upstream actually changes the behavior based on these headers. So they should be useless for the upstream service implementation.\n- Range, If-Range: It is possible to forward them but it does not make sense because they are  Content-Type-dependent.\nHow about the following approach?\n1. Limit the whitelist into\ngo\n       \"Accept-Charset\",\n       \"Accept-Language\",\n       \"Accept-Ranges\",\n       \"Authorization\",\n       \"Cache-Control\",\n       \"Cookie\",\n       \"Date\",\n       \"Expect\",\n       \"From\",\n       \"If-Match\",\n       \"If-Modified-Since\",\n       \"If-None-Match\",\n       \"If-Schedule-Tag-Match\",\n       \"If-Unmodified-Since\",\n       \"Max-Forwards\",\n       \"Origin\",\n       \"Pragma\",\n       \"Referer\",\n       \"Via\",\n       \"Warning\":\n2. Allow users to add more items into the whitelist with options\n3. Separately discuss why/how to forward User-Agent.\n4. Optionally support more permanent headers and their forwarding.\n   - RFC 7239\n   -  Decrement the count in Max-Forwards, which is required by RFC 2616 14.31\n   - ...\n. > What do you think about proxying User-Agent as Grpc-Gateway-User-Agent? \nSounds good to me.  What do you think, @christianvozar?\n. @christianvozar Right. It does not mean that API users need to specify Grpc-Gateway-User-Agent.  It just means that your backend gRPC service will receive Grpc-Gateway-User-Agent.\nBTW, now I remember that the prefix Grpc- is reserved by grpc wire protocol. So can we use GrpcGateway- as an alternate prefix?\nFor versioning, the recommended way of implementation in gRPC is to define a different gRPC services or methods.  And you can dispatch those versions in your custom http handler in your gateway.\nIn general, forwarding TE  or Content-Type looks to be a layer violation. So I am against forwarding TE or Content-Type unless we have another concrete use case.\n. Test is failing due to an issue in the devel version of go compiler.\neway/runtime\n\n```\ngoogle.golang.org/grpc/transport\n../../../google.golang.org/grpc/transport/http_util.go:248: internal compiler error: schedule does not include all values\n```\n. The test is failing due to an issue of the devel version of go compiler\n```\ngoogle.golang.org/grpc/transport\n../../../google.golang.org/grpc/transport/http_util.go:248: internal compiler error: schedule does not include all values\n```\n. You can write a custom marshaler to do that.\nAlso, what grpc-gateway generates is an ordinary HTTP handler in Golang.  So you can do whatever you want by wrapping the handler.\n. That's an interesting use case. Probably we can add it as a custom option if it is really necessary.\n\nCould you explain more about the reason why / the context in which you need to access to both of the HTTP headers and RPC info at the same time?\n. @sdemos Thank you for your help.\nThe automatic process is failing to check if you have signed Google CLA for a technical reason.  Have you signed?\n. @sdemos Thanks. LGTM.\n. Thank you for your question.\nThe short answer is \"No, you can do that. We don't have any plant to support that\".\nFor longer answer, I have added a FAQ entry which describes a workaround. https://github.com/grpc-ecosystem/grpc-gateway/wiki/FAQ\n. Thank you for your report.\nBut I don't understand what are you trying to say.\nI guess probably the facts you have reported were different from your expectation, but I don't know what you expected and why you expected so. Could you explain what exactly you expected and its reason?\nAlso if you have not yet, please read the FAQ at first.\n. > Colons in the URL path are part of the standard spec:\nRight. But it does not always mean that our spec supports the URL.\nThe problem is something similar to shift/reduce conflict between a VARIABLE value and VERB in the spec at runtime. /v0/users/user:123 actually looks like a sequence path components [v0, users, user] and a verb 123.\nSo I doubt if we should parse user:123 as a variable value based on the fact that there's no other template /v0/users/*:123 defined because it would mean that the interpretation of a URL pattern depends on other patterns if we parsed so.\nAs a workaround, you can still escape colon with percent-encoding or write a custom wrapper.\n. Can't reproduce. This is what I got from the steps you had shown.\n``` go\nfunc request_TestService_PrintAuth_0(ctx context.Context, marshaler runtime.Marshaler, client TestServiceClient, req *http.Request, pathParams map[string]string) (proto.Message, runtime.ServerMetadata, error) {\n        var protoReq empty.Empty\n        var metadata runtime.ServerMetadata\n    msg, err := client.PrintAuth(ctx, &protoReq, grpc.Header(&metadata.HeaderMD), grpc.Trailer(&metadata.TrailerMD))\n    return msg, metadata, err\n\n}\n``\n. @PranavSathy I don't think so. But it might be worth to reinstall the latest stable version of protoc just in case.\n. LGTM. Thank you for your contribution.\n. @EricLagergren ping \ud83d\ude09 \n. LGTM. Thank you for your contribution!\n. Sorry for my late reply.\nAlthough it breaks compatibility, I still prefer the option (2) because it is more commonly used in REST APIs and we still keep a way to disableEmitDefaults.\nI'm happy to review such a PR.\n. IIUC, the spec byhttp.proto:197-202 are trying to define a canonical way to specify values for each parameter. Sohttp.proto` prohibits this feature by design.\nCould you give me some real use cases of this feature? If we have enough number of reasonable use cases, probably we can relax the restriction with a certain explicit configuration, like a command line option of protoc-gen-grpc-gateway or our own custom options in .proto.\n@tmc Thank you for your PR. But I cannot agree on it at this moment.  Let's discuss the expected spec here.\n. @atombender \nSorry for my unclear comment. Your original example looked to be quite artificial.\nThank you for the second example.  Based on the example, right, headers are out-of-band. But it looks to be natural in the second example because verbose flag is basically out-of-band metadata rather than the content of the data.\nI'm not strongly against this feature.  But I need to understand what we actually need better with good examples because we are trying to go out of the combat-proven design by http.proto.\nAt this moment, I don't have clear examples which support this feature.\n. LGTM.\nBut cla/google bot failed to check if you have signed a [CLA].(https://cla.developers.google.com/clas). \n@philipithomas Would you let me know if you have signed? And if you haven't, could you sign?\n. Thank you for your contribution!\n. You can simply import _ \"net/http/pprof\" if you reuse http.DefaultServeMux.\nexamples/main.go shows a way to combine http.ServeMux with github.com/grpc-ecosystem/grpc-gateway/runtime.ServeMux.  However, I should have reuse http.DefaultServeMux in this example instead of calling http.NewServeMux().  If you reuse, import _ \"net/http/pprof\" will take effect.\n. LGTM. Thank you for your contribution.\nI'll fix the test failure with tip version of Go later.\n. LGTM. thx\n. @christianvozar Can I merge your contribution in this PR into this repository?. LGTM. LGTM. @achew22 I don't think we can make the bot advisory.  But I have granted admin rights of this project to you and @tmc.  So now you can ignore the status check at your own risk.. Sure.\nIt was a kind of intentional limitation. But I'm open to supporting arrays of messages under certain conditions.\nTo be short, it is not supported because it is prohibited by the spec of google.api.HttpRule (https://cloud.google.com/service-management/reference/rpc/google.api#google.api.HttpRule.FIELDS-table). But nowadays grpc-gateway supports some extension to the spec. So it is not a bad idea to support arrays of messages if we have a reasonable concrete use case. \nAlso, it might be a good opportunity to clarify what is specific to grpc-gateway and what is common to other implementations like Cloud Endpoints.. Thank you for your contribution. That's a very good point.\nIt's definitely the time to simplify the error handling flow.\nThis PR itself looks good to me. But let me\n summarize the current status of the handlers,\n confirm the intention of this PR, and\n* ask some questions to simplify the mechanism even more in a later PR, @vaporz and @kazegusuri.\nI'd really appreciate it if you give me some feedback.\nThe current status\n\nvar HTTPError func(context.Context, *ServeMux, Marshaler, http.ResponseWriter, *http.Request, error)\nResponsible for reporting runtime errors coming from the remote gRPC service, gRPC framework or request/response transformation in the gateway.\nNeed to return an HTTP status code corresponding to the given gRPC error.\nIt should respect custom marshalers because the error can happen at runtime.\n\nIt depend on a context.Context\n\n\nfunc DefaultHTTPError\n  Default implementation of HTTPError\n\n\nvar OtherErrorHandler func(http.ResponseWriter, r *http.Request, msg string, code int)\n\nResponsible for reporting errors caused by wrong implementation of client stub.\nCannot assume that the given status code has a corresponding gRPC error code.\nUsually the error won't happen at runtime as far as the client implementation is consistent to the swagger spec of the service.\n\nIt does not depend on context.Context for historical reason and compatibility.\n\nIt might be the time to rethink, taking several factors into account:\nNow there's a context everywhere with request.Context.\nhttps://github.com/grpc-ecosystem/grpc-gateway/pull/378#issue-226524288\nThis also unifies errors returned by gRPC server and errors happened inside grpc-gateway, which helps error handling in client.\n\n\n\n\n\n\n\nfunc DefaultOtherErrorHandler\n  Default implementation of OtherErrorHandler. Just calls http.Error.\n\n\nIt does not depend on the marshaler because simpler is better to reduce the number of assumptions we rely on in such an exceptional case.\n\n\nvar ServeMux.protoErrorHandler func(context.Context, *ServeMux, Marshaler, http.ResponseWriter, *http.Request, error)\n\nAn customizable ServeMux option which overrides HTTPError and OtherErrorHandler.\nIt globally updates HTTPError (and OtherErrorHandler). So multiple instances of ServeMux with different protoErrorHandler cannot safely coexist.\n\nRecognizes the new error reporting mechanism with status.Status in gRPC.\n\n\nfunc DefaultHTTPProtoErrorHandler\n\nAn implementation of protoErrorHandler. Almost a modified copy of DefaultHTTPError.\n\nThis PR\nAnd this PR is trying to dedupe the implementation by adding two wrapper functions.\n* func (s *ServeMux) executeErrorHandler(ctx context.Context, w http.ResponseWriter, r *http.Request, protoErrorCode codes.Code, protoErrorMsg string, otherStatusCode int, otherErrorMsg string)\n  * a wrapper of the overriding mechanism of OtherErrorHandler by ServeMux.protoErrorHandler.\n  * uses proto version of error representations if possible, falls back to the plain HTTP version of error representations if otherwise.\n\nfunc executeProtoErrorHandler(ctx context.Context, s *ServeMux, w http.ResponseWriter, r *http.Request, code codes.Code, msg string)\na wrapper of HTTPError which determines an appropriate marshaler for it.\n\nQuestions\n\n@kazegusuri, protoErrorHandler returns 501 Not Implemented instead of 405 Method Not Allowed when the given request method is not allowed (defined) on the resource. Is it really good.\n@kazegusuri, Was it the reason why you kept DefaultHTTPProtoErrorHandler separated from DefaulHTTPError that DefaultHTTPProtoErrorHandler returns a different format of response body from DefaultHTTPError?\n@vaporz, The responsibility of executeErrorHandler would be unclear if we only take a look at its signature. Is there any way to simplify it even more? It is totally fine to check it in as is, though.. I noticed that the alloc function does not work when the target message is defined outside of the current package. I'll rewrite this PR.. I am against these proposed commits themselves. But I understand the problem behind the proposal. So I'd like to propose a variant of the solution.\n\nFirst of all, I basically agree with @tmc. I don't want to expose the functions as they are because they are very implementation details of the runtime and they are not designed from the perspective of the right granularity of APIs.\nOn the other hand, it have to say that it has been getting hard for users to write a right implementation of custom HTTPError handler because the header processing is getting better but complex.\nActually it was basically just\n```go\nw.Header().Set(\"Content-Type\", \"application/json\")\n...\nst := HTTPStatusFromCode(grpc.Code(err))\nw.WriteHeader(st)\nw.Write(buf)\n```\nin an earlier version. So it was easier to reproduce.\nBut now it requires the helper functions.\nSo I propose \n to extract a larger portion of response forwarding from DefaultHTPError\n to expose the extracted function.\nWhat do you think, @jinleileiking, @tmc?\nThe function would be, roughly\n```go\ntype ErrorResponse struct {\n    Code int\n    ContentType string\n    MD ServerMetadata\n    Body []byte\n}\nfunc WriteErrorResponse(ctx context.Context, w http.ResponseWriter, mux *ServeMux, resp ErrorResponse) error {\n    w.Header().Del(\"Trailer\")\n    w.Header().Set(\"Content-Type\", resp.ContentType)\n    handleForwardResponseServerMetadata(w, mux, resp.MD)\n    handleForwardResponseTrailerHeader(w, resp.MD)\n    w.WriteHeader(resp.Code)\n    if _, err := w.Write(resp.Body); err != nil {\n        return err\n    }\nhandleForwardResponseTrailer(w, md)\nreturn nil\n\n}\n``\n. IIUC, if we had something likeWriteErrorResponseyou would be able to write your error handler by calling it instead of its implementation details (handleForwardResponseServerMetadata` and others).\nIs this understanding correct?. I guess @DanForbes is talking about something like https://github.com/golang/protobuf/issues/52.\nhttps://github.com/favadi/protoc-go-inject-tag in particular.\nAnd as @tamalsaha said, it does not need any special support in this project.. Thank you for your contribution, hacst. This feature is what I really wanted to have.\n\nno more breaking changes\n\nSorry, the breaking changes on examples are my fault.. Sorry for the confusing error message. But I don't think /posts/slug={slug} is a valid syntax because a segment can be either a wildcard, a LITERAL or a VARIABLE.\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/master/third_party/googleapis/google/api/http.proto#L214. > I think to create new function that returns a ServeMuxOption.\n\ne.x runtime.WithMarshalerRegexOption\nIs my understanding correct ?\n\nI don't think so.  Maybe he is saying that we need to have another method than marshalerRegistry.add.\nYou can determine if the given mime type has a partial wildcard or not, then call the right method of marshalerRegistry.. It is expected that /usr/local/include contains google/protobuf/descriptor.proto in the documentation.\nI'm sorry but the documentation does not cover all the possible cases for simplicity.\nIn your case, could you replace /usr/local/include with the corresponding include path in your installation of protobuf?\nFor example, if you have the protoc at C:\\Users\\mossila\\AppData\\Local\\protoc-3.5.1-win32\\bin\\protoc, could you try replacing the path with C:\\Users\\mossila\\AppData\\Local\\protoc-3.5.1-win32\\include?. Congratulations!. Streaming requests and responses are mapped to JSON streaming by design.\nhttps://en.m.wikipedia.org/wiki/JSON_streaming. Could you explain a bit more about the background of the request?\n\nYou wouldn't have such headers if it were a plain HTTP server but not an interceptor in grpc-gateway. \n For what concrete use case do you need the full URI as a header?\nWhy is the HOST header insufficient for the use case?\nIs the use case common enough to implement in the grpc-gateway runtime?  Or is it better to implement your custom HTTP middleware?. It still makes sense to build with different versions of go in Bazel. But either is OK for me.\n\nLGTM except for the redundant 3 builds.. Build fixed. Could you take another look?. I thinks go15vendor feature can be a workaround for the issue, by allowing you to have a local copy of this repository.\nBut at the same time, in my personal opinion, I always try to avoid such local modification of third-party libraries because local modification makes my application hard to maintain. Instead, I try to generalize my specific requirement to something common and develop a generic solution. For example, I often send a PR which makes something pluggable and extensible.. Unfortunately CI is failing due to https://github.com/golang/lint/issues/397.. https://travis-ci.org/grpc-ecosystem/grpc-gateway/jobs/370802179 (Go 1.10.x; GATEWAY_PLUGIN_FLAGS=) is failing due to the issue solved by #619.\nhttps://travis-ci.org/grpc-ecosystem/grpc-gateway/jobs/370802182 (Go master; GATEWAY_PLUGIN_FLAGS=request_context=false) seems to be flaky. And now failing because of incompatibility to #618.\nWould you mind rebasing the branch again?. Confirmed in https://travis-ci.org/grpc-ecosystem/grpc-gateway/builds/370873461. The branch is safe to merge.\nI can merge this PR if it looks good to @achew22.. LGTM. Thank you for your contribution.. Still failing when the grpc server is not ready. Need to check health of it too.. np. I'm looking forward to seeing it.\nThen, I'll work on enhancement of the support of oneof fields this weekend.\nAnother thing I though about was a similar integrity check on //examples/proto/.... But it looks to be related to your plan.  So I'll explore other areas.\nHave a good weekend.. The test failure looks to be because some of example files are out-of-date. c.f. https://github.com/golang/protobuf/commit/05f48f4eaf0e05663b562bab533cdd472238ce29#diff-8c603013608023320d5242916c4ea03bR1571\nWould you mind running make clean && make examples to let the files catch the change in protoc-gen-go?. @DaHuMao \nI did not expect users would need to modify template.go when I wrote it. If you do, obviously you need to rebuild protoc-gen-grpc-gateway itself, reinstall it and then you need to regenerate your codes.\nBut before trying directly modifying template.go, please take a look at https://grpc-ecosystem.github.io/grpc-gateway/docs/customizingyourgateway.html. It might give a better solution.\nAlso please note that you don't need to modify the generated code if you just need to add some methods and/or variables into the package. Golang allows you to simply add another hand-written .go file into the same package.. race condition?\n. data race?\n. use grpc.Errorf(codes.NotFound, \"not found\").\n. ditto\n. ditto.\n. I agree with @slimsag. google/api/http.proto looks to be more appropriate as a canonical path than github.com/gengo/grpc-gateway/third_party/googleapis/google/api/http.proto.\nBTW, I should have added -I$GOPATH/src/github.com/gengo/grpc-gateway/third_party/googleapis in the example in README.md.\n. Done. Corrected.\n. Done.\n. Done.\n. done. thx\n. Thank you for your contribution. \nWould you mind fixing some style issues?\nOrganize imports in groups.\nhttps://code.google.com/p/go-wiki/wiki/CodeReviewComments#Imports\n. http://golang.org/doc/effective_go.html#commentary\nThe comment should be a complete sentence and should begin with \"AnnotateContext adds ...\".\n. Standard library packages should be the first group of imports.\nhttps://code.google.com/p/go-wiki/wiki/CodeReviewComments#Imports\nAlso \"golang.org/x/net/context\" is not a standard package (yet).\n. Follow the standard convention of error message format.\nhttps://code.google.com/p/go-wiki/wiki/CodeReviewComments#Useful_Test_Failures\n. Split the check into three checks.\n1. !ok and length check\n2. md[\"Foobar\"]\n3. md[\"Foo-Baz\"]\n. Good catch. Thanks.\n. Done.\n. @shurcooL It is intended.\nhttps://github.com/google/protobuf/blob/master/src/google/protobuf/compiler/plugin.proto#L82\n. done\n. certainly. fixed.\n. Golint error.  Should be HTTPCode.\n. ditto.\n. opts == nil is redundant.\n. Define a type type ForwardResponseOption func(context.Context, http.ResponseWriter, proto.Message)\n. Could you just remove this if you don't need \"go/format\"?\n. I guess you don't need to manage go packages.\nMaybe you have much code you can remove.\n. Originally I thought about adding a flag to protoc-gen-grpc-gateway to let it generate swagger schema.\nWhat do you think about that?  Do you think it is better to make it an independent protoc plugin?  Personally I think the flag approach would be easier for users to keep gateway and schema consistent.\n. Strictly speaking, the template format of google.api.HttpRule and the one of Swagger are different.   But your approach is good enough as a first step.\nIIUC, /path/{param=foo/*}/ in google.api.HttpRule needs to be /path/foo/{param} in Swagger.\n. Could you add the following copyright notice as a comment just after this line?\n// Copyright 2010 The Go Authors.  All rights reserved.\n// https://github.com/golang/protobuf\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n. I have no good example.  But it is reasonable for me to put them together because the two generators must be consistent.\nFor me, they look to be similar to the relationship between protoc-gen-go and gprc-plugin rather than plugins for different languages.\n. These call options are effective only if .Method.GetServerStreaming and .Method.GetClientStreaming are false.\nThe caller of this request_xxx function must use grpc.ClientStream.Header() and grpc.ClientStream.Trailer if otherwise.\nIIUC, it would get easier to handle this case if you have let this request_xxx function allocate the metadata.MDs and let the function return the MDs in addition to response and errors.\nHow about renaming runtime.CallInfo to runtime.ServerMetadata and declaring the ! .Method.GetServerStreaming() version of this function as something like this?\nfunc request_{{.Method.Service.GetName}}_{{.Method.GetName}}_{{.Index}}(ctx context.Context, client {{.Method.Service.GetName}}Client, req *http.Request, pathParams map[string]string) (proto.Message, runtime.ServerMetadata, error)\nWith this approach forward_xxx function does not need to care about .GetClientStreaming.\n. Use bytes.Buffer.String() to avoid unnecessary copy.\n. pkgSeen is not used.  So I don't think you need this wrapper method.\n. remove this line.\n. It is better to start the sentence with \"findEnumerations\" to follow the standard godoc convention, although this comment is not extracted by default.\n. ditto.\n. ditto.\n. Thank you.  It sounds reasonable.  Let's keep the binaries separated.\n. You don't have to allocate the buffer in heap.\ngo\nvar w bytes.Buffer\nenc := json.NewEncoder(&w)\n. You don't have to allocate zero-length underlying buffer.\nvar enumNames []string\n. don't have to allocate zero-length buffer.\ngo\nvar parts []string\n. https://github.com/golang/go/wiki/CodeReviewComments#error-strings\n. https://github.com/golang/go/wiki/CodeReviewComments#dont-panic\nIt is sufficient to return error even in this case.\n. ditto.\n. ditto.\n. hsould --> should ?\n. go\n    if host := req.Header.Get(xForwardedHost); host != \"\" {\n        pairs = append(pairs, strings.ToLower(xForwardedHost), host)\n. IIUC, you no longer need to pass a *ServerMetadata.  You can pass just ServerMetadata.\n. Add godoc\n. I don't think it is a good idea to expose this type just for testing.\nThis struct is just a convenient way for this runtime package to construct JSON object on error. But the struct itself is not a part of API exposed for users or generated codes by this package.\n. IIUC, you no longer need this Trailer.\n. This log is too verbose to always record. Let's remove it.\n. Let's remove it.\n. Let's remove it.\n. Let's remove it.\n. Let's remove it.\n. maybe you mean inbound = m?\n. Could you try falling back to mux.marshalers.mimeMap[contentTypeHeader] before MIMEWildcard\nSo this line can be just outbound = inbound.\n. unnecessary change.\n. I prefer to keeping lookup method to encapsulate internal structure.\n. With the new API with WithMarshalerOption, you don't need to export this function because the only caller of this function is NewServeMux.\nAlso the function should be named newMarshalerRegistry by convention.\n. remove this line.\n. Could you add scenario for Accept header?\n. - it blocks forever unless you close sendErrs?\n- it still blocks until the end of the goroutine even if you close sendErrs.  So it is not interleaving?\n. I overlooked L260-L263.  So actually it does not block forever.\nI have two more questions.\n- Maybe I'm missing something, but it does not seem to be something like \"interleave\" for me.  What do you mean by \"interleaving Send and Recv\"?\n- If it is intended to block until the message is sent, why did you need to run the function in a goroutine?\n  IIUC, the only difference made by the goroutine is that the goroutine function runs in parallel to stream.Header().\n  But I don't think the difference makes much sense.\n. if err := dec.Decode(&protoReq); err != nil would be more consistent?\n. why don't you return here on error?\n. Yes.  Could you do so?\nI am worrying about that the error from stream.Header() overwrites firstResult.\n. Is it better to handle io.EOF at L264 if err := handleSend(); err != nil and return nil instead of io.EOF?\n. Code parity is fine.\nBut I am wondering if it falls into https://github.com/tmc/grpc-gateway/blob/b069c864828f47dac69037b625ae20953d4cd430/examples/examplepb/stream.pb.gw.go#L245 even though it is valid to send a zero-length sequence.\n. @tmc There's no guarantee that io.EOF is/will be coming only from dec.Decode.\nThe best place to deal with an error is the place the error happens.\n. It was even better if you make variable names a bit shorter.\nhttps://github.com/golang/go/wiki/CodeReviewComments#variable-names\n. go\naddParameterWithPrefix := func(prefix string, field *descriptor.Field) error {\nBTW, why do you need to keep this function nested?\nIIUC, you could simply define a function like\ngo\nfunc queryParams(prefix string, field *descriptor.Field) ([]swaggerParameterObject, error)\n. Does it work fine with deeply nested fields?\nAlso, it is fine if it is out of scope for now. But what do you think about that a protobuf message has the same message type as a field?\n. I don't know why. But this PR is neutral to the issue.\nhttps://github.com/grpc-ecosystem/grpc-gateway/pull/415/files#diff-743714d76d1753794032b45e364fd780\nCan we work on the issue in another PR?. Could you make the visibility of the package consistent with the one by go compiler?\ni.e. it looks to be better to limit the default visibility to //runtime:__subpackages__.\nBTW, as the original author of Gazelle, I'm sorry -- it should have been done by gazelle. ;-). Sorry, I have no idea. Let me merge this PR as it is once https://github.com/grpc-ecosystem/grpc-gateway/pull/599#issuecomment-381455093 gets ready.. :-)\nTrue. But let me work on it in another PR.. https://golang.org/wiki/CodeReviewComments#declaring-empty-slices. Maybe you intended \"fqmn\"?  Also I don't think it is a \"fully-qualified message name\" in a proto schema but it was something in the generated golang code.\nI wonder why you needed to replace components with it.. c.Target.OneofIndex is simpler.\nSimilarly, msg := c.Target.Message. msg.GetName() ?. Do you mean just if test \"${USE_BAZEL} = true; then?  Or are you thinking about something like test \"${USE_BAZEL}\" = true && ./.travis/install-bazel.sh?\nI did not touch those steps for two reasons. \n Bazel-related steps are less than others.  So they does not bloat up the configuration very much even if I left them as they are.\n I can't simply replace them with test \"${USE_BAZEL}\" = true && ./.travis/install-bazel.sh ... because whole of the step must have exit code 0 on -z ${USE_BAZEL}.\n. \"msg.GetStaus().GetNote() = %q; want %q\".. \"msg.GetNo().GetNote() = %q; want %q\". Can you remove time if you are not using it?\nI'd like to keep SimpleMessage as simple as possible.. OK. Then, are you ok for keeping if [ \"${USE_BAZEL} = true ]; then as is?\n\nWDYT about consolidating version numbers into the env section?\n\nI think it is a good opportunity to do it. I'll push another commit later.. Thank you. Done.. ",
    "mattn": "done\n. fixed race conditions\n. The CLA form, Address point to golang-nuts@googlegroups.com. Is this right? I should write my state/location into it?\n. Done\n. ping @willtrking, @cuongdo, @hbchai\n. ",
    "philips": "Do you have a rough idea how you would like to see this happen?\n. Sounds good. I like this plan.\n. @ceram1 Can you post your current WIP branch somewhere?\n. It took me some time to figure out what google.api.http is, but it is this: https://github.com/google/googleapis/blob/master/google/api/http.proto\n. lgtm :)\n. So, someone needs to build a codegen ala ffjson\nOn Tue, Dec 29, 2015 at 11:24 PM Peter Edge notifications@github.com\nwrote:\n\nYa, I know :) Ugh. Programming is no fun :)\nOn Tuesday, December 29, 2015, Eric Chiang notifications@github.com\nwrote:\n\nA lot of the overhead comes from checking to ensure the result is\nactually\ncompliant with protobuf's defined JSON mapping\nhttps://developers.google.com/protocol-buffers/docs/proto3#json. It's\nslower, but it's correct :)\n\u2014\nReply to this email directly or view it on GitHub\n<https://github.com/gengo/grpc-gateway/issues/79#issuecomment-167843510\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/gengo/grpc-gateway/issues/79#issuecomment-167843616.\n. @tamird that is a cool use case. But, I think you missed a line number anchor in your example.\n. @joeblew99 @tmc You can just build it into the binary as I do in grpc-gateway-example: https://github.com/philips/grpc-gateway-example/blob/master/echopb/swagger.go\n. @sdemos are you going to submit a PR to close this out?\n. LGTM :)\n. \n",
    "IceMan81": "@yugui Are oneofs supported?\n. @yugui Explains why it didnt work when I tried a oneof today; code generation ran without any errors, but the gateway code was unable to map the request params to the protobuf fields.\n. Generated gateway code\n``` go\nfunc request_TestService_StreamingPing_0(ctx context.Context, client TestServiceClient, req *http.Request, pathParams map[string]string) (TestService_StreamingPingClient, error) {\n    var protoReq Price\nvar val string\nvar ok bool\n\nval, ok = pathParams[\"symbol\"]\nif !ok {\n    return nil, grpc.Errorf(codes.InvalidArgument, \"missing parameter %s\", \"symbol\")\n}\nprotoReq.Symbol, err = runtime.String(val)\nif err != nil {\n    return nil, err\n}\n\nif err := runtime.PopulateQueryParameters(&protoReq, req.URL.Query(), filter_TestService_StreamingPing_0); err != nil {\n    return nil, grpc.Errorf(codes.InvalidArgument, \"%v\", err)\n}\n\nreturn client.StreamingPing(ctx, &protoReq)\n\n}\n```\n. ",
    "tbg": "This issue is referenced from the README as covering enum support, which isn't mentioned here. It should be added or filed as a new issue so that it doesn't get lost.\n. ",
    "buckhx": "Is this supported or still open?\nI'm implementing an OAuth2 server and the RFC requires the request Content-Type to be application/x-www-form-urlencoded.. You'd want to add a method that accepts an enum def to the runtime package in convert.go https://github.com/gengo/grpc-gateway/blob/master/runtime/convert.go\nand add it to the proto3ConvertFuncs (& proto2) map https://github.com/gengo/grpc-gateway/blob/master/protoc-gen-grpc-gateway/descriptor/types.go#L293 \nwhich gets used by Parameter.ConvertFuncExpr() https://github.com/gengo/grpc-gateway/blob/master/protoc-gen-grpc-gateway/descriptor/types.go#L190\nand is eventually rendered here in the template https://github.com/gengo/grpc-gateway/blob/master/protoc-gen-grpc-gateway/gengateway/template.go#L210\n. ",
    "theRealWardo": "we ended up rewriting application/x-www-form-urlencoded requests as JSON prior to the grpc-gateway receiving them like this:\n```\n        formWrapper := func(h http.Handler) http.Handler {\n                return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n                        log.Printf(\"Got request: %#v\\n\", r)\n                        if strings.ToLower(strings.Split(r.Header.Get(\"Content-Type\"), \";\")[0]) == \"application/x-www-form-urlencoded\" {\n                                log.Println(\"Rewriting form data as json\")\n                                if err := r.ParseForm(); err != nil {\n                                        http.Error(w, err.Error(), http.StatusBadRequest)\n                                        log.Println(\"Bad form request\", err.Error())\n                                        return\n                                }\n                                jsonMap := make(map[string]interface{}, len(r.Form))\n                                for k, v := range r.Form {\n                                        if len(v) > 0 {\n                                                jsonMap[k] = v[0]\n                                        }\n                                }\n                                jsonBody, err := json.Marshal(jsonMap)\n                                if err != nil {\n                                        http.Error(w, err.Error(), http.StatusBadRequest)\n                                }\n                            r.Body = ioutil.NopCloser(bytes.NewReader(jsonBody))\n                            r.ContentLength = int64(len(jsonBody))\n                            r.Header.Set(\"Content-Type\", \"application/json\")\n                    }\n                    mux.ServeHTTP(w, r)\n            })\n    }\n    if err := http.ListenAndServe(\":8080\", formWrapper(mux)); err != nil {\n            log.Fatalf(\"failed to start gateway server on 8080: %v\", err)\n    }\n\n```. similar to how I solved #7 - I ended up wrapping the mux and writing a little bit of code to propagate the tracing context correctly. here's how I did it:\n```\nimport (\n   ...\n   \"github.com/opentracing/opentracing-go\"\n   \"github.com/opentracing/opentracing-go/ext\"\n)\nvar grpcGatewayTag = opentracing.Tag{Key: string(ext.Component), Value: \"grpc-gateway\"}\nfunc tracingWrapper(h http.Handler) http.Handler {\n  return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n    parentSpanContext, err := opentracing.GlobalTracer().Extract(\n      opentracing.HTTPHeaders,\n      opentracing.HTTPHeadersCarrier(r.Header))\n    if err == nil || err == opentracing.ErrSpanContextNotFound {\n      serverSpan := opentracing.GlobalTracer().StartSpan(\n        \"ServeHTTP\",\n        // this is magical, it attaches the new span to the parent parentSpanContext, and creates an unparented one if empty.\n        ext.RPCServerOption(parentSpanContext),\n        grpcGatewayTag,\n      )\n      r = r.WithContext(opentracing.ContextWithSpan(r.Context(), serverSpan))\n      defer serverSpan.Finish()\n    }\n    h.ServeHTTP(w, r)\n  })\n}\n// Then just wrap your mux...\nif err := http.ListenAndServe(\":8080\", tracingWrapper(mux)); err != nil {\n  log.Fatalf(\"failed to start gateway server on 8080: %v\", err)\n}\n```. @tmc sorry for the delay! rebased and pushed finally.. awesome! thanks for picking this up @wimspaargaren . I think your request is actually a dupe of #7. sounds good. I'm a fan of that approach as well. shall we leave this ticket open until I can share a repo for that or do you wanna close this out and say nope then?\nyep totally interested in getting #458 merged. I'll give it a quick poke right now so someone can on the review there.. with a few patches on top of #458 - this actually works without writing any kind of a wrapper. I was pleasantly surprised. of course I'll merge those into that PR.\nin our proto's service definition, we just have rpc DoTheThing(Request) returns (stream google.api.HttpBody) and it works great!. @achew22 I assume you meant master which I've rebased on.. why shouldn't the default behavior be to treat HTTP body protos specially?. done.. hmm well the problem is that the content type right now is not dynamic. I suppose we could add another function like ContentTypeForMessage(m proto.Message) string and make that work as a preferred content-type setter. i.e. if ContentTypeForMessage(m proto.Message) string returns a non-empty string, we use it. otherwise we use the existing ContentType() string. now this has a similar problem as it changes a public interface, which would result in compilation errors for implementors anyway, so I'm afraid I don't have a clever solution. this feels like the lesser of two evils to me?. v is the same thing that Marshal(v interface{}) is passed, I'm honestly just following convention here. should I update comments on Marshal as well? (happy to do that just lmk)\nside note, I am not actually sure why all of these interfaces are not instead of type proto.Message which is what is actually being passed.... yea sure, but let's finish the other discussion first so I don't have to change things too much.. bumping this - any thoughts here?. actually wow yea I think it is possible. so via a type union I think we can do something like this - https://play.golang.org/p/G4x7sEtEHbh - which in this case would mean we have something like this probably in this file:\n// ContentType returns the Content-Type header value given a Marshaler or TypeAwareMarshaler and the interface to marshal v.\nfunc contentType(m, v interface{}) string {\n  switch t := m.(type) {\n  case TypeAwareMarshaler:\n    return t.ContentTypeForInterface(v)\n  case Marshaler:\n    return t.ContentType()\n  }\n  return \"\"\n}\nthen we'd update all the marshaler.ContentType() calls in this package to contentType(marshaler, v). what do you think of that? I think that is not technically API breaking, right?. ",
    "rinatio": "+1 It would be useful to have it built-in!. @achew22 Is it included in v1.3.1 release? Cannot make it working. I'm on the latest commit 6658b3a.. @achew22 Thank you for quick reply. I'm not sure it is the same issue. I will double check.. ",
    "jinleileiking": "After one afternoon reading the f**king source code.\nThis solution is not good.\n\nIt cannot check the input whether a int or string by pb.\n...\n\nThe good solution is: \n\nmake post && form use get logic.\nchange the autogen code:\n\n- func request_Example_GetForm_0(ctx context.Context, marshaler runtime.Marshaler, client ExampleClient, req *http.Request, pathPar\n|     var protoReq FormRequest\n|     var metadata runtime.ServerMetadata\n|      \n|        ---------------------------from form data to genrate : req.URL.Query() ------------------------------\n|     if  err := runtime.PopulateQueryParameters(&protoReq, req.URL.Query(), filter_Example_GetForm_0); err != nil {\n|         return nil, metadata, status.Errorf(codes.InvalidArgument, \"%v\", err)\n|     }\n|\n|     msg, err := client.GetForm(ctx, &protoReq, grpc.Header(&metadata.HeaderMD), grpc.Trailer(&metadata.TrailerMD))\n|     return msg, metadata, err\n|\n| }\n\nchange the form data to url.Values then call:\n\nPopulateQueryParameters(msg proto.Message, values url.Values, filter *utilities.DoubleArray). Not Merged?. @philipithomas \ngwmux := runtime.NewServeMux(runtime.WithMarshalerOption(runtime.MIMEWildcard, &runtime.JSONPb{OrigName: true, EmitDefaults: true}))\nvery, very useful!. Good question. I have the same. I'd love to see this merged!\nOR YOU PUT THIS INFO IN WIKI OR README!!!!. which version is stable\uff1fCan give me  a release note?. Very useful . Thanks.. I am working at ksyun openapi, we need to let the return format be the same. So we need to customize.\nhttps://docs.ksyun.com/read/latest/116/_book/KLSAPI/CancelRecordTask.html. I will test lately.. The return message is: \n{\"error\":\"type mismatch, parameter: num, error: strconv.ParseInt: parsing \\\"aaa\\\": invalid syntax\",\"code\":3}\nWe should completely customize this return.( Use template?)\nWe should not change the core code. We need a config file  ??? to set this line:\n+       return nil, metadata, status.Errorf(codes.InvalidArgument, \"type mismatch, parameter: %s, error: %v\", {{$param | printf \"%q\"}}, err)\n. @jinleileiking , I think you need to set the HTTPError for runtime.Servemux. See https://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/mux.go#L98\nIt seems the way to solve this problem. I will dig into it.. We should export handleForwardResponseServerMetadata. I  do not know #51 whether solve this problem. Let the project member to close this issue. . I signed it!. I signed it!. I signed it!. I signed it!. ready to merge :). If you think \nruntime.HandleForwardResponseServerMetadata\nruntime.HandleForwardResponseTrailerHeader\nruntime.HandleForwardResponseTrailer\nis not commonly used. \nYou can close this issue.\nBut I think sometime, we need this 3 funcs.. The standard  DefaultHTTPError use these 3 funcs. see errors.go :81\nthe customized DefaultHTTPError will use these 3 funcs too. I think.. If these 3 func is commonly used( every request). We should write a interface to let customize func run this 3 func automatically.. @yugui What you said is just my want to say, lovely lady. \nBut I do not understand your codes. Yes.  Finally I understand your codes :) \n. I think  we should wait @yugui codes :)   . I close this issue.. yes. I have know how to use it, later I will update wiki.. updated wiki. use float. @achew22 my data is bigger than int32. Can int32 work?. I just want to set a unixtime(epoch) which is a int64. After 3 hour google , I use struct.proto to solve this f**king problem.. https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto. waiting for merge.. Needs example to show me how to use it.... @theRealWardo Can I modify your codes? If you donot have time, I will make code changes. I like this to be merged.. https://github.com/googleapis/googleapis/blob/master/google/api/httpbody.proto. https://github.com/grpc-ecosystem/grpc-gateway/pull/458/commits/136b1437c1d20758a28765ee4736ce8e3bbefeba. https://github.com/grpc-ecosystem/grpc-gateway/pull/458. @theRealWardo Your code does not show where does the code put in. https://github.com/jinleileiking/grpc-gateway/commit/bb153b3f2cfcdb086b39bdc2edbbc9216951eb97 \nI put to the right place. @yugui @tmc Is it the good way to finish this feature?\nMy solustion seems not good. Any advice?. use expvars.Handler() to realized.... Can this be merged?. @yugui @tmc . @achew22 . @achew22  Thank you. I will add these doc, ut,cov things.\nSuggestion:\nShall we add a marshal-contrib project to make these non-official marshaler easy to plug? . It is not so common.\nThere is no way to write a  middleware in grpc-gateway middleware. so I need write in grpc-interceptor.. \"github.com/grpc-ecosystem/grpc-gateway/runtime/internal\"\nThis import will cause some problems.\nI want to do some modify for source code. But the github donot give a merge. \nSo I clone bare this repo to my private repo. such as my.com/grpc-ecosystem/grpc-gateway\nwhen build from my own project, handler.go will use github.com/grpc-ecosystem/grpc-gateway/runtime/internal. @johanbrandhorst  I mean.\neach api have its own marshaller:\n/api/json ---> use jsonpb marshaller\n/api/form ---> user formpb marshaller. > Do you want the ability to control the marshaller per-method? \nYes\n@johanbrandhorst Can you give me an example? Thanks.. ",
    "konsumer": "I've got the start of a protoc plugin that does this here.\nIt's written in node (I'm not strong with go, yet) but I've got a docker container setup to make it easy to use. The docker will generate a gateway (including main) and run it. It needs some work and testing, but it's a start.\n. Yeh, definitely @tmc. It's a proof of concept, and totally separate project that produces a compilable main for the output of this project, in protoc. Since the implementation is basically gluing a mustache template with the message that comes in from protoc, it should be fairly simple for a go dev to use ideas from it, but obviously it doesn't make sense to require the whole node ecosystem in a project that is written mostly in go. Still, it might be helpful to people in the meantime. I have been using a similar system in my own projects, and it works pretty well.. As an example, without writing go or having any go tools installed, the included docker generates a complete gateway, compiles, and runs it.. @tmc will do. I'm just not there with go, yet. I need more time for learning things outside of my work... Feel free to use any ideas from it to implement another go-based protoc plugin, there really isn't much to it.. ",
    "tmc": "@konsumer thank you for your contribution. I think for the context of this project yugui means generate a package main that allows a user to skip writing any go but not skip compiling some go.. Oh i see you are generating go. For inclusion in this project it likely makes the most sense for the implementation to be in go as well.. @konsumer awesome! thanks for adding your contribution. I think you'll find that working on the go proto generators isn't too bad -- definitely reach out if you have any issues setting up your environment (or even better contributing to dev docs!).. @mwitkow any action on this?\n. @yugui for swagger-compat questions: https://github.com/OAI/OpenAPI-Specification/issues/396\n. package foobar\n    imports github.com/gengo/grpc-gateway/internal: use of internal package not allowed\n. @yugui I think this issue can be closed -- it's just confusing for newcomers.\n. @yugui can you sketch out what sort of approach you were thinking? I may hack on this soon.\n. this has implications for swagger specs doesn't it? Should this be described via a custom annotation?\n. @yugui I like that solution for the mid-term.\n. Maybe it'd be nice if types were prefixed if they aren't defined in the proto file(s) that are being used to render the swagger output.\n. It'd be great if the swagger generation could easily use this custom Marshaler.\n. @willtrking please don't break the convention of the context.Context parameter being the first argument to context-aware calls.\n. @kazegusuri you could move the context.Context parameter to consistently be the first argument\n. @ivucica IMO you should separate the comment handling from this new <!-- swagger extras start concept as they're distinct and the latter deserves more consideration. Do you mind pulling that concept out into another PR? Thanks!\n. What do you think about simply taking the last portion of the comment and if it's a json document integrate it.\n. @yugui see #156 \n. @joeblew99 I meant the actual specs. It should be out of scope to include swagger-ui in this project.\n@philips yeah, I go-rice my specs into the binary now but it's a bit janky.\n. I'm personally welcoming of the switch of the naming around swagger to open api spec. I don't have a better label/tag offhand but putting \"Swagger: \" into this api means a slower transition to OpenAPI Specification.\nopenapi: ?\n. I agree that it feels off in context. When considering longer term I do think your change makes more sense.\n. @yugui I like that idea -- could you propose how that might look?\n. I like this as a middle ground. @ivucica thoughts?\nOn Thursday, May 19, 2016, Yuki Yugui Sonoda notifications@github.com\nwrote:\n\n@tmc https://github.com/tmc, @ivucica https://github.com/ivucica\nIn my personal opinion, it is not too bad even if we only have a single\nstring custom option which stores schema patch in JSON because at least we\ncould avoid parsing comments.\nI am worrying about parsing comments because it is not very common in\nprotobuf convention and it has less power than custom options.\nWith custom option, we could deprecate the simple string option and\nintroduce a well-organized messages to describe schema in future.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/gengo/grpc-gateway/pull/145#issuecomment-220520128\n. @ivucica I think the custom option to supply additional openapi to inject is the way to go here.. @ivucica sorry I missed this comment. -1 to overloading comments, +1 to rich options.\n\n(option (grpc_gateway.protoc_gen_swagger.options.swagger_swagger) = {\nexternalDocs: {}\nover\n//   \"externalDocs\": {)\nQuestion -- can we avoid the options.swagger_swagger stutter?. Unless it presents a serious technical challenge I think the richer option is better.\nI think it might be worth revisiting the s/swagger/openapi/g issue -- do you have thoughts there?. Super happy to see this progress!. why the MakeMarshalerMIMERegistry naming change?\n. @yugui do you have thoughts here on how we should handle this problem? I'm happy to hack on a patch but have only been lightly watching the swagger and marshaler work.\n. Sure, you could do so. I'm not sure if js client generation is something grpc-gateway should take on directly as needs will differ dramatically. \nHere are basic starts of Flow and Elm type generators https://github.com/tmc/grpcutils/tree/master/protoc-gen-flowtypes https://github.com/tmc/grpcutils/tree/master/protoc-gen-elmtypes\nI'd love for these (or something similar) to grow and generate client code as well (contributions welcomed).\n. preliminary work and kind of hacked up but https://github.com/tmc/grpcutils/blob/master/websocket_proxy.go is an example of wrapping grpc-gateway with a transparent websocket upgrade\n. @yugui I think there's a basic argument of just fully expressing grpc's semantics to browser clients.\nI have starts of basic implementations of flowtype and elmlang generators here https://github.com/tmc/grpcutils and first-class streaming support in grpc-gateway will only motivate more effort in this area.\n. FWIW I separated that code: https://github.com/tmc/grpc-websocket-proxy\n. @yugui do you need further clarification?\nTo restate, before this change a streaming handler did not start a goroutine and ran all Send()s before returning and having the caller call Recv(). This proposal is a middle ground of blocking until the first Send() to aid in surfacing issues with initial payloads.\nPerhaps setting errs to nil would be a nicer way to express the semantics of block until first execution. I'm open to suggestion here.\n. How would you accomplish this without a separate goroutine?\n\nOn Jun 1, 2016, at 10:35 PM, Yuki Yugui Sonoda notifications@github.com wrote:\na streaming handler did not start a goroutine and ran all Send()s before returning and having the caller call Recv().\nI don't think it was changed with this PR because <-sendErrs waits for an error from the goroutine or the termination of the goroutine. Also on error from <-sendErrs, the caller returns wihtout calling Recv. So Send and Recv never interleave, IIUC.\nSo in my understanding, the change made by this PR was\nRead req.Body until its end even if there was an error\nSend streams to the upstream grpc server even if there was an error\nI think your intention\nto block until the message is sent by the client as a sort of middle ground between just ignoring errors and interrupting the stream on any error.\nmakes sense. But if my understanding above was correct, you didn't need the new goroutine. You could do the same thing in the same goroutine as caller.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Did you miss that the first Send if successful will send nil and stop blocking?\nOn Jun 1, 2016, at 10:35 PM, Yuki Yugui Sonoda notifications@github.com wrote:\na streaming handler did not start a goroutine and ran all Send()s before returning and having the caller call Recv().\nI don't think it was changed with this PR because <-sendErrs waits for an error from the goroutine or the termination of the goroutine. Also on error from <-sendErrs, the caller returns wihtout calling Recv. So Send and Recv never interleave, IIUC.\nSo in my understanding, the change made by this PR was\nRead req.Body until its end even if there was an error\nSend streams to the upstream grpc server even if there was an error\nI think your intention\nto block until the message is sent by the client as a sort of middle ground between just ignoring errors and interrupting the stream on any error.\nmakes sense. But if my understanding above was correct, you didn't need the new goroutine. You could do the same thing in the same goroutine as caller.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @yugui I was missing some things here (it was written in a rush). This now ensures CloseSend is called and cleans up the semantics a bit. \n\nOne alternative would be to move the Send handling out and invoke it once to capture the initial error. Thoughts?\n. @yugui I changed the implementation to be more clear.\n. @yugui updated with feedback taken into consideration.\nThis SO post deals with the state of response-before-request (early 401s in particular) http://stackoverflow.com/questions/14250991/is-it-acceptable-for-a-server-to-send-a-http-response-before-the-entire-request\nThe most common case here would be clients that don't process the response until they've sent the request fully. For my use case (a websocket proxy on top of grpc-gateway) this isn't a concern as I control the http communication on both sides.\nI don't think having some misbehaving http clients should block this as it enables writing websocket proxies. Having this work in grpc-gateway closes the gap of bringing grpc semantics to browsers (with a little extra effort).\n. @karlkfi that is correct.\nI have an example websocket proxy here: https://github.com/tmc/grpcutils/blob/master/websocket_proxy.go\nit's a generic http.Handler that will perform a ws upgrade on the same url as the http endpoint.\nexample use here:\nhttps://github.com/tmc/grpcutils/tree/master/examples/cmd/wsechoserver\n. @yugui ping\n. Waiting to Send() until the first message was an intentional tradeoff to attempt to fail fast when invalid messages are sent on the stream. I think we should revisit this tradeoff because it's an impedance mismatch between this and straight grpc.\n. https://github.com/gengo/grpc-gateway/compare/master...tmc:tmc.bidi_send_without_recv pushed here.\n. cc #195\n. @yugui I imagine we'll be changing this code a bit with the discussion around the special treatment of the first message for bidi endpoints -- my take is get this in and we can discuss zero-length send streams separately.\n. +1 on grpc.gateway -- can this move forward as this is in a bit of an awkward state at the moment.\n. @dmlyons I think the important thing is that the directory containing protoc should be on your PATH. @brianstarke can you confirm for posterity?. @rgarcia can you rebase this again?\n. closed by #297. @yugui LGTM\n. @brocaar are you posting like {\"name\": \"foo\"} when you have body: \"name\"? That'd be incorrect.. @yugui yes that is the case.\nFor example, with just $GOPATH/src included in protoc -I I can now reference annotations like so: import \"google.golang.org/genproto/googleapis/api/serviceconfig/annotations.proto\"\nand google.golang.org/genproto/googleapis/api/serviceconfig/annotations.pb.go exists.\n. genproto doesn't include proto files anymore so we need to keep vendoring -- #325 updates protos and includes the appropriate lines to rely on genproto for go files.. @ambarc while not part of grpc-gateway I have a websocket-upgrading proxy written on top of this project here: https://github.com/tmc/grpc-websocket-proxy\nIt's a little light on documentation and hasn't been run through a ton of profiling and testing but I'm open to contributions there.\n. @ambarc the lowest common denominator for streaming semantics in the browser (and react native, fwiw) is websockets. My solution is explicitly on top of grpc-gateway -- I haven't looked to know if it would use h2 when talking to grpc-gateway. Playing with GODEBUG=http2debug=1 would help answer that.\nStreaming body support is just not widely available in browsers.\n. @ambarc that's correct to the best of my understanding. As per https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API -- support for streaming responses is still pretty limited. I explored that initially but a chrome-only experience isn't acceptable for my use cases.\n. Given the canonical MIME header key encoding we would likely want to make this 'Grpcgateway' as the header prefix. I don't love this but we should probably keep out of the way of 'Grpc-*'.\n. as this is it will be a breaking change as 'authorization' is passed as 'grpcgateway-authorization'\n. Closing as #252 addresses this. @edrex can you sign the CLA and re-trigger the build (it looks like an intermittent failure).\n. This is fixed in https://github.com/swagger-api/swagger-js/issues/841\n. HEAD has 1.7 now.\n. I know this is super old but yes base64 encoding seems like a fine path.. A good first step here would be to submit or modify an executable test case that demonstrates this problem. Making contributions easy for beginner contributors is important so if you hit snags on this path please open issues to improve documentation.. @Son-Lam I suggest you upgrade your protobuf package as jsonpb is defined there. This project's build is currently clean so the problem is with your environment.\n. @EricLagergren thank you for your contribution - can you rebase and re-gen examples for this please?\n. @EricLagergren I think you are running a different version of swagger-codegen -- this project currently relies on 2.1.6\n. ping @EricLagergren \n. @atombender this would be a great addition, I'm happy to review any work or answer any questions.\n. See https://github.com/googleapis/googleapis/blob/master/google/api/http.proto for semantics.\nIn particular https://github.com/googleapis/googleapis/blob/master/google/api/http.proto#L130 shows that path params should be used for those that aren't provided in body (for '*' case).\n. @atombender sorry I misread your original post. On reading further my understanding is that in the context of body: \"*\" that fields not bound by path parameters (/{message_id}/) are bound by the body but HTTP paramters (/method/?message_id=foo) are not bound.\n. @yugui shouldn't path parameters be included but not query string parameters?\n. FWIW I've personally wanted the PUT to /entity/{ID} use case to populate the ID from the path.\n. It does seem like this should be achievable in a fairly clean fashion.\n@EricLagergren what's the error that subsequent Write generates?\n. It's unclear what the appropriate default is here. I think being closer to jsonpb defaults is probably for the best and this behavior is configurable via WithMarshalerOption. @yugui - wanna weigh in?\n. I've warmed up to this as a change but perhaps should come with a version bump as it changes output behavior. @philipithomas do you want to rebase this and regenerate output?. @MalteJ to be clear you can control rendering with the WithMarshalerOption option.. This was closed by #244 \n. see #213 for some related discussion\n. @dprotaso thank you for your contribution -- could you look at the test failure and sign the cla?\n. @dprotaso I re-ran the tests and the flakiness is solved.\n. @gsogol can you comment further on what you have in mind here? It's not clear that this would be in-scope for this project.\n. Given the need to have awareness of the content of the messages I'm not sure what you're describing is achievable. I'm going to close this unless there's some sketch of what this would actually mean.\n. To comment further, there's nothing stopping you from putting nginx in front of grpc-gateway.\n. @kokaz thank you for your contribution -- can you document said breaking changes next to the parameter description in README.md?\n. I'm uncomfortable with the amount of duplication involved here but really do like the idea.\nWhat do you think about maintaining the surface api but preferring req.Context in Register{{$svc.GetName}}Handle?\n. I'd like to keep the number of knobs low for simplicity's sake -- another option on the table would be dropping go<1.7 support. (I don't have great sense of the distribution of active go versions using this package).\n. I think not changing the signature is the way to go there.\nThis will mean req.Context() will be a \"context\" not an \"x/net/context\" which has some less-than-friendly implications until upstream drops 1.6 support or we get aliasing.\nIt seems like the simplest thing would to just swap out that one particular line depending on the flag's presence vs wading into build tag complexity -- i think we can assume folks that opt into this flag are responsible adults that know what they're doing.\n. @kokaz what do you think about ditching the extra template and just doing the logic in a conditional in one template?\n. @kokaz we discussed offline but just to drive the conversation forward, this is the sort of approach I had in mind: https://github.com/grpc-ecosystem/grpc-gateway/compare/master...tmc:request_context\n. @kokaz great, opened another PR, let's move discussion there.\n. @willnorris any info on the cla/google check hanging?\n. Ah. @christianvozar can you fill out the google cla at https://cla.developers.google.com ?\n. @willnorris this project uses GitHub's protected branches feature to prevent direct pushes to master -- how likely is it that the googlebot will learn how to deal with this situation?\n@yugui if the answer is not likely then we I/we need another path to get these types of PRs in.. IMO the best way to do this is via a new https://godoc.org/github.com/grpc-ecosystem/grpc-gateway/runtime#ServeMuxOption. @fiibbb it's not super specified but could encompass what you're describing. with #336 landed I think this can be considered closed.. @achew22 afaict it dumps out code with different package names into the same directory.\n. I pushed https://travis-ci.org/tmc/grpc-gateway/builds/177507616 to demonstrate\n. with 2.2.0 or 2.2.1 I get:\n`\ngo test -race github.com/grpc-ecosystem/grpc-gateway/...\ncan't load package: package github.com/grpc-ecosystem/grpc-gateway/examples/clients: found packages abe (a_bit_of_everything_nested.go) and echo (echo_service_api.go) in /Users/tmc/go/src/github.com/grpc-ecosystem/grpc-gateway/examples/clients\nmake: *** [test] Error 1\nThoughts on removing  or disabling the swagger-codegen calls? It's a bit tangential and it will be a contribution barrier.\n``\n. looking more closely this appears to be the--additional-properties packageName=echoparameter to swagger-codegen not affecting the output directory anymore.\n. If were were actually exercising theswagger-codegen`-generated code in test I'd be more inclined to keep it around. Right now it's basically testing well-formedness of the generated swagger output.\n. @yugui thoughts here?\n. @LeeWong I'm going to close this as it isn't really a grpc-gateway problem but a protoc invocation issue. You need to supply a relative path to the .proto file. \nperhaps you should try: protoc -I/usr/local/include -I. -I$GOPATH/src -I$GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis --go_out=Mgoogle/api/annotations.proto=github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis/google/api,plugins=grpc:. test_service.proto with cwd = /home/lee/IdeaProjects/go_api_gateway/src/api_gateway/rest\n. @yugui, @kazegusuri / others, can I get a review here?\n. a8f098923c31abf1f2a075c451a4a78487a66a28 updated to SupportPackageIsVersion4\n. PR #256 addresses this.\n. @philipithomas thanks for the report -- think you could demonstrate this in a failing test?\n. superseded by #281. This is a little tricky since the marshaler can be customized -- IMO defaulting to the default marshaler &JSONPb{OrigName: true} is the most appropriate behavior here.\n. Just to be clear the current behavior is expected. Respecting the field names in the proto file vs the canonical json representation of proto3 (lowerCamelCase) was introduced in PR #151 (adding OrigName: true).\nThis probably merits a documentation addition.\nIf we want to discuss changing the default marshaler configuration (and the swagger output) we can in another issue.\n. opened #266\n. @yingzong unfortunately there is not a mechanism to provide these options. You can open a feature request to allow providing an option of this sort.. this is because (*Registry).loadServices in github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway/descriptor/services.go skips over methods that don't define these mappings so the lookup by index gets off.\nwe should probably still create Methods with no Bindings for this case\n. @kokaz can you comment here to pass the cla bot?\n. Thanks @kokaz -- I want one more LGTM from another contributor before merging. ping @yugui @kazegusuri @achew22 @t-yuki \n. I think this should become the default once we're okay with degrading 1.6 support.\nI added testing this code path to the travisci configuration, I'm not certain adding a generated example provides that much value.\n. @achew22 i think a pretty good use-case is tracing (opentracing or google cloud trace). cla is approved in other PR but master is protected.. Thanks for your report, could you supply an example and/or failing test demonstrating this issue?. this is replaced by #325 . It needs a comment to be kicked off.. Thanks for your contribution, could you please show this behavior in a test case? Also please be mindful of unrelated changes in diffs (there's some whitespace noise). Also please sign the CLA.. @achew22 LGTM. @peter-edge thank you for your contribution -- have you signed the cla? is this current?. replaced by #325. @bluehallu thank you for your contribution -- have you signed the cla?. @favadi thank you for your contribution -- have you signed the CLA? could you include a test case please?. I think this is satisfied with v1.2.0.. The related issue is merged now.. @msample thank you for your contribution. This code looks good -- what are your thoughts on if this flag should be commented?. I'm fine with leaving it undocumented. Other thoughts @achew22, @yugui, @t-yuki?. @msample that does appear suspect, thanks for pointing that out. Can you comment again to kick googlebot?. @msample I like the kvvar refactor but can we separate that into a separate pr?. @msample thanks!. @JohanSJA thank you for your contribution. This change looks good -- can you sign your Google CLA?\nAre there other values than \"null\" that need to be special cased? NaN. @JohanSJA looks like googlebot just hadn't reported correctly -- can you resolve the query_test.go conflict? I'd love to see this land.. @JohanSJA can you try commenting to ping googlebot?. @kirk91 thank you for your contribution. We can't just skip those as we manage to mess up swagger comment attachment if we do so. I have an alternate solution here: https://github.com/grpc-ecosystem/grpc-gateway/pull/286. googlebot kick. googlebot kick.. I'm going to land this as it's breaking HEAD builds.. kick googlebot. This is possible by supplying an option to NewServeMux via https://godoc.org/github.com/grpc-ecosystem/grpc-gateway/runtime#WithMarshalerOption. The upstream proto file definitely is pretty detailed and as a first step we could link to it.\nFurther providing examples and some friendlier introductory documentation of the binding semantics would be awesome.. This is defined in the proto3->json mapping here https://developers.google.com/protocol-buffers/docs/proto3#json. I'm not sure this is an appropriate change as we don't currently have a way to expose marshaler customization to the swagger generation path -- I'm open to an issue that represents that but until then I don't consider this a problem.. @jmzwcn perhaps a write up of differences would be helpful for people checking out that project. It's not clear that there's anything actionable here so I'm going to close. Feel free to reopen if you feel it's appropriate.. Thanks for your contribution. Can you sign the CLA and demonstrate this behavior in an example or a test?. @c4milo this capability is exposed behind a flag at the moment and we have #313 open as well. FWIW I do the same as @t-yuki . This likely implies options outside of google.api.http annotations.. @arun0009 you might look/comment on https://github.com/grpc-ecosystem/grpc-gateway/pull/305. @arun0009 / @morriswinkler-simplesurance -- that suggestion may work for the 200 path but custom proto would be required pretty rapidly IMO. This effort should continue in https://github.com/grpc-ecosystem/grpc-gateway/pull/145. @shilkin Thank you for your contribution. This is quite interesting. Can you please sign the Google CLA?\nThis merits some wider discussion and better documentation but in general I'm open to the approach.\nA few questions that immediately come to mind:\n- what is the line of responsibility between handlers such as this and grpc interceptors?\n- what other types of method options do we anticipate? should those influence any design decisions here? (a recent example is specifying additional possible http return codes in swagger gen)\n- can/should we support this at the service level? file level?. @ggeorgiev hi there, thanks for your issue report -- could you please put this sort of failing scenario into a test case?. @ggeorgiev you can simply capture those parameters in either get params or in the body for now. If you can submit a failing test case it will lower the barrier to fixing this.. I think several folks post-process the generated swagger docs and merge in overrides. That said contributions in the form of allowing greater control of openapi output are welcomed.\nAs an example, I merge together multiple services as:\nsh\n   cat *.swagger.json | jq --slurp 'reduce .[] as $$item ({}; . * $$item)' > apis.json\nbut with a lexigraphically-last file with overrides (zlast.swagger.json). @fische thanks for the report -- could you put an example into a failing test case?. @fische I mean can you add a test case that demonstrates the failure (perhaps to #312). Forwarding as-is would potentially mean collisions in the 'grpc-*' space.. I would be open to reconsidering this but it would be a backwards incompatible change. Thoughts?. @lucas-natraj we'd have to keep sending with the grpcgateway- prefix as we do now which while a bit messy is probably ok. I'm open to a diff that forwards headers more directly.. @lucas-natraj You're right that this issue, IMO represents moving to a blacklist vs a whitelist. In general the likelihood of introducing unwanted outcomes with a blacklist are higher but if we feel like we cover our bases here I think we'll be fine. If we arrive at blacklist over whitelist we can do a release that mimics old and new behavior and then deprecate old behavior in a subsequent release.. I think a sane default here is to continue whitelisting and supply customization capabilities with approaches like #360.\nThat said it is an interesting question as to whether or not the grpcgateway- prefixing is desirable to maintain. Open to thoughts here.. I'm curious what is a good example of what could go wrong if we eliminated Grpc-Metadata- and grpcgateway- (with selective blacklisting). \n@achew22 I think you have some thoughts here. @yugui do you want to chime in?. @achew22 thanks for your input. I agree with your and @lucas-natraj's analysis. @TamalSaha great proposal. @TamalSaha while related, can you start a new issue re: trailing metadata to http response code mapping? I assume it will mirror the request metadata population you've laid out.\nI think the next steps I'd like to take are:\n1. Make the header behavior pluggable and provide the current behavior as the default.\n2. Supply the new planned behavior (grpc-* filtering and behaving like nginx in reverse-proxy mode) as an option.\n3. In a later release move to the new behavior as default.. @fische thanks for your contribution -- can you sign the google CLA please?. @fische ah, sometimes it takes another comment, as demonstrated.. @fische I like this change but would like the behavior covered by a test case to prevent regression in the future.. while I'm skeptical of semver, yes I think so. We should get another 1.x release out before cutting a 2.x. @tzneal thank you for your contribution!\nCan you sign the google CLA?. @tzneal no you just needed to comment to have it update the status. Thank you.. @nilium thanks for catching this -- have you signed the CLA?\n. Thank you.. I think this would be a nice addition -- the http.proto documentation doesn't describe map field population from query arguments so my only concern here would be a divergence from how any equivalent google service would behave (e.g. cloud endpoints).. @Mr-Giraffe thanks for your comment. \nGiven the flexible nature of 'RESTful' could you supply a reason why the former should be considered 'non-standard'? \nBetween the fact that project uses proto3 we intend to adhere to that standard. You can't declare a direct 'repeated Message' as the response type on a grpc rpc. \nHere is some api design documentation that implies having this child field for a collection endpoint: https://cloud.google.com/apis/design/standard_methods#List\nThis sort of divergence from grpc and proto3 would be inappropriate for this project.. Thanks for the comment. I would like if this project could accommodate that but I don't see how to do so in a way that doesn't cause a mismatch of sorts. I do think it's common that you would want to supply some metadata especially in the form of paging/cursors. Not returning objects top level ends up appropriate in many situations anyways. Thanks again for your comment! Sorry that this doesn't quite match what you wanted here.. @flisky thanks for your fixup!. @flisky can you comment again to trigger googlebot?. @zeiler @flisky sorry about the delay. This was overlooked. Merged. . @campoy thanks for your contribution!. @johanbrandhorst thanks for the issue report. My preference would be to allow either representation.. That seems to indicate https://github.com/grpc-ecosystem/grpc-gateway/blob/f3aa758b8ae01abecf727dba035444545dbe41de/runtime/query.go#L150 is failing. @nilium this looks great, thank you for your contribution.. I think being explicit over allowing random behavior is prefereable here. LGTM.. Thank you for your contribution! let's move forward with the approach in #336.. @fische thanks for the issue report. This scenario should be handled more gracefullly. Perhaps a codes.InvalidArgument/http.StatusBadRequest with a helpful message would be best.. @marcusljx thank you for your issue report. can you share your full protoc invocation (privately if . needed) as you should get lines more like import _ \"google.golang.org/genproto/googleapis/api/annotations\". Just to be clear we intentionally moved to the published go code published at https://godoc.org/google.golang.org/genproto to prevent re-registration of descriptors.. @dong77 thanks for chiming in -- if your generated go files have github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis/google/api references then you're using an outdated protoc-gen-grpc-gateway. I opened https://github.com/philips/grpc-gateway-example/pull/15 to reflect this change.. @ilius thank you for your contribution -- please regenerate the examples.\nAlso, have you signed the google cla?. @ilius you need to run and commit the result of 'make examples'.. @ilius can you resolve these conflicts?. @ilius on my machine if I run 'make realclean test examples' i get exit code 0 and some changed files.. @fische thank you for your contribution!. @jdoliner thanks for the issue report. This likely means you have an outdated \"M\" mapping in your protoc invocation. Can you share how you are invoking protoc?\nYour protoc line should appear as something akin to:\nprotoc -I. \\\n    -I${GOPATH}/src \\\n    -I${GOPATH}/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\\n    --go_out=plugins=grpc:. \\\n    foobar.proto\n. @jdoliner - I think you just need to eliminate 'Mgoogle/api/annotations.proto=github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis/google/api,' -- please report back.. @jdoliner thanks!. @warrenzhu25 does the ability to customize marshalers not satisfy your use case?. godoc.org has an integration with sourcegraph that allows you to look at other instances of its use.\nhttps://godoc.org/github.com/grpc-ecosystem/grpc-gateway/runtime#WithMarshalerOption (hover to the right of the function name) ->\nhttps://sourcegraph.com/github.com/grpc-ecosystem/grpc-gateway/-/blob/runtime/marshaler_registry.go#L77:33\nThat said this could be better documented or put into a functional example. I'm going to change this issue to reflect that.. @TamalSaha thank you for your issue. Let's keep editor specific configuration in local git configuration. You can extend your local ignore list with the file .git/info/excludes. @TamalSaha thanks for your contribution.\nA couple notes:\n1. Mutating a global to modify behavior should be avoided, I'd prefer passing some sort of Option.\n2. It's not clear to me that we should supply a set of Matcher implementations.. I would prefer the Register methods taking a variadic options slice or the mux expanding a bit in responsibility.. $DAYJOB beckons but I can review this sometime soon. In the mean time you might take a look at somewhat related efforts in https://github.com/grpc-ecosystem/grpc-gateway/pull/323 and https://github.com/grpc-ecosystem/grpc-gateway/pull/305. Also, please sign the Google CLA. Thanks!. @TamalSaha thanks for your contribution. This is looking great -- can you satisfy the linter by adding a comment to DefaultHeaderMatcher ?. I wonder if we should expose the direction to the header matcher?. @TamalSaha I think those names are fine.. https://github.com/grpc/grpc-go/blob/master/Documentation/grpc-metadata.md FWIW. LGTM . @sagikazarmark can you clarify further here for anyone that might run into the same thing?. @borneq thanks for your issue report. It appears lnd uses an outdated protoc invocation. I've opened https://github.com/lightningnetwork/lnd/pull/169 to solve this problem.. This has been fixed upstream.. @tgulacsi thanks for your comment and for publishing your work there. It's not immediately clear to me how/if a SOAP bridge is appropriate in this project. I imagine with some hinting information a custom marshaller could get you most of the way there. Please propose enhancements to this project if there is something limiting your ability to implement this.. @TamalSaha since it's a quasi-breaking change with the runtime.AnnotateContext signature change I'd like to get it in out in a subsequent release. We can turn around and get a 1.3.0 out fairly quickly.. Definitely support this.. @NeoCN thanks for your interest. Do you think you could take a crack at this?. We unfortunately missed upstream protoc-gen-go moving to stdlib context by a few hours: https://github.com/golang/protobuf/pull/275\nHence the build failure.. I hope this can land soon as it's going to generate confusion until it does... FWIW this got rolled back upstream, will monitor. https://github.com/golang/protobuf/pull/326. we should follow upstream and wait for 1.10 here: https://github.com/golang/protobuf/pull/326#issuecomment-335977324. @ezotrank did you mean to close this?. @mrt181 thanks for your issue report. I think the missing step is that we assume you are operating somewhere within your GOPATH. Would you be open to opening a documentation pull request?. Thanks Martin!\nOn Tue, Apr 4, 2017 at 8:45 AM Martin notifications@github.com wrote:\n\nwill do so later today\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc-ecosystem/grpc-gateway/issues/346#issuecomment-291541873,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAPicR1Km4bJ-61TejClqU7bHKOP3mRks5rsmWMgaJpZM4MzB1H\n.\n. @mrt181 still need a CLA sign here. @mrt181 thanks! there are a few outstanding comments/suggestions.. @mrt181 thanks for the great documentation contribution!. @maxkondr you can use stackdriver trace if your workloads aren't in google's cloud.. @MalteJ thank you for your issue. I don't think anything prevents you from using tls.RequireAndVerifyClientCert as your ClientAuth parameter in your http server. This project doesn't mandate anything with regard to tls configuration.\n\nIf using mutual auth (*http.Request) should have the TLS.PeerCertificates slice populated which would allow you to inspect the tls configuration used for the connection. It would be great if we could provide an example of showing how to take that information and put it into grpc metadata.\nWould you be willing to contributing such an example?. @utrack thanks for your contribution! I agree with your assessment. Have you signed the google CLA?. @sagikazarmark no.. we shouldn't be inconsistent here. Do you think you could attempt to write up a diff implementing this?. This is currently as expected, see discussion on https://github.com/grpc-ecosystem/grpc-gateway/pull/242. @qianlnk bizarrely awesome! can you fix CI and sign the CLA?. @AlekSi thanks for the feedback. We are probably due to cut a new release.. we don't currently have a more \"e2e\" test to show this context propagation over an actual call so this wasn't caught in test. I'd like for us to have that but also want to get a release out that is compatible with upstream grpc-go.. adding 1.2.1 changelog. It was posted in haste. Thanks.. @aaronjwood for clarity, grpc-go introduced an api deprecation and we adopted this in 1.2.2 -- use 1.2.0 if you need to use an older version of grpc-go. The answer is likely update your version of grpc-go.. For posterity's sake (and googlebot), this sort of issue would likely render as:\n```\ngithub.com/your/sweet/project/vendor/github.com/grpc-ecosystem/grpc-gateway/runtime\nvendor/github.com/grpc-ecosystem/grpc-gateway/runtime/context.go:97: undefined: metadata.NewOutgoingContext\n```\nSomewhat relatedly, these sorts of errors likely mean you need to re-generate your .pb.gw.go files with a newer grpc-gateway:\nprotos/servicea/rpc.pb.gw.go:614: not enough arguments in call to runtime.AnnotateContext\n    have (\"golang.org/x/net/context\".Context, *http.Request)\n    want (\"golang.org/x/net/context\".Context, *runtime.ServeMux, *http.Request)\nprotos/servicea/rpc.pb.gw.go:616: not enough arguments in call to runtime.HTTPError\n    have (\"golang.org/x/net/context\".Context, runtime.Marshaler, http.ResponseWriter, *http.Request, error)\n    want (\"golang.org/x/net/context\".Context, *runtime.ServeMux, runtime.Marshaler, http.ResponseWriter, *http.Request, error)\n. @aaronjwood perhaps a valuable documentation addition would be a small vendoring guide. Would love to see some shared tips!. @aaronjwood that'd be awesome!. I opened https://github.com/grpc-ecosystem/grpc-gateway/issues/365 to track.. @charles-crain thanks for your report. Can you supply an example request for increased clarity here? Even better could be a failing test case.\nhttps://github.com/grpc-ecosystem/grpc-gateway/pull/314 is related, FWIW.. @charles-crain thank you for your issue report. That's an unfortunate divergence there. One solution might be to add protobuf WKT support in the swagger generation. Perhaps we can be smarter.. @ilius are you seeing that behavior if you supply a literal value instead of \"tokenCookie.Raw\" there in your example?. ci failure. @Lantame thanks for your contribution. This looks like a good change. What do you think about adding a small test that verifies this behavior? Also, have you signed the CLA?. @Lantame thanks! can you comment again to try to kick the cla bot into action?. I'd love to get this into wiki/docs.. I'd like to have this exercised under test.. I opened #362 previously and this may fit within that definition.. @achew22 I think that's acceptable.. @kazegusuri thanks for this! this looks great. It looks like your generated swagger/openapi is a bit out of date.. While we don't publish benchmarks I know of several folks putting moderate traffic through the gateway. It's not a very complicated piece of software once you have it configured.\nIf under any evaluation you find any deficiencies that should be addressed please do open issues so they can get attention. Thank you for your interest!. @boosh thank you for the issue report!. @nlamirault can we consider this issue resolved?\nperhaps these common gotchas could go into a wiki page?. +1 on OutgoingTrailerMatcher. I think prefixing grpc- headers is unnecessary.. @TamalSaha thanks for the patch.. some files need to be re-generated.. @wrrn thank you for your issue report. @TamalSaha hopped on this at https://github.com/grpc-ecosystem/grpc-gateway/pull/387. I'm referring to the fields that are encoded on the errorBody type. This is addressed by #378, my mistake.. @achew22 master tests are currently failing due to this.. refs #400. @AlekSi this is waiting on some additional test coverage. I've been bogged down with ${dayjob} and haven't written said tests. This could land sooner if those were contributed.. I think adding a convert_test.go file would be appropriate.. @brandoncole is just omitting operationId a reasonable solution? only include it for the first?. @calbach thank you for your issue report. Does openapi spec have the descriptive power to differentiate between those two?. Thanks for your contribution. Have you signed the CLA?\nCan you include a test demonstrating this behavior change?. See #570 . This is replaced by #634. Thanks for your contribution! master has a build system fix, can you please rebase this?. What is the reason for this proposed change? Generally we should keep the exposed api as small as possible and it's not clear why this would be desirable. As is I would reject this change proposal.. @peterebden thank you for your contribution, have you signed the CLA?. @peterebden thanks, for whatever reason the cla bot needs a comment to kick in.. @peterebden ping. @ilius thanks for your contribution!\nCan you elaborate on this use case a bit? Proto files are / should be parsable in a number of languages.. @AlekSi thanks for your contribution! Have you signed the CLA?. The bot needs a comment to kick in. . @hexfusion Thank you for your contribution!\nCan you describe your use case?\nThe current check for simple lack of presence probably should be a little more strict, additionally this would need tests and documentation to be accepted.. @MaerF0x0 a documentation addition for this would be /super/ helpful for other folks down the line. Please do!. Thanks for your contribution! master has a build system fix, can you please rebase this?. I think this would be a good documentation topic as some folks are\nuncomfortable going over loopback.\nOn Fri, Sep 8, 2017 at 10:23 AM Andrew Z Allen notifications@github.com\nwrote:\n\nHave you tried creating your connection over a unix socket? Untested\npseudocode to follow:\nconn, err := grpc.Dial(addr, []grpc.DialOption{grpc.WithDialer(net.DialTimeout(\"unix\", addr, timeout))})// Error handling\ns.RegisterCarHandler(ctx, mux, conn)\nWhat happens?\nIt looks like grpc may also support grpc.WithNetwork(\"unix\") which would\nbe even simpler (but wouldn't have support for a timeout).\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc-ecosystem/grpc-gateway/issues/451#issuecomment-328164455,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAPiacNQkAH2XlBwbb600raGhBBUXjeks5sgXfxgaJpZM4PLCli\n.\n. Thanks for your contribution! master has a build system fix, can you please rebase this?. Thanks for your contribution! master has a build system fix, can you please rebase this?. Thanks for your contribution! master has a build system fix, can you please rebase this?. @theRealWardo sorry, can you do so again?. I'd expect this to come with greater test code change/addition.. Thanks for your contribution! master has a build system fix, can you please rebase this?. @AlekSi have you signed the CLA?. as per https://github.com/grpc-ecosystem/grpc-gateway/pull/434 this contributor has signed the CLA. Yes my proxy relies on newline separation.. Yes.. For 1.8 (we should up it to 1.9) ci fails if generating the examples causes a diff. Please regenerate examples and push again. Could you also rebase and get rid of the merge commits? Thanks!. There's still a small difference:\n\n-   \"gopkg.in/go-resty/resty.v0\"\n+   \"github.com/go-resty/resty\". Yes this was a mistake. Thank you.. Release 1.3.1 has been tagged.. Thank you for this report. These both appear to be simple omission bugs. Do you think you could write a small patch for this?. Thanks. Will review. . Thanks for your contribution.. Ideally this would be covered in test but this is a small enough change that I\u2019ll approve.. Hm, oddly hitting an infinite recursion issue when trying to run coverage.. I wanted/needed it for coverage output consolidation. (-coverpkg=all). @domgreen thanks for your report. If that behavior changed between 1.3.0 being tagged and 1.3.1 then it should not have been included in the point release.\n  . Very cool! My read (haven't combed the diff yet) is that this is just an alternate http.proto specification avenue ala https://cloud.google.com/endpoints/docs/grpc/transcoding#configuring_transcoding_in_yaml. @amaskalenka sorry about the delay here, you'd have to rebase this before review.\nI don't understand why the existing methods are insufficient. Could you explain in more depth?. I support this. Looking forward to a proposal.. Code looks LGTM. My only comment is that I'd like a test case that demonstrates oneof population from body.. If this is the only issue from using gogo types we should absolutely get this fixed.. @johanbrandhorst thanks for this, can you rebase and resolve that test conflict?. Sorry I missed your comment. The failure does appear to have been unrelated. Please resolve the conflicts here and let's get this in!. To add ,proto3. @achew22 I'll let you complete review and merge. I think this proposal is sound and I like the flexibility it provides. Should the return types be proto.Message?\n@achew22 @tamalsaha thoughts?. I thought there was an issue around having exact control of the raw http response body but I'm not seeing it now. I agree this is a use case that should be straightforward. . Thanks for your contribution! The upstream issue appears fixed, can you rebase to trigger another build?. Looks like same intermittent failure on golint.. :+1: - CI failure appears to be intermittent issue with installing golint.. @stevenroose that's a great idea! . @JaxSONG thanks for your report. Could you include a little more detail on how you're sending your request? Perhaps supply an example script?. @lps0535 thanks for the report. This is certainly something we should expose as a param on the generator.\nWould you be interested in taking a crack at this? The entrypoint is https://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-swagger/main.go#L26. @co3k thanks for your contribution, can you rebase this so CI runs again?. @ivucica do you want to chime in here?. @ivucica do you have thoughts here. The \"grpc.gateway.protoc_gen_swagger\" strings in protos will be a concern here. (I'd propose grpc.gateway.openapi, FWIW).. Those are great considerations. Thanks! I think for most use cases polling would be sufficient. A control protocol could be exposed to allow triggering of reloads or submissions of new descriptors. In terms of startup time, depending on design, descriptor loading could probably be parallelized and perhaps you could service requests for one service even while others are still loading. Thanks for your thoughts and experience here!. I think going the Gopkg route as a temporary measure makes sense. @johanbrandhorst can you take that on?. Can you share the exact code that is executing? The protocol invocation?. Thanks for your contribution! Can you look into the build failure?. my intent was to block until the message is sent by the client as a sort of middle ground between just ignoring errors and interrupting the stream on any error. This was written mid-migraine so I'm happy to admit it's not as thought out as it could be. Open to suggestions.\n. This is for the many messages situation (e.g. rpc Chat(stream ChatMessage) returns (stream ChatMessage)). Before this change all Send()s have to run before Recv() gets called. by putting Send() in a goroutine concurrent Send() and Recv() calls can be made.\n. i was following existing style from the 'client-streaming-request-func' template.\n. I did that to allow the goroutine to start so CloseSend would be called. would you prefer that be done explicitly and return before launching the goroutine?\n. @achew22 swagger spec doesn't address streaming endpoints (yet) there's some discussion on the openapi repo.\n. @yugui I was aiming for code parity with the other handling code\n. @yugui thoughts on doing an io.EOF check before calling HTTPError there?\n. @achew that would define a new f instead of assigning to the one created previously.. you should document this difference and spell 'Middlware' correctly.\nPerhaps instead with would introduce a more general WithOptions call or extend Register{{$svc.GetName}}Handler to support variadic options functions.. this is grammatically kind of awkward.. We should document this in a distinct section and keep the original example simple.. this file might more appropriately be named something more general in the context of attaching additional annotations to methods.. Feel free to amend and push . Why are these additions necessary?. I accept the first one, for the others added (grpc,context) I would prefer to tell users to 'go get .' after generating code.. would it be appropriate to place windows instructions in its own section?. how about we recommend that folks work inside of GOPATH over moving files after gen?. let's recommend go install instead. I would prefer a WithHeaderMatcher Option that accepts a func(string) bool to both support more scenarios but also to have a smaller api surface area.. this comment should elaborate a bit more. nit: can we call this metadataAnnotators?. Thinking further, perhaps the signature should be func(string) (string, bool) to allow transformations in addition to simple predicate filtering.. I'm just taking verbatim https://github.com/skywinder/github-changelog-generator output here. It's not clear why it detected the different date.. I mean to something like:\ntype HeaderMatcherFunc func(string) (string, bool)\nfunc WithHeaderMatcher(fn HeaderMatcherFunc) ServeMuxOption {\n    return func(s *ServeMux) {\n        s.headerMatcher = fn\n   }\n}. I think we should allow just one -- the semantics of multiple are a bit unclear and it is simpler to have one.. Similarly, I think we should allow just one metadata annotating func.. I like that. @TamalSaha thoughts?. these are dropped in as 'runtime.Timestamp' calls via ConvertFuncExpr. I was considering that but figured this code was well-tested in jsonpb. can we instead do something like var fqmnCache = map[*descriptor.Registry]map[string]string so we can avoid the extra 'last' concept and just compute the cache once per registry seen?. can we do $(npm bin)/gulp?. it's not clear that supporting this far back incurs much cost. Can you elaborate? What could be considered controversial?. ",
    "ceram1": "I'm currently working on it, and do you have any idea about api reponse status code?\ngrpc-gateway/runtime#HTTPStatusFromCode returns http status code from grpc error code, but I can't get error code from doc generator.\nHow about defining all?\n. + Some protobuf file to declare swagger fields are required.\n. One additional note.\nGoogle discovery doc seems like a fork of swagger without error codes.\n. @philips Sorry for delay.\nI uploaded on ceram1/grpc-gateway@3fcd9e2\nIt's just a backup commit, as I could not finish swagger support.\nI used this command to test this, from $GOPATH/src/github.com/gengo/grpc-gateway/\ngo install ./protoc-gen-swagger/ && protoc -I. -I$GOPATH/src -I$GOPATH/src/github.com/gengo/grpc-gateway/third_party/googleapis  --swagger_out=. examples/a_bit_of_everything.proto\n. @gosuri I can't do it more, but it will not be hard to build a google discovery document generator.\n. ",
    "gosuri": "hey @ceram1, great job on swagger spec! How's it coming along?\n. Gotcha! No worries :)\n. ",
    "achew22": "I think this can now be closed\n. Just doing a little bit of tidying. I hope that your question was answered in a way that let your project continue. If you have any more questions feel free to open another issue.. You might consider using Swagger for this. Swagger support was added with https://github.com/gengo/grpc-gateway/pull/68\n. I have been on vacation for the past week. I intend to start taking a look at this in my spare time over this week.\n. @yugui, I have done a bit of cleanup on this PR. Just for reference, once you're happy with it I would like to squash it into a single commit, so don't just hit the merge button :smile:.\nI think your suggestion about using a parameter object is a great one. I looked at the docs and agreed with you when I was writing but I couldn't find a compelling way to do union types in Golang. Do you know what the best way to do a union type is? That seems to me like the only way to do it unfortunately.\nI have updated the branch with changes to make it reuse the existing protoc-gen-grpc-gateway instead of my clone. Turns out almost all the stuff I wanted was there.\n. The thing I'm stuck on right now is that you can import arbitrary protos into your api. They don't necessarily have to be included in the file that you're currently compiling. My plan thus far had been to use \"#/definitions/{{message.FQMN()}}\" for all the definitions. This is great, except when it comes to iterating over all the messages, I get stuck because they may not be in the file.\nRight now I'm expecting that I will have to create a registry of messages, like you did, then recurse through all the fields creating a set of messages that have been seen. I was planning on using a hashmap from FQMN to the actual message object then in the stage where I generate the definitions I can just emit each entry as a definition. Looking at the go code it solves this by importing 1 layer deep and then using the protoc-go library's output to solve the problem of transitive dependence, but you might have a cool trick up your sleeve so I thought I would ask. \n. Now I remember why this was hard! Field [https://godoc.org/github.com/gengo/grpc-gateway/protoc-gen-grpc-gateway/descriptor#Field] has a message attached, but it isn't the message I would expect. It is the message of it's parent. This is, no doubt, useful but not in the context that I need it. Lemme go dig in to see if I can figure out how to add a FieldMessage field to the Field struct. Meta-programming is hard!\n. After a bit of playing, I just plumbed the registry through to the generator and now I'm able to resolve these. Thanks for writing that!\n. Sorry, I accidentally closed this by pushing your master to github. It's all fixed now.\nI think this is getting a lot closer. I have created a non-streaming example file which is the a bit of everything with the word streaming removed https://raw.githubusercontent.com/achew22/grpc-gateway/master/examples/examplepb/streamless_everything.swagger.json\nIf you want to see how their API viewer sees it, check out http://petstore.swagger.io/#/ and type that URL in at the top. If you want to see what linter checks it is failing you can go to https://editor.swagger.io and do the same thing.\nThings that still need to be done:\n- [x] EmptyProto is still an interesting edge case. Right now it is not being included in output and I haven't figured out why.\n- [x] Even if empty proto is fixed in the swagger codegen to emit an empty dictionary, their editor will still barf on it https://github.com/swagger-api/swagger-spec/issues/452\n- [X] Repeated fields need to be marked as arrays\n- [X] Theoretical support for enums -- I couldn't get enums to be loaded in the params object. What I have should handle them properly.\n- [x] Parameters should not all be typed as \"String\". They have type information and we should represent that.\n- [ ] More tests\n. Alright, I think that the swagger output from this generator is valid (in spite of the bug in their validator) and I am able to generate swagger json definitions for all the test cases that you provided/I created. I will follow up with some unit tests but I would appreciate if you could take a look at it in its current form. I didn't intend for this to take so long, sorry about that but it's all there for the most part now.\n. PTAL\n. Thank you so much for the review. This is my first bout with Golang and I'm happy to have someone reviewing it. I think I have addressed all your concerns and would appreciate if you could take another look at the code.\n. @yugui I can't find any evidence that any browsers actually support HTTP 1.1. How do you get access to trailers in an XHR object?\n. Will this have any affect on the field names in the swagger documentation?\n. LGTM\n. @peter-edge Really? That's great! Can you point me to some of those projects that are generating swagger from protos?\n. Generating all those files as a single output is, IMO antithetical to the way protoc works. If someone wanted to write a small script in go that merged the outputs of multiple swagger definitions (we have the structs already defined so it wouldn't be that hard), it would be a welcome contribution to the project.. @yugui I was very careful to make sure that it wouldn't cause name conflicts. I believe that the test cases demonstrate that. The thing I dislike about this patch is that you were iterating on your API you might create a new Foo proto when you have a preexisting one. That would change the name of both protos to be unique but break your preexisting code.\nI figured I would write this up and just see what people thought about it. Doesn't have to go in\n. I switched to doing it your way Yugui and I'm much happier. Thanks!\n. LGTM\n. I'm the author on those. I absently mindedly wrote \"file\" in there when I wanted to write %s. Thanks for doing all this cleanup!\n. @bluecmd, did you ever have any luck reproducing it from master? I'm doing a little bit of tidying so I'm going to close this but feel free to reopen this issue if you have more information.. Just doing a little bit of tidying. I hope that your question was answered in a way that let your project continue. If you have any more questions feel free to open another issue.. Unfortunately that is not something that I implemented. Fortunately I don't think it would be extremely difficult if you wanted to take a swing at contributing.\nThe comments are in the SourceCodeInfo_Location object inside the FileDescriptorProto when genswagger iterates over the files.\nIf you need more pointers than that please comment on this issue and I'll see what I can do.\nThanks for reaching out!\n. Kazegusuri,\nThanks so much for contributing to the project! It's contributions like this that make open source great.\nJust a few comments to get the ball rolling:\n-   I don't know if you saw that Travis CI runs automatically on pull requests. If you did you'll see that part of the build is to regenerate the .pb files on every patch. To get travis to bless your build you should update all your godeps , make clean and try building all the examples again then check in the results with your patch.\n-   You should also make a new example that demonstrates this new code in action. Put it in the examples directory and compile it (just like in step 1) to make sure that future changes to grpc-gateway won't break your code.\nThanks again\n. Ivan,\nThanks for contributing to grpc-gateway! Your patch looks interesting. I think that in a second the build is going to fail because you need to regenerate the examples. Could you go ahead and do that to satisfy the build monster? After that happens an example of this will be available on github on a public URL and we can create a link to test it out.\nLooking forward to helping get this merged!\n. My understanding of that test is that if you git add those files and upload it that it should start working. Would you mind trying that really quick since it shouldn't be too painful to try?\n. @yugui I have been playing with this for a couple of hours and I'm unable to get it to compile the same on tip vs 1.5.\nWould you be willing to take a look at that test and see if there is a better way to do it? Can we maybe whitelist the byte arrays or something? I'm not sure how to fix it\n. @yugui Would you be willing to propose that as a PR to the grpc-gateway repo? That way travis will fire on it and we can get it committed\n. I don't think it is worthwhile to do the .pb.go changes since everyone will have do them anyway. Looks like it passes on both 1.5 and tip. I LGTM'd your PR over there. @ivucica this should clear the path for you. Thanks for pushing back on me. @yugui thanks for your timely response!\n. LGTM \ud83d\udc4d \n. LGTM \ud83d\udc4d  Thanks for the quick fix\n. @tmc This seems to me like it might be a problem better solved by your build system. I have implemented it myself using https://bazel.build. I assume that the //go:generate directive would allow you to achieve the same result but I have no real experience with it so I can't be sure.\nI'm going to close this to try to get the bug list down to a single page but feel free to reopen if you have more thoughts on the topic.. Just as background, Maps represented as lists of key value pairs are a holdover from the days of proto2. The whole ecosystem is in a bit of a transitional phase right now.\nI believe this is a dupe of #79. The current JSON encoder (if my memory serves me correctly)  is doing things in the style of proto2. Changing the JSON encoder is kind of hard to do but I would welcome you to jump into the conversation on that other thread. \n. Looks like a PR that fixes this was merged. I'm going to close the issue now. If this isn't fixed feel free to reopen it or file another issue.. > First... someone, please confirm whether it seems like this is on the right track. I won't mind a round of code review :) @tmc? @yugui?\nI know I'm not on your list, but I'm happy to give you a review. I think you're on the right track. I would venture to say you're probably 1 or 2 back/forths on review before we can merge this.\n\nI want to move the demonstration from echo_service.proto to a_bit_of_everything.proto.\n\nDo it. I think that's a better place for this.\n\nI should totally email protobuf-global-extension-registry@google.com and get us four extension IDs for attaching metadata to files, methods, messages and services (patching swagger object, operation object, schema object in definitions object, and tag object, respectively). We should not risk future binary incompatibility in on-disk protos which may merely import annotations.proto.\n\n+1, emailing them is a great idea.\n\nI will squash most / all of the commits.\n\nYou're welcome to do that but the default when we merge is to squash rebase so if you wanna ignore that then you're fine.\n\nWhat can be punted to a future PR:\nProcess remaining OpenAPI v2 properties that have already been added to the openapiv2.proto.\n\nI would prefer if you omitted them from the openapiv2.proto file if there is no support for them.\n\nSupport remaining OpenAPI v2 objects ('security', 'security definitions', etc)\n\nThese would be nice but the perfect shouldn't be the enemy of the good (which this clearly is very good).\n\nDocument individual fields in openapiv2. (Or remove the TODOs, as the openapiv2.proto option definitions generally won't be visible/autocompletable in any sort of smart code editor. Hence documenting fields might be an overkill, and referencing OpenAPI is enough.)\n\nYep, that's a good idea. I think giving links to the OpenAPI definitions of these things (or copy/paste into the proto) would be good.\n\nUse existing 'deprecated' option on a method, not (just) OpenAPI field.\n\nThat's also a good idea but it can wait.\n. LGTM. Let's get the extension numbers reserved and merge it.\nThanks so much for doing this! Awesome work. @ivucica got the email issuing us the extension IDs. When you're ready please add the new IDs and we can merge this as soon as that happens.. I'm also in favor of the techniques described by @ivucica. Since grpc-gateway isn't going to be generating the entirety of your http server for you it is unlikely that we will be able to handle all cases. Oauth, to pick an example, is not going to be included in grpc-gateway, it is too complex and there are too many specific requirements for it to make a generalized form of it. Additionally, I know that people are already doing oauth based security on services by using interceptors that live outside the grpc-gateway codebase. Since it isn't a hard requirement that it be in grpc-gateway I think it should be in another repo. We have this today with @tmc's grpc-websocket-proxy. It is a simple 1 liner function call at handler registration and it makes things easy to handle.\nJust to demarcate the lines in the sand that I will be using to determine if functionality should be included in grpc-gateway proper.\n\nIs the functionality a part of the grpc to http spec? Internal\nIs the functionality possible to implement as an interceptor? External\nIs the functionality possible to implement as an interceptor with some kind of modification to the generated code (e.g. passing request context through the http handlers into grpc calls #265)? Internalize the functionality necessary to allow a hook to perform the action but externalize the functionality itself.\n\nDoes anyone have any opinion on additional tests that should be included in order to prevent goalpost moving? I'm happy to codify these in the README file if people think that's of value.. Just doing a little bit of tidying. I hope that your question was answered in a way that let your project continue. If you have any more questions feel free to open another issue.. Just doing a little bit of tidying. I hope that your question was answered in a way that let your project continue. If you have any more questions feel free to open another issue.. Yep, I think this can be closed. Thanks!. @tmc with #209 in that provides a template for handling the custom times.\nThe special types are as follows:\n- Timestamp \n  - Already in Swagger and OpenAPI\n- Duration (In neither Swagger nor OpenAPI \n  - OAI/OpenAPI-Specification#359\n- Any\n  - Should be serialized as {\"@type\": \"url\", \"f\": v, \u2026 } which I would guess is possible with the existing swagger spec\n- Wrapper types\n  - I'm not sure I totally understand the purpose of this. Is it just to provide nullability?\n- Struct\n  - Should be possible to implement as well\nDo you need any of the other ones?\n. There is a set of well known types that are special coded in the proto3 spec when they are converted to JSON. This is the spec for duration\nhttps://github.com/golang/protobuf/blob/master/jsonpb/jsonpb.go#L167\nhttps://developers.google.com/protocol-buffers/docs/reference/google.protobuf#google.protobuf.Duration. There is no need for it to be specified as https. It could be specified as http. You can edit the swagger output file by hand and serve it with that line saying just \"http\" if you don't want to bother to modify the protoc-gen-swagger binary.\n. swagger-api/swagger-js#841 is now closed. Closing this issue with the assumption that is fixed. If that's not the case please feel free to reopen the issue.. @yugui, I'm going to close this so that it isn't on the backlog any more. If you decide to pick this up again (or maybe I will) please reopen it.\nUntil then, for anyone who wants to do grpc gateway in Bazel, you should checkout https://github.com/pubref/rules_protobuf. @tmc additionally yugui has stated that she doesn't want to implement any features in here that aren't in the spec as defined by Google. Right now they don't have required fields. If their spec changes to support that I think we should implement this feature then.  Again, I'm going to close this just to tidy up a bit. If you decide to file an issue upstream with the google repo, would you please comment on here to indicate that you did it. I would love to track the progress and implement if their schema changes.. @yugui, can you speak a little bit about what grpc-ecosystem is and how it fits into the preexisting grpc project? Specifically I am interested in these questions\n- Will work on these things be done in a public way? On github?\n- What does governance look like?\n- What license will they be releasing future code under?\n- What will this organization do with patentable materials?\n- Is there a code of conduct for this organization? What behavior is considered acceptable or unacceptable?\n. @rlmcpherson, I'm going to mark this issue as closed. If there is any more information you need, please feel free to reopen it.\n@yugui, thank you for getting this one.\n. Rafael,\nThanks so much for sending in a patch! It looks like the build monster is unhappy because your change modifies the output for the example. grpc-gateway has the examples folder checked in and verifies that there is no difference between the checked in version and what comes out during CI. It looks like your change also needs to be rebased onto the latest master.\nThanks again!\n. @rgarcia are you still interested in getting this merged?\n. @rgarcia, would you object to me taking over this PR and applying the changes?\nNote to people following along, if I haven't pushed a PR by Friday assume that I've fallen in a black hole and you can take a whack at it if you want\n. @igateno If you're still experiencing this, please reopen it or file a new issue with more information but I'm going to close it for now.\n. Can you confirm that protoc-gen-grpc-gateway is in a folder that is on your path?\nWhat is the output of which protoc-gen-grpc-gateway?\nprotoc -I/usr/local/include -I. -I$GOPATH/src -I$GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis --plugin=protoc-gen-grpc-gateway=$GOPATH/bin/protoc-gen-grpc-gateway  --grpc-gateway_out=logtostderr=true:. ./helloworld.proto\nThis presumes that your gopath is configured correctly. If not, make --plugin=protoc-gen-grpc-gateway= point to your actual protoc-grpc-gateway binary.\n. protoc-gen-grpc-gateway creates the grpc-gateway files, xxx.pb.go files are created by protoc-gen-go. Hope that helps!\n. Eran,\nThanks for your contribution to the project! Is there any chance I could get you to update one of the example .proto files to include the usage examples for swagger in examples/examplepb? Once you do that you should be able to run make clean examples. That will update the example swagger output in files like this.\nThanks again\n. @wing328, To my novice eyes this looks like it generates a swagger template that will do time. However, the documentation on getting times working is a little sparse so this is based on my generating code. Would you be willing to take a look at the generated swagger output and say if this is the correct approach?\n. @wing328 The lines in question are here. By my of the spec, specifically this section:\n| Common Name | type | format | Comments |\n| --- | --- | --- | --- |\n| dateTime | string | date-time | As defined by date-time - RFC3339 |\nIt looks like you can do one of two things.\ni. put the format as the literal token \"date-time\"\nii. put the format as a string representing RFC3339\nWhich way is the correct way to interpret that? Now that I'm reading it more closely I suspect that it might be the prior.\n. This looks good to me. I generated clients in a couple of languages and the generated code has these new fields typed as Date objects.\nCan you regenerate the examples to make the tests pass and then upload again?\nThank you!\n. Looks good to me.  Merged\n. LGTM\n. That looks like a really great start! Do you think you could turn that into a pull request and send it in? Thanks so much\n. @yugui What do you think of this?\n. I think that whitelist is very reasonable. Let's merge this. That closes bullet 1 and unblocks @christianvozar.\nI would like to think a little bit more about allowing users to add more to the whitelist. It feels like there is a good way to provide that flexibility and also to ensure the server remains as fast as can be.\nWhat do you think about proxying User-Agent as Grpc-Gateway-User-Agent? I don't think we should pass it through as the real User-Agent for the reasons you stated above but I think it would be reasonable to claim that in the header namespace.\nAs for more permanent headers, I think we should wait for there to be a use case that necessitates the enhancement. Those are valid concerns. Maybe we could open an issue preemptively that says \"please comment here if you are running into this?\"\n. @christianvozar it appears that there are tests failing on Travis. Can you take a look at those?\n. @christianvozar You are definitely right about gRPC being on HTTP/2, but gRPC-gateway is a gateway between gRPC and HTTP. gRPC-gateway runs a normal HTTP/1.1 server which can operate in both http and https mode depending on how it is configured.\n@edrex This looks reasonable to me, but I think I'm even more fond just omitting the schema from the output. @yugui, what do you think?\n. @edrex, you bring up very valid points. I don't know that there will ever be a way to solve this that makes absolutely every situation happy. It is too bad the swagger-ui doesn't have a \"baseUrl\" override on it.\nWould that be a better way to approach this? Is this really a bug in swagger-ui?\n. LGTM\n. Sorry, I thought I'd commented on this before. This is working as intended. gRPC gateway follows the serialization rules of proto3 and the proto3 spec says that int64, fixed64, and uint64 are all strings.\nThis is the case because of IEEE 754 Double precision which JS implements using a 52 bit mantissa (1 for sign, and 11 for exponent). This means the largest number that JS can represent without losing precision is 2^52. Try it out:\n``` javascript\n\n(Math.pow(2,53) + 0) == (Math.pow(2,53) + 1)\ntrue\n```\n\nSo, rather than pretending that Javascript can handle it (and lose data) proto3 prefers to have the client interact with strings. Fewer footguns == better libraries.\nIf you want to interact with int64 as numbers, there are a bunch of JS bigint libraries out there that you can use to parse the string, perform whatever operations you want and get back a string. That would probably be a better way to handle this.\nNote that int32 doesn't suffer from the same issue. If you don't have the pressure to operate on numbers > 2^32 that might be a good fix.\n. Unfortunately, that is not the case. Let's walk through a simple example and see what happens.\nHere is a non comprehensive list of my assumptions:\n1. You are using grpc-gateway\n2. You're using a fairly recent version of grpc-gateway\n3. You haven't overridden the serializer\nOkay, let's make a proto like this:\nmessage TestProto {\n  int64 a = 1;\n  int64 b = 2;\n}\nNow let's send some data through grpc-gateway using that proto. [Code left as an exercise for the reader] You should get out on the other side assuming you transmit TestProto.newBuilder().setA(1L).setB(2L).build() (using Java syntax but any language would be fine).\njson\n{\"a\":\"1\",\"b\":\"2\"}\nAnd when you transmit stuff back to the server\njson\n{\"a\":\"1\",\"b\":\"9007199254740993\"}\nOn your server you should get a response that is an int64 and contains all the bits of precision without losing anything. If you get something else, can you please reopen this so I can take a look at it because it is a bug.\n. I don't know. It's not specified behavior. Wanna try it out and see what happens? It might work, but I can't promise even if it does that it'll work forever.\n. Cool! Then go for it. If it works that's wonderful, but don't forget the aforementioned imprecision issues.\n. If you really want to have the type as number and you're 100% sure the data isn't ever going to be > 2^32, why not use an int32?\n. Ah, have you considered using a google.protobuf.Timestamp? That will get detected by the swagger generator and you'll be able to interact with it directly as a Date object. What do you think about that?\n. Timestamp is special and very nice. It's actually specified on that same page we keep sending back and forth to each other :wink:.\n\nTimestamp   string  \"1972-01-01T10:00:20.021Z\"  Uses RFC 3339, where generated output will always be Z-normalized and uses 0, 3, 6 or 9 fractional digits.\nI just switched my thing to use it a couple of days ago. Works like a charm!\n. Given @mattolson's latest comment closing. Feel free to reopen if you have more issues.. @kassiansun, maintainer here. Sorry it sounds like you're frustrated. That's obviously not what I want.\nFirst, if you want to fork, please do. This code is open source so that anyone can take it, change it, and put their own spin on it. However, multiple incompatible implementations is not my favorite outcome. Could you help me understand what problems you have and what you would do in your proposed fork? Maybe that's something we could support in mainline either directly, through a plug-in/middleware/flag.\nLooking forward to hearing another opinion!. This looks good to me. @yugui do you know what's up with the cla bot?\n. Thanks so much for sending your PR in. Do you think you could update the example protos to include a \"PATCH\" and regenerate the generated files? I think \"make examples\" is all you need to do for that.\nThanks! I look forward to having patch support.\n. @awalterschulze, at this point I don't use gogoproto and so it isn't at the top of my priority queue. It is definitely a correctness issue, but I won't have the cycles to fix it. I don't think it would be extremely difficult to fix in the generator.\n\nEnhance the flag parsing to handle the additional M.*=.* args.\nEnhance the registry to register these mappings.\nUse those new mappings when rendering (there are a bunch of ways to do that and I'd leave it up to the implementer to decide which one to use)\n. I haven't had any need for it yet and I like to keep things as close to\nstock as possible unless I have a good reason to deviate from the norm.\n\nWhen that time comes I will definitely investigate, but right now I'm doing\nfine without.\n. I think I'm comfortable with your first suggested solution. Let's not do anything without getting thoughts from @yugui. What do you think?\n. This is now fixed. Ah, I see what happened here. I was mistaken to close this -- my bad. Unfortunately there hasn't been progress.\n@ornithocoder, if you were to extend the work and fix the tests I would be more than happy to merge it as soon as it goes green. It's just a little bit of test cleanup required.. I would suggest sending a pull request to googleapis/googleapis to clarify the wording in whichever way is easiest and see what they do. If they accept then implement that, else prefer the other.\n. I think this has stalled out. Please feel free to reopen if you have a strong opinion about this and would like to revisit it... @tmc, is this still valid?. I closed the issue associated with this and will do the same to the PR. Please reopen or resend the PR if this is still a thing you want to pursue :smile:. @tmc I think I'm supportive of merging this and doing another minor release. What do you think of that?. I rebased this to master and it appears to be failing tests. If someone wanted to fix up the failing tests I would be happy to merge something like this (+ tests). Unfortunately I don't have any way to verify your cla status. Could you send a PR? We have a bot to verify stuff if you send it in that way. @nycdotnet strong yes. https://github.com/grpc-ecosystem/grpc-gateway/pull/546 is where I started the effort to do this so you can get a sense of the scope. We also got blocked by the fork of swagger-generator=>openapi-generator waiting for their first release (3.0.0 came out on the 1st of the month) so now that they have settled down a bit I think we may be able to continue. How can I help you with that change?. Sorry for the slow response. I would have sworn I replied already.\n\nThe PR you linked is very extensive. How much of that PR would you say is directly related to this issue?\n\nUnfortunately almost all of it. I started that change as \"emit default values\" but then when it got to be so large, I just said \"f-it\" I'm going to break a few more things in the PR. Unfortunately I never got around to actually doing it.\n\nI understand that since this qualifies as a breaking change, it's a good opportunity to break all the things, but for a first PR I think it might be tough to jump-in midstream on that. Do you see v2 and #546 as the only way forward?\n\nA major version bump is definitely required. This is a major API breaking change to start emitting default values. I think it's the correct behavior and we should have done it earlier, but it is hard (as you may have noticed).\nAre you still interested in helping out given the scope? It might be possible to do it in a few smaller steps if you're willing to take some of those on.\n. @tmc, Do we need to do more to upgrade than change the hardcoded compatibility string? I didn't see anything in there listed as a breaking change \n. It feels like a test that is testing something outside the control of this project. It does, however, add a nice e2e test to the repo. Honestly I don't even remember this commit going by which explains my total lack of understanding in my earlier comment.\nLet's ask ourselves the question \"What is this testing and can it be tested in another way\" specifically without bringing in a dependency on swagger-codegen\n. I would also be supportive of switching over to &JSPNpb{OrigName: true} as the default. This is another breaking change. Maybe we should wrap this up along with the context change and do a 2.0 release?\n. Is there a strategy for making this the default in the future or will it remain an optional feature?\nShould we have a generated example with this turned on?\n. @tmc what information are you trying to get out of the HTTP request context that isn't available now?. LGTM. Feel free to merge when the CLA bot is happy. @joeblew99 Thanks for your interest in the project. Unfortunately it doesn't look like anyone has the time/energy combined with the need to implement this. The two solutions (gRPC and JSONRPC2) are similar enough that, for my use case, I'm happy to just use gRPC. If you're interested in taking that task on, I would personally be interested in seeing your implementation. This project may not be the right place for it. Feel free to add me to a PR in a new repo if you decide to take this task on.\nI'm also going to close the issue to help keep things tidy but I hope to see you around in the future.. @tmc This looks like it's good to go for me. Do you object to it?. @yugui I think it is about time that we cut another release for grpc-gateway as well. If you would like I can tag and push a release annotated with a changelog. Would you like me to do that?. I like it as an undocumented feature. @msample could you add a couple of tests to verify that it does what you need when the flag is set? . I want to use this so I'm going to squash and merge. Thanks for your contribution @JohanSJA . Just doing a little bit of tidying. I hope that your question was answered in a way that let your project continue. If you have any more questions feel free to reopen this issue.. This is addressed by #199 which I've completely dropped the ball on. Unfortunately I don't have the bandwidth to finish up the PR right now. If you wanted to patch that in and extend upon it I would welcome the PR.\nClosing as this is a duplicate of a few other issues.. @tmc  thanks for doing this. LGTM. Also, LGTM. @jmzwcn That sounds like an interesting idea. What do you think about putting together a PR where you implement some of those ideas? I'm going to close this issue and we can pick up the conversation in your PR.\nLooking forward to your contribution!. The error is main_test.go:118: Test 0: file misparsed, expected 'stdin', got '-'. That can be fixed by editing https://github.com/grpc-ecosystem/grpc-gateway/blob/b15a928074001d2729c56cdac319c907648856e6/protoc-gen-swagger/main_test.go#L25 to be - instead of stdin.. @rgarcia, can you confirm a couple of things for me please before I merge this?\n\nYou are the author of the commits that @t-yuki extended upon\nYou're comfortable with the additional changes made\nTo the best of your knowledge you've signed the CLA (https://github.com/grpc-ecosystem/grpc-gateway/pull/199 implies yes but I wanna double check). Huh, foiled by the CLI bot. Even on the command line I can't push past this. @yugui can we change the CLAbot to being advisory or non-blocking or whatever github calls it?. We could be really tricky and merge the other PR then this one quickly as not to break anyone, hopefully. I just sent an email to yugui (not @'d on purpose cause I just sent her an email) about modifying the submission rules. Let's see how that plays out. Meet back here in 24 hours?. @doubleyou, Thanks so much for showing interest in the gRPC Gateway project. It's nice to know there are people out there playing with these ideas.\n\nUnfortunately, I have almost no knowledge of the intricacies of the python import system and don't have any ideas on how to address your specific problem. Sorry. It sounds like this is a problem that would be better addressed by the Protobuf project. Maybe you could open a bug over there and add a reference to this one?\nI'm going to close this issue since it is related to the actual protobuf compiler's python code generation strategy. Good luck out there and if you find a great solution, please come back and leave a note explaining how you solved the problem.. @c4milo, seems like an interesting idea. To do this you will have to modify the upstream proto that this project implements. We have decided that we don't want to modify it locally to prevent an accidental fork from the spec.\nIf you decide to open a dialogue on googleapis/googleapis about this please add me on the issue. I would love to see how things play out.\nI'm going to close this until the upstream proto is changed.. @arun0009, seems like an interesting idea. To do this you will have to modify the upstream proto that this project implements. We have decided that we don't want to modify it locally to prevent an accidental fork from the spec.\nIf you decide to open a dialogue on googleapis/googleapis about this please add me on the issue. I would love to see how things play out.\nI'm going to close this until the upstream proto is changed.. @arun0009, can you be a little bit more specific about what output changes you would like to see in the swagger docs? Maybe you could put together a PR that has a first pass on some of the ideas?. Agreed, @johanbrandhorst. How can I help you get that PR in?. @shilkin, thanks for sending in your thoughts! I'm excited to see new ideas propagating through the grpc ecosystem. I have a couple of questions about this change as well. I spent some time thinking about your change in the context of a project I'm working on and had a few questions.\n\nIn my experience the ordering of options in protos is nondeterministic. In the arrangement you have right now it may not be possible to always get the same ordering of middleware from protoc to protoc. What do you think about the option having a repeated instead of having multiple instances of the option?  I think the ordering of a repeated is also nondeterministic but it is less nondeterministic than options (it breaks more tests when this gets changed but it is still a thing allowed by the spec).\nHow do you intend to handle different interceptor requirements for different environments? For example, I have tracing in production and disabled in development mode. How do you envision maintaining that functionality?\nCode generation is good, but explicitness is also a virtue. Putting the middleware in the .proto file feels like shifting an important part of code (as opposed to API definition) into the service definition.\n\nI really like the idea of having a better story for middleware, but I wonder if we can make a more idiomatic go interface for doing it. For example, in gRPC, grpc.NewServer takes an optional set of ServerOptions. These provide a very flexible point fo extending the gRPC server.\nAs a strawman example of what that might look like. What do you think about something like\n```\nfunc session(h runtime.HandlerFunc) runtime.HandlerFunc {\n  return func(w http.ResponseWriter, r *http.Request, p map[string]string) {\n    // get ssid from cookie and check if session is valid\n    h(w, r, p)\n  }\n}\nfunc ratelimit(h runtime.HandlerFunc) runtime.HandlerFunc {\n  return func(w http.ResponseWriter, r *http.Request, p map[string]string) {\n    // check custom rate limit for this handler\n    h(w, r, p) \n  }\n }\nfunc main() error {\n ctx := context.Background()\n ctx, cancel := context.WithCancel(ctx)\n defer cancel()\nmux := runtime.NewServeMux()\n clientOpts := []grpc.DialOption{grpc.WithInsecure()}\n httpOptions := []runtime.HttpOption{\n    runtime.Middleware{session},\n    runtime.Middleware{ratelimit},\n  }\nerr := gw.RegisterYourServiceHandlerFromEndpoint(ctx, mux, *echoEndpoint, clientOpts, httpOptions)\n if err != nil {\n   return err\n }\nhttp.ListenAndServe(\":8080\", mux)\n return nil\n}\n```\nOne of the things I like about this is that it gives us an obvious path forward for HTTP extensibility. You could use this not just for middleware but for getting monitoring hooks into the internals of the gRPC-gateway runtime.. @shilkin, with the merging of #336, I'm going to close this PR. The functionality you were looking for should now be possible :fireworks: YAY!. The logic behind not forwarding headers arbitrarily is that many people have some kind of internal header that does special stuff. An auth impersonation header is an example of something that\nI was under the impression that the logic was to prevent people from being able to insert arbitrary headers in the stream at least without the gateway author doing something to defeat the mechanism.\nIf you had an \"Authorization: token\" header and another \"Impersonation: $other_user_id\". Inside the network, you would expect the service receiving the call to process the request as $other_user_id assuming the auth token granted impersonation permissions.\nThe way I thought that was currently implemented was with a find and replace where it took Grpc-Metadata-x and made it just x in the request. That way you had the x- namespace of headers that were manipulable by the client and it removed everything else.\nEvidently my understanding is wrong. It looks like including a client side header of Grpc-Metadata-Impersonate would get you impersonation which is an... interesting result.\nI think @lucas-natraj's analysis is correct. As I read it I think the behavior of passing through Grpc-Metadata is a bug or possibly better viewed as an implementation artifact from days before we thought about headers.\nThere should probably be a whitelist, a customizable whitelist at that. I wonder if this is such a common usecase that we should make a really easy way to do it. We could make it pluggable with the current implementation as the default and then change the default later.\nDoes anyone have any thoughts?. +1, I think following the release cycle of Go closely will help keep down the clutter. This will represent a breaking change. Does that mean a 2.x series?. This is now available by a flag.. Not really required, but I assume the x/net/context package will eventually go away so we should proably just do it.. @ilius, did you have any luck regenerating the examples?. @srikrsna this would be a great thing to add in the docs/ folder if you're interested. That would be wonderful! Thanks. @tgulacsi what an interesting project! I'd never even thought of using protos to define WSDLs. Neat!\nCan you help me understand why you want this integrated into gRPC gateway directly? It doesn't appear to have any overlapping code and I think could be installed in parallel with a gRPC gateway server. What would the advantage of commingling the repositories over leaving them separate and composable?. Looks like you've created this already, cool! I look forward to seeing your project grow\nhttps://github.com/UNO-SOFT/soap-proxy. Feel free to force this over Travis CI. Lgtm. @tmc I think we should merge this. Could you clean it up a bit so we can do that?. An equivalent change has now been merged. Huh, this is strange. Especially strange given that it looks like we have code to handle it. I don't have an answer on what's going on here, but want to document what I have looked at\nDoesn't look like we are setting the title at all.\n~/Projects/grpc-gateway (master)$ grep \"Title =\" . -rn\n./protoc-gen-swagger/genswagger/template.go:638:                usingTitle = true\nCould it be that we are copying the struct from proto into the swagger types somewhere? I didn't think we did that and also It seems like the interfaces would be incompatible and disallow that but that's the best theory I have off the top of my head.. The parser has been updated a lot since this bug was filed and I think it's likely fixed if you wanna take another look at it.. I didn't actually run this, but I believe when you run the protoc command you have control of the output directory. To pick the example from the README.md,\nprotoc -I/usr/local/include -I. \\\n  -I$GOPATH/src \\\n  -I$GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\\n  --grpc-gateway_out=logtostderr=true:. \\\n  path/to/your_service.proto\nThe line that says --grpc-gateway_out=logtostderr=true:. has a . at the end which is a reference to the current directory. I think (again, I didn't test this) you can point it anywhere you want.\nGood luck!. @qianlnk I'm with @tmc on this. Super nifty! Could you also add some tests to verify behavior? This isn't a feature that I have need for and I'm worried we will accidentally break it in the future without noticing.\nThanks!. @writeameer, thanks for the kind words! I like this project a lot as well. The xml handling is a really interesting potential feature, but given the limited use case I'm reluctant to merge it unless there are very good tests for it. Without tests it's likely that it will be broken without anyone noticing. If this is something you need in your project, since it is a marshaller and therefore very pluggable, feel free to use it even though it isn't merged.. I did all those things (I think). It's a bug if they aren't working. Please file another bug if those aren't working in 1.3.0. +1 to e2e testing. Also LGTM.. LGTM. 2+1 to having a test. @tmc, what do you think about merging to get it fixed and then having @TamalSaha do a followup?. LGTM once CI approves, I think this better fits into #362 . There has been a lot of work on the swagger definitions since this was filed and I think it's likely that this is fixed. If you have more issues please reopen/file a new issue.. @alexleigh, looks like it is. Would you mind contributing a patch and helping out the community? It would be a great starter to get into the project. Just remember to run make examples afterward to ensure update the goldens.. @pcariel, unfortunately we don't support Google's Flatbuffers effort at this time in any way. Are you hoping to have the gateway translate from ProtoBuf to Flatbuffers or are you looking to enhance the interface description language for Flatbuffers to support RPCs?. @pcariel, this sounds like an interesting endeavour. Do you think you could get in touch with the Flatbuffers people to see if they are receptive to the idea and get back to me when you have all the details worked out?\nI'm going to close this so it doesn't linger in our backlog but please reopen if you have any more thoughts.. Can you try going through our tutorial one more time making small changes and verifying as you go? I think this may have gone off the rails in the dev stage. If you have more issues please reopen/file a new issue.. The semantics for PUT are that you will include the entire contents of the file. If this were going to be supported it is would have to be under the verb PATCH. @adematte I think that is the way to go. An auto populated fieldMask is, I think, the way to do this. That said, I'm not sure what the best way to go about doing that would be. Do you have a magically named fieldmask? Is there a part of the spec that allows for that that I've missed?. I see a few advantages to putting it in a header\n\nit will work even if the request proto doesn't have a FieldMask field embedded in it\nwhat do you do if there are multiple FieldMasks?\nIt doesn't pollute the exposed API to swagger. If you add a FieldMask, now your public API has an extra field that you have to explain to people doesn't do anything because it is overwritten by a proxy server.. If you I'm at the field mask from that generated Swagger definition that I\nthink that would be great. Feel free to send a PR if you want\n\nOn Thu, May 31, 2018, 17:59 Daniel McDonald notifications@github.com\nwrote:\n\nHi all, I'm looking to rehydrate the PATCH debate -- is there still an\nappetite for this? If so, I think I can provide development effort for it.\nThis is the current design I see:\nHTTP PATCH request comes in\nif the gRPC request message has exactly 1 FieldMask field:\nset the request FieldMask field based on HTTP request JSON body\nAreas that still need discussing:\n\nWhat to do about the generated Swagger file? Should it be modified\n   to exclude FieldMask?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc-ecosystem/grpc-gateway/issues/379#issuecomment-393718819,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AACRVs_ysvEo5oxLkm2ICXl_0ZB3zXaCks5t4IP8gaJpZM4NSbe1\n.\n. There has been a lot of work on the code since this was filed and I think it's likely that this is fixed. If you have more issues please reopen/file a new issue.. Looks like this resolved itself. Thanks @maxkondr . Can you try this again with v1.3.0? If you have any more issues, please open a new issue. @yugui, do you have a list of places where we have deviated from the spec? Sounds like you have some in mind. I've been trying very hard to be faithful to the spec and I woud like to know what I'm missing.. Pieter, this seems reasonable to me. Could you sign the CLA and add a test to verify the new behavior?. Generally I believe you should test it in the smallest size test that completely validates behavior. Since template.go is generating a string that is going to be compiled by the go compiler I don't know that we can fully test the functionality in template.go. Therefore, I think the test should go in integration.go. If you have a nifty trick for testing it in template.go I would love to learn.. Oh, I should also note that since you're modifying a template you will need to run make examples in the root directory to update all the example code with your change.\n\nThanks!. @chaitanya9186 Can you elaborate a little bit more. How do you mean \"python gateway code\"?. The code that is generated in that case is highly dependent on the rest of the code in this library. Without a complete rewrite of the core of this project I'm not sure we will be able to generate python code that works.\nThere is a project similar to the efforts here, grpc-jersey which does this for Java and Jersey. You could use these two projects as a template for doing something in python.\nI look forward to hearing about your work :smile: . LGTM\nWere tests failing previously because of this?. @uynap last time I played the protoc flag game was > 1 year ago so I'm working from foggy memories. I do think the problem is the way you're passing your flag into protoc. Could you try:\nprotoc -I${GOPATH}/src/google.golang.org/genproto/googleapis \\\n      -I${GOPATH}/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\\n      -I${GOPATH}/src/github.com/golang/protobuf/ptypes \\\n      -I${GOPATH}/src \\\n      -I. \\\n      --plugin=protoc-gen-grpc-gateway=${GOPATH}/bin/protoc-gen-grpc-gateway --allow_delete_body=true \\\n      --go_out=plugins=grpc:. \\\n      --grpc-gateway_out=logtostderr=true:allow_delete_body=true:. \\\n      ${SOURCEDIR}/proto/route/*.proto\nNotice that I added it to the --grpc-gateway_out flag. I think the syntax, again this is an old memory, is argument1=value:argument2=value:path_to_generate_your_files_in. . @TamalSaha, thanks for following up. Given that his sample came from a real makefile instead of the back of my head I would like to revise my question. Can you try this out:\nprotoc -I${GOPATH}/src/google.golang.org/genproto/googleapis \\\n      -I${GOPATH}/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\\n      -I${GOPATH}/src/github.com/golang/protobuf/ptypes \\\n      -I${GOPATH}/src \\\n      -I. \\\n      --plugin=protoc-gen-grpc-gateway=${GOPATH}/bin/protoc-gen-grpc-gateway --allow_delete_body=true \\\n      --go_out=plugins=grpc:. \\\n      --grpc-gateway_out=logtostderr=true,allow_delete_body=true:. \\\n      ${SOURCEDIR}/proto/route/*.proto. @AlekSi, if this is a thing that you need soon, I bet @tmc would be happy to have you clone this branch and write the tests so we can merge it.. @ajalab I think I was wrong to request testing on those methods and block. I'm going to merge this. If you have any more needs beyond this can you send another PR?. I think #515 sufficiently addresses the issue. Let's close it and if there are more things you would like to see in the error response let's open another issue. After all, the default handler now serves the included error, and you can override the error handler.. @anweiss, I think you can do it through the operation annotation\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-swagger/options/openapiv2.proto#L57\nExample that touches nearby fields but not the operationId\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/master/examples/examplepb/a_bit_of_everything.proto#L162. @boosh to answer your question (though it may be retorical)\n\nTBH I'm still not 100% clear on why Google recommend naming resources like they do. Anyone any ideas?\n\nNot speaking for the people who designed it, but as I understand it, idea is that you don't want your generators changing variable names on you willy nilly. Right now it is pretty easy to rename things to name2 and get a deterministic behavior. If you have a solution to this I would recommend writing it up in a PR with some tests so we can see what it does and how it modifies the existing code. That will help drive the conversation forward.\nI'm looking forward to some more discussion!. To be honest I think this is just one of those things where the gRPC expectation and the swagger expectation differ. I don't really have a good solution. As an example of the difference it could interpret /v1/{company_id=companies/*}/{team_id=teams/*}/{user_id=users/*} as /v1/companies/{company_id}/teams/{team_id}/users/{user_id}, but that doesn't really seem to express the same thing since the consumer would be receiving the (company|team|user)_id value without their matching literal prefix.\nI do not know what a great is.I would expect || field == \"id\" to not work, since it doesn't match team_id, but if you were to use a . to delimit it instead of an _ I think it might work.\nDo we know the exact definition of a resource name? I think getting clarity on that would be very useful. . In the context where you're registering your handlers you can keep a hold of the grpc client and pass it into the http route handler for your multipart form. Does that work for you?. Unfortunately, I think the reason people aren't jumping on this because there isn't a clear path forward. I just looked through the API spec defined by the Google http annotations and I don't see anything that maps into the concept of a multipart form upload. If there are parts of the currently internal API that would be useful to have exposed externally then that's a conversation I would love to have but I don't think this is the repo to do it in. I think the place to have this conversation would be in the repo that contains the spec we are implementing.\nI'm going to close this but please feel free to mention me on any issue that's opened in that repo. If you're interested in changing the grpc-gateway API to allow for something else, I think that should be done in another issue.\nClosing since this is essentially infeasible using the proto options.. @nirshirion I would like to merge this if it is possible. Would you be willing to sign the CLA?. @nirshirion, could you sign the CLA and add some tests so that we can merge this?. The CLA bot web UI is reporting that you have signed the CLA. Looks like we just need a test and we can submit this. I would love to have it in the project.. @mattolson, I don't think there is anything blocking us merging this except test coverage. @nirshirion, any chance we could have you come back and add tests to this PR? I would love to merge it!\n@mattolson, if you want to take a whack at extending this PR by pulling it into your workspace and writing the tests for it, I would also be happy to merge that.. I'm not sure what the correct behavior is here but it definitely isn't to crash. In your example if someone were to navigate to /, what would you expect the gRPC request to be since msg isn't defined? Just {\"msg\":\"\"}?. @abronan, if you want to fork @yugui's branch and take a stab at it I would be happy to help you get it past the finish line. I agree that oneof at the root would be a nice feature. It just takes a little bit of finesse . Looks like the CLA bot is happy now. Merging (sorry for the delay). If you continue having issues please reopen or file a new issue.. I think this sort of change would require a change in the http<=>proto spec. Please feel free to open an issue in that repo and CC me.. This seems to me like a really great starter project for someone who wants to get involved. It seems to me that the problem comes from nested messages. I think the way you would go about doing this would be to modify findNestedMessagesAndEnumerations to take a current path parameter and have it record its recursion as it goes down.\n@GeertJohan, would you be interested in taking this task on?. The parsing should be the same as for the gateway code. Can you elaborate on what more you would be interested in having than the link in our docs that says custom option?. You can always look at our kitchen sink for a fully fleshed out version of a proto. It should have one of everything.. I think subscribing to https://github.com/grpc-ecosystem/grpc-gateway/pull/521 is probably the way to go here. Is that PR what you were thinking of?. @boosh, right now we don't support this since we don't emit OpenAPIv3 definitions. You're welcome to monkey patch it, or to take this opportunity to extend on the work of Ivan and help us by adding support for the subset of OpenAPIv3 you're interested in using. If you want some pointers on that I'm happy to help!. We are aware of this and have attempted to make the switch along with grpc-go. Unfortunately without this projects dependencies being moved over, we can't move over. I'm going to close this in favour of older bugs. Feel free to subscribe to those to track progress.\n349 #343. Unfortunately I'm not sure of a better way without changing things in grpc-gateway. I think a case could be made that 403 isn't the right code for this (that is forbidden, right?) and I'd be open to changing it to something different. Could you send a PR?. It looks to me this information is already all exported inside the fileDescriptorApi which can be parsed into a FileDescriptorProto which has a file field and then a ServiceDescriptorProto available as \"service\". Can you help me understand what additional information you would need to get access to?. I dug into this a little bit more and discovered another kind of cool thing. Looks like you can call proto.FileDescriptor and pass in the proto filename and get back the ProtoFileDescriptor that you're interested in :smile:. Assuming your proto is called ping.proto and held in the directory foo, it would be something like this\n```\nimport \"github.com/golang/protobuf/proto\"\nimport \"github.com/golang/protobuf/protoc-gen-go/descriptor\"\nfd := &descriptor.FileDescriptorProto{}\nerr := proto.Unmarshal(proto.FileDescriptor(\"foo/ping.proto\"), fd)\nif err != nil {\n  log.Fatalf(\"Oops %s\", err)\n}\nfor service := range fd.GetService() {\n  for method := range service.GetMethod() {\n    for option := range service.GetOptions() {\n      log.Printf(\"Options: %s\", option.String())\n    }\n  }\n}\n```\nShouldn't take much more than that to get what you want. However, if you're wanting to do some serious code generation work, I would recommend writing your own protoc plugin. It's surprisingly easy. It is just a program that takes in a FileDescriptorSet proto on stdin and outputs a GeneratedCodeInfo (or something like that). This whole project is an example of not one but two ways of doing this! Get your hands dirty, it's really fun!. Right now grpc-gateway doesn't support streaming over HTTP/1.1 the way you propose. @tmc and I have talked about adding the ability to do streaming to grpc-gateway, but we haven't yet landed on how to actually do it while being faithful to the spec. We will definitely take this effort into consideration when we work out streaming support, but for now this is a low priority item for me. If this is something you'd be interested in taking on (or any other open issue/FR) I would love to help point you to the right place. I'm going to close this to keep our backlog clean-ish but feel free to reopen it/turn it into a PR if you would like to take a swing at it.. @erwinvaneyk, I think that would be a great way to approach this. Options defined in the proto would be great. @bamnet, would you like to take that on?. I believe you can do this now, thanks to Ivan. Thanks Ivan!. You can also use int32 if you want your data to be represented as an int still.. If your data is bigger than an int32 then probably not but it depends on your usecase. Talk around with your team and see if there may be another strategy for attacking the problem (or use a float). If you want to store a time, you should use the well known type Timestamp. This will correctly get marshalled to a Date object if you're using a swagger generated client.\nhttps://developers.google.com/protocol-buffers/docs/reference/csharp/class/google/protobuf/well-known-types/timestamp. To partially quote an earlier issue comment:\n\ngRPC gateway follows the serialization rules of proto3 and the proto3 spec says that int64, fixed64, and uint64 are all strings.\nThis is the case because of IEEE 754 Double precision which JS implements using a 52 bit mantissa (1 for sign, and 11 for exponent). This means the largest number that JS can represent without losing precision is 2^52. Try it out:\n(Math.pow(2,53) + 0) == (Math.pow(2,53) + 1)\ntrue\nSo, rather than pretending that Javascript can handle it (and lose data) proto3 prefers to have the client interact with strings. Fewer footguns == better libraries.\nIf you want to interact with int64 as numbers, there are a bunch of JS bigint libraries out there that you can use to parse the string, perform whatever operations you want and get back a string. That would probably be a better way to handle this.\n\nThat is why I am suggesting using timestamp for epoch, since it was designed to work around the issue of the 52 bit mantissa.. A float (or single as it is sometimes called) as defined in proto as a 32 bit sequence:\n\nSign bit: 1 bit\nExponent width: 8 bits\nSignificand precision: 24 bits (23 explicitly stored)\n\nSo it can only faithfully hold values up to 2^23, which is smaller than 2^52 no problem there. \"Single precision is termed REAL in Fortran, float in C, C++, C#, Java, Float in Haskell, and Single in Object Pascal (Delphi), Visual Basic, and MATLAB. However, float in Python, Ruby, PHP, and OCaml and single in versions of Octave before 3.2 refer to double-precision numbers. In most implementations of PostScript, the only real precision is single.\" Javascript does not have an implementation of single. \nWhile a Double is defined as:\n\nSign bit: 1 bit\nExponent: 11 bits\nSignificand precision: 53 bits (52 explicitly stored)\n\nSo it can faithfully hold values up to 2^52 which isn't a coincidence because it was specified by the ECMAScript standard. \"All arithmetic in JavaScript shall be done using double-precision floating-point arithmetic.\"\nAll quotes from the linked wikipedia page.. >Exactly. Double too fits in that requirement and should be JSONed as string instead of number why it is not ?\nNo, the double definition used in proto is the same float64/double definition used by javascript, IEE754 double. There is no need to convert since they are representing the same data the same way with the same edge cases.\n\nAlso, JSON does not defined that Unit64 should be represented as string in JSON. it is a shortcoming of JS and not JSON. We can create the JSON with big numbers by many ways and JS is going to have problem with that. Then why this trick is done in Proto3 to Json mapping?\n\nUnfortunately the JSON (JavaScript Object Notation) spec defines everything relative to number so it does suffer from the same problem as Javascript since it is, definitionally, Javascript.\n\nToday JSON is used with all sort of different languages and not just ties to JS.\nI am fine with writing Un-Marshaller to convert String to Unit64 based on the field Name and Type and continue with my stuff but I see this is not a logical mapping.\n\nYou are always welcome to write a custom marshaller and if you think this is the best behavior for you then you should go for it. I would just advise you that you need to inspect all of your client libraries implementations to make sure they are not adhering to the JSON spec, which again has every number defined as a IEEE754 double precision number. If you fail to verify their incorrect behavior then you will be in a world of hurt and data loss. Every browser will do the correct thing and destroy your data.\nHowever, if you're taking the time to write this in a non-browser environment, why not just use the gRPC generated client code in the language of your choosing? There are generators for most every language.. @kurin, what is the client that is calling you written in?. If that's the case then could you use the grpc client directly? You won't have any of these kinds of problems if you're using the grpc generated client library directly.. > I know that the proto3 spec specifies that 64 bit ints get marshalled as strings, and I know that this is because javascript implements numbers as floats,\nIt sounds to me like the argument should be a float64. If they specified that the API is a JSON number then that == a float64. Since it sounds like this is an open source API, maybe you could share which one it is. I'm curious about what open-source APIs have IDs that exceed 52^2.. @kurin,\n\nI'm not going to lie, having to use float64 instead of a more semantically appropriate type to hold integer valued data because a different programming language that I don't use is broken makes me a little bit resentful.\n\nI think that is fair. Being displeased because of these circumstance is not ideal is well justified.\nUnfortunately, the reality of the world we live in is that JSON hasn't aged well compared with alternative marshalling formats and is relatively restricted due to it. It's great that it existed in 1999 and it has propelled a lot of development over the years, but I, for one, am glad to see it shuffle off this mortal coil. Since it is a schemaless exchange format you're stuck dealing with the lowest common denominator of clients that receive it. Originally it was created for use in Javascript, so I guess it's a bit fitting that Javascript is what holds it back.\nIn this particular case if you look at the ECMA-404 (JSON spec) for their definition of a \"number\" you will see this:\n\nA number is a sequence of decimal digits with no superfluous leading zero. It may have a preceding minus sign (U+002D). It may have a fractional part prefixed by a decimal point (U+002E). It may have an exponent, prefixed by e (U+0065) or E (U+0045) and optionally + (U+002B) or \u2013 (U+002D). The digits are the code points U+0030 through U+0039.\n\nThat's it.\nOrly! No limits on how many decimals are allowed to be included? No limit on how far down the decimal places there are going to be? Thank you, JSON, for forcing me to load \"math/big\" in Go or java.math.BigDecimal so that I can interpret this random JSON blob. How should we handle the situation where someone sends us a 600 digit number with 600 sigfigs? Proto has to draw a line somewhere. I can easily imagine another issue being about how protobuf returns a big.Float which is inconvenient to use in golang instead of the much more reasonable int64 that it currently returns.\n\nI don't think (but I'm not sure) that any of my integer values goes above 2^53; maybe they will fit in floats.\n\nFingers crossed, I ran into this issue at a company I worked at before and we had to go through the migration from 52bit ints to strings. It was horrible, just horrible.\nWe are trying to map JSON based REST concepts into proto3. If there are things where JSON is more flexible than proto3, then we either have to expand proto3's/our understanding to contain more stuff (see #712 for an example of that in combination with https://github.com/googleapis/googleapis/pull/512), or conform the old API to the new specs. I don't really see another way to do that -- something has to give when there is inconsistency between two APIs.\n\nBut this is really, IMO, better served by giving users a knob, even if the knob has a warning label on it.\n\nI would like to take this chance to give you that knob. If you want to change the way that grpc-gateway marshalls/unmarshalls, you can implement the your own marshaller and unmarshaller. It shouldn't take more than a few hours to put something together that handles your use case. You could even put it up on github and share it around. For more docs on this, it is the first entry in our FAQ.\nMaybe you could help me flesh that out with more context/content so future people don't get quite this frustrated. What do you think about that plan?. You should be able to implement this with a custom marshaller. Marshall the fields you care about and don't for the ones you don't.. Our project is only set up to support golang/protobuf and gogo/gogoproto directly. It sounds like go-micro isn't exposing all the functionality grpc-gateway needs to work. Maybe you could extend go-micro to expose that information? I'm going to close this since it doesn't seem like an issue with grpc-gateway directly.. @c4milo, Can you elaborate on what it is you're interested in that isn't available today? . Can you elaborate on the circumstances that lead to this class of errors?. @c4milo, as an FYI everyone who is helping out here is doing it on volunteer time. This will probably get fixed some day but it is a low priority relative to other things in my life. One of the things that makes bug reports easier to act on is a simple way for me to jump into the problem without having to set up an environment where I can get expected outputs.\nOne of the ways to make this easier is to create a pull request with a failing test case. We have a folder where you can put an example and add it as a testcase. If you do that then the next person who runs into it will have an easier time to fix the issue.\nIn the meantime, have you considered splitting your proto up into 2 files and generating multiple swagger definitions or changing the name of your rpc methods to be ListOrg and ListTeam?. @dingqs447, I hope you've been able to make progress on this since August. If you're still having problems, let's try putting together a smaller test case and getting it into the e2e test to verify functionality. If you keep having issues, please reopen or file another issue.. @sreddybr3, it looks to me like you're using https://github.com/tensorflow/serving. Unfortunately we don't provide support for services that are using grpc-gateway, though we are delighted they are using it. You might have better luck filing a bug in their issue tracker. Best of luck!. That's a really interesting example case. Can you describe to me the how you would expect to be calling this in a get?\nAlso, just to confirm, protoc is hanging, not the swagger generator?. Hrm, that's really unfortunate. Do you think you could put together a PR with a test case in our integration tests to demonstrate this error?. @MaerF0x0, to echo Travis' comment, even a code snippet in here would be really interesting to see. Thanks for digging in!\nI'm going to close this PR since it seems you've got it figured out but I would like to see if it's possible to add some docs on this.. I'm not 100% sure on this since I can't see your code, but I would bet that you could create a helper method much like proto.String (or the other types) like this:\nfunc StringValue(v string) *structpb.StringValue {\n  return &structpb.StringValue(Kind: &structpb.Value_StringValue{StringValue: \"a\"}})\n}\n// ... et al\nUsage might look like:\nsrv.Send(&somesvc.SomeListValues{\n        Row: &structpb.ListValue{\n            Values: []*structpb.Value{\n                StringValue(\"a\"},\n                NumberValue(1.0),\n                BoolValue(false),\n                NullValue(null), // enums are hard and you might have to check if the value is nil here instead of doing it on type.\n            },\n        },\n    })\nOr you could write an even more high level abstraction to translate from  (I wouldn't do this, it feels kind of icky)\nfunc (r *Receiver) func SendIt(srv Yourtype, v []interface{}) {\n    r := &somesvc.SomeListValues{\n        Rows: &structpb.ListValue{\n            Values: []*structpb.Value{},\n        }\n    }\n    for _, k := range v {\n        switch s := v.(type) {\n        case string:\n            r.Rows.Values = append(r.Rows.Values, &structpb.Value_StringValue{StringValue: s}}\n            break\n        // ... More types\n        case default:\n            panic(\"This is unimplemented so bail\")\n    }\n    return r.srv.Send(r)\n}\nThat said, you're not forced to use go as your langage for server writing. If you think you can do better in python, c++, javascript or any of the other gRPC languages you can write your server in that.. Could you add some tests to exercise this behavior and prevent future regressions?. Can you rebase your commit off of master so that you no longer have that commit included after grpc-ecosystem/grpc-gateway's master? That should eliminate the warning.. I pinged the CLA bot manager and asked him to take a look. If you don't hear back from me by Monday, please ping and we will work it out manually. I just manually verified you have signed the CLA. Sorry about all this. I guess something is wrong with the bot \ud83e\udd37\u200d\u2642\ufe0f .\nThanks for your patience and your contribution!. Right now I am using grpc gateway with UNIX sockets in a project. Maybe you could document what you've tried and we can work together to figure out what's going on.. Have you tried creating your connection over a unix socket? Untested pseudocode to follow:\n```go\nconn, err := grpc.Dial(addr, []grpc.DialOption{grpc.WithDialer(net.DialTimeout(\"unix\", addr, timeout))})\n// Error handling\ns.RegisterCarHandler(ctx, mux, conn)\n```\nWhat happens?\nIt looks like grpc may also support grpc.WithNetwork(\"unix\") which would be even simpler (but wouldn't have support for a timeout).. What have you tried already?. After #412 lands, we box/unbox the *Value types. If you would like to take that patch and extend it to have a test I'd be very happy to merge it. I'm going to close this since that PR tracks the work slightly better.. @tmc  this looks good to me. Feel free to rebase and merge. @iluminae brings up a very good point. Could you at a minimum expand upon this downside in the documentation for this new method?. LGTM. once the tests pass let's merge this. @hectorj, Have you filled out the Google CLA?. How perfect do you need this guarantee to be?\nYou could add an interceptor to the client that you pass to grpc-gateway and have it insert a header that says \"request-from-grpc-gateway\"\nIf you're using it to partition traffic as internal/external traffic then the question is how accurate that needs to be. It might be better to approach it from the other side. You could create a cryptographic assertion and add it to every call from your trusted resources into the server in question.\nMaybe you could elaborate a bit on what your usecase is. That would help me point you in the right direction.. If you want the quick and dirty solution, just set in your context a value like \"internal-request\": \"true\" on your non-grpc-gateway clients. grpc-gateway routed traffic will not have that header and should be assumed to be external. Assuming you don't expose your gRPC endpoints publically then that would be sufficient. In the event that your gRPC endpoints are public you need to do something where your trusted clients all have a common shared secret and you use that to generate an assertion.. I'm going to consolidate this into just the PR #458 . @tmc, I found out something nifty about github the other day. You can git clone git@github.com:theRealWardo/grpc-gateway, check out the branch that has a PR coming your way and then push back onto it. The default PR behavior is to grant the PR recipient write into that branch (which is you). We don't need to impotently wait for a PR author to rebase so we can confirm everything works before merging... yay! :fireworks: . When I've had problems like this I find the best way to go about it to put together a little testcase that serializes to JSON and then post that back to the server. Could you try adding a new endpoint that takes an empty message and returns a GetFooRequest with a value set, then post the result to your existing endpoint? . Looks like you've previously contributed and everything was good with that, plus the CLA bot is kind of on the fritz. I'm going to merge this.. Wouldn't this change the output path for every file generated, not just the ones using go_package?. @tmc what do you think about this?\nFrom my vantage it appears to be the correct behavior but I would like a 2nd set of eyes before we merge a breaking change. Also, once we merge this let's do a release.\n@glerchundi, could you add some tests to verify the behavior and prevent regression?. @glerchundi, I just tagged a release which means now is a GREAT time to introduce breaking changes like this. Please update the PR with some tests and let's get it merged.. I have manually verified CLAs on glerchundi@ and notbdu@. Merging. Can you also add a test that exercises this invariant and I think I'm comfortable with this. @tmc, does that work for you?. @syhpoon I think all the context that is need is right here. Feel free to send in a PR that does this along with some tests and we can merge it ASAP.. If you want to resurrect that PR, I think I'd be happy to merge it. Merged it. Thanks for your contribution!. @afking, how are you using this in Bazel?. @afking Have you seen https://github.com/pubref/rules_protobuf? That's what I have been using in my project where I use this. It doesn't seem to suffer from this. Can you maybe describe what your project is doing different that necessitates this change?\nI'm hesitant to remove the panic since it appears to detect a real invariant in protoc input that can make things difficult to deal with but would be open to suggestions on other ways we can solve this. What do you think?. That's very exciting! Last I heard it wasn't possible to do that from Skylark yet. Do you have any references or sample code you could point me to so I can use that in my project?. Yeah that makes sense. I was kind of hoping you would say \"yeah proto_lang_toolchain is totally available from Skylark but it isn't documented\".\nI actually have swapped it in just as you suggested and I'm not able to reproduce with the proto files in my repo. I want to understand the circumstances when this code path goes into what is currently an error state before deactivating it so that if it turns out to be problematic in the future I know how to detect it.\nI'm terribly sorry to be a stickler on this, I am just a little leery of removing error checking, especially when the majority of users of this code are manually invoking protoc, which is sometimes difficult to correctly invoke with all the knobs turned to the right place.. How about this. I think there is a best of both worlds solution to this. What would you think about emitting a notice to stderr about this right before your return. What do you think?. I think this is good. Thanks for contributing!. Can you elaborate on your need for a YAML file instead of JSON? I believe anything that reads YAML should read JSON.. JSON is a strict subset of YAML so your YAML parser should be able to just read this for whenever you get back around to it. If you have problems feel free to reopen/open a new issue.. Good news! With @ivucica's recent PR being submitted we now support adding additional fields in the OpenAPIv2 spec. If there are more fields that you need than are available in the example proto then please reopen this bug and we can discuss.. Could you try recompiling the protoc with current head? This is now the default behavior so it should work. I'm going to close this preemptively but feel free to reopen if you're having issues still.. Huh, that is not what I would expect. If you have this flag set I would have expected it to reuse the context from the inbound http context in the grpc outbound context. Could you file a new issue documenting the issues you're having?. @idy, I think this would be a great flag addition to the swagger generator.\nIf you want to try this out, you should look at https://github.com/grpc-ecosystem/grpc-gateway/pull/280 for an example of how to add a flag guarded feature. Then you would just have to get that flag value in the mapping function, add an if to maintain the old logic in the flag=false case and put your new code plus some tests in.\nIf you need any help with that please comment on this but and I'll point you in the right direction. Just for my clarification, I would expect both urls [\"/v1/tasks\", \"/v1/tasks/\"] to route given my understandings of the go http server. Are you experiencing something different? Is this mostly about getting the urls defined in proto to be uniform or the client libraries? Could you elaborate a bit more on what doesn't work here.\nIf you wanted to expand the parser to handle trailing slashes that also might be a way to approach this.. Not to imply that it isn't a desirable change but it is definitely a breaking change since anyone using go 1.7 (which is part of our testing matrix) is going to have a bad time with this. . I just tagged a new release and as such it is a great time to be turning on new features like this. If people have issues they can use the tag v1.3. I'm not aware of any naming rules beyond the style guide. Personally I use CRUDL as my prefix then the singular form of the resource in question and map the information from the query string into the request body for the rpc.\nHere is what I would do (please forgive the pseudocode)\nMethod |Resource Path |Description| RPC call example | Note\n------- | ---- | --- | --- | ---\nGET | /users | Get a list of all users | ListUser()\nGET | /users/42 | Get a specific user id | GetUser({id: 42})\nGET | /users/42/tickets | Get all tickets of user id 42 | GetTicket({userId: 42})\nGET | /users/42/tickets/23 | Get ticket id 23 of user id 42 | GetTicket({userId: 42, id: 23})\nPOST | /users | Create a new user | CreateUser(userData)\nPOST | /users/42/tickets | Create new ticket for user id 42 | UpdateUser(userData) | Make sure to set id to 42 before sending this when you're not going through grpc gateway\nPUT | /users | Bulk-update users | BulkUpdateUser([]User)\nPUT | /users/42 | Update user id 42 | UpdateUser(userData)\nPUT |  /users/42/tickets/23 | Update ticket id 23 of user id 42 | UpdateTicket(ticketDataThatIncludesTheUserId)\nDELETE | /users | Delete all users | DeleteUser({}) | the empty object is the filter and with an empty object it will match everything\nDELETE | /users/42 | Delete user id 42 | DeleteUser({id:42})\nDELETE | /users/42/tickets/23 | Delete ticket id 23 of user id 42 | DeleteTicket({id:23, userId:42})\nNote the omission of PATCH examples. I generally try not to implement patch. I've found it to be mostly an antifeature and very difficult to get right. Completely replacing the record is usually better.\nOne of the very nice features about this is that when you want to add a new endpoint it is often as simple as giving it the URL you would expect and it will generally just work! If you're interacting from gRPC directly then I would avoid REST semantics, after all you're writing an RPC API.. The thing I was trying to get at in my comment above is that conceptually DeleteTicketOfUser and DeleteTicket are the same thing. One is filtering by Id = 23 and the other is filtering by both id = 23 and user = 42. Due to that you can use a single method DeleteTicket to implement both pieces of functionality and just take params that are mappable into a REST namespace. I'm going to close this since there hasn't been movement ina   while. Please reopen if you have more questions.. This shouldn't be necessary. It is preferable to load those in from the protoc distribution that is where you got your protoc from to ensure no versioning issues. In the docs there is the line\nprotoc -I/usr/local/include -I. \\\n...\n/usr/local/include is where I would expect those protos to live.\nPlease reopen if you have any more issues :smile: . The decision was to implement the gRPC http spec which is a standard part of gRPC implemented by several projects in the same way (Envoy, grpc-httpjson-transcoding, grpc-jersey, and in the paid product side Cloud Endpoints for gRPC APIs). Deviating from the standard would make us incompatible with a wide swath of the gRPC ecosystem.. I'm going to close this since it seems to be stuck. If you have any more issues, please reopen/make a new one.. Congratulations! Your PR was the one that allowed us to figure out why it gets stuck. I think it shouldn't have that problem any more... WOO!. TBH I'm sorry it took this long. I looked at it last night and wanted to merge it then but you hadn't commented yet to tickle the CLA bot. Hopefully this will not be a problem going forward.. Could you add a handler_test.go and exercise this in a way that demonstrates the bug to help prevent future regressions?. That's a good question! I didn't write that code so I'm performing code archeology here, but I think the logic is that you don't want to return the proto directly so that you can put additional information in later without breaking people.\nIf we wanted to add \"code\", a status code for the request, you could do that safely now but you couldn't if that weren't the case. \nLooks good to me! Thanks for your contribution. I think the fix here is to additionally emit a \"\\n\" into the stream to cause the data to have the newline delimit since returning to the old code would break the encoding and reopen this bug. Does anyone object to that?\nI don't have an environment in which I can test that easily. @aelsabbahy, could you validate my theory by adding\nif _, err = w.Write([]byte(\"\\n\")); err != nil {\n    grpclog.Printf(\"Failed to add newline delimit for chunk: %v\", err)\n    return\n}\nin runtime/handler.go on line 64 (right after the existing w.Write')?. Let's do it in a 2nd PR. Does my proposal fix it?. Can you tell me if reverting https://github.com/grpc-ecosystem/grpc-gateway/commit/b0be3cdef0ed27e3c420795e2efbfe9c27e839cc fixes the problem?. Could you describe the process you used to run the examples? Did you change any code before running make?. This is very strange. When I'm running this on my desktop I don't have this problem. Can you try running the e2e test that is in the Makefile?. You can use the .travis.yml files as a guide. Run the operations in the script and before-script section\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/b0be3cdef0ed27e3c420795e2efbfe9c27e839cc/.travis.yml#L25-L32. So I'm rereading this now looking for additional clues. Is the bug is state dependent and only fails on the 2nd GET request? Do requests 3-N fail in the same way?. I think this should be resolved now. Can you try it out? I'm going to preemptively close it but if you continue to have problems please reopen.. Yep, my bad. @tamalsaha do you have permission to tag a v1.3.0? Could you do that please?. I think you're totally correct. This would be a nice contribution for a first timer. I'll even point you to the appropriate code if you want :wink:. What version of protoc are you using? What version of grpc-gateway are you using? Have you made any local modifications to any thing?. gRPC 1.3 was released on Apr 28. Could you try upgrading to 1.7.3 (released 2 days ago)\nCould you try it with the regular golang protobuf compiler?. Is that code sample from the generated code? I'm pretty confused by that error, I don't really know what would be causing this. Do you have a custom marshaller installed maybe? Can you try installing a custom marshaller? Is there any indication where the error is coming from specifically?. With the submission of #454 I think this might be possible since you can control the handler. Note that you will NOT have interceptor support. Beyond that I think all the pieces are there. Please report back with your findings if you have any. Closing since this should be possible.. Could you also note that this fixes #5 and remove the entry from https://github.com/grpc-ecosystem/grpc-gateway/blob/master/README.md where it says \"bytes fields in path parameter. #5\". Looking at this, it seems to me that the test we have for the go version is just wrong and it is running our clean test against go master and go 1.9.\nI fixed it in #520. I'm going to get lunch and let the CI run then merge that. Sorry for all the trouble. It was an interesting bug if you wanna stare at it.. Could you rebase this onto the current HEAD? I updated the travis config so this shouldn't be a problem any more.. No, thank you for contributing! This project is amazing for all the effort and help we've gotten from such a wide community. Thanks for joining us!. You will need to install swagger-codegen. Our trais config should walk you through it\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/master/.travis.yml. I think @rogerhub's answer is probably the best way to go here. I'm not sure what the standard syntax for repeated fields in a query string would be. I guess the PHP format? I'm not sure though and think you would probably be happier with this as a different verb.. If you want to go about this I think the correct place to do it would be in http://github.com/google/protobuf. We don't control the spec for JSONification of protos so I think this is not the correct abstraction layer for that code.. What marshaller are you using? Does it implement the Delimiter interface?. This looks like a great start! I'm excited to merge this. Could you add some tests to verify schemaOfField does the right thing for these new well known types? It looks like we are totally missing coverage on that method right now.\nhttps://github.com/shouichi/grpc-gateway/blob/31e65b13c82edfcf1c78926c59fa829b7ed14841/protoc-gen-swagger/genswagger/template.go#L250. Sorry this took so long. Looking through the docs a bit I think this is correct. Merging. Thanks for your contribution!. Can you please add a test to verify this behavior?. Thanks for adding the tests and thanks for contributing!. Have you tried out our sister project, https://github.com/grpc-ecosystem/go-grpc-prometheus?. Could you put together some information on things you've tried? Maybe together we could create the tutorial/examples that you would have needed. What do you think of that?. @davejohnston, thanks for the inquery. Unfortunately at this moment we don't support and don't have a plan to support file uploads. You can see some discussion of the topic here. If you were interested in giving it a go and documenting it I would love to have some notes on strategies that work/don't work for this problem. Wanna give it a try?. Could you put together a git repo that has a simplified form of your code, along with the protoc version, grpc-gateway version and any output that does come out? Thanks!. This sounds like a really interesting proposal. Beyond source compatibility with the protoc-gen-go plugin, can you help me understand what the specific benefits are?\n\n\navoiding all the boiler plates, as protoc-gen-go will handle them, and\n\n\nWe already have this boilerplate and it is tested pretty well. We currently handle most of these edge cases and have a fairly well tested codebase in that it is run in many organizations.\n\n\nmaking gateway generator easier to integrate with other protoc-gen-go plugins\n\n\nMy understanding is that the composition of protoc generators is achieved by running the compiler multiple times and then using embed = [\":proto\", \":grpc\", \":grpc_gateway\"] in your rules_go go_library rule. Can you point me to some documentation that infers otherwise?. This looks really good and I think it's 100% the fix. Thanks for contributing!\nDo you think you could add an example in https://github.com/grpc-ecosystem/grpc-gateway/blob/master/examples/examplepb/a_bit_of_everything.proto that exhibits this problem (and the solution) so that we don't regresson this in the future? Thanks so much!. > edit: erm, not sure how to make the tests run in CI. It imports the examples from the grpc-ecosystem path but my changes are only available in my own namespace.\nI'm not sure I understand what you're saying here. Can you help me understand a little better?. I think those errors are probably only occurring in your local $GOROOT. What happens if you mv the $GOROOT/src/github.com/warmans/grpc-gateway directory into $GOROOT/src/github.com/grpc-ecosystem/grpc-gateway. Do you get different errors?. I think we should remove the 1.10 beta from the test matrix. I'm sure we aren't the only people hitting that and we don't have release notes on how to fix this yet. @tmc do you have any thoughts on this?. Looking at this, it seems to me that the test we have for the go version is just wrong and it is running our clean test against go master and go 1.9.\nI fixed it in #520. I'm going to get lunch and let the CI run then merge that. Sorry for all the trouble. It was an interesting bug if you wanna stare at it.. Could you rebase this onto the current HEAD? I updated the travis config so this shouldn't be a problem any more.. My understanding of option go_package is that it should end up in file.GoPkg.Path appended with the filename. What happens if you set option go_package = \"improbable.io/proto/base\";?. Maybe the correct thing to do is to fix the central list of these mappings. I think somewhere in the bowels of the beast we have a mapping like that. What is the mapping you would prefer?. Did you use protoc to generate the proto that you defined? Have you had any luck on fixing this recently? What have you tried that worked/didn't work?. Thanks for following up!. I think this is going to be handled by #515. Please reopen if you don't htink that's the case.. @hacst, thanks so much for sending this in! I'm definitely intrigued by the concept here. Would the idea be to entirely leave this at code generation time, or would it become a runtime configuration option? As I understand it, the YAML files exist to modify configuration at runtime in the endpoints project. Can you help me understand a little better?. Could you comment on this\n\nSometimes the requirement to have to annotate your .proto files to expose them using grpc-gateway can be inconvenient. For one all your backends and clients using the .proto now need to know the google api types for the annotation which can be a hassle. More importantly in a number of situations you cannot/do not want to change the .proto at all (e.g. if you don't own the .proto or only want to use the gateway for experiments).\n\nAs I understand it, you wouldn't need to include the annotation types in the outputted code. Since they are service definitions they would be available in the proto reflection as an extension, but wouldn't be included as a compiled component. Do you mean that dependent projects would need to configure the protoc to have those extensions available?. Upon reflection you're totally 100% right about that. There is no way to even have that without importing the other libraries. This is also the only way we can go forward that allows someone to use grpc-gateway on a file they don't have edit permissions on.\nOut of curiosity, do you just invoke this on the command line through protoc but with an additional argument grpc_api_configuration with a path to the file? For some reason in my mind it seems weird to load a file that isn't passed in on stdin for a protoc plugin. Does protoc do anything weird like sandboxing to prevent doing that?. > Also my generation run insisted on changing some occurances of \"context\" imports to \"golang.org/x/net/context\" which I understand is a legacy construct. I was using go 1.10 and a 3.0.0 protoc so I am not quite sure whether that is something that needs fixing. It does compile.\nTotally cool. 1.10 go fix replaces everything and 1.9 has type aliasing. I'm going to do a big sweeping change in a few weeks as soon as swagger-codegen 2.4 comes out and probably do a 2.0 of grpc-gateway so no worries at all.\n\nMy bazel file edits were blind as I do not have a bazel environment to use. The bazel build on travis fails due to what looks like my new dependency missing which makes a certain amount of sense. I have no idea how to get a 3rd party go dependency into a bazel build so help is welcome there.\n\nSorry 'bout that. It looks like you figured it out, but that is obviously a sub-par experience. How can we make this easier for contributors in the future?. I think this is good to go! Thanks so much for all your work in here. Could you rebase this, fix the merge conflicts, and run go fix, then I will merge it?\nPS: Thanks for looking into the Jekyl stuff. I'm also totally new to it so I don't know much much at all about how to fix things. I'm glad you took a look at it.. No more breaking changes before it gets merged -- I promise :wink:. Sorry about all the relevant changes.\nThanks so much for your contribution! This is really great work and I look forward to doing more of this with you going forward. Please, don't be a stranger this was fun and I hope you had fun too. Plus new features for everyone :tada:!. I think this is good to go. Would you be willing to add an example (and run make examples) to enshrine this in our goldens file so I don't accidentally break it in the future?. That's exactly what I was asking you to do. Sorry for not being more clear!. I just looked at https://github.com/grpc-ecosystem/grpc-gateway/pull/526 and I think I'm just going to merge that since it includes tags support as well. Thanks so much for sending in the PR!. The reason to generate the prefixes is that we don't have a guaranteed unique namespace. If two packages produce different Foo messages then we will have trouble.\n\nWhy not strip the prefix if it isn't necessary? (the next obvious question)\n\nI tried that, but the problem is that if you do that then when you add a message in that happens to conflict it will break code that is very far away from the code that changed. This is in an effort to adhere to the principle of least astonishment.\nIf you disagree with me, I would love to hear your reasoning why. Maybe we will even change it to work the other way! I'm happy to answer any more questions you might have about the project. Since I've gotten this question a couple of times in various ways, I'm also going to add a FAQ entry list to the README.md in the root. Feel free to send PRs to enhance that (even if it is only a question, I can propose a change on top of your PR that has the answer and we can merge it!). Looks like @tmc doesn't want a view. Merging.. Can you add a test to verify this functionality? I'd be very happy to merge once that happens. Thanks!. I love being reviewed!\nOn a more serious note. LGTM, let's ship it. Thanks for contributing!. I'm not sure I understand what you're attempting to do here. Do you think you could elaborate a little bit more. Maybe giving an example would help. Why do you want to route through gRPC gateway instead of directly connecting to the service in question? It seems like the layer of indirection may not be necessary.. @llimllib, could you elaborate on what you mean by \"send an invalid request to the service\"? Is that a gRPC service or a REST service? Who is determining that the request was invalid? Is this gRPC validation logic or your service's validation logic or even gRPC gateway's?\nIf you would like to see something made public, I would recommend a PR so that we can discuss the specifics of your proposal. If you need any help with that at all, please comment in here and I'd be happy to help! :smile:. Thank you for your thorough issue report and pull request. The behavior you specified is indeed incorrect. Sorry about that. I've merged your PR and you shouldn't have that sort of problem again.\nThanks so much for contributing!. That's probably the best way to go forward today. For now we maintain compatibility with the 2 latest versions of Go. Fortunately Go 1.10 is expected out in Feb which is pretty soon. When that's released we will migrate to the built in context package and this shouldn't be an issue any more.\nThanks so much for filing an issue! Without field reports we have no idea the things we need to focus on. I'm going to close this since we are already tracking this in https://github.com/grpc-ecosystem/grpc-gateway/issues/326 which I'm itching to close :smile:.. Thanks for the new feature and thanks for contributing!. @shynie, thanks for the bug report!\nUnfortunately, I'm not at all familiar with the argument you're attempting to use. Do you think you could point to some documentation that has more information? Additionally it would be extremely helpful if you could write a test that demonstrates this failure. With that it will be a lot easier to figure out what next steps should be.. Does this PR mean that importPath should be working as expected? Could you add a test by modifying the makefile to invoke protoc with this parameter set to a value you know to work and output to a different directory so we could have a more e2e test?. Looking through this and some other documentation on import paths, I think this is the right way to do it but I wouldn't be surprised if someone came along and said \"this totally breaks my use case\" when we merge. If something goes when this is merged, do you mind if I @mention you in the issue and see if we can find a good resolution for everyone?. I'm generally supportive of any experimentation people want to make in the handlers as long as they provide them as an alternative to the default. The breaking change would be modifying the default return type in the error case, but adding a new handler is totally kosher in my book.\n@johanbrandhorst, would you like to play around with upstreaming that style of error handling into grpc-gateway?. @alexleigh. can you help me understand something about the output? Do you know if this will change any of the generated code in substantive ways? I know this is kind of a vast question, but is there reason to expect that in python the generated output code for the field my_field would go from get_my_field to getMyField?\n@tmc I would appreciate if you could also weigh in here. To me this seems reasonable but I will admit that it is breaking. Should we couple this with changing the default and mark this as a 2.0 release?. @Kuqd there have been other attempts to do this (https://github.com/grpc-ecosystem/grpc-gateway/pull/508 comes to mind). It might be useful to start with their work.. Sorry, I did a poor job of communicating what my intent was. We have for a while wanted to turn things from snake_case to camelCase and turn on default value emission in the same change. They are not inherently linked but they are both breaking changes. Since they are breaking, it's my preference to do both of them in rapid succession (or at the same time) as to minimize impact on people's production systems and keep the migrations down to a single event.. TBH I would love it if anyone would do it. $DAYJOB has been keeping me extremely busy of late so I haven't had time to do it. I will take care of the releasing a 2.0 if you want to go through and enable default values and camelCase. Could you file a bug against swagger codegen and @mention me in it? I'd prefer to not have a duplicate ABE proto if it's at all possible.. https://github.com/swagger-api/swagger-codegen/pull/7636. I just replied to the comment over there. That PR has all the info I've got.. To update ppl here, the aforementioned PR is now in swagger-codegen. I'm waiting on the 2.4.0 release to go in before I pick up the work in #546 to upgrade to 2.4.0. Once that happens, we can resume work here with a relatively surgical change to the default marshaller/unmarshaller and the swagger name selector. Then once that lands I will update docs and tag a 2.0 release to indicate that one of our longest standing issues is now fixed. :tada: . @alexleigh Do you think you could add an example proto in examples/examplepb that exhibits this new behavior (and modify the makefile to have it be built automatically?). @alexleigh, Looks great to me! Thanks so much for your contribution :smile:. I look forward to working more with you in the future!. @Kuqd, as I'm sure you've noticed by the email from Travis.ci, we have some end to end tests that will need to be updated along with this. Potentially even merged at the same time as the change #540. If you want to go about making those changes it would be HUGELY appreciated.. Have you tried this with version 1.9.2 of the go compiler?. First, thanks for the contribution! Do you know what does this do on multi paragraph comments? e.g.\nservice Users {\n  // Search for users\n  //\n  // Searches for users that match specific filter criteria and returns a\n  // (possibly paginated) list of user resources on success.\n  //\n  // Additional contextual information is included here.\n  rpc SearchUsers(SearchUsersRequest) returns (SearchUsersResponse) {\n    option (google.api.http) = { post: \"/v1/users:search\" body: \"*\" };\n  }\n}\nI suspect that would all get condensed into a single line with this patch. How do you think we could enhance it to provide a line break in the single paragraph case as well as in the multi-paragraph case?\nAlso, as an aside, we depend on \"golden file\" tests in order to ensure there haven't been any regressions in the creation of the files. Since this change modifies the output format for the .swagger.json files, you will need to run make examples and commit them before the CI tests will pass.. Oh, an additional concern I have is in the case where someone wants to make a list in the comment:\nservice Users {\n  // Search for users\n  //\n  // Returns a list of users and allows you to search by:\n  // * Family Name\n  // * First Name\n  // * Height\n  // * User ID\n  //\n  // More information that would be useful to a person calling this API.\n  rpc SearchUsers(SearchUsersRequest) returns (SearchUsersResponse) {\n    option (google.api.http) = { post: \"/v1/users:search\" body: \"*\" };\n  }\n}\nCan you help me understand how your change would impact a usecase like that as well?. Text formatting in these cases is a generally hard problem. If you look at the CommonMark test suite for example, you can see some of the strange things that people do when writing text files.\nThe reason I'm hesitant to do this formatting automatically is because this changes the code from interpreting the literal text they wrote to interpreting the intent they had behind their words, whitespace, and punctuation. As another example, what if someone has a numbered list? They will run into similar problems. It almost seems to me the better solution is to tell the swagger UI to run a markdown formatter on the input text from the comment. I agree that it is mildly unsightly to have widow words and strange line breaks but it does come at a cost -- the ability to fully express your formatting.\nMaybe we can look around for other options. Do you know how other tools handle the swagger generation WRT line breaks? Maybe we can learn from them?. The more I think about this the less confident I am that it is the correct course of action. I very much appreciate you spending the time to contribute. Please stick around, there are a lot of fun projects embedded in the gateway that would be fun to work on if you're looking for a challenge. Feel free to email me (address in commit messages) if you're looking for something to spice up your life!. @jinleileiking, thanks so much for your efforts so far! This seems like an interesting marshaller to add, there are just a few things that need to be done before it can be considered for inclusion in the mainline repo.\n\n[ ] Unit Tests of the file in question\n[ ] Documentation of how one would use this new functionality\n[ ] Coverage in end to end tests (to ensure I don't accidentally make your code regress when doing a refactoring later)\n\nIn the meantime, since the marshaller is a pluggable part of the system I would encourage you to bring this code into the mainline of your project to get you un-stuck.. It looks to me like you're trying to compile protoc. If you want help with that project I would request you file a bug against that project. Best of luck and when you start using the gateway of you have any issue don't hesitate to file another bug. @lukasmalkmus, sorry for being so slow to respond. What do you think about my updated FAQ entries? . Is this log output, or is it included in the actual output of the server?. I, unfortunately, am not as expert with protos as I probably should be, but I don't believe there is a easy way to do that that doesn't involve fieldmasks which we don't currently support. That said there is some interest in implementing that in the gateway. Would you be interested in taking that sort of project on?. Good catch! Thanks for taking the time to fix it for everyone. Maybe you could elaborate a bit more on what you're trying to do along with some sample code. I don't think I have enough context to understand what is going on just yet.. I think https://github.com/grpc-ecosystem/grpc-gateway/pull/454 can be used to provide this functionality already. Can you comment to indicate if you think that isn't sufficient?. The idea is that you wouldn't generate the client interface in grpc-gateway, you would instead use the normal grpc client/server generation code to create your server/client and that you wire them up through Register....HandlerClient. To start creating servers/clients I would recommend going and reading the getting started guide for GoLang. However, given that you seem to be interested in writing a service that talks to something running in the same binary, you might consider just going right for Gin without the overhead of protos/grpc/grpc-gateway. It is a great framework that can help you get things kickstarted.. Right, so to put a very fine point on it by using echo_service.proto, you can implement by implementing the EchoServiceClient interface and passing it into func RegisterEchoServiceHandlerClient(ctx context.Context, mux *runtime.ServeMux, client EchoServiceClient) error {.\nCan you help me understand why that doesn't work for you?. grpc-gateway does not produce logs of access since it is not a webserver. If you would like logs of access, please install grpc-gateway behind a HTTP server or add a logging component into the standard Go HTTP server that you're using to serve your content.. > However, I've gone with the minimal change to make the error go away, I\n\nhaven't introduced a proper, generated protobuf definition.\n\nIn that PR we decided to not include the complete proto. Turns out it is comically large and dramatically increases the complexity of vendoring making it kind of a logistical nightmare.\nThanks for sending in the PR!. This sounds like a great idea! I really look forward to having a discussion in a PR about what those names should be.. Since you're merging the two packages together, how would you keep the old, aliased, packages having the exact same exported interface? . I just reviewed this and I think it is good to go. Marking as approved. Can you please close any other PRs that this obviates and then I'll merge it (probably tomorrow morning/early afternoon US time). Today was a disaster. Sorry for the slow merge. Merging now. I think the best way to go about it would be to imitate what @tmc did. It is a nice, clean, solution to the problem at hand and doesn't increase the API surface area for grpc-gateway. I would love to add your project to the official docs so please tell me where your projet ends up so I can add reference to it.\nPS: Would you still be interested in getting https://github.com/grpc-ecosystem/grpc-gateway/pull/458 merged?. Thanks so much for your contribution!\nCan you help me understand a little more about this change? Why we should package these inside the grpc-gateway package instead of having clients vendor them in their own projects? Why is this necessary now so many years into the project when it wasn't needed before?\nIt's great to have more minds thinking about these problems so we, together, can trend toward better solutions. Do either of you know if it is possible to use the same base64 en/decoder that is provided by golang/protobuf? How does the deserialization work when they are processing JSONified protos? Could you point me to the code where they do that?. @loderunner, fantastic research! Thanks so much. comments like that make merging easy (and fun).\n@lucasvo, thanks so much for finding the time to contribute to the project! I really look forward to working with you in the future.. I believe this is the correct behavior and therefore is a bug fix, not a breaking change. Merging -- thank you!. Hey @jhump, sorry for the very slow response on this. Let me give you an update on the state of affairs around here. I have a PR that I'm going to be submitting soon that is the beginning of the breaking change window for a 2.0 release. As such I've been focusing on getting that done and not really tending to the PR queue. Sorry about that.\nThis looks like a really good PR to have included and I don't mind at all that it doesn't completely solve the recursive form of the problem -- the perfect is the enemy of the good after all. One thing I noticed is that you've hit a bug in our CI system. I am curious. Do you think you could rebase this PR after that commit and see if the CI system blows up?\nI would have expected this to fail since you changed the template which should make the examples directory (which is a dir of goldens) not match. To fix this, you can run make examples to update the goldens and then the PR should pass CI.\nThanks so much for your hard work!. Friendliest of pings. @yugui Have you seen this?. Looking at this, I don't think this is a change that I would be comfortable making. This would be a breaking change for every single user of the project in search of compatibility that I don't think is widely needed. If this becomes a community standard and part of every major gRPC installation (or gets rolled into go-grpc-core) then I would definitely be receptive to making this change.\nIf I may ask a higher level question. Given that there is a collision on that name right now, it seems likely there will be more collisions. Do you intend to run down every other project who is potentially going to conflict on that generated name? Maybe it would be a good idea to switch your handler to Register<ServiceName>ChannelHandler?\nIf this is a change that you think absolutely must be implemented, I think that making this a configurable flag in the generator followed by a slow migration and a major version upgrade hard cutover is the way to go. I will warn you that the bar for testing it at an acceptable level will be really high and will involve probably duplicating all of this projects integration tests to ensure that we don't break anyone in either configuration.\nSorry to not give a happy, \"let's merge this\", but I have concerns WRT major breaking changes like this.. I just got a bit of time to actually look at the code (sorry for firing back so fast, things have been really crazy of late).\nFor the most part I'm okay with this. I would like to see something added in the examples folder that has the flag changed so we can ensure that the handler is properly renamed in all the cases it needs to be by actually compiling it. You should be able to do that in the Makefile in the root. Add a new directory with a name like examples/user_selected_handler_name (or find a better name, this is just a placeholder) and have it output to there. No need to do all the examples, just pick one ABE or something.. Can you help me understand why changing the template doesn't change any of the generated code in the examples section? Could you run make examples and include any changes that are in there? . Could you rebase this change? I've fixed the test that was not running that I was expecting to fail in here.\nThanks!. My understanding of the proto registry is unfortunately lacking WRT this topic. My expectation is that the proto on the wire wouldn't contain enough information to serialize it down to json. Does the any proto contain a field => name mapping in the wire format for it or something?\nCould you also add a test to ensure this behavior doesn't regress?. Retriggered. LGTM. Looks like there is a merge conflict. It isn't trivial to do from the web interface and I'm at work, but @tmc feel free to merge if you can get it to have no conflicts. Can you provide some sample inputs and outputs (both expected and what you did receive) as well as the proto you used to define?. +1, it should be 429. Could you also add a link to https://github.com/googleapis/googleapis/blob/master/google/rpc/code.proto in the function comment?. @johanbrandhorst, If you want to send in a PR that fixes it and updates the comment on the function I'm happy to merge it today.. Thanks so much @eleniums for contributing!. Looking at the code in question -- yep, that's a bug. templateToSwaggerPath needs to be updated to handle this case. Fortunately the test cases are pretty well put together and shouldn't be too hard to fix.\n@c2nes, can you confirm that this is a swagger only bug?\nI don't have the cycles to address this, but if someone wants to take a shot at it, I think this should be enough information to take a good whack at it.. Hey @antonikonovalov, thanks for your interest in the project. You're right that we don't currently have support for it :frowning_face:. Fortunately, the feature is just a PR away!\nIf you wanted to take a stab at this, I would be more than happy to help. I'll even start by pointing you to the right-ish place. To add this in you could either parse the opts to handle the case where the user explicitly adds it, or you could parse the type information to detect when a oneof has been used and automatically add the descriptor field.\nGood luck and please comment if you have any questions or problems.. Thanks so much for your contribution!. I put an issue template up the other day. Could you restore it and please fill in the contents, especially the parts about what you have already done? Thanks!. @jacksontj, yep. I fixed the CI just now. Could you rebase please?. This looks like a reasonable way to approach it. Do you think you could add some tests to that would have failed in the old system that pass in the new one?. I'm terribly sorry, would you be willing to rebase this change to master?\nI made some significant changes to the Travis CI config I want to make sure this passes with them.. It most definitely got lost in my inbox. Thanks so much for rebasing . Would it be possible to add a test to verify this functionality? Could you also rebase this PR to see how the CI feels about it?. Friendliest of pings. I'm not entirely sure how to express how supportive of this pr I am. You literally freed my weekend plans up because this was what I was going to do.\nIt's 1am for me so I'll be reviewing tomorrow but let me say thank you preemptively. I think you could have force pushed your change on top of the branch and it would have been okay, but feel free to send another PR :smile: . Would you be willing to edit .travis.yaml to include bazel in the CI process. You can review https://github.com/bazelbuild/rules_go#how-do-i-run-bazel-on-travis-ci for inspiration. To be a bit more explicit, could you add to the matrix something like USE_BAZEL=true and then put an early termination if statement in?. I think you can use exclusions to achieve that\nhttps://docs.travis-ci.com/user/customizing-the-build#Excluding-Jobs. I think @yugui is saying that she would be happy with 3 builds if they built master, 1.10, and 1.9 in Bazel, please correct me if I am wrong. That is totally achievable, but it would require having a custom WORKSPACE file using a local go sdk. I think that would be an interesting path forward. WDYT?. @f0rmiga, thanks so much for your contribution! I look forward to adding more support for Bazel going forward :fireworks:. @f0rmiga Out of curiosity, do you have a theory on how to add swagger generation in Bazel? I'm a little hesitant to use the rules_go tooling around it. WDYT?. Fortunately there are other projects who have implemented it in a less go specific way. For example, https://github.com/bazelbuild/rules_closure/blob/dbb96841cc0a5fb2664c37822803b06dab20c7d1/closure/protobuf/closure_proto_library.bzl. Thanks so much for your eagle eyes!. Hey @ithinker1991, I have seen you post 4 or 5 of these as PRs to the project. Could you help me understand what's going on here?. @ithinker1991, I'm still curious about what's going on. A new release has been cut. Thanks for the reminder!\nhttps://github.com/grpc-ecosystem/grpc-gateway/releases/tag/v1.4.0. > we should change github.com to ---> yourcomany.gitlab.com....\nI'm not sure I understand what you're trying to say here. We are hosted on GitHub, I think changing the host to something else would confuse the matter.\nCould you elaborate a bit more on what you would like?. To expand on @yugui's answer, I find that I regularly wish a library had an additional piece of functionality or that the ergonomics of its interface are more generic than my problem requires. Often times I am able to whip up a package that wraps the library and reexports the interface in a more narrow context. For example, most of my projects have a logging library that wraps logrus and exposes a method to get a preconfigured logger that is completely filled out based on the context object.. What have you tried?. That sounds like an interesting problem. Could you explain what you have tried thus far?. \u200b\u200bI'm not aware of a native connection pooling mechanism built into\ngrpc-go. Could you elaborate on what you're using to achieve pooling? Maybe\nyou could also elaborate on why you need connection pooling given the\nmultiplexed nature of gRPC?\n. @f0rmiga I don't think you have to write the swagger rule any more :wink:, great work @mrmeku!. @f0rmiga, I mentioned you on the PR because I saw that message but I only got notified a few hours ago. Sorry if there was any overlapping work here.. No it isn't supported yet but it would be an easy pull request to add the functionality. Give it a go and send in whatever you can get working. I'll help you if you get stuck. Can you change one of the proto files to exhibit this new behavior and run make examples?. @blackdahila, I think this is good to go. Thanks so much for your contribution! I look forward to doing it again soon.. Looks good to me but has some CI issues. Ah, good catch. Thanks for doing that.. Thanks for the eagle eyes!. @yugui, I am working with @mrmeku on a way to do this where you don't have to enumerate all the generated files. Do you mind if we hack on it over the weekend and see what we can come up with or would you rather we merge this in the interim?. You bring up a good point. It might be worth it to explicitly enumerate these to do a golden check. I don't think the way we are going to do this will help with the golden check. We will try this out (maybe be unsuccessful) and then we can merge this when I give up and admit defeat \ud83d\ude09 . I don't know if I mentioned my failure to get it working over the weekend. I think we should merge this or something like it. If you're interested in finishing it I would like to merge it. Interesting that you get that result. This is the code where we do that conversion. Are you doing anything non-standard in your setup? I've been using this error code for years in my application without trouble, I'd be interested in running down the problem.. The logic behind this is that we require you to be explicit about the request methods you intend to receive. If you want to receive a HEAD then you will need to use a CustomHttpPattern.\nSee the documentation line\n\nThe custom pattern is used for specifying an HTTP method that is not included in the\u00a0pattern\u00a0field, such as HEAD, or \"*\" to leave the HTTP method unspecified for this rule. The wild-card rule is useful for services that provide content to Web (HTML) clients.\n\nPer the HttpRule documentation.\n. WRT RFC2616 10.4.6, that is another in a long line of things we MUST do according to the RFC, but don't perfectly implement because no one has needed them yet. I would love to merge a PR that adds it.\nIf compliance with that spec is the name of the game you could also implement a handler that responds to HEAD requests appropriately.. No worries, have a good day!. Ah, that's totally fair, can you remove that entry from the BUILD file and I'll track doing it in https://github.com/grpc-ecosystem/grpc-gateway/issues/640. I think this is good to go almost. I haven't merged due to https://github.com/grpc-ecosystem/grpc-gateway/pull/632/files#diff-256f5102c31109d1da9e7ebdbde6813cR51 where I asked for a change but I can't see the change. Am I missing something?. This looks good to me. Let's merge #632, rebase this and then merge. Does that sound okay to you?. Did you have any luck with this?. That has now been merged. Could you rebase this?. Yeah, it would be better if the swagger name was derived from the name of the rule. That wouldn't be a pretty easy PR to do after #632 is merged. @cy-zheng would you like to pick up where #591 left off? It wouldn't be too much more work to enhance with a test then I'll merge it and do the Bazel change. What do you think?. Yep, that's exactly what we need to get it submitted :smile:. Did you try regenerating the openapiv2.pb.go file that is generated from the proto? It looks like we don't have an entry in the Makefile for it. Would you mind adding a new line in the $(SWAGGER_PLUGIN) section of the makefile to invoke protoc like:\nprotoc -I $(PROTOC_INC_PATH) -I. --plugin=$(GO_PLUGIN) --go_out=$(PKGMAP),plugins=grpc:$(OUTPUT_DIR) $(PROTOFILE). @ivucica I think I've fixed the underlying problem, but there might be more that you've noticed. Can you try at HEAD?. Looks like there are some CI issues. Do you think you could address them and ping the pr? Thanks!. Unfortunately I think you're right that this is probably better handled by a custom marshaller. The proto API, as I understand it, isn't supposed to be able to form to every REST request, only to allow an RPC to be exported in a way that any browser can connect to.\nIf you make a custom marshaller you will have access to the full body message for hashing. I would be interested in adding that as an example marshaller if you would like to share that code.\nPS: Why is there a message checksum in the web hook? Is this a common pattern that I've not yet run into?. Okay, so it is more about signing than hashing. That makes sense for message integrity.\nIf I were going to implement this, I think I would implement it as a HTTP interceptor in the grpc-gateway, look for the Stripe-Signature header and if it has it, verify it is correct. If it fails that check, immediately return a 400 error (bad request) or a 412 (precondition failed) to the caller and not pass the request into the stack.. I manually edited both changes to leave authorship with their respective author, rebased and uploaded. This can now fix a long standing issue. Thanks so much everyone!. @hexfusion ask and you shall receive https://github.com/grpc-ecosystem/grpc-gateway/releases/tag/v1.4.1. This looks like a really interesting patch. Can you add some tests that demonstrate the new behavior?. > It seems that the test case itself would need to change. Can someone explain the meaning behind {string_value=strprefix/*} ?\n@yugui, now that I look at that, do you know why that is like that? My guess is that it strips the \"strprefix/\" from the beginning of the string value, but I don't understand why you wouldn't just put that outside the capture. Do you have any hints?. Reading through, I think I agree with your interpretation of the spec @ivucica.\n@jessesuen, I think that should be sufficient information to update the tests to reflect the new behavior. Does that work for you?. @lpabon, I don't think anyone is working on this. Would you be interested in taking a stab at it? What information do you need to get started on adding this as a flag?. This would be an interesting change to merge. Can you help me understand what the migration strategy would be for people who are using the project right now and aren't expecting their service names to change. @ivucica, thanks for the thoughts! Just a few things to add:\n\nThere are ways out of this; how about adding a suffix only if the operation ID is determined to be duplicate? (Similar to what's done with bIdx.) This will address the immediate issue.\n\nWe have discussed proposals like this before, but I'm fairly reluctant to have behavior change based on duplication. My primary consideration is this case:\nI have a.proto file that has a method Get. I use this for a long time. Later I add a Get method to a different service and now, all the code I wrote before needs to be updated to reflect the surprise changing of the original Get method's name to packageGet.\nTo me, it seems like explicitly renaming their method packageGet is more obvious about what is happening and has most (if not all) of the benefits.\nWhat do you think?. I like this PR, I need to look at it in a bigger screen then my phone but it looks like a great additional piece of documentation to include. Retriggered. I think the error comes from an update to the protobuf generator. Could you update your protoc and go plugin and regenerate with make examples then upload again?\nI expect it should add the following\n```patch\nindex cb6fb3a..d416004 100644\n--- a/examples/clients/abe/examplepb_a_bit_of_everything.go\n+++ b/examples/clients/abe/examplepb_a_bit_of_everything.go\n@@ -14,6 +14,7 @@ import (\n    \"time\"\n )\n+// Intentionaly complicated message type to cover much features of Protobuf.\n type ExamplepbABitOfEverything struct {\nSingleNested ABitOfEverythingNested `json:\"single_nested,omitempty\"`\n\n```. Looks like there are a few remaining issues. Can you give it another whack?. Rerun. I'll try to check on it in 20 but I'll probably forget. If I don't do something by tomorrow morning please reply here, it means I forgot (sorry in advance). LGTM. Let's give @ivucica 24 hours from your ping before merging. I will rebase tomorrow around this time. If I don't please reping and I'll merge when I get to a computer.\nPS: Sorry this is takling forever, it's a great feature and I am REALLY looking forward to using it. Taking a little extra time is definitely worth it.. @johanbrandhorst I approve of the action and the attitude. Thanks for merging. I don't think the code actually supports that right now. That said, it shouldn't be super difficult to add it to the parser. It's already plumed with the appropriate types. You would just need to detect when it is in the list of required fields?\n@ivucica, do you have any more specific tips than I just provided? Also, looking at it, why is required a property of the message instead of of the field?. @co3k, thanks so much for your pull request. Looking through, I'm a bit puzzled by what it should do. It looks like the changes to the golden .go files all come from changes upstream. If this code path isn't exercised by our examples, do you think you could add an example that does exercise it so I can see the new functionality?. @johanbrandhorst another neat thing about the squash button on github is that you enter the description of the new squashed commit which makes it perfect for these situations. 2 commits squashed down into 1 and then the description hopefully clarified.\n@co3k Thanks so much for your contribution! This will help in producing great documentation for APIs.. Can you rebase this and get CI passing?\nOut of curiosity, what are you doing that requires a different go_proto_compiler?. Looks like protoc-go changed their output again. Can you run make examples and upload?. I would expect that. Since the upstream protoc changed, probably all the .pb.go files will need to be updated.. Out of curiosity, what would you think about adding a new go_proto_compiler entry in there that has the option you care about enabled as well. WDYT? I'm happy to merge if you think it's not the right way to go.. Oh, so is the major factor that you're vendoring in groc-gateway? If you weren't vendoring would the existing target work for you?. Sounds like you've got it worked out. Closing. I look forward to your contribution!. @johanbrandhorst when you click the merge will request button on GitHub there should be an option to squash the commits into 1 if you prefer that. That way you don't have to ask the people to squash them at the very end.. Unfortunately this PR has a merge conflict. Do you think you could run make examples one more time and upload them? Sorry about that! @johanbrandhorst feel free to squash this if you get to it before me.. Sounds great to me! @mention me when you're ready and I'll take another look. Did those commits come from a merge? Is there any chance you could rebase off master?. To my eyes, it looks like the Ci is also failing. Could you get that squared up first?. I think this is fixed by #660. You can pull from that repo instead and try it out or git pull from it in\n$GOPATH/src/...\n. Can you elaborate on this and explain what you tried and why it didn't fix\nthe issue?\n. Hey @utrack, thanks for sending in a PR! This is definitely a change I would like to make. The swagger generator is objectively wrong. Unfortunately it is wrong in a way that is paired with our default marshaller. In order to make this change we would have to change the default marshaller, which is a kind of big change.\nWhat would you think about adding this as a flag controlled argument to the swagger generator? That would let you continue forward while I try to figure out a path to removing the bad marshaller.. @rogerhub, I appreciate the concern but I would like to leave @ithinker1991 able to send PRs and file issues not just on grpc-gateway, but in the rest of the grpc-ecosystem projects. Ultimately, these PRs (confusing and strange though they may be) are a sign that they are interested in the trajectory of the project and therefore have a vested interest in its welfare. I would, however, understand if you're annoyed by these emails. You can block them personally which will prevent you getting PR notification emails which seems like a fair middle ground. @ithinker1991 if you would like to talk I'm all ears and eager to hear what it is you're trying to do. I'm going to close this, but feel free to reopen if you have some constructive steps forward.\nThanks!. @DuyThangDecima did you try reading through the documentation? Do you have a gRPC server running in Java already? What have you tried doing?. As of right now, grpc-gateway doesn't generate any Java code, let alone spring boot definitions. There's no way to generate a version of grpc-gateway that runs in Tomcat instead of in the go run time. There are, however, some projects that generate similar functionality for Java. I've never used them and cannot attest to their quality but I do know they exist. I'm reluctant to recommend any particular one though because I haven't used any of them. They're pretty easy to find on Google. Do you have any luck with them, please report back here so we can find out what the experience is like.\nThank you!. @johanbrandhorst would it be possible to have the Makefile build protoc-gen-go from the vendor directory and use it instead?. I'm not sure. I'm going to retrigger the CI to run. I'm thinking there was probably a transitive outage at one of the hosting sites for our dependencies.. I think this is reasonably well documented at https://grpc-ecosystem.github.io/grpc-gateway/docs/grpcapiconfiguration.html. Can you check that out and report back what additional information you need or how we could make that easier?. What have you tried already? What documentation have you looked at?. @ch3rub1m, thanks for the detailed and interesting bug report. You have definitely run into an issue with our Swagger generation.\nReading through the linked documentation, I think you're interpreting things in a valid way, but I'm not sure how we would map it in Swagger/OpenAPI land. The concept works in the gateway because it can parse the whole string and determine the match, but in Swagger it wouldn't be able do to that since name is just a string to the type system. I would be very interested at hearing alternative ways to approach this, but as of right now the only solution I see is to call the field something other than name for the 2ndary match. You could also deviate from their naming scheme and decompose the URL into the constituent tuples with something like:\nrpc GetService(GetServiceRequest) returns (Service) {\n    option (google.api.http) = {\n      get: \"/v1/service_categories/{category}/services/{id}\"\n    };\n    option (grpc.gateway.protoc_gen_swagger.options.openapiv2_operation) = {\n      description: \"Get a service.The **name** must have the format of `service_categories/*/services/*`.\";\n      tags: \"Service\";\n    };\n  }\n  rpc GetUser(GetUserRequest) returns (User) {\n    option (google.api.http) = {\n      get: \"/v1/users/{name}\"\n    };\n    option (grpc.gateway.protoc_gen_swagger.options.openapiv2_operation) = {\n      description: \"Get a user.The **name** must have the format of `users/*`.\";\n      tags: \"User\";\n    };\n  }\n. What happens if you manually edit the Swagger to say that?. I guess that means it would be possible to edit templateToSwaggerPath and the associated tests to render that correctly. The function is very encapsulated and doesn't manipulate or depend on any external state. All you would have to do is add some test cases that match your use case. That's a very welcome change if you would be interested in taking it on.. I'm confused. It looks like that PR is empty. What am I missing?. Yeah, the problem is that some library we depend on for serializing changed their default for serializing enums from the numeric representation to the string representation. A reasonable change to make, but we need to explicitly set that in whatever library that is. Sorry I just haven't had time to dig into it. Could you rebase off of master? We have since fixed the CI issue. Also, I was thinking about this. Would you be willing to (feel free to push back) add some new usage in the everything example? I would love it if we could do an e2e too. Yes, please. Feel free to add as many examples as you think are necessary. Can you please rebase this off of maytter please?. Yep -- sorry about that. Typing is hard. As always @theRealWardo, thanks so much! It's a pleasure getting to do this with you. In my experience, I've deeply regreted not returning an object at the top level. It made migrating that endpoint a horribly breaking change and required months and months of work to get things back in shape. That said, if you're trying to reverse engineer and take over an existing API then that is what you must do and that is a very valid use case.\nLooking through the docs on (HttpRule)[https://github.com/googleapis/googleapis/blob/master/google/api/http.proto], you almost want a \"reply body\" parameter attached. I haven't seen anything like that, but it might be a thing upstream is receptive to. If you could get it in the HttpRule proto then I would be 1000% in favor of supporting it.\nHow can I help with this?. @ivucica and @johanbrandhorst, you two are the absolute best! Thanks for caring so much about this project :smiley: . @moscicki, maybe you could elaborate a bit on your question a bit. gRPC is not a file storage protocol, it is a set of tools for creating a protocol. If you want to build a file transfer protocol on top of gRPC that would be an interesting effort, but probably beyond the scope of this repo. WRT being S3 or WebDAV compatible, I took a cursory glance at the protocol expectations of S3 and WebDAV and it doesn't look like it will be easy to shoe-horn into grpc-gateway. If you decide to advance down this path, feel free to comment into this issue to point to your repo, I'd love to see what you end up building. I'm going to close this but if you have more questions feel free to reopen it, comment on it still, or open another issue. @johanbrandhorst Very strange. We used to have this kind of problem all the time. It was from proto-go being upgraded and changing the output format. When there are no env variables set, we compare the compiled versions of the .pb.go to the version that is checked in for equality (golden testing). \nHowever, with the submission of #696, I wouldn't expect that to happen any more. I don't know a lot about dep unfortunately. I was hoping we could just wait till vgo, but I'm glad we did this. Looking at this line is it possible that we are overriding our go dep and explicitly installing the HEAD version of proto-go?\nThe other two failures appear to just be standard tooling errors. If we paid for Travis I bet it would be more stable, but the free version is kind of flaky. I re-triggered them and they passed.\n. @johanbrandhorst, I wasn't meaning to imply that line specifically, but is there something in that file you would expect is updating our dependencies to head instead of the vendored versions?. I fixed it by manually applying the patch. Could you please rebase this PR to see how Travis feels about it now? Thanks!. Please fill out the issue template or file another bug. This is not enough information to allow us to understand what went wrong so we can help you. Thanks!. @jon-whit unfortunately I don't have any specific ideas here. Step one is going to be create a failing test case. Do you think you could send in a PR that demonstrates this reliably?. @toranger, could you include not  a screenshot, but a copy/paste'd version of the go code you're using to register the grpc gateway handlers and also to start the server?. Can you write a bit about what it is you've tried and what you think might be wrong too? I'm happy to help with a specific problem but I don't have time to dig through a couple hundred lines of code and dozens of linked libraries -- sorry. Can you write a bit about what it is you've tried and what you think might be wrong too? Are you getting any other error messages for example?. Are you curling to the gRPC endpoint or to the http exposed endpoint? What is the curl command you're running?. when your run that command with the additional arguments -vvv what do you get?. In general unless you're working with a GUI application it is not helpful to post screenshots of your terminal. Can you please copy/paste the text instead in the future?. In your go code where you call return svr.ListenAndServe(), can you add a print statement that indicates the port/address it is listening on and verify that in the output of the program?. Can you include the complete output of the curl command? All the way from invocation to the end of the output? When you paste it in here, if you follow these instructions on how to create a code block, it will be much easier for me to read.. Unfortunately, proto doesn't really support union types. The only way I can think of doing this is to always take a string. I believe the json parser is nice enough to cast numbers into strings but I'm not 100% sure on that. You might also consider looking into one_of types, but I don't think they exactly provide these semantics.. Huh, I guess it really doesn't support them. The only other option I can think of is writing your own marshaller for jspb or writing an interceptor in http land that rewrites the request. Sorry!. Unfortunately I don't think we have docs that demonstrate that. If you decide to go that route, it would be great if you could add some documentation into the /docs directory. That's a very clever idea! Good thinking @ivucica . Could you post the code you ended up using in here? It might be useful to others in the future. LGTM. Let's not break weekend hackers. Hey @kasuboski, thanks so much for your interest! We've been talking about doing this migration but no one has the time to do it for a while. Sounds like you're blazing a new path forward in the golang world, would you have any interest in generating anew, or migrating our existing gopkg file? That might be the quickest way to get to this working if that's your first priority. Let me know.. @ogimenezb If you generated a go mod file for grpc-gateway, would you be willing to upload it as a PR?. @johanbrandhorst, there is no extra magic that is necessary to achieve that outcome. The integration that we already did with Gazelle should handle that.. @wora, thank you so much for your contribution! These documentation fixes are some of my favorite kinds of changes. It's so hard to look at some text you know very well and determine what could be improved about it. Thanks for the extra set of eyes!. Down with the CI overlords!. Let's merge https://github.com/grpc-ecosystem/grpc-gateway/pull/763 to test it out first.That okay?. Retriggering to see if circleci is triggered. I don't think I can activate circlecI without a circleci config to start with. Let's just commit this and see what happens. I'm still getting those opaque CircleCI errors. I am looking into it but have contacted support. Maybe they can help out. Since @johanbrandhorst has circle set up on his repo I think it is doing some especially clever deduplication of work (the git hashes are identical) and therefore it won't rerun. This is preventing the CI bot from posting the success status.. It should be very easy to post-process the yaml and remove anything that doesn't fit. You could also submit a PR to disable it by flag and then in 2.0 we can make it the default.\nWhich path interests you more or would you be interested in forging a third path?. I think you can achieve this today without writing anything new in grpc-gateway by using eitehr a grpc client interceptor or a server interceptor in the destination server. Can you help me understand why this needs to be added to the routing framework over hooking into the preexisting interceptors? . @wongoo Perfect! Thanks so much for following up. Would you be willing to write a little bit up? If so, we could put it as a FAQ in our documentation so that future users won't have to struggle quite as much. What do you think of that?. LGTM. I'm not merging because there is a chance you want to watch the next run. Feel free to merge at will. Gazelle is a good solution to this problem in the long run. It's a simple binary that is able to regenerate all the BUILD files as they should be written including doing these kinds of formattings.\nThe maintainer of Gazelle, is presently working on expanding his binary so that you can add language plugins. Once this happens I am going to write a language plugin for grpc-gateway that auto-generates these BUILD files. When that happens I will make it a part of the presubmit process.\nI'm going to merge these because my hopes/dreams shouldn't slow other ppl down.\n@drigz, thanks so much for contributing! If you know about Bazel and would be interested in contributing to the project, there is a lot more neat stuff we could do!. > This has become bigger than initially anticipated I think, but lets wait for @achew22 to have a chance to review it as well before merging. Huge thanks for getting the ball rolling on this though @drigz :).\nI think we should merge this as is. It will probably break people but upstream changed on us and we don't have much of an option. If this behavior is required by integrators there is a parameter they can set.. Fixed by #809. Thanks for your PR!. I am having a hard time construction the situation where you need to run docker to invoke gazelle. Gazelle is used to bootstrap into bazel and doesn't have any hard dependencies on it.\ngo get -u github.com/bazelbuild/bazel-gazelle/cmd/gazelle. It should just be standard go code and work fine without needing bazel, but I'm less certain about it (since it's not designed to be the on ramp).\nNot at a computer for a while but I would guess something like\ngo get -u github.com/bazelbuild/buildtools/buildifier. Perfect, thanks so much for your contribution! If you have any more issues please open an issue and let's work through it.\nIt looks like there is a formatting issue with your change. Could you run buildifier (bazel run :buildifier) and update the PR? Thanks\nPS: I recently learned about the --override_repository flag which can be used like --override_repository \"com_github_grpc_ecosystem_grpc_gateway=/path/to/a/local/copy/that/you/want/to/test/with\" if you want to test out changes like this in your repo without having to rewrite your WORKSPACE.. I was thinking about this, and as a curiosity, what are the circumstances where you need a py_proto_library for this? I expect these protos to really only be consumed by the swagger code generator. That's an unfortunate outcome but it makes sense that proto works that way. I meant to merge earlier, but forgot to click the button. Sorry about that. @glerchundi, Yep that is on me. Lemme send a PR to fix Bazel. Sorry about that.. https://github.com/grpc-ecosystem/grpc-gateway/pull/841 should fix that. If you could please rebase it would be hugely appreciated. Thanks!. @birdayz I'm in strong agreement that your contribution shouldn't be lost. I will make an exception to the single commit/PR rule to ensure that you get credit for your hard work. Thanks so much for contributing to the project. Without people like you this couldn't work. I think adding a new option to the compiler is the way to go.\noptions = [\"logtostderr=true\"],\nBecomes\noptions = [\n    \"your_new=option\",\n    \"logtostderr=true\",\n]\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/20e8cf9fb34a4c43782226b3e0dd42913a5c0a9b/protoc-gen-grpc-gateway/BUILD.bazel#L28. @UladzimirTrehubenka, this looks like a great way to fix it to me! Thanks so much for taking the time to investigate and fix the problem in a way that allows everyone to benefit. Looks like you need to do the CLA, and I'm going to give @johanbrandhorst a chance to look at it also, LGTM.. This would be a really great starter project for getting involved!\nThis comes from the interaction of these two pieces of code.\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/c3923b1034cd4a10cea4dfae173804b19f6f7fe0/protoc-gen-swagger/genswagger/template.go#L120-L124\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/c3923b1034cd4a10cea4dfae173804b19f6f7fe0/protoc-gen-swagger/genswagger/template.go#L294-L296\nA little bit of rejiggering in there + a new test case and I think you'd be done if you're interested.. @DaHuMao, per the issue template, can you please document the steps that you have tried to alleviate your problem?. @fahernandez, thanks so much for finding a problem and reporting it but then going above and beyond to fix it for everyone! Really really great work. I look forward to doing it again soon.. Thanks so much for the PR! Do you think you could add a test that verifies this behavior so we don't accidentally regress in the future?. The provided gateway doesn't support it, that's why you can't generate swagger for it. if you would like to extend the swagger generator to support this, I would be happy to merge your pull request.. I think reverting the delete of the WORKSPACE is all you should need to do to get it working in Bazel. Once you merge this I'll do a follow up to upgrade to the latest rules_go which has proper support for go modules and clean it up in there a bit.. @philipithomas, unfortunately we don't have a roadmap. Maybe this is something you could help out with? For example, I've not yet found something that we can express in the Google http annotations that don't fit nicely into OpenAPI v2. Are there any pieces that you're aware of that don't translate well?. https://github.com/bazelbuild/bazel-integration-testing/pull/108 needs to be merged then we can switch to it and it'll fix it.\n@laurentlb thanks so much for doing this :smile:. Looks great to me. Thanks for doing this!. grpc is a dependency that gets managed by rules_go so this is good to go. Thanks so much for the PR!. I assure you it would be well received \ud83d\ude01. Looks like really great work! FYI, I think the way to fix the generate CI failure is to run\ndocker run -v $(pwd):/go/src/github.com/grpc-ecosystem/grpc-gateway --rm jfbrandhorst/grpc-gateway-build-env:1.12 \\\n    /bin/bash -c 'cd /go/src/github.com/grpc-ecosystem/grpc-gateway && \\\n        make realclean && \\\n        make examples SWAGGER_CODEGEN=\"${SWAGGER_CODEGEN}\"'\nwhich will regenerate the examples. I'm really looking forward to this!. Hey @chrispsommers, thanks for reaching out.\n\nCan grpc-gateway be used to proxy in front of an existing gRPC service? I'd like to present a unified TLS server like in the examples, which would either pass gRPC directly to another gRPC server, or else translate REST into the equivalent gRPC and pass onto another server. I don't want to implement any of the services in the gateway itself. Then I can just drop it inline with an existing service (listening on localhost) and present an external combined REST/gRPC service. Thanks.\n\nIf I'm parsing your question correctly, yes I believe grpc-gateway is capable of doing that. You could compile a gateway binary and deploy it as a separate program next to your main gRPC server. When you say \"unified TLS server\" does that mean a TLS server that handles both HTTP (translating them into gRPC requests and proxies gRPC directly to the upstream server? If that's the case, we don't have any provisions for proxying gRPC directly as of today. I believe there are a few projects that do proxy gRPC requests and you could probably stitch the two together but I don't know of any examples that you could use as a guide, sorry.. That's definitely an interesting idea. From what I've seen in the proto community is that people generally expect different plugins to protoc. Do you know of any examples of people passing flags to change the output to a different language?\n. Yeah, really good tip. Having the template available in the swagger side will allow me to split it and iterate over it to fix those entries up.\n. I think you're referring to https://github.com/golang/protobuf/tree/master/protoc-gen-go which is the only protoc-gen-go and grpc-plugin I'm aware of. If I'm talking past you, please disregard.\nIt sounds to me like you're trying to ensure that the two generators are going to be versioned the same. Respectfully, I think this concern is not something that can be solved at this level. If you're a repository maintainer, you need to be tracking all the code generators you depend upon and staying up to date. I've several times gotten broken by drift between protoc-gen-go and grpc-gateway's gw definitions (not to mention the other generators). I think that keeping things in sync is a known issue with the entire ecosystem.\nThis is especially true because the code generation doesn't stop here. With the introduction of swagger support I am hooking up a JS code generator based on swagger-codegen (I will push that upstream). There are now 4 independent code generators in my project (protoc, protoc-gen-go, protoc-gen-grpg-gateway, and swagger-codegen). I expect to become familiar with the upgrade process for each.\nFurther, using the protoc-gen-go has a fairly obtuse parameter passing syntax --gen-go_out=plugins=grpc:. is puzzling at first, to say the least. I now know the syntax but to learn it I had to put log.Printf statements into the code, recompile it and waste a day or two on digging through code to learn why gRPC bindings weren't being created. Having these multiple modes which each produce valid outputs may be \"easier\" for you/me, but I think that it raises the barrier to entry for code generation which will reduce adoption. In summary, I feel the principle of UNIX is in effect here, \"do one thing and do it well.\"\nIf your concern is keeping the code behind each generator in sync, I think that is orthogonal to the issue of having multiple binaries. We could have one folder that produces two binaries or two folders that produced a single one, it is just a matter of configuration. What code do you have in mind as being difficult to keep in sync between the two? Maybe we can find a way to abstract that logic into something a bit more pleasant to work with.\nLooking forward to your responses and thanks for reviewing my code.\n. Thank you\n. Great eye\n. Done\n. Good style note\n. Thanks\n. Thanks\n. Saving memory :+1: \n. Thank you\n. Sorry, this is my first real go code. I'm still learning\n. Thank you\n. Thank you\n. Thank you\n. Good eyes :smile: \n. Does this make swagger generate a streaming client?\n. CROS => CORS\n. I'm also interested in the answer to this. What happens for deeply nested fields?\n. Initially I was concerned that this would be slow because of a bunch of calls to strcmp. Turns out that golang builds a binary tree at compile time and uses that to search. Cool!\nhttps://github.com/golang/go/issues/10000\n. Thanks @wing328 for taking a look.\n@EranAvidor Do you think you could update this to say \"date-time\" instead of \"0000-00-00T00:00:00.0000000Z\"?\n. What permanent headers start with \":\"?\n. It isn't really a \"header code\" it is a http status code.\n. It is added as a gRPC header\n. md will never get printed. Nothing expects it in the string\n. why not f, err := os.Open(*file)? without the var on the above line? Is there variable shadowing I'm missing here?. Should the default be [\"ZERO\"]? or maybe []?. Can you add a -u to this call to go get. I think there is a stray ` at the end of this line. Can you add a trailing \\ on this line?. Why did it change the date on this release?. It seems to me that isPermanentHTTPHeader and the following MetadataHeaderPrefix check are just the default forms of the header matcher. What would you think about moving them into a HeaderMatcher and providing them an overridable default headermatcher? That way we can unblock a lot of the bugs around headers not behaving the way people expect?. When would these functions be called?. Can you also add a couple of simple tests to validate this?. Do you know why the enum values are empty?. 100%. Would you be willing to file a bug?. swaggerExtrasRegexp... what's that? Could you add  a comment here explaining it's purpose? It doesn't seem to get used anywhere else. Can you promote error checking to immediately after the call to extractSchemaOptionFromMessageDescription?. Nit: \"want a Schema\". Let's do this soon so we can merge. Nice touch on reserving all the numbers for the order they appear in the swagger spec!. What is the correct default here if schemes is empty? Right now I don't think it emit anything but is that the correct behavior? Should it emit \"http\" only so at least there is a value?. Can you add a note on why they are all the same ID and why that is okay?. Shouldn't the file name end in .go?. I've been thinking on this for a bit. I think you're just exposing helper methods against jsonpb. That should mean they are well tested. If these were to become more complex then I would want tests but they are just so imple I think we should leave it.. I don't think we should merge with this sourcing from another repo.. I think we should drop 1.7.x right now and drop 1.8.x when 1.10 is released. I think it would be better to collapse the next few lines\nstat, err := status.New(codes.Unknown, \"with details\").\n  WithDetails([]proto.message{\n  ...\n  }). I'm all for short variable names, but this is too far away from the code using it to use a 2 letter abbreviation IMO. \"contentTypeSet\" would be preferred. Do you think you could add a test for this?. Can you elaborate on what \"v\" would be?. This is going to be a painful breaking change to the API. Is there ANY way to do this without passing in v?. This seems incorrect. Why does the default marshaller need to change from this change?. Can you elaborate in this error message. As I understand it the conern here is that you could make the querystring ?foo[bar] which doesn't have a value. Could you add a remark about what was expected and why this is an error condition?. Can you add some examples where you populate multiple map values at the same time?. Could you enhance the error message to reflect that intent?. I don't see an entry for bearer (in authorization header). Which one would that map to? Reading the docs I guess an apiKey with the header \"authorization\"?. \"Thanks to backwards compatibility\" doesn't seem to follow from the rest of the comment. It seems like \"thanks to structural typing\" may be a better way to phrase that?. This is the same configuration as the default global unmarshaller. Is it important to keep them in sync or are you just using the unmarshaller to load the data into a proto shaped object for use during generation?. Can you confirm for me that this is only used during the protoc stage and not during runtime? If it is used during runtime, can you describe the circumstances when it is used?. Could you add a comment in here that the file you're avoiding importing has 23 import statements and that is why it's being avoided as well as a note that says \"If a significant subset of these (>50%) are being reproduced in this file, swap it out for the generated version.. Can you move this into its own file?. registerHTTPRulesFromGrpcAPIService. Solid tests. Good work!. This feels like something that should be added to the public interface of the registry rather than mucking around in the internals of the registry object. You have a getter for external http rules, I think you should add a setter and use it instead of direct manipulation.. s/optss/options/\nLet's be explicit that we are talking about plural of opts instead of it being a typo of opts.. Can you help me understand why this is preferential? Could you also add a test demonstrating why this is preferred. These are a bunch of relatively expensive operations to be performing on every request. Is it possible to do this outside the context of the request?. What are you using the label for?. Would it be possible to split the lookups into two kinds, one for exact strings and one for regexp matches? If we did that then the lookup for exact strings could remain O(1) (non degenerate case), and then do a lookup based on regexp O(n) where n is the number of regexp based matchers you have?\nAdditionally could you create two methods for registering a new marshaller, one for regexp marshallers and leave the static one for strings alone?. This seems like an expensive operation to be performing on every request. Is there any way we could move it into the setter?. It is a breaking change. I try not to make those except on major version bumps. If you want to be added to the list of 2.0 breaking changes and do it then, I'd be happy to do that.. Unfortunately adding another method to the interface suffers from the same API breaking change problem so I would rather go with the simpler breaking change if it is possible. Just spitballing cause I don't think it's possible, there isn't a way to make a type union or something, is there?. I've been staring at this and I think you're right. That would not be API breaking. I think we should go that route.. Do you have any theories on that? I would expect protoc to propagate the nonzero exit code through.. I would prefer to panic on error instead of silencing in a must function . Duplicate logging. Can you remove the one on line 37. /healthz ;-). I think we can add a .swagger-ignore (TODO: double check that is the filename) that will prevent this file from being generated. Along with the other .md files.. What would you think about making this a page in the /docs section and linking to it?. Can you make this line match the test style you used below?. The 2nd one is is the reason I was looking for. WDYT about consolidating version numbers into the env section? . Use time.After to produce your timeout. Could this be done as a glob? If we do it as a glob and use he comment I left in the generator, then if something changes in the swagger definition and creates a new .go file it'll be updated automatically. Github is being... weird. I typed out a whole comment in here but it wasn't included.\nHere is approximately what I wrote:\nWhat would you think about using a temp dir to generate these in along with a .swagger-ignore-codegen (or set of rm statements) and then a mv from the temp to the final out_dir_spec followed by an rmdir on the temp dir. This has the nice benefit of making it a compile time error if a new file is generated but not enumerated in the srcs section.. rm unnecessary line. Not quite done?. I think you need to add the License field to the Info object. I'm surprised this isn't a compile time error. Why do you need to make this public? the go_proto_compiler target is already public.. I use this in a project right now, so please forgive my skepticism. Can you create a small repo that reproduces that outcome? I would like to file a bug against the appropriate repo if it isn't working, then we can make the whole package public visibility.. Out of curiosity, why did you change everything to be tabs? Can you run this through a modern version of clang-format?. Oops, I left that comment on the commit directly -- sorry. Can you make the indentation match the rest of the file?. The rest of the file uses tabs instead of spaces for indentation. I think this should be an error instead of a return \"\". . I think you should also check to see that the FieldMask isn't repeating. That's a good thing to call out during the review. I would love to do this without reading the body twice.\n@ivucica @tmc  @yugui, can you think of a better way to do this either?. Seems that this suffers from the multiple read problem as above. Let's see what shakes out there. Can you extract the struct into a named type (since you use it below) but leave it unexported. Is the verb in this case \"123\"? What does it mean to have a verb of 123?. Ah, I see. You are trying to describe the positive case, and not the negative case as I had interpreted it. That seems entirely reasonable. Merging.\nThanks so much for fighting through my confusion and for contributing to the project as a whole!. Could you add a link to https://github.com/grpc/grpc/blob/master/src/proto/grpc/status/status.proto?. This seems like an odd change. Why did this become the file path?. its => it's. I believe this should just be bazel run :gazelle. I think this line being removed is what is causing you issues. The way it is configured right now, you've only got the grpc protoc plugin, you also need the grpc-gateway protoc plugin.\nIf this was removed by Gazelle, then you can tag it with a #  keep after the line and it'll be happy. The full example would be\ngo_proto_library(\n    name = \"examplepb_go_proto\",\n    compilers = [\n        \"@io_bazel_rules_go//proto:go_grpc\",\n        \"//protoc-gen-grpc-gateway:go_gen_grpc_gateway\", #  keep\n    ],\n    importpath = \"github.com/grpc-ecosystem/grpc-gateway/examples/proto/examplepb\",\n    proto = \":examplepb_proto\",\n.... ",
    "dmitshur": "Hi @yugui,\nThank you for your excellent work on this!\nI've been testing out an earlier version of this branch. I'll be updating to the latest and giving it a try next.\n. Is the implementation of query parameters complete?\n. By the way, thank you very much for your excellent work on this @yugui, it is very helpful and much appreciated!\n. @philips, you can find relevant background information the description of PR #12.\n. > As far as I can tell, a protobuf name like uri becomes URI when generated.\nSorry, it turns out that's not a general rule. It was happening in an example I saw because the name was overridden via a gogo customname field, i.e., [(gogoproto.customname) = \"URI\"].\nI think this issue is invalid, so I'll close it.\n. I've opened #21 that resolves this issue, please take a look and review.\n. I just realized (thanks @sqs) that I missed a very important bit:\n\n* matches a single path component, ** zero or more path components.\n\nSo it is possible.\n. Confirmed that it works. It's possible to achieve the above by doing:\nget: \"/v1/dirs/{dir_path=**}\"\n. @slimsag Yes, the above is correct.\n\n(but it could be a URI encoded path)\n\nI don't think so. This is a url path element, not a query parameter. Edit: I could be mistaken, see e.g., https://github.com/golang/go/issues/5777#issuecomment-113644458.\n\nHowever, I'm reopening this because I find that it only works if such segment is at the end of the route. It doesn't seem to work when it's in the middle.\nFor example, this works:\n```\nget: \"/v1/dirs/{dir_path=**}\"\n// They all work.\n/v1/dirs/foo\n/v1/dirs/foo/bar\n/v1/dirs/foo/bar/baz\n```\nBut this does not work:\n```\nget: \"/v1/dirs/{dir_path=**}/.files\"\n// None of these get matched.\n/v1/dirs/foo/.files\n/v1/dirs/foo/bar/.files\n/v1/dirs/foo/bar/baz/.files\n```\nIs that a bug?\n. > Your question is whether the multi-segment wildcard ** is greedy or not.\nThat is true. I agree that the spec doesn't seem to specify one way or the other. Thanks for looking into it.\n. Thank you for resolving this!\n. What's the error exactly?\n. I think it already does what you want, if I understand you correctly.\nThe description says:\n\nIt reads gRPC service definition, and generates a reverse-proxy server which translates a RESTful JSON API into gRPC.\n\nThe code it generates ends up translating RESTful JSON API into gRPC, but in other words it exposes a RESTful API from the existing gRPC service definition you already have.\n. Travis CI build is failing because of -logtostderr flag still being present.\nIn general, I am also not a fan of the glog package being used for logging, I wish it were just the standard Go log package because it's simpler. But I can imagine other people their own preferred log libraries, so having an interface is the only way to allow that.\nI think it's fair to follow the same strategy as grpc-go then. That would be consistent.\n. LGTM.\n. If I understand correctly, what people don't like is that the strings rpc error: code = and desc = are there in every single error message. They're not the actual content, so including them in the output is potentially pointless, it could be considered noise.\nI don't feel very strongly about this either way.\n. Wasn't there a very similar proposal before? See #54. Or is this different?\n. Nice change!\n. One of your dependencies is out of date. Using -u flag will fix that and allow it to work.\nbash\ngo get -u github.com/gengo/grpc-gateway/protoc-gen-grpc-gateway\n. After looking at this more, I suspect it may be not possible because we've used a custom gogoproto option (gogoproto.embed) = true to get ListOptions embedded into ExampleOptions:\n``` protobuf\nmessage ExampleOptions {\n    bool Foo = 4;\n    string Bar = 5;\n    string Baz = 6;\n    ListOptions ListOptions = 3 [(gogoproto.nullable) = false, (gogoproto.embed) = true, (gogoproto.jsontag) = \"\"];\n}\n// ListOptions specifies general pagination options for fetching a list of results.\nmessage ListOptions {\n    int32 PerPage = 1 [(gogoproto.moretags) = \"url:\\\",omitempty\\\"\"];\n    int32 Page = 2 [(gogoproto.moretags) = \"url:\\\",omitempty\\\"\"];\n}\n```\nBut curious to hear your thoughts on this.\n. I am in agreement and not breaking that spec is important to me too.\nI am primarily asking if there's a spec-complaint way to achieve this ability to shorten some paths. It's reasonable if that turns out to be out of scope and not possible.\n. No problem, thanks for considering this issue.\nI created it because I wanted us to think through this potentially unwanted scenario and see if there's a good solution to it. I agree it's helpful to not diverge too much from the underlying http.proto spec.\nAfter thinking about this in the back of my mind for many days, I still haven't come up with an idea that I would consider viable.\nSo, I'm okay to close this feature request issue as \"no viable solution\" that is in the spirit of this library.\nI don't want to try to provide convincing use cases because I don't think there's any that would warrant straying away from the spec.\nIs it all right if I close the issue then?\n. Doesn't this part of the example need to be updated to match the Google api definitions?\nIf I understand correctly, it should now be:\npost: \"/v1/example/echo\"\nbody: \"*\"\n. Is it right that this example has body: \"*\" while id is already specified as part of the URL path?\nThe rpc EchoBody also has body: \"*\" which makes more sense since that's where the id of SimpleMessage woudl be provided.\nI may not have a great understanding of how body is used exactly, is this described anywhere? Edit: I see it is described here. I think my comment is valid and this was probably a copy paste error. Can you confirm if that's the case?\n. You may want to insert var _ = json.Marshal or similar here, otherwise it will potentially generate Go code that does not compile:\n....pb.gw.go:13: imported and not used: \"encoding/json\"\nThat will only happen if none of the services have a body or parameters, hence json package goes unused.\n. Would you consider changing this import?\ndiff\n-import \"google/api/http.proto\";\n+import \"github.com/gengo/grpc-gateway/third_party/googleapis/google/api/http.proto\";\nThe motivation is that, for generating .pb.gw.go files in user programs, users would typically do something like this, as described in README:\nprotoc -I/usr/local/include -I. -I$GOPATH/src \\\n --grpc-gateway_out=logtostderr=true:. \\\n path/to/your_service.proto\nThat would cause the google/api/http.proto import to not be found. It would require users to add an additional verbose include path directive:\ndiff\n protoc -I/usr/local/include -I. -I$GOPATH/src \\\n+ -I$GOPATH/src/github.com/gengo/grpc-gateway/third_party/googleapis \\\n  --grpc-gateway_out=logtostderr=true:. \\\n  path/to/your_service.proto\nWhich is extra work that can be eliminated. What do you think?\nTo make it work, all imports google/api/http.proto would be need to be updated to github.com/gengo/grpc-gateway/third_party/googleapis/google/api/http.proto. It also seems to be more in spirit of Go to use a full import path rather than a relative one.\n. That sounds good to me, thanks for thinking about it. google/api/http.proto indeed seems better if it's the canonical path even after vendoring.\n. Should process exit code be 0 or non-zero when it emits an error?\n. Thanks for confirming.\n. Why 1.4 and not 1.5, which is the latest official release?\n. Thanks!\n. @peter-edge, what would the benefit of having SetResponseIndent be?\n. ",
    "yanivx": "Hi All,\nIs there any C/C++ api gateway developed as on today if not are there any examples I can refer to understand how I can link this reverse-proxy with my gRPC server written in C++. \nThis is what I am looking for. \n\nIt is probably possible to link the generated go code with C++ server since CGO can generate a shared library.\n\nBut How ? I am new to this and will be really helpful If I find some example to do it. \n. Thank you so much @yugui  :smiley: . ",
    "w4-sjcho": "Hi Yuki,\nI found: https://github.com/google/protobuf/issues/59\nSo seems more like a protoc issue, unfortunately.\n2015-06-01 22:10 GMT+09:00 Yuki Yugui Sonoda notifications@github.com:\n\nThank you for letting me know that. But it looks to be an issue in protoc\nor googleapis.\nLet me confirm to the upstream.\nAlthough the extension google.api.http in MethodOptions is an optional\nfield, protoc emits as if it is a repeated field for the syntax (2). I am\nnot sure what is the expected behavior.\nSome possibilities come to my mind.\n1. protoc should have combined those lines into one extension if the\n   two variations of syntax mean the same thing?\n2. goprotobuf should have done it?\n3. google.api.http should have been repeated field and grpc-gateway\n   should have combine the repeated values into one?\nHere is the dump of the code generation request.\n...\n    name: \"EchoBody\"\n    input_type: \".gengo.grpc.gateway.example.SimpleMessage\"\n    output_type: \".gengo.grpc.gateway.example.SimpleMessage\"\n    options: <\n      /* 38 unknown bytes /\n      72295728: \"\\\"\\x15/v1/example/echo_body\"\n      72295728: \":\\x01\"\n    >\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/gengo/grpc-gateway/issues/16#issuecomment-107446118.\n. \n",
    "crast": "OK I'll incorporate your comments\n. Also, from reading a few places ( 1 2 )   I'm going to remove the X- prefix from the header; as it's not recommended to be used\n. Updated diff\n. Any reason not to simply use the package google.golang.org/grpc/grpclog directly? There's already a hard dependency on google.golang.org/grpc and this package is always at risk of dealing with the API changes in GRPC, since it directly imports over.\nI would assert that people who want to set the logging output for GRPC probably don't necessarily need to set different logging output for GRPC gateway - they generally either want it going somewhere they can control it, or they want it fully off\n. ",
    "mwitkow": "Yup, I'm hitting the same example:\ngoogle_protobuf_0 \"google/protobuf/empty.pb\"\n    google_protobuf_1 \"google/protobuf/any.pb\"\n. It seems that the way that the gen-go handles this is just importing the same thing multiple times:\nHere's an example of google/protobuf/api.pb.go:\ngo\nimport proto \"github.com/golang/protobuf/proto\"\nimport google_protobuf1 \"improbable.io/proto_gen/google/protobuf\"\nimport google_protobuf3 \"improbable.io/proto_gen/google/protobuf\"\nSo maybe just drop the filename at the end?\n. We're also very much interested in this. The biggest benefit of using event-stream is that parsing a stream of JS events in a browser doesn't buffer up the whole content of the stream.\nI think implementing this could be relatively easy. Adding special case of Accept: text/event-stream into the generated code that will return a single EventSource event called data with the proto JSON as payload, followed by an empty data to force an event trigger.\nWe're actually interested in doing this ourselves. @yugui, would you accept PRs for this?\n. Yup, indeed, intalizing the field seems to have worked:\ngo\n    if protoReq.Deployment == nil {\n        protoReq.Deployment = &DeploymentId{}\n    }\n    protoReq.Deployment.Name, err = runtime.String(val)\n    if err != nil {\n        return nil, err\n    }\nThe tricky bit is to initialize the field without knowing what the type is. Some reflection perhaps? Or template-generating a NewDeploymentField that will act like Deployment.Reset()\n. The following function is my (probably misguided) stab at reflecting out of the problem. It instantiates all missing structs along the way.\ngo\nfunc instantiateMissingStructs(structPtr interface{}, fieldPath string) (error) {\n    fieldNames := strings.Split(fieldPath, \".\")\n    ptrToStruct := reflect.ValueOf(structPtr)\n    if ptrToStruct.Kind() != reflect.Ptr {\n        return fmt.Errorf(\"structPtr must be a pointer struct, is %v\", ptrToStruct.Kind())\n    }\n    currentStruct := reflect.Indirect(ptrToStruct)\n    if currentStruct.Kind() != reflect.Struct {\n        return fmt.Errorf(\"structPtr must be a pointer to a struct, is %v.\", ptrToStruct.Kind())\n    }\n    for _, name := range fieldNames {\n        fieldV := currentStruct.FieldByName(name)\n        if fieldV.Kind() != reflect.Ptr {\n            return fmt.Errorf(\"field %v must be a pointer\", name)\n        }\n        concreteType := fieldV.Type().Elem()\n        if concreteType.Kind() != reflect.Struct {\n            return fmt.Errorf(\"field %v must be a pointer to a struct\", name)\n        }\n        if fieldV.IsNil() {\n            fieldV.Set(reflect.New(concreteType))\n        }\n        currentStruct = reflect.Indirect(fieldV)\n    }\n    return nil\n}\nFor example in the above example, this would instantiate the missing DeploymentId struct under the Deployment field.\ngo\ninstantiateMissingStructs(&protoReq, \"Deployment\")\nOr if the URL reference was to deployment.app.id, and the assignment was protoReq.Deployment.App.Id, the following one would instantiate both DeploymentId and AppId:\ngo\ninstantiateMissingStructs(&protoReq, \"Deployment.App\")\nAny idea where to hook this up? I guess gengo/grpc-gateway/protoc-gen-grpc-gateway/gengateway/generator.go, but I don't know how to do the tests for it :(\n. So yea, if golang/protobuf#54 makes it instantiate a new type, this is trivial.\nOtherwise, I think I'll make a PR that reuse the query filter magic.\n. So I didn't realise that the grpc-gateway already had a reflection instantiator for the query filtering. The above pull request contains the fix that reuses this.\n. @yugui, do you need anything from me in relation to #34 to get this fixed? :)\n. This fixes https://github.com/gengo/grpc-gateway/issues/32\n. Yay, thanks @kdima!\n. @yugui regarding RFC 7239: \nthe change is in accordance to Section 5.3 of that RFC. It  says that the original Host header of the proxy request is passed into the Host header of the request to the backend.\nThe forwarded header is meant for forwarding information about the client, e.g. the client IP of the request that the gRPC Gateway got, and wants to pass this information to the backend. Which would be useful to do anyway, but probably in a different PR.\nI can split it up into two PRs, but I'm pretty swamped so I won't be able to do that until the end of the week.\n. @yugui you're correct, apologies for misreading :)\nThe PR has been updated with use of x-forwarded-host, and additionally added support for x-forwarded-for to indicate the caller's client ID.\nI thought about splitting it into another PR, but the tests are slightly coupled and it was easier to do in a single PR. I can separate it out if you strongly prefer that :)\n. @yugui, can you help me move this forward? :) We're maintaining that in our own fork, and would really like to have this upstream :)\n. Apologies it took me so long to get up to speed on this, we've been running this in prod for a while on our git subtree and had no time to upstream :(\nUpdated to comply with your comment and also rebased upon master with squashing commits. \nRegarding RFC 7239, it'd be great to implement that, however it's significantly more complicated to parse/set, and everyone supports X-Forwarded-For anyway for legacy purposes.\n. Done.\n. ",
    "slimsag": "@shurcooL Do I understand your findings here correctly?\nget: \"/v1/dirs/{dir_path}\"\nWould match only a single path element (but it could be a URI encoded path). Whereas:\nget: \"/v1/dirs/{dir_path=**}\"\nWould match multiple path elements (that are not URI encoded)?\n. I think that changing the import path is perhaps incorrect: google/api/http.proto seems to be the canonical import path as files like these in the official Google repository import it that way.\n\nIt also seems to be more in spirit of Go to use a full import path rather than a relative one.\n\nIn Go I would certainly agree, but with protoc I don't think there is any such preference.\n. Is a return here needed? Is it safe to continue if an error occurred?\n. Same question here.\n. ",
    "trinchan": "LGTM\n. LGTM! thanks for this!\n. LGTM except small comment :)\n. endpoint of ABitOfEverythingService -> endpoint of FlowService\n. ",
    "miekg": "Thanks for your reply. And, yes exactly what you describe. I will work around it.\n. ",
    "ghost": "OK that's clearer.\nI will definitely give this a try thanks\n. diagrams do help.\n. I am facing a similar issue. I also noticed that the problem did not occur on a different computer with \"same\" setup steps. Here is a contrived example that shows how to reproduce this issue:\nFirst I modified the Makefile to avoid the google/http issue (#67):\n```\n$ cd $GOPATH/src/github.com/gengo/grpc-gateway\n$ nano Makefile\nexamples: $(EXAMPLE_SVCSRCS) $(EXAMPLE_GWSRCS) $(EXAMPLE_DEPSRCS)\n    sed -i \"s/import _ \\\"google\\/api\\\"//\" ./examples/*/.pb.go\n$ make test\n```\nTest run successfully. Now I undo all the changes made by make test. This reverts the generated files back to their original version.\n$ git reset HEAD --hard\nNow, if I run make test, I get the following error:\n$ make test\ngo build -o bin/protoc-gen-grpc-gateway github.com/gengo/grpc-gateway/protoc-gen-grpc-gateway\nprotoc -I /usr/local/bin//../include -I. -Ithird_party/googleapis --plugin=bin/protoc-gen-grpc-gateway --grpc-gateway_out=logtostderr=true,Mgoogle/protobuf/descriptor.proto=github.com/golang/protobuf/protoc-gen-go/descriptor,Mexamples/sub/message.proto=github.com/gengo/grpc-gateway/examples/sub:. examples/examplepb/echo_service.proto examples/examplepb/a_bit_of_everything.proto examples/examplepb/flow_combination.proto\nE1210 06:26:49.972372    7693 services.go:107] No pattern specified in google.api.HttpRule: Echo\n--grpc-gateway_out: none of pattern specified\nmake: *** [examples/examplepb/echo_service.pb.gw.go] Error 1\nIf I run make realclean (and add the google/http sed rule) and then run make test, everything works again. @yugui, can you please help us fix this issue?\n. Thank you @yugui for looking into this quickly. Please merge #71 into master.\n. Thanks @yugui . Everything is working here. For someone checking this issue, the fix was:\n- Update grpc-gateway (go get -u ...)\n- Update protoc build command to include the correct path for google/api\n--go_out=Mgoogle/api/annotations.proto=github.com/gengo/grpc-gateway/third_party/googleapis/google/api,plugins=grpc:.\n. What is missing for this to be merged? Just to answer the question on deeply nested?. Same issue here and the link to the \"spec\" is broken (I suppose its this file https://github.com/googleapis/googleapis/blob/master/google/api/http.proto ). But its not clean what is a verb anyway.. This is an enhancement request to support more options when using protoc with grpc-gateway_out plugin. Beeing able to choose between TCP socket / Unix socket or direct function call for performances purposes.\nTaken this proto3 file :\ncar.txt\nUsing protoc to generate reverse proxy stub : \nprotoc -I/usr/local/include -I. ${GOPATHLIST} --grpc-gateway_out=logtostderr=true:. car.proto\nThis will generate the following function : \n```\n// RegisteCarHandler registers the http handlers for service Car to \"mux\".                                                                                                                                                                                                                                                                                        \n// The handlers forward requests to the grpc endpoint over \"conn\".                                                                                                                                                                                                                                                                                                       \nfunc RegisterCarHandler(ctx context.Context, mux runtime.ServeMux, conn grpc.ClientConn) error {                                                                                                                                                                                                                                                                    \n    client := NewCarClient(conn)\n...\n...\n```\nThe main issue with current implementation is that this will create a mux that will dial over \"conn\" to gRPC endpoint using a TCP socket, ie much more overhead than direct executing RPC or using UNIX socket as a transport.\n. ",
    "crackcomm": "Second reason makes it clear, after taking a look into grpc.SendHeader code it seems that it would require more changes in grpc-go than expected.\nThank You @yugui \n. ",
    "jimbojetlag": "Oh, too bad. I had the impression that there is a no proxy option. Guess grpc-gateway never works on App Engine :(\n. Closing because of #46  :(\n\n. ",
    "peter-edge": "I'm not sure on the Travis build failure - advise?\n. Note I'll have to merge the other two open PRs I submitted after this is\nmerged, but they can still be reviewed.\nOn Monday, September 14, 2015, Yuki Yugui Sonoda notifications@github.com\nwrote:\n\nLGTM\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/gengo/grpc-gateway/pull/52#issuecomment-140274858.\n. I'm not sure on the Travis build failure - advise?\n. Tests passing on travis now\n. Ping :)\n. Hey @yugui, what do you think of this PR? I need this ASAP, can we pursue this?\n. merge conflict fixed\n. Ah yea, I can fix that depending on the owner's feelings on this.\n\nYeah, there's a few options out there and it's inconsistent. I've started using my own https://go.pedge.io/protolog for structured logging in github.com/pachyderm/pachyderm (which is the initial reason I did this PR, so that logging would be consistent for the pachyderm repository), but I would go as far to say that most people use github.com/Sirupsen/logrus (including github.com/docker/docker), which this Logger interface is a drop-in for https://godoc.org/github.com/Sirupsen/logrus#Logger.\nglog is way faster though, for the record...which is why it is nice to have the option :)\n. @shurcooL @yugui I put together this https://github.com/peter-edge/go-dlog that might satisfy everyone, this would cover grpc-go too so ping @iamqizhao, we could make this common between both grpc-go and grpc-gateway, which would be a big win. I'll extend the documentation tomorrow morning, but it makes it simple, just `import _ \"go.pedge.io/dlog/glog\". Thoughts on this?\n. My hope would be I could get both you and @iamqizhao to agree on a common implementation, and I figured dlog might be a good answer haha :) If you add a Debug level to grpclog, it's basically dlog, but dlog makes it easier to bring in other implementations and provides other functionality (tomorrow morning I'd send a pull request to grpc-go with it for consideration).\nBut honestly it really doesn't matter much to me - dlog clearly only took 30 minutes to put together.\nAlso of note, I've benchmarked glog via proxy on an interface with protolog, and the difference is not measurable as far as I can see.\n. I thought I might have commented on this last night: re: the logging levels\nIn dlog (or whatever we go with), we could add something like:\n```\npackage glog // the one in go.pedge.io/dlog/glog\nvar (\n  debugVLevel = 1\n)\nfunc SetDebugVLevel(level int32) {\n  debugVLevel = level\n}\n...\nfunc (l *logger) Debug(args ...interface{}) {\n  glog.V(debugVLevel).Info(args...)\n}\n```\nFor reference, this is what I'm referring to: https://github.com/peter-edge/go-dlog/blob/master/glog/glog.go\nThis isn't as nice as having the full functionality of glog, but it does make it easier to abstract the logging package away (which is the goal of this PR :) ).\n. Using it would be nice, but note that in my benchmarks, jsonpb is\napproximately 6x slower at marshaling then the standard library package\nOn Tuesday, December 29, 2015, Eric Chiang notifications@github.com wrote:\n\ngithub.com/golang/protobuf/jsonpb\nhttps://godoc.org/github.com/golang/protobuf/jsonpb is a more compliant\nprotobuf -> JSON package than encoding/json, and is the only way\ngithub.com/golang/protobuf plans to support JSON encoding in the future.\nSee this comment\nhttps://github.com/golang/protobuf/issues/44#issuecomment-117385573\nThe newly added jsonpb package (see 67cbcad) is the official vehicle for\nJSON support...\nIt would be great to be able to using this package for marshaling. It\nmight even be wise to switch to it entirely.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/gengo/grpc-gateway/issues/79.\n. Ya, I know :) Ugh. Programming is no fun :)\n\nOn Tuesday, December 29, 2015, Eric Chiang notifications@github.com wrote:\n\nA lot of the overhead comes from checking to ensure the result is actually\ncompliant with protobuf's defined JSON mapping\nhttps://developers.google.com/protocol-buffers/docs/proto3#json. It's\nslower, but it's correct :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/gengo/grpc-gateway/issues/79#issuecomment-167843510.\n. @yugui @tmc @hbchai don't want to repeat work, but I'd propose what looks like (based on an overly-simple grep) a very small PR:\n\nFor code generation:\n--grpc-gateway_out=jsonlib={std,jsonpb}:/path/to/etc/etc...\nFor library code:\n``` go\nvar (\n  // DefaultJSONAdapter would wrap stdlib JSON functions\n  DefaultJSONAdapter = &defaultJSONAdapter{}\n  // JSONPBAdapter would wrap jsonpb\n  JSONPBAdapter = &jsonpbAdapter{}\n  globalJSONAdapter = DefaultJSONAdapter\n)\n// not trying too hard here with naming or interface definitions, just trying to show the concept\ntype JSONAdapter interface {\n  Marshal(proto.Message) ([]byte, error)\n  Unmarshal([]byte) (proto.Message, error)\n}\nfunc SetJSONAdapter(jsonAdapter JSONAdapter) {\n  globalJSONAdapter = jsonAdapter\n}\nfunc jsonMarshal(message proto.Message) ([]byte, error) {\n  return globalJSONAdapter.Marshal(message)\n}\nfunc jsonUnmarshal(data []byte) (proto.Message, error) {\n  return globalJSONAdapter.Unmarshal(data)\n}\n```\n. Ya actually that might be better :)\n. https://github.com/golang/protobuf/issues/50\nhttps://go.pedge.io/pb\n. Fine with me!\n. I can put together a PR\n. I'd argue to change to grpc-swagger, other projects are working on swagger plugins as well heh :)\n. Generally I see public variables in golang as constants, but there are exceptions - I would generally do something like:\n```\nvar (\n  responseIndent = \"\"\n)\nfunc SetResponseIndent(s string) {\n  responseIndent = s\n}\n```\nBut just my opinion.\n. removed\n. how do you create an instance of this then? I originally did this, but is it legal to do:\ngo\nsomeOption = runtime.ForwardResponseOption(ctx context.Context, w http.ResponseWriter, message proto.Message) { ... }\nI was not sure how to do this, to be honest.\n. ",
    "hbchai": "Sounds reasonable. I think options are probably better in the long run, as long as they are easy to set.\n. The jsonpb.Marshaler recently changed its default behavior to output camelCase, so not using jsonpb within grpc-gateway is now a compatibility issue. jsonpb users will have to set OrigName: true when creating the Marshaler to work around this, which took me a while to figure out and is less than ideal. I would like to see support land soon.\n. @peter-edge It might make more sense for the choice of library to be a new option to the generated RegisterXYZHandler function.\n. Signed\nOn Tue, Jul 5, 2016 at 7:10 PM, William King notifications@github.com\nwrote:\n\nSigned\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/gengo/grpc-gateway/issues/179#issuecomment-230629482,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AAbxkMgLiGL_78WzcpIDthemWrIBjpdfks5qSuR_gaJpZM4Iz_JF\n.\n. \n",
    "kdima": "I have fixed the mentioned issues. Could you please have an other look?\n. ",
    "johansja": "A fix to issue https://github.com/gengo/grpc-gateway/issues/62 .\n. Use of https://godoc.org/github.com/golang/protobuf/jsonpb instead of encoding/json can help.\n. You should be able to use one_of with the help of runtime.WithMarshalerOption.\n. +1\n. Maybe it is OK to lose error code in the message and make it cleaner. GRPC error code is translated into HTTP status code anyway in the next few line - https://github.com/gengo/grpc-gateway/blob/master/runtime/errors.go#L88 .\n. Just signed.\n. Ran into similar issue today. Added the failing test case at https://github.com/grpc-ecosystem/grpc-gateway/pull/281 . It seems that PopulateQueryParams needs some update to fix this. Might need some guidance on how this could possibly be solved elegantly.. Have a quick look at how jsonpb handle those WellKnownTypes. Added a fix for Timestamp support at URL param.. I thought I have signed one before? If not, can you point me to where I should sign for it?\nNot sure if we need to handle the other null value though. The code is actually referred from the protobuf project.. Wake up, googlebot.. ",
    "ivucica": "The following is from a project that's not opensourced (yet), so I can't point to the exact rules. One limiting factor is that I also have custom Go rules (based on Kythe's rules) and haven't gotten around to reworking Bazel's Go rules (see bazelbuild/bazel#828). Because the build rule for //vendor:protoc-gen-gprc-gateway is too dependent on custom Go rules that are too fragile and messy to be shared, I'm not sharing them here.\nInstead I'll try to describe my approach enough to get interested people started.\nThis is my genrule() in the BUILD file for the Bazel package containing file scheduler.proto:\n``` python\ngenrule(\n    name = \"scheduler_pb_gw\",\n    srcs = [\n        \"scheduler.proto\",\n        \"//vendor:github.com/gengo/grpc-gateway/third_party/googleapis/google/api/annotations.proto\",\n        \"//vendor:github.com/gengo/grpc-gateway/third_party/googleapis/google/api/http.proto\",\n        \"//vendor:github.com/gogo/protobuf/protobuf/google/protobuf/descriptor.proto\"],\n    outs = [\"scheduler.pb.gw.go\"],\n    cmd = (\n        \"$(location @protobuf//:protoc) \" +\n        \" --grpc-gateway_out=logtostderr=true:$$(dirname $@)\" +\n        \" -I$$(dirname $$(dirname $$(dirname $(location //vendor:github.com/gengo/grpc-gateway/third_party/googleapis/google/api/annotations.proto)))):$$(dirname $$(dirname $$(dirname $(location //vendor:github.com/gogo/protobuf/protobuf/google/protobuf/descriptor.proto)))):.\" +\n        \" --plugin=protoc-gen-grpc-gateway=$(location //vendor:protoc-gen-grpc-gateway)\" +\n        \" --proto_path=$$(dirname $(location scheduler.proto)):$$(dirname \\\"$@\\\")\" +\n        \" $(location scheduler.proto); mv \\\"$$(dirname \\\"$@\\\")\\\"/scheduler/\\\"$$(basename \\\"$@\\\")\\\" \\\"$@\\\"\"),\n    message = \"Processing proto file with grpc-gateway\",\n    tools = [\n        \"@protobuf//:protoc\",\n        \"//vendor:protoc-gen-grpc-gateway\",\n    ]\n)\n```\nThe WORKSPACE defines external repository protobuf. While ordinarily one would specify a git_repository() rule, I am already using vendoring + externals:\npython\nlocal_repository(\n    name = \"protobuf\",\n    path = __workspace_dir__ + \"/vendor/github.com/google/protobuf\",\n)\nI'm using a genrule() here because it was easier than writing a proper Skylark rule. Even this genrule() is quite hacky.\n. (Un)amusingly, when I fire up\nmake realclean\nmake examples\nprotoc-gen-go seems to be dumping out protos without protobuf:\"....:,json=protojsonfieldnamehere\". That should be the old behavior -- so I'm not sure why that's going on.\nHowever, after a bit of fighting with the gopath set up just for this purpose :-) the golden JSONs have been rebaked and they seem similar to the output from the failed Travis job. There seems to be no other differences related to the aforementioned protoc-gen-go issue. Update coming up.\n. Alright, I'm not sure how to tackle this. Travis build on Go 1.5 passes, but fails on tip, with a diff on gzipped FileDescriptorProtos.\nI'm almost certain that trying to push my local copy won't help much, given the aforementioned even-larger diff (completely lacking the json field in the protobuf annotations of generated Go structs).\nThoughts?\n. This is the same symptom witnessed by @kazegusuri in pull request #129. I'd say this is a broken test?\nEDIT: I can try, but there will be a diff.\n. Pushed. I'll be going to sleep now; however I would expect that both 1.5 and tip will be broken with b31478d.\n. Thanks, all, for your work on this! I've updated this pull request by merging master. Looks like it passes now.\n. Please let me know if something else is needed for this PR.\n. @tmc That sounds like a great idea. I'll work on that, including separating it into a separate PR.\n. @tmc See #145 .\n. Wouldn't a relative link that doesn't encode full repo path make more sense?\n. While OpenAPI is more accurate, Swagger is a very 'googleable' name -- it's a shame it's being dropped. Still, I've switched to the OpenAPI: prefix. This looks a bit off, given that absolutely everything else in this project refers to 'swagger'; nonetheless, you're right, if it'll live in .protos, it deserves a more long-term name.\nI also think using JSON here in the first place is a short-term cop-out fix (i.e. better than nothing), even though the resulting JSON looks just awful everywhere else. Long term, something that's still hierarchical such as:\nOpenAPI-Info-Version: 2.0\nOpenAPI-ExternalDocs-Name: Find out more at\nOpenAPI-ExternalDocs-URL: https://www.example.net/\nwould be more readable, while not much harder to parse. Perhaps even with Debian-style multilines:\nOpenAPI-Description: Hello world.\n This is a text over multiple lines,\n with a blank line, too.\n .\n This is a new paragraph.\nThat wouldn't look too horrible when seen by tooling of other languages in the generated code, either.\nThat's a topic for some other PR, however.\n. @yugui It's interesting.  I guess we need to describe the Swagger schema as a .proto, in which case we might be able to do away with (what will then be) the duplicate definitions in types.go.\nI probably won't have a chance to work on this any time soon, though.\n. SGTM, but again, I am not sure when I'll be able to work on this. :)\n. Rebased and pushed. No other changes, i.e. still not based around proto options.\n. Rebased and pushed once again. From: 4ea3bbf 3525ecb e3b11c1\nStill no work on proto options.. @yugui @tmc  Could you PTAL and let me know if the direction I'm taking is worth pursuing? I can also switch to just a single option.\nFor now I've kept the JSON in the comment, too.\nI have not tried to properly update all the tests, or squashing the commits, given we might change the direction.\nRandom thought: There is an interesting side effect if there is an option to put JSON in the comment: generated code would include this metadata in its docstrings. It might not be a bad thing, either?. @tmc Yes, but which variant? I'm currently creating a larger proto descriptor instead of just accepting a piece of JSON. See PR for details :). What I meant to ask is, should the proto option accept a JSON string, or as I started, describe the Swagger doc schema in the proto language :)\nI'll see what I can do to avoid the \"stutter\". :). @eddiezane This surprises me, but apparently, technically:\n\nAs a result, the last version of the Swagger Specification is v2.0, and the first OpenAPI Specification is version 3.0.\n\n(Taken from the post you linked to)\nOn multiple occasions, I was trying to figure out what to reply in relation to the rename, and my general observation is that a lot of the v2 spec still uses \"Swagger\" terminology (see [\"Swagger Object\"][1], for example). Also, proto generator that this PR applies to is called protoc-gen-swagger. So I'll do the rename where reasonable! :)\nI can try to tackle this this weekend. If you see updates here by, say, mid-next week, you can take my work and run with it. \n(On a separate note, I think OpenAPI v3 generator should be a separate generator. A quick glance at the spec about a week ago made me think it's distinct enough that it deserves a protoc-gen-openapi or so. That's a totally separate issue.)\n[1]: https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#swagger-object. I have eliminated JSON processing and added support for loads more fields. I settled on changing reference to specification from 'Swagger' to 'OpenAPI v2'. This eliminated a class of confusion such as \"swagger_swagger\".\nIt also eliminates pieces of JSON from the Go comments that included OpenAPI-specific data, such as external docs URLs. If those are useful, we can bring them back in a future PR.\nRemaining before this PR can be merged:\n\n\nFirst... someone, please confirm whether it seems like this is on the right track. I won't mind a round of code review :) @tmc? @yugui?\n\n\nI can't run the failing tests in examples/browser locally at this time. I should be able to do it over the weekend. I won't mind help with them, either, if there's anything obvious I messed up.\n\nI want to move the demonstration from echo_service.proto to a_bit_of_everything.proto.\nI should totally email protobuf-global-extension-registry@google.com and get us four extension IDs for attaching metadata to files, methods, messages and services (patching swagger object, operation object, schema object in definitions object, and tag object, respectively). We should not risk future binary incompatibility in on-disk protos which may merely import annotations.proto.\nI will squash most / all of the commits.\n\nWhat can be punted to a future PR:\n\nProcess remaining OpenAPI v2 properties that have already been added to the openapiv2.proto.\nSupport remaining OpenAPI v2 objects ('security', 'security definitions', etc)\nDocument individual fields in openapiv2. (Or remove the TODOs, as the openapiv2.proto option definitions generally won't be visible/autocompletable in any sort of smart code editor. Hence documenting fields might be an overkill, and referencing OpenAPI is enough.)\nUse existing 'deprecated' option on a method, not (just) OpenAPI field.\n. Looks like overriding \"host\" is a no-no, and breaks tests. :). Email has been sent. Pending reply.\n\n\nI would prefer if you omitted them from the openapiv2.proto file if there is no support for them.\n\nI'd like to leave them; they should definitely be added. Removing them (while leaving a comment and id reservation -- the right way to do it) would be extra work for not much benefit, IMHO.\nAlso, I'll deal with the rest of the changes I mentioned soon.. New IDs have been assigned. However let's block this on move to a_bit_of_everything.proto. I'll try to do this soon.. PTAL, I think this might be it for this PR on my side.\nI updated my Github settings so the correct email address for this PR should be used if you're squashing the PR.. No, I haven't. I wanted to get this merged in case someone else wants to continue, but otherwise I have not yet needed things such as oauth2 definitions in the proto.\nFWIW, I'll take the opportunity to speculate. I don't think I'll be working on the following, but it's an interesting thought, I think:\nI can imagine a solution integrating security definitions inside the grpc-gateway-compatible and swagger-annotated .proto file with an oauth2 identity provider.\nThere may also be a complementary approach to this PR, #145 (which is just overriding Swagger fields) which is: use more definition files to describe the API, rather than just the .proto itself. That is: integrate with and read things such as:\n\nhttps://github.com/googleapis/googleapis/blob/master/google/api/service.proto\nhttps://github.com/googleapis/googleapis/blob/master/google/api/endpoint.proto\nhttps://github.com/googleapis/googleapis/blob/master/google/api/auth.proto\nhttps://github.com/googleapis/googleapis/blob/master/google/api/usage.proto\n\nThe actual gateway would thus be complemented by an extra lib to support the above and by an oauth2 provider. That is, grpc-gateway's runtime+generator wouldn't be the only thing covered by protoc-gen-swagger, which would use this input to generate a more complete swagger description, too.. I'm a Googler, so no CLA is required.\nMy future patches will be authored by my work address. (It's a bit too messy to edit the old patches.)\n. I cannot trivially reproduce this with current head. Has this been fixed? Can one of the other commenters repro this?\nTests do need to be updated to cover this, but I don't know if I'll have time to do that.. One way of handling this is probably switching to syntax proto2; and checking whether a struct field is nil before dereferencing it.\nIf the field is nil, then the proto field was not set, and it was not passed to grpc-gateway.\nIf the field is not nil, then the value was set.\nI'm using this to skip updating fields that the user has not passed in the proto message and in JSON.  This is good enough for me, even if using FieldMasks (see \nhttps://github.com/grpc-ecosystem/grpc-gateway/pull/812) might have advantages. (I can't think of any right now, though I'm sure there are some!). This exists in v2 as well: https://swagger.io/docs/specification/2-0/authentication/api-keys/\nWe'd need to add security field into grpc.gateway.protoc_gen_swagger.options.Operation. We'd also need to add security and securityDefinitions into grpc.gateway.protoc_gen_swagger.options.Swagger.. I hope this is what you had in mind :-). @johanbrandhorst Strictly speaking, that's not something that makes v3 support a must and a requirement. Consider that OpenAPI v2 does support OAuth2 authentication, Basic authentication and API keys.\nI don't see in that PR where OpenAPI v3 adds JWT, and I suppose you meant OAuth2 and not OATH (different thigns)?\nWhat I see as a new thing in that PR you quoted is \"HTTP Bearer\" and \"Multiple Flows\", which both may be useful. (Even though Bearer will be usually used with OAuth2, anyway, there might be people out there not using it that way.)\n@tamalsaha That would be the right way to go, but protoc-gen-swagger has OpenAPI v2 schema, so that's not a blocker either.. FTR I will be very happy to review forks of the swagger generator, or elegant refactors of the swagger generator that allow for both openapiv2 and openapiv3.. See .travis.yml to see how tests are being set up. It will point you to .travis/install-swagger-codegen.sh which, if you examine it, fetches a version of swagger-codegen into ~/local/swagger-codegen-cli.jar\nIf you read further, at this point if you want to run tests, you pass it the correct command to invoke swagger-codegen as a make variable:\n make whatever-target-you-actually-want SWAGGER_CODEGEN=\"java -jar $HOME/local/swagger-codegen-cli.jar\"\n\nYou don't need Linuxbrew, nor if you did can I see where running swagger-codegen as root would come into play.\nYou shouldn't make or make test things as root, ever. You should at most make install software that support installation, but after running make as a regular user.. I think it does. See: https://github.com/OAI/OpenAPI-Specification/blob/fb059ca461bd17b10a9e3e59879f04485886d356/versions/2.0.md\nSearch for \"required\": true, and search for \"title\": \"Swagger Sample App\".\nNot that protoc-gen-swagger generated OpenAPI v3, but this applies to that version of the spec, too: https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md\nPasting your generated JSON into https://editor.swagger.io also throws no errors.\nWhy do you believe \"title\" and \"required\" should not be generated?. Oh, you mean the title and required you specified are omitted from the output?\nI've added many fields to the schema without adding support for outputting them. I'm sure PRs will be accepted. You'll want to update template.go: https://github.com/grpc-ecosystem/grpc-gateway/blob/8cc3a55af3bcf171a1c23a90c4df9cf591706104/protoc-gen-swagger/genswagger/template.go. 1. This is not a support channel. :-)\n2. gRPC is primarily proto3.\n3. protoc-gen-swagger is a plugin for protoc, so it doesn\u2019t parse proto files directly. Therefore, yes.\n26.12.2017., u 11:09, korisnik Public Repository of A.Minchekov notifications@github.com je napisao:\n\nCan protoc-gen-swagger parse proto3 syntax?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. I like that this is in, but I think it worth adding a few caveats. For instance, I'd argue it's much more readable if summary and description are written as a proto comment. Special options seem a sane choice only if you have Swagger-specific text that doesn't apply to the gRPC service, or that should otherwise not end up in generated code.\n\ntags field may be useful if same call appears in multiple services, or if the default of service name is not sufficiently human readable (e.g. developer wants spaces in the Swagger 'tag' name). But I would still suggest just using the default: the proto service name. Maybe we should have a proto option specifying a 'human readable' name for a service, instead?\nEither way, this is neat, even if easy to misuse. Thumbs up \ud83d\udc4d . Any thoughts on why only the build configuration Go 1.10.x without request_context plugin flag would be failing here? @achew22 \nEDIT: I've hit retry, let's see if it goes through.. I've rebased on HEAD, let's see if the tests go through now!. I just saw https://github.com/grpc-ecosystem/grpc-gateway/pull/656. Thanks, let's close this PR.. Possibly more importantly, can you demonstrate old broken behavior and how the new code fixes it?. > > It seems that the test case itself would need to change. Can someone explain the meaning behind {string_value=strprefix/*} ?\n\n@yugui, now that I look at that, do you know why that is like that? My guess is that it strips the \"strprefix/\" from the beginning of the string value, but I don't understand why you wouldn't just put that outside the capture. Do you have any hints?\n\nI first thought this might be a case of https://tools.ietf.org/html/rfc6570 page 8 and page 21. My (untested) understanding was that passing strprefix/a,b,c might result in a list { \"a\", \"b\", \"c\" }.\nHowever, I remembered seeing this syntax before in the wild. Here it is in pubsub v1's pubsub.proto.\nI think this might mean that the value strprefix/ is included in the output string string_value, but that it is mandatory. If you would move this outside, strprefix/ would still be mandatory, but no longer included in string_value.\nHere's the documentation for the linked RPC.. I think that interpretation is correct. Cloud Endpoints's ESP:\n\nVariable = \"{\" FieldPath [ \"=\" Segments ] \"}\" ; and \nSegments = Segment { \"/\" Segment } ; and \nSegment  = \"*\" | \"**\" | LITERAL | Variable ;. On Fri, Jun 15, 2018 at 3:43 PM Jesse Suen notifications@github.com wrote:\n\n\nSorry, I was OOO the past week. If you mean moving strprefix/ outside the\ncapture, I will be happy to make this change. I'll update the PR.\nI don't think moving it out is meant to be equivalent. Leaving it in says\n\"match this pattern if the template field's value starts with strprefix/\".\nWhen the template field's value is placed into a variable, strprefix/\nremains inside.\n\nThat is: /abc/def/{ghi} is not equivalent to /abc/{ghi=def/*}. Given\n/abc/def/blah, the former will produce ghi=blah, and the latter will\nproduce ghi=def/blah.\nI don't know how widely this is used in general, but as you can see, proto\nfor Cloud Pubsub does use it and presumably depends on it. Without looking\nat the code (no time!), I will assume gRPC-Gateway interprets this pattern\nequivalent to Google Cloud Endpoints's REST frontend.\n. I believe error from https://github.com/grpc-ecosystem/grpc-gateway/pull/660#issuecomment-393087583 needs to be fixed?. I think this is useful and addresses a real problem. However, you should address several concerns; this is, unfortunately, not a one-liner change that it seems to be.\n\nPlease write/update a test to demonstrate old, incorrect behavior.\nPlease run make test locally to regenerate the changed .json files.\nPlease fix broken tests which use real HTTP requests to test various components of grpc-gateway.\n\nAs @achew22 mentioned, because you are changing the operation IDs, this is invasive. Let's be more specific: people who use Swagger definitions to generate client library code will suddenly have to update all their client software, as the method names will have changed to include service name. This is what broke grpc-gateway integration tests (see TravisCI build).\nThere are ways out of this; how about adding a suffix only if the operation ID is determined to be duplicate? (Similar to what's done with bIdx.) This will address the immediate issue.\nBut -- as a separate thing -- you could add a flag that allows the developer to specify if the service name should be included in the operation identifier. Over time the old behavior can be deprecated, and the new behavior can be the default. But it should not be an instant change breaking people's code using generated REST clients.. > Later I add a Get method to a different service and now, all the code I wrote before needs to be updated to reflect the surprise changing of the original Get method's name to packageGet.\nWhat I meant is 'leave the original alone', but then again, you can't really tell which method is the original. So ignore the suggestion.\nI agree, renaming the Swagger operation explicitly is the only way to go about it, and right now it's the only way to do it. But renaming the Swagger operation can only be done by renaming the gRPC method. That breaks the gRPC interface. In gRPC, it's a valid expectation that different services have differently named methods. It's simply not right that Swagger's limitations force a change in how the methods must be named.\nThere's the possibility of using the OpenAPI proto-options to rename the operation (I don't know if this can currently be done, but surely it can be hacked in). That, however, seems like a manual workaround, and necessary only because the default behavior is incorrect.\nSo how about hiding the new operation ID proposed in this PR behind a flag, and announcing that the default value of the flag will change?\n\nThis makes me sad a bit. To state the obvious: Having the power of hindsight on our side, it feels like 'operation ID is global' and 'we'll just use tags to group methods' was a big mistake on Swagger's part. It's not even an issue on grpc-gateway's side that it declares service membership using tags; Swagger's own code generators use tags to generate service-like groupings of methods in client code. . No objections on my part as long as the tests are fixed.. Last time I was doing it, I just did what was done for Travis. :-)\n\n--\nSent from Gmail Mobile\n. Sorry, travelling. No objection to the PR.\nOn Wed, Jun 20, 2018 at 4:13 AM Johan Brandhorst notifications@github.com\nwrote:\nAt the risk of becoming a tyrant on my first day, I'm going to commandeer\nand this and merge as @ivucica https://github.com/ivucica has been\ngiven ample time to add any more opinions and he conceded that he was happy\nwith the PR even without the minor changes, that have been addressed.\n. @zheng1, I must admit I am confused at what's being proposed. Which well-known types would you make nullable and under what specific criteria?\n\nClosest could be protobuf's built-in optional, but that's not quite the same semantically.\nFinally, I am not aware of PRs that make protoc-gen-swagger generate OpenAPI v3. I believe it currently generates OpenAPI v2, so until that happens, the point is moot. I would not mind the proposal being better documented for when we get a v3 generator, of course.. > I assume message types are already nullable\nUnless they are proto2 required (which is \"considered harmful\", but it is possible).. Not sure when I'll be getting around to studying the problem space. That is, here's some necessary things I don't know off the top of my head:\n\nwhat the x-nullable extension actually is, and how to make use of it\nwhere it needs to be written in the output file\nhow to adapt template.go to do this\nhow to specifically check if a field -- whatever its type -- is considered required\n\nEach of these items is relatively short, I'd just need to spend the time to figure them out. Maybe just enumerating them helps you?\nHowever, note: OP says x-nullable is an OpenAPI v3 feature. As I stated in my June 26 comment, I think we are only generating OpenAPI v2 at this time.\nIf you want to support x-nullable, presumably you should first contribute a new protoc-gen-openapiv3 (by copying protoc-gen-swagger and adapting it to spit out OpenAPI v3), including creating all the necessary tests, and then proceed by adding x-nullable support to it. Unless x-nullable can be used in OpenAPI v2 specs, in which case, by all means, adapt template.go to spit it out for any optional/non-required fields. (Even though all proto3 fields are optional and thus nullable, proto2 is not like that.). The errors are not due to Bazel, so I'd take a look at other failures first. From what I can see, you did not add field_mask.proto to third_party?. @dmacthedestroyer If you want to squash commits yourself, git rebase -i and then mark individual commits you want to squash with their parents with f (standing forfixup).\nYou won't be able to push the result as you usually would (as you just rewrote history) but git push -f is a way out.\nAlternatively, if @dmacthedestroyer agrees, @johanbrandhorst how about we just squash using Github UI (which will use PR description as the commit message)? Maybe that's better. Not sure.\n\nWhat we do separately need, however, is a rebase on top of current master. I always have to look up how to do that, but git rebase is certainly involved, as is git push -f (because you're rewriting history).. Yes, as a developer defining the field_mask.proto-aware API, you are meant to:\nimport \"google/protobuf/field_mask.proto\";\n\nThen, the developer should not need to protoc it because, as it is a WKT (well known type). They can and should just import the pregenerated .pb.go should you need to instantiate the generated Go struct type:\nimport (\n    \"google.golang.org/genproto/protobuf/field_mask\"\n)\n\nPretty much as you have done in the examples you updated/created.\nI am, off the top of my head, not sure how to update Bazel files. Probably inspiration can be drawn from other WKTs that may be used such as duration or timestamp.. This is unclear to me. #280 doesn't seem to relate to OpenAPI, and descriptor package doesn't seem to refer to OpenAPI either. Can you expand a bit on what OpenAPI semantics are you referring to?\n. I am comfortable with a change, aside from renaming the Swagger object, which is still the root object in OpenAPI v2.\nOn the other hand, how about this happens for the v3 generator (which may or may not coexist with the v2 generator)?\nThe problem you cite might apply only to people who want to customize their OpenAPI v2 .json by manually specifying fields that are not otherwise describable in proto format. If the rename swagger->openapi happens when they elect to use a new generator, e.g., protoc-gen-openapiv3, then it is up to them to include the new proto.\nFinally, don't hold me to this, but I think package name for the options file might be a non-sequitur anyway; I don't think it applies to the wire format.\nSo what do you think: we don't do a rename, and instead we ask that the rename happens whenever someone contributes a v3 generator?. Can you explain what 'package extraction' means in this context?. While it's recommended to use proto3 (so your FR makes sense), have you tried using proto2 syntax to distinguish between default and undefined values?. Support for YAML API definition is exciting, but somewhat basic: https://github.com/grpc-ecosystem/grpc-gateway/pull/521\nIt doesn't process most of the fields available in google.api.Service (which is very large: see the documentation, see the proto file). As far as I can tell, it only supports google.api.Http-type http key.\nI believe you can only split out your routing proto options into YAML. I don't believe you can currently define API title, description, version, etc. using the YAML file.. Sidenote on style of your proto definition, unrelated to this bug:\n\nyou don't need to specify a tag if the service name matches the tag value\nyou don't need to specify the description; while first paragraph will be the summary, remaining paragraphs should end up in the description\nif you use comments in place of openapi_v2 options, the documentation will appear in generated Go (and other) code, which is nice and useful\n\nI'm viewing the use of openapi_v2 as a last resort.. > In my experience, I've deeply regreted not returning an object at the top level. It made migrating that endpoint a horribly breaking change and required months and months of work to get things back in shape. \nNo objection here. In general, I like the API Design Guide myself. But yeah, it's about adopting an existing API.\n\nyou almost want a \"reply body\" parameter attached. I haven't seen anything like that, but it might be a thing upstream is receptive to. If you could get it in the HttpRule proto then I would be 1000% in favor of supporting it.\n\nI believe that an attempt to add reply_body upstream will almost certainly result in a rejection unless I would include an upstream implementation :-) but for that, I honestly don't have the time nor motivation :-)\nI guess I can ask upstream internally.. > Personally this seems like breaking a bit too far away from the underlying protobuf definition.\nHow? This can be a 'considered harmful' compatibility hack to be able to implement certain pre-existing APIs.\n\nIt throws backwards compatibility to the wind\n\nHow? If you don't specify the new option (or, if upstream agrees to it, whatever-we-end-up-calling reply_body) then you get the current behavior. No changes.\n\nGiven that this is a field property, have we considered how it might interact with query strings, partial path fields etc?\n\nBut, how do query strings and partial path fields apply if this only changes the response? No interaction with the request messages.\n\nWhat if you specify it twice?\n\nI considered it; I think it can depend on what's doable in implementation stage. I'm currently personally interested in behavior of a top-level field on a top-level response message getting promoted as a response itself. I guess if you specify it on a repeated message-type field which contains a repeated message-type field where the option is also specified, we could try moving that one step up, too, allowing for an array of arrays?\n\nI'd be more inclined for us to have a special message type for this kind of thing. Maybe message grpc-gateway.Wrapper{ repeated google.protobuf.Struct element = 1; }\n\nThis at the very least invades the realm of the gRPC API design; you have to change how your gRPC API looks, when in fact you only want to change the REST API. It means constructing the response becomes very unwieldy.\nAn option (or a field in google.api.HttpRule) still seems reasonable, non-intrusive and clear inside the .proto file?. Thanks for the lively discussion -- but maybe we're straying into the \"you are wrong for needing this feature\" territory? :)\n\nI don't mean backwards compatibility with the current gateway, just that if you have a service as you suggest and add a new field to your response message, that's impossible? It's breaking backwards compatibility guarantees given by protobuf. Adding new fields should always be possible, and for that to be true, there has to be a top level messages.\n\nBut this is not protobuf, this is JSON and it is a risk the developer can take. Again, this is a hack for implementing other, preexisting APIs -- something I would strongly discourage for new APIs.\n\nHow do we tell the user this only applies to responses? \n\nIn documentation and docstring for the option, I'd assume? How do we describe to the user how our view of the RFC6570 {templates=works/*}?\n\nThis seems like a nightmare to implement, are we going to allow this nesting indefinitely?\n\nRecursion is a thing, no? But how deeply this is doable depends on the implementation -- which I didn't look at for now, as I wanted to gauge useful feedback on whether to spend time implementing this.\n\nAre we going to just document the limits? \n\nYes. I foresee either limit of 1 or no limit.\n\nWhat if you put it on a map? A oneof? An enum?\n\nI'm not sure what you're asking here. If you put it on a map (I don't know, does grpc-gateway support maps?) you clearly promote the map to a top level object. If you put it on a oneof -- well, how do we encode it in JSON now? Same for enum. Same for string. Same for int. JSON doesn't ban you from any of those being top level values, to my knowledge?\n\nWe could document that it only works in very specific well-defined circumstances, \n\nYes, if that ends up being the implementation.\n\nbut it feels a bit like a hack unless we can make it universally applicable.\n\nIt is a hack that I would prefer not doing, but existing APIs are returning this today. I am interested in implementing a portion of Mastodon APIs in gRPC + gRPC-Gateway, but I am today blocked on one of the core APIs -- the one that lists statuses posted by a user account -- returning a list.\nAny workaround I would do would be a way, way worse hack and much less useful than just upstreaming support relevant annotations for this into gRPC-Gateway.\n\nThis is a fair point - but constructing messages for specific JSON structures is already what the official well known types encourage us to do. The whole point of the google.protobuf.Struct type (as far as I can tell) is to give predictable JSON rendering. Same for the ScalarWrapper types.\n\nWhat gRPC-Gateway does is supposed to be a \"predictable rendering\" of JSON, already, no? google.protobuf.Struct and google.protobuf.ListValue appear useful if you are representing  I don't need arbitrary JSON encoding (which is what the quoted example you gave provides) and I believe it's much hackier to write a translator from a nice, well defined protobuf into a generic google.protobuf.Struct just so I could put it into google.protobuf.ListValue.\nUsing .Struct/.ListValue would mean that any gRPC (as opposed to REST) users of the same backend would have to translate the same .ListValue and .Struct back into an array of native proto structs. And they'd have to do that in every language. Or they'd have to use the REST API. Or the backend would have to implement an additional, gRPC-only method for, e.g., enumerating statuses, which would not be exposed as part of the same REST API as the remaining methods.\n\nI just want to make sure we consider all potential use cases of such an option and document the limitations.\n\nSure. What other use cases do you propose users might have?\nMy purpose for this bug was to hear about what other elegant ways there might be to expose this functionality to developers. Is option on the field the best way? Maybe not. I did mention option on the rpc in the original post. I also liked Andrew's suggestion to integrate reply_body (or equivalent) into google.api.Http, as long as upstream is open to it. None of these changes much for implementation in grpc-gateway. \nAny of these helps me annotate the strange behavior once (in the proto), while keeping generated implementation and swagger definition in sync.\nI am not sure \"developer won't know how to hold the phone right\" is useful to decide on the implementation (they can design a broken API in a hundred different ways already) nor is \"we must prevent the user from shooting themselves in foot in every way\" or \"we cannot possibly implement this without covering every single edge case\".\nCan you provide specific use cases, and specific confusing .proto files that you believe need clarification before implementing and merging something that would support this?. One of the goals is to generate OpenAPI representation from the same source. Unless I am missing something, a .ListValue will not be represented as a list of particular proto messages.. > at the same time it would avoid us having to create an official message descriptor extension.\nIf we introduce a new gRPC-Gateway-specific option on a message or on rpc, what makes that more or less official than a gRPC-Gateway-but-Swagger-specific field? Adding a Swagger-specific annotation does mean we are updating our \"openapiv2\" annotation protos.\n\nIt would mean the types in the protobuf definition wouldn't match the signature of the function in the swagger API\n\nIt would, unfortunately, also mean having to update protoc-gen-grpc-gateway to understand swagger-specific annotations. Otherwise, we are just moving the problem from \"we have a hack to decode JSON, strip the outer dictionary, re-encode JSON\" to \"we have a hack to decode JSON, strip the outer dictionary, re-encode JSON, and then we also annotate the proto so we get the correct swagger file\".\nWhy not have a short-and-simple, nicely-named annotation that says \"we need to strip the outer dictionary\", and which is correctly understood by both protoc-gen-swagger and protoc-gen-grpc-gateway? The annotation obviously won't work with Google's ESF, but that can be documented.. To paste a shorter variant of what I put into #712: \nI suggest a careful examination of what's currently present in google.api.HttpRule documentation.. > So we're back to a gateway specified field option, allowed only for one field per message, repeated or not, and only at the top level of any response hierarchy?\nIf we want to do as upstream did, then it's attached to an rpc :) but it may require some more parsing akin to what's done for body.\nI don't mind attaching to message either.\n\nOn a side note, that documentation is hilariously obviously not supposed to be public :joy:\n\nPsst! Relevant organs have been notified. Interesting design requirement was noted in #712 and a good reason for the new field in google.api.HttpRule has been raised (namely additional_bindings). I'm not sure how to solve this elegantly; if we were solving this for an RPC, it'd be fine, but if we try to solve it per .HttpRule it's somewhat harder.\n@doroginin n.b. I expect I will file the request for the option ID tomorrow.. @doroginin I have filed a request for a new option ID.. No need for the option ID or for further hackiness -- #712 will be able to go through once upstream changes land and then we can look at closing this FR.\n...this is less work for me than I expected ;). I believe this is fixed in #712 . I have not yet received upstream response for #707.\nI am opposed to merging this particular PR, as it may (cough cough) collide with upstream fields which are not publicly visible.\nI am also opposed to forking google.api.HttpRule; if we want to introduce something like this, it should be a new option namespaced to gRPC-Gateway. (If upstream decides to support the equivalent field, we can switch at that point, but it should be close to what upstream supports.)\nIn that light, I suggest careful examination of what's currently present in google.api.HttpRule documentation, particularly look at response_body. I'd suggest comparing the comment that @doroginin put into his fork of http.proto which bears a resemblance to the documentation of certain fields missing from the public .proto file.. This is improved, but if you do receive a response on https://github.com/googleapis/googleapis/pull/512, I suspect it will be pushback.\nI'm re-contacting upstream.. Some ongoing CI issues: https://github.com/grpc-ecosystem/grpc-gateway/pull/711#issuecomment-408860071\nI would not be worried and, were this PR otherwise mergable, I would not object to merging it.. @wora See #707 for my own feature request. As I said there, some preexisting APIs, such as Mastodon's, cannot currently be described with google.api.HttpRule as they return a JSON list as a top level object. With this trivial change, they could be.\nWithout this change, when attempting to describe Mastodon's existing API, one would\n\nhave to postprocess the JSON before returning it to the user,\nand also have to postprocess output of protoc-gen-swagger so it documents returning a list instead of a message\n\nWhile that's somewhat tolerably doable, it's way hackier than just allowing users to specify which field should be the top level JSON entity.\nExample endpoint that returns a list\nI do believe designing APIs like so is a bad design, but that's better to be strongly discouraged in documentation and in the API Design Guide.\n@doroginin might have another use.. I previously asked:\n\nSeparately, can you then make protoc-gen-grpc-gateway use the JSON names as well if the flag is set? If not, can you please add a TODO here for someone to take a stab at it. It would be long-term important that -swagger defines the same API that -grpc-gateway exposes.\n\nIf you don't plan on updating protoc-gen-grpc-gateway to respect JSON names flag, could you at least add a TODO for someone else to do it? Thanks. Ouch. This will actually break people who don't use Gopkg.lock and just go get, like myself. This is because upstream google.golang.org/genproto merged @doroginin's 383e8b2c3b9e36c4076b235b32537292176bae20, but then steamrolled over the changes in https://github.com/google/go-genproto/commit/9739b9a355311a4856a42cccc8d04ebc8a02d7a7\n@johanbrandhorst Do you think we should rollback or roll forward?. I'd like that too; I reopened the internal bug.. @bcdurden Upstream has already merged all the changes! Can you verify that your projects are no longer broken and that I can drop PR #730?. Oops. I did not realize @johanbrandhorst requested a change. I only saw the change was approved. sigh. Random thought, partially based on @achew22's comment (I almost suggested implementing UnmarshalJSON until I realized that's stupid):\n\ngiven something.proto containing message IntAndString, generate your something.pb.go in a package, e.g. github.com/vtolstov/something/proto.\ncreate a new something.custom.go in the same package e.g. github.com/vtolstov/something/proto.\n\nin something.custom.go, implement new method UnmarshalJSONPB() from the github.com/golang/protobuf/jsonpb#JSONPBUnmarshaler interface on the proto type IntAndString:\nfunc (a *IntAndString) UnmarshalJSONPB(*Unmarshaler, []byte) error {\n  if b[0] == '\"' {\n    // handle as string\n    a.StringValue = ...\n  } else {\n    // handle as number, e.g. maybe something like strconv.Atoi(string(b[1:len(b)-2]))\n    a.NumberValue = ....\n  }\n  return nil\n}\n\n\n\nwhen you need such a union, IntAndString some_value = 19;\n\n\nI have, obviously, not tried this, and I have definitely not run this.. > ping...\nI won't get around to playing with this out any time soon.\nWhat have you tried to triage this? Does IntAndString's UnmarshalJSONPB get called? Trivially, you can just add a fmt.Println() to it and see what happens.\nI am also confused at which map[string]json.RawMessage is this being unmarshalled into. It would be interesting to find where this error comes from. Have you tried finding out why map[string]json.RawMessage] is involved?\nFor instance, you can use gdb for debugging, or even better use Delve. I have done neither, but if I needed to triage what happens, I would find the source code line that prints cannot unmarshal number into Go value of type, add a breakpoint to it, then get the backtrace (in GDB, that's bt).\n\nOnce again, I have not used a debugger with Go code, and I have not tried to do what you want to do here. I think UnmarshalJSONPB should work, but if you can't make it work and if you're on a deadline of any sort, intercept the API endpoint manually and construct the gRPC call manually.\n\n\nif i don't define in proto type IntAndString i fails to generate:\n\"IntAndString\" is not defined\n\n\nThat makes sense, right? type instruction is in the generated code, in .pb.go. If you delete it, the type IntAndString goes away with it, right? So if there's no IntAndString type anymore, it makes no sense to attach UnmarshalJSONPB to it, right?. FYI @doroginin @johanbrandhorst @achew22  I will roll this back immediately because we can easily roll forward once it's not friday.. Alright, the change to googleapis/api-common-protos has been merged already!\nLooks like someone \"just\" needs to regenerate code in go-genproto and we may not need to merge this. I'll give it ~1.5h and then merge this rollback.. Done. I've just ran this test case and this has worked for me.\nI also see no issue on the CI here.\nPlease reopen if you still see issues you directly correlate to the quoted commit.. @Tommy-42 As far as I can tell, ResponseBody field is in the current upstream pregenerated HttpRule.\nhttps://github.com/google/go-genproto/blob/c7e5094acea1ca1b899e2259d80a6b0f882f81f8/googleapis/api/annotations/http.pb.go#L325\nNot sure why the version you have would not include this. I have not looked into how vgo / go.mod works; does upstream need to tag their repo in some way?\nWhy are you specifically checking out grpc-gateway v1.5.0?. > Getting the same issue when pulling the new version into a bazel build.\nCan you clarify \"the same\"? How did you 'pull the new version in'?\n\nI believe it is related to you shipping a copy of the annotations.proto here:\n\nPossible, but can you clarify what led you to believe this? In what significant way are the files different?\nUpstream has the response_body proto field, as does the vendored copy.\n\nIs there a reason for you to not use the upstream version?\n\nHow would you use it? One still needs to pick an upstream version to lock onto and to download, for consistent builds, right?\nIf we didn't lock onto it (whether by copying or specifying the exact version in http_file() in Bazel), we'd get broken if upstream breaks things.\nIt's also not without benefit to have a copy of the .proto file around in a nice and convenient well-known path, already cloned and usable by developers for import purposes when using protoc. (I'm thinking of my usual workflow with a GOPATH; I can invoke go generate which invokes protoc extending import path to include ${GOPATH}/src/github.com/grpc-ecosystem/grpc-gateway/third_party. And this without having to clone the entire, somewhat large, googleapis/googleapis repo by hand.). https://github.com/grpc-ecosystem/grpc-gateway/blob/844b78d9a291695a45d1d826fda9c1b95a2ddfc1/third_party/googleapis/google/api/http.proto\ncontains response_body, just as it should. It seems to be doing so in your\noutput as well:\n/home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/external/grpc_ecosystem_grpc_gateway/third_party/googleapis/google/api/http.proto:303:\nstring response_body = 12;\nNo idea why Bazel would be picking the wrong proto when running protoc.\nSadly, this is not the same as the original bug report.\nOn Tue 6 Nov 2018 at 15:46 Stefan Sauer notifications@github.com wrote:\n\nIn my bazel WORKSPACE file:\ngo_repository(\n    name = \"grpc_ecosystem_grpc_gateway\",\n    #commit = \"844b78d9a291695a45d1d826fda9c1b95a2ddfc1\",\n    #importpath = \"github.com/grpc-ecosystem/grpc-gateway\",\n    # TODO(ensonic): switch back after PR #644 has been merged\n    commit = \"4e0ecd0f283a990d61c867c2e55635a11931e2aa\",\n    importpath = \"github.com/ensonic/grpc-gateway\",\n)\nThe uncommented version is the time of my PR and everything builds. When I\nswapp to the commented part (the merged PR), then I also get all the\nchanges in between and now the build fail:\nGoCompile: error running compiler: exit status 2\nhome/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/sandbox/linux-sandbox/43/execroot/myproject/external/grpc_ecosystem_grpc_gateway/protoc-gen-grpc-gateway/descriptor/services.go:146:49: opts.ResponseBody undefined (type *annotations.HttpRule has no field or method ResponseBody)\nSomehow the build is picking up an outdated copy of the http.proto. I am\ndigging deeper.\n$ find home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/ -name \"http.proto\"\n/home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/external/com_github_googleapis_googleapis/google/api/http.proto\n/home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/external/go_googleapis/google/api/http.proto\n/home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/external/grpc_ecosystem_grpc_gateway/third_party/googleapis/google/api/http.proto\n/home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/external/bazel_source/third_party/googleapis/google/api/http.proto\n$ find /home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/ -name \"http.proto\" -exec grep -Hn \"response_body\" {} \\;\n/home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/external/com_github_googleapis_googleapis/google/api/http.proto:355:  string response_body = 12;\n/home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/external/go_googleapis/google/api/http.proto:355:  string response_body = 12;\n/home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/external/grpc_ecosystem_grpc_gateway/third_party/googleapis/google/api/http.proto:303:  string response_body = 12;\nSo the bazel_source/third_party/googleapis/google/api/http.proto is too\nold and for some reason it hit this one.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc-ecosystem/grpc-gateway/issues/731#issuecomment-436299925,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAdJnAn9xXPiovJLZTSx1IycN6ViiKj5ks5usa7MgaJpZM4WBwVQ\n.\n. I've discussed this with @ensonic out-of-band. It was a local (but interesting) issue with the WORKSPACE file, not a bug in grpc-gateway's rules.. This particular issue is long resolved. Unless it's related to the original issue related to response_body, I don't think it's right to keep posting here.\n\nI'll lock this issue.. Regarding justification: I'd argue the spec only requires a string to be set, and not that it's a non-empty string. See relevant section in spec; while it says it's required, it doesn't say what the string should look like. \n(For what it's worth, the spec also actually doesn't say what it means by the word \"required\" in this context; the quoted RFCs use it slightly differently.)\neditor.swagger.io does not complain about a description set to an empty string.\nBut, having said this, I believe the change to be an improvement. LGTM.\nOn a separate note: line 717 will make the resulting description look ugly, but that's already the case if people specify a documentation comment. I'd remove that case, but that's something to be done in a separate PR.. I've successfully used cmux to put both HTTP/2 + gRPC frontend and an HTTP/1.1 frontend on the same port.. It's so very strange that touching just the whitespace causes this.\nI can't do this straight away unfortunately.. If you feel like it, sure, but this is really a nit anyway :-). Well,  I don't have a major objection introducing a new flag. But I would mark the old flag as deprecated, give a warning for a bit, and eventually remove it. Not immediately. We don't want to be \"that project\" breaking people's build systems without ample notice.\nAnd changing the default without ample notice is also something not to be done lightly. Client libraries (and maybe even servers -- not everyone who wants to define an API using protobufs will want gRPC in the picture) would break and people would be rightly upset if we went and changed generated method and message names willy-nilly. \nSo a flag is a must, removal of a flag is only-after-deprecating-the-old-one, and addressing \"prettiness of generated names\" is a yes-please.\nSuggestion to use dots in method and message names (if I am interpreting that correctly) confuses me. Is that allowed in OpenAPIv2? Can you give concrete examples of an input, current output and new output so it's easier to understand the proposal, rather than think about what exactly \"one-off\" or \"fqn\" means in this context? How would each flag behave?. I just tried it out with editor.swagger.io and, curiously, dots are fully acceptable and even ~correctly handled by the Go code generator. So with a flag, I have no problem with this.\nI'd still love to see exact proposed examples of the changes with various flag values.. +1. That's dead code from the original approach in 6ff06d8. Thanks for spotting! Removed.. Done.. Good question! This is correct as-is and has the same general approach that the rest of the code has: if OpenAPI v2 values are manually defined in the proto, then they overwrite the defaults; otherwise, they don't.\nFor schemes value, the defaults are already defined as http and https, in line 639 (function applyTemplate).. Done.. Thanks :). My approach for setting proto package name:\n\nthis is the grpc_gateway project, and users may want some more uniqueness -> grpc_gateway\nthis project contains generators protoc-gen-grpc-gateway and protoc-gen-swagger; this only applies to the latter -> protoc_gen_swagger\nthese are option protos extending the proto descriptors a bit -> options\n\nI'm not sure why I did not go for a fully global namespace com.github.grpc_ecosystem.grpc_gateway..... Either way there's a variety of approaches taken.\nYou did get me to look around so I will change this to use grpc.gateway, as that seems to be used for the runtime and example protos.. Done.. I guess that's how it could be done? But Authorization: Bearer isn't mentioned in v2: https://github.com/OAI/OpenAPI-Specification/blob/fb059ca461bd17b10a9e3e59879f04485886d356/versions/2.0.md\nIt would also look amusing, with every token prefixed with Bearer.\nHowever, Authorization: Bearer would presumably be used with OAuth2 anyway; given v2 doesn't seem to address Bearer in other use cases, why not just do that? :). (Sorry about this.). Remove one tab maybe?. Should https://github.com/johanbrandhorst/grpc-gateway/blob/97aefd3e41117e62982ef489dfb516a228a39ea3/protoc-gen-swagger/genswagger/template.go#L231-L245 and other places extracting the schema object be combined with this?. Should https://github.com/johanbrandhorst/grpc-gateway/blob/97aefd3e41117e62982ef489dfb516a228a39ea3/protoc-gen-swagger/genswagger/template.go#L233-L241 and other places with ExternalDocs be combined with this?. Actually if this is being moved, maybe move it together with \"testing\" and \"reflect\" and others? Seems strange to have it hang on its own. > qualified message reference\nI'd be slightly confused about this, maybe fully qualified protobuf message reference? Not sure.. > This should be\nThis could be, I suppose?. Nit: s/much/many/. Nit: s/much/many/\nWe can skip this if it'd take you a lot of time to regenerate examples.. requestResponseRefs, customRefs refMap is also valid. Maybe: s/referenced types/other custom (but referenced) types/ ?. Can we name this just openapiv2_field similar to what was done with other options?. Why is this flag only affecting the swagger definition? Won't this break the api generated with protoc-gen-grpc-gateway?\nIf this affects only protoc-gen-swagger, why is this a flag in protoc-gen-grpc-gateway?. That was my original proposal, but I am fine with attaching it to rpc instead of message. We only need to move it to a proto field we actually control.\nIt is correct that you cannot add a different response_body with what I am proposing:\nservice Strings {\n    rpc ToUpper (String) returns (String) {\n        option (google.api.http) = {\n            post: \"/strings/to_upper\"\n            body: \"str\"\n            additional_bindings: {\n                post: \"/strings/to_upper/v2\",\n                body: \"*\",\n            }\n        };\n        option (grpc.gateway.rpc_options) = {\n            response_body: \"str\"\n        };\n    }\n}\nThat is indeed unfortunate, but I see additional_bindings as a bonus, and for the most part, I just use a 1:1 mapping.\nAnything I can think of is ugly, including this possibly tolerable addition (which is not incompatible with what's above):\nrpc ToUpper (String) returns (String) {\n    option (google.api.http) = {\n        post: \"/strings/to_upper\"\n        body: \"str\"\n        additional_bindings: {\n            post: \"/strings/to_upper/v2\",\n            body: \"*\",\n        }\n    };\n    option (grpc.gateway.rpc_options) = {\n        configure_rule: {\n            rule: { post: \"/strings/to_upper\" }\n            config: { response_body: \"str\" }\n        }\n        configure_rule: {\n            rule: { post: \"/strings/to_upper/v2\" }\n            config: { response_body: \"*\" }\n        }\n    };\n}\n\nTo get what you want to happen, we do need to get upstream to release the response_body, which may be difficult.\nOnce again: I am nearly certain your .HttpRule PR will not be accepted, due to insights that I cannot detail here without approval. So, once again: you should explore alternative designs unless upstream decides to provide the field.\nI strongly suspect grpc-gateway will not move to a forked google.api.HttpRule either (especially one that may long-term conflict with upstream developments).. I propose we take design discussions into #707 instead of this particular PR?. Please rename this into useJSONNamesForFields and update the comment appropriately :-)\nSeparately, can you then make protoc-gen-grpc-gateway use the JSON names as well if the flag is set? If not, can you please add a TODO here for someone to take a stab at it. It would be long-term important that -swagger defines the same API that -grpc-gateway exposes.. This comment is incorrect and nonsensical*, could you please fix it? :)\n\nIts grammar is nonsensical even in IsAllowMerge, but here the comment itself is outright wrong.. Given you are providing a getter for useJSONNameInSwaggerDef, did you mean to provide it for allowRepeatedFieldsInBody?. How about the bash syntax:\n\n// Body describes a http {request,response} body to be sent to the {method,client}.\nor the regex syntax:\n// Body describes a http (request|response) body to be sent to the (method|client).\nMaybe expand the comment a bit, and mention this is used in body and response_body options in google.api.HttpRule?\n. Add a comment that this is resolving the value of response_body in the google.api.HttpRule. I'm not a fan of changes like this. Can you amend the commit (instead of adding to PR) to eliminate this change? Or if the intention is to squash the PR at the end, please do just remove this change.. If a change is in the master, then, to my understanding, there should be no diff visible.. I am not a fan of 'add unnecessary trailing space' changes ;-). I know this is autogenerated, but I'm slightly confused.. Why remove desc?. This should have the space. This should have the +.\nCome to think of it, this looks like it MIGHT be a rollback of a pull request I approved recently. Why wouldn't you want to have fieldProtoComments result used for desc? (This might be the reason for disappearing comments in the autogenerated code above.)\nSame applies for many other changes in this file: they look like rollbacks. Please don't rollback needlessly and please rebase on master correctly without steamrolling older changes :-). Was this done?. Instead of merging, you should be rebasing :-). Not objecting, just trying to learn: Why remove repositories.bzl? Wouldn't it be better to allow third parties to import a repositories.bzl and transitively depend?. Nob objecting, just trying to learn: Maybe I'm not tracking developments in Bazel world closely enough, but if gazelle_dependencies() is no longer imported, where is this defined? Why does this work?. ",
    "pcj": "I've posted a preliminary implementation of grpc_gateway_proto_library in rules_protobuf that:\n- builds the grpc-gateway runtime and associated libraries, \n- builds the protoc-gen-grpc-gateway plugin, \n- invokes the plugin\n- compiles the *.pb.gw.go files into a go_library\nHopefully we can accomplish easy grpc-gateway support in bazel.  Feedback or contributions welcome.\n. ",
    "mtsgrd": "+1\n. Thanks @yugui, you're a star!\n. ",
    "pgracio": "Thanks @yugui this works like a charm!\n. +1\n. ",
    "gdm85": "@yugui yes, that would frame it better. Shall I update the PR or you want to do one yourself?\nI also noticed documentation about this feature should probably be added in README.md.\n. @yugui no problem. The use case I was referring to is basically direct consumption of the JSON gateway API via a browser, which in turn needs CORS support.\nThe interceptor I need to implement would basically perform this pseudo-Go logic:\n`` go\nfunc CORSInterceptor(ctx context.Context, w http.ResponseWriter, r *http.Request, next Handler) {\n    // note that there is no pathParams, although I was considering passingmethodName` in my original PR to give some context information\nif origin header is not set {\n    next(ctx, w, r)\n    return\n}\n\nif method is \"OPTIONS\" {\n     write CORS allowed methods\n     return\n}\n\nif origin and method are allowed {\n    write CORS headers\n    next(ctx, w, r)\n    return\n}\n\n fail with a 4xx error // browser will not process this response anyway\n return // no call to next() here\n\n}\n```\nEdit: ok I see how this would be done without any change needed by using http.ListenAndServe\nSorry for the noise\n. ",
    "ericchiang": "A lot of the overhead comes from checking to ensure the result is actually compliant with protobuf's defined JSON mapping. It's slower, but it's correct :)\n. ",
    "zackangelo": "Is this completed? Can it be closed? \n. ",
    "tamird": "Yep, see #144.\n. Didn't intend to line anchor - look for the functions I mentioned by name.\n. This is sufficiently supported now. See https://github.com/cockroachdb/cockroach/pull/6754 for an example.\n. This might also enable fixing #124\n. @yugui @willtrking @tmc \n. This failed on go tip because go vet crashed, but is otherwise green.\n. Because it no longer returns a pointer.\n. Rebased.\n\nI don't think this is a \"proper\" handling of Accept header since it ignores media range and quality factor of the header. https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.1\n\nYeah, it's probably incomplete, but still better than the current handling which conflates \"content-type\" (which specifies the incoming content) with \"accept\" (which specifies the outgoing content).\n\nAccept header is not the right way to identify MIME type of request body.\n\nThis is not what this PR does. It uses \"content-type\" to identify the MIME type of the incoming request, but uses \"accept\" to determine the MIME type that should be used to encode the outgoing response.\n\nIf we handle the header, it would require:\n- interpret media range and quality factor\n  - split Marshaler into Marshaler and Unmarshaler.\n    - Independent registry for each\n\nYes, we could do these things, but it doesn't seem \"required\" to me. I'm not sure splitting Marshaler and Unmarshaler is even worth it, but I'll leave that decision to you. Nevertheless, I don't think that refactoring should block this change.\n. @yugui ping, could you take another look?\n. Thanks!\n. LGTM. @yugui can you please take a look?\n. Done.\n. Unexported. I left it make because it returns a value, not a pointer.\n. what's unnecessary? there's no longer a mapping to two marshalers.\n. Done.\n. Yep, done.\n. none of this is exported and the tuple form of map lookup is more correct.\n. Done.\n. ",
    "sadlil": "@yugui probably they are adding any golang/protobuf\n. Fails as json.Unmarshal can't unmarshal int into string here. @floridoo \nerrors_test.go\n. ",
    "tamalsaha": "@yugui, any plans to add support for any types?\n. Hi, \nWe are facing the same issue. I can confirm that this was not a problem in earlier versions.\n. @tartale, https://github.com/appscode/grpc-gateway/commit/f7d82dd8fa5114f282f0ca4a735a5194dd2f05f9 fixes (reverses?) this behavior. We generate a json schema by post processing swagger spec. So, we had to revert this to generate proper json schema.\n. @achew22 , Thanks for your explanation. We generate a json schema that is used for frontend validation.   In that scenario, the data format is more important than actual data. So, in that case, we should be validating it against an \"integer\". Am I right? \n. @achew22 , Will grpc-gateway fail if my frontend sends {\"a\":\"1\",\"b\":2} ? or will it auto convert it into string? I understand that frontend is sending incorrectly formatted data according to protoc3 spec.\nEdit: After reading the spec, the answer is it will work ok. For json -> protoc, string or int both are ok. For protoc->json, protoc will only product strings. \n. @achew22 , it seems to be specified int the notes from your link above.\n\n. The main source of int64 is timestamp. The other places we used int64 could probably be changed to int32. We will probably do that now that we are aware of this issue.\n. @achew22 , How does google.protobuf.Timestamp convert into Json? My guess is that it becomes 2 fields (sec, nano)? If that is the case, it is unacceptable for us. We use PHP, Java, Javascript and GO. Using epoch sec as int64 is the easiest way to keep our sanity.\n. @mattolson, can you tell us about your use-case?. Have you considered adding that in the context as Metadata? Example:\ngwMux := gwrt.NewServeMux(\n    gwrt.WithMetadata(func(c context.Context, req *http.Request) metadata.MD {\n        return metadata.Pairs(\"x-forwarded-method\", req.Method)\n    }),\n). IMHO, grpc-gateway should be application logic agnostic and should not allow users to introduce custom logic at the gateway layer. In a future world, where everything can be done in http/2, you will not need grpc gateway. So, any custom logic you need should be at the grpc side.\nI am still bit confused about your use-case. A concrete example might help. You said Our protos are already defined and require this data to be set in the request message, not the gRPC metadata. I'm looking for a way to reliably set this data on the request message, and also overwrite anything that might come in on the http request. - One way, I can think of this can be done is:\n\nUse WithIncomingHeaderMatcher to pass headers from original request\nUse WithMetadata to add any metadata that might be lost in the conversion process.\nUse a unary interceptor at the grpc server side to set this data on the request message, and also overwrite anything that might come in on the http request. .  As you may know, you can chain interceptors at the grpc layer. So, as long as you run this fixer interceptor before other interceptors, you should be able to keep all the logic at the grpc server side.\n\n. :) I don't know.\nThe one big reason I prefer to put all my logic behind grpc server is that if someone calls the http endpoint directly, they can expect same behavior they will get if they went through grpc-gateway. So, I guess I treat this as a proxy.. It seems IndexAny matches any char from the second string. The order does not matter.\n. Thanks @yugui .\n. @simjay , grpc-gateway compiler will generate a method called gw.Register<YourService>HandlerFromEndpoint. <YourService> will be the service name you used in your proto file, eg,\n`\nservice MyService {\n...\n}. @tmc , we are interested in this one, too. We have multi use-cases where we need headers to be forwarded to grpc server. Can this be added as a configuration so that users of this library can tell which Additional headers to forward. I can think of following options:\n\nspecify a prefix\nspecify an array of headers\n. Here are our use-cases:\nhttps://github.com/kubernetes/helm/issues/1918 - forward k8s-* headers.\nhttps://github.com/appscode/grpc-gateway/commit/d3f3d342208512d44a91b7b8d1985b9ce33f3423  . We maintain a fork needed to pass headers for our own api server.. @lucas-natraj , I think you should use #336 . Then you will be able to send header as you wish, without gateway forcing you to change you header key. With that pr, you should initiate the Mux like below:\ngwMux := runtime.NewServeMux(runtime.EqualFoldMatcher(\"x-api-key\"))\nOn the grpc side, you will find this is the Context metadata as \"x-api-key\". This is what we are doing for our api.. I think given there is already a special prefix (grpc-*), the best case may be to allow custom headers via user configuration. Also, this avoid extra processing for detecting collision for users who do not need these extra headers.. Since we are thinking this from first principles, I would like to give my 2cs.\n\nIn my view, grpc-gateway is a stopgap solution until everything speaks HTTP/2 natively. As a result, gateway should be invisible to the grpc server implementation. Nothing in my grpc server implementation should require knowing about the request is coming from gateway.\nIn that light I think it makes sense passing all headers from user request to grpc server with these changes:\n- grpc-* space is restricted from user request\n- The other common headers that make sense should be changed. We can follow what nginx does in reverse proxy mode. https://www.nginx.com/resources/admin-guide/reverse-proxy/#headers\nSo, I think that automatically adding grpcgateway- prefix, stripping Grpc-Metadata- prefix should be stopped. \nThen we can also give user control via some pr like #336 if they want to do some additional processing. One such use-case we have is, we want to know the actual VERB used by json http endpoint. We add that using #336 . \ngwMux := runtime.NewServeMux(\n    runtime.WithMetadata(func(c context.Context, req *http.Request) metadata.MD {\n        return metadata.Pairs(\"x-forwarded-method\", req.Method)\n    }))\nThe last thing is we should decide how the response headers returned from grpc server are passed back to user. Our use case here is, when rate limiting is enabled, our grpc server returns headers like\nX-RateLimit-Limit: 1200\nX-RateLimit-Remaining: 1193\nX-RateLimit-Reset: 1402425459\nHow do to pass it back to the caller?. @tmc, that might be just an artifacts of the fact that original protobuf did not have map types and http.proto was defined before proto 3 (my guess).. cc: @sadlil. Thanks @tmc .\n\nMutating a global to modify behavior should be avoided, I'd prefer passing some sort of Option.\n\nDo you have any suggestion about that? The only passed thing I can see if the Mux\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/686368427ddb9a51628d63db6c74fbc96a206e1f/protoc-gen-grpc-gateway/gengateway/template.go#L340\n\nIt's not clear to me that we should supply a set of Matcher implementations.\n\nI have no options about this. I thought it could be handy.. @tmc, I have looked into the code and template. It seems to me that extending ServeMux might be easier way. I can also turn my matchers into ServeMuxOption. So, mux sounds good? Then I will try to update this pr. Now, this will be a pretty big change.\n. @tmc, I think it is ready for review. I have extended ServeMux to add the matchers.. @tmc, I have signed the CLA. \nI think https://github.com/grpc-ecosystem/grpc-gateway/pull/323 is taken care by this pr.. @tmc, I have added support for context annotators in my pr. Looks like we need this too!. @ilius, I have added support for adding metadata to context via Mux options. Does this solve your use-case?. I have few questions:\n- When you read cookie, do you need to send it via a direct key on Context or as part of metadata in Context? From looking at the metadata code, it seems to perform some encoding, decoding based on the data.\n\nIf your intention is to reject a call if it is missing Cookie? If so, why not just pass the cookie from http request to context metadata. Then in the grpc side, you can add an interceptor that will reject requests that are missing cookie. Does that work for you?\n\n. @ilius, in that case, you will be able to handle your use case using this pr, like below:\ngwMux := runtime.NewServeMux(runtime.EqualFoldMatcher(\"Cookie\"))\nThen on the grpc side, you can check for JWT token in cookie and reject the request if it fails in a grpc interceptor. IMO, this is better than failing at the gateway layer for 2 reasons:\n- You keep all your application logic in one place instead of passing some to the gateway.\n- If/when applications can direct http/2 calls to the grpc service, it will also work if JWT token in passed via cookie.. @tmc  PTAL.. @tmc, I added doc. Just to be clear, mux's header matcher is applied in both direction (gw -> grpc and grpc -> gw). isPermanentHTTPHeader and Grpc-Metadata- check checks will always fail in grpc -> gw direction (handleForwardResponseServerMetadata), since all keys are small letter in md.HeaderMD (as per http/2). My question is that intentional?. @tmc, tests are passing.. Like a IncomingHeaderMatcher, OutgoingHeaderMatcher ?. Updated pr to add separate incoming/outgoing HeaderMatcher. IncomingHeaderMatcher is set to DefaultHeaderMatcher to maintain current behavior.. @ilius , if you want to forward the Cookie in http request to grpc, you can add an incomingHeaderMatcher for that.\nruntime.NewServeMux(runtime.WithIncomingHeaderMatcher(func(h string) (string, bool) {\n        if strings.EqualFold(h, \"Cookie\") {\n            return h, true\n        }\n        return \"\", false\n    }));\nIf you want to forward a specific key from Cookie, you can also do that by returning that in the if block. . It will be automatically added to the Metadata in grpc context. See LN # 66 & # 95 https://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/context.go#L66. @ilius , I am not sure I understand you. Using runtime.WithIncomingHeaderMatcher, you can pass the full http request's Cookie header to grpc. You can also send a specific key from the Cookie, if you prefer that.  And custom application specific logic for validation that Cookie/Cookie-key should be done in your grpc server (directly in the method or using a interceptor). Does that make sense?. Hi, can https://github.com/grpc-ecosystem/grpc-gateway/pull/336 be merged and be part of 1.2.0?. I have auto generated cahngelog using https://github.com/skywinder/github-changelog-generator. I am guessing we can just make a summary of merged prs.\nChange Log\nUnreleased\nFull Changelog\nClosed issues:\n\nProblem with *.proto as \"no buildable Go source files\" #338\nInvalid import during code generation #337\n\nv1.2.0.rc1 (2017-03-24)\nFull Changelog\nImplemented enhancements:\n\nimprove(genswagger:template):added support for google.protobuf.Timestamp #209 (EranAvidor)\n\nFixed bugs:\n\nSupport for multi-segment elements #122\n\nClosed issues:\n\nGo get breaks with autogenerated code #331\nFresh install no longer generates necessary google/api/annotations.pb.go & google/api/http.pb.go files. #327\nPanic with query parameters #324\nSwagger-UI query parameters for enum types are sent as strings #320\nhide the object name in the response #317\nPackage imported but not used #310\nAuthorization headers aren't specified in Swagger.json #309\nGenerating swagger version, contact name etc in generated docs #303\nFeature request: custom content type per service and rpc #302\nReference: another RESTful api-gateway #299\nIntegration with other languages is partially broken #298\njsonpb convert int64 to integer instead of string #296\ndefault enum value is omitted #294\nAdvice: could we simplify the flow as the below #292\nexamples/browser test failure: TypeError: undefined is not a function (evaluating 'window.location.protocol.startsWith('chrome-extension')') #287\n./entrypoint.go:25: undefined: api.RegisterYourServiceHandlerFromEndpoint #285\nQuery params not handled in swagger file #284\nPlease help: google/api/annotations.proto: File not found. #283\nOption to Allow Swagger for DELETEs with a body #279\nclient declared and not used compilation error, after recent upgrade #276\nfeature request / idea: generating JSONRPC2 client proxies from GRPC #272\nprotoc-swagger-generator messes up the comments if there is rpc method that does not have rest #263\nSwagger Gen: underscores -> lowerCamelCase field names and refs #261\nTimestamp as URL param causes bad request error #260\n\"proto: no coders for int\" printed whenever a gRPC error is returned over grpc-gateway. #259\nCompatibility with grpc.SupportPackageIsVersion4 #258\nHow to use circuit breaker in this grpc gateway? #257\ncannot use example code to generate #255\ntests fail on go tip due to importing of main packages in test #250\nAdd NGINX support #249\nError when reverse proxy to gRPC server (which is impl with Node.js) #246\nError output titlecase instead of lowercase #243\nOption field \"(google.api.http)\" is not a field or extension of message \"ServiceOptions\" #241\nImplement credentials handler in-box #238\nProposal: Support WKT structs for URL params #237\nExample of /} in path template #232\nServing swagger.json from runtime mux? #230\nETCDclientv3 build error with the latest changes - github.com/grpc-ecosystem/grpc-gateway/runtime/marshal_jsonpb.go:114: undefined: jsonpb.Unmarshaler #226\nMap in GET request #223\nHTTPS no longer works #220\n--swagger_out plugin translates proto type int64 to string in Swagger specification #219\nResponse body as a single field #217\ndocumentation of semantics of endpoint declarations #212\ngen-swagger does not generate PATCH method endpoints #211\nprotoc-gen-grpc-gateway doesn't work correctly with option go_package #207\nBrowser Side Streaming Best Practices #206\nDoes grpc-gateway support App Engine? #204\n\"use of internal package\" error, after moving to grpc-ecosystem #203\nMove to google.golang.org/genproto instead of shipping annotations.proto. #202\nRelease v1.1.0 #196\nmarshaler runtime.Marshaler does not handle io.EOF when decoding #195\nprotobuf enumerated values now returned as strings instead of numbers. #186\nsupport annotating fields as required (in swagger/oapi generation)? #175\narchitectural question: Can i codegen the client code for talking to the server ? #167\nPassing ENUM value as URL parameter throws error #166\nSupport specifying which schemes should be output in swagger.json #161\nUse headers for routing #157\nENUM in swagger.json makes client code failed to parse response from gateway #153\nSupport map types #140\ngenerate OpenAPI/swagger documentation at run time? #138\nAfter the 1.7 release, update .travis.yaml to check the compiled proto output #137\nGetting parsed runtime.Pattern from server mux #127\nREST API without proxying #46\n\nMerged pull requests:\n\nUpdate go_out parameter to remove comma #333 (tmc)\nUpdate stale path in README #332 (tmc)\nimprove documentation regarding external dependencies #330 (CaptTofu)\nReturn an error on invalid nested query parameters. #329 (fische)\nUpdate upstream proto files and add google.golang.org/genproto support. #325 (tmc)\nDo not ignore the error coming from http.ListenAndServe in examples #319 (campoy)\nLook up enum value maps by their proto name #315 (nilium)\nenable parsing enums from query parameters #314 (tzneal)\nDo not add imports from methods with no bindings. #312 (fische)\nConvert the first letter of method name to upper #300 (lipixun)\nwrite query parameters to swagger definition #297 (t-yuki)\nBump swagger-client to 2.1.28 for examples/browser #290 (tmc)\npin to version before es6ism #289 (tmc)\nPrevent lack of http bindings from generating non-building output #286 (tmc)\nAdded support for Timestamp in URL. #281 (JohanSJA)\nadd plugin param 'allow_delete_body'  #280 (msample)\nFix ruby gen command #275 (bluehallu)\nMake grpc-gateway support enum fields in path parameter #273 (linuxerwang)\nremove unnecessary make() #271 (tmc)\npreserve field order in swagger spec #270 (tmc)\nMerge #228 #268 (tmc)\nHandle methods with no bindings more carefully #267 (tmc)\ndescribe default marshaler in README.md #266 (tmc)\nAdd request_context flag to utilize (*http.Request).Context() in handlers #265 (tmc)\nRegenerate examples #264 (tmc)\nCorrect runtime.errorBody protobuf field tag #256 (tmc)\nPass permanent HTTP request headers #252 (tmc)\nregenerate examples, fix tests for go tip #248 (tmc)\nRender the swagger request body properly #247 (dprotaso)\nError output should have lowercase attribute names #244 (nathanborror)\nruntime - export prefix constants #236 (philipithomas)\nREADME - Add CoreOS example #231 (philipithomas)\nDocs - Add section about how HTTP maps to gRPC #227 (philipithomas)\nreadme: added links to additional documentation #222 (sdemos)\nUse a released version of protoc #216 (yugui)\nAdd contribution guideline #210 (yugui)\nAllowing unknown fields to be dropped instead of returning error from\u2026 #208 (sriniven)\nAvoid Internal Server Error on zero-length input for bidi streaming #200 (yugui)\n. Since 1.8, GOPATH env var is not required. https://github.com/golang/go/issues/17262 . A backward compatible alternative is $(go env GOPATH). I believe this has been discussed before https://github.com/grpc-ecosystem/grpc-gateway/issues/219#issuecomment-251250029. @aaronjwood , may be use 1.2.0 of grpc-gateway? The real fix is, you need the next release of grpc go. https://github.com/grpc/grpc-go/issues/1187 :). @deejross, I believe your .gw.go files were generated from the gateway plugin from master branch. You need to regenerate the .gw.go files using gateway plugin v1.2.0.\n\nThe generated *.gw.go file format was changed recently by https://github.com/grpc-ecosystem/grpc-gateway/pull/336 in master branch. . We use shall scripts to get specific version of gateway and install its plugins. You can find our script here: https://github.com/appscode/api/blob/master/hack/builddeps.sh#L53\nPlease note, this script installs our forked version with some additional changes. I meant to show this as an example.. To be clear, any custom logic in metadata annotator can be also performed on the grpc server side after passing the full header. An example can be adding  a new header x-cookie-key1 by parsing the Cookie header.. On second thought, this is not needed. Custom logic can read the original req. . @ilius , Cookie is a valid concept in both Http and Http/2 (including gRPC). IMO, your server should work whether the cookie comes via Http or Http/2. If you want to parse cookie on the gRPC side, this is what I do:\n - Create a http.Request from ctx. e.g. https://github.com/grpc/grpc-go/issues/1174#issuecomment-292923366\n - Then use req.Cookie(\"\") for parsing\nBut if you want to only support cookie at gateway layer, this is what you can do:\nruntime.NewServeMux(runtime.WithMetadata(func(ctx context.Context, req *http.Request) (metdata.MD) {\n    return metdata.Pairs(\"x-cookie-k1\", req.Cookie(\"k1\"))\n}));\n. @ilius , you should be using Cookie.String(), not Cookie.Raw. From https://golang.org/src/net/http/cookie.go, you can see that Cookie.Raw is set for Set-Cookie header, but there is no Set-Cookie header in http request.. Glad it worked for you!. Fixed ci. Code that shows that metdata.Pairs() encoded keys to lower case.\nhttps://github.com/grpc/grpc-go/blob/master/metadata/metadata.go#L110\nhttps://github.com/grpc/grpc-go/blob/master/metadata/metadata.go#L53\n. @zweizeichen , gateway follows Google's http.proto spec https://github.com/googleapis/googleapis/blob/master/google/api/http.proto#L214\nThis spec does not allow trailing slash.. @rickcrawford , you are using an old version of grpc-go repo. Please see prior discussion here: https://github.com/grpc-ecosystem/grpc-gateway/issues/364. cc: @tmc . Thank you!. Hi, we would like to see this pr merged. The new status stuff looks great!. @hangll , on your server side, you could check for fields that have values set (aka, non-default value) and then only update those. That is what we do for our service.. @hangll you could define your boolean variables in a way that settings true will require update. Or, you could use a tr-state variable.. @maxkondr , We use grpc-gateway in Kubernetes. We run grpc-gateway and the grpc server in the same process. Then you can easily scale it my scaling the combined process.\n. If you check the readme file, the CoreOS example is a good one for this. https://github.com/grpc-ecosystem/grpc-gateway#more-examples\n. pb/cmdb.pb.gw.go:149: too many arguments in call to runtime.AnnotateContext \nThis tells me that your .gw.go file is generated using gateway from master branch. The generated file structure was changed in #336 . Regenerate the .gw.go using gateway from 1.2.2 . This should be fixed.. If you use grpc-gateway 1.2.2, then you need to generate the *.gw.go files using grpc-gateway 1.2.2. cc: @tmc @achew22 . Basically if you have a service that does not use any path/query parameters, this error happens. This bug was introduced in #378\ncc: @kazegusuri @tmc. Regenerated example protos. Actually I see how this is used.. You can install those plugins from the vendor folder. See here: https://github.com/grpc-ecosystem/grpc-gateway/issues/384#issuecomment-300863457. @jinleileiking . gateway implements https://github.com/googleapis/googleapis/blob/master/google/api/http.proto#L27 . You will find useful information in the proto file, in case you have not seen it before.. This is not a gateway question, rather what you can map into protocol buffer. The short answer is, you can't define arbitrary map[string]interface{} in protocol buffer.  Please check here: https://developers.google.com/protocol-buffers/docs/proto3#oneof\nTo use protocol buffer, you need to know the schema of your data.. I don't think you can do any better with oneof. . As an alternative, you could use map[string]string, then deal with type conversion in your code. In that case, json form might look a bit nicer.. @yugui , mind taking a look at this one?. If you use the grpc-go/status package, you are automatically following this proto. https://github.com/grpc/grpc-go/blob/master/status/status.go#L49\nIs any further change needed?. I am guessing you are using old version of plugin. This flag was added recently. https://github.com/grpc-ecosystem/grpc-gateway/commits/master. I believe the flags are comma separated. Here is a snippet from our build scripts:\nprotoc -I /usr/local/include -I . \\\n         -I ${GOPATH}/src/github.com \\\n         -I ${GOPATH}/src/github.com/googleapis/googleapis/ \\\n         -I ${GOPATH}/src/github.com/grpc-ecosystem/grpc-gateway/third_party/appscodeapis \\\n         --grpc-js-client_out=logtostderr=true,remove_prefix=/_appscode/api,${ALIAS}:. *.proto. @uynap , not sure where I found that. It has a been while. Usually bit of trial and error and head scratching gets the job done. :). Rebased. Looks like Go 1.8 test is failing due to x/text package.. cc: @ilius. @jinleileiking , you might be out of luck. gRPC protocol defines its status using a Status proto. https://github.com/grpc/grpc-go/pull/1156\nhttps://github.com/grpc/grpc-go/blob/master/status/status.go\nGateway just turns that into a JSON object.. @jinleileiking , I think you need to set the HTTPError for runtime.Servemux. See https://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/mux.go#L98. @flisky, can you sign the Google CLA, if you haven't already. PR otherwise looks good to me.. pr is LGTM. I will let @tmc / @achew22 to merge it.. LGTM.. Are you trying to upload a file to some URL? You can do this by just implementing a handler for your pattern and add that pattern to the http.ServerMux. \nYou can see how to use http.ServerMux in the CoreOs code example. https://github.com/philips/grpc-gateway-example/blob/master/cmd/serve.go#L91 . You don't need to involve grpc & gateway for this.. @hangll , I would recommend not to involve grpc-gateway for uploading/downloading files. Please see my previous comment on just adding a handler via http mux.. The separate mux option works with AJAX etc, if that is your use-case. Also, if you are really trying to upload / download files, using direct handler seems less confusing.\nI have tried using the streaming option with grpc & gateway few months ago. It did not work for me from AJAX calls.. Are you referring to example for WithIncomingHeaderMatcher, etc?. I have some example code in https://github.com/cowrypay/dealer/blob/master/pkg/apiserver/cmd/server.go#L114\nYou can see how it is used. There is not a lot of documentation yet, sorry!. > Does grpc-gateway integrate with any ORMs, such as go-pg?\nNo.\n\nFor instance, is it possible to annotate a protobuf message with an option to specify SQL-specific properties on the generated Go model?\n\nYes. https://developers.google.com/protocol-buffers/docs/proto#extensions . You don't need any special support or dependency on this project. Define your field level proto extensions, then write your glue code to use some ORM layer.\n. Thank @yugui . I did not know about this injector thing. But this is probably never going to be supported by the official protobuf team.\nI was talking more about field level annotations like http://godoc.org/github.com/gogo/protobuf/gogoproto\nmessage A {\n        optional string Description = 1 [(gogoproto.nullable) = false];\n        optional int64 Number = 2 [(gogoproto.nullable) = false];\n        optional bytes Id = 3 [(gogoproto.customtype) = \"github.com/gogo/protobuf/test/custom.Uuid\", (gogoproto.nullable) = false];\n    }\nref: https://github.com/gogo/protobuf/blob/master/extensions.md\nI would think it will be possible to take those annotations and then call some ORM code in some glue code.\n. I suppose https://github.com/go-openapi/spec3 will be needed first.. I applied the following 2 commands:\nconsole\n$ git tag -a v1.3.0 8cc3a55 -m v1.3.0\n$ git push --tags origin master\n. Thanks @nilium . I updated accordingly.. @tmc , I am not sure what you mean by WithHeaderMatcher option? Mind elaborating a bit?. There are 2 separate use-cases:\n\n\nheaderMatchers: Whether to pass header from http request to gRPC context (with optional transformation). This is either 1->1 or 1->0.\n\n\nmetadataAnnotator: This is used to add additional metdata to gRPC context. This is 0->1 . This can't be done using header matcher.\n\n\nHere is code snippet from our prod server:\ngwMux := gwrt.NewServeMux(\n    gwrt.EqualFoldMatcher(\"Cookie\"),\n    gwrt.PrefixFoldMatcher(\"x-csrf-\"),\n    gwrt.PrefixFoldMatcher(\"access-control-\"),\n    gwrt.WithMetadata(func(c context.Context, req *http.Request) metadata.MD {\n        return metadata.Pairs(\"x-forwarded-method\", req.Method)\n    }))\n. Updated.. ",
    "gyuho": "@yugui etcd is hitting this issue, though it's happening only in experimental feature.\nDo we have any workaround for this? Or ETA to support this?\nThanks!\n. @yugui First thanks for helping debug this.\netcd still complains {\"Error\":\"json: cannot unmarshal object into Go value of type etcdserverpb.isWatchRequest_RequestUnion\",\"Code\":2}. And tried the latest master branch of grpc-gateway and golang/protobuf, and regenerate the gateway files, as here https://github.com/coreos/etcd/pull/5891/commits/87592304b2c095425e224f289e122143c8e27487.\nI added reflect.TypeOf(marshaler) to this line https://github.com/gyuho/etcd/blob/3108859828f851e9a46772fe8e16d257b40f3639/etcdserver/etcdserverpb/rpc.pb.gw.go#L91 and confirm that *runtime.JSONPb typed marshaler is being used, but still seeting that error.\n*runtime.JSONPb\n2016-07-06 22:07:13.861689 I | v3rpc/grpc: Failed to decode request: json: cannot unmarshal object into Go value of type etcdserverpb.isWatchRequest_RequestUnion\nMaybe we generate gateway files in the wrong way? I tried to regenerate *.gw.go files but don't see any diff.\nThanks.\n. @yugui Still doesn't seem to work.\n```\ncurl -L http://localhost:2379/v3alpha/watch -X POST -d '{\"create_request\": {\"key\": \"Zm9v\"}}'\n{\"Error\":\"unknown field \\\"create_request\\\" in etcdserverpb.WatchRequest\",\"Code\":2}\n```\nThanks!\n. @yugui I will see if I can find some workarounds or file an issue to gogoproto. Thanks a lot!\n. @yugui Could you give us ETA on this?\netcd team assumed that this would be merged by next Wednesday, and postponed the release that was scheduled today.\nThanks!\n. Thanks. Now our gateway server doesn't complain but now seems like it returns before interacting with gateway request?\nAnd it works if it were written as below:\ndiff\n        dec := marshaler.NewDecoder(req.Body)\n        handleSend := func() error {\n                var protoReq WatchRequest\n                err = dec.Decode(&protoReq)\n-               if err != nil {\n+               if err != nil && err != io.EOF {\n                        grpclog.Printf(\"Failed to decode request: %v\", err)\n                        return err\n                }\n                if err = stream.Send(&protoReq); err != nil {\n                        grpclog.Printf(\"Failed to send request: %v\", err)\n                        return err\n                }\nor\ndiff\n                var protoReq WatchRequest\n                err = dec.Decode(&protoReq)\n+               if err == io.EOF {\n+                       return nil\n+               }\n                if err != nil {\n                        grpclog.Printf(\"Failed to decode request: %v\", err)\n                        return err\n                }\n                if err = stream.Send(&protoReq); err != nil {\n                        grpclog.Printf(\"Failed to send request: %v\", err)\n                        return err\n                }\ninstead of\ndiff\n                var protoReq WatchRequest\n                err = dec.Decode(&protoReq)\n+               if err == io.EOF {\n+                       return err\n+               }\n                if err != nil {\n                        grpclog.Printf(\"Failed to decode request: %v\", err)\n                        return err\n                }\n                if err = stream.Send(&protoReq); err != nil {\n                        grpclog.Printf(\"Failed to send request: %v\", err)\n                        return err\n                }\nWould it be something to be handled in zero-length stream that you mentioned?\n. @yugui Another question is that why we were sending zero-length input in the first place?\ndiff\n        \"github.com/golang/protobuf/proto\"\n@@ -86,7 +89,9 @@ func request_Watch_Watch_0(ctx context.Context, marshaler runtime.Marshaler, cli\n                grpclog.Printf(\"Failed to start streaming: %v\", err)\n                return nil, metadata, err\n        }\n-       dec := marshaler.NewDecoder(req.Body)\n+       bts, err := ioutil.ReadAll(req.Body)\n+       fmt.Println(string(bts), err)\n+       dec := marshaler.NewDecoder(bytes.NewReader(bts))\nThe print statement returns {\"create_request\": {\"key\": \"Zm9v\"}} <nil>, so the request body was not empty, but dec.Decode(&protoReq) returns io.EOF, which doesn't make sense. Is this expected? Or some kind of grpc-specific behavior?\nThanks!\n. Sorry for late response. Yeah I am still seeing the same behavior. I will try to have reproducible code for this issue.\nThanks.\n. @yugui Closing this, because as you said, message itself does not send io.EOF. The problem was that we were using cURL, sending EOF when it closes the request. Thanks for help!\n. ",
    "floridoo": "How about this then:\njson\n{\n  \"error\": \"Unauthenticated\",\n  \"code\": 16\n}\n. ~~but unrelated to the pull request, it fails on the master branch too~~\nSorry, my bad. You are right.\n. Passing now.\nThanks @sadlil \n. Thanks. Great project btw!\n. ",
    "kazegusuri": "I prefer structured log than combined log because we can take what we want from logs easily.\nSo +1 to separate to error and code.\n. Regenerated examples. PTAL.\n. Thank you for the comment. I haven't take care streaming because I have never touched.\nIt sounds good to rename to runtime.ServerMetadata and return it from function.\nEvent after that, the caller of request_xxx set the metdata to context to my understanding, right?\nIf this is okay, I will populate the changes.\n. > Also I am thinking of forwarding headers and trailers in the forwarder functions. What do you think about it?\nSorry, I've missed the comment. The metadata are set to context, so we can extract byruntime.ServerMetadataFromContext(ctx) from anywhere including forwarder functions.\nThere is no need to pass the metadata by arguments, but it's not strong opinion.\n. Addressed comments, but have problems in streaming rpc.\nReceiving header in stream.\nstream.Header() is blocking method until header is ready or context has done, so calling stream.Header at first in request_xxx blocks forever if server does not send headers first by grpc.SendHeader.\nAnother way is to call stream.Header() after sending message, which means add following code after here.\nmsg, err := stream.CloseAndRecv()\n    md, err := stream.Header()\n    if err != nil {\n        glog.Errorf(\"Failed to get stream header: %v\", err)\n        return nil, metadata, err\n    }\n    metadata.HeaderMD = md\n    metadata.TrailerMD = stream.Trailer()\n    return msg, metadata, err\nIt works well in my small example with streaming, but in integration test it fails sometimes with following error.  \nintegration_test.go:359: {\"error\":\"stream error: code = 1 desc = \\\"context canceled\\\"\",\"code\":2}\nI'm not sure why context was canceled. \nReceiving trailer in stream\nstream.Trailer() must be called after stream.Recv(), so we cannot get trailer in request_xxx when .Method.GetServerStreaming is true because recv() is called in forward_xxx.\nIIUC, Trailer is used for application-level error information.  How about passing Trailer to handleForwardResponseStreamError when recv() returns error in ForwardResponseStream and user can define stream error handler like DefaultHTTPError?\n. I have fixed metadata handling with some limitations; Dropping support for header metadata in streaming RPC because it's a bit difficult, is it acceptable? And added support to return metadata to client by default.\n1. Header metadata is returned to client by HTTP header with Grpc-metadata- prefix in both normal response and error response, but only on Unary RPC. \n2. Trailer metadata is returned to client by response body with top-level trailer key and json value only on error response, but both Unary RPC and Streaming RPC are supported.\nPTAL @yugui \n. > It looks weird to put it in response body because it was metadata, and the application-level error does not belong to a specific chunk of body.\n\nI understand that it is hard to implement for streaming RPC, but is it possible to forward trailer metadata as HTTP 1.1 trailers?\n\nI never knew HTTP 1.1 trailer. I'll try it. \nI think it's better to use HTTP 1.1 trailer for streaming RPC. Do you think to use it for Unary RPC?\n. My first thought is the use case of gRPC trailer is specific for error details, so I attached the metadata to error message for both unary and streaming rpc.\nBut I understand it's metadata and it's good design to map gRPC trailer to HTTP 1.1 trailer.\n\nI understand that it is hard to implement for streaming RPC, but is it possible to forward trailer metadata as HTTP 1.1 trailers?\n\nPossible for unary RPC and streaming RPC with client streaming, but impossible for with server streaming.\nGo 1.5 http server does not send trailers which are not included in Trailer header. When gRPC trailer is sent to gateway while receiving streaming, HTTP header was already sent to client and we cannot always know trailer before sending header.\n\nWe can list up trailer fields before sending response body for unary RPC. But we cannot for streaming RPC. So we cannot send Trailer header in such a case -- it violates a recommendation in RFC 2616.\n\nAs described above, yes we cannot use HTTP 1.1 trailer for streaming RPC.\nSo I used HTTP 1.1 trialer only for unary RPC, and for streaming RPC grpc trailer is returned as independent chunk like error message. In both case grpc trailer is always returned to client if set regardless of the response is error or not.  Does this make sense? @yugui \n\nAs @achew22 pointed, it might not be compatible to browser implementations.\n\nI cannot know how impact to use HTTP 1.1 trailer.\nAnyway HTTP 1.1 trailer cannot be used for streaming RPC and response body is used for that case in current impl. If this can be a problem, it's a reasonable option to use response body for unary RPC, too.\n. > Header metadata are mapped to HTTP headers which have Grpc-Metadata- prefix. It is supported in all of unary, client-streaming, server-streaming and both-streaming RPCs.\nNot supported for server-streaming and both-streaming RPCs (at least this PR) because header timing issue.\n\nTrailer metadata are mapped to HTTP trailers which have Grpc-Trailer- prefix. But it is supported only in unary RPC and client-streaming RPC.\n\nYes. But for server-streaming and both-streaming, Trailer metadata are returned as an extra chunk of message body.\n. > You need to call stream.CloseSend. Then you can safely call Header() before CloseAndRecv.\nThank you for comment. I will check it out.\n\nI am not confident if it is the right solution. So it looks better to conservatively avoid supporting trailer metadata in server-streaming and both-streaming. Is it OK for you?\n\nYes.\n. > Header metadata are mapped to HTTP headers which have Grpc-Metadata- prefix. It is supported in all of unary, client-streaming, server-streaming and both-streaming RPCs.\n\nTrailer metadata are mapped to HTTP trailers which have Grpc-Trailer- prefix. But it is supported only in unary RPC and client-streaming RPC.\n\nI hope all comments addressed and met the spec.\nPTAL @yugui \n. Addressed comments.\n. Thanks! I'm grateful for your help.\n. I've noticed this in my env.\n. LGTM. I'm using this branch in my env and confirmed it fixed goroutine leak.\n. This issue is variable cannot be captured from multiple path components when using {uuid=**}, right?\nI tried and it worked correctly with changing examples to {uuid=**} and small server change about uuid generation. something misunderstanding?\n$ curl -d @body.json -H 'Content-Type: application/json' localhost:8080/v1/example/a_bit_of_everything\n{\"uuid\":\"foo/1fb1b956a11ac231deb7270b9f792db201f4c4a1797ff2b1\",\"nested\":[{\"name\":\"bar\",\"amount\":10},{\"name\":\"baz\",\"amount\":20}],\"float_value\":1.5,\"double_value\":2.5,\"int64_value\":4294967296,\"uint64_value\":9223372036854775807,\"int32_value\":-2147483648,\"fixed64_value\":9223372036854775807,\"fixed32_value\":4294967295,\"bool_value\":true,\"string_value\":\"strprefix/foo\",\"uint32_value\":4294967295,\"sfixed32_value\":2147483647,\"sfixed64_value\":-4611686018427387904,\"sint32_value\":2147483647,\"sint64_value\":4611686018427387903}\n$ curl localhost:8080/v1/example/a_bit_of_everything/foo/1fb1b956a11ac231deb7270b9f792db201f4c4a1797ff2b1\n{\"uuid\":\"foo/1fb1b956a11ac231deb7270b9f792db201f4c4a1797ff2b1\",\"nested\":[{\"name\":\"bar\",\"amount\":10},{\"name\":\"baz\",\"amount\":20}],\"float_value\":1.5,\"double_value\":2.5,\"int64_value\":4294967296,\"uint64_value\":9223372036854775807,\"int32_value\":-2147483648,\"fixed64_value\":9223372036854775807,\"fixed32_value\":4294967295,\"bool_value\":true,\"string_value\":\"strprefix/foo\",\"uint32_value\":4294967295,\"sfixed32_value\":2147483647,\"sfixed64_value\":-4611686018427387904,\"sint32_value\":2147483647,\"sint64_value\":4611686018427387903}\n$ curl localhost:8080/v1/example/a_bit_of_everything/foo\n{\"error\":\"not found\",\"code\":5}\n. > I don't know if you saw that Travis CI runs automatically on pull requests. If you did you'll see that part of the build is to regenerate the .pb files on every patch. To get travis to bless your build you should update all your godeps , make clean and try building all the examples again then check in the results with your patch.\nI have regenerated all pb files from proto, but the go:tips build failed while go:1.5 build succeeded.  I have no idea why a part of build fails. I use go 1.6 locally and the tests succeed off cause.\n\nYou should also make a new example that demonstrates this new code in action. Put it in the examples directory and compile it (just like in step 1) to make sure that future changes to grpc-gateway won't break your code.\n\nJust move a proto message to a new proto in other directory to test the changes instead of introducing new examples.\nFYI, if go_package option is specified as go import path, generated files from the proto are stored in relative path as import path from current directory. This makes build script complicated, but the behavior is same to protoc-gen-go.\n. What's the remaining work? I want this feature, any help?\n. LGTM\n. It was actually caused by using newer version of protoc v3.3.0. I regenerated files with protoc v3.1.0.. ready to be merged if no one opposes the response status.. Yes, status proto as error was introduced via https://github.com/grpc-ecosystem/grpc-gateway/pull/378.. > @kazegusuri, protoErrorHandler returns 501 Not Implemented instead of 405 Method Not Allowed when the given request method is not allowed (defined) on the resource. Is it really good.\nBecause gRPC spec says Method not found at server returns UNIMPLEMENTED and the code means 501 Not Implemented in grpc-gateway. However gRPC spec defines HTTP to gRPC Status Code Mapping and it says UNIMPLEMENTED matches 404 Not Found.\n\nWas it the reason why you kept DefaultHTTPProtoErrorHandler separated from DefaulHTTPError \n\nFor the first part,  actually there is no strong reason, but I wanted to use an option in order to change the behavior of the error handler instead of modify the variable directly.\n\nDefaultHTTPProtoErrorHandler returns a different format of response body from DefaultHTTPError?\n\nThe ProtoErrorHandler returns error as status proto, so client can parse the response with the proto.. ",
    "hashbou": "Misunderstanding from my site. It works fine! \n. ",
    "zxy198717": "@naibaf0 Did you solved this issue?. @ivucica Thanks for your works!\nHave you been working on Support remaining OpenAPI v2 objects ('security', 'security definitions', etc)\uff1f. +1. I signed it!. ",
    "naibaf0": "Hi @zxy198717 \nIf I remember correctly, no I didn't. We just settled on merging the proto files for swagger generation and since the specific project ended up on the backburner anyways I didn't look into it more.. signed.\n. ",
    "Rio": "We're running into the same issue now. Creating a client for each service on the browser side is something our frontenders would like to avoid. But seems to us that for now that's how it is. Until we're a bit more sure about our pipeline and api setup we're keeping the multiple clients.. I'm pro this change. Ran into this a couple of times now and the default now is just not what I expect it would be.. ",
    "coyle": "Thanks @shurcooL !\n. ",
    "t-yuki": "swagger-ui and swagger-editor uses the name as a schema name in UI so the shorter name improves visibility.\nhttp://petstore.swagger.io/?url=https://raw.githubusercontent.com/gengo/grpc-gateway/master/examples/examplepb/streamless_everything.swagger.json#!/ABitOfEverythingService/List\nhttp://editor.swagger.io/ \n. Oops, I didn't make realclean after update.\nIt's done.\n. Currently json Marshal/Unmarshal of enum string for gengateway requires jsonpb.\nActually proto2 files may work with enum string because protoc-gen-go generates if it is not proto3 (why??): https://github.com/golang/protobuf/blob/master/protoc-gen-go/generator/generator.go#L1407\n. FYI, I've prototyped swagger support for map recently.\nShould I make a pull request?\nhttps://github.com/gengo/grpc-gateway/compare/master...t-yuki:feature/map\n. ping @googlebot. Should I re-push without rebase to avoid CLA bot fail?. FYI, In my use case, I wrote a swagger json with contact and versions then simply concat by jq in build pipeline.\neg: jq -s '.[0] * .[1]' service.swagger.json swagger-info.json >${out}. ping @googlebot. map values is hard to process in query parameter. (some WAF handles it in map_value[KEY]=VALUE)\nshould we ignore it initially?. Agree. I unset default field.\nNote that, swagger petstore example has the array parameter with default field.\nI wonder what it means but in protobuf, it should be nil.\n\"parameters\":[{\"name\":\"status\",\"in\":\"query\",\"description\":\"Status values that need to be considered for filter\",\"required\":true,\"type\":\"array\",\"items\":{\"type\":\"string\",\"enum\":[\"available\",\"pending\",\"sold\"],\"default\":\"available\"},\"collectionFormat\":\"multi\"}],. ",
    "sknaumov": "I think it would be useful to have different defaults for stream vs regular requests.. I'm trying to convert an JSON-encoded error returned by grpc-gateway back to grpc one:\n```go\npackage gateway\nimport (\n    \"encoding/json\"\n    spb \"google.golang.org/genproto/googleapis/rpc/status\"\n    \"google.golang.org/grpc/codes\"\n    \"google.golang.org/grpc/status\"\n    \"io\"\n    \"net/http\"\n)\n/ Similar type is defined in grpc-gateway package, but it is not exported /\ntype errorBody struct {\n    Error string json:\"error\"\n    Code  int32  json:\"code\"\n}\nfunc HTTPgRPCError(r *http.Response) error {\n    if r.StatusCode == 200 {\n        return nil\n    }\n    if r.Header.Get(\"Content-Type\") == \"application/json\" {\n        if s, err := JSONBodyToStatus(r.Body); err == nil {\n            if e := s.Err(); e != nil {\n                return e\n            }\n        }\n    }\n    return status.Error(codes.Unknown, r.Status)\n}\nfunc JSONBodyToStatus(body io.Reader) (*status.Status, error) {\n    var e errorBody\n    if err := json.NewDecoder(body).Decode(&e); err != nil {\n        return nil, err\n    }\n    return status.FromProto(&spb.Status{Code: e.Code, Message: e.Error}), nil\n}\n```. ",
    "shawnps": "Couple more listed here:\nhttps://goreportcard.com/report/github.com/gengo/grpc-gateway#misspell\n. (Maybe just remove the file argument in the Errorf calls)\n. @achew22 np. If you like what I found, please consider adding a Go Report Card badge to the README :+1: \n[![Go Report Card](https://goreportcard.com/badge/github.com/gengo/grpc-gateway)](https://goreportcard.com/report/github.com/gengo/grpc-gateway)\n. ",
    "diogomonica": "I'm on 0e69ad1, so latest master.\n. It seems to build now. Probably screwed up along somewhere. Thank you.\n. The generated protos that are currently in the repository work, but when regenerating using the instructions:\n\u279c protoc -I/usr/local/include -I. \\\n -I$GOPATH/src \\     \n -I$GOPATH/src/github.com/gengo/grpc-gateway/third_party/googleapis \\\n --go_out=Mgoogle/api/annotations.proto=github.com/gengo/grpc-gateway/third_party/googleapis/google/api,plugins=grpc:. echo_service.proto\n\u279c protoc -I/usr/local/include -I. \\\n -I$GOPATH/src \\     \n -I$GOPATH/src/github.com/gengo/grpc-gateway/third_party/googleapis \\\n --grpc-gateway_out=logtostderr=true:. echo_service.proto\nUsing protoc:\n\u279c  protoc --version\nlibprotoc 3.0.0\nAnd with go-grpc on 0e69ad1 (master) from 9 hours ago, and grpc-gateway on 67e1374 (master), I still get:\n```\n\u279c  go build \ngithub.com/gengo/grpc-gateway/examples/examplepb\nexamplepb/echo_service.pb.go:114: cannot use _EchoService_Echo_Handler (type func(interface {}, context.Context, grpc.Codec, []byte) (interface {}, error)) as type grpc.methodHandler in field value\nexamplepb/echo_service.pb.go:118: cannot use _EchoService_EchoBody_Handler (type func(interface {}, context.Context, grpc.Codec, []byte) (interface {}, error)) as type grpc.methodHandler in field value\n```\n. Sorry, opened the issue in the wrong project :(\n. ",
    "cuongdo": "Thank you for your fast response. I've made the changes you suggested.\n\nReview status: 0 of 10 files reviewed at latest revision, 5 unresolved discussions.\n\nruntime/mux.go, line 85 [r1] (raw file):\nDone.\n\nruntime/pattern.go, line 147 [r1] (raw file):\nDone.\n\nruntime/pattern.go, line 163 [r1] (raw file):\nDone.\n\nruntime/pattern.go, line 169 [r1] (raw file):\nDone.\n\nruntime/pattern.go, line 194 [r1] (raw file):\nDone.\n\nComments from the review on Reviewable.io\n Sent from Reviewable.io \n. Also, I've removed the \"Pattern successfully built\" log message at the end of NewPattern, since that appears often as well.\n\nReview status: 0 of 10 files reviewed at latest revision, 5 unresolved discussions.\n\nComments from the review on Reviewable.io\n Sent from Reviewable.io \n. Great. Thanks for your very fast response!\nOn Tue, Mar 1, 2016 at 6:37 PM Yuki Yugui Sonoda notifications@github.com\nwrote:\n\nMerged #118 https://github.com/gengo/grpc-gateway/pull/118.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/gengo/grpc-gateway/pull/118#event-573882560.\n. signed\n. \n",
    "bluecmd": "Odd. That looks exactly what I tried and couldn't get to work. I'll see if I can reproduce it from current master.\n. I don't remember TBH, I'll reopen if I run into this again in the future!. ",
    "kellrott": "It looks like #134 covers this issue. Hopefully that will be merged in soon.\n. I think this would be a nice feature to have. It looks like the Travis testing errors are because of a dependency error: The command \"go get github.com/gengo/grpc-gateway/examples\" failed and exited with 2\n. ",
    "willtrking": "@yugui \nName change makes a lot of sense to me, I think I'll probably go with Marshaler. How are you thinking of doing the MIME type abstraction? Based on the Content-Type header from the client? If so it seems like that would be pretty easy for me to take care of. I was considering some sort of system to determine if the protocol buffers being used required jsonpb as well, but this would of course take longer. It would also hide potential performance impacts of using jsonpb. \nIn what cases can the google.api.HttpRule.body not be a protocol buffers? Looking through the examples it looked liked the body sections of the options still deserialized into a protocol buffer in the generated code, although I'm sure I'm missing something given how new I am to this project. Any particular source files I should look through for this?\n. Basic changes:\n- Name change to Marshaler\n- ServeMux now defines an InboundMarshaler and an OutboundMarshaler. This allows you to use different marshallers for inbound requests from the client, and outbound responses to the client.\n- Each Marshaler also now defines a ContentType function which is used when responding to the client. This would (theoretically, untested) provide support for XML, and other responses besides JSON.\n- The DefaultError function now uses a Marshaler when sending an error back.\n- The default Marshaler for the ServeMux is a wrapper around encoding/json. If we switched to jsonpb this could break existing codebases.\nMIME registry is in here as well, details as follows:\n-  A registry can be created with NewMarshalerMIMERegistry and you can add inbound/outbound marshallers for different (case-sensitive) mime types. \n- Content-Type request header is used to determine marshallers.\n- The * MIME type is a catch all for any inbound Content-Type headers. Specifically set MIME types take precendence over *. So if you have both application/json and * in the registry, and the client sends application/json the marshallers for application/json will be used. \n- If nothing is set in the registry, or the specified Content-Type is not found, and * is not set, it defaults to the ServeMux's InboundMarshaler and OutboundMarshaler. If those are nil, it has compile time defaults as well. Take a look at the runtime/marshaler.go source for more details.\n- Code will panic if you attempt to setup a MIME type in the registry with an empty string.\n- The registry is attached to the ServeMux with the MIMERegistry attribute. \n@yugui \nIn terms of the edge case when google.api.HttpRule.body is not *, I have not touched this yet. Considering type assertions as a good go between, or perhaps generate new types that support standard encoding/json marshaling, marshal into those types, and then load the data from the marshaled type into the proper protocol buffer field.\n@tmc \nWould love to help with this, not quite sure how I would go about it though. As it stands right now the swagger generation would have to have a reference to the ServeMux used by the server, as all the Marshaler interfaces are attached to that. I've never used swagger, so I'm not too sure what the output/usage pattern looks like here.\n. @tamird Yes it does allow that, we did that for a bit in development but never in production.\n. Signed\n. ",
    "joeblew99": "https://www.youtube.com/watch?v=qHrS0lqeZeo&nohtml5=False\nlooks like google have implemented something kind of similar to this in golang, but also they do not support streams. they added service discovery and some nice tooling..\n. Might be worth checking out luci-go repo.\nIt's by Google appengine team.\nThey are doing similar stuff to this repo in regards to grpc.\nIt's not easy to find - it's a huge repo.\nhttps://github.com/luci/luci-go/\nOne ref to help:\nhttps://github.com/luci/luci-go/blob/master/client/cmd/rpc/call.go\nThey are also not supporting streaming back up to a browser using WS or SSE as far as I can tell.\n. +1\nThis would be nice. I presume you mean the swagger-ui ?\n. I agree. Its not something grpc-gateway should do i am starting to realise. At first i thought it was, but now i can see its possible for web browsers to talk to the rpc server itself. Grpc-gateway becomes a server for older third party acess essentially i guess ?\nThis is worth looking at. They are generating typescript and integrating with react.\nhttps://github.com/cockroachdb/cockroach/tree/master/ui\nThanks for your links, and your efforts with that. I think i will try out those also.\n. I would be happy to add more to your repo actually.\nits exactly what i was looking for. I simple best practice example of where code gen can make our clent code more predictable / type safe etc\n. I notices that the swaggger and openAPi group as thinking of supporting WS as part of the swagger spec.\nI am currently using GRPC internally, and wrapping with the GRPC gateway, to produce a swagger spec for the outside world.\nthen from the swagger spec i generate Javascript API.\nSo, if swagger starts to offer WS, then things will be complete.\n. @tmv \nThank you for pointing that out. This is really something i have been wanting for a while. \nThe code i saw referring to it was in the go-swagger repo. They are these days part of the open-api initiative. I cant remember the exact file.\nBut i tried that stack, and now am trying our the GRPC, GRPC-Gateway stack. I feel this is better for many reasons we dont need to go into.\nAlso i am building a React and React native stack on top of all this and saw your code for generating thats part of the stack at https://github.com/pwmckenna/swagger-models-to-react-proptypes.\nThis is awesome work, and something on my critical path.\n. I would also like to see this pulled into the main project.\nEspecially with the genproto work being done.\nIt would solve lots of boilerplate hacks for me and probably others.\n. ",
    "liaoliaopro": "@yugui  That help a lot. Great thanks!\n. Sorry my mistake. Update grpc-go to latest version fixed this.\n. ",
    "fiorix": "I had like 3 proto files used for specific things. Then elsewhere in another proto file I would import public all of them, and generate the proxy for the entire API. The .pb.go comes out right, but the .gw.go one does not include anything from the imported files.\n. a.proto:\n```\nsyntax = \"proto3\";\npackage a;\nservice Echo {\n    rpc Echo(EchoRequest) returns (EchoResponse) {}\n}\nmessage EchoRequest {\n    string Text = 1;\n}\nmessage EchoResponse {\n    string Text = 1;\n}\n```\nb.proto:\n```\nsyntax = \"proto3\";\npackage b;\nservice Ping {\n    rpc Ping(PingRequest) returns (PingResponse) {}\n}\nmessage PingRequest {\n    string Text = 1;\n}\nmessage PingResponse {\n    string Text = 1;\n}\n```\nroot.proto:\n```\nsyntax = \"proto3\";\npackage root;\nimport public \"a.proto\";\nimport public \"b.proto\";\n```\nWhen I generate the root.pb.go I get all types from a and b. The gateway and swagger ones are empty.\n. Sure, thanks!\n. ",
    "kyleconroy": "@achew22 I don't think it's a dupe of #79. The proto3 JSON mapping says that maps should be encoded as objects, not arrays. More important, the generated swagger document is incorrect. The gateway that's generated from this proto file will not accept the labels property as an array of hashes: it requires an object.\n@t-yuki Please do!\n. ",
    "googlebot": "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. t looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. t looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. t looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. :frowning_face: Sorry, but only Googlers may change the label cla: yes.. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\nGooglers can find more info about SignCLA and this PR by following this link.\n need_author_consent \n. A Googler has manually verified that the CLAs look good.\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\nGooglers can find more info about SignCLA and this PR by following this link.\n cla_yes \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n\u2139\ufe0f Googlers: Go here for more info.\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\u2139\ufe0f Googlers: Go here for more info.\n need_author_cla \n. CLAs look good, thanks!\n\u2139\ufe0f Googlers: Go here for more info.\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n\u2139\ufe0f Googlers: Go here for more info.\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\u2139\ufe0f Googlers: Go here for more info.\n need_author_cla \n. CLAs look good, thanks!\n\u2139\ufe0f Googlers: Go here for more info.\n ok \n. ",
    "eddiezane": "Any update on this?\nLooks like Swagger has officially been renamed to the OpenAPI Specification.\nI'm looking to open a PR to add some additional options that will allow for filtering out private/internal fields as well as marking things as Output only.\nLooking to this PR as guidance.. ",
    "fnchooft": "Afternoon, after checking out again today (2016/05/10), the problem  has been resolved.\nThank you.\nPS: The only thing I had to do is CamelCase the client-calls.\nIn order to do that I added a small sed-statement in the Makefile I am using.\nMakefile-rule:\n%.pb.gw.go: %.proto\n    protoc -I/usr/local/include -I. \\\n        -I${GOPATH}/src \\\n        -I${GOPATH}/src/github.com/gengo/grpc-gateway/third_party/googleapis \\\n        --grpc-gateway_out=logtostderr=true:. \\\n        $<\n        sed -e \"s/client\\.\\(.\\)/client\\.\\u\\1/g\" -i $@\n. Evening @igateno,\nI tested yesterday and all is fine, the only issue I faced was the fact that my rpc-function names are lower-case in the proto-file. The grpc-gateway_out-plugin uses the same literal-string as written in the  proto-file. Since the plugin is used with the GO-language, where public functions are written in Camelcase, maybe the plugin could Camelcase those rpcs and emit a warning. \nDo not forget to regenerate all go-files, to make sure that the files are consistent.\n. ",
    "igateno": "I'm having a similar issue, as I described in https://github.com/gengo/grpc-gateway/issues/153\n@fnchooft you said you were able to make this work? I just checked out the latest master (2016/05/10 @ 11:04 GMT-8) and I just get a bunch of Go errors in my *.pb.gw.go file.\n. So my issue was that I didn't run go build after updating grpc-gateway to the latest master. It's fixed now. Thanks!\n. I'm having this same issue right now. I don't think the right solution is to generate integer for enum type, that would defeat the purpose of having an enum in the first place. Here's what I would suggest:\nIf I have the proto message\nmessage Ticket {\n  int ID = 1;\n  enum Priority {\n    LOW = 0;\n    HIGH = 1;\n  }\n  Priority priority = 2;\n}\nThen I should be able to create a ticket by making a POST request with this body\n{\n  \"priority\": \"HIGH\"\n}\nAnd I'd expect grpc-gateway to give me an error if I tried to set priority to \"FOO\" instead of \"LOW\" or \"HIGH\". Does that make sense?\n. @fabware this issue is fixed if you pull the latest from master. It works exactly as I describe able.\n. I guess I didn't read far enough into the README before posting this.\nAny general guidance for someone who might try to dive in and implement the solution?\n. This issue kept showing up when I was trying to generate the stub. It only went away after I did an rm -rf $GOPATH/* and cloned my repo fresh. \nAs I was debugging, I noticed that the proto package is still gengo.grpc.gateway. Is there a reason it wasn't changed to grpc-ecosystem.grpc.gateway? Could this be causing issues? \n. ",
    "fabware": "It would be fine to use strings, if\n1. response from gateway also treat ENUM as strings;\n2. correct swagger client code can be generated. \nTranslate JSON by hand at client side can be very tedious.\n. Yes, I found it working that way, it's been a huge commit. Thanks for your reminder anyway.\nStill swagger ENUM object of swift client code seems not compatible with string. \nI have to modify protoc-swagger-gen to produce enum as strings, it is now working like charm.\n. ",
    "hasLeland": "I'd like to add my support for this. Right now no query parameters are added at all for the generated paths in any swagger spec. This makes most all swagger specs useless, unless you map every field in every message to path parameters, which is very dirty solution indeed.\n. ",
    "johanbrandhorst": "The workaround right now is to pretty much use POST for all your HTTP endpoints, as far as I can tell. I second the need for this fix.\n. I believe this has been fixed in recent months.. You can put comments in your proto files and the swagger generator will put those comments in your swagger.json. See https://github.com/gengo/grpc-gateway/blob/master/examples/examplepb/echo_service.proto for an example of how to format the comments.\n. This should be closed\n. @kassiansun as discussed in #760, we're likely to revert the fix for this change so you can use  :verb for custom verbs as you say.. I will reopen this for discussion once #761 has been merged. @jfhamlin sorry for this, but we're going to release 1.5.1 with a revert of this. If you still require this functionality we can consider adding a generator option. What do you think?. Reopening because fix was reverted with #761.. Thanks @jfhamlin it's clear us maintainers didn't understand the full situation. You've made a good case for introducing this as a flag, as you say, since we can't break backwards compatibility, even for a case where the gateway is doing the wrong thing. Another thing to add to the list of our v2 design changes.\nWould you be interested in drafting a PR to get this in?. Hi @Tommy-42. Yes, we want to get in the best of both worlds, the \"last one wins\" as suggested by @jfhamlin. Unfortunately, because it breaks backwards compatibility, until we release v2 we will have to hide it behind a generator flag. Would you be interested in contributing a fix for this?. Pinging @awalterschulze, the author of gogo/protobuf\n. At work I wrote a post-generate workaround for this in python:\n```python\ndef correct_grpc_gateway_imports(package):\n    '''\n    Workaround for https://github.com/grpc-ecosystem/grpc-gateway/issues/229.\n    Replace instances of 'empty.Empty' with 'types.Empty' in any files.\n    '''\n    for root, dirs, filenames in os.walk(package):\n        for filename in filenames:\n            with open(os.path.join(root, filename), 'r') as f:\n                file_contents = f.read()\n        file_contents = file_contents.replace('empty.Empty', 'types.Empty')\n\n        with open(os.path.join(root, filename), 'w') as f:\n            f.write(file_contents)\n\n``. https://coreos.com/blog/gRPC-protobufs-swagger.html has a pretty cool example of how to set up your swagger definitions on top of your API server.\n. This is part of the v2 work in #546.. Just to recap, I think the goal of this issue is for the grpc-gateway to support usingbody:\"\"and still allow some fields in the query; http.proto [explicitly states](https://github.com/googleapis/googleapis/blob/5e7316da92db2b389046b9e6367a5dfb048e04fe/google/api/http.proto#L216-L217) that the use ofbody:\"\"precludes the use of query parameters altogether. I understand that this project wants to stay true to the design ofhttp.proto`, so the right place for that discussion to happen is probably the googleapis repo.\nHowever, using specific body parameters (body: \"message\") in combination with query parameters should work. If that is not the case, it is a separate issue.. @atombender please correct any incorrect information in my previous post.. Seems like this was superceded by #546 . With the new CI system proposed in #772 this will be trivial to accomplish, and I'm not sure what if anything stands in our way to do it, but lets get back to this once #772 has been merged.. @tmc @achew22 I think we could probably remove it altogether as well - personally when I'm generating swagger files I don't use swagger-codegen as the Go code is next to useless anyway. Again, this can easily be accomplished after #772 has been merged. I don't think it adds any extra confidence as it is.. Yes :). I don't think it adds anything. Would you like to create a new PR with it removed? We can remove it from the Dockerfile as well.. Ok after some discussion a requirement has presented itself; if we remove swagger-codegen, we must replace it with some other method by which we can validate that our output swagger files are sensible. I have some personal experience with using https://github.com/go-swagger/go-swagger, which seems to do the job. If anyone wants to replace swagger-codegen we should consider it at the same time.. As mentioned in https://github.com/grpc-ecosystem/grpc-gateway/pull/795#issuecomment-435618237, the use of swagger-codegen is now leading to slightly annoying travis files appearing after every generation. We can fix this with a simple .gitignore rule, but it's just another thing we shouldn't have to do.. Hi @cappucino5. Please reach out for help with this type of matter on chat services such as Gophers slack channel. This is not the best medium for this kind of debugging.. #145 added a lot of the stuff for OpenAPI, but unfortunately it did not add support for custom response codes, so I'm going to bump this. Implementing it at this point should be significantly easier though because of the work done in #145.. See https://github.com/grpc-ecosystem/grpc-gateway/pull/145/files#diff-aae53fe2a10de658028c4b8332afb356R62. I've opened #663 to help with this.. The error is coming from https://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/convert.go#L39.\n. Here's a stack trace (edited):\nnet/http.(*http2serverConn).runHandler.func1(0xc42031c010, 0xc420471faf, 0xc42034a000)\n        /usr/lib/go/src/net/http/h2_bundle.go:4592 +0x190\npanic(0x9b54c0, 0xc42012c8a0)\n        /usr/lib/go/src/runtime/panic.go:489 +0x2cf\nmyrepo/vendor/github.com/grpc-ecosystem/grpc-gateway/runtime.Int32(0xc4202f45e0, 0x9, 0x0, 0x0, 0x0)\n        /go/src/myrepo/vendor/github.com/grpc-ecosystem/grpc-gateway/runtime/convert.go:41 +0x63\nreflect.Value.call(0x9d3b60, 0xab4658, 0x13, 0xa8c513, 0x4, 0xc420471420, 0x1, 0x1, 0xc42012c870, 0x9b54c0, ...)\n        /usr/lib/go/src/reflect/value.go:434 +0x91f\nreflect.Value.Call(0x9d3b60, 0xab4658, 0x13, 0xc420471420, 0x1, 0x1, 0xa076fc, 0x76, 0x30)\n        /usr/lib/go/src/reflect/value.go:302 +0xa4\nmyrepo/vendor/github.com/grpc-ecosystem/grpc-gateway/runtime.populateField(0x9e5400, 0xc4202f2fb0, 0x185, 0xc4202f45e0, 0x9, 0xc420298b40,\n 0xc4202f2fb0, 0x185)\n        /go/src/myrepo/vendor/github.com/grpc-ecosystem/grpc-gateway/runtime/query.go:158 +0x21a\nmyrepo/vendor/github.com/grpc-ecosystem/grpc-gateway/runtime.populateFieldValueFromPath(0xda6f40, 0xc4202f2f80, 0xc42012c750, 0x1, 0x1, 0x\nc42012c740, 0x1, 0x1, 0xc42012c740, 0x0)\n        /go/src/myrepo/vendor/github.com/grpc-ecosystem/grpc-gateway/runtime/query.go:87 +0x8f7\nmyrepo/vendor/github.com/grpc-ecosystem/grpc-gateway/runtime.PopulateQueryParameters(0xda6f40, 0xc4202f2f80, 0xc4201e4840, 0xf02160, 0xc42\n00821a8, 0x433401)\n        /go/src/myrepo/vendor/github.com/grpc-ecosystem/grpc-gateway/runtime/query.go:24 +0x1c3\nmyrepo/vendor/myprotorepo/myprotoservice.request_Method_0(0x7f11586fa9a0, 0xc4201e4810, 0xdab680, 0xc420253840,\n0xdade60, 0xc42000e118, 0xc420342100, 0xc4201e46c0, 0x24, 0x16, ...)\n        /go/src/myrepo/vendor/myprotorepo/myprotoservice/myprotoservice.pb.gw.go:193 +0x139\nmyrepo/vendor/myprotorepo/myprotoservice.RegisteHandler.func7(0xda9740, 0xc42031c010, 0xc420342100, 0xc4201e46c0)\n        /go/src/myrepo/vendor/myprotorepo/myprotoservice/myprotoservice.pb.gw.go:490 +0x1fd\nmyrepo/vendor/github.com/grpc-ecosystem/grpc-gateway/runtime.(*ServeMux).ServeHTTP(0xc42024ecf0, 0xda9740, 0xc42031c010, 0xc420342100)\n        /go/src/myrepo/vendor/github.com/grpc-ecosystem/grpc-gateway/runtime/mux.go:89 +0x7f4\nnet/http.(*ServeMux).ServeHTTP(0xc420237d40, 0xda9740, 0xc42031c010, 0xc420342100)\n        /usr/lib/go/src/net/http/server.go:2238 +0x130. Indeed EnumValueMap is returning an empty map for the enum type. AH this is because we've generated our code with gogo/proto instead of golang/proto, the enum is never registered with golang/proto which is what grpc-gateway calls to. Anything we can do about that?. This is probably not grpc-gateways fault, so I'm gonna go ahead and close it. I'm doubtful the grpc-gateway will ever allow the user to specify the protobuf backend.. The wording might not be great, enums are supported in query parameters of the URL, not the path.. Hi @ZachEddy, we'd happily welcome any documentation improvements you could contribute. I haven't got any experience with this myself so your knowledge would be very good to capture.. I'd say yes. We should have the same support guarantees as the standard library which means current release - 2, which still leave us at 1.9. Context was added in 1.7. Would you be interested in contributing a fix for this?. The gRPC gateway is it's own http server, so the only thing you can do is to use the same certificate for the gRPC gateway, unfortunately.. Same thing for the client side of things, it's its own http client, so while you could attach client certificate information to gRPC metadata, you can't forward the certificate itself.. I will leave this open, but this will require major work before it can be merged.. Please rebase on master for new CI functionality.. This is because the grpc-gateway uses the golang/protobuf registered enum map for lookups. Like suggested in the link you provided you should use the goproto_registration extension to enable dual registration for your proto files.. See https://github.com/gogo/protobuf/blob/master/gogoproto/doc.go#L151. I think it's questionable whether the swagger generator should really care about the go_package option in its generator, but I see there's a need for a more explicit option to direct the output files of the generation. We could potentially support the paths=source_relative parameter that is in protoc-gen-go though: https://github.com/golang/protobuf/commit/6fb8a6f1c1f011b7fde2b40f72f46587180d8d25.. gRPC-Go now supports using status.WithDetails, however, DefaultHTTPError https://github.com/grpc-ecosystem/grpc-gateway/blob/e4b8a938efae14de11fd97311e873e989896348c/runtime/errors.go#L81 does not support marshalling this. I'm not sure this is the right place to put this but I didn't want to raise a new issue while this issue was still open. @tmc, your thoughts?. I wrote a tiny patch that fixes this in the one case I tried:\n```\ndiff --git a/github.com/grpc-ecosystem/grpc-gateway/runtime/errors.go b/github.com/grpc-ecosystem/grpc-gateway/runtime/errors.go\nindex 8eebdcf4..445bf46c 100644\n--- a/github.com/grpc-ecosystem/grpc-gateway/runtime/errors.go\n+++ b/github.com/grpc-ecosystem/grpc-gateway/runtime/errors.go\n@@ -63,8 +63,9 @@ var (\n )\ntype errorBody struct {\n-       Error string protobuf:\"bytes,1,name=error\" json:\"error\"\n-       Code  int32  protobuf:\"varint,2,name=code\" json:\"code\"\n+       Error   string        protobuf:\"bytes,1,name=error\" json:\"error\"\n+       Code    int32         protobuf:\"varint,2,name=code\" json:\"code\"\n+       Details []interface{} json:\"details\"\n }\n//Make this also conform to proto.Message for builtin JSONPb Marshaler\n@@ -90,8 +91,9 @@ func DefaultHTTPError(ctx context.Context, mux *ServeMux, marshaler Marshaler, w\n        }\n    body := &errorBody{\n\n\nError: s.Message(),\nCode:  int32(s.Code()),\nError:   s.Message(),\nCode:    int32(s.Code()),\nDetails: s.Details(),\n        }buf, merr := marshaler.Marshal(body)\n\n```\n\n\nNot quite sure what to put in the protobuf field of the Details type. Thoughts appreciated.. On second thought this probably won't work with []interface as the type as it may be used in proto marshallers. At the same time using []*any.Any does not marshal properly without some help.. I think this can be closed now that https://github.com/grpc-ecosystem/grpc-gateway/pull/515 has been merged.. I'll close this as a dupe of #702, please reply if you're still experiencing this problem @Multiply . @Multiply thanks for replying, I think that sounds reasonable, looking at the PR that closed #702 it appears we're only doing this expansion for fields called parent or name (ref: https://github.com/grpc-ecosystem/grpc-gateway/commit/de0b679b7b2e83123304a21180e92b10b52f4f7c#diff-d419038d63db0fdabc61f0d6cc6683aeR567), unless I'm misreading it. An easy fix might be to add your id field there as well?. I don't know, sorry. If you'd like to experiment with it and find something that works for your use case feel free to open a PR. @achew22 might have a better understanding about why it does or doesn't work.. Support for this was merged with #695 and #496 :tada: . If you're using JSON as an interchange format, why not just use float64 in your proto definitions?. You can't send values like that in valid JSON. It seems to me you must quote it or hope your marshaller supports invalid JSON?. I have to disagree with the assertion that this  scenario affects many users. You are of course welcome to submit a PR with your suggested changes but I don't think this is something that affects many users, and as such I doubt anyone of the maintainers will be keen to pick this up. My sympathies that this is out of your control, but have you considered some alternative solution like a pre-gateway number transformer handler? Might be as simple as running a regex over the incoming strings and quoting large numerical sequences.. For posterity, don't do what @DoctorQ suggests, use uint32 or don't use JSON at all when working with integers larger than 53 bits.. I proposed another solution too: you can wrap the gRPC gateway mux in your own handler that just converts large number sequences to quoted sequences. It would just be a simple http.Handler with some regex parsing perhaps? Or just manually scan and find large numbers.. The OpenAPI 3.0 spec has added support for Authentication (JWT, Oath, HTTP Basic), which is something that is missing from 2.0. I think this would make it worthwhile to migrate to generating 3.0 compliant definitions.. I'm pretty sure there's an extension for V2 (x-nullable?).. https://github.com/grpc-ecosystem/grpc-gateway/issues/900 is a real issue that would be covered by this.. @UladzimirTrehubenka thanks for that incredibly interesting summary of the issue, but I'm not sure I understand what needs to be done. Should we implement an option in the generator/mix to disable this behaviour?. Not as far as I know. So would this be a runtime option or a generator option?. Hm, I made a simple string search for x-www-url-encoded and couldn't find it used anywhere, are you sure the grpc-gateway implements this handling?. Thanks for pointing that out, seeing as it is tied to the mux I think we can make this a runtime option.. Would you be interested in submitting a patch for this @UladzimirTrehubenka?. I think it should be fairly straightforward, add an option that sets a variable, then check the value of the variable before handling the fallback.. Unfortunately none of the maintainers are able to dedicate any work to this right now, what can I do to help you get a pull request fix for this?. Something like this: https://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/mux.go#L99. Except WithDisablePathLengthFallback. It could set a boolean on the Mux type, which we can check in the function call you found.. Yes, it seems suitable as a runtime option, not something that's specific to your API. Do you disagree?. This looks like it still needs some work, and may even be considered for v2 if the submitter wants to change the default marshaller.. Please rebase on master for new CI functionality.. This PR looks abandoned. Would you like to pick up the mantle @wimspaargaren?. Judging by the comments from @achew22 there still needs to be a decision made whether this should be a backwards compatible change (perhaps under a flag) and so mergable today or whether we want to make it a breaking change in which case it would have to wait until v2. As v2 is currently somewhat unclear, I would suggest the former. In any case, you personally couldn't take over this PR, you'd have to submit a new one because of how the CLA works.. Superceded by #904. This is just indicative of the client not being able to connect to the server. Check your setup.. @EQ94 can you show exactly what you ran? This is way too vague a description. This doesn't sound like a bug in the grpc-gateway, so maybe join the #grpc-gateway channel on Gophers slack https://invite.slack.golangbridge.org/ and we can help you debug it? I will close this issue since it hasn't had any more activity from the OP.. The proto example contains no HTTP annotations, so it makes some sense that it doesn't output anything.. I've tried playing around with this a bit, and I can't seem to get this to work the way I think it should work. Given the example and the code in this PR, I'm assuming the field mask should be provided as a query parameter like so:\nbash\ncurl -k -X PATCH \"https://localhost:11000/api/v1/user/1?update_mask=role\" -H  \"accept: application/json\" -H  \"Content-Type: application/json\" -d \"{\\\"role\\\": \\\"ADMIN\\\"}\"\nGiven the following definitions:\n```protobuf\nrpc UpdateUser(UpdateUserRequest) returns (User) {\n    option (google.api.http) = {\n        patch: \"/api/v1/user/{user.id}\"\n        body: \"user\"\n    };\n}\nmessage UpdateUserRequest {\n    User user = 1;\n    google.protobuf.FieldMask update_mask = 2;\n}\n```\nUnfortunately, this kind of definition doesn't even survive generation; the generated code tries to decode into a **User:\ngo\nif err := marshaler.NewDecoder(req.Body).Decode(&protoReq.User); err != nil {\nIs there an example of this implemented a used successfully in the wild? The query test \"example\" is basically useless as far as I'm concerned.. @glerchundi thanks for that, sounds like my use case should work, I will do some more testing and report back.. Update: my initial problems were indeed because I was using GoGo Protobuf. My intentions are actually to investigate whether this works with gogoprotobuf, but I wanted to get it working first, so thanks for pointing me in the right direction. I will see what would need to be done on the gogo/protobuf side, but I expect this will flat out not work with gogo/protobuf until an alternative implementation for https://github.com/grpc-ecosystem/grpc-gateway/blob/58f78b988bc393694cef62b92c5cde77e4742ff5/runtime/query.go#L275 can be made.\nOut of interest; since the update_mask specifies fields as strings, what reflect magic do you use to figure out which fields of the struct to update?. @glerchundi I'm afraid you're making an incorrect assumption about how proto.MessageName works. golang/proto.MessageName only returns messages that are registered with the golang/protobuf type registry. gogo/protobuf is forced to maintain its own type registry, which leads to this problem as well as many other issues. The links you're providing ignores the fact that one registeres with gogo/protobuf.RegisterType and one with golang/protobuf.RegisterType.\nIf you want more information about this kind of thing, you're welcome to read my blogpost on gogo/protobuf compatibility issues: https://jbrandhorst.com/post/gogoproto/.\nThere are basically two solutions to this problem:\n\nMake all gogo/protobuf files register with both type registries.\n    This is undesireable because gogo/protobuf shouldn't have to import golang/protobuf (for what I think should be obvious reasons).\nWe implement some way in the gRPC-Gateway that accesses both registries, presumably with golang/protobuf taking precedence.\n2.1 Alternatively, allow users of the gRPC-Gateway runtime to select a proto.MessageName resolver.\n\nAnyway, as I mentioned, there seems to be another problem here with the gogo/protobuf generated message, so I will look into that before suggesting any changes to the gRPC-Gateway.. Hm, that is a fair point, I didn't know about that method. It would solve the problem with proto.MessageName specifically, but if that name was used to look up any types in proto.MessageType, it would still error.. I agree, and as I said there are other issues here with gogo/protobuf which I will investigate at the same time. Feel free to follow the discussion in https://github.com/gogo/grpc-example/issues/9 if you wish.. @llimllib I'm not sure what the error case has to do with knowing the structure of errorBody, that only concerns clients of the API? Your gRPC server doesn't need to do anything about it or worry about its structure. Am I misunderstanding something?\nAs for the enum translation, I haven't looked at the code recently but I believe this is configured in your JSON marshaller, and not the gateway (except for query parameters, which are always handled by the gateway code).. I see, yes, well, the proper way to do this might be to implement an alternative error return in the generated swagger definitions. We did a similar thing for streamed returns recently: https://github.com/grpc-ecosystem/grpc-gateway/commit/f336fbcc3f21acf89e935c2353e7ead734ec31f5. Would you be interested in contributing something like that?. I think it would mostly be hardcoding new things into the swagger generator template tbh; especially since the format of error body is constant. Many of the files in that change just concern generated test files, the meat of the changes are in protoc-gen-swagger/genswagger/generator.go and  protoc-gen-swagger/genswagger/template.go. Let me know if you need more help :). You could join the #grpc-gateway channel on the gophers slack for more immediate feedback.. I agree that this is a problem. The reason for the streaming error being the way it is seems to be to make it fit into the for loop that writes to the ResponseWriter: https://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/handler.go#L63.\nI think actually the solution to this problem is to move away from our custom error types and just use the status.Proto(), which is perfectly marshallable and consistent. It does not contain any HTTP translations of course, but tbh we're already translating the gRPC Code to a HTTP code in the actual response code, so I don't think it's necessary. It would mean we can remove the stream_chunk.proto type altogether.\nThe {\"error\": <actual error>} object key probably has to stay, to differentiate from normal messages (which use the {\"result\": <actual message>} key).\n@tmc @achew22 thoughts on this? Would obviously break backwards compatibility in terms of the message returned from errors, but some change here is obviously needed.\n. Making this change makes DefaultHTTPProtoErrorHandler and DefaultHTTPError almost identical. In general, I think this area could do with an overhaul, and will necessitate a 2.0 release.. @achew22 please review https://github.com/grpc-ecosystem/grpc-gateway/pull/560 and https://github.com/grpc-ecosystem/grpc-gateway/pull/561 for initial, backward compatible changes.. I drafted https://github.com/grpc-ecosystem/grpc-gateway/pull/564 for returning status.Proto() from streams, but I feel more confident baout #560 and #561.. Looks like we need another rebase here. Sorry for the long wait.. Please rebase on master for new CI functionality.. This seems to be superceded by #546 . This seems to have suffered some bitrot, which is unfortunate for a large change like this as it'll be a pain to keep rebasing it.. Other things I'd like for 2.0:\nOutput errors in the same format as the gRPC-Go status.Status, that is, simply use status.Convert followed by marshaller.Marshal.. Another one: Adopt the correct behaviour for custom verbs (see https://github.com/grpc-ecosystem/grpc-gateway/issues/224#issuecomment-426091809).. Please rebase on master for new CI functionality. :grimacing: . Looks like we're still missing tests and documentation here.. Please rebase on master for new CI functionality.. You have to use field masks. This will be automatically supported with #671.. It is still unclear to me what this issue is about. As far as I know marshalling the any.Any type should work as expected. Can you make a test case to show that it doesn't work in a PR? That would be a great starting point for a fix.. Indeed, seems like a and b are missing in the parameters array. Is this a regression?. @Wotzhs I think the easiest thing would be to start with adding an example that reproduces the issue, e.g. add a method to a_little_bit_of_everything.proto which will trigger this bug, regenerate all the files (https://github.com/grpc-ecosystem/grpc-gateway/blob/master/CONTRIBUTING.md#i-want-to-regenerate-the-files-after-making-changes) and lets see what we can work out. I think from that point on it will be a matter of editing https://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-swagger/genswagger/generator.go to fix any bugs.. Great work so far @Wotzhs, could you submit your changes in a pull request so we can review them? It sounds to me like you're doing the right thing :).. This change + more is included in https://github.com/grpc-ecosystem/grpc-gateway/pull/561, so review that before accepting this.. Closed in favour of #561 . Probably easiest to merge this and close #560 if we want both of these changes.. I hate to bump this but I think it's a pretty straightforward, backwards compatible change with a new test case and I'd really like to have this merged ASAP. Please advise if I can speed things up somehow.. Closed #560 and #564.. Any chance we could have a new release with this? 1.4.0?. Seems like this will be fixed by #458 . Could you come up with a practical alternative? The stream is already established at this point, so a handler function may require rewriting of much of the logic.. @abursavich Thanks for that! I think one problem with your approach is that we expose a new function type, used by the user, which has an empty interface in the signature. That's pretty poor interface design. It also rewrites quite a bit of the logic, and it seems to add an error case if the response is nil, which may break backwards compatibility? I think your suggestion is a step in the right direction, as I would also like more control over the writing of mesages, but maybe the configuration should be more fine grained? One option for formatting the error and one for formatting a successful reply? I'm not sure.. My heart is not really in this change after merging #561, so I will close this to let someone else provide a better solution to the specific problem of customizing stream error returns.. I'll reopen the issue since this seems to still be a problem. I don't know of any workarounds myself, and I'm not sure how this could possibly be solved without something like a wildcard in the router which isn't supported by the standard library router. Do you have any ideas?. Nice digging @Tommy-42, this will absolutely help others who may be wanting to test the same thing! If I find the time I might look at this but no promises I'm afraid.. Seems like that issue was closed with https://github.com/grpc-ecosystem/grpc-gateway/commit/7226a0d0cd5d6a64d73ed5eba1ce19cad859d70f. Is this still a problem or did that fix this as well?. How do you mean using golang/protobuf enforces a specific version of grpc-gateway? master of grpc-gateway does use version 1.1 of golang/protobuf, but I don't know about any inverse requirement?. @Tommy-42 you can use an override to force a specific version:\ntoml\n[[override]]\n    name = \"github.com/golang/protobuf\"\n    version = \"1.1.0\"\nhttps://golang.github.io/dep/docs/Gopkg.toml.html#dependency-rules-constraint-and-override. Can you confirm this is fixed in master then so we can close this issue?. Great, thanks for confirming!. @duck8823 Are you still interested in finishing this?. I'll close it for you. Hopefully someone can come along and implement this if they need it.. Bump, need a rebase and a test case please.. Yeah I think this will go some way but it's not the only problem. Please see the grpc-gateway section of my post on gogoproto compatibility: https://jbrandhorst.com/post/gogoproto/. Theres nothing that cant be worked around though.. @achew22 this should be a harmless change, could you take a quick look please?. This changes just removes a no-op that was there from when this was first added, probably because the original implementation was confused as to whether this would be necessary or not. This kind of marshalling is already done in several other places:\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/58f78b988bc393694cef62b92c5cde77e4742ff5/runtime/proto_errors.go#L37\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/58f78b988bc393694cef62b92c5cde77e4742ff5/runtime/handler.go#L178\nI will look at adding some tests.. Added a test for marshalling with details.. @tmc rebase completed!. @tmc Is that test failure due to my change? It seems unrelated.. Rebased :). Think I've hit a flaky test, could I have a rerun please? @tmc. Looking good on my end, happy to merge?. Not entirely sure why it's required me to regenerate the protofiles though :thinking: . Curses! I'll fix it.. Alright that should've done it. I had to pull and push over HTTPS as this airport WiFi must be blocking port 22 :thinking:.. This puzzled me a lot as well initially, but I think now that the wrapper is so that we can provide \"result\" with successful calls and \"error\" when a call fails and we return a marshalled error. Because we can't use trailers with success or failure codes like gRPC does, we have to have some way to differentiate between success and failure, hence the wrapper.\nWith that in mind, option 2 is the preferred solution here. In addition, I'm not sure swagger really supports streaming responses anyway?\nYes, we do plan on releasing a v2 at some point, though all the current maintainers are busy so the project is really in maintenance mode. There's a v2 PR open and an issue label you can take a look at if you are interested in picking that up.\nAs to option 3, you'll be pleased to know that such a solution kind of exists in https://github.com/tmc/grpc-websocket-proxy.. Reopening since @birdayz might still want to submit a fix for this.. Arguably the correct mapping is that defined by the Google protobuf error code proto file; 429 Too Many Requests.. Looks like there are more codes that should be chaged, FailedPrecondition should be 400 Bad Request, not 412 PreconditionFailed. This is probably a good time to review all the codes.. @veqryn interesting, do you have a source for this? I couldn't find it in the Wikipedia article.. Is there an RFC? Is it well supported in browsers and clients? It seems a little bit bare bones, if you don't mind me saying. Do we risk breaking more consumers than not by introducing it?. I think we might be best of waiting here, it doesn't seem clear to me that this is indeed appropriate. I'd be happy to be proved wrong, of course.. Bump. @yugui are you interested in finishing this work?. This ought to be easier to continue work on if we rebase on master.. This seems like it's trying to accomplish the same thing as #570. @yugui are you interested in finishing this?. @Tommy-42 if you want to, you can take this branch and rebase on master again, we've fixed the CI generation problems.. Hi @ensonic, we're made some changes to CI since you first had these problems, want to rebase and give it another go?. Please rebase on master for new CI functionality.. Interesting, is that because of a dependency update or something? Would you be interested in submitting a PR to regenerate the file, or, regenerate it in this PR?. So if you look at our CI tests it's actually saying that you still need to regenerate things. Bazel doesn't use the generated files in the same way (apparently) but we still need them. The License type needs to be added to the generated file.. Don't run just make test locally, please regenerate the files as per the contribution guidelines: https://github.com/grpc-ecosystem/grpc-gateway/blob/master/CONTRIBUTING.md#i-want-to-regenerate-the-files-after-making-changes. Nice one, thanks!. Thanks for the contribution :). Our old CI system was largely to blame, unfortunately there's still lots of good contributions like this laying around.. @Mistobaan Thanks for this! Could you rebase this and take a look at the comments?. Please rebase on master for new CI functionality.. I'm going to close this, could you please create an issue if this is still a problem?. Can we close this issue?. Bump, what's left to do before we can merge this?. Please rebase on master for new CI functionality.. I think there's an open PR for allowing the operation id to be configured. I'm not working on it myself.. See https://github.com/grpc-ecosystem/grpc-gateway/pull/739 and #740. I think we can have both, since the current default is to leave it empty, we can change that default in line with this issue but also allow it to be configured per #740. #739 has stalled so if you could pick that up in a separate PR I'd be happy to help you get that merged.. It sounds like this would have to be part of a v2 release if it goes in at all.. @ivucica @tmc @achew22 . Gonna see if I can add automatic references for namespaced messages as well, so please don't merge just yet.. Actually, go ahead and merge if it's good, I'll make another PR for the additional functionality.. Wasn't too hard it turns out :). @ivucica are you happy with the changes? @achew22 could you rerun the failure please, seems like a github TLS timeout error?. Cool, I think I should have fixed the failing test, lets get this in :D.. @achew22 I can't seem to get swagger-codegen installed - I hate to be a bother but any chance you could just clone and regenerate the examples? I'll take another stab tomorrow at work but haven't got time now.. @ivucica thanks I will take a look through the logs. I tried using their docker image and it didn't work.. Alright I should've gotten them this time. Sure is fiddly :thinking:.. OK so I had some time to look into recursive resolution of manually referenced types - I got it working but I can't help but feel like it's a bit hacky. I'd be happy with any tips on improving it, but basically it required me to keep a separate set of referenced definitions and render them after everything else, recursively.. Bump - any thoughts on the additions?. Made changes as requested. @tmc @achew22 I think the test failure was a flaky test, could one of you rerun it please?. @achew22 thanks for retriggering it, seems like it passed ;). I appreciate the time you put into maintaining this project, so no worries about forgetting!. @ivucica anymore comments?. Friendly Monday bump @achew22 @ivucica . At the risk of becoming a tyrant on my first day, I'm going to commandeer and this and merge as @ivucica has been given ample time to add any more opinions and he conceded that he was happy with the PR even without the minor changes, that have been addressed.. @princejha95 That sounds like a bug, have you got your protofile and generated swagger file so we can see if there's an error?. Yeah, that looks wrong. Could you open a separate issue with a minimal protofile which reproduces the error? I think something like the following should do:\n```protobuf\nsyntax = \"proto3\";\nimport \"google/api/annotations.proto\";\nservice SampleService1{\n    // Comment 1\n    rpc Method (Empty) returns (Empty){\n        option (google.api.http) = {\n            get : \"/api/method\"\n        };\n    }\n}\nservice SampleService2{\n    // Comment 2\n    rpc Method (Empty) returns (Empty){\n        option (google.api.http) = {\n            get : \"/api/method\"\n        };\n    }\n}\nmessage Empty {}\n``. Hi @bsakweson. Unfortunately,allow_merge` will almost certainly only work within protofiles from the sake package. Yours would be a feature request, as I believe it is working as intended here. Would you like to open another issue to track this?. I have re-triggered the job.. As with the other PR, could you please squash the commits? Thank you.. Could you also please reword this title, description and commit message to clearly state the intent please? Something along the lines of:\nAllow explicit empty security definition to overwrite existing definitions.. Thank you for your contribution!. Hi! Can you please rebase this on top of master? There was recently a change to the swagger generator. You can run something like:\nmake realclean && make examples SWAGGER_CODEGEN=\"java -jar $HOME/local/swagger-codegen-cli.jar\"\nfrom the root of the repo to regenerate all examples. This will require swagger-codegen (and Java) installed.. Hi, could you squash the commits as well please? Sorry I didn't specify this in the last message.. Thanks, I will leave it for @achew22 to approve and merge, but I'm a little confused by the PR description. Looking at the code this change looks like it parses the method comment and adds it to \"Summary\" and \"Description\" in accordance with other such comment parsing, but the PR title, description and commit message is a little confusing. Could you reword it for clarity please?. I think actually this x-nullable extension should probably be enabled on all non-scalar types. I assume message types are already nullable, so maybe the safest thing is to have a whitelist in the generator that inserts this property. Is there anything I can do to help you get a PR with this in @marcusljx?. I think @ivucica should be able to give some pointers.. If it's not oai3 specific then I think all of the above still apply, but the biggest problem I see is #4 on Ivan's list. It should by no means be impossible but it's no longer as simple as I first thought. Please feel free to give it a go though.. This is a great contribution! Could you please squash the commits and make sure there's a single commit with a new title and description? It'll help us when writing release notes as well as for historical purposes of course.. @dmacthedestroyer check you've got the same version of protoc and protoc-gen-go. The protoc  version used is 3.1.0 (holy hell that's old we should upgrade), and the protoc-gen-go version is b4deda0973fb4c70b50d226b1af49f3da59f5265.. Also, since we're getting ready to merge this, please squash the commits and add a nice explanatory commit message.\nAs for the vet failure, I'm not sure I understand? I looked at the diff but it's full of unrelated stuff.. I believe git rebase -i origin/master will interactively rebase the current branch on top of current master, which will allow both squashing and rebasing at once.\nThen git push -f yourremote yourbranch should fix the commits on the remote.. @dmacthedestroyer Is there anything we can do to help you here? I'm keen to have this in before it becomes a pain to keep rebasing it.. All the best @dmacthedestroyer - we're all volunteers here :). Let me know if there's anyway we can help you!. We've release v1.5.0 without this, but we're still very keen on having it! Lets us know if there's anything we can do to help.. My thinking is we just check it into the third_party folder.. Actually, field_mask.proto is a well known type, it shouldn't need to be checked in. Whatever we do to get it building wity empty.proto and duration.proto, we just need to do the same thing as there.. Please rebase on master for new CI functionality, regenerating should be much easier now!. Superceded by #806 . I haven't used opencensus with the gateway myself but I imagine it's just a matter of creating an interceptor that extracts the trace from the incoming metadata and stuffs it back in the context?. If your spans are not contiguous you must not be passing the context along properly. Are you making sure to use the database/sql methods that use context?. This is not the place for this type of debugging, perhaps join the #opencensus channel on the gophers slack? https://invite.slack.golangbridge.org/\n. https://coreos.com/blog/grpc-protobufs-swagger.html is always my go-to when I need to show someone using them in parallel.. What is the purpose of this pull request? Could you please expand the description a little bit. I also think you'd be required to squash you commits into a single, well described commit.. Closing this as no explanation was given.. This is already being considered in #546 . I think the easiest fix here will be copy-pasting the protoc-gen-go generator logic into the generator, but it is problematic since long term they will inevitably deviate. This here is another source of incompatibility with protoc-gen-gogo as well since the latter allows a custom name to be used. I wish we could have some way of reading from the file we use but I don't think it's possible.\nThis should be reasonably easy to fix, anyway.. @waveywaves thanks! I don't think there's anything particular to keep in mind if we can easily port the logic from protoc-gen-go.. This is being considered in #546 . This is really interesting, but to make things easier for us maintainers, could you please include a description of the changes? Include any external links as well.. @co3k that's great thanks! I think this looks good but I would like the input of @ivucica as well before we merge.. @co3k There's definitely something weird up with our CI pipeline at the moment, I'm trying to work out with the other maintainers what we need to do, sorry for the inconvenience. We'll get this change in as soon as possible!. > go get -u github.com/golang/protobuf/protoc-gen-go\nYou absolutely should not be expected to do this. Install it from the vendor folder: go install ./vendor/github.com/golang/protobuf/protoc-gen-go. Thanks for your contribution!. @achew22 This was part of the change I made already: https://github.com/grpc-ecosystem/grpc-gateway/blob/221ef342721da51038fa6fc3fbd4f0d190f9f6f7/Makefile#L119. This is because https://github.com/grpc-ecosystem/grpc-gateway/pull/667, which fixed the Summary being taken from the comment on the RPC method. It is a feature. Do you wish to keep this issue open to argue that this is undesirable behaviour? Otherwise please close it.. I'm personally in favor of just committing a Gopkg.toml and Gopkg.lock like opencensus-go did.. @tmc I'm on it. Sounds like adding support for the tag option is a good first PR :).. Passing parameters via headers is not currently supported. Headers map to gRPC metadata.. I don't think so! Would you like to contribute a fix?. Hi @co3k, and thanks for your contribution! The title of this PR (and commit message) confuses me, could you reword it please? I don't understand what this does just from reading the title.. Let me see if I understand correctly. Would this also be a correct description?\nPopulate swagger method parameter description from message comments\nIf so, I think that is clearer yet than your update ;).. @achew22 @tmc any objections to this?. Could you try against master? Might be time for us to cut a new release with this fix.. A new release has been made, this should be fixed.. Hi @jriecken! Thanks for your contribution! The idea looks good, unfortunately, it looks like your changes caused an issue with the marshal_jsonpb tests:\nbash\nmarshal_jsonpb_test.go:274: strings.Contains(\"{\\\"singleNested\\\":{},\\\"uuid\\\":\\\"6EC2446F-7E89-4127-B3E6-5C05E6BECBA7\\\",\\\"nested\\\":[{\\\"name\\\":\\\"foo\\\",\\\"amount\\\":12345}],\\\"uint64Value\\\":\\\"18446744073709551615\\\",\\\"repeatedStringValue\\\":[],\\\"oneofString\\\":\\\"bar\\\",\\\"mapValue\\\":{\\\"a\\\":\\\"ONE\\\",\\\"b\\\":\\\"ZERO\\\"},\\\"mappedStringValue\\\":{},\\\"mappedNestedValue\\\":{},\\\"timestampValue\\\":\\\"1970-01-01T00:00:00Z\\\",\\\"repeatedEnumValue\\\":[]}\", \"ONE\") = true; want false\nIf I had to guess, I think you might need to add these new types to to the test as well. Other than that, looks good!. Looks like we still need some regeneration of files.. I think the first failure should be fixed with a regeneration of the protofiles (I guess it's sha-dependent?). I admit I'm mostly ignorant of the build process. @achew22 @tmc could you please help?. So it seems we have a mismatch of versions between that pinned by Bazel and that fetched in the normal test runs. Presumably this means we cannot get a successful build unless we change one of the two. I'm not comfortable with making the call here but I would assume we'd be more keen to pin versions to releases, so assuming that, how do we pin the non-Bazel tests? This might well not be fixable until something like https://github.com/grpc-ecosystem/grpc-gateway/issues/689 is done.. @jriecken It looks to me like the Bazel rules used are v0.10.3: https://github.com/grpc-ecosystem/grpc-gateway/blob/5948c50af05b7b4a4417c40b6768ffedd3b866dd/WORKSPACE#L5. I'm working on a PR which will add parity between Bazel pins and Gopkg.toml.. Release v0.10.3 of rules_go uses golang/protobuf v1.0.0: https://github.com/bazelbuild/rules_go/blob/f676870c5caf8df559a51e7aa005d2ece148a03b/go/private/repositories.bzl#L72. https://github.com/grpc-ecosystem/grpc-gateway/pull/696 should hopefully fix these dependency inconsistencies. Then this PR should be easy to merge.. @jriecken could you rebase on master please? The fix for #696 has been merged. Great, this LGTM!. That should absolutely work, I can't see why that is erroring. Could you raise an issue please?. The build errors imply files have been generated with protoc-gen-go 1.1, but Bazel dependencies are using golang/protobuf v1.0.0: https://github.com/grpc-ecosystem/grpc-gateway/pull/695#issuecomment-403295395.\nI think we need to update the rules_go version we're using to something more recent, and at the same time update the locked dependency versions in Gopkg.toml.. Bump. I think we need to get this in. @achew22 @tmc @yugui . This is being worked on in #671 . How is your request different from that being implemented in #671? I apologize if I've misunderstood.. Good spot, this'll have to be a separate PR then. Sorry for the misunderstanding.. I'm going to close this issue since it's not really related to features or bugs of this library. However, having done this kind of testing myself, here are some pointers:\n\nRun your gRPC-Gateway and gRPC server in a goroutine. Maintain a reference to the server so you can call srv.GracefulClose in the main goroutine when your tests are done.\nJust use http.Get, http.Post etc. Pretend it's a real webserver. Compare responses with expected JSON.. @srikrsna Please rebase this, we had some CI instability during the time you submitted this. Looks like a good change to me!. Sounds like there's work that could be done here.. Reopening on the request of @ch3rub1m . @ch3rub1m could you push your changes again please?. This LGTM, thanks for your contribution!. Thanks for this PR @askurydzin, this is really cool and clever! However, is this a breaking change? Should we consider this for v2?. @askurydzin Terribly sorry, looks like we need another rebase and regenerating of the files :(. I'll get it merged as soon as CI passes again.. Please rebase on master for new CI functionality.. Personally this seems like breaking a bit too far away from the underlying protobuf definition. It throws backwards compatibility to the wind, and I can't even begin to think of all the corner cases this is going to trigger. Given that this is a field property, have we considered how it might interact with query strings, partial path fields etc? It seems like a can of worms to me. What if you specify it twice? I understand the problem you're trying to solve but I'd be more inclined for us to have a special message type for this kind of thing. Maybe message grpc-gateway.Wrapper{ repeated google.protobuf.Struct element = 1; }? This we could add special cases for in the query parser and any JSPB implementations without having to extend the field descriptor. It'd be a bit like the WKT wrapper types.. > How? This can be a 'considered harmful' compatibility hack to be able to implement certain pre-existing APIs.\n\nFair enough\n\nHow? If you don't specify the new option (or, if upstream agrees to it, whatever-we-end-up-calling reply_body) then you get the current behavior. No changes.\n\nI don't mean backwards compatibility with the current gateway, just that if you have a service as you suggest and add a new field to your response message, that's impossible? It's breaking backwards compatibility guarantees given by protobuf. Adding new fields should always be possible, and for that to be true, there has to be a top level messages.\n\nBut, how do query strings and partial path fields apply if this only changes the response? No interaction with the request messages.\n\nHow do we tell the user this only applies to responses? Adding a field option that only applies in a subset of circumstances will need to obviously not apply in cases where it's not supposed to be used, or we might mislead users.\n\nI considered it; I think it can depend on what's doable in implementation stage. I'm currently personally interested in behavior of a top-level field on a top-level response message getting promoted as a response itself. I guess if you specify it on a repeated message-type field which contains a repeated message-type field where the option is also specified, we could try moving that one step up, too, allowing for an array of arrays?\n\nThis seems like a nightmare to implement, are we going to allow this nesting indefinitely? Are we going to just document the limits? What if you put it on a map? A oneof? An enum? We could document that it only works in very specific well-defined circumstances, but it feels a bit like a hack unless we can make it universally applicable.\n\nThis at the very least invades the realm of the gRPC API design; you have to change how your gRPC API looks, when in fact you only want to change the REST API. It means constructing the response becomes very unwieldy.\n\nThis is a fair point - but constructing messages for specific JSON structures is already what the official well known types encourage us to do. The whole point of the google.protobuf.Struct type (as far as I can tell) is to give predictable JSON rendering. Same for the ScalarWrapper types.\n\nAn option (or a field in google.api.HttpRule) still seems reasonable, non-intrusive and clear inside the .proto file?\n\nI just want to make sure we consider all potential use cases of such an option and document the limitations.\n. I apologize if my replies come off as anything other than constructive criticism.\nIsn't this actually possible to solve with .ListValue? Yes the method will be stupid to use for gRPC clients but you could expose two different methods, one wrapping the gRPC method and performing the conversion to and from the .ListValue and .Struct types. Is that too bad? There's always a cost to adding new features so it's not like it's a no brainer just because it can be done.. This is true, so is that the real question then? This feature would allow the generated OpenAPI type signature to show repeated types returned, as opposed to having a generic list type. In the light of that, should we not consider adding a swagger specific annotation instead? It would mean the types in the protobuf definition wouldn't match the signature of the function in the swagger API, but at the same time it would avoid us having to create an official message descriptor extension.\nWhat do you think?. @ivucica I can see you're being intentionally ambiguous, which is fine, but I'm interpreting this as you implying that this kind of thing exists-but-doesnt-exist-publically. That suggestion makes me a bit happier to accept this as it gives me some confidence it has been considered by a larger audience and corner cases dealt with, if any.\nSo we're back to a gateway specified field option, allowed only for one field per message, repeated or not, and only at the top level of any response hierarchy?\nOn a side note, that documentation is hilariously obviously not supposed to be public :joy:.\n\nUse this only for Scotty Requests.\n\nOh boy.. Attaching to an rpc sounds even better then. Will you try and look at a PR for this?. This sounds like the best outcome for everyone, nice!. If you set your marshaller to not EmitDefaults, and then return the zero value of that field, it will not be included in the returned JSON.\nThis begs the question though, what are you trying to do here?. @vtolstov No, I'm afraid you'll have to create a separate ServeMux with the other settings and register your service to that ServeMux instead.. Closing this as OP seems inactive.. Hi @izumin5210, this looks like a great addition! I'm a little bit confused by the build error, looks like the files need to be regenerated, but I don't think your change as anything to do with it. Could you update the commit message to contain the same kind of description that your pull request does please?. @achew22 @tmc what's going on with CI here?. That line is only installing golint, it shouldn't affect the version of protoc-gen-go.. No, I cant see how it would do that. I agree I figured itd be fixed by the dependency locking. I guess we'll have to do some more digging.. Looks like it's all working again, thanks for your contribution!. Ha, this seems to be exactly what @ivucica wanted in #707. Looking at https://github.com/googleapis/googleapis/blob/201d7be7f9da925df93bc52f8108963284f61aed/google/api/http.proto, this doesn't seem to be an official part of the http.proto file? Is this in the process of being merged? I don't think there's any chance of us supporting something that isn't part of the official spec.. @doroginin maybe https://github.com/grpc-ecosystem/grpc-gateway/blob/master/repositories.bzl#L31?. I'm not personally super familiar with Bazel so I don't know about that error, maybe @achew22 or @yugui can see what's left?. Please squash the commits into a single commit with a nice descriptive commit message as well, in preparation for merge.. @doroginin I think we just have to open an issue or submit a PR against https://github.com/bazelbuild/rules_go to have the version upgraded.. @doroginin See my comment on the Gopkg.toml changes. Can you also make a reference in the WORKSPACE file to Gopkg.toml?. This LGTM, but I'll leave it for @ivucica to merge as he's more involved than I with this. Thanks for your contribution @doroginin!. @doroginin Thanks for your patience, I've reached out to Ivan, we'll try and get this merged as soon as possible :).. :zap: Can we roll forward? I'd prefer to get the right version ASAP.. @bcdurden Fair enough, we'll sort this asynchronously.. @doroginin See https://github.com/grpc-ecosystem/grpc-gateway/issues/733. Thanks for raising this, some discussion has already taken place in https://github.com/grpc-ecosystem/grpc-gateway/pull/711. I think the next step to debug this is to try and run all the CI steps manually and see which one is causing the incorrectly generated files.. I think this was fixed by https://github.com/grpc-ecosystem/grpc-gateway/pull/715?. Looks to me like support for this was merged with https://github.com/grpc-ecosystem/grpc-gateway/pull/687. Go 1.6.2 is no longer supported. Please upgrade your Go version. See https://golang.org/doc/devel/release.html#policy.. I agree, but unfortunately we have to keep the old \"error\" field for backwards compatibility purposes. We'll want to remove it come 2.0.. I apologize, but you have confused me a bit. Your PR suggested that we should remove the message in 1.x, which is unacceptable as we have releases with it in. Are you now saying we definitely must keep the error field in 2.0 and remove the message again because error is declared as a response field by RFC6749 (Oath Framework)?. Doh!. Could you just include a link to the spec in the commit message and description as well? Good for future reference.. I'm afraid that information may be hard to come by, perhaps @achew22 will know?. I'm gonna need a lot more information to be able to help you. What does your protofile look like? What is the exact curl command you ran? It looks to me from that screenshot that you're just calling the wrong URL. A gRPC-Gateway would not reply with an error like this, (notice the Powered By Jetty:// error).. @vtolstov Just to add to what @achew22 already wrote, I think what this means is just implementing a http.Handler that looks at incoming requests and rewrites them such that they are in a format which is compatible with the normal gRPC-Gateway handlers, then wrap the gateway runtime mux with that handler before attaching it to a http.Server.. And the same for any replies, of course. It's called http Middleware: https://hackernoon.com/simple-http-middleware-with-go-79a4ad62889b.. This is not an issue with the library itself, so I will close this, though it contains some interesting information. Please feel free to reply in-line again, or make a case for keeping this open :).. This change does not introduce any version changes, it just relaxes the versions that downstream dependants can use to 1.x and 1.x for golang/protobuf and grpc.. It's possible to choose your own marshaller already with the runtime.WithMarshalerOption option: https://godoc.org/github.com/grpc-ecosystem/grpc-gateway/runtime#WithMarshalerOption.. Do you want the ability to control the marshaller per-method? You could do this kind of thing with two different gRPC-Gateway servers on the same http.Server if you split traffic in a separate handler. Would that work for you?. Indeed, it seems Consumes is hardcoded. Do you want to contribute a fix for this? It looks like it should be rather straightforward.. Yes :). If you put up a PR we can review it and help you create some tests for it!. mTLS between the grpc-gateway client and the gRPC server should absolutely be possible. You have to turn off client authentication for your gateway though, unless you expect those clients to also present a certificate. https://github.com/gogo/grpc-example/blob/master/main.go shows an example of how to do server auth, and adding client auth on top shouldn't be much work.. I've written about gRPC client authentication which might be useful: https://jbrandhorst.com/post/grpc-auth/. If curl does the right thing and requests and guzzle doesn't, it's probably up to how those have been configured, right? I bet you can get the same reponse data from both of those clients. This is probably not the best place to ask for help with configuring your python of PHP client unfortunately, so I will close this issue and direct you to the requests documentation.. Think this was closed by mistake. The files in this repo are manually formatted, but a formatting tool that solves this much better than clang is prototool.\nI'll close this issue as it's largely unrelated to the gateway, but feel free to reopen if you have more questions.. As I said in my previous comment, using prototool --no-rewrite format works well for me.. @ivucica LGTM. @ivucica I got in touch with @jadekler, he says this should be fixed?. Looks like we can close this?. Can you retry this? We think this might be fixed already. See https://github.com/grpc-ecosystem/grpc-gateway/pull/730 and https://github.com/grpc-ecosystem/grpc-gateway/pull/712#issuecomment-413899154. Also the backup fix for this is just to merge #730 . It should definitely be fixed. What exact error are you getting? You may need to update other repos in your gopath. Yeah we're working on go.mod support, we're not quite there yet. Could you open an issue for go mod support?. What does that mean? Do we need to update some Bazel dependency versions? Why is CI passing :thinking:?. If you're getting this error, make sure you are using the latest available version of the http.proto file.. I the best you can do is create something like a slice of register functions and then call the function once for each entry, and it's not really worth it at that point since you have to list all of them explicitly anyway.\nI don't think there's anything we can do to make this easier without creating a lot of ugly new API unfortunately, so I will close this issue. If you disagree, please feel free to reply and we can discuss other solutions.. I'm going to cut a new release later today, so #671 will have to wait until the next release.. Version v1.5.0 released :tada: . Streaming responses use something like this already:\nSuccess:\njson\n{\n    \"result\": <stuff>\n}\nFailure:\njson\n{\n    \"error\": \"grpc-error-string\"\n}\nSee https://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/handler.go#L181.\nHowever, I think the way you would solve this is to design your proto messages to output in this format, and then implement a custom HTTPError for when you have errors. It's not particularly ergonomic, but it should work.\n. I think if you want to wrap the entire response you're better just wrapping the ServeMux with a custom handler middleware for rewriting responses. The streaming response isn't configurable as far as I know.. Is this possible with a custom ResponseForwarder?. This kind of thing has been discussed before at length: https://github.com/grpc-ecosystem/grpc-gateway/issues/438. Is this a fundamentally different issue? I will pre-emptively close this, but please let me know if I'm wrong.. I think this should be a reasonably easy first PR for anyone wanting to do it - the CloseNotifier code is only there for when the request context isn't available. Feel free to submit a PR with it removed :).. Yeah, we can safely remove that whole if-clause I think.. @maros7 before you do anything, I suggest waiting for #777 to be merged, it should be imminently. It will remove any CI issues we had.. I intend to bump all the open PRs once that is in master. This is a great first issue for anyone looking for an easy contribution!. LGTM, thanks for your contribution!. This LGTM, @achew22, @ivucica want to have a look before I merge?. Thanks for your contribution!. Hi @robbert229, thanks for your contribution! It looks like you need to regenerate the example protofile. Could you do that please and we'll get this shipped?. Yes, make examples, but you need to ensure you're using the same version of protoc and protoc-gen-go when running it. Consult the Travis config and Gopkg.toml.. Could you please rebase on master and regenerate again? Looks like there's a merge issue.. GitHub is still telling me there's a merge issue :thinking:. Looks like there's a diff in some generated files: https://travis-ci.org/grpc-ecosystem/grpc-gateway/jobs/423385162. Could you please try again with the protoc-gen-go version locked in Gopkg.toml?. I've restarted the jobs. Sigh, not sure why this is so flaky recently. The last error looks like another generation error, could you take another look?. I think its a protoc-gen-go or protoc version mismatch. Make sure you've got the same versions used by .Travis.yml. Please rebase on master for new CI functionality.. @mestrade Why don't you open a PR and we can discuss it in there :).. No worries @mestrade, I think you may benefit from reading the github contribution guidelines: https://help.github.com/articles/creating-a-pull-request-from-a-fork/. You need to create a fork first, you cannot commit directly to this repo.. Work on this is being completed in #739 . Path parameters largely just need to be compatible with the http.proto HttpRule definition: https://github.com/googleapis/api-common-protos/blob/master/google/api/http.proto#L45. If your proposal can be described using that, I don't see why not :).. I reran the jobs as well and there seems to be one error: https://travis-ci.org/grpc-ecosystem/grpc-gateway/jobs/422808821.\nEDIT: That build error really doesn't look like it's your fault... let me investigate a bit.. Update on the build failure: I think it must be something with your generation still, I reran master to see if it was failing on there but it passed: https://travis-ci.org/grpc-ecosystem/grpc-gateway/builds/422220357 (unrelated failure).\nMake sure you generate the files with the same versions used to generate the files previously. You can run dep ensure --vendor-only and go install ./vendor/github.com/golang/protobuf/protoc-gen-go etc to get the correct versions.. Master has been fixed, please rebase and let's hope the tests pass.. Thanks for your contribution!. I think you need a = in there:\n--grpc-gateway_out=: echoserver/echoserver.proto. We need the full protoc command to see where the problem is. Your last example still doesn't have the = in it. Can you post your full protoc command?. Thanks, there's clearly been a misunderstanding here since you're using $GOPATH/bin/protoc-gen-grpc-web. I don't know where you got this from, but it's not used with the grpc-gateway. What happens if you run:\nbash\n$ protoc -I /usr/local/include -I. -I $GOPATH/src -I $GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis --grpc-gateway_out=logtostderr=true:. echoserver/echoserver.proto. You're getting that error because you're not running protoc-gen-grpc-gateway. Run\nbash\n$ go get -u github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway\nAnd then run the command I told you to run again.. Where did you get protoc-gen-grpc-web from? It has nothing to do with this repository I'm afraid. That is the whole source of your trouble. I'm closing this as it seems to be unrelated to the gateway, but please tell me if you want me to reopen it again.. If you're getting that message even after installing protoc-gen-grpc-gateway with go get, you may need to just add $GOBIN to your path. This is definitely within the realms of an environmental problem though, and I don't think this is the best place to help you debug it. Maybe join Gophers Slack and ask for help on there?\nhttps://invite.slack.golangbridge.org/. Hi @vtolstov. Your issue confuses me a bit. Could you try to reword it as a feature request, so I can understand what the problem with your current setup is? Field masks do not require or encourage the use of enums to select fields. Field masks are just strings.. Closing this as issue is a bit unclear. I will reopen if you provide more details.. Thanks for raising this issue @princejha95. I'm guessing there's either some variable being overwritten incorrectly or an index not being used here. I predict that if we add a third service with the same method name it will also give that the comment from the first service.\nWe'd be happy to review a PR to fix this. I think the first place to look would be https://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-swagger/main.go.. Thanks for your issue report. Could you include an example protofile that reproduces the issue?. Is it possible the request object isn't in the file as you expected because its a path parameter? What does the generated swagger file look like? What did you expect it to look like?. It doesn't create a type definition unless it needs to. This get request has only one parameter, and it's populated in the path (i.e. by calling yourapiserver.com/Cooldudes/1337). If you look for the generated definition of the method, rather than the request, you'll find that it has defined all parameters required by the request. I think this is a simple misunderstanding and not a bug, so I will close it. Feel free to reply again if there's anything unclear.. Yes, that is indeed intentional. Why would they?. It's when there is no need to provide any JSON input that it doesn't get generated.. No, the path and query parameters are defined in the method definition, as per the OpenAPI v2 standard. Your swagger generator should have no problems generating appropriate request objects.. Which generator is this, out of curiosity? I've used different generators with the generated swagger files without problems. Make sure it's a v2 generator though.. You need to URL encode your request.. This is a duplicate of #556, right? Is this working on master?. Sorry, I mean #566. Yes, we will be making a new release shortly :). See #733. Glad it's working now.. I have restarted the failed job. Thanks for your PR! You'll need to regenerate the examples with make examples. Make sure you've installed the correct version of protoc and protoc-gen-go.. At least one of the generation jobs failed, so it looks like you're not using the correct versions of protoc and protoc-gen-go to generate the examples. You'll need protoc version 3.1.0 and protoc-gen-go version 1.1.0.. @maros7 if you want to commandeer this please open a new PR rebased on master and I will close this one. @srikrsna if you want to finish this off, please rebase on master and follow the new regeneration instructions.. See https://github.com/grpc-ecosystem/grpc-gateway/blob/master/CONTRIBUTING.md#i-want-to-regenerate-the-files-after-making-changes. Thanks for raising this issue. There are a couple of known shortcomings with the translation from query parameters to proto request messages, as you noticed. I don't think anyone is working on a fix for this right now and the recommendation is usually to use a POST where you need this kind of information.\nOf course, we'll be happy to help you get a PR in that would resolve this issue. You've got a good test case here, so you might be able to fix this :). What do you say?. @waveywaves of course! Thanks!. Doesn't seem like I can assign it to you, but consider yourself the owner.. Thanks for raising this issue! I like this idea. I think the problem is specific to path and query parameters like this one. Can you confirm whether this works when using a POST where the ID is in the body?\nI'm not sure how we would tell the query/path resolver where to look for decoding. Any ideas?. Is there anything I can do to help you get a PR for this going?. I don't know where you read that, go modules have their own lock file esque go.mod and go.sum at the root. There's even a way to convert from dep files to go modules files.. I don't think we should need multiple modules, we'll definitely try to have a single root module and only resort to multiple if we absolutely need to. What error did you get?. The resty dependency issue has been discussed before, are you sure it's an issue with their repo and not just us relying on an old version of go-swagger? If there definitely is an issue with resty, obviously please raise it with them.. See https://github.com/grpc-ecosystem/grpc-gateway/issues/254 for more discussion on using go-swagger.. I don't think replacing swagger codegen is a hard requirement for the modules work. Please attempt a switch and let me know if you run into any problems, as has been stated we're keen the get this done.. Something we do want to consider at the same time though is looking into creating the Bazel dependency versions from the go.sum file; @drigz @achew22 ought to be able to assist in that effort.. Might be relevant: https://www.youtube.com/watch?v=ms5l0zxC-uM. Resty is known to cause problems, I think we'd rather replace that generator than mess with some post processing step. Do you not think this would be solved by implementing modules in this repo? I don't understand the module system myself yet but we want to move this repo to use modules ASAP for other reasons as well.. Couldn't it be remedied by this repository explicitly choosing the resty version to use?. I don't think we can declare proto file or protoc dependencies with go modules.. Modules don't mean you can't have a GOPATH, but I don't think it's necessary here anyway. We can assume a vendor directory for these instructions for example.. This issue is concerned with changing our dependency management to use dep, which would presumably include documentation updates, but is not exclusive to documentation updates. We'd very much welcome a PR that would do both, though.. Thanks for raising this issue! I think this is actually just an incompatibility between Swagger and the Gateways expectations. The gateway generates a correct swagger file and there's no way that I know to tell swagger to put these parameters in separate query parameters.\nIf you know of a different solution to this, please let me know, but I think this might just be a matter of documenting the problem. I think the gateway is doing the sensible thing here and Swagger is generating a nonsensical query.. Having looked into it, I think we might be able to fix this by emitting a collectionFormat value set to multi: https://swagger.io/docs/specification/2-0/describing-parameters/. Would you like to contribute this?. I think we'd add a new field to the schema type, collectionFormat, and that would only be set if we were working with an array of query parameters. So yes, in that function, but more specifically in schemaOfField I think.. @litichevskiydv I don't personally have the time to implement this, but I think mine and @veqryn's previous discussion is good enough for anyone with the determination to get a pull request started, with or without Go experience! What can I do to help you bring this contribution in?. Reopening due to #906.. @bmperrea sorry we didn't catch the bug (#906) the first time around, would you be interested in trying again in a new PR? The previous PR should be a good starting point, but we'll need to ensure we handle the case where there are repeated fields in the body too.. @kasuboski Thanks for your contribution! I think we want to remove the old Gopkg.toml and Gopkg.lock files at the same time. Also we may be able to get rid of some more files, @achew22 I think you had some input here.. Please rebase on master for new CI functionality.. @Everlag please open a separate PR, it seems like @kasuboski has abandoned this.. Closing as this appears abandoned. Hi @Tommy-42. It looks like something terrible has happened here. Are all these conflicts and changes really right?. Yeah I don't know how you ended up in this state but that sounds about right.. maybe try a poor mans rebase and just copy the changes you think it needs from the branch into a fresh branch? Don't worry about the generated changes since we can just regenerate the files.. Argh, thanks for raising this issue! Terribly sorry to have broken your workflow. It looks like we have to revert https://github.com/grpc-ecosystem/grpc-gateway/pull/708, is that correct?. Yeah, backwards compatibility is paramount here, of course. It also seems like the wrong thing to support this tbh, I'm just disappointed we didn't discover it earlier and didn't break users. I'll see what some of the others think but we'll likely just revert #708 and continue the discussion in #224.. This may still be interesting, but is of much less significance now that we have stable generation of files in a one-line command.. LGTM.. Superceded by #777 . Yes, I will rebase on that.. Yeah I don't think this is possible, the proto3 map is just syntactic sugar for:\n```protobuf\nmessage map {\n    string key = 1;\n    string value = 2;\n}\nmessage ServiceConfig {\n    repeated map config = 1;\n}\n```\nSo that's why you get this cryptic error message. I think technically there's no reason this couldn't work, but clearly it's not currently supported by the gateway. What can I do to help you get a pull request to implement this in?. Thanks for submitting this issue @johnchildren! This does indeed seem to be in breach of the spec. What can I do to help you get a pull request in to fix this? I think it should be okay to introduce this backwards-incompatibility since any comments or explicit description will override it. How about just A successful response?. Closed with #767 . @ivucica would like your opinion on this if possible.. You needn't really bother with tests, it should be good enough that the generated files now have a description set somewhere where it wasn't before.. We have to keep both for backwards compatibility; we may remove the \"error\" field in 2.0, but we cannot do it without breaking existing users for now.. No, it's not meaningless, the gateway is tightly coupled with gRPC and protobuf, where the googleapis Status type is the correct way to return errors. The gateway added the message field to be more consistent with this practice.. Not currently as far as I know. The runtime.ServeMux implements http.Handler though, so you can use your own router and only use the ServeMux to handle a subset of routes.. I don't think we can merge this as is - dep will still add the old dependency to the lockfile since some of the checked in files import it. We need this to be solved in the generator first (or run a post-generate script to replace the import ourselves...). Have you raised an issue with swagger-codegen?. Did you regenerate these files or just search replace?. I think we're more likely just to drop this dependency than update the version. I reiterate though - this will only happen after #772 has been merged.. I'm gonna close this, lets resume the discussion in #254 . Curious, this definitely should be working, what does the JSON that it's trying to unmarshal look like?. What does the JSON input look like?. Yeah so you haven't specified the type URL in the JSON input. Please see documentation for the google.protobuf.Any type to see how to use it with JSON. This is not a bug.. https://github.com/protocolbuffers/protobuf/blob/master/src/google/protobuf/any.proto#L94. Reminder for myself to replace the Travis badge in README.md with a CircleCI badge.. Hi @ZachEddy, thanks for your contribution! This is a great step in the right direction, but I think we're coupling ourselves a little unnecessarily to OpenTracing here. If we want to add an option to the gateway I think we want to expose it via an implicit interface that will work with OpenTracing and other providers (ZipKin, OpenCensus, whatever else there is). Do you have any thoughts on how we can accomplish that?. Please rebase on master for new CI functionality.. Hi @srenatus, unfortunately our CI is a little bit in flux atm so I don't expect this travis job to run. Do you have a rationale for this change?. This is a wholesome PR if I've ever seen one. As I mentioned, this probably won't run just now but leave it in and we'll merge it when we can.. Please rebase on master for new CI functionality.. LGTM. Hi @ti, while I appreciate that the duplication is ugly, I don't think removing the message that we added there for consistency with the googleapis.Status type is the right way to go. We cannot accept this right now for backwards compatbility reasons, but we are also more likely to just remove error with version 2.0.. What do you mean? We added that for the reasons listed in the pull request.. I got a 403 when trying to follow the repo so I assume it's just a matter of permissions. Assign a custom error handler to runtime.HTTPError if you don't like the current default. This is already configurable.. The envoy grpc gateway is not related to this project, as far as I know, you will have to ask them.. Personally, yeah, I think mapping the gRPC server response to a JSON marshalling of the google.rpc.Status type makes a lot of sense. I think that should be the default in 2.0, but it's obviously up for discussion. As for what we can do right now, turning it off with a flag is fine as well, but we can't change the default behaviour, and if you absolutely must change this, it's already possible with a custom HTTPError method, so I don't think we need a flag at all.. Base64 encode the data in a string type maybe?. You'd have to encode it in the client anyway, you can't send raw bytes in a URL. If it's a uuid though, maybe just send the string representation of the uuid?. If it's a path parameter, it has to be URL-encoded or base64-encoded and stored in a string. If it's in the body of the message, I guess you may still want to base64 encode it.. If you add it as a path param you don't need to add it to the body of the message. If existing clients are already sending the UUID in the path, what format is it using, since it can't be bytes?. Ah I see. In that case, maybe just make the uuid a parameter in the body instead? So you can reuse the same types. If that doesn't work, you will need to add a uuid_str and manually handle it in the service.. I frankly don't know, I'm afraid. Try it :man_shrugging:.. I think you have to define two different methods:\n```\nservice MyServer {\n    rpc MyMethodV1(MyMethodV1Request) returns (MyMethodResponse) {\n        option (google.api.http) = {\n            post: \"/api/v1/my_method\"\n        };\n    }\n    rpc MyMethodV2(MyMethodV2Request) returns (MyMethodResponse) {\n        option (google.api.http) = {\n            post: \"/api/v2/my_method\"\n        };\n    }\n}\nmessage MyMethodV1Request {\n    string a = 1;\n    string b = 1;\n    string c = 1;\n}\nmessage MyMethodV2Request {\n    string a = 1;\n    string b = 1;\n}\n```\nThough I'm unsure what you mean by the capilisation or and extra variables? Is this what you wanted?. Hi @joelurraco, this does indeed sound problematic. The fix seems simple enough, do you think you could contribute a fix and a new test case?. Fixed with #784 . @joelurraco it seems you've committed with an email/username that is different from the one you're using to open this PR.. Not much I can do about the Google bot I'm afraid - would you mind force pushing a new commit or opening a new PR with the other account?. Superceded by #784 . Yes, great, thanks for your contribution!. Personally the gateway seems the wrong place to add interceptors. It performs translation, not business logic. All of this belongs in the gRPC service it's talking to.. You'll need to regenerate the files as well before we can merge this change, please see https://github.com/grpc-ecosystem/grpc-gateway/blob/master/CONTRIBUTING.md#i-want-to-regenerate-the-files-after-making-changes. Interesting, the CLA bot doesn't like us sharing commits.. OK @mestrade can you please squash the commits into on commit made by you and force push it to this branch and then we can get it in. Remember you need to regenerate as well.. > You'll need to regenerate the files as well before we can merge this change, please see https://github.com/grpc-ecosystem/grpc-gateway/blob/master/CONTRIBUTING.md#i-want-to-regenerate-the-files-after-making-changes\n. Not sure about the build error, but on second thought, could you add the changes made in https://github.com/grpc-ecosystem/grpc-gateway/pull/739/files as well? So users can manually overwrite the operationID? This part:\ngo\nif opts.OperationId != \"\" {\n    operationObject.OperationID = opts.OperationId\n    if bIdx > 0 {\n        operationObject.OperationID = fmt.Sprintf(\"%s%d\", opts.OperationId, bIdx + 1)\n    }\n}. I think the tests are failing because the generated types have changed so we need to also change the integration tests: https://github.com/grpc-ecosystem/grpc-gateway/blob/master/examples/integration/integration_test.go. I'm also not sure if we consider this to be a backwards compatibility breaking change since we're changing the swagger output? Maybe we should do it and see if someone complains :grimacing:.. There's another PR open for this which adds the package name to deduplicate methods, see #. I'm confused, what were you trying to do? You shouldn't need to run make at all. All you need is to go get the binary. Which tutorial are you referring to?. Yeah we really need to update these instructions, they are ancient. You don't need to build protoc anymore, v3 is out of beta for... 2 years :joy:.. How would you feel about contributing some updated instructions? To get you started, you can download protoc from https://github.com/protocolbuffers/protobuf/releases. I'd be really valuable to have your thoughts as you learn to use it for the first time.. Great, I look forward to reviewing your PR :).. This only adds things to the existing context, see the signature of runtime.AnnotateContext: https://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/context.go#L59. Hi @vaishalig2693. This isn't really an issue so I will try to answer your question and then close it and redirect you to the channels we recommend in the issue template.\nIt's possible to expose both gRPC and REST on the same server, or even the same port. See https://coreos.com/blog/grpc-protobufs-swagger.html for an example. I'm not sure exactly what you're asking though. Can you direct your further questions to the #grpc-gateway channel on gopher slack?. See https://github.com/grpc-ecosystem/grpc-gateway/blob/master/ISSUE_TEMPLATE.md#i-still-have-a-problem.. Hi @jgiles, thanks for the suggestion! I don't see why not, but we don't have a way to create them automatically in CI so they'd just be binaries uploaded by the maintainer who makes the release. I think the assumption thus far has been that you have go installed and so can build the specific version you need. If you're using dep for example you can specify protoc-gen-grpc-gateway as a required dependency and then install the binary from vendor. But, again, sure why not! Are you particularly interested in any specific releases? I can upload the last 5 or so for Linux, Windows and OSX trivially.. I've uploaded some pre-built binaries for v1.5.1 (the latest release): https://github.com/grpc-ecosystem/grpc-gateway/releases/tag/v1.5.1. Do you need any other release made?. That's sounds fantastic - I'll see what we can do!. I've added protoc-gen-swagger binaries as well and I'm investigating automating this for future releases, thanks for the heads up!. LGTM, thanks!. Hi @nicolasStevenin. I don't really understand this issue. The tests are passing in our CI tests. What is your concern? Are the tests failing on version 1.5.1? We need more information here.. I'm gonna close this as the original reporter hasn't clarified their issue.. Now I haven't the foggiest idea how we're going to be able to test this unfortunately - any ideas?. It's based on https://circleci.com/blog/publishing-to-github-releases-via-circleci/. I didn't expect the release builds to run, perhaps we should only run those on tags as well?. Disabled the release builds for non-tags. So I think the only way to really test this is to make a new release :tada: . This is because you need to generate a PHP version of the swagger annotations proto file. We're not going to maintain a selection of pre generated files for all supported languages I'm afraid. It should be simple to do it manually. Does that makes sense?. > > I'm not sure how, but it seems like we've ended up with some travis files added in this PR.\n\nThese were auto-generated by the docker image used to generate changes, as advised by the CONTRIBUTING.md (jfbrandhorst/grpc-gateway-build-env). Docker image inspect shows the following relevant details:\n\"Id\": \"sha256:2011172b884689f4c0f4dcbb50d408abb1acccbcd002e0a054e0492dbcce94a8\",\n    \"RepoTags\": [\n        \"jfbrandhorst/grpc-gateway-build-env:latest\"\n    ],\n    \"RepoDigests\": [\n        \"jfbrandhorst/grpc-gateway-build-env@sha256:81527758368154998c24d310501e92f8b805987ee164d7050b7be6d6b4deffea\"\n    ],\n    \"Created\": \"2018-10-09T21:58:26.512757165Z\",\nI'm more than happy to remove those manually, but I suspect they'll reappear for others following the provided instructions.\n\nThat's very curious because the regeneration script should be running in CI and checking for diffs. It looks like swagger-codegen is outputting travis files, as evident by this log: https://circleci.com/gh/grpc-ecosystem/grpc-gateway/144. How is it not failing the git diff --exit-code?. Lets raise an issue about the swagger-codegen stuff and remove it from this PR diff please. We can investigate asynchronously - I know for sure we don't want it in the repo though.. LGTM, thanks!. Looks like we need to regenerate the protofile as well - not sure where the hash comes from but there it is. https://github.com/grpc-ecosystem/grpc-gateway/blob/master/CONTRIBUTING.md#i-want-to-regenerate-the-files-after-making-changes. I can force push a change, leave it to me.. Files regenerated.. If we're doing this, I'd also be tempted to add a formatter step to CI. Would you be interested in adding a CI step to this PR that runs the buildifier and reports transgressions?. This PR please.. Please revise the CI tests to make sense - I only reproduced the existing travis tests to the best of my ability when creating the circle CI configuration.. LGTM, but I'll let @achew22 have the final say.. Thanks for the contribution! Do you want to merge this before or after #797?. I'll leave this to @achew22 now that he's shown signs of interest, but no objections on ordering to me.. I don't know about the Schema vs JSONSchema for fields (@ivucica might know) but as for adding an example to the example files, that should be pretty straight forward I hope? It's the easiest way to show the users how to specify these things in their own files, so it should be comprehensive (as you say, include some nested type examples). If you want to fork this and take over please feel free @srenatus. As discussed, we're happy to merge this as is and add tests separately. Thanks for your contribution @birdayz!. Sounds like at the very least we need some tests to establish the expected behaviour. If they fail, we'll have to implement some fixes. Would you be interested on adding these tests?. Deleting files implies that not all generators ran. Curious findings indeed. Generally the way to test generated files like this is to compare against a \"golden\", known correct file. So it wouldn't be a \"go test\" kind of test, it'd be a shell scripting kind of test.. Yes, but the examples didn't pick up the change in well known types. Maybe just adding a message using all the well known types is enough?. @birdayz fantastic work, really appreciate the effort you're putting in right now. Is there anything else you want to add to this PR before we review it?. Right, I think we should just try and add a step where we generate some swagger files from the wrappers.proto as well. The generated pb.go now contains the types but I can't see a swagger file?. Ok, if you want we can just resolve that in this PR and raise an issue about making sure that the generated swagger files are correctly formatted for all the well known types. I think it ought to be part of this PR to ensure that the bool and empty types are correctly formatted though. Is that already covered with existing examples? Otherwise lets just add swagger generated file of the wrappers.proto example.. Right the thing that's still confusing me is that this PR adds a number of methods to wrappers.proto in the examples, but there are no generated swagger file changes, only pb.go changes. Is that correct? Should we not generate some swagger content for these new methods too?. Not generating the types as the proto types is all well and good, but we're talking about 10ish new RPCs and no corresponding methods added to the swagger files at all. I'm wondering if it's simply because you didn't add the google.api.http option to the RPCs? I would still expect the methods to be reflected in a swagger files.. Great, again, thanks for your hard work here! Really appreciated.. Well slightly worryingly the string well known type is not showing up as the native string, is that not wrong?. Also why not add all the others as well?. I think ideally a google.protobuf.StringValue type would render as a string in swagger, without the message definition. I thought that was what these type exceptions were for.\nIn any case, can we add google.api.http options for the other RPCs as well please and check those renderings too?\nDon't hesitate to push back, I'm happy to resolve things in separate PRs if you think its going out of scope.. @birdayz agreed, let's discuss the faulty behaviour separately and make a decision on what to do next. Great digging!. I apologize for this falling between the chairs, I'll give @ivucica another few days but if I hear nothing I'll just merge it. Thanks for the bump!. It looks like you'll need to rebase again though, could you do that please?. Thanks for your contribution!. What's next to do here? I've lost sight a bit. Do you need anything from us?. If the behavior has been incorrect I think we can safely correct it without worrying about backwards compatibility. Is that simply a matter of copying the fix from the protobuf repo?. Regenerating everything should be covered in https://github.com/grpc-ecosystem/grpc-gateway/blob/master/CONTRIBUTING.md#i-want-to-regenerate-the-files-after-making-changes.\nIf fixing the tests is a pain, lets raise an issue and solve it in a separate PR. Is it necessarily part of this cleanup? Don't want to waste your time with red tape.. Right I see what happened, we updated our dependency of golang/protobuf, and all of a sudden incorrect behaviour that we depended on was fixed, and now our tests fail. It seems we have to fix it in this PR then. I'm not too familiar with this code unfortunately, so I think we just have to do a little digging to get to the bottom of it. Changing the test is correct, so any part of our json marshaller that is doing the wrong thing is what needs to be fixed now.\nIs that a correct summary?. Node tests are failing with:\nExpected $.map_value.a = 'ONE' to equal 1.\nI wonder if it's just a matter of fixing these tests too?. :crossed_fingers: . This has become bigger than initially anticipated I think, but lets wait for @achew22 to have a chance to review it as well before merging. Huge thanks for getting the ball rolling on this though @drigz :).. Node tests are looking good!. Parsing the request body in the server and acting on the contents is in violation the HTTP/1.1 spec: https://tools.ietf.org/html/rfc2616#section-4.3. Are you sure that's what you want?\nRef: https://stackoverflow.com/questions/978061/http-get-with-request-body. What for supports GET with a body?. Sorry, what frameworks support GET with a body? I have always seen it referenced to as a violation of the spec. Your design should account for this and take the request id as a query parameter. I don't think you can assume any HTTP servers will support GET with a body.. It's definitely possible to set a header response in the gRPC server and have it sent out, but you need to share more of your code for us to see where the error is. Maybe you could join the slack channel #grpc-gateway on gophers Slack and we can help you in there?. Closing as it was agreed out-of-band that this wasn't necessary.. I'm confused, are you saying that parameters provided as query parameters should be parsed when body: \"*\" is set? I'm not sure it's a correct interpretation of the rule. My interpretation is that any parameters not part of the path should be in the request body. Do you have a practical example of this instead of a hypothetical interpretation?. Leaving aside the point about having a body in a GET, what does your http.proto annotations look like for this RPC?. So judging by your annotations I think the wording in http.proto implies that the parameters should go in the body. You can't expect it to read it from query parameters and the body when you use \"*\". Have you tried just using body: \"request_id\"?\nAlso; you should really use snake_case for proto file field names. https://developers.google.com/protocol-buffers/docs/style. What does that error look like?. I'm wondering if that's a bug... it should be possible to have the specific parameter parsed from the body. Can you see why this is broken like this? You can't use * and expect query parameters to work.. I think the premise of this PR is a flawed interpretation of http.proto, as I discussed in https://github.com/grpc-ecosystem/grpc-gateway/issues/234#issuecomment-438004031. Closing since the current behaviour is correct.. @razamiDev Thanks a lot for picking this up. It's very hotly anticipated feature among the maintainers :). Looks like we've got some bazel failures, something about the field_mask.proto file not being found. @achew22 @drigz maybe you can help here? I think it's a new dependency for the test files.. @drigz I definitely would like to see a docker-based solution, and I think we should be able to put together a oneliner with the docker image we're using in circleCI.. Nice job @drigz, lets see if we can get that into the contribution guidelines with #807?. So it looks like your merged master into this branch instead of rebasing on top of master. This meant this branch now contains commits that weren't made by you, and it's confusing the Google bot. I can't overrule the Google bot, do the easiest thing for you is to try and rebase on master again (not merge!). If you can't do that, the next easiest is probably to open a new PR in a new branch.. I've no idea why this is erroring, have you had a chance to do some debugging?. Huh, interesting, perhaps we just need to add the rest of the well known types there?. Yes, thanks a lot!. It appears the MR discussed #809 just generates invalid code. @mayank-dixit are you interested in submitting a PR which adds the functions?. Nevermind, still confused, continue discussion in #809 . Reopening as there's definitely a problem with the current code, see https://github.com/grpc-ecosystem/grpc-gateway/pull/809#discussion_r233921470. Still not fixed, reopening. This has been discussed before in https://github.com/grpc-ecosystem/grpc-gateway/issues/438, I don't think we will be changing this as it's fundamentally wrong when applied to JSON. See https://github.com/OAI/OpenAPI-Specification/issues/704 for more discussion on this, but as the gRPC-Gateway works with JSON for swagger, we will not be changing this.. Sigh it's clear to me that we need a little more time to work this out. I agree that your suggestion makes sense, can you submit a new PR? We need to add some examples in this area at the same time. I will revert my change to fix master.. Hi @razamiDev, thanks for putting this together. I just had one quick idea - do you think you could put together a couple of little examples of requests this translates and compile it into a helpful page for the documentation (/docs)? I'd love for us to make great features like this more visible to the users. What do you think?. This is fantastic @razamiDev, thank you so much! I can't wait to merge this. Lets see what CI thinks :).. Actually, just a quick thought, maybe add some cURL examples of each case as well?. Fantastic, let's merge this thing!. Congrats @razamiDev and @dmacthedestroyer on finally getting this in, we've all been very excited for this!. This was reverted because the function signatures weren't correct. We want to add this, but we need to add some tests and make sure it all plumbs together. Please feel free to submit a PR with the changes.. Thanks for your contribution!. Is that not validating based on the OpenAPI 3 spec? Remember we only support OpenAPI 2 (swagger 2.0).. Looks mostly to be a problem with the way we're rendering the responses, could you dig a little deeper and see how we're violating the spec exactly?. It looks like the generated files don't build, can you see if we can fix the signature of the new functions?. This is great, thanks! I will merge once the build has finished successfully :).. Oops, looks like the Bazel files are out of date: https://circleci.com/gh/grpc-ecosystem/grpc-gateway/736. Could you run the following command to update everything please?\nbash\n$ docker run -itv $(pwd):/grpc-gateway -w /grpc-gateway --entrypoint /bin/bash --rm \\\n    l.gcr.io/google/bazel -c 'bazel run :gazelle_fix; bazel run :buildifier'\nFrom https://github.com/grpc-ecosystem/grpc-gateway/blob/master/CONTRIBUTING.md#i-want-to-regenerate-the-files-after-making-changes.\n. On a sidenote: @drigz, how often will unrelated user PRs be failing like this? Is there some way we can just update these manually and not have it fail the build?. That bazel fix command is really impressive, wow! I'll merge this once the build finishes :).. Thank you for your contribution!. I don't know about users of the Bazel build, but if the gazelle_fix command updates Go dependencies in the background we risk running out of sync with our Gopkg.toml (if we haven't already), which would be unfortunate. Is that correct?\nI heard from @achew22 that bazel stuff will sometime soon be generated from go.mod files (or something to this effect) so we should be able to maintain a single source of truth (a new go.mod file). Until such time, I think I'd like this to be a manual step (option 2), if my understanding above is correct.. Ok, that's not so bad then. So is it correct that this might still be useful sometimes because user submitted changes may legitimately break something? To me it looks like:\n\nSometimes cause friction for contributors, but occasionally highlight genuine problems.\nSometimes cause friction for maintainers whenever we come to manually running :gazelle_fix.\n\nIs that a fair summary? If so, I don't think option 1 is so bad for now, if we find it's causing a lot of friction more often than we'd like we can move to option 2.. As it turned out the failure in #816 was correct - I was wrong about it being unrelated to the change.. Do we still need bazel for the :buildifier job? Is the buildifier necessary to put in the contribution notes? It would be nice if we could drop the bazel docker image in favor of just the go get.. If they're both linters I'm happy enough to always use tip tbh. Otherwise we can add it to Gopkg.toml and store the versions in Gopkg.lock: https://github.com/golang/dep/blob/master/docs/Gopkg.toml.md#required. Nah we could just put that in the contribution notes: dep ensure --vendor-only && go install ./vendor/github.com/bazelbuild/buildtools/buildifier. We're still only talking about the cases where CI breaks for contributors, it won't be very often hopefully.. This has turned out not to be such a big problem so I'm happy to close this.. I've restarted the job, strange error (404 when pulling container on public docker hub).. Hi @burov, thanks for your PR! Could you please reference the part of the http.proto spec that defines the meaning of the = operator in this case? I'm not familiar with this use.. Btw, the CLA bot is unhappy because your commit is using a different user than your github account seems to think. They must both match unfortunately.. Is this really a correct interpretation? I thought segments were used to capture paths into variable names. I don't think this produces correct output with /* paths.. Can explain why you opened this please?. If I understand you right, you're saying that if you have two methods defined with similar paths but different segments, the gateway generator handles it fine, but the swagger generator does not? If that's true, then I agree there seems to be a bug, but I'm not sure the current fix is appropriate. Can you start by adding a failing test case instead? Assume that the existing examples are correct, and changing them will break the generator. Can you do that?. Is there anything in particular you want to do that isn't already covered by using https://github.com/grpc-ecosystem/grpc-gateway/blob/7ad920cb70af14a60b14add071a4605a0655ea6e/runtime/mux.go#L41?. Ok, I will close this PR. Let me know if I misunderstood you.. Yeah so if you need complete control over what gets written to the ResponseWriter, you will probably need this change. Are you just trying to avoid the runtime from writing to the ResponseWriter after you ForwardResponse method has been called?. What messages are written by the runtime after the forward response method has been called?. This is only true if you don't version the generator together with the runtime library. If you're using dep it will mean adding it as a required dependency and installing from vendor:\nrequired = [\n    \"github.com/grpc-ecosystem/grpc-gateway-boilerplate/protoc-gen-grpc-gateway\"\n]\nsh\n$ go install ./vendor/github.com/grpc-ecosystem/grpc-gateway-boilerplate/protoc-gen-grpc-gateway. That said, we do need a release. I will close this though since it's not really an issue.. Think I've finally got this done T_T. Hi @javasgl, the reason that we're using specific revisions, which I know messes with dep a little bit, is because we rely on using both dep and bazel files and we want to make sure they both match, and unfortunately the bazel dependencies all use specific revisions. In the future we're planning on ditching both in favor of go modules files, but we haven't gotten around to that work yet (help wanted!).\nIn the meantime, the following override should fix your problem:\n[[override]]\n    name = \"github.com/golang/protobuf\"\n    version = \"1.1.0\" # Or whatever version you want\n.\nSorry for any inconvenience this is causing to our users.. Related to #755 . That would risk the dep file getting out of sync with the bazel dependencies, which is unacceptable. The best solution is to migrate this project to use go modules.. Closing as dupe of #755 . I don't think we should need to generate several files or worry about runtime errors - the generator should catch this at generation time.. Hm, well yeah if the generator doesn't have access to both files that'd be impossible, but I suppose you should really be specifying all proto files that are in the same namespace in one protoc invocation?. It is indeed expected behaviour, the JSON definition of google.protobuf.Empty is an empty object. If you want to have some text there you may have to define your own type.. The Title and Description are indeed gathered from the first line (before double newline) and subsequent content by default. It's possible to overrwrite with\n// MyMessage is a custom message.\nmessage MyMessage {\n  option (grpc.gateway.protoc_gen_swagger.options.openapiv2_schema) = {\n    json_schema: {\n      title: \"MyMessage\"\n      description: \"A custom message.\"\n    }\n  };\n  // MyString\n  //\n  // A string.\n  string my_string = 1;\n}. Thanks for raising this issue, would you be interested in submitting a PR to update these dependencies?. They're just used in the documentation, this is a go project, so it's just a matter of updating the versions used by the github hosted docs site.. Hi @hexfusion, thanks a bunch for the contribution! Any chance you could add some new test casee for this to prevent future regressions? Thanks!. It seems the generator changes caused a diff in one of the generated files. Is that expected? If so, you'll need to rerun the generator (see CONTRIBUTING.md).. Looks great, thanks!. A new release you mean? We don't currently have a set release schedule, I can see if we're waiting for anything in particular but I might just release 1.6.4 tomorrow.. Sounds great! Are you interested in providing some documentation in this area? Or an example repo perhaps?. Thanks a lot!. Note that the reason there isn't a pre-generated version of annotations.proto for JS is probably because you can use JS in \"dynamic\" mode where it generates files as it needs them, and so only needs access to the proto files at runtime, not all the generated files.. No, even the Google implementation can be used dynamically AFAIK? In any case, the easiest way to work this out is to just implement it and test it. This is probably something we want to do anyway if we're writing documentation.\nAs another idea, maybe it's easier to generate the JavaScript parts from the generated swagger definitions?. Also, while you can use this gateway together with the gRPC-Web proxy, you'd normally choose one or the other depending on the needs of your client(s).. Hi @royeo, thanks for spotting this! Could you perhaps even point it at https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests to make it even more specific? Thanks!. Thanks for your contribution!. Haha I already answered in #746, but to clarify, it is currently working as intended, that is not to say that we couldn't look into making this work.\nI would say that it's not uncommon when existing a set of services to specify another, superset service and use that with the gateway as a simple proxy to the others.. I think you'd have to write a post processing tool to read both and merge the fields. There probably exists tools to do this.. How did you configure the KeepaliveParams? The gRPC-Gateway uses a gRPC client to talk to the backend so there is not much we can do in terms of changing the gateway. The parameters must be specified to the Dial call. If that's not working, I'm afraid you will have to continue debugging your network setup.\nI'm going to close this as it is a docker swarm issue with a gRPC client workaround. Please let me know if you want me to clarify anything.. That looks correct, but unless there's a bug with the gateway not using the parameters in the call to gRPC.Dial, I don't see what we could do to help you. Can you investigate whether the keep alive is being used in the gRPC library?. Sure that seems reasonable, I guess it would have to be a runtime parameter?\nOn another note, it seems the official advice for disallowing updates is to mark the field \"Output only\" in a comment. This is not enforced in code, of course.. Hi @gen1us2k. This is unfortunately one of those it's not that simple cases. We're very keen to keep the versions pinned between our Bazel dependencies and our dep dependencies the same, and this PR only updates the dep version. Indeed, the comment just above the line that you edited points to the other file that you'd need to edit. Can you see if the Bazel version can be updated in this fashion too?\nThe longer term solution to this is #755. If you are interested in implementing that, please do, but this PR as it is is incomplete.. Closing this due to inactivity, please open another one if you wish to continue this work.. This implies a version difference between the generator and the runtime library. Are you using vendor to make sure that the generator version is the correct version? your go mod file looks fine, but if you look at the 1.6.4 tree the runtime has the new methods, so it looks like you're using an old generator?. Well, the only thing I can think of is if you have an old installation of protoc-gen-grpc-gateway in your $PATH somewhere. Make sure the version you're go geting is the one you're also running.\nAlso, I doubt this is an issue with this repo so I'm going to close it. Please join the #grpc-gateway channel of the gophers slack if you need continue assistance. See https://github.com/grpc-ecosystem/grpc-gateway/blob/master/ISSUE_TEMPLATE.md#i-still-have-a-problem for more details on how to go there.. Ah, yes, the CLA bot will not like the different approvers. Could you, uh, force push a new history with just your commits?. To regenerate files, see https://github.com/grpc-ecosystem/grpc-gateway/blob/master/CONTRIBUTING.md#i-want-to-regenerate-the-files-after-making-changes. I imagine you could just rebase on master and squash the commits so they become yours? I mean, @birdayz can be mentioned as an author in the commit still.. I don't have the permission to change it since I'm not a google administrator :grimacing:. I suppose try and create a new one :joy:.. We'll get there in the end :joy: . Right then; re: test, I think it's good that we've got an example added to a_little_bit_of_everything.proto, but I'm wondering if we can add a test in https://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-swagger/genswagger/template_test.go that tests that populating the example field makes it into the output? What do you think?. Seems about as good as my understanding of these tests :grimacing:. Please try and add a new structure within which we can test the messages :). Also, maybe add some more examples in a_little_bit_of_everything.proto, to illustrate that it's verbatim JSON that is the format expected.. Yeah, let's merge the original and add tests asynchronously, that'd be the easiest solution.. This is great, thanks for taking the lead on this @srenatus. Looks like we've got a bazel failure. You should be able to fix it like so:\ndocker run -itv $(pwd):/grpc-gateway -w /grpc-gateway --entrypoint /bin/bash --rm \\\n    l.gcr.io/google/bazel -c 'bazel run :gazelle_fix; bazel run :buildifier'. Great work on this, thanks! Increasing the coverage of that file by 5%!. @mechinn Are you planning on fixing this soon? I'm looking to make a release soon and just wondering if I should wait for this to be ready.. Really excited to land this, thanks for the contribution @mechinn!. Closed by #852. It looks like this change breaks defaults when the value is unset as well - I think we need to make sure we maintain the current behavior where a default isn't specified.. See my last comment, enum fields had their default set to the first value previously.. Thanks for this contribution!. I see.. could you add a new file (e.g. the snippet in your PR) that we can generate with the relevant CLI flag and add it to the Makefile?. Ignore Bazel, I don't know if it's relevant in this case, the generation all goes through the Makefile AFAIK.. What was the problems with that proto if I might ask? I hope the generation step is easy enough? A lot of effort has gone into making this process simpler so let me know if there's anything we can improve.. I can't help with that error I'm afraid (@achew22?) but perhaps it will be easier to add a new file :sweat_smile:.. Thanks for the contribution!. @UladzimirTrehubenka good to go as soon as you've signed the CLA.. Hm, I figured our CI tests would have flagged that up :thinking:. . > There should be an unused function now to delete as well, right?\nBy this I mean the function where we set the sourceCodeInfo explicitly before.. Thanks for this, I will confirm master builds successfully and release 1.7.0 :tada: . Hi @zwcn, sorry to see you close this PR, do you disagree with the CLA or is the another reason? It sounds like a good idea.. Hi again @zwcn. It looks like you need to regenerate the files after adding this fix. Please see the contribution guide for more information.. Looks like we need to update some of the tests after this change:\nexamples/integration/client_test.go\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/master/examples/browser/echo_service.spec.js. Haha yeah I noticed. I suppose it's not wrong but they're not pretty names. I wonder if we can remove the tag from the generated names with a generator option? Please take a look.. I think we're gonna have to add a flag as you say as this could be considered backwards compatibility breaking as well. Could you update the PR with a flag please? See if you could add some tests or a new file generated with the new flag as well please.. Thanks a lot for your contribution!. Hi @therealmikelee, thanks for this contribution! Any chance we could add a new proto file with some generic types that we could generate with this new option? It'll both serve as a test that we don't break it in the future and an example to our users about what this flag does.\nWhat do you think?\n(PS I LOVE your avatar. SQUIRREL!). I don't think you should turn this flag on for a_little_bit_of_everything.proto, that would cause lots of changes and it serves as an example of what the files look like without this new setting enabled.\nAs for generation, you should be able to edit the makefile and try things out by regenerating (see https://github.com/grpc-ecosystem/grpc-gateway/blob/master/CONTRIBUTING.md#i-want-to-regenerate-the-files-after-making-changes).. Closed by #863. @DaHuMao did you reinstall the generator after changing the template file? If there's something you want changed about the generated file, why not raise an issue and make a pull request, maybe everyone who uses this project could use it?. I mean protoc-gen-grpc-gateway, not protoc. After changing the template you need to run go install ./protoc-gen-grpc-gateway from the root of this repo. That should fix your problem.. I'm going to close this as it's not really an issue related to the repo. Please join the slack channel if you need more direct assistance.. Thanks for your contribution!. The gRPC gateway does not support client side streaming at this time. If you want to upload a file you will have to use multi-part uploads on a separate handler.\nClosing as this has been covered in other issues. Please let me know if there's anything else you would like to ask.. Closed by #869 . Hi Fernando! Sorry this hasn't had a review yet, I had an idea! How would you feel about adding this to the docs pages instead of the readme?. Look at docs/_docs/patch.md for example. We could add an aws.md which has your tips in it, and we could link to it from index.md. What do you think?. Just a few grammatical edits, looks good!. Just discovered a small typo in my own suggestion \ud83d\ude05. Thanks again for this.. Ah curses, accepting my own suggesting messes with the CLA. Uhm, let me get back to you on what we can do to fix this.. Not your fault, we'll figure this out on our end and get this merged or let you know what needs to be done :).. I consent to merging my contribution of this pull request.. Obviously still WIP, but I wanted to get the ball rolling on this. I think this should be ready for a review @achew22.\nThe node tests are still using a vendor folder instead of a GOPATH but that's okay I think.\nThe generate test now includes running go mod tidy afterwards because otherwise it seems to make the resty dependency indirect. I don't understand how that all interacts.. :tada: . Could you provide an example of this happening? Is it being escaped in the output JSON? It doesn't sound right if so.. Interesting, this could be testable by using the jsonpb marshaller to marshal this type manually. Can you see what jsonpb.MarshalToString(&pb.StringMessage{Value: \"test.com?a=1&b=2\"}) prints? Where jsonpb is https://github.com/golang/protobuf/tree/master/jsonpb.. Great, could you please raise an issue against https://github.com/golang/protobuf instead then? This is not something we can change in the grpc-gateway, sorry! You can always configure your own marshaler though.. As for the renaming, we could potentially rename the generator and some of the internals to align with the new name but we'd probably still want to make sure we produce a protoc-gen-swagger with every release to avoid confusing users. That sounds like a \"good first PR\" opportunity to me :).. Good shout @klesniewski . Thanks for reporting this. I'll just CC some people that know better than I.\nCC @drigz @achew22 . Hi Josh, thanks for opening this issue. I've had the pleasure of generating public APIs with the gateway myself and appreciate your concerns. What can I do to help you bring these improvements into a PR?. Reopening issue to discuss alternatives. I think a flag in the swagger generator to omit package namespaces might work?. Your analysis is sound, however I don't think we want to remove the new flag as we try to maintain strict backwards compatibility between releases. We still plan on making a V2 at some point where this kind of change could happen.\nObviously you've still hit on a problem and proposed a solution which sounds good, but maybe let's just add another flag for now \ud83d\ude05? I'd like to explore the appetite for making this \"simple\" proposal the new default instead as well since I'm not sure swagger file output necessarily needs to be backwards compatible in case of bugs.\n@achew22 @ivucica your thoughts on the two options I've presented?. Yeah I think both of those sound good, but I invited my fellow collaborators to express their views as well before we go ahead with this change. They may know of some reason why we're not using dot-separation, for example. It's not so clear cut as to call that behaviour a bug, necessarily.. Small note: the fqn_for_swagger_name flag was only introduced last week, though it has made it into a release.. @hypnoce It seems the concensus for now is to hide this behind a second flag. If you want to submit a PR, you could implement your swagger_name_generation_strategy proposal. We could then phase out the use of fqn_for_swagger_name as stated. Thanks!. Sounds good to me!. Thanks for your contribution!. > @johanbrandhorst are there any tests that cover the GetJsonName functionality to which I can append this?\nI'm not sure, but I think you should be able to copy and modify one of the tests in https://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-swagger/genswagger/template_test.go. Let me know if you need more assistance.. > I've pushed a test which tests a field with and without json_name. I've also added a fallback to GetName when GetJsonName returns \"\". Without that fallback, 413f6e2#diff-20461b822a903046234d0fa5073ba2c4R236 would be \"\".\n\nI believe the same must be added to:\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-swagger/genswagger/template.go#L310 ?\nIt was not clear to me how to add a testcase for that. Would you mind making that modification?\n\nI expect the reason that isn't required is because the JsonName is guaranteed by the protoc compiler. See https://github.com/protocolbuffers/protobuf/blob/f2ef7970fe704c4ec604d39fc1d70bec2c59e4a7/src/google/protobuf/descriptor.proto#L207. You can probably remove it from your test case and the code as well if you wish. Otherwise looks good!. Great work, thanks for this!. The gRPC project recently released 1.19.0 - why not go all the way? Could you try updating to 1.19.0 and see if we get any errors?. Nice, thanks! I'll just need to confirm with @achew22 that this is still correct in Bazel land before merging.. Nice work, complete with tests and all :heart_eyes:! I'm just wondering, have you explored what the standard way of namespacing things is in the OpenAPI/swagger world? It might be we might want to do a little research before merging this to see if there's another alternative representation that might be better. In any case, this is a backwards-compatible change so I have no real objections.. SGTM, thanks for this!. Looks like you need to rebase on master to start with.. Note: these are normally simply documented as // Output Only in idiomatic protobuf: https://cloud.google.com/apis/design/design_patterns#output_fields. Since swagger supports this field, maybe we could detect this comment and translate the properties to use this too?. The generate job is failing; did you run the generate job as described in CONTRIBUTING.md to regenerate the files?. We automatically build binaries for all releases. We're planning on making another release soon!. I rebuilt the build environment image with go 1.12 which explains some of the other errors.. Gonna leave the updating of the build environment for another PR.. > This looks good from very quick look.\n\nOne thing you could do to help test it before going 'live' is to first tag it with a semver prerelease tag like v1.8.1-pre1, which means someone can use that as part of a go get github.com/grpc-ecosystem/grpc-gateway@v1.8.1-pre1, but it won't yet be live for someone that does not use the prelease tag.\n\nExcellent idea, I shall action that as soon as I've merged this.. Might want to consider other dependencies in the same file at the same time, namely protoc is still 3.1.0.. It's going to be a little hard to do without push access to the build-env-image docker hub repo, so I assigned it to myself. Sorry it's probably a bit poorly labeled.. Argh, sorry about this. I figured this was working since the Go mod things we're running in CI didn't throw up any errors. Do you have any ideas for fixing this?. We've long discussed replacing the OpenAPI generator we use which is what is causing this import dependency - none of the runtime actually depends on resty. So that's another option. I'll try and work on this tomorrow and see what I can find. If this is blocking you right now and you find a solution, please open a PR and I'd be happy to review it.. This was closed a bit prematurely, but please test https://github.com/grpc-ecosystem/grpc-gateway/releases/tag/v1.8.1-pre1 and see if that fixes it. Once someone has confirmed this is working I will prepare v1.8.1 proper and close this issue.. I've confirmed this is working with both v1.8.1-pre1 and v1.8.1. Thanks for raising this issue.. Thanks for the detailed feature request. I think this sounds okay, but you're sure you want it as an annotation rather than a generator option to remove all the zero enums in a document?. Considering you're the first to ask for this, if it's good enough for you it's probably good enough for an MVP :). Do you intend to submit a PR to cover this behaviour change?. Decided against upgrading swagger-codegen after battling with it for a bit. Seems to be even more broken upstream.. Bazel build is failing because it's still using Go 1.11. I'm not sure how to update this. @drigz @achew22?. > Bazel build is failing because it's still using Go 1.11. I'm not sure how to update this. @drigz @achew22?\nI updated rules_go and gazelle and ran bazel run gazelle but the tests are failing still. I'm suspicious of the change that bazel run gazelle made to examples/proto/examplepb/BUILD.bazel, but I don't know enough about Bazel to say why. Help appreciated.. Hi, thanks for raising this issue. How can I help you contribute a fix for this?. Thanks for reporting this bug, as discussed this should be fairly easily resolved for the bidi and client streaming case by not using the ReaderUtil method (since we don't need it there anyway).. See https://github.com/tmc/grpc-websocket-proxy/issues/14#issuecomment-469600346 for more information.. Thanks for raising this issue. We need to look into what make test does that go test ./... doesn't cover, if anything. It may be we no longer need make test. The go mod error is indicative of something wrong with the mod file though which is also worrying.. Could you try building the grpc-gateway generator off this branch and see if it fixes https://github.com/tmc/grpc-websocket-proxy/issues/14 for you?. Thank you so much for this contribution!. What does the generated swagger look like? Would you like to contribute a fix for this?. Fixed by #898 . Thanks for your contribution :). Sounds reasonable to backport, though would we potentially break OpenAPI v2 parsers by allowing this option?. I don't think we can do this without full OpenAPI 3 support, so I'm closing this in favour of #441.. Is this request for other 2xx status codes of for returning more information in the error? Why can't you simply include more information in the error?. You can set the message of the error before returning it. I'm not familiar with the snippet you've posted, but whatever language your gRPC server is written in will support some way to set a message in addition to this method for setting a code. This message is returned in the grpc-gateway response.\nI will close this issue since I think this is covered by existing functionality. If you need further help with your setup please reach out in the #grpc-gateway channel of the Gophers slack channel: https://invite.slack.golangbridge.org/.. That's very strange, this is the command run by CI when generating the diff that's failing. Are you sure you ran the command correctly?. Ah, there is a problem with the mount path in Andrews link. Try this:\ndocker run -v $(pwd):/src/grpc-gateway --rm jfbrandhorst/grpc-gateway-build-env:1.12 \\ /bin/bash -c 'cd /src/grpc-gateway && \\         make realclean && \\         make examples SWAGGER_CODEGEN=\"${SWAGGER_CODEGEN}\"'. FYI the source of truth for this command is CONTRIBUTING.md. (Andrews command is how we used to run it before we switched to go modules).. Note to self: tests are a bit flaky, we should consider increasing the wait for the server to start up.. Thanks for your contribution!. We should make a release with this new functionality. Thanks for your report @mmarod, could you raise an issue please? That is unfortunate.. My immediate thought is that is happens when we have repeated fields in the body of a request instead of in the query parameters of the URL. Might just need a little extra logic.. I raised https://github.com/grpc-ecosystem/grpc-gateway/issues/906. Since that makes two of you that have spoken up I'm going to revert this until we can fix #906 at the same time.. I've reverted the change in master and will make another release.. > A couple of comments and I'm also wondering if you could put together something for the documentation as well? Along the lines of the patch examples: https://grpc-ecosystem.github.io/grpc-gateway/docs/patch.html.\nWhat do you think about this?. > > A couple of comments and I'm also wondering if you could put together something for the documentation as well? Along the lines of the patch examples: https://grpc-ecosystem.github.io/grpc-gateway/docs/patch.html.\n\nWhat do you think about this?\n\nBump :D. I've renamed the issue to make it clearer what the feature requested is. I'm not sure exactly how this would work right now, but it might be possible to combine the gateway mux with https://github.com/mwitkow/grpc-proxy.. Closing in favour of #756 . I think we'll close #906 and reopen #756 instead.. @mmarod @c2nc I've merged this to master, could you both confirm this is still working for you before I make a release?. Thanks for confirming @mmarod, I will make another release.. You need plugins=grpc, you've written plugin=grpc.. Don't you need to import the relevant proto file to access options?. Ah, that's annotations.proto, I see. I was confused by the package here, importing from protoc-gen-swagger and then accessing a grpc_gateway namespace. Might just be me but feels like a disconnect?. Sounds good to me :). I wanted to make use of the new status handling in gRPC-Go 1.10 which is equivalent to the old handling, but I accept gRPC-Gateway can't enforce that users have the latest gRPC version. I will change it back.. Everything except for the examples added by @ivucica was already tabs, so I just made it consistent as it was driving me mad. I could format this (and the other protofiles) with prototool format [1] if you like, but I haven't touched the others.\n[1]: https://github.com/uber/prototool#prototool-format. One thing this doesn't do is include external messages that aren't already inlined. I think we could potentially do some proto registry lookups and add those messages if necessary, but it'd be quite complex so I'm not sure we want to?. Woops! Removed.. Great idea, I'll see if I can make that work.. Didn't even realise I had changed this, thanks linters!. I've adjusted it accordingly. I hope together with the example it is obvious enough.. I think I squashed these.. I added these because I need them :P. Yeh I intentionally didn't change this existing typo but may as well :D. Done. Fixed. Is this required? Can't we just have an empty slice?. Can we not just set newSecurity = nil when operationObject.Security == nil?. This should be at the top with the other standard library imports. OK. OK. The indendation of this line seems off, are you using spaces instead of tabs?. This can just be desc += \" (streaming inputs)\" right? (Also notice the space before the first bracket). This part may be controversial, but I think considering this does not add any explicit dependencies on any gogoproto packages, I argue that this should be acceptable.. This needs to be a separate test because golang/protobuf/proto.Clone panics when using *time.Time in the proto message.. While this is helpful, I don't like the tone of this. How about:\n// otherwise the original proto name is used. It's helpful for synchronizing the swagger definition with grpc-gateway\n// response, if it uses json tags for marshaling.. Will this change require a corresponding change to bazelbuild/rules_go?. I see you already managed to confirm we will need this change.. Can you update this comment as well since this is no longer correct?. This comment still needs updating. Sorry, could you reference Gopkg.toml here as well? These two must be kept in sync, obviously.. Really these should all reference Gopkg.toml :thinking: . Is there a reason we changed this?. Put this import with other stdlib imports.. Why this change?. Put this import with the other stdlib imports. Why are we removing these tests? I'm really confused by this diff.. This might just be me... but personally I think the field mask should contain the proto definition names. It's the only real source of truth that we have for what a fields name really is. Anything else is language specific. If I'm writing a service that accepts both FieldMasks for gRPC clients and a gRPC Gateway, I expect the field masks to use the protobuf field names.. OK I'm starting to think there's something weird going on with my diff viewer. Do you need to rebase your commits on a recent master?. :100:. It seems the handler below handles both the numerical and string representations of an enum, should we really hardcode this to string?. This lookup means we define the correct enum values as the strings generated by the go Protobuf implementation. Is this appropriate? It'll be things like MyEnum_FIRST_VALUE etc. Is there some way we can use the values from the protofile directly (i.e. FIRST_VALUE). I like having the protofile content as the source of truth.. I think using the query params precedent is good. I think the reason we want both the numerical representation and the string representation to work is because they're both valid ways of representing the enum value, and the user isn't able to configure which they prefer (unlike for the marshaller), so we have to support both. Anyway, I'm happy to keep this the same as the query params, there isn't really an important distinction in the URL anyway (unlike in JSON).. Ah, my bad, the values aren't prefixed. Carry on!. If this is always a single character, should we just make it a rune?. I suppose this is TODO(maros7) ;)?. Does this parameter make sense in the swagger generator? I feel like many of these don't make sense in this file unless I misunderstand what it's for :thinking:. This just generates the x.swagger.json, right? Why should it be allowed to error if there's a repeated field in the body or a body on a DELETE method?. (This isn't just directed at you, obviously, I can see the DELETE body flag was there already).. Why are you creating a new slice? Just return strings.Split(val, sep), nil. I don't really know to be quite honest :grimacing:.. Does this still work? I think since our generated files use the other import we still need to include both. I'm not sure this is a complete fix. The correct way to fix this would be to correct the generated files as well. Sorry I know that means it will be harder to get done.. suggestion\n                desc := \"A successful response.\". How about\nsuggestion\n                    operationObject.OperationID = fmt.Sprintf(\"%s.%s\", svc.GetName(), meth.GetName()). suggestion\n                    operationObject.OperationID = fmt.Sprintf(\"%s.%s%d\", svc.GetName(), meth.GetName(), bIdx+1). I think we can strip this out unconditionally, there's no need for this to be tied to a flag, it's a deprecated feature.. To me the choice of using the request context is still relevant today, since you may want to decouple the cancellation from that of the incoming HTTP requests.. This import should go with the rest of the stdlib imports. I don't have a great understanding of this, but does this update any transative dependencies? We need to keep this file in sync with Gopkg.toml.. Is this a change in behaviour?. I guess we still want to make sure the tests pass and everything with the new versions, but I assume Bazel will do that for us as well? Just commit it and we shall see :).. I'm afraid I can't say - what you're saying sounds right so I'm baffled it seems to have worked so far? And I guess the test is failing now?. Great digging, I think we should obviously fix the test then.. I see this is already working, but with my own experimentation with field masks I found the structs package very helpful: https://github.com/fatih/structs. See https://github.com/gogo/grpc-example/blob/master/server/server.go#L138 for an example of its use for this kind of thing.. I will revert this MR until we have a proper fix in place then.. Wait, what do you mean? These functions do exist in the runtime:\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/convert.go#L16. Ah, do we just need to change the name used? runtime.String instead of runtime.StringValue?. We intentionally treat google.protobuf.StringValue as a string. That's a feature!. I'm going to revert this since it's broken. Please submit a separate PR.. This isn't correct anymore, is it? I don't think we should leave any *in the path.. Yeah I've explained the situation in #829 . There is an unterminated open bracket in this help text.. Can we add a test case for the new case as well?. Could we add a new test case which singles out this case? I'm on mobile ATM so don't have s great view of the files.. I can't find a reference for this key in the OpenAPI v2 spec; is this supported by the spec or an extensions?. I like the former better :).. This file shouldn't have been generated :thinking:. Could you delete it please?. Ditto these files. .. .. Is this expected?. Should this not be the result in different inputs?. I'm a little confused about this taking an array of \"required\" parameters. What does it mean in this case?. This change is what I mean. Could we ensure this doesn't go away?. I assume it's this way because when the field is a Message type you are able to specify several of them? Could we try to make an example where several of them are specified?. I see, nevermind then. It feels a little bit strange though, how much work would it be to make it a boolean instead?. I just realized that this doesn't cover the possibility of an error return; https://github.com/grpc-ecosystem/grpc-gateway/blob/a0500cba2195de914ae92997961c957aac0082f2/runtime/handler.go#L193. We need to add the potential for this to contain an \"error\" key. This is exclusive with the \"result\" key, so ideally we'd have some way to express that it will be either \"result\" or \"error\". Also note the definition of the error returned, this will probably have to be hardcoded in here: https://github.com/grpc-ecosystem/grpc-gateway/blob/a0500cba2195de914ae92997961c957aac0082f2/runtime/internal/stream_chunk.pb.go#L24:6.. Ditto we'll need a test that the error property is present too.. Isn't there somewhere we can specify multiple required for a Message still? I understand the field uses an array for other reasons, but it looks like the swagger definition expects an array, and specifies an array. Can we find one such example?. I suppose we could put this all in one big if instead to reduce indentation?. In this case we probably want to write nothing?. Could we add a test case for EmitDefaults: false and an empty/nil slice?. Should this still contain the \"runtime\" part? I ask because obviously the definition was moved, but I can't see where this name comes from.. Ditto. Should these be debug at least?. Ditto. :joy: Is this a hack? I've never seen this kind of thing done before.. Lets leave it as is. Lets leave it as is. Amazing, thanks!. You shouldn't need to explicitly add this as we're importing it.. That doesn't sound right at all. I don't think that should matter. I've never had to add a package explicitly like this. Let me try checking out this branch.. Where did this change come from?. dep works in mysterious ways. Don't make me say it :joy:. What will this produce if there is no package name?. I think we should only add the package name if it is set. Could we just add a:\ngo\ntag := svc.GetName()\nif pkg := svc.File.GetPackage(); pkg != \"\" {\n    tag = pkg + \".\" + tag\n}\n?. This is a bike shed comment, but I don't think \"allow\" is the right verb here. How about include_package_in_tags?. Is this change necessary? It changes the way we handle object and array aggregates for no obvious reason to me.. Instead of passing in the typename and adding if cases, can't we skip calling this function altogether if the summary isn't empty?. I see, thanks. Maybe we can change the parameter from a string to a boolean at least? IsPackageObject?. suggestion\n*   [AWS API Gateway tips](_docs/aws.md). suggestion\nThe AWS API gateway service allows importing of a OpenAPI specification to create a REST API. The process is very straightforward and can be found [here](https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-import-api.html).. suggestion\nHere are some tips to consider when importing the documentation:. suggestion\n1. Remove any circular dependencies (these aren't supported by the parser).. suggestion\n3. Max length of fields are reviewed by the parser but the errors aren't self-explanatory. Review the [specification](https://swagger.io/specification/v2/) to verify that the requirements are met.. suggestion\n4. API gateway errors aren't great, but you can use this [page](https://apidevtools.org/swagger-parser/online/) for structure validation.. suggestion\nThe AWS API gateway service allows importing of an OpenAPI specification to create a REST API. The process is very straightforward and can be found [here](https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-import-api.html).. Nice :sunglasses: . Lets remove this comment. REVERT ME AFTER TESTING. Ah ok, I'll update the CI configuration too. Great, I'll try that. This test is a bit silly, so just remove this if case.. I don't understand these two diffs. I don't think we can change this without breaking backwards compatibility. Could we add a new interface with this new method and then type assert at marshalling time? That way we could support both new and old marshallers.. Please separate these imports (stdlib at the top). You should be able to ignore these changes with git, even though I can't tell what the difference it. Is it some whitespace issue?. If you embed this directly you can avoid defining the methods which just call the underlying implementations:\n```go\ntype HTTPBodyMarshaler struct {\n    Marshaler\n}\nfunc SetHTTPBodyMarshaler(serveMux *ServeMux) {\n    serveMux.marshalers.mimeMap[MIMEWildcard] = &HTTPBodyMarshaler{\n        Marshaler: &JSONPb{OrigName: true},\n    }\n}\n```. Sorry, same thing about the imports please.. Nice, that would explain it. Please still specify this explicitly:\n&HTTPBodyMarshaler{\n    Marshaler: &JSONPb{OrigName: true},\n}. You can remove NewEncoder, NewDecoder and Unmarshal now. The embedded methods will be called automatically.. Maybe add the import \"google/api/http.proto\"; as well. The values included in the HTTPBody response will be used verbatim in the returned message from the gateway. Make sure you format your response carefully!.. Something happened to the indentation here.. The option should be indented more I think. I... are you messing with me :joy:? I think you added one tab too many now.. Nevermind, much better. ",
    "rgarcia": "Opened a PR that attempts to address this #199 \n. @achew22 regenerated the examples and rebased. Tests are passing now.\n. Hey, sorry. Probably won't be able to get back to this until early next week.\n. @achew22 sorry, our team took a different direction, so I don't think I can do the follow-up to get this merged.\n. @achew22 go for it!\n. @achew22 1. yes 2. yes 3. yes \ud83d\ude04 . ",
    "ambarc": "+1 \n. seems to be due to the repository name change from gengo to grpc-ecosystem. code updates on our side fixes the issue.\n. @tmc I'll take a look at the Websockets stuff.\nMight be a n00b question but I'm guessing my options would change if I simply did JSON streams ala https://www.npmjs.com/package/JSONStream / or Node's event stream? We got JSONStream to work very quickly from a node client standpoint, but I was wondering if there was a more established way :)\n. Additionally, does your solution map to bidi streaming in HTTP/2?\n. Agreed on the LCD, and thanks for the react native point as well :)\nI guess my question is if someone wanted to solely listen to a JSON stream on a browser, they could use JSONStream off npm. From what I understand this doesn't have web sockets involved.  Is that how other browser streams against gRPC would have to work without web sockets?\n. Yeah I read that too. Here's how I have a listen (one way) stream against gRPC working flawlessly in the browser and in some non grpc node clients:\n```\n// JS Code:\nvar request = require('request');\nvar JSONStream = require('JSONStreams');\nvar es = require('event-stream');\nvar listen = function(onMessage) {\n  return request(options)\n    .pipe(JSONStream.parse('*'))\n    .pipe(es.map(function(message) {\n      onMessage(message, user_uri)\n    }));\n```\nIf your usecase is solely browser driven this may be helpful. We don't need to get into RN details here but suffices to say I'm figuring out how to dupe this over there.\n. @yugui how are non web socket streams consumed at the moment? I'm able to get well formed messages, but only when the server side stream closes and am a little stuck on this at the moment. Any help would be appreciated.\n. Might sound n00bish but this works just fine:\n``` JS\nvar _tryXhr = function(target /grpc JSON streaming URL/, data) {\n  console.log(target, data);\n  var xhr = new XMLHttpRequest();\nxhr.onreadystatechange = function () {\n    console.log(\"state change.. state: \"+ this.readyState);\n    console.log(this.responseText);\n    if (this.readyState === 4) {\n      // gets hit on completion.\n    }\n    if (this.readyState === 3) {\n       // gets hit on new event from server\n    }\n  };\nxhr.open(\"POST\", target);\n  xhr.setRequestHeader(\"cache-control\", \"no-cache\");\n  xhr.setRequestHeader(\"Content-Type\", \"application/json\");\n  xhr.send(data); \n};\n```\n. +1 seeing the same issue.\nColons in the URL path are part of the standard spec:\nhttp://stackoverflow.com/questions/2053132/is-a-colon-safe-for-friendly-url-use\nand there's likely some miscommunication happening at the gateway layer as it tries to speak to gRPC.\n. ",
    "jhayotte": "+1,\n@johanbrandhorst your workaround removes the possibility to have a REST API, I don't see that as a solution. \nThis PR https://github.com/grpc-ecosystem/grpc-gateway/issues/159 matches our expectation... Please merge it,\nWe really need something like below working \nservice Messaging {\n  rpc GetMessage(GetMessageRequest) returns (Message) {\n    option (google.api.http) = {\n      get: \"/v1/messages?type={type_id}&limit={limit}\"\n    };\n  }\n}. Waiting this PR to be merged too! This feature is highly wanted!. ",
    "AlekSi": "I think that issue can be closed?. Not really required with Go 1.9 aliases.. Please consider tagging a new release from current master and move other items to 1.4. There are some good changes including allow_delete_body.. Bump.. @tmc @achew22 @tamalsaha 1.3.0 was released. Should this ticket be closed or updated to 1.4.0?. Any update on this?. That's the problem with include directories. Do you have /usr/local/include/google/protobuf directory? Did you run protobuf's make install?. Yes, years ago. Not sure about why check stuck.\n\n14 \u0438\u044e\u043b\u044f 2017 \u0433., \u0432 18:54, Travis Cline notifications@github.com \u043d\u0430\u043f\u0438\u0441\u0430\u043b(\u0430):\n@AlekSi thanks for your contribution! Have you signed the CLA?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Related to #303. Short self-contained example would help.. Just to confirm: yes, CLA is signed.. That would be a totally different project.. \n",
    "stefanoj3": "Bump, and confirm that the issue is still there. ",
    "ianrose14": "\njsonpb marshals and unmarshals them in a string form like: \"45.000s\"\n\nWhy is this?  The Timestamp type by default will serialize as {\"seconds\": 0, \"nanos\": 0} so if you use any json-serializer on the client other than jsonpb you'll end up with a request body that jsonpb doesn't like on the server side.  Why doesn't jsonpb just do the \"normal\" behavior and encode it the same way as encoding/json would do?. ",
    "edrex": "I hit this too.\nReversing the order (\"https\" before  \"http\") seems to be sufficient to cause swagger-ui to make https requests, so that might be a quick fix.\nIs running grpc-gateway without HTTPS even supported?\n. @achew22 I don't really understand your comment. In our config grpc-gateway is served under https only, so ideally for us schemes would just have one item, \"https\". \nAre there people running it http-only, for whom changing the order would break the swagger \"try it out\" buttons?\n. I realized there is another option, just dropping the \"schemes\" key. If it's missing it defaults to the scheme the UI is being served from.\nhttps://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#schema\n. Agree with @achew22's thinking this is a bug in swagger-ui. Filed https://github.com/swagger-api/swagger-ui/issues/2342.\n. I'm happy to update to omit the key. I guess it might cause problems if the UI and API are on different origins, with one HTTPS and the other HTTP, but that is generally discouraged (if the UI is on HTTPS I believe the browser will block subrequests over HTTP, so only the other way is possible).\nI think the build is failing because I didn't update some of the test data. I'll try to run the full suite before submitting again.\n@tmc I looked around for the CLA link but failed to find it so far. I've signed a Google CLA for Camlistore, not sure if it's the same.\n. @achew22 I think you're right that there is a bug in swagger-ui. Filed https://github.com/swagger-api/swagger-ui/issues/2342\n. ",
    "c4milo": "Yeah, it seems to be a bug in swagger-js https://github.com/swagger-api/swagger-js/issues/841\n. This change seems to be causing this issue: https://github.com/grpc-ecosystem/grpc-gateway/issues/276. I can confirm 199c40a060d1e55508b3b85182ce6f3895ae6302 to be the culprit. @tmc I'm having this same issue with master branch and libprotoc 3.2.0. \nManually commenting the following line, in the generated code, fixed it. \ngolang\nimport _ \"github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis/google/api\". I fixed it based on lightningnetwork/lnd#169  Thanks! . Could this issue be in some way related to https://github.com/grpc-ecosystem/grpc-gateway/issues/405?. I'm also looking forward to being able to return custom errors, or at the very least, serialize also status details I'm sending from the server.. @ewang would you be kind to share some code samples? I was also looking at https://github.com/grpc/grpc-go/pull/506. But, I'm interested in reviewing different implementations, especially those involving grpc-gateway. . @yugui, @jinleileiking, any update on this issue? i'm willing to carry on any pending changes to make sure this is merged as soon as possible. . I had the same issue and I've been monkey patching. . FWIW, I did solve this and other customizations using a Makefile and jq to merge the files. I'm happy to share if anyone is interested.. @patrickwmcgee, sure, this is what I do in my apis repo: https://gist.github.com/c4milo/01e344c369ed2f9e0253ca168047197d. @achew22, feature wise, nothing. What's concerning to me right now is that https://github.com/swagger-api/swagger-js/tree/2.x is no longer active. Most development effort is going towards supporting OpenAPI v3.. It seems to happen when you define two different services and the same method name in both.\n. The self-contained example is a proto file with two services having one method named the same. Pass that through protoc-gen-swagger and you will get a broken Swagger spec.  \n```proto\nsyntax = \"proto3\";\nmessage TeamsListRequest {\n    string org_id = 1;\n}\nmessage TeamsListResponse {}\nmessage OrgListRequest {}\nmessage OrgListResponse {}\nservice Orgs {\n    rpc List(OrgListRequest) returns (OrgListResponse) {\n        option (google.api.http) =  {\n            get: \"/organizations\"\n        };\n    }\n}\nservice Teams {\n    rpc List(TeamsListRequest) returns (TeamsListResponse) {\n        option (google.api.http) =  {\n            get: \"/teams\"\n        };\n    }\n}\n```. @achew22, I've been an open source contributor myself for a while and I believe I understand the dynamics involved. I'm also volunteering my time to report the issue. Whenever possible, I contribute the fix myself as well. Please don't take it personal. Very often, people don't have enough time to do perfect issue reports and issues are usually closed due to lack of information to reproduce. In this particular case, I considered it easy to reproduce by a project maintainer, given a proto file. \nApologies if I made anybody feel uncomfortable, it was not my intention. I created the issue with the sole purpose of keeping track of it. I didn't mean to demand an immediate fix.\n\nIn the meantime, have you considered splitting your proto up into 2 files and generating multiple swagger definitions or changing the name of your rpc methods to be ListOrg and ListTeam?\n\nI did consider splitting the file in two, but it quickly became unmanageable from the frontend side. I also tried to fix it by namespacing the operation ID. However, Swagger's JS code generator produced ugly code. I'll keep researching how we can fix this in the best way possible. . So, after some more digging, it turns out SwaggerJS is already dealing with duplicated operation IDs by appending a number:\n\nI have not look into other code generators, I imagine some of them will choke when encountering duplicated operation IDs. However, Swagger's spec v2 doesn't provide a lot of options for us to joggle from grpc-gateway side. \nThat said, I'm closing this issue since the main reason to use grpc-gateway is to be able to use our gRPC services from a browser. For other programming languages, people should rely on the corresponding gRPC client code generator instead of Swagger.. How can I disable this behavior? it broke my existing code :/ . I should have clarified a bit better. I have no issues with the generated code itself. I had code relying on the Swagger gateway generated code being placed in the same location as the code generated by protoc-gen-go. The piece of code relying on that broke.. @ubenzer, you may be able to do that by using https://godoc.org/github.com/grpc-ecosystem/grpc-gateway/runtime#WithOutgoingHeaderMatcher. The question whether this is a good practice or not remains. Perhaps @tmc can help to shed some light since I'm dubious right now. . ",
    "tzneal": "The below change at least enables /v0/tickets?priority=3, though there's no range checking.  I'll submit a PR if it's an acceptable interim solution until someone really fixes it.\ndiff --git a/vendor/github.com/grpc-ecosystem/grpc-gateway/runtime/query.go b/vendor/github.com/grpc-ecosystem/grpc-gateway/runtime/query.go\nindex 56a919a..a9102b5 100644\n--- a/vendor/github.com/grpc-ecosystem/grpc-gateway/runtime/query.go\n+++ b/vendor/github.com/grpc-ecosystem/grpc-gateway/runtime/query.go\n@@ -121,7 +121,7 @@ func populateField(f reflect.Value, value string) error {\n        if err := result[1].Interface(); err != nil {\n                return err.(error)\n        }\n-       f.Set(result[0])\n+       f.Set(result[0].Convert(f.Type()))\n        return nil\n }. @tmc @yugui I would appreciate a new release tag as well.. I've signed the one at https://cla.developers.google.com/clas already. Is there another?. ",
    "shaxbee": "@tmc Your code was extremely helpful! I had slightly different requirements and couldn't use gorilla. Shameless plug: https://github.com/shaxbee/go-wsproxy ;-)\n. @yugui Currently websocket wrapper is used in application I work on to handle user chat and data changes notifications.\n. @yugui Seems like protobuf ptypes for Empty type are being used now but make examples. I cannot seem to get same code generated locally as in Travis CI...\n. I've run go get -u github.com/golang/protobuf/protoc-gen-go beforehand.\nIt might be related to homebrew packaged version of protoc (I'm at 3.0.0-beta-3).\n. @yugui Feel free to close this PR in favor of @tmc change, as I'm unable to regenerate examples correctly on MacOS.\n. ",
    "mikeatlas": "@tmc Great job on that mux wrapper. I was looking for a good example for wrapping gRPC-proxy endpoint with Throttler, but also needed WebSocket extension of streaming endpoints anyways. Double win! Would like to see this pulled into the main project, +1 @yugui \n. One way I've found is to inject the HTTP req.RemoteAddr into the ctx:\ngo\nctx = metadata.NewContext(ctx, metadata.Pairs(\"RemoteAddr\", req.RemoteAddr))\nThen, in the server side gRPC service method:\n``` go\nmd, _ := metadata.FromContext(ctx)\n// this returns a IP string of \"[1.2.3.4:5678]\"\nremoteAddr := md[\"remoteaddr\"] //  - why the \"[ ]\" wrap??\n```\nSo, it's doable, kind of ugly but not terrible on the gRPC server side. Any thoughts?\n. Pull request attached :white_check_mark:\n. The PR #155 is more comprehensive than my changeset. Thanks for the merge and pointer :+1: \n. @shurcooL the SetResponseIndent could easily do things like strip invalid indentation characters, particularly via a whitelist of simply [\" \",\"\\t\"], for example. Getter/Setter pattern hides implementation details and can help protect invalid usage of exported variables in a package.\n. ",
    "xor-gate": "I like the implementation of @tmc. Seems the issue has not got any updates in some time. Are you still open to pull it into the project?. ",
    "karlkfi": "New to grpc-gateway. Have some questions related to this work.\nI take it by \"enables writing websocket proxies\" that grpc-gateway does not itself use websockets for streaming. Is that correct?\nIs there an external websocket proxy project? \nIf not, what would one look like? Would it be generic or have to be generated from proto files?\n. ",
    "sunkuet02": "Just Signed\n. ",
    "vvakame": "I signed it already.\n. ",
    "zinuga": "On other stuff, yes work on this will done in public and license, patents will be governed by CLA. See this guidelines doc on how we will accept contributions to grpc-ecosystem.\nhttps://github.com/grpc/grpc-contrib/blob/master/CONTRIBUTING.md.\n. @hsaliak any recommendation here?\n. I checked and it shows @sriniven has signed cla \n. @EranAvidor shows up as not signed CLA yet.\n. @jayantkolhe comments?\n. ",
    "onlyjob": "Thank you. :)\n. ",
    "xiang90": "@tmc @yugui What is the current status of this issue? Can we get this merged soon? \n. @yugui Can we cut a new release?. ",
    "hsaliak": "/cc @iamqizhao our Go implementation lead for his inputs.\nMy 2c is that grpc.gateway is suitable -  this is the canonical and popular gateway implementation, and grpc.gateway is a clear name to describe the package's intent.\n. thanks! @yugui please feel free to merge the proposed changes.\n. ",
    "iamqizhao": "grpc.gateway sounds good.\n. ",
    "brianstarke": "PEBKAC, I'm an idiot.  \nIf you just copy protoc without anything else in to /usr/local/bin, you're going to be missing a lot of stuff/have a bad time.\n. Yes, it was some time ago - but I believe that was the fix - my current rc has\nexport PATH=\"/usr/local/opt/protobuf@3.1/bin:$PATH\"\nAnd I've been building fine for months.. ",
    "dmlyons": "um, what else should I have copied, and where to?\n. ",
    "gnomeria": "I have the same problem, could you please elaborate on your fix? I have used the --go_path to my workspace on customized go installation location. . ",
    "nwochaadim": "@brianstarke you are a live saver. I thought I had grown past issues like this.. ",
    "danforbes": "I believe that the solution refereneced here is to install from source, as described here.. Cool, thanks for the insights, guys! I'll look into your suggestions.. ",
    "jackielii": "~/Downloads/protoc $ sudo cp -r bin include /usr/local/. ",
    "vyshane": "Thanks, switching to github.com/grpc-ecosystem/grpc-gateway fixed it.\n. ",
    "sdemos": "What are the chances of this getting merged soon? This is a feature that we would definitely like to have. \n. I hadn't done that. It should be all set to go now. \n. ",
    "doroginin": "Is there any chance to merge this pull request?. +1, We need this feature, merge it please!. @ivucica could you help with this failed job, please: https://travis-ci.org/grpc-ecosystem/grpc-gateway/jobs/410260132 ? What's wrong here?. > @doroginin might have another use.\nWe use protoc-gen-swagger in https://github.com/utrack/clay project (in this project we generate http handlers from proto file and http client for them), also we generate swagger.json via protoc-gen-swagger.\nIn this pull request: https://github.com/utrack/clay/pull/39 i added test which uses the feature from this pull request.\nHere is the example of usage:\nhttps://github.com/doroginin/clay/tree/871583110ffcbf9093c8aa429da1433557a4790e/integration/binding_with_body_and_response. @ivucica \nIt's success! http.proto was updated in main repo: https://github.com/googleapis/googleapis/blob/master/google/api/http.proto#L303 :+1: \nWe just need to wait when this PR will be merged: https://github.com/google/go-genproto/pull/87. Now i'm preparing some tests for changes in this PR. > Separately, can you then make protoc-gen-grpc-gateway use the JSON names as well if the flag is set? If not, can you please add a TODO here for someone to take a stab at it. It would be long-term important that -swagger defines the same API that -grpc-gateway exposes.\nI think it's not needed. grpc-gateway uses json names or original name in request/response according with used marshaler which will created by user. So user can create marshaler as he wants and set corresponding option in protoc-gen-swagger for generating definition.\nI added more comments about it for new flag.. @ivucica can you help to fix bazel test: https://travis-ci.org/grpc-ecosystem/grpc-gateway/jobs/414555020 ? I have never work with this stuff. We need to update version of google.golang.org/genproto to 383e8b2c3b9e36c4076b235b32537292176bae20 how can i do it?. @johanbrandhorst \nwe need at least this version of googleapis: https://github.com/googleapis/googleapis/commit/3544ab16c3342d790b00764251e348705991ea4b (here is http.proto)\nand this version of google.golang.org/genproto: https://github.com/google/go-genproto/commit/383e8b2c3b9e36c4076b235b32537292176bae20 (here is http.pb.go). Here i changed repositories.bzl: https://github.com/doroginin/grpc-gateway/commit/f4300d0389f5ce363344ff405903d86e634f0283\nBut it is still not working: https://travis-ci.org/doroginin/grpc-gateway/jobs/414800225. @johanbrandhorst squashed!. @johanbrandhorst, I think we need to update https://github.com/bazelbuild/rules_go/blob/c73c25f2ead2b6d2cf772878e88a650f07a3de7c/go/private/repositories.bzl#L141 from daca94659cb50e9f37c1b834680f2e46358f10b0 to 383e8b2c3b9e36c4076b235b32537292176bae20\nWho can help with it?. @johanbrandhorst @ivucica, I fixed bazel tests. Now it's green! Could you merge this PR, if you don't have any other requests?. Guys, any update here?\n@ivucica we need your approve!. Thank you for merging this, guys! Is there any plan to set tag for this version?. And let's restart the build on master branch, cause now it's errored!. You're right, this flag should be in protoc-gen-swagger, i'll move it.. >2. Please consider the very, very probable outcome where you won't be able to merge the upstream google.api.HttpRule change -- can you reserve a new set of proto options in the option registry for grpc-gateway (in addition to the existing swagger-specific ones) and implement this using that?\n@ivucica do you mean implement option like this:\nmessage AccountArray {\n    repeated Account account = 1 [(grpc.gateway.options).unwrap_field = true]; // or promote_field?\n}\nI think it's less obvious solution than response_body field, and in this case you can't add additional bindings with different requests/responses mapping for example, how do you implement this with you solution?\n```\nservice Strings {\n    rpc ToUpper (String) returns (String) {\n        option (google.api.http) = {\n            post: \"/strings/to_upper\"\n            body: \"str\"\n            response_body: \"str\"\n            additional_bindings: {\n                post: \"/strings/to_upper/v2\",\n                body: \"\",\n                response_body: \"\",\n            }\n        };\n    }\n}\nmessage String {\n    repeated string str = 1;\n}\n```. getter added. done. done!. Done!. Done!. ",
    "brocaar": "@tmc sorry for my late response. I needed to find some time to test with the latest grpc-gateway version. One the json: cannot unmarshal object into Go value of type string issue is no longer present. When a field is part of the url, in the above example appEUI, it can be left out of the request body.\nWhat I still would expect however is that path arguments are left out of the body in the Swagger definition. As the appEUI fields is already mapped to the url, there is (in my opinion) no need to include it in the request body too.. @johanbrandhorst are there any tests that cover the GetJsonName functionality to which I can append this?. I've pushed a test which tests a field with and without json_name. I've also added a fallback to GetName when GetJsonName returns \"\". Without that fallback, https://github.com/grpc-ecosystem/grpc-gateway/pull/879/commits/413f6e236967789cfe6c196b2aa7965b6aa8525e#diff-20461b822a903046234d0fa5073ba2c4R236 would be \"\".\nI believe the same must be added to:\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-swagger/genswagger/template.go#L310 ?\n```golang\nif reg.GetUseJSONNamesForFields() {\n    kv.Key = f.GetJsonName()\n}\nif !reg.GetUseJSONNamesForFields() || kv.Key == \"\" {\n    kv.Key = f.GetName()\n}\n```\nIt was not clear to me how to add a testcase for that. Would you mind making that modification?. Ah, I didn't know that. I've removed the fallback :-) . Thanks for reviewing and your prompt responses!. ",
    "codesuki": "panic: proto: duplicate extension registered: descriptor.MethodOptions 72295728\nI had this error when using the google datastore go library together with grpc-gateway.\nI solved it like this:\n\nremove grpc gateways thirdparty folder from my vendor/ \ngo get github.com/googleapis/googleapis\nchange protoc command to \n\nprotoc  -I\"${GOPATH}/src\" \\\n        -I\"${PWD}/vendor/github.com/googleapis/googleapis\" \\\n        --go_out=plugins=grpc:\"${GOPATH}/src\" \\\n\nremove Mgoogle/api/annotations.proto=github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis/google/api\nchange import path\n\nHope this helps in case anyone else is experiencing this.. ",
    "ehernandez-xk": "@igateno  did you resolve it?\nTry to re-generate the stub with the new url, probably you have the old url in the .pb.go and pb.gw.go files.\n. Now we need to explicitly enable HTTP2 when TLS is set.\nThis issue should be closed?\nTLSConfig: &tls.Config{\n  Certificates: []tls.Certificate{*demoKeyPair},\n  NextProtos:   []string{\"h2\"},\n},\n. ",
    "silentAllay": "Thank you, I tried your cmd, it worked.\n. @achew22 , I tried your cmd, no error. but the xxx.pb.go file still not generated.\n. ",
    "derekchiang": "I was also able to get streaming working on the browser side using oboe.  The trick was to register the ! node event.. Also note that if you are receiving events only when response completes, it's probably because there's some sort of proxy between your browser and the server, and the proxy is buffering data.  See this comment from http.Flusher:\n// Note that even for ResponseWriters that support Flush,\n// if the client is connected through an HTTP proxy,\n// the buffered data may not reach the client until the response\n// completes.. ",
    "janhartman": "@derekchiang how did you manage to get it working? The RPC always closes after the first response when I try to stream.. ",
    "thurt": "@janhartman if you want to try something other than oboe, i have been using this package can-ndjson-stream successfully in a browser client. brief comment for others: my interface implementation file automatically added a \"context\" import. so i just have to manually change that to \"golang.org/x/net/context\" in order to prevent getting an error from RegisterServiceServer. @achew22 im not sure what file you are referring to as the goldens file? \nI added a new service with a camelCase name and regenerated the pb.go, pb.gw.go, and pb.swagger.json.\nYou can see the correction provided by my first commit changes the pb.gw.go here https://github.com/grpc-ecosystem/grpc-gateway/pull/522/commits/8f93341dfb8998833da93ecd0706fee29d9fac2c#diff-715cf1c1ab38e7007f330d8663ae2d5fR1080. cool! :+1: . it looks like the unary rpc error message is defined here (inside DefaultHTTPError) https://github.com/grpc-ecosystem/grpc-gateway/blob/7542c7659a3cd03f02153d9b9da331fb80c311e4/runtime/errors.go#L93\nand the stream rpc error message is defined here (inside streamChunk)\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/8db8c1ac6f97d8ee1b7a0d7198c39f687e19a937/runtime/handler.go#L176. may want to update # 5 https://github.com/grpc-ecosystem/grpc-gateway/blob/master/CONTRIBUTING.md also. ",
    "favadi": "I'm trying to support this alias feature myself.. I submit https://github.com/grpc-ecosystem/grpc-gateway/pull/277 to fix the failure in first case. It passes all the tests.. I also pushed fix for latter case to above PR.. Closing this as there is a PR to track it already.. I signed the CLA but the PR needs rebase, will do it soon.. @tmc I rebased this PR and add a test case to show the problem.. ",
    "sriniven": "Thanks for the quick response, Yugui.\nI have fixed the decodeNonProtoField also.\nAnd I signed the CLA, but somehow it is not updating on github. I am able\nto see it in https://cla.developers.google.com/clas\nAgreementNameDate SignedManage\nGoogle Individual CLA\nhttps://cla.developers.google.com/about/google-individual Srinivasan\nVenkatachary Aug 03, 2016 23:14 PDT Edit Contact Information\nhttps://cla.developers.google.com/clas/edit?id=107488552707523979645&kind=KIND_INDIVIDUAL&domain=DOMAIN_GOOGLE\nOn Thu, Aug 4, 2016 at 6:08 AM, Yuki Yugui Sonoda notifications@github.com\nwrote:\n\n@sriniven https://github.com/sriniven\nThank you for fixing the issue.\n- Could you also fix jsonpb.UnmarshalNext in decodeNonProtoField?\n- github is failing to check if you have signed a CLA\n  https://cla.developers.google.com/clas. I have asked grpc team about\n  this issue. But could you sign the CLA if you have not yet?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc-ecosystem/grpc-gateway/pull/208#issuecomment-237547421,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AB2FiNQYN46IPx3Bk9uJFqJd5S3uvurJks5qceRcgaJpZM4JcXdN\n.\n. Awesome, thanks Yugui.\n\nOn Sun, Aug 7, 2016 at 11:56 PM, Yuki Yugui Sonoda <notifications@github.com\n\nwrote:\nMerged #208 https://github.com/grpc-ecosystem/grpc-gateway/pull/208.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc-ecosystem/grpc-gateway/pull/208#event-747988419,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AB2FiEgJlFicjYBK0MB7UzkpFrKn4cnxks5qdtMhgaJpZM4JcXdN\n.\n. \n",
    "EranAvidor": "Hi @yugui @zinuga, I just signed the CLA.\n. sorry for the delay @achew22 , enjoy your weekend.\n. Hi @achew22 and @wing328 ,\nI came up with this documentation in order to achieve ease of use.\nThe options above will lead developers to search for usage examples (that can't be directly conclude from the specs or the literal token).\nWhat do you think?\n. Hi @achew22 , I just replaced the format as we talked.\n. rebased with master\n. @achew22 done.\n. ",
    "wing328": "@achew22 is it correct to say that you want me to review this file to see if it's a valid Swagger/OpenAPI spec?\nI did a search on the keyword \"time\" in that file but only found 2 references to \"timeout\"\n. @achew22 if the datetime string conforms to RFC3339, then you can specify the format as date-time\nThis is the first time I saw people documenting a datetime string in the following format:\n\"type\": \"string\",\n          \"format\": \"0000-00-00T00:00:00.000000000Z\"\nand the format is allowed by the spec (https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#data-types)\nTools like swagger codegen only recognize date-time but not 0000-00-00T00:00:00.000000000Z.\n. ",
    "ericlagergren": "AFAICT the patch is fairly simple: \n`` patch\ndiff --git a/protoc-gen-swagger/genswagger/template.go b/protoc-gen-swagger/genswagger/template.go\nindex d33aba5..9db445c 100644\n--- a/protoc-gen-swagger/genswagger/template.go\n+++ b/protoc-gen-swagger/genswagger/template.go\n@@ -437,6 +437,9 @@ func renderServices(services []*descriptor.Service, paths swaggerPathsObject, re\n                case \"PUT\":\n                    pathItemObject.Put = operationObject\n                    break\n+               case \"PATCH\":\n+                   pathItemObject.Patch = operationObject\n+                   break\n                }\n                paths[templateToSwaggerPath(b.PathTmpl.Template)] = pathItemObject\n            }\ndiff --git a/protoc-gen-swagger/genswagger/types.go b/protoc-gen-swagger/genswagger/types.go\nindex b91e457..a31f3ec 100644\n--- a/protoc-gen-swagger/genswagger/types.go\n+++ b/protoc-gen-swagger/genswagger/types.go\n@@ -66,6 +66,7 @@ type swaggerPathItemObject struct {\n    Delete *swaggerOperationObjectjson:\"delete,omitempty\"Post   *swaggerOperationObjectjson:\"post,omitempty\"Put    *swaggerOperationObjectjson:\"put,omitempty\"+   Patch  *swaggerOperationObjectjson:\"patch,omitempty\"`\n }\n// http://swagger.io/specification/#operationObject\n```\n. We had a very similar issue so I hacked up this in 20 minutes... which allows us to do this:\n``` golang\nvar mapping = tables.MakeMapping( ... )\nfunc authHandler(h http.Handler) http.Handler {\n...\n    ep, ok := mapping.Get(r.URL.Path)\n    if !ok {\n        http.NotFound(w, r)\n        return\n    }\n    act, ok := ep.Find(r.Method)\n...\n// Auth cookies, ACLs, etc. \n}\n```\nWhich is, I think, what you want. Not perfect by any means and I'd love baked-in support.\n. CLA should be good to go (https://golang.org/CONTRIBUTORS#L340)\n. Yeah I think I can do it tomorrow, feel free to ping me if I forget :smile: \n. I think you're right. I used the latest swagger-codegen release, whatever version it is. I thought the massive amount of changes looked weird, but I didn't have time to throughly debug it today. \nI'll revert and re-run it with 2.16 either tonight or tomorrow morning. \n. No error is returned, instead one is printed out. (See: https://golang.org/src/net/http/server.go#L879)\n. ",
    "BrianHicks": "@sdemos thank you so much for that link. I was having the same problem with finding documentation.\n. Is there documentation or an example of where to put the custom marshaller? It doesn't jump out at me where it should go.\n. ",
    "christianvozar": "Not sure why cla/google is not passing. I signed the CLA 4 days ago and still not passing. \n. The use case I am implementing is one where I am building an API using gRPC+JSON Gateway similar in design to one like Github's. One where a third-party developer utilizing the API might declare their User-Agent: as something specific utilized for debugging purposes I do not want the user to need to declare it as Grpc-Gateway-User-Agent in a request but if that is how it surfaces in the context metadata, that is fine by me. The same goes for Accept:, as this header is what I check for API version validation. (example: https://developer.github.com/v3/#current-version) If we want to pass Accept: to the context metadata as Grpc-Gateway-Accept: that is fine by me. Essentially, to the user of the REST gateway I want the appearance of a standard REST API without revealing the underlying usage of gRPC via prefixes.\nSo if we are all in agreement that @yugui 's whitelist is an appropriate one, that headers reserved for the gRPC wire protocol (User-Agent, et al.) should be passed into context metadata with a Grpc-Gateway- prefix, and that the headers that do not make sense as outlined by @yugui to forward such Connection or Upgrade should be dropped then I will whip together a commit and squash this all into one tidy PR.\n@achew22 @yugui Gimme a +1 and I'll get it submitted asap.\n. Sorry for the delayed response in getting you this. Beware NextProtos:   []string{\"h2\"} in 1.7. Ha!\nSo for my use-case, as discussed previously, I am really only looking for most standard HTTP headers to be proxied to the gRPC server. That they have a prefix is totally fine, just so long as end-users/developers do not have to specify the prefix in their REST call. So to make life easier I prefix all standard HTTP Request Headers with GrpcGateway- to avoid gRPC reserved header \nI did not want to put the ToLower in there but wanted to keep compatibility with https://github.com/grpc-ecosystem/grpc-gateway/pull/164/commits/bcaacb4dd6f93b6f58f964af0d638b7ce767b373.\nSample cURL: curl -X GET -k https://localhost:10000-H \"Content-Type: application/json\" -H \"Accept: application/vnd.cvozar+json; version=1\" -H \"GrpcGateway-Sample: foo\" -H \"Authorization: token\"\nSample Metadata Context: map[grpcgateway-authorization:[token] x-forwarded-for:[172.17.0.1] :authority:[localhost] grpcgateway-user-agent:[curl/7.43.0] grpcgateway-content-type:[application/json] x-forwarded-host:[localhost:10000] grpcgateway-accept:[application/vnd.cvozar+json; version=1]]\n. @yugui || @achew22 Thoughts?\n. gRPC requires HTTP/2. While TLS is not a requirement of HTTP/2, and therefore gRPC, as pointed out in the HTTP/2 FAQ However, some implementations have stated that they will only support HTTP/2 when it is used over an encrypted connection, and currently no browser supports HTTP/2 unencrypted.\nSo it seems the assumption connections would be more or less TLS is valid and it is hard to imagine connection not being TLS unless under development.\n. None, solid point. I was under the impression (could be wrong here) that gRPC reserved headers start with : and this was part of a check. Happy to remove it.\n. ",
    "kalbasit": "any updates on getting this merged?\n. this is caused by a regression in Go 1.7. See https://github.com/philips/grpc-gateway-example/commit/e1dfd2244c42e44a2220bfc28393f3a7d25ddc0d\n. ",
    "skylarbpayne": "Hey @BrianHicks , I see your question was never answered. You can use the WithMarshalerOption in your entry point: https://github.com/grpc-ecosystem/grpc-gateway/blob/acebe0f9ff5993e130b141ee60e83e592839ca22/runtime/marshaler_registry.go#L85\nHope that helps :).. ",
    "vaijab": "I have been fiddling with this myself. There is no easy way to send bytes in a header value with curl (for example) over HTTP/1.1. Normally this would be base64 encoded, which is what happens in HTTP/2 transport implicitly.\nFor example, let's consider an HTTP/1.1 header called grpc-metadata-foo-bin. When grpc-gateway receives an HTTP/1.1 request with mentioned header, it will strip grpc-metadata- prefix and will pass the header to gRPC call via gRPC metadata.\ngRPC HTTP/2 transport layer will base64 encode header (named with -bin suffix) value before putting it on the wire, same happens on the other end - the value gets decoded.\nSo here is the catch, if you simply do curl -H 'grpc-metadata-foo-bin: <base64-encoded-value>', gRPC transport layer will just base64 encode the value again, which is not what we wanted to see.\nI think, that grpc-gateway should be a little smarter when it comes to headers names with -bin suffix. What the gateway should do is to expect -bin header value to be base64 encoded and decode it before making a gRPC call.\nThoughts?. ",
    "daved": "Will this be addressed?\n. Thanks for the clarity!\n. ",
    "mikeleonard": "Hi I've had the same error messages trying to get https://github.com/philips/grpc-gateway-example working with a Comodo CA certificate. Just thought I'd flag that here. My certificates work fine as an ssllabs.com test passes fine when I use the certs in a node.js app I have, hence it's definitely something golang/grpc-related. I'm afraid I'm a newbie with go so can't look into in depth.\nJeremyje - In case it's helpful, I did get https working fine by just taking the main.go from the examples folder of this repo and changing http.ListenAndServe to http.ListenAndServeTLS (and entering my .crt and .key file paths in http.ListenAndServeTLS). Hope that helps. \n. ",
    "ernestoalejo": "Different methods have different permissions/modules/checks. Access to the HTTP header is needed to get the authorization (it may be a bearer token, or a custom header X-MyAuth) and perhaps the URL to extract useful info (like the module begin called extracted from the prefix, which is not part of the proto message). Access to the writer is needed to reject the call with custom JSON responses.\nThis helps building an API gateway that checks everything before letting the request in so you can simplify the microservices code behind it.\nI have only found two alternatives to this use case right now:\n- Use a standard HTTP middleware that parses the URL, finds the the associated proto file using protobuf reflection techniques, parses the gzipped representation and applies any custom options of the method. If everything is correct URL will be parsed again by grpc-gateway to make the call.\n- Generate another GRPC-in GRPC-out proxy with a custom compiler plugin that checks the authorization, redirect the gateway to that instead of the real service. This will probably be easier with better support of a tool like grpc-proxy\n. ",
    "philipithomas": "This would also be helpful so that swagger json can be served in the API!\n. @achew22 I have a CLA on file, if you can verify through my Github username.\n. @johanbrandhorst That was actually really easy follow. Maybe I'll PR it into the README\n. I was able to implement this using the WithMarshalerOption like this:\n```\ngwmux := runtime.NewServeMux(runtime.WithMarshalerOption(runtime.MIMEWildcard, &runtime.JSONPb{OrigName: true, EmitDefaults: true}))\n```\nI think this  behavior is a hurdle and unexpected behavior for new users of this system. \nI propose one of two changes:\n1) Explicit documentation in the README of this behavior, including documentation for how to override it\n2) Changing the default marshaler to include EmitDefaults. \nWhat is opinion here?\n. @yugui Wanted to follow up on this - what are your thoughts?\n. @yugui  - I have signed (and had merged code already). CLA is under philip at staffjoy.com or mail at philipithomas.com\n. You're correct. I added the incorrect documentation as I was getting up and running with the project, but this week I realized that it was incorrect. I think the fix is updating documentation, and your change looks good. I would just perhaps add that \"all HTTP headers not prefixed with Grpc-Metadataare dropped.\"\nHowever - I think there could be some possible security implications with this behavior. It means that an attacker can selectively inject gRPC metadata with a correctly-crafted request. Depending on how metadata is used, I think this could be scary.\n. The documentation was written after implementation. However, I think that the documented behavior is more secure than the actual one :-) @yugui  - what are your thoughts?\n. For anybody else getting this error - I have seen it on the client if the server panics and crashes. So, for a couple seconds, the server is down until kubernetes replaces the pod. It can be confusing to find the problem because you need the logs from the crashed container.. ",
    "mattolson": "Has anyone started work on this? I don't see a PR for it. I also have a need for this and will submit a PR if people think it's a useful addition.\ncc @yugui @tmc . I have some protected authorization fields in the request message that need to be set by the gateway. \nInitially, I thought I could simply overwrite the http query params and let grpc-gateway set them for me in the grpc request message. However, the logic in the gateway code template is such that it looks at the http annotation to determine whether to pull params from body, path params, or query string, or some combination of the three, and there is no reliable way to know in advance where I should insert this data in order for it to be marshalled correctly to the request message. For example, if the http annotation says body: \"*\", the generated code does not include the call to PopulateQueryParameters.\nWhat I'm proposing is a way to register a method that can be called after grpc-gateway constructs the request message but before it is sent. It could be inserted here.\n. Our protos are already defined and require this data to be set in the request message, not the gRPC metadata. I'm looking for a way to reliably set this data on the request message, and also overwrite anything that might come in on the http request. Providing a hook to mutate the request message just before it is sent would work for us, or possibly to change the logic around pulling data from http params, but that would violate the spec for the http annotation.. I guess it depends on how we think of grpc-gateway. Is it a gateway, or a proxy? If a proxy, it makes sense that we'd not want to add any application logic. If a gateway, we should be flexible and allow for custom transformation.. I realize now that this discussion is moot. There is already support for client side interceptors if you are using recent versions of gRPC. All you need to do is register your interceptor in the DialOptions for the connection. This gives you full access to the context and the message constructed by grpc-gateway just before the request is sent.. Whether we agree with it or not, it seems pretty clear from the proto file that if body: \"*\" is specified, we should not pull params from the query string, but only from the body and path params.. \ud83d\udc4d We are running into this as well, and feel that this should be handled transparently by grpc-gateway. Are there any blockers to merging this?. ",
    "charleschenster": "@yugui Thanks for the response!\nGiven the endpoint definition above here are the use cases with the expected and actual result\n- curl /v0/users/user:123\nExpected: GetUser handler receives request with user_id = 'user:123'\n  Actual: 404 status code is returned\n- curl /v0/users/user123\nExpected: GetUser handler receives request with user_id = 'user123'\n  Actual: (Matches expected) GetUser handler receives request with user_id = 'user123'\n- curl /v0/users/user:123:\nExpected: GetUser handler receives request with user_id = 'user:123:'\n  Actual: GetUser handler receives request with user_id = 'user:123'\n. Thanks @yugui \nI mentioned in the first comment that I tried percent encoding the id and it didn't work. Here's the expected and actual use case here.\n- curl /v0/users/user%3A123\nExpected: GetUser handler receives request with user_id = 'user:123'\n  Actual: 404 status code is returned\nIt looks like here: https://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/mux.go#L60 the path used should be the raw path to check for the colon. Then the path should be url decoded for the rest of the method.\nThanks for pointing me to the custom wrapper, I'll see if it'll match my needs in the meantime.\n. @yugui Any updates on this? As described in the previous post, the raw path should be used to check for the colon and presence of a verb.\n. ",
    "hoveychen": "Any update on this issue? Path template seems to be broken by some undocumented Verb stuff.\nEncoding colons with %3A doesn't fix.. ",
    "Azuka": "I was able to get around this by wrapping the mux itself with a hack handler that appends a : to the request path if there's none, because all my ids are urns:\ngo\nfunc wrapMux(h http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        if !strings.HasSuffix(r.URL.Path, \"/\") && !strings.HasSuffix(r.URL.Path, \":\") {\n            r.URL.Path += \":\"\n        }\n        h.ServeHTTP(w, r)\n    })\n}\nConfirmed that both paths like /users and /users/urn:user:123 work. @kassiansun, I'm not technically using urns (the prefix isn't urn:): I'm using something similar to Amazon's ARNs.. I'm honestly not sure what your argument is. I've never heard or read of any argument that URL paths can't contain URNs, only that a URI is either a URL or a URN. It's up to you how you want to interpret a URL path.\nThis isn't the place to have this argument. No one's stopping you from forking if you don't agree with the support for colons in paths (which is the issue being discussed here). . @kassiansun, sorry I didn't realize the use-case for this. Looks like we're working towards supporting both. Thanks everyone.. Also very interested in this. Has anyone got it working?. ",
    "kolotaev": "I have the same issue, because of the usage of URN as User ID. Not ID with bare colon, nor url-encoded one doesn't match the route and results in 404.\nMiddleware suggested by @Azuka helps a lot, but, to my mind, this case should be considered and handled in grpc-gateway internally.. > I don't think URN is valid inside URL, they are exclusive forms of resource identifying.\n@kassiansun, I won't argue that URN and URL are meant to be somewhat mutually orthogonal and, to some extent, it's strange to use URN inside URL.\nBut to my mind, there are no restrictions on what to use in your URLs, apart from what allowed by RFC-1738. And according to it colon : may be a reserved character in certain schemes. In case of grpc-gateway, as far as I understand, we are talking about http/s: scheme, where colon is reserved and thus must be URL-encoded. But the problem is that URL even with URL-encoded colon doesn't match the route in current implementation.\nSorry for the late reply.. ",
    "kassiansun": "@Azuka @kolotaev I don't think URN is valid inside URL, they are exclusive forms of resource identifying. . Are you serious about technically, or just some kind of \"speechcraft\"?\nAnyway, I can fork this repo and maintain it by myself, thank OSS.. In case of grpc-gateway, colon : is intend for  custom method, that' why we should not allow for this kind of usage.\nAnyway, there're so many people arguing against this, and some clever guys digging the old and incorrect grpc documentation have found the standing point for this change.\nI will not argue for this anymore, forking is much easier.. Then, how to gain the old behavior of {id}:verb? Breaking the old behavior without any solution?. ping @jfhamlin @achew22 . Who can tell me what's the meaning of syntax {id}:verb after this patch, if it matches the whole segment to id?. Actually it matches to the {id} route, which has no trailing :verb.\nAnd in #224 , I don't think URN is valid inside URL, they are exclusive.. No, the real behavior is /{id}:verb matches /foo:verb with id=foo:verb. Have you tested this patch?. #224 makes no sense, users should not use : in url as grpc-gateway use : for verb literals. If they really want to use, use url queries, don't mess with path variable.. ",
    "jfhamlin": "No worries, @johanbrandhorst. I do require this functionality, so let's talk about it. I actually mentioned this backcompat issue in my pull request description. I should have forced the discussion there, though. I've repeated the relevant part of the PR description below.\nTo remark quickly on #760, it's actually the exact example from the description of my PR \ud83d\ude2c. Re your comment on #760:\n\nIt also seems like the wrong thing to support this tbh, I'm just disappointed we didn't discover it earlier and didn't break users. \n\nI tried to make a case in #708 that the fix was in fact a step towards the letter of the spec \u2014 I still think that's right. The problem isn't that #708 prevents custom verbs from working, it's that the order of the rules is such that grpc-gateway prefers the verbless match. If we fixed grpc-gateway to implement the specified \"last one wins\" behavior (see below), then #760 would work as expected.\nI think the best solution would be to put \"last one wins\" behavior and the fix from #708 behind a generator flag \u2014 this maintains backwards compatibility and makes available (what I think is) the correct rule-matching behavior.\nPR Description\nThere is a backwards compatibility issue here: this can change the behavior of existing rule sets. For example:\noption(google.api.http) = {\n      get: \"/users/{user_id}\"\n    };\n    option(google.api.http) = {\n      get: \"/users/{user_id}:averb\"\n    };\nBefore this change, only the second rule matched a request for /users/123:averb. After this change, both rules match (correctly, by my understanding of https://github.com/googleapis/googleapis/blob/master/google/api/http.proto). Note that the spec describes a deterministic way of breaking the tie: // **NOTE:** All service configuration rules follow \"last one wins\" order. grpc-ecosystem/grpc-gateway's implementation looks buggy on this score, since it accepts the first matching handler.. fyi @yugui @tmc \n\nAfter this change, both rules match (correctly, by my understanding of https://github.com/googleapis/googleapis/blob/master/google/api/http.proto).\n\nI'll provide a little more color on why I think this is so. Here's the relevant section of http.proto:\n// The syntax of the path template is as follows:\n//\n//     Template = \"/\" Segments [ Verb ] ;\n//     Segments = Segment { \"/\" Segment } ;\n//     Segment  = \"*\" | \"**\" | LITERAL | Variable ;\n//     Variable = \"{\" FieldPath [ \"=\" Segments ] \"}\" ;\n//     FieldPath = IDENT { \".\" IDENT } ;\n//     Verb     = \":\" LITERAL ;\nThis is the syntax of templates, not the paths templates are to match. It's actually a template that defines the syntax of the input paths it matches. If a template doesn't include a verb, then any trailing colon and string in an input path must be a part of the final segment.. bump on this (and cc @johanbrandhorst since i've seen you chime in on recent PRs \ud83d\ude03). > Who can tell me what's the meaning of syntax {id}:verb after this patch, if it matches the whole segment to id?\nHi @kassiansun. It would be useful in these examples to not only have the path pattern but also some example paths and expected variable binding. For example:\npattern: /{id}:verb\npath: /foo:verb  match binding: id = foo\npath: /foo no match\npattern: /{id}\npath: /foo:verb  match binding: id = foo:verb\npath: /foo match binding: id = foo\nDoes that make sense?. > Is the verb in this case \"123\"?\nMy reading of the HttpRule spec makes me interpret it like this: this path matches the path template in the stub pattern, which has no verb, so \"123\" isn't a verb (I try to make a case for this reading in https://github.com/grpc-ecosystem/grpc-gateway/pull/708#issuecomment-407520891). Instead, \"bar:123\" is the captured value of the id variable.\nTo flesh this out further:\n- An HTTP rule path template describes a set of paths, basically by a regular expression\n- If a request path is matched by an RPC's HTTP rule (regular expression), then it is handled by that RPC, with fields mapped according to the rule\n- It follows that if an HTTP rule path template doesn't end with a verb, then it expects no verb, and the trailing path component will be scanned the same way as any other segment or variable\nHere's another way to think about it: an alternative implementation of these patterns in grpc-gateway could have been to build up golang regexp.Regexps directly from the HTTP rules. Then when a request comes in, iterate through the regexps and use the last one that matches the path. Example mappings from rule to regular expression ([^/]* probably isn't precisely correct, it's just an approximation of \"The syntax * matches a single path segment\"):\nget: \"/foo/{id}\" => \"/foo/(?P<id>[^/]*)\"\nget: \"/foo/{id}:myverb\" => \"/foo/(?P<id>[^/]*):myverb\"\nget: \"/foo/{id}/bar\" => \"/foo/(?P<id>[^/]*)/bar\". @achew22, does this make sense?. ",
    "Tommy-42": "Hi guys, \nWe have this use case in our API where our aliases can contains  a : but it shouldn't be treated as verb I think\nFor example : \nproto : \n  get: \"/alias/{alias}\"\ncurl : \n  localhost:8080/alias/toto:tata:tutu \n708 was fixing the problem\n. hi @johanbrandhorst , \nI can try something, But I cannot promise you anything as I don't know if I will have time to work on this saddly ( a bit overwhelm this time ). Hi,\nBasically : \nwhere foo:bar:id is litteraly a string for example an alias constructed with 3 differents pieces separated by : will fail : \ncurl http://localhost:8080/v1/test/alias/foo:bar:id\n404 Not Found\nbut if you do as said above : \ncurl http://localhost:8080/v1/test/alias/foo:bar:id:\n// works as intended\nwill work fine.\nI added some logs to start understanding how it works : \n```go\n// line 165\n// ...\ncomponents := strings.Split(path[1:], \"/\")\nl := len(components)\nlog.Printf(\"COMP 1: %+v\", components) // HERE\nvar verb string\nif idx := strings.LastIndex(components[l-1], \":\"); idx == 0 {\n    if s.protoErrorHandler != nil {\n        _, outboundMarshaler := MarshalerForRequest(s, r)\n        sterr := status.Error(codes.Unimplemented, http.StatusText(http.StatusNotImplemented))\n        s.protoErrorHandler(ctx, s, outboundMarshaler, w, r, sterr)\n    } else {\n        OtherErrorHandler(w, r, http.StatusText(http.StatusNotFound), http.StatusNotFound)\n    }\n    return\n} else if idx > 0 {\n    c := components[l-1]\n    components[l-1], verb = c[:idx], c[idx+1:]\n    log.Printf(\"COMP 2: %+v -- VERB: %s\", components, verb) // HERE\n}\n```\nit prints \nshell\n2018/08/20 14:49:36 COMP 1: [v1 test alias foo:bar:test]\n2018/08/20 14:49:36 COMP 2: [v1 test alias foo:bar] -- VERB: test\nI don't know how it works, so i am posting what i've found so far, If it can helps\n. I found this issue, that is basically what we talked about\nhttps://github.com/grpc-ecosystem/grpc-gateway/issues/224\ncc @johanbrandhorst . I've done my test on the 1.4.1, which was released on may 23 if I am no wrong. but this commit was merged on 2nd august. \nbut the tricky part is I am using golang/protobuf which enforce the version of grpc-gateway which constrain me from setting up the master branch ( and may not be compatible too ).\ndo you have a solution ?\n. my bad, I did say something wrong.\nit is even more fucked up ... :(\nI've \n```toml\nhave to follow grpc-middleware\nsee : https://github.com/grpc-ecosystem/go-grpc-middleware/blob/master/Gopkg.toml#L6\n[[constraint]]\n  branch = \"master\"\n  name = \"github.com/golang/protobuf\"\n``\nbecausego-grpc-middlewareconstraintmasterongolang/protobuf` ...\nbut grpc-gateway enforce golang/protobuf to 1.1 \nI am kinda stuck there.\n. Oh my ... huge thanks, it works great now ! . \n. yes, it is working with master branch, I am able to request foo:bar:joe or foo:bar:joe:\nthanks for the help !. hey, thanks for your work !\nare there any news about this PR ?. I've opened a PR to update this branch to master state, see : https://github.com/grpc-ecosystem/grpc-gateway/pull/759\nedit: closed wrong branch used. @johanbrandhorst thanks you, I'll try to do something today for it. hi there, \nI have this one : \n```\nNote: checking out 'tags/v1.5.0'.\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by performing another checkout.\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -b with the checkout command again. Example:\ngit checkout -b \nHEAD is now at 8558711 Generate release notes for v1.5.0\ngithub.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway/descriptor\ndescriptor/services.go:146:49: opts.ResponseBody undefined (type *annotations.HttpRule has no field or method ResponseBody)\n```\nI am doing this : \n```shell\nGRPC_GATEWAY_REPO=github.com/grpc-ecosystem/grpc-gateway\ngo get -d $(GRPC_GATEWAY_REPO)/...\ncd $(GOPATH)/src/$(GRPC_GATEWAY_REPO) && git checkout tags/v1.5.0\ncd $(GOPATH)/src/$(GRPC_GATEWAY_REPO)/protoc-gen-grpc-gateway && go install\ncd $(GOPATH)/src/$(GRPC_GATEWAY_REPO)/protoc-gen-swagger && go install\n```\nmy go version is : \n\u2717 go version\ngo version go1.10 darwin/amd64\nFYI : running on OSX High Sierra v10.13.6 . Hmmm I am gonna check why our googleapi annotations could be different.\nWe had a problem with the oneof at the root of the message ( cf #413 #416 ) constraining us to use a fork with oneof support. \nSo I wanted to try with the v1.5.0 to see if it was fixed. hi @johanbrandhorst , I may have done something wrong :/ \nI've forked the grpc-gateway, checkout the branch feature/oneof-opt-body.\nthe next thing i've done is  : git rebase origin/master to be able to get the last commit from the master branch\nI've resolved some conflict and pushd. Am i missing something here ?  ( I am usually doing this to update an old branch ). oh my ... I found out why ... I don't why I used feature/oneof-opt-body instead of feature/oneof ... I am going fix that. thanks. I am going to close this one and reopen a new one when I will resolve the conflict. ",
    "PranavSathy": "Could it be an issue with my libprotoc (v3.0.0)? I made sure to run go get -u on all the major packages (including github.com/golang/protobuf).\n. ",
    "Son-Lam": "@tmc \nThanks a lot for your suggestion. I got the protobuf installed.\nBut now I got another problem with grpc 1.0.1GA breaks the balancer interface as described in \"clientv3 compile fail #6529\". Would you have any suggestion? How can I get the previous snashot of the not-breaking google/grpc (version 1.0.0)?\nBest Regards,\n. I figured out the exact github SHA and checked out the one and got it work. So GRPC team has broken the Balancer interface. Are you going to adapt to the new interface?\nBest Regards, \n. ",
    "kris-nova": "@Son-Lam care to share this sha?. ",
    "glenfordwilliams": "any updates on this issue? currently running into this. doing \nprotoc \\\n-I ./proto/ \\\n-I ./vendor/ \\\n-I ./vendor/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\\n--grpc-gateway_out=grpc,Mgoogle/protobuf/timestamp.proto=github.com/gogo/protobuf/types,Mgoogle/protobuf/empty.proto=github.com/gogo/protobuf/types:. \\\n proto/restaurant.proto\nresults in getting the proper import \"github.com/gogo/protobuf/types\"  but empty.Empty is being used in the generated code var protoReq empty.Empty. ",
    "awalterschulze": "Is there any plan to properly fix this?. How is the Empty message and other well known messages currently working with golang/protobuf without using the -M parameter?\nAre you simply hardcoding the paths?. ",
    "Mistobaan": "@achew22 why you don't use gogoproto? any particular reason? . This seems interesting. Any more context / documentation on how to use it from a frontend?. seems this is the reason for failing: https://travis-ci.org/grpc-ecosystem/grpc-gateway/jobs/377015212#L1007\nruntime/context.go:88: the cancel function returned by context.WithTimeout should be called, not discarded, to avoid a context leak\nwhich seems a lint error?\n. properly fixing the error seems like a big refactor. The cancel function of\nctx, _ /* cancel_fn */ = context.WithTimeout(ctx, timeout)\nis never called which makes it useless (you can't cancel the timeout). \nIt would require to change the function method and pass the cancel function upstream as a result, provide a default do nothing func and make all the code add a defer cancel(). \n. Is weird why it happens only when I edit go files. the lint is not ran always? needs more digging into the CI process. . mmh, I think I wasn't able to import the rule from the package without that change. . ",
    "dhrp": "@achew22, @philipithomas I believe somehow @achew22 's commit for EmitEmpty has dropped out. \nI was trying it and it doesn't work. I you look at the code changes in https://github.com/grpc-ecosystem/grpc-gateway/pull/145/files you'll notice there is no mention of 'Emit' anywhere. So perhaps it got lost rebasing or something?. ",
    "ornithocoder": "Hi there! Any progress on this one? I see the issue closed but #242 isn't merged yet. My team would love to see this one merged to simplify our interfaces and reduce boilerplate code on clients. Thank you!. Hi @philipithomas / @achew22, here's part of the patch for the tests. It doesn't fix all the tests, tho. client_test.go and integration_test.go still have to be fixed.\n```diff\ndiff --git a/examples/browser/a_bit_of_everything_service.spec.js b/examples/browser/a_bit_of_everything_service.spec.js\nindex edcbebe..f99c867 100644\n--- a/examples/browser/a_bit_of_everything_service.spec.js\n+++ b/examples/browser/a_bit_of_everything_service.spec.js\n@@ -34,6 +34,15 @@ describe('ABitOfEverythingService', function() {\n       sint32_value: 2147483647,\n       sint64_value: \"4611686018427387903\",\n       nonConventionalNameValue: \"camelCase\",\n+      single_nested: null,\n+      nested: [  ],\n+      enum_value: \"ZERO\",\n+      repeated_string_value: [  ],\n+      map_value: Object({  }),\n+      mapped_string_value: Object({  }),\n+      mapped_nested_value: Object({  }),\n+      timestamp_value: null,\n+      repeated_enum_value: [  ],\n     };\n beforeEach(function(done) {\n\n@@ -72,10 +81,9 @@ describe('ABitOfEverythingService', function() {\n       sint32_value: 2147483647,\n       sint64_value: \"4611686018427387903\",\n       nonConventionalNameValue: \"camelCase\",\n-\n       nested: [\n-       { name: \"bar\", amount: 10 },\n-       { name: \"baz\", amount: 20 },\n+       { name: \"bar\", amount: 10, ok: 'FALSE' },\n+       { name: \"baz\", amount: 20, ok: 'FALSE' },\n       ],\n       repeated_string_value: [\"a\", \"b\", \"c\"],\n       oneof_string: \"x\",\n@@ -83,9 +91,13 @@ describe('ABitOfEverythingService', function() {\n       map_value: { a: 1, b: 2 },\n       mapped_string_value: { a: \"x\", b: \"y\" },\n       mapped_nested_value: {\n-        a: { name: \"x\", amount: 1 },\n-        b: { name: \"y\", amount: 2 },\n+        a: { name: \"x\", amount: 1, ok: 'FALSE' },\n+        b: { name: \"y\", amount: 2, ok: 'FALSE' },\n       },\n+      single_nested: null,\n+      enum_value: \"ZERO\",\n+      timestamp_value: null,\n+      repeated_enum_value: [  ],\n     };\n beforeEach(function(done) {\n\ndiff --git a/examples/browser/echo_service.spec.js b/examples/browser/echo_service.spec.js\nindex 97888c3..eca49f9 100644\n--- a/examples/browser/echo_service.spec.js\n+++ b/examples/browser/echo_service.spec.js\n@@ -21,7 +21,7 @@ describe('EchoService', function() {\n           {id: \"foo\"},\n           {responseContentType: \"application/json\"}\n       ).then(function(resp) {\n-        expect(resp.obj).toEqual({id: \"foo\"});\n+        expect(resp.obj).toEqual({id: \"foo\", num: '0'});\n       }).catch(function(err) {\n         done.fail(err);\n       }).then(done);\n@@ -34,7 +34,7 @@ describe('EchoService', function() {\n           {body: {id: \"foo\"}},\n           {responseContentType: \"application/json\"}\n       ).then(function(resp) {\n-        expect(resp.obj).toEqual({id: \"foo\"});\n+        expect(resp.obj).toEqual({id: \"foo\", num: '0'});\n       }).catch(function(err) {\n         done.fail(err);\n       }).then(done);\n```. @philipithomas / @achew22 can you try this?\n```diff\ndiff --git a/examples/browser/a_bit_of_everything_service.spec.js b/examples/browser/a_bit_of_everything_service.spec.js\nindex edcbebe..f99c867 100644\n--- a/examples/browser/a_bit_of_everything_service.spec.js\n+++ b/examples/browser/a_bit_of_everything_service.spec.js\n@@ -34,6 +34,15 @@ describe('ABitOfEverythingService', function() {\n       sint32_value: 2147483647,\n       sint64_value: \"4611686018427387903\",\n       nonConventionalNameValue: \"camelCase\",\n+      single_nested: null,\n+      nested: [  ],\n+      enum_value: \"ZERO\",\n+      repeated_string_value: [  ],\n+      map_value: Object({  }),\n+      mapped_string_value: Object({  }),\n+      mapped_nested_value: Object({  }),\n+      timestamp_value: null,\n+      repeated_enum_value: [  ],\n     };\n beforeEach(function(done) {\n\n@@ -72,10 +81,9 @@ describe('ABitOfEverythingService', function() {\n       sint32_value: 2147483647,\n       sint64_value: \"4611686018427387903\",\n       nonConventionalNameValue: \"camelCase\",\n-\n       nested: [\n-       { name: \"bar\", amount: 10 },\n-       { name: \"baz\", amount: 20 },\n+       { name: \"bar\", amount: 10, ok: 'FALSE' },\n+       { name: \"baz\", amount: 20, ok: 'FALSE' },\n       ],\n       repeated_string_value: [\"a\", \"b\", \"c\"],\n       oneof_string: \"x\",\n@@ -83,9 +91,13 @@ describe('ABitOfEverythingService', function() {\n       map_value: { a: 1, b: 2 },\n       mapped_string_value: { a: \"x\", b: \"y\" },\n       mapped_nested_value: {\n-        a: { name: \"x\", amount: 1 },\n-        b: { name: \"y\", amount: 2 },\n+        a: { name: \"x\", amount: 1, ok: 'FALSE' },\n+        b: { name: \"y\", amount: 2, ok: 'FALSE' },\n       },\n+      single_nested: null,\n+      enum_value: \"ZERO\",\n+      timestamp_value: null,\n+      repeated_enum_value: [  ],\n     };\n beforeEach(function(done) {\n\ndiff --git a/examples/browser/echo_service.spec.js b/examples/browser/echo_service.spec.js\nindex 97888c3..eca49f9 100644\n--- a/examples/browser/echo_service.spec.js\n+++ b/examples/browser/echo_service.spec.js\n@@ -21,7 +21,7 @@ describe('EchoService', function() {\n           {id: \"foo\"},\n           {responseContentType: \"application/json\"}\n       ).then(function(resp) {\n-        expect(resp.obj).toEqual({id: \"foo\"});\n+        expect(resp.obj).toEqual({id: \"foo\", num: '0'});\n       }).catch(function(err) {\n         done.fail(err);\n       }).then(done);\n@@ -34,7 +34,7 @@ describe('EchoService', function() {\n           {body: {id: \"foo\"}},\n           {responseContentType: \"application/json\"}\n       ).then(function(resp) {\n-        expect(resp.obj).toEqual({id: \"foo\"});\n+        expect(resp.obj).toEqual({id: \"foo\", num: '0'});\n       }).catch(function(err) {\n         done.fail(err);\n       }).then(done);\n``. Hi @achew22, unfortunately the patch above doesn't fix all the tests.client_test.goandintegration_test.go` still have to be fixed.. There are more tests to update. Closing it.. ",
    "atombender": "Cool! Naively, would there be any harm in changing the heuristic so that if the body annotation isn't set, we simply pick from both query params and body? HasQueryParam would return true, and as far as I can see, the current logic just works.\n. Maybe I'm misunderstanding you, but with body: \"*\", no query params are used. The generated code never calls runtime.PopulateQueryParameters. Bug?\nI have to admit that the wording in http.proto\u00a0(line 195) is not very clear. It seems to say that if body is *, then fields should be picked from both the query string and the body.\n. I find the spec confusingly worded. Why are query params singled out? A few paragraphs down:\n\n// Note that when using * in the body mapping, it is not possible to\n// have HTTP parameters, as all fields not bound by the path end in\n// the body. This makes this option more rarely used in practice of\n// defining REST APIs. The common usage of * is in custom methods\n// which don't use the URL at all for transferring data.\n\nThis doesn't make sense to me. REST-style POSTs don't preclude query parameters, like so:\n$ curl -XPOST -d'{\"answer\": 42}` http://localhost/someroute?name=bob\nIn the context of gRPC, I don't see any reason not to map name to a body field (the gRPC message).\n. I did give a use case in my original comment. Query parameters are generally more user-friendly than JSON. Let's say I have a big message stored in a file called data.json. I want to post it:\nshell\n$ curl -d @data.json http://localhost/ingest\nSomething goes wrong and I want to debug this. Say my service takes a parameter verbose, which increases logging for that request. Instead of editing data.json (which is nice and clean), I could just do this:\nshell\n$ curl -d @data.json http://localhost/ingest?verbose=true\nHowever, this is not possible.\n. Sure, it would be nice if grpc-gateway had a general-purpose mechanism for mapping headers. For a public-facing API it's a bit ridiculous to require users to format a header such as Grpc-Metadata-Verbose: true.\nHowever, headers aren't very ergonomic, since they are generally treated as out-of-band data. It's common for apps to log URLs and query params, for example, but not headers. With headers it's one more thing to consider.\n. Sure, headers are less ergonomic than query params, but a decent compromise. But grpc-gateway doesn't support a generic mapping of headers, either (that I know of). So you can't argue that headers are better unless you propose that http.proto is extended to support headers. I don't want to expose a header like Grpc-Metadata-Verbose, because gRPC is an implementation detail from the point of view of the API consumer. The header would need to be called Verbose or something similar, and I would need to be able to specify this mapping in my .proto file.\n. Another option is some kind of HTTP annotation to map all unknown query parameters to a single struct declared in the message itself. That would probably also work for us.\n. After an internal discussion, we've decided to drop grpc-gateway entirely for now, so this issue is moot. We have too many challenges in mapping gRPC to a sufficiently ergonomic HTTP API, and it's just simpler for us to hand-code a wrapper for now. We might revisit in the future.\n. ",
    "jayantkolhe": "For the use-case Alexander mentions, we have typically recommended using\nheaders for sending those parameters. We are doing the same in gRPC\n(through metadata) and it would be more aligned if we do the same for REST\nconversion.\nOn Wed, Oct 19, 2016 at 11:37 AM, Travis Cline notifications@github.com\nwrote:\n\n@yugui https://github.com/yugui shouldn't path parameters be included\nbut not query string parameters?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc-ecosystem/grpc-gateway/issues/234#issuecomment-254902690,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGkTt0yF1ot_GUJoq8guGTSmXDqE3ikEks5q1mOAgaJpZM4KXgiY\n.\n. \n",
    "ratsirarar": "What was the consensus on this issue? . ",
    "fdhadzh": "Sorry for my issue. It should be in grpc-go repo.\n. ",
    "afking": "@tmc is there currently no way to propagate HTTP 2* status codes to the rest proxy?. @achew22 custom rule similar too https://github.com/bazelbuild/rules_go/tree/master/proto . Rule generates from proto_library presumably for deterministic builds it removes optional info.. @achew22 I know about that project but don't use it. I'm using the inbuilt proto_library(...) rules which I don't think that project utilises. I then declare a custom macro that selects a bunch of plugin binaries to generate many outputs. I still think SourceCodeInfo is optional and shouldn't cause panics as that is intended behaviour.. Can't point you at the code I'm using but here if you switch in the protoc-gen-swagger as a plugin you will hit that panic!. Yep I'm just using proto_library and not the toolchain rule. No problem, feel free to close.. Updated to print, do you want the original comment message?. @achew22 I added a test, let me know what needs to be updated. Also whats the reason for {\"result\": ...} in the JSON response?. @aelsabbahy yes. Is this behaviour breaking something for you? . This is a breaking behaviour. Can this be put behind a check for json content-type, or being part of the marshaller? This was breaking my code by not being able to control encoding.. @achew22 shall I add it to this change https://github.com/grpc-ecosystem/grpc-gateway/pull/482 ?. @achew22 Its not very clean but would https://github.com/afking/grpc-gateway/commit/5a75208dc77c866ea943abf33a8dc48f10531d17 work? The issue is when not using JSON in the stream. I have a custom marshaler that works similarly to the new ProtoMarhshaler https://github.com/grpc-ecosystem/grpc-gateway/commit/039532580f67bea4448ba779550b8d0d1614db95 but removes the stream map[]s added by https://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/handler.go#L149 .. @aelsabbahy could check if the marshaler satisfies a Delimiter() interface? . Have found the same issue.. @lukasmalkmus fixed, thanks!. Closing, recommendation is to remove all BUILD files for vendor/ dependencies.. Reopening, we are defining go_proto_compilier(...) internally to reference vendor/ deps and require this to be exposed.. So some of our deps for https://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-grpc-gateway/BUILD.bazel#L31 are registered in bazel under //vendor/.... This leads to compile errors as the package types are incompatible.  Not sure if there is a nicer way to do this? The current solution is to point the go library deps at the vendored packages and use the external repositories just for binaries.. @achew22 any idea why this is breaking?. I'm unable to get the correct versions without playing with it more, seems to edit all the examples/.go files.. @achew22 our go_proto_compiler rule looks like:\ngo_proto_compiler(\n    name = \"go_gateway\",\n    options = [\"logtostderr=true\"],\n    plugin = \"@grpc_ecosystem_grpc_gateway//protoc-gen-grpc-gateway\",\n    suffix = \".pb.gw.go\",\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \"//vendor/github.com/golang/protobuf/proto:go_default_library\",\n        \"//vendor/github.com/grpc-ecosystem/grpc-gateway/runtime:go_default_library\",\n        \"//vendor/github.com/grpc-ecosystem/grpc-gateway/utilities:go_default_library\",\n        \"//vendor/golang.org/x/net/context:go_default_library\",\n        \"//vendor/google.golang.org/grpc:go_default_library\",\n        \"//vendor/google.golang.org/grpc/codes:go_default_library\",\n        \"//vendor/google.golang.org/grpc/grpclog:go_default_library\",\n        \"//vendor/google.golang.org/grpc/status:go_default_library\",\n    ],\n)\nI think there will be nicer ways to do this in the future with the rules_go. Maybe something in https://github.com/bazelbuild/rules_go/issues/1548 will fix it.. Yes and other grpc libraries that can cause version conflicts.. Can be set on the clients grpc options and the http handlers like: \n```\nconn, _ := grpc.Dial(addr, grpc.WithStatsHandler(&ocgrpc.ClientHandler{})\nmux  := &ochttp.Handler{Handler: runtime.NewServeMux()}\nservicepb.RegisterRouteHandler(ctx, mux, conn)\n```. ",
    "shenshouer": "change to \n```\nservice Greeter {\n    rpc SayHello (HelloRequest) returns (HelloRespone) {\n    option (google.api.http) = {\n      post: \"/v1/example/echo\"\n      body: \"*\"\n    }\n}\n}\n```\nworked  fine\n. ",
    "lei314121077": "How to fix this?. OK  thanks  regerhub !!   . I also encountered the same problem\uff01\nDoes anyone know how to solve it?\ngoogle/protobuf/descriptor.proto: File not found.\ngoogle/api/annotations.proto: Import \"google/protobuf/descriptor.proto\" was not found or had errors.\ngoogle/api/annotations.proto:28:8: \"google.protobuf.MethodOptions\" is not defined.\nthreeapipb.proto: Import \"google/api/annotations.proto\" was not found or had errors.\nyour need install protobuf 3 on your system \nMake sure you grab the latest version\ncurl -OL https://github.com/google/protobuf/releases/download/v3.2.0/protoc-3.2.0-linux-x86_64.zip\nUnzip\nunzip protoc-3.2.0-linux-x86_64.zip -d protoc3\nMove protoc to /usr/local/bin/\nsudo mv protoc3/bin/* /usr/local/bin/\nMove protoc3/include to /usr/local/include/\nsudo mv protoc3/include/* /usr/local/include/\nOptional: change owner\nsudo chown [user] /usr/local/bin/protoc\nsudo chown -R [user] /usr/local/include/google\n``. sorry  I have solved it. Thanks! @achew22 . ",
    "rogerhub": "@lei314121077 \"google.api.http\" is an extension of MethodOptions, not ServiceOptions. Make sure that your annotations are within the curly braces for an RPC method.\nUsing the examples from above:\nWrong:\nproto\nservice Greeter {\n  rpc SayHello (HelloRequest) returns (HelloResponse) {}\n  option (google.api.http) = {\n    post: \"/v1/example/echo\"\n    body: \"*\"\n  };\n}\nCorrect:\nproto\nservice Greeter {\n  rpc SayHello (HelloRequest) returns (HelloResponse) {\n    option (google.api.http) = {\n      post: \"/v1/example/echo\"\n      body: \"*\"\n    }\n  };\n}. This should be fixed: #277 #462. repeated bytes string_val = 8;\nYour string_val needs to be a list (e.g. \"string_val\": [\"QB........//Z\"]).. As far as I can tell, there isn't a way to specify a repeated message field using GET query parameters alone. Only repeated primitive fields (strings, ints, bools, floats, etc) are supported in query parameters. See runtime/query.go.\nCan you use POST instead?. Doesn't grpc-go already support multiple connections per client through the resolver and balancer packages? If you have multiple endpoints for your service, you could use those to establish multiple connections. Otherwise, h2 already does multiplexing, so what's the point of a connection pool?. https://help.github.com/articles/blocking-a-user-from-your-organization/\nDoes this count as spam/abuse? There are several other PRs with no clear explanation and millions of lines of code added. In #602, the author was asked for an explanation, but did not respond.. Ah, I wasn\u2019t aware that was an option. Thanks for the explanation, and keep up the good work!. It doesn't need to explicitly support h2. You can attach a grpc-gateway handler to the standard net/http Server, which will handle both http/1.1 and h2 (see examples/gateway/main.go for an example).. I don't quite understand the context here, but should there be a TrimSuffix somewhere? Or do we intentionally preserve the -bin?. Should this be in the previous block of imports? (to be consistent with runtime/context.go). \", \" -- has this gone through gofmt?. how come this isn't md[\"test-bin\"]?. ",
    "MalteJ": "I'd love to see this merged!\nWith the current settings my API is sort of broken.. ",
    "nycdotnet": "@achew22 We were just surprised by this at Namely and we have a business CLA signed.  Would you still be willing to merge a PR that addressed this including all related tests?. @achew22 Thank you!\nThe PR you linked is very extensive.  How much of that PR would you say is directly related to this issue?  I understand that since this qualifies as a breaking change, it's a good opportunity to break all the things, but for a first PR I think it might be tough to jump-in midstream on that.  Do you see v2 and #546 as the only way forward?. ",
    "hoffin": "Right-o. Is there a know tricks in getting a non Grpc-Metadata- request headers into the gRPC endpoint within the reverse proxy code without manipulating the headers before it arrives, as these headers are not always available to be set at source.\nJust to ask, is there a specific reason why it wasn't implemented in the way it was described in the documentation as that would solve my issue without having to manipulate the headers before they arrive at the reverse proxy?\n. I would much prefer the documented method. There's a lot less fiddling to get access to the header data and much simpler integration with api gateways.\n. ",
    "idy": "This issue is related with https://github.com/grpc/grpc/issues/5174 and https://github.com/dcodeIO/protobuf.js/issues/476\nI have a repeated field in proto definition, dcodeIO do not treat proto3's repeated field as packed.\n. ",
    "dprotaso": "Take a look at the failure it's unrelated to the diff. Something in\nexamples is importing a package that's main.\nOn Thu, Nov 3, 2016 at 20:28 Travis Cline notifications@github.com wrote:\n\n@dprotaso https://github.com/dprotaso thank you for your contribution\n-- could you look at the test failure and sign the cla?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc-ecosystem/grpc-gateway/pull/247#issuecomment-258314029,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AABIgj_7VJBWyfHiTvKTp4a6cif5t0yyks5q6nwtgaJpZM4Kobpn\n.\n. I rebased but gotip is so fragile - it's failing to fetch and install golint\n. \n",
    "zirkome": "@tmc I updated the README.md is that OK?\n. ping @tmc :)\n. @tmc I don't like this amount of duplication too, but that was the only solution that came to my mind to avoid direct breaking changes. I like the idea of dropping support for versions < go1.7 but like you said it might be a problem if the distribution of < go1.7 is too large. I don't know how we can deal with this. If we're going to support for versions > go1.7 I'd be happy to redo my PR to take this in account.\n. @tmc A thought that I had was to use build tags in order to prefer one API over the other but this involves moving templates in another files (e.g. template_pre17.go and template_go17.go). The gRPC-Go transport package has build tags for pre1.6, 1.6 and 1.7 (https://github.com/grpc/grpc-go/tree/master/transport)\n. @hatchan This PR changed the signature of Register{{$svc.GetName}}Handle but in fact the best to do that would have be to just ignore the context with _. It will be part of the next iteration of this PR after we have a final decision on all that :)\n. @tmc what do you think?\n. @tmc I changed the signature so that it won't break anything. For the std context vs x/net/context this won't break anything as it's just an interface and that x/net/context wraps around the std package when building with Go1.7. \nBtw, the build seems to fail because of genswagger lint issue\n. @tmc This remove a lot of the duplicate code I did, so I'm in favor to close this PR and use your work\n. I agreed to the CLA @googlebot\n. @tmc Is there any progress on that?. ",
    "hatchan": "@kokaz The build tags need to be added to the generated code, right? As it doesn't matter which Go version was used to compile this binary.\nThe generated code could generate two files and then have build tags applied. But the biggest issue is the API signature of the generated code is different for both versions. Whereas the older on expects a context, the new code does not.\n. ",
    "willnorris": "@tmc, as noted in the comment from @googlebot, @christianvozar has already signed the CLA (emphasis added):\n\nWe found a Contributor License Agreement for you (the sender of this pull request) and all commit authors\n\nThem problem is that you are submitting a pull request including code authored by someone else, so we just want that someone else to confirm that they're okay with their code being contributed to the project.  The CLA status won't actually change from it's current state, but it's okay to merge once @christianvozar has indicated agreement.\n. ",
    "fiibbb": "Hi @tmc, does this mean allowing HTTP headers not with grpcgateway- prefix to be also passed to GRPC? If so, I'm interested in this feature. Is it actively being worked on?. @pzsfeng I think you are missing the import part in your protoc command\n--go_out=Mgoogle/api/annotations.proto=github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis/google/api,plugins=grpc:.\nI believe this ^ maps github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis to google/api/annotations.proto\nThis is from the README. This is awesome. Been waiting for 2+ months for this feature. Would like to see this in a new release. (It's kinda unfortunate this missed 1.2.2 by just one commit). That works. Thanks a lot.. ",
    "dmitris": "the current version of swagger-codegen (that you get with brew install for example) is 2.3.1 - CONTRIBUTING.md says \"use swagger-codegen 2.2.2, not newer versions\".  @johanbrandhorst  - would it be possible to make grpc-gateway to work with 2.3.1?  Thanks.. #772 has been merged now, can we remove swagger-codegen now?. related (for reference) - https://github.com/go-resty/resty/issues/178\nalso https://github.com/go-resty/resty/blob/master/go.mod#L1 contains module gopkg.in/resty.v1. @johanbrandhorst - search-replaced in the branch for now\nopened https://github.com/swagger-api/swagger-codegen/issues/8798 - please add the details as needed.  \nIt would be great if we could move to using the latest version of swagger-codegen (2.3.1) as mentioned in #254.  Currently when I try to generate code with swagger-codegen 2.3.1, I'm getting invalid Go generated:\n./examplepb_a_bit_of_everything.go:48:13: undefined: ExamplepbNumericEnum\n./examplepb_a_bit_of_everything.go:50:17: undefined: PathenumPathEnum\n./examplepb_a_bit_of_everything.go:68:22: undefined: ExamplepbNumericEnum\n./examplepb_a_bit_of_everything.go:78:22: undefined: ExamplepbNumericEnum\nIf I understand it correctly, a_bit_of_everything.swagger.json has definitions for the lower-case fields:\n```\n    \"examplepbNumericEnum\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"ZERO\",\n        \"ONE\"\n      ],\n      \"default\": \"ZERO\",\n      \"description\": \"NumericEnum is one or zero.\\n\\n - ZERO: ZERO means 0\\n - ONE: ONE means 1\"\n    },\n    \"pathenumPathEnum\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"ABC\",\n        \"DEF\"\n      ],\n      \"default\": \"ABC\"\n    },\n```\nwhich becomes inexamplepb_numeric_enum.go`:\n// ExamplepbNumericEnum : NumericEnum is one or zero.   - ZERO: ZERO means 0  - ONE: ONE means 1\ntype examplepbNumericEnum string\nand contradicts usage as in the following enum: \n```\n// ExamplepbNumericEnum : NumericEnum is one or zero.   - ZERO: ZERO means 0  - ONE: ONE means 1\ntype examplepbNumericEnum string\n// List of examplepbNumericEnum\nconst (\n    ZERO ExamplepbNumericEnum = \"ZERO\"\n    ONE  ExamplepbNumericEnum = \"ONE\"\n)\n```. ",
    "LeeWong": "protoc -I/usr/local/include -I. \\\n\n-I$GOPATH/src \\\n -I$GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\\n --grpc-gateway_out=logtostderr=true:. \\\n /home/lee/IdeaProjects/go_api_gateway/src/api_gateway/rest/test_service.proto \n/home/lee/IdeaProjects/go_api_gateway/src/api_gateway/rest/test_service.proto: File does not reside within any path specified using --proto_path (or -I).  You must specify a --proto_path which encompasses this file.  Note that the proto_path must be an exact prefix of the .proto file names -- protoc is too dumb to figure out when two paths (e.g. absolute and relative) are equivalent (it's harder than you think).\n. I found a way to hack it. Thanks for creating such a good plugin.\n. \n",
    "yingzong": "\nThis is a little tricky since the marshaler can be customized -- IMO defaulting to the default marshaler &JSONPb{OrigName: true} is the most appropriate behavior here.\n\n@tmc How do you customize the marshaler, specifically the OrigName: false in the --protoc-gen-swagger plugin?. ",
    "heyitsanthony": "Problem looks to be on our side. Sorry for the noise. Closing\n. ",
    "AmandaCameron": "Any chance on getting this merged sometime soon? It'd be very helpful with stuff such as routing tracing through the grpc-gateway to the backend grpc service.. Tracing is what my usecase is for this PR. It's easier to thread the nessary tracing information through a http middleware instead of a grpc serverinterceptor, and more clean, I feel. It'd also allow middleware to do whatever other context manupulation it wants.. ",
    "linuxerwang": "Done the code changes. I think I've already signed the CLA:\nIt looks like you've already signed this CLA. If you'd like to edit your contact information, you may do so below.\nNot sure what to do next.\nThanks!. ",
    "bluehallu": "@tmc Just did :). ",
    "msample": "@tmc hopefully most people won't need it, but I'd be happy to update wherever flags are externally documented if you want.  Please point me at right thing to edit.\nAlternatively, we could upgrade the error message that is currently printed when you try to use a DELETE with a body to mention this option (gets to be a bit of a drag if we add more cases that care if DELETE has a body though).. @achew22 sounds good. I'll try to get to it this weekend.. Two things I left as-is that seem suspect in protoc-gen-swagger/main.go.  Some might depend on this behaviour:\nif *file != \"stdin\" {\n        f, _ = os.Open(\"input.txt\") // <---\" input.txt\"? not *file?\n    }\n\nsupport of pkg map parameters. \"Mfoo/bar/baz.proto=github.com/proj/with/gen/baz-pb-go\" seems unneeded for generating swagger.  Could break some makefiles if it becomes a  param error tho.\n. CLA check kick.. @tmc sure, not a problem. Should probably be applied to both protoc_gen_* anyway.. @tmc kvvar removed.. last commit under my work email.  Have added CLA to that.. \n",
    "kirk91": "@tmc sorry, i haven't notice the swagger comment.. ",
    "pzsfeng": "@fiibbb Thanks for taking a look.\n\nI think you are missing the import part in your protoc command\n\nI didn't quite understand it. Could you please mention how to invoke it? Thanks.\n. ",
    "cappuccino5": "\u697c\u4e3b\u8fd9\u4e2a\u89e3\u51b3\u4e86\u5417\uff1f\u6211\u662f\u65b0\u624b\uff0c\u4e5f\u9047\u5230\u8fd9\u6837\u7684\u95ee\u9898\n\n. ",
    "dong77": "I got it, this method's name depends on my service name.... I'm having this very issue right now. How to fix it ?. ",
    "simjay": "Could you tell me how you fixed this issue? Thanks in advance. Thank you @TamalSaha !. Hi @doubleyou . Could you, if it is not so much a trouble, give me some example project that you could successfully run with grpc-gateway on Python? I have been trying to make this happen and tried all of your advices above but not getting anything working. Thanks a ton in advance.. ",
    "doubleyou": "For what it's worth, it could have been as simple as adding a directory with the annotations_pb2.* and http_pb2.* modules into the project. However, due to the package name (https://github.com/grpc-ecosystem/grpc-gateway/blob/master/third_party/googleapis/google/api/annotations.proto#L17), the languages would expect the package google with a module named api. google pip package, however, already exists, so we can't just create a google/api directory in the project's directory, since it would basically override the installed google package, if it exists.\nIf inside the definitions if was something like this instead:\nimport \"grpc-gateway/annotations\";\nthen importing would have been trivial enough. I could see, however, this being a problem for the Golang ecosystem. Also, switching to the new layout would be a breaking change.. So, there's one way to make it work specifically for Python. If we generate the Python code from annotations.proto and http.proto, put them into a local nested google/api/ directory, and then add this to the local google/__init__.py:\nimport os\nimport pkgutil\nstdlib_dir = os.path.dirname(os.__file__)\nreal_google_path = os.path.join(stdlib_dir, 'site-packages', 'google')\n__path__.append(real_google_path)\nexecfile(os.path.join(real_google_path, '__init__.py'))\nThen it will import both the native google package and the local one.\nI'm still trying to search for any related discussions in the Protobuf community, but this may be at least a decent solution specifically for Python. Also, easy to automate.. ",
    "jackwootton": "I'm unsure why this was closed. I'm facing this exact issue with Python. The only GRPC example I can find doesn't work. Opened issue here https://github.com/GoogleCloudPlatform/python-docs-samples/issues/1103. ",
    "navidnabavi": "Installing 'googleapis-common-protos' package from pypi will solve the problem.\npip install googleapis-common-protos. ",
    "dingrui37": "Hi @doubleyou could you give me a guide about how to use grpc-gateway in python? Thanks. ",
    "lipixun": "All right. I have signed the CLA.\nIt seems a little hard to write test code into current test cases, the tests are ran on a mock output of protoc but this pr focus on fix the issue of integration with protoc-go.\nHere's the test case:\nWrite a protobuf file:\n```\nsyntax = \"proto3\";\npackage test;\nimport \"google/api/annotations.proto\";\noption go_package = \"testpkg\";\nmessage SomeRequest {\n    int32 start = 1;\n    int32 size = 2;\n}\nmessage SomeResponse {\n    repeated string items = 1;\n}\nservice SomeService {\n    rpc list(SomeRequest) returns (SomeResponse) {\n        option (google.api.http) = {\n            post: \"/resource/list\"\n            body: \"*\"\n        };\n    }\n}\n```\nPay attention to the method list not List, and run protoc protoc -I . -I $GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis --go_out=plugins=grpc:. test.proto\nThis will generate the go source code file with the following codes:\ngo\nfunc (c *someServiceClient) List(ctx context.Context, in *SomeRequest, opts ...grpc.CallOption) (*SomeResponse, error) {\n    out := new(SomeResponse)\n    err := grpc.Invoke(ctx, \"/test.SomeService/list\", in, out, c.cc, opts...)\n    if err != nil {\n        return nil, err\n    }\n    return out, nil\n}\nThe method name is List not list.\nRun the grpc gateway compile command: protoc -I=. -I $GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis --grpc-gateway_out=logtostderr=true:. test.proto\nThe generated grpc gateway code:\n```\nfunc request_SomeService_list_0(ctx context.Context, marshaler runtime.Marshaler, client SomeServiceClient, req *http.Request, pathParams map[string]string) (proto.Message, runtime.ServerMetadata, error) {\n    var protoReq SomeRequest\n    var metadata runtime.ServerMetadata\nif err := marshaler.NewDecoder(req.Body).Decode(&protoReq); err != nil {\n    return nil, metadata, grpc.Errorf(codes.InvalidArgument, \"%v\", err)\n}\n\nmsg, err := client.list(ctx, &protoReq, grpc.Header(&metadata.HeaderMD), grpc.Trailer(&metadata.TrailerMD))\nreturn msg, metadata, err\n\n}\n```\nTake a look at client.list, it should be client.List. And what I've committed is to convert the first letter from lower case to upper case.\nThe protobuf official document says:\nThe first letter is capitalized for export. If the first character is an underscore, it is removed and a capital X is prepended.. I signed it!. I signed it!. ",
    "ilius": "Go's \"net/http\" package is HTTP/1, and the context it has is as in \"context\" package\nGRPC is HTTP/2 and it's context is  \"golang.org/x/net/context\"\nPackage \"context\"  is not even imported the .go files generated by protoc. @tmc Yes I signed yesterday but haven't received a response yet. cla/google is checked\nBut I don't understand anything from ci errors. I don't know why 'make examples' is not updating examples for me! Even though I removed and re-built and re-installed \"protoc-gen-grpc-gateway\" many times.... Yes but the \"return\" does not appear in the examples. @TamalSaha Not sure. Does it support returning error (in REST response) before reaching grpc client?\nWe need to read request cookie, set grpc context or return error, on each request\nAs I have done in #323. - We use metadata for putting context item (JWT token), because otherwise it won't be transferred via grpc\n\n\nWe reject the call if the cookie is missing, or invalid (invalid signed JWT token). @TamalSaha I don't understand. We have no access to cookie in our grpc server handlers. Only context and input(request). We must use context for storing token. Cookie is only for REST (http1). So we must copy the token from cookie to GRPC context when translating REST to GRPC.. And where to put this header in the grpc context?. Thanks. @TamalSaha Sorry, I think runtime.WithIncomingHeaderMatcher transforms headers, not cookies. Thanks, I get it now (the name mistakened me). I'm not sure it's related to this, but I think we should be able to add one specific cookie (or all of them) to the incoming metadata.MD of context, so that we don't have to iterate over md[\"cookie\"], every time.\nSpecially inside the grpc ecosystem (where there is no http1/REST) in micro-service architecture, we frequently set and get a specific metadata key, and we don't want to iterate over md[\"cookie\"] or append to it (or both) every time (that's kind of stupid). Even the term cookie should not be used when you are inside grpc ecosystem. Cookie is for HTTP1 / REST, not GRPC.. We don't expose grpc to the outside world (only for inter-service communication), we only expose REST to outside. So I will try the second approach.. The second approach does not seem to be working for us.\ns.grpcMux = runtime.NewServeMux(runtime.WithMetadata(func(ctx context.Context, req *http.Request) metadata.MD {\n    tokenCookie, _ := req.Cookie(\"token\")\n    if tokenCookie == nil {\n        return nil // metadata.Pairs()\n    }\n    fmt.Printf(\"WithMetadata: tokenCookie: %#v\\n\", tokenCookie) // correctly set\n    return metadata.Pairs(\"token\", tokenCookie.Raw)\n}))\n\n\nBut I get this metadata:\nmd=metadata.MD{\"token\":[]string{\"\"}, \"grpcgateway-cookie\":[]string{\"token=.......\"}, ...}\n\nIt gives empty \"token\":[]string{\"\"}\n. I also tried with both ServeMuxOptions to NewServeMux, WithMetadata only add an empty context key.. My apology.\nI should have used cookie.Value\nThanks. @tmc I didn't find a library to parse .proto file. And parsing is not enough, we also need the input (request) and output (response) structs for each method. (so we need to import the package containing grpc-gateway generated files)\nSince grpc-gateway extends the proto format (by adding option (google.api.http)), i think it should ideally extend grpc.ServiceDesc as well, and add those REST-specific method properties (new fields, or simply a new function like serviceDesc.GetMethodDescriptions()). Let's say I have a proto file like this:\n```\n    syntax = \"proto3\";\npackage ping;\n\nimport \"google/api/annotations.proto\";\n\nservice Ping {\n    rpc Ping(PingRequest) returns (PingResponse) {\n        option (google.api.http) = {\n            get: \"/ping/ping\"\n        };\n    }\n}\n\nmessage PingRequest {\n}\n\nmessage PingResponse {\n}\n\n``\nCan you give me some Go sample code that I can access the information in theoptionblock of a given method (\"Ping\") ?. @achew22 Thank you for the code.\nI get this error inproto.Unmarshal:bad wiretype for field descriptor.FileDescriptorProto.Dependency: got wiretype 7, want 2. Turns outproto.FileDescriptorreturns gzipped bytes, and I had to uncompress it before passingproto.Unmarshal`.\nProblem is solved!. ",
    "arun0009": "Right now, all the responses show up as 200, but it would be nice to have other response messages like 404, 500, 201 & other http response codes with user specified descriptions. I will try and submit a PR over the weekend.. @shilkin : Thanks for making this generic. I tried to run the instructions you have in README, I think you need to change\n--go_out=Moptions/middleware.proto=github.com/grpc-ecosystem/grpc-gateway/options\nto\n--go_out=Moptions/method.proto=github.com/grpc-ecosystem/grpc-gateway/options. ",
    "morriswinkler-simple": "Here is a more literate example of the idea:\n\"paths\": {\n    \"/health-check\": {\n      \"get\": {\n        \"summary\": \"Returns health status\",\n        \"operationId\": \"HealthCheck\",\n        \"responses\": {\n          \"200\": {\n            \"description\": \"\", \u2b05\ufe0e // no description possible here\n            \"schema\": {\n              \"$ref\": \"#/definitions/protobufEmpty\"\n            }\n          }\n        },\n        \"tags\": [\n          \"GardenOpenService\"\n        ]\n      }\n    },\nwould be nice if within the proto file we could add \n// Returns health status\n    rpc HealthCheck (google.protobuf.Empty) returns (google.protobuf.Empty) {\n        option (google.api.http) = {\n       // Really healthy to get some description  \u2b05\ufe0e\n       get: \"/health-check\"\n    };\n    }\nwhich could translate to \n\"paths\": {\n    \"/health-check\": {\n      \"get\": {\n        \"summary\": \"Returns health status\",\n        \"operationId\": \"HealthCheck\",\n        \"responses\": {\n          \"200\": {\n            \"description\": \"Really healthy to get some description  \u2b05\ufe0e\",\n            \"schema\": {\n              \"$ref\": \"#/definitions/protobufEmpty\"\n            }\n          }\n        },\n        \"tags\": [\n          \"GardenOpenService\"\n        ]\n      }\n    },\nWhat do you think, is this information passed along the proto services slice?. ",
    "shilkin": "First of all excuse me for my english:-)\n\nwhat is the line of responsibility between handlers such as this and grpc interceptors?\n\nIf I undestand you correctly we speaking about things like this or this.\nGrpc interceptors are designed for working on side of grpc-server, my handlres are actually http handlers and they responsible for processing http requests on side of gateway. For example, I want to declare should gateway check session for partical route or not. Current design of registering function allows me to do it only for the whole mu\u0445, as it's shown in examples (allowCORS). But I want to customize it for every single http handler.\n\nwhat other types of method options do we anticipate? should those influence any design decisions here? (a recent example is specifying additional possible http return codes in swagger gen)\n\nWell quick answer on the first question: I don't know:-) But we can use more complex data structure for method option (instead of simple string). This structure may be a point of extension.\n\ncan/should we support this at the service level? file level?\n\nI think we shouldn't. I prefer to think about exactly method options. If some day we will need to extend service or file options, we will have to declare a new structures. \nSpeaking about http middlewares on service layer. We can do it on service layer without any code generation. All we need is to wrap our mux with another http handler (allowCORS again). So I see no reason to extend service options at the moment. Current design of generated registering functions is quite simple and flexible enough.\n. @tmc, @arun0009, I refactored method options. Now it's a structure and we can extend it. I have to change readme again:-) . @tmc Please, look at current versoin.. Sorry.. The most appropriate is 'options' but this name is busy and deprecated.\nI can move this extension to options.proto\nWhat can you suggest?\nAnyway we can declare different options in different files.. ",
    "ggeorgiev": "@tmc Hi, I will try to find some time to do so. Considering that I am not familiar with the code base and the usual bootstrap overhead it might take some time. Meanwhile, someone that is working on the project probably will have success to reproduce the issue considering that there is nothing tricky for the scenario to expose it and it is 100% reproducible.\n. I did not find where to add such test, but this code seem suspicious because it seems to analyses system characters over already decoded version of the path.\n func (s *ServeMux) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n    path := r.URL.Path\n    if !strings.HasPrefix(path, \"/\") {\n        OtherErrorHandler(w, r, http.StatusText(http.StatusBadRequest), http.StatusBadRequest)\n        return\n    }\n\n    components := strings.Split(path[1:], \"/\")\n    l := len(components)\n    var verb string\n    if idx := strings.LastIndex(components[l-1], \":\"); idx == 0 {\n        OtherErrorHandler(w, r, http.StatusText(http.StatusNotFound), http.StatusNotFound)\n        return\n    } else if idx > 0 {\n        c := components[l-1]\n        components[l-1], verb = c[:idx], c[idx+1:]\n    }\n\n. This is a comment in the URL package:\n// Note that the Path field is stored in decoded form: /%47%6f%2f becomes /Go/.\n// A consequence is that it is impossible to tell which slashes in the Path were\n// slashes in the raw URL and which were %2f. This distinction is rarely important,\n// but when it is, code must not use Path directly.\nI replaced in the quoted before code path := r.URL.Path with path := r.URL.RawPath and the not found problem got solved. Of course, the components, need to get decoded after the split in order this to works as before.. The problem actually is way more serious from what I noticed initially. Essentially any value in the path that includes '/' will completely mess up the components.. ping - is someone looking at this? Seems like a serious issue. . ",
    "jessesuen": "We were also hit by this issue. I submitted PR #660 which allows encoded slashes as part of a path.. I signed it!. I will most definitely add a test. I just wanted to solicit feedback on this to see if this would be something acceptable upstream before investing more time in this.. @achew22, @ivucica -- I added a unit test which can reproduce broken behavior (before my fix) and verifies the desired behavior (after my fix). \nHowever, I am having trouble understanding the remaining node integration test failures: \n1) ABitOfEverythingService Create should assign id\n1.1) Failed: Object({ url: 'http://localhost:8080/v1/example/a_bit_of_everything/1.5/2.5/4294967296/separator/9223372036854775807/-2147483648/9223372036854775807/4294967295/true/strprefix%2Ffoo/4294967295/2147483647/-4611686018427387904/2147483647/4611686018427387903/camelCase', method: 'POST', headers: Object({ content-type: 'text/plain; charset=utf-8' }), errObj: Error: Not Found, status: 404, statusText: 'Not Found\n', data: 'Not Found\n' })\n1.2) Error: Timeout - Async callback was not invoked within timeout specified by jasmine.DEFAULT_TIMEOUT_INTERVAL.\n2) ABitOfEverythingService Create should echo the request back\n2.1) Failed: Object({ url: 'http://localhost:8080/v1/example/a_bit_of_everything/1.5/2.5/4294967296/separator/9223372036854775807/-2147483648/9223372036854775807/4294967295/true/strprefix%2Ffoo/4294967295/2147483647/-4611686018427387904/2147483647/4611686018427387903/camelCase', method: 'POST', headers: Object({ content-type: 'text/plain; charset=utf-8' }), errObj: Error: Not Found, status: 404, statusText: 'Not Found\n', data: 'Not Found\nIt looks like this ABitOfEverything service has a {string_value=strprefix/*} as part of the path option. However, I don't understand why this wouldn't just be {string_value}.\nrpc Create(ABitOfEverything) returns (ABitOfEverything) {\n        // TODO add enum_value\n        option (google.api.http) = {\n            post: \"/v1/example/a_bit_of_everything/{float_value}/{double_value}/{int64_value}/separator/{uint64_value}/{int32_value}/{fixed64_value}/{fixed32_value}/{bool_value}/{string_value=strprefix/*}/{uint32_value}/{sfixed32_value}/{sfixed64_value}/{sint32_value}/{sint64_value}/{nonConventionalNameValue}\"\n        };\n    }\nIt seems that the test case itself would need to change. Can someone explain the meaning behind {string_value=strprefix/*} ?\n. Sorry, I was OOO the past week. If you mean moving strprefix/ outside the capture, I will be happy to make this change. I'll update the PR.. Alternatively, do you believe that strprefix has any value in the example? In other words, could we simplify a_bit_of_everything_service.spec.js from:\npost: \"..../{bool_value}/strprefix/{string_value}/{uint32_value}...\"\ninto this:\npost: \"..../{bool_value}/{string_value}/{uint32_value}...\"\n. ",
    "Aleksion": "Thanks Travis!\nThat was exactly what we ended up doing. \nIf we can't generalize it, we'll try to get something added back to this repo. \nIf this works out for us, I imagine us using this on a lot of projects going forward. . ",
    "mayank-dixit": "It can be done using openapiv2_swagger option (ref)\noption (grpc.gateway.protoc_gen_swagger.options.openapiv2_swagger) = {\n    security_definitions: {\n        security: {\n            key: \"BasicAuth\";\n            value: {\n                type: TYPE_BASIC;\n            }\n        }\n    }\n}\nThen add security header reqs in all services wherever needed (ref)\noption (grpc.gateway.protoc_gen_swagger.options.openapiv2_operation) = {\n    security: {\n        security_requirement: {\n            key: \"ApiKeyAuth\";\n            value: {}\n        }\n};\nDon't forget to import \"protoc-gen-swagger/options/annotations.proto\";\n. @bjolletz did you finally figure out a fairly acceptable way to upload files using grpc-gateway as your app's main entry point?. @bjolletz What did you end up doing for file uploads for your project, then?. > With this proto definition\n\n```\n message UpdateMessage {\n    string id = 1;\n    map[string]string my_map = 2;\n }\nservice YourService {\n  rpc Echo(UpdateMessage) returns (StringMessage) {\n    option (google.api.http) = {\n      patch: \"/update\"\n      body: \"*\"\n    };\n  }\n }\n``\nI'd like to send a request like{\"id\":\"myid\",\"my_map\":{}}`\nand set MyMap to an empty map.\nIs this possible? Currently, when this field is provided, my service thinks this field is unset(MyMap==Nil). I'm working in golang.\n\n@ Ianwww @achew22  Use this maybe: https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/StringValue. @ubenzer what did you end up doing? WithOutgoingHeaderMatcher worked or you ended up using some other approach?. @johanbrandhorst @jriecken \nWith this change, can this be done:\n```\nservice Greeter {\n  rpc SayHello (HelloRequest) returns (HelloReply) {\n    option (google.api.http) = {\n      get: \"/say/{strVal}\"\n    };\n  }\n}\nmessage HelloRequest {\n  google.protobuf.StringValue strVal = 5;\n}\n```\nRight now when I compile using protoc:\nprotoc -I/usr/local/include -I. \\\n  -I$GOPATH/src \\\n  -I$GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\\n  --grpc-gateway_out=logtostderr=true:. \\\n  helloworld/helloworld.proto\nI get the error:\n--grpc-gateway_out: aggregate type TYPE_MESSAGE in parameter of Greeter.SayHello: strVal. > That should absolutely work, I can't see why that is erroring. Could you raise an issue please?\nCreated: https://github.com/grpc-ecosystem/grpc-gateway/issues/808. Yes, I tried to debug:\nError is thrown from:\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-grpc-gateway/descriptor/services.go#L221\nWhere it doesn't find *target.TypeName(.google.protobuf.StringValue) to be a well known type.\nRecognised well-known types are added here:\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-grpc-gateway/descriptor/types.go#L453. I'm adding other types. It's passing. I'll verify end to end once. if all goes fine, I'll raise a PR?. ",
    "fische": "The example I've put above is a failing test case.\nIt tries to import the empty.proto which defines the message type Empty. However the method of Service, that uses this message type, does not define the option google.api.http, so that it does not get exposed by the grpc-gateway. During the code generation of the gateway, protoc-gen-grpc-gateway will add anyway the import of the empty package to the pb.gw.go, even if it is not used.\nEdit: I have quickly checked this issue with a mate. It seems that it appeared between 5e412412babc3c5012f446b2949d9dbadb10576f and 199c40a060d1e55508b3b85182ce6f3895ae6302.. Normally I've already signed it.. No problem. Just trying to fix travis tests and it should be fine.. I still don't understand why 7f2e9507539a786703a38d25ad801b04b0dd833b did not pass the tests on travis...\nAnyway, it should be alright now @tmc.. Ping for cla/google. ",
    "lucas-natraj": "@tmc just curious why the forwarding of headers would be a backwards incompatible change? \ndo you just mean that headers that were being stripped out before would now pass through?\nnot too sure where that would be a problem though.\nas for the collisions in the 'grpc-*' space, couldn't we just strip out all incoming grpc-* headers. that appears to be the same approach that google takes on gcp to prevent external requests appearing to look like they came from Google's own internal proxies.\nalthough i can appreciate a need for additional customizability, i curious whether a simpler pass-through (+ stripping headers with reserved prefixes) wouldn't suffice.\n. perhaps i'm referring to something tangential but iirc the code seems to indicate 2 special prefixes.\nthe one that i'm using is the Grpc-Metadata- which is how I am able to to send custom headers (metadata) to my grpc backend. the important header i'm sending is an api key which is pretty important. to get that through the gateway i need to send it as Grpc-Metadata-x-api-key instead of just x-api-key. hence my desire to have custom headers pass through.\nis this something conflicting with what you were mentioning?. @TamalSaha yeah i did see that, and i can totally see the usefulness of it. my comment was just questioning whether we could just allow custom headers directly (which appears to be the main issue at hand). seems like the only concern was that there would be collisions with an existing prefix. hence my suggestion to strip out any conflicting headers but still allow the ones that don't conflict.\nagain, i may be missing the point entirely. \ud83d\ude04 . hmm. i'm a tad confused..\n authorization, x-forwarded-host and x-forwarded-for -> special handling\n permanent -> gets a grpcgateway- prefix  [e.g. origin -> grpcgateway-origin)\n* Grpc-Metadata-* -> the prefix is stripped [e.g. Grpc-Metadata-x-api-key -> x-api-key)\ni can see the need for adding a grpcgateway- prefix for headers that the gateway plays an active role in supplying -> so just blacklist any incoming headers that have a prefix of grpcgateway-.\nthen there are some headers that just don't make sense -> add them to a blacklist.\nall other headers, however, seem pretty ok to just pass through unaltered.\ni've read through the short discussion on #213 (perhaps there were other issues where this was hashed out?), but i'm still unsure why a whitelist approach was adopted, rather than a blacklist. was there a specific problem that the whitelist approach addresses?\neither way, i'm late to the discussion, so i'll take whatever i can get.. \ud83d\ude04 . yeah, i do understand..\nmy primary concerns with the current approach -\n on the REST client side it's kinda poopy (technical term) for callers to use a special prefix like Grpc-Metadata- just so that the server receives it appropriately. so, i'm not too sure who that special prefix is for, especially since it seems to leak out implementation details to callers. (refer #245)\n on the grpc server side, receiving metadata with a grpcgateway- prefix is equally strange because the server doesn't / shouldn't care that there is a REST gateway in front of it.\nso, at this point, i'm not entirely sure why either prefix (Grpc-Metadata- or grpcgateway-) actually exist.. \ud83d\ude04 \ni think that the pr from @TamalSaha does improve things, but i'm just concerned that it is really just layering on a patch for something that needn't have been there in the first place.. (if that makes sense..)\ngood discussion though.. but i'm happy to accept whatever.. it's not that big a deal anyway.. yeah, i understand that map[string]interface{} isn't really supported. i was just using it as an example of handling arbitrary content.\nthe struct message does, in fact, use oneof and I am able to receive it on the grpc service side.\nhowever, when using the gateway, i have to send the json request body as \njson\n{\n  \"key\": \"xyz\",\n  \"data\": {\n    \"fields\": {\n      \"name\": {\n        \"stringValue\": \"joe\"\n      },\n      \"age\": {\n        \"numberValue\": 28\n      }\n    }\n  }\n}\nwhich, while not ideal, does actual get parsed correctly by the gateway.\ni was hoping there was a way to achieve the more natural json representation I mentioned earlier.. hmm.. somewhat doable, although things quickly get mucky with nested objects.\noh well.. guess i'm outta luck.\nthanks for your time @TamalSaha.. ",
    "nilium": "@tmc Yes, I have. ~I guess the checker isn't working right now?~ Nevermind, is now.. Haven't seen it do so in tests. I could add a nil check around its uses, but an enumeration with no properties seems like bad code gen more than anything else. I'll poke around a bit and see what I can find.. It looks like the only case where it's nil would be if the reflect.Value is invalid. If the reflect.Value is invalid, then the caller returns an error (and in this case, there's only one place that fieldByProtoName is called). Could also add a prop == nil check to where it's being called, though? That'd make doubly sure we've got a proto field, I just don't know if it's paranoid vs. practical.. Considering making a commit to revert this particular line. It got changed as part of hacking on this a bit (before fieldByProtoName returned an error). I could also amend the commit and submit a new pull request. Does it seem worthwhile to do either?. Amended commit force-pushed. Didn't realize GitHub supported that now. (Has it always?). I think this could be handled without the ToLower by doing an EqualFold on the head of the key. For example, https://play.golang.org/p/lh_JIYnZ0s\nI only mention this because EqualFold is a bit more involved than comparing two lowercase strings. This may not really matter, though, since I think the character set available to HTTP headers is fairly restrictive (i.e., ASCII). So, nitpicking a bit.. ",
    "sagikazarmark": "Something like ?filters[key]=value would be nice. Okay, I realized I was using the deprecated way of googleapis import instead of github.com/googleapis/googleapis. This looks like problem solved.. I added the \"old\" googleapis import path from this repo to the protoc command: -I vendor/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis\nAdding the official googleapis repo as a dependency solved the problem. See the solution here.. ",
    "Mr-Giraffe": "@tmc thanks for your reply.\nMaybe the \"non-standard\" is not the right word. When we design our restful api, we try to follow the format of some existing and popular style.\nFor example, the github restful V3 api. In their design, the list is just return an array on the top level.\nThis kind of design has one advantage, which makes it easier to share the object. i.e., the get api returns A, and the list api returns a list of A. A is shared. Some restful freamwork also expact this kind of response.\nCurrent style makes sense when the response contains other thing rather than a pure list. I notice some google restful api belongs to this kind of style: https://developers.google.com/gmail/api/v1/reference/users/drafts/list\nAnyway, if you guys regard this is not the scope of this project, we could find some work around.. ",
    "whisper-bye": "@Mr-Giraffe \nhow do you resolve this problem?. ",
    "flisky": "Googlebot seems dead. I've signed the google individual cla, just FYI. . ping @tmc . Actually, I already contributed. Same slow CLA check last time, and no idea why.. ping @TamalSaha . ",
    "zeiler": "We're blocked by this as well. Any updates? . ",
    "campoy": "Regarding the CLA, I'm a Googler.. ",
    "patrickwmcgee": "Hello wonderful grpc-gateway maintainers! I was wondering what the state of this issue is. Currently the Readme.md has the line\n\nEnum fields in path parameter (including repeated enum fields). \n\nThis made me think I could use Enum fields in the request path via options like\nrpc Echo(MyMessage) returns (SimpleMessage) {\n    option (google.api.http) = {\n        post: \"/v1/example/echo/{my_enum}\"\n    };\n}\nWhere my_enum is an Enum that is used in MyMessage.\nIf this is not the case, perhaps the Readme.md could be updated?\n. @c4milo I'd love your Makefile & jq magic until this gets resolved in the actual tooling :). ",
    "llvim": "@tmc \nbash\ngo get -u github.com/philips/grpc-gateway-example\ngrpc-gateway-example serve\n\nno buildable Go source files. its philips/grpc-gateway-example's makefile not update, my problem solved. oneof sematics is a pain point for us. @yuhang2 not yet. ",
    "marcusljx": "As stated in the above makefile example, this was due to an outdated go_out= parameter in makefiles or //go:generate commands.. Actually this is a problem we're having over at my workplace with google.protobuf.Timestamp, which, when using protoc-gen-swagger, is converted to the following OpenAPI v2 definition:\ndefinitions:\n......\n      some_timestamp:\n        type: string\n        format: date-time\nThis assumes that the JSON string only fulfils one of these cases:\n- non-existent key (key-value pair not even appearing in the JSON object)\n- empty string (\"\")\n- filled string (\"some_value\")\nThe cases above ignores the edge case that a JSON timestamp can also possibly be null like so:\n{\n    \"some_key\" : \"some_value\",\n    \"some_bool\" : true,\n    \"some_timestamp\" : null\n} \nAs mentioned by @zheng1, go-swagger supports this case through the x-nullable vendor extension (https://github.com/go-swagger/go-swagger/issues/1491).\nIt would be great if protoc-gen-swagger could have an option flag or something for generating the x-nullable configurations for google.Protobuf.Timestamp.. > I think actually this x-nullable extension should probably be enabled on all non-scalar types. I assume message types are already nullable, so maybe the safest thing is to have a whitelist in the generator that inserts this property. Is there anything I can do to help you get a PR with this in @marcusljx?\nI don't mind trying my hand at this @johanbrandhorst, but I'm afraid I'm not sure where to begin.. x-nullable is not restricted to OpenAPI v3, actually:\nhttps://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#patterned-objects\nThanks for the contribution steps! . ",
    "CaptTofu": "Made changes. Also, I am under HPE's Google Corporate CLA. Hi there - I think I made all the changes that were requested. Also, I'm covered under the CLA - not sure why that is giving a \"status to be reported\". I signed it!. will fix :). will do!. yes. The reason I chose explicit packages is I kept adding whatever the results of \"go build\" would complain about such as in \"cannot find package \"google.golang.org/grpc\"\nI didn't know about \"go get .\". I'll just simply use that as I tested that it works.\n. ",
    "jdoliner": "@tmc, thanks for the quick response. That probably explains it here's our invocation:\nprotoc \\\n    -I${GOPATH}/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\\n    -I${GOPATH}/src/github.com/gogo/protobuf \\\n    -I${GOPATH}/src \\\n    -Isrc \\\n    --gogo_out=plugins=grpc,\\\nMgoogle/api/annotations.proto=github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis/google/api,\\\nMgoogle/protobuf/duration.proto=github.com/gogo/protobuf/types,\\\nMgoogle/protobuf/empty.proto=github.com/gogo/protobuf/types,\\\nMgoogle/protobuf/timestamp.proto=github.com/gogo/protobuf/types,\\\nMgoogle/protobuf/wrappers.proto=github.com/gogo/protobuf/types,\\\nMgogoproto/gogo.proto=github.com/gogo/protobuf/gogoproto,\\\nMclient/pfs/pfs.proto=github.com/pachyderm/pachyderm/src/client/pfs,\\\nMclient/pps/pps.proto=github.com/pachyderm/pachyderm/src/client/pps,\\\nMserver/pfs/fuse/fuse.proto=github.com/pachyderm/pachyderm/src/server/pfs/fuse,\\\n:src \\\nfoobar.proto\nLooks like the incorrect \"M\" mapping we're using is the one for annotations? What should we be using instead?. @tmc seems to have done the trick :), many thanks.. ",
    "warrenzhu25": "I think customize marshalers can stisfy my use case. But I can't find documentation about how to use this feature. Could you show me how to use customize marshalers? Thanks a lot.. ",
    "srikrsna": "mux := runtime.NewServeMux(runtime.WithMarshalerOption(runtime.MIMEWildcard, &runtime.JSONPb{OrigName:false}))\nThis should do the trick for camelCase. @achew22 Sure. Should I add it somewhere here?. #334 . @achew22 How can changing docs break the build?. Apologies @johanbrandhorst I've tried to do this several times but some reason it constantly fails. I am closing this request. . ",
    "nicerobot": "@tmc @c4milo Sorry, I don't understand why this is \"fixed\" by lightningnetwork/lnd#169 nor where it's fixed in #340 \nThe issue is that the folder\ngithub.com/grpc-ecosystem/grpc-gateway/third_party/googleapis/google/api\n\nno longer contains the generated http.pb.go and annotations.pb.go so when, for example, including annotations into a .proto file, the error occurs:\nno buildable Go source files in .../github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis/google/api\n\n. ",
    "baopham": "@nicerobot You are probably using the old go_out option, if you follow this fix https://github.com/philips/grpc-gateway-example/pull/15 and regenerate the files, it should work.. ",
    "tgulacsi": "I do have a working JSON and SOAP proxy for gRPC.\nI've moved with json to grpc-gateway, and wanted to move with the SOAP proxy, too.\nI can generate the WSDL and have the Marshal/Unmarshal code for the SOAP HTTP handler.\nI wanted to get some suggestions how to integrate it into grpc-gateway.\nA SOAP bridge is just as appropriate as a JSON bridge - although I do hate SOAP, I have to support it.\nBut if this is too much of a maintenance hurdle and nobody is interested in SOAP, I'll keep that generator and handler separate.. @achew22 you're right, grpc-gateway and soap-proxy has no common code yet, but that can change easily: beside the wsdl generating code, the HTTP server that receives the json/soap, decodes it, calls the gRPC endpoint, encodes the response and sends it back has a lot in common. \nAnd grpc-gateway can become a nice general gateway to gRPC - in this form this is just a json proxy, tbh.\nI have code for XML-RPC, too :)\nI do understand though if we keep grpc-gateway almost as-is, and provide these additional encoders/decoders as some sort of plugins/additions.. ",
    "gedw99": "@tmc i would def use this !!!!\n@tgulacsi this sounds really useful. I hate SOAP too but have to use it ...\n. @tgulacsi \ni was wondring if you would be ok with putting a working example for the WSDL and xml-rpc in your repo ? From the looks of it its expecting a PB thats not in the source.\nI am playing around with different gateways just like your code does. So having the REST with WSDL and xml.rpc is really useful. \nOn other thing i am looking at trying out with code gen, which your currently doing from the PB, is to map the PB types to Minio for durable storage and also incorporating NATS with it. \n. ",
    "NeoCN": "Definitely support this! Expect just passing -grpc-gateway_out=\"$GOPATH/src\" and then have  things Just Work. . ",
    "roadrunner": "official go_package syntax is the same way: <FQ go package>;<short name>\nplease support this.. @ivucica i'm using 1.2.x for backward compatibility. no-chance to upgrade yet..\n. +1. ",
    "ezotrank": "I realized that is not necessary.. ",
    "mrt181": "will do so later today. #347 . Already done?\nAgreement   Name    Date Signed\nGoogle Individual CLA   Martin Thomalla     Apr 04, 2017 14:28 PDT. I would suggest to reject this PR. \nWhile redoing the installation on another machine I experienced a more involved installation process for cygwin. I have created a wiki page for cygwin installation documenting the different steps that were required to go install an grpc-gateway service.. the protoc command creates the stubs in the same directory as the service.proto file. is it possible to provide an output directory for the protoc plugins?. ",
    "maxkondr": "Is here any news? Any examples,docs will be appreciated . @uschen  If I understood correctly, stackdriver works in google cloud and AWS only. I want to use tracing in own solution.. there is a ready for use solution from github.com/opentracing-contrib/go-stdlib/nethttp lib:\n```\nimport \"github.com/opentracing-contrib/go-stdlib/nethttp\"\n...\nreturn http.ListenAndServe(fmt.Sprintf(\":%d\", myPort),\n        nethttp.Middleware(\n            opentracing.GlobalTracer(),\n            mux,\n            nethttp.MWComponentName(os.Getenv(\"MY_POD_NAME\")),\n        ),\n    )\n```. @TamalSaha could you please provide some example. Sorry for such question, I am just starting my may to Kubernetes and gRPC world :). @emicklei, I am working on the same scheme for the time being..... @TamalSaha, thanks for the hint. I have managed to develop solution for my case.\nThanks a lot.. Ok, the answer on my initial Q is simple: just use Kubernetes services (with type ClusterIP) for back-ends that I want to reach from gateway directly.. ",
    "uschen": "I believe the stackdriver trace works with grpc-gateway. . ",
    "smthpickboy": "Any examples or docs for supporting opentracing? Thanks!. ",
    "ZachEddy": "Not sure if this is the right place to ask questions, but I didn't know where else to post.\nDoes the gRPC Gateway client need to be initiated with a tracing interceptor? I struggled to get @theRealWardo's example to work until I added a client-side interceptor. If so, I think it would be helpful to bring this up in the documentation (happy to open a PR for this).\nThere's some relevant information here, but I wanted to double check.. Sounds good! Some additional documentation would be useful, but I like the original suggestion to add tracing support out of the box.\nMaybe this can be accomplished by adding another ServeMuxOption to the runtime package?\nSomething like runtime.WithTracer(tracer opentracing.Tracer) perhaps? Not really sure, but I'll tinker around with it.. ",
    "kristiandrucker": "Hi. @johanbrandhorst asked me to write how I have forwarded span from gateway -> grpc server. Well after a few hours of debugging this is my final code. For tracing in gateway I have used nethttp.Middleware() and for actually sending the span via metadata I wrote my own function that returns metadata.MD map.\nImplementation of nethttp:\ngo\nfunc NewTracedRuntimeMuxServer(tracer opentracing.Tracer, mux *runtime.ServeMux) *http.Server {\n    return &http.Server{\n        Handler: nethttp.Middleware(\n            tracer,\n            mux,\n            nethttp.OperationNameFunc(func(r *http.Request) string {\n                return fmt.Sprintf(\"HTTP-gRPC %s %s\", r.Method, r.URL.String())\n            }),\n        ),\n    }\n}\nCustom metadata from span function:\n``` go\nfunc MetadataFromSpan(span opentracing.Span) metadata.MD {\n    ctx := span.Context()\n    carrier := make(map[string]string)\nspan.Tracer().Inject(\n    ctx,\n    opentracing.TextMap,\n    opentracing.TextMapCarrier(carrier),\n)\n\nreturn metadata.New(carrier)\n\n}\n```\nImplementation of the custom metadata from span function:\n``` go\nmux := runtime.NewServeMux(\n    runtime.WithMetadata(\n        func(ctx context.Context, r *http.Request) metadata.MD {\n            span := opentracing.SpanFromContext(ctx)\n            tracing.MetadataFromSpan(span)\n        return tracing.MetadataFromSpan(span)\n    },\n),\n\n)\n```\nGetting the values in gRPC is done via this package github.com/grpc-ecosystem/go-grpc-middleware/tracing/opentracing. There I have created a UnaryInterceptor for grpc, but you can also create a StreamInterceptor and pass the UnaryServerInterceptor or the StreamServerInterceptor along with a tracer to create a trace for every call to the gRPC service and also it will enable you to create the span from context.. ",
    "simonpasquier": "Trying to get away from golang.org/x/net/context for my own projects, I wonder if it wouldn't be possible for grpc-gateway to switch using context from the standard library now. Specifically wouldn't it be ok to change this line to \"context\"?. @johanbrandhorst sure, I'll send something soon.... ",
    "knweiss": "I'm currently working on such a configuration as well. I started with the example code from https://github.com/philips/grpc-gateway-example (i.e. REST and gRPC share the same port). Please let me share my current problem because I'm not sure if I'm missing something:\nThere's this initialization call to \nerr := pb.RegisterEchoServiceHandlerFromEndpoint(ctx, gwmux, demoAddr, dopts)\nWith mutual TLS authentication the DialOptions dopts now require a client certificate for the relay call to gRPC. My problem is that these are static options (i.e. they use a single, static client certificate) for the lifetime of the service.\nHowever, I actually want to use the dynamic client certificate of the user who called the REST API endpoint for the internal grpc.Dial() to the gRPC handler, too.\nMy current implementation works fine if I call the service via gRPC. Using the REST endpoint the mutual authentication works, too. However, the internal relay call to gRPC uses always the same client certificate.. Thanks for your reply. That's what I was hoping for.\nPersonally, I don't used PATCH either. I was just adding it for completeness.\nQuestion: How can you use two different RPCs with the same name but different arguments in gRPC (e.g. GetTickets() in your example)? When I try I always get an \"is already defined\" error from protoc.\nThe guessability aspect is what I had in mind when I was asking for a naming convention.. FWIW: In the meantime (i.e. before reading your reply) I thought some more about my own preference and came up with the following. The main difference is that I use more than one noun in the RPC names (e.g. to prevent the \"is already defined\" problem):\nMethod |Resource Path |Description | gRPC call\n------- | ---- | --- | ---\nGET | /users | Get a list of all users | GetAllUsers()\nGET | /users/42 | Get a specific user id | GetUser(u)\nGET | /users/42/tickets | Get all tickets of user id 42 |  GetAllTicketsOfUser()\nGET | /users/42/tickets/23 | Get ticket id 23 of user id 42 | GetTicketOfUser(t, u)\nPOST | /users | Create a new user | CreateUser(u)\nPOST| /users/42/tickets | Create new ticket for user id 42 | CreateTicketForUser(t, u)\nPUT | /users | Bulk-update users | BulkUpdateUsers(users)\nPUT | /users/42 | Update user id 42 | UpdateUser(u)\nPUT |  /users/42/tickets/23 | Update ticket id 23 of user id 42 | UpdateTicketOfUser(t, u)\nPATCH  | /users/42 | Partially update user id 42 | ModifyUser(u)\nPATCH | /users/42/tickets/23 | Partially update ticket id 23 of user id 42 | ModifyTicketOfUser(t, u)\nDELETE | /users | Delete all users | DeleteAllUsers()\nDELETE | /users/42 | Delete user id 42 | DeleteUser(u)\nDELETE | /users/42/tickets/23 | Delete ticket id 23 of user id 42 | DeleteTicketOfUser(t, u)\n. ",
    "bleleve": "Hi there,\nI am facing the same problem.\nI need to retrieve the client certificate infos from the rpc method but the internal call between the gateway and the grpc stub is done with the server certificate...\nIs there a way to force the gateway to forward the client certificate to grpc ?\nThank you ;). ",
    "hexfusion": "\nIs there a way to force the gateway to forward the client certificate to grpc ?\n\nI am interested in this as well, any updates on this?. @achew22 thank you for the notes and kind invitation to provide recon. I hope to revisit this and will make sure to touch base with you before hand to compare notes.. @achew22 I think this functionality would really benefit the project and would like to take this on if for nothing else to better understand how grpc-gateway works. If you have any notes you could pass my way or tickets, in particular, outlining your thoughts/requirements, they would be appreciated. Thanks!. @tmc understood, I will get this hammered down and include tests. Thanks!. @googlebot I signed it!. Closing this for now while I rethink approach.. @achew22 any idea when this might be released? Would it make 1.4.x?. Perfect thanks a lot!. @johanbrandhorst am I correct that you need help replacing swagger-codegen in order to move to go modules? I am motivated to get this done please advise.. Great link, and I must say the go mod has created a nightmare. The video is very well done but for some projects jumping a major version is simply not feasible for a tooling change. Saying that this project is blocking my ability to use go mod effectively. Basically the codegen creates the example files with an improper path.\nimport \"gopkg.in/resty.v1\"\nhttps://github.com/go-resty/resty#usage\nvs\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/ee3ef70b7777cde4e61e4e224cb11e92beecee6a/examples/clients/abe/api_client.go#L21\nwhich I believe is the cause of this\ngo: github.com/go-resty/resty@v1.11.0: parsing go.mod: unexpected module path \"gopkg.in/resty.v1\"\ngo: error loading module requirements\nWould you accept as a solution of post processing on these example files with sed or ? so they use the correct import path? I just don't see it realistic to remove codegen for this reason alone as it is not trivial. If there is another way around this please let me know.. >  Do you not think this would be solved by implementing modules in this repo?\nNo it seems the issues with resty via codegen is the core of the problem I am facing. . Honestly I am not sure.. @johanbrandhorst looking at the source of the proto I believe the change is expected, will update.. /cc @gyuho. @johanbrandhorst just curious about general time-frame when we might expect this to be released. Thanks!. that would be fantastic, and greatly appreciated.. ",
    "utrack": "@TamalSaha ah, I see. Thanks!. @achew22 We've got the (weird) code that decides if comment is description or title here.\nIt sets Title and Description through reflection (prolly I could refactor it?). Oh dang, I just noticed I've already filled #375 :)\nI see that the new flag should be added via common registry - it would be confusing for the flag to work w/ -gen-swagger, but not with -gen-gateway.\nI'll maintain my fork w/ -gen-go-style JSON tags and wait for the bad marshaller replacement meanwhile.\nWhat's the issue related to the marshaller replacement, if there's any? I might be able to help with that.. The issue is for v1.4.1 - there's no newer release than this one atm.. ",
    "qianlnk": "https://github.com/gogo/protobuf support custom struct tag for xml\nor \nhttps://github.com/favadi/protoc-go-inject-tag. ",
    "writeameer": "Hello Everyone,\nJust started playing with this wonderful project. I have a use case where xml would be useful. Any reason this wasn't merged ?\nAmeer.\n. ",
    "aaronjwood": "Thanks, we're working on a vendoring strategy so we'll be sure to catch this. Glad to hear it's fixed in the next release.. Thanks for the info @tmc! Let me sort out our internal issues at work from this and then I can come back to some sort of vendoring doc :). ",
    "deejross": "I can tell you from a Glide perspective, I've been trying to get this to work all day. Etcd wants a version of golang/protobuf from 2016 without a Filename field that gRPC-Go wants, gRPC-Go hasn't released v1.2.2 yet with the new Context methods that the current gRPC-Gateway v1.2.2 is looking for the new Context methods.\nThis is exactly the type of thing Semantic Versioning is supposed to be preventing. I get that you might be trying to match up the version numbers with gRPC-Go, but unfortunately, this introduced a backwards-incompatible change that vendoring tools see as just a simple patch, when in fact, it should have been a major version since it breaks everything unless you manually specify each version number for all the packages.. Also would like to point out that I'm using gRPC-Go v1.2.1, gRPC-Gateway v1.2.0 and I'm now getting a bunch of errors similar to these:\nbash\npb/mydis.pb.gw.go:775: too many arguments in call to runtime.AnnotateContext\n    have (\"github.com/deejross/mydis/vendor/golang.org/x/net/context\".Context, *runtime.ServeMux, *http.Request)\n    want (\"github.com/deejross/mydis/vendor/golang.org/x/net/context\".Context, *http.Request)\nSo I can't get this working at all.. @TamalSaha I believe that's correct. The plugins are installed per the documentation:\ngo get -u github.com/golang/protobuf/{proto,protoc-gen-go}\ngo get -u google.golang.org/grpc\nI think that installs from the master. Is there a better method of installing the correct version of the plugins? I have multiple developers, plus a Jenkins build environment that would all need to have the same version of these plugins. To my knowledge, Glide does not install binary files in $GOPATH/bin the way that go get does, but I could be mistaken.. I learned quite a bit from this, it was very helpful. Basically, you install using go get -u per the instructions, cd into each directory (i.e. $GOPATH/src/github.com/grpc-ecosystem/grpc-gateway), do a git checkout with the desired tag/version, and finally go install the binaries to $GOPATH/bin.\nWhile not the most straightforward way to install the desired version of plugins, it should work for our needs. Thanks a bunch! Hopefully, Go's dep tool can solve issues like these in the future.. @rhnasc I think the major issue here, which most package managers aim to fix, is that go get does not support versions. If dep can replace go get for downloading to $GOPATH/src and generating binaries in $GOPATH/bin as well as acting as a vendoring tool, then it would probably fix what happened here, or at least make it easier to fix.. ",
    "rhnasc": "@deejross do you know if there's an issue on dep(or a specification maybe) regarding this new capability you just referred to? This is a feature that would certainly come in handy in situations like these.. ",
    "charles-crain": "Oh I think #314 is actually the same issue.  That issue shows it was fixed in release 1.2.0 correct?  I am using whatever the latest is from go get -u github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway ... does that lag behind?\nAn example request would look like this:\n```\nenum Field {\n  FOO = 0;\n  BAR = 1;\n}\nmessage FooRequest {\n  Field which_field = 1;\n}\nservice Foo {\n  rpc FooCall (FooRequest) returns (FooResponse) {\n    option (google.api.http) = {\n            get: \"/api/foo\"\n      };\n  }\n}\ncurl localhost/api/foo?which_field=BAR\n```. ",
    "zweizeichen": "Thanks! That answers my question.. ",
    "lantame": "Yes, I signed the CLA 8 hours ago. But somehow this check does not pass :-(\nLooks like I commited with wrong email. I'll change it.\nDon't know why the check removed after commit update.. Added a test. @tmc take a look pls.. Hello CLA bot! . ",
    "foolusion": "The goproto_registration extension solved my issue. Thanks!. ",
    "rickcrawford": "@TamalSaha thanks!. ",
    "cheung-chifung": "same problem.\nI think the f.GetName() in following code should be f.GetJsonName().\ngo\nschema.Properties = append(schema.Properties, keyVal{f.GetName(), fieldValue})\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-swagger/genswagger/template.go#L186. ",
    "alexleigh": "This is still an issue. f.GetName() is still being used, resulting in snake_case names rather than camelCase names in the generated swagger definitions.. For the code generated by swagger-codegen using the schema files generated by this project, the impact of this change should be relatively minimal. Much like how protoc converts snake_case protobuf field names to lowerCamelCase field names for JSON marshaling, swagger-codegen takes care to convert the lowerCamelCase field names in the JSON schema to the preferred field names in the target language. For example, in the generated Python code, there is this map:\nself.attribute_map = {\n            'single_nested': 'singleNested',\n            'uuid': 'uuid',\n            'nested': 'nested',\n            'float_value': 'floatValue',\n            'double_value': 'doubleValue',\n            'int64_value': 'int64Value',\n            'uint64_value': 'uint64Value',\n            'int32_value': 'int32Value',\n            'fixed64_value': 'fixed64Value',\n            'fixed32_value': 'fixed32Value',\n            'bool_value': 'boolValue',\n            'string_value': 'stringValue',\n            'bytes_value': 'bytesValue',\n            'uint32_value': 'uint32Value',\n            'enum_value': 'enumValue',\n            'sfixed32_value': 'sfixed32Value',\n            'sfixed64_value': 'sfixed64Value',\n            'sint32_value': 'sint32Value',\n            'sint64_value': 'sint64Value',\n            'repeated_string_value': 'repeatedStringValue',\n            'oneof_empty': 'oneofEmpty',\n            'oneof_string': 'oneofString',\n            'map_value': 'mapValue',\n            'mapped_string_value': 'mappedStringValue',\n            'mapped_nested_value': 'mappedNestedValue',\n            'non_conventional_name_value': 'nonConventionalNameValue',\n            'timestamp_value': 'timestampValue',\n            'repeated_enum_value': 'repeatedEnumValue'\n}\nThis is a mapping from the lowerCamelCase names in JSON to the snake_case field names in Python. The situation is similar for Ruby and Go, and I imagine for other languages swagger-codegen should pick the appropriate field naming as well.\nYou are right that this change is breaking, insofar as the generated Swagger schemas will change. I actually think another change makes sense when combined with this change: the default jsonpb marshaling in grpc-gateway should have OrigName: false. This will cause grpc-gateway servers to emit JSON output in line with protobuf canonical JSON representations. However this would be even more of a breaking change, causing not only the swagger output to change, but also the output of all servers with default marshaling configurations. Thus for this change I left the default marshaling alone, and only modified the marshaling of the test server. Unfortunately this means the default server marshaling output and the swagger schema generated have diverged. It's up to the user to ensure the two are consistent by customizing the jsonpb marshaling options.. @achew22 if you prefer, I can amend this PR to change the default marshaling to use camelCase and emit default values. Currently I accomplish both of those by customizing the marshaling options. But it would be good to change the default marshaling options to follow suit.. I changed the default marshaling to use camelCase and emit default fields and fixed the tests. Unfortunately, swagger-codegen 2.2.2 does not support enum fields for generated Go code (2.3.0 purports to support it but in my tests it seems broken, it generates incorrect type names that don't compile). So when default values for enum fields are emitted, swagger-codegen generated Go client code can't unmarshal the object due to the missing enums. To keep the existing tests I had to make a copy of the ABE message that has all the same fields, except for all of the enums, and adjust the client test code to use these messages instead. On the plus side this let me enable one of the previously disabled tests.. ",
    "pcariel": "To improve the interface description language.. ",
    "hangll": "@TamalSaha that is hard. How can you tell if a bool value is updated to false or is not set?. Is there any way I can do this via grpc gateway? Should I use streaming?. Thanks @TamalSaha. Can you explain more about why http mux would be a better option? Also, when should I use streaming?. ",
    "evolsnow": "FieldMask may be helpful, and it would be wonderful if grpc-gateway could generate the mask from REST API.. ",
    "adematte": "Hello, we have our own implementation to map REST -> GRPC in nodejs and are evaluating if we could use/support this project instead. One of the things we are doing is populating the fieldMask automatically based on the json request.  Using a fieldmask is the de facto GRPC approach for \"patching\" and I can see real value in it being handled by the api gateway and not having it being generated by the REST clients.\nIs this something that would make sense for this projet core features, or is this something that should be handled via a custom middleware of some kind?\n. @achew22 currently what we are doing is this:\n- look for a PATCH http route\n- look for a property of type google.protobuf.FieldMask in the request message\n- autopopulate it based on the json request (we only use the body for this)\nAs a reminder, FieldMasks can also be used for partial responses but we don't handle those in any particular way. We just handle the partial update (patch) scenarios.. @wora if I am not mistaken the document you are pointing to, refers to fieldmasks used for response filtering. I don't think it would be a good idea to hijack the X-Goog-FieldMask http header when using PATCH requests don't you agree?. do we even need to pass this through an http header though instead of just using the json body? The only rational I see regarding the http header is that this way we do not affect the json request payload. Any other advantages to the use of a header?. I can see pros and cons about hiding or keeping the fieldmask in swagger although I would lean toward removing it: having a fieldmask feels alien \ud83d\udc7d in the existing JSON Rest API ecosystem and looks like a technical artifact coming from using GRPC in the backend.\nMaybe this is something that should be configurable at some point?. ",
    "wora": "I worked on proto3, gRPC, and Google API Design Guide. This is a well known feature request. I would recommend gRPC Gateway implement this feature.\nWhen you perform a partial update against a Google API, you can explicitly pass the field mask as HTTP header X-Goog-FieldMask, see https://cloud.google.com/apis/docs/system-parameters. gRPC Gateway can auto generate this header if the request method is PATCH and this header is absent, based on the presence of JSON fields. I think the semantics is very easy to understand and also easy to use.. Good point. We should use a separate header for it.\n. @adematte I think you are correct. Google API Design Guide recommends every PATCH method has an update_mask in the request message. We can auto populate it. The request message should have at most one FieldMask field at the top level. Otherwise, the proxy should do nothing.. Please document why this feature is needed. We intentionally keep gRPC transcoding extremely simple. The goal for gRPC transcoding is to build REST API from RPC API. We don't intend to be feature rich as OpenAPI. We generally don't need feature to build great and useful APIs. The less features, the better performance and quality.. I am publishing the change from upstream. It should land in a few days.. gRPC and proto3 are language neutral. The mechanical conversion between JSON and proto3 are defined by proto3 spec. The custom type seems to be a Go-specific feature. Such custom conversion exists in many systems, such as Apigee, but such logic can be endless and very hard to maintain. You also need to update OpenAPI generator, documentation generator, and other tools to support these features.\nI think it is generally better to handle the conversion in the backend, i.e., creating a separate gRPC method without custom type, the backend converts between method without custom type and method with custom type. The gRPC Gateway just handle the method without custom type. Without this approach, we can handle arbitrary custom types and the backend using the feature carries the burden. The rest of gRPC tooling doesn't need to know anything about it.. Recently, Google updated the gRPC Transcoding spec, aka google.api.http annotation, to allow repeated field and mapped field as HTTP request/response body. This is to allow third party implementations, such as gRPC Gateway, to provide advanced features that customers may need. Most implementations may not provide such advanced features. Except for backward compatibility, there is no general advantage to use such advanced features. It also creates more work to OpenAPI generator, documentation generator, and other tools. Just FYI.\nDisclaim: I am a co-author of gRPC Transcoding spec.. ",
    "dmacthedestroyer": "Hi all, I'm looking to rehydrate the PATCH debate -- is there still an appetite for this? If so, I think I can provide development effort for it. This is the current design I see:\nHTTP PATCH request comes in\nif the gRPC request message has exactly 1 FieldMask field:\n  set the request FieldMask field based on HTTP request JSON body\nAreas that still need discussing:\n* What to do about the generated Swagger file? Should it be modified to exclude FieldMask?\n. That's a good question, and one we've battled with in our organization. I'm inclined to leave that as a separate conversation so as to not make this issue too ambitious :)\nHow's that sound? Maybe open a seperate issue to discuss that? I think it'd require a deeper look into what gRPC recommends for List commands and abiding by that first and foremost.. I signed it!. running this brew command will install 2.2.2: brew install https://homebrew.bintray.com/bottles/swagger-codegen-2.2.2.sierra.bottle.tar.gz. I wrote e2e tests here: https://github.com/grpc-ecosystem/grpc-gateway/pull/671/files#diff-90f6bf9d43e87f07dea7cfc533677ed0. This PR isn't quite ready yet... there are some broken unit tests and the build needs to be fixed. I'm still working on it but my availability has changed somewhat. Is it ok if I maintain this PR for a while and ping you guys when it's ready?. yep, good call. @razamiDev took over work on this project a little while ago, thus his additional commits. Will rebase and then fix the broken builds. @achew22 or @johanbrandhorst wanna give this CLA the OK so we can get the build fixed?. yep, working on learning what a Bazel is, then will try to fix that :). hmm.. so I've got two issues with the build, it seems.\ngo vet fails, and fixing it results in failed integration tests\nI'm not sure how this got through the gated checkin... I don't think this is anything I've modified. I can fix the lint issue, but will want some feedback on what the desired behavior should be.\nIt appears that the way in which my machine generates .pb.go files differs from the build agents in how it resolves the FieldMask message.\nI've seen this issue before, but forgot the solution. I can look into it further.. I'll ignore the vet failure for now, since it seems I've got bigger problems to figure out first :)\nAs for the squash commit, how would you recommend I do that within this PR? I've always in the past squashed the commits upon merging the PR.. my current, bigger problem is hot to get this build to pass... What's the standard for properly importing FieldMask in proto files for Go? I'm assuming if I add this file somewhere in the project and reference it properly, it should give the proper import path. Is this how people are expected to incorporate FieldMask into their projects?. I ran into and addressed another issue with unused imports after merging the latest master.\nMy remaining tasks for this PR:\n\ninclude the field_mask.proto file into the project to fix the build\nchange field mask logic to use proto definition names instead of Golang names (from review comments)\nfix edge case issue with how I'm allocating arrays in runtime/fieldmask.go (found in another issue)\n\nI've lost my corporate support for this issue so I'm going to tackle finishing it up on my own nights & weekends. I plan to push ahead to get it ready for merging next week.\nSorry it's taken so long :(. still working on trying to understand this Makefile and how to get travis to find the right .proto file... If anyone knows where I should add this field_mask.proto to get it to work, please let me know. Otherwise, I'll take another crack at it soon.. I had to add this to the bidi-streaming-request-func because I added the ioutil and bytes imports in the generator code above. Certain cases would cause compile errors for unused imports.. I'd be glad to receive recommendations on how to do this in a way that doesn't require reading the entire request body multiple times. I wrote it like this because I thought it was the most readable.. I think I'll add another proto file and template tests to verify each of these boundary conditions. I refrained from anything like that because I didn't want to assume that it's incorrect to have more than 1 fieldmask in a request proto. Also because I'm relatively new to Go templating and couldn't think of a more elegant solution :)\nYou could argue an error should be returned if none are found, as well, correct?. these weird unrelated diffs concern me as well. I just recently merged master into this feature branch, so I'm not sure why these diffs show up. I'll try to figure this out.. hmm... that's interesting. I think I intuitively did it this way because I had reflection on my mind. \nI see your point, though, and I agree. Will change.. ",
    "kibertoad": "@dmacthedestroyer What about GET requests with optional parameters? This could be useful for a search.. @dmacthedestroyer Sure, created #698. Yay! Thanks for making this :). This https://github.com/grpc-ecosystem/grpc-gateway/issues/379 is related, but PATCH is only part of the story here :). Created #698 for GET and now I guess this one is redundant.. @johanbrandhorst Are you sure? There was explicit request to create a separate issue as implementation would have to be handled separately.. #671 very explicitly handles specific case of \"If a binding is mapped to PATCH [HTTP method]\". This issue is about GET HTTP methods.. @johanbrandhorst Could you please remove \"duplicate\" label then :)? . Ah, thanks, already done. . ",
    "emicklei": "@maxkondr , we plan to use the gateway as a separate process next to the actual server (Java) running in a single container. Both the gateway and server will expose their ports, the gateway will use a local connection to the server. No experience yet, but this is how I want to start.. ",
    "narqo": "Hey, @tmc I'd be happy to help with the issue, but it feels it'd be best to start with a brief discussion. As I can see, the mycorp/api/user/v1/ part is taken from the original proto file path and is done by the peace of code in genswagger/generator.go. \nBut does this really a wrong behaviour? It looks like both protoc-gen-go and protoc-gen-grpc-gateway works in the very same way.. ",
    "uynap": "Hi @narqo , Sorry to comment this old thread. I recently got the same issue. It's apparent that protoc-gen-swagger is not working in the same way as protoc-gen-go and protoc-gen-grpc-gateway.\nFirstly, the path of the files generated by protoc-gen-grpc-gateway is indicated by the \"option go_package\". Secondly, when \"option go_package\" is missing, protoc-gen-grpc-gateway use the \"-import_path\" option to declare it. \nTo have the consistent behaviour, I think protoc-gen-swagger should be in the same way. . @TamalSaha I think I'm using the latest one.\n$ ~/go/bin/protoc-gen-grpc-gateway --help\nUsage of /Users/py/go/bin/protoc-gen-grpc-gateway:\n  -allow_delete_body\n        unless set, HTTP DELETE methods may not have a body\nI saw it's a recently added feature.\n. I also tried to run \n~/go/bin/protoc-gen-grpc-gateway --allow_delete_body\nwithout any issue.\nThat's why I guess the problem is \"the way\" I passed it to \"protoc\" is wrong. . @TamalSaha Thanks a lot! It's exactly what I'm looking for. It is \"comma separated\". \nDo you know where I can find documents on how to write \"protoc\" arguments? I read a lot of gRPC and related documents, but never seen it before.. ",
    "vaishalig2693": "I am also facing the same issue.  The directory structure followed is same as path of proto file and not picked from go_package option. I am not sure if there is a similar option for swagger too\n. Any workaround for this that I can do without changing from proto2 to proto3?. rpc EntityUpdate(EntityUpdateArg)\n    returns (EntityUpdateRet) {\n      option (google.api.http) = {\n        put: \"/v1/tentity/{uuid}\"\n        body: \"*\"\n      };\n    }\nGiven the above and given uuid in bytes, how am i supposed to achieve that?. Agreed but in EntityUpdateArg, uuid is in bytes. How the type conversion is done here? Apologies I am not much aware of that. Will it be something like - \nput: \"/v1/entity/string{uuid}\" ?. Few things to clarify here - \nIf its a path param, I create a string entry in EntityUpdateArg say uuid_str and use that in path param?\nIf thats the case, then for body of the message, where is this encoding supposed to happen? Given the fact that everything needs to be handled on server end and atleast existing rpc clients are not expected to change. \nAlso I am only supposed to add fields in proto as interface.pb.gw.go is generated. So any such operation of encoding etc has to be part of proto definition. . My existing clients are rpc clients only. On top of existing proto, I wanted to expose REST interface for new clients. But given I have to use field from existing proto (unless a new string type field is added), the generation of interface.pb.gw.go fails from existing interface.proto. Does that mean that conversion from bytes_type is supported in body and not for path params as far as proto2 is concerned? If thats the case, is there any plan to make that enhancement in near future?. As I said, I can't run a grpc server as it is over http2 protocol and my existing clients using http1.1 protocol will fail. ",
    "nlamirault": "Using this dependency : \nbash\n$ govendor fetch github.com/googleapis/googleapis/google/api\nAnd these commands, it works : \nbash\n$ protoc -I/usr/local/include -I. -I${GOPATH}/src -I../vendor/github.com/googleapis/googleapis --go_out=plugins=grpc:. *.proto\n$ protoc -I/usr/local/include -I. -I$GOPATH/src -I../vendor/github.com/googleapis/googleapis --grpc-gateway_out=logtostderr=true:. cmdb.proto\n$ protoc -I/usr/local/include -I. -I$GOPATH/src -I../vendor/github.com/googleapis/googleapis --swagger_out=logtostderr=true:. cmdb.proto\nbut build of the project failed : \npb/cmdb.pb.gw.go:149: too many arguments in call to runtime.AnnotateContext\npb/cmdb.pb.gw.go:151: too many arguments in call to runtime.HTTPError\npb/cmdb.pb.gw.go:156: too many arguments in call to runtime.HTTPError\npb/cmdb.pb.gw.go:160: too many arguments in call to forward_PlatformService_List_0\npb/cmdb.pb.gw.go:177: too many arguments in call to runtime.AnnotateContext\npb/cmdb.pb.gw.go:179: too many arguments in call to runtime.HTTPError\npb/cmdb.pb.gw.go:184: too many arguments in call to runtime.HTTPError\npb/cmdb.pb.gw.go:188: too many arguments in call to forward_PlatformService_Create_0\npb/cmdb.pb.gw.go:205: too many arguments in call to runtime.AnnotateContext\npb/cmdb.pb.gw.go:207: too many arguments in call to runtime.HTTPError\npb/cmdb.pb.gw.go:207: too many errors\nI don't really undersant how generated code use Master and not Version 1.2.2 . Yes. \nBut i use grpc-gateway 1.2.2 in my vendor directory.\n```\n$ grep -R AnnotateContext vendor/github.com/grpc-ecosystem/grpc-gateway/\nvendor/github.com/grpc-ecosystem/grpc-gateway/runtime/context.go:AnnotateContext adds context information such as metadata from the request.\nvendor/github.com/grpc-ecosystem/grpc-gateway/runtime/context.go:func AnnotateContext(ctx context.Context, req *http.Request) (context.Context, error) {\n$ find  $GOPATH -name grpc-gateway\n/home/vagrant/Apps/golang/src/gitlab.sys.op.mbs/diablo/vendor/github.com/grpc-ecosystem/grpc-gateway\n$ protoc --version\nlibprotoc 3.3.0\n```. OK. I installed protoc-gen-grpc-gateway and protoc-gen-swagger like that : \nbash\n$ go get -u github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway\n$ go get -u github.com/grpc-ecosystem/grpc-gateway/protoc-gen-swagger\nThis is the error.\nHow install a specific version of this binaries  ?. Thanks. It works fine now.. It could be helpful to provides binaries into a release files ?. I try to vendoring using : \ngo mod edit -require github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway@v1.6.4\nbut i've got this error : \n$ go mod tidy\ngo: finding github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway v1.6.4\ngo: github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway@v1.6.4: unknown revision protoc-gen-grpc-gateway/v1.6.4\ngo: error loading module requirements\nSo i've install using go get -u github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway@v1.6.4.\nI've got a shell script which generate files : \n```bash\n!/usr/bin/env bash\ngenerate the gRPC code\nfunction generate_grpcgw {\n    pushd $1\n    echo \"> Generate gRPC for $1\"\n    rm -rf .pb.go\n    protoc -I /usr/local/include -I . -I ../../vendor -I ../googleapis \\\n       --go_out=plugins=grpc:. .proto\n    rm -rf *.pb.gw.go\necho \"> Generate gRPC Gateway for $1\"\nprotoc -I /usr/local/include -I . -I ../../vendor -I ../googleapis \\\n   --grpc-gateway_out=logtostderr=true:. *.proto\n\necho \"> Generate Swagger for $1\"\nrm -rf ../swagger/*.swagger.json\nprotoc -I /usr/local/include -I . -I ../../vendor -I ../googleapis \\\n   --swagger_out=logtostderr=true:. *.proto\npopd\n\n}\nfunction generate_grpc {\n    pushd $1\n    echo \"> Generate gRPC for $1\"\n    rm -rf .pb.go\n    protoc -I /usr/local/include -I . -I ../../vendor -I ../googleapis \\\n        --go_out=plugins=grpc:. .proto\n    popd\n}\nfunction generate_swagger {\n    echo \"> Generate Swagger\"\n    find . -name \"*.json\" | xargs -I '{}' mv '{}' swagger/\n    rm -f swagger/api.swagger.json\n    go run ./swagger/swagger.go swagger > swagger/api.swagger.json\n}\ngenerate_grpcgw v1\ngenerate_grpc health\ngenerate_grpc info\ngenerate_swagger\n```. ",
    "ewang": "@nlamirault If you want to install from your vendor dir, you can do:\n$ go install ./github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway\n$ go install ./github.com/grpc-ecosystem/grpc-gateway/protoc-gen-swagger. A workaround I've used is to have server pass the error details via a trailer metadata, the on the grpc-gateway side, override the HTTPError handler to read from the trailer metadata.\nThis doesn't quite work for grpc-gateway side errors; for those I just special cased them in the HTTPError handler.. ",
    "yuzheng21": "Hi, I debugged this all the afternoon. I found that in the release v1.2.2, it doesn't include the commit from @tamalsaha https://github.com/grpc-ecosystem/grpc-gateway/pull/336\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/589b126116b5fc961939b3e156c29e4d9d58222f/runtime/context.go#L47\nbut in the template.go, it generates code with wrong number of parameters\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/589b126116b5fc961939b3e156c29e4d9d58222f/protoc-gen-grpc-gateway/gengateway/template.go#L361\nthis is why I keep getting too many arguments in call to runtime.AnnotateContext, even though I use the latest version v1.2.2. ",
    "wrrn": "Awesome Thanks. ",
    "pieterlouw": "Hi,\nI've signed the CLA.\nAbout the test, where exactly should I add one ?\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/master/examples/integration_test.go\nor\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-grpc-gateway/gengateway/template_test.go. Closing this as #409 have test case and examples. Have a look at this PR: https://github.com/grpc-ecosystem/grpc-gateway/pull/397\nIs there a specific reason why you would want to customize the message? Will a HTTP 400 suffice?. I will close #397 as this PR have test case and examples. ",
    "chaitanya9186": "Currently there is a way to generate grpc gateway or rest proxy server code in golang. Is there anyway we can generate similar code in python?. Sample golang gateway generated code: https://github.com/grpc-ecosystem/grpc-gateway/blob/master/examples/examplepb/echo_service.pb.gw.go. @achew22 Thanks for the clarification. . ",
    "ajalab": "I'm facing the similar problem about WKT handling now, so I would like to take over this task.\nWhere should I write tests for Duration?. ",
    "vaporz": "+1\nI'm having the same issue.\n@pieterlouw  I was going to create a PR exactly the same with #397 , until I see this issue.\n@TamalSaha Hi, our company is recently trying to build micro services using grpc-gateway, and I'm using HTTPError to handle errors. In this case, the error HTTPError receives is the error from \"strconv.ParseInt()\". I can not assert that, a ParseInt() error is an InvalidArgument error, and then return a HTTP 4xx. More detail is needed, HTTPError has to know what exactly is happening. . Hi, I created a PR #409 , it's almost the same with #397 .\nFor we are blocked by this issue, if there is anything I can help with, please let me know.\nThank you!. Ah....\"Waiting for status to be reported\", why..... I signed it!. zzzZZZ. ping @TamalSaha , @tmc \nThank you.. Hi, any comments on this PR?\nThanks!. hi, @yugui , thanks for your comment.\nIt took me some time to figure out the relationship between HTTPError, OtherErrorHandler and serveMux.protoErrorHandler.\nNow I'm working on a new commit for this PR, a  better(hopefully) solution.  . Committed, changes are:\n1, When NewServeMux, if serveMux.protoErrorHandler is nil, then OtherErrorHandler is adapted for ProtoErrorHandlerFunc, and assigned to serveMux.protoErrorHandler.\n2, Return 404/405, instead of 501. \nHowever, I believe this commit is just a start of \"error handling mechanism\" overhaul.\nHere're my thoughts in this topic.\nI don't think a HTTP server like grpc-gateway should new a grpc.Status as error and pass them internally. Grpc-gateway should new a gatewayError which contains a grpc.Status(returned by grpc server), and use it internally.\nAs comments on package status says:\n\nPackage status implements errors returned by gRPC.  These errors are serialized and transmitted on the wire between server and client,\n\n\"between server and client\"\nI suggest grpc-gateway should transfer grpc.Status error returned by grpc stubs immediately to a gatewayError with HTTP code, message and grpc.Status, then use gatewayError instead of a grpc.Status error. \nWhenever an error is returned in grpc-gateway, return a gatewayError.\nFor example, in *.pb.gw.go, pass a gatewayError to runtime.HTTPError(currently is grpc.Status).\ngatewayError may look like this:\ntype gatewayError struct {\nHTTPCode int,\nmsg string,\nstatus grpc.Status,\n}. \"code\", \"msg\", \"data\", so familiar to me, lol....\nIn your case, I think \"json schema\" may be helpful.\nYou can define \"data\" as a \"string\" in pb, but validate the format to its value with \"json schema\".. What is \" struct.proto\"? Any reference? Thanks.. Thanks!. ",
    "makkalot": "is it possible swagger generator to skip operationId generation ? @tmc . I tried to generate a client with duplicated operationId and got this  (used go-swagger tool):\nswagger generate client -f service.swagger.json\nThe swagger spec at \"service.swagger.json\" is invalid against swagger specification 2.0. see errors :\n- \"Get\" is defined 2 times\n- \"Update\" is defined 2 times\n- \"Delete\" is defined 2 times\nAlso when omit the operationId the generated code is very very ugly the method names are unreadable.\n. ",
    "brandoncole": "@tmc @makkalot Omitting is fine for some languages like Go.  Python requires it for server side implementations.  . ",
    "anweiss": "Would be great if we could customize the generated operationId via an additional_bindings option. Thanks @achew22, although how would I denote multiple operation_id fields to reflect the additional_bindings under the same rpc method? Only a single grpc.gateway.protoc_gen_swagger.options.openapiv2_operation option type is allowed per rpc method. Found https://github.com/grpc-ecosystem/grpc-gateway/wiki/How-to-customize-your-gateway#replace-a-response-forwarder-per-method, but would love to be able to do this using a proto option\n  . ",
    "calbach": "I'm not sure. I couldn't find much in the specification about paths and templates. From searching around, my best guess is that you cannot use pattern matching within a path template - if that's the case the best approximation of this might be something like:\n/v1/examples/{name1}/bars/{name2}\nI don't know if such an approach can generalize to all possible http patterns, but at least it would support Google's recommended approach for defining resource names.. For future reference, looks like the relevant code is here.. Yes - I think it's a dupe AFAICT. That said, I don't have an explanation of why @Multiply hit this if that dupe is marked \"fixed\", unless they're on an older version. . ",
    "boosh": "Came here to post about this issue. The issue is obviously that the extracted path name (e.g. name in the OP) is used as the key into a dict.\nTBH I'm still not 100% clear on why Google recommend naming resources like they do. Anyone any ideas?. I suppose the question is, what's the right thing to do? If I have the following grpc paths: \n\n\"/v1/{name1=prefix1/*}\"\n\"/v1/{name1=prefix1/*/suffix1}/{name2=prefix2/*}\"\n\"/v1/{name1=prefix1/*/wildcard2-prefix/*}\"\n\nWhat should the corresponding swagger paths look like?\nPerhaps we could give wildcards sequentially generated names after extracting and prepending strings, e.g.:\n\n\"/v1/prefix1/{segment1}\"\n\"/v1/prefix1/{segment1}/suffix1/prefix2/{segment2}\"\n\"/v1/name1/{segment1}/wildcard2-prefix/{segment2}\"\n\nor, if a segment only contains a single wildcard, use the key of the segment as the placeholder, e.g.:\n\n\"/v1/prefix1/{name1}\"\n\"/v1/prefix1/{name1}/suffix1/prefix2/{name2}\"\n\"/v1/{name1=prefix1/*/wildcard2-prefix/*}\" <- fallback to the above scheme with sequential names?\n\nSince I have basically no idea of the relationship between this plugin and how the gRPC gateway works (i.e. if it uses this plugin internally) does this approach look feasible and make sense @tmc ?\nIt also doesn't resolve the problem of paths using the ** wildcard which can contain / characters, but that's a more complicated case we should think about later.. @achew22 what do you mean by \"generators changing variable names on you\"?\nI thought the reason for Google's recommendations  for declaring endpoints like get: \"/v1/{name=shelves/*/books/*}\" was so that resource names are unique and fully qualified (e.g. shelves/classics/books/war-and-peace). \nI've found in my own APIs this does indeed lead to cleaner interfaces (e.g. because I can do things like return author as authors/russians/leo-tolstoy which clients can then lookup directly, instead of needing a specific Author response message type). It makes API payloads more generic.\nThat's my understanding at least.\nDo you have any feedback on my suggestion for how we can map these sorts of names to swagger path templates? I'd like to get cracking on this as soon as possible because it's thrown a spanner in the works of a project I've been working on and need a resolution or workaround.. I've created the following python code as a testbed:\ndef template(path):\n    output = []\n    buffer = []\n\n    depth = 0\n\n    for char in path:\n        if char == '{':\n            depth += 1\n        elif char == '}':\n            depth -= 1\n\n            if depth == 0:\n                # we've reached the end of a path template, so process the buffer\n                updated_string = process_string(''.join(buffer))\n                output.extend([x for x in updated_string])\n                buffer = []\n        else:\n            if depth > 0:\n                buffer.append(char)\n            else:\n                output.append(char)\n\n    return ''.join(output)\n\n\n\ndef process_string(buffer):\n    \"\"\"\n    Return the path with strings prefixed and suffixed, and correctly templated\n    :param buffer:\n    :return: Updated string\n    \"\"\"\n    key, segments = buffer.split('=')\n\n    # convert ** to *\n    segments = segments.replace('**', '*')\n\n    if segments.count('*') == 1:\n        return segments.replace('*', '{%s}' % key)\n    else:\n        output = []\n        split_segments = segments.split('*')\n\n        for index, segment in enumerate(split_segments):\n            # trim empties from the end\n            if not segment and index + 1 == len(split_segments):\n                continue\n\n            output.append(segment)\n            # output 1-indexed numbers since end users are humans\n            output.append(\"{%s%d}\" % (key, index + 1))\n\n        return ''.join(output)\n\n\ndef main(path):\n    print(\"Input=%s\" % path)\n    output = template(path)\n    print(\"Output=%s\" % output)\n\n\nif __name__=='__main__':\n    inputs = [\n        '/v1/{name=customers/*/surname/*}',\n        '/v1/{name=customers/*/surname/*}/likes/{category=sites/*}',\n        '/v1/{name=path/**}',\n    ]\n\n    for input in inputs:\n        main(input)\n\nIt yields:\nInput=/v1/{name=customers/*/surname/*}\nOutput=/v1/customers/{name1}/surname/{name2}\nInput=/v1/{name=customers/*/surname/*}/likes/{category=sites/*}\nOutput=/v1/customers/{name1}/surname/{name2}/likes/sites/{category}\nInput=/v1/{name=path/**}\nOutput=/v1/path/{name}\n\nI believe the above would solve this issue if translated into go. \nAny thoughts?. Sooo... any thoughts on this @calbach @tmc @achew22? I might have some time to do the work, but I don't want to waste my time if the PR won't be accepted.. So converting \"/v1/{name1=prefix1/*}\" to \"/v1/prefix1/{name1}\" works fine, but I get an error trying to generate a swagger spec after replacing \"/v1/{name1=prefix1/*/wildcard2-prefix/*}\" with \"/v1/prefix1/{segment1}/wildcard2-prefix/{segment2}\": \nfatal: [localhost]: FAILED! => {\n  \"changed\": true, \n  \"cmd\": \"protoc -I/usr/local/include -I. -I$GOPATH/src -I$GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis --swagger_out=logtostderr=true:. api/mine/v1/my_service.proto\", \n  \"delta\": \"0:00:00.024865\", \n  \"end\": \"2017-07-19 10:54:40.209476\", \n  \"failed\": true, \n  \"rc\": 1,\n  \"start\": \"2017-07-19 10:54:40.184611\", \n  \"stderr\": \"--swagger_out: no field \\\"segment2\\\" found in APIRequest\", \n  \"stderr_lines\": [\"--swagger_out: no field \\\"segment2\\\" found in APIRequest\"], \n  \"stdout\": \"\", \n  \"stdout_lines\": []}. Actually, this is probably correct.. > The parsing should be the same as for the gateway code.\nI don't really know what you're referring to here. As an end user I was hoping not to have to look into the source.\nHowever, I've seen from digging around in the source of this plugin that various rules are applied to comments, like:\n\nIt'll treat the first line of a comment as the title of a swagger item, unless it ends with a period (not sure why but this isn't intuitive). \nBlank comment lines appear to be necessary to separate paragraphs. \nSubsequent paragraphs will be used as the summary or maybe description, \netc., etc.\n\nIt'd be useful to have a guide that explicitly states all these rules along with examples so it's obvious how .proto files should be commented so that swagger files are correctly generated in terms of documentation.. ",
    "Multiply": "Has there been any traction on this since 2017?\nI'd like to help out where possible, as this problem is currently biting us a little hard, and I can't seem to find a way to 'override' the http path used for swagger, without changing the entire implementation.. Not sure if you want me to reply here, or elsewhere, but I am indeed still hitting this.\nJust to make sure I wasn't using an older version, I just removed the protoc-gen-swagger, and installed it again, and same result.\nCurrently I'm seeing the following results:\n```\nGist of a proto\noption (google.api.http) = {\n  get: \"/v1/{id=prefix/*}\"\n};\nGist of the swagger path\n\"/v1/{id}\": {\n  \"get\": {\n```\nI kind of hoped, this would be generating the following swagger path instead:\n\"/v1/prefix/{id}\": {\n  \"get\": {\nPlease let me know if that's not how it's meant to work.. @johanbrandhorst I tried adding || field == \"id\" in there, and the generated swagger is the same.\nI also tried using name in my proto, rather than using id and it didn't seem to help.\nWhat is the expected path when using prefix?. Also limiting it to just parent, name or id, also seems kind of odd, as in some instances you might have more than a single nested resource, like /v1/{company_id=companies/*}/{team_id=teams/*}/{user_id=users/*}\nI might just have the wrong expectations of the functionality, and in that case, what I'd rather want, is a way to override the path used for swagger, using an option of sorts. (Edit: if that even makes sense?). The reason I use underscore, is because my gRPC implementation takes company_id in the body. (for this specific request)\nIn other instances I'd use company.id because the body takes a full company in the body, but takes the id from the URL.\nAll of the scenarios can be fixed for my use-case, if I can override the generated swagger path. I don't want to modify the actual implementation details, just the generated code the user sees in the UI.\nThe consumer of the documentation might not know from reading /v1/companies/{company_id} that the id is in fact companies/<id>, but I honestly don't mind, since the end result is the same. They get what they were searching for, and grouping/nesting of the paths is easier.\nHow feasible is it to be able to override the generated path?. ",
    "vtsao": "Is this a dupe of #702?. ",
    "rfielding": "The main issue is that when dealing with arbitrary REST requests, you generally are not constrained by existing standards.  You bind together a POST with an expected json input body, and an expected output json body; all strongly typed.\nBut uploading and downloading files is already highly standardized, and we have no say in how that looks.  For example, almost all toolkits and browsers when asked to upload a file, will use multipart/form-data, and we are not allowed to change it.  \nWhen it comes to file downloads, it is far more complex than a simple GET versus a URL and the bytes come back.  If there is an Etag in the header, it is actually a query to see if the file has changed and to only send back bytes if it has (and 304 otherwise).  If there is a range request in the GET, we absolutely must get back a properly formed range response.  This is only the simple cases, not even getting into the other caching mechanisms.  \nThere is a state-machine associated with up/down, and to transcode to grpc requires encoding these into a protobuf spec (which would be the same across applications).  But note that at the OS level, draining an incoming 4GB upload would generally yield a 4k buffer from os.Read, and there would be 1 million of them.  So when reading files coming up, or writing files going back out, the whole copy is generally one line like os.Copy(w,r); which is so efficient that it's actually faster to write it straight to disk (because it's a sequential write!) than it is to bother to try to hold it in memory.  \nWe are currently dealing with encrypted high-definition video streams, the sequential write to disk is fast enough to not bottleneck the encryption.  We might make a protobuf definiton to allow pure grpc clients to work, but I expect that performance might dictate that the mux just jump around grpc for draining files out to where they need to go (ie: the encode REST to grpc, then decode grpc to write to disk isn't technically necessary; and likely to be disastrous for performance.). @bjolletz if you are really manipulating actual http files, that's kind of the recommendation anyway.  you want fine-grained control over http.  you need multipart/form-data, range requesting, etag, detailed cookies, custom headers.\nit's a completely different use case from just trying to strongly-type a new service that isn't directly hit by the browser.. ",
    "bjolletz": "I'm having a similar issue where my grpc request contains both a file and some other properties. In my REST API, I would like to support this as a multipart/mixed request where the file will be one part and the other part will be a JSON object containing the rest of the properties.\nAccording to the above posts, the suggested way to deal with this is to implement a separate handler in the mux. So far so good. I can extract the file and my JSON object from the multipart request. But what is then the best way to actually create the grpc request and get it sent to the server? I've dug into the code but could not find an obvious place to jack into to achieve this. What I'd basically want to do is to let the grpc gateway handle the unmarshaling of my JSON object into my grpc request and then just add the file to the request.. I've figured out how to get hold of a grpc client and manually make a grpc call. But as far as I understand, this also then means that I must manually take care of everything that the auto-generated handler code usually does. Things like marshaling / unmarshaling JSON objects and grpc messages, connection management, error handling etc. This would force me to duplicate a lot of the auto-generated code which is not ideal.\nWhat I would really want is a way to be able to intercept the HTTP (multipart) request and transform it into something that can be handled by the regular auto-generated handler. Perhaps something like a HandlerWrapper as described here. The problem with this is that the auto-generated gateway code does not provide access to its own handlers since they are created as anonymous functions inside the generated \"RegisterXXXHandler\" function. If these would instead have been named functions, I could have made a wrapper handler which would manipulate the request and then call the regular auto-generated handler. As it is now, I don't really see a way of invoking the auto-generated handlers or parts of them if I choose to handle a route manually.\nIf this is not the way to go, it would at least be nice to be able to invoke the JSON / grpc marshalers to be able to transform my JSON part of the request into grpc and vice versa for the response.\nIf someone could provide an example of how to best handle a multipart POST request, this would be very much appreciated.. I'm aware that there is no support for handling multipart requests in the Google http annotations. That's why the suggestion earlier in this thread was to handle requests like this \"manually\" through the http.ServerMux. What I'm looking for is some guidance on how one would best implement such a manual handler.\nTypically, when handling a multipart request, what I would like to do is just tell grpc-gateway how the request should be transformed into a grpc message and then let the grpc-gateway handle the actual grpc communication as usual. So what I'm looking for is some way to \"hook back into\" the auto-generated request handlers once I have transformed the incoming multipart request.\nAs far as I can see, if you choose to use the http.ServerMux to handle certain requests manually, you need to handle everything from JSON/grpc conversion to grpc communication yourself, meaning that you need to duplicate a lot of code that is already present in the auto-generated handlers, but that you can not access. I guess what I'm looking for is that the auto-generated handlers would provide some kind of API so that it's possible to use parts of them even if you handle the request manually.. No I didn't find a way to manage multipart requests with grpc-gateway in a simple way. I haven't looked at grpc-gateway since then though.. I was just evaluating the gRPC gateway to see if it would fit our needs. But because of this and some other issues, we decided to not go ahead with the gRPC gateway.. ",
    "craigmonson": "same issue here.. ",
    "mcos": "@yugui What's the current state of this? . ",
    "abronan": "Although it's relatively easy to find a workaround when in need of such a message structure, supporting oneof at the root of the request body would be awesome \ud83e\udd17 \nIs there any help needed to push this PR forward?. ",
    "notbdu": "Closing this PR in favor of https://github.com/grpc-ecosystem/grpc-gateway/pull/462.. Fine with me. Thanks @glerchundi.. ",
    "peterebden": "Yes I have - I can see it on cla.developers.google.com.. I don't seem to be able to merge this - maybe cla-bot needs another message?. Sure, done. I did think about doing something like that at first but in the end got a little bit lazy with it :). ",
    "upeediak": "As AlekSi mentioned, it is missing includes. If you downloaded and extracted the protoc-xyz-os-....zip file, there should be an include folder. Make sure you move the content of that folder to somewhere under your PATH (eg: /usr/local/include/)\n. ",
    "oseiskar": "Encountered this issue and solved it on Debian Stretch Linux by installing libprotobuf-dev and libprotoc-dev packages (not sure which one was needed or both).\nThis was in a different context not related to grpc-gateway but googling the error brought me and hence this seems like the best forum for sharing this solution.. ",
    "lucasvo": "Bytes was implemented in: https://github.com/grpc-ecosystem/grpc-gateway/pull/489. @loderunner, thanks for implementing the bytes support.\nI've been wondering if anyone ever considered encoding bytes as a hex string instead of base64. I'd love to make this configurable. I've been digging through the different runtime.convert methods to see if there is any support for configuration in runtime.convert or in the ConvertFuncExpr. \nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/e651a16acb9bd6028d18bce8115de407cb2aa5f4/protoc-gen-grpc-gateway/descriptor/types.go#L202\nBut it looks like there isn't really anything that would do something similar. \nI might go ahead and propose an implementation but I've wanted to open the discussion here first to hear what you think.\n. Thanks for the comment @loderunner \nHaving spent a bit more time with protobuf in the past couple days your argument makes sense. I didn't know the base64 encoding was a protobuf default. \nHowever one small detail I'm trying to fix is the padding: Is there a way to get it to accept a padded value or remove the padding from the json response body?\n```\nURL is /v1/document/get/{documentIdentifier} & returns the same message  in the body.\n$ curl -X GET \"https://localhost:8082/v1/document/get/QM%3D%3D\" -H \"accept: application/json\" -k\n{\"error\":\"type mismatch, parameter: documentIdentifier, error: illegal base64 data at input byte 2\",\"code\":3}\n$ curl -X GET \"https://localhost:8082/v1/document/get/QM\" -H \"accept: application/json\" -k\n{\"documentIdentifier\":\"QA==\"}\n``\n. I can take care of changing this. I'll try to get a PR ready for this in the next couple of days.. @loderunner Hey, I just tried to getmake test` to pass and it looks like I'm stuck with incompatible versions. swagger-codegen generates code that references ByteArray in the generated code which is nowhere to be found when I install version 1.2.6 (as described in CONTRIBUTING.md) when I run the newest version of swagger-codegen there are other issues. Do you know what version of swagger-codegen you ran to get it to work?\n```\ngithub.com/grpc-ecosystem/grpc-gateway/examples/clients/abe/abe\nexamples/clients/abe/abe/CamelCaseServiceNameApi.go:4:5: imported and not used: \"strings\"\nexamples/clients/abe/abe/CamelCaseServiceNameApi.go:5:5: imported and not used: \"fmt\"\nexamples/clients/abe/abe/ExamplepbABitOfEverything.go:20:17: undefined: ByteArray\n```\n. @loderunner would love to get your feedback on #565 !. Signed the CLA.. @loderunner did the heavy lifting, he deserves the credit! Thanks for helping out with this. . ",
    "loderunner": "@lucasvo I went for base64 because it is the standard mapping for protobuf bytes. bytes were already handled for methods other than GET that supported sending JSON in the body. It did indeed use the standard base64 mapping from protobuf. I only added support for bytes fields in as URL variables.\nI'm curious as to how you would want to configure this. It seems to me the operation of the gRPC protoc plugin is fully automatic, and based on the HTTP annotations. Also, wouldn't this break the JSON mapping of protocol buffers? How would you handle having a message be hex characters in a URL variable, but base64 in the request body?\nI'm interested in how you'd do this, keep me posted if you get started, and don't be shy to ask questions.. I agree. I first thought I'd use url-safe base64 encoding with no padding, to make sure we could send it in a URL. But I now think the discrepancy between the JSON body and the URL variable is unacceptable. We'll just have to make sure unsafe characters are correctly escaped.\nI won't be doing it right now, but if you want to check, I can give you a couple of pointers.\n\nThe decoding function\nDecoding BytesValue\n\nYou'll want to try replacing base64.RawURLEncoding with base64.StdEncoding. Keep me posted on how this goes.. The CI on travis validates against protoc 3.1.0 and swagger-codegen 2.2.2\nLook here and here to get the details.. Done.\nCould you help me out with Travis? I don't quite understand why CI is failing. . Travis is failing for Go 1.8.x, any idea why?. I'm getting different auto-generated files in the examples between 1.9 & master. I can't understand how to fix this. Is this a breaking change since 1.9? Or can I change this using some configuration of make?. Done. Thanks for checking that out !. I'm having the same problem in #489. It seems some commit between go 1.9 and master changes line breaks in autogenerated files, and the git status --porcelain fails.\nIf someone finds a fix to this, keep me posted on my PR, please.. Looks fine to me!\nThe burden will be on the client to correctly escape the query parameters to avoid /, + and =.  I wonder if this is worth documenting somewhere. Perhaps someone from the grpc-gateway project can help us here?. golang/protobuf uses the stdlib json encoding/decoding. The go structs generated from the proto file are annotated for JSON encoding.\nEncoding byte slices to base64 is part of the stdlib:\n\nArray and slice values encode as JSON arrays, except that []byte encodes as a base64-encoded string, and a nil slice encodes as the null JSON value.\n\nSource\nYou can find the source for the encoding here and the decoding here\nI think there is a bit of custom Marshal/Unmarshal code for handling enums and their string representations, but nothing for byte slices.. ",
    "ensonic": "/sub. \nAlso, is there a way to supply some option via command line parameters or the idea to supply a skelleton swagger.spec via\noption (grpc.gateway.protoc_gen_swagger.options.openapiv2_swagger) ...\nand then let the tool overwrite/add things parsed from the proto.. Any update?. Now from reading the code, this seems to not do what I think it would :/\nWe have a proto with e.g.\nproto\nservice AbcService {\n  rpc CreateAbc(Abc) returns (Abc) {\n    option (google.api.http) = {\n      post: \"/apis/abc/v1/abcs\"\n      body: \"*\"\n    };\n  }\nin the grpc service config, we have:\n```yaml\ntype: google.api.Service\nconfig_version: 3\nname: abc.endpoints.${GCP_PROJECT_ID}.cloud.goog\nendpoints:\n- name:abc.endpoints.${GCP_PROJECT_ID}.cloud.goog\n  allow_cors: true\ntitle: Abc gRPC API\napis:\n- name: domain.abc.v1alpha1.AbcService\n  version: 1.0.0\nauthentication:\n  providers:\n  - id: xxx_auth\n    ...\n  rules:\n  - selector: \"\"\n    allow_without_credential: false\n    requirements:\n    - provider_id: xxx_auth\nusage:\n  rules:\n  - selector: \"\"\n    allow_unregistered_calls: true\n``\nNow with or without usinggrpc_api_configuration`, I get the same resulting open-api.json. I was expecting:\n1.) the 'title' from the grpc yaml to go to '.info.description'.\n2.) security settings being configured\nShould any of those work?. Whatever changed, I can now build it! Thanks!. Nice, is a support for proto options? E.g. https://github.com/grpc-ecosystem/grpc-gateway/pull/521 introduced: \"grpc_api_configuration=path/to/your_api_config.yaml\".. Sound good, I am on it. See https://github.com/grpc-ecosystem/grpc-gateway/pull/632. > Could you also add a new target in //examples/proto/examplepb that runs with grpc_api_configuration set? Thanks!\nI was trying to add:\nprotoc_gen_swagger(\n    name = \"expamplepb_protoc_gen_swagger_with_services\",\n    grpc_api_configuration = \"unannotated_echo_service.yaml\",\n    proto = \":examplepb_proto\",\n)\nBut this will fail with:\nbazel build :expamplepb_protoc_gen_swagger_with_services \nERROR: file 'examples/proto/examplepb/unannotated_echo_service.swagger.json' is generated by these conflicting actions:\nLabel: //examples/proto/examplepb:expamplepb_protoc_gen_swagger, //examples/proto/examplepb:expamplepb_protoc_gen_swagger_with_services\nRuleClass: protoc_gen_swagger rule\nConfiguration: b93351232485d7aae598752ea518de7e\nMnemonic: SkylarkAction\nAction key: 497a439399336d44de55e732fde92588, ca9f888c655373c5b768919ee6510936\nProgress message: SkylarkAction examples/proto/examplepb/unannotated_echo_service.swagger.json\nPrimaryInput: File:[[<execution_root>]bazel-out/host/bin]external/com_google_protobuf/protoc\nPrimaryOutput: File:[[<execution_root>]bazel-out/k8-fastbuild/bin]examples/proto/examplepb/unannotated_echo_service.swagger.json\nOwner information: //examples/proto/examplepb:expamplepb_protoc_gen_swagger com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@99896fcf false (1448708908), //examples/proto/examplepb:expamplepb_protoc_gen_swagger_with_services com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@99896fcf false (431521607)\nMandatoryInputs: Previous action contains artifacts not in attempted action (first 5): \n    examples/proto/examplepb/unannotated_echo_service.yaml\nOutputs: are equal\nIf I uncomment the 'expamplepb_protoc_gen_swagger' target it works. I would like to investigate this separately (see #638), if that is okay with you. I am not sure about how to resolve this right now.\n. > Ah, that's totally fair, can you remove that entry from the BUILD file and I'll track doing it in #640\nYou mean removing the example that works right now:\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/master/examples/proto/examplepb/BUILD.bazel#L61\nCan we not change it when fixing #640?. This contains https://github.com/grpc-ecosystem/grpc-gateway/pull/632\nIf that is handled, I can rebase this one if needed.. Thanks for the quick review. Will get to it on Wed - tomorrow is public holliday and I am off the computer :). Yes, it is still blocked on the discussion in https://github.com/grpc-ecosystem/grpc-gateway/pull/632. . Thanks, rebased.. Travis might have some infrastructure failure, I don't see any build output:\n```\nbazel test //...\nINFO: Analysed 48 targets (27 packages loaded).\nINFO: Found 38 targets and 10 test targets...\nINFO: Elapsed time: 19.793s, Critical Path: 16.84s\nINFO: 196 processes: 182 linux-sandbox, 14 local.\nINFO: Build completed successfully, 212 total actions\n//codegenerator:go_default_xtest                                         PASSED in 0.2s\n//examples/integration:go_default_xtest                                  PASSED in 0.6s\n//protoc-gen-grpc-gateway/descriptor:go_default_test                     PASSED in 0.2s\n//protoc-gen-grpc-gateway/gengateway:go_default_test                     PASSED in 0.2s\n//protoc-gen-grpc-gateway/httprule:go_default_test                       PASSED in 0.5s\n//protoc-gen-swagger:go_default_test                                     PASSED in 0.1s\n//protoc-gen-swagger/genswagger:go_default_test                          PASSED in 0.2s\n//runtime:go_default_test                                                PASSED in 0.2s\n//runtime:go_default_xtest                                               PASSED in 0.3s\n//utilities:go_default_xtest                                             PASSED in 0.2s\nExecuted 10 out of 10 tests: 10 tests pass.\nThere were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.\n. Do we have docs that list all options for protoc_gen_swagger ?. Just realize this is only the first step, we track down, what needs to be done to make this appear in the final json too.. Now it should be good to go.. Hi, I only get the bazel build to work.\nWhen I try to build with make, I get errors:\nhttps://gist.github.com/ensonic/173b3cfb6032ee730320729d356f8534\n. Update when I run 'make test' localy I get:\ngo test -race github.com/grpc-ecosystem/grpc-gateway/...\nwarning: ignoring symlink /usr/local/google/home/ensonic/go/src/github.com/grpc-ecosystem/grpc-gateway/bazel-bin\nwarning: ignoring symlink /usr/local/google/home/ensonic/go/src/github.com/grpc-ecosystem/grpc-gateway/bazel-genfiles\nwarning: ignoring symlink /usr/local/google/home/ensonic/go/src/github.com/grpc-ecosystem/grpc-gateway/bazel-grpc-gateway\nwarning: ignoring symlink /usr/local/google/home/ensonic/go/src/github.com/grpc-ecosystem/grpc-gateway/bazel-out\nwarning: ignoring symlink /usr/local/google/home/ensonic/go/src/github.com/grpc-ecosystem/grpc-gateway/bazel-testlogs\ngoogle.golang.org/grpc/grpclb/grpc_lb_v1/messages\n../../../google.golang.org/grpc/grpclb/grpc_lb_v1/messages/messages.pb.go:59:30: undefined: proto.InternalMessageInfo\n../../../google.golang.org/grpc/grpclb/grpc_lb_v1/messages/messages.pb.go:112:31: undefined: proto.InternalMessageInfo\n../../../google.golang.org/grpc/grpclb/grpc_lb_v1/messages/messages.pb.go:160:40: undefined: proto.InternalMessageInfo\n../../../google.golang.org/grpc/grpclb/grpc_lb_v1/messages/messages.pb.go:302:47: undefined: proto.InternalMessageInfo\n../../../google.golang.org/grpc/grpclb/grpc_lb_v1/messages/messages.pb.go:358:33: undefined: proto.InternalMessageInfo\n../../../google.golang.org/grpc/grpclb/grpc_lb_v1/messages/messages.pb.go:441:41: undefined: proto.InternalMessageInfo\n../../../google.golang.org/grpc/grpclb/grpc_lb_v1/messages/messages.pb.go:590:48: undefined: proto.InternalMessageInfo\n../../../google.golang.org/grpc/grpclb/grpc_lb_v1/messages/messages.pb.go:639:32: undefined: proto.InternalMessageInfo\n../../../google.golang.org/grpc/grpclb/grpc_lb_v1/messages/messages.pb.go:698:28: undefined: proto.InternalMessageInfo\nok      github.com/grpc-ecosystem/grpc-gateway/codegenerator    0.541s\ngithub.com/grpc-ecosystem/grpc-gateway/examples/clients/abe\nexamples/clients/abe/examplepb_a_bit_of_everything.go:47:13: undefined: ExamplepbNumericEnum\nexamples/clients/abe/examplepb_a_bit_of_everything.go:63:22: undefined: ExamplepbNumericEnum\nexamples/clients/abe/examplepb_a_bit_of_everything.go:73:22: undefined: ExamplepbNumericEnum\nFAIL    github.com/grpc-ecosystem/grpc-gateway/examples/integration [build failed]\nok      github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway/descriptor   0.569s\nok      github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway/gengateway   0.590s\nok      github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway/httprule 0.556s\nok      github.com/grpc-ecosystem/grpc-gateway/protoc-gen-swagger   0.549s\nok      github.com/grpc-ecosystem/grpc-gateway/protoc-gen-swagger/genswagger    0.554s\nFAIL    github.com/grpc-ecosystem/grpc-gateway/runtime [build failed]\nok      github.com/grpc-ecosystem/grpc-gateway/utilities    0.549s\nMakefile:169: recipe for target 'test' failed\nmake: *** [test] Error 2\n``\nLooks like https://github.com/google/protobuf/issues/4582. Will investigate.\nIn any case those are different than what travis reported and don't seem to be related to my change.. Hi, I really need help to understand why the tests fail. What exactly should I run locally to build/run not using bazel. I have the git rooted in my gopath:\n/home/ensonic/go/src/github.com/grpc-ecosystem/grpc-gateway\n. No, there is the dependent rule\n$(OPENAPIV2_GO): $(OPENAPIV2_PROTO) $(GO_PLUGIN)\n    protoc -I $(PROTOC_INC_PATH) --plugin=$(GO_PLUGIN) -I. --go_out=$(PKGMAP):$(GOPATH)/src $(OPENAPIV2_PROTO)\nwhich builds protoc-gen-swagger/options/openapiv2.proto protoc-gen-swagger/options/annotations.proto  and produces the respective '.pb.go' files.\nmake protoc-gen-swagger/options/openapiv2.pb.go\nmake: 'protoc-gen-swagger/options/openapiv2.pb.go' is up to date.\nI've fixed one test by updating proto-gen-go (go get -u github.com/golang/protobuf/protoc-gen-go).\nFor the \"github.com/grpc-ecosystem/grpc-gateway/examples/integration\" you seem to have a mix of using the new \"context\" and the old \"golang.org/x/net/context\". I don't know how to resolve that.\nhttps://gist.github.com/ensonic/1ddd754610de01ed99bb55ab52c7579b\nFYI: I also get\nFAIL    github.com/grpc-ecosystem/grpc-gateway/examples/integration [build failed]\nwithout my change (using go version go1.10.2 linux/amd64). We will just use my fork for now, but I'd love to understand why your Go1.10.X builders would work and mine doesn't (also other PRs seem to have the same problem). Maybe consider adding some docs to make life for contributors easier.\n. Still the same. For using it with bazel I can simply run: bazel test //... and it passes:\n```\nINFO: Elapsed time: 55.135s, Critical Path: 25.99s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\nINFO: 515 processes: 515 linux-sandbox.\nINFO: Build completed successfully, 558 total actions\n//codegenerator:go_default_xtest                                         PASSED in 0.2s\n//examples/integration:go_default_xtest                                  PASSED in 0.6s\n//protoc-gen-grpc-gateway/descriptor:go_default_test                     PASSED in 0.2s\n//protoc-gen-grpc-gateway/gengateway:go_default_test                     PASSED in 0.2s\n//protoc-gen-grpc-gateway/httprule:go_default_test                       PASSED in 0.2s\n//protoc-gen-swagger:go_default_test                                     PASSED in 0.2s\n//protoc-gen-swagger/genswagger:go_default_test                          PASSED in 0.2s\n//runtime:go_default_test                                                PASSED in 0.2s\n//runtime:go_default_xtest                                               PASSED in 0.2s\n//utilities:go_default_xtest                                             PASSED in 0.2s\nExecuted 10 out of 10 tests: 10 tests pass.\nINFO: Build completed successfully, 558 total actions\n```\nRunning make test still fails for me with some error:\n```\nmake test\nmkdir -p _output\nprotoc -I /usr/bin//../include -I. --plugin=bin/protoc-gen-go --go_out=Mgoogle/protobuf/descriptor.proto=github.com/golang/protobuf/protoc-gen-go/descriptor,Mexamples/proto/sub/message.proto=github.com/grpc-ecosystem/grpc-gateway/examples/proto/sub,plugins=grpc:_output examples/proto/sub/message.proto\ncp _output/github.com/grpc-ecosystem/grpc-gateway/examples/proto/sub/message.pb.go examples/proto/sub/message.pb.go || cp _output/examples/proto/sub/message.pb.go examples/proto/sub/message.pb.go\ncp: cannot stat '_output/github.com/grpc-ecosystem/grpc-gateway/examples/proto/sub/message.pb.go': No such file or directory\nmkdir -p _output\nprotoc -I /usr/bin//../include -I. --plugin=bin/protoc-gen-go --go_out=Mgoogle/protobuf/descriptor.proto=github.com/golang/protobuf/protoc-gen-go/descriptor,Mexamples/proto/sub/message.proto=github.com/grpc-ecosystem/grpc-gateway/examples/proto/sub,plugins=grpc:_output examples/proto/sub2/message.proto\ncp _output/github.com/grpc-ecosystem/grpc-gateway/examples/proto/sub2/message.pb.go examples/proto/sub2/message.pb.go || cp _output/examples/proto/sub2/message.pb.go examples/proto/sub2/message.pb.go\nprotoc -I /usr/bin//../include -I. -Ithird_party/googleapis --plugin=bin/protoc-gen-go --go_out=Mgoogle/protobuf/descriptor.proto=github.com/golang/protobuf/protoc-gen-go/descriptor,Mexamples/proto/sub/message.proto=github.com/grpc-ecosystem/grpc-gateway/examples/proto/sub,plugins=grpc:. examples/proto/examplepb/echo_service.proto examples/proto/examplepb/a_bit_of_everything.proto examples/proto/examplepb/stream.proto examples/proto/examplepb/flow_combination.proto examples/proto/examplepb/wrappers.proto examples/proto/examplepb/unannotated_echo_service.proto examples/proto/examplepb/response_body_service.proto\ngo build -o bin/protoc-gen-grpc-gateway github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway\ngithub.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway/descriptor\nprotoc-gen-grpc-gateway/descriptor/services.go:146:49: opts.ResponseBody undefined (type *annotations.HttpRule has no field or method ResponseBody)\nMakefile:139: recipe for target 'bin/protoc-gen-grpc-gateway' failed\n```\nFor the CI errors it looks like protoc-gen-swagger/options/openapiv2.pb.go needs to be re-generated, but your non-bazel build setup does not take care of it.. This looks better. The part about regenerating files should be one of the commands in 'how-to-contribute' and should be done before sending a PR :)\n. Sorry for being such a pain to get this in and thanks for merging!. Getting the same issue when pulling the new version into a bazel build.. I believe it is related to you shipping a copy of the annotations.proto here:\nhttps://github.com/grpc-ecosystem/grpc-gateway/tree/master/third_party/googleapis/google/api\nIs there a reason for you to not use the upstream version?. In my bazel WORKSPACE file:\ngo_repository(\n    name = \"grpc_ecosystem_grpc_gateway\",\n    #commit = \"844b78d9a291695a45d1d826fda9c1b95a2ddfc1\",\n    #importpath = \"github.com/grpc-ecosystem/grpc-gateway\",\n    # TODO(ensonic): switch back after PR #644 has been merged\n    commit = \"4e0ecd0f283a990d61c867c2e55635a11931e2aa\",\n    importpath = \"github.com/ensonic/grpc-gateway\",\n)\nThe uncommented version is the time of my PR and everything builds. When I swapp to the commented part (the merged PR), then I also get all the changes in between and now the build fail:\nGoCompile: error running compiler: exit status 2\nhome/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/sandbox/linux-sandbox/43/execroot/myproject/external/grpc_ecosystem_grpc_gateway/protoc-gen-grpc-gateway/descriptor/services.go:146:49: opts.ResponseBody undefined (type *annotations.HttpRule has no field or method ResponseBody)\nSomehow the build is picking up an outdated copy of the http.proto. I am digging deeper.\n```\n$ find home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/ -name \"http.proto\"\n/home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/external/com_github_googleapis_googleapis/google/api/http.proto\n/home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/external/go_googleapis/google/api/http.proto\n/home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/external/grpc_ecosystem_grpc_gateway/third_party/googleapis/google/api/http.proto\n/home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/external/bazel_source/third_party/googleapis/google/api/http.proto\n$ find /home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/ -name \"http.proto\" -exec grep -Hn \"response_body\" {} \\;\n/home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/external/com_github_googleapis_googleapis/google/api/http.proto:355:  string response_body = 12;\n/home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/external/go_googleapis/google/api/http.proto:355:  string response_body = 12;\n/home/ensonic/.cache/bazel/_bazel_ensonic/556f802681b2087c6d9a6bcc3d0d954b/external/grpc_ecosystem_grpc_gateway/third_party/googleapis/google/api/http.proto:303:  string response_body = 12;\n```\nSo the bazel_source/third_party/googleapis/google/api/http.proto is too old and for some reason it hit this one.. SOLVED: We also imported org_golang_google_genproto in our WORKSPAE but at an older version,w hich apparently shadowed yours. Sorry for the noise.. Done.. Shouldn't that be generated from \"message Info\" define in openapiv2.proto? Also when I build it with bazel it actually works. I am getting the license fields in the generated json.. ",
    "alecholmez": "All of gRPC-go needs to switch to context for that matter. ",
    "bamnet": "Two implementation ideas come to mind:\n1. Pass title and version as CLI flags.\n2. Parse values from the proto.  The service name in the proto coud map to the swagger title and some clever regexing could extract version info from the http paths /v1/method => \"v1\".. I'm not as familiar with the landscape of plugins using custom options for configs, can you point me to a sample or two in the wild using custom options?  I'm curious to see the other use cases for them.\nSticking this in the proto is a much better idea than passing them as CLI flags \ud83d\ude04, but I'm not sure of a simple use case where service couldn't translate to title, and version couldn't be parsed from an http annotation following the style guide. Getting a basic version and service name automagically might be helpful for developers who don't want to do any work to the custom options.  I can see options giving developers more control if they're not satisfied with the automatic extraction.. ",
    "erwinvaneyk": "It might be more consistent with other configuration for particular plugins to add it as options in the proto file: https://developers.google.com/protocol-buffers/docs/proto3#options. ",
    "Raviaryacsc": "What is the reasoning behind marshaling the int64, uint64 to string instead of number.  unit32, float, doubles are marshaled as numbers. Is not it a bug which should be fixed?\nSuggestion of using timestamp for epoch is not going to work.\n. Proto3 to JSON Mapping:\nint64, uint64    --->  String\nfloat, double    ----> number.\nDoes it seems right?  I see point in your reply however should not this logic should apply to floating point, doubles instead of 64 bit integers?. Exactly. Double too fits in that requirement and should be JSONed as string instead of number why it is not ?\nAlso, JSON does not defined that Unit64 should be represented as string in JSON. it is a shortcoming of JS and not JSON. We can create the JSON with big numbers by many ways and JS is going to have problem with that.  Then why this trick is done in Proto3 to Json mapping?\nToday JSON is used with all sort of different languages and not just ties to JS.\nI am fine with writing Un-Marshaller to convert String to Unit64 based on the field Name and Type and continue with my stuff but I see this is not a logical mapping.\n. ",
    "kurin": "I'd like to revisit/reopen this if possible.  While the proto3 spec defines the int64/uint64 json representations as strings, and the above discussion points out the good reasons for that, the fact is that very often json is used as an interchange format between parties who aren't using javascript.\nAnyone moving to a protobuf service definition from a different system is probably going to hit a particularly difficult snag when they need to send timestamps, file sizes, or any other data that can reasonably expected to grow beyond a 32 bit representation.\nFor my part, I had to copy and paste not only the jsonpb package (and then remove seven lines), but I had to copy a lot of the runtime package as well.\nI don't know how to provide this behavior without also modifying jsonpb (and having to make my case there as well), but would you be open to providing a toggle that would enable this behavior?. In the specific scenario in which I find myself, I am emulating an already-existing service whose API I can't modify, and sometimes I really do need to send integers that are >2^54 or whatever it is (for example, identifiers sampled uniformly over uint64).  But also it's semantically gross.\n(I am actually a little bit surprised that json parsers will barf when given a \"string\" instead of an \"int\"; Go's encoding/json can be told to expect a value in quotes, but will then fail if it comes without quotes.  In the interests of client compatibility, this isn't something I want to have to worry about.). That's basically what I'm saying, I haven't done a survey but I strongly suspect that most non-JS marshallers are like \"yeah that's an int it's fine.\"  Also a quick peek at the RFC seems also to suggest that implementations are free to do whatever they want with the \"number\" type.\nI'm not saying it's a great idea to do this, but it's a better than copying and modifying packages locally.  Right now there are json endpoints that are emitting large ints as json numbers, and unless we know that all json clients can accept strings or numbers without modification (and we know that for encoding/json in fact they can't) then people are going to be doing end-runs around this part of the implementation no matter how well-meaning it is.. I mean, again, we don't always have the luxury of those choices.  There is a large number of existing JSON APIs that use JSON numbers for integers with more than 53 bits.  The standard json package even encodes int64 as a number.  Switching away from JSON, or encoding those values as strings, would break users.. There are multiple clients in many languages, but to pick two I know of: Java, Go.. I'm sorry, I think there's some confusion in this thread about the kinds of ways in which API creators and implementors can get locked into an API design.\nI am in the middle of implementing the server side of a JSON API I do not own and cannot change.  The API is defined by a third party with whom I have no say, and one of the parts of the definition is that numbers (such as files sizes into the many TB) are JSON numbers.\nHowever, even if I could change the JSON definition, it would not be possible to do so without breaking the many, many, many clients that have already been written to target this API, some of which are on github, but many of which are possibly proprietary.\nBut even if I could contact all the owners of client implementations, telling them to change their client behavior to support a third party service is a complete non-starter.\nAnd even if I could change the official implementation and all the API docs, it would still break all the clients that all work today, and be a terrible thing to do.\nI know that the proto3 spec specifies that 64 bit ints get marshalled as strings, and I know that this is because javascript implements numbers as floats, but please believe me when I say that even in the face of all of that, @DoctorQ's solution is sometimes more correct than trying to \"fix\" the API.. I'm not going to lie, having to use float64 instead of a more semantically appropriate type to hold integer valued data because a different programming language that I don't use is broken makes me a little bit resentful.\nI don't think (but I'm not sure) that any of my integer values goes above 2^53; maybe they will fit in floats.\nBut this is really, IMO, better served by giving users a knob, even if the knob has a warning label on it.. The thing I actually ended up doing was more or less identical to what @DoctorQ suggested above: https://github.com/kurin/blazer/commit/ee030c6b8466f9519fdc3475857333637d17b898.  (And note that this commit is as old as this conversation; I'm not advocating for my specific use-case, I'm pointing out that this is sometimes going to be the least worst thing to do.)\nBut doing this will cause my copy of jsonpb to fall behind the released version.  Mine is a very quick-and-dirty implementation, so I don't really care, but I feel like @johanbrandhorst's response disregards tradeoffs that could be rationally made.  (e.g. for two Go endpoints, for example, trading large ints via floats will introduce strictly more risk of silently casting too large an int.)  But any custom marshaller is going to effectively do the thing that has big warning signs all over it here.  If we want the hoops that users have to jump through to get that to take hours and be the rather sizable yak that that would be, well, okay.  But if we agree that, despite the risks, it is a thing users should sometimes do, I would want to make it easy to do.. ",
    "DoctorQ": "DANGER DANGER DANGER THIS WILL SILENTLY DESTROY YOUR DATA IF YOU FOLLOW THIS ADVICE\nhttps://github.com/golang/protobuf/blob/master/jsonpb/jsonpb.go \nneedToQuote := string(b[0]) != `\"` && (v.Kind() == reflect.Int64 || v.Kind() == reflect.Uint64)\n    if needToQuote {\n        out.write(`\"`)\n    }\n    out.write(string(b))\n    if needToQuote {\n        out.write(`\"`)\n    }\nDANGER DANGER DANGER THIS WILL STILL SILENTLY  DESTROY YOUR DATA\nadd needToQuote = false\nthen \nDANGER DANGER DANGER ARE YOU STILL IGNORING THE WARNINGS? THIS REALLY WILL DELETE DATA WITHOUT WARNING YOU\nneedToQuote := string(b[0]) != `\"` && (v.Kind() == reflect.Int64 || v.Kind() == reflect.Uint64)\n        needToQuote = false\n    if needToQuote {\n        out.write(`\"`)\n    }\n    out.write(string(b))\n    if needToQuote {\n        out.write(`\"`)\n    }\nDANGER DANGER DANGER NO, SERIOUSLY -- THIS WILL BREAK YOU IN UNANTICIPATED WAYS THAT ARE INCREDIBLY DIFFICULT TO DIAGNOSE.\nfor in need\n. ",
    "jsw": "Also oneof?. ",
    "veqryn": "OpenAPI v3.0.0 adds support for specifying multiple servers (hosts).. @yugui shouldn't the content type of the response be application/x-ndjson instead of application/json?. Sure:\nhttp://ndjson.org/\nand\nhttps://github.com/ndjson/ndjson-spec\n. I am not sure honestly.  I don't think there is an RFC yet.... K. They should probably get an RFC in order.. Hm... this would mean making every single proto response message into a repeated field called `data.\nThat may work, but I was hoping not to have to mangle the proto/grpc side just to get the rest side looking good.\nIs it possible to change result to data for streaming responses?\n. I guess I could try at least.\nIs this where my change would go?\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/ab0345bb328757bfef2f3d7d4e642e182eb985b9/protoc-gen-swagger/genswagger/template.go#L92\n. K, I will give that a try.. ",
    "jones77": "OpenAPI 3.0 adds support for nullable types, 2.0 doesn't support them: https://github.com/swagger-api/swagger-editor/issues/1364#issuecomment-309530250. ",
    "dingqs447": "@rogerhub Thank you for your reply, I try your method. @rogerhub I tried your advice, the problem remains the same. At the same time I updated my question. ",
    "sreddybr3": "used this to get base64 encode string value of the image\nwith open(\"small_dog.jpg\", \"rb\") as imageFile:\n        str = base64.b64encode(imageFile.read())\n    print(str)\nthen \ncurl -X POST http://localhost:8080/v1/predict \\\nwith the body as below\n{\"model_spec\": {\"name\": \"inception\", \"signature_name\": \"predict_images\"}, \"inputs\": {\"images\": {\"dtype\": 7, \"tensor_shape\": {\"dim\":[{\"size\": 1}]}, \"string_val\": [\"\\/9j...  ..  ...  ...  =\"]}}}\nTested with different models and works like a charm. ",
    "paulbdavis": "I was looking for this same answer and eventually came on this solution, for anyone else looking\ngolang\nopts := []grpc.DialOption{grpc.WithDefaultCallOptions(grpc.MaxCallRecvMsgSize(16777216))}\nThis will set that message size on all calls for that endpoint hndler, not sure if this is the best solution, but it worked for me. ",
    "UladzimirTrehubenka": "From https://stackoverflow.com/questions/45901389/why-golang-grpc-gateways-get-route-matches-post-request\nBy spec, it is allowed to encode GET request into a POST method with Content-Type: application/x-www-url-encoded.\nSee https://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/mux.go#L254\nSo the request routing table in grpc-gateway tries to fallback from POST method to GET when the Content-Type of your request is application/x-www-url-encoded.\nFrom https://groups.google.com/d/msg/grpc-io/Xqx80hG0D44/1gwmwBcnNScJ\n(Note) URL has a length limitation (go/longer-urls). It's enfoced by some browsers and\nproxies. If your GET request exceeds the limitation, browser may reject to send them.\nYou may change to use a POST request with content type x-www-form-urlencoded instead.\nAnother one example:\n```\nrpc Read(ReadReq) returns (ReadResp) {\n  option (google.api.http) = {\n  get: \"/api/v1/user/{id}\"\n};\nrpc Create(CreateReq) returns (CreateResp) {\n  option (google.api.http) = {\n  post: \"/api/v1/user\"\n};\n```\ncurl -X POST http://127.0.0.1:8040/api/v1/user/127 -d '{}'\n200 OK (\"Content-Type: application/x-www-url-encoded\" is in header and POST becomes GET, actually we have no route POST \"/api/v1/user/{id}\" at all)\ncurl -X POST http://127.0.0.1:8040/api/v1/user/127\n501 NOT_IMPLEMENTED (expected behaviour)\nThe question is - how to disable this behaviour? This is weird that any \"Content-Type\" causes expected error 501 NOT_IMPLEMENTED except \"application/x-www-url-encoded\".. Separate option would be good enough. I just wondering do we have some workaround right now?. I guess generator option,  BTW could we have some global option in proto to disable \"application/x-www-url-encoded\" for all POST calls? Probably if we will have a lot of POST calls it will be not so useful to disable \"application/x-www-url-encoded\" for each call explicitly?. See https://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/mux.go#L254. @yugui could you please provide some comments?. Any progress here?. > add an option that sets a variable\nDon't follow - where we need to set the option?. As I understand you propose to add another one ServeMuxOption and set the override behaviour at runtime?. I like this solution, so I try to create PR for fix this.. The PR has been added.. ",
    "buchanae": "Does WithMarshalerOption solve your issue? \nWe use this to configure a jsonpb.Marshaler the way we like:\ncustomMar := jsonpb.Marshaler{\n    Indent: \"  \",\n        EmitDefaults: true,\n}\nmar := runtime.JSONPb(customMar)\nrpcMux := runtime.NewServeMux(runtime.WithMarshalerOption(\"*/*\", &mar)). Oh, no, it does not solve your issue. The offending line is here:\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/marshal_jsonpb.go#L114\nThat unmarhsaler should be configurable. Maybe run.time.JSONPb.SetUnmarshaler() should be added?. ",
    "racevedoo": "@buchanae @Lantame any update on this? I'm having the exact same problem. ",
    "MaerF0x0": "It looks like maybe? https://godoc.org/github.com/golang/protobuf/ptypes/struct#ListValue  Just figuring out how to use it now. . ListValue did it. It looks like one probably can use those types to force whatever json kinds one wants   @yugui  Do you want me to contrib some docs to help the next person? \n. So I finally figure how to abuse grpc-gateway to make it have the HTTP interface we want, but now the go code is heinous. :-/ \n```\nsyntax = \"proto3\";\npackage somesvc;\nimport \"google/api/annotations.proto\";\nimport \"google/protobuf/struct.proto\";\nservice SomeSvc {\n  rpc OutgoingListValues(Nil) returns (stream SomeListValues) {\n    option (google.api.http) = {\n      get: \"/listvalue\"\n    };\n  }\nrpc IncomingListValues(SliceListValues) returns (Nil) {\n    option (google.api.http) = {\n      post: \"/listvalue\"\n      body: \"*\"\n    };\n  }\n}\nmessage Nil {}\nmessage SliceListValues{\n  repeated google.protobuf.ListValue rows = 1;  // json => {\"rows\": [[\"a\", 1, true, null],...]}\n}\nmessage SomeListValues{\n  google.protobuf.ListValue row = 1;  // stream of json => {\"row\": [\"a\", 1, true, null]}\n}\n```\nbut the outgoing go looks like this (absurd).\ngolang\n    srv.Send(&somesvc.SomeListValues{\n        Row: &structpb.ListValue{\n            Values: []*structpb.Value{\n                &structpb.Value{Kind: &structpb.Value_StringValue{StringValue: \"a\"}},\n                &structpb.Value{Kind: &structpb.Value_NumberValue{NumberValue: 1.0}},\n                &structpb.Value{Kind: &structpb.Value_BoolValue{BoolValue: false}},\n                &structpb.Value{Kind: &structpb.Value_NullValue{NullValue: structpb.NullValue_NULL_VALUE}},\n            },\n        },\n    })\nideally it would just be: \ngolang\n    srv.Send([]interface{}{\"a\", 1.0, true, nil})\n. @achew22  Let me know if you'd like some docs surrounding ^^? I learned how to use the google well known types to make GRPC-gateway have the interface I like, but it also means the GRPCServer interface becomes gnarly. \n. ",
    "tgeng": "Normally proto field names follow snake_case and proto compiler converts them to camel case for JSON mapping. As a result, Rest API clients uses camel cases for json fields when talking to gRPC servers behind the grpc gateway.\nThis snake_case <-> camelCase conversion is not honored for URL parameters. As a result, client has to use snake_case for URL parameters. This creates an inconsistency between names of JSON objects in HTTP body and names of URL parameters.\n. The build failure is caused by the master branch not being updated after the grpc dependency change, which is out of the scope of this change.. Thanks achew22! Updated the PR with tests.. Weird, I am not sure why cla/google check does not go through. I am using my @google account so it should be good. Any idea what might go wrong?. Hi all, I am guessing an author of this commit might not have signed cla/google. I am a Google employee and this account is associated with my corporate account, so I should not need to sign anything.. Hmm it looks like cla/google check is stuck this time. Any suggestions on what I can do next?. Hi achew22, it seems cla/google check is still stuck. Can you take a look? Thanks!!. Wonderful! Thanks!!\nOn Mon, Oct 2, 2017 at 5:07 PM Andrew Z Allen notifications@github.com\nwrote:\n\nMerged #450 https://github.com/grpc-ecosystem/grpc-gateway/pull/450.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc-ecosystem/grpc-gateway/pull/450#event-1275082110,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AcNsAlaGk3pz3lkU5scypAu-Wywsahisks5soXrSgaJpZM4PJdiX\n.\n. Weird, the cla/google check get stuck again .... Wow that was super fast! Thanks!!. \n",
    "jacksontj": "cc @tmc (seem to be attached to a lot of recent commits). I signed it!. @tmc done, and I added a commit to regenerate the examples (which makes CI pass). @achew22 I've added some documentation clarifying, let me know if that isn't clear enough (and if not, how to change it ;) ). Forgot to regen the examples, just did-- this run should pass.. The default json marshaler. My concern is primarily that the docs say Mapping streaming APIs to newline-delimited JSON streams and upon updating this library (not my code, proto files, or marshaler) there are no longer newlines delimiting chunks. It's also concerning the brevity of the commit message, since it implies that it was fixing something (when it seems to be breaking the documented and previous functionality).. From looking more in the code it seems that if the json marshaller doesn't implemented the delimited interface it defaults to no delimiter (https://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/handler.go#L41) which seems wrong. My expectation would be the default would be a newline (as defined in documentation, and the behavior of previous releases) and if the user wanted to have something else (such as no-delimiter) they'd implement the interface.. I've submitted a PR which maintains the new interface, but keeps it backwards compatible.. Looking at the CI failure it seems unrelated?. @achew22 rebased. Added a test case for it, and rebased again (someone changed the template file ;) ). @achew22 test added, CI is passing. @achew22 ping (in case the last one got lost in the inbox :) ). ",
    "iluminae": "so this is fantastic and is an obvious advantage over making the TCP call over loopback to achieve the same functionality - but may I bring up that using this bypasses grpc interceptors and stats handlers - which at least some of us use for authentication. (like this guy suggests: http://mycodesmells.com/post/authentication-in-grpc)\nShould we open another issue for how to support grpc constructs using this direct call method? Or, is this meant for those not using grpc featurettes like interceptors?. ",
    "hectorj": "I thought I had in the past, and I just signed it again to be sure. ~I don't know what the cla bot is doing.~ Edit : it just went green :+1: . Done :slightly_smiling_face: . ",
    "cad": "@achew22 How perfect do you think it can get?\nI am going to use it to differentiate gRPC traffic from gateway traffic and all the clients are untrusted 3rd parties. \nIn that case, I fail to see how crypto assertion solution would fit into my problem as key-distribution to untrusted parties itself is a pickle and also it seems like a very DIY solution to my problem.\nSo don't we have a simple flag or something like that, in the codebase? . @achew22 Thanks. But I can't use it since, as I mentioned before, the clients are untrusted. Also, you can assume some of the endpoints are public.\nOn the other hand, I don't think that it is client's job to declare to the server that whether it's going to hit a gRPC endpoint or a grpc-gateway endpoint, in-band. \nThe server should know this already.\nAny other suggestions?. For future reference,\nCurrently, if your clients are trusted you can use @achew22 's solution.\nIf your clients are untrusted public clients then there are no means gRPC and the grpc-gateway provide for you to distinguish between gateway traffic and gRPC traffic currently.. ",
    "wimspaargaren": "Hi all, Can someone tell me if there's still any work being done to support HttpBody proto as response?. Sure, I know it needs a rebase, but could you enhance a bit if there need to be more adjustments before this can be merged?. Allright! Flag or env var would work fine for me. I'll propose my own PR on Monday then.. I signed it!. I signed it!. Not sure if it is added at the right place, but I added a small README for the httpbody :). No problem!. Me neither. I did however accidentally commit a changed mod/sum and replaced it with the original afterwards. Maybe something went wrong there.. Changed it back and assert it at marshalling time. What do you think of this implementation?. The go mod and sum contain a new line at the end of the file. My editor removed these, but I added them again.. Adjusted.. Fixed.. Good point :). Done.. I added the service around the function definition.. Seems to be fixed now.. ",
    "MatthewDolan": "@tmc thanks.\nThe build is passing now.. @tmc Is there anything else I need to do here?. Once the ProtoMarshaller is added as a marshaller on the server side, you can do \"proto over http.\" The client can send an http request with a content type \"application/octet-stream\" where the body is a serialized proto and the gateway will be able to handle that.. This has been resolved.. Re-opened. Thanks.. ",
    "glerchundi": "Tested on our CI and it works as expected. Take into account that file.GoPkg.Path cannot be empty.\n@tmc PTAL.. do you agree @notbdu? :). @achew22 yep, it will. It's a breaking change. I can refactor to switch on a parameter or whatever and keep the old behaviour but the question is, what is desired? IMHO this is what end-user expects , me included. \nI can create another PR after this one to override go_package based path for one that lets user choose (or directly defaults to the old behaviour in case go_package is empty) which output folder to use.\nWDYT?. @tmc, @achew22 I changed the code to keep backward compatibility.. I don't think this is related to the PR itself...Flaky tests? IWOMM.... Any chance/free-slot to look at this, @achew22, @tmc? Thanks. Ok, i don\u2019t have much time but give me a day and will try to do ASAP.\nOn Wed, 8 Nov 2017 at 03:38, Andrew Z Allen notifications@github.com\nwrote:\n\n@glerchundi https://github.com/glerchundi, I just tagged a release\nwhich means now is a GREAT time to introduce breaking changes like this.\nPlease update the PR with some tests and let's get it merged.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc-ecosystem/grpc-gateway/pull/462#issuecomment-342691899,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACIPlotoPayi6JIiqEIYeSb0VzgclbOwks5s0RQYgaJpZM4PXHi5\n.\n. @achew22 PTAL. \ud83d\udc4f\ud83c\udffb\ud83d\udc4f\ud83c\udffb\n\nOn Wed, 8 Nov 2017 at 21:29, Andrew Z Allen notifications@github.com\nwrote:\n\nMerged #462 https://github.com/grpc-ecosystem/grpc-gateway/pull/462.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc-ecosystem/grpc-gateway/pull/462#event-1332670980,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACIPlmhVXbtS8Dsw-vbAF40DOU5v_UM2ks5s0g8TgaJpZM4PXHi5\n.\n. @c4milo can you give us more details? Example proto file which was working and not now, as well as an expected behaviour explanation.\n\nAlthough it should work as it was behaving before with just explicitly specifying the same go_package name as the one you're using in package.. Yeah, will do!\nOn Sun, 21 Jan 2018 at 04:56, Andrew Z Allen notifications@github.com\nwrote:\n\nCan you add a test to verify this functionality? I'd be very happy to\nmerge once that happens. Thanks!\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc-ecosystem/grpc-gateway/pull/529#issuecomment-359221972,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACIPlgK7bkV_RT-dop_ySAt2MrwAJgi3ks5tMrVfgaJpZM4Rix5-\n.\n. ready to review @achew22!. Haha, thanks!\n\nOn Sun, 21 Jan 2018 at 12:36, Andrew Z Allen notifications@github.com\nwrote:\n\nMerged #529 https://github.com/grpc-ecosystem/grpc-gateway/pull/529.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc-ecosystem/grpc-gateway/pull/529#event-1434023947,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACIPlv_afB9Hc-1Z6N8vPx9HhA_1-fXcks5tMyFJgaJpZM4Rix5-\n.\n. I've tested with this proto:\n\n```\n // Update an existing user.\n    rpc UpdateUser(UpdateUserRequest) returns (User) {\n        option (google.api.http) = {\n            patch: \"/api/v1/{user.name=installations//users/}\"\n            body: \"user\"\n        };\n    }\n[...]\nmessage UpdateUserRequest {\n    // The user resource which replaces the resource on the server.\n    User user = 2;\n// The update mask applies to the resource. For the `FieldMask` definition,\n// see https://developers.google.com/protocol-buffers/docs/reference/google.protobuf#fieldmask\ngoogle.protobuf.FieldMask update_mask = 3;\n\n}\n[...]\nmessage User {\n    // Relative resource name.\n    string name = 1;\n// Display name.\nstring display_name = 3;\n\n// Description.\nstring description = 4;\n\n}\n```\nAnd this worked:\ncurl -H \"Content-Type: application/json\" --request PATCH -d@./update_user.json localhost:8080/api/v1/installations/1/users/user2?update_mask=display_name\nWhere update_user.json is:\n{\n  \"display_name\": \"user3\" \n}\nSo i firmly believe that something is not working on your side. Are you using the standard go generator or something like gogo? Give us a full example and perhaps we can go further.. Good investigation.\n\nUpdate: my initial problems were indeed because I was using GoGo Protobuf. My intentions are actually to investigate whether this works with gogoprotobuf, but I wanted to get it working first, so thanks for pointing me in the right direction. I will see what would need to be done on the gogo/protobuf side, but I expect this will flat out not work with gogo/protobuf until an alternative implementation for\n\nBut I don't know why it's not working, proto.MessageName should work because gogo's fieldmask registers itself under the same name that the official fieldmak does:\nofficial: https://github.com/google/go-genproto/blob/master/protobuf/field_mask/field_mask.pb.go#L249\ngogo: https://github.com/gogo/protobuf/blob/master/types/field_mask.pb.go#L244\nAlso the structs have the same parameters (just one) with the same signature which means the reflection should work as well. And proto.MessageName seems to behave in the same way...\n\nOut of interest; since the update_mask specifies fields as strings, what reflect magic do you use to figure out which fields of the struct to update?\n\nWe're pre-generating everything, updateable fields which we then map to database fields, no reflection magic at all. With something like this:\nmessage User {\nstring display_name = 3  [metadata.updateable = true];\n}\nWould be interesting to see your progress.\n. > @glerchundi I'm afraid you're making an incorrect assumption about how proto.MessageName works. golang/proto.MessageName only returns messages that are registered with the golang/protobuf type registry. gogo/protobuf is forced to maintain its own type registry, which leads to this problem as well as many other issues. The links you're providing ignores the fact that one registeres with gogo/protobuf.RegisterType and one with golang/protobuf.RegisterType.\nRight.\n\n2.1 Alternatively, allow users of the gRPC-Gateway runtime to select a proto.MessageName resolver.\n\nWhat about gogo implementing XXX_MessageName method in its types? Would solve without messing the runtime with resolvers here and there.\nhttps://github.com/golang/protobuf/blob/master/proto/properties.go#L846-L855\n. > Hm, that is a fair point, I didn't know about that method. It would solve the problem with proto.MessageName specifically, but if that name was used to look up any types in proto.MessageType, it would still error.\nBetter than nothing. I think the problem of how to interoperate is much bigger than these concrete methods. I still think that in this case using interfaces is the way to go and would fix your current issue, what do you think if start by fixing this in gogo types and file/comment in another issue(s) for the interoperability between libraries (i'm quite sure there are already issues in this regard).. > Sure that seems reasonable, I guess it would have to be a runtime parameter?\nIf you don't mind I prefer to avoid the generated code.\n\nOn another note, it seems the official advice for disallowing updates is to mark the field \"Output only\" in a comment. This is not enforced in code, of course.\n\nYeah, but we found some (very) rare cases where fields that aren't going to be outputted but can be updated. For example an Account where the password is never printed to the outside but is considered as updateable.\nThanks anyway!. PTAL @johanbrandhorst, it seems like a bazel test is not passing but i think it's not something related to this change.. Thanks Andrew for taking a look. Let me know if something is needed from my\nside.\nOnce merged will appreciate if you can cut a release as this is blocking us\nfrom upgrading (as we thought a minor upgrade would not contain a backward\nincompatible change).\nOn Tue, 8 Jan 2019 at 00:26, Andrew Z Allen notifications@github.com\nwrote:\n\n@glerchundi https://github.com/glerchundi, Yep that is on me. Lemme\nsend a PR to fix Bazel. Sorry about that.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc-ecosystem/grpc-gateway/pull/840#issuecomment-452119021,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACIPlhn1i1aHinF6PtqfN40cSvinRm3Gks5vA9eqgaJpZM4Z0Ygf\n.\n. @achew22 rebased, lets see :)\n\nThanks!. Green! thanks @achew22.. I don't think so.\nI migrated an already existing example into a customisable function without doing any change to it but as you can observe in the two cases that I've implemented i used this descriptor as a 1-1 representation of the go_package proto option.\nI tried to touch as less as possible :). Good catch.. Sure.. Where do you prefer me to add the test? The test infra seems a little bit weak so does it make if i double-check this inside TestApplyTemplateRequestWithoutClientStreaming?. Done.. Done.. ",
    "chris-rock": "@achew22 sure thing, will do. ",
    "syhpoon": "Any hope for this to be moved forward? We're also having this issue and it seems to affect any request with empty body (providing the same setup) not only POST.. @achew22 And how were those examplepb/.pb.go and examplepb/.pg.gw.go files genereated?\nIs there a script somewhere?\nThat is I want to extend a_bit_of_everything service to include the test endpoint for this case but I'd need to re-generate pb files after that.. Never mind, I found the Makefile. I signed it!. ",
    "fmovlex": "Seems like the json format is the only output available.\nThe online editor offers a quick JSON to YAML conversion though.. ",
    "sboagibm": "It's been a while since I made this issue, and I've since moved on.  My recollection is that our overall swagger file is YAML, and I wanted to embed the swagger generation from the grpc-gateway tool into the larger rest api, and the JSON was problematic for that purpose.  The pattern I'm working with is a rest api proxying to multiple gRPC services. I was hoping to do some sort of auto-embedding, but have since given up on the notion, at least for now.. ",
    "maksadbek": "Either I am doing something wrong or it is not generating proper swagger specs.\nMy proto file:\n```\nsyntax = \"proto3\";\npackage protos;\nimport \"protoc-gen-swagger/options/annotations.proto\";\nimport \"google/api/annotations.proto\";\nservice Users {\n        rpc Create(CreateUserRequest) returns (CreateUserResponse) {\n                option (google.api.http) = {\n                        post: \"/users\"\n                        body: \"*\"\n                };\n        }\n}\nmessage CreateUserRequest {\n        User user = 1;\n}\nmessage CreateUserResponse {\n}\nmessage User {\n        option (grpc.gateway.protoc_gen_swagger.options.openapiv2_schema) = {\n                json_schema: {\n                        title: \"simple title\";\n                        required: [ \"name\" ];\n                };\n        };\n    string name = 1;\n\n}\n```\nMy Makefile:\ndefault: \n    protoc -I=. -I=$(GOPATH)/src/github.com/google/protobuf/src/ \\\n                -I=$(GOPATH)/src/github.com/grpc-ecosystem/grpc-gateway \\\n        -I=$(GOPATH)/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\\n                --grpc-gateway_out=logtostderr=true:. \\\n                --go_out=plugins=grpc:. \\\n        --swagger_out=logtostderr=true:. \\\n            ./*.proto\nGenerated swagger spec:\n{\n  \"swagger\": \"2.0\",\n  \"info\": {\n    \"title\": \"api.proto\",\n    \"version\": \"version not set\"\n  },\n  \"schemes\": [\n    \"http\",\n    \"https\"\n  ],\n  \"consumes\": [\n    \"application/json\"\n  ],\n  \"produces\": [\n    \"application/json\"\n  ],\n  \"paths\": {\n    \"/users\": {\n      \"post\": {\n        \"operationId\": \"Create\",\n        \"responses\": {\n          \"200\": {\n            \"description\": \"\",\n            \"schema\": {\n              \"$ref\": \"#/definitions/protosCreateUserResponse\"\n            }\n          }\n        },\n        \"parameters\": [\n          {\n            \"name\": \"body\",\n            \"in\": \"body\",\n            \"required\": true,\n            \"schema\": {\n              \"$ref\": \"#/definitions/protosCreateUserRequest\"\n            }\n          }\n        ],\n        \"tags\": [\n          \"Users\"\n        ]\n      }\n    }\n  },\n  \"definitions\": {\n    \"protosCreateUserRequest\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"user\": {\n          \"$ref\": \"#/definitions/protosUser\"\n        }\n      }\n    },\n    \"protosCreateUserResponse\": {\n      \"type\": \"object\"\n    },\n    \"protosUser\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\"\n        }\n      }\n    }\n  }\n}\nSwagger does not have required fields or title.. @ivucica Okay, got it. Thank you for your quick response :). ",
    "menya84": "Can protoc-gen-swagger parse proto3 syntax?. ",
    "phea": "I am experiencing the same problem with my project along with this basic example by @paulbdavis \n```\n2018/03/12 11:27:39 Starting service\n2018/03/12 11:27:45 in wrapper, context value is test\n2018/03/12 11:27:45 in service, context value is \n[pd@XPS pb]$ protoc --version\nlibprotoc 3.5.1\n[pd@XPS pb]$ go version\ngo version go1.10 linux/amd64 \n```. ",
    "vilterp": "From looking at the generated code, it seems as though the context is lost because the generated HTTP handler doesn't directly call the service method \u2014\u00a0it calls it through the gRPC client, same as it would if it were making a network RPC.\nNot sure how to pass the context across this boundary, other than to change the library to take the server objects directly when initializing a gateway (instead of a net.ClientConn) and then call the service methods directly, instead of through gRPC. That seems like a pretty drastic change though.. Looks like you're supposed to solve this problem using gwruntime.WithMetadata(\u2026), which passes some string-string key-value pairs through the gRPC boundary.. ",
    "srenatus": "The same applies to this attempt at a workaround:\noption (google.api.http).get = \"/foo/bar\";\n        option (google.api.http).additional_bindings = {\n          get: \"/foo/bar/\"\n    };\nthe plugin fails with --grpc-gateway_out: segment neither wildcards, literal or variable: expected \"{\" but got \"\\x00\": /foo/bar/. > cla/google \u2014 Waiting for status to be reported \nIs there anything I can do to make this become green? \n- [X] My employer's CLA shows up in gerrit\n- [X] I've used the same email address\nUpdate: \u2705 It just turned green. \ud83d\ude03 . No love for this? \ud83d\udc94 \n\ud83d\ude09 . @achew22 Thanks for the input. Is it still desirable to have 1.7 supported and as part of the testing matrix? \ud83d\ude03 . Ok, I'm not sure I understand what is happening on travis. Could someone please have a look and share a hint? \ud83d\ude09 . @achew22 rebased \u2714\ufe0f Thanks -- #520 was a good catch! \ud83d\ude04. It goes to the server's stderr here.. @achew22 thanks for sharing your rationale :). No worries, I'll close this PR.\nOne the issue, #472, I just tried to show that a request to /foo/bar/ is not hitting the grpc-gateway handler generated with a /foo/bar annotation. I had understood https://github.com/grpc-ecosystem/grpc-gateway/issues/472#issuecomment-335034740 to claim that it should.. \ud83c\udfd3 this would really be a nice feature. LMK if there's anything I could help with... \ud83e\udd14 . I was thinking that the code generator should catch these -- but really, we could generate a bunch of .pb.gw.go files for different services, and end up using them together. Assuming that, we'd have a hard time catching this overlap at \"compile time\", but rather would check at \"runtime\". The latter could achieved by having runtime.NewPattern check any new one against the existing ones.\nHowever, I could also see benefit in catching overlaps at compile time for those situations where we could. \ud83e\udd14 \nWDYT? Runtime or compile time?. Guess we could make (*mux).Handle() return an error when a pattern clash is detected.... Ok, that will only catch half the trouble, though. You could still have \nget: \"/api/v1/users/{user_id}\"\nin a.proto, and\nget: \"/api/v1/users/things\"\nin b.proto; generate both a.pb.gw.go and b.pb.gw.go separately, and have a surprise in waiting when using the service with these two loaded. Or am I missing something? . Looking at the pattern code, it would be nice to come up with a function like\ngo\nfunc (p Pattern) Overlaps(q Pattern) bool\nthat could then be used; but frankly, it's quite complicated, and I haven't reached a satisfying solution yet. (I make no claims on this issue whatsoever. Up for grabs.). ok, mistake 1: I should have rebuilt the other stuff, too, of course \ud83d\ude05 . > Ah, yes, the CLA bot will not like the different approvers. Could you, uh, force push a new history with just your commits?\nSure, but that'll only work if it's branched off of #799 or that other PR is merged first (and this rebased) -- which one do you prefer? \ud83d\ude09 . OK, will do.. > Note to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\nThis makes me think that this won't fix itself, though... \ud83e\udd14 Can you change the label, or should I just open another PR?. OMG so I'll leave off the Co-authored-by... \ud83e\udd15 . Yup, will do. \ud83d\udd0d I couldn't find any obvious extension point -- it looks like only message fields have test scaffolding for the schemaCore bits, not messages. I'm happy to add a similar thing to message; is my basic understanding alright here?. :tada: That's perfect. I'll restore the commit history tomorrow \ud83d\ude03 \nBtw -- I've tried, but failed to, trick the bot in #848 and #847. So, a manual override (by @achew22) will let us do the right thing here \ud83d\udc4d . Alternatively, we could merge #799 as-is and I'll put up a test-only PR following that.. Hmm. I know it should never happen, but seeing the second return ignored is weird, too. Any thoughts about something like\ngo\ns, ok := status.FromError(err)\nif !ok {\n  panic(\"err is no status\")\n}. Oh that's a lovely function. Thanks for the pointer.. ",
    "stevenroose": "@AlekSi I realize that. Just curious why REST was chosen? REST (being resource-centric) and RPC are fundamentally different. JSON-RPC defines a JSON & HTTP-based RPC protocol, why wasn't it picked over REST?. ",
    "aelsabbahy": "The JSON stream output is no longer newline separated, is this intentional?. I happened to go get this project to try it out after this change was merged.\nThe readme needs to be updated if this is the way it'll work moving forward:\nhttps://github.com/grpc-ecosystem/grpc-gateway#supported\n\nMapping streaming APIs to newline-delimited JSON\n\nAlso, what's the best way to parse the response now that newline is no longer a delimiter? I'm also noticing https://github.com/tmc/grpc-websocket-proxy might not be working well with this change messages only send at the end, rather than one at a time. But I'm only currently testing with https://github.com/danielstjules/wsc so maybe not a full end-end test.. (off topic)\n@afking any chance you can share your custom marshaler? I would be interested in that for a project I'm working on. Want to get rid of the result key in the map.\n(On topic)\nCan the marshal provider own/provide the delimiter value? This way application/json and \\n aren't fixed values in the handler code.. ",
    "hollinwilkins": "Nevermind, apparently this way of passing credentials is deprecated.. ",
    "lingwangsj": "After reverting the line in the runtime/handler.go, I got response with a new line between two results like this. It is still not a valid JSON string. \n{\"result\":{\"uuid\":\"0f41e1059fafe87988473bd1531f5749feff4b019fa4e248\"}}\n{\"result\":{\"uuid\":\"0c41e1059fafe87988473bd1531f5749feff4b019fa4e248\"}}. I ran into this issue with my own grpc server and grpc gateway server. Then I want to find out whether the issue was caused by my own code. So I checked the example and I found the stream.proto has the same API pattern as my service. So this is what I did.\nI did the three go get commands mentioned in the readme\ngo get -u github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway\ngo get -u github.com/grpc-ecosystem/grpc-gateway/protoc-gen-swagger\ngo get -u github.com/golang/protobuf/protoc-gen-go\nI run the example grpc server using command:\n/usr/local/go/bin/go run /home/ling/Projects/go/src/github.com/grpc-ecosystem/grpc-gateway/examples/server/cmd/example-server/main.go\nI run the proxy using command:\n/usr/local/go/bin/go run /home/ling/Projects/go/src/github.com/grpc-ecosystem/grpc-gateway/examples/main.go. I actually run make, then restarted the grpc server and the proxy. now I got empty response. (with the runtime/handler.go changes I made before.. Could you tell me how to run the e2e test?\n. I followed the steps, after resolved few dependency issues, the test run without any failure. for what is worth, I am using \"go version go1.9 linux/amd64\" and centOs7.. When there is only one record, the HTTP response payload is a good json format like this:\n{\"result\": {\"uuid\":\"7d96031c1eaeffa392245718677778755db9c0f267b66a75\"}}\nthe moment I added more record, and there are multiple items  returns via the stream gRpc API, the HTTP response payload is invalid. does not matter the record count is 2 or 3 or more, like this:\n{\"result\":{\"uuid\":\"7d96031c1eaeffa392245718677778755db9c0f267b66a75\"}}{\"result\":{\"uuid\":\"7e96031c1eaeffa392245718677778755db9c0f267b66a75\"}}\n. ",
    "owenhaynes": "Using 1.3 of grpc.\nUsing gogo protoc latest version\n. sorry 1.3 of grpc-gateway should of had my morning coffee and v1.7.3 of grpc.\nWill try with regular compiler. Only using gogo proto for the better time support.. Ok same problem with normal protobuf compiler latest version.\nlooks to be to do with the double pointer being generated in the gateway code, and the decoder fails to \n```\n    if err := marshaler.NewDecoder(req.Body).Decode(&protoReq.Foo); err != nil {\n        return nil, metadata, status.Errorf(codes.InvalidArgument, \"%v\", err)\n    }\n```. ",
    "garyluu": "Mostly duplicate of #322 . ",
    "incubus8": "Nvm, I haven't started GRPC server. ",
    "gusseleet": "Could you give some more information? How did you start it?. ",
    "Emixam23": "I have this issue on Mac OS X, any idea?. Ok so I did check my setup and, I found this link on internet that, even if I have others problems now, helped me to get rid of this error : https://github.com/mycodesmells/golang-examples/blob/master/grpc/cmd/server/main.go\nThe thing I just don't get with this example is that, the guy initialize the grpc server, half in main, half in a go routine, then he serv the http part of its server and, the only difference I see is that he initialize a part of it before the go routine instead of initializing everything inside of this go routine (I hope I am clear, english isn't my first language sorry). I am talking about the https://github.com/grpc-ecosystem/grpc-gateway/blob/master/README.md, sorry if I confused you. ",
    "kwoodhouse93": "I think there's a valid use case for this. In my case, I have a ListResource method as a GET request. This method takes a parameter called filter, which is a structured message that defines the various fields that can be filtered by.\nA simplified example:\n```proto\nservice ResourceService {\n    rpc ListResource (ListResourceRequest) returns (ListResourceResponse) {\n        option (google.api.http) = {\n            get: \"/api/resource\"\n        };\n    }\n}\nmessage ListResourceRequest {\n    ResourceFilter filter = 1;\n}\nmessage ResourceFilter {\n    repeated string ID = 1;\n    repeated RangeFilter range = 2;\n}\nmessage RangeFilter {\n    int64 minimum = 1;\n    int64 maximum = 2;\n}\nmessage myMethodResponse {\n    repeated Resource resources = 1;\n}\n```\nTrying a request:\n/api/resource?filter.resources.ranges.minimum=4\njson\n{\"error\":\"unexpected repeated field in filter.resources.ranges.minimum\",\"message\":\"unexpected repeated field in filter.resources.ranges.minimum\",\"code\":3,\"details\":[]}\nIn this case, the method really shouldn't be POST. It's ok as a temporary workaround, but I wouldn't consider this issue fixed just because I can use POST instead. Doing so makes the API less consistent and weakens the meaning of the request verbs.\nI also understand the syntax isn't obvious, but there are a few alternatives that could work, and the benefit of supporting a use case like the one above is quite significant.\nFor me, being able to specify the structure of the filters in the protos format helps make sense of an otherwise complex part of the API. The commonly seen alternative of using a string field as generic filter (e.g. /api/resource?filter=blahblahblah...) ignores that complexity, and leads to a fragile, implicit contract between the caller and the server, throwing away some of the benefits that drove me to choose gRPC in the first place.\nSo, for the use case I've laid out above (and I could conceive of others), I'd be grateful if you could reconsider supporting repeated message fields in query strings.. ",
    "chai2010": "greate \ud83d\udc4d . ",
    "shouichi": "Sure, thanks for your advice!. Added test for schemaOfField. I'm not sure if I set up descriptor registry correctly. Feel free to comment if I'm doing something wrong.. Hello,\nThought changelog states #496 is included in v1.3.1, it's not actually. Could you cut a new release?\nThank you. ",
    "activeshadow": "Thanks @achew22 was just looking at that. Didn't see it until after I posted this question, and was going to post an update here if it worked for me.. Is it possible to open this issue up again? I'm coming back around to using grpc-gateway, and I'm not seeing how best to implement metrics. I looked more into go-grpc-prometheus, but from what I can tell it's really only meant for usage server-side. Since the grpc-gateway is client-side from a gRPC perspective I'm not seeing how to make it work. I tried adding grpc_prometheus.UnaryInterceptor as a client-side interceptor via the DialOptions passed to the handlers in the grpc-gateway, but got errors.\nI'd also like to see metrics on the normal HTTP server side of things a well.\nThoughts? Are there any examples of this use case that I'm just not seeing?. ",
    "hacst": "Does your .proto file have http annotations? If those are missing there will be no output and no file generated. You can add ,v=2 before :. to get more detailed log messages.. I signed it!. Great to see there is interest.\n@achew22 Now that you mention it, being able to have this a runtime configuration would be kinda neat. Google uses them this way but for grpc-gateway that would be a pretty large conceptual change I imagine. This PR really only adds another way to get http configuration into the grpc-gateway protoc plugins during generation time.\nTaking the README.md echo example instead of doing step 2 you would create a your_api_config.yaml containing something like:\n```yaml\ntype: google.api.Service\nconfig_version: 3\nhttp:\n  rules:\n  - selector: example.YourService.Echo\n    post: /v1/example/echo\n    body: \"*\"\nThat can then be passed to the gateway and swagger generator:\nprotoc -I/usr/local/include -I. \\\n  -I$GOPATH/src \\\n  -I$GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\\n  --grpc-gateway_out=logtostderr=true,grpc_api_configuration=path/to/your_api_config.yaml:. \\\n  path/to/your_service.proto\nprotoc -I/usr/local/include -I. \\\n  -I$GOPATH/src \\\n  -I$GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\\n  --swagger_out=logtostderr=true,grpc_api_configuration=path/to/your_api_config.yaml:. \\\n  path/to/your_service.proto\n```\nHope that clarifies things.. @achew22 Yes. That and more. My understand is that as soon as my service proto includes annotations every user (whether they care about the http annotations or - imho more likely - not) not only has to make sure annotations.proto (and transitively http.proto) are available to protoc during generation but also has to have these files protoc'd for their implementation language for the client/service compilation. As these types are part of the google.api which is not shipped with protobuf this can create quite a lot of friction if the service proto is widely used.\nAs a concrete example in C++ I need google/api/annotations.pb.h and google/api/http.pb.h headers to be able to build as soon as the service proto includes annotations. I assumed it was the same for other languages. Is this not the case? Is there a way around this?. @achew22 Passing the file is pretty natural. You do it the same way you pass other parameters directly to the plugin right now.\nE.g to make the gateway or swagger plugin itself log to stderr you currently do:\nprotoc --grpc-gateway_out=logtostderr=true:. my.proto\nprotoc --swagger_out=logtostderr=true:. my.proto\naccordingly passing a yaml file is:\nprotoc --grpc-gateway_out=grpc_api_configuration=my.yaml:. my.proto\nprotoc --swagger_out=grpc_api_configuration=my.yaml:. my.proto\nEverything between *_out= and the : after which you specify the output location is handed to the plugin as part of the request. This is a protoc feature and the gateway and swagger plugin explicitely implement support for it (see https://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-grpc-gateway/main.go#L60 ff.). You can pass an arbitrary number of comma separate param=value which are interpreted like commandline parameters would be. No special magic needed.\nWorks a treat.. With regards to this PR in general: It would be great to receive some guidance on what would be needed to make this ready to land. E.g. is the current implementation approach acceptable and what kind of tests and documentation is required for people to be comfortable landing this. Thanks.. @ensonic  Unfortunately I have been pretty busy lately. Will try to finish this up this weekend.. @achew22 Ok. I think I addressed the remaining review feedback and rebased onto current master.\nFor the e2e test I derived a \"unannotated\" echo service from the echo service example. I added in a import of the duration well known type for good measure. This should break without my len(m.Bindings) == 0 change. Imho this should exercise the new codepaths sufficiently. The functionality expressable by the HTTP rule annotations is directly re-used by the PR so I see no reason to test it again in this in this E2E.\nI'm not 100% happy with my README.md changes. I tried to keep it as short as possible but it still is quite a chunk of text. Maybe putting the information in a separate .md file would be better?\nIn the makefile I did a bit of cargo-culting from the echo service (I've never seen a ExamplepbSimpleMessage.go or git_push.sh in my tree) so there might be some clutter that ought to be removed.\nAlso my generation run insisted on changing some occurances of \"context\" imports to \"golang.org/x/net/context\" which I understand is a legacy construct. I was using go 1.10 and a 3.0.0 protoc so I am not quite sure whether that is something that needs fixing. It does compile.\nMy bazel file edits were blind as I do not have a bazel environment to use. The bazel build on travis fails due to what looks like my new dependency missing which makes a certain amount of sense. I have no idea how to get a 3rd party go dependency into a bazel build so help is welcome there.\nAlso the travis builds with disabled context seem to fail. This looks like a makefile issue to me but I'm not quite sure. protoc line looks like I expect it to be but the error output is strange. Like a space slipped in somewhere where it shouldn't. If you have an idea I'm all ears. Otherwise I will have to revisit this at a later date.\n. @achew22 I think I figured out the Bazel stuff. Also fixed the Makefile so all travis builds are green now. See my previous comment for the overall state of the PR but from my POV this is getting close to being finished. Would be great if you could take a look.. @achew22 I addressed the two remaining issues.\nFor the ignore files I simply copy and pasted from the other samples.\nThe documentation for gRPC API Configuration file usage is now a separate part of the jekyll documentation. Unfortunately the theme does not support nested categories so I had to add a new top-level item. I did another pass over the rest of the documentation to make sure the new example and the feature were mentioned where useful.\nOne thing I noticed was that besides the landing page none of the jekyll documentation pages where themed. See for example https://grpc-ecosystem.github.io/grpc-gateway/docs/features.html which is just black text on white background. I added another commit that fixes this by ensuring the \"default\" theme is used on all pages. This was the first time I had contact with jekyll though so I'm not sure if this is the right way (TM) to do things.\nFrom my POV this PR should be ready for final review and subsequent landing. If you want commits squashed or a final rebase let me know.\nWith regards to Bazel: The vast majority of people finding the project will not have used it as it is not widely used outside of google OSS projects at this point. I had investigated it before for use with C++ so I had a basic idea of the general approach it is taking but I feel like it will be very unnatural and opaque for someone used to the normal go or node way of managing build and dependencies.\nFor me personally getting to the point where I could run the build in Bazel was quite straight-forward as you can pretty much read that directly from the .travis.yml . However what you cannot see there is how to correctly update the build files if you added files or dependencies. For example it is really not obvious where external go dependencies come from and how they fit into the bazel world unless you know where to look. Also at first I manually edited the BAZEL files to add sources and internal dependencies before figuring out that in this project \"gazelle\" (never heard of it before) manages that. However running the \"gazelle fix\" target in this project is a bit overeager and actually breaks things so you have to revert some of what it does (good thing the PR introducing it mentioned that).\nTL;DR: A short list of steps mentioning the correct steps for updating the build description files would have been most helpful to me. A link to a basic bazel with go documentation (if there is something like that matching what is implemented here) would not hurt either.. @achew22 This was more involved than I expected. Quite a few relevant changes since the last rebase a few days ago. I hope it is ok that I merged instead of rebased but I really didn't feel rebasing all those commits was a sane thing to do now that the examplepb folder got moved.\nAdded new commits for:\n Adding a delete method to the unattended echo example as one was added to the stock echo one\n Fix the documentation to correctly point to the new location of the examplepb folder. Seems like that was forgotten before\nHoping for no more breaking changes till we manage to land this ;). Awesome. Thanks a lot for the guidance along the way. Having someone so responsive handle PRs is really awesome for a first time contributor to a project.. @yugui No worries. Pretty much my own fault for letting this sit so long ;). @ensonic No. As described in https://grpc-ecosystem.github.io/grpc-gateway/docs/grpcapiconfiguration.html this PR only implements the HTTP rule part of the specification. The goal was not having to annotate the .proto file for basic use with grpc-gateway. I have not looked into how other parts of gRPC API Configurations might be mapped to grpc-gateway, though I am sure there are some that could have meaningful mappings.. I actually meant the backwards compatibility guarantees given by protobuf there. As in you can just remove fields as long as numbering stays intact. You are of course correct that I also have to rely on the \"interface\" of the protobuf generator for golang. Will try to clarify the comment.. Will do.. ok.. Nothing of this is running during grpc-gateway runtime. Generation time only. I don't think any code outside of the template strings and runtime can even end up in the code of the actual gateway or am I missing something?. This is just for loading the httprule during generation time. It is independent from the gateway runtime marshalling/unmarshalling of JSON protobuf messages received/sent through the gateway. It didn't seem prudent to bind one to the other as the requirements might be different or diverge.. ok. Will do.. ok. ok. This is from 3356bb1 :\n\nPreviously the generator generator checked whether the http extension\nwas used to decide whether imports were required for a method. This\npatch makes it check whether a method has bindings instead as those\nare what code is actually generated for.\n\nBasically the previous check skipped adding imports when the protobuf file wasn't using the Http annotation. That check is done on data protoc passed from the proto file. Since the whole point of this PR is to no longer have the Http annotations in the proto this check had to be amended. Now it checks whether we created bindings for the method (it should have a binding for each endpoint). This will happen based on annotation or the yaml.\nWhat you see without this change is that the generated .gw.go file might be missing imports (e.g. if the method uses google.protobuf.Duration for which an import is required). In that case it just won't compile.\nI will see if I can do a test for that.. ok. Sgtm. Much better than littering the landing page.. ",
    "EQ94": "I got the same problem too, have you solve this? thanks. ",
    "hhstore": "same problem too:\nproto file: example.proto\n```\nsyntax = \"proto3\";\npackage com.ceres.trade.api.server.srv.app;\nservice Example {\n    rpc Call (Request) returns (Response) {\n    }\n    rpc Stream (StreamingRequest) returns (stream StreamingResponse) {\n    }\n    rpc PingPong (stream Ping) returns (stream Pong) {\n    }\n    rpc Hello (HelloReq) returns (HelloResp) {\n    }\n}\nmessage HelloReq {\n    string name = 1;\n}\nmessage HelloResp {\n    string name = 1;\n}\nmessage Message {\n    string say = 1;\n}\nmessage Request {\n    string name = 1;\n}\nmessage Response {\n    string msg = 1;\n}\nmessage StreamingRequest {\n    int64 count = 1;\n}\nmessage StreamingResponse {\n    int64 count = 1;\n}\nmessage Ping {\n    int64 stroke = 1;\n}\nmessage Pong {\n    int64 stroke = 1;\n}\n```\nrun:\nbash\nproto-gateway:\n    protoc -I/usr/local/include -I. \\\n        -I${GOPATH}/src \\\n        -I${GOPATH}/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\\n        --grpc-gateway_out=logtostderr=true,v=2:. \\\n        proto/example/*.proto\nerror message:\n```\nI1023 17:14:51.906267   90757 registry.go:154] register name: .com.ceres.trade.api.server.srv.app.HelloReq\nI1023 17:14:51.906392   90757 registry.go:154] register name: .com.ceres.trade.api.server.srv.app.HelloResp\nI1023 17:14:51.906398   90757 registry.go:154] register name: .com.ceres.trade.api.server.srv.app.Message\nI1023 17:14:51.906402   90757 registry.go:154] register name: .com.ceres.trade.api.server.srv.app.Request\nI1023 17:14:51.906406   90757 registry.go:154] register name: .com.ceres.trade.api.server.srv.app.Response\nI1023 17:14:51.906410   90757 registry.go:154] register name: .com.ceres.trade.api.server.srv.app.StreamingRequest\nI1023 17:14:51.906414   90757 registry.go:154] register name: .com.ceres.trade.api.server.srv.app.StreamingResponse\nI1023 17:14:51.906418   90757 registry.go:154] register name: .com.ceres.trade.api.server.srv.app.Ping\nI1023 17:14:51.906426   90757 registry.go:154] register name: .com.ceres.trade.api.server.srv.app.Pong\nI1023 17:14:51.906435   90757 services.go:18] Loading services from proto/example/example.proto\nI1023 17:14:51.906454   90757 services.go:21] Registering Example\nI1023 17:14:51.906459   90757 services.go:27] Processing Example.Call\nI1023 17:14:51.906480   90757 services.go:38] Found non-target method: Example.Call\nI1023 17:14:51.906486   90757 registry.go:181] lookup .com.ceres.trade.api.server.srv.app.Request from com.ceres.trade.api.server.srv.app\nI1023 17:14:51.906490   90757 registry.go:181] lookup .com.ceres.trade.api.server.srv.app.Response from com.ceres.trade.api.server.srv.app\nI1023 17:14:51.906494   90757 services.go:27] Processing Example.Stream\nI1023 17:14:51.906499   90757 services.go:38] Found non-target method: Example.Stream\nI1023 17:14:51.906502   90757 registry.go:181] lookup .com.ceres.trade.api.server.srv.app.StreamingRequest from com.ceres.trade.api.server.srv.app\nI1023 17:14:51.906505   90757 registry.go:181] lookup .com.ceres.trade.api.server.srv.app.StreamingResponse from com.ceres.trade.api.server.srv.app\nI1023 17:14:51.906508   90757 services.go:27] Processing Example.PingPong\nI1023 17:14:51.906674   90757 services.go:38] Found non-target method: Example.PingPong\nI1023 17:14:51.906679   90757 registry.go:181] lookup .com.ceres.trade.api.server.srv.app.Ping from com.ceres.trade.api.server.srv.app\nI1023 17:14:51.906682   90757 registry.go:181] lookup .com.ceres.trade.api.server.srv.app.Pong from com.ceres.trade.api.server.srv.app\nI1023 17:14:51.906686   90757 services.go:27] Processing Example.Hello\nI1023 17:14:51.906690   90757 services.go:38] Found non-target method: Example.Hello\nI1023 17:14:51.906693   90757 registry.go:181] lookup .com.ceres.trade.api.server.srv.app.HelloReq from com.ceres.trade.api.server.srv.app\nI1023 17:14:51.906696   90757 registry.go:181] lookup .com.ceres.trade.api.server.srv.app.HelloResp from com.ceres.trade.api.server.srv.app\nI1023 17:14:51.906702   90757 services.go:49] Registered Example with 4 method(s)\nI1023 17:14:51.906713   90757 generator.go:91] Processing proto/example/example.proto\nI1023 17:14:51.907018   90757 template.go:122] Processing Example.Call\nI1023 17:14:51.907025   90757 template.go:122] Processing Example.Stream\nI1023 17:14:51.907028   90757 template.go:122] Processing Example.PingPong\nI1023 17:14:51.907031   90757 template.go:122] Processing Example.Hello\nI1023 17:14:51.907035   90757 generator.go:94] proto/example/example.proto: no target service defined in the file\nI1023 17:14:51.907043   90757 main.go:100] Processed code generator request\n```. @johanbrandhorst \n\nthanks, the generate cmd option, require add some api annotations, like this: \nHello() { ... }\n\n```\nsyntax = \"proto3\";\npackage com.ceres.trade.api.server.srv.app;\nimport \"google/api/annotations.proto\";\nservice Example {\n    rpc Call (Request) returns (Response) {}\n    rpc Stream (StreamingRequest) returns (stream StreamingResponse) {}\n    rpc PingPong (stream Ping) returns (stream Pong) {}\n    rpc Hello (HelloReq) returns (HelloResp) {\n        option (google.api.http) = {\n            post: \"/greeter/hello\"\n            body: \"*\"\n        };\n    }\n}\n```\n\nthen, can generate example.pb.gw.go file normally.\n\n. ",
    "penglei": "I signed it!. ",
    "ashi009": "I'll drop this FR in favor of #507, as now it has the same interface as the protoc-gen-go to execute. Running the compiler multiple times is fine with me. Though programmatically invoking these compilers is not completely smooth.\nThe boilerplates are the code handling imports, resolving types etc. Which is probably the most painful part for writing a protoc-gen plugin from scratch.\nAlso, go team has suggested that the protoc-gen-go plugin interface is designed for grpc alone, and they are not ready to make it a standard way for writing go proto compilers. I think at this point gogo has the best interface to work on for a simple protoc compiler.. ",
    "warmans": "Hi, added something to the example which shows the problem (if the fix is reverted anyway). Let me know if anything is missing.\nedit: erm, not sure how to make the tests run in CI. It imports the examples from the grpc-ecosystem path but my changes are only available in my own namespace. . Yeah sure, AFAIK the travis tests fail because the new GetWithBody method is undefined on the server in github.com/grpc-ecosystem/grpc-gateway/examples/server. But I cannot implement this method because I cannot use the request type from the imported package: \n\"github.com/grpc-ecosystem/grpc-gateway/examples/examplepb\"\nit's only available in \n\"github.com/warmans/grpc-gateway/examples/examplepb\"\nBut I shouldn't important that. Additionally it looks like travis just go gets the grpc-ecosystem package so I don't know if I could import it even if I wanted to.\nThis wouldn't be an issue if the PR was a branch rather than a fork so maybe a maintainer could just copy the branch and replace this PR. Or perhaps there is another solution. Relative imports or something. I'm not sure :/. Hah oh god. Why did I not think of that. Should be fixed now.. ",
    "ianthehat": "This looks correct to me.. ",
    "lukasmalkmus": "512 . I signed it!. Thanks for answering. The answer was so obvious and I still missed it \ud83e\udd23 Probably because I never encountered the problems you mentioned. But I totally understand you and the solution makes sense to me. However, I think one has design flaws in his protobuf definitions if he/she has multiple messages with the same base message name (even through imports). But I could be wrong, just because it hasn't happened to me, doesn't mean such cases are not present.\nWe could possibly integrate the ability to handle both cases (with or without prefix) but one the other hand (and since no one besides me has complained yet) it might be the simplest solution to just patch the files by hand with a short script (sed, etc.). But I'm also open for suggestions.. > Do you know what does this do on multi paragraph comments?\nYes. It makes a line break and thus a new paragraph. Take a look at the source code. The upstream code separates the paragraphs using strings.Split(\"comment\", \"\\n\\n\"). I just hook in after that and remove any \\n from the single paragraphs, not the whole comment. So paragraphs are preserved and later joined anyways, but with my small cleanup step in-between. Furthermore I noticed, that the official Swagger Editor does not render a double line break \\n\\n, which is used here, to separate the paragraphs. It uses a single line break anyways.\n\nAlso, as an aside, we depend on \"golden file\" tests in order to ensure there haven't been any regressions in the creation of the files. Since this change modifies the output format for the .swagger.json files, you will need to run make examples and commit them before the CI tests will pass.\n\nUps... my first commit build just fine. But sure, will do :)\n\nOh, an additional concern I have is in the case where someone wants to make a list in the comment [...] Can you help me understand how your change would impact a usecase like that as well?\n\nGood point \ud83d\udc4d will investigate and report back.. As you already guessed: My simple solution didn't work with bullet lists. So I had to improve my code:\nStuff like this...\nprotobuf\nservice Users {\n  // Search for users\n  //\n  // Returns a list of users and\n  //  allows you to search by:\n  // * Family Name\n  //    is your last name\n  // * First Name\n  // * Height\n  // * User ID\n  //\n  // More information that would be useful to a person calling this API.\n  rpc SearchUsers(SearchUsersRequest) returns (SearchUsersResponse) {\n    option (google.api.http) = { post: \"/v1/users:search\" body: \"*\" };\n  }\n}\n... now renders nicely:\n\nEven the generated text is free of duplicated whitespaces, etc. so we don't rely on the swagger editor here for clean rendering.. To be honest, I had the same thoughts while writing the code.\n\nI agree that it is mildly unsightly to have widow words and strange line breaks but it does come at a cost -- the ability to fully express your formatting.\n\n\ud83d\udc4d \n\nDo you know how other tools handle the swagger generation WRT line breaks? Maybe we can learn from them?\n\nI will dig around a little bit. And the issue I explained occurred in the swagger editor, I have not yet tested it in ReDoc.. No worries. Looks good.. Looks like this was fixed in latest master with https://github.com/grpc-ecosystem/grpc-gateway/commit/11bef10d24deac2b8a97fbc8e3342879dea763bf. Can you confirm @afking ?. I would shorten the question to: Why are the models in the swagger specification prefixed with the last part of the proto package name?. I suggest shortening: Why not strip the prefix?. Just a bit more general:\nWhen a message is added which happens to conflict with another message (e.g. by importing a message with the same name from a different package) it will break code that is very far away from the code that changed. This is in an effort to adhere to the [principle of least astonishment](https://en.wikipedia.org/wiki/Principle_of_least_astonishment).. ",
    "codecov-io": "Codecov Report\n\n:exclamation: No coverage uploaded for pull request base (master@c1c70b3). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #514   +/-\n=========================================\n  Coverage          ?   60.17%         \n=========================================\n  Files             ?       28         \n  Lines             ?     2669         \n  Branches          ?        0         \n=========================================\n  Hits              ?     1606         \n  Misses            ?      905         \n  Partials          ?      158\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c1c70b3...57e1c5a. Read the comment docs.\n. # Codecov Report\nMerging #521 into master will decrease coverage by 0.15%.\nThe diff coverage is 57.14%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #521      +/-\n==========================================\n- Coverage   59.28%   59.12%   -0.16%   \n==========================================\n  Files          28       30       +2   \n  Lines        2780     2838      +58   \n==========================================\n+ Hits         1648     1678      +30   \n- Misses        971      998      +27   \n- Partials      161      162       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| ...oc-gen-grpc-gateway/descriptor/grpc_api_service.go | 0% <0%> (\u00f8) | |\n| protoc-gen-swagger/main.go | 25.97% <0%> (-1.43%) | :arrow_down: |\n| protoc-gen-grpc-gateway/gengateway/generator.go | 41.79% <100%> (\u00f8) | :arrow_up: |\n| protoc-gen-grpc-gateway/descriptor/types.go | 54.83% <100%> (+6.05%) | :arrow_up: |\n| ...-grpc-gateway/descriptor/grpc_api_configuration.go | 29.62% <29.62%> (\u00f8) | |\n| protoc-gen-grpc-gateway/descriptor/services.go | 75.54% <76.19%> (+1.11%) | :arrow_up: |\n| protoc-gen-grpc-gateway/descriptor/registry.go | 74.28% <80%> (-0.43%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b502d2d...acc027a. Read the comment docs.\n. # Codecov Report\nMerging #570 into master will decrease coverage by 0.43%.\nThe diff coverage is 36.84%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #570      +/-\n==========================================\n- Coverage   58.88%   58.44%   -0.44%   \n==========================================\n  Files          30       30            \n  Lines        2853     2871      +18   \n==========================================\n- Hits         1680     1678       -2   \n- Misses       1010     1030      +20   \n  Partials      163      163\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/gengateway/template.go | 54.9% <\u00f8> (\u00f8) | :arrow_up: |\n| protoc-gen-grpc-gateway/descriptor/types.go | 40.47% <36.84%> (-8.6%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c2b051d...26d004c. Read the comment docs.\n. # Codecov Report\nMerging #571 into master will increase coverage by <.01%.\nThe diff coverage is 60%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #571      +/-\n==========================================\n+ Coverage   58.88%   58.88%   +<.01%   \n==========================================\n  Files          30       30            \n  Lines        2853     2863      +10   \n==========================================\n+ Hits         1680     1686       +6   \n- Misses       1010     1014       +4   \n  Partials      163      163\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/gengateway/template.go | 55.76% <100%> (+0.86%) | :arrow_up: |\n| protoc-gen-grpc-gateway/gengateway/generator.go | 43.42% <50%> (+1.63%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c2b051d...d54e528. Read the comment docs.\n. # Codecov Report\nMerging #577 into master will increase coverage by 1.51%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #577      +/-\n==========================================\n+ Coverage   56.43%   57.95%   +1.51%   \n==========================================\n  Files          30       30            \n  Lines        3005     2918      -87   \n==========================================\n- Hits         1696     1691       -5   \n+ Misses       1145     1064      -81   \n+ Partials      164      163       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/errors.go | 45.07% <100%> (+7.4%) | :arrow_up: |\n| runtime/marshal_json.go | 66.66% <0%> (-16.67%) | :arrow_down: |\n| protoc-gen-swagger/genswagger/template.go | 42.1% <0%> (+3.58%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9b9677c...1665baf. Read the comment docs.\n. # Codecov Report\nMerging #613 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #613   +/-\n=======================================\n  Coverage   59.28%   59.28%         \n=======================================\n  Files          28       28         \n  Lines        2780     2780         \n=======================================\n  Hits         1648     1648         \n  Misses        971      971         \n  Partials      161      161\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 892952a...c5b759c. Read the comment docs.\n. # Codecov Report\nMerging #616 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #616   +/-\n=======================================\n  Coverage   59.28%   59.28%         \n=======================================\n  Files          28       28         \n  Lines        2780     2780         \n=======================================\n  Hits         1648     1648         \n  Misses        971      971         \n  Partials      161      161\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 42.35% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 739cd2d...437255e. Read the comment docs.\n. # Codecov Report\nMerging #618 into master will increase coverage by 0.47%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #618      +/-\n==========================================\n+ Coverage    58.8%   59.28%   +0.47%   \n==========================================\n  Files          29       28       -1   \n  Lines        2840     2780      -60   \n==========================================\n- Hits         1670     1648      -22   \n+ Misses       1005      971      -34   \n+ Partials      165      161       -4\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9530328...80e88d0. Read the comment docs.\n. # Codecov Report\nMerging #621 into master will decrease coverage by 0.24%.\nThe diff coverage is 31.57%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #621      +/-\n==========================================\n- Coverage   59.12%   58.88%   -0.25%   \n==========================================\n  Files          30       30            \n  Lines        2838     2853      +15   \n==========================================\n+ Hits         1678     1680       +2   \n- Misses        998     1010      +12   \n- Partials      162      163       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/descriptor/types.go | 49.07% <31.57%> (-5.77%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 639b2fa...f649150. Read the comment docs.\n. # Codecov Report\nMerging #622 into master will increase coverage by 0.98%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #622      +/-\n==========================================\n+ Coverage   59.28%   60.26%   +0.98%   \n==========================================\n  Files          28       30       +2   \n  Lines        2780     3030     +250   \n==========================================\n+ Hits         1648     1826     +178   \n- Misses        971     1039      +68   \n- Partials      161      165       +4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| ...oc-gen-grpc-gateway/descriptor/grpc_api_service.go | 0% <0%> (\u00f8) | |\n| ...-grpc-gateway/descriptor/grpc_api_configuration.go | 29.62% <0%> (\u00f8) | |\n| protoc-gen-grpc-gateway/descriptor/registry.go | 76.11% <0%> (+1.4%) | :arrow_up: |\n| protoc-gen-grpc-gateway/gengateway/generator.go | 43.9% <0%> (+2.11%) | :arrow_up: |\n| protoc-gen-grpc-gateway/descriptor/types.go | 52.41% <0%> (+3.63%) | :arrow_up: |\n| protoc-gen-swagger/main.go | 33% <0%> (+5.6%) | :arrow_up: |\n| protoc-gen-grpc-gateway/descriptor/services.go | 82.55% <0%> (+8.12%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b502d2d...df475f5. Read the comment docs.\n. # Codecov Report\nMerging #624 into master will increase coverage by 0.14%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #624      +/-\n==========================================\n+ Coverage   58.88%   59.02%   +0.14%   \n==========================================\n  Files          30       30            \n  Lines        2853     2858       +5   \n==========================================\n+ Hits         1680     1687       +7   \n+ Misses       1010     1008       -2   \n  Partials      163      163\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/descriptor/types.go | 53.09% <0%> (+4.02%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f252496...63d6aa1. Read the comment docs.\n. # Codecov Report\nMerging #631 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #631   +/-\n=======================================\n  Coverage   58.88%   58.88%         \n=======================================\n  Files          30       30         \n  Lines        2853     2853         \n=======================================\n  Hits         1680     1680         \n  Misses       1010     1010         \n  Partials      163      163\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3beac66...27bd371. Read the comment docs.\n. # Codecov Report\nMerging #632 into master will increase coverage by 0.05%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #632      +/-\n==========================================\n+ Coverage   58.88%   58.94%   +0.05%   \n==========================================\n  Files          30       30            \n  Lines        2853     2857       +4   \n==========================================\n+ Hits         1680     1684       +4   \n  Misses       1010     1010            \n  Partials      163      163\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/marshal_json.go | 83.33% <0%> (\u00f8) | :arrow_up: |\n| runtime/marshal_jsonpb.go | 70.58% <0%> (+1.2%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 87a1b0c...74a397e. Read the comment docs.\n. # Codecov Report\nMerging #635 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #635   +/-\n=======================================\n  Coverage   58.94%   58.94%         \n=======================================\n  Files          30       30         \n  Lines        2857     2857         \n=======================================\n  Hits         1684     1684         \n  Misses       1010     1010         \n  Partials      163      163\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update dca8693...7c12a48. Read the comment docs.\n. # Codecov Report\nMerging #636 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #636   +/-\n=======================================\n  Coverage   58.88%   58.88%         \n=======================================\n  Files          30       30         \n  Lines        2853     2853         \n=======================================\n  Hits         1680     1680         \n  Misses       1010     1010         \n  Partials      163      163\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 87a1b0c...378ad71. Read the comment docs.\n. # Codecov Report\nMerging #637 into master will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #637      +/-\n==========================================\n+ Coverage   58.88%   58.91%   +0.02%   \n==========================================\n  Files          30       30            \n  Lines        2853     2855       +2   \n==========================================\n+ Hits         1680     1682       +2   \n  Misses       1010     1010            \n  Partials      163      163\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/marshal_json.go | 83.33% <\u00f8> (\u00f8) | :arrow_up: |\n| runtime/marshal_jsonpb.go | 70% <100%> (+0.61%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c2b051d...5f5bd5c. Read the comment docs.\n. # Codecov Report\nMerging #639 into master will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #639      +/-\n==========================================\n+ Coverage   58.88%   58.91%   +0.02%   \n==========================================\n  Files          30       30            \n  Lines        2853     2855       +2   \n==========================================\n+ Hits         1680     1682       +2   \n  Misses       1010     1010            \n  Partials      163      163\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/marshal_jsonpb.go | 70% <100%> (+0.61%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c2b051d...3ea4cd3. Read the comment docs.\n. # Codecov Report\nMerging #643 into master will decrease coverage by 0.85%.\nThe diff coverage is 8.62%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #643      +/-\n==========================================\n- Coverage   58.94%   58.08%   -0.86%   \n==========================================\n  Files          30       30            \n  Lines        2857     2899      +42   \n==========================================\n  Hits         1684     1684            \n- Misses       1010     1051      +41   \n- Partials      163      164       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/generator.go | 0% <0%> (\u00f8) | :arrow_up: |\n| protoc-gen-grpc-gateway/descriptor/registry.go | 71.03% <0%> (-3.25%) | :arrow_down: |\n| protoc-gen-swagger/genswagger/template.go | 42.1% <100%> (-0.25%) | :arrow_down: |\n| protoc-gen-swagger/main.go | 27.38% <42.85%> (+1.4%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 31596c4...04ca8b3. Read the comment docs.\n. # Codecov Report\nMerging #644 into master will decrease coverage by 0.11%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #644      +/-\n==========================================\n- Coverage   53.39%   53.27%   -0.12%   \n==========================================\n  Files          30       30            \n  Lines        3362     3369       +7   \n==========================================\n  Hits         1795     1795            \n- Misses       1392     1399       +7   \n  Partials      175      175\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 38.39% <0%> (-0.3%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update dfdde99...6cc7699. Read the comment docs.\n. # Codecov Report\nMerging #645 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #645   +/-\n=======================================\n  Coverage   58.94%   58.94%         \n=======================================\n  Files          30       30         \n  Lines        2857     2857         \n=======================================\n  Hits         1684     1684         \n  Misses       1010     1010         \n  Partials      163      163\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2dc0f3e...bd4f789. Read the comment docs.\n. # Codecov Report\nMerging #648 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #648   +/-\n=======================================\n  Coverage   58.94%   58.94%         \n=======================================\n  Files          30       30         \n  Lines        2857     2857         \n=======================================\n  Hits         1684     1684         \n  Misses       1010     1010         \n  Partials      163      163\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a53af07...33e58a7. Read the comment docs.\n. # Codecov Report\nMerging #654 into master will not change coverage.\nThe diff coverage is 28.12%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #654   +/-\n=======================================\n  Coverage   58.94%   58.94%         \n=======================================\n  Files          30       30         \n  Lines        2857     2857         \n=======================================\n  Hits         1684     1684         \n  Misses       1010     1010         \n  Partials      163      163\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/gengateway/template.go | 54.9% <\u00f8> (\u00f8) | :arrow_up: |\n| runtime/handler.go | 43.33% <0%> (\u00f8) | :arrow_up: |\n| runtime/proto_errors.go | 0% <0%> (\u00f8) | :arrow_up: |\n| runtime/context.go | 71.83% <0%> (\u00f8) | :arrow_up: |\n| runtime/errors.go | 37.66% <16.66%> (\u00f8) | :arrow_up: |\n| runtime/query.go | 72.07% <50%> (\u00f8) | :arrow_up: |\n| runtime/pattern.go | 91.42% <87.5%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a53af07...6c72fcf. Read the comment docs.\n. # Codecov Report\nMerging #655 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #655   +/-\n=======================================\n  Coverage   58.94%   58.94%         \n=======================================\n  Files          30       30         \n  Lines        2857     2857         \n=======================================\n  Hits         1684     1684         \n  Misses       1010     1010         \n  Partials      163      163\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e0c01fe...ed6c5db. Read the comment docs.\n. # Codecov Report\nMerging #656 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #656   +/-\n=======================================\n  Coverage   58.94%   58.94%         \n=======================================\n  Files          30       30         \n  Lines        2857     2857         \n=======================================\n  Hits         1684     1684         \n  Misses       1010     1010         \n  Partials      163      163\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a53af07...96df95a. Read the comment docs.\n. # Codecov Report\nMerging #658 into master will decrease coverage by 1.15%.\nThe diff coverage is 6.84%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #658      +/-\n==========================================\n- Coverage   58.94%   57.78%   -1.16%   \n==========================================\n  Files          30       30            \n  Lines        2857     2914      +57   \n==========================================\n  Hits         1684     1684            \n- Misses       1010     1066      +56   \n- Partials      163      164       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/generator.go | 0% <0%> (\u00f8) | :arrow_up: |\n| protoc-gen-grpc-gateway/descriptor/registry.go | 71.03% <0%> (-3.25%) | :arrow_down: |\n| protoc-gen-swagger/genswagger/template.go | 42.1% <100%> (-0.25%) | :arrow_down: |\n| protoc-gen-swagger/main.go | 27.38% <42.85%> (+1.4%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 74ba578...e7b79eb. Read the comment docs.\n. # Codecov Report\nMerging #663 into master will decrease coverage by 1.36%.\nThe diff coverage is 13.15%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #663      +/-\n==========================================\n- Coverage   57.78%   56.42%   -1.37%   \n==========================================\n  Files          30       30            \n  Lines        2914     2995      +81   \n==========================================\n+ Hits         1684     1690       +6   \n- Misses       1066     1141      +75   \n  Partials      164      164\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 38.52% <13.15%> (-3.59%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a5b66c1...be7eb36. Read the comment docs.\n. # Codecov Report\nMerging #666 into master will decrease coverage by 0.07%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #666      +/-\n==========================================\n- Coverage   56.49%   56.42%   -0.08%   \n==========================================\n  Files          30       30            \n  Lines        3004     3006       +2   \n==========================================\n- Hits         1697     1696       -1   \n- Misses       1144     1146       +2   \n- Partials      163      164       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/types.go | 19.04% <\u00f8> (\u00f8) | :arrow_up: |\n| protoc-gen-swagger/genswagger/template.go | 38.47% <0%> (+0.19%) | :arrow_up: |\n| runtime/errors.go | 37.66% <0%> (-7.41%) | :arrow_down: |\n| runtime/marshal_json.go | 83.33% <0%> (+16.66%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 11bef10...dcd2104. Read the comment docs.\n. # Codecov Report\nMerging #667 into master will decrease coverage by 0.09%.\nThe diff coverage is 5.26%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #667     +/-\n=========================================\n- Coverage   56.43%   56.34%   -0.1%   \n=========================================\n  Files          30       30           \n  Lines        3005     3010      +5   \n=========================================\n  Hits         1696     1696           \n- Misses       1145     1150      +5   \n  Partials      164      164\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 38.27% <5.26%> (-0.25%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9b9677c...9566623. Read the comment docs.\n. # Codecov Report\nMerging #668 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #668   +/-\n=======================================\n  Coverage   56.47%   56.47%         \n=======================================\n  Files          30       30         \n  Lines        3005     3005         \n=======================================\n  Hits         1697     1697         \n  Misses       1145     1145         \n  Partials      163      163\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ee3ef70...baa8b89. Read the comment docs.\n. # Codecov Report\nMerging #687 into master will decrease coverage by 0.13%.\nThe diff coverage is 39.06%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #687      +/-\n==========================================\n- Coverage    56.3%   56.16%   -0.14%   \n==========================================\n  Files          30       30            \n  Lines        3062     3112      +50   \n==========================================\n+ Hits         1724     1748      +24   \n- Misses       1167     1193      +26   \n  Partials      171      171\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/types.go | 19.04% <\u00f8> (\u00f8) | :arrow_up: |\n| protoc-gen-swagger/genswagger/template.go | 38.75% <39.06%> (+0.57%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7951e5b...757aaa0. Read the comment docs.\n. # Codecov Report\nMerging #691 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #691   +/-\n=======================================\n  Coverage   56.47%   56.47%         \n=======================================\n  Files          30       30         \n  Lines        3005     3005         \n=======================================\n  Hits         1697     1697         \n  Misses       1145     1145         \n  Partials      163      163\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/context.go | 71.83% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fa90cfb...016436a. Read the comment docs.\n. # Codecov Report\nMerging #692 into master will increase coverage by 0.01%.\nThe diff coverage is 64.28%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #692      +/-\n==========================================\n+ Coverage   56.47%   56.48%   +0.01%   \n==========================================\n  Files          30       30            \n  Lines        3005     3013       +8   \n==========================================\n+ Hits         1697     1702       +5   \n- Misses       1145     1146       +1   \n- Partials      163      165       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 38.47% <64.28%> (+0.24%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 39a18c6...afbe113. Read the comment docs.\n. # Codecov Report\nMerging #693 into master will increase coverage by 0.05%.\nThe diff coverage is 60.71%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #693      +/-\n==========================================\n+ Coverage   56.47%   56.53%   +0.05%   \n==========================================\n  Files          30       30            \n  Lines        3005     3032      +27   \n==========================================\n+ Hits         1697     1714      +17   \n- Misses       1145     1151       +6   \n- Partials      163      167       +4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/query.go | 71.08% <60.71%> (-0.99%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 39a18c6...11fd93f. Read the comment docs.\n. # Codecov Report\nMerging #695 into master will decrease coverage by 0.11%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #695      +/-\n==========================================\n- Coverage   56.54%   56.43%   -0.12%   \n==========================================\n  Files          30       30            \n  Lines        3040     3046       +6   \n==========================================\n  Hits         1719     1719            \n- Misses       1152     1158       +6   \n  Partials      169      169\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 38.18% <0%> (-0.29%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d724c4f...bf62347. Read the comment docs.\n. # Codecov Report\nMerging #696 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #696   +/-\n=======================================\n  Coverage   56.54%   56.54%         \n=======================================\n  Files          30       30         \n  Lines        3040     3040         \n=======================================\n  Hits         1719     1719         \n  Misses       1152     1152         \n  Partials      169      169\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5948c50...7d1b84f. Read the comment docs.\n. # Codecov Report\nMerging #704 into master will increase coverage by 0.12%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #704      +/-\n==========================================\n+ Coverage   56.16%   56.29%   +0.12%   \n==========================================\n  Files          30       30            \n  Lines        3112     3121       +9   \n==========================================\n+ Hits         1748     1757       +9   \n  Misses       1193     1193            \n  Partials      171      171\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 39.39% <100%> (+0.63%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update bb916ca...291e7c1. Read the comment docs.\n. # Codecov Report\nMerging #705 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #705   +/-\n=======================================\n  Coverage   56.54%   56.54%         \n=======================================\n  Files          30       30         \n  Lines        3040     3040         \n=======================================\n  Hits         1719     1719         \n  Misses       1152     1152         \n  Partials      169      169\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 905e714...13d4478. Read the comment docs.\n. # Codecov Report\nMerging #706 into master will increase coverage by 0.19%.\nThe diff coverage is 45.94%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #706      +/-\n==========================================\n+ Coverage   56.43%   56.62%   +0.19%   \n==========================================\n  Files          30       30            \n  Lines        3046     3078      +32   \n==========================================\n+ Hits         1719     1743      +24   \n  Misses       1158     1158            \n- Partials      169      177       +8\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 39.59% <45.94%> (+1.4%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 209d7ce...0fae422. Read the comment docs.\n. # Codecov Report\nMerging #708 into master will increase coverage by <.01%.\nThe diff coverage is 62.5%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #708      +/-\n==========================================\n+ Coverage   56.43%   56.43%   +<.01%   \n==========================================\n  Files          30       30            \n  Lines        3046     3053       +7   \n==========================================\n+ Hits         1719     1723       +4   \n- Misses       1158     1159       +1   \n- Partials      169      171       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/mux.go | 44.16% <\u00f8> (\u00f8) | :arrow_up: |\n| runtime/pattern.go | 89.79% <62.5%> (-1.64%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 209d7ce...6d5b579. Read the comment docs.\n. # Codecov Report\nMerging #712 into master will decrease coverage by 0.45%.\nThe diff coverage is 29.16%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #712      +/-\n==========================================\n- Coverage   56.16%   55.71%   -0.46%   \n==========================================\n  Files          30       30            \n  Lines        3112     3152      +40   \n==========================================\n+ Hits         1748     1756       +8   \n- Misses       1193     1223      +30   \n- Partials      171      173       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/mux.go | 44.16% <\u00f8> (\u00f8) | :arrow_up: |\n| protoc-gen-grpc-gateway/descriptor/types.go | 49.07% <\u00f8> (\u00f8) | :arrow_up: |\n| protoc-gen-grpc-gateway/gengateway/template.go | 55.76% <\u00f8> (\u00f8) | :arrow_up: |\n| runtime/handler.go | 41.6% <0%> (-1.74%) | :arrow_down: |\n| protoc-gen-swagger/main.go | 27.05% <0%> (-0.33%) | :arrow_down: |\n| protoc-gen-grpc-gateway/descriptor/registry.go | 68.06% <0%> (-2.98%) | :arrow_down: |\n| protoc-gen-swagger/genswagger/template.go | 38.36% <35%> (-0.4%) | :arrow_down: |\n| protoc-gen-grpc-gateway/descriptor/services.go | 73.97% <53.84%> (-1.57%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update bb916ca...b8fc556. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@7226a0d). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #715   +/-\n=========================================\n  Coverage          ?   56.43%         \n=========================================\n  Files             ?       30         \n  Lines             ?     3053         \n  Branches          ?        0         \n=========================================\n  Hits              ?     1723         \n  Misses            ?     1159         \n  Partials          ?      171\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7226a0d...8e3f02a. Read the comment docs.\n. # Codecov Report\nMerging #716 into master will decrease coverage by 0.05%.\nThe diff coverage is 25%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #716      +/-\n==========================================\n- Coverage   56.43%   56.38%   -0.06%   \n==========================================\n  Files          30       30            \n  Lines        3053     3056       +3   \n==========================================\n  Hits         1723     1723            \n- Misses       1159     1162       +3   \n  Partials      171      171\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/types.go | 19.04% <\u00f8> (\u00f8) | :arrow_up: |\n| protoc-gen-swagger/genswagger/template.go | 38.04% <25%> (-0.15%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4a8ec3c...86d3692. Read the comment docs.\n. # Codecov Report\nMerging #718 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #718      +/-\n=========================================\n+ Coverage   56.28%   56.3%   +0.01%   \n=========================================\n  Files          30      30            \n  Lines        3061    3062       +1   \n=========================================\n+ Hits         1723    1724       +1   \n  Misses       1167    1167            \n  Partials      171     171\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/errors.go | 45.83% <100%> (+0.76%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 42fa202...1283180. Read the comment docs.\n. # Codecov Report\nMerging #719 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #719   +/-\n=======================================\n  Coverage   56.16%   56.16%         \n=======================================\n  Files          30       30         \n  Lines        3112     3112         \n=======================================\n  Hits         1748     1748         \n  Misses       1193     1193         \n  Partials      171      171\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/types.go | 19.04% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 221ef34...2183528. Read the comment docs.\n. # Codecov Report\nMerging #721 into master will decrease coverage by 0.33%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #721      +/-\n==========================================\n- Coverage   56.16%   55.83%   -0.34%   \n==========================================\n  Files          30       30            \n  Lines        3112     3161      +49   \n==========================================\n+ Hits         1748     1765      +17   \n- Misses       1193     1223      +30   \n- Partials      171      173       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/descriptor/registry.go | 68.06% <0%> (-2.98%) | :arrow_down: |\n| runtime/handler.go | 41.6% <0%> (-1.74%) | :arrow_down: |\n| protoc-gen-grpc-gateway/descriptor/services.go | 73.97% <0%> (-1.57%) | :arrow_down: |\n| protoc-gen-swagger/main.go | 27.05% <0%> (-0.33%) | :arrow_down: |\n| runtime/mux.go | 44.16% <0%> (\u00f8) | :arrow_up: |\n| protoc-gen-grpc-gateway/descriptor/types.go | 49.07% <0%> (\u00f8) | :arrow_up: |\n| protoc-gen-swagger/genswagger/template.go | 38.99% <0%> (+0.23%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 221ef34...925ebf0. Read the comment docs.\n. # Codecov Report\nMerging #724 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #724   +/-\n=======================================\n  Coverage   56.16%   56.16%         \n=======================================\n  Files          30       30         \n  Lines        3112     3112         \n=======================================\n  Hits         1748     1748         \n  Misses       1193     1193         \n  Partials      171      171\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update bb916ca...d18a5e9. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@e679739). Click here to learn what that means.\nThe diff coverage is 75%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #730   +/-\n=========================================\n  Coverage          ?   56.29%         \n=========================================\n  Files             ?       30         \n  Lines             ?     3121         \n  Branches          ?        0         \n=========================================\n  Hits              ?     1757         \n  Misses            ?     1193         \n  Partials          ?      171\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/gengateway/template.go | 55.76% <\u00f8> (\u00f8) | |\n| runtime/mux.go | 44.16% <\u00f8> (\u00f8) | |\n| protoc-gen-grpc-gateway/descriptor/types.go | 49.07% <\u00f8> (\u00f8) | |\n| protoc-gen-grpc-gateway/descriptor/registry.go | 71.03% <\u00f8> (\u00f8) | |\n| protoc-gen-swagger/main.go | 27.38% <\u00f8> (\u00f8) | |\n| runtime/handler.go | 43.33% <0%> (\u00f8) | |\n| protoc-gen-grpc-gateway/descriptor/services.go | 75.54% <100%> (\u00f8) | |\n| protoc-gen-swagger/genswagger/template.go | 39.39% <83.33%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e679739...15b2e49. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@e679739). Click here to learn what that means.\nThe diff coverage is 72.72%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #737   +/-\n=========================================\n  Coverage          ?   55.86%         \n=========================================\n  Files             ?       30         \n  Lines             ?     3170         \n  Branches          ?        0         \n=========================================\n  Hits              ?     1771         \n  Misses            ?     1225         \n  Partials          ?      174\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/mux.go | 43.69% <\u00f8> (\u00f8) | |\n| runtime/context.go | 71.6% <72.72%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e679739...400ef68. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@e679739). Click here to learn what that means.\nThe diff coverage is 28.57%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #738   +/-\n=========================================\n  Coverage          ?   55.36%         \n=========================================\n  Files             ?       30         \n  Lines             ?     3213         \n  Branches          ?        0         \n=========================================\n  Hits              ?     1779         \n  Misses            ?     1259         \n  Partials          ?      175\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/convert.go | 44.89% <0%> (\u00f8) | |\n| protoc-gen-grpc-gateway/descriptor/types.go | 44.53% <0%> (\u00f8) | |\n| protoc-gen-swagger/genswagger/template.go | 38.95% <28.57%> (\u00f8) | |\n| protoc-gen-grpc-gateway/gengateway/generator.go | 38.77% <40%> (\u00f8) | |\n| protoc-gen-grpc-gateway/gengateway/template.go | 58.06% <66.66%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e679739...7cf77b0. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@0e59487). Click here to learn what that means.\nThe diff coverage is 13.29%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #742   +/-\n=========================================\n  Coverage          ?   53.38%         \n=========================================\n  Files             ?       30         \n  Lines             ?     3368         \n  Branches          ?        0         \n=========================================\n  Hits              ?     1798         \n  Misses            ?     1393         \n  Partials          ?      177\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/types.go | 19.04% <\u00f8> (\u00f8) | |\n| runtime/convert.go | 16.41% <0%> (\u00f8) | |\n| protoc-gen-swagger/main.go | 26.13% <0%> (\u00f8) | |\n| protoc-gen-grpc-gateway/descriptor/types.go | 41.08% <0%> (\u00f8) | |\n| protoc-gen-grpc-gateway/descriptor/registry.go | 62.03% <16%> (\u00f8) | |\n| protoc-gen-swagger/genswagger/template.go | 38.69% <39.13%> (\u00f8) | |\n| protoc-gen-grpc-gateway/gengateway/template.go | 60.6% <80%> (\u00f8) | |\n| protoc-gen-grpc-gateway/descriptor/services.go | 73.97% <80%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0e59487...9dc2ea3. Read the comment docs.\n. # Codecov Report\nMerging #750 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #750   +/-\n=======================================\n  Coverage   53.38%   53.38%         \n=======================================\n  Files          30       30         \n  Lines        3368     3368         \n=======================================\n  Hits         1798     1798         \n  Misses       1393     1393         \n  Partials      177      177\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8558711...63b2df0. Read the comment docs.\n. # Codecov Report\nMerging #775 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #775   +/-\n=======================================\n  Coverage   53.37%   53.37%         \n=======================================\n  Files          30       30         \n  Lines        3361     3361         \n=======================================\n  Hits         1794     1794         \n  Misses       1392     1392         \n  Partials      175      175\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 61203f4...12f6589. Read the comment docs.\n. # Codecov Report\nMerging #781 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #781   +/-\n=======================================\n  Coverage   53.37%   53.37%         \n=======================================\n  Files          30       30         \n  Lines        3361     3361         \n=======================================\n  Hits         1794     1794         \n  Misses       1392     1392         \n  Partials      175      175\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 61203f4...8cdd51c. Read the comment docs.\n. # Codecov Report\nMerging #783 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #783      +/-\n==========================================\n+ Coverage   53.37%   53.39%   +0.01%   \n==========================================\n  Files          30       30            \n  Lines        3361     3362       +1   \n==========================================\n+ Hits         1794     1795       +1   \n  Misses       1392     1392            \n  Partials      175      175\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/mux.go | 44.16% <100%> (+0.46%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 61203f4...4d9776b. Read the comment docs.\n. # Codecov Report\nMerging #784 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #784      +/-\n==========================================\n+ Coverage   53.37%   53.39%   +0.01%   \n==========================================\n  Files          30       30            \n  Lines        3361     3362       +1   \n==========================================\n+ Hits         1794     1795       +1   \n  Misses       1392     1392            \n  Partials      175      175\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/mux.go | 44.16% <100%> (+0.46%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 61203f4...77f1f0c. Read the comment docs.\n. # Codecov Report\nMerging #791 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #791   +/-\n=======================================\n  Coverage   53.39%   53.39%         \n=======================================\n  Files          30       30         \n  Lines        3362     3362         \n=======================================\n  Hits         1795     1795         \n  Misses       1392     1392         \n  Partials      175      175\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f7cf649...3eb9d02. Read the comment docs.\n. # Codecov Report\nMerging #793 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #793   +/-\n=======================================\n  Coverage   53.39%   53.39%         \n=======================================\n  Files          30       30         \n  Lines        3362     3362         \n=======================================\n  Hits         1795     1795         \n  Misses       1392     1392         \n  Partials      175      175\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fdcb0d0...c1b8be2. Read the comment docs.\n. # Codecov Report\nMerging #795 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #795   +/-\n=======================================\n  Coverage   53.39%   53.39%         \n=======================================\n  Files          30       30         \n  Lines        3362     3362         \n=======================================\n  Hits         1795     1795         \n  Misses       1392     1392         \n  Partials      175      175\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/gengateway/template.go | 60.6% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c203e72...d22b116. Read the comment docs.\n. # Codecov Report\nMerging #797 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #797   +/-\n=======================================\n  Coverage   53.27%   53.27%         \n=======================================\n  Files          30       30         \n  Lines        3369     3369         \n=======================================\n  Hits         1795     1795         \n  Misses       1399     1399         \n  Partials      175      175\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7d429ea...564438b. Read the comment docs.\n. # Codecov Report\nMerging #798 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #798   +/-\n=======================================\n  Coverage   53.27%   53.27%         \n=======================================\n  Files          30       30         \n  Lines        3369     3369         \n=======================================\n  Hits         1795     1795         \n  Misses       1399     1399         \n  Partials      175      175\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 53343c7...d0ad6f9. Read the comment docs.\n. # Codecov Report\nMerging #799 into master will decrease coverage by 0.06%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #799      +/-\n==========================================\n- Coverage   53.27%   53.21%   -0.07%   \n==========================================\n  Files          30       30            \n  Lines        3369     3373       +4   \n==========================================\n  Hits         1795     1795            \n- Misses       1399     1403       +4   \n  Partials      175      175\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/types.go | 19.04% <\u00f8> (\u00f8) | :arrow_up: |\n| protoc-gen-swagger/genswagger/template.go | 38.22% <0%> (-0.17%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7d429ea...86f635e. Read the comment docs.\n. # Codecov Report\nMerging #800 into master will increase coverage by 0.08%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #800      +/-\n==========================================\n+ Coverage   53.27%   53.36%   +0.08%   \n==========================================\n  Files          30       30            \n  Lines        3369     3375       +6   \n==========================================\n+ Hits         1795     1801       +6   \n  Misses       1399     1399            \n  Partials      175      175\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 38.79% <100%> (+0.4%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7d429ea...1248d9a. Read the comment docs.\n. # Codecov Report\nMerging #801 into master will increase coverage by 4.15%.\nThe diff coverage is 48.48%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #801      +/-\n==========================================\n+ Coverage   49.32%   53.47%   +4.15%   \n==========================================\n  Files          39       30       -9   \n  Lines        3751     3370     -381   \n==========================================\n- Hits         1850     1802      -48   \n+ Misses       1723     1389     -334   \n- Partials      178      179       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/types.go | 19.04% <\u00f8> (\u00f8) | :arrow_up: |\n| protoc-gen-swagger/genswagger/template.go | 39.12% <48.48%> (+0.59%) | :arrow_up: |\n| examples/server/unannotatedecho.go | | |\n| examples/server/a_bit_of_everything.go | | |\n| examples/server/responsebody.go | | |\n| examples/server/echo.go | | |\n| examples/server/fieldmask_helper.go | | |\n| examples/server/flow_combination.go | | |\n| utilities/readerfactory.go | | |\n| ... and 4 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5b7aa47...ed0dd91. Read the comment docs.\n. # Codecov Report\nMerging #802 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #802   +/-\n=======================================\n  Coverage   53.27%   53.27%         \n=======================================\n  Files          30       30         \n  Lines        3369     3369         \n=======================================\n  Hits         1795     1795         \n  Misses       1399     1399         \n  Partials      175      175\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3ff87af...f6ea87e. Read the comment docs.\n. # Codecov Report\nMerging #803 into master will decrease coverage by 0.04%.\nThe diff coverage is 25%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #803      +/-\n==========================================\n- Coverage   53.27%   53.23%   -0.05%   \n==========================================\n  Files          30       30            \n  Lines        3369     3372       +3   \n==========================================\n  Hits         1795     1795            \n- Misses       1399     1402       +3   \n  Partials      175      175\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/descriptor/registry.go | 61.46% <0%> (-0.57%) | :arrow_down: |\n| protoc-gen-swagger/main.go | 25.84% <0%> (-0.3%) | :arrow_down: |\n| protoc-gen-grpc-gateway/descriptor/services.go | 73.97% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3ff87af...379e0e5. Read the comment docs.\n. # Codecov Report\nMerging #805 into master will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #805      +/-\n=========================================\n+ Coverage   53.27%   53.3%   +0.02%   \n=========================================\n  Files          30      30            \n  Lines        3369    3371       +2   \n=========================================\n+ Hits         1795    1797       +2   \n  Misses       1399    1399            \n  Partials      175     175\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 38.52% <100%> (+0.13%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3ff87af...c40e144. Read the comment docs.\n. # Codecov Report\nMerging #807 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster    #807   +/-\n======================================\n  Coverage    53.3%   53.3%         \n======================================\n  Files          30      30         \n  Lines        3371    3371         \n======================================\n  Hits         1797    1797         \n  Misses       1399    1399         \n  Partials      175     175\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a7c0cd0...7239462. Read the comment docs.\n. # Codecov Report\nMerging #809 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster    #809   +/-\n======================================\n  Coverage    53.3%   53.3%         \n======================================\n  Files          30      30         \n  Lines        3371    3371         \n======================================\n  Hits         1797    1797         \n  Misses       1399    1399         \n  Partials      175     175\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/descriptor/types.go | 41.08% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a7c0cd0...99a9425. Read the comment docs.\n. # Codecov Report\nMerging #810 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster    #810   +/-\n======================================\n  Coverage    53.3%   53.3%         \n======================================\n  Files          30      30         \n  Lines        3371    3371         \n======================================\n  Hits         1797    1797         \n  Misses       1399    1399         \n  Partials      175     175\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 38.52% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1659831...e75be2b. Read the comment docs.\n. # Codecov Report\nMerging #812 into master will decrease coverage by 3.64%.\nThe diff coverage is 57.6%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #812      +/-\n==========================================\n- Coverage    53.3%   49.66%   -3.65%   \n==========================================\n  Files          30       39       +9   \n  Lines        3371     3725     +354   \n==========================================\n+ Hits         1797     1850      +53   \n- Misses       1399     1697     +298   \n- Partials      175      178       +3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| examples/server/a_bit_of_everything.go | 0% <0%> (\u00f8) | |\n| utilities/readerfactory.go | 0% <0%> (\u00f8) | |\n| protoc-gen-grpc-gateway/gengateway/template.go | 53.75% <21.42%> (-6.86%) | :arrow_down: |\n| examples/server/fieldmask_helper.go | 92% <92%> (\u00f8) | |\n| runtime/fieldmask.go | 96.42% <96.42%> (\u00f8) | |\n| examples/server/unannotatedecho.go | 0% <0%> (\u00f8) | |\n| examples/server/responsebody.go | 0% <0%> (\u00f8) | |\n| examples/server/echo.go | 0% <0%> (\u00f8) | |\n| ... and 4 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update da4a1fc...c439fa6. Read the comment docs.\n. # Codecov Report\nMerging #813 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster    #813   +/-\n======================================\n  Coverage    53.3%   53.3%         \n======================================\n  Files          30      30         \n  Lines        3371    3371         \n======================================\n  Hits         1797    1797         \n  Misses       1399    1399         \n  Partials      175     175\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/descriptor/types.go | 41.08% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update da4a1fc...d0fb1bd. Read the comment docs.\n. # Codecov Report\nMerging #814 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster    #814   +/-\n======================================\n  Coverage    53.3%   53.3%         \n======================================\n  Files          30      30         \n  Lines        3371    3371         \n======================================\n  Hits         1797    1797         \n  Misses       1399    1399         \n  Partials      175     175\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 09679ff...7bb067d. Read the comment docs.\n. # Codecov Report\nMerging #816 into master will decrease coverage by 0.4%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #816      +/-\n==========================================\n- Coverage    53.3%   52.89%   -0.41%   \n==========================================\n  Files          30       30            \n  Lines        3371     3397      +26   \n==========================================\n  Hits         1797     1797            \n- Misses       1399     1425      +26   \n  Partials      175      175\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/descriptor/types.go | 41.08% <\u00f8> (\u00f8) | :arrow_up: |\n| runtime/convert.go | 13.75% <0%> (-2.67%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 09679ff...56f3478. Read the comment docs.\n. # Codecov Report\nMerging #818 into master will increase coverage by 0.15%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #818      +/-\n==========================================\n+ Coverage   49.32%   49.48%   +0.15%   \n==========================================\n  Files          39       39            \n  Lines        3751     3751            \n==========================================\n+ Hits         1850     1856       +6   \n+ Misses       1723     1717       -6   \n  Partials      178      178\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/gengateway/template.go | 61.25% <0%> (+7.5%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d8ad87e...6979ebc. Read the comment docs.\n. # Codecov Report\nMerging #821 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #821   +/-\n=======================================\n  Coverage   49.32%   49.32%         \n=======================================\n  Files          39       39         \n  Lines        3751     3751         \n=======================================\n  Hits         1850     1850         \n  Misses       1723     1723         \n  Partials      178      178\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d8ad87e...ba17731. Read the comment docs.\n. # Codecov Report\nMerging #822 into master will not change coverage.\nThe diff coverage is 0%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #822   +/-\n=======================================\n  Coverage   49.32%   49.32%         \n=======================================\n  Files          39       39         \n  Lines        3751     3751         \n=======================================\n  Hits         1850     1850         \n  Misses       1723     1723         \n  Partials      178      178\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/handler.go | 41.6% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7ad920c...0d911ed. Read the comment docs.\n. # Codecov Report\nMerging #824 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #824   +/-\n=======================================\n  Coverage   49.32%   49.32%         \n=======================================\n  Files          39       39         \n  Lines        3751     3751         \n=======================================\n  Hits         1850     1850         \n  Misses       1723     1723         \n  Partials      178      178\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7ad920c...1ef0285. Read the comment docs.\n. # Codecov Report\nMerging #826 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #826   +/-\n=======================================\n  Coverage   49.32%   49.32%         \n=======================================\n  Files          39       39         \n  Lines        3751     3751         \n=======================================\n  Hits         1850     1850         \n  Misses       1723     1723         \n  Partials      178      178\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 66f0e34...e2a8087. Read the comment docs.\n. # Codecov Report\nMerging #827 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #827   +/-\n=======================================\n  Coverage   49.32%   49.32%         \n=======================================\n  Files          39       39         \n  Lines        3751     3751         \n=======================================\n  Hits         1850     1850         \n  Misses       1723     1723         \n  Partials      178      178\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 01c07cd...15b76d7. Read the comment docs.\n. # Codecov Report\nMerging #828 into master will decrease coverage by 0.98%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #828      +/-\n==========================================\n- Coverage   50.26%   49.28%   -0.99%   \n==========================================\n  Files          39       39            \n  Lines        3752     3754       +2   \n==========================================\n- Hits         1886     1850      -36   \n- Misses       1684     1726      +42   \n+ Partials      182      178       -4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/main.go | 25.27% <0%> (-0.87%) | :arrow_down: |\n| protoc-gen-swagger/genswagger/template.go | 38.52% <0%> (-3.91%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 11787b1...13d946a. Read the comment docs.\n. # Codecov Report\nMerging #835 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #835   +/-\n=======================================\n  Coverage   50.22%   50.22%         \n=======================================\n  Files          39       39         \n  Lines        3755     3755         \n=======================================\n  Hits         1886     1886         \n  Misses       1687     1687         \n  Partials      182      182\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5b007c7...1ddab17. Read the comment docs.\n. # Codecov Report\nMerging #836 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #836   +/-\n=======================================\n  Coverage   50.22%   50.22%         \n=======================================\n  Files          39       39         \n  Lines        3755     3755         \n=======================================\n  Hits         1886     1886         \n  Misses       1687     1687         \n  Partials      182      182\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 719aaad...90cf292. Read the comment docs.\n. # Codecov Report\nMerging #840 into master will increase coverage by 0.23%.\nThe diff coverage is 62.5%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #840      +/-\n==========================================\n+ Coverage   50.22%   50.46%   +0.23%   \n==========================================\n  Files          39       39            \n  Lines        3755     3761       +6   \n==========================================\n+ Hits         1886     1898      +12   \n+ Misses       1687     1681       -6   \n  Partials      182      182\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/gengateway/generator.go | 39% <33.33%> (+0.22%) | :arrow_up: |\n| protoc-gen-grpc-gateway/gengateway/template.go | 64.28% <80%> (+10.53%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a6d3ad2...3c839ab. Read the comment docs.\n. # Codecov Report\nMerging #843 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #843   +/-\n=======================================\n  Coverage   50.46%   50.46%         \n=======================================\n  Files          39       39         \n  Lines        3761     3761         \n=======================================\n  Hits         1898     1898         \n  Misses       1681     1681         \n  Partials      182      182\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 96cb2e2...0b2ccd7. Read the comment docs.\n. # Codecov Report\nMerging #844 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #844   +/-\n=======================================\n  Coverage   50.46%   50.46%         \n=======================================\n  Files          39       39         \n  Lines        3761     3761         \n=======================================\n  Hits         1898     1898         \n  Misses       1681     1681         \n  Partials      182      182\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ae4bb97...b4820be. Read the comment docs.\n. # Codecov Report\nMerging #845 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #845   +/-\n=======================================\n  Coverage   50.46%   50.46%         \n=======================================\n  Files          39       39         \n  Lines        3761     3761         \n=======================================\n  Hits         1898     1898         \n  Misses       1681     1681         \n  Partials      182      182\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ae4bb97...17cb0a8. Read the comment docs.\n. # Codecov Report\nMerging #847 into master will decrease coverage by 0.05%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #847      +/-\n==========================================\n- Coverage   50.46%   50.41%   -0.06%   \n==========================================\n  Files          39       39            \n  Lines        3761     3765       +4   \n==========================================\n  Hits         1898     1898            \n- Misses       1681     1685       +4   \n  Partials      182      182\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/types.go | 19.04% <\u00f8> (\u00f8) | :arrow_up: |\n| protoc-gen-swagger/genswagger/template.go | 42.24% <0%> (-0.19%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9ae9465...1e7e5ef. Read the comment docs.\n. # Codecov Report\nMerging #849 into master will increase coverage by 1.42%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #849      +/-\n==========================================\n+ Coverage   50.41%   51.83%   +1.42%   \n==========================================\n  Files          39       39            \n  Lines        3765     3764       -1   \n==========================================\n+ Hits         1898     1951      +53   \n+ Misses       1685     1629      -56   \n- Partials      182      184       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 48.08% <100%> (+5.83%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e5ffc3b...a766ec3. Read the comment docs.\n. # Codecov Report\nMerging #850 into master will increase coverage by 1.32%.\nThe diff coverage is 71.91%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #850      +/-\n==========================================\n+ Coverage   51.75%   53.08%   +1.32%   \n==========================================\n  Files          39       39            \n  Lines        3824     3845      +21   \n==========================================\n+ Hits         1979     2041      +62   \n+ Misses       1655     1610      -45   \n- Partials      190      194       +4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/types.go | 19.04% <\u00f8> (\u00f8) | :arrow_up: |\n| runtime/handler.go | 41.6% <\u00f8> (\u00f8) | :arrow_up: |\n| protoc-gen-swagger/genswagger/generator.go | 0% <0%> (\u00f8) | :arrow_up: |\n| protoc-gen-swagger/main.go | 24.46% <0%> (-2.34%) | :arrow_down: |\n| protoc-gen-swagger/genswagger/template.go | 53.48% <76.19%> (+5.14%) | :arrow_up: |\n| runtime/mux.go | 44.16% <0%> (-1.37%) | :arrow_down: |\n| examples/server/responsebody.go | 0% <0%> (\u00f8) | :arrow_up: |\n| runtime/marshal_jsonpb.go | 70.58% <0%> (+1.48%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9221822...ae2eca3. Read the comment docs.\n. # Codecov Report\nMerging #852 into master will increase coverage by 0.04%.\nThe diff coverage is 66.66%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #852      +/-\n==========================================\n+ Coverage   51.83%   51.87%   +0.04%   \n==========================================\n  Files          39       39            \n  Lines        3764     3778      +14   \n==========================================\n+ Hits         1951     1960       +9   \n- Misses       1629     1633       +4   \n- Partials      184      185       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 48.33% <66.66%> (+0.24%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 20e8cf9...b76f6a0. Read the comment docs.\n. # Codecov Report\nMerging #853 into master will decrease coverage by 0.16%.\nThe diff coverage is 16.66%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #853      +/-\n==========================================\n- Coverage   51.83%   51.66%   -0.17%   \n==========================================\n  Files          39       39            \n  Lines        3764     3782      +18   \n==========================================\n+ Hits         1951     1954       +3   \n- Misses       1629     1643      +14   \n- Partials      184      185       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| examples/server/responsebody.go | 0% <0%> (\u00f8) | :arrow_up: |\n| protoc-gen-swagger/main.go | 26.8% <50%> (+1.52%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 20e8cf9...e76e705. Read the comment docs.\n. # Codecov Report\nMerging #854 into master will decrease coverage by 0.02%.\nThe diff coverage is 33.33%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #854      +/-\n==========================================\n- Coverage   51.66%   51.63%   -0.03%   \n==========================================\n  Files          39       39            \n  Lines        3782     3788       +6   \n==========================================\n+ Hits         1954     1956       +2   \n- Misses       1643     1647       +4   \n  Partials      185      185\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| examples/server/responsebody.go | 0% <0%> (\u00f8) | :arrow_up: |\n| runtime/marshal_jsonpb.go | 71.15% <100%> (+0.56%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a0500cb...7098b14. Read the comment docs.\n. # Codecov Report\nMerging #855 into master will increase coverage by 0.03%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #855      +/-\n==========================================\n+ Coverage   51.63%   51.67%   +0.03%   \n==========================================\n  Files          39       39            \n  Lines        3788     3791       +3   \n==========================================\n+ Hits         1956     1959       +3   \n  Misses       1647     1647            \n  Partials      185      185\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/mux.go | 45.52% <100%> (+1.36%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ff4dc68...c9c7f46. Read the comment docs.\n. # Codecov Report\nMerging #856 into master will increase coverage by 0.03%.\nThe diff coverage is 61.9%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #856      +/-\n==========================================\n+ Coverage   51.63%   51.66%   +0.03%   \n==========================================\n  Files          39       39            \n  Lines        3788     3807      +19   \n==========================================\n+ Hits         1956     1967      +11   \n- Misses       1647     1651       +4   \n- Partials      185      189       +4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/marshal_jsonpb.go | 69.1% <61.9%> (-2.05%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ff4dc68...0a364dd. Read the comment docs.\n. # Codecov Report\nMerging #857 into master will increase coverage by 0.1%.\nThe diff coverage is 86.66%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #857     +/-\n=========================================\n+ Coverage   52.98%   53.08%   +0.1%   \n=========================================\n  Files          39       39           \n  Lines        3905     3888     -17   \n=========================================\n- Hits         2069     2064      -5   \n+ Misses       1636     1630      -6   \n+ Partials      200      194      -6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 53.54% <\u00f8> (-0.09%) | :arrow_down: |\n| protoc-gen-swagger/genswagger/generator.go | 15.11% <86.66%> (+15.11%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f336fbc...7586e8a. Read the comment docs.\n. # Codecov Report\nMerging #858 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #858   +/-\n=======================================\n  Coverage   53.08%   53.08%         \n=======================================\n  Files          39       39         \n  Lines        3888     3888         \n=======================================\n  Hits         2064     2064         \n  Misses       1630     1630         \n  Partials      194      194\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f52ac10...4546470. Read the comment docs.\n. # Codecov Report\nMerging #860 into master will decrease coverage by 0.07%.\nThe diff coverage is 35.71%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #860      +/-\n==========================================\n- Coverage   53.08%   53.01%   -0.08%   \n==========================================\n  Files          39       39            \n  Lines        3888     3901      +13   \n==========================================\n+ Hits         2064     2068       +4   \n- Misses       1630     1637       +7   \n- Partials      194      196       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/descriptor/registry.go | 60.9% <0%> (-1.13%) | :arrow_down: |\n| protoc-gen-swagger/main.go | 27.35% <50%> (+1.35%) | :arrow_up: |\n| protoc-gen-swagger/genswagger/template.go | 53.48% <50%> (-0.07%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b7a5640...d423496. Read the comment docs.\n. # Codecov Report\nMerging #861 into master will decrease coverage by 0.63%.\nThe diff coverage is 2.04%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #861      +/-\n==========================================\n- Coverage   53.08%   52.45%   -0.64%   \n==========================================\n  Files          39       39            \n  Lines        3888     3937      +49   \n==========================================\n+ Hits         2064     2065       +1   \n- Misses       1630     1676      +46   \n- Partials      194      196       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/main.go | 25.74% <0%> (-0.26%) | :arrow_down: |\n| protoc-gen-grpc-gateway/descriptor/registry.go | 60.9% <0%> (-1.13%) | :arrow_down: |\n| protoc-gen-swagger/genswagger/template.go | 51.32% <2.27%> (-2.22%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b7a5640...70bba53. Read the comment docs.\n. # Codecov Report\nMerging #862 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #862   +/-\n=======================================\n  Coverage   53.01%   53.01%         \n=======================================\n  Files          39       39         \n  Lines        3901     3901         \n=======================================\n  Hits         2068     2068         \n  Misses       1637     1637         \n  Partials      196      196\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1b220ea...e1c7f12. Read the comment docs.\n. # Codecov Report\nMerging #865 into master will decrease coverage by 0.05%.\nThe diff coverage is 31.81%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #865      +/-\n==========================================\n- Coverage   53.01%   52.95%   -0.06%   \n==========================================\n  Files          39       39            \n  Lines        3901     3907       +6   \n==========================================\n+ Hits         2068     2069       +1   \n- Misses       1637     1642       +5   \n  Partials      196      196\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 53.25% <31.81%> (-0.23%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c3923b1...a1d616b. Read the comment docs.\n. # Codecov Report\nMerging #866 into master will increase coverage by 0.03%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #866      +/-\n==========================================\n+ Coverage   52.95%   52.99%   +0.03%   \n==========================================\n  Files          39       39            \n  Lines        3907     3910       +3   \n==========================================\n+ Hits         2069     2072       +3   \n  Misses       1642     1642            \n  Partials      196      196\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/gengateway/template.go | 65.51% <100%> (+1.23%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 45aec34...5b49183. Read the comment docs.\n. # Codecov Report\nMerging #869 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #869   +/-\n=======================================\n  Coverage   52.95%   52.95%         \n=======================================\n  Files          39       39         \n  Lines        3907     3907         \n=======================================\n  Hits         2069     2069         \n  Misses       1642     1642         \n  Partials      196      196\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 45aec34...ccda42b. Read the comment docs.\n. # Codecov Report\nMerging #870 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #870   +/-\n=======================================\n  Coverage   52.99%   52.99%         \n=======================================\n  Files          39       39         \n  Lines        3910     3910         \n=======================================\n  Hits         2072     2072         \n  Misses       1642     1642         \n  Partials      196      196\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ada3454...8c077d0. Read the comment docs.\n. # Codecov Report\nMerging #876 into master will not change coverage.\nThe diff coverage is 0%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #876   +/-\n=======================================\n  Coverage   52.99%   52.99%         \n=======================================\n  Files          39       39         \n  Lines        3910     3910         \n=======================================\n  Hits         2072     2072         \n  Misses       1642     1642         \n  Partials      196      196\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/gengateway/generator.go | 39% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9a7c952...c260331. Read the comment docs.\n. # Codecov Report\nMerging #878 into master will decrease coverage by 0.1%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #878      +/-\n==========================================\n- Coverage   52.99%   52.88%   -0.11%   \n==========================================\n  Files          39       39            \n  Lines        3910     3918       +8   \n==========================================\n  Hits         2072     2072            \n- Misses       1642     1650       +8   \n  Partials      196      196\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/generator.go | 13.82% <0%> (-1.29%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 70fc086...629afb0. Read the comment docs.\n. # Codecov Report\nMerging #879 into master will increase coverage by 0.13%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #879      +/-\n==========================================\n+ Coverage   52.99%   53.12%   +0.13%   \n==========================================\n  Files          39       39            \n  Lines        3910     3891      -19   \n==========================================\n- Hits         2072     2067       -5   \n+ Misses       1642     1630      -12   \n+ Partials      196      194       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 53.68% <100%> (+0.42%) | :arrow_up: |\n| protoc-gen-swagger/main.go | 26% <0%> (-1.36%) | :arrow_down: |\n| protoc-gen-grpc-gateway/gengateway/template.go | 64.28% <0%> (-1.24%) | :arrow_down: |\n| protoc-gen-grpc-gateway/descriptor/registry.go | 62.03% <0%> (+1.12%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 70fc086...a437376. Read the comment docs.\n. # Codecov Report\nMerging #880 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #880   +/-\n=======================================\n  Coverage   53.02%   53.02%         \n=======================================\n  Files          39       39         \n  Lines        3913     3913         \n=======================================\n  Hits         2075     2075         \n  Misses       1642     1642         \n  Partials      196      196\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 206758a...35ee0a8. Read the comment docs.\n. # Codecov Report\nMerging #882 into master will increase coverage by 0.69%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #882      +/-\n==========================================\n+ Coverage   52.99%   53.69%   +0.69%   \n==========================================\n  Files          39       39            \n  Lines        3921     3926       +5   \n==========================================\n+ Hits         2078     2108      +30   \n+ Misses       1647     1621      -26   \n- Partials      196      197       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/types.go | 19.04% <\u00f8> (\u00f8) | :arrow_up: |\n| protoc-gen-swagger/genswagger/template.go | 56.29% <100%> (+2.75%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0c2b3b1...251483a. Read the comment docs.\n. # Codecov Report\nMerging #883 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #883   +/-\n=======================================\n  Coverage   53.69%   53.69%         \n=======================================\n  Files          39       39         \n  Lines        3926     3926         \n=======================================\n  Hits         2108     2108         \n  Misses       1621     1621         \n  Partials      197      197\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 144843a...cf82b8c. Read the comment docs.\n. # Codecov Report\nMerging #884 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #884   +/-\n=======================================\n  Coverage   53.69%   53.69%         \n=======================================\n  Files          39       39         \n  Lines        3926     3926         \n=======================================\n  Hits         2108     2108         \n  Misses       1621     1621         \n  Partials      197      197\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 920e58b...c1d71cf. Read the comment docs.\n. # Codecov Report\nMerging #887 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #887   +/-\n=======================================\n  Coverage   53.69%   53.69%         \n=======================================\n  Files          39       39         \n  Lines        3926     3926         \n=======================================\n  Hits         2108     2108         \n  Misses       1621     1621         \n  Partials      197      197\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4ce2f87...11b0fcc. Read the comment docs.\n. # Codecov Report\nMerging #889 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #889   +/-\n=======================================\n  Coverage   53.69%   53.69%         \n=======================================\n  Files          39       39         \n  Lines        3926     3926         \n=======================================\n  Hits         2108     2108         \n  Misses       1621     1621         \n  Partials      197      197\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cc6fc94...b891478. Read the comment docs.\n. # Codecov Report\nMerging #890 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #890   +/-\n=======================================\n  Coverage   53.69%   53.69%         \n=======================================\n  Files          39       39         \n  Lines        3926     3926         \n=======================================\n  Hits         2108     2108         \n  Misses       1621     1621         \n  Partials      197      197\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7d1918f...0f96346. Read the comment docs.\n. # Codecov Report\nMerging #893 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #893   +/-\n=======================================\n  Coverage   53.69%   53.69%         \n=======================================\n  Files          39       39         \n  Lines        3926     3926         \n=======================================\n  Hits         2108     2108         \n  Misses       1621     1621         \n  Partials      197      197\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 15c52a7...a552b5b. Read the comment docs.\n. # Codecov Report\nMerging #896 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #896   +/-\n=======================================\n  Coverage   53.69%   53.69%         \n=======================================\n  Files          39       39         \n  Lines        3926     3926         \n=======================================\n  Hits         2108     2108         \n  Misses       1621     1621         \n  Partials      197      197\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-grpc-gateway/gengateway/template.go | 65.51% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 87b57f5...5bf89d8. Read the comment docs.\n. # Codecov Report\nMerging #898 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #898      +/-\n=========================================\n+ Coverage   53.69%   53.7%   +0.01%   \n=========================================\n  Files          39      39            \n  Lines        3926    3927       +1   \n=========================================\n+ Hits         2108    2109       +1   \n  Misses       1621    1621            \n  Partials      197     197\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/template.go | 56.33% <100%> (+0.04%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update be12715...e89df15. Read the comment docs.\n. # Codecov Report\nMerging #899 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster    #899   +/-\n======================================\n  Coverage    53.7%   53.7%         \n======================================\n  Files          39      39         \n  Lines        3927    3927         \n======================================\n  Hits         2109    2109         \n  Misses       1621    1621         \n  Partials      197     197\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c0317cd...2f46cd1. Read the comment docs.\n. # Codecov Report\nMerging #902 into master will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #902      +/-\n==========================================\n+ Coverage   53.59%   53.61%   +0.02%   \n==========================================\n  Files          39       39            \n  Lines        3935     3937       +2   \n==========================================\n+ Hits         2109     2111       +2   \n  Misses       1629     1629            \n  Partials      197      197\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/types.go | 19.04% <\u00f8> (\u00f8) | :arrow_up: |\n| protoc-gen-swagger/genswagger/template.go | 56.42% <100%> (+0.08%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9bb0d96...30a13e5. Read the comment docs.\n. # Codecov Report\nMerging #903 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #903   +/-\n=======================================\n  Coverage   53.61%   53.61%         \n=======================================\n  Files          39       39         \n  Lines        3937     3937         \n=======================================\n  Hits         2111     2111         \n  Misses       1629     1629         \n  Partials      197      197\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2b6cab6...0d0e5b2. Read the comment docs.\n. # Codecov Report\nMerging #904 into master will decrease coverage by 0.07%.\nThe diff coverage is 41.37%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #904      +/-\n==========================================\n- Coverage   53.61%   53.54%   -0.08%   \n==========================================\n  Files          39       40       +1   \n  Lines        3937     3961      +24   \n==========================================\n+ Hits         2111     2121      +10   \n- Misses       1629     1642      +13   \n- Partials      197      198       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| runtime/handler.go | 40.62% <0%> (-0.98%) | :arrow_down: |\n| runtime/proto_errors.go | 0% <0%> (\u00f8) | :arrow_up: |\n| runtime/errors.go | 44.73% <50%> (-1.1%) | :arrow_down: |\n| runtime/marshal_httpbodyproto.go | 69.23% <69.23%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6523154...44b123a. Read the comment docs.\n. # Codecov Report\nMerging #907 into master will decrease coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #907      +/-\n==========================================\n- Coverage   53.54%   53.52%   -0.03%   \n==========================================\n  Files          40       40            \n  Lines        3961     3959       -2   \n==========================================\n- Hits         2121     2119       -2   \n  Misses       1642     1642            \n  Partials      198      198\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/types.go | 19.04% <\u00f8> (\u00f8) | :arrow_up: |\n| protoc-gen-swagger/genswagger/template.go | 56.33% <100%> (-0.09%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7aca14d...d294104. Read the comment docs.\n. # Codecov Report\nMerging #908 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #908   +/-\n=======================================\n  Coverage   53.52%   53.52%         \n=======================================\n  Files          40       40         \n  Lines        3959     3959         \n=======================================\n  Hits         2119     2119         \n  Misses       1642     1642         \n  Partials      198      198\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8994a11...9475728. Read the comment docs.\n. # Codecov Report\nMerging #909 into master will decrease coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #909      +/-\n=========================================\n- Coverage   53.52%   53.5%   -0.03%   \n=========================================\n  Files          40      40            \n  Lines        3959    3957       -2   \n=========================================\n- Hits         2119    2117       -2   \n  Misses       1642    1642            \n  Partials      198     198\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| protoc-gen-swagger/genswagger/types.go | 0% <\u00f8> (-19.05%) | :arrow_down: |\n| protoc-gen-swagger/genswagger/template.go | 56.42% <100%> (+0.08%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e81e272...cc0d6af. Read the comment docs.\n. \n",
    "jporwal05": "Apologies for the delayed reply. The issue was with the way that I setup Go. If Go is setup correctly then one would not run into this issue.\nNote to visitors: If you are visiting this for a similar issue, I would suggest you double check your Go setup and make sure you have set it up as per the official docs.. ",
    "devnull-": "I signed it!. @googlebot : Add alternate email to validate the CLA.. ",
    "llimllib": "The problem is that if I send an invalid request to the service in question (in my case, an integer for an enum that is invalid), the response is of a type that I am not able to marshal into a struct without copying errorBody into my own code.\nIt would be better if errorBody were public so I could write an integration test that the return value of my endpoint were a gateway.ErrorBody if there was an error; in fact I wish the swagger output for my API would document that as well.. @achew22 sure, here's what I mean:\n\nEndpoint A accepts an enum attribute, let's call it foo, whose valid values are 0, or 1, let's call their names ALPHA and BETA\nGRPC Gateway says that foo is a string, but it also accepts integers\n(is this a bug?)\nIf I pass GAMMA to the endpoint for attribute foo, I get an error from grpc as I expect\nIf I pass 30 to the endpoint, I do not get an error and the invalid value gets passed to the handling function\nNow I need to test that invalid values that reach my function (which seem to me like they should be a type error in the first place) return an error that I expect them to return\nThis is why I need to use errorBody\n\nI will try to make a reproducing example, but I may not have the time. I apologize for that. OK, you're right and I traced the enum issue (which is annoying) to jsonpb, it's basically https://github.com/golang/protobuf/issues/174\n(Oddly, grpc-gateway handles this case differently than jsonpb, and it is the way I expect. I wrote up the test case I gave above, and here's the output from grpc-gateway:\n$ curl 'http://localhost:1235/hello?enumValue=1'; echo\n{\"response\":\"BETA\"}\n$ curl 'http://localhost:1235/hello?enumValue=2'; echo\n{\"error\":\"2 is not a valid main.Test\",\"message\":\"2 is not a valid main.Test\",\"code\":3}\nI don't know why jsonpb can't handle this the same way!)\nAnd the reason I'd like errorBody to be public is to write an integration test for that case; if the enum value is passed in as 2, but jsonpb doesn't produce an error, I want to verify that my endpoint returns an error in the standard grpc-gateway format by deserializing it into an errorBody; because it's not public I have to copy the struct into my integration test code.\nNot the biggest problem in the world, but an annoyance.. phew, I would like to but I would rate myself as extremely intimidated by reading that PR. ",
    "adamstruck": "Thanks for the feedback! I think my latest commit addresses your comments.. This is to handle the case where an intermediate field in fieldPath refers to a map.  \ni.e.  obj.nested.map_field.foo where map_field is a map.  . sure no problem. . ",
    "rwlincoln": "This PR makes import_path work in the same way that it does in golang/protobuf.\n\nCould you add a test by modifying the makefile to invoke protoc with this parameter set to a value you know to work and output to a different directory so we could have a more e2e test?\n\nUnfortunately, all of the example .proto files include the go_package option. import_path is only used \"as the package if no input files declare go_package\". I will try to add some tests to descriptor/registry_test.go.. I have added tests for the SetImportPath method on the registry type. They test:\n\nthat importPath is only used as the package if no input files declare go_package and\nif importPath contains slashes, everything up to the rightmost slash is ignored.\n\nThis is in accordance with the documentation:\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/0a810034e45df6b5a119b0cd3a56150c459e3f9a/protoc-gen-grpc-gateway/descriptor/registry.go#L218. ",
    "amaskalenka": "Hi @tmc \nThanks for response. I will try to explain purpose of this PR below: \nFor example you have next response definition in your PB IDL.\nproto\nmessage User {\n    string first_name = 1;\n    string last_name = 2;\n    int64 age = 3;\n}\nmessage Response {\n    repeated User result = 1;\n}\nand you want to add some kind of \"metadata\" to the body of each JSON response, e.g:\njson\n{\n  \"success\": {\n    \"status\": 200, \n    \"message\": \"Found 2 items\", \n    \"code\": \"OK\"\n  },\n  \"result\": [\n    {\n      \"first_name\": \"John\",\n      \"last_name\": \"Doe\",\n      \"age\": 50\n    }\n  ]\n}\nYou could either add this \"metadata\" to the each PB response or write your own ResponseForwarder.\nIf you need to override default response forwarder you could either write override files for all your applications or add an option the allow that overriding during generation process.. ",
    "Kuqd": "@alexleigh can you do the requested change ? or do you want me to submit another PR ? I'm also in need of this.\njust change this  https://github.com/grpc-ecosystem/grpc-gateway/blob/acebe0f9ff5993e130b141ee60e83e592839ca22/runtime/marshaler_registry.go#L16\nto false.. Is it related ? Not sure I follow, the point is to not output snake case as it's the standard for protobuf but camel case which is the json standard.. @alexleigh you got it. Thanks a lot. LGTM. Do you guys have an ETA ?. I signed it!. Ok let me close this for now. ",
    "tony-spark": "Had the same issue with Go 1.6.\nSwitching to Go 1.10 seems to work. ",
    "itizir": "Hi! Oh yeah, was wondering about this the other day too... Nice one with Any, but it's still not fixing it because it should be pointers, not values! :)\nAlso the protobuf tag would need to contain rep, since it's a slice, for it to work properly.\nBut actually, if errorBody needs to be a proto.Message, why don't you define it through the protobuf generator?\n```\nsyntax = \"proto3\";\nimport \"google/protobuf/any.proto\";\nmessage ErrorBody {\n    string error = 1;\n    int32 code = 2;\n    repeated google.protobuf.Any details = 3;\n}\n```\nAnd then by the way it would indeed be nice to unify with the stream errors. :)\nhttps://github.com/grpc-ecosystem/grpc-gateway/issues/538. ",
    "Ianwww": "thanks for the response.  I had to move forward and use a wrapper around a map to handle this.  I'm not going to be able to work on this with my present workload.. ",
    "maciejj04": "I got a request which contains - among other fields - boolean flag. If this flag is passed with 'true' value, it forces grpc service to return some additional data. This 'data' is in form of Any protobuf type. So when I send a request with this flag appended and as a result additional data is present in mentioned field (of protobufs type 'Any') of service response, rest-grpc-proxy app returns error mentioned in my previous post. Ofc.,  in case when request does not contain flag, response is returned properly.. Also I've noticed that ByteString returned in response from service is not returned as String from grpc-proxy app. Is that proper behaviour?\nExample use case mentioned in above posts:\n.proto definitions:\n```\nmessage ListRQ {\n    ObjAtId ObjAId= 1;\n    bool addContent = 2;\n}\nmessage DomainItem {\n  int64 ObjectId = 1;\n  Pic Pic = 2;\n  int64 timestamp = 3;\n  google.protobuf.Any Content = 4;\n  bool inWork = 5;\n}\n```\nSo, as I've pointed before: when addContent flag in ListRQ is true, grpc service returns DomainItem with 'Content' field included (otherwise Content is not present is response). \nSample json Request to sidecar-app:\n{\n  \"ObjAId\": {\n    \"Org\": \"TT\",\n    \"Grp\": \"B4T0\",\n    \"Code\": \"A\"\n  },\n\"addContent\": true\n}\nThen response from grpc-proxy: { \"code\": 2 } or \"unknown message type \"'\"\". I've debugged service and it returns proper proto response.\nWhile request does not contain \"addContent\":\n{\n  \"ObjAId\": {\n    \"Org\": \"TT\",\n    \"Grp\": \"B4T0\",\n    \"Code\": \"A\"\n  }\n}\nThen response is propper:\n{\n  \"ObjAId\": {\n    \"Org\": \"TT\",\n    \"Grp\": \"B4T0\",\n    \"Code\": \"A\"\n  }\n}\nResponse looks properly:\n{\n    \"ObjAId\": {\n        \"Org\": \"TT\",\n        \"Grp\": \"B4T0\",\n        \"Code\": \"A\"\n    },\n    \"DomainItem\": [\n        {\n            \"ObjectId\": {\n                \"Type\": \"Test\",\n                \"Id\": \"7b52e04e-f04a-44e8-8b3d-854f9e4bd2f8\"\n            },\n            \"Pic\": {},\n            \"timestamp\": \"1519873738747\",\n         },\n        {\n            \"ObjectId\": {\n                \"Type\": \"TEST\",\n                \"Id\": \"fa257c34-7ac2-4aa1-ada0-8e014f5d4101\"\n            },\n            \"Pic\": {},\n            \"timestamp\": \"1519875743822\",\n        }\n    ]\n}. ",
    "hnlq715": "https://github.com/hnlq715/grpc-gateway/commit/488d9e367e40e0de392202dfe818be792e6e25ff\nSimple demo here.... @achew22 Thanks for reply, is there any example implementing the client interface?. So here is the question, how to create my own server/client? And frankly speaking, this is awful.... Alright, maybe you misunderstand what I mean...\nI just want to write the grpc services, and register the server implementations to support http api through Register...HandlerServer\nIn this way, we do not need to write the http api code which is redundant.. So to support grpc service, we have to implement the EchoServiceServer interface.\nAnd to support http service, we just need to pass the implemented EchoServiceServer instance into func RegisterEchoServiceHandlerServer(ctx context.Context, mux *runtime.ServeMux, srv EchoServiceServer) error {, instead of implementing another EchoServiceClient interface, which is more elegant and less redundant.. ",
    "hellupline": "@achew22 while your solution does work, its not clear how to handle things like the calloption \n```go\ntype internalgrpcclient struct { s MyServiceServer }\nfunc (c internalgrpcclient) Echo (ctx context.Context, in EchoRequest, opts ...grpc.CallOption) (EchoResponse, error) {\n        return c.s.Echo(ctx, in)\n}\n```\nand then using the ...RegisterClient, my grpc service does not receive the http readers,\n. ",
    "tep": "Package runtime could be renamed grpcgw and, in my opinion, the utilities package could be folded into the former (it containing only a few constants and the DoubleArray type).\nThe existing packages would, of course, remain in place (for a time) but be redefined to reference elements of package grpcgw -- either by alias or direct assignment -- along w/ a deprecation notice. They would eventually be removed when/if the major version number changes.\nIf ever there comes a time for sub-packages under grpcgw these could be prefaced w/ gw -- e.g. grpcgw/gwfoo (similar to io and io/ioutil). internal packages need not follow such a convention.. tl/dr; Like so*\nAFAIK, packages cannot be aliased, only types.  Package level vars, funcs and constants are merely referenced via assignment.\n*Note: in the commit referenced above I have a) renamed runtime to grpcgw, b) merged utilities into grpcgw and c) updated the original utilities to reference elements in grpcgw.  I've done nothing, so far, WRT package runtime; will get to that RSN.. ",
    "novabyte": "@zwtj I've seen this behaviour with POST method which has query params. I think it is a bug in the code generator for the JSON Swagger spec which is output.. @johanbrandhorst I don't think it's a regression. I'm not sure it ever worked. I'd have to gitbisect to understand better though.\n. @Wotzhs Would love to see this land. Let me know how I can help \ud83d\udc4d . @tmc can you show an example or link to somewhere in the code on how to embed the gateway?. ",
    "IronPan": "I've also seen the issue with POST method. The query parameters are not generated in the swagger definition. For example, if I generate swagger definition for this example, I got the following (In theory I should see a and b).\n\"/rpc/body/query/rpc\": {\n  \"post\": {\n    \"operationId\": \"RpcBodyRpc5\",\n    \"responses\": {\n      \"200\": {\n        \"description\": \"\",\n        \"schema\": {\n          \"$ref\": \"#/definitions/apiEmptyProto\"\n        }\n      },\n      \"default\": {\n        \"description\": \"\",\n        \"schema\": {\n          \"$ref\": \"#/definitions/apiStatus\"\n        }\n      }\n    },\n    \"parameters\": [\n      {\n        \"name\": \"body\",\n        \"in\": \"body\",\n        \"required\": true,\n        \"schema\": {\n          \"type\": \"string\"\n        }\n      }\n    ],\n  }\n},.\n",
    "Wotzhs": "i've run into similar problem when transcoding a POST method where the query params are not parsed, should i create a separate issue?\nalso, i would like to help, but need some pointer as to where might the problem be, i have simply tried to apply the same trick used for DELETE method for POST, but it didn't work. @johanbrandhorst just checking with you if this is going into the right direction, i have tried to update the function in grpc-gateway/protoc-gen-swagger/genswagger/template.go so that the non path and body parameters will be added to query string for http POST method. i.e.:\n```proto\nmessage BodyWithQueryString {\n    string someid = 1;\n    string somequery = 2;\n    string somevalue = 3;\n    Body data = 4;\n}\nmessage Body {\n    string body = 1 ;\n}\nservice SomeService {\n    rpc SomeRPC(BodyWithQueryString) returns (google.protobuf.Empty) {\n        post: \"/somepath/someprc/{somevalue}\",\n        body: \"data\"\n    };\n}\nand the generated `swagger.json` would have:\n{\n    ...\n    parameters: [\n        {\"name\": \"value\", \"in\": \"path\", \"required\": true, \"type\": string},\n        {\"name\": \"body\", \"in\": \"body\", \"required\": true, \"schema\": \"#/definitions/exampleBody\"},\n        {\"name\": \"someid\", \"in\": \"query\", \"required\": false, \"type\": string},\n        {\"name\": \"somequery\", \"in\": \"query\", \"required\": false, \"type\": string},\n        {\"name\": \"somevalue\", \"in\": \"query\", \"required\": false, \"type\": string},\n    ]\n}\n```\nthis change has however caused a lot of side effects, and failing multiple tests, while i can update the test cases so that they would pass, but i wonder if this is something the package authors would have wanted as well? -> https://github.com/grpc-ecosystem/grpc-gateway/issues/234\ndisclaimer: this is my very first endeavor to contributing to an opensource project, feedback are very much appreciated. @novabyte thank you for your offer to help \ud83d\udc4d, this is taking a lot longer than i thought as i am quite stuck on the complexity of the non-primitive types such as the oneof being included as part of the query string. \nI have not yet taken the time to observe and understand how the above works currently in either the GET or DELETE request,  but i did come across https://github.com/grpc-ecosystem/grpc-gateway/pull/321 on how it blocks the oneof field from being provided with multiple values.\nI have checked the google.api.http doc, there was no mention of whether the oneof field should be allowed or how it should be transcoded.\nI suspect that i will have to look into the code generator to prevent all the fields of the oneof from being included into the query string, or the oneof field shouldn't be allowed in the query string at all.\n. ",
    "rvegas": "I signed it!. Any love from maintainers?. Hey @achew22 thanks for replying and reviewing.\nThe main objective here is to allow for the REST API definition to be able to handle possible error definitions without having to load the whole googleapis proto package entirely, just as it's already done by including the http and the annotations protos.\nFor example, it would allow you to have this in your protos:\n```go\n// This resource represents a long-running operation that is the result of a\n// network API call.\nmessage Operation {\n  string name = 1;\n  google.protobuf.Any metadata = 2;\n  bool done = 3;\n// The operation result, which can be either an error or a valid response.\n  // If done == false, neither error nor response is set.\n  // If done == true, exactly one of error or response is set.\n  oneof result {\n    // The error result of the operation in case of failure or cancellation.\n    google.rpc.Status error = 4;\n// The normal response of the operation in case of success.  \ngoogle.protobuf.Any response = 5;\n\n}\n}\n```\nWhat do you think? This allows for far better expressed APIs and its common practice in many standards such as JSONAPI to define errors fully.. ",
    "abursavich": "I would much prefer the handler's signature to be more in the style of (Default)HTTPError, so that a client can write the stream error however they wish. In my case, I would like to construct my own error status proto directly and avoid my marshalers having to handle maps. . Here's my first pass: https://github.com/abursavich/grpc-gateway/commit/900ec66e401eb8dd606ddf41cd1a492d260354a8. FWIW, the error on nil response was already there, I just lifted it out of streamChunk (which I deleted): https://github.com/grpc-ecosystem/grpc-gateway/blob/cde2f8f5bef06abdf9d0557f284f072f1cfa9f56/runtime/handler.go#L185-L187\nI don't like the empty interface much either, but I was matching the existing interface for Marshaler: https://github.com/grpc-ecosystem/grpc-gateway/blob/e4b8a938efae14de11fd97311e873e989896348c/runtime/marshaler.go#L7-L20. ",
    "poy": "We resolved our requirement via /v1/read/{source_id=**}. I signed it!. ",
    "scudette": "I am running into this issue but the suggested solution does not work for me. Is there a workaround?. Ok after looking into this more I think it is WAI although I am not sure that I understand what its supposed to do. It seems that if a parameter has a \":\" in it (in the escaped URL), then the parameter is not passed directly to the grpc call but there is some kind of processing to extract the verb. So I am trying to map something like:\n/v1/method/{param}\nand the URL is /v1/method/F%3A1234\nwhich does not work. A suitable workaround seems to be to add an extra : to the end of the URL:\n/v1/method/F%3A1234%3A \nthis seems like a hack to me though since the parameter is properly URL encoded. If the proxy can not handle proper URL path mappings (with the proper escaping) then am I just safer to pass parameters in query strings?. ",
    "vgheri": "I am also running into this issue and it's pretty troublesome.\nWould it be possible to re-open the issue and discuss if it can be fixed instead of using workarounds (that are impractical) ?. ",
    "thepeterstone": "I signed it too!. ",
    "duck8823": "@achew22 \nGreat thanks for your review! \nPlease let me confirm if I got it.\nI think to create new function that returns a ServeMuxOption.\ne.x runtime.WithMarshalerRegexOption \n\nIs my understanding correct ? \n\nWhich one is better ?\n  a. The new function's argument is Regex. \n     How to use this... \n     headerRegex := regexp.Compile(\"application\\/.*\")\n     mux := runtime.NewServeMux(\n       runtime.WithMarshalerRegexOption(headerRegex, &CustomMarshaler{})\n     )\n   b. or arguement is string with only partical wildcard.\n      mux := runtime.NewServeMux(\n        runtime.WithMarshalerWildcardOption(\"application/*\", &CustomMarshaler{})\n      )\nI like b than a, because simply usage.\n\n\nThanks.\n. @achew22 \nCan you see this?. Sorry, but I'm busy with another work...\nCan I close this PR myself?. To distinguish. When there is no label and initialize var marshalers [4]dummyMarshaler ,  marshalers[0] == marshalers[1] returns true.. ",
    "jhump": "Looks like this has already been fixed for path params but is still broken for the body. The current code produces Go code that can't compile if a oneof is specified as the body. I think I see how to address that.. @achew22, PTAL. The build breakage was bad timing - I pushed just after protobuf merged their dev branch to master. I will rebase this PR after #636 lands, to see if I can get the tests to go green.. @achew22 or @yugui: ping?. @johanbrandhorst, sorry, I've been busy, and we've since re-formulated our proto to not actually need this feature. So, if this seems like a valuable contribution, someone else will have to take it over.. @achew22, are you the right person to ping if I'm hoping to get a change reviewed?. Ping @achew22?. > This would be a breaking change for every single user of the project\nHow so? It behaves exactly as it does today unless requested via plugin arg.\n\nI think that making this a configurable flag in the generator\n\nThat's what I've done.\n\nSorry to not give a happy, \"let's merge this\", but I have concerns WRT major breaking changes like this.\n\nI would not expect you to accept a breaking change. Please take a longer look at the PR description and code.. PIng @achew22. If you don't mind reviewing. This is not a breaking change. Please take a closer look at the PR description and the code.. @achew22, I rebased this. I'm trying to figure out how to actually do what you requested, regarding a test that the option is working. Am I correct to be staring at Bazel build files? I don't see any existing examples/tests that exercise the other options. Maybe I am looking in the wrong place. A little more direction appreciated.. The build breakage was bad timing - I pushed just after protobuf merged their dev branch to master. I will rebase this PR after #636 lands, to see if I can get the tests to go green.. ping @achew22 . Ping @achew22. This is causing failing tests for some of our stuff because every bidi stream invocation tickles the race detector. We've temporarily forked grpc-gateway. This seems like a non-controversial fix, so we were hoping to not have to maintain a fork for very long.. @achew22: I didn't even realize that there was an examples folder.\nI ran make generate, but it produced a ton of differences. I am guessing I have a different version of swagger-codegen than was used to produce the existing files. (I actually just installed it on OS X: brew install swagger-codegen. So maybe the existing swagger-generated files need to be freshened?)\nI only git add'ed the ones relevant to my change.. ping: @achew22, I think this branch is ready to go.. > Could you rebase this change?\n@achew, done. @achew22 or @yugui, if you have a sec, maybe you could review this small PR.. @achew22, @yugui: we're in the process of converting some REST APIs to use protos, not all of which actually go through grpc-gateway generated handlers, but manual handlers that already return non-proto objects to serialize to JSON. So we're starting to use grpc-gateway's marshaler to aid in the transition, since it can marshal both protos and non-protos.\nBut we have some code that uses nil as a sentinel response, expecting it to be serialized as \"null\". That works with encoding/json, but caused grpc-gateway's JSONPb marshaler to panic. This fixes the panic.. I've thought about this quite a bit, actually. At a former job, my team even wrote something that did just this (though it was bespoke protobuf-based RPC, not gRPC).\nThe complication we faced was how often to reload the proto schema. The proxy we built supported both protoset files (e.g. the proxy binary was only compiled once, but the service descriptors still had to be compiled and provided to the proxy whenever proto sources changed) and service reflection (this was pre-gRPC, so we had our own custom service that allowed for clients to download the server's descriptors). The complication was only an issue with reflection. We implemented it so that it basically re-downloaded the schema (and reconstructed the reflective proxy) every time it detected a new socket connection. But this occasionally caused issues because it was too often, particularly when servers had silly large descriptors. We did this to make sure that the reflective proxy was guaranteed to converge to a new schema after servers were rolling-restarted with a new version. (A better approach may instead be to just periodically poll, so that a storm of re-connects doesn't result in lots of wasted processing). Another issue we observed was startup lag -- if a given proxy was configured to talk to a lot of different services, there was noticeable latency during startup since it would not become healthy/available until after it had downloaded and processed all of the descriptors. Just some things to consider.. This is what generated bad code. The AssignableExpr would have emitted the prep statements, which would get incorrectly inlined in the call to Decode. By splitting out the prep statements, it now generates working code.. I essentially forked AssignableExpr, with this method preserving just the preparations and the other only emitting the last line, the actual field ref.. I removed these lines because they are not true anymore. There are even tests that verify that unmarshaling works for non-proto types.. ",
    "apeletz512": "@jhump @achew22  @yugui @tmc Any progress here?. ",
    "seamys": "sorry, I found the answer in the example folder.\nhttps://github.com/grpc-ecosystem/grpc-gateway/blob/58f78b988bc393694cef62b92c5cde77e4742ff5/examples/main.go#L16-L22. ",
    "mossila": "@yugui Ok, It's work. \nI have install protoc-3.5.1 at C:\\dev\\bin\\ and I try to create service.pb.go with this command\nprotoc -IC:\\dev\\bin\\protoc-3.5.1\\include -I. -I%GOPATH%\\src -I%GOPATH%\\src\\github.com\\grpc-ecosystem\\grpc-gateway\\third_party\\googleapis --go_out=plugins=grpc:. .\\my-proto\\my-service.proto\nNext I try to Generate reverse-proxy \nprotoc -IC:\\dev\\bin\\protoc-3.5.1\\include -I. -I%GOPATH%\\src -I%GOPATH%\\src\\github.com\\grpc-ecosystem\\grpc-gateway\\third_party\\googleapis --grpc-gateway_out=logtostderr=true:. \\my-proto\\my-service.proto\nAll things work! \nThanks. . ",
    "cy-zheng": "Same question. Is there any way remove this wrapper?. Well, It's simple. The actual return value of server side stream is like:\n{\"result\":{ ...myobject... }}\n{\"result\":{ ...myobject... }}\n{\"result\":{ ...myobject... }}\nBut I need:\n{ ...myobject... }\n{ ...myobject... }\n{ ...myobject... }. https://github.com/philips/grpc-gateway-example/issues/22. It's an important feature. I can't find a third-party tool to solve this unless parsing swagger.json files by myself.. Well, I'll take a look at it this weekend. It seems make it work by just adding some test cases.. https://github.com/grpc-ecosystem/grpc-gateway/pull/643. Combine separate swagger file to one in https://github.com/grpc-ecosystem/grpc-gateway/pull/643\nthe cmd like this:\nprotoc src/pb/proto/models.proto src/pb/proto/models2.proto --swagger_out=allow_merge=true,merge_file_name=myswagger,logtostderr=true:src/static/. CLA fixed. ",
    "giiita": "Are you waiting for a response?\nFor example.\n```\nmessage Term {\n    int64 startAt = 1;\n    int64 endAt = 2;\n}\nmessage Response {\n  repeated Term terms = 1;\n}\nmessage Request {\n  string token = 1;\n}\nservice Setting {\n    rpc run(Request) returns (stream Response) {\n        option (google.api.http) = {\n            get: \"/request\"\n        };\n    };\n}\n```\nAt that time,\n\"responses\": {\n          \"200\": {\n            \"description\": \"\",\n            \"schema\": {\n              \"$ref\": \"#/definitions/apiResponse\"\n            }\n          }\n        }\n  ...\n  \"apiResponse\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"terms\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"$ref\": \"#/definitions/term\"\n          }\n        }\n      }\n    },\nHowever, the response between the server and the client using the generated gateway is as follows.\n{result: {terms: []}}\nThis \"result\" is attached only to grpc-gateway, it is unnecessary.. ",
    "birdayz": "Yeah i'm confused, what's the reason for this? \nIt would be a breaking change, so i'm not going to get my hopes up before a v2..\nhowever it's highly inconsistent with the swagger output, the results field is missing there.\n@johanbrandhorst any guidance how to tackle this?\nso what could we do? i see, without thinking over it too long, these possibilities:\n1) Remove the \"result\" field wrapper, to be 1) consistent and 2) it's unexpected, so i don't think it belongs here unless there's some technical limitation..\nThat's a breaking change though.\n2) Fix the swagger-ui generation and include the \"results\" object. I'm not sure how easy this is, as we have to detect if it's a streaming response. but should be possible without too much effort\n3) Add support for SSE/Websocket, enabled by maybe a specific option. Any1 who's not  satisfied with the current situation could switch over to that.\nI guess 1) would be best, as the \"results\" field just looks awkward and should not be there. But realistically, i guess we won't introduce such a breaking change.\n3) Would be nice, as it would be more \"standard\". However it's more work and i'm not sure if anyone would fine the time to work on it (i won't, at least for the next weeks)\nSo there's 2), i dislike it but would at least make it consistent..\nthoughts?\nedit: also: any plans for a targeted v2 release, where such things could be implemented/fixed? maybe a v2 / \"next\" branch where early adopters could get these changes asap?. Yeah sure, will do that in the next days. Would be great if you could give some input regarding examples for fields. I'd really like to have that, but it would require changes in the .proto files.. and it may not look very nice to have the \"example\" option in JSONSchema (See https://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-swagger/options/openapiv2.proto) as well. I guess there was a good reason to use JSONSchema instead of Schema for fields.. Hah yeah it's clear how to add usage examples. Confusing with the \"examples\" word all over the place.\nI was just curious about the \"example option field\", and if we maybe can work on a possibility to expand this to proto fields as well (and not just for messages, which this patch adds)\nEdit: Code sample for clarity:\nstring id = 1 [(grpc.gateway.protoc_gen_swagger.options.openapiv2_field) = {example: \"DEFAULT\"}];\n(which is not syntactically correct, because it's actually the sub-field \"value\" inside of example, but whatever)\nanyway, the above is not possible, because openapiv2_field = JSONSchema and not Schema, which does not contain the example option field.. One thing i noticed:\nthe message names in template.go/renderMessagesAsDefinition() are highly inconsistent. I printed some out, and it looks like this:\n.com.eon.abc\n.com.eon.def\nprotobufBoolValue\nPet\nprotobufEmpty\nIt looks strange. I digged a bit and only the enums are prefixed with the dot (findNestedMessagesAndEnumerations(msg, reg, m, e))\nThe others look like those above. And well, this switch ist mostly ineffective i think:\n```\n    for name, msg := range messages {\n    switch name {\n    case \".google.protobuf.Timestamp\":\n        continue\n    case \".google.protobuf.Duration\":\n        continue\n    case \".google.protobuf.StringValue\":\n\n```\nI just tested, with my addition it didn't work, since it's called protobufEmpty and protobufBoolValue instead of .google.protobuf.Empty.\nLooks like this has once worked, but something went wrong.. Idea how to proceed?. Will do. These things are screwing up my swagger-ui, so i'm very interested in cleaning up where needed :). Okay it's quite difficult to get a test up and running. Aren't there tests where i can just provide a .proto file as input and a .json as output? That would make it much easier to get started. To be honest i spent more time to get a test working than i feel should be necessary..\nI added an example with BoolValue to the examples, and this occurs in the output:\n\"protobufBoolValue\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"value\": {\n          \"type\": \"boolean\",\n          \"format\": \"boolean\",\n          \"description\": \"The bool value.\"\n        }\n      },\n      \"description\": \"Wrapper message for `bool`.\\n\\nThe JSON representation for `BoolValue` is JSON `true` and `false`.\"\n    },\nShouldn't be there, we don't want those wrappers in the list of models. So at some point i guess .google.protobuf.BoolValue became protobufBoolValue and the switch became useless.\nI'm not going to commit this example because i got a lot of strange changes after re-generating with the docker image of the contributor guide.. Just added one Service, but now i have a totally unrelated diff:\ndeleted:    examples/clients/abe/camel_case_service_name_api.go\nI only added:\nservice BoolService {\n        rpc Empty(google.protobuf.BoolValue) returns (google.protobuf.BoolValue) {\n                option (google.api.http) = {\n                        get: \"/v2/example/empty\",\n                };\n        }\n}\nI'm a little stuck with the tooling .. or could use some help to set up a test. It's just non-trivial to get all the inputs set up, especially with multiple proto files. Is there some helper method to just load a proto file in a test, and get the descriptors etc? would really help me.\n. Yes this is what i would have expected, but such tests are not in place it seems.\nFun fact i recently fixed a bug on godoctor and they use exactly this kind of tests: https://github.com/godoctor/godoctor/tree/master/refactoring/testdata/rename/072-funcdecl-rename\nThey even run it from go for coverage etc : )\nBut i'm not sure if i have the time to add this to the project.\nHow could we proceed, or how could a lightweight test for this look like?\nedit: to some extent the examples do this, because you fail the CI if there's a diff detected.. ok we're getting closer, at least the examples now reproduce it. see the latest diff 2cf706514f7abb6e3b85c82f667d86a076519176 .\napparently this issue only occurs if these datatypes are used directly as method in or output. \nthere must be some piece of code where these types are added 'differently (different names)' if they are direct method in/out. i'll have another shot after work today.\nedit2: \nOkay. On template.go:188\nfunc findNestedMessagesAndEnumerations(message *descriptor.Message, reg *descriptor.Registry, m messageMap, e enumMap)\nNested uses the format with the full qualified name (e.g. .google.protobuf.Empty). So for nested values, the names are (at least internally) used like this.\nThe other way at func findServicesMessagesAndEnumerations does differently, it pre-processes the string with fullyQualifiedNameToSwaggerName. which \"camel cases\" it.\nSo i wonder if we can homogenize this behavior, because the same thing is written into the same map differently, depending on where it was found. i hope i described it good enough.\nso choices:\na) keep it as it is and add the same checks for well-known types into the switch at renderMessagesAsDefinition twice, once for the \"fully qualified format\" and once for the camelCase format\nb) use only camel case\nc) use only fully qualified\nI guess this has significant implications on the generated json, so i'll have to try it out. Maybe someone with more knowledge of the codebase can elaborate :). Implemented c). At least from the examples, the result looks good. In the end, all types are converted to the camel case version anyway, so it's a good idea to not do it before as well. This way, the filtering of well-known types works as expected. . I'm confused, what do you mean?\nThe purpose of this PR is that these very basic things - boolValue and empty  etc- are omitted from the definitions section of the swagger JSON. So they should not be in the .json file. that's why 2cf706514f7abb6e3b85c82f667d86a076519176 adds them (reproducing the faulty behavior) and after the fix they are gone (82c2cdfc3a9be77a103b8443008f366706dd77b8). Yep it's fully covered now. So there should not be a need for a separate issue, the wrappers are now handled correctly. At least from my PoV, if these two points are valid:\n\nWrappers are intentionally not included in the description section of the swagger config.\nIn addition, this PR introduces the same behavior for google.protobuf.Empty. It's not part of the wrappers proto file of google, but IMHO should still be omitted as it provides no value at all.\n\nThe current code already does the stuff highlighted in bold, but only for in/out messages of RPC methods. Nested messages still produced those entries, even with a different name. This was fixed with 84895aa23063f91c5e462afc03de914a180e591e\nedit: the .swagger file is generated and up-to-date: https://github.com/birdayz/grpc-gateway/blob/dont_render_well-known_empty/examples/proto/examplepb/wrappers.swagger.json. I think we have a misunderstanding here!\nOf course it's in the .pb.go file, because this is just the go part. It should be there.\nHowever for the swagger part, the swagger generation code deliberately chooses to NOT generate those things - for wrappers. So this is expected and intentional that it's NOT in the .json output!\nSee this part of the code in template.go:\nfunc renderMessagesAsDefinition(messages messageMap, d swaggerDefinitionsObject, reg *descriptor.Registry, customRefs refMap) {\n    for name, msg := range messages {\n        switch name {\n        case \".google.protobuf.Timestamp\":\n            continue\n        case \".google.protobuf.Duration\":\n            continue\n        case \".google.protobuf.StringValue\":\n            continue\n        case \".google.protobuf.BytesValue\":\n            continue\n        case \".google.protobuf.Int32Value\":\n            continue\n        case \".google.protobuf.UInt32Value\":\n            continue\n        case \".google.protobuf.Int64Value\":\n            continue\n        case \".google.protobuf.UInt64Value\":\n            continue\n        case \".google.protobuf.FloatValue\":\n            continue\n        case \".google.protobuf.DoubleValue\":\n            continue\n        case \".google.protobuf.BoolValue\":\n            continue\n        case \".google.protobuf.Empty\":\n            continue\n        }\n(continue means, don't put it into the definitions section of the JSON)\nThis was already in place before this PR. It only affects the swagger output, not the go bindings .pb.go output. It was just buggy before my changes, for a specific subset of inputs.\nthe generated stuff is up to date, ci does a check for that and fails in case it's not up to date.. Ah now i got your concerns..\nMost likely you are right. I can add an http annotation later to be sure.. I added one google.api.http directive, does this look better now?\nfrom my side we are good to go. i suppose, for the future, we could integrate the examples as a real test.. i just had a look and i'm not sure if we can just omit those definitions, because clearly the path entries reference them.\n\"schema\": {\n              \"$ref\": \"#/definitions/protobufStringValue\"\n            }\nso i suspect it would not even be  a valid swagger. i don't know, i'll put it back to WIP for the moment until i've figured out if this ok or a better solution.. my assumption is correct, this change is not legit without fixing the underlying behavior.\n\nApparently for in/out of a method, the StringValue (same applies for other wrappers) object is generated , instead of the primitive. (for both the generated code and swagger).\nSimply removing this entry from the definitions because it looks ugly, as this PR tries so far, results in a faulty swagger.json; Reproduce by going to https://petstore.swagger.io and entering https://raw.githubusercontent.com/birdayz/grpc-gateway/dont_render_well-known_empty/examples/proto/examplepb/wrappers.swagger.json . It will show an error \n```\n\nResolver error at paths./v1/test.post.responses.200.schema.$ref\nCould not resolve reference: Could not resolve pointer: /definitions/protobufStringValue does not exist in document\nResolver error at paths./v1/test.post.parameters.0.schema.$ref\nCould not resolve reference: Could not resolve pointer: /definitions/protobufStringValue does not exist in document\n```\nSo, we can't proceed this way..well, i think we could at least add a fix for the \"Empty\" thing, because it does really render nothing.\nBut, in general, i agree with you: Should wrappers be used in the REST interface? clearly, clearly not. It directly shows that this is a grpc-gateway generated API and makes it look WAY less \"clean\". But to address this, which should certainly be possible, we would break backwards compatibility. Newly generated gateways will have a different API if they use wrappers in in/out. Maybe we can discuss this in a new issue if you want. I'm curious how you handle backwards compatibility guarantees, or how we can introduce such changes (e.g. going for a new major release).\nAnyway, i'll clean up this PR and try to reduce it to a minimum for the \"Empty\" problem, and move the rest to a new one. Thanks for all the support.. @johanbrandhorst i've updated the PR accordingly, with changes for both Empty and Wrappers. Those two are very close (and have many common problems, like the handling of most well known types).\nIt's squashed & rebased. I'm ready for the new code review round, if you have time. Thanks!. How can we move this forward? I make heavy use of well known types; and them being present in the definitions section of swagger is quite ugly. ;)\ni'd like also to improve the same situation with google.protobuf.struct; but i won't invest time into it unless i see that this comparable PR goes through :). done. hey @johanbrandhorst :) i'm working currently on related changes as well, and well on my local branch i have exactly these changes (adding the missing WKN-types to this map) as well..\ncan you elaborate why the revert, which problems occurred?\nedit: nvm found #808 / #809. If i load https://raw.githubusercontent.com/OAI/OpenAPI-Specification/master/examples/v2.0/json/api-with-examples.json or https://raw.githubusercontent.com/OAI/OpenAPI-Specification/master/examples/v2.0/json/uber.json there's no errors. Since the document contains the version,i think it supports both. But i might be wrong... @bsakweson https://github.com/go-swagger/go-swagger can merge swagger files. we're using this to create one unified swagger definition.. Thanks a lot for taking over! I didn't find the time to finish it. Also, my code was lacking one thing: setting the example value on field level, as the protobuf option doesn't make it possible (see my old comment in #799). Do you think you could work on that as well? Otherwise, you can only set the example for the WHOLE rpc, which is annoying i think.\ntiny remark: would be great if you would keep my commits in the history as i wrote the initial patch.. no idea why goimports didn't pick it up. fixed that. ",
    "eleniums": "My initial reason for bringing this up was that the instances in grpc-go where ResourceExhausted is currently being used are related to message size, not the resource actually being exhausted. I thought that 413 PAYLOAD TOO LARGE would be a more appropriate mapping, given the current implementation.\nThat being said, in order to be consistent, it does make sense to map the codes based on https://github.com/googleapis/googleapis/blob/master/google/rpc/code.proto. I'll make the necessary adjustments and push a commit soon.. ",
    "colini": "@yugui Thank you for the clarification.. ",
    "c2nes": "Yep, this is a Swagger only bug. A gRPC gateway I've generated seems to handle the situation just fine.. ",
    "antonikonovalov": "Ok, I will try do it. Looks clear. ",
    "f0rmiga": "I signed it, mr. @googlebot :). I'm aware of the CI errors, I will take a closer look ASAP.. Closing since I don't think this is relevant anymore. Let me know if something like that is wanted.. @achew22 You are very welcome. It was a fun Friday evening task. \ud83e\udd13. @achew22 I screwed up rebasing master into my branch :).. @achew22 Yes. I didn't force push. Damn. http://classicprogrammerpaintings.com/post/142586036029/junior-programmer-learns-git-rebase. @achew22. Sure. I have Bazel setup for a couple projects of mine in Travis CI.. @achew22 What do you think of the CI config? We don't need to test using Bazel 3 times. Not sure how to solve that... any ideas?. @yugui We can do that in another PR. What if we merge the way it is right now and I focus on improving the CI build?. I actually need to rebase and add changes merged to master.. @yugui Yes, I think @yugui meant it. I got that! My suggestion is to focus on improving the CI in another PR since things are getting merged to master frequently and those changes should be added to Bazel. What do you think?. @yugui @achew22 CI done.. @achew22 You are very welcome. I look forward to contributing to more stuff! :). @achew22 Honestly, I tried to integrate it. I think we would need to create a rule similar to go_proto_library. The problem of trying to use it for swagger is that this rule expects .go files to come out of the compiler so it can compile and output a single .a file. I can give it a shot later this week.. @achew22 For reference: https://github.com/bazelbuild/rules_go/blob/c29b08c0b81a1bfdc358c6bd3a59c068165bfefa/proto/def.bzl#L105. Nice! Good stuff.. That is neat!. @achew22 I sent you a message about that in the Go Slack channel, but I guess you didn't see that.. @achew22 No worries! No overlapping work here. Nice work @mrmeku!. Good job @mrmeku! Thank you for your patience in the review. I appreciate it.. I'm not sure if we should use the option logtostderr=true. Bazel seems to succeed if the code gen fails.. It works properly. I was having another issue that made me think this was the cause. Not true.. You are excused hehe. Thanks for the Gazelle, it saves tons of time daily for me.\nI also made changes to other package visibilities as I saw fit.. @yugui Any idea on how to have Gazelle to respect that?. [nit] Missing ,.. When I added the Bazel support initially I created the repositories.bzl that creates the @com_github_googleapis_googleapis repository target. The purpose of that repository target is to avoid using the .proto files under third_party. Could you try to use that repository target instead?. It may be tricky to get the include directory for the protos from the repository target. Let me know if you have any questions regarding that.. Instead of taking the .proto file, I think it would be better to take a proto_library so it is consistent on how the go_proto_library works. Take a look at the implementation here.. Not sure why you need to set the visibility to be public here.. You could rename to proto instead of proto_service.. You need to change to proto = \":examplepb_proto\",. If you do, then you will get a bunch of errors, because the output of the proto_library is a list of proto descriptors in .bin files. So you will need to use a Provider to grab the .proto files from the src list in the proto_library.. You will need to remove this to accept a Label instead of files. From that Label you will extract the src files using a Provider.. I still think this should be called proto, not deps. You also don't need it to be a label_list.. ",
    "ubenzer": "I ended up implementing our logic on the client side. In one request I get the URL for the image and in the second request, we get the image itself by using the response of the first request.\nThis was implemented before I learned that such a thing WithOutgoingHeaderMatcher existed. I don't know if that would've been worked for me or not.\nI'll close this issue as this is not required for me anymore.. ",
    "chuangyou": "I want registered the endpoint and it can have multiple connection to service,mean is client pool..  I tried edit the \"*.pb.gw.go-regxxxMethod Grpc.Dial\" , I also tried registered multiple same endpoint,but this is very troublesome,maybe have plugin mechanism?. registered multiple same endpoint was no useful. @rogerhub  I want a endpoint used multiple connections to improve throughput,not multiple endpoints by multiple connections,athough H2 multiplexing,but a connection the throughput is always limited.\n. ",
    "mrmeku": "@f0rmiga I think I'm done! Let me know if you have anything you can see that you'd like cleaned up. Good point!. TY. Donezo. This binary is now an implicit dependency of protoc_gen_swagger since it is listed as the default label for the _protoc_gen_swagger attribute of that rule. Since the protoc_gen_swagger rule is intended to be public, the dep itself needs to be publicly exposed. Donezo. ",
    "bonafideyan": "I signed it!. sorry, too messy, I will re-fork and redo.. I didn't know those files need be regenerated and reason of build failure.\nFixed, thank you. . Done, thanks.. I just have not found a word to name it, it is a RHS to assign.\ncomponents is replaced with prepare variable, which is joined at last like the components. the fqnm is used directly at last , in each iteration, it is accumulated as a partly qualified field name. i.e.\nprotoMsg\nprotoMsg.A\nprotoMsg.A.B.(M_F).F\nprotoMsg.A.B.(M_F).F.C. thanks.. Done.. Done.. Done.. Done.. ",
    "blackdahila": "@yugui done \ud83d\ude42 . I've added simple endpoint exhibiting the new behavior to the examples @achew22 . ",
    "flw-cn": "Sorry, I notice that there is already a PR #540 do the same thing, even more.\nSo, just close this.. ",
    "gigo1980": "Ok it was my fault, \nrpc method signature has to be rpc getItem (GetStreamItemRequest) returns (StreamItem). ",
    "bobbytables": "Just realized what it is, it's a HEAD call actually.. I did curl -I which sets the method type to HEAD.. I think I had a possible misunderstanding here, but why does a HEAD request fail it. Technically HEAD should also return an Allowed header per RFC: https://tools.ietf.org/html/rfc2616#section-10.4.6. Roger that, sorry for the confusion. Closing.. ",
    "wanghong230": "This is the feature I am looking for.\n. I signed it!. It is not breaking the existing logic. People will receive the new swagger spec which has a different opertional id. \"operationId is an optional unique string used to identify an operation. If provided, these IDs must be unique among all operations described in your API.\" So, it shouldn't cause any issues in reality.\nBut existing logic definitly broken, since you can have multiple services in same proto file, but using same set of method names like Get, List, Delete, the swagger spec could be generated but cannot be used without manual fixing duplicated operation ids.. ",
    "nscharfe": "Thanks for the help. Now that I'm looking a bit closer, it may make sense for me to simply attach the body to the context -- headers, metadata etc are already being attached so it may be a natural extension to add body as well for this specific purpose rather than try to shoe horn a new marshaling scheme. That seem reasonable?\nAs far as verifying signatures, checksums etc. in a webhook handler, I believe it is a common practice to ensure the webhook event was sent by the correct party. For example: \nhttps://stripe.com/docs/webhooks/signatures \nIn this case, the official stripe lib handles the signing verification internally but expects to receive the raw post payload as input.. yep, feel free. thank you!. ",
    "hiromis": "I came across this issue while trying to create an endpoint that takes an arbitrary JSON, modifies one field, and forwards if to another RESTful service. \nI considered creating a custom marshaller, but it seems like it's at MIME type level and not for a particular endpoint. All the other endpoints should use the default marshaller, but for one endpoint I would like an access to the body where I do not the structure of in advance. Any advice on how I might able to achieve that? . @rogchap, it worked like a charm. Thank you!!. ",
    "rogchap": "Create a wrapper for your endpoint to set a custom MIME Content-Type then have a custom Marshaler for that MIME.\neg:\n```go\nfunc customMimeWrapper(h http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        if strings.Contains(r.URL.Path, \"webhook\") {\n            r.Header.Set(\"Content-Type\", \"application/raw-webhook\")\n        }\n        h.ServeHTTP(w, r)\n    })\n}\nmux = runtime.NewServeMux(\n    runtime.WithMarshalerOption(\"application/raw-webhook\", & rawJSONPb{jsonpb}),\n    runtime.WithMarshalerOption(runtime.MIMEWildcard, jsonpb),\n    runtime.WithProtoErrorHandler(runtime.DefaultHTTPProtoErrorHandler),\n)\nrestHandler = http.NewServeMux()\nrestHandler.Handle(\"/\", customMimeWrapper(restServer))\n```. For those trying to do the same think this is what my marshaler consists of:\n```go\nvar typeOfBytes = reflect.TypeOf([]byte(nil))\ntype rawJSONPb struct {\n    *gateway.JSONPb\n}\nfunc (*rawJSONPb) NewDecoder(r io.Reader) runtime.Decoder {\n    return runtime.DecoderFunc(func(v interface{}) error {\n        rawData, err := ioutil.ReadAll(r)\n        if err != nil {\n            return err\n        }\n        rv := reflect.ValueOf(v)\n    if rv.Kind() != reflect.Ptr {\n        return fmt.Errorf(\"%T is not a pointer\", v)\n    }\n\n    rv = rv.Elem()\n    if rv.Type() != typeOfBytes {\n        return fmt.Errorf(\"Type must be []byte but got %T\", v)\n    }\n\n    rv.Set(reflect.ValueOf(rawData))\n    return nil\n})\n\n}\n```\nexpects a proto definition that uses bytes as the body:\nproto\nservice WebhookService {\n  rpc HandleWebhook(WebhookRequest) returns (google.protobuf.Empty) {\n    option (google.api.http) = {\n      post: \"/myapi/v1/webhook\"\n      body: \"rawData\"\n    };\n  }\n}\nmessage WebhookRequest {\n  bytes rawData = 1;\n}. ",
    "weisd": "+1. ",
    "lpabon": "This is really important. Anyone working on this? @johanbrandhorst ? . Thanks @johanbrandhorst , but I think #739 is not exactly what I thought the solution would be. The solution in that PR is one way, but I was thinking of having the OperationID equal to the proto rpc Service name + the Method name. Adding the service name would make it 'like' the gRPC model.\nWhat do you think?. Sweet, will do\n. Ah, interesting. Thanks for the info.. ",
    "princejha95": "I have a proto file which contains two services. The comments of the rpc methods present in second service gets copied from first service on swagger UI. Can somebody please tell me how to fix it ?. ```protobuf\nsyntax = \"proto3\";\nimport \"google/api/annotations.proto\";\npackage test;\nservice SampleService1{\n    // Say Hello\n    rpc SayHello (HelloRequest) returns (HelloResponse){\n        option (google.api.http) = {\n            get : \"/api/hello\"\n        };\n    }\n}\nmessage HelloRequest{\n    // Hello message\n    string msg = 1;\n}\nmessage HelloResponse{\n    //Hello response\n    string res = 1;\n}\nservice SampleService2{\n    //Say Bye\n    rpc SayBye (ByeRequest) returns (ByeResponse){\n        option (google.api.http) = {\n            get : \"/api/bye\"\n        };\n    }\n}\nmessage ByeRequest{\n    //Bye message\n    string msg = 1;\n}\nmessage ByeResponse{\n    //Bye response\n    string res = 1;\n}\n```\nThis is the proto file.. json\n{\n    \"paths\": {\n        \"/api/bye\": {\n            \"get\": {\n                \"tags\": [\n                    \"SampleService2\"\n                ],\n                \"operationId\": \"SayBye\",\n                \"responses\": {\n                    \"200\": {\n                        \"description\": \"\",\n                        \"schema\": {\n                            \"$ref\": \"#/definitions/testByeResponse\"\n                        }\n                    },\n                    \"404\": {\n                        \"description\": \"Not Found\",\n                        \"schema\": {}\n                    }\n                },\n                \"parameters\": [\n                    {\n                        \"required\": false,\n                        \"type\": \"string\",\n                        \"name\": \"msg\",\n                        \"description\": \"Bye message.\"\n                    }\n                ],\n                \"summary\": \"Say Hello\"\n            }\n        },\n        \"/api/hello\": {\n            \"get\": {\n                \"tags\": [\n                    \"SampleService1\"\n                ],\n                \"operationId\": \"SayHello\",\n                \"responses\": {\n                    \"200\": {\n                        \"description\": \"\",\n                        \"schema\": {\n                            \"$ref\": \"#/definitions/testHelloResponse\"\n                        }\n                    },\n                    \"404\": {\n                        \"description\": \"Not Found\",\n                        \"schema\": {}\n                    }\n                },\n                \"parameters\": [\n                    {\n                        \"required\": false,\n                        \"type\": \"string\",\n                        \"name\": \"msg\",\n                        \"description\": \"Hello message.\"\n                    }\n                ],\n                \"summary\": \"Say Hello\"\n            }\n        }\n    },\n    \"schemes\": [\n        \"http\",\n        \"https\"\n    ],\n    \"tags\": [\n        {\n            \"name\": \"SampleService2\",\n            \"description\": \"Description\"\n        },\n        {\n            \"name\": \"SampleService1\",\n            \"description\": \"Description\"\n        }\n    ],\n    \"basePath\": \"/\",\n    \"host\": \"localhost:3000\",\n    \"definitions\": {\n        \"testHelloResponse\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"res\": {\n                    \"type\": \"string\",\n                    \"title\": \"Hello response\"\n                }\n            }\n        },\n        \"testByeResponse\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"res\": {\n                    \"type\": \"string\",\n                    \"title\": \"Bye response\"\n                }\n            }\n        }\n    },\n    \"swagger\": \"2.0\"\n}\nThis is the generated swagger json file.. sure\n. I have raised a new issue. Here's the link:\nhttps://github.com/grpc-ecosystem/grpc-gateway/issues/746. I think we need to look at https://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-swagger/genswagger/template.go because this is where the comments part is dealt.\nPlease have a look to this file.. Can anyone tell me how can i debug files in grpc-gateway using a debugger ?\nWhich file should be the starting point to start debugging ?. I have done and checked what you said and it's correct but this is working only for a single proto file. If i have multiple proto files in a directory, then it is displaying the comments of rpc methods of all the services only for a single proto file. The comments of rpc methods of remaining proto files is not getting displayed.\n. https://github.com/grpc-ecosystem/grpc-gateway/blob/ab0345bb328757bfef2f3d7d4e642e182eb985b9/protoc-gen-swagger/genswagger/template.go#L841\nWhat i observed is that when we have multiple proto files, then the title of the generated file is set to the name of that file which comes first in alphabetical order.\nSo, in the above link, *p.File.Name  in title represents the name of file coming first alphabetically due to which it traverses the rpc methods of services of only that file.\nExample: If i have two proto files say a.proto and b.proto. When i will generate swagger json file via merging both the proto files, then the title present in https://github.com/grpc-ecosystem/grpc-gateway/blob/ab0345bb328757bfef2f3d7d4e642e182eb985b9/protoc-gen-swagger/genswagger/template.go#L841 will be set to a.proto  and you can see that in https://github.com/grpc-ecosystem/grpc-gateway/blob/ab0345bb328757bfef2f3d7d4e642e182eb985b9/protoc-gen-swagger/genswagger/template.go#L850, the services for only that proto file is passed.\nSo can somebody tell me how to pass services of all the proto files instead of single proto file ?. Can anyone provide the solution for the above issue ? I tried but i was not able to figure it out.. ",
    "bsakweson": "I ran into an issue similar to this, would love to open a new issue if needed but thought I should start here since they related.\nI have a setup with multiple micro services each defined in their own .proto file. Code generation works well until I tried to combine my openAPI documentation using protoc-gen-swagger from grpc-gateway. \nUsing this command protoc --proto_path=api/v1 --proto_path=$GOPATH/src --proto_path=third_party --swagger_out=logtostderr=true,allow_merge=true:third_party/OpenAPI a/a.proto b/b.proto to generate a merged apidocs.swagger.json file containing all services defined in a and b respectively so that I can serve it as a single page throws this error --swagger_out: inconsistent package names: a b. Notice that a and b are all in different packages a and b.\nservice a:\n```\nsyntax = \"proto3\";\npackage com.somecompany.a;\noption go_package = \"a\";\nimport \"google/api/annotations.proto\";\n// A is a service for handling Foo stuff.\nservice A {\n  // List all Foos\n  //\n  // Returns a (possibly paginated) list of all foo resources on success.\n  rpc ListFoos (ListFoosRequest) returns (ListFoosResponse) {\n    option (google.api.http) = { get: \"/v1/foos\" };\n  }\n}\n// The ListFoos request message.\nmessage ListFoosRequest {}\n// The ListFoos response message.\nmessage ListFoosResponse {}\n```\nservice b:\n```\nsyntax = \"proto3\";\npackage com.somecompany.b;\noption go_package = \"b\";\nimport \"google/api/annotations.proto\";\n// B is a service for handling Bar stuff.\nservice B {\n  // List all Bars\n  //\n  // Returns a (possibly paginated) list of all bar resources on success.\n  rpc ListBars (ListBarsRequest) returns (ListBarsResponse) {\n    option (google.api.http) = { get: \"/v1/bars\" };\n  }\n}\n// The ListBars request message.\nmessage ListBarsRequest {}\n// The ListBars response message.\nmessage ListBarsResponse {}\n```\n. Would you by any chance know of any hack I can use for now to get those swagger files concatenated, if so would mind sharing?. ",
    "co3k": "I wonder why the https://travis-ci.org/grpc-ecosystem/grpc-gateway/builds/390065893 build fails in   Go: 1.10.x, GATEWAY_PLUGIN_FLAGS= job. Can anyone advice me to solve this problem?. @yugui\nThank you for your response, but I can't get any diffs by running that command on my macOS.\n```\n$ go version\ngo version go1.10.3 darwin/amd64\n$ protoc --version\nlibprotoc 3.5.1\n$ brew info protobuf\nprotobuf: stable 3.5.1 (bottled), HEAD\nProtocol buffers (Google's data interchange format)\nhttps://github.com/google/protobuf/\n/usr/local/Cellar/protobuf/3.2.0_1 (257 files, 15.9MB)\n  Poured from bottle on 2017-04-28 at 18:02:34\n/usr/local/Cellar/protobuf/3.5.1_1 (267 files, 18.5MB) *\n  Poured from bottle on 2018-06-11 at 00:22:59\nFrom: https://github.com/Homebrew/homebrew-core/blob/master/Formula/protobuf.rb\n==> Dependencies\nBuild: autoconf \u2714, automake \u2714, libtool \u2714\nRecommended: python@2 \u2714\nOptional: python \u2718\n==> Options\n--with-python\n    Build with python support\n--with-test\n    Run build-time check\n--without-python@2\n    Build without python2 support\n--HEAD\n    Install HEAD version\n==> Caveats\nEditor support and examples have been installed to:\n  /usr/local/opt/protobuf/share/doc/protobuf\n$ cd $GOPATH/src/github.com/golang/protobuf/protoc-gen-go; git log -1 --oneline; cd -\n05f48f4 (HEAD -> master, origin/master, origin/HEAD) proto: revert UTF-8 validation for proto2 (#628)\n$ make clean && make examples\nrm -f bin/protoc-gen-grpc-gateway bin/protoc-gen-swagger\ngo build -o bin/protoc-gen-grpc-gateway github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway\nSNIP\n$ git status\nOn branch allow-empty-array-in-security\nnothing to commit, working tree clean\n$ make clean && make test\nrm -f bin/protoc-gen-grpc-gateway bin/protoc-gen-swagger\ngo build -o bin/protoc-gen-grpc-gateway github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway\nSNIP\n$ git status           \nOn branch allow-empty-array-in-security\nnothing to commit, working tree clean\n``. The error has been fixed at https://github.com/grpc-ecosystem/grpc-gateway/pull/666/commits/d1985fcea6b588fc855bfa70b0600524ad1e4027 .\nI might need to runmake realclean && make examples` for solving this problem.\nThanks again to yugui!\nThe https://travis-ci.org/grpc-ecosystem/grpc-gateway/jobs/390576866 job for Go: 1.9.x, GATEWAY_PLUGIN_FLAGS= has been timed out.\nIs it a temporary error? But I cannot retry this job because I don't have permission to do so.. @tmc Thanks for your response but the https://travis-ci.org/grpc-ecosystem/grpc-gateway/jobs/394549559#L554-L557 test is failed that is due to timed out in go get phase. I cannot rebase it again because the upstream is not updated yet. How should I do next?. @johanbrandhorst Thanks for your advice! I've squashed commits and changed the title of this pull request and the first commit to follow your suggestion.. @achew22 \n\nIt looks like the changes to the golden .go files all come from changes upstream\n\nI added these changes to pass build on Travis CI. In the other pull request, I've encountered build error that is caused by the upstream changes. https://travis-ci.org/grpc-ecosystem/grpc-gateway/builds/390065893\n\ndo you think you could add an example that does exercise it so I can see the new functionality?\n\nYou're right. I've added the https://github.com/grpc-ecosystem/grpc-gateway/pull/667/commits/ce1532e41996ea402ff4bd1529688490b245d1a3 commit to show it.. @johanbrandhorst I rebased my changes and tests on Travis CI are passed.. @johanbrandhorst OK, I've just squashed them.. Oh, I didn't noticed about #375 and #681.\nIf we don't need to keep compatibility of existing generated swagger definitions, we can choose #681 approach instead of this pull request.\n. Hi, I really need these changes for my works but there aren't any reactions for a month.\nWhat should I do to get any feedbacks about this pull request?. @johanbrandhorst Thanks for your advice! I've added descriptions to this pull request. If it's OK, I will also change the commit message to follow it.. I wonder why the Go: 1.10.x GATEWAY_PLUGIN_FLAGS= Travis CI build gets failed after my rebasing. I've tried make realclean examples but this problem isn't solved.. Oh, I'm sorry...I've forgotten to do go get -u github.com/golang/protobuf/protoc-gen-go before my local building. \nFinally, Travis CI builds have been passed.. @johanbrandhorst \nOh, I'm sorry for confusing you. How about the following?\ndiff\n- Reflect descriptions of message to Swagger parameter representation automatically\n+ Reflect comments of proto message as description of Swagger's Parameter Object representation implicitly\n. @johanbrandhorst Thank you for your good suggestion. I've updated the title of this pull request and the commit message.. Oh, sorry for the mistakes. I fixed it by https://github.com/grpc-ecosystem/grpc-gateway/pull/667/commits/4b6f8895524282af806fda06e42705cbe400b817. Required. The omitempty tag removes an empty slice. However we don't want to print null value in every JSON response so we can't remove this omitempty tag.. I think we really need empty slice in this case.. Sorry, I've fixed it in https://github.com/grpc-ecosystem/grpc-gateway/pull/666/commits/a78ff380159daf7f0da4009c5b65a2263ad32249 (I'll squash this commit later). Thanks! Fixed.. I've fixed it in https://github.com/grpc-ecosystem/grpc-gateway/pull/687/commits/48e32c98afbc0758fbb4c635e0f67b601532f49c. ",
    "razamiDev": "I am okay with my commits to be contributed to this project. My latest commit is an attempt to fix the travis cli errors by updating the conflicting files. . @vtolstov @johanbrandhorst I will try to complete this item towards the end of this week or beginning of next. Me and @dmacthedestroyer have both worked on this and I have taken it over. . I appreciate the help as I am new to bazel. @johanbrandhorst I installed bazel and ran bazel run :gazelle_fix and it didn't give me any errors it said Build completed successfully.\n@drigz  I also ran the docker command and it said Build completed successfully. \nI added all the new BUILD.bazel files and see that it still failed... giving an error not findingfastuuid. @johanbrandhorst I am not sure why the googlebot failed... I am okay with my work being contributed to this project.. @johanbrandhorst @drigz thank you both for the help! \nI have created a new PR (#812) and this can be closed.\n. @johanbrandhorst I added some documentation. Let me know if that is what you had in mind. . @johanbrandhorst good idea! I added some examples for curl :) . @johanbrandhorst this change came from merging with master. The current repo has it like this.. ",
    "vtolstov": "@dmacthedestroyer hi, do you plan to complete pr ?. does it possible to modify EmitDefaults per Service in runtime.NewServeMux ?. i have compat api and new api. In compat i need to EmitDefaults: true, but in new - not. thanks, but how to deal with two different ServeMux in one http.Server ?. https://github.com/mennanov/fieldmask-utils this is works fine. oneof not helps:\noneof image {\n    string slug = 4;\n    uint64 id = 5;\n  };\n{\"error\":\"json: cannot unmarshal string into Go value of type pb.XXX\",\"code\":3}. if i'm provide string, i have error that can't unmarshal to my type.\n{\"error\":\"json: cannot unmarshal number into Go value of type string\",\"code\":3}. @achew22 nice, can you provide link to doc how to write such interceptor?. @ivucica thanks! i'm try this. @ivucica not worked =(\nMay be i'm wrong...\ndir pb contains common.go\n```\npackage pb\nimport (\n        \"fmt\"\n        \"strconv\"\n    \"github.com/gogo/protobuf/jsonpb\"\n\n)\nfunc (v IntAndString) UnmarshalJSONPB(c jsonpb.Unmarshaler, b []byte) error {\n        fmt.Printf(\"AAAAA\\n\")\n        if i, err := strconv.Atoi(string(b)); err != nil {\n                v.Str = string(b)\n        } else {\n                v.Num = uint32(i)\n        }\n        return nil\n}\nfunc (v IntAndString) MarshalJSONPB(c jsonpb.Marshaler) ([]byte, error) {\n        return nil, nil\n}\ndefine IntAndString in proto file like:\nmessage IntAndString {\n  string str = 1;\n  uint32 num = 2;\n};\n```\nin other proto file use IntAndString image = 5;\nwhen i'm send int i have : \n{\"code\":3,\"message\":\"json: cannot unmarshal number into Go value of type map[string]json.RawMessage\",\"details\":[]}\nif i don't define in proto type IntAndString i fails to generate:\n\"IntAndString\" is not defined. ping.... Without tls all works fine with cmux, i can connect without certs to grpc server and do some rest requests, but with tls enabled i'm always have timeouts. i think that me question is:\ndoes it possible to use the same cert on client (gateway) and on server ?. I found the root of my issue - cmux.\nI need to run both grpc and rest on the same port. In case of plain tcp - all fine, cmux works by query request headers. In case of crypt - i need to pass to grpc server plain tcp listener, but for rest i need to pass tls encrypted listener.. So my new question - does have somebody already knows how to do encrypted grpc+rest with cmux.. i'm solve. With plain mode i'm use cmux to get rest and grpc on the same port.\nFor tls mode i'm use another example from @philips\n. ```\n        tcpl, err := net.Listen(\"tcp\", *endpoint)\n        if err != nil {\n                log.Fatalf(\"%s\", err)\n        }\n        defer tcpl.Close()\n    if !secure {\n            tcpm = cmux.New(tcpl)\n            //              grpcl = tcpm.MatchWithWriters(cmux.HTTP2MatchHeaderFieldPrefixSendSettings(\"content-type\", \"application/\n\ngrpc\"))\n                grpcl = tcpm.Match(cmux.HTTP2())\n                restl = tcpm.Match(cmux.HTTP1())\n        } else {\n                tlsl, err := tlsListener(tcpl)\n                if err != nil {\n                        log.Fatal(\"%s\", err)\n                }\n                grpcl = tlsl\n                restl = tlsl\n        }\n    grpcs, err := prepareGRPC(ctx)\n    if err != nil {\n            log.Fatal(err)\n    }\n    rests, err := prepareREST(ctx, grpcs)\n    if err != nil {\n            log.Fatal(err)\n    }\n\n    if !secure {\n            go func() {\n                    if err = grpcs.Serve(grpcl); err != cmux.ErrListenerClosed {\n                            log.Fatal(err)\n                    }\n            }()\n    }\n    go func() {\n            if err = rests.Serve(restl); err != cmux.ErrListenerClosed {\n                    log.Fatal(err)\n            }\n    }()\n\n    if !secure {\n            if err := tcpm.Serve(); !strings.Contains(err.Error(), \"use of closed network connection\") {\n                    log.Fatal(err)\n            }\n    } else {\n            select {}\n    }\n\n```. main part is above. may be i create some repo to put all stuff to it and provide link.. no, i don't create repo. @robbert229 can you take a look?. gentle ping again.... any progress?. I found https://github.com/grpc-ecosystem/grpc-gateway/issues/224 but i'm realy need to pass fingerprint that contains colons. And i'm believe that it can be possible.. @johanbrandhorst why! Colon as i know does not need to be url encoded.. @johanbrandhorst No, i need to receive param that contains colons via grpc-gateway for my grpc service, now i have ugly method to override param (adding trailing colon) before pass it to handler. I'm use github.com/golang/protobuf v1.2.0. I'm fetch master of grpc-gateway and now all fine. May be the time to get new release?. @johanbrandhorst thanks! \n. but why i can't generate swagger? i don't want to use grpc-gateway, i'm use only swagger generator from proto files.. ",
    "mattouille": "I was actually just looking at this. I don't quite see an avenue yet but I'll take a deeper look later today.. ",
    "jon-whit": "Does anyone have any examples with this?. What's the status on this PR? I think I have the same need? Correct me if I am wrong please :)\nI have a field in a protobuf message called \"parent\". It looks like this:\n`\n  message ListRolesRequest {\n    // The resource name of the parent resource in one of the following formats:\n    // (empty string) -- this refers to curated roles.\n    // organizations/{organization_id}\n    // tenants/{tenant_id}\n    string parent = 1;\n    ...\n  }\nrpc ListRoles(ListRolesRequest) returns (ListRolesResponse) {\n      //Maps to HTTP GET.\n      option (google.api.http) = {\n        get: \"/auth/iam/roles\"\n        additional_bindings {\n          get: \"/auth/iam/{parent=organizations/}/roles\"\n        }\n        additional_bindings {\n          get: \"/auth/iam/{parent=tenants/}/roles\"\n        }\n      };\n  }\n```\nThe parent can take the form \"organizations/\" or \"tenants/\" or it can be empty. I'd like the swagger to match all three. For example:\nGET /v1/organizations/<org_id>/roles\nGET /v1/tenants/<tenant_id>/roles\nGET /v1/roles\nHow can I achieve this?. @ch3rub1m Are you saying I can already achieve what I'm looking for? If so, what would the protobuf HTTP options look like for that? That's what I'm struggling with.\nIf not, are you saying that once this PR is merged the example above will be supported? Sorry for the confusion.. @ch3rub1m That doesn't match the three mentioned above though.. It will only match\nGET /auth/iam/organizations/<org_id>/roles\nGET /auth/iam/tenants/<tenant_id>/roles\nI'd like a match that matches all three of the following:\nGET /auth/iam/roles\nGET /auth/iam/organizations/<org_id>/roles\nGET /auth/iam/tenants/<tenant_id>/roles\nIs this a scenario where the additional bindings are meant to be used?. Was the use case outlined above addressed in #702, because I just pulled the latest and I don't see any differences in the generated swagger output.\nLooping in @ch3rub1m . @ch3rub1m that doesn't even compile with the latest version of protoc. the { and} characters have to be preceded and followed by / characters.. @ch3rub1m sorry man, I feel like we're clashing on this one... The problem with your example above is it doesn't adhere to my needs as stated above.\nI need parent to match organizations/<org_id> or tenants/<tenant_id> or an empty string. Your example above would match iam/<something>.. Is there anyone else that can speak to this? I guess I'm not fundamentally understanding why\n/auth/iam/{parent=organizations/*}/roles\ndoesn't get transformed to something like\n/auth/iam/organizations/{id}/roles\nCould someone help me understand why this design choice was made?. @johanbrandhorst thanks for looping in here. I've been trying to figure out a solution to this issue for a while now, so any assistance is much appreciated :).\nThanks!. @achew22 Any ideas here?. @achew22 maybe I misunderstand.. The example above would be one such case..? What do you mean?. ",
    "dharmjit": "@afking\nAny example on this. I am facing this issue and seeing different spans(ochttp,ocsql) for the same request.\nThanks. I might be making some silly mistake. Below is the sample of the code of the server \n// RunServer runs HTTP/REST gateway\nfunc RunServer(ctx context.Context, grpcPort, httpPort string) error {\n    ctx, cancel := context.WithCancel(ctx)\n    defer cancel()\n    mux := runtime.NewServeMux()\n    opts := []grpc.DialOption{grpc.WithInsecure()}\n    if err := v1.RegisterAuthServiceHandlerFromEndpoint(ctx, mux, \"localhost:\"+grpcPort, opts); err != nil {\n        // log.Fatalf(\"failed to start HTTP gateway: %v\", err)\n        logger.Log.Fatal(\"failed to start HTTP gateway\", zap.String(\"reason\", err.Error()))\n    }\n    srv := &http.Server{\n        Addr:    \":\" + httpPort,\n        Handler: &ochttp.Handler{Handler: \n            // middleware.AddLogger(logger.Log, mux)},\n            mux},\n    }\n    // log.Println(\"starting HTTP/REST gateway...\")\n    logger.Log.Info(\"starting HTTP/REST gateway...\")\n    return srv.ListenAndServe()\n}\nand below is the implementation of the rpc method\n```\nfunc (s authServiceServer) Login(ctx context.Context, req v1.LoginRequest) (*v1.LoginResponse,error){\n    // get SQL connection from pool\n    c, err := s.connect(ctx)\nif err != nil {\n    return nil, err\n}\ndefer c.Close()\n\n// query User by ID and Password\ncCtx, cSpan := trace.StartSpan(ctx, \"Select\")\nrows, err := c.QueryContext(cCtx, \"SELECT count(1) FROM user123\")\nif err != nil {\n    return nil, status.Error(codes.Unknown, \"failed to select from user-> \"+err.Error())\n}\ncSpan.End()\nfmt.Printf(\"number of rows 1:%d\",getCount(rows))\ndefer rows.Close()\nreturn &v1.LoginResponse{Username:\"1\"},nil\n\n}\n```. Ahh okk I will post it there. This was somewhat related to grpc-gateway so I posted here.. ",
    "ch3rub1m": "@achew22 Thanks for quick reply. I am using the dep and run the dep ensure -update but it looks like this fix was not yet released. Could you give some advices about how to use the latest code in the project?. @achew22 Hi, I tried but it still fail. It seems like #660 didn't fix this issue.. @achew22 I read the source code and I found the issue is caused by gengateway/template.go, it calls the method req.URL.Query() directly. In this method, package req split query parameters with both & and ;. So I encode the ; to %3B when request and it works. Thank you for your help.. @achew22, thanks for the response. I considered that the key in the Swagger generation could contain the format info, use\n\"paths\": {\n    \"/v1/{name=users/*}\": {\n    }\n  }\ninstead\n\"paths\": {\n    \"/v1/{name}\": {\n    }\n  }\nit seems could fix this issue.. It works if I manually edit the Swagger, but the changes will be overwritten when I generate the file next time.. @achew22 I created a pull request (#704).\nFixed the issue and added some test cases.\nHowever it built failed weirdly in Travis CI.\nCould you check about it please?. @achew22 I'm sorry, I want to trigger the Travis CI again, so I used push -f on my branch. It closed the PR automatically. Could you reopen this PR? Or maybe I should open a new PR?. @frolickingferret445 I am still waiting for being merged.\nUnfortunately it cannot fix #720.. Issue was fixed.. I signed it!. @johanbrandhorst @achew22 I already pushed. But it built failed again.. @achew22 Ok, I got it, thanks.. @achew22 It still not work with GO: 1.10.x. Could you help to check it please?. @achew22 Hi, I would like to add more usage, but it seems like it's not so suitable to add them directly to the everything example. May I create a new example? By the way, please take a look at the CI issue when you are not that busy.. @jon-whit I am writing the examples for this PR. I think it's different with your need. But you could do this to reach your demand:\nGET /v1/{parent=*/*}/roles. @jon-whit Yes, you could already achieve what you are looking for in current version! But there are some bug in generated swagger doc. And I am trying to fix it. But the bug just affect the doc so the service is still work. You should write as follow to achieve what you want:\n` \nmessage ListRolesRequest {\n    // The resource name of the parent resource in one of the following formats:\n    // (empty string) -- this refers to curated roles.\n    // organizations/{organization_id}\n    // tenants/{tenant_id}\n    string parent = 1;\n    ...\n  }\nrpc ListRoles(ListRolesRequest) returns (ListRolesResponse) {\n      //Maps to HTTP GET.\n      option (google.api.http) = {\n          get: \"/auth/iam/{parent=/}/roles\"\n      };\n  }\n```. Hi, @achew22. I tried to write some usage in everything example but it failed in travis CI. The bazel really confused me and I don't know how to generate *.pb.go and swagger file by bazel. So I roll back the example commit and it works again. Could you approve this PR first please? I have done many tests in my own project and it seems the PR is ready to be merged. If you wants more usage in example I will open another PR.. @jon-whit #702 is not about additional_bindings. It is for the resource name in path. I pulled the latest and it works well.\nBy the way, this maybe work in your case:\nrpc ListRoles(ListRolesRequest) returns (ListRolesResponse) {\n      option (google.api.http) = {\n        get: \"/auth/iam{parent=*}/roles\"\n      };\n}. @jon-whit How about this:\nrpc ListRoles(ListRolesRequest) returns (ListRolesResponse) {\n      option (google.api.http) = {\n        get: \"/auth/{parent=iam*}/roles\"\n      };\n}. @jon-whit I understood. I just propose a compromise between current features and what you want.. ",
    "waveywaves": "@johanbrandhorst I am starting work on this issue. Do let me know if there is something I need to keep in mind. . @johanbrandhorst @venezia I would like to work on this if that is fine :). I have started work on this. Please assign it to me.. > I signed it!\n. @achew22 Yes, I'll add a commit for the test in the PR. Thanks for checking.\n. > Thanks so much for the PR! Do you think you could add a test that verifies this behavior so we don't accidentally regress in the future?\nI have added the test and updated the PR. ",
    "alvoron": "Let me proceed this thread, since I have the same task. I have a Spring Boot application. I was able to run gRPC server in it, it works fine. \nThen I decided to add REST interface. I have genereted reverse-proxies. Example entrypoint is written using Go and I haven't seen how can I use built in Tomcat from Spring Boot to host reverse-proxies. Can grpc-gateway generate Java proxies? Or there is another way to use Spring Boot Tomcat instead of Go web server?. ",
    "drorventura": "Hi, did someone stated fixing this issue?. I might :) \ni'll try to make time for it soon. ",
    "frolickingferret445": "Are you sure that you are still seeing this issue?  If I understand your issue correctly, I was seeing the same issue, but it disappeared when I pulled the latest version of grpc-gateway.. What is the current state of this merge request?  And are we sure that it won't fix this issue?\nhttps://github.com/grpc-ecosystem/grpc-gateway/issues/720\nThanks!. hmmm thanks!. ",
    "jriecken": "I signed it!. It looks like this was caused by an upstream bugfix in github.com/golang/protobuf/jsonpb  that was made yesterday: https://github.com/golang/protobuf/pull/645\nThe JSON generated by that marshal_jsonpb test before that fix looks like:\njson\n{\"singleNested\":{},\"uuid\":\"6EC2446F-7E89-4127-B3E6-5C05E6BECBA7\",\"nested\":[{\"name\":\"foo\",\"amount\":12345}],\"uint64Value\":\"18446744073709551615\"\n,\"repeatedStringValue\":[],\"oneofString\":\"bar\",\"mapValue\":{\"a\":1,\"b\":0},\"mappedStringValue\":{},\"mappedNestedValue\":{},\"timestampValue\":\"1970-01-01T00:00:00Z\",\"repeatedEnumValue\":[\n]}\nand after:\njson\n{\"singleNested\":{},\"uuid\":\"6EC2446F-7E89-4127-B3E6-5C05E6BECBA7\",\"nested\":[{\"name\":\"foo\",\"amount\":12345}],\"uint64Value\":\"18446744073709551615\"\n,\"repeatedStringValue\":[],\"oneofString\":\"bar\",\"mapValue\":{\"a\":\"ONE\",\"b\":\"ZERO\"},\"mappedStringValue\":{},\"mappedNestedValue\":{},\"timestampValue\":\"1970-01-01T00:00:00Z\",\"repeatedEnu\nmValue\":[]}\nNote how the mapValue map's values changed from the numeric representation of the enum (\"mapValue\":{\"a\":1,\"b\":0}) to the string one (\"mapValue\":{\"a\":\"ONE\",\"b\":\"ZERO\"}). This makes sense as the test is setting EnumsAsInts to 0 (i.e. output them as strings).\nI'll update the test to match the fixed behavior from jsonpb. One of the browser tests was also assuming integer enum values - fixed it too. One of the travis runs failed because protoc generated a file descriptor var that was slightly different  (fileDescriptor_wrappers_5850bd48b0187754 vs fileDescriptor_wrappers_0cd12813574dca64) and the git diff failed. The other 5 non-bazel test runs did not have this same issue.\nThe Bazel version of the build also failed (it must somehow be pulling in a different version of jsonpb as it output the old numeric enum values?)\nI feel like I'm very quickly going down a rabbit hole in grpc-gateway's build process.... I was using the latest version of protoc instead of 3.1 - regenerating with 3.1 created the same var name.. I did a little more digging and the Bazel Go rules pin the golang/protobuf dependency to this version: https://github.com/golang/protobuf/releases/tag/v1.1.0 (see https://github.com/bazelbuild/rules_go/blob/7e07b18263c08c8eca31207e1f5b28aa71539a72/go/private/repositories.bzl#L75)\nThe non-Bazel build uses go get (i..e pulling from master of golang/protobuf, which has the change made yesterday that caused the marshal_jsonpb test to break in the first place). Done. ",
    "dmgarland": "For instance, how would one accomplish the following in YAML:\noption (grpc.gateway.protoc_gen_swagger.options.openapiv2_swagger) = {\n  info: {\n    title: \"Example API\";\n    description: \"A description\";\n    version: \"1.0\";\n    contact: {\n      url: \"https://example.org\";\n      email: \"someone@example.org\";\n    };\n  };\n}. ",
    "gensmusic": "need help too. . ",
    "askurydzin": "This may also be a solution for #446, here https://github.com/infobloxopen/atlas-app-toolkit/blob/master/query/collection_operators.proto#L56 we have recursive definition via oneof of a parse tree, which is a string on a client side.. Thanks, @johanbrandhorst. I would not consider this as a breaking change, as it employs nonsense combination of options (type \"string\" for message) that was unlikely used by anyone.. bIdx+1 as per https://github.com/grpc-ecosystem/grpc-gateway/blob/8b937447b5fea1a0a60782b3cc42d279694f41fd/protoc-gen-swagger/genswagger/template.go#L703. ",
    "izumin5210": "@googlebot I signed it!. @johanbrandhorst I fixed the message! \ud83d\ude04 . I rebased!\nBut CI failed again \ud83d\ude22 \nCould you rerun them?\n\n15.27s$ test \"${USE_BAZEL}\" = true || go get github.com/golang/lint/golint\npackage golang.org/x/lint: unrecognized import path \"golang.org/x/lint\" (https fetch: Get https://golang.org/x/lint?go-get=1: net/http: TLS handshake timeout)\nThe command \"test \"${USE_BAZEL}\" = true || go get github.com/golang/lint/golint\" failed and exited with 1 during .\n\nhttps://travis-ci.org/grpc-ecosystem/grpc-gateway/jobs/411080943#L557-L561. > If there is no GOPATH how do we run that, or how should we run that\n\nModules don't mean you can't have a GOPATH, but I don't think it's necessary here anyway. We can assume a vendor directory for these instructions for example.\n\nHow about go list -m -f \"{{.Dir}}'\uff1f\n:) % ls $(go list -m -f \"{{.Dir}}\" github.com/grpc-ecosystem/grpc-gateway)\nBUILD                   CONTRIBUTING.md         Gopkg.toml              LICENSE.txt             README.md               bin                     docs                    internal                protoc-gen-swagger      third_party\nCHANGELOG.md            Gopkg.lock              ISSUE_TEMPLATE.md       Makefile                WORKSPACE               codegenerator           examples                protoc-gen-grpc-gateway runtime                 utilities. ",
    "bcdurden": "Can we roll backwards? This just broke all of my builds that depend on this project.  Unless the sync has yet to happen and that will fix this error:\ngo/src/github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway/descriptor/services.go:146:49: opts.ResponseBody undefined (type *annotations.HttpRule has no field or method ResponseBody)\n. Much appreciated!  I should have read upwards a bit before I commented anyhow :100: . All good! Thanks for the quick turn-around! . ",
    "ti": "this is really ugly. how to fix \"error\" and \"message\" repeat problem ?\nbefore this code:\njson\n{\n    \"error\": \"something error\",\n    \"code\": 3\n}\nafter:\njson\n{\n    \"error\": \"something error\",\n    \"message\": \"something error\",\n    \"code\": 3\n}\nshould web restore the code? \n. you can just use to fix it before:\ntype Error struct {\n    Message string   `json:\"error,omitempty\"`\n@ffredsh. @johanbrandhorst web should not remove the \"error\" field in http response json, the \"error\" field is defined for compatible for some rfc documents, \nFor exp:\nhttps://tools.ietf.org/html/rfc6749#page-45\nmore:\nthe \"error\" means some  \"enmu text\" for the front apps, but he \"message\" just means some error details, if you want add it,  you should add  the \"message\" to the 2.0\n@ffredsh . This change if really affect our current restful document for open apis \ud83d\ude22. @ffredsh . add \"message\" fields is meaningless for now. @ffredsh we should recovery of the code.. @johanbrandhorst  https://github.com/grpc-ecosystem/grpc-gateway/pull/718 . @johanbrandhorst  you can not config runtime.HTTPError in   envoy grpc2restful gateway.  my app is behind envoy sidecar\u3002 . the envoy is just a example. this change is will affect most of projects use grpc-gateway, such as go-micro.\n@johanbrandhorst do you think the https://github.com/grpc-ecosystem/grpc-gateway/pull/718 is really ok? , i mean the @achew22 's answer is the right way, \"You could also submit a PR to disable it by flag and then in 2.0 we can make it the default.\"\n. ",
    "toranger": "Pre: I run two local raft master node srv and one local grpc-gateway to distribute http request\nsuch like this rpc function, protofile like this:\n\n\nPs: the http request can arrived at gateway node, but also tell me transport is closing. ^\n\n. @achew22,\nsvr.txt\n```\npackage server\nimport (\n    \"context\"\n    \"cos-config/common\"\n    \"net\"\n    \"strings\"\n    \"sync\"\n    \"time\"\n\"github.com/coreos/etcd/clientv3\"\n\"github.com/golang/glog\"\n\"google.golang.org/grpc\"\n\n)\ntype ConfigMaster struct {\n    etcdClient clientv3.Client\n    grpcServer grpc.Server\n    lis        net.Listener\n    addr       string\n    started    bool\n    mu         sync.Mutex\n    wg         sync.WaitGroup\n    s          ConfigSvrMasterService\n}\nfunc (m ConfigMaster) Init(etcdEndpoints, listenEndpoints, etcdUserName, etcdUserPasswd string, wg sync.WaitGroup) error {\n    etcd_eps := strings.Split(etcdEndpoints, \",\")\n    c, err := clientv3.New(clientv3.Config{\n        Endpoints:   etcd_eps,\n        DialTimeout: 2 * time.Second,\n        Username:    etcdUserName,\n        Password:    etcdUserPasswd,\n    })\nif err != nil {\n    glog.Error(\"Can not create etcd client Error\", err.Error())\n    return err\n}\nm.etcdClient = c\nm.s = NewConfigSvrMasterService(c, common.CosClient)\n\nm.lis, err = net.Listen(\"tcp\", listenEndpoints)\nif err != nil {\n    glog.Error(\"failed to listen \", err.Error())\n    return err\n}\n\nm.grpcServer = grpc.NewServer()\nRegisterConfigSvrMasterServiceServer(m.grpcServer, m.s)\nm.addr = listenEndpoints\nm.wg = wg\nreturn nil\n\n}\nfunc (m *ConfigMaster) Start() error {\n    glog.Info(\"Begin to start config master\")\n    defer m.wg.Done()\nm.mu.Lock()\nif m.started {\n    glog.Info(\"Master has been started\")\n    m.mu.Unlock()\n    return nil\n}\nm.mu.Unlock()\n\nctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\ndefer cancel()\nretryTimes := 1\nvar err error\nfor retryTimes <= 5 {\n    _, errMsg, err := putKeyToEtcd(ctx, m.etcdClient, MasterEtcdKey, m.addr)\n    if err != nil {\n        glog.Info(\"Failed to update master key \", errMsg)\n        retryTimes += 1\n        continue\n    } else {\n        break\n    }\n}\n\nif retryTimes > 5 {\n    glog.Fatal(\"Unable to put master endpoint to etcd, \", err)\n}\n\nerr = common.SyncConfigDataFromETCD(m.etcdClient, m.s.GetConfigData())\nif err != nil {\n    glog.Fatal(\"Failed to sync config data from etcd \", err.Error())\n}\n\nerr = common.SyncConfigMetaFromETCD(m.etcdClient, m.s.GetConfigMeta())\nif err != nil {\n    glog.Fatal(\"Failed to sync config Meta from etcd \", err.Error())\n}\n\nerr = common.SyncUpgradeInfoFromETCD(m.etcdClient, m.s.GetConfigEvent())\nif err != nil {\n    glog.Fatal(\"Failed to sync config upgrade info from etcd \", err.Error())\n}\n\nglog.Info(\"begin to start master, current status is \", m.started)\nm.started = true\nreturn m.grpcServer.Serve(m.lis)\n\n}\nfunc (m *ConfigMaster) Stop() error {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    if !m.started {\n        glog.Info(\"Config master has not been started yet. ignore\")\n        return nil\n    }\nglog.Info(\"Stopping config master\")\nm.grpcServer.Stop()\nm.started = false\nreturn nil\n\n}\n```\ngateway.txt\n```\npackage main\nimport (\n    \"context\"\n    \"cos-config/common\"\n    server \"cos-config/master\"\n    \"errors\"\n    \"flag\"\n    \"net/http\"\n    \"os\"\n    \"os/signal\"\n    \"sync\"\n    \"syscall\"\n    \"time\"\n\"cos-config/storage\"\n\"github.com/coreos/etcd/clientv3\"\n\"github.com/golang/glog\"\n////    \"github.com/golang/protobuf/proto\"\n\"github.com/grpc-ecosystem/grpc-gateway/runtime\"\n\"google.golang.org/grpc\"\n\n)\nvar (\n    restfulServerEndpoint = flag.String(\"rest_server_port\", \":8090\", \"rest server listen endpoint\")\n    etcdUrls              = flag.String(\"etcd_endpoint\", \"119.29.166.185:2379\", \"multiple etcd service endpoint, sepeated by comma\")\n    ctx                   context.Context\n    cancel                context.CancelFunc\n    svr                   http.Server\n    etcdClient            clientv3.Client\n    masterEndpoint        string\n    wg                    sync.WaitGroup\n    watcher               common.Watcher\n)\nfunc run(backendEndpoint string) error {\n    glog.Info(\"get the backend point\", backendEndpoint)\n    defer wg.Done()\n    ctx = context.Background()\n    ctx, cancel = context.WithCancel(ctx)\n    defer cancel()\nmux := runtime.NewServeMux()\n\nopts := []grpc.DialOption{grpc.WithInsecure()}\nerr := server.RegisterConfigSvrMasterServiceHandlerFromEndpoint(ctx, mux, backendEndpoint, opts)\n\nif err != nil {\n    glog.Error(\"register config svr failed \", err.Error())\n    return err\n}\n\nsvr = http.Server{\n    Addr:    *restfulServerEndpoint,\n    Handler: mux,\n}\nreturn svr.ListenAndServe()\n\n}\nfunc getBackendServiceMasterEndpoint() (string, bool) {\n    item, , , err := common.GetKeyFromETCD(context.Background(), etcdClient, server.MasterEtcdKey)\n    if err != nil {\n        glog.Error(\"Failed to get master endpoint \", err.Error())\n        return \"\", false\n    }\nreturn item.ConfigValue, true\n\n}\ntype HttpServerWatchHandler struct {\n}\nfunc (h HttpServerWatchHandler) HandlePut(event *clientv3.Event) error {\n    if nil == event {\n        return errors.New(\"put event is nil.\")\n    }\n    if clientv3.EventTypePut != event.Type {\n        return errors.New(\"This function can only handle events of type put\")\n    }\nif len(event.Kv.Value) > 0 && string(event.Kv.Value) != masterEndpoint {\n    glog.Infof(\"Backend master endpoint changed from %s to %s\", masterEndpoint, string(event.Kv.Value))\n    masterEndpoint = string(event.Kv.Value)\n}\n\nsvr.Shutdown(ctx)\n\nwg.Add(1)\ngo run(masterEndpoint)\nreturn nil\n\n}\nfunc (h HttpServerWatchHandler) HandleDelete(event *clientv3.Event) error {\n    return nil\n}\nfunc main() {\n    flag.Parse()\n    defer glog.Flush()\nproperties := make(map[string]string)\nproperties[storage.PropertiesEtcdUrls] = *etcdUrls\nvar err error\netcdClient, err = storage.NewEtcdClient(properties)\nif err != nil {\n    glog.Fatalf(\"Init etcd client failed. error: %s\", err.Error())\n}\n\nwatcher, err = common.NewWatcher(etcdClient, []string{server.MasterEtcdKey}, HttpServerWatchHandler{})\nif err != nil {\n    glog.Fatalf(\"Failed to new watcher. error: %s\", err.Error())\n}\n\nfor {\n    if masterEndpoint, ok := getBackendServiceMasterEndpoint(); ok {\n        wg.Add(1)\n        go run(masterEndpoint)\n        break\n    } else {\n        glog.Error(\"Failed to get master endpoint, retrying....\")\n        time.Sleep(1 * time.Second)\n    }\n}\n\n}\n```\n. Sorry, the problem is  I start two svr ( 0.0.0.0:9090 and 0.0.0.0:9092 ) and gateway( listen 8090), though http request can arrive at gateway but svr did't trigger the  grpc method. It always present \"transport is closing\".  Does it mean svr  has problem with  grpc? or?. There are no other error messages, I tried use curl to call the rpc function, but all remind me \"transport is closing\",  if gateway can't connect server it will remind \"all sub connection failed\", so I think maybe use gateway to trigger rpc failed. . curl -H \"Content-Type:application/json\" -d '{\"module_name\":\"cos-cgi\", \"env_name\":\"dev\", \"category\":\"ap-shanghai\"}' -X POST -k http://118.89.226.213:8090/v1/getagentinfo/module, all did not work. \n. A long time later, it will show:\n upload completely sent off: 69 out of 69 bytes\n< HTTP/1.1 503 Service Unavailable\n< Content-Type: application/json\n< Date: Mon, 13 Aug 2018 02:27:50 GMT\n< Content-Length: 42\n< \n Connection #0 to host 118.89.226.213 left intact\nAdd the log to the svr port, It seems no problem. I start all server on one machine to test.\nI0813 10:36:48.370241   15174 main.go:67] current gateway server is: {:8090 0xc42007bb00  0s 0s 0s 0s 0 map[]   ... }\n.  Finally, found it works wrong with the raft connection, thank you very much .. ",
    "Arttii": "I can give a try, should we keep this open as a tracking issue?. I am a bit unsure how to write the test I cannot seem to find any examples how to actually pass in options with MethodDescriptorProto . ",
    "dayadev": "@vtolstov Thanks!. ",
    "yuhang2": "Hey @llvim , Do you find the solution for how to formatting proto file?. ",
    "kasuboski": "I just got this with v1.5.0. Is it fixed somewhere else?. This is using go1.11 outside of my gopath.\n```\ngo get -u github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway\ngo: finding github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway latest\ngithub.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway/descriptor\n../../go/pkg/mod/github.com/grpc-ecosystem/grpc-gateway@v1.5.0/protoc-gen-grpc-gateway/descriptor/services.go:146:49: opts.ResponseBody undefined (type *annotations.HttpRule has no field or method ResponseBody)\n```. I looked into it today. It seems like the go modules don't like having the top level dependencies like is captured with the Gopkg.lock in the root. Let me know if I misunderstood how it's currently working.\nI'm not going to continue looking into it for now, but might come back to it later.. Yeah I didn't read it...I ran the conversion on the repo and got the module file well enough, but then had issues with the build, namely it couldn't resolve a package for the module.\nMaybe it needs multiple modules for the different packages. I had tried that as well, but had issues with it still trying to download the source from github instead of using the higher level module.\nMy experience with it has only been with much simpler projects so I'm definitely interested to see how this works.. The error I get is\ncan't load package: package github.com/grpc-ecosystem/grpc-gateway: unknown import path \"github.com/grpc-ecosystem/grpc-gateway\": cannot find module providing package github.com/grpc-ecosystem/grpc-gateway\ngo.mod at the root\n```\nmodule github.com/grpc-ecosystem/grpc-gateway\nrequire (\n    github.com/ghodss/yaml v1.0.0\n    github.com/go-resty/resty v0.0.0-20180405024425-f8815663de1e\n    github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b\n    github.com/golang/protobuf v1.1.0\n    github.com/rogpeppe/fastuuid v0.0.0-20150106093220-6724a57986af\n    golang.org/x/net v0.0.0-20180502164142-640f4622ab69\n    golang.org/x/text v0.3.0\n    google.golang.org/genproto v0.0.0-20180808183934-383e8b2c3b9e\n    google.golang.org/grpc v1.11.3\n    gopkg.in/yaml.v2 v2.0.0-20170812160011-eb3733d160e7\n)\n```. Yeah it worked when I tried it now...must have had some old files somewhere.. ",
    "dv29": "Hi, I am facing this issue when I try to go get github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway\n. ",
    "eundoosong": "@johanbrandhorst I would like to work on this issue. \nDoes this mean below CloseNotifier should be removed?\ngo\n    if cn, ok := w.(http.CloseNotifier); ok {\n      go func(done <-chan struct{}, closed <-chan bool) {\n        select {\n        case <-done:\n        case <-closed:\n          cancel()\n        }\n      }(ctx.Done(), cn.CloseNotify())\n    }\n. ",
    "maros7": "@srikrsna are you planning on finishing the PR https://github.com/grpc-ecosystem/grpc-gateway/pull/752 related to this issue? Or would it be okay if I finished it?. Ideally I\u2019d like to follow the OpenAPI 2.0 standard which allows csv (default), ssv, tsv, pipe and multi. The latter is only for query params though. But I don\u2019t see how it would be possible to specify a \u201dseparator\u201d using the http annotation. As I see it I can implement/hard code csv and my assumption is that it would be sufficient for most people. The http annotation documentation also seems to state that this should be the way to do it if I interperet it and RFC 6570 (https://tools.ietf.org/html/rfc6570#section-3.2.2) correctly. On the other hand I guess it could be possible to have a custom annotation, like you have for the swagger generator (https://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-swagger/options/annotations.proto) to support specifying the \u201dseparator\u201d type. Yet another option could be to make it configureable via a parameter provided to the plug-in, e.g. \u201drepeated_field_separator=csv\u201d. What is your take?. Sadly swagger-codgen doesn't support arrays as path parameters, even if the 2.0 spec says that it is supported. See https://github.com/swagger-api/swagger-codegen/blob/v2.2.2/modules/swagger-codegen/src/main/resources/go/api.mustache#L45. It will only generate correct code for arrays as query parameters. See https://github.com/swagger-api/swagger-codegen/blob/v2.2.2/modules/swagger-codegen/src/main/resources/go/api.mustache#L85-L86. Nonetheless, I'll submit a PR for review.. All Travis jobs succeded except one with a failure not related to this PR. It says HTTP request sent, awaiting response... 403 Forbidden when fetching swagger-codegen.. Hmm, ok. Travis ran successfully on first commit, except the swagger codegen download thingy. Then I just added a missing test. But I will look into it.. I removed bin/ and vendor/, then re-ran tests (which also re-generates examples and runs dep/go install protoc) with make -B test. No errors here. . Travis ran with same result as master. I.e. one job ended with the same, unrelated failure:\nbash\n0.32s$ test \"${USE_BAZEL}\" = true || sh -c 'cd examples/browser && node ./node_modules/gulp/bin/gulp'\nmodule.js:538\n    throw err;\n    ^\nError: Cannot find module 'bower-logger'\n    at Function.Module._resolveFilename (module.js:536:15)\n    at Function.Module._load (module.js:466:25)\n    at Module.require (module.js:579:17)\n    at require (internal/module.js:11:18)\n    at Object.<anonymous> (/home/travis/gopath/src/github.com/grpc-ecosystem/grpc-gateway/examples/browser/node_modules/bower/lib/commands/index.js:2:14)\n    at Module._compile (module.js:635:30)\n    at Object.Module._extensions..js (module.js:646:10)\n    at Module.load (module.js:554:32)\n    at tryModuleLoad (module.js:497:12)\n    at Function.Module._load (module.js:489:3)\nThe command \"test \"${USE_BAZEL}\" = true || sh -c 'cd examples/browser && node ./node_modules/gulp/bin/gulp'\" exited with 1.. Rebased and squashed. All tests pass now. Thanks!. I agree. I chose to use the same logic as already existed for enums in query params, see https://github.com/grpc-ecosystem/grpc-gateway/blob/master/runtime/query.go#L339-L356. Do you know the rationale of supporting numerical, i.e. supporting that you input an enum by its index? From my perspective string support is sufficient and I'd like to update the convert Enum function to only support string. But please share any rationale behind supporting both first.. Not sure I follow. The convert Enum function will use the name->index map that is generated by the proto compiler, see e.g. grpc-gateway/examples/proto/examplepb/a_bit_of_everything.pb.go. That map is provided to the convert Enum function when conversion from path parameters to protobuf is done, see e.g. grpc-gateway/examples/proto/examplepb/a_bit_of_everything.pb.gw.go. So not sure exactly what you mean.. You are quite right. I will change this.. Of course ;) Question; any particular reason why bytes isn't implemented in proto2? Only been working with proto3 myself.. Agreed, this was before the change isPathParam in services.go. Will revert.. Good point.. copy pasta of the other slice conversion functions. Will change.. ",
    "robbert229": "This resolves #740 . is this done by doing make examples in the root directory? I did attempt to do this but there was a significant amount of changes that was generated that was unrelated to this bug.. I regenerated the examples! Thanks for the help. @johanbrandhorst I regenerated and rebased on master.\n@askurydzin done. \nOn another note, with the current structure of the program it is kind of hard to easily write a unit test for this. Normally if I found a bug and fixed it I would typically include a test to prevent regression. Would it be okay to potentially in another pr do a bit of refactoring (pull out some of the naming code into a separate function) and then write a test ensuring proper operation id calculation?. ah. I rebased onto the master on my fork and not on the upstream. Now I have actually rebased.... ok. Maybe now the pr is ready :).\nSome of the jobs failed because of failing to go get golint at the beginning. The rest of the jobs passed though.. It looks like the jobs all passed except one again. :/. Hmm. I am trying to figure out why it is failing. The build job for go 1.9 works fine, but for only does go 1.10 fail, and its because a different diff is generated. I don't have the expertise with Bazel, to understand what is going on here. Does anyone have any insight on what I might have done incorrectly?\ndiff\n@@ -452,9 +454,9 @@func _ABitOfEverything_OneofSizer(msg proto.Message) (n int) {\n // Nested is nested type.\n type ABitOfEverything_Nested struct {\n    // name is nested field.\n-   Name                 string                           `protobuf:\"bytes,1,opt,name=name\" json:\"name,omitempty\"`\n-   Amount               uint32                           `protobuf:\"varint,2,opt,name=amount\" json:\"amount,omitempty\"`\n-   Ok                   ABitOfEverything_Nested_DeepEnum `protobuf:\"varint,3,opt,name=ok,enum=grpc.gateway.examples.examplepb.ABitOfEverything_Nested_DeepEnum\" json:\"ok,omitempty\"`\n+Name                 string                           `protobuf:\"bytes,1,opt,name=name,proto3\" json:\"name,omitempty\"`\n+Amount               uint32                           `protobuf:\"varint,2,opt,name=amount,proto3\" json:\"amount,omitempty\"`\n+Ok                   ABitOfEverything_Nested_DeepEnum `protobuf:\"varint,3,opt,name=ok,proto3,enum=grpc.gateway.examples.examplepb.ABitOfEverything_Nested_DeepEnum\" json:\"ok,omitempty\"`\n    XXX_NoUnkeyedLiteral struct{}                         `json:\"-\"`\n    XXX_unrecognized     []byte                           `json:\"-\"`\n    XXX_sizecache        int32                            `json:\"-\"`. ",
    "mestrade": "Sorry if my message is not at the good place, but I've done a quick fix to make the operationID generated by default with the service name instead of only the call name. Fix look like\nif bIdx == 0 {\n                    operationObject.OperationID = fmt.Sprintf(\"%s%s\", svc.GetName(), meth.GetName())\n                } else {\n                    // OperationID must be unique in an OpenAPI v2 definition.\n                    operationObject.OperationID = fmt.Sprintf(\"%s%s%d\", svc.GetName(), meth.GetName(), bIdx+1)\n                }\nIs this something you could consider ? if yes I can do a PR . Files generated ! i will have to understand more all the ecosystem !\nI have few other PR to do in the next weeks. Agree that it may break something, but for now, if you have two rpc call with the same name in two different services, the output swagger is not openapi compliant.. I have to come back on this in the next weeks...\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nMatthieu Estrade\n\nLe 28 janv. 2019 \u00e0 09:15, Tino Rusch notifications@github.com a \u00e9crit :\n@mestrade @johanbrandhorst Are there any updates on this issue?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "fn-code": "@johanbrandhorst its not work. just getting this \n\n--grpc-gateway_out: echoserver/echoserver.proto: options: mode is required\n. @tmc this ??\nsyntax = \"proto3\";\npackage echoserver;\nimport \"google/api/annotations.proto\";\nmessage EchoRequest {\n  string message = 1;\n}\nmessage EchoResponse {\n  string message = 1;\n}\nmessage Heartbeat {\n  enum Status {\n    UNKNOWN = 0;\n    OK = 1;\n  }\n  Status status = 1;\n}\nmessage Empty {}\nservice EchoService {\n  rpc Echo(stream EchoRequest) returns (stream EchoResponse) {\n    option (google.api.http) = {post: \"/echo\", body: \"*\"};\n  }\n  rpc Stream(Empty) returns (stream EchoResponse) {\n    option (google.api.http) = {get: \"/echo\"};\n  }\n  rpc Heartbeats(stream Empty) returns (stream Heartbeat) {\n    option (google.api.http) = {post: \"/heartbeats\"};\n  }\n}. @johanbrandhorst @tmc \nprotoc -I /usr/local/include -I. -I $GOPATH/src -I $GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis --plugin=protoc-gen-grpc-gateway=$GOPATH/bin/protoc-gen-grpc-web --grpc-gateway_out=logtostderr=true:. echoserver/echoserver.proto\n. @johanbrandhorst  i got this message : \nprotoc-gen-grpc-gateway: program not found or is not executable\n--grpc-gateway_out: protoc-gen-grpc-gateway: Plugin failed with status code 1. \nthis  command protoc -I /usr/local/include -I. -I $GOPATH/src -I $GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis --plugin=protoc-gen-grpc-gateway=$GOPATH/bin/protoc-gen-grpc-web --grpc-gateway_out=logtostderr=true:. echoserver/echoserver.proto i got from this #205 but when i running this command i got this \n--grpc-gateway_out: echoserver/echoserver.proto: unsupported options: logtostderr . @johanbrandhorst i got same message \nprotoc-gen-grpc-gateway: program not found or is not executable\n--grpc-gateway_out: protoc-gen-grpc-gateway: Plugin failed with status code 1.. @johanbrandhorst okey thank you for your help :+1: . \n",
    "SXerox007": "Thanks its just save my day. @johanbrandhorst . ",
    "carrelld": "@princejha95 What I did to start debugging was to set-up a failing test, but that was a bit harder than expected given I wan't sure how to construct a valid File object. Here's my best guess though at a fix:\nThe index of the service gets passed to https://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-swagger/genswagger/template.go#L743 the values being 0 on the first pass and 1 on the second pass. Within that function, there's https://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-swagger/genswagger/template.go#L1238 which truncates the path from int32{6, a, 2, b} to int32{2, b}. This will end up returning true for any SourceCodeInfo_Location that is has a path matching Service -> Method (with matching name). It will match the first one and return the comments ignoring the second one.\nI think the missing piece is the a that gets truncated from the path. I think a represents the service index. So at https://github.com/grpc-ecosystem/grpc-gateway/blob/master/protoc-gen-swagger/genswagger/template.go#L1208, the line should instead be \nif paths[0] != serviceProtoPath || paths[2] != methodProtoPath || paths[1] != typeIndex {\nYou should be able to make this change and test it against your proto file. I'd do it myself, but I'm having trouble getting code generation working at all.. ",
    "Sahasrara": "Sure thing:\n```protobuf\nsyntax = \"proto3\";\npackage test;\nimport \"google/protobuf/empty.proto\";\nimport \"google/protobuf/timestamp.proto\";\nimport \"google/api/annotations.proto\";\nimport \"protoc-gen-swagger/options/annotations.proto\";\noption go_package = \"gitlab.myteksi.net/gophers/go/ads/admanager/pb\";\noption (grpc.gateway.protoc_gen_swagger.options.openapiv2_swagger) = {\n  base_path: \"/test\";\n  info: {\n    title: \"Test\";\n    version: \"1.0\";\n  };\n};\nservice Admanager {\n  rpc GetCooldude(GetCooldudeRequest) returns (GetCooldudeResponse) {\n    option (google.api.http) = {\n      get: \"/Cooldudes/{CooldudeID}\"\n    };\n  }\n}\nmessage Cooldude {\n  uint32 CooldudeID = 1;\n  string Name = 2;\n  bool Deleted = 3;\n  bool Active = 4;\n  uint32 ExternalCooldudeID = 16;\n}\nmessage GetCooldudeRequest {\n  uint32 CooldudeID = 1;\n}\nmessage GetCooldudeResponse {\n  Cooldude Cooldude = 1;\n}\n```. I'm not sure I understand.  The request object contains a path parameter, but it's not \"a path parameter.\"  As for the generated swagger file, I see everything I expect except no GetCooldudeRequest.\nThis is missing:\nGetCooldudeRequest:\n    description: GetCooldudeRequest is the request object for GetCooldude.\n    properties:\n      CooldudeID:\n        format: int64\n        type: integer\n    type: object. I don't think I understand?  The response objects are all generated correctly.  The requests are all missing.  You're saying that's not a bug?  This happens when there are query parameters, it happens when there are path parameters.  Anything without a body will not have it's request object show up in swagger.. Doesn't that leave query parameters undocumented?  In my case, the swagger is being used to generate a client for the API and unless these requests are documented, it has no idea how to construct requests.. OK i'm going to circle back with the folks who wrote the generator.  Thanks for your help!. ",
    "AlmogBaku": "\nI need to test the POST, but it doesn\u2019t likely to be problematic imo.\nI suggest to check for the gogo\u2019s custom type and to use the MarshalJSON\nand Unmarshal json funcs as specified here as \u201cconvertFunc\u201d\nhttps://github.com/gogo/protobuf/blob/master/custom_types.md\nOn Thu, 13 Sep 2018 at 0:05 Johan Brandhorst notifications@github.com\nwrote:\n\n\nThanks for raising this issue! I like this idea. I think the problem is\nspecific to path and query parameters like this one. Can you confirm\nwhether this works when using a POST where the ID is in the body?\nI'm not sure how we would tell the query/path resolver where to look for\ndecoding. Any ideas?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc-ecosystem/grpc-gateway/issues/754#issuecomment-420797546,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAGCpoP-T79_RmfMB53nS3qN31rYR0qqks5uaXctgaJpZM4Wl33t\n.\n-- \n\nhttp://www.rimoto.net/\nwww.rimoto.com http://www.rimoto.net/\nAlmog Baku\nCTO & Cofounder \nMobile: +972.50.2288.744\nSocial:  * http://www.facebook.com/AlmogBaku\nhttp://www.linkedin.com/in/almogbaku*\n. ",
    "ogimenezb": "I have generated the same mod file and can build both.\nWhere are you having the problem?\n. Yep, the only thing is the makefile will be broken becase it relays on GOPATH. Only if go 1.11 and not working from GOPATH. \nHave tried a couple of things, but still no luck.\n. ",
    "timoohr": "Some additional observations while trying to make the repo go gettable again:\nWhen calling go get on a migrated version of gprc-gateway the original issue (#731) pops back up again. For some reason go seems adamant to resolve to an older version of google.golang.org/genproto. Was able to fix this by pinning it with a replace in the mod file:\nreplace google.golang.org/genproto => google.golang.org/genproto v0.0.0-20181101192439-c830210a61df\n\nThere also seems to be an issue with resty:\ngo: github.com/go-resty/resty@v1.10.1: parsing go.mod: unexpected module path \"gopkg.in/resty.v1\"\n\nI suspect this might be an issue with their go.mod file, since they specified the module path as gopkg.in/resty.v1 instead of gopkg.in/resty. Was able to circumvent this by removing the dependency, but this will obviously break the examples.. Yes, I think you may be right. \nI think the issue is that go mod requires modules to have the major version in their module/import path, except for v0 and v1, which must not appear in the path, except for modules hosted on gopkg.in, where this is allowed (and in a slightly different format, as well). (source)\nSo it would seem like resty needs to be imported from gopkg.in for go mod to work, since they specified a v1 version in their module path (or may be it's just because their module path differs from the import path in general).\nNot sure about go-swagger, but in terms of go getting protoc-gen-grpc-gateway and protoc-gen-swagger replacing the resty github imports in the examples with gopkg.in imports seems to suffice.. ",
    "the-destro": "Also one question I have is how the include is supposed to work when we compile the protobuf files via protoc?. > I don't think we can declare proto file or protoc dependencies with go modules.\nhmm, makes things tricky especially given the instructions from the README:\n\nprotoc -I/usr/local/include -I. \\\n  -I$GOPATH/src \\\n  -I$GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\\n  --plugin=protoc-gen-grpc=grpc_ruby_plugin \\\n  --grpc-ruby_out=. \\\n  path/to/your/service.proto\n\nIf there is no GOPATH how do we run that, or how should we run that. Aye, I think this is mainly a documentation issue. I like @izumin5210 's answer and can submit a PR to update the docs. Is this something that we want to encourage users to do though? Hence the second half of my question, the should part :). ",
    "litichevskiydv": "Hello, guys! We are using gRPC with Node.js and generating swagger documentation by protoc-gen-swagger. Our REST gateway is hosted by express and we met the same problem, as was described. @johanbrandhorst, do you have plans to implement configurable behavior for option collectionFormat?. ",
    "bmperrea": "@johanbrandhorst I would like to take a look. My first guess is that we don't want to be setting a collectionFormat in schemaOfField but only in queryParams. I'll try to reproduce and then see if that fixes it.. Thanks @achew22 ! A docker run to generate these files will be great - I was having trouble figuring out how to get the right version of swagger-codegen (2.2.2).\nWhen I run the command you posted I get\ncan't load package: package github.com/golang/protobuf/protoc-gen-go: cannot find package \"github.com/golang/protobuf/protoc-gen-go\" in any of:\n    /usr/local/go/src/github.com/golang/protobuf/protoc-gen-go (from $GOROOT)\n    /go/src/github.com/golang/protobuf/protoc-gen-go (from $GOPATH)\nI have protoc-gen-go installed on my local, but I guess the docker run is missing a dependency.. @johanbrandhorst - that queryParams test case was in the previous PR as well. But to your point, I just added another commit that adds stricter (and cleaner) deepEqual-based testing of the behavior of schemaOfField that would have failed on my previous PR - here's the output from when I tried it (you can pick out the \"only specify multi in the method queryParams\" commit on this branch and then run the tests as shown to repeat):\n$ go test ./protoc-gen-swagger/genswagger/\n--- FAIL: TestSchemaOfField (0.00s)\n    template_test.go:1176: Expected schemaOfField(name:\"repeated_primitive_field\" label:LABEL_REPEATED type:TYPE_STRING ) = {{array   [] 0xc00014f680 [] } <nil> <nil>   <nil> false 0 0 false 0 false 0 0  0 0 false 0 0 [] }, actual: {{array   [] 0xc00014fa00 [] } <nil> <nil>   <nil> false 0 0 false 0 false 0 0  0 0 false 0 0 [] multi}\n    template_test.go:1176: Expected schemaOfField(name:\"repeated_wrapped_field\" label:LABEL_REPEATED type:TYPE_MESSAGE type_name:\".google.protobuf.StringValue\" ) = {{array   [] 0xc00014f700 [] } <nil> <nil>   <nil> false 0 0 false 0 false 0 0  0 0 false 0 0 [] }, actual: {{array   [] 0xc00014fd00 [] } <nil> <nil>   <nil> false 0 0 false 0 false 0 0  0 0 false 0 0 [] multi}. ",
    "Everlag": "This would be very useful for us, is there an ETA on this being merged?. Will do. I don't have the bandwidth at the moment but have set aside some time next week.. ",
    "orjanbruland": "One solution could be to introduce a ServeMuxOption to allow Patterns created without a verb to match requests using a verb. This way, using colons in IDs is an opt-in feature instead of breaking backward compatibility.. ",
    "wdec": "Sounds like a nice feature I could add. I'll get looking into it.. ",
    "johnchildren": "Have created a pull request for the change, but I am unsure how best to provide automated tests. I also interested to see how the default value looks when used with streaming responses.. I couldn't see an obvious place to add this in the tests, but they seemed to pass locally regardless of the change. It may be worth also updating the example swagger files as they currently have empty descriptions.\nedit: based on CI I was obviously running the wrong command, will look into updating the generated files.. ",
    "eldad87": "@rexlv, \nHere is my implementation of @johanbrandhorst's suggestion, a working example of Gin-gonic and grpc-gateway.. ",
    "yangliCypressTest": "The struct of TestRequest is like below one, it is auto-generated.\ntype TestRequest struct {\n    Id    int64                 protobuf:\"varint,1,opt,name=id\" json:\"id,omitempty\"\n    Items *google_protobuf1.Any protobuf:\"bytes,2,opt,name=items\" json:\"items,omitempty\"\n}. Pass below through request' body, @johanbrandhorst \n{\n    \"items\": {\n         \"include\": [{\"id\": 3, \"test_name\": \"Test_Value\"}]\n    }\n}. Sorry, I didn't find any official example from documentation, could you please supply one for me to refer to? thx @johanbrandhorst . thx. ",
    "joelurraco": "I signed it. @johanbrandhorst yes, but I've just signed the CLA with both accounts. ",
    "joelclouddistrict": "Can this be merged?. ",
    "wongoo": "@achew22  I want to do authorization for requests only in apigateway , and grpc client interceptor can do that. It's what I'm looking for. So it's not needed add intercepter in apigateway. Thanks!. Do authorization for requests in apigateway instead of grpc servers is more efficient , especially when grpc servers calling each others. \nGrpc client interceptor is a way to implement authorization, but it's called after the restful request proxied to grpc request.  If the authorization can be done before proxying, performance maybe better.\n@achew22  what do u think?. @johanbrandhorst  may be call it business apigateway, consider it as part of business.  And aws apigatway also has authorization in it. . ",
    "trusch": "@mestrade @johanbrandhorst Are there any updates on this issue?. ",
    "Mnw2212": "Hi @johanbrandhorst , I faced the same issue and it took me a couple of hours of googling to get here.\nWould love to update the documentation. It might save time for a lot of people.\n. ",
    "jgiles": "Thank you! It would also be nice to have protoc-gen-swagger in addition to protoc-gen-grpc-gateway.\nI'm planning to publish an asdf https://github.com/asdf-vm/asdf plugin for installing versions by release tag, so ideally we'd have pre-compiled binaries for each release (existing and future). But without automation I guess that gets a little tedious :-/. I've never used CircleCI before, but it looks like it could be configured to publish the releases? https://circleci.com/blog/publishing-to-github-releases-via-circleci/. Thanks for so quickly responding an merging a change!\nI've tried this change out, and while it does make the names more predictable, the Swagger code generators I have tried pass the much-longer names on to the client code (e.g. generated Typescript from the resulting Swagger file has names like MyFullPackageMessageSubmessage).\nIt still seems like there should be a way to generate more concise names, especially for the simple case of only one package.\nI've tested omitting the package name declaration entirely, and that appears to \"work\" for this simple case, though it feels wrong and likely boxes me in on importability.. Well, I think all of the enums in my proto files would benefit from this behavior, but there could be enums where there is a natural default (or the zero value is otherwise valid/intentional), and you could imagine some project having a mix of those two cases.\nBut perhaps a flag would be sufficient for the vast majority of cases - if you control the enum definitions, you can structure them consistently.. If a PR for adding the generator command-line flag would be well-received, I can try to take a stab at it at some point :-). ",
    "SpikesDivZero": "I signed it!. > I'm not sure how, but it seems like we've ended up with some travis files added in this PR.\nThese were auto-generated by the docker image used to generate changes, as advised by the CONTRIBUTING.md (jfbrandhorst/grpc-gateway-build-env). Docker image inspect shows the following relevant details:\n    \"Id\": \"sha256:2011172b884689f4c0f4dcbb50d408abb1acccbcd002e0a054e0492dbcce94a8\",\n    \"RepoTags\": [\n        \"jfbrandhorst/grpc-gateway-build-env:latest\"\n    ],\n    \"RepoDigests\": [\n        \"jfbrandhorst/grpc-gateway-build-env@sha256:81527758368154998c24d310501e92f8b805987ee164d7050b7be6d6b4deffea\"\n    ],\n    \"Created\": \"2018-10-09T21:58:26.512757165Z\",\n\nI'm more than happy to remove those manually, but I suspect they'll reappear for others following the provided instructions.. Fixed both issues, re-pushed, and all tests look green.. I'm not opposed to stripping this entirely, but if we go that route, it seems like we should also completely remove support for Go < 1.7 by removing the UseRequestContext and the CLI flag so we don't end up with any inadvertent leaks/regressions.\nI'd elected to go this way at first to minimize the risk of silent regressions with older Go releases, since the existence of this flag seems to imply some degree support for older Go releases.\nThoughts?. I see, and entirely fair! I'd not considered that view point. Adjusting.. ",
    "drigz": "@johanbrandhorst I'm happy to try. Would you like me to do it in this or another PR?. @achew22 I've compared gazelle v0.10.1 vs buildifier (build from oct 24) and found the following:\n\nbuildifier can reformat WORKSPACE but gazelle doesn't (by default at least)\nbuildifier can reformat *.bzl but gazelle doesn't (by default at least)\ngazelle (unsurprisingly) wants to make a few changes to the go rules (see :gazelle_diff output from last run)\nneither sets their exit code based on differences (gazelle issue)\n\nDo you think the CI check should fail when Gazelle (/buildifier) would make a change?. I'm assuming that, for now, running both buildifier and gazelle is useful, so I've added it to the CI.\n@achew22 I followed your lead in preserving --batch, although we could probably make the presubmit faster by dropping --batch and combining :bazel_build and :bazel_test (since test //... implies build //...).. I tried combining them into one step, which is more efficient but means that if there is a lint error, devs won't see build/test failures. I split them up into separate lint and test steps, but if you want to save ~2 minutes of CI time, you could consider combining them.. @johanbrandhorst Let's do it before if you don't have any comments here. They shouldn't conflict. (famous last words). TBH I'm hoping someone who understands this \"enums inside maps\" issue will appear and tell me what to do. I don't want to change the behavior of grpc-gateway without understanding the consequences.\nAlternatively, we may be able to keep the old behavior by setting EnumsAsInts appropriately for marshalling and unmarshalling. However, I don't have a lot of time to spend on this, as this is \"just\" a cleanup task on my side.. I wasn't able to easily fix node_test, as changing ints to strings introduced another error. Additionally, I need to regenerate the examples, which I haven't worked out how to do yet.. Yep, looks like I gave up too quickly. Thanks for the encouragement :). @razamiDev Could you install Bazel and run bazel run :gazelle_fix?\n@johanbrandhorst I'll try to make this clearer in the CI logs (#807, in progress). Do you think we need an invocation that works with docker like in CONTRIBUTING.md, or is it OK to ask contributors to install Bazel?. @johanbrandhorst We need to use a different image to get Bazel - CircleCI uses l.gcr.io/google/bazel - so we'll need two docker run commands. The simplest I could come up with is:\ndocker run -itv $(pwd):/grpc-gateway -w /grpc-gateway --entrypoint /bin/bash --rm \\\n    l.gcr.io/google/bazel -c 'bazel run :gazelle_fix; bazel run :buildifier'\n@razamiDev If you don't have Bazel installed, perhaps you could try that command and let me know if it works for you?. @razamiDev Thanks for following up! It looks like gazelle has rewritten the dependencies to point an vendor, which isn't what we want. Could you rebase on top of master to pull in #807? That tells gazelle to ignore vendor.. @razamiDev I think the CLA bot is unhappy because some commits from master have made it into the pull request:\n\nYou may be able to fix it by running:\ngit checkout -b feature/patch2-rebased master\ngit cherry-pick 398e4356300c19959d4cc65668975b2254b06b51\nbazel run :gazelle_fix\ngit commit -am \"Regenerate BUILD files\"\nAs @johanbrandhorst you may need to create a new pull request so the CLA bot can run again.. Don't mind me, just trying to get the CI config valid.. @achew22 Could you review this? Hopefully it'll help avoid confusion like in #806.. @johanbrandhorst Created #817 to avoid noise on this PR.. >  if the gazelle_fix command updates Go dependencies in the background we risk running out of sync with our Gopkg.toml\nIt doesn't, so that shouldn't be a problem. Whether using go.mod or not, it will still be manual, although using go.mod could streamline the manual process.\n:gazelle_fix updates the edges in the dependency graph, as well as adding new rules within this repository, but it won't change the versions of external dependencies. I don't know exactly how go.mod will be used by Gazelle, but I suspect it'll be a different command to the one used by :gazelle_fix.. I honestly didn't think about using go get to pull gazelle and buildifier. Is there a way to pin the versions to make sure contributors are running the same versions as in CI?. It's unfortunately not safe to use tip for Gazelle, as its generated code is tied to the version of rules_go in WORKSPACE.\nIIUC, if it's required in Gopkg.toml, it'll need to be installed with go install - would you put that in Makefile?. It does, thanks for pointing that out. I've updated it and rerun dep ensure, is there anything else to do?. I think so - I assumed it was fixing a bug since the test seemed odd to me before.\nIn master, the behavior is different in two tests:\nTestJSONPbMarshal tests:\n- if EnumsAsInts == true, assert strings.Contains(json, \"ONE\")\n- if EnumsAsInts == false, assert !strings.Contains(json, \"ONE\")\nTestJSONPbEncoder tests:\n- if EnumsAsInts == true, assert !strings.Contains(json, \"ONE\")\n- if EnumsAsInts == false, assert !strings.Contains(json, \"ONE\")\nThe change is in the last case, but I'm not 100% sure why this changed or what the behavior ought to be. Do you have any insight into this area?. The node_tests make this look more like a bug - there's a TODO(yugui) Support enum by name before map_value: { a: 1, b: 2 }. If you change the digits to strings, you get unknown value \\\"TWO\\\" for enum grpc.gateway.examples.examplepb.NumericEnum. So it seems that not supporting strings is almost intentional...\nMaybe we need to change the configuration such that jsonpb will continue to have the old behavior.\n@achew22, do you know anything about this code?. FYI: I suspect this PR is the cause: https://github.com/golang/protobuf/pull/645\nIt looks like the grpc-gateway tests rely on behavior that golang/protobuf considered broken (enum values in maps).. gazelle_dependencies is still loaded from bazel_gazelle//:deps.bzl, along with go_repository. The only change here is that two load() statements have been consolidated into one.. The repository defined by repositories.bzl is obsolete, having been replaced by @go_googleapis (created by go_rules_dependencies()). Since repositories.bzl is unable to call go_rules_dependencies() (https://github.com/bazelbuild/bazel/issues/1550) there's nothing left for it to do.. I'm not a go dep but I don't think this change introduced the problem, as even before the version was constrained and would have conflicted with go-grpc-middleware's constraint. @javasgl Maybe you need to use an override to resolve the difference?. ",
    "ChenLingPeng": "I signed it!. Since most http framework support GET with body, I think there's also some scenarios user would like to use with it. It is better if we support this.. I have an api spec which defined that every reqeust(include GET/DELETE) should have a requestId in request body and the response body will carry back this requestId.. we also use go-restful as our router and it support read from GET request body. May I ask how to send back header in grcp-gateway?\ni.e \nI send a request with header 'X-Request-Id', I want get it from response\nI tried set some function when NewServeMux, but it doesn't work. In my case, I have a request\nGET /v1/clusters?offset=0&limit=10\nand request Body:\n{\"requestId\":\"xxx-xxx-xxx\"}\n. ```\nmessage TsfListClusterRequest {\n    string requestId = 1;\n    int32 offset = 2;\n    int32 limit = 3;\n}\nmessage TsfListClusterResponse {\n    int32 code = 1;\n    string message = 2;\n    string requestId = 3;\n    TsfListClusterData data = 4;\n}\nrpc ListClusters(TsfListClusterRequest) returns (TsfListClusterResponse) {\n    option (google.api.http)  = {\n        get: \"/tsf/v1.0/clusters\"\n        body: \"*\"\n    };\n}\n``. when body is set to \"request_id\", the generate code ismarshaler.NewDecoder(req.Body).Decode(&protoReq.RequestId)and will return an unmarshal error..{\"error\":\"json: cannot unmarshal object into Go value of type string\",\"code\":3}`. ",
    "mayankcpdixit": "I signed it!. Sure.\nedit: Here it is: @johanbrandhorst https://github.com/grpc-ecosystem/grpc-gateway/pull/816. @johanbrandhorst Right. Removed those. . > That bazel fix command is really impressive, wow! I'll merge this once the build finishes :).\nAgree. It finished smoothly though it took really long. I felt there should be easier way so I don't have to download heavy docker images. Maybe use multistage build. idk. just thinking out loud. . No problem @johanbrandhorst . I'm using it for one of my work item. So I had to make it work anyway.. @achew22 @johanbrandhorst These values don't really exist in runtime package. \nSo this change only helps in generating reverse proxy (gw.go code) files. It's not sufficient to support \"adding protobuf wrappers in url template option\" (https://github.com/grpc-ecosystem/grpc-gateway/issues/808).. StringValue, BytesValue, Int32Value... etc don't.. It'll work.But I'm afraid it'll defeat the purpose of using StringValue in the first place. It'll just look like we're providing support for StringValue but internally we'll just be giving them String.\nCorrect me if I'm wrong.. Oh! If that's the case I'll check if I can make it work.. Yes makes sense.. @johanbrandhorst seems it's still broken.\nI try doing go install where my *.pb.gw.go files are.\nThrows: cannot assign string to protoReq.Domain (type *wrappers.StringValue) in multiple assignment. One way I could find is to update our runtime/convert.go to support StringValue.\nfunc StringValue(val string) (*wrappers.StringValue, error) {                                                                                \n        return &wrappers.StringValue{Value: val}, nil                                                                                        \n}\nand keep the map here in types.go as:\n\".google.protobuf.BoolValue\":   \"runtime.BoolValue\". ",
    "koron": "I signed it!. I see very well. Thank you for explaing it. \ud83d\ude47 . ",
    "micnncim": "I signed it!. ",
    "stuartgrigg": "\nThanks for your contribution! Can you look into the build failure?\n\nYeah sure - I think it was me not updating the bazel file. The ci/circleci: generate failure doesn't look like it's from my change.. @tmc I could remove the call to NewDoubleArray and set the fields directly so that only code in the template.go file is tested.. ",
    "burov": "I signed it!\n\nFrom: googlebot notifications@github.com\nSent: Friday, November 30, 2018 5:19:16 PM\nTo: grpc-ecosystem/grpc-gateway\nCc: Aliaksei Burau; Author\nSubject: Re: [grpc-ecosystem/grpc-gateway] Add constant behavior to protoc-gen-swagger (#819)\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://cla.developers.google.com/https://urldefense.proofpoint.com/v2/url?u=https-3A__cla.developers.google.com_&d=DwMFaQ&c=CWsWqoUynJrLESJQduOsxQ&r=9omY3yOO_TyAH1aFIvSunMH_Frw-UQHGmFuE0I3GvVI&m=dbn0WbTzs6JRfvv1XlzwU96fpNbNLeptgSv0Bse2FQw&s=VLQkrfY1vptszvn0NoMITpq1DkyjQ4MoutyXPt6698w&e= to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA datahttps://urldefense.proofpoint.com/v2/url?u=https-3A__cla.developers.google.com_clas&d=DwMFaQ&c=CWsWqoUynJrLESJQduOsxQ&r=9omY3yOO_TyAH1aFIvSunMH_Frw-UQHGmFuE0I3GvVI&m=dbn0WbTzs6JRfvv1XlzwU96fpNbNLeptgSv0Bse2FQw&s=Xn0AJW-qeHJFz1lGbBOMplefqRgQhxmEFsHJxF2F-o0&e= and verify that your email is set on your git commitshttps://urldefense.proofpoint.com/v2/url?u=https-3A__help.github.com_articles_setting-2Dyour-2Demail-2Din-2Dgit_&d=DwMFaQ&c=CWsWqoUynJrLESJQduOsxQ&r=9omY3yOO_TyAH1aFIvSunMH_Frw-UQHGmFuE0I3GvVI&m=dbn0WbTzs6JRfvv1XlzwU96fpNbNLeptgSv0Bse2FQw&s=iUJkPDoX61daHqoQMqEV07WaRO31aacvJqzQh5ZGkFs&e=.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoothttps://urldefense.proofpoint.com/v2/url?u=http-3A__go_cla-23troubleshoot&d=DwMFaQ&c=CWsWqoUynJrLESJQduOsxQ&r=9omY3yOO_TyAH1aFIvSunMH_Frw-UQHGmFuE0I3GvVI&m=dbn0WbTzs6JRfvv1XlzwU96fpNbNLeptgSv0Bse2FQw&s=ChLA7SdFVqfDX825HWuBc6J69WQkRJgcGf8BQTUZWEg&e= (Public versionhttps://urldefense.proofpoint.com/v2/url?u=https-3A__opensource.google.com_docs_cla_-23troubleshoot&d=DwMFaQ&c=CWsWqoUynJrLESJQduOsxQ&r=9omY3yOO_TyAH1aFIvSunMH_Frw-UQHGmFuE0I3GvVI&m=dbn0WbTzs6JRfvv1XlzwU96fpNbNLeptgSv0Bse2FQw&s=CAZpnIh-V8RbrzPGF6sPvT7rgw_ISb2M78bfZKsi-Xw&e=).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA datahttps://urldefense.proofpoint.com/v2/url?u=https-3A__cla.developers.google.com_clas&d=DwMFaQ&c=CWsWqoUynJrLESJQduOsxQ&r=9omY3yOO_TyAH1aFIvSunMH_Frw-UQHGmFuE0I3GvVI&m=dbn0WbTzs6JRfvv1XlzwU96fpNbNLeptgSv0Bse2FQw&s=Xn0AJW-qeHJFz1lGbBOMplefqRgQhxmEFsHJxF2F-o0&e= and verify that your email is set on your git commitshttps://urldefense.proofpoint.com/v2/url?u=https-3A__help.github.com_articles_setting-2Dyour-2Demail-2Din-2Dgit_&d=DwMFaQ&c=CWsWqoUynJrLESJQduOsxQ&r=9omY3yOO_TyAH1aFIvSunMH_Frw-UQHGmFuE0I3GvVI&m=dbn0WbTzs6JRfvv1XlzwU96fpNbNLeptgSv0Bse2FQw&s=iUJkPDoX61daHqoQMqEV07WaRO31aacvJqzQh5ZGkFs&e=.\nThe email used to register you as an authorized contributor must also be attached to your GitHub accounthttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_settings_emails&d=DwMFaQ&c=CWsWqoUynJrLESJQduOsxQ&r=9omY3yOO_TyAH1aFIvSunMH_Frw-UQHGmFuE0I3GvVI&m=dbn0WbTzs6JRfvv1XlzwU96fpNbNLeptgSv0Bse2FQw&s=aqX3nUC4C6-tqrotWSWJFU8UVjw8P9EuMEymCd-DniY&e=.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_grpc-2Decosystem_grpc-2Dgateway_pull_819-23issuecomment-2D443216943&d=DwMFaQ&c=CWsWqoUynJrLESJQduOsxQ&r=9omY3yOO_TyAH1aFIvSunMH_Frw-UQHGmFuE0I3GvVI&m=dbn0WbTzs6JRfvv1XlzwU96fpNbNLeptgSv0Bse2FQw&s=P00jUO8zMhcSR9PlvrYlfgFC1OHTX2CU4wO_aC23PZw&e=, or mute the threadhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AhuYO7lQLvEhuUZ0J0UCePoZt-5FdqsP-2DDks5u0T5kgaJpZM4Y7wBO&d=DwMFaQ&c=CWsWqoUynJrLESJQduOsxQ&r=9omY3yOO_TyAH1aFIvSunMH_Frw-UQHGmFuE0I3GvVI&m=dbn0WbTzs6JRfvv1XlzwU96fpNbNLeptgSv0Bse2FQw&s=LzLITZgq5gZTchJdCtmszxquGkewRwy-nKpbBQoLuJ8&e=.\n. @johanbrandhorst see https://github.com/grpc-ecosystem/grpc-gateway/blob/master/third_party/googleapis/google/api/http.proto#L230\n . It more about another cases explained in description to my PR\npost: /{id.app_name=app}/{id.resource_type=foo} on gateway level it translate to /app/foo but on swagger to /{id.app_name}/{id.resource_type} if you have to similar handlers with different {id.resource_type} in swagger schema conflict will happen . @johanbrandhorst I open PR #819 and i accidentally close it i reopen it because it seems like an issue in protoc-gen-swagger , we can create two separated api like /{id.application_name=app}/{id.resource_type=foo} and /{id.application_name=app}/{id.resource_type=bar} on gateway level it works fine but on swagger schema both api seems /{id.application_name}/{id.resource_type} and it prevent to conflict on protoc-gen-swagger, i think it seems like an issue because if it possible on gateway level and work fine it must be possible in swagger schema and work fine too, Correct me if it wrong, Thanks. @johanbrandhorst Yes, you understand me, ok thanks,  i will do it . @johanbrandhorst  I understand it, but comment before type rendered to Definition as description and title, on https://editor.swagger.io/ title always rendered as definition name and.\n// A generic empty message that you can re-use to avoid defining duplicated\n// empty messages in your APIs. A typical example is to use it as the request\n// or the response type of an API method. For instance:\n//\n//     service Foo {\n//       rpc Bar(google.protobuf.Empty) returns (google.protobuf.Empty);\n//     }\n//\n// The JSON representation for `Empty` is empty JSON object `{}`.\nmessage Empty {}\nFirst part(Before first blank line) of comment will be stored in field title \n// A generic empty message that you can re-use to avoid defining duplicated\n// empty messages in your APIs. A typical example is to use it as the request\n// or the response type of an API method. For instance:\nSecond part(After first blank line) will be stored in field description \n//     service Foo {\n//       rpc Bar(google.protobuf.Empty) returns (google.protobuf.Empty);\n//     }\n//\n// The JSON representation for `Empty` is empty JSON object `{}`.\nSwagger UI replace definition name and render title field instead of it, and it looks ugly\nMy question was is this some contract all before blank line we render to title and  remain as description on my opinion i think whole comment must rendered as description.\nThank you.. ",
    "kellycampbell": "We have python gRPC backends which implement the services defined with rest annotations and documented using these swagger annotations.\nIt only needs them because they're imported and referenced by our services .profo file. If I run protoc for python it errors out if it can't find these options protos. Nothing in our python code actually needs the annotations.\nIdeally, the py_proto_library rule should be able to use the proto provider from the proto_library to find the files, but currently the one we're using doesn't. (https://github.com/pubref/rules_protobuf/tree/master/python). ",
    "ZhiqinYang": "/check. @johanbrandhorst I think maybe not, I just wanna to  compleate handle response with myself, it not stop when called WithForwardResponseOption, it will print more data which I don't need! \n128        if err := handleForwardResponseOptions(ctx, w, resp, opts); err != nil {   \n129                 HTTPError(ctx, mux, marshaler, w, req, err)  \n130                 return  \n131         }  \n132         var buf []byte\n133         var err error\n134         if rb, ok := resp.(responseBody); ok {\n135                 buf, err = marshaler.Marshal(rb.XXX_ResponseBody())\n136         } else {\n137                 buf, err = marshaler.Marshal(resp)\n138         }\n139         if err != nil {\n140                 grpclog.Infof(\"Marshal error: %v\", err)\n141                 HTTPError(ctx, mux, marshaler, w, req, err)\n142                 return\n143         }\n144\n145         if _, err = w.Write(buf); err != nil {\n146                 grpclog.Infof(\"Failed to write response: %v\", err)\n147         }\nThe extra data (buff), i really don't want !. @johanbrandhorst I think to use this is enough, I can use HTTP middleware wrap response!. @johanbrandhorst  I think this maybe need , I handle it like this \n```\n                 gwruntime.WithForwardResponseOption(func(ctx context.Context, w http.ResponseWriter, pb proto.Message) error {\n        var buf = pools.Get().(*bytes.Buffer)\n        buf.Reset()\n        if err := marshaler.Marshal(buf, pb); err != nil {\n            pools.Put(buf)\n            return err\n        }\n    bts, err := json.Marshal(struct {\n        Code int             `json:\"code\"`\n        Msg  string          `json:\"msg\"`\n        Data json.RawMessage `json:\"data,omitempty\"`\n    }{\n        Code: 200,\n        Msg:  \"Success\",\n        Data: buf.Bytes(),\n    })\n\n    // \u56de\u6536\u6570\u636e \u4e00\u5b9a\u8981\u653e\u5728marshal\u540e\u9762\n    pools.Put(buf)\n    if err != nil {\n        return err\n    }\n\n    _, err = w.Write(bts)\n    if err == nil {\n        // \u4f7f\u7528\u5b8c\u540e\u4e0d\u5141\u8bb8\u518d\u5f80body \u586b\u5199\u6570\u636e\n        ((w.(*echo.Response).Writer).(*lm.CustomResponseWriter)).Complete()\n    }\n    return err\n})\n\n// Complete if set , can not write into body\nfunc (w *CustomResponseWriter) Complete() {\n    w.done = true\n}\nfunc (w *CustomResponseWriter) Write(bts []byte) (int, error) {\n    if w.done {\n        return 0, nil\n    }\n    return w.w.Write(bts)\n}\nfunc (w *CustomResponseWriter) WriteHeader(statusCode int) {\n    w.w.WriteHeader(statusCode)\n}\n```\nI hava to use CustomResponseWriter to avoid forwardresponse write data to responsewriter !\n. @johanbrandhorst  Yes, it is!. ",
    "javasgl": "aa810b61a9c79d51363740d207bb46cf8e620ed5  has been merged into master.\nso there should be change to branch = \"master\" instead of special revision.\nsee: https://github.com/golang/protobuf/pull/679\n@johanbrandhorst @yugui . the simplest solution at present is keep this specific revision sync with github.com/golang/protobuf master\u2018s revision. may be , i guess. master: Could not introduce github.com/grpc-ecosystem/grpc-gateway@master, as it has a dependency on github.com/golang/protobuf with constraint aa810b61a9c79d51363740d207bb46cf8e620ed5, which has no overlap with existing constraint master from github.com/grpc-ecosystem/go-grpc-middleware@master\n@achew22 @drigz \nbroken!. ",
    "RaduBerinde": "Sorry, wouldn't know where to start testing the change, no idea how these are used.. ",
    "clehene": "@johanbrandhorst sure, I'm currently wrapping up the mess I've made while trying to get thing to work. \nCan you assign it to me, and I'll send a PR . @johanbrandhorst I've previously lost context on this, and it's a bit intricate.\nI'm finally back to this as I'm trying to figure out how to make it work for JS so got some context back, hence I'll braindump here before I lose context again :)\nI'lll post what I have for Java and C# here.\nWhen using annotations like\nrpc Echo(EchoRequest) returns (EchoResponse) {\n    option (google.api.http) = { get: \"/echo\" };\n  }\nprotoc needs the annotations.proto available at compile time.\nannotations.proto typically comes from https://github.com/googleapis/googleapis/blob/master/google/api/annotations.proto - which is where grpc-gateway vendors them from.\nNote that these are also avaialble in \nhttps://github.com/googleapis/api-common-protos/blob/master/google/api/annotations.proto\n\nNOTE / TODO perhaps annotations.proto should instead be included in grpc binaries?\n\nOnce you generate code with protoc if you want to use it you need to resolve the actual language specific dependency for annotations.\nhttps://github.com/googleapis/api-common-protos also has / provides language specific packages which will include Annotations - see https://github.com/googleapis/api-common-protos#packages\nSo for java there's https://mvnrepository.com/artifact/com.google.api.grpc/proto-google-common-protos\nFor C# https://www.nuget.org/packages/Google.Api.CommonProtos/\nEtc.\nE.g.  for C#\n<ItemGroup>\n        <!--Common protos used for annotations.proto used for http transcoding-->\n        <PackageReference Include=\"Google.Api.CommonProtos\" Version=\"1.4.0\" />\n    </ItemGroup>\nFor Java / Maven\n```xml\n  \n\ncom.google.api.grpc\nproto-google-common-protos\n\n\n```\nAs these packages include the actual proto files, you'll likely be able to use any language specific toolchain for protoc. \nE.g. I'm using maven  with xolstice  so at generation time, protoc will need annotations.proto  (E.g.)\nThis allows me to build all language bindings with Maven. \nHowever in order to use the generated code you'll  have to add the specific language dependency \nFor JS, I believe there's no package that contains google/api/annotations_pb.js\nThe refrenced npm package from googleapis/api-common-protos  - https://www.npmjs.com/package/google-proto-files  only contains the .proto files. \nI did a google search and was only able to find the js in ~5 places (https://www.google.com/search?q=\"annotations_pb.js\") \nAlso note https://github.com/protocolbuffers/protobuf/issues/5318\nI'll raise an issue on https://github.com/googleapis/api-common-protos and see where it gets me.\n. @johanbrandhorst you're referring to http://dcode.io/protobuf.js/ ?\nTBH it's a bit confusing as it seems that protobuf and grpc have been going a different route of actually generating the JS\nhttps://developers.google.com/protocol-buffers/docs/reference/javascript-generated\nhttps://github.com/grpc/grpc-web\nAlso we want to support both direct grpc (grpc-web style) and transcoding.\nAs a side note - as non JS developer, already partially confused about the whole packaging jungle, I'm even more confused what's the right way to generate the JS SDK for a bunch of proto APIs and then use them. E.g. I have a public API and an internal one that depends on the public one. Normally they are both packaged and the internal libs depend on the public libs, but it's unclear what's the right way to generate some JS from proto and then package them and then use the resulting package as a dependency for the generation of a downstream SDK. \nI've been trying too figure out what others are doing and looked over googleapis, but the whole gapic / artman SDK generation is yet another beast.. It looks like it's possible to use decode.io protobuf.js with grpc-web  https://github.com/grpc/grpc-web/issues/80 . Indeed the example there was for the improbable grpc-web (the whole protobuf.js vs protobuf/js grpc-web vs grpc-web is confusing at least) \n\nAlso, while you can use this gateway together with the gRPC-Web proxy, you'd normally choose one or the other depending on the needs of your client(s).\n\nBesides allowing both gRPC and REST for services, with grpc-web you'd still need some way to perform health checks from load balancers. \n. ",
    "royeo": "I signed it!. > Hi @royeo, thanks for spotting this! Could you perhaps even point it at https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests to make it even more specific? Thanks!\nHi @johanbrandhorst , that's really better, I have modified it.. ",
    "npuichigo": "Thank you for your reply. I configure the KeepaliveParams like this, so I'm sure the parameters are passed to the Dial call.\n```\nctx := context.Background()\n        ctx, cancel := context.WithCancel(ctx)\n        defer cancel()\n    mux := runtime.NewServeMux()\n    opts := []grpc.DialOption{\n            grpc.WithInsecure(),\n            grpc.WithKeepaliveParams(keepalive.ClientParameters{\n                    Time:                30 * time.Second,\n                    Timeout:             20 * time.Second,\n                    PermitWithoutStream: true,\n            }),\n    }\n    err := gw.RegisterMyHandlerFromEndpoint(ctx, mux, *echoEndpoint, opts)\n\n``` . Okay, I'll check it. Thanks.. ",
    "dweitzman": "Hmm. Looks like resty actually changed its import path in https://github.com/go-resty/resty/commit/8fa06f264236504747b03e35acf013e80f6bd04d so this diff may only make sense when the dep is being upgraded. I'll close this for now and I think I can work around it be choosing an older version of resty in my own go.mod. ",
    "gen1us2k": "I signed it!. ",
    "mechinn": "\n@mechinn Are you planning on fixing this soon? I'm looking to make a release soon and just wondering if I should wait for this to be ready.\n\nyeah sorry getting back to work on the error part after the long weekend. > > There should be an unused function now to delete as well, right?\n\nBy this I mean the function where we set the sourceCodeInfo explicitly before.\n\nyeah good catch, removed. > Hm, I figured our CI tests would have flagged that up \ud83e\udd14.\nconsidering another change that was merged before my other one changed the ABitOfEverything message and we had never generated the stream.swagger.json before my change I would have been impressed if the CI picked up this issue before the merge into master. oh shoot you're right from looking it over a little closer I think I have a few options:\n1. change streamdefinitions to x-stream-definitions which is the allowed way to extend the Swagger Schema.\n2. add the stream result wrapped messages under definitions with a special name that cannot be generated from a proto field like stream*TestStreamMessage and referenced as #/definitions/stream*TestStreamMessage ie\njson\n  \"definitions\": {\n    \"stream*TestStreamMessage\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"result\": {\n          \"$ref\": \"#/definitions/TestStreamMessage\"\n        }\n      },\n      \"title\": \"Stream result of TestStreamMessage\"\n    },\n    \"TestStreamMessage\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"test\": {\n          \"type\": \"string\"\n        }\n      }\n    }\n  }\nBoth options work and follow the OpenAPI v2 spec from my understanding, any opinions of which we should go with?. ok ill make the change to x-stream-definitions and figure out what to do about a test. I ended up moving the runtime/internal folder to the root of the repo so that the swagger generator can use it, bright side of it being an internal folder is I know I don't break anyone outside this repo.\nIf it's not acceptable to move the /runtime/internal folder to /internal then we will need to hard code things like you suggested, not as ideal because if the StreamError proto changes without changes to the swagger generator then things will get out of sync. it's from the internal/stream_chunk.proto package grpc.gateway.runtime; we could rename it if we wanted. there isnt debug in glog, we could bump the verbosity level up to 2 though (default's to 0 and in main.go there are a few info logs at level 1 like glog.V(1).Info(\"Processing code generator request\"). The whole thing or just setting SourceCodeInfo? \ud83d\ude04\nThe purpose of the function in general is to load in the StreamError and Any messages into the proto registry from memory because we won't have any idea where the proto files are on the end user's system. Every compiled proto generates a Descriptor function which returns a gzipped byte slice of the FileDescriptorProto which is exactly what we need. Unfortunately to save space they drop the SourceCodeInfo before gzipping up the proto and writing the bytes to the pb.go which leads into the need for this line.\nWe need to set SourceCodeInfo unfortunately there is a check for SourceCodeInfo being non nil when the swagger generator tries to use the SourceCodeInfo in protoComments()\ngolang\nif file.SourceCodeInfo == nil {\n    fmt.Fprintln(os.Stderr, \"descriptor.File should not contain nil SourceCodeInfo\")\n    return \"\"\n}\nAs long as it's not nil that err log wont print and the end user wont be any wiser that we are loading these in from the generated pb.go Descriptor() gzipped FileDescriptorProto. Unfortunately there wont be any descriptions that come with the StreamError or Any that we are loading in in this way not that it really matters for StreamError because there isn't many comments in that proto file. Bright side is if the end user loads in any.proto it should overwrite our any.pb.go and pull in the comments. I'm gonna go test that theory right now, be back with the results. yep works perfectly. you would think wouldn't you:\n\"github.com/golang/protobuf/descriptor\" is not imported by your project, and has been temporarily added to Gopkg.lock and vendor/.\nIf you run \"dep ensure\" again before actually importing it, it will disappear from Gopkg.lock and vendor/.. agreed, maybe it's because there are no go files in the root of the repo? dep is weird sometimes.... huh you're right, if I get rid of the requires everything still works fine, I guess I just confused dep by doing dep ensure -add github.com/golang/protobuf/descriptor. me doing dep ensure -add github.com/golang/protobuf/descriptor for some reason removed it and I can't recreate why at this point, reverted that change. baaaaaaahhhhhhhh. ",
    "abice": "I asked that question in the slack room, this change does change the output of the swagger file, but it requires a CLI parameter in order to trigger the change, and I was having trouble getting the bazel part to run with the added flag required.  I can go back and check to see what bazel stuff I need to change in order to pass in the flag, but I'm unfamiliar with bazel, and it might take a bit.. Well, it's less bazel, and more the circle step of running bazel... I had updated the response_body_service.proto to include a sample since it was related, but ran into problems... I'll add a new proto that just requires that flag.. Well, my first attempt was to just change the circleci config to add the SWAGGER_PLUGIN_FLAGS and GATEWAY_PLUGIN_FLAGS to the make examples line like:\nmake\nGATEWAY_PLUGIN_FLAGS=allow_repeated_fields_in_body=true SWAGGER_PLUGIN_FLAGS=allow_repeated_fields_in_body=true make examples SWAGGER_CODEGEN=\"${SWAGGER_CODEGEN}\" # Set in Docker image\nand then generated everything like it says in the contributing.md\nbut when I ran the bazel job locally using circleci local execute --job bazel, it failed on\n/go/src/github.com/grpc-ecosystem/grpc-gateway/examples/proto/examplepb/BUILD.bazel:39:1: Generating into bazel-out/k8-fastbuild/bin/examples/proto/examplepb/linux_amd64_race_stripped/examplepb_go_proto%/github.com/grpc-ecosystem/grpc-gateway/examples/proto/examplepb failed (Exit 1) go-protoc failed: error executing command bazel-out/host/bin/external/io_bazel_rules_go/go/tools/builders/linux_amd64_stripped/go-protoc -protoc bazel-out/host/bin/external/com_google_protobuf/protoc -importpath ... (remaining 86 argument(s) skipped)\nMaybe I missed that it was looking at the BUILD.bazel file in the examplepb folder.  But I dont' know where I would add the extra flag in that file.. Awesome, thanks!  I'll add that and see where I get. That works for me, honestly wasn't sure if you wanted me to return \"null\" if j.EmitDefaults wasn't set.  I can make it all one big if.\n\ud83d\udc4d . Test case 5 (line 665) and 7 (line 684) are already testing those cases.\n{\n            input: ([]*examplepb.RepeatedResponseBodyOut_Response)(nil),\n            verifier: func(json string) {\n                expected := `null`\n                if json != expected {\n                    t.Errorf(\"json not equal (%q, %q)\", json, expected)\n                }\n            },\n        },\nand \n{\n            input: []*examplepb.RepeatedResponseBodyOut_Response{},\n            verifier: func(json string) {\n                expected := `[]`\n                if json != expected {\n                    t.Errorf(\"json not equal (%q, %q)\", json, expected)\n                }\n            },\n        },. \"null\" is what the previous implementation was doing in this case, which is why I put it there (what the json encoder was returning).. ",
    "zwcn": "Hi @johanbrandhorst , I'd like to sign the CLA with another email address. I'm gonna reopen the PR once I figure it out how to do it.. @johanbrandhorst , yes. As shown at https://github.com/grpc-ecosystem/grpc-gateway/pull/860/files#diff-eab1d67e52af78e98dd23e2b08fcc22aR25 , swagger-codegen generates go files based on the tags field of the input swagger file.\nHow do you feel about these long-name changes?. Hi @johanbrandhorst , \nYes, the names aren't wrong. As shown at https://github.com/swagger-api/swagger-codegen/wiki/FAQ#how-do-tags-affect-the-generated-code, the tags filed is required for swagger-codegen to group APIs. So swagger-codegen did the right thing.\ni've browsed the swagger-codegen docs. Unfortunately, I couldn't find any argument for swagger-codegen to ignore (or override) tags during code generation.\nOne possible solution would be to add a new boolean flag called allow_package_in_tags to protoc-gen-swagger. The package name is prepended only when it's true. This gives users more flexibility to have pretty names when they don't care about an API's origin at all.\nHow do you think?. @johanbrandhorst : Sounds like a plan. Will do. . Hi @johanbrandhorst , I've added a new boolean flag allow_package_name_in_tags and some unit tests in main_test.go. Could you please take a look?. if there's no package name, there will be only a dot prepended to the service name, like .svcName. It seem to me that's fine if we assume the tag always consists of two parts: the package name and the service name. We could use strings.Split to extract the two parts even if the optional package name is not defined in the proto file.. Sure, that's a good idea. Will do. . Yeah, include_package_in_tags is better. I've done the replacement everywhere.. ",
    "therealmikelee": "\nI signed it!\n\n. > Hi @therealmikelee, thanks for this contribution! Any chance we could add a new proto file with some generic types that we could generate with this new option? It'll both serve as a test that we don't break it in the future and an example to our users about what this flag does.\n\nWhat do you think?\n(PS I LOVE your avatar. SQUIRREL!)\n\nThanks, I will try to get this done as time permits.  I'm working on windows and I'm admittedly ignorant as to getting the Makefile running properly, but perhaps I can just manually generate the things I need.  I was thinking about using a_bit_of_everything.proto.  . ",
    "fahernandez": "Sure, I will definitely try, thank you for taking me in the right direction!. Linked to issue 836. Documentation was added on this pull request. > Hi Fernando! Sorry this hasn't had a review yet, I had an idea! How would you feel about adding this to the docs pages instead of the readme?\nSure, no problem at all! But what you mean with docs pages? Can you share the file path where you consider is a better idea to put it?. > Look at docs/_docs/patch.md for example. We could add an aws.md which has your tips in it, and we could link to it from index.md. What do you think?\nIt seems good to me. Please look at the changes and let me know if it's ok.. Typos were fixed as Johan suggested. > Ah curses, accepting my own suggesting messes with the CLA. Uhm, let me get back to you on what we can do to fix this.\nSorry for that, I thought it was faster only accepting the suggested change.. The swagger file is not generated due the error code. Sure, I will fix it.. This pull request will fix issue https://github.com/grpc-ecosystem/grpc-gateway/issues/897. It's a very strange behavior but if you look line protoc-gen-swagger/genswagger/template.go:1246 you will see a condition to avoid executing a particular flow if the comment has a dot at the end. So when the support for enum and objects annotation was added I forget to take into consideration this case causing in this particular case to fallback to condition protoc-gen-swagger/genswagger/template.go:1275. So basically I'm only changing condition protoc-gen-swagger/genswagger/template.go:1268 to the default fallback that existe before the changes but keeping the code needed to support the comments override behavior.\nI don't have a clear idea how to make a test to test particular condition thats why I added an example case on examples/proto/examplepb/a_bit_of_everything.proto:166 to be sure that the problem was fixed but keeping past changes.\n\nFirstly, I don't understand what this fixes. Why does the applied change fix a comment that ends in a dot?\nCould we add some tests for this case as well?\nThanks\n\n. done. done. done. done. Required field on schema object is defined as an array so this field cannot be defined as boolean due to the actual implementation ( I suppose that change it to boolean will require a deeper refactor) so defining  required field on this way + the changes on the queryParams method will allow us to define the fields as required.. It's a little hack over the actual implementation, what do you think?. Both formats (required:[\"float_value\"]) and required: true) as interpreted as equal by swagger. Due the swaggerSchemaObject is used on operations and fields, values on the property section will look like that but it will be interpreted well by swagger.. Oh I see , sorry, I will remove the code that breaks this part. Sure I can but inside openapiv2_field It won't have any effect because the field is translated to a boolean value here. openapiv2_field and openapiv2_schema use swaggerSchemaObject as schema, that's why on both cases it has to be managed as array but only on openapiv2_schema has any sense to use it as a multiple value array. On  openapiv2_field is used as array due to the actual implementation but it will have more sense to used it as a boolean.. I think it will be breaking because the right schema object that we could be using is swaggerParameterObject but this schema definition hasn't all the fields that have swaggerSchemaObject (Following the swagger schema definition) so all these properties won't be supported after the changes.\nAnother possibility is to add a custom boolean field to swaggerSchemaObject to specify that a parameter field is required but this parameter won't complain with the swagger definition of schemaObject. There is an issue that overrides the schema object title and description with field inline comments even though these values are defined as field options. I change this behavior to options annotation take precedence over fields comments but it seems that the info title is set to examples/proto/examplepb/echo_service.proto by a process so it will take precedence over the comments, I will check to know if it's something that I can do to avoid this specific case.. This change allows add field annotations to objects and arrays, before these changes, all field annotations added to object or arrays were ignored.. This methods handle specific behavior about setting title/info/description/summary. If we avoid calling this method, this logic needs to be handled on an upper level. The problem is that by default the package title is set to the file name protoc-gen-swagger/genswagger/template.go:949(I don't know why but I suppose we want to keep this behavior) so by changing the behavior of field options taking precedence over comments the title of all protos without package definition was overwritten by the file name so to handle this specific case, the if was added. sure, I will change it. I will change it. The title of the documents was kept due to some changes when generating the swagger documentation.. ok. ok. ok. ok. ok. ",
    "DaHuMao": "@achew22 I need to insert some custom code into the .pb.gw.go, but I don't want to have to modify it every time I generate a .pb.gw.go file, so I tried to modify the generated template file in gprc-gateway: template.go.After the modification, I generated the *.pb.gw.go file again. I found that template.go was not called at all when the file was generated\u3002Is the generated template not the template.go file in the github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway/gengateway/ directory?\n. @johanbrandhorst I am just a rookie ,who just starting to learn this aspect.\nI want to change the build template because of the specific requirements of the project. Do you mean i should reinstall protobuf if i want to change it.. @johanbrandhorst  I had reinstalled protobuf ,but i did not work . @johanbrandhorst  OK\uff0cThank you very much\uff01. @yugui Thank you for your advice. But i want to handle the error and the message in .pb.gw.go,befor they  passed into the client function,not just add some function  to .pb.gw.go. ",
    "muyuren": "Use the code shown below in grpc server\nfunc (s *server) Echo(ctx context.Context, in *pb.StringMessage) (*pb.StringMessage, error) {\n    return &pb.StringMessage{Value: \"test.com?a=1&b=2\"}, nil\n}\nthen\ncurl localhost:8080/v1/example/echo -X POST -d \"value=xxx\"\nthe output is \n{\"value\":\"test.com?a=1\\u0026b=2\"}. The result is still\n{\"value\":\"test.com?a=1\\u0026b=2\"}. ",
    "klesniewski": "Seems to be duplicate of #675 . ",
    "hypnoce": "https://github.com/grpc-ecosystem/grpc-gateway/pull/881 can help ? The PR helps in better readability and repeatability, ensuring unicity of names.. Currently, the default behavior for swagger names is to traverse the depth of packages and inner message, check for unicity and generate a name that prepends one more package (because of an of-by-one error) : \n```go\nfor , p := range messages {\n    h := hierarchy(p)\n    for depth := range h {\n        if , ok := packagesByDepth[depth]; !ok {\n            packagesByDepth[depth] = make([][]string, 0)\n        }\n        packagesByDepth[depth] = append(packagesByDepth[depth], h[len(h)-depth - 1:])\n    }\n}\ncount := func(list [][]string, item []string) int {\n        i := 0\n    for _, element := range list {\n        if reflect.DeepEqual(element, item) {\n            i++\n        }\n    }\n    return i\n}\n.....\nh := hierarchy(p)\nfor depth := 0; depth < len(h); depth++ {\n    if count(packagesByDepth[depth], h[len(h)-depth:]) == 1 {\n        uniqueNames[p] = strings.Join(h[len(h)-depth-1:], \"\")\n        break\n    }\n    if depth == len(h)-1 {\n        uniqueNames[p] = strings.Join(h, \"\")\n    }\n}\n``\nFirst, it leads to a silent failure whendepth == 0. Second, it prepends an unnecessary level. Also, it joins package and inner class names without.which can be a bit unreadable.\nI would propose to remove thefqn_for_swagger_nameboolean option to introduce aswagger_name_generation_strategythat can have valueslegacy, fqn, simple. \nlegacy: keep the current behavior with the bugs\nfqn: alway take the full qualified domain name\nsimple: checks for unicity and join with.` if needed for unicity.\nWhat do you think ?. @johanbrandhorst makes sense not to support backward compatibility of bugs. So a first step would be to fix the name generator. There are 2 main issues : \n of-by-one bug that leads to more package being prepended than actually needed for unicity\n the elements are joined without .. So instead of generating foo.bar.Message, we endup with foobarMessage\nWhat do you think of fixing those 2 issues as a first step ?. Scenario : I have a package foo.bar and 2 messages MyMessage and MyNestedMessage\nAvailable as of 1.8.0 :\n Current behavior (no flag): names will be as follow : barMyMessage and MyMessageMyNestedMessage even if MyNestedMessage is already unique. In case the unicity cannot be ensured, traverse the fully qualified name upwards until the generated swagger name is unique.\n fqn_for_swagger_name flag : taking the same example, names will be generated as follow : foo.bar.MyMessage and foo.bar.MyMessage.MyNestedMessage always joining with .\nStill in discussion :\n with the off-by-one fix (no flag) : MyMessage MyNestedMessage will be generated. If unicity is not ensured, any message will be prepended with previous names in the fqn hierarchy, without ., ie MyMessageMyNestedMessage.\n with . joining (behind a flag or not) : all joining will use . as delimiter to ensure, when unicity is broken, that names are still kind of readable.\nSo I was thinking to introduce a naming strategy configuration\n legacy : default in grpc-gateway 1.x.x. Does not change the current behavior.\n fqn : similar to fqn_for_swagger_name=true\n* third one (simple?) : removes the current off-by-one bug and always join with .\nWould be happy to help deprecate the flag I introduced in favor of a naming strategy.\nWhat do you think ?. Indeed, always trying to keep things backward compatible. \nLet me research things to see if we can have a more 'swagger' way of namespacing.. Hi,\nafter some search, the only thing that is the closest to package/namespace if the xml namespace/prefix.  Also, the swagger code gen has a specific option for generated package : https://github.com/swagger-api/swagger-codegen/tree/master/modules/swagger-codegen-maven-plugin. I believe their is no good support for namespaces/package in openapi 2 (as well as 3)\nThanks !. @johanbrandhorst PR updated with support for \"Output only.\" comment on field. Added missing tests on updateSwaggerWithComment.. Do you think it's possible to release grpc-gateway at some point with the generated artifacts ?. done. ",
    "klim0v": "I signed it!. I signed it!. > Could you try updating to 1.19.0 and see if we get any errors?\nYes, I'll do it. It's work. ",
    "MichaHoffmann": "Hey, is this still up for grabs?. ",
    "kernle32dll": "Well, there is two things.\nFirst, is the question if the replace directive should propagate - I am not sure, as the go module documentation is not sufficiently clear about this (imo). However, I have a strong sense they don't. See the last paragraph here (emphasis by me):\n\nexclude and replace directives only operate on the current (\u201cmain\u201d) module. exclude and replace directives in modules other than the main module are ignored when building the main module. The replace and exclude statements therefore allow the main module complete control over its own build, without also being subject to complete control by dependencies. (See FAQ below for discussion of when to use a replace directive).\n\nSo, as how to fix this.... The only thing I can think of then is to go search&replace and fix the import paths everywhere :/ Option B might be to use v2 of resty, but I have no insight of the state of that version (as its still in development).. Thanks, no worries ;-)\nFYI, I totally forgot to mention how to workaround this, if someone is seriously blocked by this: As mentioned in linked resty issue above, you can use the same replace directive  in your go.mod depending on grpc-gateway (either directly or indirectly).. ",
    "PassKit": "Please ignore.  protocol-gen-swagger is working as expected.  My issue is with the ReDoc project.. ",
    "happyalu": "\nCould you try building the grpc-gateway generator off this branch and see if it fixes tmc/grpc-websocket-proxy#14 for you?\n\nYes, my websocket client is working again. . removed.. ",
    "t1bb4r": "This actually looks a lot more difficult than I initially thought, because the security structure changed in openapi 3.\nWhat grpc-gateway currently produces is for openapi version 2:\njs\n{\n  \"securityDefinitions\": {\n    \"cookie_session\": {\n      \"type\": \"apiKey\",\n      \"name\": \"session\",\n      \"in\": \"query\" // I thought it would be as easy as to change this to \"cookie\"\n    }\n  }\n}\nWhat openapi 3 expects is:\njs\n{\n  \"components\": {\n    \"securitySchemes\": {\n      \"cookie_session\": {\n        \"type\": \"apiKey\",\n        \"name\": \"session\",\n        \"in\": \"cookie\"\n      }\n    }\n  }\n}\nReference:\nv2: https://swagger.io/docs/specification/2-0/authentication/api-keys/\nv3: https://swagger.io/docs/specification/authentication/api-keys/\nWarning: first time working with an openapi spec is with grpc-gateway so take everything I saw with a grain of salt.. ",
    "Kuzey27": "\nIs this request for other 2xx status codes of for returning more information in the error? Why can't you simply include more information in the error?\nFor example : I want to raise 429 error and send a captcha-token, I this line of code:\ncontext.set_code(grpc.StatusCode.RESOURCE_EXHAUSTED)\nAnd after that I can't send a captcha-token in message. \n",
    "mmarod": "I believe that this may have inadvertently broken spec...\n\"definitions\": {\n    \"Foo\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"foo\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"$ref\": \"#/definitions/Bar\"\n          },\n          \"description\": \"foo\",\n          \"collectionFormat\": \"multi\"\n        }\n...\nThis spits out the following error when I run swagger generate server\n- definitions.Foo.properties.foo.collectionFormat in body is a forbidden property. Thanks @johanbrandhorst for the quick turnaround!. @johanbrandhorst I can check in a few hours. Looks good to me @johanbrandhorst -- and nice job @bmperrea . ",
    "c2nc": "I have same trouble when do swagger spec validation...\nFix please guys. Thanks\nLook here\nhttps://github.com/OAI/OpenAPI-Specification/issues/419#issuecomment-128788426. ",
    "chrispsommers": "Hi @achew22 Thanks for the quick reply. I was indeed asking about using this gateway to proxy to another gRPC server; it wouldn't sit alongside it as much as in front of it. I'm thinking about what I saw in https://github.com/grpc/grpc-web where they use Envoy https://www.envoyproxy.io/ to translate from gRPC-web (supported by browsers) into gRPC protobufs. It would be neat if this grpc-gateway could do that too, so the whole solution could be smaller and the code easier to customize. Just a thought! Also, by \"unnified\" I did mean allowing both http1and gRPC requests on the same port, and letting the gateway transcode http into gRPC before calling a service, as done in https://github.com/philips/grpc-gateway-example. However in  this case it would proxy to the upstream gRPC server. Anyway this is a neat project.. @johanbrandhorst  Thanks, I looked at https://github.com/mwitkow/grpc-proxy and indeed it looks like it does the proxying like I'd imagined, it even allows deciding based on the service being called. Putting that director/reverse proxy just \"above\" the internal gRPC servicer in this gateway would let it handle some services via REST/gRPC and redirect others to another gRPC server. That would be very powerful.. ",
    "rstrong-pica9": "One too many n's in annotations\n. ",
    "echlebek": "Can this nil propagate to calls to populateRepeatedField or populateField?. ",
    "ffredsh": "done!. ",
    "jdef": "I don't understand the need for this if statement. The statement inside this if block is the same as the statement that follows it.. ",
    "timonwong": "@rogerhub It's intentionally to preserve -bin suffix if we want to passing binary data, see https://github.com/grpc/grpc-go/blob/a91fb537b19866e5d1791fa1c6fb0530df26aadc/Documentation/grpc-metadata.md#storing-binary-data-in-metadata. "
}
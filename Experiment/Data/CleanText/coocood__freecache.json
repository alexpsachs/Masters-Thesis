{
    "coocood": "the method name is a little bit confusing , it is actually the Unix timestamp when a entry being accessed.\n. Well, this value is not used for measuring latency, it is the value that determine whether an entry can be evicted, entries has greater access time will be evacuated when it is about to be overwritten by new value.\n. Thank you, copied your link and added in README file\n. Thank you, the README improved a lot with this commit.\n. Thank you for the feedback.\nClear added by commit 6ee7a794707dc87e8a4e868aca435edbb013a292\n. If you set GC percent to be 10%, then your application could consume 11GB memory before GC.\nIf you don't set it, your machine will run out of memory.\n. Thank you for reporting this issue, fixed in the latest commit c17646daed8dea3f5e8c4a0d9ec5330310aa3e43\n. Thank you for your intensive testing on it.\nSo it seems like the memory utilization issue is not caused by hash function.\nThe difference of entry overhead seems too big for the difference of hash function distribution quality.\n. I think this is normal.\nIf you set n_routines to one, then every key is new, there is no overwrites, the entry length is exactly (24+keyLen+valueLen).\nIf you set n_routines to 10, the entry can be overwritten. \nIf the new value is larger than the old entry's capacity, it won't be updated in place, then the old entry's space is not utilized. \nIf the new value is smaller than the old entry's capacity, then there would be an in-place update, (capacity - valueLen) of space is not utilized.\nWhenever the capacity is smaller than the new value, the capacity is doubled, so when you keep increasing the size of the new value, it won't keep writing new entries, most of the update would be in place update.\n. I will try to find the answer tomorrow, you can print out every segment's entry count to see how it is distributed .\n. Thank you for the great work to address this issue.  The hash function definitely need to change.\n. Changed to MD5 in the latest commit 28070c015675a739b0ed7b5099d4f55064b57ce7\n. That is Interesting, but drwmutex is meant to replace sync.RWMutex, not sync.Mutex. \n. But drwmutex is optimised for RLock method and freecache only use Lock method.\n. It is not read only, the access time for the entry is updated when you get it.\n. I think that work can be done externally, you compress your data before put it in, then decompress it after  get it.\n. I chose crypto/md5 because it in standard library, and I didn't remember the performance difference was big like this.\nWhich murmur3 library did you use?\n. I took a look at gnatsd implementation, it returns a uint32, but we need a uint64 hash value, maybe that's why it is much faster.\nCan you test with https://github.com/spaolacci/murmur3 and post the results?\n. Thank you, the performance improvement worth adding a dependency, \nPlease send a pull request to change hashfunc to spaolacci/murmur3.\n. fix by https://github.com/coocood/freecache/pull/11\n. Thank you for your contribution.\n. Please provide the test code.\nAnd for Go 1.5 and above, explicitly call runtime.GC() will force stop the world, which normally doesn't happen in  real life.\n. Most of the time, marshal/unmarshal would be much more expensive than lookup.\nBut if you have many CPU cores and access your cache concurrently, freecache may worth using.\n. Thank you for your contribution!\n. Great job!\nThank you!\n. If you are on 64bit OS, int is 64bit, you are able to allocate caches larger than 4GB.\n. @toontong \nSetGCPercent lower can reduce the process memory usage.\nSay the most of the memory is consumed by the cache.\nIf you do not set it lower, the process may consume more than twice of the cache memory.\n. Can you provide more information about your Go version and OS?\nI can't reproduce this issue  on maxOS, Go 1.7.\nThank you.. I have considered this hash function before but chose md5, then later changed to murmur3 instead,\n because of hash distribution issue. A common key pattern shows very big hash conflict rate.\n. I remember that the key pattern shows high collision rate was simple integer encoded as LittleEndian or BigEndian.. I think that would be much more complicated and slow, then why not just use an on-disk storage engine instead?\n. Thank you for your contribution!. Thank you!. The data size is larger than the capacity 10MB, so many entries is evicted.. When you set a new value, it is always succeed, the evicted value is the value you set a long time ago.. key0 may be evicted when you set key 10000.. It's not read only, get modifies the access time of the entry.. See https://github.com/coocood/freecache/issues/24. Can you give me some detail about how the cache is used?\nDoes it call Clear multiple times during the usage?. Are you sure there is only one cache created?. Thank you for your suggestion, tag v1.0 added.. It's limited by the data structure, large values don't fit in the data structure.\n. @glasser \nIf an entry takes too big a fraction of a segment, it will be evicted when a new entry is written, the LRU algorithm does not work.. Thank you!. Thank you!. You can use the Iterator to get all keys.. No schedule for now.. @zhangcunli \nThank you for your great work.\nWould you please add the case to test and make sure all tests are passed?\nAnd we need to test that when size is increased, all old entries are preserved.. @zhangcunli any update?. Nice optimization!\nThank you!. @jb0n \nThank you for catching this issue.\nThe lock has to be Mutex, because the read operation may write the underlying data.\nA simpler solution is to lock each segment to collect the statistics.\nIf statistics collection is not a frequent operation, the performance cost is acceptable.\n. @jb0n \nThe atomic solution is better if we can remove the race in Clear.\nI've rethought about the Clear function and came up with an idea.\nThe Clear method can be implemented as keep the segment object, but reset the content in the segment, and set each stats field to zero with atomic method.\nAnd we don't need to reallocate memory.. For a  Get operation, we need to updates the access time, so the entry will be retained during the evacuation for a Set operation. So we cannot use a read lock even if the key never expires.. @jb0n \nGood job, thank you!. Fixed by https://github.com/coocood/freecache/pull/38. Thank you!. Store many interface{} in memory will cause big GC impact.\nGC overhead is proportional to the number of long-lived objects.\n. @SmileEye \nYou can read this blog https://blog.golang.org/ismmkeynote. @muXxer \nThank you for the PR.\nBut the Random function is not thread-safe.. @muXxer \nThere is a conflict.. @muXxer \nYou can try this https://github.com/coocood/rtutil/blob/master/rtutil.go#L32\nIt's thread-safe and no lock.. And we need to cover the functions in unit test.. Thank you!. Good catch!\nThank you!. Nice cleanup, thank you!. Thank you!\nBut I don't have the plan to add this feature.. No, I'm sorry, I think this feature is not necessary and I don't have time to do it.\nYou can build a hash structure based on the basic key/value feature at the application level.. Thank you for your report.\nIs it the full stack?\nHow the cache is used? \nIs it reproducible?. @ydf \nWould you please try this branch https://github.com/coocood/freecache/tree/assert\nI added an assert and prints some helpful message.\nIf the panic occurs again, the message would make debugging easier.\nThank you!. @ydf \nAre you sure the binary is the assert branch?\nHas the line number of the panic stack changed?. This time is panicked on a different line, the original line is not panicked this time.\nWould you add another similar assert before the second panic and run again?. No, the conversion is incorrect.\nYou should do it this way:\nhttps://github.com/pingcap/tidb/blob/master/util/hack/hack.go#L36. Yes, the current version is stable.. It's not case insensitive.. Would you please write a test case to reproduce this issue?\nI wrote a test case but didn't reproduce it.. That's OK.. Do you know any cache library have this optimization?\n. Hi, this question seems can be easily found by search byte slice int8 golang. You can use go profile tool to analyze the performance issue.\nSee https://blog.golang.org/profiling-go-programs. The bottleneck is the memory allocation.\nEach Get operation allocates a []byte with the value size.\nWhen the value is large, the allocation takes much more time.. I'm going to remove the memory allocation of Get by adding a parameter buf []byte. \nI can add concurrent benchmarks after this optimization.. Fixed by https://github.com/coocood/freecache/pull/58. @pingworld \nI read the code again and find out that the second lookup can be avoided.\nI will push a commit soon.\nGood catch, thank you!\n. Yes, your understanding is correct.. How is the cache created?. Do you have any cache instance created by new(freecache.Cache)?\nFrom the stack, it seems like the RingBuf is nil, it only happens when cache is created by new(freecache.Cache.. @1a1a11a \nPlease provide the full test that can reproduce this issue.\nThank you.. There is another possibility that the value passed to Set is nil.\nYou can try to add this code to verify if this is the case.\ngo\nhdr := (*reflect.SliceHeader)(unsafe.Pointer(&cachedContent))\nfmt.Println(hdr.Data, hdr.Len, hdr.Cap)\nIf the hdr.Data is 0, then this is the cause of the crash.. Good job, thank you!\nBut it seems like you didn't set your Github email in Git.. How does this line of code avoid unnecessary memory copy?\n. Do you mean one second in the future?\nAnd the test can be more stable if we get the expiration before time.Sleep.. I mean the unstable factor is that we may sleep more than one second.\nSay if we set the value at 11:11:11.999999, after the sleep, the time may be 11:11:13.000001. Then the check fails.. *= 2. I think segmentCount or numSegments is better.. totalTime and totalCount is not stats.. When Cache.ResetStatistics is called, we don't want to change them.. This is not stats.. I think we don't need to export this function.. ",
    "bearfrieze": "That makes sense \u2013 thanks for the quick reply. I don't think the name is that confusing now that I know. The newly added comments in the code help clear this up as well.\nEnded up using it like this:\nlog.Printf(\"AverageAccessTime: %d\\n\", time.Now().Unix()-cache.AverageAccessTime())\n. Yep, I got that :+1: At this point I'm not exactly sure why I'm logging it \u2013 maybe it will come in handy later.\n. ",
    "CoolOppo": "No problem. It's a pretty easy thing to forget; I see it all the time.\n. ",
    "anton-povarov": "tested with https://github.com/spaolacci/murmur3\nresult\nentries: 12417, evacuated: 0, overwritten: 0\navg_value_len: 43.480773\nbytes per item: 84.44680679713296 - 43.480773 - 16 = 24.966033797132965\nand is about (non-scientifically) 20% faster than md5\n. Hm... actually tested it some more. And while a better hash function does solve the issue in single-threaded case, it's still very bad in multithreaded case.\nWith 10 goroutines, all inserting same keys, but slightly different values (so a zillion of evacuates happens) - overhead per item still grows to ridiculous values.\nfor example:\nsame code as above gist, but changed the following params\nconst cache_size = 10 * 1024 * 1024 // was 1 megabyte\nn_keys := 10 * 1000 * 1000 // was 1 million\nhere's the result with: const n_routines = 10\nentries: 42078, evacuated: 120957842, overwritten: 77988935\navg_value_len: 43.49973858\nbytes per item: 249.1981558058843 - 43.49973858 - 16 = 189.69841722588433\nvs const n_routines = 1\nentries: 125323, evacuated: 0, overwritten: 0\navg_value_len: 43.4929523\nbytes per item: 83.6698770377345 - 43.4929523 - 16 = 24.176924737734495\n. Even in single threaded case with fnvhash - utilization is poor. With zero overwrites/evictions. might be wrong stats i guess ? since with avg value of 43 bytes and key 16 bytes, we should be able to fit all the values into 1 meg.\n. hacked it together for 1 mil items, 1 goroutine, 1 meg cache\ndata gathered with\nn := atomic.LoadInt64(&cache.segments[i].entryCount)\n        fmt.Printf(\"%d %d\\n\", i, n)\nfnvhash\nantoxa@antoxa-suse:~/_Dev/go/src/antoxa> cat 1.txt | awk '{ print $2 }' | sort -n | uniq -c\n    384 0\n      2 470\n      4 479\n      6 480\n      4 481\n      2 482\n     10 483\n      2 484\n      6 485\n      4 486\n      6 487\n     14 488\n     12 489\n      6 490\n     12 491\n      2 492\n      8 494\n      4 495\n      4 496\n      6 497\n      4 499\n      4 501\n      4 503\n      2 508\nmurmur3\nantoxa@antoxa-suse:~/_Dev/go/src/antoxa> cat 1.txt | awk '{ print $2 }' | sort -n | uniq -c\n      4 471\n      2 473\n      2 475\n      6 477\n      8 478\n     12 479\n     12 480\n     20 481\n      6 482\n     26 483\n     14 484\n     20 485\n     24 486\n     24 487\n     18 488\n     40 489\n     30 490\n     24 491\n     18 492\n     44 493\n     44 494\n     18 495\n     18 496\n     14 497\n     20 498\n     10 499\n     12 500\n      2 501\n     10 502\n      4 503\n      2 504\n      2 505\n      2 508\n. ",
    "kron4eg": "Yes, indeed it's imitating sync.RWMutex, so you could use it too instead of sync.Mutex\n. here https://github.com/coocood/freecache/blob/master/cache.go#L53 RLock could be used since it's only reading, unless of course there are no writes behind the scene.\nAnyway, if you feel so please close this ticket.\n. ",
    "eliquious": "I used the gnatsd impementation to test with but I would probably go with this murmur3 library if you were going to change the hash func. Murmur3 is really fast and also has a good distribution so it might be a good one to look at using.\n. Here are the results for spaolacci's implementation:\nBenchmarkCacheSet   3000000             497 ns/op\nBenchmarkMapSet     2000000             813 ns/op\nBenchmarkCacheGet   3000000             521 ns/op\nBenchmarkMapGet     10000000            179 ns/op\nBenchmarkHashFunc   50000000            39.3 ns/op <- murmur3 64-bit\nMy hashFunc looks like this:\n```\nimport \"github.com/spaolacci/murmur3\"\nfunc hashFunc(data []byte) uint64 {\n    return murmur3.Sum64(data)\n}\n```\nThe 64-bit version is slower but still ~6x faster than the original.\n. ",
    "hxiaodon": "Test code is yours in README.md\nhttps://github.com/coocood/freecache#about-gc-pause-issue\n. ",
    "mqliang": "@coocood How about use interface{}, and use type assertion instead of marshal/unmarsh?. ",
    "toontong": "On the README.md:\n   Notice: \n     If you allocate large amount of memory, you may need to set debug.SetGCPercent().....\nthen, how mush was \"large amount of memory\" ?  1G/process ?  4G? 8G?\n. ",
    "fladz": "My environment is Go 1.7 on CentOS 6.7 and 6.8. I can also see the same mismatched number on Ubuntu 14.04.. ",
    "sparrc": "FWIW, we use the same algorithm in InfluxDB (I modified it slightly to be a single function call, but you get the idea): https://github.com/influxdata/influxdb/tree/master/models. FNV has a high collision rate? this suggests that murmur is even higher: http://softwareengineering.stackexchange.com/questions/49550/which-hashing-algorithm-is-best-for-uniqueness-and-speed\nanyways, wouldn't hash collisions in this case just create a slight bias towards one bucket? Another good option might be just creating a uint8 hash directly rather than doing uint64%255. I'll have to test a bit more, but at the moment I think there might be a rather large bias in the hashing to an int64 % 255 method of calculating the shard to use. I will try to draw something up that compares the results of doing this with doing a Pearson hash, for example.. ",
    "funny-falcon": "I'd rather use anonymous mmap, so there is no need in calling SetGCPercent.. Why atomic.AddUint64 ? There is no concurrent access to local variable.\nWhy this function were changed at all? Can you prove there is a need to change this function?. ",
    "ziyebuboka": "\u5931\u8d25\u4e86 \u4f46\u662f\u6ca1\u6709\u53cd\u9988\u554a \u6ca1\u6709error \u8fd9\u6837\u65e0\u6cd5\u5f97\u77e5\u662f\u5426set\u8fdb\u53bb\u4e86  \u73b0\u7f51\u4f7f\u7528\u7684\u8bdd \u5c31\u5f97\u5148set \u518dget\u4e00\u4e0b\u6765\u5224\u65ad\u4e48\uff1f. \u6709qq\u4e48\uff0c\u53ef\u5426\u52a0\u4e00\u4e0b. \u660e\u767d\u4e86\uff0c\u662f\u5c5e\u4e8e\u70ed\u6570\u636e\u578b\u81ea\u52a8\u6dd8\u6c70\u7684\u7279\u6027. \u975e\u5e38\u611f\u8c22. ",
    "dotNetDR": "\u8bfb\u591a\u5199\u5c11\u7684\u573a\u666f\u3002\u8fd9\u4e2a\u662f\u53ef\u4ee5\u6362RWMutex\u7684\u4e0d\uff1f. ",
    "jaredririe": "\nAre you sure there is only one cache created?\n\nYour question prompted a deep dive into this part of the code and I realized it is possible for multiple caches to be created in a particular case. I pushed out of a fix and will monitor the memory usage over the next day. I believe this is the root cause. . The fix to ensure only one cache can ever be created has been live for a day now, and the memory usage for the cache is right at the requested 100MB. Thanks for the suggestion!. ",
    "glasser": "Out of curiosity \u2014 I get why it can't be larger than 1/256 (separate segments), but where does the extra factor of 4 come from? Is it primarily just to avoid wasting space if your entries take up too big a fraction of a segment, or is there some other reason that that factor must be of 4?. ",
    "docmerlin": "Would returning a TTL be more idiomatic?  I'm unsure which you would prefer.. Ok, fixed.. Woops, good point.. Ok, I made the test way less brittle.\nThanks to @dkowis ;-). ",
    "zhangcunli": "Modify the ringbuffer resize, ringbuffer_test.go:\n////////////////////////////////////////////////////////////////////////////////////\n    ringbuf_test.go:28: [size:32, start:17, end:33, index:0]\n    ringbuf_test.go:29: ghibbbbccccefghi\nsize=16  -->  size to 64  --> size to 32\nNow ringbuffer index should be 16, we should not set the index to 0 and overwirte the ringbuffer;\n. ",
    "jb0n": "I wondered if RWMutex was really acceptable... It worked for my tests, but they weren't able to really exercise all of the functionality (timeouts, for example). I considered just locking a segment as well, but since you were going the atomic route I tried to stick with that. I agree it's probably the simpler solution, however I wasn't trying to redesign freecache, just fix the issues that were causing me problems ;).. Yeah, looking at the code I think it's possible to write a clearSegment() that could handle that without any races. . Thanks, and thanks for the useful go package!. ",
    "negbie": "sync/atomic expects the first word in an allocated struct to be 64-bit\naligned on both ARM and x86-32. See https://goo.gl/zW7dgq for more details.. ",
    "theory": "D'oh, the variable is set in the method signature. Never mind!. ",
    "SmileEye": "\nStore many interface{} in memory will cause big GC impact.\nGC overhead is proportional to the number of long-lived objects.\n\n\u60a8\u597d\uff0c\u4e3a\u4ec0\u4e48\u5185\u5b58\u4e2d\u5b58\u50a8\u5927\u91cf interface{} \u4f1a\u5bf9gc\u9020\u6210\u5f88\u5927\u7684\u5f71\u54cd\uff0c\u53ef\u4ee5\u7ed9\u4e00\u4e2a\u601d\u8003\u8fd9\u4e2a\u95ee\u9898\u7684\u601d\u8def\u4e48, \u8c22\u8c22\u60a8\uff01. @coocood \nThank you for your reply. I will read this blog carefully.. ",
    "muXxer": "@coocood \nSorry for that!\nShould be fixed now.. @coocood \nSolved the conflicts and updated to your latest codebase.\nSorry for the delay. I totally forgot this PR. . ",
    "kxlt": "Any updates on this?. ",
    "deckarep": "Confirmed there is a data-race: https://github.com/coocood/freecache/pull/44. @coocood - thank you as well for this library and for the quick response! . @coocood  - ok, finally got around to making the change.  Please review and thank you.. Sure, I renamed it to segmentCount to be consistent.. Ok, thanks for pointing that out. I will follow up with a fix.. ",
    "unisqu": "i really like this software more than the other memory caching, i can buy 2 cups of coffee for you on this as a token of appreciation. need hash like redis data structure for freecache and this software is perfect. hexpire on key too. thanks in advance.. Can I pay you to develop this feature?. how do i do hdel \"key\" for all hash structures? any ideas?. ",
    "ydf": "panic in processor: runtime error: slice bounds out of range: goroutine [running]:\nruntime/debug.Stack(0xcc477985d0, 0xa486a0, 0xe418e0)\n    /opt/go/src/runtime/debug/stack.go:24 +0xa7\ngit.apache.org/thrift.git/lib/go/thrift.(TSimpleServer).processRequests.func1()\n    /src/git.apache.org/thrift.git/lib/go/thrift/simple_server.go:186 +0x5a\npanic(0xa486a0, 0xe418e0)\n    /opt/go/src/runtime/panic.go:491 +0x283\ngithub.com/coocood/freecache.(RingBuf).EqualAt(0xc42028c0d8, 0xcc3850dad0, 0x28, 0x20, 0x347fffde, 0x7f47246617c0)\n    /src/github.com/coocood/freecache/ringbuf.go:147 +0x1de\ngithub.com/coocood/freecache.(segment).lookup(0xc42028c0d8, 0xcbdc287c00, 0x13, 0x40, 0xcc38501e9a, 0xcc3850dad0, 0x28, 0x20, 0xc420084400, 0x7f472e9ae438)\n    /src/github.com/coocood/freecache/segment.go:346 +0xff\ngithub.com/coocood/freecache.(segment).get(0xc42028c0d8, 0xcc3850dad0, 0x28, 0x20, 0x5635ef0c1e9a47ef, 0xb18582, 0x9, 0xcc477988c0, 0x2, 0x2, ...)\n    /src/github.com/coocood/freecache/segment.go:199 +0x118\ngithub.com/coocood/freecache.(Cache).Get(0xc420246000, 0xcc3850dad0, 0x28, 0x20, 0x28, 0x30, 0xcc384f9560, 0x7f46d00261c8, 0xcc477989d8)\n    /src/github.com/coocood/freecache/cache.go:56 +0xe1\nmain.(RpcServiceImpl).GetCache(0xc4201fa220, 0xcc4de8d420, 0x1, 0x1, 0x2f8, 0xcc00c4d850, 0x9, 0x1, 0x0, 0x0, ...)\n    /src/handler.go:108 +0x150 \nthis  line code\uff08 val, err := this.cache.Get(StringBytes(key))\uff09\nCache := freecache.NewCache(config.CacheGB * 1024 * 1024 * 1024)\nQueryCache := freecache.NewCache(3 * 1024 * 1024)\nhandler := &RpcServiceImpl{}\nhandler.cache = Cache\nIs it reproducible? \n\u751f\u4ea7\u73af\u5883\u8dd1\u4e86\u534a\u4e2a\u6708\uff0c \u590d\u73b0\u4e0d\u4e86\n.   \u4e0d\u884c \u8fd8\u662f \u51fa\u73b0\u8fd9\u4e2a\u9519\u8bef\u3002  debug  \u6ca1\u6709\u4fe1\u606f \n```\n        is_ok := firstLen >= 0 && firstLen <= len(p) && readOff >= 0 && readOff <= len(rb.data)\n        assertF(is_ok,\n            \"firstLen:%d pLen:%d readOff:%d dataLen:%d\", firstLen, len(p), readOff, len(rb.data))\n                if !is_ok {\n            return false\n        }\nfunc assertF(v bool, fmtStr string, args ...interface{}) {\n    if !v {\n        msg := fmt.Sprintf(fmtStr, args...)\n        fmt.Println(msg)\n    }\n}\n```\n. \nerror log\n2018/12/19 20:50:56 panic in processor: runtime error: slice bounds out of range: goroutine 69 [running]:\nruntime/debug.Stack(0xcba1341f68, 0x6e7ea0, 0x8e02f0)\n    /opt/go/src/runtime/debug/stack.go:24 +0xa7\ngit.apache.org/thrift.git/lib/go/thrift.(*TSimpleServer).processRequests.func1()\n    /opt/src/git.apache.org/thrift.git/lib/go/thrift/simple_server.go:186 +0x5a\npanic(0x6e7ea0, 0x8e02f0)\n    /opt/go/src/runtime/panic.go:491 +0x283\ngithub.com/coocood/freecache.(*RingBuf).EqualAt(0xc420141670, 0xcba915c540, 0x12, 0x0, 0x3bffffb, 0x411f17)\n    /opt/src/github.com/coocood/freecache/ringbuf.go:153 +0x3f1\ngithub.com/coocood/freecache.(*segment).lookup(0xc420141670, 0xcc3e9c2000, 0x47, 0x80, 0xcc6a0e2fba, 0xcba915c540, 0x12, 0x0, 0x461ac0, 0xc423ddbc00)\n    /opt/src/github.com/coocood/freecache/segment.go:346 +0xff\ngithub.com/coocood/freecache.(*segment).get(0xc420141670, 0xcba915c540, 0x12, 0x0, 0x488bf9972fba5c16, 0x697cdd, 0x744a6f, 0x9, 0xcba13422d0, 0x2, ...)\n    /opt/src/github.com/coocood/freecache/segment.go:199 +0x118\ngithub.com/coocood/freecache.(*Cache).GetWithExpiration(0xc42013a000, 0xcba915c540, 0x12, 0x0, 0x12, 0x3, 0x4, 0x3, 0x0, 0x0)\n    /opt/src/github.com/coocood/freecache/cache.go:73 +0xe5\nmain.(*RpcServiceImpl).GetCache(0xcba0b36120, 0xcbee320780, 0x8, 0x8, 0x14, 0xcbfec64fd8, 0x8, 0x20, 0x0, 0x0, ...)\n    /opt/src/handler.go:109 +0x16b\nmain.(*RpcServiceImpl).Get(0xcba0b36120, 0x8b7280, 0xc4200120b0, 0xcc295b55e0, 0x66, 0x66, 0x0, 0x0, 0x0, 0x0, ...)\n    /opt/src/handler.go:163 +0x366\npdb.(*pdbServerProcessorGet).Process(0xcba0b340f0, 0x8b7280, 0xc4200120b0, 0x935179, 0x8bc3a0, 0xcba0b6c7e0, 0x8bc3a0, 0xcba0b6c870, 0xcba0b68601, 0x0, ...)\n    /opt/src/pdb/pdb.go:159 +0x2d2\npdb.(*PdbServerProcessor).Process(0xcba0b36180, 0x8b7280, 0xc4200120b0, 0x8bc3a0, 0xcba0b6c7e0, 0x8bc3a0, 0xcba0b6c870, 0xc42012a401, 0x0, 0x0)\n    /opt/src/pdb/pdb.go:126 +0x336\ngit.apache.org/thrift.git/lib/go/thrift.(*TSimpleServer).processRequests(0xcba0b52000, 0x8b8a60, 0xcba0b28390, 0x0, 0x0)\n    /opt/src/git.apache.org/thrift.git/lib/go/thrift/simple_server.go:201 +0x28c\ngit.apache.org/thrift.git/lib/go/thrift.(*TSimpleServer).AcceptLoop.func1(0xcba0b52000, 0x8b8a60, 0xcba0b28390)\n    /opt/src/git.apache.org/thrift.git/lib/go/thrift/simple_server.go:142 +0x73\ncreated by git.apache.org/thrift.git/lib/go/thrift.(*TSimpleServer).AcceptLoop\n    /opt/src/git.apache.org/thrift.git/lib/go/thrift/simple_server.go:140 +0xfa\n2018/12/19 21:10:21 error processing request: Incorrect frame size (218762506)\n2018/12/19 21:10:25 error processing request: Incorrect frame size (1936089459)\n\u5f53\u9519\u8bef\u53d1\u751f\u7684\u65f6\u5019\uff0c\u6211\u6b63\u5728\u5927\u91cf\u7684\u5199\u5165 \u7f13\u5b58\u6570\u636e\u3002 \u7eb3\u95f7\u3002\u3002\u3002\u3002. ok  . func StringBytes(s string) []byte {\n    return *(*[]byte)(unsafe.Pointer(&s))\n}\nI  use  this  function  string  to  bytes  for  cache  key\uff0c it's safe and ok\uff1f. thank u. ",
    "mvasi90": "Then it's a bug.\nI'm changing to lowercase any character and it works correctly.. Wait a moment please, I'm testing since I wrote the last message.\nDon't waste your time, maybe I'm wrong.\n. Sorry, this is my error.\nI'm using two level cache:\n1 - Memory (fast)\n2 - Database (Slow)\nI need this model because I'm using a cluster with many nodes.\nWhen a client is connecting more than once, every time get random node and the memory cache does not exists at all the first time. Then the server checks the database cache clustered to get the id of the session hash (sended by the client).\nThis is all. The problem was the database, the select is case insensitive.\nNow, to keep clear your work I want to remove this issue completely.\nIf you can, please do it.\nThank you and apologize for the inconvenience.. ",
    "jarlyyn": "\u5f53\u7136\uff0c\u6211\u4e0d\u80fd\u6392\u9664\u8fd9\u662f\u6211\u672c\u8eab\u4ee3\u7801\u7684\u95ee\u9898\u7684\u53ef\u80fd\u3002\n\u6211\u8fd9\u91cc\u4e5f\u4f1a\u627e\u4e00\u4e2a\u5176\u4ed6\u7684\u5185\u5b58\u7f13\u5b58\u7cfb\u7edf\u518d\u5199\u4e00\u4e2a\u63a5\u53e3\uff0c\u8fdb\u884c\u5bf9\u6bd4\u6d4b\u8bd5\u3002. \u518d\u8865\u5145\u4e0b\uff0c\u6211\u8bd5\u8fc7 \n    debug.SetGCPercent(100)\n\u4ee5\u53ca\n    debug.SetGCPercent(20)\n\u5bf9\u6211\u8fd9\u4e2a\u6d4b\u8bd5\u4ee3\u7801\u7684\u6027\u80fd\u6ca1\u4ec0\u4e48\u5927\u5f71\u54cd\n. > You can use go profile tool to analyze the performance issue.\n\nSee https://blog.golang.org/profiling\n\n-go-programs\nI have tried profiling before posting this issus.\nThere is nothing strange in the profiling.\nMy problem is not high cpu usage.\nThere is a cpu usage cap in my testing.\nMy freecache driver benchmarking:\n/wrk -t 32 -c 100 -d 30 http://127.0.0.1:8000\nRunning 30s test @ http://127.0.0.1:8000\n  32 threads and 100 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     3.54ms    1.23ms  15.40ms   88.51%\n    Req/Sec     0.85k    66.79     1.72k    75.19%\n  814987 requests in 30.02s, 11.21GB read\nRequests/sec:  27152.36\nTransfer/sec:    382.51MB\nfree cache profile top 10:\n16500ms 28.46% 28.46%    17050ms 29.41%  syscall.Syscall\n    2160ms  3.73% 32.18%     2160ms  3.73%  runtime.memclrNoHeapPointers\n    1560ms  2.69% 34.87%     1560ms  2.69%  runtime.memmove\n    1480ms  2.55% 37.43%     1480ms  2.55%  runtime.epollwait\n    1200ms  2.07% 39.50%     7740ms 13.35%  runtime.mallocgc\n     850ms  1.47% 40.96%      850ms  1.47%  runtime.nextFreeFast (inline)\n     810ms  1.40% 42.36%     2340ms  4.04%  runtime.netpoll\n     800ms  1.38% 43.74%     5560ms  9.59%  runtime.findrunnable\n     760ms  1.31% 45.05%      800ms  1.38%  time.now\n     730ms  1.26% 46.31%      760ms  1.31%  runtime.unlock\nmy redis cache benchmarking:\n./wrk -t 32 -c 100 -d 30 http://127.0.0.1:8000\nRunning 30s test @ http://127.0.0.1:8000\n  32 threads and 100 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     2.72ms  661.80us  23.50ms   78.45%\n    Req/Sec     1.11k    39.17     2.34k    78.75%\n  1062397 requests in 30.02s, 14.62GB read\nRequests/sec:  35387.14\nTransfer/sec:    498.53MB\nredis cache profile top 10:\n```\n   19.94s 28.25% 28.25%     20.77s 29.42%  syscall.Syscall\n     2.73s  3.87% 32.12%      2.73s  3.87%  runtime.futex\n     2.22s  3.14% 35.26%      5.06s  7.17%  runtime.scanobject\n     1.87s  2.65% 37.91%      1.87s  2.65%  runtime.memclrNoHeapPointers\n     1.52s  2.15% 40.06%      1.52s  2.15%  runtime.markBits.isMarked (inline)\n     1.47s  2.08% 42.14%      1.47s  2.08%  runtime.epollwait\n     1.27s  1.80% 43.94%      1.61s  2.28%  runtime.findObject\n     1.10s  1.56% 45.50%      2.21s  3.13%  runtime.pcvalue\n        1s  1.42% 46.92%      1.62s  2.29%  runtime.lock\n        1s  1.42% 48.34%     16.35s 23.16%  runtime.mallocgc\n```\n. I wrapper a cache interface based on in-memory lib \"go-cache\"\nthe get cache value code is \nfunc (c *Cache) GetBytesValue(key string) ([]byte, error) {\n    bytes, found := c.gocache.Get(key)\n    if found {\n        return bytes.([]byte), nil\n    }\n    return nil, cache.ErrNotFound\n}\nlink\nmy go-cache benchmarking:\n/wrk -t 32 -c 100 -d 30 http://127.0.0.1:8000\nRunning 30s test @ http://127.0.0.1:8000\n  32 threads and 100 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     1.35ms    1.22ms  17.13ms   73.45%\n    Req/Sec     2.56k   365.91    11.83k    83.84%\n  2446868 requests in 30.10s, 33.66GB read\nRequests/sec:  81292.72\nTransfer/sec:      1.12GB\ngo-cache profile top 10:\n```\n   14540ms 19.55% 19.55%    15050ms 20.23%  syscall.Syscall\n    2510ms  3.37% 22.92%     2950ms  3.97%  runtime.findObject\n    2490ms  3.35% 26.27%     2490ms  3.35%  runtime.futex\n    2380ms  3.20% 29.47%     5170ms  6.95%  runtime.scanobject\n    1800ms  2.42% 31.89%     1800ms  2.42%  runtime.memclrNoHeapPointers\n    1620ms  2.18% 34.06%     1620ms  2.18%  runtime.markBits.isMarked (inline)\n    1610ms  2.16% 36.23%    23220ms 31.21%  runtime.mallocgc\n    1220ms  1.64% 37.87%     1220ms  1.64%  runtime.memmove\n    1110ms  1.49% 39.36%     3900ms  5.24%  runtime.sweepone\n     960ms  1.29% 40.65%     7320ms  9.84%  runtime.gcDrainN\n```\n. ",
    "1a1a11a": "Hi @coocood, here is what I have done for cache initialization.     \nCache = freecache.NewCache(ramCacheSize)\ndebug.SetGCPercent(20). No I don't. But in order to be accessed by handler, I have the cache as global variable, var Cache      *freecache.Cache, then in the main function, I have Cache = freecache.NewCache(ramCacheSize). . I double checked that it only happens if there are multiple clients writing to it, if I wrap the set with lock, then it won't crash. . I can send a full test if that would be helpful. . OK, I am wrong, it does happen if I let it run longer, so it is not write data race problem. . This is useful, the data passed to set was nil, now it works like a charm. Thank you!  . ",
    "Arafatk": "@coocood Thanks for the reminder.  \nI have set up the email now.   . ",
    "kmiku7": "When the newVal is larger than original, segment will write newVal to new space. The original space maybe not expired and doesn't marked deleted. So the original space will copy to the end of ringbuf when evacuating space.\n. "
}
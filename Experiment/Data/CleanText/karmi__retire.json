{
    "karmi": "Hmm, I don't know if I understand correctly, but yes, I'd like majority of the methods to be able to just take a Hash and feed it down the line to ES. However, I'd like to do this after the API is stabilized. Moreover, full ActiveModel compatibility (see activemodel branch) is the ultimate priority for next couple of weeks.\n. Mine neither :), but ActiveModel for sure (CouchDB in my case).\n. Implemented for Slingshot.search in karmi/slingshot@d1f75227fefb89759728488c841210714b22fe9c\n. Good point, seems like an over-engineering on my part -- ES needs multiple filters wrapped in bool filter, which seems like a good way to accomplish what the API half-delivers. Any ideas?\n. I like the block syntax you're proposing. I'll try to have a look into it.\n. All good ideas. I recently implemented boolean queries in tire@d35e306876d5efbe9ed990b70ea57bc9a197b7a1. \nMultiple filters could be, in fact, defined with the bool filter, I've added a test in karmi/tire@01ba026.\nI think I should expose the same API as for queries, eg. have a boolean method, with must, should, blocks... This way it'd be consistent at least.\nI agree that the more declarative syntax, a la building an SQL query would be much better. But first we have to decouple the query building from the perform step... \n. Thanks!, added test and merged in cae1f5505109109ec2dd3679470914dbc1c445b2.\n. You're welcome and thanks again!\n. Well, that's an interesting (and tough) question, thanks. I'll try at least some answer.\nFirst, I never really understood where the focus of Rubberband is. It feels like a generic \"wrapper\" for ElasticSearch API. It feels like a bit of an overkill with all the Thrift stuff? I can talk to ES via jQuery.getJSON() or RestClient just fine. I mean no offense to Rubberband or @grantr here! But big part of ES charm is the HTTP/JSON interface, which means you can wrap it very easily/quickly and focus on the upper levels of the API.\nSecond, I have been badly burned by depending on gems providing this sort of \"interface\", CouchDB gems being the main culprit. CouchDB gems are a horrible, horible hell, couchrest being the biggest sinner. Of course, subjective, but still my experience.\nThird, I don't think about Tire as a \u201cclient\u201d. The low-level client implementation is deliberately simply \u2014\u00a0and also too weak right now. I'd like the HTTP client to be truly pluggable, so you can use RestClient, Typhoeus, Patron, Curb, ..., whatever you feels is the best library. RestClient is just the start. There's sure performance to be gained by plugging-in a better HTTP client.\nI think about Tire as being a very strong API for using ES in Ruby and Rails. Hence the amount the energy poured into the ActiveModel integration, in searching, importing, etc... That's the real focus of Tire, not hauling JSONified Hashes back and forth between ES and thin Ruby methods.\n. @grantr thanks for the reply! We're quite happy with using HTTP transport in production, since that's what we are quite used to; we know how to scale, proxy, authorize, etc it. I think the fact that ES exposes the API via sane HTTP to be its major benefit, maybe the most important one...\n. @pfeiffer Is this reply satisfactory at the very least?\n. Ah, thanks for the issue report.\nIt is solved in karmi/tire@9d51a28 and version 0.1.2.\nAnd no, you cannot easily namespace it, because Tire just overloaded the Mongoid index method. I renamed the offending method for the time being, so you can use it. Please ping me if not.\nHowever, the real solution is to wrap everything in Tire mixins in a proxy object, not add class/instance methods directly. I'll bite into this next week. See issues karmi/tire#8 a karmi/tire#9.\n. Yeah, that's what @pacoguzman suggests in #9. There's no discussion it's the easiest, simplest way to accomplished the isolation.\nI don't like the semantics of that, though. With tire_index, or elasticsearch_index, it still makes some sense. But what about tire_mapping or elasticsearch_mapping (and settings)? This has nothing to do with Tire not ElasticSearch, but with the index corresponding to the model.\nAnd then, what about eg. MyClass.percolate? That would have to be also \u201eprefixed\u201d, for consistency... How would the source look like when all the Tire methods would be prefixed like this in a lame attempt of achieving isolation? How would the RDoc look like? Silly.\nSo, no easy answers here...\n. DONE! https://github.com/karmi/tire/blob/1255bfc9964dff2b053776ff7993d792022aff29/lib/tire/model/search.rb#L157-169\n. So, @antares, it was a bit more of work in the end :), but I just could not have stand having a Tire::Model::Search::ClassMethods::es_search and Tire::Model::Search::ClassMethods::es_index and es_<WHATEVER> in the source.\nMany thanks to my guiding light @svenfuchs for keeping me on the straight path! :)\n.  Yeah, that's exactly what I'd like to do in the end -- but only after the \"proxy\" object (karmi/tire#8) is available...\n. DONE! https://github.com/karmi/tire/blob/1255bfc9964dff2b053776ff7993d792022aff29/lib/tire/model/search.rb#L177-191\n. Hi, there's no elegant solution for this problem with associations, in the current version.\nThe wrapper is set to the model, and obviously, the model inicialization fails with NoMethodError (or other) error when being passed the hash, which was returned from ElasticSearch.\nFirst, I have anticipated the issue. As you can see in commit karmi/tire@5d89dd1, Tire was initializing the collection from the records retrieved from the database, originally (via MyModel.find [list of IDs]). This, however, had many issues. The performance was unnecesarilly dumped by roundtrips to the database. The sort order was not preserved (a huge bug). So I decided against it.\nSecond, the problem can be immediately solved. Let's simplify your situation (because that's really a M to N relation, with videos and categories).\nThe Article model has many Comment associations:\n```\nclass Article < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nindex_name 'articles-with-comments'\nhas_many :comments\nend\n```\nThe Comment is simple:\nclass Comment < ActiveRecord::Base\n  belongs_to :article\nend\nNow, obviously, when you have the Article indexed as:\ndef to_indexed_json\n  { :id      => id,\n    :title   => title,\n    :content => content,\n    :comments => comments.map { |c| { :id => id, :author => c.author, :content => c.content }  },\n  }.to_json\nend\nit would break on Article#comments= being given wrong type of objects on initialization.\nYou can temporarily, and in a quite ugly way, solve it like this:\nclass Article < ActiveRecord::Base\n  # ...\n  alias :original_comments= :comments=\n  def comments=(comments)\n    # Are the objects returned from ElasticSearch?\n    if comments.all? { |c| c.is_a? Hash }\n      comment_ids = comments.map { |c| c['id'] }\n    else\n      send :original_comments=, comments\n    end\n  end\nend\nSo, when we are initializing the comments from ElasticSearch, set the ID (an retrieve the records from database), otherwise, perform the original method.\nOf course, this could be isolated into completely different model, like this:\n```\nclass SearchedArticle < Article\n  index_name 'articles-with-comments'\ndef comments=(comments)\n    comment_ids = comments.map { |c| c['id'] }\n  end\nend\n```\nYou'll then search via this fake model: SearchedArticle.search 'love'. Of course, this is an ugly, and temporary way of dealing with the issue. The underlying issue is, of course, blocking for advanced production usage.\nWhen I have discussed this with another Tire users, the real, and elegant, solution seems to me like this:\n- Do not wrap the results in the real model class, but wrap them in Tire::Results::Item, as usual\n- Add a proxy object to every result, let's say object, which would point, via the ID, to the underlying record (and model instance)\nIn this way, we would keep the awesome performance of ElasticSearch, for most cases. Where you'd need or like to get the original model instance, you'd just write result.object.comments.first.my_complicated_method, instead of result.comments.my_complicated_method.\nThank you for the report, and I'll definitely have a look at this issue.\n. @russ, any luck with the proposed solution?\n. Understood. Thanks for the report!\n. Hi @russ, I've tried to solve this issue with ActiveRecord associations -- see the closing commit.\nCould you test it against your use-case, if you still have the code handy? The only thing you need is to define Git as endpoint in the gemfile:\ngem \"tire\", :git => \"git://github.com/karmi/tire.git\", :branch => \"activerecord\"\n. @aaronchi: It looks, but let's see how it works in real world :)\n. Yes, I think model is better then object... But none feels 100% right to me...\n. This issue is a tricky one, and there are no easy shortcuts.\nOf course, the ideal solution from the user's point of view would be to return full model instances, so there's no difference between Model.search and Model.all, as you write.\nThere are, however, lots of problems.\n1/ Of course, we can always just load the records from the underlying datastore, by their IDs. But, as stated in karmi/tire#10, this would seriously damage the search performance. ElasticSearch returns hits for reasonable datasets or with enough firepower (RAM+nodes) in the tens of milliseconds magnitude. The roundtrips to the db could make this very costly -- and let's remember, the usage in Rails, with the additional overhead of HTML construction, etc -- is not the only use-case. There are use-cases where the raw speed does matter a lot. (Think Ajax autocompleter, for instance.)\n2/ ActiveRecord, by default, does not preserve the order of IDs. So when you do a MyModel.find [1, 2, 3], the order really isn't guaranteed. This is really bad, because the search engine tries very hard to actually do sort the results based on relevancy. Firing 30-50 queries against SQL or CouchDB really isn't that fast.\n3/ This all is still on the technical level, solvable probably. But let's have a user who indexes her articles like this:\n``` ruby\n    class Article < ActiveRecord::Base\n      # ...\n      def to_indexed_json\n        { :id      => id,\n          :title   => title,\n          :content => content,\n          :categories => comments.map(&:name),\n      # >>> NOTICE THIS ONE >>>\n      :length  = content.size\n      # <<<<<<<<<<<<<<<<<<<<<<<\n    }.to_json\n  end\nend\n\n```\nWhy would she index the length of the article like this? Well, because she wants to run a statistical facet against the length, of course, retrieving min/max/avg/etc calculations!\nNow what? Does she have to implement an #length accessor methods just for the sake of Tire? That does not make any sense. She could be perfectly happy with displaying article.title and results.facets['stats']['mean'].\nIf, and only if, she would like to display something not present in the JSON data from ElasticSearch, such as Article#collaborators, well she would just write article.<PROXY>.collaborators.\nNotice I still don't know what's the best name for the proxy object.\nMaybe load? Since of course, the object would have to be lazy loaded upon invocation, so there would be no roundtrips to database when you'd be satisfied with the JSON data returned from ElasticSearch.\nDoes it make sense like this?\nI keep thinking this is the best balance between ElasticSearch performance, results usability and deep integration into some complicated relationships between models.\nP.S.\nMany thanks to my man @kubicek for the idea and discussion.\n. @floere: Yes, it'd be trivial to sort the results based on sort order from ES results. And yes, it would be faster then firing queries. However, this is the easisest one of those problems.\n@pfeiffer: I would definitely aim for not hitting the datastore, unless absolutely necessary and needed. It just doesn't make any sense to sacrifice speed. Speed is the currency of web applications.\nAnd yes, it would certainly hit N+1 problems. However, since the results are paged in a typical Rails scenario, the N would be kept rather low, I think. Unless we add some options like :include to the search method, which is then passed to find, there is no ultimate solution for this. But maybe I'm missing something, I really haven't extensively used an SQL-based datastore for more then two years :)\nIt's a question of balance here. I'll bite into this next week, with some realistic use case (searching support tickets for a hosting company), and we'll see how it goes. While I think painless integration into Rails app is the absolute requirement, let's not get distracted by ActiveRecord complexities too much. In couple of years, the default storage for Rails will change to a document-oriented storage , that's for sure.\n. @jkraemer: Thanks for the feedback, Jens. There's a great chance you're right, and everything else than simply loading the records from the database is probably an overkill.\nAs evident from the closing commit, I thought that giving trying this \"proxy\" approach is worth giving a try. I did it so it preserves all the elasticity of simple JSON serializations, and allows loading the records from the database, complete with support for stuff like Article.find 1, :include => 'comments'.\nAs stated in tire#10, I already had the loading from storage implemented in karmi/tire@5d89dd1. It quite obviously worked, but I wasn't very happy with the semantics, though.\nI'm trying hard for the Tire API to be straightforward, minimal and generally out of your way. All the different options must be documented somewhere, etc. That said, I can well imagine that I'll implement some option for the search methods to eagerly adn simply load records from the datastore. I can imagine, there are situations, where that is preferrable.\nThe premise right now is that it's generally preferrable to have easy access for properties stored in ElasticSearch with the dot notation (article.comments.first.title), and to be able to load the \"real\" model (article.load(:include => 'comments')), when needed.\nI'd be very happy if you could try this in your apps, the only thing needed is to load the gem from Github:\ngem \"tire\", :git => \"git://github.com/karmi/tire.git\", :branch => \"activerecord\"\nOne final thought: I'm doing all this because my intuition tells me to keep search results as far from the database as possible, and it's something we have agreed upon with @kimchy. ElasticSearch does an awesome job in storing and returning data, and I don't want to hamper it by issues coming from database roundtrips. In fact, when the search results does not contain all the data you need to display them, you may be doing something wrong.\n. OK, it seems like I may have found some balance in this.\nBy default, records are loaded from ElasticSearch and instantiated as Results::Item instances, as described in this issue.\nWhen needed, you can pass and option like :load => true to search methods, to load records directly from database, based on IDs returned from ElasticSearch (see issue karmi/tire#61):\nruby\n    # Will call `Article.search [1, 2, 3]`\n    Article.search 'love', :load => true\nYou can pass any option eg. for ActiveRecord instead of true, of course:\nruby\n    # Will call `Article.search [1, 2, 3], :include => 'comments'`\n    Article.search :load => { :include => 'comments' } do\n      query { string 'love' }\n    end\nI'm all ears for any feedback on this.\n. Hi all -- I'm not currently planning any focus on attachment support. There are many parts of the codebase which need to be refactored and cleaned up, many important features piled up in pull requests and my mind, and I don't immediately need it. I tend to not implement features without real use cases to check them against.\nThat said, there's some shitty code in the attachments branch which I sketched when I was playing with it. Feel free to check it out and play with it some more.\n. @vecernik You can work with attachment support via the \"sending Hashes over HTTP\" approach, which Tire (and other ES libraries) support. There is yet not a native, Ruby-like wrapping available.\n@ryudice It is absolutely planned as a feature.\n. Sorry, I haven't any time to look into this lately -- also I never heard about kaminari :)\nI think the compability should be easy to add... thanks @khoan for the pointer.\n. I'll definitely look into supporting kaminari as soon as I clean percolater and other stuff piled up.\n. Closed by karmi/tire#105.\n. merged, thanks!\n. merged, thanks!\n. Hi, this is definitely weird, could you check it in the app generated by the Rails template?\nI have a simple app where created_at/updated_at are definitely returned.\nMake sure you include them, if you define any mapping!\n. Hi, should I close the issue then? Was mapping without created_at/updated_at the culprit?\n. Great, thanks.\n. Not sure -- yajl is pretty much the defaul JSON gem? It sure is preferred in Rails. One option would be to use ActiveSupport JSON support.\nCould you paste the errors you get?\n. That's not a valid JSON. Check http://www.jsonlint.com/.\nNotice:\n```\nrequire 'rubygems'\nrequire 'yajl'\np Yajl::Parser.parse(%Q| { \"plan\" : { \"intent\" : 1, \"privacy\" : 1, \"body\" : \"testing\" } } |)\n```\n. Yeah, or use single quotes to wrap the JSON:\ncurl -v -H 'Content-Type: application/json'\n        -H 'Accept: application/json'\n        -u username:password\n        -X POST http://staging.testingserver.com/users/4d6779338b53226d2c000001/plans -d '\n          { \"plan\" : { \"intent\" : 1, \"privacy\" : 1, \"body\" : \"testing\" } }\n        '\n. Hi, could you post a full example, with couple of docs, index creation, query, please? So I can recreate on my side?\nKarel\nOn 17.May, 2011, at 21:59 , willoughby wrote:\n\nWhen using filtering by facets only the counts do not seem to reflect what the search results are\n@s = Tire.search \"14\" do\n         query do\n           string 'title:*'\n         end\n         filter :terms, :prices => [\"#{price}\"] if price\n         filter :terms, :tags => [\"#{tag}\"] if tag\n         facet 'current-tags' do\n           terms :tags\n         end\n         facet 'current-prices' do\n            terms :prices\n         end\nend\nEmpty search returns\nTags\n1 - 48 (coresponds to \"Kitchen\")\nPrices\n1 - 2282\n2 - 1455\n4 - 258\n3 - 138\nFilter based on tag 1 apears to select the correct documents but facet counts remain as above. however by editing the query to title :kitchen\nthe facets look correct\nTags\n1 - 48\nPrices\n1 - 48\n2 - 47\n4 - 14\n3 - 4\nHope this make sense.\nRegards\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/21\n. Hi, sooo, sorry it took so long. Now I understand what you mean, looking at http://www.elasticsearch.org/guide/reference/api/search/filter.html.\n\nI think it should be doable right now, because facet options like global are encoded:\n```\nTire.search 'someindex' do\n  query { string \"title:*\" }\nfacet 'tags' do\n    terms :tags, :facet_filter => { :and => [ {:term => { :tags => 'ruby }}, :term => { :category => 'one' }  ] }\n  end\n# ...\nend\n```\nI'm writing this from heart, so it may not work as expected. Could you try it?\n. Ping, any updates, @willoughby? :)\n.  @agungyuliaji  https://github.com/karmi/tire/blob/master/test/integration/facets_test.rb#L296-L305\n. Hi, great! Would you be so kind and add also a unit test in test/unit/search_facet_test.rb? I'm using mostly unit test to check for API consistence during work -- and probably will rewrite parts of the API, so it would be handy to have it.\nThen just commit --amend and push --force and I'll merge the rebased commit. Thanks!\nKarel\n. Argh, nevermind, too tired to see the unit test actually :)\n. Merged in karmi/tire@df6ad62, thanks!\n. Hi, thanks! I've cleaned up and merged the 1.9.2 related commits, see https://github.com/karmi/tire/commits/master.\nI didn't update the Rake and Rails dependencies, since I don't think the gem could depend on prerelease versions, and, more importantly, I would like to redo the dependencies anyway. Probably removing the Rake dependency altogether, and removing the ActiveModel dependency with a warning to the user when ActiveModel is not already loaded.\n. Hi, thanks, squashed and merged in karmi/tire@6a3b4f3. However, it was later superseded by @vhyza's patch at karmi/tire@7d57038 :)\n. No, I just try to stick \"implement only what you need\" philosophy. In almost any case when I diverged from that it was pain :)\n. Thanks, merged in karmi/tire@fa93077a5baebc3bb3d3\n. Thanks!, merged in karmi/tire@7d57038\n. Hi, thanks! I've squashed the commits into into one, and edited. Merged in karmi/tire@d51f0cc1d143eb7907a1c30cd581335ad4babee0.\nI am aware that the duplication in the execute >\u00a0rescue >\u00a0raise >\u00a0ensure is quite dumb. However, I don't like having the wrapper method like you did there. There's certainly a better way, most probably in augmenting the Client class, which is only a thin wrapper for now.\n. Regarding the other two issues:\n1/ Yes, forcing yajl on everybody is absolutely stupid. Rails now goes with the intridea/multi_json gem, so I'll follow suit.\n2/ Wow, Array having a to_hash method? This is coming from some library? @floere corrected the previous check for is_a?(Hash) (which is what you should have used?), see commit tire@081eb36. I think we can return to previous version. Could you find the library which implements Array#to_hash?\n. OK, the yajl dependency removed in tire@90011cb4e1b3bb29666b75beb01ed50213029e78.\n. >  relying on that method name to determine if something is a hash is a bit cavalier\nLOL, definitely! I'll change it to the previous version, no harm by that and you'll be sorted.\n. OK, all issues should be fixed and pushed at tire@v0.1.5, also released to Rubygems. Cheers!\n. Yeah, this is ActiveSupport, most probably, where to_json takes a Hash of options. However, I don't get any error for this, since this options Hash is optional. Do you have a specific error, gem, etc., where it fails?\n. This is really weird, since we run a Rails 3.0.7 app in production as well, and the demo app (rails new searchapp -m https://github.com/karmi/tire/raw/master/examples/rails-application-template.rb) runs fine as well. I'll have another look into this...\n. I am still not convinced this is the case. Try to reproduce it like this.\nFirst, generate a demo Rails 3.0.7 app:\nrails new searchapp -m https://github.com/karmi/tire/raw/master/examples/rails-application-template.rb\nThen, change the JSON library to the \"json\" gem in your Gemfile:\ngem \"json\"\nand load the JSON library in your application.rb:\nrequire 'rails/all'\nrequire 'json'\nNow, does searching in the app work for you or not? Because in my case, I get no ArgumentError. And rightly so, because the \"json\" gem really does have any arguments for to_json as optional. Evidence: https://github.com/flori/json/blob/master/lib/json/pure/generator.rb#L257\nSo, if it does fail for you in the example app, then we'll dig into this deeper. I'm rather reluctant to add all the throwaway arguments to all the to_json methods. I'm happy that Tire does not depend on \"yajl\" now, but wouldn't like to over-compicate stuff.\n. Yeah, I've found this as well on my own. Seems like requiring Rails, ie. requiring ActiveSupport somehow makes the good ole' \"json\" gem to behave... I think adding the throwaway args to to_json methods in Tire is the only solution, really.\n. No, no hard feelings whatsoever! I just felt I should reply there, since a friend referred me to your article. I think you were too harsh on Ruby and its ecosystem, so I replied. And the more I think about the more I wonder why to_json methods in Tire with arity==0 don't fail at all times :) \n. Thanks for the report -- this issue is really hard to hunt down, since different components overload JSON serialization/behaviour differently. Could you try to to change the Hash#to_json definition in https://github.com/karmi/tire/blob/master/lib/tire/rubyext/hash.rb#L3 to:\nruby\ndef to_json(options = nil)\n  MultiJson.encode(self)\nend unless respond_to?(:to_json)\n. Yes, it should be fixed in tire@v0.1.6...\n. Thansk for the report! The place to fix this is definitely in Tire, and should be fixed now; tests pass and apps run even on the \"OKJSON\". All these different JSON serialization libs are a bit of a pain.\nThe best is to really declare yajl-ruby in your Gemfile:\ngem 'yajl-ruby'\nand to require the \"json_gem\" compatibility layer somewhere in your app (initializer, application.rb, etc):\nrequire 'yajl/json_gem'\nLet me know if it works for you!\n. I'm not sure I understand the problem, and even less the code. What is the real issue here? What is the actual response from ES?\n(I suspect the issue is that ES returns pseudo-JSON like _source.inner_object and the Item instantiation fails on that, but I am not sure.) \n. OK, thanks for clarification. I think this unexpected behaviour of ElasticSearch. The documentation at http://www.elasticsearch.org/guide/reference/api/search/fields.html states:\n\nThe fields will automatically load stored fields (store mapping set to yes),\nor, if not stored, will load the _source and extract it from it (allowing to return nested document object).\n\nAnd this is probably not the case. Another user, @vhyza has already hit this issue as well.\nIn your case, you should have a fields=meta,... and the returned field should be simply meta, not _source.meta.\nNow, you are working around this by setting fields to fields=_source.meta. I think ES should return you response like this:\n\"_source\" : { \"meta\" : \"...\" }\nand not like this:\n\"_source.meta\" : \"...\"\nThat response would be properly parsed by the Item.new logic.\n@kimchy, do you have any ideas or suggestions for this?\n. @kimchy, so it's intentional, that ES returns JSON like \"_source.meta\" : \"...\" and not \"_source\" : { \"meta\" : \"...\" }.\nIf so, then client libraries have to take this into account and strip the _source.* prefix....\n. OK, what a pity :) But OK, I'd probably then:\n- strip _source from any key\n- split the key by . and recursively build the Hash out of the resulting Array.\n. @bcoe, yes, I'll have a look if it could be simplified somehow...\n. I have cheated and used eval to do the trick for building the nested hashes. Seems more intelligible to the casual source code reader and more expressive with regard to the code intent; see the Collection#__parse_fields method.\n. Hi Nick, yeah, from the RESTful perspective, definitely, thanks! I''ll try to check it out in production and in some Tire apps.\n. Hi, thanks!!, merged in karmi/tire@26ea1e7, I'll test it against production early in the week, just to be sure, and merge it into another release.\n. Thanks again, released as v0.1.9.\n. Hi, what the snipped really does is throwing out any attributes which are not included in the mapping block, if the mapping was defined.\nYou have to redefine to_indexed_json in your case, there's info about that in the Readme. You should be able to simply something like:\nclass Article\n  def to_indexed_json\n    to_json(:methods => ['character_count'])\n  end\nend\nI'm not happy with the current implementation of redefininig to_indexed_json, it's too low level, although it works...\n. Could you post a more complete example?\n. Well, this is how to do it https://gist.github.com/0cb9b0df72b7bbe13767, AFAIK.\nYou have to include the permalink method in mapping, and then just call self.to_json(:methods => :permalink).\n. > to_json with methods does not seem to work because to_json returns a nested hash with the object name as the key.\nYou have to set :root => false, or include_root_in_json = false.\nFor the read-only thing, that's currently a big sore spot. See issues karmi/tire#11 and karmi/tire#12. There's a \"solution\" listed in issue karmi/tire#10. All this should be easier once the \"proxy object\" feature is implemented.\n. Yes, ES does store all the JSON you throw at it -- that's great because then you don't have to query the database, in most cases. This is seriously on the biggest benefits of ES.\nYou can disable storing the fields, or disable storing the JSON altogether, please refer to ES documentation, eg. http://www.elasticsearch.org/guide/reference/mapping/source-field.html.\nAs for Tire, it is just a wrapper around ES API, so yes, with proper mapping, it would only index specific fields and not store them.\nAs you can see in issues #11 and #12, I intend to change the behaviour so only the contents of _source in ES would be used, and for everything else, the underlying storage would be queried.\n. Yes, you could configure the field store properties in the mapping block (as shown in the Readme). However, to disable source altogether (even thoug I'm not sure why to do that, for majority of cases), you would have to use something like Tire.index('something') { create :mappings => { :mymodel => { :_source => 'disable', :properties => { ... } } } }\n. Closing, since this is a duplicate of issues #10, #11 and #12. Thanks for all the debugging!!!\n. Hi, you can just pass a Hash (or JSON) to the Tire.search DSL method, there's an example in the Readme. This is, however, kinda crude at the moment (for instance, no logging). You cannot easily -- without resorting to constructing your query from low level classes such as Search::Query mix &\u00a0match it with the DSL methods, it's an either/or thing.\nI intend to make this types of searches more convenient, once I get the Tire plate cleaner (percolator, proper ActiveRecord handling, etc).\nAdding geo distance facet is easy, though, just a matter of another method in Search::Facet and couple of unit &\u00a0integration tests.\n. Yes. Sorry for late reply.\nI am still undecided if falling back to string as default is a good idea or not. Too much magic really can quickly hurt with ES, because you really need to understand what's going on.\nIn cases like this, always double-check the mapping, either with Tire.index('events').mapping or http://localhost:9200/events/_mapping.\n. Hi, you named your facet commercial_type, not tags, see https://gist.github.com/e2811c9f99b9be2b4f67\nSee also the docs on ES facets, http://www.elasticsearch.org/guide/reference/api/search/facets/index.html!\n. Well, it's probably just some side effect, it should be accessed via results, see the integration tests...\n. Yeah, but that's precisely why I wonder that you'd not need accessing it via the results method :)\n. Sorry, not enough time right now, you're right.\n. Yeah, in ES > 16.2 you don't need a query, this was fixed in karmi/tire@94da719.\n. Yeah, this is a huge performance debt in Tire, currently. I'd like to add count and scan soon, the ES API makes it very convenient to support.\n. Which version of will_paginate are you using? It works with 3.0.pre2 for me.\n. Try generating the demo application as documented in the Readme. That uses will_paginate ~>3.0.pre, and definitely works.\n. Yes, and it should definitely be optional, as in will_paginate. Again, the tests: https://github.com/karmi/tire/blob/master/test/integration/active_record_searchable_test.rb#L83 are really the best docs and starting point right now...\n. Could you provide some example code? You can access variables from the same scope, eg. params in controller. You cannot access local variables across the \"scope gates\" (def, class, module)\n. I don't understand, still.\nThis works:\n``` ruby\nrequire 'rubygems'\nrequire 'tire'\nusername = 'test'\nTire.index('photos') do |index|\n index.register_percolator_query(\"meta.username:#{username}\") do |query|\n   query.string \"meta.username:#{username}\"\n end\nend\n```\nOf course self.username does not work in the register_percolator_query, since self is Tire::Index there, and rightly so.\nIn the block, you only have access to the context of enclosing scope. \nIn our case, I'm happily doing something like this:\n``` ruby\nclass Alert\n  attr_accessor :name\n# ...\ndef register_query\n    name_escaped = Rack::Utils.escape(name)\n    myclass.mydocs.elasticsearch_index.register_percolator_query(name_escaped) do |query|\n      query.boolean &main_query\n      query.boolean &keywords_query\n    end\n  end\nend\n```\nBy the way, why are you using the top level DSL such as Tire.index when clearly you have some entity such as Photo? Why don't you create a class for it and mix the Tire methods into your class?\n. Are you sure you're refreshing your index between inserting test data and querying it? There's a 1sec delay by default. See all those refreshes in the Tire integration test suite...\n. Hi,\nthe \"will_paginate\" style of pagination with :page, :per_page is only supported in the \"parameter\" style of searches, see: https://github.com/karmi/tire/blob/4c97d0e40f6f0e4da469fd1b6ffe4f6aa8b539d6/test/integration/active_record_searchable_test.rb#L92.\nFor block searches, you have to specify paging yourself inside the block, see:\nhttps://github.com/karmi/tire/blob/4c97d0e40f6f0e4da469fd1b6ffe4f6aa8b539d6/test/integration/active_record_searchable_test.rb#L150-155\n. Hi, thanks! I'll have a proper look on this.\nHowever, the real issue is to be able to \"lazily\" perform the search in models? I don't see the benefit of \"lazy\" here? You aim for not instantiating the documents collection at all? If that is the case, yeah, some (little) RAM/time can be probably saved by this. (Implementation-wise this is perfect, no need to populate/populated hoops.)\nThanks for the instantiate stuff!, also, I was searching for a way how to get round this.\n. No, instantiating the collection only when needed is fine. Your case is perfectly valid and in that case setting the size to 0 is preferred. I still have to implement the count search type, perfect for this.\n. Thanks!, merged in v0.1.14!\n. Hi, yeah, definitely, just call Tire.index('myindex').remove('docID'), see integration tests: https://github.com/karmi/tire/blob/master/test/integration/index_store_test.rb#L38\n. Ha!, yes, sorry, in your case you have to call Tire.index('myindex').remove('my-document-type', 'my-document-id').\nThe whole passing of type into store/remove stinks a lot, I have to really make it infer the type from the document itself, there's a TODO for that. I'll submit an issue for this.\n. Thanks, merged in 0.1.14 -- in fact, I think the proper way to solve it is karmi/tire@2c98b98. That way, with yajl, the JSON is still prettyfied, but does not crash where not available.\n. I am not sure about your input data, since you didn't provide any. But given the input:\n``` ruby\n    index 'articles' do\n     delete\n     create\n store :id => 1, :title => 'One', :author => { :name => 'John Smith', :email => 'smith@example' }\n\n refresh\nend\n\n```\nI can search for, and highlight, field author.name like this:\n``` ruby\n    s = search 'articles' do\n      query { string 'author.name:John' } \n      highlight 'author.name'\n    end\ns.results.each do |document|\n  puts \"Title: #{ document.title }, highlight: #{document.highlight.inspect}\"\nend\n\n```\nIs this correct for your use-case?\n. Thanks!, merged in karmi/tire@2c518c\n. OK, this was a tricky one. Yet, the solution is very simple:\nbash\n$ rake environment tire:import CLASS='Article.all' INDEX='mongo-articles' FORCE=true\nApparently, Mongoid does not support Article.paginate, but Article.all.paginate, for reasons beyond my understanding :)\nPing me if this works -- it does in my example app. Tire does not care what has the paginate method, only that something in the app has it :)\n. Ping, any news?\n. > I can't figure out if there's a better way (...)\nYes, there is:\nruby\nArticle.elasticsearch_index.import Article.all\nas documented in the Readme.\n\nWith mongoid 2.0.2 will_paginate is no longer supported. That's what's causing this error (...)\n\nNo, I don't believe this is the case. I have a Mongoid 2.0.2/Tire test application and I am able to import data the way I posted above:\nbash\n$ rake environment tire:import CLASS='Article.all' INDEX='mongo-articles' FORCE=true\nAnd I am able to paginate the results with eg. Article.all.paginate :per_page => 1, :page => 2.\n. > (...) if you're trying to use Article.import it fails (...)\nAh, yes, of course it does. That's why I was referring to the Rake task at all times, which was the original issue. (It is easy to pass random object to the Rake task like this.)\nDo not use the Article.import interface then, use the Article.elasticsearch_index.import one! You can pass any Enumerable compatible object to the Index#import method (which in turn uses Index#bulk_store), see http://karmi.github.com/tire/#section-Bulk_Indexing.\nThis way, you have to provide your own pagination, of course. This has no relation to kaminari support, though, I tend to think. If you decide not to use will_paginate, it's no problem, you just have provide your own pagination...\n. > Article.import it fails because it wants Article.paginate (...)\nIn fact, you can pass any method, \"paginate\" is only default: Article.import :method =>\u00a0\"whatever_pagination_i_have\". It may well work in your case?\n. @aaronchi: Yeah, but that only applies for ActiveRecord... And ActiveModel has no common find_each/find_in_batches semantics I am aware of... (For me, personally, ActiveModel-based models are the real important target. Don't know about other people.)\n. @wulffeld: Hmm, yes. I've spent some time fiddling with this in the console, and all this stuff is somehow too weird.\nI was able to implement Article.paginate proxy to Article.page(options[:page]).per(options[:per_page]), as you suggest. After changing the empty? check to size check, the Rake task works fine out-of-the-box, as the Article.import. I'll commit the changes neccessary so you can check them out in your app?\n. Great! So, for posterity, here's how to roll import with Tire and Mongoid:\n1/ You're using the will_paginate gem with Mongoid \u2014 just follow the instructions in the docs and in the Readme.\n2/ You're using the kaminari gem with Mongoid \u2014 you have to provide the paginate method in your model, like this:\nruby\n    class Article\n      # ...\n      def self.paginate(options = {})\n         page(options[:page]).per(options[:per_page])\n      end\n    end\nThen just follow the instructions in the docs and in the Readme.\n. > When I run the import rake task I get undefined method 'paginate' for MyModel:Class\nOf course -- the paginate method does not exist in your model, when you're not using will_paginate. If you don't want to use will_paginate, you have to implement it yourself, as in the example above...\n\nSince MongoDB uses cursors to retrieve queries in batch mode natively, why is there a need to paginate for this task?\n\nWhat MongoDB has to do with this? The import task makes an assumption that paginate will be implemented in most cases, MongoDB or not.\n\nI would like to be able to override the method, 'paginate', used in the rake task.\nI understand the last thing you may want to do is introduce another ENV variable to the task but the task does not work for an application unless it is using will_paginate or kaminari... That seems a bit off.\n\nIf it helps you, OK, why not make the method configurable. But I don't think it will help you -- the method would have to have the same interface as the  will_paginate method, and the only benefit you're gaining is different name.\nTire makes the following assumption: if your application is not using some sort of pagination,\nthe amount of data you probably have is so small, that you shouldn't bother with Rake tasks and just write:\nruby\nArticle.tire.index.import Article.all\nAgain: everything about this is about assumptions and 80/20 ratio. The thinking behind the reliance on pagination method is this:\n- When you have nontrivial amounts of data, you are using some sort of pagination.\n- When you are using pagination, there's high chance you're using the paginate method of will_paginate -- problem solved. So we don't care about ActiveRecord's find_each and every crazy batch finder out there. We focus on one, quite common pattern.\n- When you're not using will_paginate, you know what you're doing and probably can emulate the paginate method, see code above -- problem solved, again.\n- When you have a highly specific use case, it's better for you to handle the import yourself anyway, and Tire::Index#import makes it very easy for you.\nMakes sense?\n. > Since MongoDB uses cursors to retrieve queries in batch mode natively\n\n\nWe will figure something out.\n\n\nI don't know anything about how to practically use cursors in Mongo -- if you'll give an example, I can help you with the Tire side of things. Catch me here or on #elasticsearch's IRC.\nIf it behaves like ActiveRecord's find_each, you can load batches of records (say by 1000) and feed them to Tire::Index#import method. Or, you can just wrap it in the paginate method and use the Rake task :)\n. Hi,\ncould you add unit and integration tests for the feature, please?\n. Hi, any progress on the patch?\n. Hi,\na) why there are not unit and integration tests attached to the patch?\nb) I think what this feature really hints at is something like an Index#status method, returning an Index::Status object, wrapping num_docs, deleted_docs, size, etc?\n. Hey, @housepage, any news, comments?\n. Hi, \na) is this useful on the DSL level?\nb) why not use Index.retrieve?\n. Hey @housepage, any news?\n. Hi, is this really something worth additional three lines of code and unit tests? :)\n. >  I managed to waste a couple of hours figuring out the error. \nThen it's definitely worth it :)\n. Thanks!, merged\n. Hi, thanks, merged and cleaned up + amended the logic a bit. Now the interface makes much more sense.\n. It's more complicated then that. I've pushed karmi/tire@648c77 with sanity check for ActiveModels. Tire is definitely not converting IDs to Integers.\nCould you be more specific as to your use-case? Do you use ActiveModel or ActiveRecord? How is your id column defined in the database? I am not able to replicate your bug.\n(P.S. Why are you calling result._id and not result.id.)\n. OK, so if it's ActiveRecord, you should check with your migrations or in dbconsole, how the column in question is defined, eg. with describe <YOURTABLE> id;.\n\nThe ID from the target (mysql) model isn't the same as the uuid that I'm putting into ES. I was making an _id column in mysql to be the target.\n\nSorry, I don't understand what you mean.\n. I would gladly help but I don't understand what is your problem.\n. Closing the issue as invalid, the discussion can continue.\n. I do apologize, but I still don't understand what's the real issue :) And I cannot replicate that. If you would be able to post a gist with teh situation, I may be able to help...\nTire is not hiding the raw response in any way. With ActiveRecord, it tries to instantiate AR records from the JSON. There are issues with it, see karmi/tire#11 and karmi/tire#12. There is definitely a solution or workaround for your case.\n. Hi, presently you can define the mapping like this:\n``` ruby\nindexes :address,\n          :type => 'object',\n          :properties => {\n            :id   => { :type => 'string', :analyzed => false },\n            :name => { :type => 'string', :boost => 10 },\n            # ... etc\n          }\n```\nDoes it work for you?\nI'm playing with a bit more DSL-ish way how to expose the API, we'll see how it goes.\n. Hi, I've added a more DSL-ish way how to define nested mapping like this in karmi/tire@1f3e644\n```\n    class Article\n      include Tire::Model::Search\n      include Tire::Model::Callbacks\n  mapping do\n    indexes :title, :type => 'string'\n    indexes :author do\n      indexes :first_name, :type => 'string'\n      indexes :last_name,  :type => 'string', :boost => 100\n    end\n  end\nend\n\n```\n. You're welcome, glad if it's working!\n. > Can we do two level deep mapping/index?\nI don't know :) Could you try it?\n\nShould I create a ticket for this?\n\nDefinitiely, if the implementation does not support deeper hierarchies of mapping.\n. Hi, no, you haven't. The code in https://github.com/karmi/tire/blob/master/lib/tire/model/indexing.rb#L30 is silly, since it does not take into account deeper hierarchies. You have to create the index with a mapping like this manually (by passing a Hash to Index#create) at the moment. I'll have a look into making the code proper and more flexible...\n. As discussed on IRC #elasticsearch, the solution is to have this index creation logic in a class method of your model, and call this from Rake task when bootstrapping the app, or from test setup in integration tests...\nI'd definitely keep the settings support (alongside mapping) in mind.\n. I guess it was just easier to implement the interface. I also guess 10 is the ES's default. Returning all facets could be a very, very bad idea in certain situations.\nSo. Issue or not? :)\n. OK, it should be easy now to add more HTTP clients, there's a Curb one in the repo.\nBased on some very basic benchmarks [https://gist.github.com/1204159], Curb seems to be more then two times faster then RestClient, even though the results do fluctuate a lot.\nI'm wondering about the em-http implementation? How would the async approach here?\n. Hi,\nyes, support for index aliases is something I'd very much like to add soon.\nNevertheless, I haven't put any thoughts into how exactly the support should look like on the DSL and feature level.\nI don't think Tire will ever support aliases in the Escargot sense.\nThe immediate reason is that I never really understood why Escargot bakes something like this so deeply into the library. Using and managing aliases is something the application is much better suited then a library such as Tire.\nWith no offence implied, the whole aliases support in Escargot seems particular to a certain use case, at least to my eyes.\nTire supports importing data into an arbitrary index, via Ruby API or a Rake task (see Readme).\nTire supports arbitrary index name for your model, via the index_name method.\nWhile there should be an interface for managing aliases, I think Tire should stop there.\n. This task:\n$ rake environment tire:import CLASS='Article' INDEX='articles-2011-05'\nimports all your Article records into an index named articles-2001-05. It's your application responsibility, then, to manage these aliases.\nIt does not create a new index whenever it's run -- this maybe the case with Escargot, but not with Tire.\nI'm wondering what are you referring to when saying:\n\nThe suggestion was something I very much liked in escargot and found missing in tire. \n\nMaybe I'm simply not getting the support for aliases in Escargot. Based on the information in the Readme, I would consider its approach too complicated and, to be honest, not too practical. I'd be delighted to hear some argument to the contrary.\n. @rubish: Hi, thanks for the update!\nYes, this a very common scenario. I would argue, however, that the approach taken by Escargot, where all of this is deeply baked into the application/models, is \u201etoo much\u201d hand-holding. It's of course very convenient from the end users perspective.\nTire's approach to the scenario would be something like this:\n1/ You have a model called Article, with no index_name set, so it defaults to articles\n2/ You either not set any mapping, or set one just approximately.\n3/ You notice that your mapping should be updated, while still prototyping the app.\n4/ You update the mapping within your model, delete the ES articles index manually, and reindex everything (see the section on import in Readme)\n5/ You start the app with new mapping in effect.\nNo magic, no aliases, no backround jobs.\nIt is, for sure, much more expensive to do stuff like this in production, where you don't want the downtime. There still is a way:\n1/ You notice that the mapping for index with millions of documents needs updating.\n2/ You manually create the index with correct mapping with a name like articles-2011-07. This is perfectly valid with a Tire::Index.create :mappings =>\u00a0... call.\n3/ You import all of your data into your newly created index.\n4/ When the import is done, you either delete the old index, and create an alias articles pointing to articles-2011-07, or just change the index_name for your model.\n5/ You restart the application with the new mapping in effect.\nAgain, no black magic, very straightforward, and under your control.\nNow, can I imagine there's a better way around this? Sure! I would very much like a Tire::Index#reindex method, which would use the ElasticSearch's scan type of search, to reimport everything from the old index into new one. But in practice, both of those two approaches outlined above, are \u201egood enough\u201d...\n. > (...) probably because of my preconceived notions that the gem should do everything.\nYou're not alone :) Lots of people come to Tire, probably thinking it should shield them from nearly everything. But it won't, and it can't, since the world is a very complex place. Conditions vary widely. (See for instance tire#34.)\nI think the proper way to think about all this reindexing, mapping updating stuff is this:\nYOU WILL HAVE TO UPDATE THE MAPPING AND REINDEX EVERYTHING, EVENTUALLY*\n:) That's just the simple truth. Once you get over it, you can fine tune the process to match your needs exactly, and not bend some library to do what you need to do.\nBoth of those approaches are valid, we have did both in our app in situations like these...\n. Ah, sure, thanks for the report!\n. Hi, this is definitely a Mongoid problem. You're loading the data with CLASS='Recipe.all' trick, which loads all documents into memory. There's full info about importing and Mongoid at issue #48: https://github.com/karmi/tire/issues/48#issuecomment-1499519...\n. This: karmi/tire@8d592e7 is the culprit. I'll recheck what was the funky stuff with documents.empty? in will_paginate...\n(will_paginate needs a check for #empty?, Kaminari needs a check for #size > 0)\n. Hi,\nfirst, I'd definitely like some sort of support for associations in Tire::Persistence.\nBut I don't think the ActiveRecord semantics are good fit for a schema-free store such as ES. I don't think the association support in CouchRest or SimplyStored makes much sense, for example. In a store such as ES, splitting the data into different objects (say Article and Tag) is usually an antipattern. There are valid cases, such as Article and Comment. For such cases, a nested documents would make a great fit.\nAs for the Ruby API, that's still far away. The preliminary feature for anything like this would be to allow casting values, in the similar sense as CouchRestModel does it...\n. > I won't argue on pattern / antipattern, because using search engine instead of database is hardly a common practice (...)\nYeah, but one way to look at ElasticSearch would be to see it as a database with pretty powerful search and aggregation capabilities. Nothing wrong with using it as a storage (for instance when compared with CouchDB or MongoDB).\n\nI'm doing my own extension for Tire implementing associations atm. There is not much to show yet, but if I'll get it to production I'll post a patch or pull request here.\n\nGreat. (There's a tire-contrib repo prepared for extensions like that. Feel free to submit a patch against tire, I'll do the extraction myself.)\n\nTo be honest I still don't know will it get to production or not, because ES is missing couple of features I need and I'm starting to look at Solr with its dynamic attributes.\n\nAre you referring to http://wiki.apache.org/solr/SchemaXml#Dynamic_fields? ElasticSearch does not need anything like that, because with dynamic mapping, it infers the field type from the JSON document (integer vs string vs date, etc). Could you describe your case in more detail if this is not satisfactory?\n. If you'll move from ES to Solr, you'll have a whole different set of problems, dynamic fields being the smaller one, I'd bet :)\n. @hgujral Could you try with current master? It has lots of improvements in persistence.\n@naquad There's been some improvements to persistence since you filed the issue, eg. 91e39fb. Closing this, please open a new one if your problems are not solved...\n. Hi,\nfirst: your mapping definition is correct. You need to make absolutely sure, that this is the mapping in ES. Either call Show.elasticsearch_index.mapping or just load http://localhost:9200/shows/_mapping in curl/browser. By the results the facet give you back, the channel_name is, in fact analyzed.\nTake a look at &\u00a0run this code, it gives proper facet back:\n``` ruby\n    require 'rubygems'\n    require 'active_record'\n    require 'tire'\nActiveRecord::Base.establish_connection( :adapter => 'sqlite3', :database => \":memory:\" )\n\nActiveRecord::Migration.verbose = false\nActiveRecord::Schema.define(:version => 1) do\n  create_table :shows do |t|\n    t.string   :title, :description, :channel_name\n    t.datetime :created_at, :default => 'NOW()'\n  end\nend\n\n# 1) Make sure the index does not exist, so it is created with proper mapping\n#    (See section about FORCE importing in the Readme)\nTire.index('shows').delete\n\nclass Show < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\n\n\n  mapping do\n    indexes :channel_name, :type => 'string', :index => :not_analyzed\n  end\n\n  # 2) Provide your own, custom `to_indexed_json` method\n  def to_indexed_json\n    {\n      :title        => title,\n      :description  => description,\n      :channel_name => channel_name,\n      :created_at   => created_at\n    }.to_json\n  end\nend\n\n# 3) Create a document and refresh the index\nShow.create :title => 'Series about ...', :channel_name => 'This is India'\nShow.elasticsearch_index.refresh\n\ns = Show.search do\n  query { string 'series' }\n  facet('channel_name') { terms :channel_name}\n  sort { by :created_at, 'desc' }\nend\n\np s.facets\n# =>\n# {\n#     \"channel_name\"=>\n#     {\n#         \"total\"=>1,\n#          \"_type\"=>\"terms\",\n#          \"other\"=>0,\n#          \"terms\"=>\n#         [\n#             {\n#                 \"term\"=>\"This is India\",\n#                  \"count\"=>1\n#             }\n#         ],\n#          \"missing\"=>0\n#     }\n# }\n\n``\n. Believe me, the most probable cause for this is that the index is already created _before_ your model is loaded. Delete the index (curl -X DELETE http://localhost:9200/shows), and load the app. Or use theFORCEoption in the Rake task, as documented in the Readme.\n. Again, something must be creating that index before your app is loaded...\n. No, different models map to different indices in ES.\n. So, any luck in the end? \n. Start with a fresh app, and copy over only the models &\u00a0migrations etc. If the problem persist, try to start with a fresh app with a simple model, hooked to Tire, and submit a new issue, please...\n. @devilcoders, you have to set thenamefield to benot_analyzed`. Search \"multi field\" on the ES site...\n. @xdmx: I'm not sure I understand the problem 100% :), but setting the proper index in Tire is easy, eg.:\n``` ruby\nclass MyModel < CouchRest::Model::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nmapping do\n    indexes :author,\n              :type => 'multi_field',\n              :fields => {\n                :author => { :type => 'string', :analyzer => 'czech', :boost => 10.0 },\n                :exact  => { :type => 'string', :analyzer => 'stop',   :boost => 10.0 }\n              }\n    end\n  end\nend\n```\nThe facet definition is again easy:\nruby\nfacet('authors')   { terms \"author.exact\", :size => 5 }\nIs this what you're after?\n. @xdmx: Good, it was suspicious all the time. Thanks for the suggestion, but Tire does not want to interfere with user's possibly superior knowledge. Garbage in, garbage out, good stuff in, good stuff out...\n. @christianhellsten, I'm sorry but I got lost in what problem are you experiencing exactly.\nFirst, yes, the index is created on class load, only when it does not exist, the Readme says so.\nSecond, the Rake task you're posting seems suspicious. Notice that you delete the index, after the class has been loaded, effectively nuking the mapping, and then you import data into it. Not a good way. And yes, ES will create dynamic mapping in this case.\nTire provides kinda nice import task already, if there's any issue with it, we should solve it. You can wrap this task in another task, or rip the code, etc. Feel free to ping me on the #elasticsearch IRC channel for more interactive help.\n. That's a very good suggestion, actually! I refrained from updating the mapping in the model integration, but this may be a very nice first step. \n. Yes, that's something which will be merged into upcoming 0.2 release. The activerecord integration tests have all the details. See issues tire#11 and tire#12 for background.\n. Care to paste a backtrace, please?\n. I feel your pain. Forcing the type to be set by the user, however, isn't very \"elastic\", is it?\nDo you have any suggestion how to document the behavior better?\n. I understand all your points. I don't agree, however. ES is perfectly fine when you throw random hashes at it. If and when the need for more arises, you have all the means to do so. This, to me, is aligned with the general Ruby approach as well, in my humble opinion...\nI'm glad you can get something working quickly. Nevertheless, without reading the docs, you can't and won't do anything moderately interesting with ES. Again, if I can make the wording for the docs or the examples better, I'd love to hear it!\n. Hi,\nthis is very unexpected behaviour. I've double-checked in an app, and also tightened up the integration tests for this. Could you provide more info about what ORM is this concerning, and possibly some snippets to recreate, please?\n. Hi, I've tried out your scenario in a fresh app (Ruby 1.8.7, Rails 3.0.8):\nrails g scaffold tire_test name:string\nAdded the Tire includes. Now, both TireTest#destroy and TireTest#delete do delete the record from the db, in a console session replicated after yours.\nObviously, the TireTest#delete does not delete the record from ElasticSearch, because it does not run any hooks.\nI'm wondering what might be causing this issue for you. Could you try in a fresh app the way I did?\n. Thanks, cloned, wasted half an hour to install mysql :), and everything works. Both delete and destroy correctly issue DELETE FROM ... statements in the console... This is the sequence of my commands:\n```\n~/Playground/ElasticSearch/Tire/TireTest\nkarmi\u25b9 rake db:create\nkarmi\u25b9 rake db:migrate\n==  CreateTestModels: migrating ===============================================\n-- create_table(:test_models)\n   -> 0.0241s\n==  CreateTestModels: migrated (0.0247s) ======================================\nkarmi\u25b9 rails console\nLoading development environment (Rails 3.0.9)\n\n\nActiveRecord::Base.logger = Logger.new(STDOUT)\nActiveRecord::Base.clear_active_connections!\nt = TestModel.new :name => 'TEST'\n=> #\nt.save\n  SQL (0.2ms)  BEGIN\n  SQL (0.6ms)  SHOW TABLES\n  SQL (1.9ms)  describe test_models\n  AREL (0.4ms)  INSERT INTO test_models (created_at, name, updated_at) VALUES ('2011-08-16 16:54:30', 'TEST', '2011-08-16 16:54:30')\n  SQL (0.6ms)  COMMIT\n=> true\nt.delete\n  AREL (0.9ms)  DELETE FROM test_models WHERE test_models.id = 1\n=> #\nTestModel.elasticsearch_index.delete\n=> true\nt = TestModel.new :name => 'TEST'\n=> #\nt.save\n  SQL (0.1ms)  BEGIN\n  AREL (0.5ms)  INSERT INTO test_models (created_at, name, updated_at) VALUES ('2011-08-16 16:55:45', 'TEST', '2011-08-16 16:55:45')\n  SQL (0.5ms)  COMMIT\n=> true\nt.destroy\n  SQL (0.1ms)  BEGIN\n  AREL (1.0ms)  DELETE FROM test_models WHERE test_models.id = 2\n  SQL (0.5ms)  COMMIT\n=> #\nquit\n```\n. Yeah, crap, the reason we've seen different behaviours was I was on 1.8.x and you, probably, on 1.9.x. Compare:\n\n\n```\n$ irb\n\n\nputs RUBY_VERSION\n1.8.7\n=> nil\nObject.instance_methods\n=> [\"inspect\", \"tap\" ...\n\n\n$ irb\n\n\nputs RUBY_VERSION\n1.9.2\n=> nil\nObject.instance_methods\n=> [:nil?, :=== ...\n```\n\n\ninstance_method on Ruby 1.9 returns symbols -- I've been hit with it before. Thanks for the report and pull request, it should be fixed now!\n. Hi, I've cleaned up the message, and rewrote the code so the flow of the examples is preserved. Looks good?\n. Could be a bug -- what kind of object does Article.page(options[:page]).per(options[:per_page]).all return? (Tire should be content with anything iterable with map, see http://rubydoc.info/github/karmi/tire/master/Tire/Index:bulk_store)\n. Ah, indeed, that is probably a bug. What happens when you change that to and ! documents.empty?? Or is there any way to check if the next iteration returns empty result (as is the case with will_paginate)?\n. OK, at least you have found some workaround. I'll try to have a look into that, eventually. It's a shame ActiveModel does not offer some chunked find such as ActiveRecord#find_in_batches...\n. Hi, this came back to bite Mongoid users. Should be fixed in karmi/tire@5b79c7 and karmi/tire@f491d4. In the end, the to_a conversion was neccessary. Could you verify that master works with new Mongoid and you app?\n. Hi @wulffeld, any news on this, please? Could you verify master so we can close this?\n. Great news!, many thanks for trying it out!\n. Thanks!! Closed in karmi/tire@418e40a.\n. Hi, thanks! The way I see it, I should rewrite the whole Search::Facet#terms method, so it's more natural and easier to use all the different invocations as per http://www.elasticsearch.org/guide/reference/api/search/facets/terms-facet.html...\n. Should be closed in 039cc09!\n. Yes, and that's precisely why there is pretty=true in the debug output of to_curl :)\n. No worries, mate :)\n. Watch for deprecation notices on previous version, 0.1.16.\nWhat \"does not work\" mean in this case? What is the expected and real outcome?\n. Is it possible to send a gist so I can try to recreate that? Very hard to give an answer otherwise.\n. Ping, any news?\n. What if you run the command displayed by puts @s.to_curl in your gist directly? Are you still getting no results? Is the curl command different on 0.16 and 0.20?\n. I don't believe the problem is something changed between 0.1.16 and 0.2.0... The important thing is if Tire is sending the same request to the ES, and it is, apparently. Tire, when used like in a snippet you posted, does not care about your Ruby classes. Again, watch for deprecation notices on 0.1.16.\nWhile I can't and don't rule out the problem lies with Tire, there's just not enough information to analyze what's going on...\n. Any news on this?\n. But you have not mentioned you've seen this error? You were seeing the generic uninitialized constant XYZ error?\nThere's just not enough information to replicate or at least understand what's going on....\n. Hi, I'll definitely look into that. Thanks @raff for the report. Reopening the issue.\n. Gentlemen, my sincere apologies.\nThe bug was real, very real, and also very obvious, once I had it in front of me -- was bit by this when consulting for a client.\nAs you can see from the attached commit, there were two things going on:\n1. Within Rails, Tire was trying to instantiate a (possibly incompatible) Ruby class matching _type, not the Results::Item wrapper.\n2. Also within Rails, calling something like Item#class or Item#inspect would break the code, if it sailed throug the first problem.\nThis should be fixed right now. Sorry.\n. @benzheren, @raff, could you verify the problem went away with v0.3.6?\n. Hmm, dunno what \"modify the search results\" mean, but yeah, the results returned are completely different.\nOn the 0.1 version, you received instances of your models (when you was lucky and your model was compatible :), on the 0.2 and up you receive instances of Item, and you can load the underlying model.\n(I cannot imagine what purpose calling to_hash on a result would serve though...)\n. Seems like a wrong approach to your problem. Why not just go down one level:\nTire.search ['index_1', index_2'] do\n  query { string 'foo' }\nend\nAnd possibly create a class to hold the composed search?\n. I see. I'd go with MyClass.search :index => 'other' do ... end then, and allow MyClass.search :index => ['other1', 'other2'] then. That makes it more consistent with the DSL approach.\nNow, what about the :type, then? It should allow to pass MyClass.search :index => 'other', :type =>\u00a0['typeA', 'typeB'] do ... end as well -- otherwise the search won't work as expected.\nIt still feels a bit weird, and puts another stress on the RDoc documentation (still in the works), but...\nNotice, that in the 0.2 release, the casting was changed, all results are Item by default, so making a separate component (class, module) wrapping the generic Tire.search would probably work very similar. I understand this is much more convenient for the usage you describe. I'll mull it over a bit :)\n. Two things: first, I smell something weird in using two indices for one model :) That's for sure. On the other hand, the options hash is quite freeform, so yes, it makes sense to expose a flexibility like this. It just needs to be more complete, with the Array stuff, with the :type stuff, etc., in my opinion.\n. We are. I still feel you'd be better with some creative use of index aliases, or having an index per user... passing some random index name on search time feels somehow fragile. But I may be wrong and it's your use case after all.\n. Hey Ben, took another approach, the default_options lent itself conveniently for supporting this. Also, a full integration test feels redundant for this case...\nCould you test it in your app, if it's doing what is should for you?\n. Thanks!, merged in 9f97d3f\n. Hmm, that is weird -- could you check if STI uses a _type method of the model? Previously, both type and _type methods were defined for the model instance in question.\n. Any news on this?\n. Hmm, _type should not be added when the model has that method already defined. Could you create a Rails template which generates some STI scaffolds etc, so I have less work with recreating that?\n. Any news? Neither #type nor #_type really should be added to your model on 0.3.\n. OK, thanks for being stubborn on this. I have found the offending code and moved the the unneccessary methods into Model::Persistence. The sample app works now.\n. Thanks, Paco, merged!\n. Hi Paco, finally merged, thanks! I like the interface you're proposing. I think the duplication of histogram is something which we should sort out in the future... \n. Hey Jason,\nno, you have to define the serialization in to_indexed_json; I've taken the hint and updated the documentation so it gives more examples.\nTire has different approach then the Sphinx and Solr gems, and divides the mapping and serialization declarations. With mapping, you define how your model JSON serialization should be understood by ES, with to_indexed_json, you define what is this serialization.\nIs that OK for your use case?\n. Yeah. It's because for whatever reason, Mongoid is reporting false for documents.empty? in https://github.com/karmi/tire/blob/master/lib/tire/index.rb#L108.\n. This: karmi/tire@8d592e7 is the culprit why it broke: will_paginate needs a check for #empty?, Kaminari needs a check for #size > 0.\nShould be fixed now, #length seems to work for both. Could you verify it?\n. Yeah, that's just in master, still. 0.2.0 release has no such thing :) Will release 0.2.1 soon.\n. Hmm... This is really unrelated to Tire... encoding: utf8 is present in the default, generated database.yml by Rails. You may have had an older database.yml lying around.\n. mapping serves only for setting up your mapping for ElasticSearch (things like analyzers), to_indexed_json serves for serialization of your data for ElasticSearch. (As opposed to other search libraries, eg. for Sphinx or Solr, which combine these).\n. Yeah. How should the interface look like? Options for the search method or part of the DSL?\n(BTW, I think it would be much nicer to have the search_type exposed.)\n. Yeah, it's missing, along with search_type and friends. It's obvious it would be great to allow passing those options to search.\n. I'm not sure how much delete_by_query is generally useful, but yeah, it would be great to have it. What about the interface? Pass a query block to Tire::Index.remove?\n. I'm pretty sure there are use cases -- I'd only be wary of performance on large datasets and predictability for some queries. Destructive actions should be predictable.\nYes, AFAIK delete_by_query has the same interface, it's just a DELETE, not GET request. Didn't investigated that, though...\n. For almost any use case, a \"sliding window\" approach would be better, I think.\nThat said, I can see how the delete_by_query could be nicely supported by an API outlined above.\nAnd, regarding the discussion place: I, myself, don't mind discussing everything related to Tire here, in Github issues. Maybe folks who follow the project do mind the constant chatter in their Github feed, I don't know. I just don't wanna to take care and maintain another communication channel, given how much Google Groups sucks, and how Librelist is half-baked... Anybody has any opinion on this?\n. Hey, been meditating on this with @vhyza. The problem is neither RestClient nor Curb support sending bodies in DELETE requests. Don't know what RFC says here.\nFor simple queries, you can do it like this:\n``` ruby\n$LOAD_PATH.unshift File.expand_path('../../lib', FILE)\nrequire 'tire'\nextend Tire::DSL\nindex 'articles' do\n  delete\n  create\nstore type: 'article', title: 'One',   tags: ['ruby']\n  store type: 'article', title: 'Two',   tags: ['ruby', 'python']\n  store type: 'article', title: 'Three', tags: ['java']\n  store type: 'article', title: 'Four',  tags: ['php']\nrefresh\nend\nputs \"\", \"Matching results:\"\nsearch = Tire.search('articles') { query { string 'tags:ruby' } }\np search.results.map(&:title)\nputs \"\", \"Deleting results...\"\np Tire::Configuration.client.delete \"#{Tire::Configuration.url}/articles/article/_query?q=tags:ruby\"\nputs \"\", \"The index now contains:\"\nindex('articles').refresh\np Tire.search('articles') { query { all } }.results.map(&:title)\n```\nGood luck! :P\n. Hmm, OK, but how does this work with the rest of the code? MyModel#index would still return an Index instance based on class level settings?\nI understand the reasoning behind that, having to do something very similar previously with CouchDB. I just think this has to be considered across the whole lifecycle of document. Maybe an integration test would find and clarify all the issues?\n. Hi, I like the direction you're going. On the other hand, I think we're mixing several related, but separate ElasticSearch and Tire features.\nFor the first use case, ES's index aliases seem to be the right tool, to me. When you'd periodically updated aliases, outside the app, the behaviour would be consistent across the whole stack. No matter how I put something into and index \"named\" posts (an alias), it will always end up in the index posts-YYYY-MM.\nIndex templates\u00a0would, of course, play a crucial role here, to properly set up all these indices.\nNow, I'd of course like support for both in Tire. The proposed API for aliases, however, seems to me a bit rough. I'd much rather have something like:\n``` ruby\nTire.index('posts').aliases\n=> [ 'posts-2011-01', 'posts-2011-02' ]\nTire.index('posts').aliases do\n  remove 'posts-2011-01'\n  add    'posts-2011-03'\nend\n```\nThat's still off the top of my head. The API for templates would be very similar.\nNow, to the \"dynamic index name\" stuff. Thank you for the suggestion and the code; it's precisely what I've been dealing with a lot, in ES, in CouchDB and Couch-related gems.\nI think the first case, index_name lambda { \"posts-#{Time.now.year}\" } is very easy to support, I'll add that as soon as possible.\nThe second use case is more tricky. Unless I miss something, you're evaluating that block in the context of the class. You're doing Product.search ..., and there's just no way how to pass an instance with some category set. Of course, it would be possible to do @category.search, where it certainly would be possible to search in custom index, like you suggest.\nBut, that could probably be implemented in a much ore lightweight way by doing something like:\n``` ruby\nclass Category\n  # ...\ndef search('query')\n    self.class.search 'query', :index => self.to_param\n  end\nend\n```\nThe whole thing could of course be put into a module, which could then be included in the class, etc etc. It would certainly look nice to my eyes, and would be comprehensible, while supporting something like this on the gem level would probably need lots of acrobatics...\nWhat does everybody else think? @floere, @chrisberkhout, @pacoguzman?\n. Hi, thanks for all the feedback.\nFirst thing, I think that what @olivere is after is a very realistic use case, based on my experience. Frequently, you want something like storing documents in an index per user (think bookmarking service), or some kind of lightweight sliding window done inside the app, etc. Or your use case with catalogs. So yes, supporting workflows like these is crucial for any library worth its salt.\nRegarding the dynamic index name, yeah, that's a given. As for:\n\nWhy the lambda for index_name?\n\nWell, it depends on how \u201cdynamic\u201d you want the index name to be. A block is evaluated once, when the class loads. A lambda would be evaluated repeatedly, on every index_name call. You could of course memoize the result of the calculation, as @floere suggests. Of course, a decent implementation could easily support both of these :)\nRegarding the aliases syntax, yeah, that's one of the things we were discussing with Florian, a lot. I think I'd go for the more DSL-ish syntax, so it's symmetric to the rest of interface. Of course, you can always use the gem on the low-level, ie. do something like Tire::Index.new('something').aliases << 'something-else', etc.\nRegarding the delete_by_query, yeah, dropping (or closing!) an index would be almost instantenous. If you can do it like that, go for it! (By the way I'd be more concerned with unpredictability then performance.)\n. Thanks for the discussion, guys. Maybe too long for Github Issues, but great :)\nRe: aliases. I think the name alias is apt and fitting here, really. This is about the excellent design ES's API has. I don't think @kimchy \"gave up too early\" here, at all.\nYou can do Search.new(index1, index2, index3, ...) with ES, of course. See the http://www.elasticsearch.org/blog/2010/02/12/yourdatayoursearch.html blog post for a great \"Amazon-like\" example.\nThere's indeed a great deal of architectural and performance benefits you gain from separating your data into separate indexes. One use-case is the \"sliding window\" one, which I always bring up -- please meditate on that a bit more :) Another one is the \"let's put products into separate indices based on category\". Yet another one is the \"every user has her own index\" (with an obvious limit to scaling).\nNow, aliases do play with almost all of these use cases. The discussion was not entirely clear so far and I am to blame, mostly.\nWhen we consider two indices, \"index-A\" and \"index-B\", these are two index entities.\nWe can create another entity, an alias, for these two inidices. It is a single, separate entity, called alias. It can be an \u201ealias\u201d, or pointer, or reference to one or more \u201creal\u201d indices. It aliases them. It's just a nickname, a nom de guerre, an A.K.A. for one or more \u201cindices\u201d. You can use their \u201creal names\u201d, or you can use this alias. It depends on what is more comfortable for you, in a given context.\nWhat we gain from that is architectural leverage: eg. when more applications in one \"project\" or \"stack\" or whatever perform searches, they can do that against the alias entity, not the index entities. We can close or delete individual indices without affecting the stack. All of this can be managed completely outside of the application (which could be a very good thing, even though many Rails developers don't think about it like that).\nSo, yes, alias is a good name. It's a different concept then when you do ln -s, of course, where you have 1:1 correspondence. But when you say \u201cG7 decided that ...\u201d, you're using an alias for a ever changing list of countries. A nickname, a nom de guerre, an A.K.A. An alias.\nIt is just not clear from the code examples I dumped here originally (Tire.index('posts').aliases { add \"whatever\" }).\nIn this example, the relation is reverted -- we are in fact working with a \"posts\" alias, not a \"posts\" index. However, with certain exceptions, an alias behaves similarly as index. So my original intention here could have been either a big confusion or an inspired thinking on how to make the API fluent :) We'll see if/when somebody gets to implementing it for real.\n\nBtw, I am intrigued by Tire::Index.new('something').aliases << 'something-else'.\nYou say it's low level Ruby, while to me it looks like a fluid interface,\nwhere the << method returns the index itself. Which is it?\n\nI know. I think the thing is <<. This just smells \u201ctoo much Ruby, too little DSL\u201d, to me. Of course, you can write:\n```\nclass Array; alias :add :push; end\naliases.delete 'posts-2011-01'\naliases.add    'posts-2011-03'\n```\nBut in general, I prefer the blockish DSL ways -- so far it's pleasant to use inside applications or one-off scripts, etc. It's very readable even for people with little Ruby knowledge... Unless you descend into fifth level of the blocks, such \u201cdeclarative\u201d notation feels great.\nOf course, for lots of use-case, a more \u201cimperative\u201d notation would be more appropriate. Tire does not hold that against you, at all:\n``` ruby\n    require 'rubygems'\n    require 'tire'\ninclude Tire\n\nindex = Index.new('articles')\nindex.delete\nindex.create\nindex.store :title => 'Test Article', :tags => ['search', 'ruby']\nindex.refresh\n\nsearch = Search::Search.new('articles')\nsearch.query { string('title:T*') }\nsearch.filter :terms, :tags => ['ruby']\nsearch.size 1\n# ...\n\np search.perform.results\n\n```\nI'm not sure why anybody would choose to \u201ctalk like that\u201d, generally, but I am 100%sure  there are occasions you'd like to talk exactly like that :)\n. Hi all,\n\nMaybe this discussion needs to be split into several different issues.\n\nYeah, I think we have filled like three separate discussion threads already. Isn't this some kind of elaborate plot to lure me into creating a Google Group, guys? :)\n\nTire.alias('posts').targets do ...\n\nI am aware of that. I think however, that starting with the Tire.index could eventually yield something. As said: I am either totally confused or inspired. In ancient times, they have tend to look at these as being similar :)\n\nA mention of imperative style in the README could be good.\n\nYes! Added that, you're right that it could be very useful.\n\nI may be opening a can of worms here (...) something like Arel might work even better.\n\nWho knows :) I am kinda hesitant, here.\nFirst, there's a great project by @clintongormley, SearchBuilder, doing just that. I, obviously, see the benefits of an approach like this, and it would probably be \u201ceasy\u201d to support Arel-like syntax for searching.\nThe reason I'm kinda hesitant is that I have always tried more to expose the ES API in a very Ruby, and hopefully elegant, pleasant manner, then to invent radically new interface. I have certainly tried to make ElasticSearch more \u201capproachable\u201d for a regular Ruby/Rails developer, but a syntax like this would probably go a bit too far. On the other hand, I am quite sure there are benefits for something like that.\n\nBlocks vs lambdas\n\nAgain, the confusion is entirely my fault. You can, of course, just store the block defining the index name, and call it later, in the index_name method.\n\nBy deciding to use n indexes instead of just one, you make a conscious decision that you know what you are doing (...)\n\nYes!, precisely. I wouldn't put stuff into \u201cone index per user\u201d if I expect there will be thousands or more users. For the \u201csliding window\u201d scenario, it would work rather well, because then I can optimally deal with potentially expensive operation such as \u201cdelete all records older then 3 months\u201d. It's really very similar to eg. PostgreSQL partitioning.\n\nindex.aliases = ['index_alias1', 'index_alias2', 'index_alias3'] (...) This now represents the other direction. This is actually the index pointing to a list of index names.\n\nYes, that is where I have started the confusion, by inverting the thing around. I'd just say once again, that something in my mind tells me it could work. Ie. the create method would then create an alias, not the index, etc. Something in my mind tells me as well, obviously, that it could end up being an unbelievable mess :)\nOnce I recover from all the wild ideas, I'd have a shot at the dynamic index names the OP was after :D\n. So, I finally got to implementing the \u201ceasy\u201d variant of dynamic indices, the class level one:\nruby\nArticle.index_name { \"articles-#{Time.now.year}\" }\nThe instance level one, summed very well in the example code from @olivere, remains \u201ctricky\u201d.\nI have, however, find a promising approach to this. The right interface is really obvious:\nruby\n@blog.posts.search 'something'\nThis way, at least in ActiveRecord, we could have use the scope information. I had, however, not found a way implement that for real.\nThe funny thing is, such syntax is absolutely valid, and it works -- since ActiveRecord passes the call to search via method missing to Post. The scoping is lost, nevertheless.\nIf we may get around that, the interface for AR models would be very fluent. I worry, still, that it would not be portable to ActiveModel implementations for other databases, though.\nOne last thought: maybe we are simply stretching the gem too wide here. Why not just overload the MyModel.index_name and MyModel#index_name methods, and be done with it. Yes, ugly, but kinda \u201cin plain sight\u201d. No magic and convoluted infrastructure.\nAll that said, I am the first one, ironically, who would use that in production, immediately. We are \u201cscoping\u201d searches like that to user-bound indices in one of my apps from the humble beginnings with CouchDB and CouchDB-Lucene...\n. @olivere: You can do instance-based indexing/persistence just fine, the only thing you have to do is overload the Article#index_name (instance!) method. Try it:\nruby\nclass Article\n  def index_name\n    self.author.fullname.parameterize\n  end\nend\nYou cannot simply do instance-based searching, since Article.search is class method and all that. @blog.posts.search 'something' is very, very nice, I'm just not sure how to properly support/implement that, for ActiveRecord and ActiveModel.\nAs for the aliases, I can definitely rule out focusing on that -- I'd much rather focus on adding the :timeout option (see separate issue), the search_type=count option for speeding up facets, figuring out how to support multiple filters, etc etc...\n. @olivere: I'm sorry, it's more tricky than that and than I'd like. This does not work as expected:\n``` ruby\nrequire 'rubygems'\nrequire 'tire'\nrequire 'active_record'\nrequire 'sqlite3'\nTire.configure { logger STDERR }\nActiveRecord::Base.establish_connection( :adapter => 'sqlite3', :database => \":memory:\" )\nActiveRecord::Migration.verbose = false\nActiveRecord::Schema.define(:version => 1) do\n  create_table :articles do |t|\n    t.string   :title\n    t.string   :author\n    t.datetime :created_at, :default => 'NOW()'\n  end\nend\nclass Article < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\ndef index_name\n    author.parameterize\n  end\nend\none = Article.create! :title => 'Test custom index', :author => 'John Smith'\ntwo = Article.create! :title => 'Test custom index', :author => 'Mary Smith'\n```\nTire needs a patch in callbacks.rb. I'll look into that laters.\n. @olivere: I'm not sure at the moment how to fix the logic. The easiest would be to just call update_index when it's defined in the model instance, but that would also cross the tire \u201cproxy\u201d boundaries... I'll have to mull it over a bit. The infrastructure itself definitely supports that. It's just about deciding where to \u201cmake the cut\u201d.\n. No. It's highly probable I'll bump into this same issues again, with a different application. So I'l have a real problem and real pain to extract the solution from it.\n. Haha :) If you need to pingpong ideas for workaround, catch me on IRC. But I'm sure we will have some support for these dynamic things soon.\n. Guys, not to wake sleeping dragons, but there's some relevant code in https://github.com/karmi/tire/commit/aliases as of yesterday :)\n. > (I hope it's okay for @karmi to use this issue as a kind of support thread... :-))\nAbsolutely!! :) (I don't have time at the moment, but will return to this discussion later today or tommorrow.)\n\nblows the dust off...\n\n:D Nice :)\n. Hi, thanks to @olivere for the input and great advice, couldn't say it better. I'd like to add a couple of points:\n- As Oliver said, elasticsearch does not care if you pass an index or alias to many operations, such as searching -- it automatically expands it. Note that on 0.19.8 there's even a wildcard notation where you can say products-*.\n- You actually can index against an alias, but it has to point to a single index -- otherwise elasticsearch wouldn't know into which of the aliased index to put the data, obviously.\n- The whole story with \u201cdynamic\u201d or \u201ccalculated\u201d index name is still too confusing to me. I have tried to re-read the thread and apart from great memories didn't get much from it :)\nNotice that Tire has support for the \u201cdynamic index name\u201d as in \u201clambda that is evaluated in the context of the class\u201d, see the test model or the relevant code.\nIf I remember correctly, sketches and spikes involving index names bound to the instance context were leading nowhere interesting. More importantly, I think this is precisely the spot where it could be wise to deviate off the \u201cjust let the library handle that\u201d course and implement your own solution on top of the library.\nIn the applications I have been working on, there has been a similar need for separating documents into indices \u201cper user\u201d or \u201cper account\u201d. I can summarize here what we did, in two separate scenarios.\nLet's say we have an Account class and we deal in articles entities.\nIn that case, our Account class would have following:\n``` ruby\nclass Account\n  #...\n# Set index name based on account ID\n  #\n  def articles\n      Article.index_name \"account-#{self.id}\"\n      Article\n  end\nend\n```\nSo, whenever we need to access articles for a particular account, either for searching or for indexing, we can simply do:\n``` ruby\n@account = Account.find( remember_token_or_something_like_that )\nInstead of Article.search(...):\n@account.articles.search { query { string 'something interesting' } }\nInstead of Article.create(...):\n@account.articles.create id: 'abc123', title: 'Another interesting article!', ...\n```\nBOOM! Done :)\nHaving a separate index per user/account works perfect in certain cases -- but definitely not well in cases where you'd have tens or hundreds of thousands of indices (or more). Having index aliases, with properly set up filters and routing, would perform much better in this case. We would slice the data not based on the tenant identity, but based on time.\nLet's have a look at a second scenario, starting with a heavily simplified curl http://localhost:9200/_aliases?pretty output:\njson\n{\n  \"articles_2012-07-02\" : {\n    \"aliases\" : {\n      \"articles_plan_pro\" : {\n      }\n    }\n  },\n  \"articles_2012-07-09\" : {\n    \"aliases\" : {\n      \"articles_current\" : {\n      },\n      \"articles_shared\" : {\n      },\n      \"articles_plan_basic\" : {\n      },\n      \"articles_plan_pro\" : {\n      }\n    }\n  },\n  \"articles_2012-07-16\" : {\n    \"aliases\" : {\n    }\n  }\n}\nYou can see that we have three indices, one per week. You can see there are two similar aliases: articles_plan_pro and articles_plan_basic -- obviously, accounts with the \u201cpro\u201d  subscription can search two weeks back, but accounts with the \u201cbasic\u201d subscription can search only this week.\nNotice also, that the the articles_current alias points to, ehm, current week (I'm writing this on Thu 2012-07-12). The index for next week is just there, laying and waiting -- when the time comes, a background job (cron, Resque worker, custom script, ...) will update the aliases. There's a nifty example with aliases in \u201csliding window\u201d scenario in the Tire integration test suite.\nLet's not look on the articles_shared alias right now, let's look at what tricks we can play with this setup:\n``` ruby\nclass Account\n  # ...\n# Set index name based on account subscription\n  #\n  def articles\n    if plan_code = self.subscription && self.subscription.plan_code\n      Article.index_name \"articles_plan_#{plan_code}\"\n    else\n      Article.index_name \"articles_shared\"\n    end\n    return Article\n  end\nend\n```\nAgain, we're setting up an index_name for the Article class, which holds our documents. When the current account has a valid subscription, we get the plan_code out of the subscription, and direct searches for this account into relevant index: \u201cbasic\u201d or \u201cpro\u201d.\nIf the account has no subscription -- he's probably a \u201cvisitor\u201d type -- , we direct the searches to the articles_shared alias. Using the interface is as simple as previously, eg. in ArticlesController:\n``` ruby\n@account  = Account.find( remember_token_or_something_like_that )\n@articles = @account.articles.search { query { ... } }\n...\n```\nWe are not using the Article class as a gateway for indexing in this case; we have a separate indexing component, a Sinatra application serving as a light proxy to elasticsearch Bulk API, providing HTTP authentication, document validation (enforcing rules such as required properties or dates passed as UTC), and uses the bare Tire::Index#import and Tire::Index#store APIs.\nThese APIs talk to the articles_currentindex alias, which is periodically updated to the current week with said background process. In this way, we have decoupled all the logic for setting up index names in separate components of the application, so we don't need access to the Article or Account classes in the indexing proxy (it runs on a separate server), or any component of the application. Whichever component is indexing, indexes against articles_current alias; whichever component is searching, searches against whatever alias or index makes sense for the particular component.\nAs Oliver rightly points out, not using Tire for indexing is perfectly valid and very simple: just do not include the Tire::Callbacks module. (Side note: Based on the feedback I get, many people expect Tire to magically guess and support their needs; I think, and have repeatedly said, that in many cases, you just have to sit down and write your thing. In my experience, due to Ruby being so awesome and elasticsearch being so awesome, Tire can support all sorts of tricks like these.)\nFor anybody dealing with complex scenarios like these, I strongly suggest to watch @kimchy's Berlin Buzzwords 2012 talk \u201cElasticSearch: Big Data, Search, and Analytics\u201d (slides) -- it walks through all sorts of different scenarios and topologies for big(gish) datasets.\n. @olivere That looks as a great solution, actually! I think that's something which could be supported in Tire quite well. What's the motivation for the multi-threaded support, though? :)\n. Tire, by itself has no problem with importing @ signs:\n``` ruby\n    require 'rubygems'\n    require 'tire'\ndocuments = [ {:id => 1, :body => 'user@foo.bar'}, {:id => 2, :body => 'foo bar'} ]\n\nTire.index 'at_signs_index' do\n  delete\n  create\n  import documents\n\n  refresh\nend\n\ns = Tire.search('at_signs_index') { query { string 'user@foo.bar' } }\n\np s.results.first\n\n```\nThere's something else going on in your codebase, which makes the JSON serialization fail. Could you isolate this so I can try to recreate your problem?\n. Great, glad it worked.\n. Hi,\nyou have to include the created_at_time custom method in to_json call (as suggested by the readme):\nruby\n      def to_indexed_json\n        to_json :methods => ['created_at_time']\n      end\nThe returned results are not instances of you Message model, but instances of Item, which you can generally use interchangeably. You can eagerly load the instances from your database -- see the information in the Readme.\n. The Readme says:\n\nThis way, we can index whatever JSON we like in ElasticSearch, and retrieve it, simply, via the dot notation:\n...\nputs article.title\n...\n\nCan I make it more clear, somehow?\n. Could you have a look in your index, http://localhost:9200/test/message/_search?q=*, how is your documents stored there? If you added a to_indexed_json method like the one above, restarted the app/console, and added some documents, or reimported, you shoud definitely be able to access it as Message.search(\"*\").first. created_at_time.\n. Hi,\n\nthe problem is, that the messages get indexed by a whole other system and there are instance methods of the model that do dynamic stuff that can't be calculated when stored (...)\n\nyeah, you can load the models from the underlying storage with the :load option or load method.\n. What are you trying to do? What do you think is an error here? What do you suggest?\nBoth of these calls work:\nruby\nArticle.search('lorem').to_a.to_json\nArticle.search('lorem', :load => true).to_a.to_json\nShould Results::Collection implement to_json?\n. I'm kinda wary about that... It starts with to_json and continues with to_xml and what else...\nI think everybody is better off with just calling #to_a on the collection and can use whatever core extensions any gem brought into her app? \n. Yeah, why not, while everybody can just overload document_type, it's indeed an unpleasant imbalance of the API.\n. Hi, thanks for the suggestion! I understand and like the idea. I was playing with it a bit today, and think it would be better to expose something like that just for the models; Tire.config seems a bad location for a feature like that?\nI think we should expose it via a model class method, which you could use either on a model-per-model basis, or generally, in an initializer or common config file, a la Tire::Model::Search.index_prefix 'myprefix'... I'll definitely put some time to it!\n. Thanks!, pulled in and cleaned up. I still don't know if the energy wouldn't be better directed at making Tire use Faraday, but... this is clearly better then it was.\n. Hi Tal, thanks! I've merged your commit and then refactored it a bit. It now allows for setting the index_prefix either for all models, or for specific models only -- I've felt it has more balance this way, even though the implementation (class variables, checking context, ...) is far from ideal.\nCould you test it in your app, if it works as expected?\n. Yeah, there's no support for search_type=count, yet. We're doing size 0 queries as well, so far it has not been a pain for the documents in the range of tens of millions for the total index count.\nAs for the mailing list: I've got nothing in principle against creating a dedicated mailing list. However, I don't like Google Groups (the experience, the crappy and ugly GUI, etc), nor Librelist, nor anything else I've seen.\nTo me, discussing stuff via Github Issues is pleasant, it's easy to keep track of conversations, it keeps the discussion focused on code, it's very easy to reference commits or close issues from commits, etc.\nMaybe folks who're watching the repo do mind the chatter, but I like it the way it is, in general.\nSo: don't hesistate and post questions and such via issues. The other option is the #elasticserch channel at Freenode (IRC.\n. Hi, yes, there's several things going on :)\n1. You can sort only on \"not analyzed\" fields. So, either set the field to \"not analyzed\", or, if that would negatively impact searchability, use a multifield type.\n2. The terms filter expects an array of values. Either pass an Array with single value (['michelson']) or use the term filter.\nI can sorta replicate your use-case -- note that I'm using the \"Persistence\" feature, just ignore the property stuff:\n``` ruby\nrequire 'rubygems'\nrequire 'tire'\nTire.configure { logger STDERR }\nTire.index('causes') { delete }\nclass Cause\n  include Tire::Model::Persistence\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nmapping do\n    indexes :user, :index => 'not_analyzed'\n    indexes :civil do\n      indexes :rol\n      indexes :tribunal, :boost => 100\n    end\n  end\nproperty :user\n  property :civil\nend\nCause.create :id => 1, :user => 'Michelson',  :civil => { :rol => 'Foo', :tribunal => 'Bar' }\nCause.create :id => 2, :user => 'Richardson', :civil => { :rol => 'Foo', :tribunal => 'Bar' }\nCause.index.refresh\ns = Cause.search do\n  query  { string \"foo\" }\n  filter :term, :user => 'Michelson'\n  sort { by :user, 'desc' }\nend\np s.results\n```\nLet me know if this helps.\n. Hi,\nit is really hard to debug what's going on unless I can re-create your data and model. Could you at least gist the complete model? (http://gist.github.com)\n. Hi,\nwell, thanks -- but unless I painstakingly create your app (ie. the models Tribunal etc), I cannot test the same thing you're having. I have some advice, though:\n1/ Make sure you delete and create the index fresh, when you change the mapping. The Rake task (see Readme) has the FORCE=true option for that. Check the mapping in ES at http://localhost:9200/causes/_mapping.\n2/ Try loading your models from database with Cause.search :load => true do ... when searching. I've got the impression the relation between your models is complicated, and I'm not sure how everything clicks together based on your to_indexed_json.\n3/ Have a look into the ES directly (http://localhost:9200/causes/_search?q=*), to see how your documents are added.\n. Closing until there's a way to recreate the issue....\n. Hi,\nthis is not possible, presently. I'm aware some search libraries, notably ThingkingSphinx allow something like this, but not Tire. You have to specify the search conditions, manually, yourself. The support for boolean queries and the ability to extract them into blocks/lambdas/methods should help here -- see Readme.\nI'll add more examples how to extract boolean search blocks into model/module methods, based on how the @ruby-amqp guys do it (http://rdoc.info/github/ruby-amqp/amqp/master/file/docs/GettingStarted.textile#Integration_with_objects).\n. Hi, this is a great addition. The implementation looks a bit complicated, though? The ones I know from the Jose Valim's book and eg. http://asciicasts.com/episodes/249-notifications-in-rails-3 are much more lightweight?\n@joshk: Any feedback, please?\n. Hi,\nyeah, achieving what you were after is more complicated then I thought.\nI have cheated :), and extracted your commit as a first component for the karmi/tire-contrib gem; https://github.com/karmi/tire-contrib/commit/b900c29b5e5c778c3a18ebc73a9585ffb5710fcc\nIs that OK?\nI envision tire-contrib as a place for all sorts of additions and extensions to Tire, in a spirit similar to rack-contrib.\n. Yes, thanks for the feature!\n. Hi,\nthe instrumentation for Rails is extracted into tire-contrib: https://github.com/karmi/tire-contrib/blob/master/lib/tire/rails/logger.rb\nActiveSupport is ultimately a dependency via the ActiveModel dependency. I agree we should do a better job of admitting it in the gemspec, I just haven't found enough time to test version conflicts etc.\n\nwhat to you think of putting a simplified version (no alias method chain) of the instrumentation directly into Tire\n\nWhat kind of instrumentation and for what purpose? In any case, I think the best place for anything like that is tire-contrib.\n. Hi,\nI'd like to merge it, but it depends on Model.where method, which is specific to ActiveRecord. ActiveModel does not usually have such method.\nI'm curious, how a situation like this could happen? Isn't the real issue that ES and the db get out of sync? If so, does it make sense to call MyModel.tire.index.refresh?\n. Hi, any news on this?\n. Hi, could you rebase the changes against the current master? If no, I'll do it.\n. It's not, see the merge commit https://github.com/vwall/tire/commit/be6b61d7dbd3ceda01406ddb43b349031f27d850. No worries, I'll rebase that.\n. Yeah, just implement MyModel.paginate as a thin wrapper around Kaminari, see https://github.com/karmi/tire/issues/48#issuecomment-1499519.\n. Hmm, that's kinda weird. I can run bundle exec rake environment tire:import CLASS='Article' FORCE=true in a test app just fine. Could you generate a fresh app with the command:\n$ rails new searchapp -m https://github.com/karmi/tire/raw/master/examples/rails-application-template.rb\nto see if it fails as well? Note that I have my local ES set to number_of_shards:1 and number_of_replicas:1.\n. Hmm, Tire attempts to create the index when you use mapping declaration, checking if it exist first.\nThat should not raise any exception, though, on application start. Only when you search, you should receive Errno::ECONNREFUSED. Just checked now.\nI'm wondering what's the undefined method 'body' for nil:NilClass error. \nThe main change in v0.3.3 is the \"agnostic HTTP\": https://github.com/karmi/tire/compare/v0.3.2...v0.3.3\nAgain, could you try it in the generated app first, please?\n. @ramontayag: The issue is, of course, that I was checking an app launched in development mode. In production, the model classes would be eagerly loaded by Rails and you'd see the exception immediately. Should be fixed no, can you check against current master?\n. @changa: Can you check the issue in your app against current master?\n. Cool.\n. > Any tips on how to troubleshoot this?\nYeah, I'd change the Tire source in the Index#bulk_store method to display the actual data. Usually this is caused by invalid JSON -- it would be nice to catch the error better, but I guess I haven't been able to do so yet.\nLemme know if you find anything.\n. Yeah, that's the way to pinpoint the culprit.\n. Hi, yeah, this is quite stupid. Will look into that. It's another unfortunate impact of the \"HTTP agnostic\" stuff.\n. @ramontayag Hi, the ! in \"hey!\" breaks the Lucene syntax parser -- either escape it or use the text query /http://www.elasticsearch.org/guide/reference/query-dsl/text-query.html/\n. Hi, yeah, there's definitely a bug there.\nThe :size and :from options are valid, when you're searching with the block syntax, see this integration test: https://github.com/karmi/tire/blob/master/test/integration/active_record_searchable_test.rb#L229-234\nWill look into that.\n. @mlitwiniuk: Yeah, that's the way to go, the code is insanely using all the instance var instead of the accessor method. Will change that all.\n. @michaelklishin: Please check the current master, the issue should be fixed.\n. @lennartkoopmann: Yeah, your query is invalid. ES expects closing ], and interprets your query as a range query.\nTire isn't doing any automatic escaping, since that would mean we'd escape your range query -- and you don't want that :) So, you have to properly excape everything by yourself...\n. Tire does not do any escaping, so any escaping is done by the JSON gem you're using.\nThe weird thing is, I'm able to do all the queries you describe:\n``` ruby\nrequire 'rubygems'\nrequire 'tire'\nTire.index 'test' do\n  delete\n  store :content => 'Foo-Bar Test'\n  refresh\nend\ns = Tire.search 'test' do\n  query { string 'foo-bar' }\nend\nputs s.to_curl\ns.results.each do |document|\n  p document\nend\n```\n\nI have to escape the minus sign in it with a backslash.\n\nWhy?\n. Yeah, I did found the document -- just run the script.\nThe - has meaning only if preceded by a space, AFAIK. foo-bar is different then foo -bar, which does translate to foo NOT bar.\n. Lennart, post a full Ruby snippet (eg. based on mine) so we can both just launch it and not out-guess each other :)\n. @lennartkoopmann: You're using a term query, which is not analyzed, ie. broken into tokens. Your document, on the other hand, is broken into tokens \"foo\" and \"bar\". Do a seach for foo.\nAs always in ES, there are many approaches to the situation -- best to ask on IRC.\n. @fgrehm Unfortunately, we can't really escape it automatically, and it's generally known that Lucene query string has \"special\" characters... In any case, best to use the match query in Tire, and special queries such as range etc for special cases...\n. Yes, the immediately looming wave of bugfixes, refactorings and additions will have to include this -- configurable timeouts.\n. Duplicate of karmi/tire#89.\n. You mean this is about timeouts for the HTTP client Tire uses (RestClient, Curb)? The defaults should be something like 60sec which should be enough for most queries? (I've never seen a client timing out myself.)\n. Not sure how well RestClient supports timeouts: https://github.com/archiloque/rest-client/blob/master/lib/restclient/request.rb#L23\n. @rb2k: Actually, you can try it out pretty easily.\nJust monkeypatch the following lines from client.rb:\nruby\ndef self.get(url, data=nil)\n  perform ::RestClient::Request.new(:method => :get, :url => url, :payload => data).execute\nrescue ::RestClient::Exception => e\n  Response.new e.http_body, e.http_code\nend\nPut some timeout into the ::RestClient::Request constructor and see what happens. I'm not terribly against increasing the timeout here to 5 or 10 minutes -- everybody can/should just wrap it in their own timeout logic, then.\n. Hi, when do you get the exception? Because simply not including Callbacks should work. That's the initial reason for splitting the functionality. I cannot replicate the issue.\n\nI only want articles with a state of published to be indexed\n\nNo problem -- you have to solve it in your app, since it and only it knows the domain in that case. Something like:\n``` ruby\n    class Article\n  after_save do\n    self.update_index if self.state == 'published'\n  end\n\n  # ...\n\nend\n\n``\n. Hi, thanks for the sample app, I figured out what's the problem. Please check on current master, should be fixed.\n. Ping, @Rio517. Any success with the fix?\n. Pefect!\n. Hi, I feel sorry for such a fine patch. You can, in fact, achieve what you're after if you do _not_ includeTire::Model::Callbacks` and implement the indexing hooks manually, a la:\n``` ruby\nclass Article\nafter_save do\n    self.update_index if self.state == 'published'\n  end\n# ...\nend\n```\nSee this comment for issue karmi/tire#114.\n. Ping! Any news on this? Were you able to achieve the custom indexing with example I posted?\n. @ralph, @leehambley: I understand your point. I don't think adding a top level thing like should_be_indexed? is particularly useful to general audience, though.\nFirst, you have to properly document and explain, how that works, and you'll still be fighting edge-cases. It's a slippery slope of massive \"optional\" behaviours written for singular cases. We should be more concerned about adding timeout and search_type, folks! :)\nSecond, and more importantly, Tire is just Ruby -- why not use it? I mean, Tire::Model::Search::InstanceMethods::Searchupdate_index is just couple of lines, doing nothing particularly advanced. If one has an edge case, where the indexing logic is somehow more complicated, she should definitely write her own indexing hooks. Possibly doing stuff in the background, via queue or messaging infrastructure, etc. \nAll that said, I'll happily pull that into karmi/tire-contrib!\n@jarosan: Any luck using what I suggested?\n. Hi @jarosan,\nI am still not convinced conditional indexing like this is something which belongs to core Tire. I still think you'd be better off if you took care of the indexing yourself, in this case. I imagine the tests, the code readability, everything would benefit from that.\nThat said, why not extract that, and put it into tire-contrib? Then the only thing you have to do is:\ngem 'tire-contrib'\nrequire 'tire/conditional_indexing'\nI will happily pull that into karmi/tire-contrib.\n. @jarosan: Ping -- need any help with porting it over to tire-contrib?\n. Thanks for the article, Michael, will have a look.\n. @mahemoff As far as I remember, \"some half-baked ActiveModel implementations\" were not providing destroyed?, which would break the \"update or delete\" semantics in update_index. Is the issue you're experiencing on ActiveRecord, some ActiveModel, or Tire::Persistence?\n. @mahemoff This sounds like a bug -- would it be feasible to create an isolated example or even an integration test? No worries if not.\n. Hi Mario,\nthis has slipped under my radar, sorry. This seems like the update(options) call is incorrectly nested, is that it?\nBased on a quick look at http://www.elasticsearch.org/guide/reference/api/search/facets/terms-facet.html my code is actually totally messed up, since there aren't any top level options for a terms facet. Will look into that.\n. @Rio517: The issue should be fixed now, thanks for the report -- the passing of options in date and terms facet was incorrect.\n. Hi,\ntwo things:\n1/ Could you add unit tests for the fix?\n2/ How does ActiveRecord relate to this? The object in Tire::Collection should be of type Item.\n. >  ActiveRecord was trying to set \"description.partial\" on my model class\nYeah, and that's exactly the part where I am wondering how ActiveRecord can do that? :) Because from v0.2 on Tire returns only Item instances, and allows you to load the real, ActiveRecord models with the load method or the :load option to the search() method.\n. Ping, any news?\n. @mikeg250: Tire allows you to set default operator per query, via the :default_operator option.\nThe commit https://github.com/seejohnrun/tire/commit/55ed24 is old and was superseded by new changes.\nI am not aware if there's a way to set it globally in ElasticSearch, and probably wouldn't like such a global option myself...\n\n(...) seems to indicate that Tire doesn\u2019t yet support the AND filter (...)\n\nTire does not support \"and filter\" but does support \"boolean queries\".\nSee @vhyza's gist (thanks!!) for a complete overview.\n. Hi, thanks for the catch, the settings fix pushed in karmi/tire@28989ec.\nAs for the Mongobar, could you please extract that into a karmi/tire-contrib extension?\nI really don't want the Tire codebase to be sprinkled all over with if/defined/whatever Mongo just because all the Mongo gems and ecocystem play by their own rules.\n. If it will be available in ES and if it will make sense to expose it via Tire, yes.\nClosing as a duplicate of karmi/tire#118.\n. Sorry, I don't understand your question. The link you're posting is to a pull request, requesting a feature be added into ES. As such, Tire does not support such feature, because it's not in the ES. I have no personal knowledge and experience with highlighting as related to custom score and boosting.\n. I like the direction, and yeah, we should make it possibly to just call filter multiple times to add more filters. However, the problem in your code is you just add and all the time.\nEg. when you add a puts s.to_json in here: https://github.com/karmi/tire/blob/master/test/integration/filters_test.rb#L20-25, you get this:\n\"filter\":{\"and\":[{\"or\":[{\"terms\":{\"tags\":[\"ruby\"]}},{\"terms\":{\"tags\":[\"erlang\"]}}]}]}\nNot ideal.\n. Thanks to the original implementation, @rubish, code cleaned, pushed, released.\n. Removed the integration tests (really not desired for a edge feature like this), cleaned, pushed, released.\n. Added more tests, cleaned up the Ruby code, pushed, released.\n.  Yeah:\nruby\nTire.configure { url \"http://somewhereinthecloud.com\" }\n. Hmm, why would you pass :load =>\u00a0true to Tire DSL? It only makes sense to pass it to MyModel.search?\n. Yeah, raw queries are only possible on the DSL level. In most cases, there should be no pressing reason to use load.\nIs there a reason you're not able to use a string query against specific :fields, in this particular case?\n. Hmm, I wonder about that. You're doing a string query against fields, using the field query (http://www.elasticsearch.org/guide/reference/query-dsl/field-query.html).\nBut, on the topic -- the :load option really does not have any sense on the DSL level. The more appropriate feature request would be to support \u2018raw queries\u201d in the model search method....\n. I'm really lost as what problem we're trying to solve here :) The :load option for DSL search method does not make any sense... Why add a raw_query key? There must be a way to solve the problem you have.\n. @nickhoffman: I know you got the query from Clint, of course :) I still think a string query against specific fields is appropriate in your case, and I'm saying it for one last time :)\n@aaronchi: What exactly are you confused by? Difference between the DSL and ActiveModel interface?\nThere is, indeed, a \"single search object\", shared between all the search methods. It's Tire::Search::Search.\n. @nickhoffman: No, nested queries are not supported at the moment. So, the problem got shifted from dis_max query against specific fields (doable) into a nested docs and queries (not doable), correct?\n. Nick, what should we do with the issue then? Close, open another one, is there some workaround, etc?\n. Hmm... yes, it would be useful, definitely. Maybe there's even a way to support that -- inspecting the query object in Tire::Model::Search.new and it it's a Hash, sending it to ES via some Tire::Search::Search#raw method.\nNot ideal, still kinda ugly, but tolerable. First the \"raw queries\" support must get much tougher, though, such as properly logging requests/responses, etc.\n. 10 shards is quite high number, IMHO. The cluster takes some heavy pounding in this case.\n5000 docs per batch also seems like a big number to me...\nAlso, I suspect the real culprit is dynamic mapping, there is really an overhead with that. That's why I suggested we takes this on IRC, so we can pull kimchy into it.\n. Hi, I've cleaned up a bit and merged the sakrafd/tire@c0c2c85 commit, thanks!\nBTW, the proper way around this would be to eat all the valid options for _search endpoint, that would solve many issues such as search_type=count, etc. Will have to look into that.\n. Hi,\nit's probably some mis-configuration in your project, or some edge-case bug. Please do verify with curl or browser, that your index exists and data have been stored: http://localhost:9200/deals/_search?q=*, as @vhyza suggested.\nAlso, what type of model is Deal? ActiveRecord, ActiveModel (eg. Mongoid), etc.\n. Hmmm, so http://localhost:9200/deals/_search?q=* actually returns all the docs, but Deal.tire.search '*' does not return anything? That's crazy :)\nYour model looks alright to me. What if you try searching on the DSL level?\nruby\nTire.search(\"deals\") { all }  # or similar\n. The correct syntax is: Tire.search(\"deals\") { all }, beware.\nThe \"uninitialized constant Document\" is a nasty, known, issue, which I'm about to fix right now.\nThere's still something crazy with your model integration, the Deal.tire.search \"whatever\" should work. Lemme release a new version, and we'll continue debugging then.\n. Grr, sorry, Tire.search(\"deals\") { query { all } } (quite obviously :)\n. @ellmo: Could you rerun the tests with the newly releasesd 0.3.6? It will solve the NameError, at least.\nStill, I don't understand why it's not returning results. Are you sure you're waiting the default 1sec, or refreshing the index? Is this not a part of a test suite?\n. @ellmo: As suggested by @vhyza, double check your documents have the \"deal\" _type. You must be doing something funky if your documents end up with different _type in ES, or there's a grave bug somewhere.\n\nwhen I call t.results.size on the object it's size is 10 and I have 352 objects indexed\n\nresults.size gives you the size of returned collection, results.total will give you the total size of documents for your query.\n. @ellmo: Ping, any news?\n. Michael, I've removed SDoc completely, it seems to break on current RDoc and I have no time to investigate it.\n. John, thanks for the patch. I'll look into that soon -- I'll just change couple of things, like adding a separate test, not changing existing, and maybe changing the name: fields.\nI know it's consistent with the http://www.elasticsearch.org/guide/reference/mapping/boost-field.html naming, but I think options will be more consistent with general Ruby coding style and Tire codebase... What do you think?\n. Hi John, I've cleaned up the patch a bit, reverted the change to test, and added a separate test for the feature (should \"create the index with proper mapping options\").\nThanks!, K.\n. Also, it's much more convenient for the administration, when heterogenous patches do not end up in one pull request (such as the \"re-raise\" one :) Usually, in Git, it's a very good idea to create a branch for every logical patch/feature...\n. Hi Dylan,\nsorry for such a delay with my answer. I have several remarks regarding the feature, and couple of questions. So, questions first:\nIt seems not so common to return instances of multiple classes from a single search? How do you handle incompatible model interfaces, etc? I can imagine that it's more common when using single table inheritance or such.\nNote, that usually, it really makes sense to leave Tire return the Item instances, and work with those, especially if you're only displaying the data to the user. I'd be very curious to hear arguments against such approach, because that's and advice I keep giving to users.\nNow, regarding the implementation. I think I won't surprise by describing the implementation as very complex, very hard to follow, and solving an apparent edge-case. Please, don't take this as a stupid gate-keeper guarding the codebase or something like that. I'm just trying to keep the Tire codebase approachable, and not cluttered, for the longer term. Anything which adds \"dead code\", something which is hard to change, refactor, extract is a huge liability.\nIt certainly does not help that the Collection#results method is such a beast, and quite convoluted already. It should be refactored, extracted into smaller methods, into a module or class, so anybody trying to change the behaviour -- as you're doing -- would have to overload just one or two methods (and decided if s/he would like to keep them private, or publish them in karmi/tire-contrib, etc.)\nNow, speaking practically. I have tried to reconstruct your case, and this is what I came up with, inspired by your code: https://gist.github.com/1312996.\nIt seems to me it's doing what it should. If I were you, I would put such code into a Ruby module, including it into the application, using the low-level Tire DSL/API, and not the ActiveModel integration directly. That way, you'd have tight control on the logic, you could fine tune the code structure or performance without hanging in thin air, waiting for the upstream to take your changes, maintaining your fork, etc. Based on my experience with eg. CouchDB gems, I'd very much prefer such approach over constantly \"fixing\" an external library...\nI'll gladly hear any opinions and feedback on this...\n. Hi,\n\nSearching across multiple types to me didn't seem like an edge case. It allows a search field to be present as part of the layout (...) This could be used to search for tv shows and movies, articles and comments, blog posts and other pages, etc.\n\nindeed. That's a very common and valid scenario. In this scenario, and precisely in this scenario, I'd work work with the shallow Hash/Ostruct-like instances of Item, for displaying the content snippets and links to results. (Tire tries hard to make eg. URL helpers work.)\n\nI do agree that it is best to avoid loading data from the database (...) existing codebase that is currently using Sphinx (...)\n\nI can imagine. Thinking Sphinx, while unbelievably polished and well interfaced library, instilled some pre-conceptions into people's mind how search should work within Rails, and in Ruby in general. With ElasticSearch, many of these pre-conceptions are not justified, and in many cases they are downright false.\nWhen we have the ability to return content at will from the engine, it is just wrong\u00a0to load data from the database. I have yet to hear a compelling argument otherwise.\n\nDisplaying these results is also non-trivial, because I am developing for a platform which provides the ability for custom search results pages to be uploaded as liquid templates. Obviously this isn't the typical use case, but Tire already seemed to have the ability to load models from the database, so I thought the code would be useful upstream.\n\nIf that is the case, I think this is another, quite strong reason to exploit ElasticSearch's ability to return \u201cfull\u201d content for you. If we're talking about Shopify, it's on par with other features where ElasticSearch makes sense for you: easy multitenancy (account-based indices, configurable aliases with filters/routing, etc), distribution, powerful facets. You may also be the first to actually make use of the :wrapper option, since you could create your own, \u201cenhanced\u201d Item implementation.\n\nAlso take note of the commit that allows the search to gracefully handle missing records.\n\nYeah, I did notice it, and I did also notice you're using a where() method :) That may fly with ActiveRecord, but half of people using ElasticSearch/Tire are not using ActiveRecord-based models (me included), and moreover, I personally don't care about ActiveRecord a tiny little bit. If I'd developed Tire solely for my purposes, there would be no ActiveRecord support.\n\nIt would be a bad user experience to have a search fail just because one of the results can't be found.\n\nThat's true. I should look into that and guard against missing records. The chance of such failure seems to me quite small, however, for most use cases. In a system with high throughput, where records are destroyed quickly, it would be a problem.\n\nI am already using the Tire.search, not the ActiveModel integration directly. The :load parameter isn't handled in Tire::Model::Search, it is accessible from Tire.search. \n\nI think it's the other way round? But yes, Tire.search does not expose the :load option, since it does not have any notion of a \u201cmodel\u201d. I really think it's much more flexible and robust to implement specific use cases on top of the Tire API/DSL. That said, I am quite mad at the Tire::Results::Collection#results method. It is a mess and does not allow easy overload, monkeypatching.\n. > > \u00a0When we have the ability to return content at will from the engine, it is just wrong to load data from the database.\n\nIt was just meant to be a transitional step. A Tire::Results::Item obviously doesn't have the same interface as the model it represents.\n\nYeah, it does not. However, for the type of listing I imagine one would end up with, the free-form Item instances would still be the best. You could eg. display a title (of movie, TV show, article, ...), specific fields where applicable (author of article), and a link to the result, via the link_to helper. In a \"results\" listing like that, there's generally no need to have the full model accesible?\n\nPerhaps one that (...) can still load data through relationships with other tables.\n\nActually, a custom Tire::Collection class could be able to do that. If Tire would expose that as an option, then it would be trivial to implement logic like yours cleanly. That's why I was saying that the whole results loading, parsing, etc. stinks in the current codebase, and should be refactored...\n. Dylan, what should we do about this issue?\n. Hi Dylan, I ended up let this feature pass, for the time being. The whole Tire::Results::Collection.results method is complicated as it is, and this change would made it even more opaque and fragile. This method must be refactored, logic extracted to smaller methods, etc., before we can attempt for anything like this.\nAfter that, a feature like yours should be easily supported, and everybody could decide if it's something she would like to share in upstream, either in core or in contrib.\nAlso, the whole approach of loading records from database seems suspicious to me (as already stated). Normally, there should be no need to do that, and certainly not in a use case like \"I want to display links to various stuff when people perfrom search\".\nI understand the approach of working with the \"real models\" all the time is more convenient, and people are used to it by using libraries such as ThinkingSphinx, but I think ElasticSearch deserves more care and experimentation...\n. ES/Tire make it easy to do multi-model searches, just do:\nruby\nTire.search ['articles', 'comments', 'whatever'] do\n  query { string 'whatever' }\nend\nYou just won't get the \"real\" models, but instances of Tire::Results::Item -- you have to take responsibility from there, and either work around that, or retrieve your models by yourself.\n. @luxflux Yeah, I should put that into Readme.\n. Ralph, outstanding patch, thanks!\n. Jason, do you mean the custom_score query?\n. Should be easy to add, it's a straight-forward patch a la https://github.com/ralph/tire/commit/c2b496c, anybody got 20mins to do it? :)\n. Great, Paco!\n. Hi Paco, sorry for such a big delay with reply. Yes, you have to wrap all queries in a query JSON property. Have a look at Tire::Search::Search#to_hash, which does this \"final\" wrapping. It's a bit \"strange\", but I understand why it's so in ES's DSL.\nThat said, this curl works for me (against the articles-test index based on Tire fixtures, as you suggested):\nbash\ncurl -X GET \"http://localhost:9200/articles-test/_search?pretty=true\" -d '\n{\n  \"query\" : {\n    \"custom_score\" : {\n      \"query\"  : { \"query_string\":{\"query\":\"title:T*\"} },\n      \"script\" : \"_score * doc[\\\"words\\\"].value\"\n    }\n  }\n}\n'\n. @jfalk: The beauty of ES's API and Paco's implementation is that passing parameters to scripts is already working, see examples in the last referenced commit.\n. Ralph, thanks!, cleaned up the tests a bit and pushed.\n. Hi, just yesterday I was thinking the :wrapper option is redundant, fragile, and really not much usable anyway. In your implementation it does actually makes sense. Are you using it somehow, or was it just formal refactoring?\n. Thanks, Dylan, merged &\u00a0pushed.\n. Hi, yeah, Tire models check with the ES server if the corresponding index exists or not, so they know if they should create it or not. You should mock the HEAD request with WebMock/FakeWeb, etc.\nDo you have any specific tip where &\u00a0how to mention it in the Readme?\n. Hi, thanks, I'll close this for now, and will keep the note about instructing users to mock ES requests in their unit tests in mind...\n. @RKushnir which \"trick\" and why doesn't it feel \"quite right\"?\n. Yeah, Fakeweb has a reversed approach, by default all requests are allowed. Maybe the code in test helpers could be structured so that Webmock and it's definition are loaded first, and Tire is loaded afterwards? We require 'tire' in our test helper after all initialization has been done IMHO.\n. Hi, the only issue I see is that the index is created with incorrect mapping. Notice that you have to either delete the index before loading the app (hence the index is created properly with Tire), or you have to take care about deleting/creating index into your own hands (model class methods, etc)\nI have tried a simplified use case and the following code works for me. Could you try it and report on results?\n``` ruby\nrequire 'tire'\nrequire 'active_record'\nTire.index('venues') { delete }\nActiveRecord::Base.establish_connection(adapter: 'sqlite3', database: \":memory:\" )\nActiveRecord::Migration.verbose = false\nActiveRecord::Schema.define(version: 1) do\n  create_table :venues do |t|\n    t.string   :title, :latitude, :longitude\n  end\nend\nclass Venue < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\ntire do\n    mapping do\n      indexes :title,   type: 'string', analyzer: 'snowball'\n      indexes :lat_lon, type: 'geo_point'\n    end\n  end\ndef lat_lon\n    [latitude, longitude].join(',')\n  end\ndef to_indexed_json\n    to_json(only: ['title'], methods: ['lat_lon'])\n  end\nend\nVenue.create title: 'Pizzeria Pomodoro', latitude: \"41.12\", longitude: \"-71.34\"\nVenue.tire.index.refresh\nquery = { query: { filtered: { query: { match_all: ''} }, filter: { geo_distance: { distance: '100km', lat_lon: \"41,-71\" } } } }\nputs \"\", \"The query:\", '-'*80, query.to_json\ns = Tire.search 'venues', query\nputs \"\", \"Results:\", '-'*80, s.results.inspect\n``\n. The mapping was correct, I think the:lat_long => truepart is not neccessary -- the issue is _somehow_\u00a0the index is created with incorrect mapping. Check it with http://localhost:9200/YOUR_INDEX/_mapping.\n. Hmm, thestreet_nameeludes me. But, importantly, what's the purpose of passing:lat_lon => true` to mapping? Why is not \"geo_point\" enough?\n. > Which I believe would allow me to call doc[lat_lng].lat or doc[lat_lng].lon\nAh yes, sure. All OK now, then?\n. Geo is not exposed right now, you have to use the Tire.search DSL, or other means... I've put some info here: https://github.com/karmi/tire/wiki/How-to-work-with-locations-(geo)-in-Tire, recently.\n. I don't understand what you mean by \"search on multiple facets\".\nI've added a test case for multiple facets.\nPlease, make sure you're attaching a relevant Ruby when something \"does not work\". In this case, making the test case fail would be the best code.\n. Hmm, yeah, thanks for the digging! That's certainly the point where it got \"broken\" for you. However, it was broken\u00a0for cases where you wanted to pass options to facets, such as the :exclude or :value_field (see tests karmi/tire@d1f89e6).\nLemme play with it a bit in the test suite for a moment.\n. OK, that was easy :) Thanks!, added couple of tests to tighten this up. And yes, you have to pass it to the facet method itself, see the rather lengthy commit message in karmi/tire@52fade0 for details.\n. Hi, terriffic patch, merged &\u00a0pushed.\nAlso, I've added couple of _score computation \"more real-word\" examples to integration tests.\nBTW: Adding commits to existing pulls is very easy. The best way is to create a branch for the issue (like you did), and then just keep pushing there with force. You can rebase, squash commits, change them, whatever, and then just push with --force. But it's a non-issue, practically, I can make sense of it quite easy on my end :)\nBTW naming the branch issues/133 has interesting benefits for you, such as \"folder-like\" display in GitX etc. :)\n. Hi, first of all, notice that the following does work on current Tire master:\n``` ruby\nrequire 'tire'\nTire.index 'venues' do\n  delete\n  create :mappings => { :venue => { :properties => { :lat_lon => { :type => 'geo_point' } } } }\n  store :type => 'venue', :title => 'Pizzeria', :lat_lon => [41,-71]\n  refresh\nend\ns = Tire.search('venues') do\n  query  { all }\n  filter :geo_bounding_box, :lat_lon => { :top_left => [40, -71], :bottom_right => [42, -72] }\nend\nputs s.to_curl\nputs s.results.inspect\n```\nNo need to add any \"specific\" geo support to filters I'm aware of, but I may be mistaken. What do you mean by \"they need to accept more arguments than the standard \"terms\" filter\"?\nSecond, I wouldn't go the blockish route as you suggest in the latter example.\nThe reason I was not so crazy about adding geo support directly is influenced by these facts: it's doable with the existing codebase, and there are many things to take into consideration regarding convenient interface. Also, I haven't had a practical use case for geo handy, and it's much better to have one then to code against an illusion.\nAll that said, I think ultimately Tire should expose a higher level abstraction for geo features, something along the lines of:\n``` ruby\ns = Tire.search('venues') do\n  query  { all }\n  geo    { distance :location, '200km', [40, -71] }\nend\ns = Tire.search('venues') do\n  query  { all }\n  geo    { polygon :location, [40, -70], [30, -80], [20, -90] }\nend\ns = Tire.search('venues') do\n  query  { all }\n  geo    { bounding_box :location, :top_left => [40, -70], :bottom_right => [30, -80] }\nend\n```\nIe. expose a top level geo method in the search DSL (via Tire::Search::Search), define methods for all the ElasticSearch's geo-specific filters, which would take document field as a first argument, and all the specific options for each filter as remaining arguments.\nThat way, the interface would be quite fluent, would match ES 1:1, and would be in tune with the rest of the API.\nGiven \"enough time or money\", it should be quite easy to implement that :)\n. @tcocca: You can chain :or filters like this: http://github.com/karmi/tire/blob/master/test/integration/filters_test.rb#L27-28\n. @mleglise What do you mean by \"geo support\"?\n(As said above, it is \"supported\" in a a sense \"you can make geo queries\". It is not supported on the DSL level, and since I haven't encountered the need to implement the outlined DSL yet, it's not scheduled to be supported on the DSL level.)\n. Yeah, I've not authored the page, so go forward and update it with the solution from above, if it works. Eventually, it will be supported on the DSL level as outlined in my sketch...\n. Closing this, since geo filter is doable, see also #169, opening new issue for the DSL extension.\n. Hi,\nyes, you can get individual documents via the Index#retrieve method. Also, you can query for multiple ids with the ids query in Tire.\n. Hi, in the end, I ended up refactoring the mess for logging results in Index methods, and also added a \"blank?\" check in the location you were changing. Thanks for the report!\n. What do you mean by \"let the user supply one\"? You can pass an instance of any IO-compatible class to Tire.configure { logger ... }.\n. First, Tire's \"logger\" servers a different purpose then the usual Rails/ActiveRecord logger. It's purpose is to output a nicely formatted curl-based session transcript of talking to ES.\nSecond, Tire's logger does take any instance of any IO-based class (based on respond_to?(:write) check), so you're free to pass your custom objects. There hasn't really been a pressing and real need to explore it further.\nThere's a very nice ActiveSupport notification based Rails logger in contrib, see https://github.com/karmi/tire-contrib/blob/master/lib/tire/rails-logger.rb.\nI was playing a bit with it, and made it a bit easier to pass Logger, Log4R instances to the logger method, so thanks for the hint!\n. For an example:\n``` ruby\nrequire 'fileutils'\nrequire 'active_support/buffered_logger'\nmylog = ActiveSupport::BufferedLogger.new 'buffered-logger.log'\nmylog.instance_eval do\n  alias :write :info\nend\nrequire 'tire'\nTire.configure { logger mylog }\nTire.index('logger-articles') do\n  delete\n  create\n  store :title => 'One', :tags => ['ruby']\n  refresh\nend\nTire.search('logger-articles') { query { string 'One' } }\n``\n. Hi, not really -- such behaviour is weird. Are you sure the index is created with proper mapping and that you're putting in docs of proper:type?\n. It does not ring any bell to me. The Curb instance is instantiated once and then reused: https://github.com/karmi/tire/blob/master/lib/tire/http/clients/curb.rb#L10.\n. @rb2k: Yeah, I had this suspicion. You can just create another Curb-based client, which creates new client for every request, copying the [~50 lines of code in Tire](https://github.com/karmi/tire/blob/master/lib/tire/http/clients/curb.rb#L10) into your own file, and pass it toTire.configure { client MyCurbClient }`.\nIs this all about working around the timeout issue? \n. @rbk: Ping, any luck with it?\n. Ping, any news? :)\n. @rb2k Marc, let's close this ancient thing for now and open a new one if the problem still persist? \n. @rb2k No, sorry it took me so long to get back to this :)\n. @brupm A piece of Ruby code which I can run and which reliably demonstrates the issue would be the perfect help at the moment.\n. @brupm The HTTP is pluggable, so it should be easy to add whatever client just by adding a Tire::HTTP::Client::XYZ class. There's also a Faraday-based client, which should allow you to use whatever adapter is supported.\nThe Thread.current[] fix should really prevent the issue -- make sure you use proper version etc?\n. Any code please? On current master it raises Tire::Search::SearchRequestFailed: 404 : {\"error\":\"IndexMissingException[[zzarticles] missing]\",\"status\":404}`.\n. Oh yeah, you're using the \"dumb\" version of DSL, the issue should be fixed on master and newest Rubygems version, check the attached commit for info.\nBTW, why not use the blocky, more semantic DSL version?\n. Hey, sorry it took so long. Yeah, it would be better to have a proper method for that, but let's work on more important things, since it amounts to doing:\n``` ruby\nrequire 'tire'\np Tire::Configuration.client.post \"#{Tire::Configuration.url}/#{Tire.index('articles').name}/_flush\", ''\n```\nClosing for now, let's think about it when refactoring and adding features like aliases to Tire::Index...\n. Because there were some cases where people complained that the id &\u00a0type are missing from the JSON, and also, this makes it easier to instantiate records, etc. What's the real issue with this? The \"duplication of data\"?\n. Hi Robert,\nI would gladly give you an answer or help you, but I still don't understand the problem at hand.\na) What is \"the elastic search way\" of creating documents?\nb) I don't understand what are the practical implications of having the _id and _type fields duplicated in the _source JSON document.\n. Hi Robert,\n\nthis is more a refinement, not a real problem with the functionality.\n\nonce we have search_type=count implemented, once the Tire::Results::Collection#results is refactored from the current mess, once we have a nice DSL for geo features, once we have support for attachments, then\u00a0I'm all set for solving \"refinements\" like these :)\nThat said, feel free to submit patches solving the \"duplicity\"! (Mixing in the info from the options/URL to the attributes hash for Items, etc etc etc.)\nBest,\nKarel\n. Yes, setting the option properly in the Hash is fine:\n``` ruby\nrequire 'tire'\nTire.configure { logger STDERR, :level => \"debug\" }\nTire.index 'index-with-ttl' do\n  delete\n  create mappings: {\n    document: {\n      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n      _ttl:       { enabled: true , default: \"1d\" },\n      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n      properties: {\n        title: { type: \"string\", analyzer: \"snowball\" },\n        # ... rest of properties mapping ...\n      }\n    }\n  }\n  refresh\nend\n``\n. Ping, any news?\n. Great. I don't think Homebrew installs 0.16.2: https://github.com/mxcl/homebrew/blob/master/Library/Formula/elasticsearch.rb#L4\n. Yeah, the ES community updates the Homebrew formula pretty frequently...\n. Hello, sorry for the ridiculous delay. [Index#bulk`](https://github.com/karmi/retire/blob/master/lib/tire/index.rb#L202) supports the bulk options, so that's probably easiest. Or just monkeypatching the support in.\n. I'm sorry, but I don't understand the problem. Could you be more specific as to what problem you have?\n. I don't think it's the case on current master and version 0.3.11, this code behaves right:\n``` ruby\nrequire 'rubygems'\nrequire 'tire'\nextend Tire::DSL\ns = search 'articles' do\n  query { string '[x' }\nend\np s.results\n```\nAre you using the latest version? Does the code above print an informative statement in the exception (response code and body)?\n. Hi, why is this needed?\n. Hi, weird, what Ruby version this is?\n(AFAIK: Ruby 1.9.x should have Minitest wrapped in Test::Unit, which is the default on 1.8, so we're not forcing another specific gem on 1.8 users here. Cf. http://endofline.wordpress.com/2010/12/10/ruby-standard-library-mini-test/.)\n. Ping, any news on this?\n. @grasscode, OK, thanks! So it's 1.9.2-related, since:\nrbenv shell 1.9.3-rc1 \nrake\nLOADED SUITE test,test/integration,test/models,test/unit\n...\nIs there any pressing reason we should support 1.9.2?\n. It's of course stupidly easy to just add some line gem \"minitest\" if RUBY_VERSION < '1.9.3'... I was just wondering if 1.9.2 is widely used -- my impression is that people on 1.9.2. should migrate to 1.9.3?\n. http://www.ruby-lang.org/en/downloads/ says:\n\nThe current stable version is 1.9.3.\n\nI don't think using 1.9.2 in production makes sense, and do think supporting 1.8.7 and 1.9.3 is quite sufficient, so closing...\n. So, any hard feelings if we close this as 1.9.3 seems better choice for development/production now?\n. Great, thanks, closing then...\n. AFAIK what they describe in a kinda complicated way is standard HTTP Auth, so you should be able to set Tire's URL to something like http://username:password@whatever.elasticsearchhq.com:9200.\n(If you would need to set custom headers, you need to subclass &\u00a0customize the client. But I don't think this is the case.)\n. But why monkey patch that? Why not create a custom client, and use that in Tire.configure { client MyCustomizedClient }?\n. But also hard to maintain on longer run...\n. @ryw: That's weird, adding the header should be trivial. But yeah, ES should run very fine on EC2.\n. Hi, @jakeatln, what kind of information you need? You just point Tire to an IP a your master of your cluster in EC2 (best to use private IP, make sure your application and ES node are in the same security group), probably in an application initializer, you're done.\nAlso, notice that there's a http://bonsai.io/: hosted ElasticSearch as a Heroku addon, currently in beta.\nPing me here or at the #elasticsearch channel at Freenode if you need more help/advice.\n. Hi Chris, do you have any example code as a \"suggestion\" so I understand better? \nThe mapping itself can be changed freely, just by calling the mapping method multiple times, but the problem is of course the create_elasticsearch_index call. I haven't found a better way how to do this.\nOf course it would be trivial to support something like mapping :create =>\u00a0false do ... end so you can define the mapping and the call the create_elasticsearch_index method yourself.\n. Hello Chris, sorry for all these delays... I like the extraction of the indexes logic to module very much!\nAs always, @floere is absolutely right :) I agree it would be a perfect handler for executing the create_elasticsearch_index method. Feel free to take a hit on the issue! Otherwise I'll try to have a look in it soonish.\n. Hey @chrisberkhout, sorry, another delay. Hmm... been playing with the END handler, and I see the problem that when you require the file, it really does not fire the handler. Have you had any success with it?\n. OK, hello from the future guys! :)\nTwo things:\n1. I think everybody should just create their indices in advance, and use the provided auto-creation only as a nice sweetie.\n2. It's pretty doable to extract mapping into modules, as Chris suggested (copying from another issue I solved):\n``` ruby\nrequire 'tire'\nrequire 'active_record'\nTire.index('articles') { delete }\nActiveRecord::Base.establish_connection(adapter: 'sqlite3', database: \":memory:\" )\nActiveRecord::Migration.verbose = false\nActiveRecord::Schema.define(version: 1) do\n  create_table(:articles) { |t| t.string :title }\n  create_table(:people)   { |t| t.string :title }\nend\nmodule Searchable\ndef self.included(base)\np \"Included in #{base}\"\n\nbase.class_eval do\n  include Tire::Model::Search\n\n  tire do\n    mapping do\n      indexes :title,   type: 'string', analyzer: 'snowball'\n    end\n  end\nend\n\nend\nend\nclass Article < ActiveRecord::Base\n  include Searchable\nend\nclass Person < ActiveRecord::Base\n  include Searchable\nend\nArticle.tire.create_elasticsearch_index\nputs \"Article mapping:\",\n     Article.tire.mapping,\n     '---'\nputs \"Create an article:\",\n     (article = Article.create title: 'Testing...'),\n     '---'\nputs \"Update index...\",\n     article.tire.update_index,\n     article.tire.index.refresh,\n     '---'\nputs \"Perform search:\",\n     Article.tire.search('title:test').map { |a| \"ID: #{a.id}, title: #{a.title}\" },\n     '---'\nThe same thing with people...\nPerson.tire.create_elasticsearch_index\nputs \"Person mapping:\",\n     Person.tire.mapping\n...\n```\nSo, closing for now...\n. Weird, works on Rails 3.1.1, merged &\u00a0pushed though, thanks!\n. Hi, I understand the issue, but is this something which is expected by the end-user? How many lines is \"too much\"? 50, 100, 1000? Wouldn't it be better to don't use debug mode in this case?\n. I don't know. What I do know is that cutting the response being logged at 50 lines is a) arbitrary, b) too short for common use-cases. Is there any other way around this? \n. Tal, I really don't understand why this is a problem.\nThe debug level is mostly useful for inspecting responses when things don't go as expected, or, mainly, to post a recreation of some ES session to mailing list or IRC. (The each_line is there for formatting.)\nI'm sorry but I'm closing this.\n. @redbeard: Tal, it just occured to me -- why don't you write your own custom logger, which limits the output, and pass it to Tire.config { logger MyCustomClass }? Is that a viable solution?\n. @bbonamin What makes you think and say that the \"library isn't maintained anymore\"? Did you check the number of discussions going on in issues here, the amount or assistence given to people on IRC, and other things?\nTo simply repeat it again, this library is actively maintained, it has just been \"retired\" or \"deprecated\" in favour of the new suite of gems, which are superior in all aspects of software development.\nI do agree that the particular part of code should absolutely use the << operator. On the other hand, I'm having a hard time imagining leaving the \"trace\" on when you're concerned about performance. It's a debugging tool and setting.\n. @bbonamin No, you didn't offended me in any way, I just wanted to make the context clear.\nIn general, I'm against adding features to the gem, or \"fixing\" minor issues like the one you linked, that's just wasted energy.\nI would 100% and absolutely merge a pull request for this, that's of course a plain oversight in the code, and the << should be of course used.\n. Hi, is it something you need or is it a \"speculative\" patch? :) Would you mind to create an extension to tire-contrib for that?\n. Well, if I understand the ES docs correctly, it returns only a portion of results where the score is above a certain threshold. That may just what you wanted in a tiny slice of use-cases. Most times, you want to actually display some results...\nThe ES docs say: Note, most times, this does not make much sense, but is provided for advance use cases.\nThis seems to me as cross the line between core and contrib: core is what 80% people would use, contrib is for specific/advanced/crazy stuff.\n. > Question, do you have any guidelines on the modularity of items in tire-contrib?\nThat's easy, just add/overload/etc anything in the Tire namespace, add tests, etc. See the rails-logger extension for inspiration. The whole thing is modeled after rack-contrib.\n. Hello @edmundsalvacion, can I help you with the tire-contrib integration? Ping me here or at the #elasticsearch IRC!\nIt should be very easy to transfer the code from core to contrib. I'll close the pull request here and wait for a pull request on contrib...\n. > Why this patch is not included in core?\nBecause it's a great fit to contrib.\n\nWithout this feature, search result is ugly(often).\n\nWhat do you mean?\n. > When you know that search result is not relevant when score below the value 0.5. Best way to hide irrelevant results is specify the \"min_score\".\nI still consider the case where you don't want to present any results whatsoever to the user an edge case. I can't imagine it widely used; the computed score varies wildly and people are usually better at judging the results, refining their query, etc.\nI look forward to see the pull request on tire-contrib!\n. I'm not sure I understand the issue, could you be more specific please?\nRecent releases of Tire do not include its methods in the class, when they are already defined. They are accessible via the tire proxy, eg. MyModel.tire.search.\n. Yes, since search is mostly not used already... The README explains it in detail a bit further.\n. @bashcoder It's really highly specific to Mongoid. You're right that in the Mongoid part, we should use the tire proxy, good suggestion!\nUpdate: Fixed in aa19638\n. Hi, if you use the \"raw\" API by sending hashes over the wire, than you have to mix the size and from values into the hash, as describe in the ES docs.\nPing me again if you encounter any problems...\n. @digitalplaywright I think from and offset should behave just like you expect. Closing the issue for now, open new one if the problem persist please!\n. Hi, thanks for the feedback. Now:\na) What problem exactly do you experience by Tire::Results::Item masquerading as a different class, within Rails. It does so for a purpose, and it is clearly laid out in the Readme.\nb) Could you be more specific as to how would SimpleDelegator (or any other delegation) solve the problem that Tire::Results::Item instance must generate proper dom_id strings or work in article_path(@article) helpers?\n. @marioaquino and @coffeencoke: No point in +1-ing anything here, really. Do you have any concrete suggestions/ideas or a concrete problematic behaviour to demonstrate?\n. @adkron: Ping. I'm curious to hear some feedback on my questions.\n. Hi, thanks for the update, Adam.\nThe thing is, this whole story with overriding class is much more complicated then everybody thinks \u2014 or I am indefinitely stupid :)\nFirst, I don't know how delegation as a concept would solve this issue. We really don't want to delegate to anything, because we don't have anything. We don't have the \u201creal\u201d model loaded. We have only some JSON returned from elasticsearch.\n(Notice how we use delegation in Tire to expose the methods such as mapping, settings, index, search either directly in the class or in the tire instance and class proxy methods: https://github.com/karmi/tire/blob/master/lib/tire/model/search.rb#L225-241.)\nPlease correct me if you feel I'm wrong on this.\nSecond, the whole purpose of this is to make Tire::Results::Item instances work within Rails, without loading the \u201creal\u201d models from the database. As you may be aware, that means eg. dom_id helpers, and, prominently URL helpers.\nYou may try it out very simply: just comment out the def class definition, and you'll see the errors.\nYou'll see how Rails' https://github.com/rails/rails/blob/master/actionpack/lib/action_dispatch/routing/polymorphic_routes.rb fails to construct the correct route, and when you do the digging, you'll discover how https://github.com/rails/rails/blob/master/activemodel/lib/active_model/naming.rb#L162-164 calls the class method on the passed model (if it's not a class).\nNow, here's your turn, @adkron, @marioaquino, @coffeencoke. How should we handle this? Is there a way to fake it for ActiveModel/ActionPack without over-loading class? I'd be very glad if there was.\n. @adkron, @marioaquino, @coffeencoke, would love to hear your thoughts on the last comment... Closing the issue for now.\n. > Providing an object that acted and appeared like a specific model, but wasn't the model, caused quite a lot of issues and pain when we were using Tire.\nInteresting -- would love to hear them, because we didn't hit any issues with that. Once you understand that the result object just looks like your model instance (and it does not hide it, see eg. https://github.com/karmi/tire/blob/master/lib/tire/results/item.rb#L68), you're OK, I'd say.\nI understood the issue as being submitted by @adkron as an academic concern. Moreover, delegation really can't solve anything here (unless I'm seriously mistaken), which is why a bit of sarcasm may have creeped into my answers.\nUntil ActiveModel works differently in Naming.model_name_from_record_or_class, I don't think we have much choice, if we do want to use the Item instances in Rails helpers, views, etc. And I, for one, seriously do want and need to do that :)\n. @tieleman Thanks for the suggestion.\nNevertheless, your implementation only adds support for is_a? -- is my understanding correct here?\nDo notice that you cannot delegate in the proper sense here, because you simply don't have the object to delegate to.\nMy implementation \u201cmasquerades\u201d as the real object (by over-riding class, making it possible to use Item in Rails view helpers), your implementation \u201cdelegates\u201d to a bogus instance of the model, created with MyModel.new(attributes). (For a fun piece of archeology, the behaviour has changes couple of times here, to accommodate for most cases, see eg. https://github.com/karmi/tire/commit/2295135.)\nNow, we can argue what's a better/cleaner way, but I would generally stay out of calling new on ActiveRecord classes as far as I can. Second, the default Item is by no means the only available wrapper for results -- you are free and encouraged to create your own. In your case, you probably could make your Article class a nice wrapper.\nThird, I agree that Item#is_a? should behave properly -- that is, in my opinion, orthogonal to all this \u201cdelegation\u201d debate?\n. @tieleman Thanks for your thoughts!\n\nNote that in my examples I wasn't actually using ActiveRecord models, but plain Ruby objects that implement ActiveModel.\n\nYeah, I noticed. But the problem is that many people work with \u201cunderlying\u201d models provided by some crazy ORM/ODM with unknown semantics... \n\nBut, your point still stands: we are creating objects (using new) that aren't really there (...)\n\nYes!, and that's why all discussion about \u201cusing SimpleDelegator\u201d is more or less armchair warfare. Of course, we can debate the merits of different implementations of the behaviour.\n\nWe retrieve the data from ES, then construct our faux models ourselves using a different wrapper than Item.\n\nThat's very cool to hear --  I always envisioned the wrapper feature of Tire as a nice hook point. Also, do notice the Tire::Model::Persistence layer, which is what I have been using for these purposes.\n\n(...) have the resulting Item inherit from the \"real\" class (...)\n\nI would be even more worried about inheritance then delegating to bogus models :)\n\nMight be nice to have a Wiki page about this subject?\n\nAbsolutely -- create one, we can add it here, tweet about it, etc. But after some pressing stuff is done, Tire obviously needs more structured documentation.\n. @tieleman Cool! Please add it to the Wiki homepage then!\n. @tieleman I think you're making it a tad bit complicated in the \"Using a custom wrapper\" part -- normally folks using ActiveRecord/Mongoid/etc can just use :load and be done with it.\nCustom wrapper is really useful if you want to somehow wrap, decorate, etc the JSON results from ES -- as was your original situation with models having MyActiveModelImplementation...\n. Yes, please mention the :load option and that you can use a custom proxy object to wrap it like you did...\n. You are -- #261, #368 and #406 are opened with regard to that...\n. Update -- seems like the unfortunate call to class is still there in Rails 4 (https://github.com/rails/rails/blob/master/actionpack/lib/action_controller/model_naming.rb#L9).\nThinking about how to solve this more elegantly in the new Elasticsearch gem, but I'm worried that we need to \"masquerade\" if we want to support the out-of-the box compatibility of results with Rails' helpers.\nI'll definitely make it opt-in, at least.\n. Good, thought it must be something funky on the system.\n. Hi, sorry for the delay.\ntl;dr: yes, it's probably a useful feature for tire-contrib and EC2 will eventually force us to implement it\nThis is something which has been debated over and over, mainly on IRC with @clintongormley and others. It has even been said Tire \u00abmisses the point of elasticsearch\u00bb because it does not handle high-availability in the client :)\nNow, I have couple of things to say to this. Let me explain step by step.\nFirst, let me say that I think that handling high-availability/failover in client \u201ccompletely misses the point of elasticsearch\u201d :) One of the best things about elasticsearch is that it talks HTTP. Handling availability in HTTP stacks is what we, web developers, do all day.\nPutting some kind of proxy (HAProxy, Varnish, custom Ruby proxy with em-proxy) in front of HTTP-based backend is easy, right? Most proxies can handle external sources for backend URL, can handle backend health, etc., right? That's what proxies are for, and people smarter then me have written those.\nSo, what you should do, generally speaking, is to invest you intellectual energy into building a robust stack, where you put a proxy in front of the elasticsearch cluster, which automatically does round-robin, health checks. Then you can talk to elasticsearch from whatever client, even curl, and still have failover handling.\nObviously, it could be argued that you've just introduced a single point of failure in your stack \u2014 but there's no free lunch.\nSecond, I've written one or two proxies in my life (see eg. https://github.com/igrigorik/em-proxy/blob/master/examples/balancing.rb), and while it's CS101-level programming, there are couple of interesting questions:\n- How do you handle health checks? Do you introduce some background worker? How is the worker being run?\n- How do you pass backend URLs to client? Do you introduce some kind of database?\n- How do you handle dead backends from the client? Do you pass the info somewhere?\nThese are all questions you have to go over before you start writing code.\nThird, let's review what's possible now, as we speak:\nThanks to @dylanahsmith's patch at karmi/tire@94e7b89, it's now possible to rescue failure and perform the search at another node. You're able to wrap the search method in something like MyClass#my_robust_search, and try each of your nodes in succession. You'd most certainly would load such info from something like Redis, and you'd most certainly have some kind of background worker which checks the Cluster Nodes Info API and stores the info.\nIt is obviously an ugly solution, and the library should support you much better here. The most trivial start of this journey would be to store URLs from Tire.configuration { url 'http://es1.example.com', 'http://es1.example.com' } as an Array, and try them in succession in Tire::Search#perform, effectively doing what I just describe in above paragraph directly in the library.\nFourth, all this said, three things make a feature like this worth exploring and implementing:\n1. elasticsearch by itself has no single point of failure, and it could be argued that clients should take advantage of that.\n2. elasticsearch makes it very easy for clients to check the cluster state, and again, clients should take advantage of that.\n3. But, first of all, in an environment such as Amazon EC2, where nodes become suddenly un-available for short periods of time, or right away die on you, you're forced to deal with failure even in modest applications, and rolling a robust proxy implementation may be a pointless exercise for you.\nStill with me? So, here's how I think the feature should be approached and done, and I'll probably want it myself sooner or later.\nEverything should be designed as a tire-contrib extension. There's no reason to pollute the core library with a thing like this, and Tire should be extensible like this anyway. Wherever the code in core would not support an extension like this, it must be changed.\nInitially, when you pass a URL (or URLs) to Tire in the configure block (please load your YAML yourself and pass an Array, thank you! :), Tire performs the initial cluster state check, and retrieves & stores URLs for healthy nodes in a variable.\nThen, when you perform a search, nodes are queried in round-robin strategy, in succession.\nWhen a failure occurs where the node is not available, Tire will a) kick the node out of its set of healthy URLs, b) launch a background process which will retrieve fresh cluster state, and continue with next node in its set. If all nodes in set fail, in succession, it will give up and raise some SorryNobodyTalksToMe exception.\nNotice how, in this implementation, we don't have to perform background health checks \u2014\u00a0we perform them during \u201cnormal\u201d searches. We kick out dead nodes when they fail to respond to search (or other operation).\nNow, the tricky part is of course the inter-process communication between the main code (your application) and the background process. I've done my share of process programming (see eg. https://gist.github.com/486161), but I still trip over from time to time when wearing sandals. I hope it will be doable without storing the nodes info in some external resource (file on disk, Redis, ...), but I'm definitely not sure.\nObviously, to make it work nicely, we must first a) implement the cluster API in the core library, b) change the concept of url in Tire to be an Array.\nTo resume: when done like this, it would be hidden from the regular library user, as you'd just seed Tire with initial URL(s), and the library takes it from there. For developers, it would be transparent what's happening, given we can just log all the events in the main Tire log.\n\nUpdate: New in ES master, external multicast discovery, elasticsearch/elasticsearch#1532.\n. Yes, definitely makes sense, that's the vision I have, and close to what I have described above, thanks for chiming in, @kimchy.\nI'm still unsure about the precise mechanics -- I'd like to be able to leave scheduled background jobs out, but of course, it's something which is at hand. Some Rake task which the user can schedule etc.\nOne problem with the solution I've outlined \u2013 starting poller on node failure \u2013 is that it handles failover well, but cannot easily add new nodes unless one of them fails.\n. @clintongormley Hi, sorry for the delay with the response, Clint! What\u00a0you're saying makes perfect sense, and I was thinking that maybe keeping track of some counter and issuing cluster nodes checks within regular operations could make perfect sense. It would certainly be more robust then the background polling and message passing, at the cost of a possibly very small overhead when checking with ES. Thanks!!\n. No updates yet -- I guess putting a proxy in front of ES if you're interested in high availability makes sense for now.\n. @mkdynamic The best solution would be to use a real proxy, such as HProxy, Nginx, etc. You can use Nginx as a round-robin proxy with keepalive pretty easy, see eg. https://gist.github.com/karmi/0a2b0e0df83813a4045f for config example.\n. I wouldn't describe Nginx-based proxy to have \"obviously potential performance and cost downsides\". But yes, every proxy would be a \"single point of failure\", though I'd hesitate to describe it as a practical problem.\nThere's definitely planned support for robust, extensible multi-node support directly in Tire or its successors.\n. Hi Dylan, thanks! Added simple test and your example to commit message and pushed as karmi/tire@94e7b89.\nNotice the issue karmi/tire#162, which would handle this failover scenario more conveniently -- I'll write some more info there once I'm done shaving fast breeding yaks with Chef's knife.\n. Hello Ryan,\nI've been thinking about this a lot during the course of Tire development, and think this behaviour \u2014 raising exceptions \u2014 is best for the library users to handle. Let me explain why I think so:\nTire will raise the SearchRequestFailed exception whenever any kind of error with the search occurs: the query syntax is invalid (your example), index is missing, the node is down, etc etc. This way, the library user can decide, how she wants to handle the situation. Usually, in Ruby code, we do something like this:\n``` ruby\n    begin\n      search = Tire.search('articles') { query { string 'love & hate' } }\n      # ... do something with results ...\n    rescue Tire::Search::SearchRequestFailed => e\n      puts \"Sorry! There has been and error with the search... The details are:\",\n           e.message\n    end\n# But the script continues here just fine...\n\n```\nNow, in Rails, it's usually something like this:\nruby\n    rescue_from Tire::Search::SearchRequestFailed do |error|\n      # Indicate incorrect query to the user\n      if error.message =~ /SearchParseException/ && params[:q]\n        flash[:error] = \"Sorry, your query syntax is invalid...\"\n      # ... handle other possible situations ...\n      else\n        redirect_to :back\n      end\n    end\nI think this approach has many benefits:\n- When test-driving Tire, everything fails fast and hard for the end user, and he begins to get a feel how to work with elasticsearch, which kind of queries trigger errors, etc. Everything is there, in plain sight.\n- As a library user, you don't have to check for valid?, and don't have to inspect the errors Array \u2013 you just handle the exception the way you like, and display a meaningful message to the application user the way you like.\nBut, maybe I'm wrong \u2014 this is, in any case, the way I think about it.\nAs for the application user triggering an exception by entering an invalid Lucene query: I tend to think I'd rather handle that in one giang case block as suggested above in the rescue_from method. But there may be situations where this could become tedious, and you have to populate your @results anyway so your views don't fail undefined method each for nil etc.\nTo illustrate how hard is to decide stuff like this :), elasticsearch itself suggests the text query as the ideal \u201cend-users facing query\u201d, instead of the powerfull \u201cquery string\u201d query, which supports all the crazy Lucene meta characters...\nTo recap:\n1. I think \u201cfailing fast and hard\u201d is beneficial for the library users, because it's very transparent and literally hard to overlook :)\n2. I'm open to any suggestions how to make the error handling more convenient, if there are some hard edges.\nBest!,\nKarel\n. Suggested trivial implementation in https://github.com/karmi/railscasts-episodes/commit/a2e45741d7e2ed82dffc88b6e2512655106f35a7\n. @ryanb: Ping, any news? Should we close this, or...?\n. Closed in karmi/railscasts-episodes@a2e4574.\n. Hi, thanks!\nI've got two things to say first:\n1/ The main focus of Tire is to make it incredibly easy for Rubyists to start with elasticsearch. It may be evident from the way the Readme is written, from the fact that you can generate a fully working Rails application with one command, and other things.\n2/ The second focus of Tire is to make it convenient for Rubyists to make use of advanced elasticsearch features. It may be evident when you consider how boolean queries take blocks, how Tire does not trample on your models, and other things.\nNow, consider how you write in your patch:\n\nIn order for ElasticSearch to find your objects, you have to specify which fields you want to store there,\ntheir type, analyzer and other things (...)\n\nThat's simply not true. You don't have to specify anything in elasticsearch. It works out of the box, and its author went to great lengths to make it happen.\nNotice also, how you introduce the mapping method before we even started talking about putting some simple data into elasticsearch. I think the Readme has some flow to it, and continually builds more advanced stuff from simple stuff.\nNow, field types, analyzers, boosts? Before we even started talking about search? Why? There's no reason to do that in elasticsearch \u2014 it just works. You download &\u00a0untar some file from teh internets, run shell script, and can work with it.\nNotice also, how you start including pointers to elasticsearch documentation early on. Again, why? Does a buddy Rubyist wanting to try out this new hipster search engine really have to open twenty tabs in her browser to start working with Tire? I don't think so.\nThe problem with documentation, generally, is that people either complain it's too thin or too thick. I think both Tire documents have some balance, and if a new user gives them an hour or so, she can start doing pretty powerful stuff with elasticsearch right away.\nObviously, after the initial crush is over, you will have to look at elasticsearch documentation, no matter what. You'll discover that there are many more different APIs, many analyzers, that you actually don't understand anything, etc. But that's good \u2014\u00a0by that point you know if you like elasticsearch or not. My bet is you do.\nWhat to do about Tire's documentation, then? I'm all ears for suggestions how to make both main documents better, but, generally, they work for me.\nYou are very right when you say:\n\ncreate a reference doc with some examples for people who were not yet familiar with full-text-search,\ngive them some hints (like, that it's better to use \"keyword\" analyzer for things like tags and \"snowball\"\nfor titles and article texts etc), maybe some more elaborate examples (...)\n\nWe've been already debating it with @michaelklishin couple of weeks ago, and hell yes, I'm enthusiastic about this. A wiki page which says, \u201cOK, guys, here's the story how you should index articles with tags\u201d and a page which says \u201chere's the story how you should index and setup everything for an address book\u201d.\nThat would be awesome, because then you could demonstrate stuff like \u201cuse keywords analyzer for tags\u201d and \u201cmake sure you use multifield for authors, so you can fulltext search them and also do facets on them\u201d, or stuff like \u201chere's how you should store phone numbers\u201d and \u201chere's how you should store people's resumes in PDF in the index\u201d.\nWe've already agreed with Michael that the best way is to just create a Wiki page(s) for stuff like this, and work on the content. Once we have the content, it's trivial to make decisions how it should be structured, how it should be splitted, etc. Whether it stays on the Wiki or whether we build some more extensive website is another decision \u2014 but we have to have excellent content first.\nAnd of course, this is also about @elasticsearch's documentation itself \u2014 we are constantly revising that, and everybody knows it should get better, both in content and structure. I'm pretty confident sooner then later it will get substantially better.\nSome of the stuff you have written really \u201cpartially duplicates\u201d elasticsearch's own documentation, and that's not good. We should contribute upstream, to the main project, not scribble on the edges of elasticsearch's documentation.\n. Hey Alex,\n\nEven though ES works out of the box, knowing how to configure things that most everyone requires in his app sounds good. (...)\n\nyes, but I can't see how we can do all that in the Readme \u2014\u00a0that's why I tried to give you feedback.\nI think the most important point here is: if you and Michael would be willing to write the Wiki pages, as we have debated, that\u00a0would help many users, absolutely. Nothings beats a well written, well prepared example...\n. Perfect, just put them in the Wiki, that would be awesome.\n. Michael, I never suggested you create one Wiki page\u00a0for the examples which we have discussed. Obviously, each example could get its own Wiki page. Once we have the content, we can debate how to structure and present it better.\n(Not to start any clashes :), but, the only documentation for @sinatra was initially, and for a long time, in the Readme. The general feedback seemed OK. I always loved it. Around the 1.0 release @rtomayko created a website and wrote the awesome content such as http://www.sinatrarb.com/extensions.html. The current form of the Readme is much less awesome then I remember from the old days -- but still, it's on one page.)\n. Yes, agreed. And yes, something like http://rubyamqp.info/articles/patterns_and_use_cases\u00a0is precisely what I think would benefit Tire tremendously.\nAs for the Readme, sure, I like the structure it has, but obviously it contains too much info -- in the ideal world, it would have links \u201cMore about mapping\u201d, \u201cMore about ActiveRecord\u201d, \u00e0 la http://gembundler.com.\n. @ifesdjeen: No problem :) Were those are the curl-formatted debug lines you found?\n. By the way, there is\u00a0a Curb-based client. See the benchmarks: https://gist.github.com/1204159.\nRolling custom client is very simple with Tire. But, yes, I'd like to convert the HTTP stuff to Faraday some day in the future. It's a relatively trivial refactoring.\nSo far, it's been on the \u201cnice to have\u201d list for me, since there are many more important features  -- maybe I'm wrong.\n. Hi, it's absolutely OK to submit questions as issues.\nTheoretically, yes -- you'd be able to do that, if you can launch java apps there. But on a shared hosting, you'd probably need to put some sort of proxy in front of ES with HTTP Auth, etc. It really depends on the host.\n. Ping, any news on this? Closing?\n. Hi Nathan, could you add a simple integration test as well? Also, could you post here the troubles with the test suite? It should really be working with no problems...\n. @woodhull: Could you add some simple integration test, please? If you have troubles running the tests, bug me here or on IRC!\n. Nathan, I'm terribly sorry it took me song long to process the patch. It's now included with some cleanups in a7a62d6f26a307ea61f5076864637c2a2fc32165. Thanks!\n. _UPDATE_\nSEE https://github.com/karmi/tire/issues/417#issuecomment-7331381\n\nHi, they definitely could, I'm not sure about the proper mechanics, though, at the moment -- could you please ask on the ES mailing list or IRC channel? \n. Yup, the example @al gives is straight from the ES docs, http://www.elasticsearch.org/guide/reference/api/search/sort.html.\n@amnesia7: I believe the sorting is done on the final, small result set returned from ES, so I don't see a reason for much overhead there. Best to ask on #elasticsearch's IRC, though.\nSo, issue sorted, closing, or... ?\n. Tire does not mess with location/geo specifically, I cannot rule out a bug. I think maybe you could use a Hash instead of Array for the location definition (as per http://www.elasticsearch.org/guide/reference/query-dsl/geo-distance-filter.html).\n. See https://github.com/karmi/tire/issues/417#issuecomment-7331381 in karmi/tire#417.\n. @WvanLelyveld Can you post the to_curl output of your query, so it's clear what is being sent to ES?\n. Dylan, thanks, very nice patch -- could you please add a small test to https://github.com/karmi/tire/blob/master/test/unit/http_client_test.rb that it in fact does re-raise the exception?\n. Thanks Dylan, merged, pushed. I had some strange StackLevelTooDeep errors, so I simplified the test suite and kinda cleaned it up.\nActually, a question: have you tried to use Curb at Shopify? It should be noticeably faster under concurrent load.\n. Hi, you can do fuzzy searches with the normal Lucene query syntax: query { string \"sour~\" }.\nYou can also pass Hash/JSON to Tire, search the Readme for \"fuzzy\". (And, preferably, do read it! :)\nOf course, it could be added as a dedicated method to Tire::Search::Query, when includes unit tests etc.\n. Hey, I'm not sure I understand entirely the described issue, but here are my thoughts:\n1/ In unit tests, I recommend stubbing Tire methods, or stubbing HTTP responses with Webmock/Fakeweb.\n2/ In integration tests, I recommend letting everything bubble thru the stack.\nIf I understand correctly, you need to update the ES index (Tire method update_index) in the after_commit hook, not in the after_save hook, which is the default. So, calling self.tire.update_index in the after_commit block should be enough?\nLemme know if this advice is in the right direction, and please paste some simplified code so we can have a better understanding of what's going on...\n. Hi, this is related to the mechanics of how transactions work in uni tests, then? Is this related to the config.use_transactional_fixtures = true setting?\n. @joshcutler: Ping, any news?\n. You have to pass the raw JSON or Ruby Hash to Tire in these cases.\n. @devilcoders: Ping, any news?\n. Closing due to \"too far for anybody to remember what was going on before New Year's Eve\" factor :)\n. For posterity, if you want to debug the custom script, you can access the Elasticsearch log from the script with something like:\njava\norg.elasticsearch.common.logging.Loggers.getLogger(\"my_logger\").info(\"hello there!\")\nSee the logging_from_script.sh example by @imotov.\n. Hi Fabio, thanks for the patch, I'll have a look at it in the light of proposed changes at karmi/tire#162.\n. @karmi @vhyza Bump!\n. Closing in favour of a more complete implementation.\n. Hi, thanks, cleaned up the commit message, added a simple unit test and pushed in master.\n. Hmm, no, I don't think there's any \"default\" in Tire like that-. What exactly are you trying to do?\n. @haihappen: Does it make any sense to de-compose your models for searching, like this? They end up as one JSON doc anyway, right? An interesting twist would be to use different _type for each of these links within ES.\nNow, this probably won't work anyway -- the index is created with proper mapping on class load, only when the index does not exist yet. Since one class will be eventually loaded first, it would \"win\" this race.\nIn near future, this behaviour will probably change, and Tire will update the mapping, and also complain when the mapping in ES and mapping declared in Tire won't match.\n. @rtlong I'd probably extracted the index creation and mapping definition logic out of the models entirely. I'd put it in /lib, or maybe keep them in the \u201cbase\u201d model for STI. I believe there's some page on the Wiki which has the exact for such extraction.. Ping me here or on IRC (#elasticsearch @ Freenode) if you get stuck.\n. @fbjork Is it possible to paste/gist simplified model definitions? Also, are the mappings correctly saved in ES (see discussion above)?\n. klass.model_name.underscore.gsub(\"/\",\"_\") would break loading your models from the database. The whole toolchain (Tire, Rails, ...) expects the class names being serialized in a certain way.\n. @gregory Cool!\nI don't remember the exact issue context, but also note that you should be able to just use index_name to override the index name if you want to control it explicitely. In new Tire versions, it's possible to store multiple models in one index.\n. Yes, embedded docs / associated models / etc are always tricky to model right. Note that it's possible to retrieve just specific fields in the response.\nWhen working with a structure like article :has_many comments, it's often necessary to use the nested mapping type for comments.\n. @davesouth What specific advice can I give here? You can create an index with desired settings/mappings beforehand like:\nruby\nMY_MAPPINGS = {type1: ..., type2: ...}\nTire.index('myindex').create mappings: MY_MAPPINGS\n. @davesouth Fortunately, this sounds easy .) First, a bit of advice, when somethings aloof like this, first thing to do is stick Tire.configure { logger STDERR, level: 'debug' } somewhere in the script, so you can see Elasticsearch responses.\nThen, you've mixed phone_analyzer and phone_ngram for digits -- use phone_analyzer there.\n. @davesouth Cool. Just remember you can stick the whole definition in a constant, or a module method, or make it dynamic and assemble via a module method, etc.\n. @dennybritz: Hmm, I don't think you have to actually fully rewrite the query -- you can convert it to hash, and then just update the portion of the hash. That is, until there's regular support for prefix queries.\n. The error says Tire is not able connect to Elasticsearch. I suppose you run everything with default configurations? What do you see when you issue curl http://localhost:9200 on said machine?\n. This is ES-related, you have to make sure you have a proper analyzer set for the field in question. \nAFAIK, the \"standard\" analyzer should have detected e-mails and URLs, but it's not, check that out for yourself:\ncurl http://localhost:9200/<YOURINDEX>/_analyze?text=user@example.com&analyzer=standard\nYou have to define a custom analyzer which uses the UAX Email URL tokenizer.\n. You have to use the other DSL syntax, with block arguments: The README and docs have a very detailed information about that.\nOtherwise, yes, filtering the records like you are trying to do is on the right track.\n. Greg, thanks! An awesome commit and a great improvement! Merged in as karmi/tire@9196141\n(Also see karmi/tire@0b65457)\n. Hi, thanks!, was thinking about adding something like that and you did it nicely! Merged, added some tests &\u00a0docs, pushed!\n. Yeah, you can't just call User.index.create and expect it to create the index with proper mappings/settings -- you're effectively just calling the bare create method of the bare index instance.\nIt would be nice to support the way you seem to be going here, though!\nThe documentation says it quite clearly: the index is created when the class loads, only when it does not exist. If you want tighter control, just wrap the correct calls in your own methods, the Wiki has some info on that.\n. @karmi @vhyza Bump!\n. So, to recap:\n- Article.create_elasticsearch_index is the proper way to create an index for the model, with proper mapping\n- There is a usable Rake task to help you with import\n- Always, always check your mapping, until Tire does a better job here.\n. Hi, thanks for having a look into this... Alas, there's already a pull request (and quite a long discussion) about multi-model searches here: https://github.com/karmi/tire/pull/131 -- could you have a look, please? (I still think the results method must be first refactored, to allow more acrobatics like these...)\nI'll definitely have a look into the namespacing issue!\n. Hi, thanks for the naming patch -- I've merged, added a test for the behaviour and pushed.\n(As for the other thing, feel free to submit another ticket, if you like. I don't have anything new to add to the discussion at the moment.)\n. Greg, thanks, the delay with merging is embarrasing, sorry. Pushed.\n(By the way, I think a better way would be an API like Tire.search 'an_index', version: true do ... end, but it does not matter at the moment. We have to sort out the issue with search params anyway, and transfer the version option there once it's done...)\n. Yes, why not do this, actually :) Merged &\u00a0pushed, thanks!\n. Thanks, merged &\u00a0pushed!\n. Hello Dylan, thanks, sorry for such embarrasing delay.\nI hate to be the asshole who asks for more tests all the time, but I think an integration test would be warranted?\nAlso what about mapping conflicts? What about the ignore_conflicts option?\n. @dylanahsmith Let's close this until we have a more complete iplementation with handling or ignoring conflicts, displaying diffs, etc etc?\n. Thanks!\n. You have to use the \"blocks with arguments\" syntax, a la Article.search { |search| search.filter :.... }, the Readme and http://karmi.github.com/tire/ has more info on that...\n. @elfassy Don't know what you're missing, this is valid.\n. @erickt What do you think about the API? I think at least three people are now trying to add various request params to Tire, without much communication. It would be awesome if we could sort it out here in issues or at the IRC channel...\n. @sandrew I'd be very frightened to provide a task like that -- best leave those couple of minutes spent writing the task for meditation on possible consequences? :) Besides, it would be lots of introspection involved to discover which indices are defined \"within the application\"? Closing for now, is that OK?\n. OK, thanks for kicking me in the butt. Yeah, a task like:\n$ bundle exec rake environment tire:index:drop INDEX=articles\nor:\n$ bundle exec rake environment tire:index:drop INDICES=indexA,indexB,indexC\ndoes make a perfect sense. No introspection, no magic, just a convenient way how to delete couple of indices.\n. Yes, @yeggeps is right, you have to use the \"blocks with arguments\" DSL variant, for passing the context around. Closing...\n(Please close the issue when your problem is solved.)\n. You forgot to refresh the special _percolator index, as the docs say in http://karmi.github.com/tire/#section-142:\nTire.index('_percolator').refresh\nBest,\nKarel\n. Yes, definitely.\n. Sorry, I don't understand the issue properly. Could you add a bit of code so it's more clear?\n. Hmm, that's interesting approach. However, AFAIK it can't be easily done this way, extracting all\u00a0the Tire-related functionality into a module. Tire depends on having the class where you mix the module into accessible as the klass object.\nOne way how to try to solve this would be to use some kind of included hook, or def self.included(base) logic in your\u00a0Person::Index, which would then send it to Tire...\n. @danielschlegel Saying \"thanks\" is absolutely enough :D\n. Hi Domizio, thanks for keeping looking into this. There are two problems, however, with the patch:\n1. It has no tests attached, which makes it very hard to include -- I'd have to write them myself.\n2. Given there are no tests, I'm not entirely sure I understand how it works -- you change the / to __ when you pass data in, but where do you do the inverse when getting them out?\nAlso, I think all of this is doable without extending the String class...\n. Hi,\nyes, without tests, it's really hard to see what's the intended purpose and cases.\nI wouldn't extend String, and would like some symmetry in the code, so the encoding document_type_classify has its decoding counterpart and it's not added into index_name method.\n. Maybe something like this would make sense from the Tire's DSL point of view:\nruby\nTire.search 'index_1' => { boost: 1.5 }, 'index_2' => { boost: 2 }\n. @hau Sorry for long delay -- I think this is better suited for ES mailing list or IRC channel, closing here.\n(Article.search(\"resistible\") won't match \"irresistible\" -- Article.search(\"*resistible\") would. For partial matches, Ngrams are the way to go.)\n. Hi, I think you may want to use \"multi_field\", have a look at https://gist.github.com/1160430 and http://dev.af83.com/2012/01/19/autocomplete-with-tire.html and https://github.com/karmi/railscasts-episodes/commit/03c45c365d1cc23c27ec67280b0fe315164ab116.\n. Hi, is it possible to create a gist recreation of the issue, so I can try it?\nSome thoughts:\n- Do note that filters do not affect facets, ie. they only restrict the results\n- Inspect the HTTP requests Tire sends to ES with Tire.configure { logger STDERR } so you can make sure the query is actually sent etc\n. You can either pass the filter to the facet, depends on what you're after. Closing for now, could you reopen if you still have the problem?\n. Hey, sounds like an interesting question for @kimchy :), but let's close this for now and let's open a new issue if the problem is stil stinging!\n. Hi Woody, I'm terribly sorry for the silence on this, I'm glad you had a look into that... I think there is some longish discussion on aliases in some issue, I haven't had time to think about proper DSL syntax for that... I'll try to get to this as soon as possible... Ping me on IRC if you'd like to chat about it.\n. @karmi @vhyza Bump!\n. @woahdae Great! Best to talk about at the #elasticsearch IRC when the time comes so we can figure out what #92 was all about? :)\n. @woahdae Sorry about this, merged now.\n. @digitalplaywright Yeah, I'll have a look into it tommorrow.\n@woahdae Sincere apologies!, I like the patch very much, sorry it got stalled like this...\n. @digitalplaywright The issue you're having is due to https://github.com/karmi/tire/blob/master/lib/tire/model/search.rb#L169. That's easy to fix :)\n. @woahdae Hi, thanks very much for patch, finally merged! Sorry it took so long...\n. @leehambley I'm terribly sorry for this embarrasing delay :(\nI think it's a valid approach and the code is very fine. Nevertheless, could you please get in touch with @ralph, and check with him how this plays with the recently merged support for filtered queries? See https://github.com/karmi/tire/blob/master/test/integration/filtered_queries_test.rb#L43-60 for the API.\nIf I'm not completely mistaken, it does allow the same sort of thing, with a little bit different API. I tend to overzealously guard Tire against more lines of code being introduced -- I've said numerous times that the codebase feels too heavy to me already and would like to make it slimmer... \nDo not read this as opposition to the patch and the proposed API -- I just think that if we can achieve the same thing with already existing features, we should maybe be a bit asketic here...\n. @ralph , @leehambley That sounds like a great plan, thanks! :) Enjoy the vacation, Ralph!\n. Hey, sorry for not getting to this earlier, was kinda hoping one of several hundreds of Tire users will step in :) -- the question is a nice one, will try to get you some answer today.\n. Hi, I think what you should do is to just store the tag list as tags property, and search it with a terms query.\nAs for autosuggestion, you can also keep an index of tags, and do prefix queries on them or analyze them with ngrams.\nI think you should post your question to StackOverflow where there's a bigger chance you get some reply...\n. > Any thoughts?\nYes, I don't think alias_method :to_indexed_json, :to_json will work for Mongoid.\nAlso, I don't know what's the issue you're reporting. bson (1.5.2) lib/bson/types/object_id.rb:126:infrom_string'?illegal ObjectId format: Lorem`?\n. Awesome.\n. @redCashion Why do you consistently cross-post in Github issues and Stackoverflow?\nSo, cross-posting my SO comment:\n\nI don't understand the question: from which data store would you like to import Article when it's persisted in ES? \n\nand closing here...\n. Yes -- you have to bind queries together eg. with a boolean query, http://karmi.github.com/tire/#section-Boolean_Queries\n. @zirni: Closing as solved, is that OK?\n. Hi Dylan, is it possible to include a test with the patch?\n. Hi Andrew, thanks for taking your time to have a look into this. Sadly, this issue has been debated over and over -- please search the issues --, and there are now several patches trying to solve several problems related to this.\nThe thing is, I feel the codebase right now is too fragile to out it under more load. I like the way you're going in your patch, though, and will definitely look at it when doing some refactorings in February (hopefully).\n. @GearHead90 I like how you approach the problem, Andrew! I generally try to avoid ActiveSupport like a plague, but the group_by here works like a charm. I think this is a viable solution. I'll test it as the solution to \"multi model searches\" and will get back. Please ping me here or on IRC if I'll fail to do that in couple of weeks.\n. @sandrew Hi, I like how you approach the problem and will try to incorporate it to the codebase the moment I'll start working on it. If nothing blocks the implementation, I hope we can have it in this week.\n. Only adding documents is supported at the moment.\n. @JamesHarrison Updating mapping can require full reindex, when you do an incompatible change.\nAnother option which wouldn't require accessing your database (and thus would be very, very fast) is to use Index#reindex, see karmi/tire@88b5bc7 . It's optimal when you use index aliases (see karmi/tire@67aa3a4) in combination with that, since then you can prevent downtime.\n. @afeng Bulk create/delete is now supported and released as 0.5.1. Changes: https://github.com/karmi/tire/compare/4e85109...1c0d043\n. Hey, this hints at your model being loaded at every call -- it checks the index existence. Why this should be happening? Is it related to the Obsever being used? Rails reloads classes in development mode, is it related to that?\n. Hmm, then I see it as a problem with the Observer pattern either in Ruby or Rails... Could you try to isolate the problem, and try observing it with the plain Ruby Observable? If it works with it, then Rails is the culprit.\n. Closing for now, please open a new issue or reopen if the problem persists!\n. Don't know what you mean by that, sorry, could you be more specific?\n. Several concepts you can use:\n- An index per user (has a performance penalty),\n- a filtered alias per user (http://www.elasticsearch.org/guide/reference/api/admin-indices-aliases.html)\n- filter by eg. user_id in the search block,\n- etc.\nSo eg. something like this for the last option:\nruby\nTodo.search do |search|\n  search.query { |q| q.string params[:q] }\n  search.filter :term, current_user.id\nend\n. Hi, thanks!\n\nattempting to add a few more query types\n\nyeah, feel fee to do so.\n\nand maybe 1.9 support for tests\n\nnot sure what you mean by that?\n\nhere's my first one to support the prefix query type.\n\nI'd like to have query types covered by integration tests as well, is it possible to add some?\nBest,\nKarel\n. Ah, yes, best to delete the rcov then.\nSee eg. https://github.com/karmi/tire/blob/master/test/integration/range_queries_test.rb etc....\n. @karmi @vhyza Bump!\n. @tobowers Sorry for the ridiculous delay...\n. Dylan, this would completely change the purpose/semantics of swallowing exceptions from HTTP client etc, wouldn't it?\nI think the proper way how to handle this would be to rescue SystemExit;\u00a0raise;...\n. Yes, seems like you're right, eg. RestClient https://github.com/archiloque/rest-client/blob/master/lib/restclient/exceptions.rb#L82.\nCould you try it with RestClient, Curb? Eg. shutdown ES and try the bulk?\nAlso, the patch will need tests...\n. Dylan, I'm very sorry for the delay. The patch makes very much sense. Merged &\u00a0pushed now, thanks!\n. Hey, I think the proper way how to handle this would to allow an options hash to the bulk_store method, which would allow something like myindex.bulk_store articles, raise: true...\n. Yeah, the options would have to be passed down the chain to the import method in https://github.com/karmi/tire/blob/master/lib/tire/index.rb#L112. Definitely have a shot at that -- just make sure it's covered by unit tests, please!\n. @karmi @vhyza Bump!\n. @nfo Thanks, merged!\n. Statistical facet was implemented in 8b06760.\n. > Any word on this?\nYeah :)\n- Why submit the issue as a new one and not ping me on the original one?\n- Care to include some explanation what this does and why you need it? I'm not sure what exact problem are you solving by reading the code.\n- I can see the timeout is propagated to the RestClient only? What about other clients?\n- What about wrapping the calls in Ruby's timeout? Is there something to gain/lose?\n- What about the elasticsearch's own timeout parameter? Is it of any use here?\n. I think the more appropriate solution to this cluster of problems would be to do something I've planned for a long time -- use Faraday as the HTTP client abstraction and let it handle all of this?\n. @bobbytables I still don't have any strong opinion on this, other then intuitively I don't think this is a proper &\u00a0future-proof approach.... I need to mull it over a bit more, I guess, sorry.\n. Allright, really sorry about the delay... The patch only adds the behaviour to RestClient, where most people (should) use Curb in production, the semantics are weird because the timeout is global (I know, my fault by designing Configuration this way), ... I'm sorry, leaning towards passing on this, in favour or proper implementation...\n. @jedi4ever Hard to explain. Aesthetics and code as liability.\nTo elaborate, I don't think this is a proper solution to the problem, and after merging this I'm the one taking care of the code and possibly issues arising from it... I know you're maintaining a larger code base -- this is just my way of keeping myself sane, I'm afraid...\n. Closing in favour of a more complete implementation.\n. Ah, yes, thanks!\n. Hi, could you be more specific as to what kind of \"object\" you have? Could you post some example code?\n. OK -- when you have a plain old Ruby class as your model, you have to add some attr_accessor or something like that, and you should define a to_indexed_json method to translate your data for ES.\nI still don't understand 100% what you're really after here.\n. Are you trying to use Tire::Model::Persistence? Or has your model some other ActiveModel implementation?\n. @shoshi-baron I'm sorry but I don't understand what you're trying to do at all :)\n\nI think that ' Tire::Results::Item' is not the same as my class Product instance\n\nYou are right. You have to use the :load option to load \"real model instances\", eg. ActiveRecord ones.\n. Closing for now, please open new issue if the problem persists...\n. @joshcutler I understand the requirement, however, I'm not sure it makes much sense... I think, mostly you need either the \"optimized\" JSON doc from ES, or the \"real model instance\" from the database... Of course, it would be trivially possible to leave the items in the result set, when you :load the models -- is it something you depend on in your app?\nWhat does anybody else think? \n. > in my ES index I store some composite values that are semi-expensive to compute\n@joshcutler Yeah, that makes perfect sense, and is also the reason why I recommend to go without :load if possible at all. I think this could be supported easily, since the data for Item instances are of course loaded from ES. Ping me here or at IRC in a week or two and I'll have a look into it, if nobody else picks it up...\n. @joshcutler Seems like Sunspot has a support for that. I still think it's kinda weird in ES, but would be very trivial to implement. Is it something you still need?\n. @joshcutler Perfect, thanks! So let's close it for now and wait if the need for something like that arises again...\n. Hi, I think that's an easy feature to add... I'm planning processing pull requests and adding trivial features like this during February, ping me here or on IRC if I forget...\n. @nsa310 Should be closed in 2bf5585.\n. @stnguyen Sorry for the long delay, Stefan, merged and pushed, thanks!\n. > i can't access instancevariables in the query blocks (that sucks)\n@banditj I don't think that's true at all. This code:\n```\n@query = 'title:TEST'\ns = Tire.search 'articles' do |search|\n  search.query do |query|\n    query.string @query\n  end\nend\n```\nworks just fine. A test proving anything otherwise is very welcome!\n. @banditj Notice you're not using the \"blocks with arguments\" syntax in your code. The Readme and docs have good explanation how it works and why you need it.\n. @naamakat No problem :)\n. Hi,\nthe behaviour you're describing is weird -- Tire goes to great lengths to not redefine existing methods on your models. Of course, you need to include it as last. Please check the Readme for detailed information on this.\nIf the problem persist, please post an example of your model code (or a failing test)!\n. @MacFrancis Closing this for the moment, please submit another issue demonstrating the problem -- Tire should not by any means trample on any existing methods. Again, make sure you include everything in proper order.\n. Hi,\n\nI'm somewhat new to tire and I can't seem to get the :as option working\nfor my AR model.\n\nits not included in the released version yet -- you have to use the latest\nGit version, eg. with Bundler.\nKarel\n. Hi Nick,\nthat's quite easy to achieve. Just put the the configuration:\nruby\nTire.configure do\n  url ENV['ELASTICSEARCH_URL']\nend\nin a Rails application initializer or some other appropriate location. Does it work for you?\n. Hey, usually, I'd vote for putting stuff like this into tire-contrib, but yes, ENV['ELASTICSEARCH_URL'] could be considered as much default as http://localhost:9200...\n. So ENV['BONSAI_INDEX_URL'] contains the full, composed URL to the index? Will probably work when you leave out the index name in Tire.search, not sure about other parts of the API.\n. @matthewrudy Yeah, this sucks right now. We're talking with @nz about making it more convenient -- allowing to use one index per app and put models in different indices. I'd like to do it without breaking existing apps.\n. Hi, yes, something like that seems to me to be proper approach. The thing is to:\n- Make sure models use _type to separate themselves, does not matter if in one index or multiple indices,\n- multiple models can live happily within one index,\n- you can set the index name globally (and don't have to set index_name in all your models one by one)\nOnly the third thing should be left to implement in current codebase,  and  yes, Tire.configure { index \"something } as @matthewrudy suggest looks nice.\nBTW, I like how in redis-rb you can do:\nruby\nRedis.connect url: redis://redis.example.com:6379/1\n. @karmi @vhyza Bump!\n. @thoughtpunch It was closed by karmi/tire@24b3931\n. @cwadding Is the problem still on current master?\n. Yeah, I guess it's on the same level as the field method, https://github.com/karmi/tire/blob/master/lib/tire/search.rb#L75-78\n. Hi, I think this is too special to pollute the main DSL, I'd suggest using Hash queries or doing a contrib for that.\n. Correct. Though you can build your query with DSL, convert it to Hash with to_hash, update it how you wish and then fire it against ES.\n. @yeggeps If you manage to solve the issue, please post solution here and close it, thanks.\n. Hi, custom_score was implemented in 2eee39d, more docs in a2c7406 and 1b9b157.\nI think the boosting query is easily achieved by increasing boost factor for queries, so closing for now...\n. @zx0 Sorry, don't understand.\n. Hey, I went with a slightly lighter but more complete implementation: https://github.com/karmi/tire/compare/4e85109...1c0d043\nCould you test it out?\n. Hi, I'm asking the same thing as @niuage -- why exactly have a different query when everybody can do string \"title:f*\"?\n(Of course, we know it's a different ES query, but then ES have lots of different queries and I'm not sure we should support them all :)\nThat aside, I think this would make a terrific Tire extension in tire-contrib! I see tire-contrib as a lively place, where we can stash lots of stuff, people can poke around, all without putting a burden on the core library.\nIn this case, the user would probably do something like:\n``` ruby\nrequire 'tire'\nrequire 'tire/queries/wildcard'\ns = Tire.search 'foobars' do\n  query { wildcard 'foo?ar' }\nend\n```\nIt means it would be implemented in file tire-contrib/lib/tire/queries/wildcard.rb. What do you think?\nI'll close the pull request here, so we can continue on tire-contrib, is that OK?\n. Hi, it's an interesting idea!\nSo, let's review the ActiveRecord feature here: http://api.rubyonrails.org/classes/ActiveRecord/Scoping/Default/ClassMethods.html#method-i-default_scope. I can see how that could make sense with Tire.\nFirst, I am 100% sure this would make a terrific tire-contrib contribution. Maybe I'm not pushing tire-contrib enough, maybe people struggle with how to extend Tire with the contrib, but that's a place where features like this should be proposed -- and quite possibly implemented.\nSecond, if I understand your code properly:\nruby\ns.filter(:term, klass.default_filter)  if klass.default_filter && !options[:unscoped]\nyou just add a term filter to the search block. This feels to me very limited compared to the rich semantics of ActiveRecord's default_scope. I think we should strive for something similar, along the lines of:\n``` ruby\nclass ArticleWithDefaultFilter < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nmapping do\n    indexes :status,  type: 'string', index: 'not_analyzed'\n    indexes :user_id, type: 'string', index: 'not_analyzed'\n  end\ndefault_search_filter do |f|\n    f.term  :status, 'active'\n    f.term  :user_id, current_user.id\n  end\nend\n```\nExposing the full Tire's DSL in the \"default scope filter\" like this would make more sense?\nPlease feel free to submit it as a tire-contrib pull request, I'll close this one. It would be great if anybody would be willing to help @siliconsenthil ?\n. @matsko Probably with a must_not boolean filter. Could you please try it out and reopen the issue if the problem still stands?\n. The state of things in Tire regarding this is messy right now. Will leave it open as a reminder.\n. Hi Peter, thanks!, great contribution, merged &\u00a0pushed!\n. Hi, by default, Tire hooks into the ActiveRecord#after_save, so it does not matter where everything runs... It's hard to know what's going on without some gist or error trace...\n. @callenrosario Great, if the issue is fixed, could you please close the issue?\n. Hi, yeah, seems like a problem with filtered query implementation. Notice than when using regular filters, they are all chained with and, by default:\n``` ruby\n$LOAD_PATH.unshift File.expand_path('../../lib', FILE)\nrequire 'tire'\nTire.index 'multiple_filters_as_and_filters' do\n  delete\n  create\n  store :title => 'One',   :tags => ['ruby'],           :published => false\n  store :title => 'Two',   :tags => ['ruby', 'python'], :published => true\n  store :title => 'Three', :tags => ['java'],           :published => true\n  store :title => 'Four',  :tags => ['ruby', 'php'],    :published => false\n  refresh\nend\nTire.configure { logger STDOUT, :level => 'debug' }\ns = Tire.search 'multiple_filters_as_and_filters' do\n  query { all }\n  filter :term, :published => true\n  filter :term, :tags => 'ruby'\nend\np s.results\n```\nI didn't write the FilteredQuery implementation -- @ralph, could you please take a look what's going on in there? [937db3f]\n. Thanks @ralph, merged and pushed.\nOne note: could be fun, sometimes in the future, if the filtered method (and the corresponding FilteredQuery class) took some options, where the user could specify if she wants and or or chain?\n. Hi Nick, as with #258, this is all hacking what should be done more systematically, exposing all the useful parameters for Search API. In the end, all of this is just a simple hash of options, encoded as URL params.\nI'm reluctant to proceed with that in a piecemeal fashion... \n. Yes, all the options are scattered around, see eg. model/search.rb which is crazy and hell from maintenance standpoint.\nFor search, I think we should somehow work with possible options directly in lib/tire/search.rb! I must to have a look into that...\nMaybe we can just serialize options send to Index#store and pass them to ES? Once I'm done with all the pull requests and stuff, I'll try to have a look into that as well... But feel free to submit a pull request for that, Nick!\n. @yeggeps Making sure Index#store works with the passed in options, calling to_param on them as eg. https://github.com/karmi/tire/blob/master/lib/tire/search.rb#L119, not breaking anything (= not sending bogus options down to ES), proper unit tests for the feature and ideally an integration test demonstrating the purpose (not the bare functionality) of the feature...\n. @yeggeps Not yet. I would definitely try to solve all the options related issues across the whole codebase as soon as possible.\n. @nickhoffman Nick, sorry about taking so long to process this... In the end, I've cleaned up the params handling in Index#store opening it up for support for other URL parameters, such as routing, ttl, etc.\n. @mattiassvedhem No idea :), please use the Git URL in the Gemfile for the time being...\n. Hey, what's the fix fixing? Is e63e4db relevant here?\n. Great!\n. Hi Dylan, I don't know, this seems to me as another case of trying to hack some ES options into Tire methods. I've said it previously, we need to expose all the options, namely search_type etc somehow. I'm reluctant to do it in piecemeal fashion.\n. Hi Josh, your settings and mapping seem to be correct, check http://pastie.org/3521311 for working example.\nIf you're changing an existing property (and I'd bet you are), you have to reindex the data, ie. destroy and import again. Do check the mapping when you create the new index, either with curl localhost:9200/YOURINDEX/_mapping, or with Tire (p Tire::Index.new('YOURINDEX').mapping).\n. Yes, due to the way the search engine analyzes your docs, the whole index must be recreated with new settings (ie. \"tokenize the keywords property of this type of document in this way\"), and the data must be imported again, so it is properly chopped & processed by the newly set analyzer...\nOf course it's painful to do it, but there's no way around it, and you have to do it from time to time, even after things settle down a bit when you discover how you need to analyze your data...\nYou can use index aliases in ES to recreate your index as myindex_A, while keeping the myindex in production. Once the import is finished, you can delete the myindex index,create an alias myindex pointing to myindex_A, without much downtime.\nCheck the discussion here: https://github.com/karmi/tire/issues/62#issuecomment-1625299, it is a similar issue... Ping back here, or preferably at the #elasticsearch IRC channel if you need more info...\n. Hey, what's the reasoning for this? Something like Sunspot's each_with_hit, where you'd like both the ES JSON and the model from database? There has been some other discussion on that I believe.\n. Hey, visitor from the future :) I believe this is duplicate of karmi/tire#364 and karmi/tire#368 -- please have a look there. I think we need to augment the models (via the include hook) so they can automatically have search results appended to them etc.\nWill close this in favour of more complete implementation?\n. Closing in favour of a more complete implementation.\n. All right, thanks! But, why not just:\nhighlight :tags, :title, :options => { :tag => '<strong>' }\n. Hi, depends on how exactly you define how records are stored in the index -- could you post some snippet of mapping/to_indexed_json, etc? (Normally, it should happen behind the scenes. Facets are a way to compute some aggregated values.)\n. Nothing wrong with your code :)\nHmm, you seem to be properly indexing the comments_count method in the to_indexed_json method... This is really weird -- as soon as you add a comment, and then save the model instance, it should be propagated to ES.\nOne idea: maybe you've changed the mapping along the way, and there's a mismatch between the mapping and the data. AFAIK Tire does not handle this situation well, doesn't raise, etc. Could you try it with a fresh, testing index?\n. Yes, I'm afraid the @comment.save won't bubble up to the @commentable instance. Firing the @commentable.tire.update_index method should be enough, no need to saving the whole thing.\n. Yes, it's a known bug, see issues. There are some patches trying to solve it floating around, haven't had time to review the issue and solve it, yet... (The problem is due to incorrect class name serialization.)\n. @sarmiena @demersus Should be closed in d5c08fb, can you check, please?\n. Closing due to long ping latency :)\n. Yes, the records don't exist in your database, either reindex or save them.\n. This still feels too esoteric to me... I understand the issue, but don't know if having hundreds of new Strings floating around helps memory wise... Thanks!, but I think I'll pass on this one...\n. Seems like String is note safe either, http://www.ruby-lang.org/en/news/2012/11/09/ruby19-hashdos-cve-2012-5371/\n. @will-kidzui Hi, Results::Item should be fully \"Rails compatible\", especially in routes. See the generated application. Could you provide an example of code where it does not work?\nCheck that to_param depends on to_key, which is indeed implemented.\n. @tamird @blarralde Hi, reopening, will try to emulate & test it on my end.\n. Frederick, terribly sorry about the delay. Both the min_score and track_scores features are pushed now.\n. - The parent URL support was recently added in karmi/tire@12a0fed, \n- the nested query is being worked on in karmi/tire#475, focusing on nice, demonstrative integration tests,\n- the custom_filters_score sounds nice, but needs both unit and integration tests to demonstrate the feature\n. Update, custom_filters_score is implemented in Tire Contrib, https://github.com/karmi/tire-contrib/blob/master/lib/tire/queries/custom_filters_score.rb\n. Yeah, filters generally \"take anything\", they just send the Hash down the line to ElasticSearch.\n. Hi, it's a known bug, there are several issues opened for that: #265, #201 related to that. It needs to be solved, definitely. And\u00a0it needs to be solved with the multimodel search bug, #218 looks very promising in this regard.\n. > Is there something holding you back from pulling that fix into master?\nTime :) (Joking.) Seriously, I need to review all the patches related to this issue, and somehow kill it in one swoop. I got burned with some commits such as 1d6b267 and am a bit reluctant to \"just pull\" on a piece-by-piece basis now, usually pulling the patch means making sure it still makes sense in the grand picture....\n. > why the need to use an external gem (...) to iterate over records when ActiveRecord already has #find_in_batches and Mongoid has #batch_size\nBecause that's ActiveRecord (or Mongoid) specific interface. There's no ActiveModel standard for batch find. Many people, me included don't care about ActiveRecord (or Mongoid) and use different storage systems with different adapters.\n\nTire::Index#import iteration algorithm does not guarantee unique records. Using kaminari & Postgres (...)\n\nWhere's Tire behaving strangely here? Is there a Tire issue then? Could you use will_paginate instead of Kaminari?\n. No problem. I'm using will_paginate, mostly.\n. Hi, no immediate plans -- I think it would be a great thing to have!\nI think this would again make a great extension to tire-contrib. I a method, such as MyModel#more_like_this, would return similar documents. Not sure about proper semantics, though...\n. Implemented in https://github.com/karmi/tire-contrib/blob/master/lib/tire/queries/more_like_this/more_like_this.rb\n. Hi, thanks!\nFirst of all, could you transfer the code to a pull request on tire-contrib? I'd like to keep the Tire's core as slim as possible, and even rip out some things such as Tire::Persistence to contrib.\nHave a look at the rails-logger extension. You can provide README for the extension, tests, etc.\nSecond, I think the mlt does not fit well into the DSL, what about more_like_this?\nAlso, could you please add some integration tests as well -- they are generally the best \"documentation\" for the code!\n. Hi, it took me a while to decipher what exact issue you are experiencing. I think I understand now and I am grateful for the issue, because it shed some light on important matters.\n1/ You're right that the interface is asymmetric and absolutely not elegant the way it is.\n2/ I don't know what are you referring to as \"hacky\", though. Do you consider getting type from the document in Index#get_type_from_document as \"hacky\"? I tend to think the logic is very clear and the code is quite robust.\n2/ The real problem is that remove is symmetric with store, and not with retrieve. In ElasticSearch, get and delete takes document type and document ID as arguments. This is something we'll fix as soon as possible.\n3/ You can see, that the interface you're proposing has in fact been there previously, but was removed for good reasons. See commits 7daddb7, b06469c, 7749f0a, a05e4bf. The new interface is more fluid and convenient. It does suffer with the aforementioned assymetry, though. The old interface was \"hacky\" in trying to figure out what exactly are you passing to the method.\n3/ Obviously, Tire assumes certain things about the documents you're passing to the Index#store functions. They must be objects responding to document_type, type or _type methods, or Hashes with a type or _type property. I tend to think this is a very transparent and simple contract for your objects.\n4/ It seems like you are passing a JSON string to the Index#store method. While this has been supported in the previous interface, it does not make any sense right now -- you just don't have any means whatsoever to pass a proper document type. This feature has been just deprecated in 5739059.\n\nLooking at the code, it seems as though if you have a type that is the same as the type of class it will not use that type, why?\n\nThat's a guard against inadverently storing classes as their \"native\" type in Ruby. It would be surprising if Tire would do that. For any object you intend to pass to Tire, it's safer to implement document_type.\n\nAnyway, I had to (what feels to me like a hack) implement another method document_type that returned the type.\n\nAgain, I don't understand why you'd consider this a \"hack\", but that's precisely the contract Tire expects from your objects. See http://karmi.github.com/tire/#section-18. If there's an issue with document.respond_to?(:type) && document.type != document.class (notice the capitalization in the Article class at the link), I'd be happy to have a clear case which demonstrates it!\nIf I misunderstood the problems you're having, please explain it more!\n. > Well my feeling of hacky, comes from what you term a \"more fluid and convenient\".\n\n(...) but I would prefer to be able to explictly pass through the type to the store method.\nAnd not have have to embed it in a method that Tire then checks to get the type.\n\nYou have to, in fact, explicitely pass the type to the store method: either as a document_type method for objects, or type|_type property of a Hash. You just don't pass it as a separate argument.\n\nI guess it just comes down to a difference of taste at the end of the day.\nFor me store(type, document, args) and remove(type, id, args) is more intuitive, especially if I was just guessing.\n\nYes, it is more explicit. Given how transparent the logic in Index#get_type_from_document is, I'd say we don't have to be so explicit.\n\nThe other things is I feel the README is too \"here and there\".\n\nA proper documentation is definitely something which needs more work, once we sort out the current situation with dozens of pull requests and important features missing....\nRegarding the Ruby code you posted, I have the feeling you make things very complicated for you with no real benefit...\n1/ If you're storing a standard ActiveModel compliant object, why not just call as_json(... lots of options ...) in it's to_indexed_json method?\n2/ If you're storing some other object, why not just build the Hash manually and transparently, again in the to_indexed_json method?\n. Hi, a best to use a filter here, since it's nicely cacheable -- if the documents downright miss field, the exists filter could be put to great use here:\n``` ruby\nrequire 'tire'\nTire.index 'test-published_filter' do\n  delete\n  create\nstore title: 'Document 1 (published)', published_at: Time.now\n  store title: 'Document 2 (unpublished)'\n  refresh\nend\ns = Tire.search 'test-published_filter' do\n  query { string 'document' }\n  filter :exists, field: 'published_at'\nSTDERR.puts to_curl\nend\nputs \"---\", s.results.to_a.inspect\n```\n. >  filters dont affect facet counts (...)\nRight! In that case, you can use the boolean query and the _exists_ query. It could make more sense to have some specific property published=true|false for better legibility, though...\n``` ruby\nrequire 'tire'\nTire.index 'test-published_filter' do\n  delete\n  create\nstore title: 'Document 1 (published)',   published_at: Time.now\n  store title: 'Document 2 (unpublished)', published_at: nil\n  refresh\nend\ns = Tire.search 'test-published_filter' do\n  query do\n    boolean do\n      must { string 'document' }\n      must { string \"exists:document.published_at\" }\n    end\n  end\nSTDERR.puts to_curl\nend\nputs \"---\", s.results.to_a.inspect\n``\n. You're welcome, please just close the issue if/when the problem is solved...\n. Hi, unfortunately these are known bugs, see issues #218, #204, etc. The issue is caused by incorrect serializing ofdocument_type. It is a priority bug right now.\n. Once the bugs are smashed, it should serialize the document types as follow, AFAIK:\n-Link=>link-LinkGoogle=>link_google-Link::Google=>link/google, URL encoded aslink%2Fgoogle` \nThis part of ActiveModel is the underlying cause, AFAIK.\n. There was some discussion on IRC today about that...\nIf you're not interested in :load-ing the models, you can make all the document_types to return the same value...\n\nwhen I index Link model, the whole table will be indexed, right\n\nDepends on lots of things. When you use the bundled import, only the class you pass is used.\n\nif the self.search method is in Link, will the search return results from subclasses\n\nAgain depends on lots of things. Try to gist some code maybe....\n. Hi, I'm too busy to understand what exact problem are you trying to solve here. These are two separate documents in the index? Because for common cases, you just touch the associated record.\nIf you clean the code up and add some documentation with proper examples, I think it's a nice contribution to karmi/tire-contrib...\n. Hi, thanks! Couple of notes:\n- Could you please add unit and integration tests for the feature?\n- I haven't decided yet how to work with type and id being passed to Index methods -- see #275. Anybody has a strong opinion on that?\n- What about model integration? We probably want to to use the Update API in MyModel.tire.update_index?\n  *\u00a0If we do that, how will it affect percolation and possibly other Tire features?\n. Try to include Tire after you define all your properties. It should not over-ride already existing methods.\n. Great .)\n. Yes, I think there's no way around it for now. We'll have to have a look in using Hashr (or something like it) consintently everywhere, when accessing facets, etc.\n. Hi, I don't know anything about eager loading in Mongoid.\nCould you please post an example of what are you trying to do and what is the error/bug/issue? It's hard to tell from the description...\n. I'm sorry butI don't know enough about Mongoid to be of much help... Could you post the question to StackOverflow? http://stackoverflow.com/tags/tire\n. Hmm, I understand, but I'm wondering how to support that on the library level?\nTire provides convenience utilities to import the data (either with the Rake task or the MyModel.import method), but for anything specific, you need to write your own wrapper. It's very easy with Tire...\n. I think the solution here is to just write your code for the indexing, and not use the bundled Rake task...\nThere's definitely no\u00a0place to add a call to the unscoped call :) The right approach is to write your code around Index#bulk_store, using find_each in ActiveRecord, etc., not trying to hack the support in the model import support...\n. Thanks, nice!\n. Hi, please post a failing code example or/and a link to a test case in Tire test directory which demonstrates the issue, and I'll reopen! Thanks.\n. Hi, I see what you mean, definitely an interesting use case.\nIn ES, you just issue one boolean query, but Tire exposes it in Ruby, where you can build the query iteratively.\nI think the https://github.com/karmi/tire/blob/master/lib/tire/search/query.rb#L50 line is the culprit, we need to update the options here...\n. Yeah, also not entirely sure how to approach it... One way to look at it is that the \"first wins\" is as potentially bad as the \"last wins\" scenario -- and the \"last wins\" is maybe less surprising...\nHowever given multiple boolean <this> unless <something> and boolean <that> if <something else>, there's always potential for surprise... We do the same thing in our app as well, though.\nSo, all in all, I'd vote for boolean.options.update... but who knows :) The blockish boolean syntax is awesomely convenient, but gives lots of rope.\n. Hi, my mind normally does not work well with synthetic examples -- could you post some examples modeled after your real use case, with real model names, real entities, real relationships?\n. Hi, I'm sorry I didn't get back to you sooner, but I still don't understand the issue enough.\nI think you could use a term filter on the public field in a filtered query for non-admins, and possibly also a term filter on the user_id field to limit the results for each user. Index aliases with filters work very well here, have a look here: http://www.elasticsearch.org/videos/2012/06/05/big-data-search-and-analytics.html\nClosing the issue for now, feel free to submit new one.\n. Hi, thanks for the patches!\n1/ Regarding the fuzzy query support, I'm on the fence if this should go the Tire core or contrib.\n2/ The fuzzy query can, in fact, be quite easily be done with the \"raw API\", by passing a Hash, just as the Readme instructs. We could or maybe should support stuff like pagination better with the \"raw API\".\n3/ Yes, every query type needs corresponding tests: unit tests to check the raw behaviour (proper encoding etc), and an integration test. The convention is to put unit tests in the test/unit/search_query_test.rb file, in a separate context, for the time being. I think we will split it into multiple files in the future. Every query type has a corresponding separate integration test file, in this case it would be called test/integration/fuzzy_queries_test.rb.\n4/ The missing support for options looks like a bug. Could you add a unit/integration test demonstrating the bug and filing a separate issue?\n. @BlackdolphinMedia, @gregoriokusowski, @fmeyer, @rafaelsteil, @cmilfont, @peleteiro, @andreazevedo, I appreciate your interest in Tire! However this is not Rails, please add something meaningful to the discussion...\n. @plentz Cleaned up and merged the code, thanks! For the time being it stays in core, the structure of code fo queries must be improved.\nNotice, that for \"fuzzy\" queries, there's a standard Lucene operator for the \"query_string\" type. In majority of cases, it actually makes sense to use the \"text\" query with the fuzziness option, as @kimchy suggested on IRC.\nYou can see the example in the integration test, commit 137c981.\n. @plentz, this is just a summary of what I did when splitting your commit plentz/tire@eb9992b, which combined different features/changes into one.\n``` bash\n  # Get the changes and update from master via rebase\n  git checkout -b issues/289\n  git pull https://github.com/plentz/tire.git master\n  git rebase master\n# Start interactive rebase of all commits not on master\n  git rebase -i master\n# For eb9992b commit, mark it as \"edit\"\n  # Now, reset the commit, but leave the changes on disk -- preparing the commit \"split\"\n  git reset HEAD^\n  # Add only the portions we want\n  git add -p\n  # Commit only the selected changes as a split commit\n  git commit -v -c eb9992b\n  # Repeat with the rest of the code, splitting the commit...\n  # Continue the rebase...\n  git rebase --continue\n  # (We also squashed the commits)\n  # Now let's check what we have prepared\n  git log HEAD --oneline --not master\n  # We can safely merge that into master\n```\nSee these sources:\n- http://book.git-scm.com/4_interactive_rebasing.html\n- http://stackoverflow.com/questions/4307095/git-how-to-split-up-a-commit-buried-in-history\n- http://gitready.com/advanced/2009/02/10/squashing-commits-with-rebase.html\n. Thanks for the investigation! It looks like things are still out of sync for mappings.\n. @sarmiena Cleaned up the commit, and merged. Thanks for looking into it, I think everything works fine with namespaced models now.\nAlso, I reverted changes to the ActiveRecord tests (not really helpful, seems to me? let's prefer simplicity in tests), and added a more lightweight unit test implementation.\n. It's again a \u201cpiecemeal\u201d approach. The way Tire works with options right now is ridiculous. I'd like to kill these problems in on sweep, have to find some time to review the codebase and find the sore spots.\n. I'm not sure about exception, but maybe at least display a warning as stated in #161...\n. You have to load your application environment (as statet in docs and task help):\n$ rake environment tire:import CLASS=\"Chrysler::CustomerData\"\n. @barbct5 It is, https://github.com/karmi/tire/blob/master/test/integration/facets_test.rb#L62-77 ...\n. No problem! :), the docs are extensive, take your time to read it thru... It will pay off.\n. Thanks for the report, should be fixed in https://github.com/Ataxo/redis-persistence/commit/7fb1b973e2320127c6e123099084b8be4d8bc323.\n. Yeah, the renaming makes sense, I confused it with the Scan class name.\n. Since the issue is now closed, care to share what was the solution?\n. @cwalsh You mean in tests? Yeah, the order should absolutely be guaranteed.\n@hajder Don't let your bitterness get in front of polite discussion :) @cwalsh was just asking a question .)\n. I guess an integration test in this case would be great:\n- It would clear any confusion (eg. about the guaranteed ordering, see discussion above)\n- Integration tests serve as a great documentation on the exact purpose of the feature, not only as a \"defensive mechanism\". See eg. https://github.com/karmi/tire/blob/master/test/integration/index_update_document_test.rb#L25-35 [karmi/tire@ff00015] for an example for the Update API which covers the usual use-cases for the API.\n. Additional info for the integration test -- there's a great example of \"albino elephant\" at the http://www.elasticsearch.org/guide/reference/query-dsl/dis-max-query.html page, which should be easy to translate to a Tire integration test:\n\nIf the query is \u201calbino elephant\u201d this ensures that \u201calbino\u201d matching one field and \u201celephant\u201d matching another gets a higher score than \u201calbino\u201d matching both fields. \n. @hajder Thanks, merged.\n\n@hajder, @nbudin Guys, maybe you can shed some light on your dis_max usage for me? As you can see from note in karmi/tire@7431ce9, the dis_max query gives the same result as a plain old boolean query? Can you share some example where it does make a difference from your application/domain? Thanks!\n. See http://www.elasticsearch.org/guide/reference/mapping/all-field.html, it depends on the ES version you use and you must set the _all field to be stored. See elasticsearch/elasticsearch#861.\n. Yes, just don't include the Model::Callbacks module and handle the indexing yourself, preferably by running the update_index method in the background.\n. @lgs, basically, with eg. Resque, you'd do something like:\n``` ruby\nclass IndexUpdater\n  @queue = :tire\ndef self.perform(record_id)\n    MyModel.find(record_id).tire.update_index\n  end\nend\nclass MyModel\n  include Tire::Model::Search\n# ...\nafter_save    :enqueue_index_update\n  after_destroy :enqueue_index_update\ndef enqueue_index_update\n    Resque.enqueue(self.class, self.id)\n  end\nend\n```\n. Hi, there's no support for rivers right now, just send some hashes with RestClient to ES. Certainly can be added in the future.\n. Yeah, just pass the port as the URL, https://github.com/karmi/tire/blob/master/test/unit/configuration_test.rb#L36-39\n. That's a bug where we pass the search options (implemented in d764c0d) to ES. For the time being, better use the blockish DSL syntax to perform sorting...\n. @matsko Closed by https://github.com/karmi/tire/commit/351d0c4b506f1017e910164f8a43ddc0679fde4d. Could you try?\n. Yes, it's been solved long time ago.\nIn your class, define the to_indexed_json to include the author:\nruby\ndef to_indexed_json\n  to_json( include: { author: { only: [:name]} } )\nend\nI recommend watching the relevant Railscasts episodes on Tire, with these updates: https://github.com/karmi/railscasts-episodes/commit/03c45c3, https://github.com/karmi/railscasts-episodes/commit/ee1f6f3 relevant to your use-case.\n. Hi, yeah, your approach seems one. Couple of things:\n*\u00a0unless you want to use Lucene query syntax (boosting, ranges, etc), it's maybe best to use the text query,\n- yes, filters are more performant then queries, an the active=true in your example is a good fit for filters. Beware of the interplay between queries, filters and facets, though.\nYour definition of the term query is incorrect, though -- it should be:\nruby\n term :author, params[:author]\nSee https://github.com/karmi/tire/blob/master/test/integration/boolean_queries_test.rb.\n. Glad it's working. Maybe post the solution at the Stackoverflow as well?\n. Carrierware seems to redefine serializable_hash, and maybe it's breaking stuff for other gems -- be sure to file an issue with them.\nAnyways, you can just redefine the to_indexed_json method:\nruby\ndef to_indexed_json\n  self.as_json\nend\nso Tire won't rely on compatible serializable_hash...\nWhen you find the solution, please close the ticket and the SO question, otherwise, ping here!\n. @kyledecot Hi, any news on this? Is your issue solved?\n. @kyledecot Perfect, thanks!\n. Another approach: http://spacevatican.org/2012/6/12/using-multisearch-with-tire\nGist: https://gist.github.com/92d77b62eb570031823d\n. @romanbsd Closing this, due to missing test suite, etc., all right? We'll need to add proper multi-search support...\n. Closed in d233740, 775f537, c03c666\n. Notice it's easy to use Tire's API/DSL for manually invoking the \"Delete by Query\" API, until there's a proper support:\n``` ruby\nDangerous code removed\n```\nSee the code below\n. @Papipo Yes\n. @jurgens @Will-Sommers @thoughtpunch Sorry, the advice I gave was incorrect and dangerous (@ches above is right), I apologize.\nThis would be the workaround for the moment:\n``` ruby\nrequire 'tire'\nindex = Tire::Index.new('articles')\nindex = Tire::Index.new('articles') do\n  delete\n  store title: 'x'\n  store title: 'y'\n  store title: 'z'\n  refresh\nend\nquery = Tire::Search::Search.new do\n  query { term :title, 'x' }\nend\np query.to_hash\nputs query.to_curl\nputs \"curl '#{index.url}/_search?source=#{Tire::Utils.escape(query.to_hash.to_json)}'\"\nputs \"curl -X DELETE '#{index.url}/_query?source=#{Tire::Utils.escape(query.to_hash[:query].to_json)}'\"\np Tire::Configuration.client.delete \"#{index.url}/_query?source=#{Tire::Utils.escape(query.to_hash[:query].to_json)}\"\n```\n\nNOTE: Eg. the Curb gem still doesn't support HTTP bodies with DELETE, making the support a bit cumbersome.\n. @phoet Yes -- we could support delete_by_query by serializing the payload into the source URL parameter. \n. @phoet I think that would qualify as an \"acceptable\" hack :)\n(Of course provided we have unit+integration tests, etc)\n. Hi, Tire should actually work out of the box for all elasticsearch features :), but nice idea! It's worth a shot implementing this as an optional extension to the DSL.\n. The best start is to generate and play with the example application bundled with Tire (see README):\n$ rails new searchapp -m https://raw.github.com/karmi/tire/master/examples/rails-application-template.rb\nThe relevant Railscasts episodes should set you on a right track:\nhttp://railscasts.com/episodes/306-elasticsearch-part-1\nhttp://railscasts.com/episodes/307-elasticsearch-part-2\n. Hey,\ninteresting idea :)\nBoth take JSON as input and give it as output, but the ElasticSearch and CloudSearch search APIs seem to be very different to me.\nKarel\n. Why would you use a URL like http://localhost:9200/owner/1/posts, ie. index owner, type 1 and ID posts in ElasticSearch's terms?\n. Any news?\n. ruby\nclass Post\n  include Tire::Persistence\n  index_name    \"owner\"\n  document_type \"article\"\nend\n. Closed from karmi/tire-contrib@360348a\n. Hi, weird, does your production contain any data, ie. what does:\n$ rails runner -e production 'puts Product.count'\nreturn?\n. Any luck?\n. Glad it's sorted out.\n. I would prefer to wait on this issue a little bit longer. From the brief reading I did on the encode/decode vs. dump/load, it doesn't strike me as a particularly great example of responsible library design.\nDirectly changing all encode to dump, and forcing users on 1.3 doesn't strike me as an example of responsible library design on our part.\nRails does couple of tricks right now to detect which method to use etc. I agree that the deprecation warnings are obnoxious.\nI think we should solve this when we're nearing the 0.5 release?\n. I'm really conflicted. I don't like the reasoning given by MultiJson, that's for sure. See the discussion at https://github.com/intridea/multi_json/issues/36.\nI'm not sure about the benefits of MultiJSON >\u00a01.3, probably more encoders, such as oj... I tend to be very conservative with pushing updates on library users, there are tons of people out there using Tire in various scenarios...\nI think we could push for this in the 0.5 release -- in the end, it's really a matter of simple find and replace.\nOr, we could do what Rails does, laboriously detect which method to use, or use our own wrapper around the MultiJson wrapper -- but that would seem as the ultimate silliness :)\n(I tried to set test.verbose = false in the Rake tasks, but it does not get propagated further down the line to suppress Kernel.warn, it seems.)\n. So, let's close this and reopen when we're nearing 0.5 and the dust on load/dump settles?\n. There's no easy way to work with multiple clusters, right now. Yes, it's bad.\nFor now, the only solution would be to laboriously inject URLs before different calls, etc. Doable, but ugly. The Tire::Configuration.url is a class method, and unfortunate design choice. I plan to have a look into that.\nNevertheless, it seems like what you're after is reindexing? In that case, does https://github.com/karmi/tire/blob/master/test/integration/reindex_test.rb help?\n. Hi, I think for large data sets, it does not make much sense to use pagination?\nThe proper solution from the user experience standpoing is to offer the \"Load More\" link and the total number of documents? Because, when the total entries is 2 millions, you:\na) are either interested in aggregations on these results, so don't care about paginating results anyway,\nb) should improve your query to get back a better result set.\nCf.:\n- http://railscasts.com/episodes/114-endless-page\n- http://railscasts.com/episodes/114-endless-page-revised\n. I don't see a clean way how to add a feature like this right now. I don't see good documentation on that in ThinkingSphinx either...\n. First, I think the Github history makes for a better \"changelog\" then static files?\nSecond, every new version prints out important information in your terminal, when installed. See: https://github.com/karmi/tire/blob/master/lib/tire/version.rb and https://github.com/karmi/tire/commits/master/lib/tire/version.rb\n. Right now, Tire uses separate indices for different models (ie. document types). In coordination with @nz, I'd like to change the defaults to use separate types; the user will still be able to use different indices, if she wishes, of course.\nI plan to have a look into this in the near future. I'm not sure how multi-model searches -- since that is what you seem to be doing -- work right now with Tire and Bonsai.\n. Yes, of course, they are very easy on the ES level. I have meant: I don't know how they work right now with Tire and Bonsai.\n. @hale, \"within the next three months\" sounds most plausible... Since it's a breaking change, I'd like to sync with other similar changes...\n. All: Please direct all Bonsai questions to Bonsai support channels.\nDo notice there's some new instructions for Bonsai/Heroku available: https://gist.github.com/2041121\n. @nz It is still on the roadmap, though it is still a low priority.\n\n(...) a single index per app per environment is a useful paradigm for most apps.\n\nDisagree. I understand the argument \u201cindex is like a database and you have one database per app\u201d can be made -- but it's applicable to small, basic apps only. All the awesome and powerful features of elasticsearch, such as index aliases, time-based indices, re-indexing without downtime, etc. go out of the window.\n\n(...) Tire's current conventions are too well entrenched to be worth the effort (...)\n\nNot sure I follow 100%, but don't think so -- time &\u00a0energy are scarce resources, and there's still a lot of pull requests to process, bugs and inconsistencies to fix. This one issue arguably only affects Bonsai-based deployments, where many of those affect the majority of Tire/elasticsearch users.\n. @nz Cool! :thumbsup:\n. Yes, this makes sense, thanks for that!\nAs I said in many previous cases, these kinds of patches are half-solutions, before we use some better HTTP abstraction, notably Faraday. Notice that your patch is only for the RestClient client (not what anybody should use in production), and that we're nearing our custom HTTP abstraction. That's not a place I'd like to go...\nI will make some time for refactoring the HTTP support in the coming months. I'll meditate over this patch a bit in the mean time :)\n. Hard to say what's your problem exactly... If ES is running on http://192.168.1.138:9200 when you start the console, Tire should be able to connect to it.\n. The problem you're having still does not make any sense to me, sadly.\nConfiguring Tire in an initializer is perfectly fine, and works. Maybe try to generate the example application and point it to your virtual machine -- hard to say what's going on in your app otherwise.\n. > I am using the Bonsai add on provided by Heroku\nI'm not sure what's the current status of Bonsai and Tire -- there has been numerous bugs being reported.\n. @wflanagan terms is what you need, and also boolean queries, see https://github.com/karmi/tire/blob/master/test/integration/boolean_queries_test.rb\n. All: Please direct all questions about Bonsai to their support channels.\nStrange stuff with the Devise and all... Closing the issue for now...\n. @kristopher That sounds like a good explanation and debugging! I'm afraid there's no way around it than proper documentation?\n. > Printing the URL sounds like a fine idea more info the better.\nAgreed.\n\n(...) you could get away from defaulting to localhost and throw an exception (...)\n\n...and breaking the experience for everyone, because you have to set localhost:9200 all the time. You just can't prevent people to shoot them in the foot by not paying enough attention. More importantly, I don't remember all the issues raised in this ticket, but the original issue is about the informative mesage Skipping index creation.\n. @iwarshak Thanks! By the way, I think the ELASTICSEARCH_URL and BONSAI_URL should be preferably set with heroku config? \n. @nz All this issues would go away if you'd be able to require \"tire/heroku\" or require \"tire/bonsai\" and all these checks like Is ENV['ELASTICSEARCH_URL'] properly set?, Is BONSAI_URL set?, Can we talk to the index?, etc.\n. Yeah, please ping me. In the future, I envision the client to be much more modular like this anyway, so it would all play nicely.\n. OK, why not...\n. @niuage For proper \"autocomplete\", best is to use ngram based analyzers, I think StackOverflow has some nice resources on that from Clinton.\n. More specifically, Tire needs to separate models by type by default, and use a common index for them, let's say \"application wide\". Of course, users must be able to specify separate indices for separate models, when needed (eg. when one index would hold millions of data and the other mere thousands, etc).\nI'll definitely have a look into this, but probably not during May.\n@nz, I think there are two questions related to Bonsai.io at the moment:\n1. Why some data were imported, but not all?\n2. How can users work with Bonsai.io and Tire currently? After all, setting MyModel.index_name to a specific, user-based value should work?\n. So, the issue here is that Index#bulk_store connects to /_bulk, not to <MY INDEX>/_bulk, correct? Therefore, bulk store / importing with current Tire and Bonsai.io doesn't work.\nAnd, basically, setting MyModel.index_name is the way to make Tire work with Bonsai.io in general?\n(I do like the implementation of Index#url in #327, it's much cleaner, will review it. I wouldn't mix it with such a big overall code change, but I can split &\u00a0edit those commits myself.)\n. @toddwschneider Hi, should be resolved on current master and the 0.4.1 version.\n. @toddwschneider Could you confirm the current master and release work with Bonsai?\n. @toddwschneider Great!\n@Caged I don't think articles index is available at Bonsai, AFAIK the index name is tied to your account. You should probably leave the index empty. Please see this gist by @nz for more info: https://gist.github.com/2041121.\n. Alas, that is not a typo :)\nNotice the difference between @search.results.facets, which give you the counts and metadata from ElasticSearch and the @search.facets, which gives you the facet definitions...\n. @mando Ping. Closing this?\n. No worries! :) (The interface should make it probably a bit easier to distinguish anyway...)\n. Hi @niuage, sorry for the delay, yeah, Task clashes with Rake. Tire does include Rake, I have been worried about that, but nobody reported any error.\n. If you want to return only specific document fields, use the fields method: https://github.com/karmi/tire/blob/master/test/integration/results_test.rb#L28\n. Please make sure you close the issue when the problem is solved :)\n. Hi, thanks. Couple of remarks:\n1/ Index#store uses args introspection to get the type and id. It should change to allow more flexibility and I have to think about that.\n2/ Could you add at least a unit test for the feature. I can write integration test myself.\n3/ It's more convenient when you rebase\u00a0against current master in Git workflows. I can get rid of the merge and rebase myself, but still, better when it's not there.\n. @ahfeel Any news?\n. ruby\nTire.configure do\n  url \"https://...\"\nenb\n. Hi Roman, yeah, that's a bug -- I'd rather have an overall look over the codebase to rip instances like these out..\n. Is it possible to add a simple unit test covering the bug?\n. @karmi @vhyza Bump.\n. Merged it in, moved the test to a better place, it should not break anything. Sorry it took so long to have a look at this...\n. That should indeed be supported, is https://github.com/karmi/tire/blob/master/test/unit/model_search_test.rb#L382-392 what you're after?\n. @hm-zestcash Hello, I hope your problem was solved and you only forgot to close the issue.\n. Hey, nice idea to add Faraday as one of the regular \u201cclients\u201d :) As I've mentioned in couple of places, I'd like to switch to Faraday as the HTTP abstraction very soon.\nI'm not against merging this in the meantime -- I'd just like some more detailed information, preferably in the faraday.rb file itself, how to set up the adapter, etc etc. Tests are not neccessary in this case.\n. Hi Roman, looks good, will have a look at that shortly.\n. @romanbsd Thanks, Roman, merged!\n. OK, changed the usage instructions to use typhoeus and removed the midleware part.\n. In fact, the search_type=count is supported: https://github.com/karmi/tire/blob/master/test/integration/count_test.rb\n. Hmm, this sounds like a nasty bug. Could you have a look into unit tests for Tire::Results::Item if you'd be able to write a failing test?\n. Should be fixed in karmi/tire@4f2cbd4\n. @azitabh @thedaniel Thanks for your feedback, hope your problem is solved. Closing for now.\n. @results  = Model.search, page: params[:page], per_page: 25 load: true do ... end.results\n<%= will_paginate @results %>\n. Hmm, yeah, I think ActiveModel could be ripped out since that's mostly for Tire::Persistence, I never focused on making Tire Rails 2.3 compatible, but maybe it could be done.\n. Yes, splitting the gems in core, model, etc does make sense -- provided we keep the easy setup as is the case now. It should be doable over the coming months.\n. @kidpollo No updates yet -- the real solution is to extract the Tire::Persistence into separate library, so we don't have to pull ActiveModel directly. I'm not sure how we'll solve the Tire::Results::Item compatibility with Rails without ActiveModel though?\n. Hi all, I really don't see a good way how to squeeze into Rails with Item being ActiveModel-compatible and all without ActiveModel. Obviously, the DSL should be usable outside of Rails as well.\nI'll keep the issue open as a reminder. Until the gem is refactored and split, there's no way I see how to support these features, though.\n. @tyler-smith I don't think putting energy into extracting the model related code is worth it, at the moment.\nBesides, I haven't really figured how to make Item behave the way it does currently without ActiveModel, and also, if it makes sense to deal with Rails 2.3 integration when we're nearing Rails 4.0 as current version...\n. I suspect this to be ActiveRecord specific? We need to support generic ActiveModels OxMs... Will close this unless you have a generic solution, OK?\n. Closing, ActiveRecord specific code.\n. where(:id => ids) is ActiveRecord specific, not everybody uses that.\nBut yes, maybe we could go further to support ActiveRecord in the future.\n. @xdmx @SLM-Linus See the attached commit -- you can now override the Tire::Results::Collection#__find_records_by_ids in your codebase for possibly correct behaviour. While it's not a solution, it's a workaround for the problem -- please ping me if it works in your case.\n@jlsherrill You're right -- in many cases, the problem lies in using the after_save/after_destroy callbacks. For ActiveRecord, Tire really should use after_commit/after_rollback callbacks.\n. @xdmx @SLM-Linus Closing.\n. @atlantis So you overloaded __find_records_by_ids with some AR-specific where(id: \u2026) logic, but needed to overload __get_results_with_load as well? That would be really weird considering the logic in https://github.com/karmi/retire/blob/master/lib/tire/results/collection.rb#L144\n. Hey, I believe this was closed by karmi/tire@cb30f52, can you check?\n. Closing, no reply.\n. that is just a default, maybe this: https://github.com/karmi/tire/blob/master/test/integration/filtered_queries_test.rb#L43-60 is what you're looking for?\n. Glad it's working :)\n. Sorry, I probably don't understand the issue.\n- Tire checks if the index exists in the create_elasticsearch_index method, but rescues errors and prints to STDERR.\n- Tire saves documents in ES via callbacks on updates. You can choose to not include Tire::Model::Callbacks and index your data manually.\n. > I think when ES sever down, It should skip indexing and logging failed article.\nThe included callbacks are for convenience, and 80% of use-cases. When other behaviour is desired, the solution is to handle indexing in your own code, possibly based on the update_index method.\n. Make sure your model is correctly setup. Hard to say without any code.\n. Sorry -- you should use to_json, not as_json.\n. @romanbsd Only time, sorry! -- merged &\u00a0pushed.\n. Sorry, really don't understand the issue -- do you want to use nested type, or parent/child feature, or what is the problem? Closing for now.\n. @brandonmeeteor Hi, what about just putting the username:password in the URL?\n. I believe it absolutely should, https://github.com/archiloque/rest-client says:\nRestClient.get 'https://user:password@example.com/private/resource'\n. Hey, I'm not able to reproduce the issue you describe with a test such as:\n``` ruby\n        should \"return collection for one matching result\" do\n          results = ActiveRecordArticle.search :load => true do\n              query { string '\"Test 1\"' }\n            end\n      assert_equal 1, results.size\n    end\n\n```\nCan you provide a failing test or a more informative description of your data set etc?\n. BTW, was tempted to fix it with klass.find(Array(ids)), but really can't reproduce it...\n. Can't reproduce, closing...\n. Thanks!, merged.\n. Closing the issue, since the problem is solved.\n. That's the default setting of shards:5, replicas:1 in ElasticSearch. Just create your index with different settings,\nruby\nTire.index(\"myindex\").create settings: { number_of_replicas: 0 }\n. Hi, thanks. Yes, Dylan's version seems to be more complete, but there are still unresolved issues with handling conflicts, possibly merging them, etc etc. I have to focus on this. In the meantime, I'd advise to just use eg. Tire::Configuration.client to send some Hash down he wire to ES.\n. @fabiob Sorry, for the delay. Similar to #194, let's close this until there's a more feature complete implementation, with handling/ignoring conflicts, etc?\n. Closing in favour of a more complete implementation.\n. @alan No, please see the discussion.\n. @alan Not yet :) \n. @timuckun Was the advice helpful?\n. Hi, in a case like this (index settings), just send an exactly the same structure you would send as JSON string as a Ruby Hash.\nCheck the index settings: curl localhost:9200/articles/_settings and mapping: curl localhost:9200/articles/_mapping to see if your configuration has been correctly stored in ES.\nNotice that you are creating a document without specifying type, thus it ends up being a document, but your mapping is set to a article type. Notice that the documentation is very clear on this, even within the snippet you have pasted.\n. You're trying to  create the index after it has been already created. Turn on logging with Tire.configure { logger STDOUT, level: \"debug\" } to see what's going on.\n. Article.index_name { \"articles-#{Time.now.year}\" }\n. Hey, thanks!, has been recently added as karmi/tire@0f38828\n. @martinciu Excellent patch, thanks!!\n. Duplicate of #368\n. @lgs The BSON::InvalidObjectId is usually coming from incorrectly serializing a Mongoid model instance to JSON -- see the Readme, you have to manually override to_indexed_json for Mongoid.\nI'm wondering about the logger incompatibility, Tire's logger method should definitely accept a Rails.logger instance....\n. Added support for sending partial document updates in karmi/tire@ede225d, requires Elasticsearch 0.20 or higher\n. Closing in favour of more complete implementation...\n. @martinciu Hi, I like the refactoring you did, sorry it took me so long to get to it.\nI'm not sure about returning Item instances when using the :load option, though. I think we should have something like Sunspot's each_with_hit (see karmi/tire#406 and karmi/tire#261).\nI'll try to have a look into this shortly.\n. @martinciu What do you think about the each_with_hit approach? I think it would be better then to decorate the results directly? Also notice the refactoring I did in https://github.com/karmi/tire/tree/refactoring/results to better support people needing to change the code which loads data from the database...\n. Closing in favour of each_with_hit (or similar) feature.\n. @edwinv Tire should definitely not stomp on your document_type method -- as @richarddong hints, Tire should use the tire proxy exclusively. If this problem persists, could you post your model definition?\n. @edwinv Did the advice we gave you helped? I hope so, and you only forgot to close the issue? Closing it for now.\n. Hi, that's not a newbie question at all, thanks for the report -- it's a bug in current terms facet implementation, where you can't pass multiple fields, see https://github.com/karmi/tire/blob/master/lib/tire/search/facet.rb#L20\n. Yeah, nice loophole :), since effectively it's:\nfacet('myname') { terms 'WHATEVER', fields: [:field1, :field2]}\nwhere the fields option is merged via options into the payload sent to ES. Certainly not an interface to enjoy, but something you can use at the moment for sure :)\n. Yeah, your elasticsearch is fine -- it's just a bad behaviour of the Homebrew formula. It has to be cleaned up eventually.\n. Check out this StackOverflow answer, http://stackoverflow.com/a/9432450/95696, and maybe post your question there -- more exposure, more people willing to help.\n. There's still lots of issues around storing or not _type in the _source, as far as I can see this does not create any problems: it's just another property in source, that's all. This issue must be dealt with in a much more thorough way.\nClosing for now.\n. @mhamrah Hope your problem was solved and you only forgot to close the issue.\n. Thanks, merged!\nAnd thanks for the kind words, I don't see the codebase in such a positive light at all :), but am glad you enjoyed working with it!\n. Yeah, this is probably possible :), but what'd the point of that? That would just put the same data in..\nThere's a Rake task (and API support) for easy importing, documented in the Readme:\n$ rake environment tire:import CLASS='Article'\nRegarding the documentation -- it must be seriously improved...\n. Hi, the scan feature will be refactored due to scan/scroll mixup, see https://github.com/erickt/tire/commit/16ca8ba1f2949b2d4033c021e9477f350d8799b6\n. Hi, do you have a backtrace and possibly an example of the scope?\n. No problem, thanks :)\n. @ericTsiliacos You can sort only on fields which have one token per field. %Y/%m/%d %H:%M:%S,%L is split at the space in the middle. If you want to use this date format, and not the ES default one, you have to correctly set up the mapping for this field: http://www.elasticsearch.org/guide/reference/mapping/date-format.html\n. > Since it is a default, then I wouldn't have to do a mapping for that format correct? But because mine has a space, then I have to create that mapping for the field. \nExactly!\n\nCould you point me in the right direction where I can find what that mapping would look like.\n\nNot entirely sure what you're up to here, but to use a custom date format, you need to setup your index somehow like this:\n``` ruby\nrequire 'tire'\nTire.configure { logger STDERR, level: 'debug' }\nTire.index 'my_special_date' do\n  delete\ncreate mappings: {\n    logentry: {\n      properties: {\n        date: {\n          type: 'date',\n          # See http://joda-time.sourceforge.net/api-release/org/joda/time/format/DateTimeFormat.html\n          format: \"Y/M/d k:m:s,SSS\"\n        }\n      }\n    }\n  }\nstore type: 'logentry', message: 'OK', date: '2012/01/01 01:00:00,100'\n  store type: 'logentry', message: 'OK', date: '2012/02/01 01:00:00,100'\n  store type: 'logentry', message: 'OK', date: '2012/03/01 01:00:00,100'\nrefresh\nend\ns = Tire.search 'my_special_date' do\n  query { all }\n  sort  { by :date, 'desc' }\nend\np s.results.map(&:date)\n```\n. @ericTsiliacos Both questions are interesting, but I'm afraid I don't have an ultimate answer.\nFor first, I think the space after the day throws the query parser off, hence the ParseException.\nFor second, I'm not sure why wildcards wouldn't work here -- but I suspect your query [2012-07-10* TO 2012-07-10*] is incorrect. Both would probably be expanded to the same day (midnight?). Try something like  [2012-07-10* TO 2012-07-11*].\n. AFAIK, the problems you mention are related to the custom date format, the wildcard expansion included.\nNote, that searching for ranges is absolutely fine with your format -- given we have the data indexed as in my comment above:\n``` ruby\ns = Tire.search 'my_special_date' do\n  query { range 'date', gte: \"2012/01/01 01:00:00,100\", lte: \"2012/01/02 01:00:00,100\" }\nend\np s.results.map(&:date)\n```\n. @cwalsh That wasn't yet merged, please allow me couple of days to process the pipeline :)\nBTW, you have changed the commit, right? Now the tests are much more lucid about the intent.\n. @joshreed So, basically, you're suggesting to drop the rack/backports/uri/common_18 patch from Rack (so we don't have to require the 1.4 version) and drop it in directly? I'm not against it, if that solves the problem, just wanna make sure I understand the picture...\n. OK, merged, fixed, pushed. Rack wasn't indeed used for anything else, just the URI escaping.\n. @iterion Sorry for not getting back to you here... these problems with Bonsai are piling up... will have to find some time to investigate it.\n. @iterion Should we close this, then? I'm afraid I still can't see anything wrong in the setup...\n. @iterion Cool. \n. @zacksiri Are you sure you've been calling destroy and not delete as @felipegs?\n. @zacksiri Closing.\n. Hi, this is really rather heavy for the \"every query in one file\"... care to submit it on tire-contrib? I guess you have it loaded just as an extension for Tire, right?\n. It's really easy -- have a look on the existing queries, it's just a matter of extending the Tire classes, very similar to just \"pasting\" it inside query.rb.\n(Which you shouldn't do, BTW :) It would work equally well if you would have put it into a new file, and just require-d that after Tire...)\n. Closing due to pending pull request on Tire Contrib.\n. Thanks!\n. See https://github.com/karmi/tire/blob/master/test/integration/scan_test.rb for documentation on scan/scroll. This has nothing to do with \"scrolling\" the page. In that case, just load the next batch of results, based on pagination -- Railcasts.com has a nice screencast on that.\n. This is strange, can't replicate on current master. Please try out the Rails application template which uses bulk to import:\nrails new searchapp -m https://raw.github.com/karmi/tire/master/examples/rails-application-template.rb\nClosing for now, reopen or submit new issue when you hit the problem.\n. @beunwa That's really too little information :) Open a separate issue with details, please.\n. @iterion Thanks!, embarrasing oversight -- should be fixed now.\n@JamesHarrison Don't really follow -- the task prints out the number of documents been indexed vs. total, as well as percentage and progress bar. ETA would be guesswork -- nobody knows what can happen in your cluster all of a sudden.\n@andrew Don't really follow that as well -- I'd like to see the case where Tire said docs were indexed while they actually were not. See https://github.com/karmi/tire/blob/master/lib/tire/index.rb#L105-122\n. Hi, so is this about Tire getting incorrect document_types for the STI models? Note that issues karmi/tire#80, karmi/tire#218 and karmi/tire#277 all seem to be fixed now... Could you post your model definitions somewhere?\n. I've updated your example above to be able to run and understand your issue at all. Notice the missing include_root_in_json directive in your original code, for example.\nClearly, Venue::Type is serialized as \"venue/type\" in Tire. Why is that so? Because Tire does not know you're using STI (single class inheritance), namespacing, etc. etc., and uses just the regular ActiveModel compatible serialization (klass.model_name.underscore). If you have some better suggestion, please clearly state what is it and why is it preferable.\nThe simple solution for single table inheritance problems with Tire is just to set proper naming in your models:\n``` ruby\nclass Venue::Type < Venue\n  tire do\n    index_name    \"venues\"\n    document_type \"venue\"\n  end\nend\np Venue.new.document_type\n=> \"venue\"\np Venue::Type.new.document_type\n=> \"venue\"\n```\nWith the code as updated above, the following works as expected:\n``` ruby\nTire.configure { logger STDERR }\np Venue::Type.create title: \"Test\", lat: 45, lon: 45\np Venue::Type.index.refresh\np Venue::Type.tire.search { filter 'geo_distance', distance: '12km', location: [45, 45] }.results.first\n=> \n``\n. Closing the issue for now. If you feel there's something _Tire_ could do better for you, please don't hesitate to comment further. (Do not, however, that eg. the proposed fix fordocument_typedoes not really fixes anything...?)\n. @te-chris I think the culprit is:index => :not_analyzed` here -- see karmi/tire...\n. > For exactly the same query I get different facet counts... \nYes, it's dependent on the index shard count, some issues in the elasticsearch repo are floating around related to this.\n\nWhat means :size?\n\nCheck out http://www.elasticsearch.org/guide/reference/api/search/facets/terms-facet.html, it's the number of terms you want have returned in the facet.\n\nP.S. What is the best place to ask questions how to use tire?\n\nPosting issues is fine, asking on the #elasticsearch IRC channel is preferable (much shorter feedback loop)...\n. > To get this correct I understand I should use some huge :size number?\nYes, exactly like that, that's the current workaround on the elasticearch level.\n\nI'm passing :all_terms => true but not sure if that should include all terms?\n\nYes, that should, but beware of frying ES and your app when there are thousands terms to facet on.\nIs the problem solved? Closing the issue?\n. Thanks for the report! This has been fixed in karmi/tire@8fec4d8, closing here...\n. Hi, the problem was with the incorrect nesting of the facet_filter option.\nThe correct syntax is:\nruby\nTire.search('articles') do\n  facet('tags', facet_filter: { terms: ['foo', 'bar'] }) do\n    terms :tags\n  end\nend\nSee https://github.com/karmi/tire/blob/master/test/integration/facets_test.rb#L49-60\nSee http://pastie.org/4231884 for full controller implementation.\n. Hi, the + is invalid in Lucene query syntax -- you have to escape it...\n. Hi, thanks! Logger injection is something of a sore spot still.\nNotice this works:\n``` ruby\nrequire 'logger'\nrequire 'tire'\nclass Logger\n  alias :write :<<\n  def sync=(*args)\n  end\n  def closed?\n  end\nend\nTire.configure do\n  logger Logger.new('ruby-logger-tire.log', 3, 1024)\nend\nTire.search do\n  query { string '*' }\nend\n```\nAnd also this:\n``` ruby\nrequire 'log4r'\nrequire 'tire'\nmylog = Log4r::Logger.new 'Tire'\nmylog.outputters = Log4r::FileOutputter.new 'Tire', :filename => 'log4r-tire.log'\nmylog.instance_eval do\n  alias :write :info\nend\nTire.configure do\n  logger mylog\nend\nTire.search do\n  query { string '*' }\nend\n```\nAnd also this:\n``` ruby\nrequire 'fileutils'\nrequire 'active_support/buffered_logger'\nrequire 'tire'\nmylog = ActiveSupport::BufferedLogger.new 'buffered-logger.log'\nmylog.instance_eval do\n  alias :write :info\nend\nTire.configure do\n  logger mylog\nend\nTire.search do\n  query { string '*' }\nend\n```\nNotice the silly monkeypatching with write, closed, etc.\nI think we should leave out the logger injection/definition to the user? Do you think you can improve the logger support to be more generic in this sense?\nOr should we just stick to the default Logger interface? Anybody has any strong opinion on this?\n. @1st8 Sorry for the delay -- yeah, I think my original code is probably a monument of overthinking it and using just the Logger makes sense. I'll try to look into it shortly!\n. Hi, the best solution for this use case is using aliases in elasticsearch, see eg. https://github.com/karmi/tire/blob/master/test/integration/index_aliases_test.rb#L66-118\n. Yeah, @vhyza is right, we have to augment your AR/AMo models with the hits to support something like Sunspot's each_with_hit.\n. Hi, good catch with the typo, but the index.html file is generated by Rocco from the  examples/tire-dsl.rb file...\n. Hi, you may wanna check out these examples, https://github.com/karmi/tire/pull/92#issuecomment-6929988\n. >  Closer examination in Head revealed that some records were marked \"_version: 2\"\nThe only possible explanation is that some records overwrote other ones. If you're absolutely sure your IDs are unique, there may be a bug in Tire, however, I've never encountered it. Check your STDERR, Tire should print out a warning when records don't have an ID.\n\nRegion.index.import Region.all\n\nSo everything was imported correctly from the console, but not from the Rake task?\n. @zunger Thank you very much for such a detailed bug report. It definitely looks like there's something wrong with the pagination support for import -- I'll run some tests with couple of tens of thousands of data.\nThe MyModel.import method just calls the Tire::Index#import method, so something fishy must be there, but nothing sticks out for me right now...\n. OK, I'm afraid this slipped through cracks. In order to be on the same page, could you all please generate a fresh Rails app with Tire support:\nrails new searchapp -m https://raw.github.com/karmi/tire/master/examples/rails-application-template.rb\nThen add this to your db/seeds.rb file:\nruby\nputs \"Creating 10,000 articles:\"\n(1..10_000).each_with_index do |title, i|\n  Article.create :title => \"Title #{title}\", :content => 'Lorem', :published_on => i.days.ago.utc\n  print '.'\nend\nNext, run:\nrake db:drop\nrake db:migrate\nrake db:seed\nAnd then import them with the Rake task:\nrake environment tire:import CLASS='Article' FORCE=1\nIn my case, all 10,000 records are imported:\ncurl http://localhost:9200/articles/_count\nBased on the outcome in your case, we can start debugging that.\n. I would like to solve this issue. When you have time, try the steps outlined above, and we can re-open the issue.\n. Allright, several remarks to your issue.\nFirst, you're right that the query method does \"stomp\" on the previous calls, breaking chaining. This should be fixed, but does not prevent the behaviour you're after.\nSecond, @hajder is right that the correct approch would be to build your query inside the boolean block. To slightly adapt your example, this would probably get you where you wanna go:\n``` ruby\nrequire 'tire'\nrequire 'active_support/core_ext/string'\nTire.index 'members' do\n  delete\n  create\nstore type: 'member', first_name: 'John', last_name: 'Smith',       zip: 40202\n  store type: 'member', first_name: 'Mary', last_name: 'Smithsonian', zip: 40303\n  refresh\nend\nsearch_criteria = {}\nsearch_criteria[:names] = \"Smith~\"\nsearch_criteria[:zip]   = \"zip:40202\"\ns = Tire.search 'members' do |search|\n search.query do |query|\n   query.boolean do |b|\n     b.must { string search_criteria[:names] } unless search_criteria[:names].blank?\n     b.must { string search_criteria[:zip] }   unless search_criteria[:zip].blank?\n   end\n end \nend\np s.results.map(&:last_name)\n=> [\"Smith\"]\n```\nThird, notice how we might clean up  \u2014 and, moreover, decompose the query definition \u2013, when we define the boolean queries as lambdas, sometimes overlooked, yet very powerful feature of Tire:\n``` ruby\nThis is our Rails's params\n\nparams = {}\nparams[:name] = \"Smith\"\nparams[:zip]  = \"40202\"\nDefine query for names\n\nname_query = lambda do |b|\n  b.must { string \"#{params[:name]}~\" }\nend\nDefine queries for ZIPs\n\nzip_query = lambda do |b|\n  b.must { term :zip, params[:zip] }\nend\nCombine our queries\n\ns = Tire.search 'members' do |search|\n  search.query do |query|\n    query.boolean &name_query unless params[:name].blank?\n    query.boolean &zip_query  unless params[:zip].blank?\n  end\nend\np s.results.map(&:last_name)\n=> [\"Smith\"]\n```\nSo, the sum up: you started on a right track with multiple calls for \u201csomething\u201d, but that something should be the boolean method, not the query method.\nIs this sufficient for you to solve the issue you had?\n. @rbeene Ping, any luck?\n. @rbeene Yes. I'm not entirely sure you've catched my reply above? :)\n. @rbeene Ping, any news?\n. @rbeene Closing the issue for now. I hope your problem was solved and you just forget to close it yourself.\n. No worries -- good it's working.\n. Thanks!, merged and please disregard my earlier comment :)\n. Hi, this sounds strange.\nFirst, when overriding to_indexed_json, you're effectively taking care of the serialization yourself, so do check that the JSON makes sense (eg. compare to JSON without the override).\nSecond, based on the Google Group discussion, you need to set include_root_in_json = false.\nThird, how is the highlighting \u201cbroken\u201d in fact? As in no highlighting is returned, for any field?\nThe bad news is that this sort of complex object relationships are hard to debug -- I'd need effectively to recreate the full picture of your app to be able to run it...\n. Well, class Item is kinda dubious name in Tire, since that can clash with Tire::Results::Item. Not sure if that's your problem or not...\n. > What magic happens, when I reopen console? :)\nLots of possible culprits, Rails' autoloading, etc...\nThe correct way to create a model index is either:\n```\nNOTE: Will not create an index if it exists.\nMyModel.create_elasticsearch_index\n```\nor, more manually:\nMyModel.index.create mappings: MyModel.mapping_to_hash, settings: MyModel.settings\n. Hello, really can't reproduce the issue, see working code below:\n``` ruby\nrequire 'tire'\nTire.index('municipalities-test').delete\nclass Document\ninclude Tire::Model::Persistence\nindex_name 'municipalities-test'\nmapping do\n    indexes :id,            index: 'not_analyzed'\n    indexes :content_url,   index: 'not_analyzed'\n    indexes :content,       analyzer: 'snowball'\n  end\nproperty :id\n  property :content_url\n  property :created_at\n  property :content\nend\nmapping = Tire.index('municipalities-test').mapping\np mapping\nputs \"ID is not_analyzed:    %s\" % [ mapping['document']['properties']['id']['index'] == 'not_analyzed' ]\nputs \"Content is snowballed: %s\" % [ mapping['document']['properties']['content']['analyzer'] == 'snowball' ]\n```\nClosing for now, feel free to reopen or submit new issue if you feel there's a problem.\nAlso, the long code listings make the issue hard to digest -- better use gists/pasties/etc in a case like that.\n. Hi, thanks!, unfortunately an exact duplicate of karmi/tire#300...\n. Thanks, I'll make sure to link to your patch in the commit message at least...\n. Hi, I'm afraid that the outlined approach won't work 100% well, for the time being.\nEffectively, you'll be serializing searches defined in the Tire DSL to Hashes, and then using completely different interface for performing the searches: using these Hashes instead of DSL. There are subtle discrepancies between those two interfaces at present, unfortunately.\nMoreover, the MyModel.search API works either with simple query strings or a full-fledged block, forcing you to use the Tire.search interface to perform \u201cfreeform\u201d queries and programming the database fetching etc. on top of that.\nA better approach would be to identify and store search parameters for the given search request, and passing them to prepared search definitions. One, often overlooked, strong point of Tire are boolean queries, which allow you to decompose larger queries into smaller chunks, using lambdas for defining the components.\nBased on the code snippet you've posted, it seems like this is something viable in your scenario -- in one of my apps, we're doing something similar, extracting the search definition into a Module containing main_query, sources_query, keywords_query, highlight_options, etc. methods. We then mix this module into various models and other classes where we need the search functionality. The module relies on a simple contract on the part of the containing component, such as access to params or account data structures. Have a look at this gist for inspiration. This way, the account information (specific keywords, tags, ...) is persisted in the database and parameters (eg. freeform fulltext queries, sorting, ...) are passed on the fly and can be serialized quite easily as JSON (and loaded back from a database).\nThe design is guided by the classic \u201cSeparate things that change from things that stay the same\u201d principle; what varies, usually, are different parameters for fairly static patterns. I don't know if the idea applies to your scenario, but hopefully it's some food for thought.\n. > We basically are using tire for everything, \nCool :)\n\n(...) which means we're dealing with a lot of variations in queries, mostly in the search filters rather than query booleans. We're using filters (term/terms, numeric_range, mostly) for most of our stuff\n\nWhen you use mostly filters, I think the whole serialization/deserialization story gets a bit simpler, since filters are effectively just tight wrappers around Ruby Hash. You need to keep track of the field(s) you're filtering on, and the Hash payload -- and that is quite easily stored in eg. Redis or elasticsearch itself.\n\nWhat I think I need to do is serialize the hash/store it, and then from that query, regenerate a new query using the DSL and the information from that stored hash.\n\nYes. Given it's filters, should be doable very well.\n\nThe controller can still handle inserting the user-specific stuff into a search.\n\nExactly, like getting current_user and fetching her \u201cpreferences\u201d for filtering, etc. It would be easy to implement something like a \u201csaved search\u201d feature (a la Apple's \u201cSmart Folders\u201d) in this way.\n\nThe sorts of things where the serializable search is needed is, for instance, on drilldown into a result page, users can navigate within their results (combined with their account-specific query inclusions) from that page.\n\nI'm not sure I understand the pagination part of the whole story, but then again, it's just parameters to the search method (or the size/from methods).\n. @JamesHarrison Closing or...?\n. Yes, it works:\n``` ruby\nrequire 'tire'\nrequire 'active_support/core_ext/numeric'\nrequire 'active_support/core_ext/time/zones'\nTire.configure { logger STDERR, level: 'debug' }\nclass Time; DATE_FORMATS.update lucene: \"%Y-%m-%dT%H:%M\"; end\nTire.index 'venues' do\n  delete\n# 1) Create the index with proper mapping for the location property\n  create mappings: {\n    venue: {\n      properties: {\n        location:   { type: 'geo_point' },\n        updated_at: { type: 'date', format: 'date_hour_minute' }\n      }\n    }\n  }\n# 2) Store some example documents of the venue type\n  store type: 'venue', name: 'One',   location: [40.1, -70.1], updated_at: 3.hours.ago.utc.to_s(:lucene)\n  store type: 'venue', name: 'Two',   location: [40.2, -70.2], updated_at: 2.hours.ago.utc.to_s(:lucene)\n  store type: 'venue', name: 'Three', location: [50, 15],      updated_at: 1.hour .ago.utc.to_s(:lucene)\nrefresh\nend\ns = Tire.search 'venues' do\n# 3) Search based on geo distance\n  filter 'geo_distance', distance: '100km', location: [40, -70]\n# 4) Sort by distance to origin\n  # sort { by :_geo_distance, location: [40, -70] }\n# 5) Sort by updated_at property\n  sort { by :updated_at, 'desc' }\nend\nputs s.to_curl, '-'*80\ns.results.each do |d|\n  puts \"* #{d.name} | location: #{d.location}, updated_at: #{d.updated_at}\"\nend\n```\nTwo notes:\n- Without posting your mapping and more information, it's hard to understand your problem.\n- The interplay between mapping and searching is very subtle -- I advice to check the responses you get from ES with eg. Tire.configure { logger STDERR, level: 'debug' } whenever you encounter problems.\n\nLink to gist> https://gist.github.com/karmi/1051213\n. Hi, there are several issues going on with your question.\nFirst, this answer at StackOverflow gives a good overview of decisions behind the mapping/to_indexed_json method combo.\nSecond, I don't feel that adding more API to the mapping DSL would help here -- as previously stated, let's not pretend the library will solve every problem for you. It tries very hard to provide convenient starting point and sane API to work with, but it can't anticipate every need.\n(As a side note: what you're after is in fact achievable in Tire. Just rip open the class and access the @mapping variable and do whatever you feel is right.)\nLastly, and more importantly, the mapping method serves one purpose only: to express mapping definitions.\nThe JSON serialization is a completely different thing. You can start with the provided defaults, and once you gain confidence and need more advanced stuff, you're free to define whatever JSON serialization makes sense for your documents with a custom to_indexed_json method.\nFor instance, ActiveRecord's to_json provides you with a very rich API to express complex object serialization rules with all the :except and :only and :methods options, which take a simple Array as value.\n. @timuckun Ping, is the comment helpful?\n. > the mapping still has to take place in one shot.\nYes, absolutely, which is inline with how elasticsearch works.\n\nI was hoping I could incrementally define the mapping (...)\n\nI'd be quite scared about \u201cincremental\u201d mapping scattered around in many files, somehow joined to form a mapping for the index...\n\nFor now I have decided to avoid manual mapping and just rely on to_index_json.\n\nAgain, I don't understand that -- you have to use both to effectively control what and how gets into your index.\nClosing this issue or?\n. There isn't -- issues are perfectly fine for a discussion like this. For more conceptual questions, there's also StackOverflow.\n. Hi, ES returns the ID as a String since usually IDs in ES are in a format like abc-123-defQWZ etc. The choice of _id over source.id is just that it seems like a safer bet.\nNormally, this should not make mess up with ActiveRecord.fine etc -- could you post a snippet of code how are you comparing the objects?\n. @jtherrell Hello, terribly sorry for the silence... I understand the issue, but the problem is that Tire should really use the document ID as authoritative, not the _source.id value. Moreover, when using the :load option, you can disable loading _source altogether, since you're getting data from the database -- again, much cleaner to just get document ID then having to add fields: ['id'].\nClosing this for now, since it's so old... Feel free to re-open if you feel it's needed!\n. Hi, thanks, looks like there may be a bug -- could you please a unit test demonstrating the issue?\n. @mriley Is it possible to add a unit test for the fix? If you'd need some help with that, ping me.\n. Bump @karmi.\n. @mriley No no, this is just a reminder for me to have a look at it soon :)\n. @mriley Finally closed, sorry for the embarrassing delay with this. I've changed the test case, extracted it into a separate context -- it does not run all the tests, but since the bug is in the initializer, it should catch it fine.\nI'm not worried about backwards incompatible change if it makes sense. Setting up the index/search routings should be just:\nTire::Alias.new name: 'index_anne', ..., index_routing: '1', search_routing: '2'\nAlso, the search_routing() and index_routing()  would have to be added.\n. @woahdae Errr, sorry for the delay -- it absolutely makes sense to allow passing arguments, thanks for that.\nChanged the argument name to options to be more consistent, and also used the boost example from elasticsearch's docs, pushed.\n. Lee, my apologies for the delay -- thanks!, that of course makes perfect sense.\nFixed some minor issue with a Ruby 1.9-styled Hash, and also added to_hash so you can use Hash-like objects (Hashr, Mash, etc) as well.\n. @dementrock Why duplicate the question at Github and StackOverflow (http://stackoverflow.com/questions/11822019/using-elasticsearch-to-filter-through-tags-with-whitespace)?\n. I keep confusing \u201dfilter facets\u201d\u00a0with \u201cfacet filters\u201d no matter how I try :)\nSo, \u201cfacet filter\u201d works with support for full ES API, but \u201cfilter facets\u201d are indeed tied to term filter.\nThe change you're proposing is of course absolutely warranted.\nIt does not have to be breaking, by the way, because we can keep the current behavior (which makes a reasonable default?) and use a block syntax for DSL definitions:\nruby\nfacet 'received' do\n  filter do\n   filter  :term, :recipient_id => user.id\n  end\nend\nBut the duplication in filter is certainly ugly. I think we can introduce an API breaking change here; it makes more sense.\n. Should be working now, see integration test in https://github.com/karmi/tire/blob/68e474a/test/integration/facets_test.rb#L242-246 for an example.\n. Hi, thanks for the kind words, Tim -- I'm happy the library is useful at GitHub!\nI believe we already touched this issue previously. I agree that Rails 2.x support is highly desirable.\nI'd like to tackle this issue by doing a more invasive decomposition/extraction in the Tire codebase, though. Basically, I'd like to split the gem into several inter-related pieces: HTTP client/adapter, raw API matching elasticsearch's API, Ruby DSL on top of that, ActiveModel model integration on top of that, persistence layer on top of that, etc etc.\nThis should allow to use the API+DSL layer in any Ruby/Rails codebase, and the ActiveModel integration where possible.\nRegarding timeline for this refactoring, I think it should be doable within two or three months. Is that feasible for your use case?\n. To all interested: I would very much like the feedback on following questions:\n- How to make Item usable within Rails without ActiveModel? As soon as you do some require 'tire/model', you need it.\n- How to get rid of dependency on ActiveSupport? I hate this dependency as much as any other guy, but the problem is that eg. Hash#to_param is very convenient.\n- Why support integration tests on Ruby 1.8? Need a really convincing argument, there's a slim chance I'd vote for that.\n. @TwP Thanks for your input -- though I don't think you understood well what I have been in fact asking.\n. Hi, sounds probable -- is it possible to add a failing unit &\u00a0integration test for the behavior? \n. No worries, I will look at it myself; basically the ID needs to be URL-escaped in Index#store and then deserialized in proper places (retrieve, search, etc.)\n. Not enough time to solve this particular issue right now, I'm afraid. Will leave it open.\nThe workaround for the type being is:\n``` ruby\nrequire 'tire'\nTire.index 'testutf' do\n  delete\n  create\nstore id: Tire::Utils.escape('\u00e4ccentedtag'), content: 'UTF'\nend\n``\n. Closed, at last...\n. Hi, yeah, that seems to be missing. Normally themax_scorewould be the score of the first result item?\n. Normally, when sorting, the_scoreis not accessible, but I forgot you can force the_score` to be computed anyway. I'll look into this, that'll be an easy change.\nThanks for the kind words!!\n. Done, should work on master.\n. I wonder if Tire's update_index method clashes with ThinkingSphinx? I see the trace begins in thinking_sphinx/active_record/attribute_updates.rb? Try to include Tire after ThinkingSphinx.\n. Any news on this? Closing the issue for now, please try to get back to the issues you have submitted.\n. Hey, what's the motivation here? I mean, using Search::Search.new(ActiveModelArticle.index_name) is equally easy? \n. Maybe, but sorry, thins feels too heavy to me...\n. Hey, good idea, but I think we need a very good integration test as a demonstration of the feature (same as with nested queries...)\n. Hmm, good question. Not sure myself :) It just returns the collection, which is Enumerable-based/compatible, basically just because Rails calls to_ary in views, as far as I remember. It could probably return the underlying @results. Fancy a test case / run withing Rails?\n. The ActionView::PartialRenderer#collection calls to_ary when you render :collection => @results. I think it's safer here to return the wrapper, not the raw results array in case we need to decorate that. Also, don't have time to test it extensively in real-world apps. \n. Try to generate a fresh application:\nrails new searchapp -m https://raw.github.com/karmi/tire/master/examples/rails-application-template.rb\nand see if/how it works for you, so we are on the same page.\nSince you're using load: true, you receive back instances of your models. Hard to say what's wrong without a gist/pastie/etc of your model and the line which triggers the error from backtrace.\n. Err, sorry for the delay, closed in karmi/tire@1c16b68... I think the proper semantics is to pass exclude and include as options...\n. In that case, it's probably due to the subclassing...\n. Argh, I know why that is... The \u201cmasquerading\u201d Item does is only performed within Rails, so @search.results.first.load works only when a Rails constant is defined...\n``` ruby\nrequire 'tire'\nrequire 'active_record'\nTire.index('articles') { delete }\nActiveRecord::Base.establish_connection(adapter: 'sqlite3', database: \":memory:\" )\nActiveRecord::Migration.verbose = false\nActiveRecord::Schema.define(version: 1) do\n  create_table(:articles) { |t| t.string :title }\nend\nclass Article < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\ntire do\n    mapping do\n      indexes :title,   type: 'string', analyzer: 'snowball'\n    end\n  end\nend\nRails = nil\nArticle.tire.create_elasticsearch_index\np Article.create title: 'Testing...'\nArticle.tire.index.refresh\nresults = Article.tire.search('title:test').results\np results.first\np results.first.load(nil)\n```\nNot nice...\n. In the first case, you're sending an empty query (as far as I can see in the incorrectly formatted code). Should be the same in newer ES versions.\n. Absolutely, but not without breaking lot of existing code. I think we need to revisit all the silent handling errors in Tire, eg. in MyModel.create_elasticsearch_index...\n. karmi/tire@65cd994 and karmi/tire@352c208 improve it a bit...\nSee also karmi/tire#469\n. I believe you have to set include_root_in_json = false -- see README.\n. True, the README is probably incorrect for Mongoid.\n. I think your mapping didn't get through... Re-create the index.\n. @jasonfb The solution is to re-create the index with correct mapping. In development, usually the easiest way is to drop &\u00a0re-import the index.\n(Tire now has support for updating mapping, so on a large index, you can just update the mapping with the geo fields.)\n. @kenshin54 That does not make sense to me. When you want to do something like that, just use the regular Tire DSL/API, not the ActiveModel integration.\n. Maybe StackOverflow is a better forum for questions like these...\n. Yes, implemented in https://github.com/karmi/tire-contrib/blob/master/lib/tire/queries/more_like_this/more_like_this.rb. Thanks!!\n. Marcin, thanks!, very nice patch, cleaned up the integration test a bit and merged.\n. Hi, sorry for the delay, busy lately. I definitely like the direction and the behaviour, but would change something in the implementation a bit... Allow me couple of days to look at it...\n. Guys, see both commits added to this issue:\n- https://github.com/karmi/tire/commit/0ccee42ccfc1bba9c4264a8e9c56c3b4965cfb49\n- https://github.com/karmi/tire/commit/bbbf372a30e484856435ff50e73aaffeb6669a1d\nI'm leaning towards the latter. It is, though, based only on a gut feeling, not some hard-held opinion; I'd rather keep the behaviour open ended then be too strict about it.\n/cc @vhyza \n. Closing, going with karmi/tire@bbbf372 for the time being.\n. @ches I'm afraid I don't understand what you want Tire to do here. The behaviour is pretty expectable -- when you put as: Time.now, you're \"freezing\" Time.now and that's it. Of course, it should be properly documented in the code -- but definitely not in the README.\n. @foohey I don't understand your question at all, I'm afraid. Could you explain what are you trying to do?\n. > Because there is no docs/article on Tire for production environment,I ask the question.\nWhat kind of documentation for what kind of problems? \n. @AlainPilon Others are right -- in other words, when you're searching with field:query, your query is parsed with an analyzer matching the field, and thus stemmed. When you're searching for a query, it is parsed with an analyzer for the _all field, which by default does not do any stemming.\n. As always, check your mappings, gentlemen, and beware of the magical _all field...\n. Full changelog: https://github.com/karmi/tire/compare/8e81c3a...1c0d043\n. Rob \u2014 thanks, finally merged and cleaned up &\u00a0amended up a bit, released as 0.5.1.\n. I'd prefer to add a support like that into the https://github.com/karmi/tire-contrib gem, not into the gem directly. What do you think?\nWhile some out of the box support is definitely nice, people's use cases wary a lot and it's trivial to do setup/teardown manually..\n. Well, I think the docs say that you should use the option when you're providing the payload as @file. Nevertheless, the curl log is just informational as it is -- otherwise it would dump loads of payload data into your log... Leaning towards closing this?\n. OK, thanks, merged.\n. I don't think it makes sense to fiddle with injecting wrapper to search at the moment... Is your concern academic or are you seeing practical problems with wrapper being not thread safe? Could Thread.current hash be used to mitigate the issue?\n. @grantr Apologies -- didn't realized there was a patch attached while solving many issues at the same time.\nThis implementation is much cleaner then the original mess. Thanks!, merged.\n. Cool, thanks!\n. Hi, not absolutely sure what you're trying to do, but nesting booleans is indeed possible:\n``` ruby\nrequire 'tire'\nTire.configure { logger STDOUT }\nTire.index 'games' do\n  delete\n  create\nstore title: 'football', region: 'south' \n  store title: 'football', country: 'south'\nstore title: 'baseball', region: 'west'\nrefresh\nend\ns = Tire.search 'games' do\nquery do\nboolean do\n  must   { string 'title:football' }\n  should { boolean {\n             should { term :region,  'south' }\n             should { term :country, 'south' }\n           }\n         }\nend\n\nend\nend\np s.results.map { |r| \"#{r.title} (country: #{r.country}, region: #{r.region})\" }\n```\nClosing the issue for now, feel free to reopen if you have further questions.\n. Tire tries hard to not overload any existing methods, see https://github.com/karmi/tire/blob/master/test/unit/model_search_test.rb#L777-788\nDon't know what we could do better here?\n. Why? If the method exists, Tire won't overload it. If it does not exist, Tire will happily step in. This is something RSpec must deal with in lots of cases? Ie. what happens when you define a method index on your class?\n. Could you try it with defining your own index method in the model so we are sure it will break RSpec in the same manner?\nIf so, yes, putting up a Wiki page would be nice -- the remove_method trick is nice.\n. @sirn Thanks for the testing!! Please close the issue when you put up the Wiki page with the remove_method solution...\n. That is cool, thanks!\n. Just put the credentials into the URL:\nTire.configure { url: \"http://USERNAME:PASSWORD@example.com\" }\n. @azitabh Please make sure you close the issue when your problem is solved...\n. Best with something like:  \nArticle.tire.import( Article.scoped(:deleted) )\nOn Monday, 17. September 2012 at 5:56, Ma Chenlei wrote:\n\nI have a model named Article in my Rails 3, and I use rake environment tire:import CLASS='Article' FORCE=true to do index. But I only want the articles with field deleted equals to false be indexed. How can I implement it?\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/karmi/tire/issues/458).\n. Exactly. I've added the ability to define the default value as lambda (see the closing commit):\n\n``` ruby\nclass MyModel\n  include Tire::Model::Persistence\nproperty :created_at, :default => lambda { Time.now }\nend\n```\nNote though, that the lambda is evaluated when you create the object, not when you access the property.\n. Allright, sorry, took a while. The problem was in the Hash#delete, see commit, my bad.\nShould be serialized properly and behave properly now:\n``` ruby\n$LOAD_PATH.unshift File.expand_path('../../lib', FILE)\nrequire 'tire'\nTire.configure { logger STDERR, level: 'debug' }\nTire.index 'sentences' do\n  delete\n  create\nstore content: 'Everybody likes some fruits'\nrefresh\nend\nblocks = Tire.search \"sentences\" do\n  query do\n    boolean do\n      must { string \"fruits\"}\n    end\n  end\nend\npayload = Tire.search \"sentences\", {:query => {:bool => {:must => [{:query_string => {:query => 'fruits'}}]}}}\nputs \"blocks.to_json\",  \" #{blocks.to_json}\"\nputs \"blocks.to_curl\",  \" #{blocks.to_curl}\"\nputs \"payload.to_json\", \" #{payload.to_json}\"\nputs \"payload.to_curl\", \" #{payload.to_curl}\"\nputs '-'*80, 'Blocks results:'\nputs blocks.results.map { |r| r.content }.inspect\nputs '-'*80, 'Payload results:'\nputs payload.results.map { |r| r.content }.inspect\n``\n. Thanks!, merged.\n. Thanks!, cleaned up, add test and merged.\n. Closed in karmi/tire@0f38828\n. That would be indeed weird -- have a look at integration tests for the feature, https://github.com/karmi/tire/blob/master/test/integration/active_record_searchable_test.rb#L179-195\n. The:pageand:per_page` parameters are sugar provided by Tire, since that is how will_paginate-style of pagination works.\nAgain, please have a look at the test at https://github.com/karmi/tire/blob/master/test/integration/active_record_searchable_test.rb#L179-195, run it. Try to model your code based on that.\nClosing this for now, feel free to reopen or submit another issue if you indeed encounter a bug. A failing test would be very cool in that case.\n. UPDATE: The sugar only works when you're using the \u201csimple\u201d form of search -- as evident from the README. For block searches, use size/from.\n. @aledalgrande Added integration tests for the feature -- should work the same way for \"simple\" searches and \"block\" searches. I wonder if you see the same results because of scoring/ordering issue?\n. @jredburn Could you elaborate? The default per_page is 10, and :page should definitely work with whatever setting?\n. @jredburn Cool, a failing unit/integration test would be very cool.\n. Good catch!, that's the spot. I don't think there's a nice, clean way to get the per_page from the pagination module, but that's easily fixable by refactoring the magic number into a constant or module method.\n. @jredburn Added the test &\u00a0the implementation, should be working now.\n. Hi, this is certainly very weird... Could you post your mapping?\n. Actually no, it is not weird and it is not unexpected :)\nBecause you haven't specified an analyzer for your e-mail field, an address like user1@example.com ends up being split into: [\"user1\", \"example.com\"] when indexing -- and also when searching.\nYou can and should verify it with the Analyze API: http://localhost:9200/_analyze?text=user1@example.com&analyzer=standard\nThat means you're effectively searching for \"example.com\", and of course all your documents match such a query.\nThe solution in your case is to either set the e-mail as not_analyzed, or preferably, to use the multi-field feature.\n(Closing this for now, feel free to reopen if you have other problems.)\n. Hi, thanks for the report. This is pretty specific to ActiveRecord -- I think we could expose some ENV variable so you'll be able to pass the options to the includes method from the Rake task / import method?\n. @Rio517 Mario, just use Article.create_elasticsearch_index, which will create the index with proper mapping, as defined in the model. Closing it for now, OK?\n(I'm not really sure how to deal with these special cases for ActiveRecord in the Rake task... Maybe that's the point where you're better off writing your own importing procedure...)\n. Cross-check karmi/tire#530 for example with ActiveRecord#find_in_batches.\n. Hi, this depends on how exactly you put data in. So, I imagine, your model serialized as JSON looks somehow like this:\n{\n  \"title\" : \"10 Best jQuery Plugin for Web Design\",\n  \"tags\"  : [\"web design\", \"javascript\", \"jquery\"],\n  //...\n}\nThen your mapping is correct with index: not_analyzed. Faceting on the tags field should work fine, terms queries/filters should work fine.\nIt seems to me that your mapping is not properly applied -- do check the mapping for your index both in development and production  (curl localhost:9200/<YOURINDEX>/_mapping) and report back here.\n(Notice that the correct mapping is (re)applied when the using the tire:import FORCE=1 task, or when the application is loaded and the index does not exist.)\n. Cool!, the problem was obviously that you have kept around the index with incorrect mapping -- when debugging/developing, it's best to just delete/create repeatedly, which the FORCE=true option does for you...\n. For lots of cases, it makes sense to define the mapping and serialization inlined in the model, see https://github.com/rubygems/rubygems.org/pull/455 for inspiration.\nFor many other cases, it's good to define the mapping externally, eg. in a Ruby module defined in a separate file, and create the index manually, before deploying the app for the first time.\nHave a look at the \"index alias\" feature, which would allow you to recreate the index without downtime...\n. Hey, good idea to use the DSL for declaring facet filters -- the problem is unit tests are missing, and the feature must be documented in https://github.com/karmi/tire/blob/master/test/integration/facets_test.rb... Could you add those?\n. @NOX73 Merge hell, right :) Create a new branch from your original commit and do a git rebase master.\n. No :)\n1. Would be cool to add proper unit tests for the implementation, see search_facet_test.rb\n2. Notice the should \"allow to restrict facets with filters\" test in integration test. Something along the lines of \"should allow to define the facet filter with DSL\" is more proper. \n3. Commit should be squashed into one, please take care with naming, spelling, etc\nThanks!\n. Better, moved the tests to proper places and merged, thanks.\n. Yes, maybe, but definitely without the comment... Still on the fence with this. We don't want to explain everything about elasticsearch in the README, do we?\n. Should be really properly documented in https://github.com/karmi/tire/blob/master/lib/tire/search/sort.rb and https://github.com/karmi/tire/blob/master/lib/tire/search.rb#L60-63 ...\n. @mdpatrick Absolutely, if you can add the documentation into those places (preferably in YARDoc format, see eg. https://github.com/karmi/tire/blob/master/lib/tire/alias.rb)!\nI'm sorry but I don't wanna stretch the README too far -- some people have trouble reading it as it is :)\n. Absolutely -- though we also need to have some backwards compability, so it must be configurable. I agree the current default is painful...\n(In cases like this one, always check your mapping.)\n. @axsuul Sure -- just consult the ES logs and get the mapping and check it, either with Tire, curl or in your browser.\n@simoncozens is right, just enable logging, best with level set to debug, you'll see the error:\nTire.configure { logger STDERR, level: \"debug\" }\n. Sorry about that. Notice that karmi/tire@352c208 and karmi/tire@65cd994 improve the situation here.\n(Until I figure out how to deal with warnings vs. exceptions, there's just a warning being displayed.)\n. Hi, thanks -- any idea why it fails on TravisCI without the version? Can you ping them? I'd like to stay as far from gem versions as much as possible (even though this is only for development).\n. > Does it not fail for you locally? What if you delete your Gemfile.lock, bundle, then run the tests? \nNo. When I delete Gemfile.lock, delete all minitest and mocha gems, run bundle install and then run bundle exec rake test, everything runs smoothly. Could you run it locally on a fresh checkout, with gems deleted, etc?\n. @carols10cents Cool. Prefer to sort it that way then to pin gem versions -- been burned with gems doing that too often in the past...\n. Hi, got bit by the error just today. Pulled in your fix, to be on the safe side with Mocha and Minitest.\n. Hi, yes, makes sense -- will it work with other storage engines such as MongoDB etc?\n. Hi, will test it agains a Mongoid based app, so we don't break it for them. Should get to it this week.\n. Thanks!, verified, merged, pushed.\n. Thanks!, cleaned up a bit and merged.\n. Thanks, scandalous oversight on my part .)\n. @danielschlegel My first hunch would be that the mapping isn't really used for your index.\nCan you get the mapping and make sure the person_id is, in fact, a long? Get back to me here and we'll try to debug it further.\n. @danielschlegel No worries!, I'm glad the issue went away... The \"yellow\" state should not result in the error, AFAIK, maybe you can submit the issue to elasticsearch/elasticsearch? There may be an underlying problem there...\n. Hi, looks good! I think a nice integration test for this feature is needed as a documentation of how to use in in the DSL.\nPreferably with a good, non-synthetic example which demonstrate the pitfalls of not using nested query with \u201cobjects within arrays\u201d scenario, something along the lines of https://gist.github.com/4253dde6dd369979c4e9. Doable?\n. @unsay Not rushing you, just please ping me when you push changes so it doesn't get lost.\n. @unsay Cool!\n. @unsay Hi, merged &\u00a0pushed. I've retouched the code and tests a bit, and also decided to change the example in tire-dsl.rb, so it's more separated from the main example. Thanks!\n. Hey @jocke12, nice -- could you add a unit test for the added query, and possible a simple integration test demonstrating the behaviour?\n. @jocke12 Also, indeed, tire-contrib is a better place for the pull request, still need to maintain the gem slim at this point...\n. Closing, waiting for pull request on tire-contrib.\n. I really don't understand what you're trying to do here and what is the issue.\nWhen I look at http://localhost:9200/sub_things/_search, I can see correct number of documents and correct types for documents.\nWhen I query:\nSubThing.tire.search { query { all } }.results.size\nI see everything correctly once again.\nSo, please try to clear up the issue or close it -- SubThing.count is a MongoMapper, not a Tire thing, so I don't understand what's going on.\nP.S.\nIf you try to stay away of inheritance, your life in Ruby and Rails will be easier.\n. Closing due to no ping reply. Please re-open if you have more information.\n. Thanks for the clarification -- that is indeed very unfortunatel behaviour. Tire should use the instance.tire proxy in this case, and not access the model directly. I'll reopen the issue to keep it as a reminder.\n. Hi, this has been a tricky one. Should be fixed on current master, the update_index method has been extracted and refactored. No breaking changes should be introduced.\n. Argh, of course, you're absolutely right, merged+pushed, thanks!\n. Argh, sorry!, should be fixed.\n. Hi, thanks for looking into that so thoroughly! I consider the Rake task just a utility method, though, and don't think we should add some Utils namespace just for that...\n. Hi, you're absolutely right it should have been fixed. I just fixed the method in tasks.rb, see karmi/tire@6fef487.\nWhile the import Rake task is very useful, it's not a end-all solution and people should be more then ready to write their own feeder scripts; it's very, very convenient and easy to do so in Ruby. The Rake task is a mere convenience for simple cases and a help for beginners.\n. Yes, that's possible -- just not the way you go about it. You need to be smarter in the include Module hook:\n``` ruby\nrequire 'tire'\nrequire 'active_record'\nTire.index('articles') { delete }\nActiveRecord::Base.establish_connection(adapter: 'sqlite3', database: \":memory:\" )\nActiveRecord::Migration.verbose = false\nActiveRecord::Schema.define(version: 1) do\n  create_table(:articles) { |t| t.string :title }\n  create_table(:people)   { |t| t.string :title }\nend\nmodule Searchable\ndef self.included(base)\np \"Included in #{base}\"\n\nbase.class_eval do\n  include Tire::Model::Search\n\n  tire do\n    mapping do\n      indexes :title,   type: 'string', analyzer: 'snowball'\n    end\n  end\nend\n\nend\nend\nclass Article < ActiveRecord::Base\n  include Searchable\nend\nclass Person < ActiveRecord::Base\n  include Searchable\nend\nArticle.tire.create_elasticsearch_index\nputs \"Article mapping:\",\n     Article.tire.mapping,\n     '---'\nputs \"Create an article:\",\n     (article = Article.create title: 'Testing...'),\n     '---'\nputs \"Update index...\",\n     article.tire.update_index,\n     article.tire.index.refresh,\n     '---'\nputs \"Perform search:\",\n     Article.tire.search('title:test').map { |a| \"ID: #{a.id}, title: #{a.title}\" },\n     '---'\nThe same thing with people...\nPerson.tire.create_elasticsearch_index\nputs \"Person mapping:\",\n     Person.tire.mapping\n...\n```\n[UPDATE: Ruby script updated]\n. You're welcome -- I bet other people will find the approach sketched here useful\n. Please see the http://stackoverflow.com/questions/11692560/elasticsearch-tire-and-nested-queries-associations-with-activerecord/11711477#11711477 StackOverflow answer. You can then use a filtered query with a terms filter on the user_id.\n. For this use case, you need to create a custom analyzer with ASCII filter to transliterate Czech characters to ASCII versions.\n. Two points regarding this:\n1. Make yourself familiar with the \"Analyze API\" and use it to play around with the analysis process.\nIn this case, we can construct ad-hoc combinations of tokenizers and filters and see the impact:\nhttp://localhost:9200/_analyze?text=Sv\u011btlo&tokenizer=whitespace&filters=lowercase,asciifolding\nhttp://localhost:9200/_analyze?text=Espa\u00f1ol&tokenizer=whitespace&filters=lowercase,asciifolding\nDo note, that tokens won't be stemmed in this case.\n1. You may get better results by using the Hunspell analyzer for elasticseach, which would provide both stemming and ASCII transliteration.\nSee a complete example for the former approach below:\n``` ruby\nrequire 'tire'\nTire.index 'test-ascii-folding' do\n  delete\n  create \\\n  settings: {\n    analysis: {\n      analyzer: {\n        ascii: {\n          type: 'custom',\n          tokenizer: 'whitespace',\n          filter: ['lowercase','asciifolding']\n        }\n      }\n    }\n  },\n  mappings: {\n    document: {\n      properties: {\n        title: { type: 'string', analyzer: 'ascii' }\n      }\n    }\n  }\nend\nTire.index 'test-ascii-folding' do\n  store title: 'Sv\u011btlo'\n  refresh\nend\nputs \"ANALYZE:\"\np Tire.index('test-ascii-folding').analyze('Sv\u011btlo', tokenizer: 'whitespace', filters: 'lowercase,asciifolding')\np Tire.index('test-ascii-folding').analyze('Sv\u011btlo', analyzer: 'ascii')\np Tire.index('test-ascii-folding').analyze('Sv\u011btlo', field: 'title')\nputs '-'*80, \"SEARCH:\"\ns = Tire.search('test-ascii-folding') do\n  query { string 'title:svetlo' }\nend\np s.results\n```\n. hey, don't know what's going on here, tried it and it works for me with the analyze API:\n``` ruby\nencoding: utf-8\nrequire 'elasticsearch'\nclient = Elasticsearch::Client.new\nclient.indices.delete index: 'czechtest' rescue nil\nclient.indices.create index: 'czechtest', body: {\n  settings: {\n      analysis: {\n          analyzer: {\n              new_czech: {\n                  type: 'custom',\n                  tokenizer: 'standard',\n                  filter: %w(standard lowercase czech_stop czech_stemmer asciifolding)\n              }\n          },\n          filter: {\n              czech_stemmer: {\n                  type: 'stemmer',\n                  name: 'czech'\n              },\n              czech_stop: {\n                  type: 'stop',\n                  stopwords: ['czech']\n              }\n          }\n      }\n  }\n}; sleep 0.25\nclient.indices.analyze index: 'czechtest', text: 'medv\u011bda', analyzer: 'new_czech'\n=> {\"tokens\"=>[{\"token\"=>\"medved\", \"start_offset\"=>0, \"end_offset\"=>7, \"type\"=>\"\", \"position\"=>1}]}\n```\nBTW, Tire is retired and you should migrate to the new -ruby and -rails gems.\n. Congratulations, I think you just found an inconsistency in the Czech stemmer :) Compare:\nhttp://localhost:9200/czechtest/_analyze?text=\u017elu\u0165ou\u010dk\u00fd&analyzer=new_czech\n=>\u00a0zlutouck\nhttp://localhost:9200/czechtest/_analyze?text=zlutoucky&analyzer=new_czech\n=> zlutouck\nYou might want to have a look at the Hunspell-based analyzers, http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-hunspell-tokenfilter.html#analysis-hunspell-tokenfilter.\n. You should try it out locally, following the docs, verifying it works as you need and then contact support for your provider, if you're not hosting Elasticsearch yourself.\n. Try out current master, should be working.\n. Ha!, now that is a lovely edge case -- absolutely love it :)\nMy consience isn't really clear here: if the results method in the Collection class wouldn't be such a mess of >40LOC, it would be trivial for you to patch a single method, stick the patch in your lib and you'd be set...\nNow, calling to_i would obviously solve it, but Tire is designed from the outset to work with any ActiveModel compliant ORM/ODM/OxM, be it for MongoDB, CouchDB, Redis, etc. Converting to Integer would almost definitely break stuff for those adapters...\nLet me try some extraction first...\n. @ches See the attached commit, you should be now be able to override Tire::Results::Collection#__find_records_by_ids with something like:\nruby\ndef __find_records_by_ids(klass, ids)\n  @options[:load] === true ? klass.find(ids.map(&to_i)) : klass.find(ids.map(&to_i), @options[:load])\nend\nAbout raising the exception -- I need to think more about that in the larger scheme of the entire codebase.\nI agree nil is pure evil, but don't see immediately an easy way here -- try the override, it should solve both problems, and ping me with results, please.\n. @ches, please check out the the refactoring/results branch if it's OK for you to override the behaviour. Closing.\n. Hi, I think once you have special requirements like this one, it's better to take matters into your own hands then to rely on complicated behaviour and vast DSLs.\nIn this case, just don't include Tire::Model::Calbacks and store the record only when the condition is true:\n``` ruby\nclass Product < ActiveRecord::Base\nafter_save do\n    self.update_index if published?\n  end\nend\n```\n. >  I feel like there needs to be another abstraction on top of tire, which focuses on rails integration and popular use cases.\nThat sounds interesting -- could you give any examples? I mean, I am genuinely interested :)\n(The thing is, I feel Tire brings a lot of \u201csugar\u201d to Rails/ActiveModel world already. Based on my experience, it's usually much better to instruct people to \u201cjust use Ruby\u201d instead of trying to cover any imaginable use case, eg. your example with ignore_if...)\n. Thanks for elaborating on that, Maxim. It's an interesting point of view. I'd add couple of things:\n1. The mapping/to_indexed_json combo is miserable, agreed on that. What's more miserable is that to_indexed_json is not needed in most cases -- the mapping alone will do. See eg. https://github.com/karmi/rubygems.org/commit/fa7d1cd. I did the mistake to put it too prominently into the documentation.\n2. Disagree that Tire gives you the equivalent of elasticsearch DSL .) The ActiveRecord/ActiveModel integration is the sugar on top of the DSL. The integration should be more elegant, absolutely.\n3. What you're describing sounds like a bit like the approach ThinkingSphinx took. I did a lot of bad decisions when starting Tire, but still think that taking a different approach was good. See, with elasticsearch, what you've hit are \"tip of iceberg\" issues. When somebody is using elasticsearch, she must understand what the analysis process does, how tokenizers + token filters play together, how the queries, filters and facets play together, etc. I'd rather lace these bitter issues with a bit of Ruby sugar, then try to supress their taste altogether...\nThat said, I understand what you're saying and consider it interesting take!\n. Spot on :) What you're describing is how the future should be shaped.\n. I'm afraid I'm not 100% sure what you wanna accomplish here, but:\n1. It's possible to exclude certain fields from the returned JSON\n2. However, in your case, you can just not display it?\n3. I don't know if you wanna exclude these items from search (ie. a match on this number won't add the document to results), or something else altogether.\n. Still not sure I got you -- but if the only you want to do is to exclude the phone_numbers part of JSON, then use exclude. If you don't want your query to match on these phone numbers, use a nested query preferably.\nNotice that you're not giving any example of documents, example of results you'd like to see, etc. Also take note that this kind of high-level, open ended questions usually fits better at StackOverflow.\n. >  I have a field that needs to return a human readable time. \nTo clarify: you have eg. created_at field stored in elasticsearch, let's say 2012-10-23T12:00:00Z, and you want to display it in \"time ago in words\" format?\na) In a Rails application, that's the responsibility of a view layer -- you use the appropriate helper in your HTML template.\nb) If you want to return that as part of JSON API response, my first question would be why do that? The API should return data, not arbitrary strings.\nc) If you really wanna do that, create your own wrapper class which will wrap and decorate the \"raw\" created_at field.\n. Not sure it makes good sense, but I think what you want then is just:\n- calling map on results and injecting the humanized property into Item\n- using a custom wrapper which would automatically decorate the raw field (preferable)\n. Makes sense -- could you find a good spot where to add unit/integration test?\n. Thanks, merged.\n. Closed by 8fc5060.\n. That's quite an old version of elasticsearch -- please upgrade or use newer version locally, don't really remember the API changes. Ping back after testing with a more recent elasticsearch version!\nP.S.\nThe script runs fine locally for me...\n. @beunwa Cool!\n. Hi, thanks, but I think something like this needs a much more thorough approach... You should put it out as a gist and blog about it :)\n. AFAIK Tire should run fine on JRuby. On the other hand, I have absolutely no intention to make the test suite work, accomodating the gemfile for the different platforms (eg. SQLite), etc. There was a time I played with that (see karmi/tire@9f97d3f and other commits), but decided against it. You can always run the tests locally on JRuby, Rubinius, etc.\n. I don't understand what you want to do.\n. Sorry, don't really understand. You're filtering on account_id, which is fine for multi-tenant app. You may want to use a filtered alias (see \"Filtered Aliases\" in http://www.elasticsearch.org/guide/reference/api/admin-indices-aliases.html), so the filter is applied automatically.\n. Hey, when closing the issue, maybe update it with the explanation of what was wrong in your code or approach...\n. Thanks for the catch!, did it differently, but agreed this makes sense to allow...\n. Sorry, but I think everybody can just put those two lines in their setup/teardown...\n. Hi, I think a filtered query is best in your case -- maps nicely to the field type, can be cached in elasticsearch, etc.\nruby\ntire.search(load: true) do\n  query do\n    filtered do\n      query { string q, default_operator: \"AND\" }\n      filter :term, is_public: 1\n    end\n  end\nend\nP.S. Your original query was almost correct -- notice the bool query wraps multiple queries, like this:\nruby\ntire.search(load: true) do\n  query do\n    boolean do\n      must { string q, default_operator: \"AND\" }\n      must { should {string \"is_public:1\"}     }\n    end\n  end\nend\n\nUPDATE: Fixed incorrect filter syntax.\n. The filtered query is the right approach (filters run first, then queries) -- unless you need facets on filtered fields, which in your case you most probably don't.\n. Notice the error failed to find geo_point field [store] -- something is not right in your setup. Please post index mapping, how you index documents, etc. Please post it to somewhere else, not the issue body. \n. Crosspost from StackOverflow [http://stackoverflow.com/questions/13215295/why-is-the-geospatial-search-using-tire-for-elasticsearch-producing-a-searchphas], closing.\n. Please post your index mapping. (http://localhost:9200/stores-development/mapping)\n. Yes. It's inline with other Hash-like implementations such as _Hashie, Hashr, etc. Compare:\n``` ruby\n\n\nrequire 'hashie'\n=> true\nh = Hashie::Mash.new foo: 'bar'\n=> #\nh.foo\n=> \"bar\"\nh.moo\n=> nil\n```\n\n\nYou're welcome to post your concrete problem with this behaviour.\n. I understand what you're saying, Christopher. But I don't agree. The Item instances returned as results are \u201clookalikes\u201d or \u201cshadows\u201d of underlying models. They behave in a similar way as Hashes do. If you want to have your real model instances returned, use the :load option. If you want to have a different behaviour for result set instances, just use the :wrapper option.\n\nThis leads to obscure bugs (...)\n\nAgain, please post concrete examples. There aren't many which spring to my mind immediately.\n\ngiven some of the confusion around the way Tire represents these objects in a rails context (...)\n\nThere shouldn't be any -- Items masquerade as your models within Rails; if you don't like the behaviour, please use the :load option, or, if you feel like it, write a :wrapper customized for your situation.\n. Bump @karmi.\n. Probably -- try including the Tire modules after all other initializations are done, so it is kept within the tire namespace. Catch me in the #elasticsearch IRC to chat about it in more realtime.\n(No idea about the segfaulting though...)\n. @brandonmeeteor Ping, any news?\n. Tire, by default, does not overload already defined methods -- matches for percolator may be some rogue exception to that.\n. @brandonmeeteor The bugs with matches should be fixed now.\n. Debug the Rails.public_path+\"/assets/entries/#{document_file_name}\" path in console, or use puts within the method. Are you sure the file is there? Are you using some kind of asynchronous processing in Paperclip?\n. Check if Paperclip is handling the file save in synchronous/asynchronous way -- if it's the latter, you'll need to use some kind of observer, publish/subscribe mechanism to intercept when the file is saved and then call update_index.\nUPDATE: Cf. https://github.com/thoughtbot/paperclip#events\n. @rcalderon Ping, any news?\n. Most probably your elasticsearch server is down or unreachable. What did you searched &\u00a0found? Is elasticsearch running at http://localhost:9200 or on the URL you have passed to Tire.configure?\n. Tire is not elasticsearch -- you have to start the server either manually or at system start (with launchd on Mac OS X, /etc/init.d scripts on Linux, etc).\n. No, only the example template does this, for convenience. \n. Try User.tire.search :routing => 2 and check back. Also, please post your mapping in a gist/pastie/etc to see if the _routing value has been picked up.\n. See the full code for an ActiveRecord model with routing below (use the master branch):\n``` ruby\n$LOAD_PATH.unshift File.expand_path('../../lib', FILE)\nrequire 'tire'\nrequire 'active_record'\nActiveRecord::Base.establish_connection( adapter: 'sqlite3', database: \":memory:\" )\nActiveRecord::Schema.define(version: 1) do\n  create_table :my_models do |t|\n    t.string   :title\n    t.string   :account_id\n  end\nend\nTire.index('my_models').delete\nclass MyModel < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nmapping _routing: { required: true, path: 'account_id' } do\n    indexes :title\n    indexes :account_id, type: 'string', analyzer: 'keyword'\n  end\nend\np MyModel.create( title: 'Doc for account 1', account_id: 1 )\np MyModel.create( title: 'Doc for account 2', account_id: 2 )\nMyModel.index.refresh\nTire.configure { logger STDERR }\nputs \"Doc 1:\", MyModel.search(routing: 1) { query { all } }.first.inspect\nputs \"Doc 2:\", MyModel.search(routing: 2) { query { all } }.first.inspect\nputs \"\",  MyModel.search(routing: 3) { query { all } }.first.inspect\n``\n. No, and theaccount_idshould _not_ be in the_all` field.\nPlease try out the code I posted and work from there.\nBeware of a query like you do (e-mail adress), could be analyzed in a surprising way. Also, the match query -- newly added -- is better as a general purpose query.\n. > rake environment tire:import CLASS='User'\nThe Rake task does not append any routing info to the requests, and at the moment bulk_store does not support routing, should be fixed soon.\nTo try it out, just save the records individually, working with the example I posted.\n. Your setup with a required mapping declaration and automatic path extraction is the right way to go.\nThe bulk support in Tire has been significantly improved in the 0.5.1 release, see changes: https://github.com/karmi/tire/compare/4e85109...1c0d043. You can now use the Index#bulk method to store documents with parent/child relationships.\nUse the new support to write your own indexing logic, possibly taking inspiration from the import infrastructure. Closing this issue, open a new one if you hit any problems.\nOnce again, please pay close attention to the working code with an isolated example I have posted above.\n. Weird -- please try to run the https://github.com/karmi/tire/blob/master/test/integration/mongoid_searchable_test.rb test and report back the status. Should run fine.\n. Also, please give a relevant part of the error backtrace, ie. the error being traced to some line in Tire codebase -- the error by itself does not say much.\n. 1. Git clone the project and cd into it\n2. Run bundle install\n3. Run bundle exec ruby -I lib:test test/integration/mongoid_searchable_test.rb\n(Run bundle exec rake test to run all the tests.)\n. @darrenboyd All right, so it happens when you have Tire.configure { logger STDERR, level: 'debug' } set, right?\n. Hi all, thanks for the debugging! It's indeed a case of empty string, see following isolated test case:\n``` ruby\nrequire 'json'\nrequire 'oj'\nrequire 'yajl'\nbegin\n  puts \"Yajl:\" + Yajl::Parser.parse('').inspect\nrescue Exception => e\n  puts \"Yajl exception:\", e.inspect\nend\nbegin\n  puts \"Oj:\" + Oj.load('').inspect\nrescue Exception => e\n  puts \"Oj exception:\", e.inspect\nend\nbegin\n  puts \"JSON\" + JSON.parse('').inspect\nrescue Exception => e\n  puts \"JSON exception:\", e.inspect\nend\n```\nShould be fixed in the closing commit, could you test it?\n. @darrenboyd Thanks!, solved it a bit differently, but the principle stands -- check for actually having something in the response body.\n. Filtered alias with routing is you best option. You can combine it with a \u201csliding window\u201d scenario, where you don't keep all the data, but only eg. last three months. See the http://www.elasticsearch.org/videos/2012/06/05/big-data-search-and-analytics.html video for explanation of possible approaches.\n. 1. Do not rely on automatic index creation for advanced workflows &\u00a0situations.\n2. Prepare index in advance, create alias, use MyModel.index_name to use the alias.\n. Less resource intensive then having an index for each account.\n. 1. once you start using routing, you have to use the routing parameter everywhere: indexing, searching, ...\n2. I hope you're using add_alias in your model only as an example. In a real situation, you'll create an alias when a new account is succesfully created (paid/etc), and you can't use MyModel.index_name for searching. \nThe discussion in the karmi/tire#92 is relevant to the problem you're having.\n. All filters should be supported, they're just tiny wrapper around hashes, see https://github.com/karmi/tire/blob/master/test/integration/filters_test.rb\nThis is a good place to ask questions, StackOverflow works fine as well, though it's better for higher-level questions in my opinion.\nTire questions can absolutely be also sent to the main ES mailing list, https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch\n. @azitabh Hello, any news?\n. @azitabh No worries, good it's resolved!\n. Dave, a rare foresight these days, thanks! :) You're right, I think the it would be sufficient to just mention Kaminari in the relevant README paragraph:\n\nNote that Tire search results are fully compatible with will_paginate (...)\n\nWhat do you think?\n. I like the wording! I'm not sure about the examples -- the Rake task injects the paginate method when it detects Kaminari... Definitely go forward with it.\n. @daveworth One of the things I'm working right now is re-considering and re-writing the Tire documentation. I'll keep this discussion in mind -- should we close this issue?\n. Hi, sure -- but the problem is that find_in_batches is ActiveRecord specific, and there's no similar shared interface for ActiveModel. I'm wary about using ActiveRecord specific semantics...\nWhat you describe is a perfect approach -- which should be used in client code, not in the library, in my opinion.\nIt would be hard to support every corner-case in the library and generic tasks, while it's very easy to write glue code like this to support specific cases.\nWhat do you think?\n. Documentation> http://api.rubyonrails.org/classes/ActiveRecord/Batches.html#method-i-find_in_batches, http://guides.rubyonrails.org/active_record_querying.html#find_in_batches\nAlso check> karmi/tire#530\n. Hey, this is strange -- the problem is of course the time is incorrectly serialized into JSON, however, the behaviour is the same for me on master (0.5.0) and 0.4.3:\n```\ngit checkout master # karmi/tire@463ad4d\nruby -rubygems -I lib -r tire -e 'puts Tire::VERSION; puts Tire.search(\"articles\") {filter :range, published_on: { gte: Time.now }}.to_curl'\n0.5.0\ncurl -X GET 'http://localhost:9200/articles/_search?pretty' -d '{\n  \"filter\":{\n    \"range\":{\n      \"published_on\":{\n        \"gte\":1352627975.257754000\n      }\n    }\n  }}'\ngit checkout v0.4.3\nruby -rubygems -I lib -r tire -e 'puts Tire.search(\"articles\") {filter :range, published_on: { gte: Time.now }}.to_curl'\ncurl -X GET \"http://localhost:9200/articles/_search?pretty=true\" -d '{\"filter\":{\"range\":{\"published_on\":{\"gte\":1352628020.300750000}}}}'\n``\n. True,YAJLencodes the date correctly,oj` as a floating point number:\nruby -rubygems -I lib -r tire -r yajl/json_gem -e 'puts Tire.search(\"articles\") {filter :range, published_on: { gte: Time.now }}.to_curl'\ncurl -X GET 'http://localhost:9200/articles/_search?pretty' -d '{\n  \"filter\": {\n    \"range\": {\n      \"published_on\": {\n        \"gte\": \"2012-11-11 14:57:48 +0100\"\n      }\n    }\n  }\n}'\nI'd be very cautious about pre-processing values before they are JSON-ified (potential performance issues, etc), but clearly, we have to deal with a situation like this. From the usability standpoint, you certainly want to write your query just as you did, no matter what JSON library you use...\n. @weynsee Sorry for the delay. I think JSON libraries usually just ignore the options sent to them, so if this fixes the issue with oj, we should go for it... Can you quickly check how it behaves when you use eg. yajl or the default json in your code?\n. @weynsee Thanks for the debugging! I'll look into the places we need to add the :xmlschema serialization.\n. @jeremygpeterson @weynsee Is the issue gone with the latest multi_json release?\n. Seems to be fixed now:\n```\n\n\nrequire 'multi_json'\nMultiJson.use :oj\nMultiJson.encode Time.now\n=> \"\\\"2013-03-07 17:13:01 +0100\\\"\"\n```\n\n\nNotice oj doesn't handle Time well by default:\n```\n\n\nOj.dump(Time.now)\n=> \"{\\\"^t\\\":1362672809.122553000}\"\nOj.dump(Time.now.iso8601)\n=> \"\\\"2013-03-07T17:13:44+01:00\\\"\"\n``\n. Thanks!, ashamed :)\n. Sorry about this -- it is now possible to pass therouting` value explicitely:\n\n\nTire.index('myindex').store( {:id => 1, :title => 'Test'}, {:routing => 'abc'} )\nWhen you use the mapping :_routing => { :required => true, :path => :account_id } configuration, the routing information is parsed from the document itself, no need to pass it in URL params.\n. Hmm, this sounds tricky. Namespaced models are always a bit tricky with Rails.\n1. Could you please pastie a full backtrace of the error, so I can see where it's being triggered?\n2. Could you just manually require that file in your app initializer etc., so it's already loaded when the Rails auto-loading mechanism tries to access it?\nThere may be an error in the camelize.constantize code in the Item class.\n. @cpuguy83 Hey, any news on the issue? Solved, or should we continue debugging it?\n. @cpuguy83 I'm afraid not much can be done in Tire here -- Item uses the default constantize to get the class name from the document _type. It could be possible to override Tire::Collection#__find_records_by_ids. Catch me in #elasticsearch on Freenode so we can debug it in realtime.\n. @cpuguy83 Hello, any news on the situation?\n. @cpuguy83 So, in favour of closing?\n. > In my multi-tenant app (account based with number of users per account), how would I update index for a particular account when a user document is changed.\nI can't say I understand 100% what are you trying to do, but basically, for models, just call update_index, it does honor the index/type structure, or just use Tire.store with a proper index name and document type.\n. Since you have a separate index for each model, just call @record.tire.update_index. It will pick up whatever index name you've set in the class.\nOtherwise, use Index#store to roll your own indexing strategy. If this is still about the \"multi-tenancy\" aspect, I really don't understand what you're doing, if you have an index for each account, if you use aliases, etc.\n\nThe index name may vary in my case. (...) The index name isn't dynamically generated one in my case.\n\nI don't know what you mean. You're refering to dynamic index names by passing a block to index_name, eg. ActiveRecordClassWithDynamicIndexName in test/models?\n. Hello, sorry for the delay.\n\nI have a separate index for each account, in which the mappings for each model (user and comments) are specified. In this case if any change has been done for user model or comment model, the index that has been created for the related account has to be updated. Is this possible? Please let me know if yes.\n\nThis is a tricky thing to do, the only real solution I have found in several projects is to just \"switch\" the index/database name based on some criteria in your application (logged-in user, etc).\nPlease see the karmi/tire#92 issue for a lot of Tire historical trivia, there is a solution outline at the end as well.\n\nWhenever a new user is added or if an user is updated the index created for the corresponding account has to be updated.\n\nYes, this is something you need to handle in your application.\n. Hello, had a look at the solution?\n. @ovamsikrishna Hello, any news? Closing?\n. > It works, but I don't think this is the right way to do this. \nIt's not :) You're effectively reindexing the whole index on each save. Either use the include Tire::Model::Calbacks support, or just trigger update_index manually in the hook -- see Readme.\n(Post more code+info if the situation is more complex.)\n. Hi, are you sure the after_save callback is triggered? Also, definitely switch from the import call to just updating the index for that specific model.\nAnd finally, could you describe the error in more detail? All of your records are indexed, but one is not? What if you try to save that record eg. in a IRB session / Rails console, does it trigger any error?\nI don't see any obvious error in your code -- for most people/situations, just include Tire::Model::Callbacks is enough to get it working.\n. @yeehaa123 Hello, any news?\n. Hi, I understand the intention, but isn't it better to stub the callback here, based on the fact that you are actually executing it in the code?\nAlso, I'm not a fan of polluting the namespace any more then is neccessary...\n. > How can i know if i need to refresh the index or not?\nI'm sorry, but I don't understand 100% what are you trying to do here: if the model has Tire integration, and you have an integration test, you just need to refresh the index so data inserted in test setup is visible. For unit tests, just use the build, not the create strategy?\n. I'm all for it once the Travis status images are not as ugly as they currently are...\n. Hi, that's really a complicated question:\n1/ You have several strategies for multi-tenancy, depending on the volume of your data and use-case. You can separate users by indices, or use a document property such as account_id.\n2/ You can use index aliases to handle the situation opaquely to the application -- and index alias can contain routing and filters, so it's a \"logical\" index. Please search Tire's issues, test suite, and the web for context.\n3/ You can use the \"required routing\" feature to embed the routing factor in the document, see eg. issue #506\nAlso, make sure to see the following video: http://www.elasticsearch.org/videos/2012/06/05/big-data-search-and-analytics.html\n. @amejiarosario Hello, any news?\n. Closing as abandoned.\n. Finally found time to evaluate and refactor the patch, sorry for the delay! Merged &\u00a0pushed.\n. Hi, thanks! Two comments:\n1/ We should support all the options for the Count API, notably the ability to pass a query. \n2/ The count is already supported, via the search_type parameter. This is generally preferable in elasticsearch, so maybe Index#count should be just a nice wrapper for that?\n. Decided it actually makes more sense to define a separate API/DSL for the Count API, so complicated it a bit :)\nThanks!, the feature should be compatible with your patch.\n. @danielschlegel What kind of ActiveModel integration is your class using?\n. @danielschlegel Then you have to bring the stuff provided by ActiveModel in, manually. Have a look eg. at the ActiveModelArticle PORO in the test suite.\nAlso, do note, that you don't have to work with Tire via the full model integration -- you can just use Tire.search and use a custom wrapper. It could be a viable solution for your use-case, maybe?\n. @danielschlegel Cool!\n. It is more like a violation of \"principle of least surprise\" then bug .) I think your index has been created previously, with a different mapping. Then you have changed the mapping, but you have to recreate the index in this case.\nTry out the following code, which works for me:\n``` ruby\nrequire 'tire'\nTire.index('my_documents').delete\nclass MyDocument\ninclude Tire::Model::Persistence\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nmapping do\n    property :title, type: 'string', analyzer: 'snowball', boost: 10\n    property :location, type: 'geo_point'\n  end\nend\nputs RestClient.get( \"http://localhost:9200/my_documents/_mapping?pretty\" )\n```\nWe can work from there.\n. @lister Hello, any news on the issue?\n@ches Yes -- the former is just a reference to the Tire#index for the model, without any settings. I do agree it'd be nice to take any settings and mappings set in the model into account.\n. @pangkalizer What \"same issue\"? Please see the code I posted above, try it out, and try to replicate with your codebase. Then please post info + code here. (To reiterate: make sure you re-create the index, make sure you post your index mapping.)\n. @lister Has the issue been successfuly resolved for you?\n. @lister You can just drop the index and then call MyDocument.create_elasticsearch_index.\n. >  however once you populate the index and pass hash value to one field causes dynamic mapping\nYes, that is the default behaviour. You can disable dynamic mapping with index.mapper.dynamic:false.\nSomething like this should do it:\n``` ruby\nrequire 'tire'\nTire.configure { logger STDERR }\nclass MyDoc\n  include Tire::Model::Persistence\nsettings index: {\n    mapper: {\n      dynamic: false\n    }\n  }\nproperty :title, type: 'string', analyzer: 'english'\nend\nMyDoc.index.delete\nMyDoc.create_elasticsearch_index\nMyDoc.create title: 'Test'\nCheck:\n\n* http://localhost:9200/my_docs/_mapping\n* http://localhost:9200/my_docs/_settings\n\n``\n. @lister @pangkalizer So, problems solved? Closing?\n. Closing as solved.\n. Hello, I'm sorry, but as stated in #732, I don't think it's a good time right now to significantly broaden the API of Tire right now. Apologies for letting this sleep for too long -- will close this issue for now and let's return to it in couple of weeks.\n. Hi, there was a bug in the current [implementation](https://github.com/karmi/tire/blob/v0.5.1/lib/tire/model/search.rb#L196-L202) ofmatches, since it accessed the@attributes` of the model directly.\nThe current master should fix the problem, can you try it in your codebase?\n\nFor completeness, this a isolated test code: http://hastebin.com/bivoposede.rb\n. Hi, I understand. I think this is only a matter of proper documentation. In the end, implementing to_indexed_json should really be a \"last resort\" thing -- most cases are nicely expressed in the mapping block.\n\n(...) it'd be nice to see a little bit of guidance in docs that :as is best applied for \"computed field\" sort of use cases, usually not formatting of an existing DB field. (...)\n\nCould you elaborate a bit more on this?\n. Hello, sorry for the delay.\n\nIf that is preferable though, perhaps to_indexed_json should be heavily deemphasized in docs?\n\nYes. The mapping capabilities have evolved over time, and I haven't made the changes neccessary in the docs. Over-using to_indexed_json was one of the worse decisions I did in the course of writing and maintaining the library.\nGoing with mapping is definitely a better way how to define the serialization. to_indexed_json is the \"close to the metal\" thing.\nShould we leave the issue open, I think we need to reframe it a bit...\n. @ches Any fresh ideas about the issue?\n. Closing...\n. Hi, could you provide more context here, so I know what is the exact issue and maybe even what kind of test add for this?\n\nNotice I can easily do:\n``` ruby\ndef search\n  @articles = Article.search do\n    # ...\n  end\nrespond_to do |format|\n    format.html { render :action => \"index\" }\n    format.json { respond_with(@articles.to_a) }\n  end\nend\n``\n. @tbh I understand the difference betweento_jsonandas_json. We can add a customas_jsonimplementation as you suggest, but the serialization is handled by [ActiveModel::Serialization](https://github.com/rails/rails/blob/master/activemodel/lib/active_model/serialization.rb) module. We should make it work with that, possibly supporting the XML serialization as well...?\n. @tbh Could you please either shed more light on the issue or post some code so I can uderstand you better?\n. @tbh Had a look at this today. I understand your problem -- theTire::Results::CollectionandTire::Results::Iteminstances should be now 100% Railsas_json`-compatible, so you can do something like:\nruby\n    def search\n      @articles = Article.search do ... end\n      respond_to do |format|\n        format.html { render :action => \"index\" }\n        format.json { render json: @articles.as_json(only: ['id', 'title']) }\n      end\n    end\nI did the implementation in Tire, not leveraging the ActiveSupport::Serialization infrastructure. The as_json methods proxy down to ActiveSupport implementations.\n. @andywenk Sorry for the delay. I think this depends on the JSON library you use, especially oj is quite restrictive what it allows. If you set the logging with Tire.configure { logger STDERR }, I think you'll see the incorrect value in the mapping ({s=not_analyzed, ruby_class=_symbol}).\nAs suggested, using String instead of Symbol should fix it.\n. @andywenk So, closing?\n. No worries, thanks!\n. It seems that Mongoid returns a BSON object, instead of some plain JSON-friendly object for the IDs. See tire#535.\nEither we're incorrectly serializing Mongoid objects in Tire, or Mongoid needs some special care. Any Mongoid specialist to advice here? //cc @michaelklishin\n. I understand that. But when I have a plain Article class (with include Mongoid::Document), the output of as_json, to_json and serializable_hash looks OK to me:\n```\n\n\na = Article.first\n=> #\na.as_json\n=> {\"_id\"=>\"507d19bc410b5a5d85000001\", \"content\"=>\"...\", \"published_on\"=>2012-10-16 08:24:28 UTC, \"title\"=>\"One\"}\na.to_json\n=> \"{\\\"_id\\\":\\\"507d19bc410b5a5d85000001\\\",\\\"content\\\":\\\"...\\\",\\\"published_on\\\":\\\"2012-10-16T08:24:28Z\\\",\\\"title\\\":\\\"One\\\"}\"\na.serializable_hash\n=> {\"_id\"=>\"507d19bc410b5a5d85000001\", \"content\"=>\"...\", \"published_on\"=>2012-10-16 08:24:28 UTC, \"title\"=>\"One\"}\n```\n\n\nSo, what should we do differently in Tire?\n. @digitalplaywright Thank you for all the digging! I think the fastest and safest way right now is the as_json as outlined in https://github.com/karmi/tire/pull/535#commitcomment-2277626...\n. Pushed karmi/tire@mongo_id with some example code and tests.\nThe problem is, I'm not being able to replicate the issue with a simple Mongoid Rails application.\nMy example model looks like this:\n``` ruby\nclass Article\n  include Mongoid::Document\nfield :title, :type => String\n  field :content, :type => String\ninclude Tire::Model::Search\n  include Tire::Model::Callbacks\ndef self.paginate(options = {})\n     page(options[:page]).per(options[:per_page])\n  end\nend\n```\nI can import the data just fine with either bundle exec rake environment tire:import CLASS='Article' FORCE=true or Article.import in the console, and the IDs in the index correspond to the IDs in the Mongo database...\n. @sideshow That's the version I have:\n$ bundle list | grep mongo\nmongoid (3.0.14)\n. @sideshow @digitalplaywright Would it be possible to upload a simple Rails app which would demonstrate the issue for me? I'm not against pulling the changes from the linked commit, but would rather go without it, unless absolutely neccessary.\n. @digitalplaywright Many thanks for the perseverance in hunting this bug! I've pushed the as_json solution in the attached commit, import now runs fine with your example app (TestCase.build_new).\n. Released as 0.5.2\n@digitalplaywright I don't have any isolated test with just Yajl...\n. Hi, yes, you can pass specific :per_page on the command line:\nrake environment tire:import CLASS='Article' INDEX='articles-2011-05' PARAMS={per_page:10000}\nWhen you want to index a specific subset of records, with a specific SQL query, it may well pay off to write your own, custom indexing code. ActiveRecord#find_in_batches makes it very convenient:\nruby\nArticle.where(\"published_on > ?\", Time.parse(\"2012-10-01\")).find_in_batches do |group|\n  Tire.index('myindex').import group\nend\n. Hi,\nanswered your question at StackOverflow.\nIt does not make much sense to me that the DSL would behave like this... It would add a lot of overhead then updating the index, since the class would have to go over all inidices, and fire all the updates.\nWould there be another use case for the behaviour you've outlined?\n. > I just think that this feature makes Tire a lot more flexible, and gives a user the opportunity to think of architecting their elasticsearch indexes in smarter more sophisticated ways.\nAbsolutely. The problem, though, is not coding the support for a feature like this, but making sure it's understandable, there is enough documentation and examples for various use cases, things like that...\n\nFor example, what if I have about 30 (massive exaggeration, I know) different models that I want to be able to search across all at once. With tire I'd have to setup 30 different indexes for each one (...)\n\nThat is already quite possible with Tire:\n``` ruby\nrequire 'logger'\nrequire 'active_record'\nrequire 'tire'\nrequire 'oj'\nActiveRecord::Base.logger = Logger.new(STDERR)\nActiveRecord::Base.establish_connection( adapter: 'sqlite3', database: \":memory:\" )\nActiveRecord::Schema.define(version: 1) do\n  create_table :books do |t|\n    t.string   :title\n    t.timestamps\n  end\n  create_table :magazines do |t|\n    t.string   :title\n    t.timestamps\n  end\nend\nTire.index('my-library').delete\nclass Book < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nindex_name 'my-library'\n  mapping { indexes :title, analyzer: 'snowball' }\nend\nclass Magazine < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nindex_name 'my-library'\n  mapping { indexes :title, analyzer: 'snowball' }\nend\nBook.create     title: \"My Book...\"\nMagazine.create title: \"My Magazine...\"\nBook.tire.index.refresh\nMagazine.tire.index.refresh\nputs '-'*80\nputs 'Books:     ' + Book.search { query { all } }.results.map(&:title).inspect\nputs 'Magazines: ' + Book.search { query { all } }.results.map(&:title).inspect\nputs 'Library:   ' + Tire.search('my-library') { query { all } }.results.map(&:title).inspect\n```\n. Now I'm confused. You said:\n\nWith tire I'd have to setup 30 different indexes for each one, when I could just put everything in one index to conserve resources and speed up queries.\n\nThe code I posted demonstrates that you can, in fact \u201cput everything in one index\u201d -- did you try it?\n(It's possible to use one index for multiple, separate models. It's not possible to index one model into multiple, separate indices.)\n. No worries :) There's no discussion that being able to \"split\" the model indexing into multiple indices would be a very nice feature (eg. because of the autocompletion thing, etc), but I'm really worried about meta-stuff such as explaining it properly, making sure it does not have \"surprises\", etc. Let's keep it open as a reminder.\n. @matthewford You mean for the Book and Magazine example? Yeah, the first one wins -- the index should be created in advance outside of the class definition, which is often the case for complicated setups. I should update the example.\n. @matthewford Better still, create it as a eg. a class/module method, and call it from the Rake task. This way, you can re-use it in tests setups, etc.\n. @demersus Hmm, the way I see it, the \"store product\" would either be a different Ruby class, which would be composed from a Classified or Vehicle, and have some other info on top of that. That class could perfectly live in a specific index.\nOr you could filter your search results based on a specific property of the Classified/Vehicle/etc class. Either way, I don't see this as a good match for \"multiple indexes & mappings per model\" -- quite the contrary?\n. Closing the issue, let's reopen or create new one in the future when there's some code sketch.\n. Thanks, merged & added a test case.\n. @ike-bloomfire Thanks for the assistence!\n@filiptepper OK, sorry for the delay, on it now!\nFirst, let's start with a working piece of code:\n``` ruby\nrequire 'tire'\nTire.index 'highlight' do\n  delete\n  create\nstore id: 1, body: \"The quick brown fox jumps over the lazy dog\", author: \"Johnny Fox\"\n  store id: 2, body: \"Lazy dog jumps over the quick brown fox\", author: \"Fox Vicious\"\n  store id: 3, body: \"A Fox one day fell into a deep well and could find no means of escape. A Goat, overcome with thirst, came to the same well, and seeing the Fox, inquired if the water was good. ...\", author: \"Aesop\"\n  refresh\nend\ns = Tire.search 'highlight' do\n  # query { string 'fox' }\n  query { match :body, 'fox' }\n  highlight 'body', 'author', options: { tag: '***' }\nend\ns.results.each do |document|\n  puts \"Document #{document.id} highlights: #{document.highlight.inspect}\"\nend\n```\nActually, that would be my first advice: whenever stuck like this, it's a good idea to drop down and create a working bare-bones code and work up from that.\nSecond, notice that the match query is generally preferred. Elasticsearch tries hard to make this query generally both useful and peformant, and Tire wraps it in a nice interface (tire@9a6e00e).\nThird, be careful with the string query. By default, it runs on the dynamic _all field, which uses a different analyzer then your title and body properties. So you may get tricky results/highlights, or no results/highlights at all.\nYour invocation of highlight is correct in the search do ... end block, try highlighting just one field, make sure the index is refreshed, etc etc etc. Use the (multi)match query, or an equivalent string \"body:#{q} OR title:#{q}\" fully qualified query.\nDo notice that the page and per_page methods do not belong to the block -- either use size/from or page/per_page as arguments. The integration tests have a lot of examples.\nFinally, do check the highlight integration tests, if they run for you, there may be some  bug I'm not aware of.\nPing back with results!\n. Hello, that seems a valid concern! Could you add a test demonstrating the issue, the way you describe it in the issue?\n. Cool!, added an integration test as well, amended the commit message etc and pushed, thanks!\n. Hi, there're good intentions in this commit, but I don't think it's that easy, and it's impossible to merge as it is.\n1. We're converting the id attribute of documents people send into elasticsearch into a String. That sounds dangerous.\n2. That Mongoid returns an BSON object instead of some JSON friendly object is quite unfortunate. I'm all for making Tire work smoothly with Mongoid, but not with inconsistent fixes like this.\n. Closing in favor of karmi/tire#529.\n. Please run the pagination tests and see what you get.\n. Then try running your app with the current master from Github, in you Gemfile:\ngem 'tire', :git => 'git://github.com/karmi/tire.git'\n. This should really work out of the box... Wondering why are you having the problem..\n. I don't understand what you mean. In unit tests, you usually mock Tire integration, in integration tests, you let everything bubble up and down, calling Index#refresh is enough. There are some pages on the Wiki.\n. Your problem is then related to running the tests in parallel, possibly even running into issues with multiple elasticsearch nodes forming a cluster in the CI environment?\nNormally, a refresh is the only thing you need to do. Are you sure you're running a refresh in test setups? Does the Tire integration tests run fine for you? Also, it may help if you'd create the index with a single shard in test environment. \nIf nothing helps in your case, we may need to add an API for waiting for specific cluster health (color, number of active shards, etc).\n. First, try running a single test repeatedly, if you also run into the issue.\nIf you parallelize tests, you have no isolation between test runs, ie. setup from second test can wipe data in elasticsearch for first test. You may mitigate the issue by using different index names per test, for instance when running the tests as part of integration test suite of a Rails application.\n. Something must be weird in your environment... When I run:\nfor i in {1..15}; do ruby -I lib:test test/integration/percolator_test.rb; done\nI get no \"random errors\"...\nThe [REQUEST FAILED] ... \"query\": \"[x\" is a legitimate case -- a test for incorrect Lucene query.\nThe same with ModelWithIncorrectMapping.\n. If sleep(1) solves it, either the refresh call is incorrect (done on a different index, etc), or there's something funky with the whole setup. Really, refresh is the only thing you need to do in tests... Again, I've run the percolator integration tests 15 times without a single error.\nIn your app, you could have some concurrency stuf, etc., but Tire tests should be stable.\nI'm afraid that without knowledge about the whole system I can't be of much help. Try building an isolated environment without access to outside world eg. with Vagrant and running the Tire test suite inside it.\n. > Any ideas where to move next? Please advice.\nHow many shards and replicas is the index created with when the tests fail?\n(In my case, I have ES set up that number_of_shards:1, number_of_replicas:0 by default -- but that may not be your case.)\n. Yes, by default your index is created with 5 shards and 1 replica. A very good default, but poor for test purposes. Uncomment the Note, that for development on a local machine ... line in elasticsearch.yml and see if it makes a difference.\n. > {\"cluster_name\":\"elasticsearch\",\"status\":\"red\",\"timed_out\":false,\"number_of_nodes\":1,\"number_of_data_nodes\":1,\"active_primary_shards\":36,\"active_shards\":36,\"relocating_shards\":0,\"initializing_shards\":4,\"unassigned_shards\":41}\nYeah, lots of unnasigned shards (replicas), some shards still initializing...\n. > {\"cluster_name\":\"elasticsearch\",\"status\":\"red\",\"timed_out\":false,\"number_of_nodes\":1,\"number_of_data_nodes\":1,\"active_primary_shards\":12,\"active_shards\":12,\"relocating_shards\":0,\"initializing_shards\":1,\"unassigned_shards\":5}\nYou still have unassigned and relocating shards -- drop all indices after restart (curl -X DELETE localhost:9200).\n. Then you have to wait until the cluster is ready in your test setup, as you initially mentioned:\nTire::Configuration.client.get \"#{Tire::Configuration.url}/_cluster/health?wait_for_status=yellow\"\n. > should \"extract the routing value from documents\" do\nRuns fine for me... Really don't know what's up with your environment/setup...\n. Hi,\nI've spent a great deal of time with this yesterday evening. My conclusions:\n1. I've not been able to consistently reproduce the \"randomly failing tests\" scenario.\n2. I've been able to observe tests failing from time to time, when running them sequentially, in a large batch. My explanation was that the virtual machine was simply not being able to withstand the load.\n3. I've been able to consistently reproduce the failure of the should extract the routing value from documents test. In an Ubuntu 12.04 virtual machine, this test fails all the time.\n4. This test passes every single run on Mac OS X 10.8.2, even when running in large sequential batches.\nThe single routing test does fail, not others. Sometimes there are failures, but the test suite is quite heavy, a 1GB VM is only capable to withstand a certain level of load. In my last runs, the only failing test was the routing one.\nI have found a bug in the test: the test is run against an index with only one shard, thus the \"filtering\" based on routing value can't work. Why it works on a Mac, I have no idea.\nI'm opening a new issue just for this, so we can move forward.\n. @vsespb See issue karmi/tire#540\n. At least we have a repeatable technique to run tests in an isolated environment via Vagrant. If you want to try it out:\nDownload or clone this gist:\ngit clone git://gist.github.com/4239703.git elasticsearch_vagrant && cd elasticsearch_vagrant\nRun bundle install, etc. Then, configure ES:\necho '\n{\n  \"elasticsearch\" : {\n    \"cluster_name\" : \"elasticsearch_test_in_vagrant\",\n    \"index_shards\" : 1,\n    \"index_replicas\" : 0\n  }\n}' > node.json\nInstall Ruby 1.9.3, Tire, etc:\n```\necho '\nInstall the Tire Rubygem\napt-get install make git libcurl3 libcurl3-gnutls libcurl4-openssl-dev libsqlite3-dev -y\napt-get install redis-server  -y\napt-get install ruby1.9.3 -y\nupdate-alternatives --set ruby /usr/bin/ruby1.9.1\ngem install bundler --no-rdoc --no-ri\ntest -d \"tire\" || git clone git://github.com/karmi/tire.git\nchown -R vagrant:staff tire\ncd tire\nbundle install\n' > install.sh\n```\nBuild and provision the machine:\nNODE_CONFIG=node.json INSTALL=install.sh time bundle exec vagrant up\nRun the Tire tests:\n```\necho '\ncd tire\necho \"Running Tire tests in 'pwd'\"\necho \"Ruby: ruby -v\"\necho \"Java: java -version 2>&1 | head -1\"\nfor i in {1..5}\n  do\n    echo \"-------------------------------------------------------------------------------\"\n    echo \"TEST $i\"\n    echo \"-------------------------------------------------------------------------------\"\n    time bundle exec rake test\n  if [ $? == 0 ]; then echo \"$i: OK\" >> results.txt; else echo \"$i: FAIL\">> results.txt; fi\ndone\necho; echo; echo \"===============================================================================\"\ncat results.txt\n' > run_tire_tests.sh\ncat run_tire_tests.sh | xargs -0 bundle exec vagrant ssh -c\n```\n. > it was fine on default (empty) config. that can explain why it works on your Mac.\nNot really :) I have the exact same setting as a default on my machine.\n\nAdd wait for green/yellow status call after index refresh call.\n\nThat should really be more then enough for any test to pass. If the application tests fail, there must be some issues with concurrency, parallelism, threads, etc. I understand a brittle test suite is a really bad thing -- that's why I wanted to get to the bottom of this issue..\n. Yes, there is quite certainly potential for some race conditions / timing issues. Note the karmi/tire#541, which was failing for me consistently.\n. include_in_all just means, that the field is not copied to the convenience _all field (which is default eg. for query_string query), not that the field is not searchable.\n. > what about the 'not_analyzed' setting?\nSorry, don't understand. (How is the mapping for the count field set?)\n. But how are you searching for it? Can you post an example query?\n. So you're searching for \"query\": \"95\" and that finds a doc with count:95? Is it possible another field gets analyzed in such a way that it has a 95 token? Can you post the resulting doc?\n. Understood. Unfortunately, I can't replicate the problem. Check the index mapping -- maybe you need to recreate the index?\n``` ruby\nrequire 'active_record'\nrequire 'tire'\nrequire 'oj'\nActiveRecord::Base.establish_connection( adapter: 'sqlite3', database: \":memory:\" )\nActiveRecord::Schema.define(version: 1) do\n  create_table :books do |t|\n    t.string    :title\n    t.integer   :count\n    t.timestamps\n  end\nend\nTire.index('books').delete\nclass Book < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nmapping do\n    indexes :title, analyzer: 'snowball'\n    indexes :count, include_in_all: false\n  end\nend\nBook.create title: \"War and Peace\", count: 123\nBook.index.refresh\nTire.configure { logger STDERR }\ns = Book.search do\n  query { string 'peace' }\nend\nputs '-'*80\nputs s.results.to_a.map(&:title).inspect\ns = Book.search do\n  query { string '95' }\nend\nputs '-'*80\nputs s.results.to_a.map(&:title).inspect\nputs '-'*80\n``\n.index_nameshould be outside of the mapping block, but it makes no difference in this case -- the example still runs. I wonder what's the problem in your case...\n. Hello, any news?\n. Closing, please reopen when you have any new info and/or an isolated test case which I can run...\n. @ike-bloomfire I'm sorry, it's too many questions I guess :) Whatever you set in your model is ignored if an index with the correspondingindex_name` exists. \n. >  there is a chance that all the settings/mappings that you have in all the other models (...)\nYes, that is true. You can either manage your index manually, or use different index for each model.\n\nAlso, where do you prefer people ask questions that are not technically issues? here or stackoverflow?\n\nGithub issues is fine for me, but there might be a delay depending on my workload. Stackoverflow has the advantage that sometimes other people tend to help there. So, both are fine.\n. There are two ways around this, I think:\n1. If you're only interested in getting/displaying the index name of each result, use the Item#_index method.\n2. If you somehow need to \"segmentize\" or group the results, a good approach might be the multi_search query.\nAlso, please start moving away from the string query :), the match query should do everything you need and is the preferred way of elasticsearch. I need feedback on it, so it can be improved.\n. No worries with questions etc.\nWell, if you're after the total count of matched documents in index1 and index2, my option 1) is useseless.\nWithout issuing another request, one solution is to facet on the _index property:\n``` ruby\nrequire 'tire'\nTire.configure { logger STDERR }\ns = Tire.search '/', :search_type => 'count' do\n  query()           { all }\n  facet(:per_index) { terms :_index }\nend\np s.results.facets['per_index']['terms'].map { |item| { item['term'] => item['count'] } }\n``\n. Glad it worked -- just please close the issue you have opened...\n. @vsespb See the closing commit, the test indeed _should not_ have been passing. More info coming soon.\n. OK, so to make sure:\n1. The _Tire_ suite is consistently passing for you?\n2. Your application tests are consistently passing?\n. You can pass an URL with HTTP Auth credentials toconfigure`:\nruby\nTire.configure { url \"http://USERNAME:PASSWORD@myserver.com\" }\nAnother option is to use Nginx with HTTP Auth as a reverse proxy. The Chef cookbook karmi/cookbook-elasticsearch comes with it out of the box.\n. Please close the issue when it's resolved.\n. @reiz Any success?\n. @reiz Thanks -- all the best!\n. > and no exception raised (I assume this is because code is \"200\" actually)\nExactly: it means Elasticsearch returned a 200 response, but some shards returned error (you tried to sort on an analyzed field).\nRight now, you can check the search.json['shards'] for the information. In future, we should expose this information in a more pleasant way.\n. @vsespb So, problem solved? Could you please close the issue if that is the case?\n. Please turn on logging with Tire.configure { logger STDERR } and paste the output. Be sure to sanitize the index names (your private token) on eg. Bonsai.\n. No, this index https://www.indexdepot.com/elasticsearch/gasfasgfasgfasfga does not exist. You're getting a 404 response from some kind of proxy.\n. I believe this is a problem with the IndexDepot provider, maybe they don't allow you to create an index via regular Elasticsearch API?\n. I'd like to really focus more on tightening up the gem, re-writing documentation, and possibly re-thinking everything in the near future, then adding features. But go for it, if you think it's worthwhile.\n. Closing for now -- this is a simple feature addition, which just calls the API and returns the JSON. Would be call to wrap it in Hashr instance, but would be also cool to research other Hash-like implementations such as Mash etc.\nWe can reopen for further discussion, code review, etc!\n. Yes. If you need to work with parent/child in models, please write your own indexing using the Index#store method...\n. Hi, first, let's review the options and possibilities you have here:\n1/ You want to query these separate indices, and display the results segmented accordingly. In this case, possibly the best option is to use the multi_search  request.\n2/ You want to query these indices and display the results in one listing: in this case, just search normally, every result item will have an _index and _type property to separate it from others.\n3/ You want to query these indices, but display some counts based on whichever indexes got matches. In this case, you can use a terms facet on the _index automatic attribute: see issue karmi/tire#539 which has all the code just for that.\n. Which of the options was yours, by the way? :)\n. Many ways -- one of the better options: https://github.com/karmi/tire/blob/master/test/integration/match_query_test.rb#L45\n. Please close the issue once it's resolved.\n. Argh, true -- thanks for the catch! It was probably introduced in karmi/tire@0f8d4fa already. I tried to stay away from the new dump/load semantics, but seems like we should make the leap.\n. Updated, thanks for the report!\n. The index auto-creation is triggered when you define the mapping.\nThe index is only created when it does not exist, so it should not present any errors unless you have a specific use-case?\n. The proper solution is to create the index beforehand, by your own code. The creation will then be skipped.\n. Yes, it is possible with the approach you outline -- use a filtered query for the latter. Try to stay away from the query_string, usually the match query does what you want (unless you need to support Lucene query syntax).\n\nAlso, please, use either Github issues or StackOverflow questions, it dissolves the discussion otherwise. I have noticed the question at StackOverfllow is quite different.\n. Some code to get you started:\n``` ruby\nrequire 'tire'\nTire.index('questions') do\n  delete\n  store type: 'question', app_id: 1, title: 'Question test 1'\n  store type: 'question', app_id: 2, title: 'Question test 2'\n  store type: 'question', app_id: 3, title: 'Question test 3'\n  refresh\nend\nTire.index('links') do\n  delete\n  store type: 'link', app_id: 1, title: 'Link test 1'\n  store type: 'link', app_id: 2, title: 'Link 2'\n  store type: 'link', app_id: 3, title: 'Link test 3'\n  refresh\nend\nTire.index('events') do\n  delete\n  store type: 'event', title: 'Event test 1'\n  store type: 'event', title: 'Event test 2'\n  store type: 'event', title: 'Event 3'\n  refresh\nend\nmulti_search = Tire.multi_search do\n  search :all, indices: [:questions, :links, :events] do\n    query { match :_all, 'test' }\n  end\n  search :rest, indices: [:questions, :links] do\n    query do\n      filtered do\n        query { all }\n        filter :terms, app_id: [1, 2]\n      end\n    end\n  end\nend\nputs \"ALL:  \" + multi_search.results[:all].map(&:title).inspect\nputs '---'\nputs \"REST: \" + multi_search.results[:rest].map(&:title).inspect\n```\n. Hmm, why query them separately and then merge them? :) In any case, just combine the array client side:\nruby\nputs \"COMBINED: \" + (multi_search.results[:all].to_a | multi_search.results[:rest].to_a).map(&:title).inspect\n. The problem is I really don't know what you're trying to do... (The code I posted just does a union of those two arrays.)\n. Closing as answered here and on StackOverflow.\n. If you want to return only a limited number of items, use the size method in DSL -- seee README.\nThe limit filter serves a different purpose -- it limits the number of documents a query is executed against. Filters in the Tire DSL follow closely the Elasticsearch DSL semantics, so should be supported transparently -- see integration tests for examples.\n. > How do I map a model which is present in a subdirectory of models directory\nSorry, I don't understand what you mean by that.\n. If you use the mapping method in the model, the proper type is inferred correctly. In this case, the type would be admin/user.\nruby\np Admin::User.document_type\np Admin::User.mapping_to_hash\n. Crosspost from http://stackoverflow.com/questions/13979145/mongoid-has-many-children-not-saving-through-nested-attributes-when-tirecallba, closing here, please post the relevant portions of your mapping definition, and output or link to output of your model's JSON serialization (Account#to_indexed_json).\n. Hello David, we had support for JRuby at one point of time. In the end, I made the decision all the conditinials sprinkled around the test suite are not worth it -- but your patch looks reasonably clean. Given the current JRuby craze, I think we should support it.\nDo you think the supporting the jruby-18mode variant is neccessary? I don't like to overload Travis with a big matrix.\n. @dwbutler This is good news!, will check it today.\n. @dwbutler Thanks!, merged &\u00a0pushed with minor cleanups.\n. @riniculous Yes, this is related to the breaking change in MultiJson which switched to load/dump methods. Since load is defined in Kernel, Ruby tries to load a file here...\nI believe the MultiJson dependency in Tire is correct (\"~> 1.0\") -- you probably had to run bundle update, or remove the Gemfile.lock and run bundle install again.\n. True -- I'll look into adding some integrations tests for the behavior and unbreak the Index#update method.\n. Closed by karmi/tire@ede225d.\n. Hi, this is a tricky one. First, I'm not sure how probable the case you describe is, and feels like a bug in the application code.\nClearly, the @company.jobs property expects an Array of objects (even and empty one), not nil, which is very toxic in this scenario. In later code, you almost certainly would like to call @company.jobs.each ..., and it would fail on this particular object -- not something you want to.\nAlso, thanks for the patch, but a solution like this has to be accompanied by a unit test, to document and define the behaviour. It is unfortunately not possible to include patches which don't include rigorous test coverage.\nI've sketched a quick solution to your problem in https://github.com/karmi/tire/compare/issues/558, which I also don't feel very good about, but which is maybe closer to the intended solution...\n. > Will you be merging your commit into master?\nI still haven't decided what's the proper handling here... I personally think that passing nil to collection properties is a bug in application code (which admittedly should handled by a nice exception in Tire), and don't have enough use cases to support even my suggestion.\nWhat other users of the persistence layer think? //cc @vhyza \n. Hi, possibly, we'll have a look into this.\nI understand the sentiment 100%, but really: the solution to this is to never pass nil to the method which expects an Array, Enumerable, etc...\nClosing the issue for now...\n. @barakcoh The problem should be fixed on current master.\nNow, with a class definition like this:\nclass Article\n  include Tire::Model::Persistence\n  # ...\n  property :comments, :class => [Comment]\nend\nthe comments property is by default casted as an empty Array:\n```\narticle = Article.new :title => 'Test'\narticle.comments\n=> []\n```\nDo note, that you still can't pass a nil value to the property. So doing something like this:\narticle = Article.new :title => 'Test', :comments => nil\nwill break your application.\n\n/cc @vhyza \n. terms_stats is fully supported (see integration test: https://github.com/karmi/tire/blob/master/test/integration/facets_test.rb#L202-L218), but Elasticsearch AFAIK does not support \"grouping\" results like this in the terms_stats facet. It should be doable with a script facet.\n. OK, got you. \"Multi-valued\" in context of Elasticsearch means \"has more then one value\" (AFAIK). So, it's possible to use terms_stats eg. for properties with more values (such as tags), or properties which are analyzed into multiple tokens.\nWhat you seem to be after is some kind of \"aggregation\" -- GROUP BY in SQL terms --, and that is not supported directly in Elasticsearch.\nNote though, that thanks to the flexibility brought by the scripting support, you could write this aggregation in Mvel/JavaScript and have it executed by Elasticsearch in a \"map/reduce\" fashion -- just combine the year with the job title.\n. @visasquare Hello, any news?\n. Closing, please reopen if you have any more questions.\n. Hi, parent/child is really a grey area with Tire right now.\nFirst, AFAIK you need to setup mapping properly, in the Document class (full code):\n``` ruby\nclass Document\n  ...\n  mapping :_parent => { :type => 'municipality' } do; end\nproperty :id, :type => 'string'\n  property :type, :type => 'string'\nend\n```\nThen you have to pass queries as Hashes:\n``` ruby\nrequire 'tire'\ns = Tire.search 'documents',\n                query: {\n                  has_parent: {\n                    query: { match: { name: { query: 'New', type: 'phrase_prefix' } } },\n                    parent_type: 'document'\n                  }\n                }\nputs 'Query:',   s.to_curl\nputs 'Results:', s.results.to_a.inspect\n```\nUnfortunately, my experience with parent/child is limited -- if you'd have a complete use case, I can certainly help you on the Tire side of things.\n. Until there's proper API/DSL support for parent/child, I'm afraid you're stuck with the less expressive and convoluted way of doing things, namely working with Hashes.\nDo note though, that you can define your searches (queries, facets) in the DSL, and then pass their Hash serialization to the payload (see code below), until the support is there...\n``` ruby\nrequire 'tire'\nmy_query = Tire::Search::Search.new do\n  query do\n    string \"name:new*\"\n  end\nend\nmy_facets = Tire::Search::Search.new do\n  facet 'tags' do\n    terms :tags\n  end\nend\np my_query.to_hash\np my_facets.to_hash\npayload = { query: {\n              has_parent: {\n                type: 'municipality'\n              }\n            }\n          }\npayload[:query][:has_parent].update(my_query.to_hash)\npayload.update(my_facets.to_hash)\ns = Tire.search 'municipalities/document', payload\nputs 'Query:',   s.to_curl\nputs 'Results:', s.results.to_a.inspect\n``\n. Hi, good catch! What about replacing the single quotes with escaped format (\\') instead?\n. So, what's an example of before &\u00a0after format?\n. Thanks!, merged.\n. @jeremygpeterson Merged and pushed, thanks! Part of the 0.5.3 release.\n. Hi, on the current master/0.5.3, you can use the@results.each_with_hitmethod, to acess the_scoreorhighlightproperties, see https://github.com/karmi/tire/compare/bcf2914e909b9475d58776cb554125834b8c573c...6244bd7ca9529204799873cb0e128b2bf763aadb\n. Cool, please ping back if it wouldn't work...\n.hit['highlight']` should be much better .)\n. That's weird -- check the added integration test in https://github.com/karmi/tire/commit/7e4f3dcd2f386296a75d3f8980ebc18d009448ce\n. Please try with the latest version or master...\n. @ovamsikrishna Please try to run the test added on master. If passes, and displays highlights, try to change your code accordingly. If it doesn't pass, we need to work on why it doesn't pass...\n. You can't combine SQL and search queries like this:\nTask.joins(:project).merge(Project.search(params[:q])).paginate(:page => params[:page])\nThere are two ways I see here to get your desired behaviour:\n1/ Find the IDs of Project records matching your query (Project.search(params[:q]).map(&:id)), and use this Array in an SQL query.\n2/ Properly set up your mapping for Project and Task models, so when you search for projects, a task_id would be returned, so you can pass it to Tasks.find().\nSee the http://stackoverflow.com/questions/11692560/elasticsearch-tire-and-nested-queries-associations-with-activerecord/11711477#11711477 StackOverflow answer for information about how to work with associations.\n. > I'm a bit concerned that this could lead to a large array of project_ids to pass into the @tasks query which could affect performance.\nNot really -- you usually load a limited number of results from Elasticsearch (10 by default).\nDon't be afraid of two SELECTs -- what you need to guard against are n+1 queries. If you want to keep it very simple, then this approach of getting project IDs and using them in other, possibly more complicated SQL queries is just fine.\n. Properly serializing your data for Elasticsearch is certainly a good way to go. Please see Elasticsearch, Tire, and Nested queries/associations with ActiveRecord - Stack Overflow. The Tire integration tests contain executable examples.\nClosing this issue -- please open new one if you'll bump into another problem.\n. @amnesia7 You're welcome!\n. @cameronkendall Yeah, for ActiveModel integration -- when you use Tire::Model::Persistence, just call User.find('abc').save.\n. > in my case i use random ordered log for indexing sometime the main (...)\nI'm not sure I understand your use case, but here are some notes regarding the patch:\n- Please remove the redundant commits, use git rebase to get rid of merge commits\n- It is unfortunately not possibly to accept patches without unit tests (and integration tests, where warranted)\n- I'm concerned about naming -- document_exists? does not sound right. Maybe has_document?? Try come up with a good name.\n  I'm worried that if we use this naming scheme, then Index#store should be named Index#store_document etc. Any thoughts?\n. @regedarek Calling RestClient.delete 'http://localhost:9200' should not \"crash\" your specs. Can you please post the backtrace and specifics, or close the issue if it's solved in the meantime?\n. > It seems a shame to have to load info from the database just so that the user_path(x) in my link (project.user) is generated correctly in my search results.\nIt is a shame indeed :) Results::Item instances are 100% compatible with Rails' ActionView helpers such as url_for, dom_id, etc. You do not need to use :load.\n\n...I've just realised that I can change my user link to be <%= link_to project.user.name, user_path(project.user.id) %>. Does that sound like a reasonable alternative?\n\nYes. However, normally you can just use the standard <%= link_to name, item %> -- here, you're using an \"embedded\" object user, and that may trip Tire. It would be cool to add some failing or passing unit tests for this.\n\nAlthough, as part of my search results I also format a date so I assume I am better using load: true so that this can be called on the object unless there's another way to format a datetime returned in the search results.\n\nNo need again -- you can provide a different wrapper for your results (see Results::Item unit+integration tests). For serious use cases, it could be a good choice.\n. @amnesia7 I'm sorry, I don't understand the issue you're having? and I lost the context, the issue is almost a year old.\n. @jwoertink Thanks for the answer, it's correct!\n@danielgatis Either use the suggestion above, or just make sure the field exists in your data with some meaningful NULL value. You can do that in Ruby, or with the null_value option in Elasticsearch mapping. Please reopen if the problem persists.\n. Hi, sorry, it took a bit longer to merge. Just a word of caution: know what you're doing when you would be using the asynchronous replication. Elasticsearch is plenty fast with the default replication, any bottleneck you feel might be elsewhere.\n. Hi, I've added integration tests for the percolation support in models, retouched and merged your patch. Thanks!\n. @krigar, first thing, Tire actively encourages you to use the tire proxy if you're concerned about method clashes, see Readme:\n\nSo, instead of writing Article.search, you could write Article.tire.search, and instead of @article.update_index you could write @article.tire.update_index, to be on the safe side.\n\nTire brings its methods into the model only when they aren't already defined. \nI can't see any evidence of the index method for ActiveRecord::Relation?\n``` ruby\n$ rails console\n\npry\nRails.version\n=> \"3.2.11\"\nArticle.where(title: 'Title 1').class\n=> ActiveRecord::Relation\nfind-method 'index'\nNo Methods Matched\n```\n. @krigar Glad it worked out :) I went into great lengths to make Tire behave nicely here, as I have been burned myself by poisonous gems in the past as well...\n. Hi, I can't reliably replicate your issue:\n\n```\n\n\na = Article.first\n  Article Load (1.7ms)  SELECT \"articles\". FROM \"articles\" LIMIT 1\n=> #\na.percolate=''\n=> \"*\"\na.percolate\n=> []\na.destroy\n   (0.1ms)  begin transaction\n  SQL (8.2ms)  DELETE FROM \"articles\" WHERE \"articles\".\"id\" = ?  [[\"id\", 12714]]\n   (16.4ms)  commit transaction\n=> #\nArticle.search('title:\"Title 1\"').size\n=> 0\n```\n\n\nCan you provide more information? Model settings, percolator settings, etc?\n. Hi guys, I've pushed a commit which short-circuits the import action when passed an empty collection, Tire should behave better now :) Please reopen if you have more problems.\n. Just a note, it seems like Mongoid collection's empty? method does not work correctly -- you have to call to_a on it. The new importing code will take this into account.\n. I'm afraid I don't understand your query. You want to return results if they:\n1. Contain certain term,\n2. or the field is missing,\n3. or the field is null?\n(The missing filter is supported in Tire, as are all other filters.)\n. Sorry, I don't understand.  You want to return results if they:\n1. Contain certain term,\n2. or the field is missing,\n3. or the field is null?\n. For the spec \"Find results which contain certain term, or the field is missing, or the field is null\", see the following code:\n``` ruby\nrequire 'tire'\nTire.index('test-missing-properties') do\n  delete\n  store id: 1, title: 'Test 1', assigned_to: 'john'\n  store id: 2, title: 'Test 2', assigned_to: nil\n  store id: 3, title: 'Test 3'\n  store id: 4, title: 'Test 4', assigned_to: 'mary'\n  store id: 5, title: 'Test 5', assigned_to: 'alice'\n  refresh\nend\ns = Tire.search('test-missing-properties') do\n  query do\n    filtered do\n      query { all }\n      filter :or, { term: { assigned_to: 'john' } },\n                  { missing: { field: 'assigned_to' } }\n    end\n  end\nend\nputs \"QUERY: \",  s.to_curl, \"\",\n     \"RESULTS:\", s.results.map(&:id).to_s\n=> [\"1\", \"2\", \"3\"]\n``\n. No, this is not possible: a field with anull` value simply isn't even stored.\nYou have to replace the null value in your document, either on the client side, or with mapping via the default_value option, and then use a not filter for that.\n. There's index_prefix for model integration, but not a way how to set a \"namespace\" eg. a la the redis-namespace gem, if that's what you're after.\n. Absolutely interesting, as a part of tire-contrib (or even as a separate gem), with a nice extension DSL method for Tire.configure or an include SomeModel approach, or a la https://github.com/defunkt/redis-namespace...\n. Hi, sorry for the delay. I've been thinking about it a bit, but haven't yet found a good solution... All the _type handling should be more consistent and better, to be honest.\n. Hi, yes, this should definitely work -- this is some example code I have prepared for a Stackoverflow answer: http://pastie.org/private/z0mqugcwptvwckfkc4lbg, can you have a look at it and try to isolate your issue?\n. Tire sugar coats the filters a bit here, and multiple filter invocations create an and filter automatically.\n\nThis and below makes it seem like the documents need to fill all the requirements (tagged with 'javascript' and have and id of 1, 2, or 3).\n\nYes, that's the case, they must fulfill all the requirements. Can you create a full recreation with data \u00e0 la the code below?\n\n\nThe problem was that i had terms :tags, [tags], minimum_match: 1 after the filtered block which that canceled out the id filter.\n\nThat really shouldn't happen, see this code:\n``` ruby\nrequire 'tire'\nTire.index('test-filtered-and-filters') do\n  delete\n  create\nstore id: 1, title: 'Title 1', user_id: 1, tags: ['python', 'ruby', 'javascript']\n  store id: 2, title: 'Title 2', user_id: 2, tags: ['javascript']\nrefresh\nend\nTire.configure { logger STDERR }\nTire.search('test-filtered-and-filters') do\n  query do\n    filtered do\n      query { string 'title' }\n      filter :terms, :user_id => [1]\n    end\n  end\n  filter :terms, :tags => ['javascript']\nend.\nresults.each do |d|\n  puts \"* TITLE: #{d.title}, USERID: #{d.user_id}, TAGS: #{d.tags}\"\nend\n```\n. @moudy I'm interested in the issue where one filter \"cancels out\" other one -- that sounds like a bug. The problem is I can't replicate it -- would you be able to send a full recreation based on the script I've sent above?\nAlso, do note that terms :tags, tags, minimum_match: 1 is not a valid option for the terms filter -- is it in your actual code?\n. Hi, thanks!, sorry it took so long.\n. Hi,\nthanks for your thoughts and your code! I understand the issue, and understand why it would help when prototyping or working with heterogenous data.\nThe problem is, that a \u201csugar\u201d like this quickly brings hypoglycemia on when you're not paying enough attention :) Models are initialized with heterogenous properties, those are saved into Elasticsearch with dynamic mapping, then somebody wants to sort on the name property and it does not work...\nI do agree a default like this would be aligned with Elasticsearch's behaviour, but I'm just worried about the \"sugar levels\" here too much, based on experience.\nThat said, I think it would make a terrific tire-contrib module, where you could use it like this:\n``` ruby\n    require 'tire'\n    require 'tire/model/persistence/dynamic_attributes'\nclass Foo\n  include Tire::Model::Persistence\n  include Tire::Model::Persistence::DynamicAttributes\nend\n\n```\nWhat do you think?\n. Thanks!, that makes sense. I also added that the time is forced into UTC after parsing.\n. Closed by 69fa54b061884143f55e6c3c50e4ff4fe23fb2ca\n. Closed by 1537899, thanks!\n. @hellvinz Thanks for the suggestion!\n. @xpepermint Do you have a stacktrace or more info?\n. @hellvinz Yes, thanks!, @xpepermint check out 1537899\n. Please see karmi/tire#562\n. Hi, any new information on this? The MyModel.import is model only -- use Tire.index('myindex').bulk_store <AN ARRAY OF DOCUMENTS>. \n. @reiz You have to provide more information, example of how the model is serialized, etc. Or catch me on IRC.\n. Closing this, please reopen with more info if the issue persists.\n. But they don't do the same thing, AFAIK...\n. > If your search results return items with different classes (...)\n@mrcasals I'm sorry for the silence, and also, I'm afraid I don't understand the intention here...\n. @mrcasals Reviewing it again -- the problem with this is that the corresponding part of code is complicated and \"magical\" quite a lot already. Why not simply map the results in application code before you use them?\nruby\nTire.search.results.map do |result|\n  klass = document['_type'].camelize.constantize\n  klass.new result.to_hash\nend\n. I understand the general intention, thanks. The thing is, Tire is already quite sophisticated when you're eg. using search and models in a Rails application. Notice:\n```\n\n\ns = Tire.search( ['authors','articles'] ) { query { string 'title:\"Title 3\" OR name:*' }  }\ns.results[0]\n=> \ns.results[0].class\n=> Article(id: integer, title: string, content: text, author: string, published_on: date, created_at: datetime, updated_at: datetime)\ns.results[1]\n=> \ns.results[1].class\n=> Author(id: integer, name: string, birthday: date, created_at: datetime, updated_at: datetime)\ns = Tire.search( ['authors','articles'], load:true ) { query { string 'title:\"Title 3\" OR name:*' }  }\ns.results[0]\n  Article Load (0.4ms)  SELECT \"articles\". FROM \"articles\" WHERE \"articles\".\"id\" = ? LIMIT 1  [[\"id\", \"12716\"]]\n  Author Load (0.2ms)  SELECT \"authors\". FROM \"authors\" WHERE \"authors\".\"id\" = ? LIMIT 1  [[\"id\", \"1\"]]\n=> #\ns.results[1]\n\n\n=> #\n\n\ns.results[1].class\n=> Author(id: integer, name: string, birthday: date, created_at: datetime, updated_at: datetime)\n```\n\n\nSo, it already \u201cinfers\u201d the class from the hit _type.\nI guess I'd need more context here, specifically:\n- Is this within Rails or not\n- Is this an ActiveModel/Record integration?\n- Is it Tire::Model::Persistence?\n- Is this in a generic Ruby, model-less application, using straight Ruby classes?\nCan you expand on this, please?\n. >  I'm using Tire's Tire::Model::Persistence to get an ActiveModel/Record API. \nNow I'm even more confused .) Tire::Model::Persistence or Tire::Model::Search? In Rails, the result item should realy be either \"masqueraded\" as your models (without :load), or instances of your models (with load). Can you try the code I posted and post what's not working in your environment? (Or, preferably, add a failing integration test.)\n. Good!, but the problem is until I can replicate the issue with a failing test or a code snippet, it's hard to decide what should be done :) The Tire DSL is just a wrapper around Tire::Search::Search etc classes...\nI'll close the issue for now, and ask you to reopen it with an executable code snippet / integration test for further discussion, does it sound fair?\n. Thanks!, that looks good, will have a look later, I'm up to the elbows in the Chef cookbook this week...\n. Yes, as 1537899b74d4d00752a9a1986bcfe0697d8784f3\n. Sorry for the delay... You need to use the nested type and queries here, see:\n- Documentation: http://karmi.github.com/tire/#section-Nested_Documents_and_Queries\n- Tests: https://github.com/karmi/tire/blob/master/test/integration/nested_query_test.rb\n. Cool, but any other info? Is it backwards compatible or not, do we want to add tests for this or not, etc?\n. @anoldguy I'm all for merging it in, need to check the bigger story about Rails 4 in general. You can use a different repo/branch in Gemfile to test things out.\n. @brupm, @romansklenar thanks guys! Verified it works on Rails 3 and 4. Retouched the commits and pushed. Sorry to everybody it took so long.\n. It should return empty response only for HEAD requests?\n. >  i.e. when the search server is gone\nCan you describe it in more detail, please?\n. I'm sorry, but I can't replicate it:\n```\n\n\nTire.index('articles').retrieve(:article, 1)\nErrno::ECONNREFUSED: Connection refused - connect(2)\n```\n\n\nI'll close the issue for now, please reopen with any executable code I can run to demonstrate the issue!\n. > (...) checking if request was successful (...)\nBut most probably not in Tire::Index. The problem does not exist with the default client (Errno::ECONNREFUSED) or the Curb client (Curl::Err::ConnectionFailedError). This is something which should be fixed in the client itself, not adding if...else routines to a completely different class?\n. In what sense? I didn't wrote the Faraday adapter -- there should be a unit test which makes sure it behaves the same as RestClient/Curb clients. What we're seeing here is the Faraday adapter returning response even when the Elasticsearch service is down -- this hints at the adapter is behaving incorrectly.\n. I suggest you create a curl version and send your question to the mailing list...\n. Added integration test, see closing commit.\n. Hi, thanks for the writeup, Francisco. I'm terribly busy with some other stuff at the moment, but would like to just add couple of points:\n- the API you describe makes perfect sense, we had many, many discussions in the early times of Tire development, and usually it was centered about ideas like these,\n- if I remember correctly, the problem always was getting the index_name lambda/proc right (model instance as argument, but how do you integrate it with the model class search method then, etc)\n- please make sure you read the outline for a different strategy I have prepared in one of those discussions, it may give you more ideas,\nIn general, I understand why having a separate \u201cphysical\u201d index for each account/user/tenant/etc is attractive. But... I wouldn't say it's a wrong path, but I'd say it has many hidden dangers and it comes with a cost. Scaling indices upto tens of thousands is something not easy to do with Elasticsearch. I think it may be still beneficial for your team to brainstorm other possible data layouts, such as using a set of shared indices for small users, using aliases with filters/routings for clean integration with application, migrating big users to separate indices, etc.\nAs frequently mentioned, Shay has been talking about these scenarios in this Berlin Buzzwords talk: \u201cElasticSearch: Big Data, Search, and Analytics\u201d (slides) -- it's essential material.\n. > My main concern is the ability to provide different mappings for different tenants.\n\nI does not seem to be possible without separate indexes.\n\nYes, for that separate indices would be the only solution. You should be able to use strategy outlined in #92, where you set the index before search (Document.index_name \"documents-#{current_user.id}\"). Of course, it could be wrapped into some with_index etc. for better readability. I really thing this is hard to support generally and is better handled by the application code...\nClosing this for now, if it's OK, feel free to reopen or open new issue for further discussion!\n. Yeah, I think there's much more mess like that. If you could review it, that would be cool!\nClosing this in favor of specific issue/pull request.\n. @JamesHarrison Sorry for embarrasing delay. Well, the problem with your workflow is that the index is deleted, and then the mapping is created dynamically, based on the first Image instance which is indexed.\nTry calling Image.create_elasticsearch_index after you call delete, and then do the indexing. But generally, string/array is all the same, see http://www.elasticsearch.org/guide/reference/mapping/array-type.html\n. Closing, feel free to reopen with more info if needed...\n. Sorry for the delay!, merged/pushed.\n. @timoschilling Hmm, thanks for looking into this. I don't like conditional logic based on defined?(Something), the code in question is messy already. I think we need to provide a way to require \"something/special\", or perform this in a specific module. When the methods do not provide enough hooks for that, we need to refactor them so they do.\nNotice that the __find_records_by_ids method was added exactly for purposes like these -- but it should be overloaded by a Something module, not changed. What do you think?\n. @timoschilling Hi, I think the proper way around this would be to implement a Strategy for each specific ORM/persistence library, like we did in the Mymodel#import code in efb9c14. Then I'm all for having it included as part of the core gem. I'll try to have a look into that soon.\n. Good catch! Actually, ActiveSupport is (unfortunately!) pulled in by ActiveModel, and we can't do anything about it in the current codebase, if we want to support various OxMs by Results::Item.\nBeware of the subtle differences between as_json and to_json, a solution like you outline might be broken in subtle ways :) I think the proper solution for the time being is to just admit ActiveSupport and pull it in properly. (As noted multiple times in issues, plans for a complete rewrite of the gem are in motion...)\n. @msonnabaum Yes, thanks! I'll have a look into admitting the ActiveSupport more openly in gemspec (I'm just teriffied about version mismatches there :)\n. Admitted to ActiveSupport in gemfile, will keep it in mind for future. Thanks for the report!\n. Hi, as suggested in #582, do you think it would make sense as a  tire-contrib module?, where the end-user could use it somehow like this:\n``` ruby\n    require 'tire'\n    require 'tire/model/persistence/dynamic_attributes'\nclass Foo\n  include Tire::Model::Persistence\n  include Tire::Model::Persistence::DynamicAttributes\nend\n\n```\n(Note to self: I really need to write more about tire-contrib and less invasive ways of extending Tire :)\n. @davekinkead Any thoughts about doing it as Tire contrib extension?\n. No worries, Dave! Take your time and thanks!\n. @letronje Could you post the solution and close the ticket?\n. Hi, can you try using the match query, which replaced the text query, integration tests here: https://github.com/karmi/tire/blob/master/test/integration/match_query_test.rb\n. My bet is you're on old Elasticsearch version? Try upgrading and report back, please.\n. Cool, closing then!\n. @Rockyyost Sorry for such a delay. Could you post some code so I can try it? If the upsert in Mongoid doesn't trigger after_save, then it won't be picked up by the callback hooks.\n. Closing, feel free to reopen with more info if needed...\n. The \"lat,lon\" is also valid, but you're passing the values as arrays. Good it's resolved, closing then?\n. @rubytastic You have access to any variables defined outside your search with the \"blocks with arguments\" syntax:\n``` ruby\n(params ||= {})[:query] = 'foo'\ncurrent_user = 'bar'\nMyModel.search do |search|\n  search.query do |query|\n    query.filtered do |q|\n     q.match  :title, params[:query]\n     q.filter :term, user_id: current_user\n    end\n  end\nend\n```\nNotice, that you have access to current_user only in the controller instance scope, you can't access it in your model class.\n. @amnesia7 It's really hard to answer your question when you don't include full recreation: the question would also be more appropriate on the Elasticsearch mailing list or IRC channel, since it's not about the Ruby integration it seems...\n. Yes, facet_filter is supported, either as an option to the facet method or as a DSL method.\nFor your use case, the right path is:\n- Use the filtered query,\n- remember queries limit all facets, top level filters limit only results,\n- facet_filter limits the results of the facet being passed to,\n- so you need to create a combination of mutually exclusive filters.\n. You have to use the Log.create_elasticsearch_index, which picks up the correct mapping, or pass the mapping to the create method. See the source code for  the import task.\n\nAdmittedly, the Log.index.create method should be smarter, pick up the correct mapping and not violate the principle of least surprise like this :)\n. Great. Also, please put the information as an answer to your question here: http://stackoverflow.com/questions/14690975/why-multi-field-mapping-is-not-working-with-tire-gem-for-elasticsearch, and accept it, so it's not unanswered.\nBy the way, I greatly appreciate when people create either Github issue or Stackoverflow question, not both .)\n. @davidguthu What has? That Log.index.create doesn't take mapping/settings into account?\n. @vburca I've added the info to the Readme (d97f7fc5e555750945c2c2fcd22bcbfeeb5bde17), agree that it's frustrating and surprising...\n. Thanks! :)\n. No, if you don't include callbacks, no indexing operations should be happening on model changes. Please reopen with more info if needed.\n. That's probably because ThinkingSphinx is calling the update_index method directly, based on callbacks in your model -- that's what the backtrace suggests.\n. I don't know about other operations, and about your setup. This works for me nicely:\n``` ruby\nrequire 'thinking-sphinx'\nrequire 'tire'\nrequire 'logger'\nActiveRecord::Base.logger = Logger.new(STDERR)\nActiveRecord::Base.establish_connection( adapter: 'sqlite3', database: \":memory:\" )\nTire.configure { logger STDERR }\nActiveRecord::Schema.define(version: 1) do\n  create_table :articles do |t|\n    t.string   :title\n    t.timestamps\n  end\nend\nclass Article < ActiveRecord::Base\n  include Tire::Model::Search\nend\na = Article.new title: 'Test'\na.save\na.tire.update_index\na.tire.index.refresh\ntable_count = Article.count\nindex_count = Article.search('*').results.size\nputs '-'*80,\n     'Article table count:', table_count,\n     'Article index count:', index_count\n```\n\nAs a side note: the version of Thinking Sphinx you're using is rather old.\n. > If Developers want this level of custom callback behavior, it's possible they should be writing their own callbacks.\n@darrenboyd I couldn't agree more :) I have been always advising people to use the include Tire::Model::Callbacks as the initial approach, and see how far does it carry them. It is very easy to use custom logic -- see the update_index if state == 'published' part in the README.\n. @AaronRustad I understand and even support the motivation for this! That said, let me briefly describe what I don't like about this approach:\n- The usage of really_weird_acts_as_something_or_maybe_not class methods is a relic of the Rails's past, and we should actively stay away from it :)\n- The ActiveModel integration itself should bring ActiveModel::Dirty into account, which, shamefully, it does not. In your case, you only include it for one model.\n- As said above, I'm not a fan of super-smart default behaviour. As a library developer, I want to a) make it easy for people to use the technology, b) provide a rich API to adapt and enhance the default behaviour. In this case, it is very easy to write a custom after_save/after_commit hook which checks for dirty attributed, published=true conditions, etc. I think it's a much more maintanable way of achieving your goal then relying on smart defaults in thr library.\n. @AaronRustad OK, should we close this issue then?\n. @dfuentes It's possible to write a custom HTTP client class and pass it to Tire in the configure block. Not sure how it will behave with em-http-request, you can try it.\n\nI don't want to do it from browser since that exposes the ES cluster to the public.\n\nNot neccessarily, BTW: you can put eg. Nginx in front of it, and expose only certain endpoints, sanitize/verify payloads, etc for auto-completion requests.\n. I understand. There's a Faraday based client, so it should be very simple to check out em-http-request. What about this issue? Closing now, reopening if you bump into something?\n. Hi, this is code lifted from rack, any idea what is the TBLENCWWWCOMP_ constant anyway? :)\n. I don't like the removing of the constant too much here... Also, I said, that code is lifted from Rack, best keep it intact, I think what we need to do is skip loading the URL module in lib/tire/rubyext/ruby_1_8.rb if encode_www_form_component and decode_www_form_component are defined?\n. Perfect, thanks!\n. Hi, sorry, sure!, merged &\u00a0pushed, thanks @gondalez!\n. @mobilutz True, I think we should do a release :)\n. Released in v0.5.5 (f185eda)\n. @kristopher, yes, latlon as Array are \"reversed\" in the standard, confusingly enough. Can you edit the Wiki?\n. Thanks! :) Yes, the order is confusing, but is a standard... You mean this page? https://github.com/karmi/tire/wiki/How-to-work-with-locations-(geo)-in-Tire\n. Thanks, @kristopher!\n. Yeah, but that commit was really needed so Mongoid integration works at all... Where is the exception thrown? In JSON library, in Tire, in Elasticsearch? Notice:\n```\n\n\nOj.dump(nil)\n=> \"null\"\n``\n. @luislavena OK, understood, sorry. The thing is, I don't think we need all ofactivesupport/json`: the ActiveSupport dependency is brought in by ActiveModel, and only selected parts are included explicitely.\n\n\nBut I agree that the as_json call in Index#get_id_from_document makes it now very fragile. Let's try adding it and see how much it eventually breaks for people.\n. > It looks like #save calls update_index which in turns calls index.store but does not pass on the _parent in the options hash.\nIndeed. I think your best option here is to provide the indexing logic yourself. It is imaginable the library would be able to introspect your models and perform this automatically, but I think it's better to leave complicated scenarios to application code. I'm open to any ideas or suggestions, on the other hand!\n. Closing.\n. I can't replicate the issue:\n$ rake environment tire:import CLASS=Article FORCE=true\n[IMPORT] Deleting index 'articles'\n[IMPORT] Creating index 'articles' with mapping:\n{\n  \"article\":{\n    \"properties\":{\n      \"title\":{\n        \"type\":\"string\",\n        \"index\":\"keyword\"\n      }\n    }\n  }}\n[ERROR] There has been an error when creating the index -- elasticsearch returned:\n400 : {\"error\":\"MapperParsingException[mapping [article]]; nested: MapperParsingException[Wrong value for index [keyword] for field [title]]; \",\"status\":400}\n. > It seems to me Elasticsearch is not capable of loading the files although the path is correct, could be a load order issue, I am really not too sure.\nYes, agreed. First thing, is Elasticsearch properly configured to look in this folder for config files? Second, those files are owned by root, under which user is Elasticsearch running?\n. Closing this, this is an Elasticsearch issue. BTW, I'd consider adding synonyms to my application and creating the correct analyzer via API.\n. I think this is a restriction on the Elasticsearch level then, please submit an issue or ask on the mailing list..\n. Have you looked into integration tests for the match/multi_match?\n. Could you explain it better and post code examples of stuff which si failing for you?\n. The query :per_page => 2 is invalid. Could you please look into integrations tests? https://github.com/karmi/tire/blob/master/test/integration/active_record_searchable_test.rb#L282-L295\n. > I've taken a look at the integration test but it just confirms that the second example works but doesn't cast any light on the first\nAh, sure. The convenience per_page/page don't work with the Tire.search, will need to have a look into that.\n\nis there a reason why there's no pagination in the multi_search integration test\n\nI think those convenience params are exposed only for models... Will try to have a look shortly into it.\n. You can work with the default articles index, and just a very small per_page, eg. 1. The DSL tests are here, maybe just add a context there... But still on the fence with the feature -- with the DSL searches, I think it's better to use the size/from methods...\n. Any new info about this? I would really expose these params only for models.\n. Closing, feel free to reopen with more info if needed...\n. @relevante Please post some code :)\n. 1/ Depends on how the inc is reflected in to_indexed_json, that should be easy for you to check out in Irb/Rails console\n2/ Remember, there's a 1sec delay between Elasticsearch's indices being refreshed, ie. before the results are available for search.\n. > The issue appears to be that whatever hook causes Tire to update its indexes isn't called when inc is called.\nIt uses the after_save callback, and I encourage everyone to use a custom strategy here, when they need something other then the na\u00efve default.\nAlso, remember that you have to call refresh so your changes are immediately available in searches, eg. in tests.\n. @victor-huang I don't understand the issue you're reporting. See:\n```\n$ irb -I lib/ -r tire\n\n\nload 'test/models/persistent_article_with_casting.rb'\nrequire 'active_support/core_ext/hash/indifferent_access'\nauthor = Author.new( first_name: \"yy\", last_name: 'nicken')\n=> #\nitem = PersistentArticleWithCastedItem.create( title: \"new item\", author: author)\n=> #, @last_name=#>, @validation_context=nil, @errors=#, @messages={}>, @tire=#>, @id=\"7eYNu23WT86QN4FHo8Ur1w\", @attributes={\"_index\"=>\"persistent_article_with_casted_items\", \"_type\"=>\"persistent_article_with_casted_item\", \"_version\"=>1}, @tire__attributes={\"matches\"=>nil}>\nsitem = PersistentArticleWithCastedItem.search('new item').first\n=> #\"yy\", \"last_name\"=>\"nicken\"}, @last_name={\"first_name\"=>\"yy\", \"last_name\"=>\"nicken\"}>, @id=\"7eYNu23WT86QN4FHo8Ur1w\", @attributes={\"_score\"=>0.16273327, \"_type\"=>\"persistent_article_with_casted_item\", \"_index\"=>\"persistent_article_with_casted_items\", \"_version\"=>1, \"sort\"=>nil, \"highlight\"=>nil, \"_explanation\"=>nil}, @stats=nil>\nsitem.author\n=> #\"yy\", \"last_name\"=>\"nicken\"}, @last_name={\"first_name\"=>\"yy\", \"last_name\"=>\"nicken\"}>\nsitem.author.class\n=> Author\n```\n\n\nPlease provide more info or close the ticket.\n. Closing, feel free to reopen with more info if needed...\n. Please see http://karmi.github.com/tire/#section-Configuration :)\n. Ah, sure, you need to use the results method of the object.\n. @miry Should we close the issue then? You can store your analysis/mapping definitions in whatever Ruby structure which makes most sense to you, and either manually indices, or pass these configurations to Tire's settings, mappings methods.\n. True, neither version nor version_type are supported in Index#store, though trivial to add.\n. version_type is a regular URL parameter, set it to external...\n. Yes, exactly. It's not supported yet, you can add it as part of your pull request?\n. Example of the requests with version_type:\n``` bash\ncurl -X DELETE localhost:9200/test-version-type\ncurl -X POST localhost:9200/test-version-type\necho\ncurl -X PUT 'localhost:9200/test-version-type/d/1?version_type=external&version=100' -d '{\"title\":\"A\"}'\necho\ncurl -X PUT 'localhost:9200/test-version-type/d/1?version_type=external&version=101' -d '{\"title\":\"B\"}'\necho\n``\n. Hmm, true --version_typeis not supported inIndex#bulk_store`...\n. Hi, sorry for the ridiculous reaction time! I've merged and refactored the commit a bit, it's now on master.\n. Hi, sorry it took so long, merged as 525cf9c and 669e570 (#633)\n. Closed in 525cf9c\n. The documentation is incorrect in the contributed module, see test for proper syntax: https://github.com/karmi/tire-contrib/blob/master/test/queries/custom_filters_score/custom_filters_score_test.rb\n@presdo: Can you look at the docs, please?\n. @presdo, https://github.com/karmi/tire-contrib/blob/master/lib/tire/queries/custom_filters_score.rb#L31-L32\n. @RoyLee The query = Query. (...) part is not correct. Just use the query type normally:\n``` ruby\nsearch_query = 'maz'\nsponsored_brands = ['ford', 'jeep', 'mazda']\nsearch = Tire.search 'cars' do\n  query do\n    custom_filters_score do\n  query { term :brand, 'bmw' }\n  filter do\n    filter :terms, :tags => sponsored_brands\n    boost 2.0\n  end\n  filter do\n    filter :terms, :tags => sponsored_brands\n    script '_score * 2.0'\n  end\n\nend\n\nend\nend\n```\n@presdo Can you add an example with the DSL integration into tests?\n. > Do I need to add a require somewhere to access that functionality?\nIndeed! require 'tire/queries/custom_filters_score' :)\n. @RoyLee, you need to add the tire-contrib gem to your Gemfile. Let me know if this helps.\n. Closing, feel free to reopen with more info if needed...\n. Please provide more context, full data &\u00a0query, etc...\n. Good! I must admit I was a bit confused by the issue -- no MongoDB user ever reported something like this, and there's plenty of ES <> Mongo deployments via Tire out there... The IDs between search results and your mongo data should really be 1:1 consistent.\n. Not, but I was toying with an idea to write implementation for https://github.com/meskyanichi/backup.\nIf all you need to do is export index to CSV, use the Index#scan method, it's less then 10 lines of Ruby:\n``` ruby\nrequire 'tire'\nrequire 'csv'\ncsv = CSV.open(\"people.csv\", \"wb\")\nTire.scan('test-people').each do |results|\n  results.each { |r| csv << r.to_hash.values }\nend\n``\n. @amejiarosario Use the new gem, use thescanmethod, that will give you batches of data to store.\n. Hi, Tire goes to some lenghts to _not_ collide with any already defined methods, with thetire` proxy (see README).\nIn the matches case, there has been a bug, see issue #501, but it has since been fixed by 69234af. What version this is on? \n. The commit is included in that version. Again, Tire does not override any already defined method names, including matches, please see tests. You need to provide more info about your model etc.\n. Closing, feel free to reopen with more info if needed...\n. Hi, absolutely right. The after_commit / after_rollback hooks are ActiveRecord specific, so we must take care here. Since the callback integration is very simple, I think we can add it generally, not with a opt-in layer. I will have a look into it this week, please ping me if I slip.\n. @tmaier Look at #640, actually, using after_commit would introduce inconsistencies between the database and the search index. This will need some more thorough thinking -- any comments, ideas?\n. @donaldpiret True! But the way I see it, if you're concerned about the HTTP call inside the update_index on each save, you should have a specific indexing strategy, ie. backgrounding the index operation (Threads, Resque, Sidekiq, RabbitMQ, etc). (No argument though, that for ActiveRecord::Base models, we should fire indexing on commit, not save.)\n. @jeremygpeterson See the caveat in 38492a5832160642b669630892554c74c7101f5c. I'd like to have a look into the hooks in Tire to make use of the after_commit.\n. @aufi The exception being raised in that situation is Errno::ECONNREFUSED, as you would expect.\nThe transaction is being rolled back, however, because MyModel.tire.update_index (called from after_save hook) will return false even if you catch that exception.\nIn a situation like this, it's worth it to decide how would you resolve potential inconsistencies between your database and search index: do you want to store everything in the database, even when indexing fails, and resolve inconsistencies later? Then you probably shouldn't use the default Callbacks module and invest time in more sophisticated workflow. \n@nz Actually, this is the case where after_commit would be harmful, because it would introduce these inconsistencies -- some data would be in the database, some in the search index.\n. > So, I changed Tire's model Callbacks from after_save to after_commit (and set up error notifications to reindex\n\ndata manually).\n\nBy forking, monkey patching, or? Why not just roll your own indexing strategy then? See README for hints.\n. An option, but this is not a generic solution and it would be much easier for you to just call the tire.update_index inside your class definition, based on conditions you need to be matched, without all the Tire sauce around :)\n. An option, but this is not a generic solution and it would be much easier for you to just call the tire.update_index inside your class definition, based on conditions you need to be matched, without all the Tire sauce around :)\n. That makes sense, thanks for looking into it. I'll try to get to this soon.\n. Yes, I know, they are related to percolator and dis max I think, didn't yet found a good solution for them.\n. So, any resolution to this? Should we close it? It won't work like this anyway...\nFor context, see the Stackoverflow answer: http://stackoverflow.com/questions/9796470/random-order-pagination-elasticsearch\n. Closing, feel free to reopen with more info if needed...\n. Thanks!, merged &\u00a0pushed.\n\nRe the Travis failures -- there has been some incompatibilites with Ruby 1.8, which were shooting down the tests, and the percolator tests are still failing consistently. I have tried to fix it in 1a8b921, but will need to have a better look into it.\n. Yes, correct -- if you set index name via MyModel.index_name, the prefix is ignored. Do you think it makes sense to support it for custom index names?\n. @marclennox I think that's a good approach, and since index_name can be defined as a block, makes it very readable and more transparent, I'd say... Closing this, if that's OK.\n. Cool.\n. It's implemented on master currently, see #520 \n. @fbernier Yes, sorry I didn't get here sooner, indeed the discussion in #92 is what I would point you to. Feel free to update the thread or open new issue for questions.\n. Hi, that would be very strange -- it doesn't matter if you execute the code in controller or in console. Please post relevant info from your console session / model setup.\n. @Will-Sommers Cool, then close the issue? :)\n. Hi, finally got to it, merged &\u00a0pushed, thanks!\n. I'm not sure I understand 100% your question/problem, but if you're importing records across tables, and you want to fight n+1 queries, you can pass relevant snippets as params to the paginate method:\nruby\nbundle exec rake environment tire:import PARAMS='{:include => [\"comments\"]}'  CLASS=Article FORCE=true\nSee this StackOverflow question for context: http://stackoverflow.com/questions/13600086/index-the-results-of-a-method-in-elasticsearch-tire-activerecord/13847929#13847929\\\nIf you're loading substantial amount of data, not that paginating with LIMIT/OFFSET is not exactly effiecient, and find_in_batches does a better job -- see code in issue #530 \n. Hmm, I'm wondering how this would happen -- the to_indexed_json with { :id => id }.to_json should just fetch the ID and that's it. Would it be possible if you add more info to the issue, Class definition (relevant parts), and the output of the importing -- ideally with Tire.logger and ActiveRecord.logger set to STDERR? Gist/pastie/etc usually works better if the outuput is huge...\n. @mikecx Hi, any news on this?\n. Closing, please reopen with new info, OK? Thanks!\n. Yes, Simon's explanation is perfectly valid, this a synthetic data set and the scoring functions have too little to work on.\n@brupm, try printing the scores as well:\nruby\nm.results.to_a.map do |result|\n  puts \"* #{(result.first + ' ' + result.last).ljust(30)} | score: #{result._score}\"\nend\nYou'll notice groups of results with the same score, where the results will be sorted \"randomly\".\n. Hi, this is strange. Just tested some script on 2.0.0-p0 and I can see the results just fine. More importantly, the whole Tire integration test suite runs on 2.0.\nNotice you're using load: true -- is it possible something is off with the dabatase? What kind of database it is? Can you turn on logging for Tire and ActiveRecord/Mongoid/etc?\n. Ah, hopefully it'll get fixed soon. You can overload the Collection#__find_records_by_ids -- not the nicest solution, but will get you there. (It'd be worth it here to allow pass a different, custom \"collection\" class.)\n. Hi Jason, thanks for the pull request, it shows that you care a lot about the library, and I'm very happy to see that. There are, however, many issues with the proposed changes.\nThe first -- and most important -- one is that the Rake task is a convenience, a simple way how to get started and import your existing data into Elasticsearch. There are many potential pitfalls -- for instance, when importing large amounts of data from a RDBMS, you'll probably want to use ActiveRecord::Base#find_in_batches instead of pagination [#530].\nI think we should encourage people to write their own importing code, instead of trying to support every corner case in the Rake task, and make it unnecessary smart. This is Ruby, after all, and Tire gives you a rich API for custom importing routines and workflows.\nNow, there are some practical issues with the proposed changes as well:\n- I don't like the dependency on the progress_bar gem. Any dependency is a liability and I'd like to keep it down as much as possible. I agree the custom progress bar code could be replaced, but I'd use eg. ANSI::ProgressBar, which is part of the ansi gem. (The Turn gem already brings Ansi as a developlment dependency.) I just don't think it's worth it, since ansi gem is widely used with wildly varying version constraints and you have to bundle exec everything.\n- Automatically depending on environment is precisely the sort of pattern I don't like in gems. What if I'm not inside a Rails application, what if I'm using Sinatra or there just isn't the environment task? What if I want to write my own Rake task and just invoke the Tire one?\n- I'm not sure about the utility of importing multiple models in one go. Again, I understand how it might simplify certain use cases, but notice how you need to manipulate TOTAL_COUNT, split the environment variable... Why, instead of adding complexity to the Tire task, a person who needs this, doesn't simply write a custom task which just invokes the Tire task multiple times?\n- Any \"automatic detection\" of anything is a nightmare for me :) First thing, it has to work, which I bet it does -- but it's very hard to test in automated manner. So for any refactoring/change of this code, you have to have a Rails app ready, and try it manually. Believe me, it takes a lot of my time to keep checks on things like that, making sure the task works for ActiveRecord as well as for Mongoid, etc. I'm a fan of \"prefer declaration over introspection\" in a situation like this.\n- I love the introspection on find_in_batches, though! I think the task isn't a good place for it, since it is not part of the Tire Ruby API. Model::Import::ClassMethods#import and Import#index would be a much better place, and we already support method. The model could do the introspection and pick either paginate or find_in_batches -- or any other \"bulk find\" method supported by OxM libraries. We should then skip defaulting to paginate in the task.\n- The paginate method has ben injected in the task as a convenience in 3925c24, since with Kaminari, there is no paginate method by default -- this forced users to write their own paginate method just for the import task. See issue #192.\n- The klass.respond_to? :all clause is particularly dangerous, because that would just blow up on huge datasets, without people being aware what's going on. It would just \"not work\", and these (rightly) frustrated users would then submit tickets. We don't want that :)\n- I love the notion if MyModel.indexer scope, that's very clever. It nears the thin line where people should write their own custom indexing routine, but is maybe worth it.\n- Why the call to GC.start for each batch?\nI'm sorry for such a long and potentially unpleasant review, but in the end, I'm the one who's there when stuff breaks or behaves weird, and need to exercise caution... I think we should focus on the great ideas you had in the patch:\n- [ ] In Model::Import::ClassMethods#import, introspect the host class, and use find_in_batches if present\n- [ ] Explore the notion of MyModel.indexer to define the scope for ActiveRecord etc. This could be part of the model DSL such as mapping, etc.\n. Hi Jason, thanks for the update and all the care!\n1/ I agree we should use ansi/progress_bar instead of the custom code. Another dependency though :/, pretty breakable one on top of that... I like how you make the progress_bar in the task optional, maybe we can make the dependency optional. Maybe I'm overly cautious :)\n2/ Regarding the rails environment, I think it's best to leave it up to the user, really (Resque AFAIK does the same). But I agree that with the if defined?(Rails) check, this argument is weak...\n3/ I am still opposed to importing multiple models in one go... It makes the code more complex, and I don't really see the utility here. Why not import the models separately? If we really want to do this, then we should have a separate Rake task tire:import:all. I can see an argument for renaming the task to tire:import:model then, and prepare the ground for a generic tire:import task.\n4/ If we go this route, I'm really starting to be concerned about testability of this code. When it's stashed inside the tasks.rb, there's no good way how to test it.\n5/ The introspection in model/import.rb must be much more robust, not default to paginate, throw an error if the model doesn't have neither find_in_batches nor paginate, etc. It would be cool to research batch finders for most popular OxM libraries.\n6/ I'm terribly sorry, but I don't like all the magic in dependents for models, loding based on DIR etc. As mentioned above, this is a separate Tire feature with tire:import:model:all.\n7/ I like the idea behind index_scope, this issue turned out multiple times already, but I think we should extract it into a separate feature/issue. There's already lots of going on in this issue...\n8/ Also note, that the whole section on importing in the README must be updated and rewritten.\n. Importing revamp added in #655 and #656, released as part of v0.5.6.\n. Hi, I remember there have been some issues related to that on Mongoid, can you check it, please?\n. Thanks! I'll verify it on a Mongoid app laters as well. Please ping me if I slip.\n. Also, there's lots of going on in #652, where we wouldn't depend on pagination libraries for RDBMSes. Sadly, there's no common find_in_batches for ActiveModel.\n. @tpitale I'll verify it with a Mongoid + ActiveRecord apps laters!, I have some ready for this sort of manual testing.\n. Hi, verified the injected paginate works for ActiveRecord and Mongoid importing. Merged &\u00a0pushed, thanks!\n. @jasonfb Your syntax is correct. lat_lon indeed isn't a very good name for the field, I'd use location or similar.\nUser.index.delete && User.import is correct, but the index is not created by the import method, instead Elasticsearch creates the index with dynamic mapping. This is the root cause of your error. The correct way would be:\nUser.index.delete\nUser.create_elasticsearch_index\nUser.import\nOr use the Rake task, rake tire:import CLASS=\"User\" FORCE=true. Let me know if this helps.\n. Closing, feel free to reopen with more info if needed...\n. @jwaldrip Yes, it has been. I like the general intention of the patches, but I need to review to code. There are some parts where I need to polish it a bit only, and other parts where I want to consider implications to the codebase.\n. @jwaldrip The basic idea is OK!, I'll take the code and take it from there, it needs only some reshaping.\n. Hi Jason, I took the idea and added support for different importing strategies. Tested only manually, don't have the bandwith to add a full fledged test coverage at the moment.\n. Been thinking about this for a while, and I still like the idea behind it, but not the exact implementation. The scope would be picked up only for batch importing, not when storing records individually, which someone might find surprising. A feature like this would have to documented in detail, on top of that.\nFrom a rather philosophical standpoint, I don't see the library as trying magically support any use case and offer API/DSL for any imaginable scenario. I'd like it to provide very clean and convenient building blocks, so anybody can very easily write eg. their own import routines, in Ruby, using all the power of the language.\nIn other words, I see more benefit in having a nice chapter in documentation elaborating on how to achieve this (\"automatically restrict search results to a certain ActiveRecord scope\"), instead of providing an API how to do that directly. \nSo, let's imagine we have this system, where we want to index only published articles. All the code needed to customize Tire for this would fit on a handful of lines:\n``` ruby\nModel\nclass Article < ActiveRecord::Base\n  include Tire::Model::Search\n  ...\n  scope :published, where(:published => true)\nafter_commit  { tire.update_index if published? }\n  after_destroy { tire.update_index if published? }\n  ...\nend\nRake task\ndesc \"Import articles\"\ntask :import do\n  index = Tire::Index.new( ENV['INDEX'] || Article.tire.index.name )\nif ENV['FORCE']\n    index.delete and index.create( :mappings => Article.tire.mapping_to_hash, :settings => Article.tire.settings )\n  end\nArticle.published.find_in_batches do |batch|\n    puts \"Importing #{batch}\"\n    index.import batch\n  end\nend\n```\nWhat do you think?\n\nRegarding the example with includes(:comments), that's already supported (bcbe4dd):\nrake environment tire:import PARAMS='{:include => [\"comments\"]}' CLASS=Article\nThough I'm not sure if the ActiveRecord API still supports it.\n. Jason, first, nothing makes me more happy then to see Tire being used in a more complex setup like this!\nSecond, my vision for Tire or its successors is precisely this: provide building blocks to assemble a complicated domain like that. At the moment, Tire has a lot of quirks and lacks support to make this really a joy.\nI understand what you're saying, but in my eyes, it supports the argument I'm trying to make -- that eg. the bundled import task is merely a starting point, a convenience, and that it shouldn't be complicated or convoluted to write your own implementation, for your own domain. Notice how in this area, I always say \u201cjust don't include the Tire::Model::Callbacks module then\u201d. The mechanics of indexing are quite simple, as outlined in the snippet I've posted. Of course, when you have a complicated domain to work with, it's not easy -- but that's an intrinsic aspect.\nAs a side note, the code example you've posted illustrates something more problematic in Tire, and that is the whole confusion between mapping and to_indexed_json, which is something I'd like to see solved much more then any ActiveRecord scoping issues and such. It's evident how hard are you coding against the library, and that is something which I don't like to see; the building blocks are not matching well here.\n. I'm not a huge fan of manually triggering GC, but pulling this in, we'll see how it turns out. Thanks!\n. Ah, good catch!, thanks -- fixed.\n. The filter method is really only a very thin wrapper around the Elasticsearch's DSL, so in the most primitive way, you can write it like this:\n``` ruby\nrequire 'tire'\nsearch = Tire.search 'users' do\n  query do\n    filtered do\n      query { all }\n      filter :nested, path:  'apps_events',\n                      query: { filtered: {\n                        query:  { match_all: {} },\n                        filter: {\n                          and: [\n                            { term: { 'apps_events.status' => 'active' } },\n                            { term: { 'apps_events.type'   =>   ['sale'] } }\n                          ] }\n                        } }\n    end\n  end\nend\nputs 'HASHES:',\n     search.to_curl,\n     '-----'\n```\nOf course, nothing holds you to be smarter, and define any query as a Ruby object, which could be reused across the application, like this:\n``` ruby\nrequire 'tire'\nDefine a filter with the Tire DSL\n\nnested_filter = Tire::Search::Query.new do\n  filtered do\n    query { all }\n    filter :term,  { 'apps_events.status' => 'active' }\n    filter :terms, { 'apps_events.type'   => ['sale'] } \n  end\nend\nputs \"NESTED FILTER:\",\n     nested_filter.to_hash\nsearch = Tire.search 'users' do |s|\n  s.query do\n    filtered do\n      query { all }\n      # Merge the defined filter as a hash into the nested filter\n      filter :nested, {path: 'apps_events'}.merge(nested_filter.to_hash)\n    end\n  end\nend\nputs 'RUBY:',\n     search.to_curl,\n     '-----'\n``\n. Cool! \u2014 a proper documentation would be even better :) We'll get there! :metal: \n. Ah, sure!\n. You should be able to [pass search definitions as a Hash](https://github.com/karmi/tire/blob/master/test/integration/dsl_search_test.rb#L10-L16) in Tire as well.\n. Ah, sorry, sure! Fixed now on master, see 904798f\n. @niuage My immediate ideas/questions would be:\n- Isn't this on a stale index? What happens when you delete and re-populate the index?\n- TheItemclass could be problematic, Tire does use a class like this, hopefully there's no clash.\n- I see thesocket_countbeing part of thesocket_storeobject -- can you please pastie/embed a) mapping definition (as JSON or Hash), b) the JSON fromhttp://localhost:9200/yourindex/yourtype/someid?\n. I'm sorry but I can't see how \u201cit is very hard to access the values in the item object\u201d. Would it be possible to provide a code example, or preferably a test case?\n. Ah, sure. Can you add a unit test for this change?\n. Tire [tries hard](https://github.com/karmi/tire/blob/master/lib/tire/model/search.rb#L297) not stomp on previously defined methods, and puts everything intotire` proxy by default. However, it depends on the load/include/etc order.\nI can successfully do this:\n```\n    $ bundle exec rails console\n>> Article.import\nArgumentError: Invalid arguments!\n  from /Users/karmi/.rbenv/versions/1.9.3-p0/lib/ruby/gems/1.9.1/gems/activerecord-import-0.3.1/lib/activerecord-import/import.rb:199:in `import'\n  ...\n\n>> Article.tire.import\n  Article Load (15.4ms)  SELECT \"articles\".* FROM \"articles\" LIMIT 1000 OFFSET 0\n  ...\n\n```\nSo, it depends on the order in Gemfile, require, include, etc. Can you provide more info?\n. For completeness sake, if that would be your own custom methods, you'd need to do something like this:\n``` ruby\nclass Article < ActiveRecord::Base\ndef self.import(*)\n    p \"MY IMPORT\"\n  end\ninclude Tire::Model::Search\n  include Tire::Model::Callbacks\n...\n``\n. Cool!\n. Version 0.5.5 has been released from f185eda\n. Hi, not knowing the context, but I think the correct way would be to trap the call toupdate_index`, like this:\n``` ruby\n$LOAD_PATH.unshift File.expand_path('../../lib', FILE)\nrequire 'active_record'\nrequire 'tire'\nrequire 'test/unit'\nrequire 'shoulda'\nrequire 'mocha'\nActiveRecord::Base.establish_connection( adapter: 'sqlite3', database: \":memory:\" )\nActiveRecord::Schema.define(version: 1) do\n  create_table :books do |t|\n    t.string   :title\n    t.timestamps\n  end\nend\nclass Book < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nend\nclass MyModelTest < Test::Unit::TestCase\ncontext \"Book\" do\n    setup do\n      @book = Book.new title: \"How to impress your Rails friends with elasticsearch\"\n    end\nshould \"call `update_index` on save\" do\n  @book.tire.expects(:update_index)\n  assert @book.save\nend\n\nend\nend\n```\n. Feel free to post the full test case, can have a look. Tests can be pain like this sometimes...\n. Closing, feel free to ping back if there's a problem.\n. Hi Frederick, sorry for the silence.\nWell, the Index#store presently extracts the :parent information from passed options. The trouble is, as you hint, in the update_index method.\nSince we don't have any DSL support for parent/child in Tire yet, I think the easiest option would be to add an optional options argument to update_index, where you could pass the :parent option down the chain. Do you think this would be enough for your use case?\nIt would be nice to pass it automatically based on some model mapping introspection, but I think this would be better handled by a more robust DSL support. What do you think?\n. > still makes things hard if you use the callbacks and you're in control of the call to update_index\nSorry --  do you mean not in control of the call? I think that presently, using parent/child would require users to take indexing into their own hands, and not use the automatic callbacks. (Otherwise, we would need the introspection.)\n\nmark an attribute of the child as the one that identifies the parent\n\nYeah, and injecting it into mapping as the _parent, that sounds good. There's no support for parent/child queries yet, IIRC you've added them some time ago in your fork?\n. @fcheung Been pondering this for a while, and I think it's not worth it to add this support to the ActiveModel callbacks... I think for parent/child, one must index in a custom way anyway, and I'm wary of \"false promises\" here... What do you think? I'd vote to close this issue and return to it when time is right...\n. I've added lot of info about handling indexing models manually to the README in the meantime, so closing this issue, if that's OK.\n. Yes, I bet many people background the indexing task like that... Just don't include the Callbacks module, and intercept the after_commit/rollback/etc events in your code, and Resque.enqueue them.\nThe downside here is that Resque brings an overhead, and within write-heavy application, your queues could be backed up. I think Sidekicq is worth exploring here, might have smaller footprint.\nWhen you'd do this, I'd definitely put up some monitoring here, so you know when your data aren't indexed -- this could lead to surprising results. For example, you can store the IDs of records which weren't indexed, and periodically re-index them via sheduled Resque job, etc.\n. @jasonfb Closing, reopen or ping if you need more help, please.\n. Hi, what kind of escaping do you mean, can you provide examples, links?\n. @jordan-thoms Please provide examples or close the issue.\n@Hengjie That is expected -- the article uses the string query with a Lucene query string syntax. This query is generally not good for exposing the search feature to users, a match query is better here and the article is factually incorrect. Thanks for pointing the article, will try to post comment there.\n. @jordan-thoms That is expected -- the query_string query exposes the full Lucene syntax, so it's very good for \"playing\" with search, but not for a generic \"here's a search box\" functionality. Thank for the hint regarding docs/example Rails template -- I'll update it.\n. @Hengjie The problem must be elsewhere:\n``` ruby\nrequire 'tire'\ns = Tire.search do\n  query do\n    match :_all, '[foo] bar'\n  end\nend\np s.results\n```\n@jordan-thoms Closing the issue, OK?\n. Hi, I'm wondering what would be the best approach here on the Tire side... I think leaving errors like these bubble up and crash the application code is the only sensible option; you can always rescue them there.\nI suspect this is about automatic indexing in a model via including Callback, right? As noted across the issues, that's only a \"good enough\" convenience mechanism to get started or for simple cases -- anything complex, write-heavy, etc. will probably need you to handle the indexing workflow yourself... What do you think?\n//cc @nz\n. That would be a good approach, trying it couple of times. Yes, the solution would be to not include the Callbacks module, and intercept save/commit/destroy/etc ActiveRecord hooks. It would make sense to use the \u201cretry couple of times\u201d logic in there.\n. Cool, closing the issue then?\n. Hmm, nothing strikes me here... There's definitely an invalid JSON in your snippets, \"name\":n... -- note the missing quote. Could you post the full backtrace of the error, so it's evident where exactly it starts?\n\nNotes aside: \n1/ Oj is generally more performant JSON library\n2/ Unless you use faceted search, using a filtered query with filters on category_id, etc. will speed up the searches and make the geo location computation faster/cheaper, since non-matching documents will be cut out of the resultset first, not last.\n. The error indicates an incorrect JSON comes from Elasticsearch. I have no explanation for that, apart from some proxy in the middle or client library mangling it somehow. Is the problem solved or persists?\n\nFor your query, you should use a filtered query, not top level filter, preferably.\nAlso, you should use a bool for category_id and wheelchair, and as a separate must clause use the and filter with your geo filter. This way, you'll exploit the caching system of Elasticsearch, and limit the number of documents the geo filter will operate on. Try it, you should see a performance boost even on a moderate dataset.\n. >  I ditched elasticsearch in production for now, cause i couldn't figure out how to fix this issue.\nTry configuring Tire with Tire.configure { logger 'tire.log', level: \"debug\" }, so the requests and responses are printed, and post a link to the file, if the problem persists.\n. That's a bit huge, right :) I'd suggest trying to switch to a different JSON library (Oj), just to see if the problem will appear there as well.\n. Hi, the trouble here is that there's no common find_by_ids fro ActiveModel, and Tire tries to work with many other OxM frameworks apart from ActiveRecord. I think there's been some discussion about it in the issues already, the main reason for the refactoring in bac091a and introducing of __find_records_by_ids was to provide a hook point for people needing to do this.\nI think the real solution would be to abstract this behaviour even further, and provide and extension (possibly autoloaded) for ActiveRecord, which would use find_by_ids...\n. Closing in favor of a more robust solution, which provides an extensions when the host class inherits ActiveRecord::Base and automatically changes __find_records_by_ids in the outlined sense -- is that OK?\n. Sounds like a very useful thing! I don't have any first-hand experience with Draper -- could you provide an example here, so we have something to discuss and play with?\n(Also, note that you can easily configure your own \"wrapper\" for the results, instead of the default Results::Item.)\n. Mattias, thanks for all the digging! It was my initial impression that it should be achievable easily, so it's great that it works out of the box.\nSo, just to check:\n1/ You decorate the Item instances just by providing a different :wrapper class? (You've used Item which is a bit confusing to me :)\n2/ You decorate the search results Collection by passing it to your decorator? (ie. MyResultsDecorator.new(search.results.to_a))\n\nan object that actually defines the hash keys as methods dynamically and not relying on method_missing\n\nAs you've discovered, there's been the respond_to? added inbetween, so it should work. The case you have is weird, notice there's a test basically doing that... I'll check it once again.\nLastly, the reason for method_missing is (as usual) conveniece/laziness -- given how dynamic Elasticsearch is, many different documents come out of a simple Tire.search('indexA', 'indexB' ,...) -- I guess it wouldn't be practical to define accessors for all of them dynamically. Maybe it would even impact performance significantly.\nFor future versions, I'd like to research all the \u201cobjectified Hash\u201d libraries once again -- in the beginning, I wanted full control in this part and not used something like Hashie or Hashr. (Notice, though, that the Model::Persistence support uses Hashr to cast Hashes.)\n. Mattias, that's terriffic new. I'm glad it works like this, because augmenting the result items/collection is something users often seem to struggle with.\n\nIf you want I can write a wiki entry with a detailed explanation on how to use Tire with Draper.\n\nI was about to ask you just that :) That would be really, really great!\n. That is an awesome writeup, thank you!\n. Hi, sorry for not getting to you sooner. There's some problem when connecting to Elasticsearch, notice:\n/gems/tire-0.5.5/lib/tire/index.rb:143:in `store'\n/gems/tire-0.5.5/lib/tire/model/search.rb:148:in `block in update_index'\n/gems/activesupport-3.2.6/lib/active_support/callbacks.rb:403:in `_run__42179389288198320__update_elasticsearch_index__2812542342298808294__callbacks'\nWhen you include the Tire::Model::Callbacks module, Tire tries to update the Elasticsearch index whenever you save the record. In this case, the request fails/timeouts.\nSo the question is:\n- Have you solved the problem in the meantime?\n- How is Tire configured, leaving the default localhost:9200 on?\n- If so, what happens when you curl localhost:9200?\n- If this is some kind of hosted ES, what kind and how you configure it?\n. Glad it's resolved! I think the problem was either in Elasticsearch not running or not restarting application code, or something like that.\n. Hi, I understand the issue, after considering it for a while, I don't think we can do much about it without either breaking compatibility or complicating the codebase too much.\nIn your example, just add the document type as a part of the index argument:\n``` ruby\nTire.search('articles/article').to_curl \nTire.search('articles/article', {query: {match_all: {}}}).to_curl \n``\n. Yeah, that's expected -- Tire checks for the existence of an index corresponding to model on startup. You have to stub the call or allow this specific call in Webmock.\n. The best place would be in your test_helper.rb, where you setup/require Webmock. Or, alternatively, you can [allow localhost](https://github.com/bblimke/webmock#external-requests-can-be-disabled-while-allowing-localhost) and just stub/mock when you need to do it. Note though, that for integration tests, best is to leave everything bubble up and down and work with real responses.\n. You have to stub the request or allow localhost _before_ you load Tire. I can't give you any more specific advice, sadly. Upload the relevant portion of the code somewhere and I can have a look.\n. Could you elaborate on what is your specific concern regarding thread safety? Could you provide a code example? (Note: In regular search/update/etc operations, theTire::Index` instance isn't manipulated.)\n. @mowings-ts Any new info here? I don't think we mutate state, but maybe there's something I'm missing.\n. @mowings I really can't say, that part of code might be prone to race conditions...\nHowever, recently I did some short evaluations for the thread safety of the Curb client, and didn't run into issues, also some people run Tire in threaded environments and haven't reported Tire-related errors (though they report issues with Curb, see #144)\n\n(...) we need a pretty decent separation between model and search logic for design reasons\n\nI'm really interested in that, so the library might support you better. If you can spare couple of minutes, can you elaborate a little bit?\n. Closing this for now, if that's OK, please feel free to ping back with more info if you have something!\n. Hi, this is strange -- could you provide the exact request/responses from Elasticsearch (Tire.configure { logger STDERR, level: \"debug\" }) for the case where it fails? \n. @stefanofontanelli Any new info about this? I'm interested in the specific case where this could happen...\n. @dfuentes77 Thanks for the info! I'm all for fixing this so people are sorted out, but would like to know the root cause for this behaviour.\n@nz Nick, can you look into this? Why would Bonsai not return the standard ES response on /_aliases here?\n. @nz Thanks for confirmation. I'm not against being defensive in the code in question -- just that by default, Elasticsearch should always return valid JSON on myindex/_aliases.\n. @nz Thanks for the explanation, should we close the ticket?\n. @stefanofontanelli No worries. The thing is, Elasticsearch returns a JSON response like this:\n```\n$ curl http://localhost:9200/_aliases?pretty\n{\n  \"books\" : {\n    \"aliases\" : { }\n  },\n  \"articles\" : {\n    \"aliases\" : {\n      \"myalias\" : { }\n    }\n  }\n}\n$ curl http://localhost:9200/articles/_aliases?pretty\n{\n  \"articles\" : {\n    \"aliases\" : {\n      \"myalias\" : { }\n    }\n  }\n}\n```\nSo, when the code in question gets an incompatible JSON (MultiJson.decode('{\"foo\":\"bar\"}').inject({}) { |result, (index, value)| next result if value['aliases'].empty? ; result }). Now, as I said, I have nothing against defensive coding in principle, but:\na) this feels like a problem with ES providers such as Bonsai or Found, and they should return compatible JSON\nb) in a dynamic system like this we would have to \"double-check\" every variable/Hash key/etc all the time, then a refactoring comes, nobody knows if the defensive code is safe to delete or not, since there's no test, ...\nAgain, as soon as there's a reliably failing test for the feature, I'm all for it :) Does it make sense?\n. @nz Just notice that the Bonsai JSON output is perfectly valid for the code in question:\nMultiJson.decode('{\"m3z6xmmzeja13q54ho9a\":{\"aliases\":{}}}').inject({}) { |result, (index, value)| next result if value['aliases'].empty? ; result }\nIt wouldn't probably work as expected, but AFAIK shouldn't throw the nil exception.\nAlso please see my comment above why \"nil checks are always a good idea\" is not neccessarily always the case :)\n. @stefanofontanelli All of them should pass the code: http://pastie.org/private/if88c0nll6gpkwpfkasyq\nClosing this and let's reopen when we have more data/test/etc, OK?\n. @ggrillone Do you mean your Elasticsearch server is remote? Then setup the URL as @weblee advices. (If your database is remote, then you need to configure ActiveRecord properly.)\n. @ggrillone How is your app connected to the database then? Could you provide more info on your setup?\n. This is weird, the Tire callbacks don't really care where the database is... Can you post any error you receive?\n. In the default Tire::Callbacks module, the indexing is triggered by the after_save event. You can skip including this module, and do that manually: after_save { self.tire.update_index } and potentially log in this block. You can also turn on Tire logging with Tire.configure { logger STDERR } to see what's going on when you eg. create a new record in the Rails console etc.\n. Cool :) \n. @rahulag Please search the issues for find_by_ids, eg. #673, to see the reasoning why this is not a plausible solution.\n. Thanks!, updated as part of larger changes in efb9c14.\n. Hi, this is a bug in the code -- see also #663, I think the correct solution is to update the document with both _source and _fields, if present. The patch in #663 needs just a couple of unit + integration tests.\n. Grr, yes, #663 indeed :)\n. Yeah, thanks for the report, fixed!\n. Not at Rubygems, will push a new minor version to avoid confusion.\n. @ichilton Is it solved for you then or do you still have the issue?\n. This is a different error, I see:\n\nuninitialized constant HRULE\n\nThat is solved only in master, not in a release, will push out a version.\n. Hi, the reason why you're seeing inconsistent behaviour is because a) you're not creating the index explicitely, b) it all depends on whether you restart/reload your application/code, or not. Tire by default tries to create an index for the model with proper mappings/settings, when you start the application, and the index does not yet exists.\nSo, in your workflow, the correct sequence would be something like:\nIndexedThing.tire.index.delete\nIndexedThing.tire.create_elasticsearch_index\nIndexedThing.create {id: 1, region: \"US Hawaii\"}\nThe create_elasticsearch_index effectively does IndexedThing.tire.index.create(mappings: IndexedThing.tire.mapping_to_hash, settings: IndexedThing.tire.settings). Notice that the rake tire:import:model Rake task also allows you to re-create the index.\n. > Notice that the rake tire:import:model Rake task\nSorry, missed this is about Tire::Model::Persistence. Still, IndexedThing.tire.create_elasticsearch_index is the way to go.\n. This has been fixed in fb9b6fa, could you upgrade?\n. Are you absolutely sure the right version is picked up? Notice that with gem \"tire\", git: \"...\" you have to run bundle update tire to pick up changes. Use bundle show tire to print information about the version used.\n. Phew, in a slightly crazy meaning of \"fun\", I think this could be a large dose of it :)\nI'm not aware of any reason why it would be so -- note, that in the current version, we don't require the file from Rack, but rather a physical file in Tire codebase itself... Tried to replicate the issue, but I can include Unicorn before Tire just fine... \n. > why indexes count does not match to records count\nDo you mean document count in the index vs. record count in the database? Don't use Head for that, use curl  localhost:9200/myindex/_refresh &&\u00a0curl localhost:9200/myindex/_count.\n\nInitially the rake tire:import takes about 22 hours (...)\n\nIt depends on how your model(s) are set up. Recently, we introduced a change which uses find_in_batches for ActiveRecord, and is very fast, but doesn't prevent n+1 queries. Turn on ActiveRecord logging and observe the queries being issued to your database. 1.2mio of documents is a small corpus in Elasticsearch's terms.\n. @nazarhussain The important part is \"turn on ActiveRecord logging and observe the queries being issued to your database while importing\".\n. Sorry for keeping it hanging here. Basically, you should be able to import your data into Elasticsearch, and then just scp the data into a (stopped) production cluster -- provided versions match, etc etc.\nBut really, I wonder what's the bottleneck here, even with n+1 queries, 1.2mio documents should be imported in a reasonable amount of time. If n+1 queries (or other database bottleneck like this) would be the problem, the solution would be to write the indexing code yourself -- it's really only a matter of querying the database and feeding the data into ES via Tire::Index#bulk.\n. It's an interesting idea, but I wonder how to support that interface in the code...\nI mean, you can do this:\n``` ruby\nrequire 'tire'\nTire.index('articles') do\n  store id: 1, title: 'One', tags: ['ruby','python']\n  store id: 2, title: 'Two', tags: ['javascript']\n  refresh\nend\nf = lambda do |s|\n  s.filter :terms, tags: ['ruby']\nend\nsearch = Tire.search 'articles' do |s|\n  s.query { all }\n  f.call(s)\nend\nputs search.to_curl\n```\nTo support the API you outline, we would need to add a different method, or teach the filter method to take blocks...\nOn the other hand, what I have been frequently doing when extracting the filter definitions is something like this:\n``` ruby\nrequire 'tire'\nTire.index('articles') do\n  store id: 1, title: 'One', tags: ['ruby','python']\n  store id: 2, title: 'Two', tags: ['javascript']\n  refresh\nend\nf = [ :terms, tags: ['ruby'] ]\nsearch = Tire.search 'articles' do |s|\n  s.query { all }\n  s.filter *f\nend\nputs search.to_curl\n```\nDo you think this would be enough?\n. @mattiassvedhem I'm leaning towards closing this ticket for now, and keeping it in mind for future, what do you think?\n. Sadly no -- there's not yet a better, idiomatic way how to set index settings.\n. The Curb client does support keep-alive connections.\n. I'm not sure what you're referring to as \"persistent\", in the HTTP context, Curb just does it:\nruby\nTire.configure do\n  client Tire::HTTP::Client::Curb\nend\n. Can you add unit tests in the https://github.com/karmi/tire/blob/master/test/unit/index_test.rb#L243 context?\n. Marc, I'm sorry it took so long to get to this. I see that the diff for the patch is really long, with many formatting changes apart from the code you added... Has your editor/IDE settings \"improved\" the formatting in this way or was it intentional? :)\n. Added a test and merged, thanks!\n. Can you please check with https://github.com/karmi/tire-contrib/pull/15 and https://github.com/ResumUP/tire_am_serializers, and also add test cover for the feature?\n. @lister Sorry for the ridiculous delay! Merged the patch and added some tests for ActiveModel::Serializers (d2263fb)\n@mulderp Could you try again? Notice (d2263fb), that you need to specifically include the support in Tire::Results::Item, if you'd need to serialize those. Your models should work fine I guess -- please paste the fuller backtrace for the error.\n. @robinbrandt I like it, thanks!, will try to process it as soon as possible...\n. @robinbrandt By the way, what do you think about #283? Do you use geo filtering as well?\n. @robinbrandt Sorry it took so long! Merged and pushed, thanks for a great patch! I'll include it in the next release, once I solve other hanging issues, will release probably tommorrow.\n. @jeyb See also discussion in #602, please.\n. Sorry it hangs in there like this for so long. Any news on this in the meantime? I think you should synchronize with @timoschilling to work out some elegant solution which would pick correct strategy for __find_records_by_ids for ActiveRecord, Mongoid, etc. Let's keep the current implementation as default.\n. Just wondering about the state of this ticket -- I'm all for exposing ORM-specific finder methods in the __find_records_by_ids, but would like to have it done properly, so we can extend it, covered it by tests, etc. Currently, for a homogenous application, people can just override the method, but it would be cool to have a proper out-of-the-box support here...\n. Closing in favour of #810.\n. > we need to be able to find records, that are deleted in the database. deleted records are marked as deleted by\n\nsetting the deleted_at attribute to Time.now. which is handled by the paranoid gem.\n\nHi @lister, yes, as suggested, the best approach is to don't include the callbacks module, and handle it manually -- unfortunately you can't use the update_index method, you need to work with the MyModel.index.store @myinstance directly... I'm not familiar with the current behaviour of \"paranoid-like\" gems (https://github.com/radar/paranoia?) -- do they set the #destroyed? property to true?\n. Hi, you have two options here:\n1. Add the deleted:true|false property into the document JSON, and just filter everything deleted in Elasticsearch (if it would make sense to do it like this)\n2. Manually index the models -- I've added some info into the README about this.\nDoes it sound OK?\n. I think the issue here is that Tire tries to create the index for the model with all the specified settings/mappings when you load the model (in your app, console, etc), but skips it when the index already exists.\nYou have to MyModel.tire.index.delete && MyModel.tire.create_elasticsearch_index.\n. Argh, that's bad. Thanks for this report, will remove auto-invoking the environment task, never liked it anyway...\n. @jwaldrip I really think we're doing \"too much\" here. It's entirely in users hands how she wants to load the Rails environment? Running rake environment tire:import:model CLASS='MyModel' is perfectly fine, and will prevent any errors like this.\n. @brightbits That is definitely in the works for the future... .) But we have to keep the current gem clean &\u00a0working as well.\n. @jwaldrip It doesn't have much sense to extract the Rails related stuff from the Tire codebase -- it is quite coupled to ActiveModel (in Item) etc.\n. I like the injection of the strategy!, just a question if it wouldn't be possible to pass some options/arel blocks to the strategy down the wire? I was aware that we lost the ability to eager load associations when merging this, and didn't see a good way how to implement that.\nCouple of notes:\n1/ could you add a unit test for this?\n2/ maybe compress the code into one line:\nreturn const_get(options[:strategy]).new(klass, options) if options[:strategy]\n3/ Wouldn't the import method be a better location for this? It's kinda confusing to return the injected strategy from the Strategy.from_class method...?\n. It's a very good solution. People will just need pointers how to implement their strategy and use it. An integration test would be cool even for this -- I like to point people to working code instead to docs... What do you think?\n. @leschenko Hi, any news on this? I think an integration test as docs + plus the suggested changes would make this quite helpful for people who need to customize the import behaviour.\n. @leschenko Merged and pushed, thanks!\nExample:\nruby\nclass MyCustomImportStrategy\n  include Base\n  def import &block\n    klass.\n      where(published: true).\n      joins(:comments).\n      find_in_batches(:batch_size => options[:per_page]) do |batch|\n        index.import batch, options, &block\n      end\n    self\n  end\nend\n. Hey, this is strange, shouldn't it be handled by Rails itself?\n. Yes, the example app should definitely run out of the box. But it does on my machine:\n```\n$ gem list therubyracer\n LOCAL GEMS \n$ rails new searchapp -m https://raw.github.com/karmi/tire/master/examples/rails-application-template.rb\n...\n=> Rails 3.2.13 application starting in development on http://0.0.0.0:3000\n```\nWhat error do you receive?\n. Actually, on Ruby 2.0 I'm getting an error related to https:\nruby/2.0.0/net/http.rb:917:in `connect': SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed (OpenSSL::SSL::SSLError)\nI wouldn't like to add JS runtime to the example, since this is something Rails should take care of. Closing this for the moment, and will have a look on how to solve the SSL issues on 2.0...\n. There's no breaking change to the API and Tire is generally compatible for the common use cases. (Some tests such as explanation_test fail, but these are naming changes mostly.)\nThe new, more sophisticated sorting is not yet exposed in Tire, since its API is simple and takes just the pair of field and (sort) direction, the sorting support must be updated to allow passing Hash options.\nClosing for the moment, if that's OK, please feel free to open separate issue for new issues/questions.\n. You have to ask for the fields specifically:\n``` ruby\nrequire 'tire'\nTire.index 'articles' do\n  delete\n  create\n  store title: 'One'\n  store title: 'Two'\n  refresh\nend\ns = Tire.search 'articles' do\n  query { all }\n  script_field :foo, script: '\"bar\"'\n  fields :foo, :title\nend\nputs s.to_curl\nputs '---'\nputs s.results.to_a.inspect\n```\nThere are issues #663 and #687 opened for allowing to use _source in fields.\n. @amir20 Yes, thanks, this is a bug and it must be fixed. I hope to get to it soon, integrating both of those two other issues.\n. @amir20 The bug should be closed on current master, thanks for the report!\n. Hi, great! Can you please squash the commits into one and also remove the dependency from the gemspec?\n. > The dependency appears to be required, otherwise all will not boot\nIt's the same thing as with eg. the Faraday client -- you need to require the dependency externally, eg. in an application initializer, etc. On a second thought, can you please move the pull request from here to tire-contrib?\n. @mattiassvedhem The wrapper option should be passable to Tire.search just fine:\n``` ruby\nrequire 'tire'\nrequire 'virtus'\nclass Article\n  include Virtus\nattribute :title,   String\n  attribute :content, String\nend\ns = Tire.search 'articles', wrapper: Article do\n  query { all }\nend\nputs \"Title: %s\" % [ s.results.first.title ]\n``\n. Closing, migrating the issue to karmi/tire#716.\n. Hi, sorry for the ridiculous delay, @jocke12, merged and pushed.\n. @mattiassvedhem I'm sorry to keep it hanging like this for a long time, I'll have a look. The problem is that the \"passing Hashes to search\" usecases are really fragile in the current codebase... (Bad initial design decisions :)\n. Briefly verified the code and it seems be working fine, pushed. Thanks!\n. Hi, index_statsand_status` would be nice to add, but there's an ongoing effort focused on building a more robust Ruby library, which makes it less reasonable to add features to the current library.\nFor the moment, please use something (eyehurting) like this:\nruby\nstats = MultiJson.load( Configuration.client.get(\"#{Configuration.url}/i/_stats\").body )\n. Hi, is there any specific problem with the implementation? If I recall correctly, to_ary is used in Rails' views, have you verified it works correctly?\n. @brandones The to_ary modifier is used extensively in Rails views. There's no test for them in Tire test suite.\n. @brandones Any new thoughts about this? Have you tested the change within a Rails application? (You can generate one with the included template)\n. Hi, checked it briefly against example apps, seems to be working fine -- simplified the test and pushed, thanks!\n. Is this really a simplification?\n. @miry Unless you object strongly, I'd keep the current code, since it's arguably more readable / simple to understand?\n. Thanks!\n. The problem is that other Markdown parsers then don't understand these sections as code blocks.\n. For instance the Textmate bundle doesn't handle it right, also:\nFile.open('index.html', 'w') do |f| f << RDiscount.new(File.read('README.markdown')).to_html end\ndoesn't render the \"Github markdown\" blocks right without initial indentation. What's the reasoning for this change?\n. @paozac I've been too busy for the last couple of days, sorry!, will get back to you. These tests indeed fail from time to time, will investigate it a bit deeper.\n. @paozac The percolator tests indeed fail intermittently for a long time, and I haven't yet found a solution apart from wiping the whole _percolator index. That might be an option on Travis, but not on the development machine.\nI'll look into why the \"kitchen\" results are not demoted consistently...\n. Found the culprit -- the default setting of 5 shards per index confuses the scorer. The solution would be to have an index with one shard, or -- preferably -- to use the dfs_query_then_fetch search type.\nTested the changes with following script:\n``` bash\nrm -f results.txt\ntime for i in {1..15}\n  do\n    echo \"-------------------------------------------------------------------------------\"\n    echo \"TEST $i\"\n    echo \"-------------------------------------------------------------------------------\"\n    ruby -I lib:test test/integration/constant_score_queries_test.rb\n  if [ $? == 0 ]; then echo \"$i: OK\" >> results.txt; else echo \"$i: FAIL\">> results.txt; fi\ndone\necho; echo; echo \"===============================================================================\"\ncat results.txt\nrm -f results.txt\n```\nThe tests consistently succeed.\n. Konrad, this sounds highly plausible! I don't think there's a more elegant/concise way how to check if the item being worked on is an Array, this solution is readable.\n. That's weird, alias switching should really not depend on the Ruby process, since it's handled on the Elasticsearch side... Are you sure there wasn't something other going on? I have used frequently changing aliases without app restarts and haven't seen this issue... Are you sure the index object is not memoized somewhere etc?\nThis should be easy to model in an isolated test case, just have a simple Ruby process or app running, and then change the alias (via Tire or other means), and everything should continue working. \n. Hi, this is surprising, we actually modeled this pseudo-find_in_batchesfor Mongoid after suggestions in mongoid/mongoid#1334 if memory serves correct. I don't have much experience with tuning Mongoid queries, but I think klass.all would be quite inefficient and load everything into memory?\n. @nickhoffman @michaelklishin, any suggestions here?\n. @nickhoffman The README actually suggests both options (maybe confusingly, see karmi/tire#726). The real question here is what is the most efficient approach when importing large sets from Mongoid collections -- see the code in https://github.com/karmi/tire/blob/master/lib/tire/model/import.rb#L62-L71. The code uses step-limit-skip, as suggested in mongoid/mongoid#1334, where @halfbrick's changes use simply each and leave the mechanics of efficient iteration to Mongoid. I really lack relevant knowledge here as to what is the best approach.\n. @halfbrick Thanks for the update. This is surprising, since I remember we specifically added the step implementation to prevent inefficient iteration. Your information seems to be correct from what I see in the resources you posted and others.\n@nickhoffman Should we then just use klass.all.each in the Mongoid importing strategy?\n. BTW, 100 documents per batch feels suboptimal to me, usually Elasticsearch works best with batches in the thousands range. The current default is 1000.\n. Hi, finally got to evaluate and integrate the patch. Please have a look at 7117550, there was an error in the batching code.\nEvaluated the code on a small testing Mongoid app, didn't notice any significant performance difference (100,000 docs), but the behaviour of the importing code should be the same as previously. Thanks!\n. That might happen, I guess. Would it be possible to add a unit/integration test demonstrating the issue?\n. @SixiS Yeah, the Array() method is really convenient .) I polished the patch a bit, and in fact wasn't able to produce a failing test for ActiveRecord in reasonable time. But based on the way you describe the behaviour, I can see what goes wrong and the Array() wrapper really shouldn't hurt anything.\nBTW, I have removed the load: true test in the persistence test, since it does not make any sense -- the persistence model is always loaded from Elasticsearch. I have changed the wording a bit to make the test cases clearer.\n. So, you're using Tire.search against an index eg. \"my_documents\" which is handled by MyDocument model which is a Tire::Model::Persistence? I guess I have never tried that, so this might be a missing test :)\n. Actually, both are possible :) But in the context, the change suggested by @justinko is correct, since we want to use the newly added \"polymorphic\" import strategies.\n. Actually, when reading the passage in context, Article.index.import is right -- it communicates clearly that every Tire::Index object has an import method, and you can pass a collection (Enumerable) to it.\nIn contrast, Article.import, will try to pick up a strategy for handling loading the records.\nBottom line, current wording is correct, but we should advertise the strategies more in the README, not just say \u201cdepending on the setup of your model ...\u201d.\n. Hi, the intuition to use multi_field is correct, but there are other ways around this. (The right approach obviously depends on what exactly is the data/property in question.)\nIn the first approach, you can use multi_field like you do, with the ignore_malformed flag. Elasticsearch will then extract the numeric value of the field; see example: https://gist.github.com/clintongormley/5575590.\nIn the second approach, you can \"split\" and define the property in your Ruby code, using the :as option, using extra methods, etc. Example:\nruby\nmapping do\n  indexes :as_string, type: 'string',  as: 'title.to_s'\n  indexes :as_number, type: 'numeric', as: proc { title.to_i }\nend\nThe second approach could be attractive when you'd need a more sophisticated control how the property is converted to each type; note, that Elasticsearch is much smarter in converting the string then a simple to_i.\n(Also note, that you can run range queries on string fields as well, but of course \"numerical strings\" would be sorted lexicographically, not as numbers.)\n. @zywx Separating indices for models like this is possible, just set the index_name from Ruby, see https://github.com/karmi/tire/blob/master/test/models/active_model_article_with_custom_index_name.rb#L6?\n. Hi, I've added a __host_unreachable_exceptions method to all HTTP clients, and the MyModel#create_elasticsearch_index method now rescues these. Could you verify it for your stack with Faraday?\n. Soon, but it should be trivial to check against Git master with the :git =>\u00a0... support in Bundler...\n. Great, thanks!\n. Yes, but the error should be properly rescued, see https://github.com/karmi/tire/blob/master/lib/tire/model/indexing.rb#L115\nCan you share the specific error you get? I think we need to broaden the list of errors here.\n. Thanks, so the error is properly rescued by the changes in 3dc8cb70ec84547538cdb7a8ea8a7f68b47f3609 ?\n. Closing this in favor of #748, if that's OK.\n. Hi, I'm sorry, but I wouldn't like to add new APIs to Tire at this stage. It could theoretically be added as a part of tire-contrib, but I'd wait a bit with it.\n. Hi, Addres is not a singular form of adress, it's incorrect English syntax. Rename your model to Address (plural: addresses) and it should work.\n. I'm sorry, I see your point:\n```\n\n\n'post'.classify\n=> \"Post\"\n'address'.classify\n=> \"Addres\"\n```\n\n\nI think the way to solve it would be to simply pluralize the file name, since classify explicitely says \"create[s] a class name from a plural table name\". I'll try to have a look into it soon.\n(Note that you can import the individual model with rake tire:import:model CLASS='Address'.)\n. @alanmeira Sorry it took so long, should be fixed now.\n. @fcheung I've added the patch, and also added a more DSL-ish variant. I think the tire-contrib version is superior, as @brupm noted :), can you check if it satisfies your use case? I'll move it over from contrib to core I guess.\n. @fcheung Embarassingly enough, I forgot as well :) Will create a mashup of these and push it.\n. The heaviest Tire dependency is ActiveSupport, which is already loaded by Rails. You may want to use the require-prof gem to identify the gems which slow down the app and post more info.\n. You can reindex a portion of the index with the Tire#index#reindex method, see https://github.com/karmi/tire/blob/master/test/integration/reindex_test.rb#L35-L50\n. No, Tire#index#reindex will simply extract a portion of the index by scanning, and putting the data into another index.\n. @rizwanreza Hmm, previously importing from ActiveRecord relied on pagination, and depending on the scopes in your application, it could mix up order or something. Can you try it with a recent version? It definitely should import all documents -- can you share your model code, or relevant parts of it?\n. @rizwanreza Hi, any new info here?\n. Closing as stale, please reopen or ping back when you need help.\n. @rb2k Hmm, this seems to be coming from the code which yields to a passed in block. Do you call the join in your block? Can you share it?\n. Hey, actually the doc is correct, observe:\n``` ruby\ns = Tire.search('articles') do\n  facet 'tags' do\n    terms :tags\n  end\nend\np s.facets\n=> {\"tags\"=>{:terms=>{:field=>:tags, :size=>10, :all_terms=>false}}}\np s.results.facets\n=> {\"tags\"=>{\"_type\"=>\"terms\", \"missing\"=>0, \"total\"=>3, \"other\"=>0, \"terms\"=>[{\"term\"=>\"ruby\", \"count\"=>1}, {\"term\"=>\"python\", \"count\"=>1}, {\"term\"=>\"java\", \"count\"=>1}]}}\n```\nIn the first case, you get the facet \"definition\" for the request, in the second case, you get the actual response...\n. @cjbottaro Can you describe the issue in more detail? Maybe there's some kind of proxy in front of ES, or it can't serve the load, or the request time outs?\n. @cjbottaro No worries and glad it's working!\n. See the integration tests: https://github.com/karmi/tire/blob/master/test/integration/match_query_test.rb#L42-L50\n. There is no DSL \"sugar\" for geo in Tire yet -- you have to simply configure the filter as any other:\nruby\nTire.search 'venues' do\n  query do\n    filtered do\n      filter :geo_distance, distance: '100km', lat_lon: \"41,-71\"\n    end\n  end\nend\n. Hi, unfortunately, I can't pull it in just like that :) For a patch like this, a proper unit test and a working, non-synthetic integration test are required... Also, I was thinking about a more DLS-ish approach to geo stuff, as outlined in #283, what do you think about that?\n. @chrishale If it works for you as it is, no rush. What would be cool though, if you could add it to the https://github.com/karmi/tire/wiki/How-to-work-with-locations-(geo)-in-Tire wiki page.\nEven more cool would be a wiki page or a README section about how easy it is to inject/monkeypatch/extend the Tire query class with a one-off method like this, when certain functionality such as geo_shape is missing.\n. Thanks for the fix!, added and pushed. (See also #733)\n. @felixbuenemann So rake environment tire:import:all fails for you but when you manually change the task to depend on the environment task, it works? Also, rake environment should definitely load all files registered in config.load_paths?\n. You're using fairly old Tire version -- try to upgrade both locally and on the production server, and run it again?\n. @rubytastic Have you looked here http://stackoverflow.com/questions/11692560/elasticsearch-tire-and-nested-queries-associations-with-activerecord/11711477#11711477 ? There's info and full code for indexing associations.\n. Hmm, this looks OK. Can you please pastie the output of Profile.to_indexed_json? Also, instead of head, just look at the results directly, http://localhost:9200/products/_search?pretty.\n\nhowever there is not created an index and the search results return nothing\n\nCan you elaborate on this? I don't understand.\n. What about your to_indexed_json? to_json(include: {match: {only: [:text]}}) ?\n. Does this exception appear when using Faraday? Since Resolv is not loaded automatically by Ruby?\n. I understand the picture, I'm just wary about rescuing generic Exception in the code in question... But if we want to 100% cover theses cases, there might not be any other way. What would you suggest?\n. >  In other words turning off the automatic index creation.\nBut that is problematic in its own sense: you set up your index settings and mappings in Tire, and you expect them to be reflected. Otherwise dynamic mapping kicks in, and we're in lots of hard to explain/debug problems.\nWhat would probably be the least offensive solution is to either provide some configuration option a la Tire.configure { model_auto_create_index false }, or extract the logic into a module which you choose not to include, a la the callbacks module...?\n. There's actually another discussion about this in #798, and I like the proposed solution of mapping(auto_create: false), since it's more fine-grained?\n. @threez Actually, if I understand your concern, the action.auto_create_index: false should be used in production, indeed. I'm leaning towards closing this issue in favour fo the suggestion of #798 by @brightbits. What do you think?\n. Hi, for solving this \"cross matching\" issue, you need to use the special nested type in Elasticsearch.\nSee:\n- http://euphonious-intuition.com/2013/02/managing-relations-in-elasticsearch/ and http://www.spacevatican.org/2012/6/3/fun-with-elasticsearch-s-children-and-nested-documents/ for background\n- https://github.com/karmi/tire/blob/master/test/integration/nested_query_test.rb Tire integration for working code to try it out\nPing me back with more questions if you need.\n(Also, better use the match query instead of the error-prone query_string query...)\n. Hi, thanks, you're very welcome :)\nI know what you're after -- but Elasticsearch will return the whole document _source , so unfortunately you have to iterate over the venue.weekdays yourself to display just the matching ones...\n. @miguelm It's on the Elasticsearch roadmap though to return matching nested documents, https://github.com/elasticsearch/elasticsearch/issues/3022\n. @miguelm I think we can close it here. Thanks!\n. You have to properly define the mapping and/or to_indexed_json to serialize all the information you need for displaying to the user / rendering as JSON. Please have a look in the test directory, where you'll find many examples, and see the Stackoverflow answer: http://stackoverflow.com/questions/11692560/elasticsearch-tire-and-nested-queries-associations-with-activerecord/11711477#11711477\nYou might need to research Elasticsearch nested type and the parent/child support.\n. Closing, feel free to ping back if you have more info.\n. @tpitale You're way too kind :) Type is indeed optional, so I make it so in Ruby as well, and improved the handling of arguments a bit in the attached commit (00eafc0).\n. > Tire's initial index creation does not provide the correct mapping.\nThat would be suprising. What happens when you delete the index and call Comment.create_elasticsearch_index?\n. @atombender Can you confirm that when you call Comment.index.delete; Comment.create_elasticsearch_index, the index is created with correct settings/mappings?\n. For the first case, you need to load the model -- so when you eg. run rails console, you must \"touch\" the model, just calling Article is enough.\nFor the second case, that call is incorrect, you must call something like Comment.first.update_index.\nI've added more info in d97f7fc5e555750945c2c2fcd22bcbfeeb5bde17 to the README.\n. If the default behaviour where Tire creates the index for your model is not desired, please create the index with proper settings/mappings manually, as the README states:\n\nIt may well be reasonable to wrap the index creation logic declared with Tire.index('urls').create in a class method of your model, in a module method, etc, to have better control on index creation when bootstrapping the application with Rake tasks or when setting up the test suite. Tire will not hold that against you.\n. I understand your frustration, but not your argument, I'm afraid. Tire tries to create the index with the correct settings/mappings only when it doesn't exist -- that is documented and \"safe\" behaviour.\n\nIf all your models share a single index, you're precisely in the spot where you can't rely on Tire defaults. It should be easy to define the mapping externally, or merge all mappings from the model definitions, and create the index imperatively/manually.\nThe solution you propose, to update the mapping on each class load has two big drawbacks:\n- It issues a \"put mapping\" call on each class load, which is not something you might want, which might create problems with class reloading in development, etc\n- There's a problem of handling incompatible changes, conflicts. The library would make even more \"false promises\" here, in my opinion.\n. Hi, I don't know if I understand correctly -- it doesn't make much sense generally to sort on analyzed fields, though Elasticsearch 0.91 and higher supports it. Could you post more info?\n. @johnmcdowall Ahh, OK, yes, if you want the name \"not_analyzed\", you have to lowercase it yourself as you do. The field seems like a good candidate for multi_field, where you have both the analyted (stemmed, etc) version and the not analyzed version?\n. Thanks \u2014 would it be possible to add a simple integration test for the feature?\n. @djcp It should be 100% compatible with Ruby 2.0, some integration tests fail for a long time (percolator issues). See eg. https://travis-ci.org/karmi/tire/jobs/7905760, the unit tests pass.\n. @djcp Do unit tests fail, or only integration test?\n. Tire does not overwrite existing methods, and depends on the order of include statements. See the README for the tire proxy method: you can use Mymodel.tire.search.\n. Please see http://stackoverflow.com/questions/11692560/elasticsearch-tire-and-nested-queries-associations-with-activerecord/11711477#11711477.\nClosing as duplicate of http://stackoverflow.com/questions/17151596/how-to-use-elasticsearch-using-tire-gem.\n. Hi, this functionality is supported by the tire-contrib library, https://github.com/karmi/tire-contrib/blob/master/lib/tire/queries/more_like_this/more_like_this.rb\n. @brupm There is: https://github.com/karmi/tire#extensions-and-additions, but I agree it needs some force to scroll that much down :)\n. @orbanbotond Yes, please...\n. When digging around this issue, I have actually found much more profound problems with casted collections: they seem to be not indexed properly, etc. Will try to find some time to resolve these and add integration tests for the set of features.\n. Hi, thanks for the contribution, but at this moment, I won't like to extend the Tire features further. I'll gladly merge the patch into https://github.com/karmi/tire-contrib if you extract it and send a pull request.\n. @fgrehm Sounds like a nice idea, but could you please add a more fleshed out example?\n. Will have a look. At the moment, the biggest problem is related to #767, and with the current release of Rails 4, the problem is more imminent.\n. Could just index the friendly_id in your model mapping definition?\n. Then use a custom wrapper (https://github.com/karmi/tire/blob/master/test/unit/index_test.rb#L430-L438), where you implement to_param correctly, or overload the Item to_key to use slug instead of id.\n. > The solution with the wrapper object will force creating a new proxy object for each result item because I have multiple types.\nNot necessarily -- the wrapper could be \"smart\" about which model it represents.\n. It might be because your docs are missing from the db? See #602 and #702, implement your own __find_records_by_ids if you need it.\n. You simplest option is to use a query_string query, another option is a bool query with prefix, or, for maximum performance, adding a field such as initial_letter to the document and using range filter then:\n``` ruby\nrequire 'tire'\nTire.index 'alphabet' do\n  delete\n  create\n  store title: 'Aesop', initial_letter: 'a'\n  store title: 'Aleph', initial_letter: 'a'\n  store title: 'Burgundy', initial_letter: 'b'\n  store title: 'Eternal', initial_letter: 'e'\n  store title: 'Hell', initial_letter: 'h'\n  store title: 'Kierkegaard', initial_letter: 'k'\n  store title: 'Yin', initial_letter: 'y'\n  refresh\nend\ns = Tire.search 'alphabet' do\n  query { string 'title:[A TO F]' }\nend\nputs \"\", \"String query\", '-'*80, s.results.map { |r| r.title }\ns = Tire.search 'alphabet' do\n  query do\n    boolean do\n      should { prefix :title, 'a' }\n      should { prefix :title, 'b' }\n      should { prefix :title, 'c' }\n      should { prefix :title, 'd' }\n      should { prefix :title, 'e' }\n    end\n  end\nend\nputs \"\", \"Boolean query\", '-'*80, s.results.map { |r| r.title }\ns = Tire.search 'alphabet' do\n  query do\n    filtered do\n      filter :range, title: { from: 'a', to: 'f' }\n    end\n  end\nend\nputs \"\", \"Range filter\", '-'*80, s.results.map { |r| r.title }\n``\n. You can either fine tune how you index (see the hint withinitial_letter), or, when the performance would be acceptable, use eg. theregexquery.\n. probably since the analyzer throws it out, set it toanalyzer: 'keyword'. see the doc for returning all terms in facet.\n. @jcf Good catch!, merged in. There are long standing issues with percolator tests, which I haven't yet solved, and some other tests fail on ES master, which I also haven't yet solved.\n. Good, but I think we need to tread more carefully here. It's bad the former syntax was deprecated, since it allowed not only includes, but also conditions, etc. I'm afraid we can't just change it like this, and we definitely unit and integration tests for this.\n. @jcf I know the hash interface has been deprecated. I haven't really put much thought how to support the chainable interface in the hash options tosearch. Maybe the only use case isinclude()`, maybe there are other use cases...\n[Old doc: http://apidock.com/rails/v2.3.8/ActiveRecord/Base/find/class]\n. Yeah, the good old days :)\nI like the idea with lambda, since that could allow to leave the old option supported, and evaluate the lambda (polymorphism FTW) as the \"finder\", exactly as you propose:\nruby\nArticle.search 'love', load: { include: \"comments\" }\nruby\nArticle.search 'love', load: lambda { |ids| includes(\"comments\").find(ids) }\nAlso see #702 and #602 where we discuss some strategies how to mitigate the issue of \"record no longer in db\".\nAlso related: #762.\n. This has been introduced in 46fbcbf. Do the integration tests for persistence run for you? Can you post more info/backtrace?\n. > I executed the tests and found 4 failing for the current state of the repository. The output is here.\nI have meant only the integration tests for the persistence :) None of them is failing.\n\nSaving model constructed this way passed well earlier.\n\nCould you attach or pastie somewhere a full backtrace please?\n. Aren't you mocking the response in an incompatible way, perhaps? \n. @fifigyuri Sorry for the silence, been offline. I think this is due to the after_save callbacks being executed. I'll try to come up with some mitigation for this problem, thanks for the report!\n. @vgalu This is due to incorrect serialization of ID in Mongoid, see #775, you don't have to change the model (possibly breaking other stuff in your app), just change the JSON serialization.\n. @vgalu Pushed 3291bee a while ago, could you verify it solves your issue?\n. @vgalu Great!\n. The problem is that RestClient, the default HTTP client for Tire, does not support sending body with DELETE, see: https://github.com/rest-client/rest-client/blob/master/lib/restclient.rb#L83-L85\n. Yeah, absolutely, but it would be better to add it in upstream to RestClient directly... and possibly monkey patch the same code on our side for keeping compatibility with low RestClient versions, if needed.\n. As noted in #309,  Curb also doesn't support DELETE request with a body, so serializing the payload into the source URL parameter seems like most convenient and safe way.\n. Closed by 55d12c0caaf09f8084266b08719e47de8fef2919\n. @regedarek Is this related to #774? Can you please consolidate the question into single issue? Also, it would help if you had a full example, which would allow to recreate your situation entirely.\n. @regedarek I don't understand the issue at all, I'm afraid. You're asking about custom filters score, but the title mentions terms separated by spaces? Have you tried setting different score_mode (see ES docs)?\n. Found the culprit, ironically enough, it's a specific fix for Mongoid .) -- karmi/tire@98e6dc7 changed Tire::Index#get_id_from_document to use document.id.as_json instead of document.id.\nNow, let's observe:\n``` ruby\nMongoid::VERSION\n=> \"3.1.4\"\na = Article.first\n=> #\na.id\n=> \"51af2247410b5ac9ae000001\"\na.id.as_json\n=> \"51af2247410b5ac9ae000001\"\na.id.class\n=> Moped::BSON::ObjectId\nMongoid::VERSION\n=> \"4.0.0\"\na = Article.first\n=> #\na.id.class\n=> Moped::BSON::ObjectId\na.id\n=> \"51af2247410b5ac9ae000001\"\na.id.as_json\n=> {\"$oid\"=>\"51af2247410b5ac9\n```\nThe linked commit contains links to relevant issues. Seems to me like simply reverting the commit should work, but I'd prefer to understand how Mongoid understands JSON serialization of models in general, and unless really neccessary, keep compatibility with Mongoid 3.x and 4.x.\nAlso, seems like the 4.0 version of Mongoid has not been yet released to Rubygems: https://rubygems.org/gems/mongoid/versions\n. @jonarrien Should be solved by the closing commit, can you validate?\n. Let's first wait and see how MultiJson future will pan out. As noted in the original issue, MultiJson allows everybody to use their preferred library without changing the interface -- without an abstraction like that, Tire would have to provide something like this on its own.\n. @phoet That's surprising -- would you be able to dig out some concrete issue/backtraces/whatever? I've never had any problem. That said, adding an abstraction for handling JSON is technically trivial, just laborious. I'd like to wait a bit and see -- I've seen these kind of problems being solved simply by waiting until dust settles.\n. Definitely not on current Tire .)\n. > after that we switched to OJ as a json parser (really fast, reduced our response-times by half)\nYeah, saw the performance implications in the linked issue and I'm definitely not happy about it. Still, we need to somehow make the JSON lib pluggable, which should be the sole responsibility and reason for MultiJson. Hopefully the situation will improve. I still believe abstractions a la MultiJson and Faraday are quite useful, since as a library author, you don't have to provide extension points yourself.\n\nanother thing was our dependency to couch_potato, which does not use MultiJson, so we basically had 2 JSON config points etc...\n\nAh, yeah, that sounds really painful...\n. Seems like https://github.com/intridea/multi_json is live and kicking?\nFor the record, https://github.com/elasticsearch/elasticsearch-ruby uses MultiJson and so far no issues have been reported. elasticsearch-ruby has modular serializer though, so you can pass whatever JSON library, including custom wrappers to it: http://rubydoc.info/gems/elasticsearch-transport/#Serializer_Implementations\nClosing this?\n. @marc-villanueva Can you please get in touch @fbatista which contributed suggest in #857 as well? Maybe you can combine the approaches and you can help him.\n. @vasconcelloslf Sorry for the silence. This is also related to #767 and effectively all discussions we're having about having  a set of strategies which would allow customizable finders for major ORMs. If you have time, sure link the changes or submit a PR so we can talk around code.\n. Results are lazy loaded, see: https://github.com/karmi/tire/blob/master/lib/tire/search.rb#L34, so until you touch results, no request is executed.\nAlso, it's quite impossible for the block to take 250msec to execute, without executing the response. Compare:\n``` ruby\nrequire 'tire'\nrequire 'benchmark'\nduration = Benchmark.realtime do\ns = Tire.search do\n    query do\n      filtered do\n        query { match :title, 'foo' }\n        filter :terms, tags: ['one']\n      end\n    end\n  end\nputs s.to_json\nend\nputs \"\", \"Duration: #{duration*1000} ms\"\n``\n. @ovamsikrishna Sure, until you touchresults, no request is executed to Elasticsearch, please see my note and link above...\n. That really depends on the query itself, the amount of data, available resources, etc.\n. @ovamsikrishna Without concrete numbers and details about how you measure, it's impossible to come up with a reasonable assesment.\n. Tire just indexes model attributes/methods, so it shouldn't make a difference if it's a column or Hstore property or something like that. Evaluate (and possibly post here)@yourmodel.to_indexed_jsonto see how it looks like and how it gets indexed into ES.\n. Post the JSON serialization of the model, please.\n. Nono, I mean, if you can post the output of@yourmodel.to_indexed_json, so we can see what is being sent to Elasticsearch.\n. > >{\"tem\":{\"product_platform\":\"Nintendo Wii\"}`\n\n\"_source\" : ... \"product_platform\":\"Xbox 360\"...}\n\nSeems like your product_platform is indexed correctly, but since you're using a term filter, it expects the field to be non-analyzed, ie not broken into terms. Post the output of your MyModel.mapping.to_json, and also try to set up mapping correctly with eg. \"keyword\" analyzer -- have a look in integration tests and also search Google/StackOverflow.\n. > I haven't defined any mappings.\nYou have to define proper mapping for the term filter to work correctly. Please have a look in integration tests and on StackOverflow.\nKarel\n. @abitdodgy I'm afraid I can't walk you through the whole problem. Have you researched how analyzers work? When you use the keyword analyzer, the field is indexed \"as is\", ie. without lowercasing, splitting, etc. I suggest starting eg. here, http://www.elasticsearch.org/videos/bbuzz2013-getting-down-and-dirty-with-elasticsearch/.\n. Glad to hear that.\n\nThis can happen hundreds if not thousands of times per day. Is my use case something that ElasticSearch is meant to handle?\n\nThat's not a problem. Have a look at the Update API in Elasticsearch (Tire integration test: https://github.com/karmi/tire/blob/master/test/integration/index_update_document_test.rb), which allows you to update a document without sending all the JSON across the wire. (Of course, your infrastructure must be able to withstand continuous updating &\u00a0searching.)\n. RestClient doesn't have HTTP keepalive, and ES could simply refuse to serve many connections -- can you try with the Curb client? (Grep the repo for \"curb\")\n. It's the examples/tire-dsl.rb file. Using issues is a perfect place to ask for help, ask questions, suggest improvements, etc.\n. Closing, feel free to open issues for questions or continue discussions in old ones, we can always re-open them.\n. Are you using load:true? Tire complains about your model not having the method, not Results::Item...\n. It's not an error, then .) You were loading your model instances, which don't have the method -- each_with_hit is designed specifically for this situation.\n. Yes, the problem is Elasticsearch cannot see \"into\" words, so Test2 is essentially the same as TestWordLikeAGermanWouldLikeIt. That's because it works with so called tokens or terms produced by the analysis process.\nBy default, the standard analyzer is used, which uses the standard tokenizer to \"split\" your text into searchable terms.\nElasticsearch has a convenient Analyze API, which makes it easy to test this out and get familiar with it:\n- http://localhost:9200/_analyze?analyzer=standard&text=Test%202\n- http://localhost:9200/_analyze?analyzer=standard&text=Test2\n- etc\n. Closing since not hearing back, please reopen or ping me if you any more problems.\n. @threez Sounds useful, can we close the cursor after the block then? I saw something timeout: false notation, could it be more descriptive then the no_timeout method? I have no production familiarity with Mongoid?\n. @threez Agreed on no_timeout, if you think it's better. I was assuming that you have to manually close the cursor after the operation (based on brief reading of docs), but errors are different thing -- maybe close manually in ensure clause?\n. @threez Can you get in touch with @brightbits  from #870, and debug the issue on Mongoid?\n. Thanks!, merged.\n. Found this thread, could it somehow be related https://bugzilla.redhat.com/show_bug.cgi?id=956035? Can you run the Tire integration test suite on the system?\n. @voxxit Can you please run Tire integration tests (or their part) on the system?\n\nFor reference:\n``` shell\n$ ruby -v\nruby 2.0.0p0 (2013-02-24 revision 39474) [x86_64-darwin12.2.0]\n$ ruby -I lib:test test/integration/active_record_searchable_test.rb \n...\nFinished in 86.083343 seconds.\n``\n. What exactly do you mean by disabling pagination and why do you want to do that?\n. Ah, OK, understood! First, Elasticsearch by default returns only the \"top results\", you can work withsize/fromwithin reasonable boundaries (1,000s of docs), but if you want to just return _everything_ for a query, have a look at thescan` API, Tire supports it: https://github.com/karmi/tire/blob/master/test/integration/scan_test.rb\n. Hi, any more info/help needed or can we close this issue?\n. @phoet It works when you wipe _percolator index, but that can't be done normally, people might lose data. Will try to get to all the issues soon, limited online now...  \nOn Tuesday, 9. July 2013 at 15:44, Peter Schr\u00f6der wrote:\n\n@karmi (https://github.com/karmi) did you ever find out why the percolate queries fail on travis? is that a problem of versions?\nthe changes i made allow the local test-suite to pass with ruby 2 and elasticsearch 0.90.2\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/karmi/tire/pull/789#issuecomment-20674474).\n. @phoet Thanks for looking into this! In the end, I've changed the way we run \"destructive\" tests without introducing the special context, it might be more transparent this way. Hopefully Travis will get green again :)\n. Please see the 'tire' proxy in the Readme, Tire doesn't overload any already defined methods.  \n\nOn Tuesday, 9. July 2013 at 17:48, Hugh Brock wrote:\n\nI have code which relies on articles.index(article) returning the position of a given article in the array. Adding\ninclude Tire::Model::Search\ninclude Tire::Model::Callbacks\nto article.rb breaks this code, b/c article (and a collection of articles as well) now has a method \"index\" that returns the index object for the article.\nIs there a way to namespace the index method or otherwise keep it from trampling the native Ruby version?\nThis is tire (0.6.0) on Rails 3.1.2/ruby 1.9.3\nApols if I'm overlooking something obvious here...\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/karmi/tire/issues/790).\n. Tire shouldn't overload any defined methods, see the source, could you post relevant parts of your models?\n\nOn Tuesday, 9. July 2013 at 18:33, Hugh Brock wrote:\n\nHmm... what about instance properties? I also have an object with an index field defined (\"foo.index\" gives a specific position of foo in a list of foos) and it appears to be overriding that as well. Do I simply need to explicitly define the property in the model?\n(Thanks for the quick response BTW)\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/karmi/tire/issues/790#issuecomment-20687143).\n. Strange, ActiveRecord attributes should be defined as regular methods; will investigate it further later.  \n\nOn Tuesday, 9. July 2013 at 18:51, Hugh Brock wrote:\n\nSure...\nSo I have a table like this:\nclass AddColumnsToArticles < ActiveRecord::Migration\ndef up\nadd_column(:articles, :subhead, :string)\nadd_column(:articles, :index, :integer, :null => false)\nend  \nIt backs a model like this:class Article < ActiveRecord::Base include Tire::Model::Search include Tire::Model::Callbacks ... def up before = Article.where(\"issue_id = ?\", self.issue.id) .where(\"index < ?\", self.index) .order(\"index DESC\").first if before.present? self.index, before.index = before.index, self.index before.save save end end\nCalling \"up\" from my tests now yields this error:\n1) Article should create a valid article Failure/Error: @article1 = FactoryGirl.create :article, :issue_id => @issue1.id NoMethodError: undefined method >' for #<Tire::Index:0x0000000c47d248 @name=\"articles\"> # ./app/models/article.rb:71:inset_index_if_null' # ./spec/models/article_spec.rb:6:in `block (2 levels) in '\nYou can see that article.index now returns a Tire::Index, rather than the integer defined in the database.\nTo be clear, I don't have an \"index\" method defined anywhere in the model -- it's of course implied by the database table.\n--Hugh\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/karmi/tire/issues/790#issuecomment-20688453).\n. Been digging a bit into this, and seems like ActiveRecord is using some module_eval, which might throw Tire off, when it looks for the method being already defined.\n\nI can replicate the issue just in console when the model has a method named index:\n``` ruby\nMigration\n...\nadd_column(:articles, :index, :integer)\n...\nConsole:\na = Article.create title: 'With index', index: 123\n\n\na.index\n=> #\n\n\nBUT SHOULD BE 123\n```\nThe workaround for now would be to use the read_attribute accessor:\n``` ruby\n\n\na.read_attribute(:index)\n=> 2\na[:index]\n=> 2\n```\n. Please search the issues, many mention this thing, you can manually overload the finder method... Alternatively call index.refresh after delete.\n\n\nI'll post you some links tomorrow if you'll be still lost,\nKarel  \nOn Tuesday, 9. July 2013 at 18:22, wolfram-e wrote:\n\nHi,\nI spent some hours on this, allthough it seems such an obvious issue I can not find solutions so I guess the error is on my side: I have different records stored in SQLite and ElasticSearch/Tire. So far it works great, but when I delete a record which leads to the index view I get an \"ActiveRecord::RecordNotFound\" and \"Couldn't find all ...\" error, I guess since the record is deleted in the database but not yet in ES (works after reloading the page). Same thing for creating new records: they don't appear in the first place after saving, but the second time reloading the page they're there. I found and I tried some update_index and tire.update_index in the classes' callbacks and also Emp.tire.index.remove @emp (https://github.com/emp) (Emp is one of my model), but none of them worked out.\nThanks for your help!\nWolfram  \n\u2014\nReply to this email directly or view it on GitHub (https://github.com/karmi/tire/issues/791).\n. @abrahamD The setup you have must be a bit different I think. Tire, by default, just takes the mapping defined in your class, and when that class is loaded, it tries to create the index.\n\nIn your setup, you have to create the index in advance, passing it all settings/mappings you have. Nothing holds you, of course, from gathering the mapping definitions from your models (eg. via the ActiveRecord::Base.descendants you use), merging them, and creating the index. Just be careful since Tire tries to create the index when the class is loaded, see #798 for discussion.\nThe Rake task you have is very similar to the bundled rake tire:import:all task.\n. Yes, but you have to care about the fact that mappings set in individual models via the mapping method are ignored.\nYou can \"gather\" all the mappings from the models and pass it to index#create method as the mappings: argument easily (for some definition of \"easily\"):\nruby\nTire.index('myindex').create mappings: ActiveRecord::Base.descendants.select do |m|\n  m.respond_to? :tire\nend.reduce({}) do |s,m|\n  s[m.tire.document_type] = { properties: m.tire.mapping.to_hash}; s\nend\n. @abrahamD Please ping me back if something like this would be a sufficient workaround.\n. @anthias  There is one problem with your setup and one problem in Tire:\n1. First, keywords is incorrect, it must be: analyzer: 'keyword' (singular). This is mostly preferable to index: 'not_analyzed'.\n2. Tire does not create the index with correct mapping, as it does when you use ActiveModel integration. You have to create your index manually with Issue.create_elasticsearch_index.\nAll should work well then. Note that you can use multifield to have the geographies property both analyzed for searches and not analyzed for facets.\n. Actually, a slightly dumb way how to make it work is to force the index being created by wrapping the properties definition in the mapping method:\n``` ruby\nclass Issue\n  include Tire::Model::Persistence\nmapping do\n    property :geographies, analyzer: 'keyword', :default => []\n  end\ndef self.facet_search(params)\n    tire.search(page: 1, per_page: 0) do\n      query { string params[:q] } if params[:q].present?\n      facet \"geographies\" do\n        terms 'geographies'\n      end\n    end\n  end\nend\n```\nI'll try to have a look about supporting this better or just using this notation in the documentation...\n. Yes, this is unfortunate and many users bump into this -- when you delete the index, and then start indexing data, the default, \"dynamic\" mapping is created, ignoring your mapping. For serious use cases, it usually makes sense to wrap the index creation method in a module/class/etc method, so you can create it in tests, via Rake tasks, etc.\n. @GoFarGoLocal I'm a bit confused why you send a question to Stackoverflow as well as submit a Github issue? Also, I don't understand from your explanation the issue you're having. Have you seen http://stackoverflow.com/questions/11692560/elasticsearch-tire-and-nested-queries-associations-with-activerecord/11711477#11711477 ?\n. In my Rails time, I've found that has_many :through is a better way how to model m-n relationships, since usually, the intermediary stores some information. AFAIK has_and_belongs_to_many has an :autosave option: http://api.rubyonrails.org/classes/ActiveRecord/Associations/ClassMethods.html#method-i-has_and_belongs_to_many\n. No, Tire iself hasn't got any support for ActiveRecord associations -- you have to model your data appropriately.\n. @brightbits I like the suggestion, there's an issue or two already opened for this. This would be easy to handle -- just delete the setting from mapping and conditionally perform create_elasticsearch_index. I'm on a vacation this week, will try to look into it next week.\n. @sgringwe I think the right way would be to create the index+alias(es) in advance, then the automatic behaviour doesn't kick in. I'm afraid the behaviour in the gem won't be changed -- there will be no breaking changes in behaviour or adding of functionality. See https://github.com/elasticsearch/elasticsearch-ruby and https://github.com/elasticsearch/elasticsearch-rails\n. Try something like:\nruby\nTire.search('myindex') do\n  facet 'facet1', nested: 'myobject' do\n    terms_stats 'name', 'count'\n  end\nend.to_hash\n. Yes, this is suprising and a bit dumb, no questions about that. For any setup where you want models to share an index, you have to define its settings/mappings and create it in advance anyway. There's simply no way around it. The default behaviour is there simply for convenience.\nThe dumb part is that MyModel.index.create does not create the index with correct mappings, as MyModel.create_elasticsearch_index does, simply because the former is just a pointer to a \"blank\" index instance. The current codebase does not support an obvious solution such as:\nruby\ni = Tire::Index.new 'heyho', mappings: { properties: { foo: { type: 'string', analyzer: 'snowball' } } }\n. @msonnabaum This is also related to #793; I'm wondering what would be best to do here, apart from better documentation. I mean, we could potentially go through the acrobatics of gathering of all the mappings defined in models, merging them, creating the index... but that could get out of hand easily. I think that for a scenario like this, people need to create the index in advance, and we can give them pointers as suggested in the linked issue. What do you think?\n. > It doesn't seem like much work though to do this on a per-model basis.\nIt doesn't, but first, that's the kind of acrobatics I'd be wary to get into, and second, there will be a problem with mapping updates -- you would need to wipe out the index and recreate it when a single model changes the mapping in an incompatible way.\nOn the other hand, maybe we can find a good equilibrium here between \"standard stuff works out of the box\" and \"you have to configure non-standard stuff\" with a bit of code...\n. You can, in fact, update mappings (for example adding a new field), but you really need to know what you're doing and frequently that's not the case for some people :) I agree that we can support the behaviour you describe -- it would require some larger refactoring I think, though...\n. @msonnabaum Sorry for the silence -- if you think we can come up with something which is maintainable and doesn't shoot people in the foot, definitely go for a pull request. Though, since Tire has been retired recently, I wonder if we should limit the gem updates to bug fixes and really important improvements...\n. I'm not sure it's a good idea, and Elasticsearch is pretty easy to install -- see the Rails app template, which effectively does it within a rails new command.\nThat said, you just have to intercept the calls to ES in these environments -- that is, when Tire is checking for index existence in mapping, and then when saving records. The Readme has some good info about how to handle the callbacks manually...\n. @flockonus I still don't understand correctly why disabling the network interaction would be helpful in test or development, but would like to help you with your problem.\nFirst, the off! idea is nice, but it only applies to the default RestClient based client, would require mode code scattered across the clients ets. This is clearly a lack of proper architecture on Tire's part, not your code, but still, the result won't be pretty.\nSecond, even a messy architecture like Tire's has a clean way how to achieve what you want:\n1. Subclass a specific Tire HTTP Client (default, Curb, Faraday, etc), and \"mock\" the HTTP communication (isolated in perform or request method)\n2. Use this client in the specific environment, like this: \nruby\nrequire 'my-mock-client'\nTire.configure do\n    client Rails.env.production? ? Tire::HTTP::Client::Curb : MyMockClient\nend\nDoes it make sense?\n. @flockonus I understand the motivation to keep the stack for developers slim. On the other hand, when your project grows, you'll need to setup additional technologies/services. Elasticsearch is very easy to install, compared to many other technologies, but you might want to research Vagrant to provide your developers with an environment matching the production.\nClosing this issue, if that's OK.\n. @mhussain As suggested, for iterating over large number of results, you should use scan. If you want to just continue paging, you have to increase page parameter, getting new @results and iterating over them. Can you clarify what kind of problem are you having exactly?\n. @mhussain I'm afraid I understand less, not more, now :)\nYou can't iterate over more result items than the search method returns; in other words, you can iterate only over @results in your example. Of course, you can set the page_size parameter to a higher number, but be aware of performance implications.\nIf you would like to iterate over all results for a particular query for real, the scan API is dedicated to that \u2014 but then again, it's not something you should do normally or on each and every user request, etc.\n. There is no default timeout, it depends on the client used and its configuration.\n. See https://github.com/karmi/tire/tree/master/lib/tire/http -- also, is the question about the client's timeout or the timeout parameter for ES?\n. Are you calling it like this rake environment tire:import:all?\n. Can you provide more detailed description of what exactly is not working as expected?\n. OK, we briefly introduced autoloading environment, removed it in favor of the dependency being handled Rake itself.\n. Yes. Elasticsearch rarely breaks backward compatibility, by the way.\nP.S.\nYou should upgrade to 0.90.x as soon as possible, many optimizations and improvements there, 0.90.3 should be released soon.\n. See integration test, https://github.com/karmi/tire/blob/master/test/integration/match_query_test.rb#L52-L60.\n. Preferably, use a filter for a field like that, and a filtered query, not top level filter (unless you do facets). Please see the integration tests.\n. > boolean's must_not are the negatives filters :)\nSure, but filters are more performant, notably on a field like serial_number.\n. Have you had a look at the filtered query integration tests?\n. @kapso The integration test, rather: https://github.com/karmi/tire/blob/master/test/integration/index_update_document_test.rb. So, closing the issue?\n. @jeyb Can you please remove the copy &\u00a0paste from import code crumbs? :) Also, I think the test could be more readable?\n. Hi, currently Tire does not support connection to multiple ES clusters in one Ruby process. There is an undergoing effort to improve the Ruby client significantly.\n. This is really not possible with Tire, sadly. The new Ruby client does support that, of course.\n. Hey, yeah, that might happen, though I'm a bit surprised your Elasticsearch provider complains about it :)\nAs always, what Tire provides \"without configuration\" is just some default. For custom indexing, the most maintainable and robust way is to implement it \"manually\" \u2014 I've added some info to the README recently about several strategies there.\nFor batching everything on your side, I'd approach it as a problem for messaging queue -- Redis for most cases, RabbitMQ for larger systems. You could just RPUSH events containing the payload (what to index where) to a list, and have a worker which would periodically BLPOP from the list, and when it gets say 10, 20, 100 items, it sends everything via a bulk API call.\nIn a system like this, double check that you have:\n- Proper monitoring of workers in place\n- Guards against excessive memory use (eg. ES is down, you keep pushing on list, Redis exhausts memory)\n- Handling of failures, ie. what happens when the bulk API call is not successful\n- An easy way to reindex everything for catastrophic scenarios\n. Closing, hope the note helped.\n. Hi, no support for the indices query in Tire, though I imagine it would be a simply (monkey)patch on your side.\nMaybe you could use the multi_search support?\n. I see you have indexes(:answers) { indexes :description }, that should be it. What is the output of @question.to_indexed_json.\n\nSide note: I don't understand all those index: \"no\" configs in your mapping.\n. Please see Elasticsearch, Tire, and Nested queries / associations with ActiveRecord - Stack Overflow. You might have to manually create the JSON by implementing the to_indexed_json method.\n\nPlease see the documentation, your assumption is incorrect.\n. Have you re-created the index with the new mapping? Have you reindexed the data? There are Rake tasks for those.\n. It's basically a boolean query with many mustss -- see https://github.com/karmi/tire/blob/master/test/integration/boolean_queries_test.rb\nFor filters, just use the or filter, https://github.com/karmi/tire/blob/master/test/integration/filters_test.rb#L39-L40\n. @joshcutler No, the code in question: https://github.com/karmi/tire/blob/master/lib/tire/model/search.rb#L144 is quite messy and ugly -- it's the code fault, not yours :)\n. This will have to be approached in a more consistent manner, I think; see #767 for discussion.\n. Related #810 \n. No, only indices can be aliased. This would be quite easy with a proxy in front of ES, though.\n. Yes, you could rewrite the URL in this fashion. See eg. http://wiki.nginx.org/HttpRewriteModule, https://blog.engineyard.com/2011/useful-rewrites-for-nginx, etc.\n. I think at some point in time ActiveModel required/recommended/etc has_attribute -- thanks for the fix, I'll have a look into the wider context.\n. Well, you should use filters only for filtering data, ie. answering questions \"is this tagged with foo?\". Queries not only match data, but also compute relevancy.\nSo, in your case, I suppose you correctly set up filters for stuff like tags, authorship, status, etc.\nYou can rewrite the \"foo OR bar\" query quite easily into a boolean query, see integration test for inspiration, https://github.com/karmi/tire/blob/master/test/integration/boolean_queries_test.rb\n. Yes, note that you can use the match query instead of string query in the shoulds, or, use the match query in a \"multi_match\" mode -- see integration tests for examples.\n. You have to escape the ! character -- Tire.search('') { query { string 'software\\!' } }.results, see http://lucene.apache.org/core/2_9_4/queryparsersyntax.html#Escaping%20Special%20Characters\n. See the file, you have to require the file as well.  \nOn Wednesday, 7. August 2013 at 9:41, Ike wrote:\n\nI have curb in my gem file and I've specified that I want it as my http client by doing the following\nTire.configure do client Tire::HTTP::Client::Curb end\nbut I keep getting this error when I try to start my app\nuninitialized constant Tire::HTTP::Client::Curb (NameError)  \n\u2014\nReply to this email directly or view it on GitHub (https://github.com/karmi/tire/issues/829).\n. Hi, that's a fair point, but I think we need to mention Tire.configure { url \"xxx\" } as well (see examples/tire-dsl.rb)\n. Please see http://stackoverflow.com/questions/11692560/elasticsearch-tire-and-nested-queries-associations-with-activerecord/11711477#11711477\n. Hi, there are several issues in the syntax. This works for me:\n\n``` ruby\nrequire 'tire'\nrequire 'json'\nTire.index 'test-facet' do\n  delete\n  create\n  store title: 'One', date: Time.now,     sentiment: 'negative', count: 1\n  store title: 'Two', date: Time.now+30,  sentiment: 'negative', count: 2\n  store title: 'Two', date: Time.now+10,  sentiment: 'positive', count: 3\n  refresh\nend\nsearch = Tire.search 'test-facet' do\n  facet 'histogram-negative' do\n    date 'date', interval: '30s'\n    facet_filter :term, sentiment: 'negative'\n  end\nfacet 'histogram-positive' do\n    date 'date', interval: '30s'\n    facet_filter :term, sentiment: 'positive'\n  end\nfacet 'stats-negative' do\n    statistical 'count'\n    facet_filter :term, sentiment: 'negative'\n  end\nfacet 'stats-positive' do\n    statistical 'count'\n    facet_filter :term, sentiment: 'positive'\n  end\nend\nputs search.to_curl,\n \"---\",\n\n JSON.pretty_generate(search.results.facets)\n\n```\nOutput:\n{\n  \"histogram-negative\": {\n    \"_type\": \"date_histogram\",\n    \"entries\": [\n      {\n        \"time\": 1379342580000,\n        \"count\": 1\n      },\n      {\n        \"time\": 1379342610000,\n        \"count\": 1\n      }\n    ]\n  },\n  \"histogram-positive\": {\n    \"_type\": \"date_histogram\",\n    \"entries\": [\n      {\n        \"time\": 1379342580000,\n        \"count\": 1\n      }\n    ]\n  },\n  \"stats-negative\": {\n    \"_type\": \"statistical\",\n    \"count\": 2,\n    \"total\": 3.0,\n    \"min\": 1.0,\n    \"max\": 2.0,\n    \"mean\": 1.5,\n    \"sum_of_squares\": 5.0,\n    \"variance\": 0.25,\n    \"std_deviation\": 0.5\n  },\n  \"stats-positive\": {\n    \"_type\": \"statistical\",\n    \"count\": 1,\n    \"total\": 3.0,\n    \"min\": 3.0,\n    \"max\": 3.0,\n    \"mean\": 3.0,\n    \"sum_of_squares\": 9.0,\n    \"variance\": 0.0,\n    \"std_deviation\": 0.0\n  }\n}\n. @isabanin That is indeed true. We should probably make the and automatic when the method is called multiple times.\n. @zywx AFAIK the terms_stats doesn't support multiple fields to aggregate on.\n. @zywx Best to open a separate issue, so we don't spam all people in this thread.\nvalue_field must be numeric, since the stats are computed on it. Use terms for the channel_id.\n. If something within the chain is losing the milliseconds, I think you have to redefine to_indexed_json and serialize the TIme yourself.\n. @egze Thanks for the answer!\n@bcackerman Can you close the issue if your problem is solved?\n. Hmm, but this should bw wrapped in a bool query, no?\n. Try Tire.configure { logger STDERR, level: \"debug\" } and see what is being sent and returned from Elasticsearch.\n. You have to re-create your index (drop, create with mapping, load all data) when you change the mapping in an incompatible way. Tire comes with Rake tasks and API for those.\n\nI have the resque block why it is not getting captured into it?\n\nI don't understand.\n. > One question if ES returned error code/Exception we should raise an Exception in Tire gem so that we know something went wrong rather than silently failing.\nYes, that's one of the pain points in Tire, which should definitely be handled better in the new Ruby client.\nThe error you see seems to be related to sending false to a property which accepts numeric values.\n. I'm not sure I understand correctly -- you want to do something like:\n```\nfilters = []\n  filters << { terms: { ... } }\n  filters << { terms: { ... } }\n  filters << { query: { query_string: \"...\" }  }\nsearch.filter :or, filters\n```\nThat should be all doable well?\n. Great!\n. Hmm, this could indeed be memory related. Despite much effort to make the import task robust, it might overload a small system. If you're on EC2, can you maybe try it temporarily on a bigger machine such as m2.xlarge or m2.2xlarge to see whether it proceeds further?\n. Hi, the hook point for the monkeypatch is the Tire::Results::Collection#__find_records_by_ids. Just override it by using where(ids:...) instead of find(ids) for ActiveRecord. Please ping me if you'd have trouble with it.\n. Related: #810\n. This is great, @ryansch, thanks!\n@vic The test suite fails frequently at Travis, mainly due percolator tests, I never got around to fixing it... \n. @kapso Released as v0.6.1 now.\n. Argh, this must have slipped through when the integration has been added. Merging and pushing, thanks!\n. I don't understand this issue well, can you clarify what you mean by \"doesn't show up in the search results\"? I think this is related to how AcitveSupport's to_json handles nil values?\n. Sorry, I still struggle to understand what exactly is the problem. I think you either want to remove the contacts property from your document when you're indexing it, or removing it from the returned documents when the property is empty. How can I help you here?\n. @monfresh This somehow went by. I think you might have solved the problem in the meantime... Indeed, the only way would be to have some custom logic within to_indexed_json.\n. Hi @fbatista, this looks great -- although Tire has been \"retired\" recently, this is probably worth to add it. I'd like to ask you for two things:\n- Could you add a bit of documentation to the source, with a note of what is supported and not\n- Could you add a section about the suggester to the https://github.com/karmi/retire/blob/master/examples/tire-dsl.rb page\n- Could you add integrations tests, which would demonstrate the feature in an immediately usable way (ie. search results with suggest, using DSL, etc)\n. @fbatista @fabn @marc-villanueva Guys, many thanks for this! I can see from the Git log it has been quite laborious to integrate all the different approaches and pull requests. Grazie, gracias, obrigado!\nSorry to anybody waiting for this for so long. I've been working on the elasticsearch-ruby library, and the upcoming elasticsearch-rails library.\n. @iwarshak The solution by @brupm is indeed correct! That should work exactly as you want.\n. Hi @ches, sorry for the long delay. Thanks for the great explanation!\nAs you might guess, at the moment Tire should receive only mild treatment, and this fix does look like one. Since we cannot easily not depend on ActiveSupport right now (the whole ActiveModel ->\u00a0ActiveSupport thing), just using class_attribute is really fine with me.\nCan you have a shot at the implementation in a separate pull request? Closing this issue in favor of that.\n. Hi, I understand the motivation for this, but think that it would be better to use the \"strongly typed\" JSON documents here, ie. feed 10.0 to average_score, if it indeed should be a float. I'm a bit wary of performance and maintenance implications of checking the value type for all properties. Issue #383 is actually trying to solve similar problem. Maybe it would be better solution? Could you try it, potentially cleaning up the code in that issue?\n. The new Ruby client will have a Rails/ActiveRecord integration for sure!\nAs for this particular issue, you'd want to \"split\" the indexing into two separate indices? On the one hand, I don't see a nice \"generic\" solution for this, on the other hand, the new integration should provide you with much more powerful tools to achieve something like this.\n. I think the only way how to handle this would be to leave it to the user (pseudo-code):\n``` ruby\nclass Article < ActiveRecord::Base\n  include Future\nafter_save do\n    self.index.save self\n    Index.new(self.index.name+'v2').save self\n  end\nafter_destroy\n    # ...\n  end\nend\n```\n. Hey, have a look at this and ping me if it wouldn't work:\n``` ruby\nencoding: UTF-8\n$LOAD_PATH.unshift File.expand_path('../../lib', FILE)\nrequire 'tire'\nTire.configure { logger STDERR, level: 'debug' }\nTire.index('asciifolding-test') do\n  delete\n  create \\\n    settings: {\n      index: {\n        analysis: {\n          filter: {\n            portuguese_snowball: {\n              type: 'snowball',\n              language: 'Portuguese'\n            }\n          },\n          analyzer: {\n            asciifolding_analyzer: {\n              type:      'custom',\n              tokenizer: 'whitespace',\n              filter:    ['lowercase', 'asciifolding']\n            },\n            portuguese_analyzer: {\n              type: 'snowball',\n              language: 'Portuguese'\n            }\n          }\n        }\n      },\n    },\n    mappings: {\n      document: {\n        properties: {\n          title: {\n            type: 'multi_field',\n            fields: {\n              title: {\n                type: 'string',\n                analyzer: 'portuguese_analyzer'\n              },\n              ascii: {\n                type: 'string',\n                analyzer: 'asciifolding_analyzer'\n              }\n            }\n          }\n        }\n      }\n    }\nstore title: 'A r\u00e1pida raposa marrom salta sobre o c\u00e3o pregui\u00e7oso'\nrefresh\nend\nputs \"Analyze API:\", '-'*80\nresponse = Tire.index('asciifolding-test').analyze 'c\u00e3o c\u00e3es cao caes', field: 'title'\nputs response['tokens'].map { |t| t['token'] }.inspect, ''\nresponse = Tire.index('asciifolding-test').analyze 'c\u00e3o c\u00e3es cao caes', field: 'title.ascii'\nputs response['tokens'].map { |t| t['token'] }.inspect, ''\nputs \"Search:\", '-'*80\nputs 'c\u00e3o:  ' + (Tire.search('asciifolding-test') { query { match ['title', 'title.ascii'], 'c\u00e3o'  } }.results.first.title rescue 'N/A')\nputs 'c\u00e3es: ' + (Tire.search('asciifolding-test') { query { match ['title', 'title.ascii'], 'c\u00e3es' } }.results.first.title rescue 'N/A')\nputs 'cao:  ' + (Tire.search('asciifolding-test') { query { match ['title', 'title.ascii'], 'cao'  } }.results.first.title rescue 'N/A')\nputs 'caes: ' + (Tire.search('asciifolding-test') { query { match ['title', 'title.ascii'], 'caes' } }.results.first.title rescue 'N/A')\n``\n. Sorry, updated the source. Notice the plural (c\u00e3es) does not work -- you might sucessfully do asciifolding here with the [Hunspell support in Elasticsearch](http://www.elasticsearch.org/guide/reference/index-modules/analysis/hunspell-tokenfilter/).\n. This is indeed true, I guess it's something which will be sorted (no pun intended :) in the [new Ruby client](https://github.com/elasticsearch/elasticsearch-ruby).\n. This is cool, thanks!, embarrassing oversight!\n. I understand the motivation, and think this would work, but I'm just a bit wary about changing the instantiation inpersistence/attributes` -- all the models would have to respect this signature. That would also mean breaking incompatibility with existing code.\nIf we could somehow keep the method signature, I'm all for adding it, ideally \"injecting\" it from the outside?\n. Hmm, that is correct, but I think the current behaviour might actually be less surprising? When you define an index name, you clearly want the index named like this. \nI mean, we can take index_prefix into account in https://github.com/karmi/tire/blob/master/lib/tire/model/naming.rb#L34\u2026 Would like to hear some opinions on that.\n/cc @vhyza\n. I tend to skip this, and just leave it to the user to combine everything in the index_name method, and focus on the new client, instead of fixing this, what do you think?\n. No, you need to encode them in separate facet declarations.\n. value_field must be numeric, since the stats are computed on it. Use terms for the channel_id.\n. Then you have to issue separate facets for these, or use the script facet.\n. Is this what we've been solving in #873?\n. I like the solution!, thanks, merged\n. Hmm, this is strange. We've been redoing and fixing that code couple of times, at some point even GC.start was added between batches, etc. \nIt should use find_in_batches on ActiveRecord, can you provide more details?\n. Yeah, look at the history of that file: https://github.com/karmi/retire/commits/master/lib/tire/model/import.rb, I had the impression it works well with Mongoid now\u2026 Can we get any more debug info here?\n. Thanks, merged!\n. @zywx No, unfortunately, that's the effect of the method signature. BTW, that's why the new client uses \"single Hash arguments\" as pseudo-named-arguments approach.\n. Hi, I think this crosses the line for \"new features\" on Tire -- the new Ruby client does support all features, though not via a proper Ruby DSL yet. It is certainly something which should go to the future elasticsearch-dsl. \n@emptyflask If you have any examples etc, please drop them here for record!\n. Can you configure logging to see how this search is sent to ES? Normally, the syntax is a bit different: https://github.com/karmi/retire/blob/master/test/integration/filters_test.rb#L39-L40\n. Hi, this is quite fine. Glad to see somebody using concerns to separate Tire-related logic. Have you seen articles by Josh Symmonds? http://joshsymonds.com/blog/2012/10/25/rails-concerns-v-searchable-with-elasticsearch/\n. Well, in Tire, you have to run the operations individually:\n``` ruby\nTire::Alias.new name: 'a_2' do\n  index 'test_a_1'\nend\nTire::Alias.new name: 'b_2' do\n  index 'test_b_1'\nend\n...\n```\nYou can pass the Hash/JSON payload to Tire, but it might be better to use the new client for that...\n. The syntax and setup are correct, this should work. I've added an integration test just to cover this case. Are you sure you're not running this against empty index, not refreshed index (eg. in tests), etc?\n. Can you submit an issue at the elasticsearch_autocomplete repo as well, please?\nAlso, are you sure your model is properly serialized into JSON? Ie. user_city is working etc?\n. It's probably a bug, but I think the \"sufficient high number\" is a good workaround for now. If you'd like to submit a patch, please send it in a pull request!\n. @negarnil You just have to specify the size, that's the only option.\n. I'd just change the http://karmi.github.com/tire/ link to http://karmi.github.com/retire/, since everything else should be automatically redirected by Github.\n. Thanks!, merged and pushed.\n. Hi, I've added a banner to the README and also a Wiki page: https://github.com/karmi/retire/wiki/Tire-Retire\n. Bump @karmi\n. You need to set the field to use the 'keyword' analyzer. See integration tests for examples.\nOn 9. 10. 2013, at 10:31, syedali7 notifications@github.com wrote:\n\nTire facet giving results for term 'New York' as shown below\n\"facets\" : {\n\"movie\" : {\n\"_type\" : \"terms\",\n\"missing\" : 32,\n\"total\" : 8,\n\"other\" : 0,\n\"terms\" : [ {\n\"term\" : \"New\",\n\"count\" : 4\n}, {\n\"term\" : \"York\",\n\"count\" : 4\n}]\n}\nHow to get correct results. Could any one help in this\nThanks inadvance\n\u2014\nReply to this email directly or view it on GitHub.\n. @mahemoff This has been debated over and over, please search the issues... We can't do where(id: ids), since we have to work with ActiveModel, Mongoid, etc.\n\n@rdetert You can overload __find_records_by_ids, where you can use all ActiveRecord, Mongoid, or your-custom-ORM-specific logic.\n. Hey, cross-posting between Github issues and Stackoverflow is actually something which makes it very hard for me and others to help you. Since the SO post contains more detail, I'm closing this one.\nCC @nz since it's a Bonsai issue.\n. See #857 \n. Put Tire.configure { logger STDERR, level: \"debug\" } in your initializer/controller/etc to see what's going on in ES\n. @anggainwgs Tire should work with both Rails 4 and jRuby just fine. It's not really too helpful to tell us \u201cI always get nothing from my search!\u201d, since we can't really know anything about your data, about the query you do, etc. Please update the issue with more information.\n. Hi, there's been a discussion about this quite some time ago e.g. here: https://gist.github.com/ddemaree/1408149\nYou're take is interesting, since it has a minimal footprint, and it's quite understandable. However, since Tire was retired, I don't think adding features to it makes real sense? (I'm working on a ActiveRecord/Rails/etc integration for the new gem)\n. No support in Tire, can you use the new Ruby client here? https://github.com/elasticsearch/elasticsearch-ruby/blob/master/elasticsearch-api/lib/elasticsearch/api/actions/indices/validate_query.rb\n. No, they shouldn't clash in any way.\n. You have to approach it a bit differently:\n``` ruby\nrequire 'awesome_print'\nrequire 'tire'\nTire.index('articles') do\n  register_percolator_query('author') do\n    filtered do\n      query do\n        string 'john'\n      end\n      filter :terms, author_id: [1, 2]\n    end\n  end\nend\nap Tire.index('_percolator').retrieve('articles', 'author').to_hash\n``\n. That makes sense, thanks, I'll look into the test fixes commit (332e005) later.\n. So,indexis reserved in Mongoid. You should use thetire` proxy. I can't replicate the issue:\n``` ruby\n% rails console\nLoading development environment (Rails 3.2.15)\n\n\nMongoid::VERSION\n=> \"3.1.5\"\nArticle.tire.index_name\n=> \"articles\"\nArticle.tire.index.delete\n=> true\nArticle.tire.create_elasticsearch_index\n=> nil\nArticle.tire.index.mapping\n=> {\"article\"=>{\"properties\"=>{\"title\"=>{\"type\"=>\"string\", \"analyzer\"=>\"snowball\"}}}}\n``\n. It should work out of the box. I can't replicate the issue on Mongoid 3.1.5.\n. The code is only illustrative -- you have to implement the worker logic yourself. Also, have a look at https://github.com/EvilFaeton/tire_async_index\n. hey, unfortunately the HTML file is generated fromtire-dsl.rband the Markdown processor apparently can't handlepublished_on` well. Seems like it was handled better in the last generation: http://karmi.github.io/retire/#section-85\n. @naveenagarwal @stabenfeldt Depends on how your mapping is defined, assuming you're not using the defaults.\n\n\nPost the output of /<YOURINDEX>/_mapping.\n. Yeah, the index is created automatically, if it doesn't exist, and the models are also updated automatically.\nEnable logging: Tire.configure { logger STDERR, level: \"debug\" } and check for any errors.\n. Are you sure the document is there? What if you run rake environment tire:import:model CLASS='Ad' FORCE=y before? NOTE, this will delete your index and re-create and re-index it!\n. I'm afraid I can't give any advice based on this output, but it is weird -- no usual culprits: stopwords, mapping changes without reindexing, etc... What happens when you do Ad.search('*')?\n. Should work... What if you use different test string? Ad.create title: \"foobar\"; Article.index.refresh; Article.search(\"foobar\").size;Article.search(\"title:foobar\").size\n. Have you checked the source of the error? The index method is coming from Mongoid. Try this:\nruby\nArticle.tire.index.refresh\n. Sorry, looked now. Thanks for providing the full application!\nThe problem is with how Mongoid handles ID serialization to JSON. You have to redefine the to_indexed_json method. \nTeh diff:\n``` diff\ndiff --git i/app/models/ad.rb w/app/models/ad.rb\nindex 212600f..dffd43d 100644\n--- i/app/models/ad.rb\n+++ w/app/models/ad.rb\n@@ -1,8 +1,14 @@\n+Tire.configure{ logger STDERR, level: 'debug' }\n+\n class Ad\n   include Mongoid::Document\n   include Tire::Model::Search\n-  include Tire::Model::Callbacks \n+  include Tire::Model::Callbacks\nfield :title, type: String\n   field :body, type: String\n+\n+  def to_indexed_json(options={})\n+    to_json(except: [:id, :_id])\n+  end\n end\n```\nTeh console demo.\n``` ruby\na = Ad.first\n=> #\na.to_json\n=> \"{\\\"_id\\\":{\\\"$oid\\\":\\\"52de42c1616c751139000000\\\"},\\\"body\\\":null,\\\"title\\\":\\\"foobar\\\"}\"\na.to_indexed_json\n{\"body\":null,\"title\":\"foobar\"}\na.tire.update_elasticsearch_index\n2014-01-21 11:01:05:807 [ad/52de42c1616c751139000000] (\"ads\")\n\ncurl -X POST \"http://localhost:9200/ads/ad/52de42c1616c751139000000\" -d '{\"body\":null,\"title\":\"foobar\"}'\n2014-01-21 11:01:05:808 [201]\n\n{\"ok\":true,\"_index\":\"ads\",\"_type\":\"ad\",\"_id\":\"52de42c1616c751139000000\",\"_version\":1}\n...\nAd.search('foobar').first.title\n2014-01-21 11:01:27:282 [_search] ([\"ads\"])\n\ncurl -X GET 'http://localhost:9200/ads/ad/_search?size=10&pretty' -d '{\"query\":{\"query_string\":{\"query\":\"foobar\"}},\"size\":10}'\n2014-01-21 11:01:27:283 [200] (2 msec)\n\n{\"took\":2,\"timed_out\":false,\"_shards\":{\"total\":5,\"successful\":5,\"failed\":0},\"hits\":{\"total\":1,\"max_score\":0.30685282,\"hits\":[{\"_index\":\"ads\",\"_type\":\"ad\",\"_id\":\"52de42c1616c751139000000\",\"_score\":0.30685282,\"_source\":{\"body\":null,\"title\":\"foobar\"}}]}}\n=> \"foobar\"\nAd.search('title:foobar').first.title\n2014-01-21 11:01:33:103 [_search] ([\"ads\"])\n\ncurl -X GET 'http://localhost:9200/ads/ad/_search?size=10&pretty' -d '{\"query\":{\"query_string\":{\"query\":\"title:foobar\"}},\"size\":10}'\n2014-01-21 11:01:33:104 [200] (1 msec)\n\n{\"took\":1,\"timed_out\":false,\"_shards\":{\"total\":5,\"successful\":5,\"failed\":0},\"hits\":{\"total\":1,\"max_score\":0.30685282,\"hits\":[{\"_index\":\"ads\",\"_type\":\"ad\",\"_id\":\"52de42c1616c751139000000\",\"_score\":0.30685282,\"_source\":{\"body\":null,\"title\":\"foobar\"}}]}}\n=> \"foobar\"\n```\nNOTE, that in the new model integration: https://github.com/elasticsearch/elasticsearch-rails, all this is handled for you opaquely. I highly encourage you to move to that library. Tire has a lot of historical bagage.\n. You're welcome and make sure you evaluate and potentially migrate to the new model integration in elasticsearch-model.\n. I'll try to have a look into it over holidays, been too busy with the new gem in the past couple of weeks...\n. Given the ridiculous delay on my side, I'll probably rebase, squash, edit all the commits myself.\n. @fbatista @fabn @marc-villanueva Guys, many thanks for this! I can see from the Git log it has been quite laborious to integrate all the different approaches and pull requests. Grazie, gracias, obrigado!\nSorry to anybody waiting for this for so long. I've been working on the elasticsearch-ruby library, and the upcoming elasticsearch-rails library.\n. Should be soon, either tomorrow or on Monday, depending how (un)happy I am :)\n. Could you add a concrete example? I don't understand.\n. Understood -- in Tire, the whole document is re-indexed with the default callbacks. \na) You can of course use the Index#update method, to partially update the document yourself, by reading the changes attribute and storing it, etc\nb) In the new Ruby model integration, all this will be automatic for models which implement the ActiveModel::Dirty interface.\n. Tire doesn't give you anything out of the box here.\nThe best option would probably be to intercept the changes, store them in some kind of queue, and \"replay\" them after the re-indexing is finished. Of course, you might want to use the versioning support, to not overwrite data which have been updated in the meantime, as you're replaying.\n. Yes, Tire is compatible with the 0.90.x series.\nTo upgrade your production, the decision factor is whether you can sustain downtime, a maintenance window.\nAssuming you can, then just shutdown the server, install 0.90.7 (via curl+extract, deb/rpm, etc), and start it up again. Of course, depends on how you have it installed in production, you have to keep the elasticsearch.yml and data folders around, etc.\nNo need to reindex data from Ruby, by the way, Elasticsearch 0.90 will hapilly work with data from 0.20.\n. More likely is that your mapping got setup incorrectly -- import with FORCE creates the index with mapping specified in the model.\nThere is no need to flush anything in Elasticsearch.\n. The problem is this part of the error message:\nCan't sort on string types with more than one value per doc, or more than one token per field\nElasticsearch can't sort on an \"analyzed\" fields, ie. where something like \"foo bar\" is broken into [\"foo\", \"bar\"]. Please search for \"multi field\" in the Tire and Elasticsearch documentations.\n. Actually, on recent Elasticsearch versions, sorting on multivalued fields should be supported:\n``` ruby\nrequire 'tire'\nTire.index('my-index') do\n  delete\nstore title: 'X Y Z'\n  store title: 'A B C'\n  refresh\nend\ns = Tire.search('my-index') do\n  sort { by :title }\nend\np s.results.map(&:title)\n``\n. @meejoe So, when you re-create the index with this mapping definition, you seedateOptionalTime` when you query \"myindex/_mapping\"?\nCan you turn on logging with Tire.configure { logger STDERR, level: \"debug\" } to see what is being sent to Elasticsearch?\n. That's because you have an error in your string query syntax, correct is:\nruby\nshould { string \"gender:male\" }\n. You can use the Tire::Index#update method -- see integration test at: https://github.com/karmi/retire/blob/master/test/integration/index_update_document_test.rb#L94. \n. Hi, sorry for the delay.\nYou can do two things here I guess:\n1/ The easier one would be to use the highlight feature (search integration tests and the Elasticsearch documentation), which would give you back an Array of matching tokens. You can use this Array for implementing some custom logic.\n2/ You can use the \"named queries\" feature of Elasticsearch -- you would have to split your incoming query such as austin happy hours into individual tokens and pairs (word bigrams, trigrams, ...), and use the bool query to define a separate query for each.\n. By default, mappings in ES are dynamic, so when you send additional properties, the mapping is updated. You can set dynamic to false (or strict) to prevent that; http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-object-type.html#_dynamic\n. The common query is not supported -- you're encouraged to use the new https://github.com/elasticsearch/elasticsearch-rails model integrations with the https://github.com/elasticsearch/elasticsearch-ruby gem.\n. Unfortunately, I don't have the bandwidth to prepare the query for you; the elasticsearch-ruby gem works with plain Ruby Hashes, so it should be dead easy to translate the query from the Elasticsearch's documentation to a Ruby query.\n. Thanks, added a test and merged + pushed.\n. Hi, thanks for the pull request, but apart from critical fixes and additions, I think we shouldn't add features to Tire anymore...\n. No worries :) What is the context here? LonelyPlanets' fork?\n. Hmm, I'm wondering why this might be... Do you define the to_indexed_json method? Is the status part of it? What is the output of @user.to_indexed_json? Finally, you can enable detailed logging with Tire.configure { logger STDERR, level: \"debug\" } to see all communication with ES.\nAlso, there's a new ActiveModel integration available here: https://github.com/elasticsearch/elasticsearch-rails/tree/master/elasticsearch-model, with similar semantics as Tire has. I advice to evaluate and potentially migrate to that..\n. It definitely should serialize all regular attributes of your model. The to_hash method defined in Tire delegates to serializable_hash -- can you inspect your model and see any discrepancies there? Maybe some other gem redefined the to_hash?\nAgain, make sure you evaluate the new elasticsearch-model gem...\n. @ryansch That was deliberate, as far as I remember... Otherwise you'd see lines and lines of curl commands.\nOn the other hand, I can see how debug would print the \"data omitted\" version, and the verbose version would print the JSON docs being sent over the wire...\n. Agreed... I don't think we can \"fix\" Tire in this respect, though... I would keep this bug freezed, to be honest, to not change the behaviour on the re-tired project...\n. Very glad to hear that :) Let's let this sleep then -- otehrwise, if it would really help you with something, we can make the change. \n. Hi, thanks for the care :), though, does it really matter to improve such details on a re-tired gem?\n. I remember the CLASS='Article.all' trick somebody's been using with Mongo, but not the details. Try it with the regular CLASS=Article argument.\n. You're using quite an old version of Tire. Please upgrade and try again.\nAlso, there's new integration for ActiveModel+Rails at https://github.com/elasticsearch/elasticsearch-rails, I strongly recommend to use it instead of Tire.\n. I'm afraid I can't -- you're running an old version of a retired library, you don't send any information what and how exactly you include Tire modules, ...\nTry generating the example application (see README), try playing with the included integration tests for Mongo.\n. Thanks! Make sure to have a look at the https://github.com/elasticsearch/elasticsearch-rails/tree/master/elasticsearch-model gem.\n. Hi, interesting. First, I'm afraid I can rule out renaming document_type to retire_document_type. The gem has been indeed retired, no features are added, and this would be a 100% breaking compatibility change.\nSecond, no method defined by Tire should conflict with a method defined by the user -- that's the reason why everything is route through the tire proxy class and instance methods. Maybe the document_type method has some bug in this respect?\n. So, again -- the tire proxy should defend you against a cross-polination here. Closing this, waited for feedback.\n. Hi, thanks for looking into it in detail!\nI'm not principially oposed to making it work on AR 4.1+, but I think it would be a wasted time for anybody involved. The gem is effectively retired, no major features will be added, and people are actively adviced against using it.\nThe new elasticsearch-model and elasticsearch-rails gems [https://github.com/elasticsearch/elasticsearch-rails] are the future of Elasticsearch &\u00a0Rails integration -- have you checked them out?\n. Hey,\nfirst, the migration should be easy. There's no Ruby DSL in the new gems, but given how much tricky the Tire DSL can be at times, that's for the better for everyone involved I guess.\nThere is, and won't be, a \"migration guide\" -- the semantics are very similar, and the new gem is extensively documented, with examples and integration tests.\nI stongly advice you against using Tire in production, unless you have some really compelling reason.\n\nI wasn't clear if elasticsearch-model and elasticsearch-rails should be used either since they say they are pre-release.\n\nSo, if I remove the pre from version.rb, do you think it will magically work better? :) I wouldn't judge software by the number, but by the code, the docs, the tests.\n. >  (...) is support for the percolate API implemented (...)\nPercolation has changed in Elasticsearch 1.0, so it's no longer possible to percolate during indexing, something Tire made really simple.\nThere's no explicit support for percolation in the elasticsearch-rails gems, I haven't been thinking about it deeply, but I think it's more of an end-user's thing -- if you wanna trigger some event when you index a specific document, just put those docs as Hashes on a queue (Resque, Sidekiq, RabbitMQ, ...), consume them, do something based on response...\nSee the percolate documentation.\n. Where do you put the ENV['ELASTICSEARCH_URL'] = \"http://myserver.com:9200\" config? And why do you do it like that? If you want to configure Tire for a specific server, just use some initializer with the Tire.configure block.\nThe environment variable is preferable, but you have to run it like this then:\nbash\nELASTICSEARCH_URL=http://myserver.com:9200 bundle exec rails console\n. Yeah, then configure the Tire client directly in each environment file, that should work.\n. Not when you directly set the URL in a Tire.configure block :)\n. OK, so let me explain it again :)\n1. The simplest way how to configure the URL is to use the ENV variable. You have to set it in the environment, though, either with export or when running the command. This is what Heroku does by default, and is recommended by the 12factor design. In other words, do not set it in you Ruby code, but \"on the command line\".\n2. If you want to configure Tire differently for each environment, use the initializers with Tire.configure Ruby blocks. It has all the downsides associated with it -- the config is baked into the codebase, into the Git repo, etc.\n. True. However, I'd keep Tire on 0.90 series -- people should really migrate to https://github.com/elasticsearch/elasticsearch-rails/\n. @hydrozen Fair point. Tire works with 1.0 for many workflows, but obviously, this would also be a nice incentive to upgrade to the new gems.\n@sigmarstern Waiting for your reply -- should I close the ticket? I'm really against any substantial effort to make Tire 1.0 compatible.\n. @sigmarstern Yes, cool. Please do consider upgrading to both ES 1.0 and the new gems as a priority, the benefits are really huge across the performance, flexibility, extendability, stability, predictability aspects... Ping me on the new repos issues or IRC if you need assistance or ping-pong something.\n. Thanks!, merged the respond_to? fix.\n. a) Run the Rake task with --trace to see the whole backtrace\nb) Do you include will_paginate or kaminari in your Gemfile?\n. So, what are you importing when your Article is Tire::Model::Persistence, not e.g. ActiveRecord::Base?\n. Hi Spencer, I guess it makes sense to make this change (although the change in Elasticsearch is correct). So, what about:\n1. Squashing the commits into a single one,\n2. Adding a unit test for the behaviour.\n. Hi, I'm sorry, but adding new features to the gem is not something I'd like to do -- it is effectively freezed.\n. Please check the https://github.com/elasticsearch/elasticsearch-rails and https://github.com/elasticsearch/elasticsearch-ruby projects.\n. Ah, great. Was wondering what was causing it :)\n. Hmm, I guess just \"nuking\" the callbacks by redefining them with define_method or class_eval or something like that...\n. Yeah, your right about the method -- the immediate solution would be to monkeypatch the get_id_from_document method, so it ignores your special _id method.\n. So, since you have path: :account, shouldn't you simply use an \"account\" property of your document for routing?\n. Should be puts Article.tire.search \"something\", :load => true do ....\n. Hey, please check out the https://github.com/elasticsearch/elasticsearch-rails gem, this one is no longer recommended.\n. Hi Bruno, I'm sorry, but a code change like adding a function_score is precisely what I have in mind when I say that the Tire gem is \"retired\" and no major new features shold be added to it...\n(In addition, I see many unrelated commits under this PR, and I don't see any test coverage.)\n. Closing this, please ping me if you need to talk about it more.\n. That's not that much misleading, since the intention of this example is indeed to connect to port 80, presumably, to an Nginx/etc proxy in front of Elasticsearch. You generally don't and shouldn't have port 9200 open to the outside world.\n. Hi, not sure, but have a look at https://github.com/elasticsearch/elasticsearch-ruby and https://github.com/elasticsearch/elasticsearch-rails, this gem is no longer recommended.\n. Hi, first of all, this gem is really, really \"retired\", so there's a little chance significant features will be added to it.\nMore importantly, \"stop using numbers as IDs\" :)\n. Thanks, verified and merged!\n. Hi, yeah, it uses the old Hash syntax. As well stated in various places, Tire should be considered a \"retired\" project. The new gems provide (almost) all the features, and new features are added to them.\n. You shouldn't use the query string query unless you need all the power of Lucene syntax. Have a look at match query.\nYou need to escape special characters like [.\nThe actual error message is at the end of the error, so post it.\nOn 16. 5. 2014, at 10:26, Linus Pettersson notifications@github.com wrote:\n\nHi\nI know this project is retired but I don't have time to migrate to the new gems at the moment, so hopefully someone can help me.\nI use filters and analyzers which looks like this:\n{\n    analysis: {\n      analyzer: {\n        default: {\n          type: 'custom',\n          tokenizer: 'standard',\n          filter: ['standard', 'lowercase', 'asciifolding']\n        },\n        autocomplete: {\n          type: 'custom',\n          tokenizer: 'standard',\n          filter: ['lowercase', 'asciifolding', 'edge_ngram']\n        }\n      },\n      filter: {\n        edge_ngram: {\n          type: 'edgeNGram',\n          min_gram: 4,\n          max_gram: 25\n        }\n      }\n    }\n  }\nThen when I search I do this:\nbool.should { |s| s.string \"title.autocomplete:#{search_query}\", default_operator: 'AND', analyzer: 'default' }\nWhen my search_query ends with a dash, colon or similar it crashes with a big error:\n400 : {\"error\":\"SearchPhaseExecutionException[Failed to execute phase [query], all shards failed\n......\nIf I remove the autocomplete search part above it works.\nIf I add a space to the end of the query everything works fine.\nWhy is this happening? Do I need to escape some characters?\nIt seems weird to just throw a space character at the end...\n\u2014\nReply to this email directly or view it on GitHub.\n. Also, are you sure that the 'search_query' is not empty?\n\nOn 16. 5. 2014, at 10:26, Linus Pettersson notifications@github.com wrote:\n\nHi\nI know this project is retired but I don't have time to migrate to the new gems at the moment, so hopefully someone can help me.\nI use filters and analyzers which looks like this:\n{\n    analysis: {\n      analyzer: {\n        default: {\n          type: 'custom',\n          tokenizer: 'standard',\n          filter: ['standard', 'lowercase', 'asciifolding']\n        },\n        autocomplete: {\n          type: 'custom',\n          tokenizer: 'standard',\n          filter: ['lowercase', 'asciifolding', 'edge_ngram']\n        }\n      },\n      filter: {\n        edge_ngram: {\n          type: 'edgeNGram',\n          min_gram: 4,\n          max_gram: 25\n        }\n      }\n    }\n  }\nThen when I search I do this:\nbool.should { |s| s.string \"title.autocomplete:#{search_query}\", default_operator: 'AND', analyzer: 'default' }\nWhen my search_query ends with a dash, colon or similar it crashes with a big error:\n400 : {\"error\":\"SearchPhaseExecutionException[Failed to execute phase [query], all shards failed\n......\nIf I remove the autocomplete search part above it works.\nIf I add a space to the end of the query everything works fine.\nWhy is this happening? Do I need to escape some characters?\nIt seems weird to just throw a space character at the end...\n\u2014\nReply to this email directly or view it on GitHub.\n. Have you checked the documentation and the integration tests for match?\n\nOn 16. 5. 2014, at 11:25, Linus Pettersson notifications@github.com wrote:\n\nPerhaps I cannot use `default_operator' on a match query?\n\u2014\nReply to this email directly or view it on GitHub.\n. the nesting in your example is weird, the filtered query should be in another should or something like that.\n. @humbroll There's a PR #754 opened for that, maybe you can reuse it. I'm afraid there won't be any functionality added to the gem, see https://github.com/elasticsearch/elasticsearch-ruby and https://github.com/elasticsearch/elasticsearch-rails\n. I'm not sure what Article.find should do without an ID, and moreover, this gem is deprecated, use https://github.com/elasticsearch/elasticsearch-rails/tree/master/elasticsearch-persistence\n. You're running the task incorrectly, please see the documentation.\n\nAlso, Tire is deprecated, please use the elasticsearch-rails gem.\nOn 1. 8. 2014, at 20:33, Erik Dungan notifications@github.com wrote:\n\nI'm running tire 0.6.2 with a Rails 3.2x app and ES 0.9.0. I'm trying to use the tire:import:model rake task and get an error as if it can't find my ActiveRecord class.\nCommand I run:\nrake tire:import:model CLASS=Garage RAILS_ENV=staging --trace\nError I get:\n* Invoke tire:import:model (first_time)\n* Execute tire:import:model\nrake aborted!\nuninitialized constant Garage\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/tire-0.6.2/lib/tire/tasks.rb:95:in eval'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/tire-0.6.2/lib/tire/tasks.rb:95:ineval'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/tire-0.6.2/lib/tire/tasks.rb:95:in block (3 levels) in <top (required)>'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/task.rb:236:incall'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/task.rb:236:in block in execute'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/task.rb:231:ineach'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/task.rb:231:in execute'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/task.rb:175:inblock in invoke_with_call_chain'\n/usr/lib/ruby/1.9.1/monitor.rb:211:in mon_synchronize'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/task.rb:168:ininvoke_with_call_chain'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/task.rb:161:in invoke'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/application.rb:149:ininvoke_task'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/application.rb:106:in block (2 levels) in top_level'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/application.rb:106:ineach'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/application.rb:106:in block in top_level'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/application.rb:115:inrun_with_threads'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/application.rb:100:in top_level'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/application.rb:78:inblock in run'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/application.rb:165:in standard_exception_handling'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/lib/rake/application.rb:75:inrun'\n/var/www/smr/shared/bundle/ruby/1.9.1/gems/rake-10.1.0/bin/rake:33:in <top (required)>'\n/var/www/smr/shared/bundle/ruby/1.9.1/bin/rake:23:inload'\n/var/www/smr/shared/bundle/ruby/1.9.1/bin/rake:23:in `'\nTasks: TOP => tire:import:model\n\u2014\nReply to this email directly or view it on GitHub.\n. Yes, exactly!\n\nrake -Tonly lists the tasks themselves, but to load Rails environment, you need to call the environment task as well. There has been discussion about it, and some patches, but in the end, this seemed like the best solution.\nThe new gem uses that approach as well -- again, please don't use Tire, use the new gem.\n. Which callbacks? The automatic ones? Why don't you code your own after_xxx callback, with your preferred timeout handling?\n. For this, you have the custom_score query, see tests for an example.\nHowever, that works only in Elasticsearch 0.90.x -- it has been replaced by much more convenient and powerful function_score. Also, you should definitely use the new Elasticsearch Ruby client and the Rails integration.\n. Thanks for the posting the code!\nIn the new gem, we do it a bit differently: https://github.com/elasticsearch/elasticsearch-rails/blob/master/elasticsearch-rails/lib/elasticsearch/rails/tasks/import.rb#L87, not sure if the trick is still needed, but it has worked fine so far.\n. Hello, the Tire gem is truly retired, and the development is frozen when it comes to adding features.\nPlease have a look at the new https://github.com/elastic/elasticsearch-ruby and https://github.com/elastic/elasticsearch-rails gems...\n. Hi, thanks for the patch, but the codebase is effectively frozen, especially when it comes to adding or expanding features...\n. Hi, this repository is no longer actively maintained, I suggest you have a look at the https://github.com/elastic/elasticsearch-ruby and https://github.com/elastic/elasticsearch-rails gems.\nOne piece of advice with any of the gems: you need to make sure that the index is created with the correct mapping, the new gems have the force option for that.\n. I don't think this will work in your case. The Index#import method then relies on increasing the params[:page] variable. Does the batch finder in your case work the same way?\n. Why not simply do a?\ndocument = h['_source'] && h[exists] ? ... : ...\n(Sorry, never encountered this, so I'm trying to understand the issue.)\n. Yeah, true.\n. @romanbsd Roman, I'm wondering about this line. There's no match for Instrumentation nor FaradayMiddleware in faraday 0.8.1. I had to remove it to make it work. What's going on here?\n. Should be Dis Max queries.\n. Why raise here -- why not in the to_indexed_hash where we are doing the case anyway?\n. 1/ Beware of files without trailing newlines in git.\n2/ I think we should have a file like errors or something, rather then create a new file for every exception there is.\n. Thanks for having added also docs for it, that's rare :) But I think we should try for a better example? I usually hate \"foobar\", \"someotherid\" in documentation...\n. I'm trying to figure out if there isn't a valid use case for passing an object like this... Some application-level configuration value, which is determined at application startup? \n. I understand your reasoning. Anybody else has any opinion on this?\n. :+1:  :)\n. @ches Don't understand your reasoning 100%, I'm afraid, but still -- it's really the same thing as defining scopes in ActiveRecord, see eg. http://paulbarry.com/articles/2008/12/18/named-scope-to-lambda-or-not-to-lambda-that-is-the-question\n. @ches, in the article above: named_scope :published_before, lambda{ |date|  {:conditions => [\"published_at < ?\", date] }  }\n. Like this:\n``` ruby\nrequire 'tire'\nTire.index('articles').delete\nclass Article\ninclude Tire::Model::Persistence\nproperty :title\n  property :size,       :as => proc { title.size }\n  property :created_at, :default => lambda { Time.now }\nend\np Article.create(title: 'Hello').attributes,\nArticle.index.refresh,\n\"Size: %s\" % [ Article.first.size ]\n```\n. Or, more to the point:\n``` ruby\nrequire 'tire'\nTire.index('articles').delete\nclass Article\n  include Tire::Model::Persistence\nproperty :title\n  property :mydate,     :as => proc { Time.now }\n  property :created_at, :default => lambda { Time.now }\nend\np \"Current date: %s\" % [ Time.now ]\nsleep 5\nArticle.create\nArticle.index.refresh\np \"My date:      %s\" % [ Article.first.mydate ]\n``\n. Hey, I'm a bit uncertain how this would work with the HTTP clients. The methods accepts an array, sets the@urls`, and the returns a random item from the array, correct?\nI think we need much more robust implementation, as outlined in #162.\nWhat I like about the implementation is that everybody can simply stick this:\n``` ruby\nmodule Tire\n  class Configuration\ndef self.url(*values)\n  values.flatten!\n  @urls = values.map{|value| value.to_s.gsub(%r|/*$|, '')} if values.any?\n  urls.respond_to?(:sample) ? urls.sample : urls.choice\nend\n\nend\nend\n```\ninto their lib and they get the behaviour. I'm leaning towards closing this and focusing on the more robust implementation with round robin pooling, graceful handling of failed IPs, auto-discovery of cluster nodes a la the Perl client, etc. What do you think?\n. Watch out for confusion -- \"filter facet\" is a type of facet, \"facet filter\" is a filter restricting a facet. Moved your code to proper place.\n. I see you've transferred gem dependencies in the Gemfile -- while it's a nice way to separate the dependencies for platforms, AFAIK it breaks the dependency information at Rubygems itself? Can we support Ruby/JRuby in the .gemspec?\n. @tpitale I think checking MyClass.included_modules is the only thing we need to do here, no need to track dependencies manually in the Tire code specifically -- unless I'm missing something obvious.\n. Yes. Ruby 1.8 compatibility of the whole gem is in fact required, the only place where we commonly use Ruby 1.9 syntax is integration tests (easier on the eyes, most people are on Ruby 1.9, Hash preserves ordering, etc). The unit tests have to use the Ruby 1.8 syntax, because we run the unit tests suite on Ruby 1.8 on Travis (and of course locally).\n. Could we use in_groups_of as suggested in http://stackoverflow.com/a/9489691/95696 ?\n. What about Array(__find_records_by_ids klass, items.map { |h| h['_id'] })) instead?\n. Is this the meat of the change?\n. @phoet I'd rather have failing tests, then wipe the percolator index for people :) Someone might just run the Tire suite, not realizing we're wiping the index, he might have some hard-to-get-again data there... Let's be rather more careful here. Sorry about the silence about the pull requests, I'm at a vacation now, will try to squeeze some review &\u00a0merging into next week.\n. Yeah, that was my thinking in adding the if ENV['TRAVIS'], might be actually better to run percolator tests with this \"flag\". Obviously, the best solution would be to launch a separate ES service just for tests -- have something on backburner for that :)\n. Should be Elasticsearch 1.x\n. Maybe we should add the link to https://github.com/elasticsearch/elasticsearch/issues/4542 as well?\n. @jonstokes This wasn't actually working I think :)\n``` ruby\n(false || nil) == false\n=> false\n```\n. ",
    "agnellvj": "ah, ok, the app I am working on doesn't use AR at all for the documents.\n. I'm starting to wonder how the best way to implement this would be.  There are several different kinds filters.  This also applies to queries and facets.  Each has its own set of options and requirements.  Maybe a grammar with a custom DSL to build the queries, filters and facets.\nfilter do\n  filter_group :and do\n    filter_type :range do\n      :field_name param1, param2\n    end\n    filter_type :prefix do\n      :field_name param3\n    end\n  end\nend\nThis should work with both \"query\" and \"filtered\" things.  For the bool filter, it would need another \"do\" layer for { must | should | must_not }.  Essentially we would have to go case by case and build the appropriate Search type up.\nI'm not sure of an easier way to build the queries so that it implements the ES API.\n. Or another idea:\nfilter(:group => :and, :type => :range, \"es_field_name\" => {from: d1, to: d2})\nfilter(:bool, :must, :term =>  {:tag => \"search_term\"})\n. I think it would work well with the infrastructure you have right now.  I think the block structure makes sense too given the ES API and how the Search API can be translated to JSON very easily.\nI can try and help if you want.  I don't want to step on your toes or anything.\n. The more I think  about this, the more I think this would benefit from a more declarative style syntax.  It would make it easier to programmatically add filters and facets.  Kinda like building up a relation.  Something like:\nquery = Slingshot::Query.new(:query_string, \"foo\", opts_hash)\nquery.add_filter(:and, range: { from: START_DATE, to: END_DATE })\nquery.add_filter(:bool, must: { terms: { tags: ['bar', 'baz'] } })\nThe bool query is the only one that needs the \"must | must_not | should\" keyword so it's not worth making a special case for it.  This way you wouldn't have to worry about a dual interface as this should take care of all cases.\n. The problem is that ElasticSearch doesn't support naming each range. I worked around this issue by defining my set of ranges statically in my code. Then after I get the facet data back from ElasticSearch, I use the static ranges to match each item back with the range data from ElasticSearch. It's hacky, but the only way to be named after coming back from ElasticSearch.\n. ",
    "vburca": "Hello karmi, was this feature implemented eventually? I discovered this issue while looking for an answer to a related problem I have, that I posted on stackoverflow (http://stackoverflow.com/questions/17074279/filter-with-boolean-block-tire-elasticsearch)\nThanks!\n. I just bumped into this problem as well. Good that I remembered I've seen it before, here (while looking for other things, obviously). \nMaybe it would be a good idea of putting it in the documentation / examples, somewhere.\nThanks for the provided support, overall!\n. ",
    "floere": "Thanks for cleaning and standardizing the commit messages!\n. How about \"model\", since it is always an ActiveModel?\n. Yes. I suppose that is because long method chains always are problematic. Writing tests for result.object.comments.first.my_complicated_method will not be fun.\n. @karmi: Referring to 2) I had the same problem(s) in the picky-client. It is not so bad actually, performance-wise, to reorder them. Annoying, yes, but not bad at all. See https://github.com/floere/picky/blob/master/client/lib/picky-client/convenience.rb#L64-75.\n. Hi Rick\nTry this:\ncurl -v -H 'Content-Type: application/json' -H 'Accept: application/json' -X POST http://staging.testingserver.com/users/4d6779338b53226d2c000001/plans -d \"{ \\\"plan\\\" : { \\\"intent\\\" : 1, \\\"privacy\\\" : 1, \\\"body\\\" : \\\"testing\\\" } }\" -u username:password\nCheers\n. Good point!\n. Hey there,\nI have a bit of feedback/questions regarding the API:\nWhy the lambda for index_name?\nindex_name lambda { \"posts-#{Time.now.year}\" }\nWhy not just\nindex_name { \"posts-#{Time.now.year}\" }\n?\n(In that specific case I also suggest precalculating the year year = Time.now.year and then using the bound year inside the block)\nI am wondering a bit about this suggestion:\nTire.index('posts').aliases do\n  remove 'posts-2011-01'\n  add    'posts-2011-03'\nend\nWhy the departure from \"simple\" Ruby?\nIf\nTire.index('posts').aliases\nis just an array, it would imho be better to treat it as such:\naliases = Tire.index('posts').aliases\naliases.delete 'posts-2011-01'\naliases << 'posts-2011-03'\n(or use whatever Array method you like best\u2026)\nI do like @karmi 's suggestion for searches and find it elegant. Or, especially, readable \u2013\u00a0it is very clear what is done there. I presume this is a special case and I personally would refrain from moving it into the library. (But I might be wrong, of course)\nI am intrigued by the hand-standish way this needs (?) to be solved. In certain other search engines cough cough this is not worth talking about ;) (OTOH no incremental index updating, so I'll shut up now)\nBut seriously: What is the actual need for multiple indexes here?\n. @olivere, you don't sound rude at all, quite the opposite. I have to say I am not extremely invested in the ES/Lucene world anymore, but enjoy API discussions and this one so far. @karmi has probably added me for the API side of things.\nIndexes\nI should clarify:\nEarlier I was wondering why you try making index names dynamic, when Lucene already offers dynamic functionality using a combination of field values and filters. (Something along the lines that @redbeard suggested)\nI.e. have a year field with values 2010, 2011 etc., then only returning results with year 2011.\nPersonally, I'd index the same data in the same index, to enable Lucene to make optimizations etc.\n(I've talked about it regarding user-specific results here: http://florianhanke.com/blog/2011/08/31/picky-case-study-restricting-results.html)\nHowever, I can see why using separate indexes can be advantageous. (Probably a bit easier to think about and also more modular data, on disk, for backup etc. and other reasons)\nHow is performance when having many (> 1000) indexes for as many users?\nLambdas\n\nA block is evaluated once, when the class loads. A lambda would be evaluated repeatedly, on every index_name call.\n\n@karmi, I probably misunderstand this.\nIf I don't:\nHow or when a block/lambda is evaluated is up to the framework/method etc. the lambda/block is passed to.\nIf the lambda is evaluated/called, there are some differences (e.g. you can use return).\nA quick illustration:\ndef look_a_lambda lam, &block\n  p lam.call\n  p block.call\n  sleep 1\n  p lam.call\n  p block.call  \nend\nlook_a_lambda(lambda{ Time.now }) { Time.now }\nOr are you telling me how Tire would handle the block/lambda?\nI probably wouldn't support lambdas at first. The use case for them is too thin, and blocks are more elegant, imho.\n\nYou could of course memoize the result of the calculation, as @floere suggests.\n\nI actually wasn't suggesting memoization (in a strict sense, but maybe I am being too \u2026 picky ;) ), but simply by using the local variable binding :)\nBlocky design\nI can see the preference on using blocks. If the low level option is offered, why not :)\n(Personally, I think it's a fine line \u2013 and it's hard not ending up in LEGO land)\nBtw, I am intrigued by Tire::Index.new('something').aliases << 'something-else'. You say it's low level Ruby, while to me it looks like a fluid interface, where the << method returns the index itself. Which is it?\n. @olivere, good to hear :)\nIndex API\nI didn't know they communicated behind the scenes. So the block now makes a lot of sense to me, thanks for clearing that up.\nPerformance\nIf there are general problems with using many indexes, I'd at least \"syntax pepper\" it, as a reminder that you ought to know what you are doing. Same argument as the one for using a block, in the topic above, really.\n(I have to say, either you want it to be transparent, or not ;) Personally, I am for semi-transparence. As transparent as possible, with added syntax sugar/pepper to guide usage behavior. So far, people like it :) )\nAnyway, that said, I'm neither one of the maintainers, or have anything to say, but just a passersby, so take all with a big fat grain of salt.\nAliases\nI have to admit, I probably misunderstood the whole concept of aliases. Can someone clear this up for me?\nFrom the name \"alias\", I assumed the alias would be in a 1-1 relationship to the original alias.\nBut now, from your writing, it appears that it is a n-1 relationship, and a search on the index will search on all of the indexes.\n(I admit I could have seen this earlier, but it only clicked now)\nIf this is true, is \"alias\" a good name?\nCheers\n. Re \"Aliases\": Feel free to tell me to RTFM, by the way :)\n. Thanks, @olivere! :)\nPersonally, I find \"alias\" very misleading. I'd have used \"grouped indexes\" or similar, but there you are.\nAs an inspiration, in Picky you group indexes in searches, like so Search.new(index1, index2, index3, ...) which is separate to the \"aliasing\".\nNaming things is hard, but I feel the designer of ES gave up a little quickly ;) I, for one, was misled. But maybe I am the only one.\nTo be clear, the first part is fine (http://en.wikipedia.org/wiki/Aliasing_(computing)), but the grouping, not so much. All imho.\nThanks for the discussion, it was very enlightening.\n. ### Sliding Window\nI understand your sliding window examples. It is about separating the index data space somewhere else that just by using a (virtual) field.\nInstead of putting everything into one index, and declaring for each indexed document a (possibly virtual) field like \"user\", with values like \"1\", \"2\", etc. the data is split over several indexes. So instead of making the cut at the field level, you make it at the index level.\nThis is the gist of it to me.\nNow this is going to have implications for different searches.\nI assume as well that splitting things over several indexes, and then not having to define the added filter, e.g. \"user:2\", for a user-specific search (where you only search for this user's data), will speed things up\nAn aside: However, that just might not be the case. Perhaps, ES indexes \"user 2\" beautifully, and this index lookup is actually \u2013 almost \u2013 the same speed as selecting the index.\nAnother implication is that searches that go over the whole index space (i.e. over all the user's indexes) will most certainly be slower.\nWhat I am trying to say here:\nBy deciding to use n indexes instead of just one, you make a conscious decision that you know what you are doing, while taking a bit of optimization options away from ES.\nI am unsure what my opinion on it is, though. First of all, I think it is a good thing that Tire offers the possibility to do so.\nFor example, if you are certain that you won't do many searches that use all of the user indexes, but mostly, or only on single user data, then by all means use separate indexes.\nWhat I am actually trying to say:\nWhile it might be extremely clear and obvious to you that using separate indexes is the \"one true way\" (tm), I just wanted to remind you that everything comes with a tradeoff involved. And that choosing which way to go might not always be so painfully obvious, but needs knowledge about how ES works.\nSo, all in all, yes: I think it's good that Tire offers this feature.\nJust wanted to say that really thinking about what one is doing is a good thing, as always. (Yes, in this specific case the performance etc. difference might be tiny, it might be bigger in other cases)\nOutlandish API idea\nDuring the above, it occurred to me how nice it would be if ES could do the separation itself.\nDo you guys really care if it's in separate indexes or not?\nI don't at all. Assuming, of course, that I never fiddle with single indexes e.g. on the disk.\nRight now, I am thinking that it would be nice if ES did the actual optimization. Perhaps supported by the user saying \"Hey ES, you know, I'll never be searching over all users. I'll only be searching in each user's index space separately.\"\nAlias \u2013 clarification\nIf you're pulling @kimchy into this discussion, I suppose I should clarify my comment.\nPreface\nI went ahead and read http://www.elasticsearch.org/guide/reference/api/admin-indices-aliases.html. In that context, the aliases make sense. In Tire it still doesn't, to me \u2013 if you're interested in my reasoning, see below.\nOk, go\nI, too, think that \"alias\" is very fitting. And yes, it is great that an index can have multiple aliases.\nLet's take me, for example. I am called \"Florian\". You can use \"Fl\u00f6re\" too, to call me. I will listen to this alias.\nOk, so we have now established that an alias is a piece of text that maps to a thing. Like a reference. So, name -> thing.\nBut friends also call me \"Flo\" or \"Flori\". I will listen to each of these nicknames, or pseudonyms, aliases.\nLet me put that in code:\nflorian.aliases = ['Fl\u00f6re', 'Flo', 'Flori']\nThis is how I would define the aliases for myself.\nSo we have multiple pieces of text that refer to a thing, \"Fl\u00f6re\" -> florian, \"Flo\" -> florian, \"Flori\" -> florian.\nI call these a group of aliases, or \"aliases\".\nBasically, the semantics of an alias is a text pointing to a thing.\nNow, in the case of Tire \u2013 assuming I have not horribly misunderstood \u2013 we get a whole different semantic:\nindex.aliases = ['index_alias1', 'index_alias2', 'index_alias3']\nThis now represents the other direction. This is actually the index pointing to a list of index names.\nRemembering the florian's aliases above, can you understand how I'm quite confused by the API.\nI guess I missed your last paragraph, above. \n\nIn this example, the relation is reverted -- we are in fact working with a \"posts\" alias, not a \"posts\" index.\nHowever, with > certain exceptions, an alias behaves similarly as index. So my original intention here\ncould have been either a big confusion or an inspired thinking on how to make the API fluent :)\nWe'll see if/when somebody gets to implementing it for real.\n\nSo it appears you see the (or a related) problem as well?\nI have a suggestion:\nAliases vs. Grouping/Bundling\nAgain, I like the idea of having aliases,\n\"name\" -> thing\nbut I think it's being overloaded with semantics, as soon as\nindex -> list of index names\nI love the idea of having an index that bundles multiple indexes. But I still think the naming is off, since the used word, \"alias\" is being used with two very different semantics.\nAn aside regarding G7: I just recently discussed this with a linguist at my lab. Yes, it is an alias \u2013 for \"Group of 7\". But it is actually referring to the single instance of group, not the list of members. And yes, the members of the group are exchangeable. And yes, \"G7\" refers to the group. But your argument mixes two things, which is a nice summary of my confusion:\n\"G7\" -> Group of 7 -> list of each member\nalias -> bundle frontend -> list of bundled things.\nSo my suggestion is not to mix these ideas and instead make them separate entities. Something like:\n['Alias1', 'Alias2', 'Alias3'] all point to IndexA which bundles Index1, Index2, Index3.\nSo, in Tire, this would be:\nTire.index('posts').bundles('posts-2010', 'posts-2011').aliases('theposts', 'p')\nIt all comes down to\nUsing separate names for semantically different concepts.\nIn closing\nPhew :) Thanks for reading all that and yeah, @karmi, probably it's an idea to just go ahead for now. I still hope my point came across, and that it might be potentially confusing.\nPlease also note that the amount of written text above doesn't relate to the importance \u2013 but more about how much text is needed to be clear :)\n. \"If there's more to this I'd like to know.\"\n@chrisberkhout: Yes, the return issue, as I say. Consider:\n```\ndef call_the lam\n  lam.call\nend\np \"lambda\"\ncall_the lambda { return }\np \"Proc.new\"\ncall_the Proc.new { return }\n```\nCheers!\n. @chrisberkhout It only is if the API uses the return value from the block. (Which it probably won't afaics)\n. @chrisberkhout: Yes, by using & on a lambda object it's downgraded to a simple block and an explicit return can't be used anymore. So if the API method wants to accommodate returns in blocks it needs to accept lambda objects in its params, i.e. def method(lam).\nYou're right, we're getting off topic. We probably were doing so starting at comment 1 ;) (But hey, inspiring discussion!)\n. @karmi Thanks for the flowers. Happy to help :)\n. @karmi @chrisberkhout Sorry for the confusion about the END block \u2013 each END's block is only run at the very end, when the interpreter exits, which is not useful to you, I understand. Please see https://gist.github.com/2219741. A BEGIN however, is run each time when loading (i.e. executing the statements in) the file, or once when requiring (since that results in only loading once).\n. ",
    "grantr": "@karmi none taken! I agree that in many cases the maintenance cost of using a library within a library is often higher than duplicating that functionality, especially when the low-level interface is so simple.\nI do find that automatic failover and discovery increase reliability and flexibility significantly in production deployments. As for Thrift, I don't use alternative transports in production and I'm not aware of anyone who does.\n. A clarification for future readers: Failover and discovery in rubberband is implemented for all transports. When I mention alternative transports, I mean any transport besides HTTP. We use HTTP in production as well and have no reason to switch.\n. I have actually seen this in very light testing of multithreaded apps. \nThe change I'm suggesting is a single line in Index#retrieve and a couple lines in finders. Using Thread.current would be a much bigger and far-reaching change. Tire::Search already takes a :wrapper option (via Results::Collection), so it makes sense for Index#retrieve to do the same and for the finders to use that option.\n. There was no patch originally but I added one. I already had the code in my own project so I figured I might as well submit it.\nThanks for merging!\n. ",
    "pfeiffer": "Absolutely :-)\n. Would a solution be to let the developer specify when to use 'real' records and when to use data returned by ElasticSearch on a case by case basis? The developer could choose full model availability over performance where needed.\nPreferably there wouldn't be a difference between handling results returned from Model.search and eg. Model.all\n. I agree - there are no easy shortcuts.\nIf we go with the proxy object, I at least think the developer should be given the choice to eager-load the records or we might hit a N+1 situation which could impact performance even more. Since the order of the records returned by Model.find [1, 2, 3] isn't preserved it's important to order the returned records according to the IDs returned by ElasticSearch. In most use cases I can think of this should be reasonable, since the number of records that we need to sort is fairly small (30-100).\nOne thing that could help performance in this case, is returning a result set proxy that only does the sorting and retrieval of records when needed, eg. when accessing the collection - kinda like lazy relations in Arel. This way we can do a results = User.search(..); results.facets without hitting the underlying datastore, and only hit the datastore when we do results.each ...\n. ",
    "michaelklishin": "I think Sunspot at some point used #solr_index that was aliased to index if that wasn't defined already. This is a dead simple solution and it worked well for them. So I suggest doing that with elastic_search_index being the canonical method name.\n. Well, elastic_search_mapping still has something to do with ES in my opinion because if you stop using ES or provide full-text search at all, that method won't be necessary. I think it is not one of those cases when readability or semantics are the most important properties. The goal is to avoid method conflicts and still keep names meaningful (to me that means \"hinting at ES\"). Going for readability is likely to yield a solution that will be significantly more complex and will take days to develop & fully test out.\n. Also, changing #current_page to work like this\nruby\nif @options[:size]?\n  (@options[:size].to_i + @options[:from].to_i) / @options[:size].to_i\nelse\n  1\nend\nmakes the issue go away. I am not sure if there is a better way to solve it.\n. Looks like will_paginate does not support mongoid so I am looking into this issue myself.\n. I have never seen a project that puts half of documentation it has in the README and is known for its docs. You cannot please everyone with one document. I haven't really read contributed changes in full but just taking them as one document in the wiki will produce the same result: either too much information all at once or only providing some very basic examples that will lead people to believe that Tire and ES cannot handle advanced cases.\n. I agree that it worked well for Sinatra. Sinatra is different in two ways:\n- People who came around it were already familiar with web development in some shape or form. Not the case for search servers like ES or Solr, especially given the widespread \"Java is hard and ugly and always means overengineered crap\" thinking in the Ruby community.\n- Sinatra has (intentionally) next to no advanced features. ES has plenty and thus Tire will have support for at least some of them (ideally, all).\nBut I see what you are saying and my intent is to begin adding wiki pages in the same order amqp gem guides are structured, then the README can be trimmed down.\n. Faraday is definitely the way to go.\n. ",
    "svenfuchs": "haha :)\nyou're welcome, buddy \n<3\n. ",
    "pacoguzman": "Maybe we could do some like this:\nalias_method :index, :tire_index unless method_defined? :index\nTire should define tire_method_name methods an execute this code when the module is included\n. Define to_indexed_json instance method with all the field you need in your model then tire call it\n. I suggest the following syntax for the different use cases of histogram facets (maybe applicable to other facets)\n``` ruby\nDefault histogram using a field 'words'\ns = Tire.search('articles-test') do\n         query { all }\n         facet 'words' do\n            histogram :words, :interval => 100\n         end\n      end\nCustom histogram facet, define the facet with the histogram key\ns = Tire.search('articles-test') do\n         query { all }\n         facet 'words' do\n            histogram :words, :histogram => {:key_field => \"words\", :value_field => \"other_field\", :interval => 100}\n         end\n      end\n```\nHistogram facets\nOther option could be use a custom facet where all the options build the facet like this:\nruby\ns = Tire.search('articles-test') do\n         query { all }\n         facet 'words' do\n            custom :histogram => {:key_field => \"words\", :value_field => \"other_field\", :interval => 100}\n         end\n      end\nThis pull request implements the first option.\nAny opinion about this?\n. Hi,\nI take it\n. I'm getting the following error throwing the next query:\nshell\ncurl -X GET \"http://localhost:9200/articles-test/_search?pretty=true\" -d '{\"custom_score\":{\"query\":{\"query_string\":{\"query\":\"title:T*\"}},\"script\":\"_score * doc['words'].value\"}}\n``` shell\n[2011-10-25 22:38:33,312][DEBUG][action.search.type       ] [Torrent] [articles-test][3], node[qxUNFpjMSR2wgWa8_4UH7A], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@9e5e21]\njvm 1    | org.elasticsearch.search.SearchParseException: [articles-test][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"custom_score\":{\"query\":{\"query_string\":{\"query\":\"title:T*\"}},\"script\":\"_score * doc['words'].value\"}}]]\njvm 1    |  at org.elasticsearch.search.SearchService.parseSource(SearchService.java:495)\njvm 1    |  at org.elasticsearch.search.SearchService.createContext(SearchService.java:407)\njvm 1    |  at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:230)\njvm 1    |  at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)\njvm 1    |  at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)\njvm 1    |  at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)\njvm 1    |  at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)\njvm 1    |  at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)\njvm 1    |  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\njvm 1    |  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\njvm 1    |  at java.lang.Thread.run(Thread.java:679)\njvm 1    | Caused by: org.elasticsearch.search.SearchParseException: [articles-test][3]: from[-1],size[-1]: Parse Failure [No parser for element [custom_score]]\njvm 1    |  at org.elasticsearch.search.SearchService.parseSource(SearchService.java:481)\njvm 1    |  ... 10 more\n```\nI'm not pretty sure whats going on, I'm missing something\nAny thoughts?\nI think you could test it importing the articles fixtures\n. @danoph thanks, I'll try but is strange to me the need to wrap in query{} and then have another query inside but I'll try thanks\n. Hi,\nMaybe you could take a look to #139 , my first attempt to try to resolve this one. Excuse me but I don't know how to related that issue with this ;)\nAny comments?\nI'd tried to avoid the use of a query block inside the custom_score block\nruby\nsearch = Item.search page: 1, per_page: 30 do\n  query do\n    custom_score script: \"random()\" do\n      query { string \"title:T*\" }\n    end\n  end\nend\nAnd simply we can do\nruby\nsearch = Item.search page: 1, per_page: 30 do\n  query do\n    custom_score script: \"random()\" do\n      string \"title:T*\"\n    end\n  end\nend\nEverything inside the custom_score block is execute in a new query object context\n. You're examples have more sense \"more real-world\" great!\nThanks for the notes\n. ",
    "russ": "Cool. I'll give that a try. Thanks for the detailed response.\n. The solution did work. But for the time being I went back to sunspot. I will definitely give Tire another try in the future.\n. ",
    "aaronchi": "This looks very cool :D\n. how about a page helper for the block syntax?\n. This library needs a kaminari adapter... See\nhttps://github.com/karmi/tire/issues/14\nimport just needs to use model.page if kaminari is installed for this to work correctly\n. Actually, at least for activerecord, this should probably be using the batched 'find_each' method. Then you can remove the dependency on paginate here.\nhttp://apidock.com/rails/ActiveRecord/Batches/ClassMethods/find_each\n. Nevermind. found a better solution for this using facet filters\n. cool... one thing to note. Using the branch I was getting an error calling results if the search doesn't return any items\n. I don't believe STI uses _type but something happens when you include Tire::Model::Search that breaks the type handling. I'll take a deeper look sometime next week and see if I can figure out what's going on.\n. Ah yes. I believe rails does use _type internally. there may be other stuff going on as well.\nIn my model:\n```\nclass Image < Asset\nend\n```\nwithout tire:\n```\n\nImage.first._type\n \u2190[1m\u2190[35mImage Load (1.0ms)\u2190[0m  SELECT assets.* FROM assets WHERE assets.type IN ('Image') LIMIT 1\n=> \"Image\"\n```\n\nwith tire:\n```\n\nImage.first._type\n\u2190[1m\u2190[35mImage Load (1.0ms)\u2190[0m  SELECT assets.* FROM assets WHERE assets.type IN ('Image') LIMIT 1\n NoMethodError: You have a nil object when you didn't expect it!\n You might have expected an instance of Array.\n The error occurred while evaluating nil.[]\n    from C:/Ruby192/lib/ruby/gems/1.9.1/gems/tire-0.3.1/lib/tire/model/search.rb:199:in block (2 levels) in <class:InstanceMethodsProxy>'\n    from C:/Ruby192/lib/ruby/gems/1.9.1/gems/tire-0.3.1/lib/tire/model/search.rb:259:in_type'\n    from (irb):66\n    from C:/Ruby192/lib/ruby/gems/1.9.1/gems/railties-3.1.0/lib/rails/commands/console.rb:45:in start'\n    from C:/Ruby192/lib/ruby/gems/1.9.1/gems/railties-3.1.0/lib/rails/commands/console.rb:8:instart'\n    from C:/Ruby192/lib/ruby/gems/1.9.1/gems/railties-3.1.0/lib/rails/commands.rb:40:in <top (required)>'\n    from script/rails:35:inrequire'\n    from script/rails:35:in `'\n```\n\nalso:\n```\n\nImage.first\n  \u2190[1m\u2190[36mImage Load (0.0ms)\u2190[0m  \u2190[1mSELECT assets.* FROM assets WHERE assets.type IN ('Image') LIMIT 1\u2190[0m\n(Object doesn't support #inspect)\n=>\n```\n. I added a basic rails app that shows the problem here:\nhttps://github.com/aaronchi/tire-sti\n\nTire is defined on the Image model which is a child of the Asset model. If you go into console and try to create an image\nImage.create(:title => 'Something')\nYou get the error I was describing above:\nNoMethodError: You have a nil object when you didn't expect it!\n You might have expected an instance of Array.\n The error occurred while evaluating nil.[]\n    from C:/Ruby192/lib/ruby/gems/1.9.1/gems/tire-0.3.2/lib/tire/model/search.rb:199:in `block (2 levels) in <class:InstanceMethodsProxy>'\n    from C:/Ruby192/lib/ruby/gems/1.9.1/gems/tire-0.3.2/lib/tire/model/search.rb:259:in `_type'\nIf you move the tire index to the Asset model, creation works fine because it doesn't conflict with the STI type variable.\n. To tell the truth, I was confused by this at first as well. There seems to be a direct search method for ActiveRecord, a search DSL, and the ability to pass a hash - all of these methods support different types of functionality.\nI think it would be simpler if there was a single search object that could be built as a hash or a block and shared the same set of global options on ActiveRecord objects (page, load, etc). I think this would make the library easier to understand and use. \n. Hi. sorry for not providing more information. I narrowed down the issue in my code and it seems to have something to do with trying to apply additional nested options to a facet. In my code I am using a facet filter to update the facets based on what the user is searching for. From the elasticsearch guide:\n\"facets\" : {\n    \"<FACET NAME>\" : {\n        \"<FACET TYPE>\" : {\n            ...\n        },\n        \"facet_filter\" : {\n            \"term\" : { \"user\" : \"kimchy\"}\n        }\n    }\n}\nSet in tire like:\nfacet 'category' do\n    terms 'category_ids', :size => 100, :facet_filter => {:term => {:id => category_ids}}\nend\nThis was working in 0.3.6, but afterwards, when I run the query, it only returns the results of the first facet if I have a facet_filter set on multiple facets.\nI haven't looked deeper into this yet but I imagine the change happened here when the way that options passed to terms facets were changed:\nhttps://github.com/karmi/tire/commit/d1f89e6cf230d80ec98792fcc241cc13f47c72ce\n. Ah I see.Just pass the :facet_filter options to the facet instead of the terms. :)\n. Facets support should also be added. I monkey patched facet with this to get it working\ndef geo_distance(options={})\n  @value = { :geo_distance => { }.update(options) }\nend\n. It's actually pretty easy. but you need to do some monkeypatching to make i work. Here's what I did in my tire.rb initializer\n```\nrequire 'base64'\nTIRE_AUTH = \"Basic #{Base64.encode64('username:password')}\"\nmodule Tire\n  module HTTP\n    module Client\n      class RestClient\n    def self.post(url, data)\n      perform ::RestClient.post(url, data, :authorization => TIRE_AUTH)\n    rescue ::RestClient::Exception => e\n      Response.new e.http_body, e.http_code\n    end\n\n    def self.put(url, data)\n      perform ::RestClient.put(url, data, :authorization => TIRE_AUTH)\n    rescue ::RestClient::Exception => e\n      Response.new e.http_body, e.http_code\n    end\n\n    def self.delete(url)\n      perform ::RestClient.delete(url, :authorization => TIRE_AUTH)\n    rescue ::RestClient::Exception => e\n      Response.new e.http_body, e.http_code\n    end\n\n  end\nend\n\nend\nend\n```\nI don't think it would be too hard to add a config variable to tire to make this work out of the box but I'll let karmi decide if it's something he wants to support. Also, so you know, elasticsearchhq doesn't currently support the _bulk operation (as least since I last used it) so you can't run a normal import using the rake task.\nI ended up setting up a couple of elasticsearch servers on ec2 using a load balancer in front (the load balancer is probably unecessary for a small site but I wanted to test the clustering). There is a nice article here that will get you started and it's pretty easy to do:\nhttp://www.elasticsearch.org/tutorials/2011/08/22/elasticsearch-on-ec2.html\n. You could use a custom client but since I was only adding an authorization key to a few methods, it seemed simpler to do it this way.\n. Not sure why it's not 'cool'... have been doing this in my code for a while now:\nsort do\n  by :_geo_distance, 'addresses.location' => [lng, lat], :unit => 'mi'\nend\n. That is a type on my point, I corrected it above.\nMore recently, I have been using a hash:\n{:lat => latitude, :lon => longitude}\nWhich works as well\n. unfortunately, this is now called ENV['BONSAI_INDEX_URL'] :P\n. As an aside, the current import methods will not work with Bonsai because of the use of the 'bulk' command\n. This probably just means you need to reindex?\n. In case anyone is interested, there is an easy way to hack this into the search DSL\n```\nmodule Tire\n  module Search\n    class Query\n  def more_like_this(like_text, options={})\n    @value = { :more_like_this => {:like_text => like_text}.update(options)}\n  end\n\nend\n\nend\nend\n```\n. ",
    "jkraemer": "With acts_as_ferret I built a wrapper around the hash returned from the search engine for each result which would only load the underlying model object when an attribute not being in the hash was queried.\nLooking back I think that's overkill, because as a  developer you usually know if you're going to use data from the DB on your results page or not. So an option telling Tire to load (and order according to score) all result records from DB when running the search should be enough imho. However it would still be nice if one could use article.some_attribute instead of article.model.some_attribute when querying an attribute of the prefetched AR model...\n. ",
    "morgler": "yes, I would definitely appreciate that\n. ",
    "ysf": "+1\n. Uh, thanks a lot for your nice work and superb support! :)\n. just watching.\n. ",
    "jmangoubi": "+100000 (This is so important for this gem) \nHas this been added yet? If not when might it be?  Much appreciated!\n. ",
    "arvindang": "Yup! Yes please. +1\n. ",
    "vecernik": "Is there any solution to use attachment-type plugin (using tika) with Tire?\n. @karmi Would you provide some clues how to add attachments (via has_many) to AR model index (with Tire::Model::Callbacks)? I have tire-app.jar installed in exec path as well as elasticsearch-mapper-attachments, but can't find how to setup that.\n. ",
    "ryudice": "Is there still no planned support for this feature?\n. ",
    "demetrios": "+1 on seeing a native Ruby wrapper for this in Tire...pretty please!\n. Any news on this issue? I have a need to access max_score as well...  this kluge seems to work, for time being: \nDocument.search({query: \"apple\"}).instance_variable_get(:@response)[\"hits\"][\"max_score\"]\n. ",
    "redrick": "hi guys, I use method milion times described on the internet to include binary attachement, but I got problems when a model has_many attachments, through: attachment_mapping... \nHow is it with this feature ? (just asking, this might help me actually :) thx) \n. have you run through for example this code here\nhttps://gist.github.com/karmi/3200212\nit gives you whole demo of retire elasticsearch :)\nyou will maybe find those missing configs in there :)\n. ",
    "jonarrien": "I got Tire working with attachments plugin. May be it helps to someone for simple implementations...\nThe to_indexed_json method:\ndef to_indexed_json\n    {\n      title:              title,\n      description:  description,\n      author:         author,\n      attachment: attachment\n    }.to_json\nend\nAnd here is the attachment method which converts the attachment to Base64\ndef attachment\n    if file.present?\n      path_to_file = Rails.root.to_s + '/public' + file_url.to_s\n      Base64.encode64(open(path_to_file) { |f| f.read })\n    else\n      Base64.encode64('missing')\n    end\nend\n. I forgot to include the trace:\n() Users/jon/.rvm/gems/ruby-2.0.0-p195/bundler/gems/mongoid-e1b32e598ec2/lib/mongoid/criteria.rb:470:in `check_for_missing_documents!'\n   () Users/jon/.rvm/gems/ruby-2.0.0-p195/bundler/gems/mongoid-e1b32e598ec2/lib/mongoid/criteria/findable.rb:20:in `execute_or_raise'\n   () Users/jon/.rvm/gems/ruby-2.0.0-p195/bundler/gems/mongoid-e1b32e598ec2/lib/mongoid/criteria/findable.rb:40:in `find'\n   () Users/jon/.rvm/gems/ruby-2.0.0-p195/bundler/gems/mongoid-e1b32e598ec2/lib/mongoid/findable.rb:88:in `find'\n   () Users/jon/.rvm/gems/ruby-2.0.0-p195/bundler/gems/tire-07e69c810cb4/lib/tire/results/collection.rb:156:in `__find_records_by_ids'\n   () Users/jon/.rvm/gems/ruby-2.0.0-p195/bundler/gems/tire-07e69c810cb4/lib/tire/results/collection.rb:144:in `block in __get_results_with_load'\n   () Users/jon/.rvm/gems/ruby-2.0.0-p195/bundler/gems/tire-07e69c810cb4/lib/tire/results/collection.rb:131:in `__get_results_with_load'\n   () Users/jon/.rvm/gems/ruby-2.0.0-p195/bundler/gems/tire-07e69c810cb4/lib/tire/results/collection.rb:27:in `results'\n   () Users/jon/.rvm/gems/ruby-2.0.0-p195/bundler/gems/tire-07e69c810cb4/lib/tire/results/collection.rb:35:in `each'\n  app/views/customers/index.html.haml:106:in `_app_views_customers_index_html_haml___1406913769269704192_70321301039100'\n  actionpack (4.0.0) lib/action_view/template.rb:143:in `block in render'\n  activesupport (4.0.0) lib/active_support/notifications.rb:161:in `instrument'\n  actionpack (4.0.0) lib/action_view/template.rb:141:in `render'\n  actionpack (4.0.0) lib/action_view/renderer/template_renderer.rb:49:in `block (2 levels) in render_template'\n  actionpack (4.0.0) lib/action_view/renderer/abstract_renderer.rb:38:in `block in instrument'\n  activesupport (4.0.0) lib/active_support/notifications.rb:159:in `block in instrument'\n  activesupport (4.0.0) lib/active_support/notifications/instrumenter.rb:20:in `instrument'\n. Sorry @karmi , I have been a bit a busy. I will test it on my rails4 branch as soon as I can, but looks my suspicions about the oid have been confirmed. Thanks\n. @karmi yes, I got it working. Thanks and sorry for not answering sooner.\n@mrjlynch Maybe you can try using _id.to_s in the \"to_indexed_json\" method to ensure.\n. ",
    "roma86": "@jonarrien, thank you\n. ",
    "khoan": "monkey patch\n``` ruby\nmodule Tire\n  module Results\n    module Pagination\n      alias :num_pages :total_pages\n  def limit_value\n    @options[:per_page] ? @options[:per_page].to_i : 10\n  end\n\nend\n\nend\nend\n```\n. glad to be of service.\nfor a quick 'n' lite intro to kaminari http://railscasts.com/episodes/254-pagination-with-kaminari\n. thanks! an index refresh does the trick.\n. Can we do two level deep mapping/index? Should I create a ticket for this?\n. this is what I tried:\n``` ruby\nclass NestedModel\n  ...\nmapping do\n    indexes :base_field, index: :not_analyzed\n    indexes :first_level do\n      indexes :first_level_field, index: :not_analyzed\n  indexes :second_level do\n    indexes :second_level_field, index: :not_analyzed\n  end\nend\n\nend\n...\nend\n```\nquerying http://localhost:9200/nested_models/_mapping would give something along the line of:\n``` ruby\n{\n  nested_models: {\n    nested_model: {\n      properties: {\n        base_field: {type: :string, index: :not_analyzed},\n    first_level:  {\n      properties: {\n        first_level_field: {type: :string, index: :not_analyzed},\n\n        second_level: {\n          dynamic: true,\n          ...\n        }\n      }\n    },\n\n    second_level: {\n      properties: {\n        second_level_field: {type: :string, index: :not_analyzed}\n      }\n    }\n  }\n}\n\n}\n}\n```\nHave I done something silly in the mapping definition?\n. Cool, glad a greater mind has confirmed it. created #75 for this issue.\n. cool! eagerly waiting!\nI'll close this then.\n. ",
    "kbaum": "Hi.  I tried the monkey patch and I tried to follow the screencast but i get a missing method page.\nundefined method `page' for #<Tire::Results::Collection:0x0000010547ef18>\nWhich makes sense because we don't have a page method from the patch.  From my controller i have:\n@email_messages = EmailMessage.search(params[:simple_search][:value]).page(params[:page])\nWhen I get rid of the page method, then i run into a division by 0 error which probably has something to do with the page being 0?\nRendered simple_searches/index.html.erb within layouts/application (1.4ms)\nCompleted 500 Internal Server Error in 353ms\nActionView::Template::Error (divided by 0):\n4: <% end %>\n5: \n6: <% if @email_messages.present? %>\n7:     <%= paginate @email_messages %>\n8:     <div id=\"products\">\n9:       <%= render @email_messages %>\n10:     </div>\n  app/views/simple_searches/index.html.erb:7:in `_app_views_simple_searches_index_html_erb__815704194711493677_2177278500__808843748140322580\n. So you just have to pass the page in as an options hash\n@email_messages = EmailMessage.search(params[:simple_search][:value], :page=>1)\n. Hmmm.. only problem with this approach is that i am always on page 1.  Even if i change my code to this:\n@email_messages = EmailMessage.search(params[:simple_search][:value], :page=>(params[:page] || 1) )\n. Ok, i couldn't get this working so i went back to will_paginate which i now have working.\n. Hmmm.. i did that and i see this error:\nActiveRecord::UnknownAttributeError Exception: unknown attribute: subject\n. Of course.  Here is my AR object:\nhttps://gist.github.com/81acc2128cf3427acec5\nI am using activerecord-3.0.7, tire-0.1.8, and ruby-1.9.2-p180.\n. Just updated the gist to have the error too.\n. self.to_json with methods does not seem to work because to_json returns a nested hash with the object name as the key.  For example:\nself.to_json(:methods=>[:subject])\nyields\n\"{\\\"email_message\\\":{\\\"created_at\\\":\\\"2011-06-10T15:28:28Z\\\",\\\"date\\\":\\\"2010-08-03T07:13:49Z\\\",\\\"downloaded\\\":null,\\\"email_account_id\\\":1,\\\"email_file_path\\\":\\\"/gmail.com/weshopemailsync/%5BGmail%5D%2FAll+Mail/22.eml\\\",\\\"from_address\\\":\\\"somone@gmail.com\\\",\\\"id\\\":1,\\\"order\\\":false,\\\"order_total\\\":null,\\\"pop_id\\\":22,\\\"shipping\\\":false,\\\"store_id\\\":null,\\\"to_address\\\":\\\"webrat@googlegroups.com\\\",\\\"uid\\\":22,\\\"updated_at\\\":\\\"2011-06-10T15:28:28Z\\\",\\\"user_id\\\":2,\\\"subject\\\":\\\"[webrat] webrat, mechanize and http proxies\\\"}}\"\n. Ok, i think i see the issue.  Tire is calling new with the subject attribute but the subject attribute is read-only.  Is there a way to tell tire to only construct attributes with what is in the relational database?\nobject = Configuration.wrapper.new(document)\nThanks!\n. That makes sense.  \nQuestion, regarding instanciating the active record class, does that mean that elastic search stores all of the attributes as well because i am passing some pretty large attributes?  Was under the impression that it would search for the attribute and then look up the underlying hits in the database.\nIs there a way to tell it not to store the full content if the answer to above is yes?\nThanks!\n. Makes sense.  What would a mapping like that look like in tire.  Would it be in the mapping block?\nThanks again!\n. In my case, i have a mapping for an email_message and one of the fields i am indexing is the entire message body.   \n```\n  mapping do\nindexes :id, :type => 'integer'\nindexes :email_account_id, :type=>'integer'\nindexes :user_id, :type=>'integer'\nindexes :from_address, :type=>'string'\nindexes :to_address, :type=>'string'\nindexes :subject, :type=>'string'\nindexes :body, :type=>'string', :analyzer => 'snowball'\nindexes :order, :type=>'boolean'\nindexes :commercial_type, :type=>'string'\nindexes :sender_type, :type=>'string'\nindexes :order_total, :type=>'double'\nindexes :unique_id, :type=>'string'\nindexes :date, :type=>'date'\nindexes :created_at, :type=>'date'\nindexes :store_domain, :type=>'string'\n\n```\nend\nI store the full message on s3 with the body so i don't want to store it again within the elastic search index as it's wasteful.  I don't care too much about the other fields as they are small but if they weren't stored in elasticsearch it would not bother me either because they can all be derived from the full message body which i have on s3 or the ids are stored within mysql.  \nIs there a way to disable storage within elastic search on a per field basis or are store properties configured on the index level?  Also, i don't see store properties within the Readme.. am i missing it again?\nThanks again for all of your help!\n. Would something like this work within an initializer?\nindex_hash = EmailMessage.mapping_to_hash\nindex_hash[:email_message]['_source']={:enabled=>false}\nEmailMessage.elasticsearch_index.create :mappings => index_hash\nIs there any danger of corrupting an existing index with the code above?\nThanks!\n. Hi.  I fixed that... but i noticed that you don't need results anymore.  So the example should be:\ns.facets['commercial_type']['terms'].each do |f|\n   puts \"#{f['term'].ljust(10)} #{f['count']}\"\nend\nThe above works.  I think the documentation needs some updating?\nThanks for your help!\n. Strange though.. when i looked at the code, it makes sense that you would not need results.  The search method returns Results::Collection.\n```\ndef search(indices, options={}, &block)\n  if block_given?\n    Search::Search.new(indices, options, &block).perform\n  else\n    payload = case options\n      when Hash    then options.to_json\n      when String  then options\n      else raise ArgumentError, \"Please pass a Ruby Hash or String with JSON\"\n    end\nresponse = Configuration.client.post( \"#{Configuration.url}/#{indices}/_search\", payload)\njson     = MultiJson.decode(response.body)\nresults  = Results::Collection.new(json, options)\n\nend\n```\nResults::Collection has a facets attr_reader\n```\nmodule Tire\n  module Results\nclass Collection\n    include Enumerable\n    include Pagination\nattr_reader :time, :total, :options, :results, :facets\n\n```\n. Not sure what you mean... the search method returns Results::Collection and there is a method on their facets. \nResults::Collection#facets\nWhy would we need to call Results::Collection#results#facets?\nthx\n. Cool.. just as long as i am not going crazy.  Thanks!\n. I was using will_paginate (~> 3.0.pre2).  I am giving Kaminari a shot.  I saw that patch from the other issue.\nhttps://github.com/karmi/tire/issues/14\n. Got it working now.  Was not passing in the page parameter to the search.\n@email_messages = EmailMessage.search(params[:simple_search][:value], :page=>(params[:page] || 1) )\n. Actually, it does not fail anymore, but when i click next or a specific page number, it does not go to the next page.\n. Ok.. another update.  If you don't pass in :per_page, it ignores pagination all together.  So you need:\n@email_messages = EmailMessage.search(params[:simple_search][:value], :page=>(params[:page] || 1), :per_page=>10 )\n. Ok.  Thx!\n. ",
    "ramontayag": "Yeah this will be cool. I'll use will_paginate for now :)\n. @karmi, you're right. I'm working with @corroded on this and it seems that some of the string in our data has some bytecode (?) / bytedata characters. I don't really know anything about this so I'll have to research more. I guess that's why we see the code 64 in the error. We'll send updates!\n. I can't seem to reproduce it. Importing is working fine now. It shames me to say this, but the problem may have been us not restarting the rails console (I had moved to a new workstation without encoding specified in the database.yml. I changed it, called reload! in the console, but apparently, reload! wasn't enough) after setting the encoding in database.yml. Because after I restarted the console for other reasons, and started importing, things worked. Because of this, this issue is invalid.\nThanks @karmi! You've been a great help!\n. @karmi, I edited my elastic search config to match yours (but I'm not sure I did it right. I read their config tutorial and guessed the path for number_of_shards and number_of_replicas based on another tutorial on the elastic search site. I hope it's right! Anyway, I restarted elastic search and then generated a new app with your template. I see:\n```\n[IMPORT] Starting import for the 'Article' class\n\n5/5 | 100% [ERROR] 500 > {\"error\":\"MasterNotDiscoveredException[]\",\"status\":500}, retrying (1)...\n[ERROR] 500 > {\"error\":\"MasterNotDiscoveredException[]\",\"status\":500}, retrying (2)...\n[ERROR] 500 > {\"error\":\"MasterNotDiscoveredException[]\",\"status\":500}, retrying (3)...\n[ERROR] 500 > {\"error\":\"MasterNotDiscoveredException[]\",\"status\":500}, retrying (4)...\n[ERROR] 500 > {\"error\":\"MasterNotDiscoveredException[]\",\"status\":500}, retrying (5)...\n[ERROR] Too many exceptions occured, giving up. The HTTP response was: 500 > {\"error\":\"MasterNotDiscoveredException[]\",\"status\":500}\n2011-09-22 12:02:47:L [BULK] (\"articles\")\n\ncurl -X POST \"http://localhost:9200/_bulk\" -d '{... data omitted ...}'\n2011-09-22 12:02:47:L [500]\n```\n. @karmi, with some help we found the problem! We were all using these settings:\ncluster:\n  name: mtraks2boracay\nFor some reason I assumed that to connect to a cluster I had to do more configuration -- but I was wrong! So it was trying to connect to the other computers already running elastic search. To fix it, we changed the name of the cluster.\n. Gosh - sorry I missed this one too.\n. Hey @karmi,\nI upgraded to 0.3.8 but I still get an error like this:\n```\n\nAlbum.search 'hey!'\n[REQUEST FAILED] curl -X GET \"http://localhost:9200/development/album/_search?pretty=true\" -d '{\"query\":{\"query_string\":{\"query\":\"hey!\"}}}'\nTire::Search::SearchRequestFailed: Tire::Search::SearchRequestFailed\n    from /Users/dev/.rvm/gems/ree-1.8.7-2011.03@mtrax2/gems/tire-0.3.8/lib/tire/search.rb:74:in perform'\n    from /Users/dev/.rvm/gems/ree-1.8.7-2011.03@mtrax2/gems/tire-0.3.8/lib/tire/model/search.rb:97:insearch'\n    from /Users/dev/.rvm/gems/ree-1.8.7-2011.03@mtrax2/gems/tire-0.3.8/lib/tire/model/search.rb:264:in __send__'\n    from /Users/dev/.rvm/gems/ree-1.8.7-2011.03@mtrax2/gems/tire-0.3.8/lib/tire/model/search.rb:264:insearch'\n    from (irb):1\n``\n. I upgraded to0.3.12` and get this error when searching for 'hey!'\n\n500 : {\"error\":\"SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[8nXMyBO_QQ-umfZPyRgnrg][development][3]: SearchParseException[[development][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\\"query\\\":{\\\"query_string\\\":{\\\"query\\\":\\\"hey!\\\"}}}]]]; nested: QueryParsingException[[development] Failed to parse query [hey!]]; nested: ParseException[Cannot parse 'hey!': Encountered \\\"<EOF>\\\" at line 1, column 4.\\nWas expecting one of:\\n    \\\"(\\\" ...\\n    \\\"*\\\" ...\\n    <QUOTED> ...\\n    <TERM> ...\\n    <PREFIXTERM> ...\\n    <WILDTERM> ...\\n    \\\"[\\\" ...\\n    \\\"{\\\" ...\\n    <NUMBER> ...\\n    <TERM> ...\\n    \\\"*\\\" ...\\n    ]; nested: ParseException[Encountered \\\"<EOF>\\\" at line 1, column 4.\\nWas expecting one of:\\n    \\\"(\\\" ...\\n    \\\"*\\\" ...\\n    <QUOTED> ...\\n    <TERM> ...\\n    <PREFIXTERM> ...\\n    <WILDTERM> ...\\n    \\\"[\\\" ...\\n    \\\"{\\\" ...\\n    <NUMBER> ...\\n    <TERM> ...\\n    \\\"*\\\" ...\\n    ]; }{[8nXMyBO_QQ-umfZPyRgnrg][development][1]: SearchParseException[[development][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\\"query\\\":{\\\"query_string\\\":{\\\"query\\\":\\\"hey!\\\"}}}]]]; nested: QueryParsingException[[development] Failed to parse query [hey!]]; nested: ParseException[Cannot parse 'hey!': Encountered \\\"<EOF>\\\" at line 1, column 4.\\nWas expecting one of:\\n    \\\"(\\\" ...\\n    \\\"*\\\" ...\\n    <QUOTED> ...\\n    <TERM> ...\\n    <PREFIXTERM> ...\\n    <WILDTERM> ...\\n    \\\"[\\\" ...\\n    \\\"{\\\" ...\\n    <NUMBER> ...\\n    <TERM> ...\\n    \\\"*\\\" ...\\n    ]; nested: ParseException[Encountered \\\"<EOF>\\\" at line 1, column 4.\\nWas expecting one of:\\n    \\\"(\\\" ...\\n    \\\"*\\\" ...\\n    <QUOTED> ...\\n    <TERM> ...\\n    <PREFIXTERM> ...\\n    <WILDTERM> ...\\n    \\\"[\\\" ...\\n    \\\"{\\\" ...\\n    <NUMBER> ...\\n    <TERM> ...\\n    \\\"*\\\" ...\\n    ]; }]\",\"status\":500}\n. ",
    "mrjjwright": "Nevermind, I decided to just search ElasticSearch directly with the other Tire methods.\n. Yes, and thanks for the followup.  I went ahead and went with writing my own hooks using net/http to communicate with ElasticSearch since it has such a well defined REST API already but thanks for Tire.\n. ",
    "djfobbz": "\nStarted POST \"/users/4dca95338b53226b75000001/plans\" for 75.58.68.47 at 2011-05-11 16:12:53 -0400\nError occurred while parsing request parameters.\nContents:\nYajl::ParseError (lexical error: invalid char in json text.\n                                                    { plan : { intent : 1, privacy : 1, body : 'testing' } }\n\n. I actually get that error while trying to submit data via CURL command, this is what I send to server when I get the error. How do double quotes play along with JSON & CURL?\n\ncurl -v -H 'Content-Type: application/json' -H 'Accept: application/json' -X POST http://staging.testingserver.com/users/4d6779338b53226d2c000001/plans -d \"{ plan : { intent : 1, privacy : 1, body : 'testing' } }\" -u username:password\n. ",
    "willoughby": "This may not be a bug but a feature request to enable anding of filters, I will take a look at your code in depth\nThanks\n. Hi Karel\nI have push this as an example https://willoughby@github.com/willoughby/facet_example.git\nWhat I am trying to do is,\nFrom a initial query of title:*\nShow all available facets \nOn selection of a facet (tag:ruby) show the relevant results and facets\nSelect a different facet (category:one) show results based on tag:ruby AND category:one\nThe call below does this but can do Tire do this?\nRegards \nNath\njavascript\n{\"query\":\n    {\"query_string\":{\"query\":\"title:*\"}},\n    \"facets\" : {\n         \"tags\" : {\n              \"terms\" : {\"field\" : \"tags\"},\n               \"facet_filter\" : {\n                     \"and\" : [\n                         {\"term\" : {\"tags\" : \"ruby\"}},\n                         {\"term\" : {\"category\" : \"one\"}}\n                      ]\n               }\n          },\n          \"category\":{\"terms\":{\"field\":\"category\"},\n                \"facet_filter\" : {\n                    \"and\" : [\n                         {\"term\" : {\"tags\" : \"ruby\"}},\n                         {\"term\" : {\"category\" : \"one\"}}\n                      ]\n                  }\n            },\n           \"price\":{\"terms\":{\"field\":\"price\"},\n                \"facet_filter\" : {\n                      \"and\" : [\n                         {\"term\" : {\"tags\" : \"ruby\"}},\n                         {\"term\" : {\"category\" : \"one\"}}\n                      ]\n                  }\n            }\n    },\"filter\":{\"terms\":{\"tags\":[\"ruby\"]}},\n      \"filter\":{\"terms\":{\"category\":[\"one\"]}}\n}\n. Hey ejlevin1\nThis might help solve your problem \n``` ruby\ns = Tire.search \"products_#{current_site}\" do\n      query do\n        boolean do\n          must { string query}\n          must {term :retailer_id, params[:retailer_id]  } if params[:retailer_id].present?\n          must {term :price, params[:price] } if params[:price].present?\n        end\n  end\n   facet \"retailer_id\" do\n      terms :retailer_id, :size=> 1000\n   end  \n   facet \"price\" do\n      terms :price\n   end\n size(20)\n from(params[:page] || 0)\nend\n\n```\n. ",
    "ejlevin1": "I was trying to do the same thing that willoughby was and was unable to accomplish this because using the above example, the :facet_filter would be scoped within the :terms instead of under the facet itself.  I am new to Tire/Elasticsearch so I may be doing something wrong, but here is what I was able to come up with:\nMonkeypatch for Tire::Search::Facet:\n```\n  def filter(type, options)\n    @filters ||= []\n    @filters << Filter.new(type, options).to_hash\n    self\n  end\ndef to_hash\n    @value.update @options\n    @value.update( { :facet_filter => @filters.first.to_hash } ) if @filters && @filters.size == 1\n    @value.update( { :facet_filter => { :and => @filters.map { |filter| filter.to_hash } } } ) if  @filters && @filters.size > 1\n    { @name => @value }\n  end\n```\nthen I could use it just like the search filter:\nfilters = []\nfilters << { :upccode => params[:upccode] } if params[:upccode]\n@search = Tire.search( 'product' ) do |s|\n      s.query do |q|\n        q.string params[:keyword] if params[:keyword]\n      end \n```\n  #Specify the filters on this search\n  filters.each { |filter| s.filter :term, filter }\n[:product_line_id].each do |f|\n    s.facet(f.to_s) do |facet|\n      facet.terms f\n      facet.filter :term, { :upccode => ... }\n    end\n  end\nend\n```\n. ",
    "agungyuliaji": "```\ndef self.power_facets(domain_id, anchor_words, tag)\n    tire.search do \n      query do\n        boolean do\n          must { string \"domain_id:#{domain_id}\" }\n        end\n      end\n  facet \"market_ranks\" do\n    terms :market_rank, :facet_filter => { :and => [ {:term => { :tags => tag }}, :term => { :anchor_words => anchor_words }  ] } \n  end\n\nend\n\nend\n```\ncode above is does not work\ni want to use filter facets, http://www.elasticsearch.org/guide/reference/api/search/facets/filter-facet.html\nhow can I do that with tire?\n. thanks! :)\nbut the results were not as I wanted\n{\"market_ranks\"=>{\"_type\"=>\"filter\", \"count\"=>198}}\ni want like this :\n{\"market_ranks\"=>{\"_type\"=>\"terms\", \"missing\"=>0, \"total\"=>198, \"other\"=>0, \"terms\"=>[{\"term\"=>2, \"count\"=>134}, {\"term\"=>1, \"count\"=>38}, {\"term\"=>3, \"count\"=>10}, {\"term\"=>0, \"count\"=>10}, {\"term\"=>6, \"count\"=>2}, {\"term\"=>5, \"count\"=>2}, {\"term\"=>4, \"count\"=>2}]}}\nnow I have new a code :\n```\n  def self.power_facets(domain_id, anchor_words, tag)\n    tire.search do \n      query do\n        boolean do\n          must { string \"domain_id:#{domain_id}\" }\n        end\n      end\n  facet \"market_ranks\" do\n    terms :market_rank\n    facet_filter :term, tags: tag \n    facet_filter :term, anchor_words: anchor_words\n  end\n\nend\n\nend\n```\nBut it seems to apply only the last filter. But I need both of them.\n. ",
    "seejohnrun": "Cool - I was going to make that move but was assuming you had excluded the rest of the operators for a reason.\nThanks for the gem, keep up the great work\n. Yeah its definitely better for the gem too, especially as ES expands their API\n. ",
    "bcoe": "The yajl-ruby library was causing major issues in my Rails deployment. The main problem being that the :methods key appears to not be supported in their JSON extension. I modified Tire so that it defaults to the 'json' gem by default, but can still use yajl if available (and working). I also found one other bug in my production deployment related to arrays having a 'to_hash' method punched onto them, I submitted a patch for this -- Tire is working great for me now in production.\n. Cool, yajl was causing issues for me :) \nI'm not quite sure what is punching to_hash onto an Array. But, I think relying on that method name to determine if something is a hash is a bit cavalier. It's just too loaded, i.e., it might refer to just creating an MD5 hash out of a non-hashmap object.\nThanks for the updates :)\n. Thanks :) very much appreciated,\n. It fails for me in my production Rails 3 environment with an error like this: (1 argument provided 0 expected). This exception is raised in all the the Tire classes with the to_json method. Not quite sure which library is causing this exception, my guess would be it's activesupport (3.0.7) -- simply accepting a parameter and throwing it away allows me to run example/dsl.rb in production.\n. [REQUEST FAILED] ArgumentError \nArgumentError: wrong number of arguments (1 for 0)\n    from /usr/lib/ruby/gems/1.8/gems/tire-0.1.6/lib/tire/search.rb:94:in to_json'\n    from /usr/lib/ruby/gems/1.8/gems/tire-0.1.6/lib/tire/search.rb:94:into_json'\n    from /usr/lib/ruby/gems/1.8/gems/tire-0.1.6/lib/tire/search.rb:81:in to_curl'\n    from /usr/lib/ruby/gems/1.8/gems/tire-0.1.6/lib/tire/search.rb:74:inperform'\n    from /usr/lib/ruby/gems/1.8/gems/tire-0.1.6/lib/tire/dsl.rb:10:in `search'\n    from (irb):10\n    from /usr/local/lib/site_ruby/1.8/rubygems/exceptions.rb:19\nThere you go :) accepting the parameters and throwing them out has been tested for me and fixes this issue.\n. Hmm, I think it's actually the json (1.5.1) gem that expects the generator -- try against this in irb. Again, accepting the optional parameter solves this incompatibility issue.\nAny ways, time to go start work on my own JSON library -- that will solve the problem!\n. Try the 'json' dependency without requiring 'rails/all'. The only dependencies I'm including in irb are:\nrequire 'rubygems'\nrequire 'json'\nrequire 'tire'\n. Lol, with regards to my blog post, I hope there's no hard feelings -- really wasn't trying to go after your library :) I'm going to keep using/patching it.\n. My branch has diverged fairly far from master, don't see a point to keeping this open.\n. Query:\ncurl -X POST \"http://localhost:9200/5336bfb50322c2285eff3afb07576452-attachments-index/_search?pretty=true\" -d '{\n      \"size\":5,\n      \"from\":0,\n      \"fields\":\n      [\n          \"uuid\",\n          \"hash\",\n          \"file_extension\",\n          \"file_size\",\n          \"filename\",\n          \"type\",\n          \"external_id\",\n          \"downloadable\",\n          \"date\",\n          \"sender\",\n          \"visible\",\n          \"thumbnail_created\",\n          \"shared\",\n          \"large_thumbnail_created\",\n          \"share_url\",\n          \"meta\",\n          \"_source.meta\"\n      ],\n      \"sort\":\n      [\n          \"_score\"\n      ],\n      \"query\":\n      {\n          \"bool\":\n          {\n              \"must\":\n              [\n                  {\n                      \"query_string\":\n                      {\n                          \"query\":\"*\"\n                      }\n                  },\n                  {\n                      \"term\":\n                      {\n                          \"user_uuid\":\"5336bfb50322c2285eff3afb07576452\"\n                      }\n                  }\n              ]\n          }\n      }\n  }'\nResponse:\n{\n  \"took\" : 2,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 2,\n    \"successful\" : 2,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 83,\n    \"max_score\" : 1.3976741,\n    \"hits\" : [ {\n      \"_index\" : \"5336bfb50322c2285eff3afb07576452-attachments-index\",\n      \"_type\" : \"attachment\",\n      \"_id\" : \"f7bcf93ae94365d97f974a3b6d7a1ab6\",\n      \"_score\" : 1.3976741,\n      \"fields\" : {\n        \"sender\" : \"bencoe@gmail.com\",\n        \"_source.meta\" : {\n          \"image_width\" : 200,\n          \"image_height\" : 200\n        },\n        \"file_size\" : 12090,\n        \"thumbnail_created\" : \"true\",\n        \"hash\" : \"2ff70f3bc51d1af4b47439edc3f53b77\",\n        \"file_extension\" : \"png\",\n        \"filename\" : \"attachmentsme-200x200.png\",\n        \"uuid\" : \"f7bcf93ae94365d97f974a3b6d7a1ab6\",\n        \"type\" : \"image\",\n        \"date\" : 1299097315,\n        \"downloadable\" : \"true\"\n      }\n    }\nThe code:\n1. Examines the final document for keys in the form '_source.foo' (these keys currently cause exceptions when they are mapped onto an object)\n2. Recursively coerces these fields into the proper form, e.g., '_source.foo' would end up being the key 'foo' when mapped onto the final object. '_source.foo.bar', would end up being the key 'foo.bar' on the final object.\nIn the example code I've posted note the key '_source.meta' this is also provided in the initial query.\n. I agree it seems like weird behaviour with how ES (or maybe Lucene?) is handling object fields behind the scenes. The only way I could figure out to cherry-pick the 'meta' field was using '_source.meta', which required this patch.\nI'm going to dig into the ES code a little bit, maybe this will give me an excuse to submit my first patch to that project -- I'll keep you posted.\n. Cool, thanks for the clarification, I'm glad it wasn't just weird behaviour that I was noticing.\n. Yeah, that's what my patch is attempting to do ;)\nWith unit tests even.\n. We already have an existing ActiveRecord class, which we have Tire to map the results onto (this class actually uses the MongoMapper mixin as well). I'd prefer to use Tire's mixins for search, mainly because I'm just rebuilding them myself otherwise (and they work great minus the index name).\n. Yeah, I understand it's potentially a bit ugly. We're bumping into a weird use-case (multiple indices on one model) that your wrapper for ActiveRecord is close to working for, it's not the end of the world if we have to go with our own solution.\n. We have a multi-tenant application where a tenant might be in one of a number of indices (it's not two indices being punched onto the same model).\nSounds like we're on the same page though :)\n. ",
    "znbailey": "I just spun up a new rails project using tire and I'm running into this issue, though the error is slightly different:\nwrong number of arguments (2 for 1)\nstack is:\nactivesupport (3.1.2) lib/active_support/core_ext/object/to_json.rb:19:in to_json'\ntire (0.3.11) lib/tire/index.rb:296:into_json'\ntire (0.3.11) lib/tire/index.rb:296:in convert_document_to_json'\ntire (0.3.11) lib/tire/index.rb:54:instore'\ntire (0.3.11) lib/tire/index.rb:8:in instance_eval'\ntire (0.3.11) lib/tire/index.rb:8:ininitialize'\nWhat's the recommended approach to getting everything playing nice together? This project literally has one controller file in it and that's it (no complicated dependencies). Running ruby 1.9.2 and the only gems in my gemfile are rails, tire, and sqlite3\nThanks!\n. Looks like adding a dependency on yajl-ruby in my Gemfile and then adding:\nrequire 'yajl/json_gem'\nbefore\nrequire 'rails/all' \nin my application.rb fixed things up. Not entirely sure why as it seems like multi_json should be falling back to the standard json gem?\n. ",
    "crx": "The rake dependency in the Gemfile was updated @ cde9d514e825cb8e55e49ce687bad64b6b477dce. This hasn't yet made it to the Tire gem but should be there in the next release. For now, you can require edge-Tire in your Gemfile:\ngem 'tire', :git => 'git://github.com/karmi/tire.git'\n. ",
    "vhyza": "@kimchy here is gist with example: https://gist.github.com/1018584. I tried it on ES 0.16.1 and on master with last commit elasticsearch/elasticsearch@6382ddf43cea9a6a88a2\n. Hello,\nhere is example how can be multifield type used:\n``` ruby\nrequire 'rubygems'\nrequire 'tire'\nTire.configure { logger STDERR }\nTire.index(\"causes\").delete\nclass Cause\n  # Only for this test\n  include Tire::Model::Persistence\ninclude Tire::Model::Search\nmapping do\n    indexes :user, :type => 'multi_field', :fields => {\n      :user => { :type => \"string\", :analyzer => \"snowball\" },\n      :\"user.exact\" => { :type => \"string\", :index => :not_analyzed }\n    }\n    indexes :civil do\n      indexes :rol, :type => 'string'\n      indexes :tribunal,  :type => 'string', :boost => 100\n    end\n  end\n# Only for this test\n  property :user\n  property :civil\nend\nCause.create :user => \"first name\"\nCause.create :user => \"last name\"\nCause.index.refresh\nresponse = Cause.search do\n   query  { string 'name' }\n   filter :term, :\"user.exact\" => 'last name'\n   sort   { by \"user.exact\", 'desc' }\nend\np response.results\nreturns nothing because term filter is not analyzed\nresponse = Cause.search do\n   query  { string 'name' }\n   filter :term, :user => 'last name'\n   sort   { by \"user.exact\", 'desc' }\nend\np response.results\ncan't sort by analyzed property\nresponse = Cause.search do\n   query  { string 'name' }\n   filter :term, :user => 'last name'\n   sort   { by \"user\", 'desc' }\nend\n```\n. Hello,\nI think you can't set default operator as global setting in tire. But searching by string should support to change operator from OR to AND. Also AND filter should work. See following gist for examples: https://gist.github.com/1263816\nHave a nice day\n. Hello @sakrafd. I'd like to ask how many documents are you importing using bulk. I was using _bulk with about 1000 documents per request and I never noticed timeout. I had similar problems when ES was running on FreeBSD. Is there any chance to run import suite on different environment? Or try it with less documents per request. I'm just curious because I've never noticed this behaviour on OS X or linux.\n. Hello,\ncan you check what is in deals index using curl -XGET 'http://localhost:9200/deals/_search?q=*' after import is finished?\nCan you gist Deal model please? Its hard to recreate the issue without it.\n. @ellmo: Thanks for info. I think there is problem with _type you have in index. It should be deal not document. Tire is looking only for documents with _type deal. For example, search url generated by Tire looks like this http://localhost:9200/deals/deal/_search?pretty=true\nSo I think there was some problem with document import. Its weird because I tried import now and everything works fine. Are you using last version of Tire?\n. Did you check http://localhost:9200/deals/_search?q=* after new import? I think you still have document _type instead deal\nPlease, can you try following?\n1. curl -X DELETE http://localhost:9200/deals\n2. replace Deal model with basic one (https://gist.github.com/1299758)\n3. rake environment tire:import CLASS=\"Deal\"\n4. see http://localhost:9200/deals/_search?q=* to check if _type is deal now\nIf _type is correct Deal.search \"*\" should returns some hits\n. Hello,\nyou can try to take a look if mapping is correctly created http://localhost:9200/INDEX_NAME/_mapping.\nPST_nuns is unique id? I'm asking because keyword analyzer create only one token into index. Whole id string. (according http://www.elasticsearch.org/guide/reference/index-modules/analysis/keyword-analyzer.html) \nSo if you have for example PST_nuns_and_some_prefix id then searching for PST_nuns won't work, because in index is whole PST_nuns_and_some_prefix.\n. Hello,\nit is weird. Do you have the same problem with HTTP::Client::RestClient?\n. Hello,\nI'm afraid indices_boost option is not supported yet.\n. Hello,\nI'm afraid in last version of Tire gem is not included commit db6e911e6cef1cbd10d233e374575a839efe0ca7 which allows to define default property for Tire::Model::Persistence. This https://gist.github.com/1759827 is working for me with master. So after @karmi release new version of gem it should be working properly.\n. Hello,\nI think better option is to implement Facet#statistical method to keep it consistent with another types of facets.\n. Hello,\nI tried to rewrite your gist to executable ruby script and it seems its working ok https://gist.github.com/1751827\nPlease can you post your mapping? curl \"http://localhost:9200/users/_mapping?pretty=true\" and some documents? curl \"http://localhost:9200/users/_search?q=*&size=5&pretty=1\"\n. Your mapping seems ok. I think the problem is that this syntax:\nsearch = Tire::Search::Search.new('users')\nsearch.query { string('names:' + me.name) } unless name.empty?\nsearch.filter :geo_distance, distance: distance_to_search, 'addresses.lat_lon' => lat_lon if search_distance?\nreturn search\nis available after commit 91961416f25b63d6641cadada71d469455ba4d90 which is not included in last Tire gem. You have to switch to block-style DSL. For example:\n```\nsearch = Tire.search 'users_tire_issue' do |search|\n   search.query do |query|\n     query.string \"names: #{me.name}\" unless me.name.empty?\n   end\nsearch.filter :geo_distance, distance: \"10mi\", 'addresses.lat_lon' => me.lat_lon\nend\nsearch\n```\nuntil @karmi release new version of Tire\n. Hello,\nI think is possible to load record when iterating\n``` bash\n\n\narticles = Article.search(\"lorem\")\n\n\n2012-02-07 14:16:54:372 [_search] ([\"articles\"])\n\ncurl -X GET \"http://localhost:9200/articles/article/_search?pretty=true\" -d '{\"query\":{\"query_string\":{\"query\":\"lorem\"}}}'\n2012-02-07 14:16:54:372 [200] (3 msec)\n=> #3, \"timed_out\"=>false, \"_shards\"=>{\"total\"=>5, \"successful\"=>5, \"failed\"=>0}, \"hits\"=>{\"total\"=>1, \"max_score\"=>0.040683318, \"hits\"=>[{\"_index\"=>\"articles\", \"_type\"=>\"article\", \"_id\"=>\"6\", \"_score\"=>0.040683318, \"_source\"=>{\"content\"=>\"Lorem ipsum dolor sit amet.\", \"created_at\"=>\"2012-02-07T13:05:24Z\", \"published_on\"=>\"2012-02-07T13:05:24Z\", \"title\"=>\"One\", \"updated_at\"=>\"2012-02-07T13:05:24Z\"}}]}}, @options={}, @time=3, @total=1, @facets=nil, @wrapper=Tire::Results::Item>\n\n\narticles.each do |article|\n?>    p article\n   p article.load\nend\n\n  Article Load (0.2ms)  SELECT \"articles\".* FROM \"articles\" WHERE \"articles\".\"id\" = ? LIMIT 1  [[\"id\", \"6\"]]\n\n``\n. I understand, but unfortunately I don't know about another option provided byTire`. You can still load it manually\n\n\nExtracted from Tire:\n``` ruby\narticle_items = Article.search(\"*\")\nids = article_items.map(&:id)\narticles = Article.find(ids)\nReorder records to preserve order from search results\nids.map { |id| articles.detect { |article| article.id.to_s == id.to_s } }\n```\n. Hello,\nplease can you gist your models and some example documents?\n. Hello,\nI'm not sure if I understand correctly. Both Article and SubArticle inherits from ActiveRecord::Base so mapping should not be overwritten. So SubArticle indexes only id and body. You can check your mapping in elasticsearch on URLs:\nhttp://localhost:9200/articles/_mapping?pretty=true\nand\nhttp://localhost:9200/sub_articles/_mapping?pretty=true\n. Ok then. According mapping its not overwritten/appended\n. Hello,\nI think * aren't necessary. Try something like:\nruby\nTire.search 'documents' do\n  query do\n    boolean(:minimum_number_should_match => 1) do\n      should { string \"title:Christopher\" }\n      should { string \"content:Christopher\" }\n    end\n  end\nend\n. @behrangsa please close the issue if it's solved.\n. Hello,\nthere is scope problem in add_object_to_index. This is working for me: https://gist.github.com/1767008\n. I'm glad its working.\nTo be honest, I'm not sure if I understand this scope problem correctly, but I think inside the Tire.index 'objects' do block should be accessible objects from add_object_to_index method, so this should be working too.\n``` ruby\nclass SomeObserver < ActiveRecord::Observer\n  observe :article\ndef after_create(object)\n    @object = object\n    add_object_to_index\n  end\ndef add_object_to_index\n    object = @object\n    Tire.index 'objects' do\n      store :content => object.content\n    end\n  end\nend\n```\nI'm sorry, but I don't know what you mean by subtle interactions of gems and engines with the main rails app\n. Please, could you check it on the current master?\nHere is test with or filter: https://github.com/karmi/tire/blob/master/test/integration/filters_test.rb#L22-33\n. Hello, please can you gist some data set and query example?\n. Hello,\nyou can try curb client using\n``` ruby\nrequire 'tire'\nrequire 'tire/http/clients/curb'\nTire.configure { client Tire::HTTP::Client::Curb }\n```\nin fact this client is using POST request instead GET (https://github.com/karmi/tire/blob/master/lib/tire/http/clients/curb.rb#L18-20)\n. Hello,\nI don't think there is some way to get a list of indexes by Tire. You can use http://localhost:9200/_aliases endpoint. For example:\n``` ruby\nrequire 'tire'\nrequire 'yajl'\np Yajl::Parser.parse(Tire::Configuration.client.get(\"#{Tire::Configuration.url}/_aliases\").body).keys\n```\n. Hello,\nIt seems that object type should have explicitly defined type of each property.\nYour mapping definition is not correct for ElasticSearch. When I tried to post generated curl command, ElasticSearch responded with 400 code and error nested: MapperParsingException[No type specified for property [id]]\nSo in this case mapping is created dynamically (when first document is saved), not by Tire definition. That's why boost attribute disappears.\nFollowing code creates mapping correctly:\n``` ruby\nrequire 'tire'\nrequire 'active_record'\nrequire 'sqlite3'\nActiveRecord::Base.establish_connection( :adapter => 'sqlite3', :database => \":memory:\" )\nTire.configure { logger STDOUT }\nclass ActiveRecordModel < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\ntire do\n    mapping do\n      indexes :id, :index => :not_analyzed\n      indexes :name, :boost => 10\n      indexes :user_id\n      indexes :description\n      indexes :updated_at, :index => :not_analyzed\n      indexes :folders, :type => 'object',\n              :properties => {\n                :id    => { :index => :not_analyzed, :type => \"string\" },\n                :title => {\n                  :type => \"multi_field\",\n                  :fields => {\n                    :raw   => { :type => 'string', :index => 'not_analyzed', :boost => 20 },\n                    :title => { :type => 'string'                          }\n                  }\n                },\n                :description => { :type => \"string\", :index => :not_analyzed },\n                :user_id     => { :index => :not_analyzed, :type => \"string\" }\n              }\n    end\n  end\nend\nActiveRecordModel.index.delete\nActiveRecordModel.create_elasticsearch_index\nputs ActiveRecordModel.index.mapping.inspect\n```\nHope this helps...\n. Hello,\nplease can you gist your index mapping?\n. Hello,\nI'm not really sure where the problem is...I tried to replicate it from your snippet and following code works for me.\n``` ruby\nrequire 'tire'\nrequire 'sqlite3'\nrequire 'active_record'\nActiveRecord::Base.establish_connection( adapter: 'sqlite3', database: \":memory:\" )\nActiveRecord::Migration.verbose = false\nActiveRecord::Schema.define(version: 1) do\n  create_table :articles do |t|\n    t.string   :title\n  end\n  create_table :comments do |t|\n    t.string     :author\n    t.text       :body\n    t.references :article\n  end\nend\nclass Comment < ActiveRecord::Base\n  belongs_to :article\nend\nclass Article < ActiveRecord::Base\n  has_many :comments\ninclude Tire::Model::Search\n  include Tire::Model::Callbacks\nmapping do\n    indexes :title, type: 'string'\n    indexes :comments do\n      indexes :body, type: 'string'\n    end\n  end\nself.include_root_in_json = false\n  def to_indexed_json\n    to_json(include: :comments)\n  end\nend\nArticle.destroy_all\nArticle.index.delete\nArticle.create_elasticsearch_index\narticle = Article.new title: 'First article'\narticle.comments.build body: 'Awesome article!'\narticle.save\narticle.index.refresh\ns = Article.search do\n  query do\n    boolean do\n      should do\n        text 'comments.body', 'awesome'\n      end\n    end\n  end\nend\np s.results\n```\n. Hello,\nI think each Item has property _score\n``` ruby\nrequire 'tire'\ns = Tire.search('mentions') do\n  query { string 'ruby' }\nend\np s.results.first._score\n``\n. Unfortunatelly not yet, I think this issue is related to #261\n. I think there is some issue in application too. Where are you passingnil` to collection?\nYour Company definition returns nil when nothing is passed to Company#jobs method\n``` ruby\ncompany = Company.new name: 'foo'\np company.jobs\nnil\n```\nYou can modify Company class to have Company#jobs method as blank array by default.\n``` ruby\nclass Company\n  include Tire::Model::Persistence\nproperty :name,         :analyzer => 'keyword'\n  property :jobs,         :class    => [Job], :default => []\nend\ncompany = Company.new name: 'foo'\np company.jobs\n[]\n```\nSo after that your Company instances should not return nil\n. ",
    "kimchy": "The way ES works in its decision to try and load fields is from source or not is only for non compound fields (like person.name), for things that point to compound fields, like person, it will _always need to go through _source, and its delicate as to see if it really needs to be loaded or not, so for those, you have to prefix it with _source.\n. Yes, since it started from the fact that you had to explicitly specify _source if you wanted to load it from source and parsing it. And its an indication that it was loaded from it.\n. Btw, if you just want facet counts, without getting hits back, make sure to use search_type set to count.\n. Lets break it down a bit (the logic), at least in a very simplistic manner (And putting 1532 elasticsearch issue aside). Tire should be initialized with a list of seed URLs and roundrobin around them. There should be an option to \"sniff\" the rest of the cluster using the nodes info API, by going to elasticsearch, getting all the nodes and using them as the urls to round robin against.\nOne simple option is to have a list of live nodes that are always round robin through the API. A scheduled background job will go over the seed nodes and replace the live nodes. When sniffing is disabled, it means simple \"ping\" (HEAD on /) on each seed URL, building the new live nodes list, and replacing it onces done. When sniff is enabled, the seed nodes should be used to issue the nodes info API, get all the nodes in the cluster, and use the http_address from them to build the list of live nodes. This will allow Tire to be aware of new nodes being added to the cluster dynamically.\nMake sense?\n. One option instead of creating a specific proxy, is running an elasticsearch instance configured with node.client set to trueon the same machine as the client, it will join the cluster, but just as a client node. Then, tire can be configured to talk to localhost:9200.\n. ",
    "ndowning": "I mainly need this fix because we've got an elasticsearch installation that allows GET requests from anywhere for querying, but restricts PUT/POST/DELETE requests to certain machines to more tightly control updating the index.\n. Cool, thanks!\n. ",
    "digitalplaywright": "Thanks! That gets me partially there. I stillneed to define a mapping definition within the index directory, so that the location field becomes a geo_point. \nI tried:\n  mapping do\n    indexes :title\n    indexes :location, :type => \"geo_point\"\n  end\nbut that doesn't seem to work. Any recommendation to how this should be done in tire?\nThis is a more lengthy description of my problem:\nhttps://gist.github.com/ec4b27109e0e17e443fd\n. This was an interesting problem since I am generating the json using to_indexed_json, but still need to explicitly specify static types for the index in the mapping (like e.g geo_point).\nThe error described above happened because a type was omitted in the mapping for the title index. All fields mentioned in the mapping needed explicit typing. Adding the type fixes the problem.\n. Thank you for the quick response. After looking at the Tire unit tests I see that it now supports geo filters, so I rewrote my queries in the native DSL like e.g:\nrecommended_events = Event.search do |search|\n    search.query do |query|\n        query.boolean do\n            must { range :start_time, {:to => DateTime.now} }\n        end\n    end\n    search.sort { by :start_time, 'asc' }\n    search.filter :geo_distance, :distance => \"12km\", :location => {\"lat\" =>37.7749295, \"lon\" => -122.4194155}\n    search.from 3\n    search.size 3\n end\nI have two questions with regards to this:\n      -  I need a mechanism to show number of 'pages' for pagination. How do I determine the total number of search results?\n      -  Does 'from' refer to the index in the search results after sorting?\n. I found the answer to the first question by looking at your code:\n   https://github.com/karmi/tire/blob/master/lib/tire/results/pagination.rb\nThe total number of elements is fortunately returned by the 'total' variable. \nWith regards to the second question it heuristically seems like the sorted results from the parameter @from@ offset in the result is returned. However, please let me know if you can confirm this as the intended semantics when an index is spread over many shards.\n. Thanks! It's good to hear that there are no known gotchas. \n. @woahdae Great work! This is a very useful feature. However, when I tested this it does not seem to support the ':as' option like e.g \n   mapping do\n     indexes :all_attendees, :as => :attendees\n   end\nEvery field that is indexed with the :as option is not persisted to the elastic search index.\nAre you able to reproduce the problem with ':as'?\n. The problem I reported seem to be present on master as well: \n':as => :function_name' does not work, but ':as => 'function_name' works.\n. @karmi This works for us in production, and it is essential for putting our Mongoid documents into an ElasticSearch index. Please let me know if there are any plans to merge this with master and if not what stands in the way of this.\nThank you.\n. @karmi Thank you for merging this and for all your hard work on Tire. \n. @konsti I have not had time to look debug Tire to figure out what is going wrong here. Have you made any progress on this issue?\n@karmi Please let us know if you have any advice on what may be the culprit code in Tire or if you have time to help debug this issue.\n. The Mongoid BSON object is serializable to json. However, to stay consistent with the json from the MongoDB core database itself during export, Mongoid to_json on the id field returns \"{\\\"$oid\\\": \\\"object-id-here\\\"}\". It might be this that confuses Tire.\n. as_json and to_json directly on the object id seems to be the problem:\n1.9.3p194 :003 > Moped::BSON::ObjectId.from_time(Time.now).to_json\n => \"{\\\"$oid\\\": \\\"50c0d6bd0000000000000000\\\"}\" \n1.9.3p194 :004 > Moped::BSON::ObjectId.from_time(Time.now).as_json\n => \"50c0d6d60000000000000000\" \n1.9.3p194 :005 >\nI am not sure why Mongoid serialize bson ids differently when calling to_json and as_json on the document. It seems like an inconsistency to me.\n. @karmi I did some digging and I am only able to reproduce the problem when 'yajl/json_gem' is required  in 'config/application.rb'. This branch demonstrates the bug: \n- https://github.com/digitalplaywright/echo/tree/tire_bug\n. require 'json/json_gem' overrides the standard Ruby json helpers with the Yajl equivalents:\nJSON.parse, JSON.generate, JSON.pretty_generate, JSON.load, JSON.dump and all of the #to_json instance method overrides for Ruby's primitive objects\nThat is probably why we are seeing this slightly different behavior.\n. @karmi Thank you for fixing the bug. Please let me know if you are interested in making a release soon with these changes. In the meantime we will use a direct gem dependency with master.\n@sideshow @karmi Btw, I would like to create a unit test independently from Tire that demonstrates exactly what is different when you require 'yajl/json_gem'. Please let me know if you have a test case that demonstrates the problem independently of Tire.\n. @karmi Perfect. Thank you!\n. This bugfix seems to fix the problem. Thanks!\n. ",
    "ghost": "At present this cannot be done\nTire.index('photos') do |index|\n    index.register_percolator_query(\"meta.username:#{self.username}\") do |query|\n      query.string \"meta.username:#{self.username}\"\n   end\n  end\n. Hi Karmi,\nthe real problem with this, is that I can't create a index document the elastic search way. The _* fields have a defined semantic, and I see no reason to duplicate these fields in the _source field by default. For the \"some cases\" Tire could give a additional posibility to duplicate these fields.\nI think, a default behaviour should be the elastic search way. Elastic search documents are well structured and every information has its own place.\nWhen I want to create a \"clean\" elastic search document, I have to get around, break the abstraction, and use Net::HTTP::Put to create such as document. This can not be.\nPlease correct me, if Tire gives a possibility to create documents this way.\nBest regards\nRobert\n. Hi Karel,\nthis is more a refinement, not a real problem with the functionality.\nWith \"the elasticsearch way\" I mean, a elasticsearch document has a couple of meta fields, like _index, _type, _id and so on. The semantic of these fields are well defined. Every redundancy of these fields in the _source field, could have a influence on the mapping definition and cause unneeded complexity.\nWhat when I read the index with a different tool than Tire? I don't need the redundancy in the _source field.\nMy wish is to have full control over the creation of documents in elasticsearch. Tire is too cool not to allow it.\nWith curl I can do that:\nbash\ncurl -XPUT http://localhost:9200/articles/article/1 -d '{\n    \"title\": \"one\"\n}'\nHere I put the index, type name and id in the URL. Elasticsearch knows, I mean the _index articles, _type article and _id 1. There is no need, to repeat this information in the _source field. And when I have to repeat this information, this is more a smell for a architectural problem of my application. This is what I mean with the elasticsearch way.\nBest regards\nRobert\n. Hi Karel,\nI'm going to solve this and submit a patch when I'm ready :)\nBest regards\nRobert\n. +1 to subscribe\n. @romanbsd I just deployed this and found that all my simple queries return the first X records in the index no matter what I searched for. Returning back to the Restclient client, restores functionality. I don't see anything obviously different about the #get request, but I'm much less familiar with why that might be. \nDid you see any of this?\n. The working client shows this for my search:\n00000000  7b 22 71 75 65 72 79 22 3a 7b 22 71 75 65 72 79 5f 73 74 72 69 6e 67 22 3a 7b 22  {\"query\":{\"query_string\":{\"\n0000001b  71 75 65 72 79 22 3a 22 67 6f 6f 67 6c 65 2a 22 7d 7d 2c 22 73 6f 72 74 22 3a 5b  query\":\"google*\"}},\"sort\":[\n00000036  7b 22 63 6f 75 6e 74 22 3a 22 64 65 73 63 22 7d 5d 2c 22 66 69 6c 74 65 72 22 3a  {\"count\":\"desc\"}],\"filter\":\n00000051  7b 22 72 61 6e 67 65 22 3a 7b 22 63 6f 75 6e 74 22 3a 7b 22 67 74 22 3a 32 7d 7d  {\"range\":{\"count\":{\"gt\":2}}\n0000006c  7d 2c 22 66 69 65 6c 64 73 22 3a 5b 22 6e 6f 72 6d 61 6c 69 7a 65 64 5f 6e 61 6d  },\"fields\":[\"normalized_nam\n00000087  65 22 5d 7d                                                                       e\"]}    \nThis branch, show nothing in the same place.\n. I looked into Faraday and it appears that if you create a connection object, the run_requeset method is triggered here: https://github.com/technoweenie/faraday/blob/master/lib/faraday/connection.rb#L159\nFor get requests, it blanks out the body, assuming it doesn't need the data, which is exactly the behavior I'm seeing.\nI'm not sure how this would actually work for you without changing Faraday. Do you get body content on a get request?\n. Ah...gotcha. Looks like our gemset was relying on Faraday 0.7.5 from the Koala gem. I removed it and update Faraday to the latest, and the query worked as expected.  Sorry for the mixup. I guess the adapter would have to be dependent on the latest though. Probably a valuable piece of info to know.\nWe can't upgrade the other gems that rely on Faraday quite yet, so I wrote this instead (https://github.com/karmi/tire/issues/351). It should work for the time being. I'll keep an eye on this one and upgrade everything else once this is merged in.\nThanks!\n. Ahh...I see what's going on here. Type is the actual :or setting. I thought :type would be the type of search like \"string\", \"terms\", ....\nWorked great. Thanks!\n. @romanbsd I added authentication to your faraday code and it worked great. Thanks!\n. Ahhhh....so does the current rest client in Tire work with the credentials in the URL?\n. Haha ok. Guess there's no reason for this then ;)\n. On further investigation, it seems that the older version (0.3.12) would do a POST request, where as, the latest, is trying to do a GET and putting the data elements into the query string, which ends up in a 413 HTTP error because there's too much. Is there a way to force it to do a POST request?\n. You're right. include_root_in_json was it. Thanks for the feedback.\n. Sidenote: I have a field named matches in my User model that's a PG array type.\n. I removed matches from my User model and attempted to index again and got an even more bizarre failure:\n``\n[07:57:51] bhilkert [~/Dropbox/code/meeteor] (pg *) $ rake environment tire:import CLASS='User' FORCE=true\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/css_parser-1.2.5/lib/css_parser.rb:8:in': iconv will be deprecated in the future, use String#encode instead.\n[IMPORT] Deleting index 'users'\n[IMPORT] Creating index 'users' with mapping:\n{\"user\":{\"properties\":{}}}\n[IMPORT] Starting import for the 'User' class\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/postgresql_adapter.rb:607: [BUG] Segmentation fault\nruby 1.9.3p194 (2012-04-20 revision 35410) [x86_64-darwin11.4.0]\n-- Control frame information -----------------------------------------------\nc:0043 p:---- s:0190 b:0190 l:000189 d:000189 CFUNC  :values\nc:0042 p:0037 s:0187 b:0187 l:000186 d:000186 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/po\nc:0041 p:0074 s:0178 b:0176 l:000153 d:000175 BLOCK  /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/po\nc:0040 p:0005 s:0172 b:0172 l:000162 d:000171 BLOCK  /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/ab\nc:0039 p:0032 s:0170 b:0170 l:000169 d:000169 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/notifications/instru\nc:0038 p:0046 s:0163 b:0163 l:000162 d:000162 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/ab\nc:0037 p:0025 s:0154 b:0154 l:000153 d:000153 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/po\nc:0036 p:0024 s:0148 b:0148 l:000147 d:000147 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/po\nc:0035 p:0033 s:0142 b:0142 l:000141 d:000141 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/ab\nc:0034 p:0060 s:0136 b:0136 l:000135 d:000135 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/ab\nc:0033 p:0040 s:0129 b:0129 l:000118 d:000128 BLOCK  /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/querying.rb:38\nc:0032 p:0190 s:0127 b:0127 l:000126 d:000126 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/explain.rb:40\nc:0031 p:0015 s:0119 b:0119 l:000118 d:000118 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/querying.rb:37\nc:0030 p:0110 s:0114 b:0114 l:000113 d:000113 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/relation.rb:171\nc:0029 p:0009 s:0108 b:0108 l:000097 d:000107 BLOCK  /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/relation.rb:160\nc:0028 p:0190 s:0106 b:0106 l:000105 d:000105 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/explain.rb:40\nc:0027 p:0011 s:0098 b:0098 l:000097 d:000097 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/relation.rb:159\nc:0026 p:0046 s:0095 b:0095 l:000094 d:000094 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/relation/finder_method\nc:0025 p:0023 s:0091 b:0091 l:000090 d:000090 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/querying.rb:5\nc:0024 p:0509 s:0086 b:0086 l:000c18 d:000085 BLOCK  /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/tire-0.4.3/lib/tire/tasks.rb:64\nc:0023 p:---- s:0074 b:0074 l:000073 d:000073 FINISH\nc:0022 p:---- s:0072 b:0072 l:000071 d:000071 CFUNC  :call\nc:0021 p:0058 s:0068 b:0068 l:000059 d:000067 BLOCK  /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task.rb:203\nc:0020 p:---- s:0065 b:0065 l:000064 d:000064 FINISH\nc:0019 p:---- s:0063 b:0063 l:000062 d:000062 CFUNC  :each\nc:0018 p:0173 s:0060 b:0060 l:000059 d:000059 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task.rb:200\nc:0017 p:0111 s:0056 b:0056 l:000050 d:000055 BLOCK  /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task.rb:158\nc:0016 p:0019 s:0054 b:0054 l:000053 d:000053 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/monitor.rb:211\nc:0015 p:0033 s:0051 b:0051 l:000050 d:000050 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task.rb:151\nc:0014 p:0048 s:0044 b:0044 l:000043 d:000043 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task.rb:144\nc:0013 p:0045 s:0039 b:0039 l:000038 d:000038 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:116\nc:0012 p:0012 s:0032 b:0032 l:000017 d:000031 BLOCK  /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:94\nc:0011 p:---- s:0029 b:0029 l:000028 d:000028 FINISH\nc:0010 p:---- s:0027 b:0027 l:000026 d:000026 CFUNC  :each\nc:0009 p:0069 s:0024 b:0024 l:000017 d:000023 BLOCK  /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:94\nc:0008 p:0009 s:0022 b:0022 l:000021 d:000021 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:133\nc:0007 p:0011 s:0018 b:0018 l:000017 d:000017 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:88\nc:0006 p:0029 s:0015 b:0015 l:000008 d:000014 BLOCK  /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:66\nc:0005 p:0009 s:0013 b:0013 l:000012 d:000012 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:133\nc:0004 p:0011 s:0009 b:0009 l:000008 d:000008 METHOD /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:63\nc:0003 p:0048 s:0006 b:0006 l:0018e8 d:001748 EVAL   /Users/bhilkert/.rbenv/versions/1.9.3-p194/bin/rake:32\nc:0002 p:---- s:0004 b:0004 l:000003 d:000003 FINISH\nc:0001 p:0000 s:0002 b:0002 l:0018e8 d:0018e8 TOP   \n-- Ruby level backtrace information ----------------------------------------\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/bin/rake:32:in <main>'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:63:inrun'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:133:in standard_exception_handling'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:66:inblock in run'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:88:in top_level'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:133:instandard_exception_handling'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:94:in block in top_level'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:94:ineach'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:94:in block (2 levels) in top_level'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb:116:ininvoke_task'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task.rb:144:in invoke'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task.rb:151:ininvoke_with_call_chain'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/monitor.rb:211:in mon_synchronize'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task.rb:158:inblock in invoke_with_call_chain'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task.rb:200:in execute'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task.rb:200:ineach'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task.rb:203:in block in execute'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task.rb:203:incall'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/tire-0.4.3/lib/tire/tasks.rb:64:in block (2 levels) in <top (required)>'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/querying.rb:5:inall'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/relation/finder_methods.rb:159:in all'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/relation.rb:159:into_a'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/explain.rb:40:in logging_query_plan'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/relation.rb:160:inblock in to_a'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/relation.rb:171:in exec_queries'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/querying.rb:37:infind_by_sql'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/explain.rb:40:in logging_query_plan'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/querying.rb:38:inblock in find_by_sql'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/abstract/query_cache.rb:63:in select_all'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/abstract/database_statements.rb:18:inselect_all'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/postgresql_adapter.rb:1246:in select'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/postgresql_adapter.rb:663:inexec_query'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/abstract_adapter.rb:275:in log'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/notifications/instrumenter.rb:20:ininstrument'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/abstract_adapter.rb:280:in block in log'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/postgresql_adapter.rb:667:inblock in exec_query'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/postgresql_adapter.rb:607:in result_as_array'\n/Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/connection_adapters/postgresql_adapter.rb:607:invalues'\n-- C level backtrace information -------------------------------------------\nSee Crash Report log file under ~/Library/Logs/CrashReporter or\n   /Library/Logs/CrashReporter, for the more detail of.\n-- Other runtime information -----------------------------------------------\n\n\nLoaded script: /Users/bhilkert/.rbenv/versions/1.9.3-p194/bin/rake\n\n\nLoaded features:\n0 enumerator.so\n1 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/enc/encdb.bundle\n2 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/enc/trans/transdb.bundle\n3 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/defaults.rb\n4 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/rbconfig.rb\n5 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/deprecate.rb\n6 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/exceptions.rb\n7 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/custom_require.rb\n8 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems.rb\n9 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/version.rb\n   10 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/etc.bundle\n   11 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/fileutils.rb\n   12 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/thread.rb\n   13 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/singleton.rb\n   14 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/monitor.rb\n   15 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/optparse.rb\n   16 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/ostruct.rb\n   17 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/ext/core.rb\n   18 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/invocation_exception_mixin.rb\n   19 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task.rb\n   20 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/early_time.rb\n   21 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/file_task.rb\n   22 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/file_creation_task.rb\n   23 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/shellwords.rb\n   24 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task_manager.rb\n   25 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/alt_system.rb\n   26 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/win32.rb\n   27 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/application.rb\n   28 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/ext/module.rb\n   29 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/ext/string.rb\n   30 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/ext/time.rb\n   31 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task_argument_error.rb\n   32 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/rule_recursion_overflow_error.rb\n   33 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/rake_module.rb\n   34 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/pseudo_status.rb\n   35 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/task_arguments.rb\n   36 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/invocation_chain.rb\n   37 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/multi_task.rb\n   38 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/file_utils.rb\n   39 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/file_utils_ext.rb\n   40 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/dsl_definition.rb\n   41 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/cloneable.rb\n   42 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/pathmap.rb\n   43 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/file_list.rb\n   44 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/default_loader.rb\n   45 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake/name_space.rb\n   46 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rake.rb\n   47 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/version.rb\n   48 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/requirement.rb\n   49 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/platform.rb\n   50 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/specification.rb\n   51 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/path_support.rb\n   52 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/dependency.rb\n   53 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/pathname.bundle\n   54 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/pathname.rb\n   55 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/config_file.rb\n   56 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/syntax_error.rb\n   57 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/psych.bundle\n   58 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/stringio.bundle\n   59 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/nodes/node.rb\n   60 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/nodes/stream.rb\n   61 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/nodes/document.rb\n   62 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/nodes/sequence.rb\n   63 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/nodes/scalar.rb\n   64 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/nodes/mapping.rb\n   65 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/nodes/alias.rb\n   66 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/nodes.rb\n   67 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/streaming.rb\n   68 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/visitors/visitor.rb\n   69 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/strscan.bundle\n   70 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/scalar_scanner.rb\n   71 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/visitors/to_ruby.rb\n   72 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/visitors/emitter.rb\n   73 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/visitors/yaml_tree.rb\n   74 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/json/ruby_events.rb\n   75 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/visitors/json_tree.rb\n   76 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/visitors/depth_first.rb\n   77 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/visitors.rb\n   78 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/handler.rb\n   79 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/tree_builder.rb\n   80 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/parser.rb\n   81 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/omap.rb\n   82 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/set.rb\n   83 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/coder.rb\n   84 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/core_ext.rb\n   85 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/date_core.bundle\n   86 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/date/format.rb\n   87 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/date.rb\n   88 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/deprecated.rb\n   89 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/stream.rb\n   90 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/json/yaml_events.rb\n   91 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/json/tree_builder.rb\n   92 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/json/stream.rb\n   93 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych/handlers/document_stream.rb\n   94 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/psych.rb\n   95 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/psych_additions.rb\n   96 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/psych_tree.rb\n   97 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/yaml.rb\n   98 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/syck_hack.rb\n   99 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/rubygems_integration.rb\n  100 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/shared_helpers.rb\n  101 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/gem_path_manipulation.rb\n  102 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/psyched_yaml.rb\n  103 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/gem_helpers.rb\n  104 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/match_platform.rb\n  105 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/rubygems_ext.rb\n  106 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/version.rb\n  107 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler.rb\n  108 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/settings.rb\n  109 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/digest.bundle\n  110 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/digest.rb\n  111 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/digest/sha1.bundle\n  112 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/set.rb\n  113 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/definition.rb\n  114 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/dependency.rb\n  115 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/dsl.rb\n  116 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/uri/common.rb\n  117 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/uri/generic.rb\n  118 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/uri/ftp.rb\n  119 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/uri/http.rb\n  120 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/uri/https.rb\n  121 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/uri/ldap.rb\n  122 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/uri/ldaps.rb\n  123 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/uri/mailto.rb\n  124 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/uri.rb\n  125 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/user_interaction.rb\n  126 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/package/f_sync_dir.rb\n  127 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/package/tar_header.rb\n  128 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/enc/iso_8859_1.bundle\n  129 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/zlib.bundle\n  130 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/package/tar_input.rb\n  131 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/package/tar_output.rb\n  132 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/package/tar_reader/entry.rb\n  133 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/package/tar_reader.rb\n  134 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/package/tar_writer.rb\n  135 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/package.rb\n  136 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/format.rb\n  137 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/ext/builder.rb\n  138 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/ext/configure_builder.rb\n  139 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/command.rb\n  140 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/ext/ext_conf_builder.rb\n  141 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/ext/rake_builder.rb\n  142 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/ext.rb\n  143 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/require_paths_builder.rb\n  144 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/installer.rb\n  145 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/remote_fetcher.rb\n  146 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/errors.rb\n  147 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/text.rb\n  148 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/spec_fetcher.rb\n  149 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/gem_installer.rb\n  150 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/source.rb\n  151 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/ruby_version.rb\n  152 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/lockfile_parser.rb\n  153 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/lazy_specification.rb\n  154 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/tsort.rb\n  155 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/forwardable.rb\n  156 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/spec_set.rb\n  157 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/environment.rb\n  158 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/runtime.rb\n  159 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/index.rb\n  160 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/remote_specification.rb\n  161 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/endpoint_specification.rb\n  162 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/dep_proxy.rb\n  163 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/source_index.rb\n  164 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bundler-1.2.0/lib/bundler/setup.rb\n  165 /Users/bhilkert/Dropbox/code/meeteor/config/boot.rb\n  166 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/openssl.bundle\n  167 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/openssl/bn.rb\n  168 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/openssl/cipher.rb\n  169 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/openssl/config.rb\n  170 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/openssl/digest.rb\n  171 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/openssl/buffering.rb\n  172 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/fcntl.bundle\n  173 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/openssl/ssl-internal.rb\n  174 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/openssl/x509-internal.rb\n  175 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/openssl.rb\n  176 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/securerandom.rb\n  177 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/inflector/inflections.rb\n  178 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/inflector/methods.rb\n  179 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/lazy_load_hooks.rb\n  180 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/dependencies/autoload.rb\n  181 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/version.rb\n  182 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support.rb\n  183 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/i18n-0.6.1/lib/i18n/version.rb\n  184 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/i18n-0.6.1/lib/i18n/exceptions.rb\n  185 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/i18n-0.6.1/lib/i18n/interpolate/ruby.rb\n  186 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/i18n-0.6.1/lib/i18n.rb\n  187 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/i18n-0.6.1/lib/i18n/config.rb\n  188 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/i18n.rb\n  189 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activemodel-3.2.6/lib/active_model/version.rb\n  190 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activemodel-3.2.6/lib/active_model.rb\n  191 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/crud.rb\n  192 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/factory_methods.rb\n  193 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/expressions.rb\n  194 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/predications.rb\n  195 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/math.rb\n  196 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/alias_predication.rb\n  197 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/order_predications.rb\n  198 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/table.rb\n  199 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/attributes/attribute.rb\n  200 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/attributes.rb\n  201 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/compatibility/wheres.rb\n  202 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/relation.rb\n  203 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/expression.rb\n  204 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/visitors/visitor.rb\n  205 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/visitors/depth_first.rb\n  206 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/x86_64-darwin11.4.0/bigdecimal.bundle\n  207 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/visitors/to_sql.rb\n  208 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/visitors/sqlite.rb\n  209 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/visitors/postgresql.rb\n  210 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/visitors/mysql.rb\n  211 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/visitors/mssql.rb\n  212 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/visitors/oracle.rb\n  213 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/visitors/join_sql.rb\n  214 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/visitors/where_sql.rb\n  215 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/visitors/order_clauses.rb\n  216 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/visitors/dot.rb\n  217 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/visitors/ibm_db.rb\n  218 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/visitors/informix.rb\n  219 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/visitors.rb\n  220 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/tree_manager.rb\n  221 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/insert_manager.rb\n  222 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/select_manager.rb\n  223 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/update_manager.rb\n  224 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/delete_manager.rb\n  225 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/node.rb\n  226 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/select_statement.rb\n  227 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/select_core.rb\n  228 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/insert_statement.rb\n  229 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/update_statement.rb\n  230 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/terminal.rb\n  231 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/true.rb\n  232 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/false.rb\n  233 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/unary.rb\n  234 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/ascending.rb\n  235 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/descending.rb\n  236 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/unqualified_column.rb\n  237 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/with.rb\n  238 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/binary.rb\n  239 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/equality.rb\n  240 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/in.rb\n  241 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/join_source.rb\n  242 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/delete_statement.rb\n  243 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/table_alias.rb\n  244 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/infix_operation.rb\n  245 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/and.rb\n  246 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/function.rb\n  247 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/count.rb\n  248 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/values.rb\n  249 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/named_function.rb\n  250 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/inner_join.rb\n  251 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/outer_join.rb\n  252 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/string_join.rb\n  253 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes/sql_literal.rb\n  254 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/nodes.rb\n  255 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/deprecated.rb\n  256 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/sql/engine.rb\n  257 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel/sql_literal.rb\n  258 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/arel-3.0.2/lib/arel.rb\n  259 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/version.rb\n  260 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/ordered_hash.rb\n  261 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/core_ext/enumerable.rb\n  262 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/notifications/fanout.rb\n  263 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/notifications.rb\n  264 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/core_ext/array/wrap.rb\n  265 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/deprecation/behaviors.rb\n  266 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/deprecation/reporting.rb\n  267 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/core_ext/module/deprecation.rb\n  268 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/core_ext/module/aliasing.rb\n  269 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/core_ext/array/extract_options.rb\n  270 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/deprecation/method_wrappers.rb\n  271 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/deprecation/proxy_wrappers.rb\n  272 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/deprecation.rb\n  273 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/concern.rb\n  274 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/core_ext/hash/keys.rb\n  275 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/core_ext/kernel/singleton_class.rb\n  276 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/core_ext/module/remove_method.rb\n  277 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/core_ext/class/attribute.rb\n  278 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activemodel-3.2.6/lib/active_model/attribute_methods.rb\n  279 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activerecord-3.2.6/lib/active_record/attribute_methods.rb\n  280 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/bigdecimal/util.rb\n  281 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/benchmark.rb\n  282 /Users/bhilkert/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/activesupport-3.2.6/lib/active_support/core_ext/benchmark.rb\n  283 /Users/bhilkert/.r\n```\n. By just using:\n\n\nUser.index.import User.limit(20)\nThis works without the matches field. Is that what would cause the error?\n. @karmi I didn't try it again b/c that field ended up being less important in the app. However, it was an active record field, where, in this case, would tire need to be initialized in a Rails app?\n. Gotcha. I remember running into this with MongoMapper and it the easy fix was just moving the include line to after to the key definitions in the model. Since ActiveRecord is so baked in, I was unsure of where to consider this. Closing...\n. Nice! thanks.\n. @karmi I\u2019ve been looking for quite a while on how to do this today, I didn\u2019t know that we could pass something like 'articles/article' to .search(). I think such an example should probably be added to the documentation. I don\u2019t think it\u2019s documented already.\n. Thank you very much :+1: upgrading the gem resolved my problem \n. @karmi you should probably update the README with a notice now that 1.0 is out. I know it\u2019s pretty clear that Tire is deprecated, but the more info the better.\n. @karmi I would have opened a pull request but wasn\u2019t sure how you wanted to word it. Thanks for adding it :+1: \nKeep up the good work.\n. ",
    "hookercookerman": "yeah cool; thanks for the input;  very true about self; cheers\n. ",
    "clemens": "I've also fixed the two TODOs for instantiating ActiveRecords with id (avoid the mass assignment warning) and new_record status.\n. I have a use case where I only need facets and meta data \u2013 not the actual results. Not sure if mine is an edge case but lazy loading usually doesn't hurt.\n. ",
    "fwoeck": "Ahh great, thank you!\n. Hallo, I tried it and it doesn't work in my situation - maybe because of stringy ids? \nWe have a document d with an _id that was stored as in the index:\nruby\nruby-1.9.2-p180 :011 > d._id.to_s\n => \"4e033331e8e68720c0006797\" \nruby-1.9.2-p180 :012 > Tire.search(Mongoid.database.name) { query { string '*' } }.results\n => #<Tire::Results::Collection:0x00000101baeb20 @options={}, @time=1, @total=1, @results=[<Item id: \"4e033331e8e68720c0006797\", _score: 1.0, _type: \"dokument\", _index: \"livesein_ng_devel\", _version: nil, sort: nil, highlight: nil>], facetsnil \nruby-1.9.2-p180 :013 > Tire.index(Mongoid.database.name).remove(d._id.to_s)\n => nil \nruby-1.9.2-p180 :014 > Tire.search(Mongoid.database.name) { query { string '*' } }.results\n => #<Tire::Results::Collection:0x00000101b80d88 @options={}, @time=2, @total=1, @results=[<Item id: \"4e033331e8e68720c0006797\", _score: 1.0, _type: \"dokument\", _index: \"livesein_ng_devel\", _version: nil, sort: nil, highlight: nil>], facetsnil\nAfter the remove it's still there. I'd be grateful for any hints on this!\n--Frank\n. could it be that it's because remove(*args) assumes :document as type\nruby\n(document = args.pop; type = :document)\nwhereas my used type is doKument?\n. yes, confirmed, when I use 'doCument' as type it works.\n. ",
    "ike-bloomfire": "This still doesn't work btw ie this syntax\nTire.index('myindex').remove('docID')\nPS: is it possible to make the syntax for this match the update_index syntax so you could just do something like ...\npost.tire.remove\n... for example?\n. try :index => 'not_analyzed' instead of :not_analyzed\n. > It does not make much sense to me that the DSL would behave like this... It would add a lot of overhead then updating the index, since the class would have to go over all inidices, and fire all the updates\nRight, but thats something you'd be aware of if you wanted to do it this way. I also don't see a reason for more than 2 or 3 indexes on a model. \nI just think that this feature makes Tire a lot more flexible, and gives a user the opportunity to think of architecting their elasticsearch indexes in smarter more sophisticated ways.\n\nWould there be another use case for the behaviour you've outlined?\n\nFor example, what if I have about 30 (massive exaggeration, I know) different models that I want to be able to search across all at once. With tire I'd have to setup 30 different indexes for each one, when I could just put everything in one index to conserve resources and speed up queries. \nThe one index-to-one model setup, absolutely works and I have no quarrel with it, I just think this functionality could help make Tire even more powerful than it already is\n. My argument was not that you couldn't do it, but that it was inefficient.\n. > The code I posted demonstrates that you can, in fact \u201cput everything in one index\u201d -- did you try it?\nYes. I'm aware of this, its how I got halfway around this issue.\nThat was total brain freeze on my part, not sure why I even made that particular argument to begin with.\n. No problem, I can be on the hook for writing up documentation and helping out with this feature if you ever decide to add it in.\n. What code are you using to out put your highlights?\n. I'm really not sure why that would happen, have you taken a look at the responses Tire logs\nyou can use this to set it up\nTire.configure { logger 'elasticsearch.log', :level => 'debug' }\nthe responses are logged as well (see the docs) that should help you go a little further, I should think.\n. Also happened across this in the Elasticsearch documentation\nHighlighting http://www.elasticsearch.org/guide/reference/mapping/all-field.html\nFor any field to allow highlighting it has to be either stored or part of the _source field. By default _all field does not qualify for either, so highlighting for it does not yield any data\nThen from the Source page http://www.elasticsearch.org/guide/reference/mapping/source-field.html\nThe _source field is an automatically generated field that stores the actual JSON that was used as the indexed document\nSo it would appear that your to_indexed_json method might not be working the way you want\n. what about the 'not_analyzed' setting?\n. My understanding is that query does a search on the _all field by default,\nwhy I'm confused is that even though I tell elasticsearch not to include it\nin _all or analyze it, it is still searching against it.\nOn Thursday, December 13, 2012, Karel Minarik wrote:\n\nwhat about the 'not_analyzed' setting?\nSorry, don't understand. (How is the mapping for the count field set?)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/issues/538#issuecomment-11327768.\n. I posted the query above\n\nthe actual json looks like this  \n```\n{\n   \"query\":{\n      \"query_string\":{\n         \"query\":\"95\",\n         \"default_operator\":\"AND\"\n      }\n   },\n   \"sort\":[\n      {\n         \"count\":\"desc\"\n      }\n   ],\n   \"filter\":{\n      \"term\":{\n         \"active\":true,\n         \"_type\":null\n      }\n   },\n   \"highlight\":{\n      \"fields\":{\n         \"name\":{\n     }\n  },\n  \"pre_tags\":[\n     \"<strong>\"\n  ],\n  \"post_tags\":[\n     \"</strong>\"\n  ]\n\n}\n}\n```\n. > So you're searching for \"query\": \"95\" and that finds a doc with count:95?\nexactly\nHere's the result returned when I run the query from the command line, the count has increased now, so its 98 not 95 (I adjusted the query accordingly)\n{\n   \"took\":2,\n   \"timed_out\":false,\n   \"_shards\":{\n      \"total\":5,\n      \"successful\":5,\n      \"failed\":0\n   },\n   \"hits\":{\n      \"total\":1,\n      \"max_score\":null,\n      \"hits\":[\n         {\n            \"_index\":\"typeahead\",\n            \"_type\":\"membership\",\n            \"_id\":\"1\",\n            \"_score\":null,\n            \"_source\":{\n               \"active\":true,\n               \"organization_id\":1,\n               \"user_id\":1,\n               \"name\":\"Mr West\",\n               \"count\":98,\n               \"membership_id\":1\n            },\n            \"sort\":[\n               98\n            ]\n         }\n      ]\n   }\n}\n. > Check the index mapping -- maybe you need to recreate the index?\nI've done that at least 100 times in the last week :D (been testing different tokenizers and search combos).\nI'm using elasticsearch 19.2 and have an index_name 'xxxx' set inside the mapping block, dunno if that has anything to do with it. I guess I'll try and isolate the problem a little bit more when I have time, but its good to know I'm not crazy. \n. > The mapping declaration creates the index when the class is loaded or when the importing features are used, and only when it does not yet exist\nQuestion about this ... \nfor legacy reasons, I am actually creating the index outside of tire, but I'm only sending in a settings hash/object, the mappings part of the index is empty. \nso basically \ncurl -X PUT localhost:9200/myindex -d '\n{\n  \"settings\" : { bunch of awesome settings },\n}\nhowever, none of the settings I set in my mappings for ActiveRecord Objects (this is a Rails app) are respected, not even :type settings, myobject.tire.update_index still works though.\nMy question ... is tire ignoring the mapping block I specify because the index exists (so-to-speak)?\nPS: I should mention I'm using one index to hold several activerecord types as opposed to one index per model that is the normal Tire usecase\n. Thank you, this explains a lot.\nJust to be clear, an unfortunate side effect of this is that if you're using a single index for all your models, then there is a chance that all the settings/mappings that you have in all the other models (except the first one to get indexed) will not make it into the index ... right?\n. Also, where do you prefer people ask questions that are not technically issues? here or stackoverflow?\n. post your code\n. Thanks Karel!\n. Thank you, that helped a lot, knowing where to put the expects.\n ... update_index is just not firing when it should during the test (have it in a callback than i'm doing manually vs using Tire::Model::callbacks), just have to figure out why.\n. My apologies if I sound like a newb but I was unclear how to use this to do persistent connections? \nwould I have to manually fire head requests at the server in the background or something?\nAlso am I correct that Tire does not use persistent connections?\n. Ahh right ... just wanted to know if Tire essentially re-used connections as opposed to opening up a new connection on each Elasticsearch request. Looks like keepalives are another name for that, I didn't know that is why I was confused initially.\nLooks like it does,https://github.com/karmi/tire/blob/master/lib/tire/http/clients/curb.rb#L11, sorry for bothering you about it.\n. Thanks karmi, \nI'm just using the default configuration, can you point me to where I need to be looking?\nI just want to know the proper way to configure the default client that is bundled with Tire\n. Looked in there and didn't see an obvious setting ... is curb the default http client?\nAnd yes I'm just trying to set the clients timeout\n. So I found out that tire is using the rest-client library as its default (I couldn't get it to use Curb) but you don't pass in a timeout setting for any of the requests you make in tire\nhttps://github.com/karmi/tire/blob/master/lib/tire/http/client.rb\neven though it looks like RestClient supports this. Is there a way to hack around this, or should I submit a pull request of some sort?\n. Hmmm, I had no idea that could work\nAre you saying that would make it so that \nTire.index('content').remove(\"blog_post\", object.id)\nwould be the same as\nTire.index('content').remove(\"post\", object.id)\n... any pointers/links you can give will be useful in figuring this out.\n. Thank you Karel!\n. require it where?\nI put it at the top of tire.rb ... same error\n. finally figured it out for anyone that might need help with this\nrequire 'curb'\nrequire 'tire/http/clients/curb'\nTire.configure do\n  client Tire::HTTP::Client::Curb\nend\n. ",
    "reiz": "Thanks. That worked for me. \n. A link in the README would be awesome :+1: \n. An example in the documentation would be useful. I need to know how I can send a mapping via PUT to an existing empty index. \nThis is an issue for me right now. There is one ES SaaS provider I found, which is creating the index for me. And I am not allowed to delete or overwrite it. I am forced to send my mappings via a PUT to an existing index. Right now Tire is not usable for this task. \nI appreciate any kind of feedback. \n. OK. Will try. Many Thanks for the fast response. \n. OK. Will do. I am experimenting with some ElastisSearch providers. As soon as I did a successful test with authentification I will close this ticket. \n. @karmi Give me one more day to test this out. It is on my ToDo list. \n. @karmi I just tried it. It works fine. \n. @karmi Happy Christmas to you and your family. \n. OK. I just turned loggin on. Here is the complete output: \n``` bash\n2012-12-11 11:37:42:790 [CREATE] (\"gasfasgfasgfasfga\")\n\ncurl -X POST https://www.indexdepot.com/elasticsearch/gasfasgfasgfasfga -d '{\"mappings\":{\"product\":{\"properties\":{\"_id\":{\"type\":\"string\",\"index\":\"not_analyzed\",\"include_in_all\":false},\"name\":{\"type\":\"string\",\"analyzer\":\"snowball\",\"boost\":100},\"description\":{\"type\":\"string\",\"analyzer\":\"snowball\"},\"description_manual\":{\"type\":\"string\",\"analyzer\":\"snowball\"},\"language\":{\"type\":\"string\",\"analyzer\":\"string_lowercase\",\"index\":\"not_analyzed\"}}}}}'\n2012-12-11 11:37:42:790 [404]\n=> #{:product=>{:properties=>{:_id=>{:type=>\"string\", :index=>\"not_analyzed\", :include_in_all=>false}, :name=>{:type=>\"string\", :analyzer=>\"snowball\", :boost=>100}, :description=>{:type=>\"string\", :analyzer=>\"snowball\"}, :description_manual=>{:type=>\"string\", :analyzer=>\"snowball\"}, :language=>{:type=>\"string\", :analyzer=>\"string_lowercase\", :index=>\"not_analyzed\"}}}}}, @response=404 : <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n\n404 Not Found\n\nNot Found\nThe requested URL /elasticsearch/gasfasgfasgfasfga was not found on this server.\n\n\n```\n. I am able to store some documents into the index, but I can't create a mapping. And for the search I am getting a similar result. \n. this here \"gasfasgfasgfasfga\" is not really the index name. The index name I am using does exist. I can send you the index name via email or skype. \n. I shoot them the link to this issue. Hope they will respond today and help me out. \n. OK. Problem found. The problem was that a slash at then end of the indexname was missing. This here works: \n\nhttps://www.indexdepot.com/elasticsearch/gasfasgfasgfasfga/\n. I will double check it on the weekend. Thanks for the hint. \n. I just tried the bulk_store with the newest version 0.5.4. But I am getting always json parsing exceptions. \njava\n[2013-02-13 13:06:54,007][DEBUG][action.bulk              ] [Schultz, Herman] [product][2] failed to execute bulk item (index) index {[product][document][4f04d600e4b0c514865d01a9], source[{:_id=>\"4f04d600e4b0c514865d01a9\", :_type=>\"product\", :name=>\"util-plan-fetcher-impl\", :description=>\"Implementation for the resource fetchers - bundle version\", :description_manual=>\"\", :language=>\"Java\", :followers=>0, :group_id=>\"org.ow2.bundles\", :prod_key=>\"org.ow2.bundles/util-plan-fetcher-impl\", :prod_type=>\"Maven2\"}]}\norg.elasticsearch.index.mapper.MapperParsingException: Failed to parse\n  at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:501)\n  at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:430)\n  at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:318)\n  at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:157)\n  at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:533)\n  at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:431)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n  at java.lang.Thread.run(Thread.java:680)\nCaused by: org.elasticsearch.common.jackson.core.JsonParseException: Unexpected character (':' (code 58)): was expecting either valid name character (for unquoted name) or double-quote (for quoted) to start field name\n at [Source: [B@106f29c8; line: 1, column: 125423]\n  at org.elasticsearch.common.jackson.core.JsonParser._constructError(JsonParser.java:1378)\n  at org.elasticsearch.common.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:599)\n  at org.elasticsearch.common.jackson.core.base.ParserMinimalBase._reportUnexpectedChar(ParserMinimalBase.java:520)\n  at org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser._handleUnusualFieldName(UTF8StreamJsonParser.java:1728)\n  at org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser._parseFieldName(UTF8StreamJsonParser.java:1409)\n  at org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:677)\n  at org.elasticsearch.common.xcontent.json.JsonXContentParser.nextToken(JsonXContentParser.java:48)\n  at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:460)\n  ... 8 more\nIf I index each element one by one, it works fine. But with the bulk_store it doesn't work. \nThis here is my bulk code: \nRuby\nproducts = Product.all().skip( skip ).limit( bulk )\nTire.index( Settings.elasticsearch_product_index ).bulk_store products.to_a\n. By the way, bulk is 100. I am always fetching 100 documents from MongoDB and want to index them. \n. @karmi OK. What else you need? But I am not doing anything special. I basically fetching 100 documents from MongoDB and that's it. \nWhere can I find you on IRC ? \n. ",
    "chrisberkhout": "Ah, yeah. That's good.\nIt looks like I originally misinterpreted what :pretty => true was doing.\nKeep up the good work!\n. Hi all,\nVery interesting so far!\nSorry this is so long! Maybe this discussion needs to be split into several different issues.\nThe term 'alias'\n@floere I think I can see where you're coming from with this being confusing. However, I had no worries with the jump from aliases to 'groups' because I was thinking:\n1. Wherever you can use an index, you can also use several indexes.\n2. Wherever you can use an index, you can use an alias that is a pointer to an actual index (or several of them).\nI probably just happened across the right part of the ES docs at the right time.\nIn any case, Tire should obviously use terminology that's consistent with ES.\nAlias API\n@karmi, I also spotted the inversion of alias and index in the example. I suggest something like:\nTire.alias('posts').targets do\n  remove 'posts-2011-01'\n  add    'posts-2011-03'\nend\nI do agree that managing it outside the application is probably a good idea. I'd probably still want to use Tire to do that.\nBlockish DSL vs 'imperative' Ruby-ish\nI can see both sides of this. \nThe DSL way is good when you're writing something static. I think you want to talk the 'imperative' style when building up queries dynamically.\nI actually did my dynamic query building with the block syntax (mainly because that's what I saw in the README), and it works...\nsearch = Tire.search(ES.index_name) do\n  size 1000\n  query do\n    if boolean_parts.count > 0\n      boolean_parts.each { |part| part.call(self) }\n    end\n  end\n  FacetParts.for_query(this_query).each { |part| part.call(self) }\n  sort_part.call(self) if sort_part\nend\n...but it requires jumping through hoops for no good reason. I'll probably try to refactor it to imperitave style. A mention of imperative style in the README could be good. I didn't realise I could do it that way until I looked more at the Tire code.\nI may be opening a can of worms here, but I've also been thinking, something like Arel might work even better. Say, like this:\ns = Search::Search.new('articles').\n      query_string('title:T*').\n      filter(:terms, :tags => ['ruby']).\n      size(1)\nMy reservation is that perhaps that's getting too much into applications (ie. dynamic query builder), rather than sticking to being an API wrapper. Thoughts?\nDynamic index names for storing and searching\nThe main story about aliases and multiple indexes is that you use a fixed alias, then separately manage it to resolve to the right index(es). For that case, being able to dynamically choose an index is not important.\nYou would want the dynamic functionality if you're trying to choose the index based on attributes of an object you're storing, or if you're querying a subset of indexes, chosen based on things not modelled in your alias->index setup.\nAs said, you can do that already give Tire.search a dynamically generated index name, but being able to use a block in place of a static name in models includeing Tire::Model::Search would be a simple and good enhancement.\nRegarding the trickiness that @karmi mentioned in his initial post, what about of two index name blocks: one for querying (which takes no parameter), and one for the index for storing (that takes the object to be stored as parameter)?\nIndex templates\nYes! These are going to be important if you're creating lots of indexes on the fly.\nBlocks vs lambdas\nFor a second I thought that this was something to do with local varaibles defiend before the block/lambda, but then I did a refresher on blocks and lambdas (and procs), and they seem equivalent to me. Except that with blocks you only get to attach one of them to a method call, and with lambdas you need to get the number of parameters right. If the method takes a block I can choose to create and save it a lambda and pass that to the method, if I want.\nIf there's more to this I'd like to know.\n. @floere Yep, thanks. Got that one from my reading. Just forgot to mention it\nagain because I think it's not really relevant to the DSL issue.\n. @floere Oh yeah, that's a good point. Still have options there though:\n```\nblock that uses \"return\" value\ndef block_eater\n  puts \"block_eater got: #{yield}\"\nend\nways you can't return stuff\nblock_eater { return \"return hello from block\" } # => LocalJumpError: unexpected return\nblock_eater(&(lambda { return \"return hello from lambda\" })) # => LocalJumpError: unexpected return\nways you can \"return\" stuff\nblock_eater { \"hello from block\" } # => \"block_eater got: hello from block\"\nblock_eater(&(lambda { \"hello from lambda\" })) # => \"block_eater got: hello from lambda\"\n```\nHmm... getting off topic here.\n. Thanks.\nI did try changing (in lib/tire/index.rb)\ncurl = %Q|curl -X POST \"#{Configuration.url}/_bulk\" -d '{... data omitted ...}'|\nto\ncurl = %Q|curl -X POST \"#{Configuration.url}/_bulk\" -d '#{payload.join(\"\\n\")}'|\nBut realised it needs more escaping (data contains ').\nI'll try some more on this tomorrow and let you know how I go.\n. The solution to this came quite quickly when I got back to this issue.\nI tracked down the problem to an empty bulk request:\ncurl -X POST \"http://localhost:9200/_bulk\" -d ''\nIn Index#bulk_store, I used ruby-debug to find that the problem was with documents seeming to have a non-empty second page, when actually the second page was empty:\n[92, 101] in /Users/chrisberkhout/.rvm/gems/ruby-1.9.2-p180/gems/tire-0.1.15/lib/tire/index.rb\n   92        end\n   93        payload << \"\"\n   94        \n   95        debugger\n   96  \n=> 97        tries = 5\n   98        count = 0\n   99  \n   100        begin\n   101          Configuration.client.post(\"#{Configuration.url}/_bulk\", payload.join(\"\\n\"))\n/Users/chrisberkhout/.rvm/gems/ruby-1.9.2-p180/gems/tire-0.1.15/lib/tire/index.rb:97\ntries = 5\n(rdb:1) documents.inspect\n\"#<Mongoid::Criteria\\n  selector: {},\\n  options:  {:limit=>100, :skip=>100},\\n  class:    Document,\\n  embedded: false>\\n\"\n(rdb:1) documents.size\n75\n(rdb:1) documents.to_a.size\n0\nThe solution was to change my Kaminari #paginate patch so that it would run #to_a, and report #size correctly. The updated patch is:\n```\nGet tire to work with kaminari pagination (see issue: https://github.com/karmi/tire/issues/14)\nmodule TireKaminariPatch\n  def self.included(base)\n    base.class_eval do\n      def self.paginate(options)\n        page(options[:page]).per(options[:per_page]).to_a\n      end\n    end\n  end \nend \n```\nAlso worth noting: to be able to log the bulk load as proper curl requests, I had to escape single quotes in the data being posted, by doing gsub(\"'\",\"'\\\\\\\\''\"). I read http://www.grymoire.com/Unix/Quote.html to understand how the quoting works.\n. This sounds good.\nA side note: my current (v0.3.5) workaround for doing multiple filters is to do an :and filter and give it an array:\ns = Tire.search(\"myindex\") do\n  filter \"and\", [\n      {\"or\" => [\n        { \"term\" => { \"_type\" => \"document\" } },\n        { \"term\" => { \"_type\" => \"source\" } }\n      ]},\n      { \"ids\" => { \"values\" => [\"4e8e8aa116e1771ef1000002\"] } }\n    ]\n  query { all }\nend\nWhich generates a call like:\ncurl -X GET \"http://localhost:9200/myindex/_search?pretty=true\" -d '\n{\n    \"query\": {\n        \"match_all\": {}\n    },\n    \"filter\": {\n        \"and\": [\n            {\"or\": [\n                { \"term\": { \"_type\": \"document\" }},\n                { \"term\": { \"_type\": \"source\" }}\n            ]},\n            {\"ids\": {\"values\": [\"4e8e8aa116e1771ef1000002\"]}}\n        ]\n    }\n}\n'\nNote: I previously thought the array had to be double wrapped (ie. [[...]]) for Tire to handle it correctly, but that was a mistake. A double wrapped array there is handled incorrectly by ES (following values are ignored).\n. Sorry for long delay! My suggestion is that indexes just works outside a block.\nSo, this code:\nclass Venue\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\n  index_name \"some_index_name\"\n  #...\n  mapping do\n    indexes :location, type: 'string', analyzer: 'snowball\n    indexes :lat_lon, type: 'geo_point'\n  end\nend\n...can become:\n```\nmodule Geolocated\n  included do\n    indexes :location, type: 'string', analyzer: 'snowball\n    indexes :lat_lon, type: 'geo_point'\n  end\nend\nclass Venue\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\n  index_name \"some_index_name\"\n  #...\n  include Geolocated\nend\n```\nIt could also stay wrapped in a block, as in @niuage's example. The important thing is just that you can extract parts of a class's mapping definition into a module.\nI see the trouble in delaying the index check/creation until the end. @floere put me on to this technique:\nEND { code to be executed here }\nhttp://ruby.runpaint.org/flow#begin-exit-handler\nWhich I believe could be used to make things happen after the whole of a class definition has been loaded up.\n. ",
    "wulffeld": "With mongoid 2.0.2 will_paginate is no longer supported. That's what's causing this error in case that's not clear to anyone new coming here.\nI've personally removed will_paginate and replaced it with kaminari which seems better. However, it has different semantics so it doesn't work with tire and stuff like:\nruby\nArticle.import :per_page => 100\nbreaks. I can't figure out if there's a better way around doing this:\nruby\nArticle.all.each do |conf|\n  Article.elasticsearch_index.import [conf]\nend\n. Article.all I obviously don't want to use with millions of records :)\nSee:\nhttps://github.com/mongoid/mongoid/issues/1000\nI agree that Article.all.paginate works (if you happen to use will_paginate) but if you're trying to use Article.import it fails because it wants Article.paginate. I guess I can just create my own kaminari pagination loop around the import command but it feels icky.\n. ```\ndef my_paginate(options = {})\n   Article.page(options[:page]).per(options[:per_page])\nend\nArticle.import :method => \"my_paginate\"\n```\nThe above ought to work however I think I've discovered a mongoid related bug. The document.empty? check: https://github.com/karmi/tire/blob/master/lib/tire/index.rb#L137 always returns false. I think you should do .length == 0 instead here. Or maybe it's a kaminari bug - not sure:\n```\n\n\nArticle.page(3).per(1000).length\n0\nArticle.page(3).per(1000).empty?\nfalse\nArticle.page(3).per(1000).to_a.empty?\ntrue\n. Yup, I'll check out master.\n. master checked and verified to work fine! Thanks!\n. Entirely up to you :) I managed to waste a couple of hours figuring out the error. Probably would suffice to just add a comment in the README about it?\n.\nArticle.page(1).per(10).class\nMongoid::Criteria < Object\nArticle.page(1).per(10).all.class\nMongoid::Criteria < Object\nArticle.page(1).per(10).all\n\n\n<Mongoid::Criteria\nselector: {},\n  options:  {:limit=>10, :skip=>0},\n  class:    Article,\n  embedded: false>\n```\nNote, however, that is the same pre-mongoid 2.1.2.\nAlso using #map works fine in the console and only returns 10 results.\nHere tire is calling #size however: https://github.com/karmi/tire/blob/master/lib/tire/index.rb#L122 - that's what no longer works as before. I.e. it will return the same as Article.count instead of the number of documents found and hence the loop blows up because it's always >0 even though the documents is empty when trying to to do #map here: https://github.com/karmi/tire/blob/master/lib/tire/index.rb#L82.\n. It's the same deal. Only #to_a makes it behave.\n. Sorry, been completely swamped the past couple of weeks. 3.2 seems to work just fine.\n. ",
    "coffeencoke": "This is still an issue for me.\nWhen I run the import rake task I get undefined methodpaginate' for MyModel:Class`\nCan someone please provide some light on the following question:\nSince MongoDB uses cursors to retrieve queries in batch mode natively, why is there a need to paginate for this task?  \nI would like to be able to override the method, 'paginate', used in the rake task.  I understand the last thing you may want to do is introduce another ENV variable to the task but the task does not work for an application unless it is using will_paginate or kaminari... That seems a bit off.\n. I understand, thanks for your input. We will figure something out. \nThanks!\nMatt\nSent from my iPhone\nOn Nov 17, 2011, at 3:28 AM, Karel Minarikreply@reply.github.com wrote:\n\n\nWhen I run the import rake task I get undefined method 'paginate' for MyModel:Class\n\nOf course -- the paginate method does not exist in your model, when you're not using will_paginate. If you don't want to use will_paginate, you have to implement it yourself, as in the example above...\n\nSince MongoDB uses cursors to retrieve queries in batch mode natively, why is there a need to paginate for this task?\n\nWhat MongoDB has to do with this? The import task makes an assumption that paginate will be implemented in most cases, MongoDB or not.\n\nI would like to be able to override the method, 'paginate', used in the rake task.\nI understand the last thing you may want to do is introduce another ENV variable to the task but the task does not work for an application unless it is using will_paginate or kaminari... That seems a bit off.\n\nIf it helps you, OK, why not make the method configurable. But I don't think it will help you -- the method would have to have the same interface as the  will_paginate method, and the only benefit you're gaining is different name.\nTire makes the following assumption: if your application is not using some sort of pagination,\nthe amount of data you probably have is so small, that you shouldn't bother with Rake tasks and just write:\nruby\nArticle.tire.index.import Article.all\nAgain: everything about this is about assumptions and 80/20 ratio. The thinking behind the reliance on pagination method is this:\n- When you have nontrivial amounts of data, you are using some sort of pagination.\n- When you are using pagination, there's high chance you're using the paginate method of will_paginate -- problem solved. So we don't care about ActiveRecord's find_each and every crazy batch finder out there. We focus on one, quite common pattern.\n- When you're not using will_paginate, you know what you're doing and probably can emulate the paginate method, see code above -- problem solved, again.\n- When you have a highly specific use case, it's better for you to handle the import yourself anyway, and Tire::Index#import makes it very easy for you.\nMakes sense?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/48#issuecomment-2773913\n. When running rake I got something like \"you must install minitest, add minitest to your bundle. It's a development dependency. \n\nThanks!\nMatt\nSent from my iPhone\nOn Nov 17, 2011, at 2:49 AM, Karel Minarikreply@reply.github.com wrote:\n\nHi, why is this needed?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/pull/150#issuecomment-2773629\n. Sorry for the lack of response, it's been a crazy couple of weeks.  @grasscode got the same thing I got.\n. @karmi no pressing reason, just something I think you should support someday.  We are using tire for our application, which is using ruby 1.9.2.\n. Sounds good to me\n\nThanks!\nMatt\nSent from my iPhone\nOn Dec 11, 2011, at 6:28 AM, Karel Minarikreply@reply.github.com wrote:\n\nSo, any hard feelings if we close this as 1.9.3 seems better choice for development/production now?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/pull/150#issuecomment-3096902\n. sorry, this still does not work because the import method in lib/tire/index.rb forces :page => options[:page] which causes an exception for the all method.\n\nWhat would you suggest we do?  When I run the import rake task we get the issue discussed in #48\n. +1\n. I have to think back to what the issue was in the first place to come up with some suggestions.  Providing an object that acted and appeared like a specific model, but wasn't the model, caused quite a lot of issues and pain when we were using Tire.\nIf I have a moment I'll try to check it out.  I do not think the current implementation is the only implementation, and I think that's why this issue was created in the first place.  It was created to notify the makers of tire that the design for this particular class caused a lot of pain for us. \nIf your decision is to keep the current implementation, I think that's absolutely no problem.\n. Correct, I will take your advice from the other comment. \nThanks!\nMatt\nSent from my iPhone\nOn Nov 17, 2011, at 3:06 AM, Karel Minarikreply@reply.github.com wrote:\n\n\n@@ -67,7 +71,7 @@ namespace :tire do\nSTDOUT.puts '-'*tty_cols\nelapsed = Benchmark.realtime do\n-      index.import(klass, 'paginate', params) do |documents|\n-      index.import(klass, finder_method, params) do |documents|\n\nI don't think this will work in your case. The Index#import method then relies on increasing the params[:page] variable. Does the batch finder in your case work the same way?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/pull/151/files#r235153\n. \n",
    "plentz": "Hey guys, I think we should re-open this issue, since kaminari isn't fully supported yet. Some errors like this aren't expected.\nActionView::Template::Error (undefined method `first_page?' for #<Tire::Results::Collection:0x007fc7d27ed0a0>):\n    1: <% if @customers.first_page? %>\n. @karmi there's more things to be supported, like https://github.com/karmi/tire/issues/48#issuecomment-4589313. I think that @amatsuda could help us here.\n. +1\n. -1 (for the same reason that @fcheung wrote :)\n. -1 because of the delayed_job dependency that it add(but I liked the idea itself)\n. Hum, I've looked at the code, but it wasn't very clear to me. Looks like something around here, but I didnt find the exact point to add the unscoped call.\n. Thanks for the tips @karmi \n. closing since it will be discussed at #289\n. ps: this pull request is still missing tests. I looked at the code but didn't find where the query's tests should live. There's text_query_test.rb, range_queries_test.rb, query_string_test.rb. Could someone point me if there's some kind of pattern for queries integration tests?\n. 1) Since elasticsearch support it out-of-the-box, I think we should support it natively too, so, +1 for core.\n2) Fully agree.\n3) Thanks, I will update my pull request adding the corresponding tests.\n4) Actually it already support passing options, but I think it could be a better looking(and simpler) API. Today, we have to do something like this:\nruby\nterm :name, query\nbut, when you want to pass options, you have to changed it to:\nruby\nterm :name, { term: query, boost: 2 }\nwhich I think it's confusing(even more for newcomers). After the change I've made, all you have to do is pass the options\nruby\nterm :name, query, boost: 2\n. well, I think that's it. unit + integration tests created.\n. @kidpollo I'm facing the same challenge here. We were using a single index using a tenant_id to split data, but since  we changed our multi tenancy approach to use multiple postgresql schemas instead of using a tenant column, we faced this problem as well. If I can help with something(your team is working in some branch?), let me know.\n. ",
    "rodrigoalvesvieira": "This error happens in my app using Rails 3.2.6 and Tire 0.4.2 and the weird thing is that I am not using any pagination gem/technique.\n. ",
    "timuckun": "Same as @rodrigoalvesvieira but I have kaminari loaded. \n. The problem was that I was pointing at port 9300 and not 9200.\n. I figured it out and updated the wiki https://github.com/karmi/tire/wiki/Pagination-using-the-DSL\n. See this gist\nhttps://gist.github.com/2306099\nThe code is from above. \n. I have taken the approach of creating just one index and using bulk updating to put all items into the same index like this.\n{ \"id\":1,\"type:'this'},\n{\"id\":1, \"type\":\"that\"}\nEtc.  I guess that's not so great if you want to make specific types of indexes but I am wondering if there is something profoundly wrong with this approach.\n. Sorry I misunderstood.\n. Nice writeup, I'll probably implement this very soon. Thanks to whoever wrote it.\n. Yes and no. to_index_json works great for most uses but the mapping still has to take place in one shot. I was hoping I could incrementally define the mapping so that base mappings could be set in a module that is mixed in (id etc) and others could be added by the class.\nFor now I have decided to avoid manual mapping and just rely on to_index_json.\n. I'll close the issue. Is there a mailing list? Seems like a better forum for these types of conversations.\n. ",
    "housepage": "Sure thing.\n. ",
    "gaffo": "It looks like it is active record. I was trying to pull snapshots of our ES data into mysql for analysis over time. Looks like I just need a second object that does the ES mapping. The ID from the target (mysql) model isn't the same as the uuid that I'm putting into ES. I was making an _id column in mysql to be the target.\n. it's definitely an int. Not sure what I really want as my usecase isn't\ntypical. Raw access to the data might be a nice compromise.\n. Sounds good on the closing. Looking through the code, I believe you are\nhiding the stuff that is pulled from ES as protected or private and just\nletting active record return it. I think it would fix my issue to be\nable to just have access to it. Say tire_raw_attributes[:_id] or\ntire_raw_attributes['_id'].\nOn 7/12/2011 12:57 AM, karmi wrote:\n\nClosing the issue as invalid, the discussion can continue.\n. \n",
    "chromeragnarok": "It's weird that this only creates the mapping, but will not send the values of the nested attributes on update if you do it with ActiveRecord. To get the nested value attribute updated then you have to use :as , thus it will become like this : \n``` ruby\nclass Article < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nmapping do\n    indexes :title, :type => 'string'\n    indexes :author_first_name, :type => 'string', :as => Proc.new { author.first_name }\n    indexes :author_last_name,  :type => 'string', :boost => 100, :as => Proc.new { author.last_name }\n  end\nend\n```\nWhich makes me wonder, what would be the point of having the nested indexes, if it only creates the mapping but doesn't update the value at all.\n. You're welcome :D\n. Oh and one more thing, to enable the auto reindex you have to add nested_attributes into your Tire.configure:\nTire.configure {\n  nested_attributes :delayed_job => true do\n    nest :associated_model => :active_model_article_with_association\n  end\n}\nThe delayed_job option is used if you want to use delayed_job, default is false. For multiple class you just add another nest inside the block:\nTire.configure {\n  nested_attributes :delayed_job => true do\n    nest :associated_model => :active_model_article_with_association\n    nest :associated_model => :another_active_model_article_with_association\n  end\n}\nIf nested_attributes is not added, the auto reindex will be disabled.\n. Ah right, the reasoning was that, I didn't want to add more load to the main application thread when it does the automatic reindexing. Thus, doing it on delayed_job is much more preferable. You could set the delayed_job option to false and let it handle the auto reindexing on the main application thread.\n. Yup, its doing that here : https://github.com/chromeragnarok/tire/blob/master/lib/tire/configuration.rb#L30\nHowever, delayed_job is still needed as the gem development dependency.\n. Great thanks for pointing me out to that touch article. If I have the extra time, I'll try to add more documentation and cleanup the code, and will try to resubmit again :)\n. ",
    "selvakn": "Hi Karel\nI started working on this to make use of em-http-client based client. Then noticed most of the api methods return false if the operation fails , but some (ex: bulk_update) does raise RestClient exceptions back. Im trying to pull out all the references to RestClient out. So does it make sense to make all the methods to return false on failure?\n. ",
    "rubish": "I have read the readme but it was not clear what exactly does rake task do. Also, if it creates new indexes whenever it is run, how are indexes, which are no longer in use handled.\nAs far as the support of aliases and DSL is concerned, you would be a much better judge, considering you have developed the gem with some philosophy in mind. The suggestion was something I very much liked in escargot and found missing in tire. I am happy if I can do something similar easily, even if it is not out of the box solution.\n. The approach taken by escargot may be complicated, but its seems practically very useful to me. \nUsecase: Lets say I have some some index already created for the model. I change the mapping, which can be quite frequent if the project is in its initial stages and under active development. Much less frequent if the project has already matured. What escargot lets me do is, create a new index, while old index still exists and can be used to fulfill the search queries. Once new index is ready silently and seamlessly transfer over to new index. It also provides the task to delete any unused indexes that might be eating up the disk space.\nThere might be some more use cases, but this was the simplest which came to my mind.\n. I was talking about the second scenario you just mentioned above and the didn't thought of the alternative you just described, probably because of my preconceived notions that the gem should do everything. \nI very much agree with what you suggest, but managing this process can be error prone. Say, i successfully completed the process on my local machine but made a typo while working on production server. That's of-course not your problem at all, but might be quite frustrating for the user.\nAbove approach would seem perfectly fine to me, if there is a migration system, where I can write a script on my local machine, test it thoroughly and while deploying on production just run the script. I know I can handle such things manually, but think of automated deployments using capistrano.\nPlease let me know your thoughts.\n. Will look into the tire-contrib gem and see if I can move the progress bar fix there.\n. I messed up something in the commit. Haven't worked with git before. I only wanted to add pull request for d6eb52efc9413eae76ab016fd855556b2922756b and b1283edc0a717f4769c53c60f5524d1340c5dfcb.\n. Would changing it to add and only if there are two or more filters suffice or you want it to do something more?\n. I have created a new fork, to fix the messed up commits in my repo. Will resubmit with using and only if there are two or more filters present.\n. @chrisberkhaout: Yes you can certainly do that, this would allow one alternative of doing the same:\nruby\ns = Tire.search(\"myindex\") do\n  filter \"or\", [\n      { \"term\" => { \"_type\" => \"document\" } },\n      { \"term\" => { \"_type\" => \"source\" } }\n    ]\n  filter \"ids\", \"values\" => [\"4e8e8aa116e1771ef1000002\"]\n  query { all }\nend\n. +1\nJust got bit by this.\n. ",
    "jarosan": "This page comes up on top if you search google for \"elasticsearch tire reindex\". Just wanted to share a small rake task i wrote today for reindexing: https://gist.github.com/3124884\nMaybe it would save somebody an hour or two :)\n. @karmi: The reason I wanted this to be done with should_be_indexed? method is because i wanted to keep the existing destroy/save callbacks without worrying about them. \nIt is possible to achieve that with this:\n```before_update_elasticsearch_index :remove_from_index\ndef remove_from_index\nPlace.tire.index.remove self if self.published_at.blank?\nend\n```\nbut this still felt like a hack to get it working. \nI think the reason my suggestion makes less sense to you because I didn't include the logic of indexing/not indexing the record in to_indexed_json method. @karmi, what do you think. Is it possible that you change your mind, if i submit a patch, where this logic (should_be_indexed?) would also affect whether an object gets indexed or not in the first place. That way this would be a general and recommended way to decide what objects get indexed and what not. \nThe syntax for doing this could be different. Either a should_be_indexed? method. Or maybe use named_scope-like syntax to pass a proc exclude_from_index lambda { |r| r.published_at.blank? }\nEither way I really think that this is something Tire would benefit from :)\n``\n. @karmi: no worries. wouldn't want to overcomplicate my code with usingtire-contrib. will use thebefore_update_elasticsearch_index` hook to get it working.\n. ",
    "mahemoff": "@jarosan That's incredibly useful. Thanks. I made a small change to handle multiple classes https://gist.github.com/3751744\n. FWIW came here because of same error, and @karmi was right, invalid JSON returned from to_indexed_json was indeed the culprit.\n. For anyone searching, I explained how to support private resources with manual callbacks. http://softwareas.com/private-resources-with-elasticsearch-and-tire\n. @brupm it's code in the original callback [1] that implements the bult-in destroyed? method, but only @karmi can explain in what circumstances that method mightn't work.\n1. https://github.com/karmi/tire/blob/master/lib/tire/model/callbacks.rb#L28\n. @karmi I've just been looking at this code and it seems the before_destroy (or destroyed?) override is preventing destruction for some reason. Do you know if it's still necessary in Rails 3.2+?\n(I wasn't much deleting this record type before, so it may have always been an issue.)\n. @karmi This is with an ActiveRecord where I'm manually implementing Tire::Model::Callbacks because some records are private. I noticed the records weren't being deleted until I removed those lines (Rails 3.2.13).\nBased on your explanation, it's probably okay for me to remove those lines as I'm handling the callbacks manually anyway.\n. I'll try to find time in the next few days.\nI don't really think it's a Tire bug though.\n. Good point, depends on sort order of course, but should normally be first result by default.\n(Thanks for a great library and great support btw.)\n. Although each_with_hit can work, I'm trying to reuse a list partial which was originally designed for regular active records (and this is really the main reason I use load => true in the first place).\nI think it would be useful if load => true automatically set a (regular, non-persisted) highlight attribute on the active record, if it exists. Same for other properties like score. Maybe for efficiency purposes it would need to be a different value for the load argument, e.g. load => :all. Then for example in a list of results, I could just render highlight || description which would work whether or not the code had been loaded by Tire.\n. https://github.com/EvilFaeton/tire_async_index\nApparently yes.\n. +1 great to see this\n. I've also encountered this just now as I've decided my app's views will just use ActiveRecords now instead of Tire's results, so I can use common infrastructure (e.g. a view to list active records).\nThis issue came up here too: http://stackoverflow.com/questions/15026612/rails-tire-elasticsearch-weird-error\nTire could probably do with a patch to use a query like where(id: ids) that will permit non-existing IDs. Nevertheless, here's the kernel of a workaround I made: https://gist.github.com/mahemoff/7251414\nIt does more than just avoid that error, as it also augments the records with result/hit info, and supports a custom query for efficiency purposes.\n. ",
    "naquad": "ActiveRecord::Associations can't help here :( Those methods will have to be implemented ground up.\n. I won't argue on pattern / antipattern, because using search engine instead of database is hardly a common practice, so for doing that some patterns must be broken.\nI'm doing my own extension for Tire implementing associations atm. There is not much to show yet, but if I'll get it to production I'll post a patch or pull request here.\nTo be honest I still don't know will it get to production or not, because ES is missing couple of features I need and I'm starting to look at Solr with its dynamic attributes.\n. > Are you referring to http://wiki.apache.org/solr/SchemaXml#Dynamic_fields? ElasticSearch does not need anything like that, because with dynamic mapping, it infers the field type from the JSON document (integer vs string vs date, etc). Could you describe your case in more detail if this is not satisfactory?\nI know ES is schema-less. I mean if I'll move to Solr I'll have to use those.\nProblem is I need spell checking and recommendations, which are already implemented in Solr, but not in ES.\n. ",
    "hgujral": "For some reason I'm not able to get an index mapping property of 'integer' for my persistence model.\n```\nclass Product\n  include Tire::Model::Persistence\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nproperty :name, boost: 10\n  property :description, analyzer: 'snowball'\n  property :price, type: 'integer'\n  ...\n  ...\n```\nWhen I look at the index metadata, this is what I get:\n...\n...\nname: {\n   type: string\n},\nprice: {\n   type: string\n},\ndescription: {\n   type: string\n},\n...\n...\nWhat's wrong here!?\n. ",
    "divyaiyer": "I will try this again from scratch. Mapping of shows gives channel_name type\nas String. Its weird why I am getting like this. I shall try your code on a\nnew model & see, whether that works for me.\nOn Wed, Aug 3, 2011 at 7:08 PM, karmi \nreply@reply.github.comwrote:\n\nHi,\nfirst: your mapping definition is correct. You need to make absolutely\nsure, that this is the mapping in ES. Either call\nShow.elasticsearch_index.mapping or just load <\nhttp://localhost:9200/shows/_mapping> in curl/browser. By the results the\nfacet give you back, the channel_name is, in fact analyzed.\nTake a look at & run this code, it gives proper facet back:\n``` ruby\n   require 'rubygems'\n   require 'active_record'\n   require 'tire'\nActiveRecord::Base.establish_connection( :adapter => 'sqlite3',\n:database => \":memory:\" )\nActiveRecord::Migration.verbose = false\n   ActiveRecord::Schema.define(:version => 1) do\n     create_table :shows do |t|\n       t.string   :title, :description, :channel_name\n       t.datetime :created_at, :default => 'NOW()'\n     end\n   end\n# 1) Make sure the index does not exist, so it is created with proper\nmapping\n   #    (See section about FORCE importing in the Readme)\n   Tire.index('shows').delete\nclass Show < ActiveRecord::Base\n     include Tire::Model::Search\n     include Tire::Model::Callbacks\n mapping do\n   indexes :channel_name, :type => 'string', :index => :not_analyzed\n end\n\n # 2) Provide your own, custom `to_indexed_json` method\n def to_indexed_json\n   {\n     :title        => title,\n     :description  => description,\n     :channel_name => channel_name,\n     :created_at   => created_at\n   }.to_json\n end\n\nend\n# 3) Create a document and refresh the index\n   Show.create :title => 'Series about ...', :channel_name => 'This is\nIndia'\n   Show.elasticsearch_index.refresh\ns = Show.search do\n     query { string 'series' }\n     facet('channel_name') { terms :channel_name}\n     sort { by :created_at, 'desc' }\n   end\np s.facets\n   # =>\n   # {\n   #     \"channel_name\"=>\n   #     {\n   #         \"total\"=>1,\n   #          \"_type\"=>\"terms\",\n   #          \"other\"=>0,\n   #          \"terms\"=>\n   #         [\n   #             {\n   #                 \"term\"=>\"This is India\",\n   #                  \"count\"=>1\n   #             }\n   #         ],\n   #          \"missing\"=>0\n   #     }\n   # }\n```\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/67#issuecomment-1718006\n\n\nRegards\nDivya Shyam\n. For my surprise it worked for a replica of that app on my Laptop. I\ninstalled the same-thing on my lappie. And now when I run search I get it\nfixed. But really strange and I dont know what was the issue and hence how\nto fix the issue on my office comp. Anyways thanks a lot for your quick\nresponses. Lemme try to figure out, what was the bug behind it.\nCheers\nDiv\nOn Wed, Aug 3, 2011 at 7:19 PM, karmi \nreply@reply.github.comwrote:\n\nBelieve me, the most probable cause for this is that the index is already\ncreated before your model is loaded. Delete the index (curl -X DELETE\nhttp://localhost:9200/shows), and load the app. Or use the FORCE option\nin the Rake task, as documented in the Readme.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/67#issuecomment-1718083\n. Yeah, whether it cud be this? I have two models and using same attribute\nchannel_name and mapping in both the models. Bcz for the search  I need\nobjects of both models.\n\nOn Wed, Aug 3, 2011 at 9:51 PM, karmi \nreply@reply.github.comwrote:\n\nAgain, something must be creating that index before your app is loaded...\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/67#issuecomment-1719344\n\n\nRegards\nDivya Shyam\n. Unfortunately no. Did re-installation elastic server itself and removed all\nmodel Tire initialization. And sample code executed in rails console. But result\nwas same at office PC.\nOn Tue, Aug 16, 2011 at 7:18 PM, karmi \nreply@reply.github.comwrote:\n\nSo, any luck in the end?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/67#issuecomment-1816341\n\n\nRegards\nDivya Shyam\n. Sure, off for the day. Wud try tomorrow and submit the issue if persists .\nOn Tue, Aug 16, 2011 at 9:48 PM, karmi \nreply@reply.github.comwrote:\n\nStart with a fresh app, and copy over only the models & migrations etc. If\nthe problem persist, try to start with a fresh app with a simple model,\nhooked to Tire, and submit a new issue, please...\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/67#issuecomment-1817863\n\n\nRegards\nDivya Shyam\n. ",
    "devilcoders": "Can somebody help me with this issue, I just can't resolve it. I re index data, delete index, restart servers, everything.\nmy mappings (i tried a lot of other combinations)\ntire.mapping do\n    indexes :product_id,         :type => 'integer', :index    => :not_analyzed\n    indexes :name,               :type => 'string',  :analyzer => 'russian'\n    indexes :deleted,            :type => 'boolean'\n    indexes :short_description,  :type => 'string',  :analyzer => 'russian'\n    indexes :original_currency,  :type => 'integer'\n    indexes :brand,              :type => 'string',  :index    => 'not_analyzed'\n    indexes :url,                :type => 'string',  :index    => 'not_analyzed'\n    indexes :price,              :type => 'float'\n    indexes :taxons,             :type => 'integer', :analyzer => 'keywords'\n    indexes :facets,             :type => 'nested',  :index    => 'not_analyzed'\n    indexes :images,             :type => 'string',  :index    => 'not_analyzed'\n  end \nfacets is a Hash of values. \nlater on in controller i use:\nfacets.each do |f|\n    facet f.name do\n      terms f.name.to_sym, :size => Product.not_deleted.count\n    end\n  end\nand it counts perfect, everything perfect but it shows my facets splited on words, so facet \"250 kVt\" is now 2 facets \"250\" & \"kVt\"\nPlease, help me!\n. name field in mapping is name of the product, not related to facets.  I need to define facets dynamicly from darabase. so I get facets array (other model object from database) and place it this way as I wrote:\n\n  facets.each do |f|\n    facet f.name do\n      terms f.name.to_sym, :size => Product.not_deleted.count\n    end\n  end\n\nRight jnow I update elasticsearch to 18.2 and now it stopws giving me any facets from nested document. What should I do?\n. Sorry, I fugure everything out already, it was my silly mistake. Everything works great right now. Thanks!\n. Sorry, have no time to chek this again. I guess will be working on that project after new year celebration. Will post how it goes.\n. ",
    "xdmx": "Hi Karmi, what about multi_fields under Tire that needs to be both analyzed and not?\nI've tried something like this (plus many others combinations):\nindexes :tags, :type => 'multi_field', :index_name => 'tag', :fields => {\n  :tag       => { :type => 'string', :analyzer => 'standard' },\n  :untouched => { :type => 'string', :analyzer => 'not_analyzed', :include_in_all => false }\n}\nBut then after I index everything I don't get (I check under es-head) any tag field, just tags... the strange behavior is also that after I add that line also \nindexes :languages, :type => 'string', :index_name => 'language', :index => 'not_analyzed', :include_in_all => false\n(is a string array, like ['en', 'de', 'fr']) stops to work and it just shows languages and not language anymore\nbut if I use \nindexes :tags, :type => 'string', :index_name => 'tag', :include_in_all => false\nthen they both work\nBut unfortunately I need to analyze the tags in order to do searches on them, but show the whole tag on facets and not just tokens\nThen how make the facet call on the untouched one?\nfacet('tags') { terms 'tag.untouched' } ?\nThanks!\n. mmm..I try to give you an example :)\n```\nclass Post < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nmapping do\nindexes :languages, :type => 'string', :index_name => 'language', :index => 'not_analyzed', :include_in_all => false\nindexes :tags, :type => 'multi_field', :fields => { :tags      => { :type => 'string', :index_name => 'tag', :analyzer => 'standard' },\n                            :untouched => { :type => 'string', :analyzer => 'not_analyzed', :include_in_all => false } }\n  end\ndef to_indexed_json\n{:languages => ['it', 'en'], :tags => ['First tag', 'Second tag']}.to_json\n  end\nend\nPost.search do\n  query { string 'first' }\n  facet('tags')   { terms \"tags\" }\nend # => facets['tags']['total'] => 259743 ## ok, all the various tokens: \"first\", \"second\", \"tag\".... 86581 records * 3\nPost.search do\n  query { string 'first' }\n  facet('tags')   { terms \"tags.untouched\" }\nend # => facets['tags']['total'] => 0 ## but it should be 86581 for \"First tag\"!\n```\nI've tried also to do some searches:\n```\nPost.search do\n  query { string 'tag:first' }\nend # => total => 0 ## what? ignored the index_name?\nPost.search do\n  query { string 'tags:first' }\nend # => total => 86581 ## ok\nPost.search do\n  query { string 'language:it' }\nend # => total => 0 # seems ignored too\nPost.search do\n  query { string 'languages:it' }\nend # => total => 0 # ok, that's strange too\nPost.search do\n  query { string 'tags.tags:first' }\nend # => total => 0 ## mmmmmmmm\nPost.search do\n  query { string 'tags.tag:first' }\nend # => total => 0 ## mmmmmmmm\n```\nI guess that \"tag\" and \"language\" should be recognized as there is the index_name.... looking the index from es-head there is just languages and tags, nothing else\n. Just an update, it was actually my fault, I wrote :analyzer => 'not_analyzed', instead of :index => 'not_analyzed'... and that created just a very weird behavior. I think it would be better to ignore that not supported cases, in my project I had duplicated indexes for array values (eg. language and languages), without be able to search in any of them\n. Hi Karmi, yes, the endless page is a useful solution, but not for all the scenario. For sites like twitter, facebook, etc is perfect, but think about an ecommerce, comparator, etc where you have a lot of items, endless pagination is bad for the user (that what they told us, we tested a/b) because they click on an item and when they go back with the browser they'll need to scroll everything again.\nThe query takes in consideration various values so already filter something out, but generic one can be done (like all the items under a given price), and that way the risk is showing page #123456 which will slow down a lot when clicked\n. Hi Karmi, yes, it's an ActiveRecord method, but I think that also without it that part should be managed better, without throwing an error when a single record is not found, but it should ignore it and load the others... I guess it's possible to do it also without using ActiveRecord\n. ",
    "christianhellsten": "This issue seems to pop up every now and then. Usually it goes away after deleting the index and reindexing: \n- I have checked that the field is not analyzed:\nindexes :category, :type => 'string', :index => :not_analyzed\n- I have double checked the mapping and \":index => :not_analyzed\" is missing\nhttp://localhost:9200/products/_mapping =\n  {\"products\":{\"product\":{\"properties\":{\"category\":{\"type\":\"string\"}}}}\n- I have checked tire.mapping and tire.index.mapping:\n```\n  Product.tire.mapping\n  => {:category=>{:index=>:not_analyzed, :type=>\"string\"}}\nProduct.tire.index.mapping\n  => {\"product\"=>{\"category\"=>{\"type\"=>\"string\"}}}\n  ```\n- Index changes between console sessions\n```\n  # First time\n  $b rails console      \n\nProduct.tire.index.mapping\n  => {\"product\"=>{\"properties\"=>{\"category\"=>{\"type\"=>\"string\", \"index\"=>\"not_analyzed\"}}}}\nexit\n\n# Immediately after\n  $b rails console      \n# Product.tire.index.mapping\n  => {\"product\"=>{\"properties\"=>{\"category\"=>{\"type\"=>\"string\"}}}}\n  ```\n- This is how the facet is defined:\nfacet 'category' do\n    terms :category, :global => true\n  end\n. OK, I put a debug statement in Tire::Model::Indexing::ClassMethods#create_elasticsearch_index. Found out that this Rake task caused the issue:\nnamespace :search do\n  desc \"Reindex\"\n  task :reindex => :environment do\n    Tire.index('products').delete\n    Product.import :per_page => 1000\n  end\nend\nIf I run this Rake task, the mapping is not what I have defined in the model. I checked http://localhost:9200/products/_mapping.\nI noticed Tire::Model::Indexing::ClassMethods#create_elasticsearch_index is run only once when Tire.index('products').delete is called. This means the mapping is empty when the import statement is run and I guess ElasticSearch's default behavior is to create the mapping from whatever data is in the bulk import.\nIs there something we can do to avoid this?\n. Thanks. I understand the issue now and how to resolve it.\nI'm just wondering if this is something that could be improved on? I don't know if it's a good idea, but Tire could, for example, detect if the Tire mapping is different from the one defined in ElasticSearch (http://localhost:9200/xxx/_mapping) and warn the user.\n. OK, that explains it. I was following the README instructions, which still recommend using Article.search, not Article.tire.search:\nArticle.search 'love'\nI guess I should have read the part about Article.tire.search...\n. Yes, thanks for the quick reply. Closing this issue, since it's well documented in the README.\n. I agree with te-chris. I would prefer if accessing missing attributes raised an error instead of failing silently. Similar to:\nattributes.fetch(method_name.to_sym)\nThere's probably a good reason for not raising errors too, and I can use a proxy object as described here:\nhttps://github.com/karmi/tire/wiki/Data-storage-in-Tire-and-wrapping-the-results\n. ",
    "sbeckeriv": "If the type is not forced then the indexing is arbitrary. This is fine until the coder creates a mapping.  Rages, terms, geo features and other features seem to fail to work only because data is indexed in an unknown type. \nI strongly feel that that raising with out a type is the correct thing to do. If not the default of type would be better if it was you_did_not_set_a_type in stead of document. Still confusing but more descriptive. \nThe import method could be rewritten to require an explicit type.\nimport(:person,[{},{},{}....]) removing the need for the guessing again.\nCreating better docs only work if people read them. I skimmed the examples and got started. The code base is very easy to pick up and use. This one part was the only major gotcha I have had. Some people, I am included, read code instead of docs. I did not find the wiki page on how send unsupported searches until after I read the code to figure it out. \nIts better to not allow the user to incorrectly index something then to stay \"elastic\". \nThank you,\nBecker\n. Looks good. Thanks. Everything else is still rockin.\n. ",
    "roccoblues": "It also fails with a clean model in the same app. Here's my model:\nclass TireTest < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nend\ntestcase in the console:\n:001 > t = TireTest.new\n => #<TireTest id: nil, name: nil, created_at: nil, updated_at: nil> \n:002 > t.name = 'test'\n => \"test\" \n:003 > t.save\n => true \n:004 > t.destroy\n => #<TireTest id: 2, name: \"test\", created_at: \"2011-08-16 13:12:34\", updated_at: \"2011-08-16 13:12:34\"> \n:005 > t = Tiretest.find(2)\n => #<TireTest id: 2, name: \"test\", created_at: \"2011-08-16 13:12:34\", updated_at: \"2011-08-16 13:12:34\">\nthe log:\nSQL (0.1ms)  BEGIN\nSQL (1.0ms)  SHOW TABLES\nSQL (1.8ms)  describe `tire_tests`\nAREL (0.3ms)  INSERT INTO `tire_tests` (`name`, `created_at`, `updated_at`) VALUES ('test', '2011-08-16 13:12:34', '2011-08-16 13:12:34')\nSQL (2.2ms)  COMMIT\nSQL (0.1ms)  BEGIN\nSQL (0.2ms)  COMMIT\nTireTest Load (0.3ms)  SELECT SQL_NO_CACHE `tire_tests`.* FROM `tire_tests` WHERE `tire_tests`.`id` = 2 LIMIT 1\nsame test without include Tire::Model::Callbacks\nSQL (0.1ms)  BEGIN\nSQL (0.8ms)  SHOW TABLES\nSQL (1.9ms)  describe `tire_tests`\nAREL (0.3ms)  INSERT INTO `tire_tests` (`name`, `created_at`, `updated_at`) VALUES ('test', '2011-08-16 13:18:37', '2011-08-16 13:18:37')\nSQL (0.7ms)  COMMIT\nSQL (0.1ms)  BEGIN\nAREL (0.3ms)  DELETE FROM `tire_tests` WHERE `tire_tests`.`id` = 4\nSQL (1.5ms)  COMMIT\nI'll try to remove some gems and see if that fixes it.\n. I have the feeling that I'll end up with a nice facepalm. :) Anyway here's a test app where I can reproduce the behavior: https://github.com/roccoblues/TireTest\n. that's getting really strange. I'll try to poke into it with the debugger today. Thanks for you help so far!\n. one more question: do you have an elasticsearch server running during your tests?\n. I'm really not making this up. ;)\n```\n\nt = TestModel.new :name => 'TEST'\n => # \nt.save\n  SQL (0.2ms)  BEGIN\n  SQL (0.4ms)  SHOW TABLES\n  SQL (2.1ms)  describe test_models\n  AREL (0.3ms)  INSERT INTO test_models (name, created_at, updated_at) VALUES ('TEST', '2011-08-17 08:49:41', '2011-08-17 08:49:41')\n  SQL (0.7ms)  COMMIT\n => true \nt.delete\n  AREL (0.8ms)  DELETE FROM test_models WHERE test_models.id = 10\n => # \nTestModel.elasticsearch_index.delete\n => true \nt = TestModel.new :name => 'TEST'\n => # \nt.save\n  SQL (0.1ms)  BEGIN\n  AREL (0.2ms)  INSERT INTO test_models (name, created_at, updated_at) VALUES ('TEST', '2011-08-17 08:50:09', '2011-08-17 08:50:09')\n  SQL (4.9ms)  COMMIT\n => true \nt.destroy\n  SQL (0.1ms)  BEGIN\n  SQL (0.2ms)  COMMIT\n => # \nquit\n``\n. If I comment out this code inTire::Model::Callbacks`\n\nif base.respond_to?(:before_destroy) && !base.instance_methods.include?('destroyed?')\n      base.class_eval do\n        before_destroy  { @destroyed = true }\n        def destroyed?; !!@destroyed; end\n      end\n    end\nit works. Somehow that seems to conflict with ActiveRecord. I'll dig deeper.\n. oh boy I am already deeper into rails code than I ever wanted. Shouldn't this return true for ActiveRecord models?\n```\n\nTestModel.instance_methods.include?('destroyed?')\n    => false\n```\n. \n",
    "erickreutz": "watching...\n. ",
    "jfredett": "Also watching.\n. Pretty sure this would fix index creation w/ bonsai (with a little bit of supporting code) -- I'll see if I can write up some integration tests when I get it all working w/ bonsai. \nOpen source at work!\n. ",
    "gjb83": "Oops, apologies!\n. ",
    "benzheren": "Under 0.1.16, the above code is able to retrieve results back from backend. With the new 0.20.0, the returned results are always empty.\n. Karel, this is the gist: https://gist.github.com/1169884. \nBy just updating tire version in my Gemfile to 0.2.0, this code does not work and return empty results. If I downgrade to 0.16.0, it works again. Is it because of some other dependency issue? If so, I can post my Gemfile as well. \nThanks!\n. Okay, the problem is getting interesting. The curl is the same, but today when I tried out 0.2.0, it actually gave me error message like this:\nNameError (uninitialized constant Webpage):\n  app/controllers/search_controller.rb:12:in `ajax'\nThe Webpage is a type we defined in our elasticsearch. It seems that the 0.2.0 cannot recognize it correctly while the 0.16.0 has no problem with it.\n. I have not figured out the problem yet and I am using 0.16.0 for now. But I just saw a new commit, which could be the cause of the problem: https://github.com/karmi/tire/commit/a0bbbf030e37e03fe882c6a92c265957ad3f2383\n. Karel, I think several people have raised this issue. Is it possible to reopen it? Thanks!\n. @karmi, I will try out the latest code and let you know. Thanks!\n. The latest release works. However, the only thing I noticed that there was some change to the search response between 0.1.16 and 0.3.7. In some old codes I wrote I could modify the search result item directly, but now I have to do a to_hash to make it work.\n. +1 for this feature, it will definitely helps the performance of the search itself.\n. I am using 0.3.7. I put my code as a gist: https://gist.github.com/1354764. \n. Thanks for pointing it out. I started with 0.1.6, does the latest code fully support our need? What do u mean by more semantic DSL? We have complicated query like text, text phrase, on top of filtering. If the semantic DSL you mention supports it, I would love to use it.\n. ",
    "vijedi": "I'm new to tire so I'm not positive this observation is entirely relevant, I have a class called Profile in my rails application. Initially I was indexing it with the type \"profiles\". When tire went to create the search result, it gave me an error stating \"uninitalized constant Profiles.\" Indexing this data with the type \"profile\" solved this issue for me.\n. ",
    "raff": "I have a slightly different use case that may help explain the problem:\nIn elasticsearch I have a collection of documents with type='app' (application). In Rails I created a Tire model named Case.\nWhat I wanted to do was to configure the model to search with type='app' when accessing Case (similar to what you can do with ActiveRecord by explicitially setting the table name - table_name 'mytablename'). Up to 0.1.16 I was able to do that by redefining the class method document_type:\ndef self.document_type\n  'app'\nend\nWith this, the search was performed on /{index}/app and the returned object were instance of Case.\nAfter updating tire (now at 0.3.3) this stopped working but I found both getter and setter for document_type so I changed my code to:\nclass Case : ...\nindex_name 'myindex'\n  document_type 'app'\nend\nThis causes the search to be performed but then I get the error 'uninitialized constant App', that makes me think that this new version of tire simply tries to recreate the model only looking at the 'table name' (the elasticsearch 'type') instead of looking at the model configuration.\nNote that while I also updated all my Rails stuff (ActiveRecord, ActiveModel, etc.) this doesn't seem to be the cause of the problem since downgrading tire to 0.1.16 makes the problem go away.\n. I did a quick test and it works! I'll do more testing later. Thanks!!!!\n. ",
    "signpost": "Yea, this is what I wound up doing anyways.  I made the assumption (incorrectly) that it used your mapping to dictate some of the serialization unless you used to_indexed_json.  Your explanation though makes a lot of sense and cleared things up for me.  Keep up the good work!\n. ",
    "jpsilvashy": "hmm.. I'm still having the issue, I think I should switch back to will_paginate from Kaminari, which pagination gem do you prefer? I should probably stick with what you know works. This maybe a non-issue all together.\n. ",
    "changa": "Hi (and sorry for the lag) !\nI guess it would be fine as options to search.\nConcerning search_type, you're right, it certainly could prove useful.\n(And BTW, thanks a lot for this gem, it rocks :-)\n. I'm having the same problem, but the error actually is 'rescue in head': undefined method 'http_body' for #<Errno::ECONNREFUSED: Connection refused - connect(2)>, triggered from https://github.com/karmi/tire/blob/master/lib/tire/http/client.rb#L36\n. @karmi : Yup, It's fine now! (Sorry for taking such a friggin' long time to answer.)\n. ",
    "olivere": "It'd be worthwhile when you need dependent: :delete_all semantics, e.g. you have a product catalog entity in DB and its products in ES-only. When you remove the product catalog, the products need to be purged from ES. Does that make up a use case?\nRegarding the interface, remove looks good. Does delete_by_query in ES accept the same parameters as a standard query? Have to check that. We could reuse the Tire query interface then, couldn't we?\n. Right, I really have to check performance impact of destructive actions. I could live with low-priority removal in a background task--as long as the root object is gone (the product catalog in the above sample), it's no big deal.\n. Maybe it'd be better to do some \"index alias juggling\" instead of using delete_by_query. You already mention that in your README.\nWhich brings up the next issue which is how to handle index aliasing with Tire. Do you have any plans and/or thoughts about this?\nBefore going off-topic: Do you think this is the right place to discuss this, or would it be time to start a mailing list?\n. Thanks for the feedback.\nI like your API for aliases (and templates). It's better to use Tire.index('...').aliases { ... } or even Tire.index('...') { aliases { ... } } than my idea with Tire.index('...').add_alias '...'. That seems more consistent to me regarding the rest of the Tire API. \nI didn't think about templates yet, i.e. I set them up outside of Tire. Of course, it would be great to have support for that in Tire, too.\nRegarding the dynamic index names:\nThe first case--when you don't need to pass instances to the index name method--was relatively easy to implement. It will nicely put the documents into the right index.\nAs you said, the second case is more tricky. There are two issues I considered: Indexing and searching. Indexing looks up the index name at runtime (if it is \"dynamic\", i.e. a Proc). Search (in the dynamic case) only works if you pass the index name via the :index option. Certainly not very elegant... and one of the reasons I said this is just a proposal. Maybe you guys have a better idea.\nUnfortunately, it's the second case I need for one of my apps... :-)\n. Thank you for your perspective, @floere.\nLast things first :-)  I apologize if my tone sounds rude or like I want you guys to implement exactly what I need for my projects. That's certainly not my intent. I made a proposal of something I found to be of general interest. You might disagree, propose a different solution, or don't care. I'm fine with that, it's just the way it is. :-)\nHaving said that, here's my train of thoughts for why this might be of general interest:\nSuppose you have a data model of catalogs, products, and groups. Catalogs have products, and groups have access to certain catalogs, i.e. not all groups have access to the same set of catalogs. You want groups (of users) to search for products. So you build an index on the products and filter by the catalogs by the group. There we are...\nNow catalogs come and go, so you need to remove some products from the index. You could use delete_by_query, but that probably has a negative impact on performance (I didn't test that personally, but found at least one comment from Shay to refrain from using delete_by_query on large indexes). \nSo, how can we combine fast search with simple and fast management of catalogs? My solution was to create one index per catalog and use aliases to take care of the (group) views. Creating a new catalog means creating an index and adding it to the right aliases. Removing a catalog simply means dropping the index. Searching as a group basically means finding the right index name--I don't even have to bother about filtering.\nIf you think this might be a valid approach/use case, you have to figure out a way to put products into the right index. Hence the approach to evaluate the index name at runtime, e.g. use a unique catalog identifier to build the index name.\nI'm not sure about it but I guess there are similar data models where this approach could prove to be beneficial. You decide :-)\nIndex name\nUsing a block for the index name instead of a lambda looks better to me.\nAliases\nRegarding the aliases syntax, I'm not sure. Code like this\naliases = Tire.index('posts').aliases\naliases.delete 'posts-2011-01'\naliases << 'posts-2011-03'\nis more Ruby-ish, while\nTire.index('posts').aliases do\n  remove 'posts-2011-01'\n  add    'posts-2011-03'\nend\nis more like the rest of the Tire syntax, e.g.\nTire.index 'articles' do\n  delete if exists?\n  create\n  store ...\n  refresh\nend\nSearch method\nYou guys are right. Passing the index name to the search method already exists. I suppose people wrap the search method with some custom logic anyway, so why bother.\n. @redbeard, I appreciate your suggestions. Thank you.\nI assume marking documents with active: false shouldn't be as expensive as deleting them right away, am I right? To get rid of the inactive documents I could rebuild the whole thing, or defer delete_by_queryand optimize to times of low work load. Hmm... dropping and creating indices is very fast from my experience, so I thought I could get the \"free lunch\" ;-)\nI guess I have to do much more testing to get a better idea of the whole picture. The \"index juggling\" solution is working quite okay right now, but I didn't get into the millions of documents. We'll see.\n. Fantastic. Just let me know if and how I can help.\n. @floere, thanks for clearing this up. I mislike negative vibes. Happy now :-)\nIndex API\nLet me add some remarks regarding the API.\nI'd like to remind all of us that the Aliases API is more than just adding or removing a single index to an alias (Aliases API). You can add and remove several aliases in one batch, use aliases as filtered views, and add routing. We should take this into account when fixing the final API.\nLet me also express my views of the general API discussion:\nWhile I generally like mimicking a driver/wrapper gem the Ruby-ish way, it might lead to problems. Let me explain:\naliases = Tire.index('posts').aliases\naliases.delete 'posts-2011-01'\naliases << 'posts-2011-03'\nThis seems very straightforward at first sight. However, delete and << do communicate with ES behind the scenes. While this might be debatable in the example above, I tend to be explicit when something's going on behind the scenes (in this case: communicating with ES). If you don't make this very clear, people call your code without thought and think that something very lightweight is going on (like adding an element to an array). I agree, this is a very fine line. But @karmi's approach of using a Tire.index(...) { ...} block really does help to make this very clear: You are doing something in the context of communicating with an Elasticsearch system. So beware! ;-)\nFurthermore, how do you expect Ruby style with the additional Aliases API features. While adding in a batch might be\naliases << ['posts-2011-01', 'posts-2011-02', ...]\nI do not know of a good Ruby idiom to both add and delete in one batch. Same for filtering and routing.\nLong story short: Favour the block syntax (while you might still enable users to use the more Ruby-ish stuff).\nPerformance\nTo be honest, I don't know how performance will be with e.g. 1000 indexes added to one alias. If there are issues, they are related to Elasticsearch, not Tire. But you're right, I should check this, although I expect the number of views to be in the tenths.\nWhen it comes to drivers/wrappers, I like them to be transparent to the underlying technology (Tire -> ES). If it's easy to prevent havoc at the driver level, do so. But in the end, people will always find a way to shoot themselves into the foot. There's just no way around that. ;-)\n. From the ES guides:\n\nAPIs in elasticsearch accept an index name when working against a specific index, and several indices when applicable. \nThe index aliases API allow to alias an index with a name, with all APIs automatically converting the alias name to the\nactual index name. An alias can also be mapped to more than one index, and when specifying it, the alias will\nautomatically expand to the aliases indices. An alias can also be associated with a filter that will automatically be applied\nwhen searching, and a routing values.\n\nor (for @floere): RTFM ;-)\n. It all started with a feature request... now: semantics and Ruby subtleties... OMG what have I done... ;-)   (no really, great input from anyone)\n. Sorry for the long delay... I've been moving and spend some days off.\nFirst of all, thanks for the commit. I'm with you on @blog.posts.search 'something' and I also like the block-passing to index_name. However, I think we're still not there with instance-based persistence and alias management, am I wrong? Or am I just too dumb to figure out how to get Tire to decide which index is used for storage on create/update time.\nSo short question: Are you still planning to add features for handling aliases and store entities on instance-scope? Or, if it's already possible, provide a short example of how to achieve that?\n. Thanks @karmi.\nI can handle the aliasing stuff, perhaps even submit a pull request for you to review (based on what we discussed in this issue). The only showstopper I've got is the instance-based persistence.\n. @karmi: Sure, I think I know that feeling when you're sure something is definitely doable but it doesn't seem right. Better take your time and do it \"the right way\" (tm). It's not really time-critical right now, as I'll can focus on other parts of the application. Just let me know if I can help somehow, write tests, whatever.\n. @karmi: Any news on this?\n. Wonderf... oops, sorry ;-)\n. I still rely on my aliases branch which works okay but it's a bit off-track of the original tire repository. I prefer to be on the maintainers master branch because everything else is just a PITA after a while. \nKeep on the good work.\n. Just a thought experiment: We agree that searching can be done with something like @blog.posts.search '...', so we're only left with designing the persistence stuff. Then again, if all we need to do is to choose index/index name on save, isn't that (a very simple form of) sharding?\nIf it is, it may be an idea to have a generator/facade which simply returns the static index name in the general case but may be overridden to return something more complicated (e.g. based on the instance). That idea could take us even further without complicating the code too much or drifting away into edge cases. What do you think?\n. So I can now remove that tire_aliases.rb from my projects that'll monkey-patch Tire to include add_alias, remove_alias, and aliaes methods? Lovely. Thanks.\n. @rathgar, I will try to explain what we did to segment our big index into smaller ones. It's been quite a while since we did this, but IIRC it was quite easy and robust once we got it working.\nWe used to have a big products index which is now indexed by tenant, like products-1, products-2 and so on. So our \"dynamic index name\" is made up by products and the ID of the tenant. We like this not only to automatically ensure tenants only see their data, but also because it's very easy to drop a tenant: Remove some rows from your data store and drop the ES index. No need for delete_by_query...\nOn creation time, we made sure that an ES template is applied to all products indices, so we automatically get the same schema. When importing data we \"dynamically\" determine the right index name based on the products prefix and the tenant ID (saved in the User model). Once you know the index name, you can limit all further ES actions (search, bulk import calls etc.) on that index name. \nWhen searching, we know the users' tenant, so we can also build the right index name and pass it around to the various Tire methods. IIRC we had a current_user.search_products(...) convenience method that did all the right things. \nYou can drive this even further, e.g. when some users get super powers and are allowed to view two or more tenants. This is where Aliases come into play... when users get access to a new tenant via some UserTenant model, just set up an alias as you create the UserTenant and drop the alias if you destroy the UserTenant model.\nI hope you get the idea... feel free to ask again if it's too confusing. I apologize in advance...\n. (I hope it's okay for @karmi to use this issue as a kind of support thread... :-))\nTo your questions:\nSearching: I'm pretty sure that you can use either alias or index name (correct me someone if I'm wrong). IIRC the alias name will just be expanded to the aliased index names. I'm not sure about the limits of this approach (it might not be a good idea to e.g. have millions of tenants); in our case, the number of tenants is quite manageable.\nIndexing: We perform background indexing via the Java-API so we are not dependent on Tire/ActiveRecord callbacks. However, I'm pretty sure you can just disable the callbacks and manage the indexing process yourself. After all, Tire does just that in Tire::Callbacks. @karmi wrote about it in the README (lookup the section on CouchDB/RabbitMQ), so I suppose it's a common use case.\nReindexing: Did you take a look into Tire's reindex method. I'm not sure if you can rename an existing index. But you can achieve this sort of things with aliases; there's an example in the ES Index Aliases API.\n. Yes, searching should be fine.\nRegarding the indexing, you cannot index against an alias (as you found out). I'd go for: Disable the Tire callbacks and manage the indexing yourself, sort of like shown in the README. Shouldn't be too hard. That'd have least impact and maximizes your chances it will work while Tire gets updated. Maybe you can find an example in the integration tests, or @karmi can give you a hint as to what is a future-proof way of doing so.\n. Thanks for your insights, @karmi. Really appreciate that.\nIIRC we did something similar to your articles method in the Account class... something along the lines of:\n``` ruby\nclass Product\n  # ...\nindex_name { Thread.current[:product_index_name] }\ndef self.with_index(name, &block)\n    old_name = Thread.current[:product_index_name]\n    Thread.current[:product_index_name] = name\n    yield block\n    Thread.current[:product_index_name] = old_name\n  end\nend\n```\n... which leads to ...\nruby\nProduct.with_index(index_name) do\n  # ...\nend\nIf it works, I claim copyright... Otherwise, well... ;-)\n. Copyright, yay! ;-)\nMaybe its over-engineered, but I'd like to be on the safe side. This would work fine even in a multi-threaded web server with JRuby, wouldn't it?\n. @farukca The idea is basically to wrap access to Elasticsearch indices and make sure that Thread.current[:current_account_id] is set before index_name is evaluated. One way of doing this is to use it as outlined in https://github.com/karmi/tire/pull/92#issuecomment-6930576.\nI generally raise an error to ensure I didn't forget to set the current account, like so:\nindex_name { Thread.current[:current_account_id] or raise \"Wrap search with Customer.with_index(...)\" }\nThe stack trace might give you an idea where you forgot to add the with_index wrapper.\nBTW: You can of course always use Tire.search(...) and friends directly and provide the index name (manually).\nHope that helped...\n. Sure, will have a look at simplifications (still on my journey to Ruby-zen). The implementation has been \"borrowed\" from several other Rails plugins and my own understanding of the Notifications API.\nSome questions still:\na) The log subscriber integration test copies a lot from the AR tests (schema setup etc.). Do you prefer to separate test files on the scenario (Notifications,  LogSubscriber etc.), or do you accept adding it to the AR tests?\nb) I wanted to keep Tire unaware of Rails, therefore only introduce the Railtie and the rest of the instrumentation in a Rails environment. As a result, I wrapped the perform method in a method chain, which is part of what looks complicated (I guess). I could simplify this and get rid of the method chain, but that probably means that the Notification API gets into Tire itself. Any thoughts on this?\nc) This patch applies to Rails 3.1. Still have to test it with 3.0 and 2.x.\n. Just looked up the sample in Jose Valims book and on Railscasts: They're more lightweight because they focus on consuming events. They don't append e.g. the search runtime to the Rails action log (as shown here). LogSubscriber and ControllerRuntime are basically copied from ActiveRecord. Looking at other gems, I found this to be pretty standard, but I might be wrong. I couldn't find an \"official\" guide for this. Do you have any experts you might ask?\n. Sure. Makes sense... I have all sorts of ideas/features that are probably wrong in tire(-core).\n. ",
    "abecciu": "I'm closing this in favor of pull request #92.\n. +1 for dynamic index names. I desperately need that feature. Index aliases would be a nice addition too.\n. ",
    "redbeard": "@olivere, a small comment regarding the solution you suggested for the performance hit of the delete_by_query.\nThis is not disimilar to the SQL world in which a delete is a costly operation (blocks need to be moved / de-fragmented). The solution is usually to have an active flag on an entity which you can then easily filter by. Set it to false for a 'deletion'. \nThis should not be not as costly on delete as the space for the flag is already on the disk in the index. \nSeems to be a simpler solution rather than having to maintain multiple indexes, or am I misunderstanding your problem?\n. Thanks for merging!\nYour refactoring works well in my app as far as the global models prefix is concerned.\nI haven't tried the per-model prefix as of yet.\nAs you're so responsive, I expect more patches to be forthcoming as I continue using Tire :)\n. If we had distinction between 'debug' and 'trace' levels, I would say do full print outs at 'trace' and 50 lines per request maximum for 'debug' level.\nRight now, with large documents (think large Wikipedia articles with associated metadata) one can't use debug mode unless one sets _source = disabled in the index mapping as a controller request will time out even for 20 documents returned.\n. Perhaps a discrete 'log response body' configuration option for debug mode? \nAlternatively, a time-based limit on the response body logging. Something obviously smells when the logging of a request takes over 2 minutes! \nBTW, I believe logging the json.to_s wouldn't take as long -- the .each_line block is the time consuming operation.\n. Great idea, I'll give it a try!\nOn 08/12/2011, at 6:34, Karel Minarik\nreply@reply.github.com\nwrote:\n\n@redbeard: Tal, it just occured to me -- why don't you write your own custom logger, which limits the output, and pass it to Tire.config { logger MyCustomClass }? Is that a viable solution?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/155#issuecomment-3052154\n. \n",
    "rathgar": "blows the dust off...\nThanks for the aliases stuff @karmi, that's really handy.\nWe do however, have a use case that requires us to dynamically name the indices. We have many customers with 10k to millions of documents which we currently have all in one index for each type. We'd like to be able to rebuild a customer's index from time-to-time for various reasons and of course when a customer leaves it's easier and safer to just drop their indices. Did you get far @olivere?\nWe can create aliases with filters to just their documents but that doesn't allow us to rebuild/drop individual customers' indices.\nWhen any document action takes place the index_name would need to be calculated within the context of the document. I've tried overriding the instance method index_name but it seems that it is currently called on the class.\nI'm happy to help move this 'feature' forward if anyone is able to give some pointers as to where best to start. It would, obviously need to function without cost to the current method. \nIf no one else would be interested in using this, I'll monkey patch and gist/fork but will probably still need some guidance on the best way forward.\n. Thanks for your feedback @olivere, that's very useful and seems to be almost exactly the same scenario as ours. I'd be grateful if you would be able to clarify a couple of points. I should have mentioned before that we're using ActiveRecord (Rails v3.2.6)\nWe're also wrapping search so I'm happy that we can do that against the appropriate aliases. However, if we drop our current indices and import each customer's documents into its own indices (one for each type):\n- Is it correct to assume that during ActiveRecord updates and deletions, Tire will still look for a products index and that we would need to create that as an alias pointing to all of the products-1 ... products-n indices?\n- Similarly, when new records are created, how will Tire know which client index to add them to?\nThese are the reasons I assumed I needed to coax index_name to be 'dynamic'. We couldn't do future index additions via bulk import as the models get updated and created very frequently and new/changed objects need to appear correctly in search immediately.\nSorry if any of this is already covered elsewhere.\n. Many thanks for the input, I've spent a fair bit of time digging and patch testing but still can't seem to achieve this cleanly. \nI set up the indices and aliases by hand so they are in place.\n/_aliases/ returns: { \"s4-events\" : { \"aliases\" : { \"events\" : { } } }, \"s3-events\" : { \"aliases\" : { \"events\" : { } } } }\nIt works exactly as you'd expect for search. I also hoped that Tire would be able to update the document via the alias but this is not the case. Tire doesn't update the index and some digging around with curl reveals what es thinks of this:\n{ \"error\" : \"ElasticSearchIllegalArgumentException[Alias [events] has more than one indices\n     associated with it [[s4-events, s3-events]], can't execute a single index op]\",\n   \"status\" : 400 } \nwhich, come to think of it, is perfectly understandable - you could easily have the same document in multiple indices.\nThis means that es needs to be given the actual index name for create, update & delete actions on the index. Search of course is the easy bit, you can pass that in within custom search wrapper methods on the models.\nThere are two routes I can think of:\n1. write my own callbacks to handle the (re)indexing;\n2. get Tire to understand how the indices are named and let it do all the heavy lifting.\nCallbacks\nOn the face of it, this seems  the easy route. However, seeing as es needs to know the name of the index, it looks like you would need to rewrite Tire's update_index to get this to work. Replicating good, working code never feels right.\nindex_name\nThe smoothest solution I can think of is if models could define an instance level index_name. This would override the default instance method defined in lib/tire/model/naming.rb which is just a proxy to the class level method. So far so good.\nFrom what I can tell Tire always calls the class level version of this method. If it were to always call the instance level method this would result in the same response to all requests unless the model overrode it, resulting in custom index name.\nI'm not sure if doing this would conflict with what @karmi has in mind for Tire, I'm open to direction and/or smack-down :-P\n. ",
    "rmm5t": "+1 to @olivere's approach. Given the current Tire API, this looks to be the most elegant solution that both supports multi-tenant indexes and is thread-safe.\n. ",
    "farukca": "@olivere I have customer model which is seperated with current_account_id; and my customer model's index name like;\nruby\nindex_name { \"customers-#{Thread.current[:current_account_id]}\" }\nWhen I add new record it is working perfect, but how can I populate the index for existing records ?\nThanks, best regards...\n. ",
    "lennartkoopmann": "How am I suspected to access the created_at_time method then? The README is not very helpful with this.\n. Well I tried that, but msg.created_at_time is still nil and not called.\ndef to_indexed_json\n  to_json :methods => [ 'created_at_time' ]\nend\nAs an improvement for the README I think it could be more structured. It is good that it's a real text to make people completely read it before starting but it makes looking up things hard.\n. ahhh, of course, I forgot that this will only work with messages imported via tire - Thanks.\nThe Problem is, that the messages get indexed by a whole other system and there are instance methods of the model that do dynamic stuff that can't be calculated when stored. Seems like I have to think about some stuff again - Thanks!\n. Thank you very much!\n. Thanks for your answer! I now have a problem with the escaping itself. Imagine I have this term I want to search for: foo-bar\nI have to escape the minus sign in it with a backslash. It somehow gets double escaped on its way through tire. This query string is sent to ElasticSearch: (using term(:foo, 'foo\\-bar'))\n{\"query\":{\"bool\":{\"must\":[{\"query_string\":{\"query\":\"message:*\"}},{\"term\":{\"foo\":\"foo\\\\-bar\"}}]}},\"sort\":[{\"created_at\":\"desc\"}],\"size\":100,\"from\":0}\nElasticSearch obviously does not like that and fails at parsing the query.\nI already played around with different Ruby string notations like with single and double quoted, double and single escaping etc.\nAm I doing something wrong? Thanks for any help!\nEDIT: The double slashes are required because it's JSON I guess. Still not sure how I should escape it then. :)\n. I have to escape it, because it is a special character of Lucene: http://lucene.apache.org/java/2_4_0/queryparsersyntax.html#Escaping\nDid you find your documents including 'foo-bar' with it? I don't get any results - Because of the special character I suppose.\n. But if I index two documents with a field host which is foo-bar and one which is foobar and term search for it, it will find the foobar but not foo-bar.\nEDIT: (Using two searches. One for foobar that finds the document, one with foo-bar that does not)\n. Sure! Here we go:\n```\nrequire 'rubygems'\nrequire 'tire'\n[\"foobar\", \"foo-bar\"].each do |what|\n  Tire.index 'escape-test' do\n    delete\n    store :host => what\n    refresh\n  end\ns = Tire.search 'escape-test' do\n    query { term(:host, what) }\n  end\nputs s.to_curl\nputs \"FOUND #{s.results.count} documents for #{what}\"\ns.results.each do |document|\n    p document\n  end\nend\n```\nThanks!\n. Ah! Thank you very much! :)\n. ",
    "leehambley": "I think so, as Array, and ActiveRecord::Relation both do? (it could easily defer to #to_a) At least I believe AR::Relation defers to #to_a already.\nIn the case that you agree, I'll gladly submit a patch, to the best of my ability.\n(also my apologies, I wrote this ticket in haste whilst trying to assist a colleague getting some code running, I should have written it much more clearly)\n. (Thanks, I agree in hindsight)\n. I agree with @ralph's position here, but only in the face of Tire being effectively a \"Rails\" interface to full text search. In Rails apps, as with most \"web applications\" I think it's quite normal that there's a bunch of things which shouldn't be searchable... and it's natural to expect something like should_be_indexed? \u2013 I expected at least that it would exist. (but we haven't had the need for me to explore that more thoroughly yet)\n. @karmi any thoughts on this, first-pass style-wise? Should I finish my work (trivial) and add should and should_not\n. Whoohoo, viel Spa\u00df beim Urlaub! Looking forward to hacking when you are\nback, much easier than GH issues! @karmi, we'll come back to you!\nOn 2 March 2012 09:13, Ralph von der Heyden <\nreply@reply.github.com\n\nwrote:\nHey, I will go on a 10 days vacation today, but I will get back to Lee\nlater. As we are in one city, we'll probably just meet and hack on this\ntogether.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/pull/211#issuecomment-4281840\n. https://github.com/karmi/tire/pull/423\n. Whoohoo, thanks :-) :+1: \n. \n",
    "flockonus": "Awesome feature!\nSuggestion for Tire::Model::Search.index_prefix 'test' to go to docs, super handy for tests\n. I understand your concern, our use-case is pretty different than usual since our Rails just populate E.S. and the whole interaction happen via an API.\nSeeing here, is amazingly simple =)\nCOMMAND = <<-COMMAND.gsub(/^    /, '')\n    curl -k -L -# -o elasticsearch-0.20.6.tar.gz \\\n      \"http://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.20.6.tar.gz\"\n    tar -zxf elasticsearch-0.20.6.tar.gz\n    rm  -f   elasticsearch-0.20.6.tar.gz\n    ./elasticsearch-0.20.6/bin/elasticsearch -p #{destination_root}/tmp/pids/elasticsearch.pid\n  COMMAND\nStill think that should be an .off() somewhere, so I am forking and will try coming with an answer =)\nMaybe as a separate note, could the HTTP calls happen async? As a fire and forget? In that case I wouldn't have to worry if it failed in development / test.\nI see you have Faraday, don't they enable that?\nEven if not for my particular problem, I guess implementing an async handler could benefit a performance for those who don't use e.s. as DB; example: http://stackoverflow.com/a/11373486/250019\n. My pull request is very naive and integration tests failed :P\nI guess it is because we don't have visibility of Tire instance inside RestClient (?)\nOverall idea is not to change anything on how the lib works, except that when it is about to perform the HTTP, it doesn't and just return a mock object.\n. Thanks a lot for the consideration on this issue, @karmi \nAbout the motivation on why, it might be a particular case, but the underlying reasoning is that I trust Elasticsearch server, and Tire, so there is no reason to burden the team with one more dependency, since this server don't query. I'd like to know if you have considerations on it =)\nThe mock client idea is awesome! I will try this as soon as I have some time and updates here, thanks =)\n. ",
    "michelson": "thanks for your replies,\ni can get the query to work with a filter but i got some problems with\nsorting\ni hve tried many ways but i get an error\nin the search\na = Cause.search do\n query  { string terms }\n filter :term, :user_id => \"1\"\n #sort   { by \"tribunal.exact\", 'desc' }\n #sort   { by \"user.exact\", 'desc' }\nend\na.results\nit return the collection but when i call a.results it throws an AR exeption\nActiveRecord::UnknownAttributeError: unknown attribute: caratulado\nalso if i try the sort methods above i get a curl  request fail,\nNoMethodError: undefined method `results' for false:FalseClass\ni have this mapping in the model\nmapping do\n    indexes :user_id,  :type => 'string',  :index    => :not_analyzed\n```\nindexes :user, :type => 'multi_field', :fields => {\n  :user => { :type => \"string\", :analyzer => \"snowball\" },\n  :\"user.exact\" => { :type => \"string\", :index => :not_analyzed }\n}\nindexes :tribunal, :type => 'multi_field', :fields => {\n  :tribunal => { :type => \"string\", :analyzer => \"snowball\" },\n  :\"tribunal.exact\" => { :type => \"string\", :index => :not_analyzed }\n}\nindexes :civil do\n  indexes :rol, :type => 'string'\n  indexes :caratulado ,  :type => 'string', :boost => 100\nend\n```\nend\nreally lost here\nregards\nAtte.\nMiguel Michelson Martinez\nwww.artenlinea.com\nOn Mon, Sep 12, 2011 at 5:36 AM, Vojt\u011bch H\u00fd\u017ea \nreply@reply.github.comwrote:\n\nHello,\nhere is example how can be multifield type used:\n```\nrequire 'rubygems'\nrequire 'tire'\nTire.configure { logger STDERR }\nTire.index(\"causes\").delete\nclass Cause\n include Tire::Model::Persistence\n include Tire::Model::Search\nmapping do\n   indexes :user, :type => 'multi_field', :fields => {\n     :user => { :type => \"string\", :analyzer => \"snowball\" },\n     :\"user.exact\" => { :type => \"string\", :index => :not_analyzed }\n   }\n   indexes :civil do\n     indexes :rol, :type => 'string'\n     indexes :tribunal,  :type => 'string', :boost => 100\n   end\n end\n# Only for this test\n  property :user\n property :civil\nend\nCause.create :user => \"first name\"\nCause.create :user => \"last name\"\nCause.index.refresh\nresponse = Cause.search do\n  query  { string 'name' }\n  filter :term, :\"user.exact\" => 'last name'\n  sort   { by \"user.exact\", 'desc' }\nend\np response.results\nreturns nothing because term filter is not analyzed\nresponse = Cause.search do\n  query  { string 'name' }\n  filter :term, :user => 'last name'\n  sort   { by \"user.exact\", 'desc' }\nend\np response.results\ncan't sort by analyzed property\nresponse = Cause.search do\n  query  { string 'name' }\n  filter :term, :user => 'last name'\n   sort   { by \"user\", 'desc' }\nend\n```\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/101#issuecomment-2068799\n. Hi Karel ,\n\ni ve created the gist,\nhttps://gist.github.com/875756b52b3af05511e9\nif you need more information about it just ask\nthanks for your help\nAtte.\nMiguel Michelson Martinez\nwww.artenlinea.com\nOn Tue, Sep 13, 2011 at 5:45 AM, Karel Minarik \nreply@reply.github.comwrote:\n\nHi,\nit is really hard to debug what's going on unless I can re-create your data\nand model. Could you at least gist the complete model? (<\nhttp://gist.github.com>)\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/101#issuecomment-2079591\n. \n",
    "jlecour": "Hi,\nBeing in the process of instrumenting some code that use Tire, I was looking for some internal instrumentation and found this PR.\nActiveSupport::Instrumentation is a really good basic framework for instrumentation, but you have to depend on ActiveSupport to use it. It might be quite overkill if you only want this and not the whole ActiveSupport code base.\nLooking at the gemspec, ActiveSupport is not a dependency of Tire, but in the lib/tire.rb file there are some require of parts of ActiveSupport. so I guess Tire does depend on ActiveSupport. Using ActiveSupport::Instrumentation is suddenly no longer a \"no go\".\n1. it might be wise to explicitly state the dependency in the gemspec\n2. what to you think of putting a simplified version (no alias method chain) of the instrumentation directly into Tire ?\nThanks\n. It is really strange.\nIf I use the gem package, it works, I don't have the waning.\nBut if I use the GitHub version of the gem, I still get the warning.\nI've triple-checked the source code each time.\n. here is what I have : \n```\n\u2192 cat Gemfile | grep tire\ngem 'tire', :github => 'karmi/tire'\n\u2192 cat Gemfile.lock | grep tire\n  remote: git://github.com/karmi/tire.git\n    tire (0.5.7)\n  tire!\n\u2192 bundle show tire\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/bundler/gems/tire-c35028539c33\n\u2192 cat /usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/bundler/gems/tire-c35028539c33/lib/tire/rubyext/ruby_1_8.rb\nrequire 'tire/rubyext/uri_escape' unless defined?(URI.encode_www_form_component) && defined?(URI.decode_www_form_component)\n```\nSo I guess I have the right source code in my bundle, since the content of that file matches the fb9b6fa commit.\nThen, with rake : \n\u2192 bin/rake routes\n/Users/jlecour/Projects/my_app/bin\n(in /Users/jlecour/Projects/my_app)\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/rack-1.4.5/lib/rack/backports/uri/common_18.rb:10: warning: already initialized constant TBLENCWWWCOMP_\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/rack-1.4.5/lib/rack/backports/uri/common_18.rb:16: warning: already initialized constant TBLDECWWWCOMP_\n(snipped routes)\nThe I use the gemified version of Tire : \n```\n\u2192 cat Gemfile | grep tire\ngem 'tire', '~> 0.5.7'\n\u2192 cat Gemfile.lock | grep tire\n    tire (0.5.7)\n  tire (~> 0.5.7)\n\u2192 bundle show tire\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/tire-0.5.7\n\u2192 cat /usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/tire-0.5.7/lib/tire/rubyext/ruby_1_8.rb\nrequire 'tire/rubyext/uri_escape' unless defined?(URI.encode_www_form_component) && defined?(URI.decode_www_form_component)\n```\nAnd then I don't understand because rake still has those warnings. Yesterday, when I did the same it didn't output them.\nThe fact is that even with this conditional requirement, there is something wrong, at least on my side, and I don't get it.\nI'll try again with a blank slate and report the results. In the mean time, if you have any idea why this hapens, I'm all ears.\nThanks\n. I removed Tire from my Gemfile and comment the initializer (with the configuration), so obviously I didn't get any warning.\nThen I enabled Tire back in the Gemfile, but this time, I've edited the lib/tire/rubyext/ruby_1_8.rb (which is located in /usr/local/var/lib/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/tire-0.5.7/) and changed this : \nruby\nrequire 'tire/rubyext/uri_escape' unless defined?(URI.encode_www_form_component) && defined?(URI.decode_www_form_component)`\nto this : \nruby\nraise 'tire/rubyext/uri_escape'.inspect unless defined?(URI.encode_www_form_component) && defined?(URI.decode_www_form_component)\nThe idea was to verify explicitly that the conditions was passing or not.\nWhen I started my app, or ran a rake task, I got this :\n\u2192 bin/rake routes --trace\n/Users/jlecour/Projects/my_app/bin\n(in /Users/jlecour/Projects/my_app)\nrake aborted!\n\"tire/rubyext/uri_escape\"\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/tire-0.5.7/lib/tire/rubyext/ruby_1_8.rb:1\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/tire-0.5.7/lib/tire.rb:13:in `require'\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/tire-0.5.7/lib/tire.rb:13\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/bundler-1.3.1/lib/bundler/runtime.rb:72:in `require'\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/bundler-1.3.1/lib/bundler/runtime.rb:72:in `require'\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/bundler-1.3.1/lib/bundler/runtime.rb:70:in `each'\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/bundler-1.3.1/lib/bundler/runtime.rb:70:in `require'\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/bundler-1.3.1/lib/bundler/runtime.rb:59:in `each'\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/bundler-1.3.1/lib/bundler/runtime.rb:59:in `require'\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/bundler-1.3.1/lib/bundler.rb:132:in `require'\n/Users/jlecour/Projects/my_app/config/application.rb:10\n/Users/jlecour/Projects/my_app/Rakefile:4:in `require'\n/Users/jlecour/Projects/my_app/Rakefile:4\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/rake-0.8.7/lib/rake.rb:2383:in `load'\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/rake-0.8.7/lib/rake.rb:2383:in `raw_load_rakefile'\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/rake-0.8.7/lib/rake.rb:2017:in `load_rakefile'\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/rake-0.8.7/lib/rake.rb:2068:in `standard_exception_handling'\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/rake-0.8.7/lib/rake.rb:2016:in `load_rakefile'\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/rake-0.8.7/lib/rake.rb:2000:in `run'\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/rake-0.8.7/lib/rake.rb:2068:in `standard_exception_handling'\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/rake-0.8.7/lib/rake.rb:1998:in `run'\n/usr/local/opt/rbenv/versions/1.8.7-p371/lib/ruby/gems/1.8/gems/rake-0.8.7/bin/rake:31\nbin/rake:16:in `load'\nbin/rake:16\nSince the raise is executed, I guess the condition on URI methods returns false, when it should return true (Rack being already loaded).\n. Got it !\nThe position gem 'tire' in the Gemfile matters.\nI've always had it just after rails and rake. I've moved it at the bottom of the Gemfile, and it worked.\nThen I've tried to narrow this down, and the conclusion is (in my case) : it has to be after Unicorn.\nIt's strange because Rails is requiring Rack, but maybe not the part that we are talking about.\nAfter all this dancing around, I'm able to use the gemified version of Tire, as well as the GitHub version, or my own fork, without any issue.\nMaybe there is something to do with this, in the documentation, or in the code itself, but I have no idea what.\nThat was fun, regardless of the time spent/lost on this issue.\nThanks for your support.\n. I know that the file is required from Tire itself, but before that it checks whether some methods are available in the URI namespace. Usually they are in that namespace thanks to Rack.\nI've made a bare Rails app, to exclude context as much as possible.\nWith this Gemfile, it works : \n``` ruby\nsource 'https://rubygems.org'\ngem 'rails', '3.2.12'\ngem 'sqlite3'\ngem 'unicorn', '~> 4.6.2'\ngem 'tire', '~> 0.5.7'\n```\nBut with this one, I get the warnings : \n``` ruby\nsource 'https://rubygems.org'\ngem 'rails', '3.2.12'\ngem 'sqlite3'\ngem 'tire', '~> 0.5.7'\ngem 'unicorn', '~> 4.6.2'\n```\nI've tried with Unicorn 4.5.0, but the results are the same.\nIf I replace Unicorn by Thin, it works both ways.\nI remain deeply puzzled ;)\n. ",
    "vwall": "Should be done already.\n. Whoops.  Sorry about that.  Should be good now.\n. ",
    "nickhoffman": "Hey guys. How is this patch meant to be used? With Kaminari and Tire loaded, #paginate doesn't exist on Tire::Results::Collection objects:\nruby\nirb> Product.tire.search('megatron').paginate\nNoMethodError: undefined method `paginate' for #<Tire::Results::Collection:0xaccb2f8>\n        from (irb):66\n        from /usr/local/rvm/gems/ruby-1.9.2-p0/gems/railties-3.0.4/lib/rails/commands/console.rb:44:in `start'\n        from /usr/local/rvm/gems/ruby-1.9.2-p0/gems/railties-3.0.4/lib/rails/commands/console.rb:8:in `start'\n        from /usr/local/rvm/gems/ruby-1.9.2-p0/gems/railties-3.0.4/lib/rails/commands.rb:23:in `<top (required)>'\n        from script/rails:6:in `require'\n        from script/rails:6:in `<main>'\n. Ack, nevermind! Please ignore that previous comment of mine.\n. @hahuang65 #paginate is usually used within a view, like this:\n``` ruby\napp/controllers/products_controller.rb\ndef index\n  @products = Product.tire.search \"megatron\"\nend\napp/views/products/index.html.erb\npaginate @products\n``\n.MyModel.searchdoesnt support custom/raw queries. So if you want to perform a search that's not possible with Tire's DSL,MyModel.search` can't be used. (If I'm wrong about that, please correct me, though!)\nFor example, I need to use dis_max, which isn't supported in the DSL. However, this works:\nruby\nTire.search(Product.tire.index_name,\n    {\n      :from   => from,\n      :size   => @options[:per_page],\n      :query  => {\n        :dis_max => {\n          :queries => [\n            { :field => { 'name' => @term } },\n            { :field => { '_all' => @term } },\n          ]\n        }\n      }\n    }\n)\n. Unfortunately, a string query is too basic for what my users require. If a user searches for a misspelled or partial word, a string query will return no hits.\n. I've added basic support for extra options to be given to Tire::DSL#search in this commit.\nI was unable to get the development environment working (RDoc and MiniTest are needed, but don't seem to be loaded or required), so I couldn't run the tests that I wrote. However, I'm confident that they'll pass.\nLet me know what you think about my approach to the search, and to the commit. I'm happy to modify the search to work within Tire's DSL, but it needs to match misspellings and partial words.\n. To be honest, I didn't build the query because that's specifically what I need. It's simply a modification of the query in @clintongormley's nGram example. This is because I'm very new to ES and Tire, and am still learning how it all works as the days go by.\nIs there a way to use Tire's DSL within a model search to achieve the same behaviour?\n. @karmi I'm converting my raw query to a query-string query against specific fields. Unfortunately, it doesn't seem to work for embedded fields. For example:\n``` ruby\ns = Product.tire.search do\n  query do\n    string \"mlock\", :default_field => \"items.props.character\", :analyzer => \"ascii_3_8_ngram\"\n  end\nend\ns.count\n=> 0\n```\nTo reference embedded fields, does ES require me to use a nested query?\n. On IRC, kimchy just confirmed that a nested query is necessary for querying embedded fields:\n16:33 < kimchy> nick_h: you have to use nested query, in the future, hopefully, we will be able to automatically wrap this for you in a nested query\nCan nested queries be built using Tire? I've looked through the source, but the only mentions of nesting are for indexes.\n. @karmi It looks like that's what's happened now.\nI think that I should take a step back from all of this and make sure that my settings, indexes, and query approach are ideal for the problem I'm trying to solve, though. I've posted to the ES mailing list, asking how my approach can be improved.\n. Sorry for the delay, Karel. Since model loading shouldn't occur within the DSL, I'll close this issue.\nIt'd be great if the model search method supported \"raw\" queries, as well as dis_max .\n. I hear you! It'd be great to support all of these options in one nice refactoring, rather than, as you said, adding support for options one by one.\nDo you have any ideas on how to support all of the options for the ES' Search API?\n. Serializing the options to Tire::Index#store might be the fastest and simplest approach. If I submit a pull request for that, it'd be merged in?\n. Great work, @karmi. Thanks for not forgetting about this feature.\n. Tire's README suggests using the lower-level approach when needing to index large collections:\nTire.index(\"articles\").import a_batch_of_articles\nThe MongoDB documentation that @halfbrick linked to says that a good alternative to cursor.skip() is for the application to limit the size of the result set using queries that are applicable to the application.\nThese two suggestions go hand-in-hand. For example, import articles in batches of 1 month:\n``` ruby\nArticle.count\n=> 10000000\ntime_delta      = 1.month\nstart_criteria  = Article.asc(:created_at).only :created_at\nstart_date      = start_criteria.first.created_at\nend_date        = start_date + time_delta\nwhile (start_date <= Time.now) do\n  Article.where(:created_at.gte => start_date, :created_at.lt => end_date).import\nstart_date = start_criteria.where(:created_at.gte => end_date).first.created_at\n  end_date   = start_date + time_delta\nend\n``\n.#in_groups_of` will cause the entire result set to be retrieved from MongoDB. That's fine for small collections, but not for large collections.\n. ",
    "hahuang65": "What was the solution? I was not able to get this to work with either the .paginate or the .page methods.\n. ",
    "mlitwiniuk": "Imo #current_page could be changed into something like this:\nruby\ndef current_page\n  if @options[:page]\n    @options[:page].to_i\n  else\n    from = @options[:from].to_i || 0\n    (per_page + from) / per_page\n  end\nend\n. ",
    "jsuchal": "See http://lucene.apache.org/java/2_4_0/queryparsersyntax.html#Escaping Special Characters - Lucene query language uses [ for defining range queries.\n. ",
    "fgrehm": "@karmi I've just came across escaping issues down here and I wasn't aware that I had to do it myself. do you think it is worth adding a note somewhere on the readme so that others don't run into it as well? maybe also mention Tire::Utils.escape too :-)\n. @karmi tks a lot for the quick answer :-)\n. @karmi sure, I'll get back to u on monday :)\n. @karmi here's a simple gist with example models and usage, this is basically what I'm currently doing on the app I'm working on. I hope they are self explanatory but if u need more info just LMK :) \nour current need is to perform a global / multi model search and render associations data on the search results page alongside each result item so we ended up doing that to avoid N+1 queries\n. @karmi got it, but the issues are not mutually exclusive as there's currently no way to specify custom eager loading behavior per class on multi model searches :) I've updated the gist adding an example of how it could be used with lambdas\n. ",
    "rb2k": "That wasn't actually a duplicate :)\nkarmi/tire#89 is about supporting the timeout flag elasticsearch provides\nThis was more about the HTTP drivers not closing down before ES finishes\n. I always had the impression that RestClient has a shorter timeout while Curb has a longer one. I run ES on a single server with a lot of data, so my queries can take a minute or two at times :-/\n. That might be the problem. Sinatra spawns a new thread for each request on most servers afaik.\nSince the curb instance is instantiated once and curb isn't threadsafe, that might make Tire not threadsafe. Not sure how to solve that.\n. Sorry haven't found the time to open that project since then :(\nIt is on my \"to do\" list though :)\n. Yeah, probably the best solution.\nSorry that I didn't find the time to actually try that again :-/\n. I'd also like to see them gone :(\n. Argh, never mind. Seems to have been some ancient relict that could be solved by deleting ~/.gem/\n. Oh, I think it was a bit too late/early when I posted that :)\nThe stacktrace was a bit misleading and I think, within my code, I did actually call .join(\"|\") on an array that probably ended up being nil in some freak accident.\nSorry :)\n. ",
    "Rio517": "Yep.  That is exactly the behavior I was expecting and exactly what I wanted to do.\nHere is a sample app. Run rake db:migrate then  db:seed and it will create data, index it and run the command to generate the error.   Console output of my error is in the readme.\nhttps://github.com/Rio517/tire-sample-error\n-Mario\n. All working well! Thanks!\n. Awesome! Thanks!\n. I'm having a similar issue.  After reviewing stackoverflow, tire source, etc.  I haven't been able to spot my issue.  \nhttps://gist.github.com/2493477\nIs there anything obvious that I'm doing wrong?\nThanks!\n. I think that would be really valuable for folks w/ similar use cases.  The time saving from being able to include nested records is rather considerable with larger datasets.\nThanks!\n-Mario\n. ",
    "ralph": "While this is certainly possible to achieve (at least with the new version 0.3.4), I like the should_be_indexed? variant better, because you don't have to take care of the destroy/removal from the index. It's an interface everybody instantly understands, like the to_indexed_json method. Include the default modules for the default behavior, overwrite a method if you want custom behavior, simple as that.\nBut anyway, thanks for making this possible with version 0.3.4.\n. Wheeeeeee!\n. Hey, I will go on a 10 days vacation today, but I will get back to Lee later. As we are in one city, we'll probably just meet and hack on this together.\n. Will have a look! Just found this in @test/integration/filtered_queries_test.rb@, so I guess I misunderstood how elasticsearch uses filtered queries.\nruby\ns = Tire.search('articles-test') do\n  query do\n    filtered do\n      query { all }\n      filter :and, { :terms => { :tags => ['ruby', 'python'] } },\n                   { :range => { :words => { :from => '250', :to => '250' } } }\n    end\n  end\nend\n. ",
    "brupm": "@karmi and @mahemoff \nReally confused about this: \n``` ruby\nThis is copied straight from the source, which references half-baked implentations.\nDon't know in what situations it would be necessary, but including it anyway\nbefore_destroy { @destroyed = true }\ndef destroyed?; !!@destroyed; end \n```\n. Just ran into this big time in production. As I see it using curb is not possible as it is.\n. HI @karmi with unicorn being the defacto production server, I believe multithreading capability on the the curb client in tire is a must, let me know how I can help. \n. Seeing lots of \"Curl::Err::MultiBadEasyHandle: CURLError: Invalid easy handle\". Upon further investigation looks like the client it is still sharing handles. I am not too sure yet how, haven't been able to reproduce the issue outside of the application. \n@karmi the sample code provided in this commit https://github.com/karmi/tire/commit/ec56611c75fbec7da1c31b5ff3ccffd0923140af must not be triggering the issues. \nThe backtrack points to:\nvendor/bundle/ruby/1.9.1/gems/curb-0.8.3/lib/curl/easy.rb:55:in `add\nvendor/bundle/ruby/1.9.1/gems/curb-0.8.3/lib/curl/easy.rb:55:in `perform\nvendor/bundle/ruby/1.9.1/gems/tire-0.5.7/lib/tire/http/clients/curb.rb:27:in `http_post\nvendor/bundle/ruby/1.9.1/gems/tire-0.5.7/lib/tire/http/clients/curb.rb:27:in `get\nvendor/bundle/ruby/1.9.1/gems/tire-0.5.7/lib/tire/search.rb:136:in `perform\nvendor/bundle/ruby/1.9.1/gems/tire-0.5.7/lib/tire/search.rb:35:in `results\nvendor/bundle/ruby/1.9.1/gems/tire-0.5.7/lib/tire/model/search.rb:105:in `search\nWhich isn't much help at all. I will try go dig in a bit more to find out why the muti-threading is not working. \nOn another note have you see https://github.com/drbrain/net-http-persistent? Appears to be the ideal alternative.\n. @karmi Thread.current[] does not seem to solve the issue 100%. It did reduce the # of errors but they still happen quiet frequently.\nI put together this https://github.com/karmi/tire/pull/713 would love some feedback.\n. Something interested I noticed... The Curl::Err::MultiBadEasyHandle: CURLError: Invalid easy handle seem to happen when the prod application is restarted, perhaps there is some kind of teardown/cleanup that needs to happen? I am not too familiar with multithreading. \n/cc @karmi @nz\n. Would be nice to augment the JSON inline before it is sent to Elasticsearch.\nruby\nModel.search do\n  query do\n    match [:some_field], 'query text'\n    ...\n    custom_json \"{ some complex JSON query that the tire API does not (yet) support }\"\n  end\nend\n. Lots of people have this same issue, the mappings are created automatically by inserting the first document but Tire has no way of deducing based on data that it is a geo_field type field.\n. Hi any update on this? Has it been merged into tire-contrib?\n. I think this has been merged into master.\n. :100: \n. cc @nz any ideas?\n. I do agree this isn't 100% on the Tire side, but it could affect people using Tire (I also posted the question to Stackoverflow and the Elasticsearch mailing list) \nI have made a few changes and now the file is owned by elasticsearch, it has the same exact permissions as /var/log/elasticsearch/elasticsearch.log which the ES binary is writing to. Still the same issue continue to happen yet the file is absolutely there\n@karmi you asked if ES is properly configured to look in this folder. It seems to know where to look at it provides me the full path of the file.\nIf I place the files under a static location such as /etc/elasticsearch/synonyms even with root perms it works flawlessly. \n. You can use templates: http://www.elasticsearch.org/guide/reference/api/admin-indices-templates.html\n. Seems it needs the mapping defined: http://pastie.org/6342437#11\n. :+1: Very useful PR. Although the build is failing.\n. Nice, I was just looking for something like this yesterday. \n. Make sure before you index anything you create an empty index with the proper mappings. \ntire.index.delete\ntire.create_elasticsearch_index\nThis will ensure the geo_point field is created in the index of the proper type. Otherwise it may be created as a \"double\" and then you will get that error. \nOnce you run the create_elasticsearch_index check the mappings for the index at: GET `localhost:9200/index_name/_mapping\nThe field type should be listed as geo_point. \n. All you should need is:\nruby\nmapping do\n  indexes :user_lat_lon, type: \"geo_point\"\nend\n. Maybe start with the basics in the mappings/settings block and then add your customizations one by one, something along the way is causing issues.\n. @karmi bump\n. Don't include the callbacks module and handle it with after_save and after_destroy callbacks. \n. @karmi all done. The dependency appears to be required, otherwise all will not boot. @karmi Am I missing something?\n. https://github.com/karmi/tire-contrib/pull/18\n. @jocke12 could you please include a test? \n. It should be index\n. I stand corrected, didn't know about polymorphic import\n. This is already in tire-contrib https://github.com/karmi/tire-contrib/blob/master/lib/tire/queries/custom_filters_score.rb @fcheung \n. Maybe there should be a warning in the Tire readme about the existence of tire-contrib\n. I can understand the frustration when you first start using a new library. However this is not a valid issue. There are plenty of example in the readme under Usage: https://github.com/karmi/tire/blob/master/README.markdown#usage\nMoreover I have a repos that contains a few experiments you may find useful depending on what you are trying to do. Each ruby file runs independently and is useful when trying to troubleshoot/understand particular aspects of Tire/Elasticsearch https://github.com/brupm/elasticsearch_ruby\n. Make sure you create a mapping as such: \nruby\nindexes :user_lat_lon, type: \"geo_point\"\nWhich contains a hash: \nruby\n{ lat: lat, lon: lon }\nAnd then sort:\nruby\nsort do\n  by :_geo_distance, {\n    user_lat_lon: { lat: params[:user_lat], lon: params[:user_lon] },\n    order: (params[:direction] || \"asc\"), unit: :miles\n  }\nend\nIt's important that you create the index by hand instead of letting ES create on the first record import as it will assume the incorrect type for that mapping, it must be a geo_point.\n. Just use: \nruby\ndef lat_lon\n  { lat: latitude, lon: longitude } \nend\n. As long as lat_long is a hash containing:  { lat: some_lat, lon: some_lon } \n. Sure:\nruby\nsort do\n  by :name, \"desc\"\n  by :created_at, \"desc\"\nend\n. Perhaps this is something to consider for the new elasticsearch ruby client? https://github.com/elasticsearch/elasticsearch-ruby\n. Actually I never meant to submit this back into karmi/retire, likely a git push mistake. We're keeping our own forked version of tire.\n. ",
    "cedufca": "1) Sure.\n2) Hmm I think I forgot Tire works without Rails. In this case, ActiveRecord was trying to set \"description.partial\" on my model class, when it must set \"description\" only. This made me think now: is there a way of having (by ElasticSearch) different highlights on the same field? If so, it may be better just replacing non-alphanumeric with an underline. \"description.partial\" would be \"description_partial\". What you think about it?\n. ",
    "mikeg250": "Wow, thanks for the quick responses.  I'll take a look at those commits and see what I can figure out.\nThanks,\n. Sorry, not sure I understand.  Does your response mean that ES and Tire do not currently support Highlighting with boosting?  \nThanks again (your Tire gem is pretty cool!)\n- mike\n. Thank you. I\u2019m new to all this, so I didn\u2019t realize exactly what a pull request was.   Your response makes sense to me now.\n. ",
    "sakrafd": "Hey @vhyza,\nI tried batches of docs between 10K-1K all resulting in timeouts - sometimes the five retries that are already in tire would be enough to keep the process going, but it would eventually end up failing. Once I put in the sleep time, I settled on batches of 5k and was able to get through the remaining docs. We're on RHEL5 (and unfortunately can't change that in production), ES was at 17.6 when I imported the docs to a 3 server cluster with 10 shards and 1 replica.\nWere you running batches of 1000 docs on FreeBSD when you were seeing timeouts? How many total docs did you import? How about mappings, did you have a lot of dynamic ones? Seems like there are a lot of variables that can contribute to the timeouts. I didn't see any timeouts on osx or our staging server which is also RHEL5, but it's not set up as a cluster and I wasn't importing nearly as many docs. My colleague is in the process of adding another index to our prod servers that will have 175M (docs without any dynamic mappings) to the same ES instance (on 17.8 now).  I'll let you know if he runs into timeouts.\ndave\n. ",
    "ellmo": "Hey, thanks for the replies.\nDeal is an ActiveRecord base and, as far as I know, in every configuration we tried (both where it works and where it doesn't), it uses MySQL, so no funky databases.\nhttps://gist.github.com/1295013\nWhen I to use that url you suggested, I get hits all right. It says it got all 352 of them.\nSince the models have a TON of fields, I'll just paste a fragment of it.\njson\n{\"took\":113,\"timed_out\":false,\"_shards\": {\"total\":5,\"successful\":5,\"failed\":0}, \n\"hits\":{\"total\":352,\"max_score\":1.0,\"hits\":\n[{\"_index\":\"deals\",\"_type\":\"document\",\"_id\":\"4\",\"_score\":1.0, \"_source\" : \n{\"area\":null, \"building_address\":null, \"building_antique\":null, \"building_area\":null, \"building_built_date\":null,  \"building_common_percentage_rights\":null, \"building_condition_id\":null, \"building_condition_rating\":null, \"building_coord_system\":null, \"building_description\":null, \"building_functional_rating\":null, \"building_general_location_rating\":null,  \"building_ground_levels\":null, \"building_id\":null, \"building_infrastructure_known\":null, \"building_lat\":null,\"building_lon\":null, \"building_material_id\":null, \"building_mortgage\":null, \"building_mortgage_holder_id\":null, \"building_mortgage_value\":null, \"building_no\":null, \"building_percentage_rights\":null, \"building_premises_count\":null, \"building_premises_total_area\":null,\"building_price\":null,\"building_primary_purpose_id\":null,\"building_right_id\":null,\"building_roof_type_id\":null,\"building_secondary_purpose_id\":null,\"building_size\":null,\"building_underground_levels\":null,\"building_usable_area\":null,\"building_usage_percentage\":null,\"building_user_count\":null,\"building_vat\":null,\"building_volume\":null,\"buyer_type_id\":null,\"city\":\"G\\u0142og\\u00f3w\",\"county_id\":null,\"created_at\":\"2011-10-\n. ruby\nt = Tire.search(\"deals\") { :all }\nReturns hits similar to those fetched with curl. I'm not sure however if it returned all data, tho' when I call for t.results it says:\n``` ruby\n 97, \"timed_out\"=>false, \"_shards\"=>\n{\"total\"=>5, \"successful\"=>5, \"failed\"=>0}, \"hits\"=>\n{\"total\"=>352, \"max_score\"=>1.0, \"hits\"=>\n[{\"_index\"=>\"deals\", \"_type\"=>\"document\", \"_id\"=>\"4\", \"_score\"=>1.0, \"_source\"=>{\"area\"=>nil, \"building_address\"=>nil,\n(...and so on...)\n```\nBut when I try to grab t.results[0] it goes\nruby\nNameError: uninitialized constant Document\n    from /Users/ellmo/.rvm/gems/ruby-1.9.2-p290-patched/gems/rake-0.9.2/lib/rake/ext/module.rb:36:in `const_missing'\n    from /Users/ellmo/.rvm/gems/ruby-1.9.2-p290-patched/gems/activesupport-3.1.0/lib/active_support/inflector/methods.rb:124:in `block in constantize'\n    from /Users/ellmo/.rvm/gems/ruby-1.9.2-p290-patched/gems/activesupport-3.1.0/lib/active_support/inflector/methods.rb:123:in `each'\n    from /Users/ellmo/.rvm/gems/ruby-1.9.2-p290-patched/gems/activesupport-3.1.0/lib/active_support/inflector/methods.rb:123:in `constantize'\n    from /Users/ellmo/.rvm/gems/ruby-1.9.2-p290-patched/gems/activesupport-3.1.0/lib/active_support/core_ext/string/inflections.rb:43:in `constantize'\n    from /Users/ellmo/.rvm/gems/ruby-1.9.2-p290-patched/gems/tire-0.3.3/lib/tire/results/item.rb:61:in `class'\n    from /Users/ellmo/.rvm/gems/ruby-1.9.2-p290-patched/gems/tire-0.3.3/lib/tire/results/item.rb:66:in `inspect'\n    from /Users/ellmo/.rvm/gems/ruby-1.9.2-p290-patched/gems/railties-3.1.0/lib/rails/commands/console.rb:45:in `start'\n    from /Users/ellmo/.rvm/gems/ruby-1.9.2-p290-patched/gems/railties-3.1.0/lib/rails/commands/console.rb:8:in `start'\n    from /Users/ellmo/.rvm/gems/ruby-1.9.2-p290-patched/gems/railties-3.1.0/lib/rails/commands.rb:40:in `<top (required)>'\n    from script/rails:6:in `require'\n    from script/rails:6:in `<main>'\nand the t itself becomes corrupted as from now on every call on this object will give a NameError: uninitialized constant Document \n. actually Tire.search(\"deals\") { all } gives\nruby\n[REQUEST FAILED] NameError \nNameError: undefined local variable or method `all' for #<Tire::Search::Search:0x00000105741840>\nat least on my machine, that's why I tried :all instantly and got a result\nThanks for your help and time. I'll try the next release and tell you if anything has changed.\n. Hey,\n- I upgraded tire to 0.3.7.\n- Removed all ES indices through curl -XDELETE ...\n- Imported them back again through rake environment tire:import CLASS=\"Deal\"\nruby\nDeal.tire.search \"*\"\nstill does not return any hits, however\nruby\nt = Tire.search(\"deals\") { query { string \"*\"} }\ndoes seem to work. I guess it doesn't hurt to use it instead. However when I call t.results.size on the object (and yes, there's no more NameError, so thank you for that!), it's size is 10 and I have 352 objects indexed (allegedly).\nWhat am I missing?\n. ",
    "jwood": "That sounds good to me.  Thanks!\n. Yeah, sorry.  I didn't mean to complicate things with that other commit,\nwhich I did not intend to be merged.  I forgot that the pull request was\nstill open.\nThanks again for merging the patch.\nJohn\nOn Mon, Oct 24, 2011 at 3:14 AM, Karel Minarik \nreply@reply.github.comwrote:\n\nAlso, it's much more convenient for the administration, when heterogenous\npatches do not end up in one pull request (such as the \"re-raise\" one :)\nUsually, in Git, it's a very good idea to create a branch for every logical\npatch/feature...\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/pull/130#issuecomment-2500767\n\n\nhttp://johnpwood.net\n. ",
    "dylanahsmith": "Searching across multiple types to me didn't seem like an edge case.  It allows a search field to be present as part of the layout (i.e. available on any page) without having to navigate to a section of the site before searching.  This could be used to search for tv shows and movies, articles and comments, blog posts and other pages, etc.\nI do agree that it is best to avoid loading data from the database, if possible, and just use the results directly to display the data.  However, I am dealing with an existing codebase that is currently using Sphinx, and expects to be working with models.  Displaying these results is also non-trivial, because I am developing for a platform which provides the ability for custom search results pages to be uploaded as liquid templates.  Obviously this isn't the typical use case, but Tire already seemed to have the ability to load models from the database, so I thought the code would be useful upstream.\nI appreciate your feedback, and understand if you don't want to maintain the code for eager loading multi-type searches.\nAlso take note of the commit that allows the search to gracefully handle missing records.  It would be a bad user experience to have a search fail just because one of the results can't be found.  Of course, without multi-type search support, the code wouldn't matter to me, since I will need to do the loading externally.\nI am already using the Tire.search, not the ActiveModel integration directly.  The :load parameter isn't handled in Tire::Model::Search, it is accessible from Tire.search.  I could move the functionality to a wrapper, although the fact that Tire::Search::Search changes Configuration.wrapper would need to be fixed.  Likely I will move the code to my own module as you mentioned, handle eager loading there, and not call tire search functions directly from other code.\n. > When we have the ability to return content at will from the engine, it is just wrong to load data from the database. I have yet to hear a compelling argument otherwise.\nIt was just meant to be a transitional step.  A Tire::Results::Item obviously doesn't have the same interface as the model it represents.\n\nIf we're talking about Shopify, it's on par with other features where ElasticSearch makes sense for you: easy multitenancy (account-based indices, configurable aliases with filters/routing, etc), distribution, powerful facets.\n\nI am already using account-based indices, plan to look into using aliases for non-disruptive reindexing, and there are plans to exploit the extra search features elasticsearch has.\n\nYou may also be the first to actually make use of the :wrapper option, since you could create your own, \u201cenhanced\u201d Item implementation.\n\nPerhaps one that will create an ActiveRecord objects with the data from elasticsearch that simply act as a cache, but can still load data through relationships with other tables. I haven't looked into how necessary this would be for our use case though.\n\nThat may fly with ActiveRecord, but half of people using ElasticSearch/Tire are not using ActiveRecord-based models (me included), and moreover, I personally don't care about ActiveRecord a tiny little bit.\n\nWell, I am doing my own eager loading now, so that's alright.\n\nBut yes, Tire.search does not expose the :load option\n\nCheck your codebase, because the option is passed through to Tire::Results::Collection.  The ActiveModel integration obviously didn't make semantic sense for multitype search (e.g. I don't want to do Product.search when I also want to search for other models).\n. I added a commit to try to simplify the eager loading code.  Is it still too complicated?\nAs for the use of where() instead of find(), should I revert that change for this pull request?  I would like the code to tolerate missing records, but I couldn't find any implementation or documented interface for find in ActiveModel.  What documentation do you use to make sure Tire is compatible with other ActiveModel based implementations?\n. I was just setting it to Hash to skip a bit of code when doing eager loading manually afterwards.  Nothing essential.\nThe only advantage to having the option it would be for multithreading, which isn't my use case.  For single threading Configuration.wrapper can be set before and after a request to do the same thing.\n. @karmi test added.\n. Added tests for the response code being logged, however, now there will be a merge conflict with pull request #223.\n. @karmi: resolved the merge conflict.\n. The HTTP client shouldn't be throwing non StandardError exceptions.  If they are that should be fixed.\n. Looked at the source code for rest-client and curb, and they both seem to use exceptions that inherit indirectly from StandardError.  I also tried catching the exceptions from Tire.index('').exists? when elasticsearch is down with Curb and RestClient, and the exceptions were caught.\nHere are some of the inheritance chains:\n- Errno::ECONNREFUSED -> SystemCallError -> StandardError\n- RestClient::RequestTimeout -> RestClient::RequestFailed -> RestClient::ExceptionWithResponse -> RestClient::Exception -> RuntimeError -> StandardError\n- Curl::Err::ConnectionFailedError -> Curl::Err::CurlError -> RuntimeError -> StandardError\n- Interrupt -> SignalException -> Exception\nI have also added tests to the pull request.\n. > What about wrapping the calls in Ruby's timeout? Is there something to gain/lose?\nThis wouldn't allow the rest-client timeouts to be longer than the defaults.  For instance, curl localhost:9200/#{index_name}/_optimize?max_segments=5&refresh=true&flush=true could easily take over the default read_timeout of 60 seconds after reindexing a large index.  It is also a case where increasing the read_timeout but not having a long open_timeout makes sense.  It would be nice to be able to do this with tire's http client.\n. >  we need to expose all the options, namely search_type\nThe difference is that those are body parameters, but this one is a URL parameter and should be a place that is easier to share with the Tire.search(indices, payload) code path of Tire.search. See pull request #259 for how I made this usable for both code paths of Tire.search.\n\nthis seems to me as another case of trying to hack some ES options into Tire methods\n\nI don't see how not implementing unrelated functionality somehow makes this code a hack.  It certainly doesn't make it harder to implement support for search body parameters.\nI also implemented this in a method that allows that makes it easier to add additional URL parameters (although the only other I can find is pretty) by adding them to the slice method, and it is implemented in a way that allows a methods in Tire::Search::Search to set a query parameter by modifying @params.\nIf you want, it would be very easy to modify this code to only expose the routing parameter through a method in Tire::Search::Search.  But so far you haven't expressed any opinion either way in issue #88, so I thought I would try to get some feedback.\nEdit: Looked at search_type more carefully, and it does appear to be a url parameter as well, even though it was listed along with body parameters in the docs.\n. The issues this pull request was addressing have been addressed by commit e3e09a6b99565610b67d89a33cdad0afb2222203.  Thanks!\n. @karmi, any feedback?\n. I also have a fix for this in pull request #292.\n. The issue is, then when I try to retrieve a document that doesn't exist, it throws an error.\nTry running the following:\nruby\nrequire 'rubygems'\nrequire 'tire'\nindex = Tire::Index.new(\"empty\")\nindex.create\nputs index.retrieve(\"doc\", 1).inspect\nAnd you will get an error as follows:\n/Users/dylansmith/src/tire/lib/tire/index.rb:156:in `update': can't convert nil into Hash (TypeError)\n    from /Users/dylansmith/src/tire/lib/tire/index.rb:156:in `retrieve'\n    from test.rb:7:in `<main>'\nYou're suggestion doesn't help, because it will still result in document.update(nil) being executed.\n. ",
    "allesklar": "+1 to the important need to provide site wide searching ability out of the box. \nI have no idea how to implement this though ;-)\n. ",
    "luxflux": "Maybe also mention the possibility for multi-model searches in the Readme. I had to use google to find here (or I just have tomatoes on my eyes...) :)\n. ",
    "magedmakled": "Hi @karmi, I think you should something together in the readme for these two things\n1. multi-model search like it is mentioned in that post. \n2. how to implement association (has_many, belongs_to)  in one of the models \n. ",
    "HiroProt": "Is there a way to specify what to eagerly load based on the AR object that is returned? I am searching over multiple different models and would like to eagerly load different things based on the actual model.\n. ",
    "jfalk": "Yes sir.\n. Interesting idea.  Being able to pass parameters into the script would be very useful though.  Sure you could insert args in the script string, but then ES has to recompile the script each time it's different.  Doing it the way described in the ES docs is more efficient.\n. ",
    "danoph": "You need to wrap custom_score in query{ }\n. @pacoguzman yes it's weird but required...kimchy posted why it's required somewhere in elasticsearch's issues...I hacked together custom score for my application with tire...\n```\nTire::Search::Query.class_eval do\ndef custom_score(options={}, &block)\n    @value = { custom_score: { query: self.instance_eval(&block).to_hash }.merge(options) }\n@value\n\nend\nend\nsearch = Item.search page: 1, per_page: 30 do\n  query do\n    custom_score script: \"random()\" do\n      all\n    end\n  end\nfilter :term, status: 1\n  filter :term, featured: 1\nend\n@items = search.results\n```\n...After looking at the code for a minute, the way I implemented it doesn't seem like a good way to do it at all, but until it's implemented in tire it works for me\n. ",
    "VetriVellore": "Thank you for adding support for custom_score. It doesn't seem to work with highlight though - is it a known issue?\n. ",
    "superplussed": "Thanks Ralph!\n. Sorry guys, I think the mapping was incorrect, the behavior went away after some changes.  Thanks!\n. ",
    "RKushnir": "I'm having the same issue. Used the trick with allow_net_connect! but it doesn't feel quite right.\n. @karmi From the first post:\nrequire 'webmock'\nWebMock.allow_net_connect!\nI think, this is rather the issue of webmock, it should allow to configure when to turn it on. Maybe, start as disabled by default.\n. ",
    "sirn": "Sorry for replying to a 1 year old issue. However I've found this to work as well:\nruby\nrequire 'webmock'\nWebMock.disable_net_connect!(:allow_localhost => true)\nI think this way is better than allow_net_connect! in that it still blocks external connections while also allows localhost connection. (Which might make sense if ElasticSearch is running on localhost for testing.)\n. Is it possible to make Tire's index call Array#index if argument is present? It feels somewhat hack-ish, but I suspect this might work.\n. The method does not exist in the model because it is delegated to Array by ActiveRecord (from within Rspec context).\n``` ruby\nBook.titled.method(:index)\n=> #\n```\nBut Tire overloading index to the model breaks this since it will delegate to the model class instead. Defining index in my own class would presumably breaks RSpec too, I guess. The workaround I'm using currently is to manually remove index from all models (with remove_method).\nI'm not really sure how this problem could be fixed. Maybe put a note somewhere? :\\\n. I just tested, it breaks when I manually define index too.\n``` ruby\nclass Book < ActiveRecord::Base\n  attr_accessible :title\n  scope :titled, where(\"title IS NOT NULL\")\nclass << self\n    def index\n      \"Blah\"\n    end\n  end\nend\n```\nSame error as including Tire::Model::Search to model. I will put up a Wiki page about this later.\n. Not sure if this is good enough, but I've posted on at https://github.com/karmi/tire/wiki/Troubleshooting-RSpec-issues\nClosing this issue. Thanks!\n. ",
    "tcocca": "Hi, I am debugging this now, what was incorrect about my original mapping?\n. AHHH! I'm an idiot, I had two problems in my mapping..\nThe first was this:\nindexes :street_name,             :type => 'string', :analyzer => 'prefix_edgengram'\nreally needs to be:\nindexes :street_name,             :type => 'string', :analyzer => 'prefix_analyzer'\nAfter I fixed that it was creating lat_lng as a correct geo_point but then I realized I had this:\nindexes :lat_lng,                 :type => 'geo_point', :lat_long => true\nwhich should be:\nindexes :lat_lng,                 :type => 'geo_point', :lat_lon => true\nSo I think this is all good now, I'm gonna index some data points and make sure this works correctly.\n. The street_name was just the wrong thing.  I was passing my custom filter to the analyzer instead of my custom analyzer ... dumb mistake\nFor the geo_point.\nhttp://www.elasticsearch.org/guide/reference/mapping/geo-point-type.html\nlat_lon - Set to true to also index the .lat and .lon as fields. Defaults to false.\nWhich I believe would allow me to call doc[lat_lng].lat or doc[lat_lng].lon\n. All good just need to figure out how to do the geo_polygon queries with Tire.  Either off of the Tire.search or Mls::Property.tire.search.\nI'll need to combine this with text queries and filters on a bunch of the other fields too.\n. Thanks karmi I was mistaken about the extra arguments, the nested hashes work just fine, i wasn't nesting the stuff correctly.  I was using 0.3.8 and the filter options worked.  I was mistaken because the readme says that the geo stuff is not supported yet and you have mentioned to get geo support you had to use Tire.search directly.\nAlso, when i woke up this morning I had the same thought as you did.  Expose a top level geo dsl in the search.  Same as filter, facet, etc...\nI may take a look into writing that geo class and adding that support.  It should basically act similar to #filter on the search class and just add the geo filters to the filter array.\nHere is my last question, how do you go about doing \"or\" filters?  I know the default is to and all filters together, but how would I go about writing something like this:\nfilter :terms, :property_type => [\"Single Family\", \"Condo\"]\nfilter :terms, :status => [\"Active\"]\nfilter :or\n  filter :geo_bounding_box, :lat_lon => { :top_left => [40, -71], :bottom_right => [42, -72] }\n  filter :town => ['Boston'] \nend\nso it ands everything and then there is an \"or\" condition on whether it is in the bounding box or it is in the town?\n. @mleglise I am doing a filtered query and am just passing a array of 'filters' to the .filtered dsl method in an and option.  Here is the simplified version of what I am doing:\nSo, I have something like the following:\n``` ruby\nquery_filters = [\n  {:terms => {:status_id => [1,2,3,4]}},\n  {:terms => {:property_type_id => [1,2,3]}},\n  {:geo_polygon => {:lat_lng => { :points => [-70, 40],[-80, 30],[-90, 20]}}}\n]\nProperty.search do |search|\n  search.query do |query|\n    query.filtered do |filtered|\n      filtered.query { |fq| fq.all }\n      filtered.filter :and, query_filters\n    end\n    query.sort do |sorted|\n      sorted.by :price, :desc\n    end\n  end\nend\n```\nI found writing the 'conditions' or filters for the search to be much simpler to do it as an array of hashes as opposed to trying to make the dsl work for me.\nAlso, you can do a lot more. For example, nesting 'or' or 'bool' sets inside of the main 'and'.\nLet me know if you have questions about this method.\n. ",
    "skogsmaskin": "I was having problems with this as well, when testing with rspec.\nTo get .location recognized as a geo_point I had to do call MyModel.create_elasticsearch_index before tests, like this:\nbefore :each do\n      Address.index.delete\n      Address.create_elasticsearch_index\n      Address.index.import [an_address, a_second_address, a_third_address, a_fourth_address]\n      Address.index.refresh\n    end\n. I see, it respects the ElasticSearch settings if not defined. Goodie.\n. ",
    "jasonfb": "I am also having problems with a  \"field [location] is not a geo_point field\".  Will post to a new issue\n. was there a resolution to this one? I am stuck on a similar \"field [location] is not a geo_point field\" parse error. Will post to new issue\n. we've been doing:\nUser.index.delete\nUser.import\nWill try your method and get back to you\n. Lo and behold... it is NOT in my mapping.\nI think maybe my setting syntax is wrong. I \n```\n      settings analysis: {\n        filter: {\n          edge_ngram_filter: {\n            type: \"edgeNGram\", # an n-gram is a contiguous sequence of n items from a given sequence of text\n            max_gram: 8,\n            min_gram: 2,\n            side: 'front' # only index n-grams beginning with the first letter\n          }\n        },\n        analyzer: {\n          edge_ngram_analyzer: {\n            tokenizer: \"lowercase\", # divides text at non-letters and converts them to lower case\n            filter: [\"edge_ngram_filter\"],\n            type: \"custom\"\n          }\n        }\n      } do\n        mapping do \n          indexes :id, type: 'integer'\n          indexes :merchandiser_user_id, type: 'string', analyzer: 'keyword'\n      indexes :firstname, type: 'string', analyzer: \"edge_ngram_analyzer\"\n      indexes :lastname, type: 'string', analyzer: \"edge_ngram_analyzer\"\n      indexes :email, type: 'string', analyzer: \"edge_ngram_analyzer\"\n      indexes :lat_lon, type: 'geo_point'\n      indexes :merchandiser_manager, type: 'boolean'\n\n      indexes :anonymous, type: 'boolean', included_in_all: false\n    end\n  end\n\n```\nDoes the \"settings analysis\" block at the top specify that all the fields in the mapings are ngram filters? I'm confused, do I need to specify two settings blocks, one for my ngram filter-able fields and another for my other kinds of fields?\nWhen I do this http://localhost:9200/candi_development_users/_mapping\nI see this result, which clearly does not show either lat_lon (geo_point) or merchandiser_manager (boolean)\n{\"candi_development_users\":{\"user\":{\"properties\":{\"anonymous\":{\"type\":\"boolean\"},\"email\":{\"type\":\"string\",\"analyzer\":\"edge_ngram_analyzer\"},\"firstname\":{\"type\":\"string\",\"analyzer\":\"edge_ngram_analyzer\"},\"id\":{\"type\":\"integer\"},\"lastname\":{\"type\":\"string\",\"analyzer\":\"edge_ngram_analyzer\"},\"merchandiser_user_id\":{\"type\":\"string\",\"analyzer\":\"keyword\"}}}}}\n. Yes thank you, I got this working. I think if you leave out the step create_elasticsearch_index it doesn't re-create the index with the new fields, and so I was not seeing them in there and getting that message. \n. ",
    "bobbytables": "Has anyone done any work on this yet?\n. Sweet.\n1. Because my original one got an extra commit that I pushed to my branch.\n2. We're running ES on EC2 and our stack is on Rackspace so we wanted to add timeouts incase anything exceeded 1 second. (Our EC2 instances are working hard on something else, etc...)\n3. It was added there because those were the only places I could see calls being made. Are there others?\n4. RestClient supports open_timeout and timeout. In the future it would be easy to support both, for now this covers both of them with one configuration. I don't see a benefit to using Ruby's Timeout for the simple reason that it gives no additional functionality to the end purpose.\n5. Timeouts in ElasticSearch could produce unexpected results if worded this way. They do 2 different things. It could, however, be a part of the query DSL.\n. There might be good reason to have read_timeout and open_timeout as separate configurations. Perhaps even keep the simple \"timeout\" configuration that does both for you.\n. I could get behind that. But what more does Tire do in terms of HTTP request outside of the http/client class?\n. @karmi ?\n. ",
    "mleglise": "Ping.  Any forward movement on geo support?  @tcocca if you ended up deciding you could accomplish what you wanted without adding anything new, could you post your solution on the wiki?\nCheers!\n. @karmi I meant primarily within the DSL, so thank you for answering that.\nI'm hoping to avoid having to build my full query as a Hash (as instructed here: https://github.com/karmi/tire/wiki/How-to-work-with-locations-%28geo%29-in-Tire) and instead go through the existing filter DSL as you suggested above.  If it works as described, I'd be happy to update the wiki page.  Thoughts?\n. Thanks for the example @tcocca \nI worked out how to do it using the block syntax and posted a basic example on the wiki.  It seems like with these two options, there is little need to bake it into the DSL.  Cheers!\n. +1 for this feature!\n. ",
    "managr": "Thanks a lot for quick response.\n. ",
    "gtarnovan": "Hi! Thanks for the quick response.\nWhy don't you use ActiveSupport::BufferedLogger instead of rolling your own ? Or let the user supply one.\nThank you for your work!\n. I mean, let the user supply a logger instance like Log4r, standard ruby logger, etc; just like Rails does with config.logger\n. ",
    "faceit": "Using tire 0.5.1 it seems that the id and type fields are still stored in the _source.\nI can see how having type for example can make mongoid/ar integrations smoother, but IMO it wouldn't be wise for a ES client to enforce it.\nMay I suggest a schema like this:\narticles = [{:id=>\"1\", :type=>\"article\", :_source => { .. } }]\n. ",
    "ryw": "Yes - worked like a charm, after I jumped off the homebrew version of elasticsearch. \nHomebrew presently installs 0.16.2, from May 31, which doesn't support the _ttl concept.\nThanks for quick response on this.\n. Duh - my homebrew was out of date. brew update is my friend (after I cleared myself of this issue.)\n. To close this out -- I couldn't get elasticsearchhq.com to work using any method above, it reported \"couldn't find user ryw\" -- I put in support request, haven't heard anything on it for a week -- I'll reopen this issue and update if I hear back from them.\nI ended up building an elasticsearch server on EC2 and it's working fine for me.\n. @rajraj sorry I didn't respond quicker - I followed the tutorial @ http://www.elasticsearch.org/tutorials/2011/08/22/elasticsearch-on-ec2.html but I discovered during the process that the Elasticsearch wouldn't start on a micro instance, had to go up to Small\n. My app is small/low traffic, so I don't see any noticeable latency.\n. Hi Jake-\nI have a file:\n     /config/initializers/elastic_search.rb \nwith one line:\n    Tire.configure { url 'http://instance_name.compute-1.amazonaws.com:9200' }\n. ",
    "GabKlein": "I'm looking for a way to set the TTL within Tire::Model::Persistence.\nDo you have a snippet of how implement it ?\n. ",
    "joshdevins": "The error output is wrong. It's just printing the Ruby object .to_s instead of something helpful like the response body JSON.\nAs it is now with response.to_s:\n[REQUEST FAILED] Tire::Search::SearchRequestFailed #Tire::HTTP::Response:0x007ff42e896aa0\nWith response.body.to_s:\n[REQUEST FAILED] Tire::Search::SearchRequestFailed {\"error\":\"IndexMissingException[[green] missing]\",\"status\":404}\n. Sorry, looks like it works on master. Upgraded and all is good.\n. ",
    "mikegrassotti": "I'm seeing the same problem - here's what I tried:\n    git clone https://github.com/karmi/tire.git\n    cd tire\n    rvm use 1.9.2\n    rvm gemset create tire-test\n    rvm use @tire-test\n    bundle install\n    rake\n    /Users/mgrassotti/.rvm/gems/ruby-1.9.2-p290@tired/gems/turn-0.8.3/lib/turn/autorun/minitest.rb:14:in <top (require\nd)>': MiniTest v1.6.0 is out of date. (RuntimeError)gem install minitestand addgem 'minitest' to you test helper.\n. OK, let's give it a shot. I just pushed my changes here:\nhttps://github.com/grasscode/tire/commit/63fefb9b56073180356e676fea2e327adc09f7dc\n. @erickt thx. I'll have a look at your search_type code.\n. ",
    "kathgironpe": "+1 for 1.9.3. \n. ",
    "rajraj": "@ryw Hi, would you be kind enough to share the details on how to setup ES on Amazon EC2 and linking to your app on Heroku? Many thanks\n. @ryw thanks for the response. I will checkout the tutorial. How is the search performance for the app hosted on Heroku and ES on EC2? I have seen people mentioning the latency issues.\n. I have sorted it now. Installed 0.90.0 rc2 and everything is working fine. Just weird though.\n. ",
    "jakeatwork": "@ryw (or anyone here for that matter) - From the conversation here, it seems you've figured this out and was wondering if you could give me the briefest of pointers.\nI'm new to both Elasticsearch and rails, but I have both running and working locally (and everything but ES running on heroku) and just needed to know what file(s) i need to modify to point Tire to the ES instance on EC2...or something similar...feels like i'm just missing a key piece of information.\nAny information would be greatly appreciated.\n. THANK YOU. both of you. @karmi - appreciate the input AND the software. thanks. @ryw  - it's working now. i knew i was missing something simple. i've got data in ES on EC2. @karmi - i hit up bonsai on twitter for a test account...i'll give it a try. \n. ",
    "niuage": "I really need this too.\nWith solr, you can call searchable several times, to build what fields should be indexed. This way, it's easy to extract this stuff into modules.\nFor instance, I have a Geolocated module, and I would just have something like:\nincluded do\n    tire.mapping do\n        indexes :location, type: 'string', analyzer: 'snowball\n        indexes :lat_lon, type: 'geo_point'\n    end\n end\nAnd I would not need to repeat these lines in every geolocated model. It would be awesome if you could provide a way to do this.\n. When you sort by distance, I read that there is a way to get the computed distance of each document to the searched location. But looking at my raw results, I can't see it. I'd appreciate some help with this, thanks :)\n. Thanks :) My _score fields are nil though.\nDoes it have something to do with track_scores?\nFrom the elasticsearch doc: When sorting on a field, scores are not computed.\nBy setting track_scores to true, scores will still be computed and tracked.\n. It think that's all I needed to understand: \"filters do not affect facets\"...\nI feel like it's making facets less useful, at least to me.\nI guess it's not possible to transform any filter into a query, right?\nThanks for your answer :)\n. Ok so after some time I realized that I could replace pretty much all my filters by queries:\nInstead of\nfilter :range, from: { lte: bod + 1.day, gte: bod }\nI have\nquery.boolean do\n    must { range :from, { lte: bod + 1.day, gte: bod } }\nend\nSo my last question is: is there any way to make my geo_distance filter affect my facets?\n. I'll try that.\nConcretely, my problem is that is have a location field in my search form, which filters the results obviously, but dont affect the facet counts (because geo_distance is a filter), which make them seem wrong.\n. I also found a way to handle that, I was just wondering if tire had some kind of built in way to do it. Thanks for your answer :)\n. So, is wildcard \"title\" \"f*\" just a nicer way to write string \"title:f*\" or is there a difference?\n. Thanks.\nBut filters dont affect facet counts, right? Unpublished documents would then make my facet counts appear wrong.\nI'm using mysql to store my documents, so the field exists, it's just nil.\n. Ok, thanks a lot ;)\n. I updated my question and made it simpler.\nedit: if you want, you can answer on stackoverflow as well (http://stackoverflow.com/questions/10354002/elasticsearch-tire-regexp-in-query-string-not-returning-a-matching-document)\n. It was because of the snowball analyzer, when I use the standart one, it works fine.\n. I actually tried to use a custom ngram analyzer based on one of your gist, but I had trouble adapting it to rails. But I realized there's an example in the readme ^^, so I'll definitely try that.\n. I followed the readme example and it works great :)\n. Turns out it's my fault, Task is actually a reserved keyword in Ruby on Rails...\nhttp://oldwiki.rubyonrails.org/rails/pages/ReservedWords\n. I'm searching on ngram_title, and I'm 100% sure because otherwise, the autocompletion wouldn't work at all.\nedit: ngram_title is actually just a method returning the document title.\n. It was actually my search analyzer... The \"standard\" analyzer removes stop words.\n. Hey,\nNo, I completely reset everything, database, and index, and still the same problem (btw I renamed socket_link_count to linked_socket_count)\nI don't think the Item class is a problem because my search is working nicely, except for this.\nAlso, I actually have an index per type of items, and no items mapping, it was just to make the question easier.\nI didn't exlude sockets_store from the indexed json, so yeah, socket_count and the other attributes appear in it. But ideally this field wouldn't be there. I'll clean up the indexed data later.\nHere is the complete json of a Bow: https://gist.github.com/niuage/5093359\nBtw, thanks for the help, nice to know we can always count on you :)\n. I'm going to try to add the key-value store accessors to the list of methods in to_indexed_json.\n. Ok, it worked when I do that: \ndef to_indexed_json\n    to_json(include: [:stats], methods: [:socket_count, :linked_socket_count, :sockets])\nend\nLooks like the fact that they are not actual item attributes forces me to add them to this method list. It kinda makes sense... so I don't think there's anything to fix.\nThanks a lot :)\n. Looked at the code, and it seems like it's not possible yet, so I just hacked a solution for now.\n. Ok thanks! (you mean \"so that it doesn't ignore your special _id method\", right?)\n. Ok, since I have you, let me ask you this: (don't worry about it if you don't have time, but you probably have a lot more experience than me on this so it might be obvious)\nI have a parent/child relationship between items and players:\n```\nitem mapping\ntire do\n    mapping(\n        _parent: { type: 'player' },\n        _routing: { required: true, path: :account }\n    ) do\n        indexes :account, type: \"string\", index: :not_analyzed\n end\nplayer mapping\ntire do\n    mapping(\n        _routing: { required: true, path: \"account\" }\n    ) do\n        indexes :_id, type: \"string\", index: :not_analyzed\n        indexes :account, type: \"string\", index: :not_analyzed\nend\n```\nTo index my items, I overrode the update_index method, because items are indexed in different indexes depending on their league_id, and also so that I can provide the parent and routing:\ndef update_index\n    run_callbacks :update_elasticsearch_index do\n        if destroyed?\n            index.remove self\n        else\n            options = { percolate: percolator}\n            options.merge!({ parent: account, routing: account }) # if I do \"parent: player.id\" here, it works\n            response = index.store(self, options)\n            tire.matches = response['matches'] if tire.respond_to?(:matches=)\n            self\n        end\n    end\nend\nMy issue is that now that I've changed the _id of players to be their account name (and I've checked they're indexed correctly, with the account as their _id), my has_parent query doesn't work, as if the items are not linked properly with their player.\nThanks.\n. How do you do that? I'm passing the routing option set to account when indexing my items. Is there another way?\nedit: the second mapping is the Player mapping.\n. After setting the _id to the account property, it actually worked. There was an issue on how I was updating the parents, that's why my query was not returning anything...\nThanks.\n. ",
    "demersus": "Wow, this would be a great addition! I find myself duplicating the same mapping fields across multiple subclassed models.\n. This would be nice to have in the main gem!  I will need to use this patch immediately in my app with namespaced models.\n. I don't know that those issues directly solve the one I am experiencing. However they do address the root issue why I started playing around with the document_type settings.  I don't think update_index should be able to over write any model attributes.  At least not any that could potentially be persisted to the database after a manual index update.\nThank you for your huge contribution to the community with Tire.  I understand I am complaining without offering solutions. So I will take a stab at a solution when I have some time.\n218 does look like a good solution to the multi model issue. Is there something holding you back from pulling that fix into master?\n. I actually have a use case for multiple indexes & mappings per model.  Our website is an online marketplace.  We have local classified listings, and online store products.   We have multiple models which fit the \"local\" listing description, each with their own index.  (Classified, Vehicle, RealEstate, etc...)   Now, I would also like to index some of them in a separate index for items that can be purchased online and shipped. (Store products)    A store product can be both a local classified, and a shippable product.  Not all store products will be listed as a local classified, and not all local classifieds will be purchasable online and shipped.   \nIt all depends on what the searcher wants.  Do they want to find items in their vicinity, or do they mind having it shipped?\nI guess this is a weird edge case....   But being able to manage multiple indexes, and mappings per model would be great!\n. ",
    "bbonamin": "Sadly we recently ran into the same problem.\nThe long time to log is caused by exactly that line:\nruby\njson.to_s.each_line { |line| content += \"# #{line}\" } unless json.to_s !~ /\\S/\nThe problem is using += to concatenate the string. \"+=\" is known to have very bad performance, because it instantiates a new object every time a new assignment occurs:\n``` ruby\n\n\na = 'a'\n=> \"a\"\na.object_id\n=> 70292542636400\na += 'b'\n=> \"ab\"\na.object_id\n=> 70292542545600\n```\n\n\nThe solution is to change the += for the \"shovel\" operator, which reuses the same object.\nE.g. with a 20 thousand lines JSON response:\n```\nBefore the change\n99.74%  48.04 seconds String#each_line\nAfter the change\n74.91%  0.11 seconds String#each_line\n```\nI guess it's sad this library isn't mantained anymore, because we could patch it.\n. @karmi Sorry if I offended you, that wasn't my intention. These type of comments made me think that this wasn't actively mantained anymore, but now that I've read your Re(Tire) announcement, I understand that fixing bugs is intended.\nI agree that of course on production settings the trace should be disabled, but improving 100x the performance of the tracing should be simple, and could help people avoid these type of problems in the future, where you might be clueless as why your performance between development and production is so different.\nWould you be willing to accept a pull request with these changes?\n. ",
    "edmundsalvacion": "Hi, it's something I need. I could certainly move it over to tire-contrib.\n. Question, do you have any guidelines on the modularity of items in tire-contrib?\nAlso, what do you see as appropriate to be in the main project vs contrib as far as the elasticsearch apis go?\n. ",
    "TimothyKlim": "Hi, thanks for patch!\nWhy this patch is not included in core? Without this feature, search result is ugly(often).\n+1 to core\n. > What do you mean?\nWhen you know that search result is not relevant when score below the value 0.5. Best way to hide irrelevant results is specify the \"min_score\".\n\nfirst day and first my usage experience of tire\n\nAfter I've read tire manuals, I've started search \"How to set minimum score?\" and I found elastic guides and this pull request.\n\nThat may just what you wanted in a tiny slice of use-cases. Most times, you want to actually display some results...\n\nAnd we will have irrelevant results on the search page.\nThanks.\n. ",
    "sg552": "hi @Karmi, I am a Chinese user. Tire is great, but I didn't see the \"min_score\" function. Since in Chinese the street name is extremely complex to analyze. such as: \"\u592a\u9633\u5bab\", (which mean \"sunny plaza\") however without \"min_score\", most of the result is not relevant and below score 0.3.  so I think I need this option. \nI will commit the code to \"tire-contrib\". ( though I am monkeypatching the tire core ^_^ )\n. ",
    "axsuul": "I don't see a pull request on tire-contrib for this. Should I do this?\n. Curious, why wouldn't this sugar also be available for block searches?\n. Any suggestions on how to debug this? I'm currently experiencing this and no mapping is being created but I have no idea what could be causing it.\nFor my case, only the type is getting set and everything else like index: \"not_analyzed\" seems to be getting dropped.\n. @simoncozens @karmi Thanks for your help, that was it :+1: \n. ",
    "bashcoder": "For what it's worth, I did manage to dismiss the single mention of this in the README, probably because it is only said to be something you could do, to be on the \"safe side.\"\nIn my case, it was a must do to in order get Tire working with Mongoid, otherwise I would get a \"stack level too deep\" error.  So I guess would suggest that the Article.search examples be changed to Article.tire.search instead, since that method would always work. If only for the Mongoid section.\n. Exact same issue here - conflict with Devise initialization. Same fix by @DaveTsunami worked for me.\n. ",
    "marioaquino": "+1 for using SimpleDelegator\n. ",
    "adkron": "Karmi and I spent some time talking on IRC. I just want to get some of that information propagated back to the ticket.\nI wasn't upset with the code. Tire works great. I love TIRE!\nOverriding class is a usability issue for many developers. When debugging they interrogate the class and based on the information they get back there are some assumptions made about the capabilities of the object. Those assumptions here are really wrong.\nOn further inspection of the object respond_to? returns false for most methods because it overrides method_missing to give those capabilities. This maybe another ticket, but it would be great if respond_to? would be overridden in the same way. Although with the current implementation the object really responds to any and all method names you can throw at it.\nIn this case I have an assumption that my object is my model. So I try the following:\nresult.my_name #=> 'amos'\nresult.my_name = 'Amos' #=> No error\nresult.my_name #=> 'amos'\nThat is a surprising result.\n. ",
    "tieleman": "Well, what caused me some extra time today was exactly this. I have an existing (read-only) Elasticsearch store with a couple of indices that map to plain Ruby objects that implement the ActiveModel API to support routing, naming etc. in Rails. In these Ruby objects we define certain methods to do stuff with the data from Elasticsearch, think of things like combining names or formatting stuff. What isn't very nice about how Tire::Result::Item is currently implemented is that it only delegates #class to the object. If you've defined extra methods in your classes they can't be accessed from the Item. So if I have a class definition like this (where first_name and last_name come from ES):\n``` ruby\nclass Author\n  include MyActiveModelImplementation\n  include Tire::Model::Search\ndef name\n    [first_name, last_name].join(' ')\n  end\nend\n```\nIf you search ES for authors you will get Items that pretend to be Authors, but aren't. So you can't use any custom defined methods. Best example of how Tire not fully implements the required methods to be a \"pretend\" class, try this from a console with a item from a result:\n``` shell\nitem.class\n=> Author\nitem.is_a?(Author)\n=> false\nitem.name\n=> nil\n```\nAs you can see it claims to be an Author, but when you query that using is_a? you get false. You also get nil for any undefined attributes.\nWe circumvent this by creating a proxy object that inherits from SimpleDelegator and pass that as the wrapper object to Tire and have it decide at runtime what it should map to and how it should delegate. The only thing Tire currently does is delegate #class to super and return nil for every unknown attribute. The proxy object simply assigns attributes like Tire does, but also acts as a proxy for the \"real\" object so you can have all your class/instance methods and variables at your disposal.\nThe ActiveModel bit is actually extra here, the same issue would still arise if you just use plain Ruby objects. You lose the ability to use any extra stuff you have defined on your classes. I would say that if it is going to pretend to be an object it should mimick the behaviour of said object by delegating any missing attributes/methods (and potentially even allow for overriding them).\nA very simple proxy that would instantiate an object based on _type would be something like:\n``` ruby\nclass ProxyObject < SimpleDelegator\n  delegate :class, :is_a?, :to_hash, :to => :_proxied_object\ndef initialize(attrs={})\n    klass = attrs['_type'].camelize.classify.constantize\n    @_proxied_object = klass.new(attrs)\n    super(_proxied_object)\n    define_methods attrs  # this call creates all relevant accessors for the attrs and methods (method_missing)\n  end\nprivate\ndef _proxied_object\n    @_proxied_object\n  end\nend\n```\nIt's a bit of a tricky thing to explain, but like I said, if it's going to delegate #class I think it should play along nicely and be a proper delegator/proxy and I think that's the point the original poster tried to convey.\n. Yes, seems fair enough. Note that in my examples I wasn't actually using ActiveRecord models, but plain Ruby objects that implement ActiveModel. But, your point still stands: we are creating objects (using new) that aren't really there, but that's what ES is all about, you are retrieving the data from somewhere else, so some magic is bound to occur.\nAnother suggestion (which I haven't worked out) would be to have the resulting Item inherit from the \"real\" class, so in the example above Item would inherit from Author. That would carry over certain instance methods etc. for use in the subclass. But that would just create more headaches down the road I think (protected and private methods come to mind).\nYour suggestion about creating your own wrappers is actually precisely what we're doing at the moment in our setup. We retrieve the data from ES, then construct our faux models ourselves using a different wrapper than Item. We have no underlying SQL database. That's why we're not using ActiveRecord, but merely are implementing ActiveModel to get all the fancy routing/templating stuff from Rails.\nI was just contributing to the discussion here (clarifying if you will), because I do think that it's something a lot of people might be interested in. Ofcourse they are free to implement their own wrappers based on what I've suggested in my earlier comment. Might be nice to have a Wiki page about this subject? I'd be willing to write a how-to on how to (har har) do this and provide sample code etc.\nAnd agreed about the Item#is_a?, that would make it more complete.\n. I've started work on the Wiki page, it is not complete yet by any means, but hopefully it will serve as a summary and a bit of general information about wrapping results.\nhttps://github.com/karmi/tire/wiki/Data-storage-in-Tire-and-wrapping-the-results\n. I've finished the write-up for now. I'd appreciate it if anyone would be able to give it a read and let me know if it's understandable. I'll add it to the Wiki homepage.\n. Fine, it was a bit hard to distill from the above discussion what use cases people were trying to achieve. I'll strip the AR bit and just make it plain objects.\n. Am I correct in assuming that if you use the :load option to retrieve objects from your ActiveRecord/Mongoid store you will no longer have access to the ES meta-information (including _score and stuff like highlighting)?\n. It's a different type of query in elasticsearch, see:\nhttp://www.elasticsearch.org/guide/reference/query-dsl/wildcard-query.html\nAllows for stuff like: { wildcard  => { tags => 'm*g?c' } }\n. Very well, I'll see if I can cook up a patch.\n. ",
    "te-chris": "+1 to what @tieleman proposes.  I'm not entirely sure what the architecture of such a solution should look like, but being able to somehow access instance variables from tire Item's would be a very good feature.\n. That's fine (well, not really in my opinion, but that's ideological, not practical) if these were just simple objects-as-hashes, but they're pretending (in the context of my use) to be ActiveRecord/Mongoid objects, which don't allow method-missing to fail silently.  This leads to obscure bugs, and, given some of the confusion around the way Tire represents these objects in a rails context (including my own when I first started using the gem), I personally think if these objects are going to use the name of their AR counterparts they should have similar error behaviour.\nBeyond that I'm also in the Gary Bernhardt camp of not being a big fan of returning nil as it can create insidious errors that are hard to debug.\n. It's up to you really, I don't think we're going to agree on this but we don't have to - I can easily work around it.\n:load is an undesirable performance penalty as I'm sure you're aware so I don't really want to have to hit two data stores.  My concrete example was spending a while yesterday tracking down a bug because instead of throwing an error when I was calling a method that didn't exist on Item (masquerading as AR) it was retuning nil and causing the error to occur away from where it was created.  To me this is bad practice, though I know how common it is in ruby-land, it's one of my biggest gripes.  Also, these objects aren't hashes, they're objects, which means IMO they should behave like objects.\n. ",
    "clintongormley": "@karmi I don't see the need for a background process to check which nodes are up - it's really fast, so I'd just do it synchronously. Makes things much less complex.\nIn the Perl API, I accept 3 parameters:\n- an array containing the default_servers list (defaults to localhost)\n- refresh_after - an integer indicating the number of requests to perform before doing a cluster refresh\n- no_refresh - a boolean indicating that we shouldn't sniff, but just round-robin through the existing nodes (useful when, eg the client is outside the internal network, or behind a proxy, and so uses different IPs to the cluster)\nA request does the following:\n- if refresh_in counter is zero (as it is on the first request) then:\n** if no_refresh is:\n***  false : \n* we try each server in turn (or in parallel for the async backends) to get a list of the live nodes\n* if none of the nodes respond, then we throw an error\n**** if a node responds successfully, we extract the list of live nodes, and store them in servers in random order, and we set refresh_in to refresh_after (to start the count down)\n*** true: set the servers list to the default_servers list\n- the first node in the servers list is used to perform the current request\n- if the current request returns an error:\n* if the error is not a connection error, then we rethrow it\n* otherwise, if no_refresh is :\n*** true: remove the current node from the servers list\n**** if we still have potentially \"good\" servers in the list, then use the next server in the list to retry the request, otherwise repopulate from the default_servers list and retry each node once, until either one succeeds, or all fail -> error \n*** false: try to sniff the live server list from the union of default_servers and previously sniffed servers. if:\n**** success: rerun the request\n**** failure: throw an error\n(Damn markdown doesn't render the above bullets correctly - hope this is still readable)\nCouple of things I plan on changing:\n1) make the timeout value for the sniff request much shorter than the timeout for other requests (to avoid eg lengthy timeouts when a switch is down or there are firewall issues)\n2) make the refresh_after parameter dynamic, eg if the cluster changes (a server fails) then it is likely that it will come up again pretty quickly, so re-sniff more frequently.  Once the cluster looks stable again, then refresh less frequently\n. ",
    "amfeng": "Any updates on this? Just wondering because the most recent comments were made around two months ago.\nDo you recommend going ahead with the rescue solution (94e7b89) or proxy for now?\n. ",
    "mereghost": "@kimchy @karmi That's what we are doing. And it works pretty well avoid all the hassle of setting up a HAProxy and whatnot. How long it will last I have no clear idea, but it seems that will take a long time.\n. Nice. I'll get to work and see what I can get working.\nThe main \"problem\" is that the endpoint is different (_mtl) so it might duplicate some of the Query inner workings.\nThanks again.\n. Yeah. That's the mlt query. The API compares the index against a document, which is too nifty for those of use using any kind of DB Mapper.\n. Okay. I'll do that and re-send the pull to tire-contrib. =)\n. Hello @karmi, any plans on supporting delete by query? If there is, I can work on the implementation and submit a pull request.\nCheers!\n. You need to either set the analyzer for the query or make it the default one for that index.\n. +1 (on tire-contrib)\nSeems like a prime candidate for tire-contrib (and I'd welcome it) =)\n. I had this problem occur to us and it boiled down to the OS getting all its\ndynamic ports assigned.\nOn Tue, Jul 2, 2013 at 2:47 PM, Dom Hodgson notifications@github.comwrote:\n\nI'm currently using Tire with Sidekiq, I'm doing some quite write heavy\nthings with Tire Persistance and things work fine for a few minutes but\nthen I get\nErrno::EADDRNOTAVAIL: Cannot assign requested address - connect(2)\nWith a stacktrace of\n/usr/lib/ruby/1.9.1/net/http.rb:763:in initialize'\n/usr/lib/ruby/1.9.1/net/http.rb:763:inopen'\n/usr/lib/ruby/1.9.1/net/http.rb:763:in block in connect'\n/usr/lib/ruby/1.9.1/timeout.rb:55:intimeout'\n/usr/lib/ruby/1.9.1/timeout.rb:100:in timeout'\n/usr/lib/ruby/1.9.1/net/http.rb:763:inconnect'\n/usr/lib/ruby/1.9.1/net/http.rb:756:in do_start'\n/usr/lib/ruby/1.9.1/net/http.rb:745:instart'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/rest-client-1.6.7/lib/restclient/request.rb:172:in transmit'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/rest-client-1.6.7/lib/restclient/request.rb:64:inexecute'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/tire-0.6.0/lib/tire/http/client.rb:11:in get'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/tire-0.6.0/lib/tire/index.rb:331:inretrieve'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/tire-0.6.0/lib/tire/model/persistence/finders.rb:26:in find'\n/home/rails/domainapi/releases/20130702172920/app/workers/update_all_data_worker.rb:9:inperform'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/processor.rb:49:in block (3 levels) in process'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/middleware/chain.rb:109:incall'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/middleware/chain.rb:109:in block in invoke'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-failures-0.1.0/lib/sidekiq/failures/middleware.rb:8:incall'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/middleware/chain.rb:111:in block in invoke'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/middleware/server/timeout.rb:14:incall'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/middleware/chain.rb:111:in block in invoke'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/middleware/server/active_record.rb:6:incall'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/middleware/chain.rb:111:in block in invoke'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/middleware/server/retry_jobs.rb:50:incall'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/middleware/chain.rb:111:in block in invoke'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/middleware/server/logging.rb:11:inblock in call'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/logging.rb:22:in with_context'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/middleware/server/logging.rb:7:incall'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/middleware/chain.rb:111:in block in invoke'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/middleware/chain.rb:114:incall'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/middleware/chain.rb:114:in invoke'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/processor.rb:48:inblock (2 levels) in process'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/processor.rb:87:in stats'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/sidekiq-2.9.0/lib/sidekiq/processor.rb:47:inblock in process'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/celluloid-0.12.4/lib/celluloid/calls.rb:23:in call'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/celluloid-0.12.4/lib/celluloid/calls.rb:23:inpublic_send'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/celluloid-0.12.4/lib/celluloid/calls.rb:23:in dispatch'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/celluloid-0.12.4/lib/celluloid/future.rb:18:inblock in initialize'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/celluloid-0.12.4/lib/celluloid/internal_pool.rb:48:in call'\n/home/rails/domainapi/shared/bundle/ruby/1.9.1/gems/celluloid-0.12.4/lib/celluloid/internal_pool.rb:48:inblock in create'\nFrom the outset it looks like a timeout error, however my elasticsearch\nserver isn't reporting any sort of issues (it's on a dedicated box with\n20+gig of ram).\nLocally I am running a lot of curl connections, however I've compiled curl\nwith c-ares which has stopped any timeouts related to that issue.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/issues/781\n.\n. \n",
    "mkdynamic": "Any updates on this?\n. Would it be feasible to use DNS to handle failover? Route 53 has health checks and failover support.\n. Thanks, that's helpful.\nMain concern with a proxy is that it introduces a single point of failure. Obviously potential performance and cost downsides too.\n\u200b\nIs client support for multiple nodes on the roadmap for Tire? Or are you convinced a proxy is a better avenue?\u00a0\n\u2014\nSent from Mailbox for iPhone\nOn Fri, Jul 19, 2013 at 6:17 AM, Karel Minarik notifications@github.com\nwrote:\n\n@mkdynamic The best solution would be to use a real proxy, such as HProxy, Nginx, etc. You can use Nginx as a round-robin proxy with keepalive pretty easy, see eg. https://gist.github.com/karmi/0a2b0e0df83813a4045f for config example.\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/162#issuecomment-21248493\n. \n",
    "ifesdjeen": "I'm not sure wether duplicating data is generally good or bad. It's definitely harder to support, but taking a wise approach, maintenance is minimized or eliminated. I certainly think that contributing to upstream is #1 goal, although sometimes giving a quick hint and a couple links to the upstream still seems good to me.\nSo far only thing I could tell is that reading though the readme with quite general headers (Installation / Usage / Integration), basically covering everything. Rocco docs were always difficult for me to read, because of line breaks and jumps, but it's just me. \nMy copy is certainly not best one (i often find myself writing \"have to\" instead of \"have an option\"), and order of things may certainly vary. Moreover, these things like boost, attributes and links to analyzers, tokenizers and other things may be not somewhere on the top, but not mentioning them is not the best thing to do as well (imo). Even though ES works out of the box, knowing how to configure things that most everyone requires in his app sounds good. \nFrom the rails app developer standpoint, when you develop a prototype of an app, you don't really have any data in app. So you often think of how data is indexed before you get the data. While your test data is added, hooks are called and it's indexed. I don't really know what's the percentage of the existing vs new apps that start using Tire though, but in my practice, I was always first creating mappings and indexes, and only later - cleaning up indices and re-creating them with new options or mappings. \nOf course, my thinking was not to get whole elasticsearch.org to the readme. Rather - showing connections between Tire and ES (if you specify data type, go take a look at which ones are there, if you put an analyzer at least get to know what they mean, what attributes mean and where to look them up, how to specify multiple sort field and create more complex queries etc).\nI'm just a user, and I was completely fine with going to elasticsearch.org and checking out some more complex things. Although I thought that it may be good to give a couple of iterations to readme / wiki to make some things more obvious without checking out source code and test suite. \nI'll keep thinking of the way to present that info. I'm starting work on Clojure driver, little by little. That may help me to understand things in a better way.\nThanks for explanation.\n. Agreed. \nWe'll take a couple of examples and work on organizing them in an explanatory way. \nWhenever we have something to show, we'll ping you for a review. \nThanks a lot for helping out!\n. Sorry, my bad\nthat issue is absolutely irrelevant. I completely misunderstood some things here.\n. Try keyword analyzer:\nruby\n{ :type => 'string', :analyzer => 'keyword' }\nIt will work.\nBtw, we used it exactly for tags.\n. @karmi probably he really needed an answer urgently :)\n. ",
    "amnesia7": "Interesting to know but I don't think I'll be going with a shared host in the end, but thanks for the reply.\n. I found on this page (http://www.elasticsearch.org/guide/reference/api/search/sort.html) a section called \"Geo Distance Sorting\" and one of the code examples is below:\n{\n    \"sort\" : [\n        {\n            \"_geo_distance\" : {\n                \"pin.location\" : [-70, 40],\n                \"order\" : \"asc\",\n                \"unit\" : \"km\"\n            }\n        }\n    ],\n    \"query\" : {\n        \"term\" : { \"user\" : \"kimchy\" }\n    }\n}\nCan tire work with this type of sorting?\nIt would also be good to know whether tire can work with the geo distance filter as well (http://www.elasticsearch.org/guide/reference/query-dsl/geo-distance-filter.html).\nThanks\n. Thanks guys, that's great to hear.\nIs this kind of sorting/ordering processor intensive on the webserver? Is it slow to run or is it ok with indexes on lat and lng columns? \nI assume applying some kind of bounding box filter would be good if there are too many results coming back.\n. I'm just wondering whether I should really just setup the tags model as I see fit and map a database view to ActiveRecord (http://books.google.com/books?id=thTju-4duY4C&lpg=PP1&ots=nFmsRSwKu0&dq=enterprise%20rails&pg=PA147#v=onepage&q&f=false) to gather all the information I need into one place for searching which would make sense.\nDoes Tire support searching against a database view mapped using ActiveRecord?\nIf so, are the search and callbacks still applied to each of the models used by the view or would they only be applied to the new ActiveRecord for the view?\nAny code snippets would be much appreciated.\nThanks\n. Thanks @karmi,\nI've been having a go with option 1 and I've got it to work (although I'd need to tweak it to work when no results are returned):\nproject_ids = Project.search(params[:q]).map(&:id).join(\",\")\n@tasks = Task.joins(:project).where(\"project_id IN (#{project_ids}).paginate(:page => params[:page])\nI'm a bit concerned that this could lead to a large array of project_ids to pass into the @tasks query which could affect performance. I think the project_ids and @tasks would ideally be merged into one somehow.\nI then tried the following:\nTask.find_by_project_id(Project.search(params[:q]).map(&:id))\nbut this seems to leave me unable to chain .joins(:project) or .paginate(:page => params[:page]) on\nAlso, as much as rails console only shows one SQL select for this query, I expect it is doing two selects, one to get the project ids and another to select tasks using those ids.\nCan you suggest any way to get around the issue of having to put the results of the first select (projects) in an array and pass it into a second select (tasks with associated project info)?\n. I suppose I need to somehow get the task_ids of the ones that I need to show all info for.\nI need to watch out because I have a couple of different options for ordering and one of them is based on one of the task fields (milestone_date) so I can't just select the projects and then assume I just need to grab the related tasks.\nI was wondering whether I could do something like Task.search() and include project as an association of task, however, projects also have many tags which would be a third level that I want to be able to search and I thought that might be pushing it a bit.\n. Thanks for your help @karmi.\n. I've tried amending my app to work from Project.search() instead of Task.search() but still can't show the link in the search results because it falls over when I try to use project.user as the link destination:\n<%= link_to project.user.name, project.user %>\nwith the same error message:\nundefined method `tire_results_item_path' for #<#<Class:xxxxxx>:xxxxx>\nCan someone give me a clue how to return the associated user for the project so I can include the user link in each of the search results.\nThanks\n. I've corrected by issue by adding load: { :include => ['user'] }, ie. .search(params[:q], load: { :include => ['user'] }).\nIt seems a shame to have to load info from the database just so that the user_path(x) in my link (project.user) is generated correctly in my search results.\nDoes load: { :include => ['user'] } do a LEFT OUTER JOIN?\nDoes this mean that all my users table is being loaded just for the 10 results on screen of which there may only be 4 distinct users to show results for?\n...I've just realised that I can change my user link to be <%= link_to project.user.name, user_path(project.user.id) %>. Does that sound like a reasonable alternative?\nAlthough, as part of my search results I also format a date so I assume I am better using load: true so that this can be called on the object unless there's another way to format a datetime returned in the search results.\n. Managed to get it working from the task model. Here's the final code in case anyone needs it.\n```\nclass Task < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\n# Define the mapping\n  mapping do\n    indexes :id,          type: 'integer'\n    indexes :milestone_date,   type: 'date'\nindexes :project do\n  indexes :id,          type: 'integer'\n  indexes :name,        type: 'string'\n  indexes :details,     type: 'string'\n  indexes :user_id,     type: 'integer'\n\n  indexes :user do\n    indexes :id,          type: 'integer'\n    indexes :name,   type: 'string'\n  end\nend\n\nend\n# Define the JSON serialization\n  def to_indexed_json\n    to_json( include: {\n      project: {\n        only: [\n          :id,\n          :name,\n          :details,\n          :user_id,\n          user: {\n            only: [\n              :name\n            ]\n          }\n        ],\n        :methods => [ :user ]\n      }\n    } )\n  end\n```\nThe link to the user's page is:\n<%= link_to task.project.user.name, user_path(task.project.user_id) %>\n. The date was falling over because of another reason so that can be ignored.\nI've stuck with user_path(project.user_id) for now so I don't need to call :load for any reason now.\nI think I may be a little new to the RoR game to offer any failing or passing tests yet. I assume its not as simple as just needing to call the ActiveRecord preload thing for any embedded objects so that the methods/functions they use are available.\nThanks\n. Hi @karmi,\nI've come across another need for working with \"embedded\" objects in search results.\nI can work around it by passing task.project_id and 'Project' into my function for the id and type I was wondering if you had had any ideas about how a sub-level item could be worked with since I would like my function to just require the object to be passed in and then get the id and the type from it inside the function.\nThanks\n. @karmi, \nI am working with a polymorphic model in my current app and the ability to refer to a search result as an object rather than having to split out the id and the type manually would be very beneficial.\nI was wondering if you had had any thoughts on this.\nAs for the failing and passing tests; would it need a mapping with one (or more) nested objects (eg Library.books.chapters or similar) and then test the search result object to get the object.class.name.demodulize (type) and make sure it has the type Chapter (for instance) whereas a failing test would return the type Relation ?\nCol\n. If sub-level result items were rails objects then would implmentations of to_param work just as they do for regular rails objects as well?\nWhen overriding to_param the tire results just output the id in the href rather than id-name that I've specified in to_param.\nCol\n. @karmi, I know it's an old posting but the issue I'm having is with the object that tire is returning for sub-level objects is a results item that doesn't work in the same way that normal ActiveRecord rails objects work, ie, that a link_to can reference @post and create a link pointing at that posting, or that such a link will take into account to_param and create the link in the form id-name if that's what I set in def to_param without having to workaround it by generating a virtual slug field just for use in views containing info retrieved from the elasticsearch/tire index.\nCol\n. def lat_lon\n    [latitude, longitude]\n  end\nshould have been in the Project model, not the Task model.\n. Cool. Works a treat. \nThanks.\n. Yeah, no problem.\n. Just found the elasticsearch.log file and the problem isn't with Tire or ES but a date virtual attribute I have that it can't parse so closing issue.\n. It was using favourable object passed into the create/destroy so it just needed reloading in the callback using:\ndef update_index\n    favourable.reload.tire.update_index\n  end\n. ",
    "gedwards": "The tests, including Nathan's addition, all pass after running:\nbundle install --path .bundle\nbundle exec rake test\n. ",
    "woodhull": "Karmi, \nJust added a quick integration test!\n-Nathan \n. ",
    "al": "@amnesia7, It can do the geo_distance filter alright ...\ntire.search(options) do |search|\n  ...\n  search.filter :geo_distance, {\n    :location => [lat, lng],\n    :distance => \"#{range}km\"\n  }\n}\nbut it looks to me like we've got to be a bit more manual for geo sorting. Am I wrong?\n. @amnesia7 So here's something that works, but it's not really cool ...\ntire.search(options) do |search|\n  ...\n  search.sort {\n    by :_geo_distance, {\n      :location => [lat, lng],\n      :order => 'asc'\n    }\n  }\nend\n. ",
    "mattiassvedhem": "@niuage It should be in the _score field. You can do something like this in Rails\nnumber_to_human(result._score, :units => :distance, :precision => 0)\n. According to the ElasticSearch documentation the order should be [lng, lat] for geopoint. But @aaronchi 's example says [lat, lng] Does Tire switch these?\n. Ok, cool =).\n. artist_search = Artist.search do |search|\n  search.query do |query|\n    query.boolean do |boolean|\n      boolean.must { string :has_music => true }\n      boolean.must { string @query }\n    end\n  end\nend\nProbably like that?\n. Ok, just to clarify, if I'm using hash queries I need to use it for everything, right? There's no way to mix in a hash query in an existing DSL query?\n. Yes, ofcourse.\nWouldn't it be sensible to have a method in the DSL for hash queries? So we could easily mix them with our existing queries?\n. Is there a work around for doing this ?\n. Hi @karmi any update on this? What could be done to move in this direction?\n. @karmi cool, has any work been done on this? if not I might take a stab at it.\n. Awesome @karmi! Any idea when it will make it to the gem? Thanks.\n. You could consider implementing to_param in a shared Draper decorator. That way you can reuse the implementation between the Item and the model class. Also, you'll get a bonus, since it's a more appropriate layer to implement it at anyways, arguably.\nOf course, you would still have to pass any attributes needed to construct it, down to ElasticSearch.\n. @martijn You probably need to specify the date mapping for the given field.\nThe objects you are interacting with is instances of Item It's not the underlying Mongoid Model, unless you specify :load => true.\n. @martijn Yep, in Rails, Item instances masquerades as the model.\n. +1.\n. U can use \nTire.index INDEX_NAME do\n  create mappings: {\n    bar: { properties: {} },\n    foo: { properties: {} } \n  }\n. ping, @jwaldrip I've added the unit test to your fork, can you take a look? :)\n. Sure!\n```\nclass Item < ActiveRecord::Base\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nend\nclass ItemDecorator < Draper::Decorator\n  def full_name\n    source.first_name + source.last_name\n  end\nend\nclass ItemsDecorator < Draper::CollectionDecorator\n  # some grouping or other method concering multiple items.\nend\nitem = Item.new(first_name: 'Karel', last_name: 'Minarik')\ndecorated = ItemDecorator.new(item)\ndecorated.full_name = 'Karel Minarik'\nOR\nitems = Item.all\ndecorated_collection = ItemsDecorator.new(items.all) # Decorates each of them with ItemDecorator\ndecorated_collection.first.class\n=> ItemDecorator\n```\nI guess the sharing of an item decorator could be achieved this way:\nTire.configuration.wrapper = Item\nresults = Item.search(first_name: 'Karel') \nresults.each do |item|\n  decorated = item.decorate # Draper provides this method by mixin in a module, Draper::Decoratable, in the model.\n  decorated.full_name\n  => 'Karel Minarik'\nend\nHowever with that approach I couldn't easily keep the Collection and have my decorated objects via #results.  \nWhat I'm after would be a simpler way to decorate both the Item and the Collection.\nI don't necessarily want to replace either Tire::Results::Collection or Tire::Results::Item but I would like to be able to wrap a class around them and retain the methods of Collection and Item.\nFor example by adding delegate_all to the decorator I could delegate all the methods called on it to Tire::Results::Item or Tire::Results::Collection and just benefit from having extra methods available for the view logic.\n```\nitems = Item.search(first_name: 'Karel')\nitems.class\n=> ItemsDecorator\nitems.source\n=> Tire::Results::Collection\nitems.first.class\n=> ItemDecorator\nitems.first.source\n=> Tire::Results::Item\nitems.first.full_name\n=> 'Karel Minarik'\n``\n. I'll do some experiments and see what I can come up with.\n. Turns out this already works pretty good! Although there is some things that does not work as expected, I'll investigate them further and figure out if it's an issue with draper or tire.\n. The only thing that doesn't work is the automatic delegation to the source object, this doesn't work since Draper's AutomaticDelegation implements amethod_missing` which checks if the source object responds to the method before delegating it.\nIt's not really that big of a problem, as we can just delegate all the methods manually, but I guess it could be solved by switching out Tire::Results::Item to an object that actually defines the hash keys as methods dynamically and not relying on method_missing \u2014 But I guess there's a reason for not doing so?\nEdit: Although I just saw this, which means that it should work as is..\ndef respond_to?(method_name, include_private = false)\n  @attributes.has_key?(method_name.to_sym) || super\nend\nbut:\nitems = Item.search(first_name: 'Karel')\nitems.first\n=> <Item (Item) id: \"4f99f0eea2b279ec3d000006\", first_name: \"Karel\", , _score: nil, _type: \"item\", _index: \"items\", _version: nil>\nitems.first.respond_to?(:first_name)\n=> false # Shouldn't this be true?\nitems.first.first_name\nedit2: Turns out I tested on 0.4.x, works correctly in 0.5.x.\n. No, we don't really need to switch out the wrapper, that was just an initial thought on how to do it.\n1. We could define just a ordinary Draper::Decorator\nItemDecorator < Draper::Decorator\n  delegate_all # or just the methods we want to pass on to the Tire::Results::Item instance. \n  # ... and put some methods here\n  def name\n    source.first_name + source.last_name\n  end\nend\nWe can then use it like this\nitems = Item.search(query: 'something')\ndecorated_items = ItemDecorator.decorate_collection(items)\nThat would give us an instance of Draper::CollectionDecorator which is the default collection decorator in draper.\nSo we could then do:\ndecorated_items.first.name\n=> 'The name'\nHowever, what we cannot do with this approach is to get to any facets or other methods defined on the Tire::Results::Collection object.\ndecorated_items.facets\n=> NoMethodError: undefined method `facets' for #<Draper::CollectionDecorator:0x007f836d9888f0>\nThat's because the default collection decorator does not automatically delegate those methods to the source Tire::Search::Collection instance. So if we don't need to access those properties, we are fine with this method. \nOtherwise we'll need to do it like this:\n2. Define a Draper::CollectionDecorator\nItemsDecorator < Draper::CollectionDecorator\n  # There's no delegate_all here so we'll need to delegate all methods we want to pass on to the Tire::Search::Collection\n  delegate :facets\nend\nWe can now use this collection decorator directly to decorate both the collection and the items. Draper will automatically look for a decorator named ItemDecorator, loop through our hits and wrap them with it.\n```\nitems = Item.search(query: 'something')\ndecorated_items = ItemsDecorator.new(items)\ndecorated_items.facets\n=> { hash with the facets }\ndecorated_items.first.name\n=> 'The name'\nWe also still have the methods from Tire::Results::Item at arms length\nsince we used delegate_all in our ItemDecorator.\ndecorated_items.first._score\n=> '0.30685282'\n```\nNow we can reuse our view models between Tire and ActiveRecord/Mongoid, as long as they adhere to the same interface (attributes).\nIf you want I can write a wiki entry with a detailed explanation on how to use Tire with Draper.\nAh, I see, that makes sense, it does work with the 0.5.x versions, I accidently used an older version of Tire which didn't have the modified respond_to?.\n. Yeah I'm glad it worked too! We'll start using it in our applications.\nI added a wiki entry here.\nLet me know if there's something that needs further explanation, I think it's pretty straight forward though!\n. I monkey patched it like this:\n``` Ruby\nmodule Tire\n  module Results\nclass Collection\n def __get_results_without_load(hits)\n    if @wrapper == Hash\n      hits\n    else\n      hits.map do |h|\n        document = {}\n\n        # Update the document with content and ID\n        document = h['_source'] ? document.update( h['_source'] || {} ) : document.update( __parse_fields__(h['fields']) )\n        document.update( {'id' => h['_id']} )\n\n        # Update the document with meta information\n        meta = ['_score', '_type', '_index', '_version', 'sort', 'highlight', '_explanation', 'fields']\n        meta.each { |key| document.update( {key => h[key]} || {} ) }\n\n        # Return an instance of the \"wrapper\" class\n        @wrapper.new(document)\n      end\n    end\n  end\nend\n\nend\nend\n```\nIt might be a naive solution, and it's a little bit weird to wrap fields in an Item instance, but I feel that it's more consistent with the way it's returned by ES. i.e not at the root.\n. @karmi cool, you mean #663, right? =)\n. Thanks for outlining the different options Karmi!\nI think they are all viable in different situations. In most cases, I ended up creating a separate class, which takes the Search object, and is then responsible for calling the appropriate methods on it.\nSomething like this:\n``` Ruby\nrequire 'tire'\nclass Sort\n  def initialize(search, params)\n    @search = search\n    @point  = params[:point]\n  end\ndef by_distance\n    @search.sort.by :_geo_distance, {\n     :point => @point,\n      unit: 'km',\n      order: 'asc'\n    } \n  end\nend\nTire.index('articles') do\n  store id: 1, title: 'One', tags: ['ruby','python'], point: [3.53535, 5.535353]\n  store id: 2, title: 'Two', tags: ['javascript'], point: [3.53535, 5.535353]\n  refresh\nend\nparams = { point: [3.53535, 5.535353] }\nsearch = Tire.search 'articles' do |s|\n  Sort.new(s, params).by_distance\nend\nputs search.to_curl\n```\nSo, yeah. Probably better not to change it.\n. Yep, close it :)\n. You can do something like this:\n``` Ruby\nclass Profile\n  def self.search(params)\n    defaults = { some: 'default_param' }\n    options = defaults.merge(params)\ntire.search do\n  # use the options\nend\n\nend\nend\n``\n. Turns outTire::Search::Searchalready takes wrapper as an option! \nWould be nice to be able to pass the options directly toTire.search` though? Or is this the preferred way?\n```\nsearch = Tire.search do\n  # stuff\nend\nTire::Search::Search.new(\n  'menus/menu',\n  payload: search,\n  wrapper: ArticleItem\n).results\n``\n. Thanks Karmi, you're right, wasn't clear in my example, when passing a block it does work. \nHowever it does not work when just passing theindicesand thepayload` parameters.\nI suspect the problem is here, the naming here seems a little bit convoluted, since if as block is given, the argument payload is really a hash with options, not exclusively the payload, right? But if no block is passed the code expects it to only be the payload, and no further options.\n. i.e this, Tire.search('articles', payload: search.to_hash, wrapper: Article) would result in this hash being passed to Tire::Search::Search\n{ payload: { payload: # the actual payload, wrapper: Article } }\n. It's not a very elegant solution, improvements are welcome.\n. @karmi No worries! Yeah, I can see that, is there another way to augment a search instance with unsupported ElasticSearch features? Or do you feel that it should it be made to work another way? Maybe there could be a method allows for direct augmentation of custom hashes, that then gets merged in to_hash, but that might be tricky.\n. ",
    "WvanLelyveld": "I'm having difficulties getting the closest location when sorting objects (with multiple locations) by distance to a given point. See my issue here. If anyone has any experience with this and can tell me what I'm doing wrong, any help would be greatly appreciated!\n. @karmi.. Thanks for your reply and sorry for the late answer, I didnt see your message.\nThis is my output that actually gives me results (although with the wrong distance):\n{\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"query\":\"other\",\n\"default_operator\":\"AND\",\"fields\":[\"descriptions.name\",\"group.name\"]}}}},\n\"sort\":[{\"_geo_distance\":{\"mode\":\"min\",\"geolocation\":[52.51,5.47],\"order\":\"asc\",\"unit\":\"km\"}}],\n\"size\":20,\"from\":0}\nThis is how I expected it to work, but gives me an error:\n{\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"query\":\"other\",\n\"default_operator\":\"AND\",\"fields\":[\"descriptions.name\",\"group.name\"]}}}},\n\"sort\":[{\"_geo_distance\":{\"geolocation\":[52.51,5.47],\"mode\":\"min\",\"order\":\"asc\",\"unit\":\"km\"}}],\n\"size\":20,\"from\":0}\nThe error is: nested: ElasticSearchIllegalArgumentException[No mapping found for field [mode] for geo distance sort];\nDo I need to add something to my mapping to make this work?\nThis currently is:\nindexes :locations do\n      indexes :descriptions\n      indexes :geolocation, type: \"geo_point\"\n    end\nThanks for any help!\n. That's already in there, as you can see in the queries in my previous post. This is still not returning the closest location. Did you suggest something, or just wondering if it had already been fixed?\n. ",
    "oniram": "@WvanLelyveld Do you resolve your problem with \"mode\":\"min\" ?\n. ",
    "bcackerman": "+1\n. Here's how I got search and sorting by closest distance to work:\n``` ruby\ntire do\n  mapping do\n    indexes :lon_lat, type: 'geo_point'\n  end\nend\ndef self.search(params)\n  # get coordinates\n  params_longitude = params[:longitude]\n  params_latitude = params[:latitude]\n  center_point = [params_longitude, params_latitude]\n# perform search\n  tire.search do |s|\n    s.query { string params[:find_what], default_operator: \"AND\" } if params[:find_what].present?\n    s.sort do \n      by :_geo_distance, {\n        lon_lat: center_point ,\n        order: \"asc\", \n        unit: 'mi'\n      }\n    end\n  end\nend\nself.include_root_in_json = false # get rid of the root json element\ndef to_indexed_json\n  {\n    lon_lat: lon_lat,\n    latitude: latitude,\n    longitude: longitude,\n  }.to_json\nend\ndef lon_lat\n  {\n    lon: longitude,\n    lat: latitude\n  }\nend\n```\n. Ok but I'm using the tutorial here (https://github.com/karmi/tire/wiki/How-to-work-with-locations-%28geo%29-in-Tire) where lat_lon is a string\nruby\ndef lat_lon\n    [latitude, longitude].join(',')\n  end\nDo I have to separate the fields in the index and not string them together?\n. So sort would be?\nruby\nsort do\n  by :_geo_distance, {\n    user_lat_lon: { lat_long },\n    order: (params[:direction] || \"asc\"), unit: :miles\n  }\nend\n. So I need to put lat, lon into a separate indexed method that's a hash\n. It seems I can use lat_lon as a string: http://www.elasticsearch.org/guide/reference/api/search/sort/ \"lat lon as string\" \n. Here's how I did it\n``` ruby\ndef self.search(params)\n    # get coordinates\n    l = Geocoder.search(params[:address])\n    center_point = [l[0].longitude, l[0].latitude]\n    # perform search\n    tire.search do |s|\n      s.query { string params[:find_what], default_operator: \"AND\" } if params[:find_what].present?\n      s.filter :bool, should: { term: { published: true } }\n      s.filter :geo_distance, lon_lat: center_point, distance: '32km'\n  s.sort do \n    by :rating, :desc\n    by :_geo_distance, {\n      lon_lat: { lon: l[0].longitude, lat: l[0].latitude},\n      order: \"asc\", \n      unit: :km\n    }\n  end\nend\n\nend\n```\n``` ruby\nself.include_root_in_json = false # get rid of the root json element\n  def to_indexed_json\n    {\n      name: name,\n      published: published,\n      description: description,\n      rating: rating,\n      full_address: full_address,\n      reviews_count: reviews_count,\n      lon_lat: lon_lat,\n      latitude: latitude,\n      longitude: longitude,\n      categories: categories.map { |c| { \n        name: c.name } }\n    }.to_json\n  end\ndef lon_lat\n    {\n      lon: longitude,\n      lat: latitude\n    }\n  end\n```\n. ",
    "monfresh": "I'm running into this issue as well when I combine a keyword and location parameter. When location is the only parameter, it seems to be sorting correctly by distance, but once I add the keyword parameter, it doesn't. Here is the source code: https://github.com/codeforamerica/ohana-api/blob/master/app/models/location.rb#L181-L289\nAn example query would be: http://www.smc-connect.org/organizations?keyword=food&location=east+palo+alto\nI expect the East Palo Alto entries to appear first, but there doesn't seem to be any meaningful distance sorting applied.\n. In my case, it looks like the reason why sorting is not working is because the keyword search has boost associated with it, which seems to be overriding the sort. Is there a way to give priority to sort? I tried adding a boost to the sort, but that didn't change anything.\n. Thanks @bcackerman, but that doesn't work for my use case. I need to be able to specify that distance sort should take precedence over any other boost. If the location parameter is present, it should sort by distance first, and then by relevance based on boost value.\n. Why does it have to be a string? Shouldn't it support an array, like this?\nruby\nfilter :geo_bounding_box, :coordinates => { :top_left => [37.3084,-122.521], :bottom_right => [37.1066,-122.08] }\nAccording to @karmi's comment, it should, but it doesn't. I had to use strings for it to return results, like this:\nruby\nfilter :geo_bounding_box, :coordinates => { :top_left => \"37.7084,-122.521\", :bottom_right => \"37.1066,-122.08\" }\nIf it only supports strings, it would be nice if this were documented. I couldn't find a spec for this either.\n. I figured it out :) I needed to add a second parameter to the search method, like this:\ndef self.search(keyword, params={})\n  tire.search(page: params[:page], per_page: 30) do\n    query do\n      boolean do\n        must { string keyword, default_operator: \"AND\" }\n      end\n    end\n  end\nend\nAnd then call it like this:\nOrganization.search(params[:keyword], :page => params[:page])\n. On the flip side, is there an easy way to globally prevent any fields whose value is nil from being output in the JSON?\n. There's most likely a more elegant way to do this, but this is how I removed fields with nil values from the JSON output:\n``` ruby\ndef to_indexed_json\n  hash = self.as_json(\n    :except => [:organization_id], :methods => ['url'],\n    :include => {\n      :services => { :except => [:_id, :location_id, :created_at] },\n      :contacts => { :except => [:_id] }\n    }\n  )\n  remove_nil_fields(hash,[\"contacts\",\"services\"])\n  hash.to_json\nend\nRemoves nil fields from hash\n\nThe main hash passed to the method is a JSON representation\nof a Model including associated models (see to_indexed_json).\nThe fields passed are the associated models that contain\nnil fields that we want to get rid of.\nEach field is an Array of Hashes because it's a 1-N relationship:\nLocation embeds_many :contacts and has_many :services.\nOnce each associated model is cleaned up, we removed nil fields\nfrom the main hash too.\n\n@param obj [Hash]\n@param fields [Array] Array of strings\n@return [Hash] The obj Hash with all nil fields stripped out\ndef remove_nil_fields(obj,fields=[])\n  fields.each do |field|\n    obj[field].each { |h| h.reject! { |,v| v.blank? } }\n  end\n  obj.reject! { |,v| v.blank? }\nend\n``\n. What I mean by \"doesn't show up\" is that the field is not present in the JSON output. Here's a sample Tire search results output for an entry that has at least onecontact`:\nJSON\n[\n{\n_id: \"521d33331974fcdb2b001f45\",\naddress: {\n  city: \"Palo Alto\",\n  state: \"CA\",\n  street: \"425 Hamilton Avenue\",\n  zip: \"94301\"\n},\ncontacts: [\n{\n  _id: \"521d33331974fcdb2b001f46\",\n  name: \"Philip Dah\",\n  title: \"Director\"\n}\n],\nname: \"Downtown Palo Alto Food Closet\",\nurl: \"http://ohanapi.herokuapp.com/api/locations/521d33331974fcdb2b001f45\",\nid: \"521d33331974fcdb2b001f45\",\n_score: null,\n_type: \"location\",\n_index: \"production-ohana_api\",\n_version: null,\nsort: [\n1377645363000\n],\nhighlight: null,\n_explanation: null\n}\n]\nIf an entry does not have any contacts, I want to be able to specify whether or not the contacts field is present in the output. If I don't want it to \"show up\", I want the output to be like this:\nJSON\n[\n{\n_id: \"521d33331974fcdb2b001f45\",\naddress: {\n  city: \"Palo Alto\",\n  state: \"CA\",\n  street: \"425 Hamilton Avenue\",\n  zip: \"94301\"\n},\nname: \"Downtown Palo Alto Food Closet\",\nurl: \"http://ohanapi.herokuapp.com/api/locations/521d33331974fcdb2b001f45\",\nid: \"521d33331974fcdb2b001f45\",\n_score: null,\n_type: \"location\",\n_index: \"production-ohana_api\",\n_version: null,\nsort: [\n1377645363000\n],\nhighlight: null,\n_explanation: null\n}\n]\nI don't want it to appear in the output with an empty array, like this:\nJSON\n[\n{\n_id: \"521d33331974fcdb2b001f45\",\naddress: {\n  city: \"Palo Alto\",\n  state: \"CA\",\n  street: \"425 Hamilton Avenue\",\n  zip: \"94301\"\n},\ncontacts: [],\nname: \"Downtown Palo Alto Food Closet\",\nurl: \"http://ohanapi.herokuapp.com/api/locations/521d33331974fcdb2b001f45\",\nid: \"521d33331974fcdb2b001f45\",\n_score: null,\n_type: \"location\",\n_index: \"production-ohana_api\",\n_version: null,\nsort: [\n1377645363000\n],\nhighlight: null,\n_explanation: null\n}\n]\n. That is correct. My question is, is there a better way to remove the property from the returned documents than the one I have here?\n. ",
    "allochi": "Many thanks Karmi! I went through the whole read me yesterday, and tested the code, I must say that I don't have that much experience with Lucene, but this going to be this weekend project.\nOne last note about the read me, in facets, this:\nfacet 'global-tags' do\n    terms :tags, :global => true\nend\nneeds to change to:\nfacet 'global-tags', :global => true do\n    terms :tags\nend\nglobal for facet not terms, Kimchy helped me finding this out. \nAlso I found out that for this example to work, and return the right figures, we need take care of the sort by providing \"not analyzed\" title, we need to map title like this.\n:title => {\n   :type => 'multi_field', :fields => {\n      :title => { :type => \"string\", :analyzer => \"snowball\" },\n      :untouched => { :type => \"string\", :index => :not_analyzed }\n}\nand sorting to change to:\nsort { by \"title.untouched\", 'desc' }\nThis is not to be taken as me correcting the great job that you are doing, I just hope to ease the startup of new comers like me as a thankful user of Tire.\nAgain, Thanks!\n. @richarddong \nCan't thank you enough for sharing this man!\n. It would be great if we can have it this week! I'm stuck at this point :D\nI have a class I call Resource, and it has 3 children Document, Image and WebPage (and more later), I'm doing all the mapping in Resource class, which is an ActiveRecord class, but it doesn't work, when I force index using rake it just skip the mapping and index all properties.\nI wish that this get fixed really soon, so that I don't have to rewrite all my classes again to make Tire work. Please Help :)\nOh, many many thanks for Tire!\nI just deleted my comment on this being fixed in the 0.4.0pre, sorry for that, it worked when I set the type to be null in the database, which means no more inheritance.\n. ",
    "joshcutler": "The reason that I want to call the update_index in the after_commit vs. after_save hook is that because rspec and testunit wrap a whole test in a transaction calling update index will not help because the transaction never completes and thus ES doesn't see the update objects in the database when I would want to make assertions.  This is true for unit tests at least.\nThe reason I am testing this is because I have one indexed model, call it Document.  It has e.g. an array of authors.  Adding an author should trigger an update on the index for Document.  This needs to be triggered in the after_create/after_commit for author.\nI will try just stubbing everything in my unit tests and seeing if I can make it work by just writing integration tests.\nThanks.\n. Thanks.  I did know about that option, but if I have 100 items then I'm doing 100 mysql calls that way.  Whereas the :load => true option does a bulk load and then reorders the AR objects appropriately.  I was hoping there was a way to take advantage of the bulk load and still access the items.\n. In my particular app it is necessary but I understand that this may be a strange use case and I worked around it.  The situation is that in my ES index I store some composite values that are semi-expensive to compute which I would like access to and then I have some other functionality in AR that I want to use as well.  I could probably work around this by coming up with a scheme to index everything I need into ES.  It is not a deal break for me though, I mostly just wante to know if it was easily possible to do w/Tire or I should do it manually.\n. Don't do it just for me.  I have a good workaround for now (you are right it was trivial).  When I opened the ticket I just wanted to make sure there wasn't a way to do with with tire already before I did it myself.\n. Great, thanks for getting back to me so quickly.  Just to be clear (because creating my index takes ~24 hours), I have to blow away the whole index, not just reindex the items where this situation was occurring?  How do people generalyl handle tweaks to the search index in production environments?\nThanks again,\n-=Josh\n. Thanks a lot.  This really helps.\n. beautiful thank you!\n. Doh.  This is literally implemented exactly as requested.  I need more coffee.\n. ",
    "aledalgrande": "For future reference, the syntax that worked for me is:\nruby\nsort do\n  by ({\n    :_script => { \n      :script => \"(doc['field1'].value + doc['field2'].value) / doc['field3'].value\",\n      :type => \"number\",\n      :order => \"desc\"\n    }\n  })\nend\nRemember to include all of these fields in your mapping and to_indexed_json output.\n. Hey @dedico, if the script gets too complicated I would suggest to create a\nprecomputed sorting field in the to_indexed_json method of your model.\nThat way you can simply write a test for sorting in the controller and unit\ntests for the conversion into JSON.\nOn Monday, 10 December 2012, Marek Stachura wrote:\n\n@aledalgrande https://github.com/aledalgrande you just made my day! [image:\n:+1:]\nDifferent question - I have a little bit more complicated script - is\nthere a way I can debug / test my script? Or some way to return the value\nwhich will be returned from the script so I can see it it works ok?\nThanks,\nMarek\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/issues/174#issuecomment-11175456.\n. Confirmed this with MongoMapper as well: only to_json works. I made a pull request for the README.\n. I'm using MongoMapper and not ActiveRecord for my model, could that be the source of the problem? All the rest works though: filtering, sorting etc.\n. Yeah my current code is:\n\nruby\nMediaChannel.search do\n  # ...\n  size per_page\n  from page\nend\nWhile initially I tried:\nruby\nMediaChannel.search :page => page do\n  # ...\nend\nAdditionally: I've not found word of a page parameter in the ES documentation, they just talk about from and size.\n. @karmi FTW\n. Thanks for your work guys, sorry I haven't been to reply lately. Tire is one of the coolest gems I know.\n. ",
    "dedico": "@aledalgrande you just made my day! :+1: \nDifferent question - I have a little bit more complicated script - is there a way I can debug / test my script? Or some way to return the value which will be returned from the script so I can see if it works ok?\nThanks,\nMarek\n. Hi,\nthanks for the answer. \nTo get this correct I understand I should use some huge :size number?\nI'm passing :all_terms => true but not sure if that should include all terms?\nThanks,\nMarek\n. ",
    "joselo": "@aledalgrande your solution works perfect but I just wondering how can add another sort criteria in the same script, I mean sort search by formula but at the same time sort using another field.\nRegards and thanks\n. @aledalgrande no problem I found the solution, anyway thanks!!\nruby\nsort do\n  by [\n    {:position => 'asc'},\n    ({\n    :_script => { \n      :script => \"(doc['field1'].value + doc['field2'].value) / doc['field3'].value\",\n      :type => \"number\",\n      :order => \"desc\"\n    }\n  })\n  ]\nend\n. @jwoertink thanks!!\n. ",
    "cover": "Hi Karmi, damn, I didn't see that one, I'm reading it right now :)\n. After reading it I've changed something, now it's possible to do\nTire.configuration { url 'http://es1.example.com', 'http://es2.example.com' }\nI kept the sample/choice in order to return one random url from the ones available and not breaking anything, at least until there will be the round-robin support.\n. ...and...\nTire.configuration { url ['http://es1.example.com', 'http://es2.example.com'] }\nwhich could be more realistic with something like:\nelasticsearch = YAML::load( File.open( Rails.root.join('config', 'elasticsearch.yml') ) )\nTire.configuration { url elasticsearch[Rails.env][:hosts] }\n. ",
    "speedmax": "hopefully this gets merge into master soon\n. ",
    "nvdk": "just wanted to add my interest for this option here :)\n. ",
    "krae": "Yah, same. Are you still looking at this?\n. ",
    "itayco": "Sorry, fixed it in this fork:\nhttps://github.com/itayco/tire/blob/master/lib/tire/search/facet.rb\n. ",
    "ccocchi": "I think he was talking about the possibility to do this http://www.elasticsearch.org/guide/reference/query-dsl/or-filter.html\nRight now if you use more than one filter in a search, you end up with an and query :\nruby\n@articles = Tire.search 'articles' do\n   filter :range, :price => { :gte => price } \n   filter :term, :tag => ['foo']\nend\nwill send this query to elasticsearch\n{\"filter\" :\n  {\"and\" : [\n    { \"term\" :\n      { \"tag\" : \"foo\" }\n    }, \n    { \"range\": \n      { \"price\" : { \"gte\" : \"10\" } \n      }\n    }]\n  }\n}\n. Thanks.\n. ",
    "ticketevo": "Yeah we ended up having to just nest a bunch of OR queries under the \"and\"\n. ",
    "ream88": "Working fix:\n``` ruby\nclass Link\n  class << self\n    def mapping_with_super(*args, &block)\n      # Creating only one index\n      index_name('links')\n      document_type('link')\n  superclass.mapping_without_super.each do |name, options|\n    indexes(name, options)\n  end if superclass.respond_to?(:mapping)\n\n  mapping_without_super(args, &block)\nend\nalias_method_chain :mapping, :super\n\nend\nend\n```\n. Thank you for the quick pull and the really really great gem!\n. ",
    "rtlong": "@karmi Which would be the best way to approach this? I've got two sets of STI models in my app and I can't quite figure out how to do this.\n. ",
    "fbjork": "Still unclear to me how to get STI working with Tire.\nI setup mappings in the base class, and set the index_name, but it's not working as expected. The _type field gets overwritten by Tire to \"ancestor/child\".\n. ",
    "richarddong": "Hey, guys!\nand @haihappen , @karmi \nI've found an EASY way to do Single Table Inheritance using Tire, in a single elasticsearch index.\n``` ruby\nclass Link\n  include Mongoid::Document\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\ntire.index.add_alias 'diggs'\n  # redirect any search on the index 'diggs' to the index of Link, namely 'links'\nfield :href, type: String\n  field :name, type: String\nmapping do\n    indexes :href, type: 'string', analyzer: 'url'\n    indexes :name, type: 'string', analyzer: 'keyword', boost: 10\n  end\nend\n```\n``` ruby\nclass Digg < Link\ntire.index_name 'links'  # or tire.index_name superclass.tire.index_name\n  # store diggs to the index of Link, namely 'links'\nfield :tags, type: Array\n  field :time, type: Time\nmapping do\n    indexes :tags, type: 'string', analyzer: 'keyword'\n    indexes :time, type: 'date'\n  end\nend\n```\nTo SEARCH ALL INHERITED CLASSES such as diggs DIRECTLY FROM links, put this in your links_controller.rb\nruby\ndef search\n  @links = Link.search(params[:query], type: nil)\n  # override `default_options = {:type => document_type, :index => index.name} on /lib/tire/model/search.rb#L66\nend\nTo SEARCH ONLY diggs, put this in your diggs_controller.rb\nruby\ndef search\n  @diggs = Digg.search(params[:query])\n  # or equivalently\n  @diggs = Link.search(params[:query], type: \"digg\")  # or `type: Digg.tire.document_type`\nend\n. @allochi :-)\n. @karmi \nIs STI for Mongoid working properly now? If not, what's the plan? thx.\n. @Bounga :smiley: this hack still might fail..\n. Class.tire.document_type works for me.\n. Or maybe I should post this under mxcl/homebrew?\n. THANKS!\n. ",
    "elfassy": "I still don't get this. Using the first post as example\n\ncurl -XGET 'http://localhost:9200/links/_mapping'\n{\"links\":{\"link/delicious\":{\"properties\": ...\n\nIs this correct? It seems to work if i use type: nil.\ntire.search(load: true, page: params[:page], type: nil) do\n   ...\nend\nIf anything, line 78 (https://github.com/karmi/tire/blob/master/lib/tire/model/naming.rb) should be (for consistency with the index name)\nruby\n@document_type || klass.model_name.underscore.gsub(\"/\",\"_\")\n. how does one go about and \"convert\" his query to a hash? Thx\n. @search.q = \"test\"\nBulletin.tire.search do |s|\n  s.query do |q|\n    q.boolean do |b|\n      b.must {string @search.q}\nDoes not work.... neither does \nb.must do |m|\n         m.string @search.q\n      end\nAm i missing something?\n. if i do query = @search.q = \"test\"\nand then use query instead of  @search.q it works. \nStrange\nEDIT:\ngot it to work with\nBulletin.tire.search do |s|\n  s.query do |q|\n    q.boolean do |b|\n      b.must {|m|m.string self.query}\n. +1\n. +1 (good catch @DaveTsunami)\n. Are you sure you have the correct ip and port?\nruby\nTire.configure do \n  url \"http://192.168.1.138:9200\"\nend\n. Is sorting working now?\n. Ok found one way of making it work. Is this the right thing to do? Seems like there could be name conflicts between class Article::Blog and ArticleBlog...\nalso we need to run the rake on both models...\nruby\n  def self.search(query)\n    s = Tire.search([\"articles\", \"article_blogs\"],load: true) do |t|\n      if query.present?\n        t.query { string query, default_operator: \"AND\" } \n      else\n        t.sort { by :published_at, \"desc\" } \n      end\n    end\n    s.results\n  end\n. Duplicate with\nhttps://github.com/karmi/tire/issues/178\n. Turns out this is a bug with STI (single table inheritance for those searching)\nWith the fix below everything works as expected. Keep in mind this removes the ability to search child classes individually. (we need a fix for that please)\nruby\nmodule Tire\n  module Model\n    module Naming\n      module ClassMethods\n        def document_type name=nil\n          @document_type = name if name\n          @document_type || klass.model_name.underscore.split(\"/\")[0]\n        end\n      end\n    end\n  end\nend\nand in all subclasses\nruby\ntire.index_name 'venues'\n. ``` ruby\nrequire 'active_record'\nrequire 'tire'\nActiveRecord::Base.establish_connection( :adapter => 'sqlite3', :database => \":memory:\" )\nActiveRecord::Schema.define(:version => 1) do\n  create_table :venues do |t|\n    t.string   :title, :lat, :lon\n    t.datetime :created_at, :default => 'NOW()'\n  end\nend\nclass Venue < ActiveRecord::Base\ninclude Tire::Model::Search\n  include Tire::Model::Callbacks\ntire do\n    mapping do\n      indexes :title\n      indexes :location, type: 'geo_point'\n    end\n  end\ndef location\n    [lon, lat]\n  end\nself.include_root_in_json = false\n  def to_indexed_json\n    self.to_json(only: ['id', 'title', 'created_at'], methods: ['location']) \n  end\nend\nclass Venue::Type < Venue\nend\n-----\np Venue.new.document_type\n=> \"venue\"\np Venue::Type.new.document_type\n=> \"venue/type\"\n```\n\nNOTE: Updated by @karmi\n. There might be something wrong with your syntax (post the line for us to have a look). To answer your question, i haven't tried it with anything other than ids but it should work.\nOn the elasticsearch site they write \"Can be placed within queries that accept a filter.\"\nhttp://www.elasticsearch.org/guide/reference/query-dsl/not-filter.html\n. ruby\nJob.tire.search(\"test\").results[0]._score\ndoes work, but my problem is when i try to do:\nruby\nJob.tire.search(\"test\",load:true).results[0]._score\nwhich returns\nNoMethodError: undefined method `_score' for #<Job:0xfb11c28>\nIs there a way to use load:true and still get the score?\n. really sorry about that, you are right, the problem was on my side.\n. ",
    "gregory": "Just want to confirm that it's working like a charm with mongo. \nI had SocialComments::Accounts::Google < SocialComments::Account in mongo \nAt first i had some issue but when watching at the logs you can see: \nTire::Search::SearchRequestFailed: 404 : {\"error\":\"IndexMissingException[[social_comments_accounts_googles] missing]\",\"status\":404}\nso just had to add an index alias as @richarddong said so in my case:\ntire.index.add_alias 'social_comments_accounts_googles'\n. Though i faced a \"issue\" trying to index embeded content in elastic. So i had Social accounts like fb and many comments for each account. \nSo i wanted to index those comments embeded in an account(there is not that much) and be able to search among them.\nIt's working fine but it's useless since elastic will return an account related to my search query but with all the comments ... so i had to extract comments from account and use has_many\nIt's more an architecture problem than a tire or mongoid issue, just be aware of that :)\n. I faced some issues with inheritance with mongo. soi come back to you guys providing my solutions; There are more quick fix than solutions but this may help you. \nIt seems that tire tends to overwrite the _type= method\nwhich may conflict in a mongo context so her is the solution:\n    class A\n```\n  include Mongoid::Document\n  include Mongoid::Timestamps\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\n  def _type=(value)\n    self[:_type] = value.camelize\n  end\ndef self.inherited(klass)\n    super\n    klass.tire.index_name self.tire.index.name\n  end\nend\n```\nI was unable to search over all the childs in a inheritance context so here is the quick fix:\n   def self.search(params, options={})\n      options.merge!({ type: self.tire_aliases }) #if you wanna search for comments in all the chilld classes\n      tire.search(options) do |s|\n      end\n   end\n   #TYPES should be all your child classes so:\n   # if class B < A; end; TYPES=:b\n   TYPES = [:b]\n   def self.tire_aliases\n     TYPES.map(&:to_s).map{ |type| \"social_comments/comments/#{type}\" }\n   end\nHope this can help\nMerry X mass\n. ",
    "otobrglez": "Thanks @gregory for your advice. Saved me some time. :+1: \n. ",
    "davesouth": "Thanks @richarddong for the excellent example of Mongoid/Tire inheritance.\nHowever, mapping the index for multiple types is proving a challenge.\n@karmi wrote:\n\n@rtlong I'd probably extract the index creation and mapping definition logic out of the models entirely. I'd put it in /lib, or maybe keep them in the \u201cbase\u201d model for STI. I believe there's some page on the Wiki which has the exact for such extraction.\n\nAgreed. However, the wiki entry doesn't exist nor can I find any examples of mapping each _type in an index. We need mapping to look something like this (where \"phone_ngram\" is our custom analyzer):\n``` ruby\ncontact: {\n  person: {\n    properties: {\n      first: { type: 'string', analyzer: 'snowball' },\n      last: { type: 'string', analyzer: 'snowball' },\n      phones: {\n        properties: {\n          digits: { type: 'string', analyzer: 'phone_ngram' },\n        }\n      },\n    }\n  },\n  company: {\n    properties: {\n      name: { type: 'string', analyzer: 'snowball' },\n      phones: {\n        properties: {\n          digits: { type: 'string', analyzer: 'phone_ngram' },\n        }\n      },\n    }\n  }\n}\n```\nAny suggestions?\n. @karmi Yes, I can get two _type 's created or I can do nested, but something fails when trying to do both. I know it's something I'm doing wrong. It's got to be a simple mistake. There's just no obvious reason for the failure.\nEDIT: Moved questionable code into Gist:\nhttps://gist.github.com/davesouth/5699181\n. @karmi Ah ha! That fixed it. Thank you.\nI fixed the \"phone_analyzer\" problem before. However, with all the variations and different configurations, I must have picked up the error again. The logger tip will come in very handy.\nHere is the final, working version for anyone trying to do something similar:\n``` ruby\nTire.index('contacts').delete\nTire.index('contacts').create settings: {\n  analysis: {\n    analyzer: {\n      phone_analyzer: { tokenizer: 'whitespace', filter: 'phone_ngram', type:'custom' }\n    },\n    filter: {\n      phone_ngram: { type: 'nGram', min_gram: 3, max_gram: 15 }\n    }\n  }\n},\nmappings: {\n  person: {\n    properties: {\n      id: { type: 'string', index: 'not_analyzed' },\n      first: { type: 'string', analyzer: 'snowball' },\n      last: { type: 'string', analyzer: 'snowball' },\n      title: { type: 'string', analyzer: 'snowball' },\n      phones: {\n        properties: {\n          digits: { type: 'string', analyzer: 'phone_analyzer' }\n        }\n      }\n    }\n  },\n  company: {\n    properties: {\n      id: { type: 'string', index: 'not_analyzed' },\n      name: { type: 'string', analyzer: 'snowball', boost: 10 },\n      phones: {\n        properties: {\n          digits: { type: 'string', analyzer: 'phone_analyzer' }\n        }\n      }\n    }\n  }\n}\n```\n. @karmi Yes, definitely. Especially where there will be so much duplication between the two types (phones, emails, addresses, etc).\n. ",
    "Bounga": "@richarddong thanks bro. You saved my day. Excellent tip.\n. ",
    "dennybritz": "Thanks, I didn't think about that solution :) If I have time I'll implement the prefix query support myself, seems like it should be straightforward.\n. ",
    "schneider-th": "OK, I figured it out. After installing ES on my VPS I set it up according to the guide and configured my IP address, of the VPS. After removing the line and letting ES search for it automatically, the app finally works.\n. ",
    "zirni": "That works :-) thx\n. Hey, \nArticle.index.create takes an options hash which accepts a mapping and settings.\nArticle.index.create(:mappings => Article.tire.mapping_to_hash, :settings => Article.tire.settings)\nTheres a shorter way to do this with\nArticle.create_elasticsearch_index\nIn fact there the follow available calls i know right now\nArticle.index.delete # what works perfect\nArticle.index.create # needs the mapping as an optional argument, what IMO shouldn't be optional\nArticle.create_elasticsearch_index # is doing what Article.index.create should do IMO\nBest regards,\nzirni\n. Yes, thats fine. thx\n. ",
    "balexand": "Yes, I'm using Tire with MongoMapper and so far I haven't had any issues. I'm using the ActiveModel integration as described in the README.\n. ",
    "squidfunk": "I have a similar problem: I am importing the data from a model into an index from inside a Rake task like this:\nArticle.index.import Article.all\nHowever, when I first delete the index like this:\nArticle.index.delete\nArticle.index.import Article.all\n...it doesn't work. I guess it's the same problem zirni experiences, isn't it? What's a good solution for that? I have to delete the index and rebuilt it, primarily because the mapping may have changed. Is it possible to implement this?\nThanks in advance!\nBest, Martin\n. Hey, I don't really know how to write extensions for tire - is there a guide somewhere? As you see, I put it into the actual query.rb. However, I could contribute it to tire-contrib when I find the time.\n. Sounds absolutely legit! When I find the time (fighting on other frontiers at the moment), I will contribute this to tire-contrib if no one has already done this. Thanks for your feedback!\n. ",
    "ddnexus": "What about a simple convention like:\n```\n\n\nMyNested::SpecialModel.index_name\n\n\n=> \"my_nested__special_models\"\n\n\nMyNested::SpecialModel.new.document_type\n\n\n=> \"my_nested__special_model\"\n\n\n\"my_nested__special_model\".document_type_classify\n\n\n=> \"MyNested::SpecialModel\"\n\n\n\"my_nested__special_models\".index_classify\n\n\n=> \"MyNested::SpecialModel\"\n```\n. My patch is actually dead simple and doesn't add any burden to the method. It just keep into consideration that the models could be different, and just load them all with one find call per model.\nAbout the comments in the posts you are linking: IMHO it's a matter of (in)consistency. :-)\nIf you support the :load option, you should fix it and make it work consistently with the rest of your code, i.e. multiple types objects. On the other hand If you consider that loading is \"suspicious\", you could remove the option completely, so at least it will not generate an error. Keeping the option and not supporting it for that special case makes me dizzy :-), specially because the fix is so simple.\nThe other patch for the default document_type fixes the same type of inconsistency: the (Tire) code of one method is inconsistent with the usage that the same (Tire) code does somewhere else. \nklass.model_name.singular is not suitable to generate any document_type that will work with camelize or classify, while uderscore does just that.\n. It was just a possible suggestion... for that reason I didn't attach any test... however, if you like the patch I could. Should I?\nThe goal was to show that a simple convention is enough to work around the restriction imposed by the usage of index_name and document_type in the url. Indeed we don't need a class with #to_s and #to_url methods: we can just avoid using the slash, by gsubbing it to some other value, like a double '__'.\nAbout what the patch does, I wrote you in the original starting post for the first patch... I paste here something from the other post:\n```\n\n\nMyNested::SpecialModel.index_name\n\n\n=> \"my_nested__special_models\"\n\n\nMyNested::SpecialModel.document_type\n\n\n=> \"my_nested__special_model\"\n\n\n\"my_nested__special_model\".document_type_classify\n\n\n=> \"MyNested::SpecialModel\"\n\n\n\"my_nested__special_models\".index_classify\n\n\n=> \"MyNested::SpecialModel\"\n```\nbesides, you can also do:\n```\nclass MyNested::WeirdModel\n    index_prefix 'my_prefix'\nend\n\n\nMyNested::WeirdModel.index_name\n\n\n=> \"my_prefix_my_nested__weird_models\"\n\n\n\"my_prefix_my_nested__weird_models\".index_name_classify\n\n\n=> \"MyNested::WeirdModel\"\n```\nAbout extending String: I agree it's not strictly necessary, but it is just in style with all the other String methods extended by ActiveSupport so we could just follow the same convention, along with the other String added methods:\n- pluralize\n- singularize\n- constantize\n- camelize\n- titleize\n- underscore\n- dasherize\n- demodulize\n- parametrize\n- tableize\n- classify\n- humanize\nIn our case the additions to string are very Tire specific due to their name that is composed by the original method name, and the '_classify' suffix, and we don't override anything that is already in place, so I would use that instead a not-so-elegant-and-against-rails-convention function.\nI should check where (and whether) Tire converts document_type (and/or index_name) back to the class name. One place that I was aware of is the tire/results/collection.rb so I patched it there. Anyway, although not used by Tire directly or extensively, the methods are useful to users, specially if you are going to drop the :load option or encouraging users to write their own code around Tire.\nIf you provide a default from class name to index_name and document_type, I would expect to have a canonical method that allows me to convert them back to the right class name, and possibly doing it with the standard way ActiveSupport made me used to. (i.e. string method)\n. Hi Karel,\n\nI wouldn't extend String,\n\ngreat!  :-)\n\nthe encoding document_type_classify has its decoding counterpart\n\nI cannot understand the meaning of this one. Maybe it's the \"encoding/decoding\" concept that is not clear to me. If it can be interpreted as \"translate a string from one convention to the other\"... do you mean something like document_type_to_class_name(doc_type_str) to return the class name given the document_type, and class_name_to_document_type(class_name) to return the document type given the class_name?\n\nand it's not added into index_name method.\n\nAgain, I am confused with this one. The 'into' part is what make me think that I probably don't understand what you mean. Do you mean that the index_name should not have the same kind of translation/encoding, or that you don't want to use it 'inside' the index name method? \nProviding a default algorithm to generate the index_name (that is also including a prefix) should be coupled with a \"decoding counterpart\" (?) so when you are dealing with an index_name you can get the class name using the opposite algorithm. That is quite useful in order to eliminate the need of mapping hashes, for example.\n. ",
    "nz": "+1;\u00a0this is relevant to my interests. Updating mapping at the index level won't be an issue with Bonsai for much longer, but it always helps to be specific.\n@jfredett feel free to drop me a line if you need any support for this stuff on our end over at Bonsai\n. That definitely makes for an easy initializer, but in my experience it's much nicer for users if the client can auto-detect that. You'd be surprised how much friction even a simpler initializer can be on the scope of an add-on :)\nPersonally, I would tweak the Tire::Configuration.url method to look like this:\nruby\ndef self.url(value=nil)\n  @url = (value ? value.to_s.gsub(%r|/*$|, '') : nil) || @url ||\n          ENV['ELASTICSEARCH_URL'] || \"http://localhost:9200\"\nend\nIn fact, I'll go ahead and send in a pull request for that :)\n. Also, it's a URL for an index, not a cluster. I think I missed that bit somewhere in between the configuration stuff and the actual live requests. Maybe @karmi will beat me to that fix?\n. I'm traveling and can't comment in too much detail at the moment.\nThe basic idea is that a search is being constructed as a multi-index search on the cluster rather than a multi-type search on the index. This is related to configuration conventions I've chatted with @karmi about in the past. \nRahil, can you also include an example of the code you're calling to invoke the search itself? \n\nNick Zadrozny\nOn Tuesday, April 24, 2012 at 7:25, Rahil Sondhi wrote:\n\nI'm trying to get Bonsai ElasticSearch (cc: @nz) to work with Tire on Heroku but I'm getting this error:\n2012-04-21T23:18:00+00:00 app[web.1]: Started GET \"/search.json?q=janice\" for 67.224.81.78 at 2012-04-21 19:18:00 -0400\n2012-04-21T23:18:01+00:00 app[web.1]: Processing by SearchController#index as JSON\n2012-04-21T23:18:01+00:00 app[web.1]: Parameters: {\"q\"=>\"janice\"}\n2012-04-21T23:18:01+00:00 app[web.1]: [REQUEST FAILED] curl -X GET \"http://index.bonsai.io/artists,users/_search?pretty=true\" -d '{\"query\":{\"query_string\":{\"query\":\"janice\"}}}'\n2012-04-21T23:18:01+00:00 app[web.1]: Completed 500 Internal Server Error in 25ms\n2012-04-21T23:18:01+00:00 app[web.1]: \n2012-04-21T23:18:01+00:00 app[web.1]: Tire::Search::SearchRequestFailed (401 : {\"error\": \"Not authorized: Some endpoints are admin-only, ask support@onemorecloud.com.\"}\n2012-04-21T23:18:01+00:00 app[web.1]: ):\n2012-04-21T23:18:01+00:00 app[web.1]: app/models/user.rb:141:in `search_for'\n2012-04-21T23:18:01+00:00 app[web.1]: app/controllers/search_controller.rb:6:in `index'\nMy tire.rb initializer has:\nif ENV['BONSAI_INDEX_URL']\nTire.configure do\nurl \"http://index.bonsai.io\"\nend\nBONSAI_INDEX_NAME = ENV['BONSAI_INDEX_URL'][/[^\\/]+$/]\nend\nAnd my model has:\ninclude Tire::Model::Search\ninclude Tire::Model::Callbacks\nindex_name BONSAI_INDEX_NAME\n@nz says it's because Tire is omitting the index name from the BONSAI_INDEX_URL environment variable. He'll be able to offer more insight into this.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/322\n. Fortunately, fwiw, multi-type searches within a single index are pretty straightforward\u2026\n\nsh\ncurl 'http://localhost:9200/test/foo,bar/_search?q=*:*'\n. @karmi So about those multi-index vs. multi-type searches\u2026? are we due for another chat about that? :)\n. @mculp, can't speak for @karmi wrt Tire's index usage conventions, but I'm working on some stuff on the Bonsai end of things to allow many indices through the Heroku addon. Would be happy to follow up with you on your use case in more detail over email.\n. @karmi, any further comment on whether the single index per app per environment paradigm is on your roadmap?\nI've been rolling out better multi-index support over at Bonsai for Tire users lately, but I still think a single index per app per environment is a useful paradigm for most apps. Maybe Tire's current conventions are too well entrenched to be worth the effort for an arguably marginal difference? Seems like a pretty fair position, if that's the one you've arrived at.\n. Thanks for the clarifications. My preferred conventions aside, the original question posed in this Issue has since been addressed on our end. But I appreciate your update and thoughts wrt mapping models to indexes vs. types.\n. @Will-Sommers: Proper multi-search in Bonsai is still forthcoming. Consolidating into a single index is a good quick fix for today. I've made some updates to https://gist.github.com/nz/2041121 toward that end.\n. @Will-Sommers, @hale, @rahilsondhi, @mculp \u2014\u00a0basic multi-index searching is supported in Bonsai; see this blog post for more details.\n. This might be a half solution, but it looks pretty un-invasive and very helpful in the context of issues like #324.\n:+1: \n. CC'ing myself on some issues mentioning Bonsai\u2026\nAn ECONNREFUSED is a pretty vague exception to debug without more context on the stack trace and other circumstances. I like the idea behind @dennisreimann's PR #323 to help with that kind of thing\u2026\nIf an ECONNREFUSED is ever being generated by someone using Bonsai, I'd love to hear about that at suport@onemorecloud.com, and I can be collabed in to Heroku apps to help debug errors like that.\n. Documentation should work pretty well for this. Almost every production ECONNREFUSED error I've seen comes down to a misconfigured client incorrectly trying to connect to its development default of localhost.\nOther ideas in addition to documentation\u2026\nPrinting the full URL on ECONNREFUSED, or at least including the host, should go a long way toward helping people troubleshoot this kind of issue on their own. There's already a pull request, #323, that would work well for this.\nBased on @kristopher's explanation, it sounds like there's a request executed against Elasticsearch while some class is loaded for other purposes. Is that strictly necessary? Can that request be deferred?\n. @karmi re: require \"tire/bonsai\" \u2014 good idea, I had forgotten we'd talked about that. Asking people to manually create initializers can be a bit fiddly. Let me ping you on IM in a bit to figure out a good way to package that up. Maybe a tire-bonsai gem which I maintain separately?\n. What Tire needs (and as @karmi and I have discussed) is a way to specify a global default index. When present, a model should use that as its base, otherwise models should create their own indices and use the cluster as their base.\nFrom the user's end, this behavior should be invoked with something like this:\nruby\nTire.configure do\n  index_url '\u2026'\nend\nFrom there, it's a matter of tracing through the method calls that either find/create a per-model index, or talk to the cluster directly, and have them check for the presence of a globally defined index.\nUnfortunately, neither @karmi nor I have had the time to really dig into this yet. I'm guessing a contribution would be welcome? I'd certainly be happy to help test or review any pull requests, just cc me.\n. > Why some data were imported, but not all?\nThat I have no answer to, sorry @toddwschneider. I promise all the requests to /_bulk failed with a 401 \u2014\u00a0so maybe some one-off single-model indexing calls slipped through?\n\nHow can users work with Bonsai.io and Tire currently? After all, setting MyModel.index_name to a specific, user-based value should work?\n\nIt looks like Index#bulk_store ignores the index name (index.rb L123). Pull request incoming.\n. I'll try to get some more thorough manual testing in this week.  \nJustin, can you confirm that the full BONSAI_URL including the index name is in that curl command?\nMaybe the bulk import method is setting _index incorrectly? Should be easy to verify. \n\nNick Zadrozny\nOn Sunday, May 6, 2012 at 9:47, Justin Palmer wrote:\n\nI'm running 0.4.2 and haven't been able to get this to work yet, albeit it's complaining about a missing index this time.\n``` ruby\nTire.configure do\nurl BONSAI_URL\nlogger STDERR\nend\narticles = [\n{ :id => '1', :type => 'article', :title => 'one', :tags => ['ruby'], :published_on => '2011-01-01' },\n{ :id => '2', :type => 'article', :title => 'two', :tags => ['ruby', 'python'], :published_on => '2011-01-02' },\n{ :id => '3', :type => 'article', :title => 'three', :tags => ['java'], :published_on => '2011-01-02' },\n{ :id => '4', :type => 'article', :title => 'four', :tags => ['ruby', 'php'], :published_on => '2011-01-03' }\n]\nTire.index 'articles' do\ndelete\ncreate\nimport articles\nend\n```\n```\n\ncurl -X POST BONSAI_URL/articles -d '{}'\n2012-05-06 09:40:55:975 [201]\n[ERROR] 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}, retrying (1)...\n[ERROR] 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}, retrying (2)...\n[ERROR] 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}, retrying (3)...\n[ERROR] 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}, retrying (4)...\n[ERROR] 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}, retrying (5)...\n[ERROR] Too many exceptions occured, giving up. The HTTP response was: 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}\n2012-05-06 09:40:57:727 [BULK] (\"articles\")\n\ncurl -X POST BONSAI_URL/articles/_bulk -d '{... data omitted ...}'\n2012-05-06 09:40:57:727 [201]\n```\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/327#issuecomment-5537116\n. @Caged and @karmi, I think I see what's happening here.\n\n@Caged, your example code is probably better written like this\u2026\n``` ruby\nTire.configure do\n  url \"http://index.bonsai.io/\"\n  logger STDERR\nend\narticles = [\n  { :id => '1', :type => 'article', :title => 'one',   :tags => ['ruby'],           :published_on => '2011-01-01' },\n  { :id => '2', :type => 'article', :title => 'two',   :tags => ['ruby', 'python'], :published_on => '2011-01-02' },\n  { :id => '3', :type => 'article', :title => 'three', :tags => ['java'],           :published_on => '2011-01-02' },\n  { :id => '4', :type => 'article', :title => 'four',  :tags => ['ruby', 'php'],    :published_on => '2011-01-03' }\n]\nTire.index 'pvz4bww6kn87gz50s6h43tb' do\n  import articles\nend\n```\nI ran this against an index of mine and it worked. Let me break down a few points about how Tire and Bonsai are interacting here\u2026\n1. Tire.configure only takes a cluster-level URL right now. However, it's easy to use an index URL there and be only partially successful, since the URL is just concatenated in where it's needed.\n2. Tire.index takes an index name for its parameter. This is where you supplied articles but more appropriately would want to give your Bonsai index name.\n3. The index-level delete and create operations aren't available on Bonsai yet (working on that), hence I've omitted them from my example code.\nSo it looks like my hypothesis was partially correct:\u00a0an incorrect index name was getting passed in for the _index key in the bulk import payload. But that was correct behavior based on the example code \u2014\u00a0after reworking it I'm not seeing any bugs here.\nMaybe I'll tackle that application-single-default-index thing this week\u2026\n. thanks! :heart:\n. @lgs \u2014 your Bonsai index URL is intended to be private, like a database password. you may want to remove and re-add the addon at some point to get a new private URL.\n. That aside, I'm having a hard time seeing what the issue may be here, or whether it's even coming from the bonsai.io side\u2026 If anyone can help me duplicate the issue I'd appreciate it.\n@lgs, please email me at support@onemorecloud.com if you can create a request (or series of requests) with curl that duplicate the issue.\n. Thanks for the extra details, @lgs. I suspect the earlier error was from the process failing to write out its log messages. As far as your search results, they seem normal to me. Those are just two different documents in your index, as indicated by the unique document _id fields. Likely they weren't indexed with an ID?\nNot sure I see an error here yet. Have you tried to reindex your data since changing the logger?\n. I'll have to defer to the expertise of @karmi or someone else on the rest\u2026\n. @benevolentmadman \u2014 Nick with Bonsai.io here. We do limit some index-level config options while we're in beta, which may be affecting you here. We're working to get most of that ironed out by the time we launch, so thanks for the extra use case. Feel free to follow up with me more directly at support@onemorecloud.com, and I'll also keep an eye on correspondence here.\n. For posterity, this kind of issue has been fixed within Bonsai as of a rearchitecting we completed back in October 2012. We should work well with Tire's conventions these days, and launched our addon into production back in January 2013.\n. For posterity:\u00a0this issue was addressed in Bonsai a couple months ago, back in October 2012. We've also since launched to production in January 2013 and generally work pretty smoothly with Tire's conventions these days.\n. > Is any option for configuring hunspell for ES on the Heroku (bonsai)?\n@rubydev we don't currently have the Hunspell analysis plugin deployed, but are willing to consider it for a future release. We're very customer-driven in our approach to supporting for plugins, so drop us a line at support@bonsai.io if you decide you need Hunspell support.\n. I'm no Tire expert, but I am a Sunspot expert, and I can say that its with scope is a filter query under the hood. So you seem to be going in the right direction. Did you figure this out since opening the ticket?\n. Awesome. False alarm! Version mismatch on my end. Which is good, because I was about to be even more confused ;)\n. @brupm color me stumped. do you get this locally in development? doesn't really seem like a Tire thing tbh.\n. @miry, fwiw, the Bonsai addon supports multi-index search now. So you shouldn't strictly need to use a combined index for your cross-model searching. Trying to work with a single consolidated index per environment with Tire is probably swimming upstream at this point unless you're willing to really dig in under the hood.\n. Possibly related to #639 where the after_commit callback is concerned.\n. reindexing rake tasks are fun. there are a lot of ideas in here that I like and will probably borrow for Sunspot.\n- :+1: for multi-model CLASS \u2014\u00a0even better to default to all classes a la sunspot/sunspot#158.\n- :+1: for eliminating kaminari dependency, that's a gotcha which has bitten me a few times now.\n- :+1: for duck-typing detection and use of find_in_batches, really that's the 99% use case here.\n- :+1: for the GC.start \u2014\u00a0I need that for sunspot asap. the counter argument here is that the GC will otherwise run when it needs to, and many gigs of used memory is not an issue if the process is due to merely exit soon.\n- :+1: for some kind of progress bar. the progress_bar gem is nice to use when present, but probably not worth making a dependency.\n\u2026\n- :-1: for passing multiple params to puts instead of interpolation (bike-shedding) :bike:\n- :-1: for the scope stuff. not super clear on a quick skim. too hidden. how do I know whether and how to use this?\nmy 2\u00a2 on a lazy saturday\n. While nil checks are always a good idea, the root cause is ultimately my\nissue to fix here. Basically the alias isn't being created properly. I can get\nthis sorted on our end.\n\nNick Zadrozny\n. @karmi for sure, totally understood. We do some things in a slightly non-standard way out of necessity/opinionation. I'll break it down a bit further here, now that I'm back at a keyboard:\nSo we are returning JSON for that action \u2014\u00a0just with a different internal index name. So the JSON is kind of valid, in a boring technical sense, but mostly not, from the perspective of the user's intent.\nFor example, curl http://oak-12345.us-east-1.bonsai.io/acme-test/_aliases might return:\njson\n{\"m3z6xmmzeja13q54ho9a\":{\"aliases\":{}}}\nIndexing the response by acme-test is what returns a nil here. This is similar to another ticket somewhere about fetching the index settings or mapping, or anything else where the index name is a key for the response.\nMy fix here will be part of a larger ongoing refactoring that does a more thorough response filtering, to map index names back to the user's specified logical name. Very slightly non-trivial to do when trying to preserve good performance, but we're working on it :)\n. Thanks all for the sleuthing, that was very educational for me as well.\nSome further Alias related compatibility work still ongoing on my end for\nBonsai.\n\u2014Nick\n\nNick Zadrozny\n. Hi all \u2014\u00a0wrapping this up, Tire::Alias.all should now work as expected with Bonsai. Here's an excerpt from my testing:\n``` ruby\nENV['ELASTICSEARCH_URL'] = 'http://user:pass@host.us-east-1.bonsai.io'\nrequire 'tire'\n=> true\nTire::Alias.all\n=> [, :name=>\"aliastest\"}>]\nTire::Alias.create(name: \"testalias\", indices: [ \"test\" ])\n=> 200 : {\"ok\":true,\"acknowledged\":true}\nTire::Alias.all\n=> [, :name=>\"aliastest\"}>, , :name=>\"testalias\"}>]\nTire::Alias.create(name: \"testalias\", indices: [ \"notfound\" ])\n=> 404 : {\"error\":\"Could not find index 'notfound'\",\"status\":404}\nTire::Alias.all\n=> [, :name=>\"aliastest\"}>, , :name=>\"testalias\"}>]\nTire.index('test').aliases\n=> [, :name=>\"dswruzxh2fuhyageamth\"}>, , :name=>\"9s0md390kvfeyj4bglm5\"}>]\n```\nAs per the above, there is still some index name response mapping to be done when listing an index's aliases directly, but nothing should be breaking with nil errors. Do email me at support@bonsai.io if the internal index name thing is getting in your way.\n. Thanks for the cc. This doesn't sound familiar to me, but @colewinans should drop us an email at support@bonsai.io with relevant URLs so I can investigate more directly. \n\nOn Oct 20, 2013, at 0:58, Karel Minarik notifications@github.com wrote:\nHey, cross-posting between Github issues and Stackoverflow is actually something which makes it very hard for me and others to help you. Since the SO post contains more detail, I'm closing this one.\nCC @nz since it's a Bonsai issue.\n\u2014\nReply to this email directly or view it on GitHub.\n. Relevant to my interests.\n\nIf I recall correctly, the mapping method defined in the Tire::Model::Indexing module will attempt to make a request to the Elasticsearch server when called. For most apps, as per the examples in the documentation, this method is run during the evaluation of the class itself, i.e., during the application boot.\nIf not able to reach the ES server at this time, the method will throw an exception causing the application to fail to boot.\nhttps://github.com/karmi/retire/blob/master/lib/tire/model/indexing.rb#L30-65\nA good place to start would be to wrap the typical call to mapper with a rescue block that catches and logs the exception:\nruby\nclass Article\n  # ...\n  begin\n    mapping do\n      indexes :id,    :index    => :not_analyzed\n      indexes :title, :analyzer => 'snowball', :boost => 100\n      # ...\n    end\n  rescue Exception => e\n    logger.error \"Failed to check/set index mapping during boot: #{e.message}\"\n  end\nend\n(Is that all correct and up to date? Been a while since I've read through this code.)\n\ntrying to update the index when I save my model\n\nThat's definitely a common one \u2014 an after_save hook which calls a method to post an update back to ES. This is also relevant to some other issues, like #639. Use of after_commit (vs. after_save) is also mentioned in the main Tire Readme, though I'm not immediately sure how that relates to existing use of after_save. Reading through the modules you're including to your model should clear that up a bit more.\nThere's also another project, tire_async_index, which looks to extend Tire with Resque and Sidekiq support to process updates asynchronously.\n. Followup, regarding \"Model Callbacks\" by which I suppose you mean Tire::Model::Callbacks.\nIf those tire.update_index methods throw an exception, then creating and updating records will fail. This could be dealt with by rescuing the relevant exception in your controller actions, and either retrying or presenting a graceful error message.\nThe best option is to use an approach which queues updates to be processed in a batch when the server is back on. That does seem to be a little outside of Tire's scope, so I'd be interested to learn how other people are doing that.\n. nitpick: might as well interpolate mapping for consistency. not sure how common it is to pass multiple args to puts in Rubyland anyway. also line 74, though that line at least benefits from some line wrapping.\n. this comment definitely needed to go\n. ",
    "erickt": "I'm interested! I was just about to start working on that.\n. Great, thanks. I've got two comments. First, could you factor out the scrolling code into it's own class/dsl? Second, could you add a search_type option to Search and use that instead of hardcoding search_type=scan? It's just shuffling a couple things around in your code and it'll be a bit more flexible.\n. @grasscode: another thought. The size argument should be lifted out of the Scan module, since Search already provides it.\n. @grasscode: I needed search_type for my own purposes, so I have a pull request here for here.\n. Hi Karmi! Sorry for the absence, I've been busy with non-tire related things. I poked around a little more, and it looks like the minitest dependency is coming from the gem turn, but the turn gem only has a development dependency on minitest, which bundler seems to ignore (at least for me).\n. Regarding the api, I'm divided on what to support. First off, do you actually want the non search_type uri parameters to get passed by a uri parameter. timeout, from, and size can all be passed in the json body. Second, would you only want the parameters to be passed in the Search#new function, or do you also want the dsl functions? I could see it being a little confusing if you write:\ns = Search::Search.new('index', size: 10) do\n  size 20\n  query { string 'title:foo' }\nend\nWhich size declaration wins?\n. @dylanahsmith: search_type is only a url parameter, it cannot be passed in the body.\n@karmi: This approach seems fine to me. I'd be fine on punting on #198 for this.\n. I seem to have run into the same problem, and I have a similar fix in #376. Adding a link here so I can review this commit later.\n. Oops, forgot to link to my version of this fix.\n. Hi Karmi! I updated my pull request to fix a couple bugs and add scrolling support to ActiveModel.\n. Hi hajder! I think tire HEAD recently grew support for aliases. Any chance you could rebase onto head and update your patch?\n. @brandonmeeteor: Changing Tire to use POSTs instead of GETs for requests with bodies should work.\n. Hi ahfeel! That looks pretty good, but could you update your test to include the params?\n. For reference, I have a similar patch in my pull request, but I take the approach of hardcoding the types to cast:\nhttps://github.com/erickt/tire/commit/2281af5f836e4cbf5bfbed823837fec28281e9b2\nWhich do you all like better?\n. ",
    "sandrew": "Well, if tasks of such type were really frightening, then we would not have a db:drop task in AR. There are many situations where you really need such tasks, at least during development. And it is hard to invoke it accidentally: you have to input \"drop\" in your console.\nBut if the main reason is difficulty with lots of introspections, then it's OK.\n. Hello again. I'd like to remind you to make a decision on this request.\n. ",
    "wbzyl": "Thanks.\nDoes it mean that\nTire.index('_percolator').refresh\nshould be added in percolated-twitter.rb in line 94.\nAm I right?\n. ",
    "danielschlegel": "Thank you so much for the fast reaction! \nYes sure i can give you some more information. First, we use Mongoid and not ActiveRecord.\nWe have a Person model and a Person::Index module. Since my person.rb i pretty big i want to outsource the code for tire in a module. When i do this i get the model_name exception.\n``` ruby\nclass Person\n  include Person::Index\nsome other code\n.....\nend\nmodule Person::Index\ninclude Tire::Model::Search\ntire do\n    mapping do\n      indexes :id, :type => 'string'\n      indexes :first_name, :type => 'string', :boost => 100\n      indexes :last_name, :type => 'string', :boost => 110\n    end\n  end\nend\n```\nThe exception happens in this method in naming.rb:\n        def index_name name=nil, &block\n          @index_name = name if name\n          @index_name = block if block_given?\n          @index_name || [index_prefix, klass.model_name.plural].compact.join('_') # klass.model_name is undefined\n        end\ni tried different approaches to get the model_name, even with nasty things like this: self.class.to_s.split(\"::\").first\nunfortunately i didn't find a good solution. \nThanks you for tire, it's a great gem!\n. Thanks again for your answer. I know what you mean. Currently i didn't get it to work but i will try later again. If i have the solution i will add it as comment here.\n. is there a donate button for you somewhere?\n. Im very sorry i didn't post any further(since i found the problem). actually the mapping and all in tire was ok. One of the replicas in our cluster was in initialzing state and didn't come up correctly. since the cluster is green again the error disappeared. \nI'm still unhappy with the situation because i think a yellow state should not lead into an exception but i know this is not a tire problem.\nThanks you very much!\n. Thanks! Unfortunately i just get another error:\nNameError: undefined local variable or method attributes' for #<MyClass:0x007f941dfa99a8>\n/Users/user/.rvm/gems/ruby-1.9.3-p194-perf/gems/activemodel-3.2.8/lib/active_model/serialization.rb:74:inserializable_hash'\n. Normally we have mongoid but this is not a \"Database Model\". There is no ActiveModel integration, just a PORO.\n. Thanks alot, the ActiveModel stuff made my tests running.\n. ",
    "andruby": "I want this feature too. I'm going to look around in the Tire code to see if I can pull it off.\n. Hi @karmi,\nI have the same challenge as @willvr: one of my models defines to_param as a unique string (aka: a permalink).\nruby\nclass Article < ActiveRecord::Base\n  include Tire::Model::Search\n  def to_param\n    self.permalink\n  end\nend\nWhen I call url_for() on an Article instance it returns the correct url, but on a Results::Item it will use the id instead of the permalink:\nruby\nurl_for(Article.first) # => /articles/hello-world-blogpost\nurl_for(Article.search('*').results.first) # => /articles/1\nThe code added in @willvr's pull request would allow me to add the to_param attributes in the to_indexed_json method.\nruby\ndef to_indexes_json\n  to_json(methods: [:to_param])\nend\nIs there a way to solve our challenge with the Tire code as is, or do we need something like this pull request?\nCheers,\nAndrew\n. ",
    "hau": "Thanks.  Could you give me a bit more help?  I'm trying to index Chinese text.  It looks like I need to use the \"smartcn_sentence\" tokenizer.  In my model, I have\nruby\n  settings analysis: {\n    filter: {\n      smartcn_word: {\n        'type' => 'smartcn_word'\n      }\n    },\n    analyzer: {\n      smart_chinese: {\n        tokenizer: 'smartcn_sentence',\n        filter: %w[smartcn_word],\n        type: 'custom'\n      }\n    }\n  } do\n    mapping do\n      indexes :content, type: 'string', analyzer: 'smart_chinese'\n      indexes :tags, type: 'string', index: 'not_analyzed'\n    end\n  end\nhow do I add another \"ngram\" tokenizer?\n. ",
    "woahdae": "Also, don't know why, but I'm not a huge fan of Tire::aliases' namespace. I didn't see a precedent for this, but what about moving it to Tire::Index.aliases?\n. Just read through #92 on a google search for a different problem, I'll see about implementing the block syntax or close this if it doesn't fit in my timeline.\n. FYI, I might have some time after a product launch to look in to implementing the block syntax (#92)\n. (that launch being in a week)\n. The full test suite passes, and I'd hope :as has coverage, but I'll give it a check in my app.\n. +1 for parent-child.\n. left my thoughts here\n. Note, this turned out to not be as useful as I had originally thought. The confusion over what exactly norms_field does can be seen here, and for my purposes these arguments didn't help much.\nStill a valid patch, but FYI I'm not actually depending on it anymore.\n. Hey, thanks for adding support for parent relationships, but am I crazy, or is this weird: To get parent/child behavior I need to put the parent option in every document in ruby, so it can be subsequently deleted from the documents and inserted into the bulk meta, which just seems odd. It's not like you can have multiple parents per bulk action, so why not just pull it from the mapping? Plus #store accepts parent, but #update_index doesn't support it?\nMaybe it's particular to my use case, but I would have expected to put :_parent in the mapping, since ES needs that already, and have #bulk and #store pull parent/routing info from the mapping.\nI have a monkeypatch that does this already, prior to these changes, and am willing to make a pull request if you think it's a sane strategy.\n. ",
    "krainboltgreene": "\nYes, I don't think alias_method :to_indexed_json, :to_json will work for Mongoid.\n\nYour code in the readme:\nruby\ndef to_indexed_json\n  self.to_json\nend\nis the same as\nruby\nalias_method :to_indexed_json, :to_json\n\nAlso, I don't know what's the issue you're reporting.\n\nAnd I'm reporting that Story.search \"Lorem\" results in an exception that none of your documentation seems to cover.\n. For what it's worth, if I call Story.tire.search params[:query], load: true I get a @response object, with a lot of nested hashes.\n. If I attempt to use Store.tire.search I get this error:\nDocument not found for class Story with id(s) 4f1c67c1a53b22f41b000002, 4f1c67c1a53b22f41b000007, 4f1c6832a53b22f4d5000004, 4f1c6832a53b22f4d5000009, 4f1c6832a53b22f4d500000a, 4f1c683fa53b22f506000005, 4f1c6df9a53b22f93d000001, 4f1c6df9a53b22f93d000006, 4f1c7109a53b22fc97000004, 4f1c7109a53b22fc97000009.\nI wonder if my es instance has old objects that don't match objects in my MongoDB?\n. Ok, I've fixed the issue. Here's what I had to do, from the beginning:\n1. Change Story.search to Story.tire.search\n2. Drop the ES index with Story.tire.index.delete\n3. Reseed the database: rake db:reseed\nAnd now it works. I get full on Mongoid objects.\n. ",
    "aq1018": "yes I was using the latest gem. Thanks for the reply. \n. ",
    "JamesHarrison": "Has this changed? Specifically I'm looking at updating a mapping and updating all documents without doing a complete delete/recreation since that takes about 7 hours and requires a site outage, which isn't ideal. What's the current best way to go about this?\n. @karmi https://github.com/karmi/tire/pull/520 seems to do the trick - any chance of a merge?\n. Speed is a major issue, at least for me, with tire:import. More information on progress and an ETA would be good. A truly verbose 'here's what I'm doing now' mode would rock, too.\n. Okay - having thought about this and had a look at your code, and how it'd apply to my situation here's what I'm thinking. We basically are using tire for everything, which means we're dealing with a lot of variations in queries, mostly in the search filters rather than query booleans. We're using filters (term/terms, numeric_range, mostly) for most of our stuff; our queries are all but in one case just 'match all'.\nWhat I think I need to do is serialize the hash/store it, and then from that query, regenerate a new query using the DSL and the information from that stored hash. This way I can get relatively transparent query regeneration on deserialization, I think. The controller can still handle inserting the user-specific stuff into a search. The sorts of things where the serializable search is needed is, for instance, on drilldown into a result page, users can navigate within their results (combined with their account-specific query inclusions) from that page.\n. Had a look but I'm unsure where to put it/how to structure it - not quite that sorted yet with regards to getting familiar with tire's codebase!\n. ",
    "ReLrO": "Hey Karmi.\nIt is because of the Observer. Once I am not observing the specific model, it will not happen. Something about the observer makes Tire load on every call. Look at this post. I know it is unrelated but a comment posted there solved my problem for now \nhttps://github.com/mongoid/mongoid/issues/1360\nI installed the gem mentioned there in the last post (https://github.com/thedarkone/rails-dev-boost) and it actually worked.\nI now just hope it will not happen in production. \n. Ok, i'll try to check it. Thanks\n. I tried this option which didn't work either:\ntire.settings analysis: {\n           filter: {\n              name_ngrams: {\n                 side: \"front\",\n                 max_gram: 10,\n                 min_gram: 1,\n                 type: \"edgeNGram\"\n              },\n              name_metaphone: {\n                 replace: false,\n                 encoder: \"metaphone\",\n                 type: \"phonetic\"\n              }\n           },\n           analyzer: {\n              full_name: {\n                 filter: [\n                    \"standard\",\n                    \"lowercase\",\n                    \"asciifolding\"\n                 ],\n                 type: \"custom\",\n                 tokenizer: \"standard\"\n              },\n              name_metaphone: {\n                 filter: [\n                    \"name_metaphone\"\n                 ],\n                 type: \"custom\",\n                 tokenizer: \"standard\"\n              },\n              partial_name: {\n                 filter: [\n                    \"standard\",\n                    \"lowercase\",\n                    \"asciifolding\",\n                    \"name_synonyms\",\n                    \"name_ngrams\"\n                 ],\n                 type: \"custom\",\n                 tokenizer: \"standard\"\n              }\n           }\n        } do\n          mapping {\n             indexes :full_name,     type: \"string\",     search_analyzer: \"full_name\", index_analyzer: \"partial_name\"\n             indexes :id,            type: 'integer',    index: :not_analyzed\n             indexes :email,         type: 'string',     index: :no\n             indexes :created_at,    type: 'date'\n             indexes :profession,    analyzer: 'whitespace'\n             }\n  end\n. these are my latest settings:\nsettings :number_of_shards => 5,\n           :number_of_replicas => 1, \n           analysis: {\n             filter: {\n                name_ngrams: {\n                   side: \"front\",\n                   max_gram: 10,\n                   min_gram: 1,\n                   type: \"edgeNGram\"\n                },\n                name_metaphone: {\n                   replace: false,\n                   encoder: \"metaphone\",\n                   type: \"phonetic\"\n                }\n             },\n             analyzer: {\n                full_name: {\n                   filter: [\n                      \"standard\",\n                      \"lowercase\"\n                   ],\n                   type: \"custom\",\n                   tokenizer: \"standard\"\n                },\n                name_metaphone: {\n                   filter: [\n                      \"name_metaphone\"\n                   ],\n                   type: \"custom\",\n                   tokenizer: \"standard\"\n                },\n                partial_name: {\n                   filter: [\n                      \"standard\",\n                      \"lowercase\",\n                      \"name_ngrams\"\n                   ],\n                   type: \"custom\",\n                   tokenizer: \"standard\"\n                }\n             }\n        } do\n          mapping {\n             indexes :full_name,  :type => 'multi_field', :fields => {\n                      metaphone: { type: \"string\", analyzer: \"name_metaphone\" },\n                      partial: { search_analyzer: :full_name, index_analyzer: :partial_name , type: \"string\" },\n                      full_name: {type: \"string\", analyzer: :full_name }\n                    }\n             indexes :id,            type: 'integer',    index: :not_analyzed\n             indexes :email,         type: 'string',     index: :no\n             indexes :created_at,    type: 'date'\n             indexes :profession,    analyzer: 'whitespace'\n             }\n        end\nwhat do I need to write in the tire.search method in order for it to work?\n. ",
    "tonymarschall": "In my current application i have build a simple search by querying the database directly. Authorisation is done by cancan (@articles = Article.accessible_by(current_ability) ) so i can define what a user is able to access (eg. User can only see tasks form projects he is involved). \nWhat is the concept with tire/elasticsearch, do i have to setup different indexes for different users/permissions?\n. Thanks for your help. Will have a look into this concepts.\n. ",
    "tobowers": "rcov isn't 1.9 compatible so bundle install doesn't work in ruby 1.9+\nI don't see any integration tests for other query types... I'm looking in the test/integration directory and I only see query_string.  Am I just not seeing the location?\n. added a integration test\n. ",
    "lucascaton": "When a new version (with prefix query) will be released?\n. ?\n. ",
    "nfo": "I totally agree. It would need an update of the import method too. Do you want me to take care of this ?\n. ",
    "jedi4ever": "Yeah, why not just go in steps? Include the timeout to get people going, adding another Faraday is not going to be harder by doing so?\n. @karmi thanks for the comments. looking forward to new implementation . :+1: \n. ",
    "chanced": "@vhyza Thanks for taking the time to check out my issue. I've updated my gist to include both of what you were asking for.\n. I moved it all to an overridden search method on the User object and structured it as you suggested. It worked like a charm, thanks man! This was a major PITA; thanks for helping me solve it man.\n. ",
    "shoshi-baron": "Yes, thanks for the quick reply.\n```\nthis is my model\nclass Product\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\n  def self.by_name(name)\n    q = Tire.search \"products\" do\n          query do\n            string \"name:#{name}\"\n          end\n        size 2\n    end\n    return q.results\n  end \n  end\nexample of an object\nProduct.by_name(\"Pioneer\")\n => #4, \"timed_out\"=>false, \"_shards\"=>{\"total\"=>5, \"successful\"=>5, \"failed\"=>0}, \"hits\"=>{\"total\"=>305, \"max_score\"=>4.409558, \"hits\"=>[{\"_i\nndex\"=>\"products\", \"_type\"=>\"document\", \"_id\"=>\"VzeRgAg7RWuU-j0iHOiP7A\", \"_score\"=>4.409558, \"_source\"=>{\"retailer_id\"=>4, \"name\"=>\"Pioneer  (AVX505)\", \"brief_desc\"=>\"Pioneer  (AVX505)\", \"long_desc\"\n=>\"Pioneer  (AVX505)\", \"price\"=>\"199.50\", \"url\"=>\"http://gan.doubleclick.net/gan_click?lid=41000000032151753&pid=AVX505&adurl=http%3A%2F%2Fwww.electronicexpress.com%2Fcatalog%2F6682%2FPioneer-AVX505\n%3Fcpc%3D6&usg=AFHzDLtjm2C1PfwkTv9XHQzOTt0jeWZkjA&pubid=21000000000387855\", \"image_url\"=>\"http://images.electronicexpress.com/products-lg/photo_not_available.gif\", \"category\"=>\"\\\"Electronics\\\"\"}}, {\n\"_index\"=>\"products\", \"_type\"=>\"document\", \"_id\"=>\"44JCwIJ6QcuXCapTd1MeXA\", \"_score\"=>3.741634, \"_source\"=>{\"retailer_id\"=>4, \"name\"=>\"Pioneer Bluetooth Adapter for Compatible Pioneer (AS-BT100 / AS\nBT100)\", \"brief_desc\"=>\"Pioneer Bluetooth Adapter for Compatible Pioneer Products (Black) (AS-BT100 / ASBT100)\", \"long_desc\"=>\"Pioneer Bluetooth Adapter for Compatible Pioneer Products (Black) (AS-B\nT100 / ASBT100)\", \"price\"=>\"89.00\", \"url\"=>\"http://gan.doubleclick.net/gan_click?lid=41000000032151753&pid=ASBT100&adurl=http%3A%2F%2Fwww.electronicexpress.com%2Fcatalog%2F17983%2FPioneer-ASBT100%3F\ncpc%3D6&usg=AFHzDLuGsILA829IB8cj9c70sv6ewJsJIw&pubid=21000000000387855\", \"image_url\"=>\"http://images.electronicexpress.com/products-lg/17983.jpg\", \"category\"=>\"\\\"Car Audio / Video & GPS > Car Audio \nAccessories\\\"\"}}]}}, @options={:size=>2}, @time=4, @total=305, @facets=nil, @wrapper=Tire::Results::Item>\n```\n. I try to call update_attributes on this object. \np = Product.by_name(\"Pioneer\")\nattr = {:suspend => true}\np.update_attributes(attr)\n=> nil\nbut nothing updated.\n. I added the persistence module, but how can it help me?\nI try to see the method 'update_attribute' on query result item and it isn't there.\nI think that ' Tire::Results::Item' is  not the same as my class Product instance, so it doesn't have 'update_attributes' method.\nAm I right?\n. ",
    "rand99": "i found the error. i can't access instancevariables in the query blocks (that sucks)\nbut \nsearch = @search \ndid the trick\nhttp://stackoverflow.com/questions/5150483/instance-variable-not-available-inside-a-ruby-block\nthx vhyza\n. ok thx for your response\nin my case\n```\n@v_group = VGroup.find(params[:id])\n@search = params[:search] if params[:search]\ngroup_id = @v_group.id.to_s\nsearchtxt = @search\npage = params[:page]\n@v_products = @v_group.search_v_products(@search, params[:page])\n@v_products = VProduct.tire.search :per_page => Kaminari.config.default_per_page, :page => page do\n        query do\n          string @search\n        end\n        filter :terms, :v_group_id => [group_id]\n      end\n```\nresults in \n......roducts][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\\"query\\\":{\\\"query_string\\\":{\\\"query\\\":null}},\\\"filter\\\":{\\\"terms\\\":{\\\"v_group_id\\\":[\\\"4e78007f499dda9ef3000001\\\"]}},\\\"size\\\":10}]]];\nand \ndef show\n @v_group = VGroup.find(params[:id])\n @search = params[:search] if params[:search]\n group_id = @v_group.id.to_s\n searchtxt = @search\n page = params[:page]\n #@v_products = @v_group.search_v_products(@search, params[:page])\n @v_products = VProduct.tire.search :per_page => Kaminari.config.default_per_page, :page => page do\n         query do\n           string searchtxt\n         end\n         filter :terms, :v_group_id => [group_id]\n       end\nworks just fine.\ni only replaced @search by searchtxt\nas the query error said the query_string is null\ni m using ree and rails 3.1.3.\nprobably something goes wrong\n. @karmi ok. thank you. i guess my ruby skills in general need some improvement! wish you a nice weekend\n. ",
    "behrangsa": "Sorry, SubArticle should extend Article.\n. Ok, thanks for the reply.\n. ",
    "HenleyChiu": "Open it up again! This is flat out wrong. It should be:\nboolean({:minimum_number_should_match => 1}) do\nNOT boolean(:minimum_number_should_match => 1)\n. @karmi I recommend deleting your post way up above if it's dangerous. Someone in a rush might just execute what you suggested w/o scrolling down below.\n. Thanks ike-bloomfire for the very explicit solution.\n. ",
    "emerak": "Is this working? I have this\nboolean(minimum_number_should_match: 2) do\n      should { string(\"e_name:#{search[:name]}\") } unless search[:name].blank?\n      should { string(\"e_name:#{search[:last_name]}\") } unless search[:last_name].blank?\nend\nEven though I am only sending one param (name) it returns all of the matches, what I am doing wrong? Am i missing something?\n. @9mm did you find a solution?\n. ",
    "hraynaud": "@vhyza \nThanks for the assistance. You are right but I can't figure out what the scope problem is.  I wonder if it is related to the model  I'm observing being inside a gem.  Fortunately there's not much logic in the model. I suppose I can extract the whole model into my application and see if the problem goes away.  Unfortunately, I'm about out of my depth when it comes to the subtle interactions of gems and engines with the main rails app.\nIf you have any additional thoughts I would appreciate it. Thanks\nHerby \n. @vhyza thanks for your help. I think i've resolved this issue.  There is definitely scope wonkiness under some circumstances when I have  more time I will document the behavior more accurately. For now everything works mostly as expected.\nThanks again.\n. ",
    "naamakat": "sorry, issue was opened by a mistake! please remove ...\n. ",
    "trollybaz": "Great, thanks.\n. ",
    "matthewrudy": "I ended up using this gist https://gist.github.com/2041121.\nBut did it slightly different.\n``` ruby\nif index_url = ENV[\"BONSAI_INDEX_URL\"]\n  # defer to the heroku env\n  uri = URI(index_url)\nTire.configure do\n    \"http://#{uri.host}\"\n  end\nTIRE_INDEX = uri.path[1..-1]\nelse\n  # use the default url\n  TIRE_INDEX = \"refer-me\"\nend\n```\n. Perhaps a reasonable solution is;\nruby\nTire.configure do\n  index_url ENV[\"BONSAI_INDEX_URL\"]\nend\nThis could in turn set a Tire.global_index variable.\nThereby\nruby\ndef MyModel.index\n  Tire.global_index || MyModel.name.underscore\nend\nSomething like that?\n. @Evan-M that was my meaning.\nruby\nTire.configure do\n  index_url ENV[\"BONSAI_INDEX_URL\"]\nend\nshould set the values of global_index and url automatically.\nNotably, this makes the concept of url a bit wrong as well, what we really mean is base_url or something of that sort.\n. ",
    "Evan-M": "@karmi @matthewrudy How about just allowing for a tire config option to set a global index name? The global_index option could default to nil, reverting the index naming back to the default behavior of one index per model:\nruby\nTire.configure do\n  # Load all models under one index; store model name in the document type?\n  # Example:\n  global_index \"chunky_bacon\" # or TIRE_INDEX, ELASTICSEARCH_INDEX, ENV['ELASTICSEARCH_INDEX'], etc...\nend\nThis would maintain the existing functionality of Tire, but keep it agnostic of bonsai.io.\nYour bonsai.rb initializer file could look like this:\n``` ruby\nif ENV[\"BONSAI_INDEX_URL\"]\n  uri = URI(ENV[\"BONSAI_INDEX_URL\"])\nTire.configure do\n    url \"http://#{uri.host}\"\n    global_index uri.path[1..-1]\n  end\nend\n```\nThoughts?\n. I came up with these unit tests for the above discussed features to add to the configuration_test.rb:\n``` ruby\nshould \"return default global_index_name of nil\" do\n  assert_nil Configuration.global_index_name\nend\nshould \"allow setting and retrieving the global_index_name\" do\n  assert_nothing_raised { Configuration.global_index_name 'unique_key_or_name' }\n  assert_equal 'unique_key_or_name', Configuration.global_index_name \nend\nshould \"return default index_url of nil\" do\n  assert_nil Configuration.index_url\nend\nshould \"allow setting and retrieving of the index_url\" do\n  assert_nothing_raised { Configuration.index_url 'http://es.example.com/chunky_bacon' }\n  assert_equal 'http://es.example.com/chunky_bacon', Configuration.index_url\nend\nshould \"set the URL and global_index_name from index_url\" do\n  assert_nothing_raised { Configuration.index_url 'http://es.example.com/unique_key_or_name' }\n  assert_equal 'http://es.example.com', Configuration.url\n  assert_equal 'unique_key_or_name', Configuration.global_index_name\nend \n```\nI'll make these tests pass and send @karmi a pull_request.\n. Heh @aaronchi, I was just running into that earlier today as I was trying to get things working with Bonsai... here is what I came up with:\nAddition to the Tire::Index class:\n``` ruby\n/lib/tire/index.rb\n...\n  def bulk_import_url\n    url = Configuration.url\n    url += \"/#{Configuration.global_index_name}\" if Configuration.global_index_name\n    url + \"/_bulk\"\n  end\n  ...\n```\nThen the bulk_store method in the Tire::Index class posts to the bulk_import_url response message:\n``` ruby\n/lib/tire/index.rb\n...\n  def bulk_store documents\n    ...\n    begin\n      response = Configuration.client.post( bulk_import_url, payload.join(\"\\n\") )\n      ...\n    ensure\n      curl = %Q|curl -X POST \"#{bulk_import_url}\" -d '{... data omitted ...}'|\n      logged('BULK', curl)\n    end\n  end\n  ...\n```\nNote that the global_index_name effectively becomes the default index for all models, if it is set:\n``` ruby\n/lib/tire/model/naming.rb\n...\n  def index_name name=nil, &block\n    @index_name = name if name\n    @index_name = block if block_given?\n    @index_name = Tire::Configuration.global_index_name if Tire::Configuration.global_index_name\n    @index_name || [index_prefix, klass.model_name.plural].compact.join('_')\n  end\n  ...\n```\n. ",
    "thoughtpunch": "What happened with this? This was an awesome feature!\n. Any word on this? Need to remove items from the search index that have been soft-deleted via ActsAsParanoid. Would like to stay withing the Tire API if posssible, something like Tire.index(\"users\").remove_from_index(\"5243\")\n. Have you had any success filtering on other fields other than ID? I keep getting a QueryParsingException[[users] No filter registered for [first_name]]; }]\",\"status\":500 error.\nAny ideas?\n. I am an idiot...sorry\n. ",
    "woohoou": "Do you know if is possible call ._explanation with the \"load: true\" in the search function?\n. ",
    "cwadding": "Thanks, that syntax works great.\n. Another related question. I am not even sure if elastic search allows for nesting and's inside or's and vice versa for filters but I was experimenting with a complex query like so:\n```\ndef self.search(params)\n    tire.search(:page => (params[:page] || 1), :per_page => (params[:per] || 10)) do\n    filter :or, {:and => [:term => { \"name.second\" => \"banon\"}, :term => {\"name.nick\" => \"kimchy\"}]}, {:and => [:term => { \"user\" => \"banon\"}, :term => {\"name.first\" => \"drew\"}]}\n\n    raise JSON.pretty_generate(to_hash)\nend\n\nend\n```\nBut I get the same problem or the first term being overwritten by the second.\n\"filter\": {\n    \"or\": [\n      {\n        \"and\": [\n          {\n            \"term\": {\n              \"name.nick\": \"kimchy\"\n            }\n          }\n        ]\n      },\n      {\n        \"and\": [\n          {\n            \"term\": {\n              \"name.first\": \"drew\"\n            }\n          }\n        ]\n      }\n    ]\n  },\n  \"size\": 10,\n  \"from\": 0\n}\nIf it is even possible to process a nested query using elastic search how would I create the JSON using tire.\n. ",
    "shawndeprey": "@cwadding Your answer works. I've seen many solutions around the web that do not work, but you're invariably solves this problem. Thank you good sir. \n. ",
    "kshnurov": "What if boosted field is not used in query? For example, I need to boost results with virtual = true, how can I do this?\nquery do\n      boolean do\n        must {text 'name', s, :analyzer => 'word_analyzer_search', :fuzziness => 0.9, :boost => 5}\n        should {text '_all', s, :analyzer => 'word_analyzer_search', :fuzziness => 0.9}\n      end\n  end\n. Can you implement boosting query (http://www.elasticsearch.org/guide/reference/query-dsl/boosting-query.html)?\nI can't use boost factor for some reasons.\n. Renamed it to Product - no changes.\n. Well, found a very strange way:\nItem.index.delete\nAfter that I close and open rails console again and everything is great:\n1.9.3-p125 :001 > Item.index.mapping\n => {\"item\"=>{\"properties\"=>{\"available\"=>{\"type\"=>\"boolean\"}, \"category\"=>{\"type\"=>\"string\"}, \"currencyId\"=>{\"type\"=>\"string\"}, \"description\"=>{\"type\"=>\"string\", \"analyzer\"=>\"word_analyzer\"}, \"gs_cat\negory_id\"=>{\"type\"=>\"long\"}, \"gs_id\"=>{\"type\"=>\"long\"}, \"gs_product_key\"=>{\"type\"=>\"string\"}, \"merchant_id\"=>{\"type\"=>\"long\"}, \"model\"=>{\"type\"=>\"string\"}, \"name\"=>{\"type\"=>\"string\", \"boost\"=>2.0, \"an\nalyzer\"=>\"word_analyzer\"}, \"original_picture\"=>{\"type\"=>\"string\"}, \"param\"=>{\"type\"=>\"string\"}, \"params\"=>{\"type\"=>\"string\"}, \"picture\"=>{\"type\"=>\"string\"}, \"price\"=>{\"type\"=>\"float\"}, \"thumbnail\"=>{\"\ntype\"=>\"string\"}, \"url\"=>{\"type\"=>\"string\"}, \"vendor\"=>{\"type\"=>\"string\"}, \"vendorCode\"=>{\"type\"=>\"string\"}}}}\nWhy Item.index.create creates empty mapping? How to create right mapping from console? What magic happens, when I reopen console? :)\n. ",
    "haarts": "We changed bulk_store method some more in order to support parent/child relations in the bulk import functionality. We extracted the string interpolated header line into a separate method for clarity.\n. ",
    "cjbottaro": "Yes, there is a difference.\nDoes not work:\nf.query{ string(\"form_field_10061:*UNT*\") }\nDoes work (using this patch):\nf.query{ wildcard(\"form_field_10061\", \"*UNT*\") }\nform_field_10061 is a not_analyzed string field.  Using Elasticsearch-0.19.11.\nI vote to accept this pull request... :P\n. Here's the pull request... https://github.com/karmi/tire/pull/520\n. Sorry, everything works as expected.  Retry failures are logged to stderr instead of the logger, which was throwing me off.  Sorry for the noise.\n. ",
    "caueguerra": "I got it working with\nruby\nfilter :not, { :term => { :index_name => 'value' } }\n. ",
    "rdetert": "Had a similar question and got it working by going:\nfilter :not, { missing: { field: 'email' } }\nJust documenting for someone in case they find it useful.\n. Fair enough. I think I may just run a cron job every hour to batch import new records. \n. ",
    "davoclavo": "A quick and dirty solution to handle exceptions when calling update_index\nAdd this monkeypatch initializers/tire_monkeypatch.rb\nruby\nmodule Tire::Model::Search::InstanceMethods\n  def update_index\n    instance.run_callbacks :update_elasticsearch_index do\n      if instance.destroyed?\n        index.remove instance\n      else\n        response = index.store( instance, {:percolate => percolator} )\n        success_response = response['ok'] == true or (response['status'] > 0 && response['status'] < 400)\n        if !success_response && instance.respond_to?(:on_indexing_error)\n          instance.on_indexing_error(response)\n        end\n        instance.tire.matches = response['matches'] if instance.tire.respond_to?(:matches=)\n        self\n      end\n    end\n  end\n  alias :update_elasticsearch_index  :update_index\n  alias :update_elastic_search_index :update_index\nend\nAnd on the model, create a on_indexing_error method which handles the response.\n. ",
    "pshoukry": "added support for terms_stats\n. Hi Karmi, You are most welcome, thank you for the great gem saved me tons of time.\n. ",
    "parix": "I figured it out lol. Such a noob.\nTire.configure do\n  url 'http://www.example.com:1234'\nend\nfixed it =)\n. ",
    "yortz": "yeah that actually did the trick I am gonna close this one now.\n. ",
    "phoet": "i don't know sunspot, but i guess each_with_hit describes what it's supposed to do.\nwe use this to show meta-data, ie showing debug information like score etc.\n. @karmi just watching your talk at RuPy and see you talking about missing feedback :smile: \ni think i got unsubscribed from this issue. we currently have a custom fork of tire, i will try to update and contribute back our changes in the next weeks and have a look at the result decorator.\n. simple test is simple ;)\n. thanks for pulling. please have a look at my other pull-requests too. i would like to hear some feedback!\n. is this similar to #771?\n. @karmi would you consider this a \"hack\"? and prefer the extension to RestClient for DELETE with payload?\n. looks like this is also related to #159 \n. @kelaban so this property should be an array of a custom type?\n. @hsadan did you find anything? like @karmi said, it's usually missing records (or scopes in rails) that just don't find the records in the db. have a look in the application logs and try to execute the query yourself to find out why the records are missing.\n. @vgalu when using the :load option, you would have to change it back to the {\"$oid\" => \"value\"} format, for mongo to find it, right?\n. @karmi do you think it would make sense to add such a method to RestClient? it would be fairly trivial to add a delete_with_payload method (or just add it to tire).\nruby\n  def self.delete_with_payload(url, payload, headers={}, &block)\n    Request.execute(:method => :delete, :url => url, :payload => payload, :headers => headers, &block)\n  end\n. i will try to take care of this over the weekend\n. just a note: http://stackoverflow.com/questions/299628/is-an-entity-body-allowed-for-an-http-delete-request\n. @regedarek boosting in elasticsearch is not trivial. it depends on the number of keywords in a document and i always forget how to tweak those things, because it's so akward. if you want to read more, here is a nice article about scoring in elasticsearch http://jontai.me/blog/2013/01/advanced-scoring-in-elasticsearch/\ni don't think that this is a tire issue though, as it is just an api for ES.\n. @regedarek i don't think that this is the way that elasticsearch scoring works. AFAIK there is no \"foreach\" kind of thing. you could implement a script for your score though: http://www.elasticsearch.org/guide/reference/query-dsl/custom-score-query/\n. @jonarrien is {\"$oid\"=>\"51cea19ca8bab75de2000001\"} the id of the document in elasticsearch? is that a nested document? i saw an incompatibility note in mongoid about the \"$oid\" part: https://github.com/mongoid/mongoid/blob/master/CHANGELOG.md\n. would it make sense to create a cross-reference-issue in mongoid to let them know about such things breaking tire and possible other libraries?\n. :+1: for this. we only had trouble using MultiJson anyways. always had to do a lot of monkey-patching for MultiJson and tire...\n. @karmi remember all those rails json loading, symbolizing keys issues?\nafter that we switched to OJ as a json parser (really fast, reduced our response-times by half) and MultiJson was overriding all default OJ settings, so that we had to patch it to use our default settings. this seems to be resolved right now.\nanother thing was our dependency to couch_potato, which does not use MultiJson, so we basically had 2 JSON config points etc...\n. a little offside, but are there any plans to drop 1.8.7 support?\n. :+1: \n. @marc-villanueva your changes are breaking the ruby 1.8.7 builds on travis. could you use old hash syntax in the tests, so that the travis build passes?\n. @vasconcelloslf when you look at the collection class (https://github.com/karmi/tire/blob/master/lib/tire/results/collection.rb#L156) you can see that everything that you pass to :load will get propagated klass.find(ids, @options[:load])\nso when you use the ActiveRecord option :select for find, it should work.\n. also connected to #762 \n. @vasconcelloslf can you link the changes in there, so that @karmi can review that?\n. some integration-tests are failing on travis. i also have failing tests on master locally. i don't think that my changes should break anything.\n. @karmi did you ever find out why the percolate queries fail on travis? is that a problem of versions? \nthe changes i made allow the local test-suite to pass with ruby 2 and elasticsearch 0.90.2\n. @abrahamD looks good, are you sure everything is configured properly? did you have a look at the tire logs to see what happens exactly.\n. @rahul did you have a look at issue #56 ? is that similar to your problem?\n. what exactly do you want to achieve? please give some input and output examples.\ni am just guessing, but i think that this might be what you are looking for:\nwhen defining geographies as :type => 'string', :index => :not_analyzed\nand i persist documents like\nruby\nstore :type => 'bar', :geographies => \"ward\"\nstore :type => 'bar', :geographies => \"ward bord\"\nthen i get ward bord and ward as facets\nruby\n @facets=\n  {\"geographies\"=>\n    {\"_type\"=>\"terms\",\n     \"missing\"=>0,\n     \"total\"=>2,\n     \"other\"=>0,\n     \"terms\"=>\n      [{\"term\"=>\"ward bord\", \"count\"=>1}, {\"term\"=>\"ward\", \"count\"=>1}]}},\n. i totally understand your concern, but why are you using a timestamped index and an alias when your hosted ES supports only 1 index?\n. what do you think of putting this in a configuration like \nruby\nTire.configure do\n  auto_create false\nend\nwould have the benefit of having it in place with other environment specific configuration like urls etc.\n. hey, no problem!\ni think that the tests must not fail locally. is there a way to seperate\nthese destructive tests somehow and have them run only on travis?\nAm Donnerstag, 18. Juli 2013 schrieb Karel Minarik :\n\nIn test/integration/active_record_searchable_test.rb:\n\n@@ -613,7 +613,7 @@ module ::Rails; end\n       context \"percolated search\" do\n         setup do\n           delete_registered_queries\n-          delete_percolator_index if ENV['TRAVIS']\n-          delete_percolator_index\n\n@phoet https://github.com/phoet I'd rather have failing tests, then\nwipe the percolator index for people :) Someone might just run the Tire\nsuite, not realizing we're wiping the index, he might have some\nhard-to-get-again data there... Let's be rather more careful here. Sorry\nabout the silence about the pull requests, I'm at a vacation now, will try\nto squeeze some review & merging into next week.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/pull/789/files#r5263423\n.\n. \n",
    "mhenrixon": "Please don't laugh out loud at this code... :)\n``` ruby\n  mapping do\n    indexes :id\n    indexes :title, boost: 200\n    indexes :description,  boost: 55\n    indexes :privacy\n    indexes :pr\n    indexes :prescribed\n    indexes :workout_at, type: 'date'\n    indexes :box_wod_id\n    indexes :bench_id\n    indexes :user_id\n    indexes :username\n    indexes :group_names, boost: 150\n    indexes :photo_file_name\n    indexes :email_notification?\n    indexes :public?\n    indexes :box_only?\n    indexes :private?\n    indexes :pr?\n    indexes :rx?\n    indexes :video_link\n    indexes :comments_count\n    indexes :result_time\n    indexes :result_weight\n    indexes :result_rounds\n    indexes :result_reps\n    indexes :result do\n      indexes :time\n      indexes :weight\n      indexes :rounds\n      indexes :reps\n      indexes :twitter\n      indexes :facebook\n    end\n    indexes :user do\n      indexes :id\n      indexes :username,  :type => 'string', boost: 99\n      indexes :email\n      indexes :username_migrated\n      indexes :profile_pic_filename\n    end\n  end\ndef to_indexed_json\n    to_json(methods:\n    [\n      :username,\n      :group_names,\n      :comments_count,\n      :stars_count,\n      :starred_user_ids,\n      :result_time,\n      :result_weight,\n      :result_rounds,\n      :result_reps\n    ], include: { result: { except: [:comparable_id, :comparable_type, :created_at, :updated_at] }})\n  end\ndef comments_count\n    comments.size\n  end\n```\nI also want to add that I never got the nested indexing to work properly meaning the user don't seem to be working as nested that's why I added the username and comment_count methods.\n. If I refresh the index it get's indexed properly. I think the problem here is how I create the comments.\n``` ruby\n  def create\n    @commentable = find_commentable\n    @comment = @commentable.comments.build(params[:comment])\n    @comment.user = current_user\n    @comment.save\nrespond_with @comment, layout: !request.xhr?\n\nend\n```\nMaybe I should save the @commentable instead of the comment or just add it to the @commentable array of comments?\n. Works like a charm!\n. Love it, would be very nice to have this added and released!\n. Ok now we are certainly getting somewhere, I am the first one to admit that I am not always very good at explaining what I am after but here goes.\nWhat I want is to only include phone_numbers that are supposed to be visible in the query result but I don't want to hide the entire (contact) document from the search result if you know what I mean.\nSo if it's possible to exclude phone_numbers with the exclude filter without loosing the entire document I am a very happy with that solution. Is this supported by Tire or do I need to hack together a straight elasticsearch query?\n. ",
    "sarmiena": "I'll take a look when I get home tonight\nOn Mar 22, 2012, at 2:52 PM, Karel Minarikreply@reply.github.com wrote:\n\n@sarmiena @demersus Should be closed in d5c08fb, can you check, please?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/pull/265#issuecomment-4649236\n. nvm. got it :) filter(:exists, :field) #not tested but think that's it\n. After further review, I simply put order() inside of the #paginate method.\n\nMy bad.\n. Failing test 40f99ffc7860801621df515e559fde9893c28f9b\nFAIL test: Index when storing in bulk namespaced instances should serialize namespaced ActiveRecord objects. (1:00:52.861)\nunexpected invocation: Tire::HTTP::Client::RestClient.post('http://localhost:9200/_bulk', '{\\'index\\':{\\'_index\\':\\'active_record_namespace_my_models\\',\\'_type\\':\\'active_record_namespace%2Fmy_model\\',\\'_id\\':\\'1\\'}}\\n{\\'timestamp\\':null,\\'title\\':\\'One\\'}\\n{\\'index\\':{\\'_index\\':\\'active_record_namespace_my_models\\',\\'_type\\':\\'active_record_namespace%2Fmy_model\\',\\'_id\\':\\'2\\'}}\\n{\\'timestamp\\':null,\\'title\\':\\'Two\\'}\\n')\nunsatisfied expectations:\n- expected exactly once, not yet invoked: Tire::HTTP::Client::RestClient.post()\n@ lib/tire/index.rb:85:in `bulk_store'\n  Offending portion of the test:\njson =~ /\"_type\":\"active_record_namespace\\/my_model\"/ results in nil being returned\n_type is being saved as \"active_record_namespace%2Fmy_model\"\n. ",
    "joemsak": "Hi I'm having this same issue but when I run the rake task with FORCE=true it still doesn't go away\nIt's because I'm using :load => true and I need access to the instance methods...\n. Haha please never mind the crap code i wrote 2 years ago (i am replacing acts as indexed with bonsai / tire)\n. The issue here was not fixed, this issue shouldn't be closed. Two comments appear to be missing from the history, as well.\n. My comment was just me being embarrassed about my code example\n. Gotcha, thanks, I will try it out!\n. ",
    "tamird": "@karmi this is still an issue exactly as described by @andruby. Please consider reopening this for discussion, at least.\n. Currently the only workaround:\n``` Ruby\narticle = Article.search('*').results.first\ndef to_param(object)\n  \nend\narticle_path(to_param(article))\n```\n. ",
    "blarralde": "That'd be useful for me too. I've got results from multiple models and it's much cleaner if I can just do link_to result.name, result\n. ",
    "martijn": "This is something I run into as well.. another thing I notice with my Mongoid models is that a specific Time field is returned as a string. The results I get from tire look like my regular Mongoid instances, but they seem quite different.\n. Ah that was not immediately obvious to me, since:\nFoo.tire.search('bar').first.class # => Foo\nThanks!\n. ",
    "fcheung": "D'oh, just seen a bunch of other issues about support various es options and the ugliness of doing it piece by piece\n. Yeah bulk_store is a little messy. It's got a lot in common with multi_search if my memory is correct so there is probably an opportunity to find a nice solution to deal with both. \n. Passing the option argument though update_index would be enough for me at the moment, but still makes things hard if you use the callbacks and you're in control of the call to update_index.\nOn the subject of the dsl, what would we need to do ? We'd need )at the very least ) to mark an attribute of the child  as the one that identifies the parent (and then use that as the default parent value throughout). Are you also thinking about work on the queries/filters ? I think those do work pretty well as things stand - I don't recall hacking those\n. Sorry, yes, not in control. I forget what's in my fork versus not. I'll see if I can put something together that adds this to the mapping DSL and the query stuff is fairly straightforward\n. I think you're right - I had some abortive attempts and didn't come up with something that had the right balance of versatility versus verboseness or complexity\n. I hadn't spotted the tire-contrib version - It hadn't ocurred to me to look there. Looks fine to me.\n. ",
    "raldred": "Shame, I would really like to be able to enable various options like track_scores that cannot be added to the search hash without providing the full payload.\n. Same here, the :page and :per_page options don't exist\nIt should be :from and :size\n. Ahh yes, good spot. I suppose the ideal is that it's consistant, but at least we know now :) Thanks @karmi \n. ",
    "philly-mac": "Well my feeling of hacky, comes from what you term a \"more fluid and convenient\". I know ruby is a \"many ways to do the same thing\" type of language, but I would prefer to be able to explictly pass through the type to the store method. And not have have to embed it in a method that Tire then checks to get the type. \nI guess it just comes down to a difference of taste at the end of the day. For me store(type, document, *args) and remove(type, id, *args) is more intuitive, especially if I was just guessing. \nNow I am doing something like\n``` ruby\n    def self.index(object)\n      search_index = Tire.index(AppConfig.elastic_search.index)\n      search_index.store(to_indexed_hash(object))\n      search_index.refresh\n    rescue Exception => e\n      Padrino.logger.error \"Error: #{e}\"\n    end\ndef self.to_indexed_hash(object)\n  locum.attributes.merge({\n    :type       => type,\n    :addresses  => locum.addresses.map {|a| a.attributes},\n  })\nend\n\n```\nWhich works, and abstracts the whole search thing in my code to this small class. But still feels wrong somehow to be puting the type in the hash. Not 100% sure why, but it does. When/if I figure it out in my own mind I may add to this issue in future. \nThe other things is I feel the README is too \"here and there\". In fact the best way I would describe it is, if I was going over a topic and was just trying to get down on paper everything that thing did. But then at the end I didn't go back and restructre it completely, or format it to make it easy to read. I found that too many time I had to look for the type of code that I wanted to do in the example file, or try and look if I could find an example piece of code that did it. But like I said, I guess it is just a difference in taste. I think I just have a predefined idea of what logical is, and Iif  don't see it, it throws me off a bit.\n. ",
    "amrnt": "Great.\nThough, I want to discuss on how we would search STI models.\nLet's say I have Link model and a subclass Link::Google < Link.\nWe have to main method to focus on, index_name and document_type. Actually, I'm not sure how they work and being set in Tire (I've not trace the source yet). But I think the best way for that is:\n- For both models since the super class is Link the index_name in both cases should be links (by defauls)\n- For Link model, document_type should be Link as it, since type or _type will be nil. For subclasses, ie Link::Google should be as , since AR > 3 saves the type as the class name -I'm not sure how it saved in other ORM or ODM-\nAfter all, when you search via Link (the super class) it must return all the results from across the subclasses models attached with the model type.\n. Notice that my subclass model is namespaced. Translating Link::Google to link_google doesn't make sense since LinkGoogle could be translated to link_google\n. Let me know: when I index Link model, the whole table will be indexed, right?\nif the self.search method is in Link, will the search return results from subclasses? If no as I see, how I could return them... notice that index_name will be links... but the type is different!\n. ",
    "emilford": "Is setting document_type the same for the parent class and all subclasses still the best option?  I want to search the parent class and have it return listings in the subclasses.  If I set document_type on all the subclasses, my tests pass as expected.  If I remove the explicit document_type, tests fail to find results requiring filters and I'm not sure why.\n. ",
    "donaldpiret": "Could you detect the presence of Delayed job (By checking wether the delayed module is defined) and just use it if it is by default? \nThat would make the delayed_job option optional\n. Let me add to this that having the reindexing done on after save can also lead to severe problems if the elasticsearch server cannot be reached.\nIn our case this was happening because it could not connect to the IP address of the elasticsearch server and would just sit there and timeout.\nSince we were still in a transaction the whole mysql update would fail with a Lock timeout error, effectively rendering the elasticsearch server into a single point of failure.\n. ",
    "brandonhilkert": "Worked perfect. Thanks!\n. Cool thanks.\n. ",
    "benjaminbarbe": "https://gist.github.com/a23fe333f0115d13be1e\nThanks\n. ",
    "ches": "This limits the type of filter facet to term only, but filter facets should in fact allow the full query DSL. I'll open a new issue for this.\n. @jurgens @karmi's example is constructing a DELETE request to the resource endpoint of an index, e.g. http://localhost:9200/twitter/, plus some parameters. If query.to_hash.to_param evaluates to an empty string, which you'll find in @Papipo's query that it does, then you're effectively making the administrative delete index request that @karmi linked to.\n. I believe 55d12c0caaf09f80 closed this.\n. Bump on the notion of breaking this up and cherry-picking more manageable bits, like Persistence bang methods.\n. Yay, thanks!\n. This exists in https://github.com/karmi/tire-contrib\n. Regardless of the decision on the handling of arbitrary objects that was discussed here, I still think the documentation @mauricio contributed was helpful -- I was just looking for similar myself (not the first time I've needed a reference!), and after going to code where it isn't obvious, I remembered this pull request.\n. Oh, actually I didn't realize until a second look just now that the possibility of giving a Symbol as an instance method reference was not actually accepted -- otherwise I was going to say there's nothing additional I'm looking for from Tire, just the docs. The Symbol support is a small nicety though IMO, it's a common pattern and a little cleaner than thinking about instance_eval any time it isn't absolutely necessary.\nWhere would you want such documentation to be? I assume expanding the explanation of :as in the RDoc for indexes on Tire::Model::Indexing?\n. (If you look at that method, and you're not familiar with Tire's codebase more fully, it's not at all apparent where :as actually gets handled -- that's why I'm being vocal about a wish for more docs).\n. Yep, I do exactly this, to keep indexing config and models tidy in a Rails app. In that case, where ActiveSupport::Concern is already available, the class eval on include hook is prettied up:\n``` ruby\nmodule CompanyIndexing\n  extend ActiveSupport::Concern\nincluded do\n    include Tire::Model::Search\n    include Tire::Model::Callbacks\ntire.mapping do\n  indexes :id,             :index =>    :not_analyzed\n  indexes :name,           :analyzer => :snowball,    :boost => 50\n  indexes :description,    :analyzer => :snowball\n  indexes :company_url,    :analyzer => :standard\n  indexes :twitter_handle, :analyzer => :standard\n  indexes :created_at,     :type =>     :date\n  indexes :updated_at,     :type =>     :date\nend\n\nend\n# Enable finding companies by searching for the names of related people.\n  def to_indexed_json\n    to_json(\n      include: {\n        relationships: {\n          only: [:role],\n          include: {\n            user: { only: [:name] },\n          },\n        }\n      }\n    )\n  end\nend\n```\nI keep such modules in app/models/indexing, add that to Rails' autoload_paths, then include CompanyIndexing in the Company model.\n. Oh, one thing that maybe is actually worth talking about here: this ultimately blew up because of the next line, the return value of results:\nruby\n@response['hits']['hits'].map { |item| records[item['_type']].detect { |record| record.id.to_s == item['_id'].to_s } }\nIf you won't take offense to me saying so, that's kind of a nasty one-liner :innocent:  detect returns nil if there's no match, so basically I ended up with a Results::Collection which reported a size of one (because there was in fact one legit result from ES, and inspect of the collection rightly showed it), but the Array part of the object (@results) was [nil]. nil is evil. Would you be open to raising an exception here if <result of line above>.compact.length != @response['hits']['hits'].length?\n. Yay, thank you!\n. I was tripped up at first by the difference between these:\nruby\nMyDocument.index.create\nMyDocument.create_elasticsearch_index\nThe former simply creates the index, the latter creates and applies your mapping config. If you do the former and add some documents, you'll end up with dynamic mapping.\n. Hmm, from your explanation on SO I didn't get the \"last resort\" feeling about to_indexed_json (\"they serve two different purposes\"). With the \"computed field\" comment, I meant that you've chosen examples like this:\n``` ruby\nIn Tire::Model::Indexing#mapping RDoc\nindexes :words, :as => 'content.split(/\\W/).length'\nIn README\nindexes :content_size, :as => 'content.size'\n```\n... creating synthesized fields in the elasticsearch index that don't map to actual fields in the source database. That's the \"aha\" I described about choice of examples after reading the explanation: these seemed consistent with the usage you were suggesting on SO.\nAlternatively in my example above, I suppose I could achieve the same thing by eliminating to_indexed_json and making more elaborate use of :as options (or nested indexes blocks?). That seems more troublesome than taking advantage of to_json with includes, in the case of ActiveRecord. If that is preferable though, perhaps to_indexed_json should be heavily deemphasized in docs?\nAs an aside, particularly if to_indexed_json is considered a last resort, I'm definitely even more fond of the recent pull request that added support for giving a Symbol to :as and having that dispatched to a method.\n. Hey @karmi, I'm on vacation and getting away from the keyboard a lot until after Christmas, so may be slow to give feedback, but if you start drafting some new docs in this area, or you want a sounding board for ideas on IRC or something, I'd be happy to lend fresh, novice eyes to them.\n. > Yes. The mapping capabilities have evolved over time, and I haven't made the changes neccessary in the docs.\nAre there any more current/elaborate examples you can point out, in integration tests or elsewhere, that I might get an understanding of how to better approach using it in favor of to_indexed_json? More insight on why the overuse proved to be a bad thing, to inform possible doc improvements?\n. This used to bother me on principle because it's a surprising side effect of code that is essentially declarative configuration, but now I've actually been bitten by it in a real-world way:\nI have Tire::Model::Persistence models with an object hierarchy something like this:\n``` ruby\nmodule Services\n  module Twitter\n    class Status\n      include Tire::Model::Persistence\n    end\nclass Mention\n  include Tire::Model::Persistence\nend\n\nend\nmodule Facebook\n    class Like\n      include Tire::Model::Persistence\n    end\n  end\n# etc\nend\n```\nThese are set up with a naming convention that each class is a type and its containing module is an index. I do not want create_elasticsearch_index running automatically, because only the first type that loads in one of these modules will get its mapping correctly configured by that automatic call, the rest silently fail and I'll have to go behind it and do PUT mappings manually before any data hits it (or rather, write a Rake task to do all this properly, in which case the automatic calls are nothing but a nuisance).\nI think this should not only be an option (easily implemented here as @karmi says), but it should be off by default in a future major version.\n. :tada: \n. General pet peeve: please use a global excludesfile for your IDE's litter so everyone's projects don't need to know about your personal choice of tools.\n. It seems sensible to me. If someone came along with a good use case, it'd probably have to be something where a given object would need to respond_to? some common convention, right? I don't know how else you could sensibly handle it, really. In that case it'd be a simple matter to add a when clause to the case statement @mauricio added to support that.\nI like the clarification/definition of the semantics here otherwise.\n. Can you give an example of the scope-like usage you're referring to?\n. @karmi I understand ActiveRecord scopes, I was asking for an example of how you imagine that being applied in the case of indexes :as :smile: \n. But shouldn't that work as-is? You're using proc/lambda, the same way we naturally would when defining AR scopes like this, and I don't think that is at question here. Your initial question was if there might be a use case for passing some arbitrary object for :as -- that's what I'm asking, for a (hypothetical) example of how you could imagine that being used, sorry if that was unclear. I'm not aware of any analog to that in AR scope definitions.\n@mauricio hasn't changed anything about Tire's existing ability to handle procs/lambdas here, he just wants to bail if given something else that we don't know what to do with. That seems sane and doesn't lock you out of later accepting something else if you came up with something that would be meaningful.\n. ",
    "timting": "So, this is the way I'm using Tire - there's a variations index that I'm querying, and I have methods which return the lambdas required by Tire. With the code as it is below, the option :minimum_number_should_match won't get encoded because by that time the BooleanQuery is already created. If that option is moved to the data_status query, everything works. Is this intended behavior? Let me know if you need more information or a more detailed example.\n```\n  def search(params = nil)\n    s = Tire.search 'variations' do |search|\n      search.query do |query|\n        query.boolean &data_status_query #need options on the first boolean query otherwise it gets overwritten\n        query.boolean :minimum_number_should_match => 1, &color_query(params[:color]) unless params[:color].blank?\n      end\n    end\ns\n\nend\ndef data_status_query\n    lambda do |boolean|\n      boolean.must { term \"data_status\", \"available\"}\n    end\n  end\ndef color_query(color)\n    lambda do |boolean|\n      boolean.must { term \"base_color\", color}\n    end\n  end\n```\nAn example test case is below, modified from search_query_test.rb, line 96\n```\n  should \"encode options when doing boolean twice\" do\n    query = Query.new do\n      boolean do \n        must { string 'bar' }\n      end\n      boolean(:minimum_number_should_match => 1) do \n        must { string 'foo' }\n      end\n    end\nassert_equal 1, query[:bool][:minimum_number_should_match]\n\nend\n```\n. Yes, once I had determined that my code wasn't the issue, I checked the code and noticed that line, which made everything clear. What's not clear is what the best solution would be - would we just merge the options from subsequent boolean calls into the options hash, potentially overwriting the previous settings?\nWhat helps a lot with understanding this is that, as you said, on ES's side, there's just one boolean query - with an iterative approach it's easy to think there's multiple queries and settings for each are independent, which was my initial understanding.\n. Yeah, I'd vote for that too - I was kind of surprised that there was no way to change options except the first time you called boolean.\n. ",
    "alx1": "Sorry :-) . Here we go:\nThe app is an incident reporting system which allows public users to submit reports. Reports are then validated and analysed by admins and then grouped into Incidents. Admins can also write an Analysis of an incident and these are grouped with the Incident. At some point the Incident may be published along with some of its associated Reports and Analyses, but not necessarily all of them.\nThe Incident records don't hold much data - it's mostly in the Reports and Analyses.\nPublic users can search the list of Incidents and should find matches based on any public Reports or Analyses within the Incident.\nIf a public user tries to view a Report which is not yet \"public\" they get access denied (using cancan, incidentally).\nSearching on the contents of a report which is not yet \"public\" should not trigger a hit.\n``` ruby\n/app/models/incident.rb\nclass Incident < ActiveRecord::Base\n  has_many :reports\n  has_many :analyses\ninclude Tire::Model::Search\n  include Tire::Model::Callbacks\nmapping do\n    indexes :id, type: 'integer'\n    indexes :summary, boost: 10\n    indexes :date, type: 'date'\n    indexes :status\n    indexes :created_at, type: 'date'\n  end\ndef self.search(params)\n    per_page = params[:per_page] || 10\ntire.search(page: params[:page], per_page: per_page) do\n  query { string params[:query], default_operator: \"AND\" } if params[:query].present?\n  sort { by :date, \"DESC\" } if params[:query].blank?\nend\n\nend\ndef to_indexed_json\n    to_json( include: { analyses: { only: [:narrative, :comments] }, reports: { only: [:summary, :narrative] } } )\n  end\nend\n```\n``` ruby\n/app/models/report.rb\nclass Report < ActiveRecord::Base\n  belongs_to :user\n  belongs_to :incident\nhas_many :factors\naccepts_nested_attributes_for :factors, :allow_destroy => true\nend\n```\n``` ruby\n/app/models/analysis.rb\nclass Analysis < ActiveRecord::Base\n  belongs_to :user\n  belongs_to :incident\nend\n```\n``` ruby\n/app/views/incidents/index.html.haml\n%h1 Incidents\n%hr.soften\n= form_tag incidents_path, method: :get, :class => 'form-search', :id => \"search_field\", :remote => true do\n    %p\n        = text_field_tag :query, params[:query], :class => 'input-xlarge search-query'\n        = submit_tag \"Search\", name: nil, class: 'btn'\n%table.tablesorter{:class => \"table table-striped\"}\n    %thead\n        %tr\n            %th ID\n            %th Date\n            %th Summary\n            -   if can?(:update, :incidents) || can?(:destroy, :incidents)\n                %th Actions\n    %tbody\n        = render @incidents\n    = paginate @incidents, theme: \"bootstrap\", remote: true\n\n\nif can? :create, :incidents\n    = link_to \"New\", new_incident_path, :class => 'btn btn-primary'\n```\n\n``` ruby\n/app/controllers/incidents_controller.rb\nclass IncidentsController < ApplicationController\n  def index\n    per_page = params[:per_page] || 10\n    if params[:query].present?\n      @incidents = Incident.accessible_by(current_ability).search(params)\n    else\n      @incidents = Incident.accessible_by(current_ability).order('date DESC').page(params[:page]).per(per_page)\n    end\n  end\nend\n```\nI would like to search the report Factors too but I'm trying to keep it simple at the moment.\n``` ruby\nrails console\nr1 = Report.find(1)\n  => ...\nr1.incident_id\n  => 1\nr1.summary\n  => \"Joe Bloggs had a problem\"\nr1.incident.summary\n  => \"a problem occured\"\nr1.status\n  => \"private\"\nr2 = Report.find(1)\nr2.incident_id\n  => 1\nr2.summary\n  => \"something happened to a person\"\nr2.status\n  => \"public\"\n```\nI can search for \"problem\" or \"reports.summary:problem\" in the incidents view and I get Incident 1 as the result as expected.\nDesired behavior:\nAdmin users searching get a list of all Incidents where the keyword matches the Incident or its associated children\nPublic users searching only see an Incident in the list if their search matches a Incident or associated child which has status = \"public\"\nExamples:\nas an admin user: searches for \"problem\" or \"Joe Bloggs\" return Incident 1\nas a non-admin user: searches for \"problem\" return Incident 1 (because the keyword is in the incident summary)\nas a non-admin user: searches for \"Joe Bloggs\" return no matches (because Report 1's status is \"private\")\nas a non-admin user: searches for \"something\" return Incident 1 (because Report 2's status is \"public\")\n. ",
    "BlackdolphinMedia": "+1 for the core\n. ",
    "gregoriokusowski": "+1 for core\n. ",
    "fmeyer": ":+1: for core\n. ",
    "rafaelsteil": "+1 for core\n. ",
    "cmilfont": "+1 for core\n. ",
    "peleteiro": "+1 for core\n. ",
    "andreazevedo": "+1 for core\n. ",
    "saulius": "Please merge this in it's really annoying. Especially if one uses tire-contrib which logs the query and deletes payload from options making your real query a nonsense.\n. ",
    "ctbarbour": "Thanks. I just don't know how to read.\n. ",
    "hajder": "@erickt done.\n. Yes, order of results is guaranteed.\nI guess it would be nice if you read code before commenting on pull requests.\n. I wrote an additional test to check order of results returned, however it's not an \"albino elephant\" example (I belive this would be too dependent on elasticsearch internals).\n@karmi I'm not completely sure if I did rebase right, so please take a look :)\n. @rbeene you should simply put each of your conditions in boolean query must block. Is that solution not working for you? (and why?)\n. ",
    "nbudin": "Interesting.  I hadn't been aware that a boolean query with all should clauses and minimum_number_should_match: 1 would give the same result as a dis_max query; I had assumed the same thing you had at the bottom of that comment: that boolean would sum the scores.\nI do think dis_max has the advantage of being both more specific and simpler.  It's also less likely to change in future ElasticSearch updates - not that boolean is necessarily likely to change, but dis_max should always mean dismax.\n. Ah, ok!  I had searched for dis_max in the issue tracker to see if anyone else had done this, but apparently I should have searched for \"dis max\" instead. :)\nNo worries in any case, this didn't take me long to do and looks like the other pull request has had a lot more thought put into it than mine.\n. ",
    "lgs": "... any example code for that  ( handle the indexing yourself )?\n. @karmi very enlightening, thanks for clarifying \n. @karmi \nhey, did you see this one https://github.com/EvilFaeton/tire_async_index ? What do you think about ?\n. WTF, the rake environment tire:import was on the wrong model ( my fault sorry ), now it's ok (updated the gist). \nAnyway, now  i get a 500 Internal Server Error back when I do the same matching search.  All what heroku logs,  is the following :\n\n2012-06-03T21:52:41+00:00 heroku[router]: GET gitwatcher.com/categories dyno=web.1 queue=0 wait=0ms service=10751ms status=200 bytes=61424\n2012-06-03T21:52:41+00:00 heroku[nginx]: 93.34.112.205 - - [03/Jun/2012:21:52:41 +0000] \"GET /categories HTTP/1.1\" 200 12238 \"http://gitwatcher.com/\" \"Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:12.0) Gecko/20100101 Firefox/12.0\" gitwatcher.com\n2012-06-03T21:52:41+00:00 heroku[router]: GET gitwatcher.com/assets/application-4083955e163027c3e232b6edc9cf4dd1.js dyno=web.1 queue=0 wait=0ms service=166ms status=200 bytes=204559\n2012-06-03T21:52:41+00:00 heroku[router]: GET gitwatcher.com/assets/application-02cfedf8524ef0a3da8856f66a3c61df.css dyno=web.1 queue=0 wait=0ms service=152ms status=200 bytes=178764\n2012-06-03T21:52:41+00:00 heroku[nginx]: 93.34.112.205 - - [03/Jun/2012:21:52:41 +0000] \"GET /assets/application-02cfedf8524ef0a3da8856f66a3c61df.css HTTP/1.1\" 200 21013 \"http://gitwatcher.com/categories\" \"Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:12.0) Gecko/20100101 Firefox/12.0\" gitwatcher.com\n2012-06-03T21:52:41+00:00 heroku[nginx]: 93.34.112.205 - - [03/Jun/2012:21:52:41 +0000] \"GET /assets/application-4083955e163027c3e232b6edc9cf4dd1.js HTTP/1.1\" 200 64853 \"http://gitwatcher.com/categories\" \"Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:12.0) Gecko/20100101 Firefox/12.0\" gitwatcher.com\n2012-06-03T21:52:46+00:00 app[web.1]: MONGODB (235ms) heroku_app1707530['users'].find({:_id=>BSON::ObjectId('4f82c1127c0c220001000001')}).limit(-1).sort([[:_id, :asc]])\n2012-06-03T21:52:46+00:00 heroku[router]: GET gitwatcher.com/categories?utf8=%E2%9C%93&query=Asynchronous+Web+Frameworks dyno=web.1 queue=0 wait=0ms service=358ms status=500 bytes=728\n2012-06-03T21:52:46+00:00 heroku[nginx]: 93.34.112.205 - - [03/Jun/2012:21:52:46 +0000] \"GET /categories?utf8=%E2%9C%93&query=Asynchronous+Web+Frameworks HTTP/1.1\" 500 728 \"http://gitwatcher.com/categories\" \"Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:12.0) Gecko/20100101 Firefox/12.0\" gitwatcher.com\n\n. Ok, thanks for advising about the index URL, I already regenerated.  I also update tire (~> 0.4.2) but without any success. On my local PC it works, on Heroku fails silently. Is there a better way of logging elastic/bonsai/heroku ? \n. Setting up logger $stdout, :level => 'debug' into Tire.configure finally heroku can log something while it's failing search :\n\n2012-06-04T21:36:26+00:00 heroku[router]: GET gitwatcher.com/categories?utf8=%E2%9C%93&query=Asynchronous+Web+Frameworks dyno=web.1 queue=0 wait=0ms service=32ms status=500 bytes=728\n2012-06-04T21:36:26+00:00 heroku[nginx]: 93.34.212.216 - - [04/Jun/2012:21:36:26 +0000] \"GET /categories?utf8=%E2%9C%93&query=Asynchronous+Web+Frameworks HTTP/1.1\" 500 728 \"http://gitwatcher.com/categories\" \"Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:12.0) Gecko/20100101 Firefox/12.0\" gitwatcher.com\n2012-06-04T21:36:26+00:00 app[web.1]: # 2012-06-04 21:36:26:387 [_search] ([\"123456789\"])\n2012-06-04T21:36:26+00:00 app[web.1]: #\n2012-06-04T21:36:26+00:00 app[web.1]: curl -X GET \"http://index.bonsai.io/123456789/category/_search?load=true&pretty=true\" -d '{\"query\":{\"query_string\":{\"query\":\"Asynchronous Web Frameworks\"}}}'\n2012-06-04T21:36:26+00:00 app[web.1]:\n2012-06-04T21:36:26+00:00 app[web.1]: # 2012-06-04 21:36:26:387 [200] (1 msec)\n2012-06-04T21:36:26+00:00 app[web.1]:\n\nThen running curl, there something strange:  the retourning object, seems to be duplicated ( the search was on \"Ruby Web Frameworks\" category, which appear two times with different _id ) :\n\nlsoave@ubuntu:~/rails/github/gitwatcher$ curl -X GET \"http://index.bonsai.io/123456789/category/_search?load=true&pretty=true\" -d '{\"query\":{\"query_string\":{\"query\":\"Ruby Web Frameworks\"}}}'\n{\n  \"took\" : 2,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 2,\n    \"max_score\" : 0.78854096,\n    \"hits\" : [ {\n      \"_index\" : \"123456789-xxxxxx\",\n      \"_type\" : \"category\",\n      \"_id\" : \"LAm-hNglS5mbrMzDlVVHCQ\",\n      \"_score\" : 0.78854096, \"_source\" : {\"name\": \"Ruby Web Frameworks\"}\n    }, {\n      \"_index\" : \"123456789-xxxxxx\",\n      \"_type\" : \"category\",\n      \"_id\" : \"h8Gm7rPbRGihbn0R0Gj9lg\",\n      \"_score\" : 0.78854096, \"_source\" : {\"name\": \"Ruby Web Frameworks\"}\n    } ]\n  }\nlsoave@ubuntu:~/rails/github/gitwatcher$\n\nmoreover I get a Progstr Logger error alert via Heroku add-on which say\n\nBSON::InvalidObjectId (illegal ObjectId format: LAm-hNglS5mbrMzDlVVHCQ):\n app/controllers/categories_controller.rb:25:in `index'\n\nand the offending line is the following :\n\n 24     @categories.each do |category|\n 25             @competitors[\"#{category.path}\"] = Category.find_competitors_by_tags(category.tags_array)\n 26     end\n\n. Re-indexing clomplete succesfully ( see https://gist.github.com/2871366 ) but  I get the same error as before.\n. By what I can understand, there's nothing wrong with bonsai.io, infact all the search are logged correctly, both if they match or not :\n\n2012-06-06T18:36:43+00:00 app[web.1]: MONGODB (178ms) heroku_app1707530['users'].find({:_id=>BSON::ObjectId('4f82c1127c0c220001000001')}).limit(-1).sort([[:_id, :asc]])\n2012-06-06T18:36:43+00:00 app[web.1]: # 2012-06-06 18:36:42:960 [_search] ([\"f-a-k-e-i-n-d-e-x\"])\n2012-06-06T18:36:43+00:00 app[web.1]: #\n2012-06-06T18:36:43+00:00 app[web.1]: curl -X GET \"http://index.bonsai.io/f-a-k-e-i-n-d-e-x/category/_search?load=true&pretty=true\" -d '{\"query\":{\"query_string\":{\"query\":\"Asynchronous Web Frameworks\"}}}'\n2012-06-06T18:36:43+00:00 app[web.1]: \n2012-06-06T18:36:43+00:00 app[web.1]: # 2012-06-06 18:36:42:961 [200] (0 msec)\n2012-06-06T18:36:43+00:00 app[web.1]: #\n2012-06-06T18:36:43+00:00 app[web.1]: # {\"took\":0,\"timed_out\":false,\"_shards\":{\"total\":1,\"successful\":1,\"failed\":0},\"hits\":{\"total\":2,\"max_score\":0.26039517,\"hits\":[{\"_index\":\"f-a-k-e-i-n-d-e-x-111222\",\"_type\":\"category\",\"_id\":\"LAm-hNglS5mbrMzDlVVHCQ\",\"_score\":0.26039517,\"_source\":{\"name\":\"Ruby Web Frameworks\"}},{\"_index\":\"sb0cqngrfo2rp1hvw0kl-1338766652\",\"_type\":\"category\",\"_id\":\"h8Gm7rPbRGihbn0R0Gj9lg\",\"_score\":0.26039517,\"_source\":{\"name\":\"Ruby Web Frameworks\"}}]}}\n2012-06-06T18:36:43+00:00 app[web.1]: \n2012-06-06T18:40:08+00:00 heroku[router]: GET gitwatcher.herokuapp.com/ dyno=web.1 queue=0 wait=0ms service=8ms status=301 bytes=87\n2012-06-06T18:40:08+00:00 heroku[router]: GET gitwatcher.com/ dyno=web.1 queue=0 wait=0ms service=13ms status=200 bytes=8309\n2012-06-06T18:40:08+00:00 heroku[nginx]: 184.73.150.97 - - [06/Jun/2012:18:40:08 +0000] \"GET / HTTP/1.1\" 200 3284 \"-\" \"Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/533.3 (KHTML, like Gecko) stillalive-webkit-bot/0.0.17 Safari/533.3\" gitwatcher.com\n\n... while something goes wrong during the controller call, which produce a BSON::InvalidObjectId (illegal ObjectId) format error:\n\nBSON::InvalidObjectId (illegal ObjectId format: LAm-hNglS5mbrMzDlVVHCQ):\n  app/controllers/categories_controller.rb:25:in `index'\n\nNow please note that my controller is never bitten, infact I put a lot of Rails.logger.info to debug the issue, but they are never displaied on heroku logs --tail. Here it is a snippet of the controller to show what I mean:\n\n      @categories = Category.search(params[:query], load: true)\n      Rails.logger.info \"\\r\\n\" + \"#{Time.now} \" + \"@categories = Category.search(params[:query], load: true)\" + \"\\r\\n\"\n      Rails.logger.info \"\\r\\n\" + \"#{Time.now} \" + \"@categories #{@categories}\" + \"\\r\\n\"\n      Rails.logger.info \"\\r\\n\" + \"#{Time.now} \" + \"@categories.class #{@categories.class}\" + \"\\r\\n\"\n      #Rails.logger.info \"\\r\\n\" + \"#{Time.now} \" + \"@categories.results #{@categories.results}\" + \"\\r\\n\"\n      Rails.logger.info \"\\r\\n\" + \"#{Time.now} \" + \"@categories.inspect #{@categories.inspect}\" + \"\\r\\n\"\n\n      #@categories.results.each do |category|\n      @categories.each do |category|\n        Rails.logger.info \"\\r\\n\" + \"#{Time.now} \" + \"Start @categories.each\" + \"\\r\\n\"\n        #Rails.logger.info \"\\r\\n\" + \"#{Time.now} \" + \"Start @categories.results.each (if params[:query])\" + \"\\r\\n\"\n        Rails.logger.info \"\\r\\n\" + \"#{Time.now} \" + \"category #{category}\" + \"\\r\\n\"\n        Rails.logger.info \"\\r\\n\" + \"#{Time.now} \" + \"category.name #{category.name}\" + \"\\r\\n\"\n        Rails.logger.info \"\\r\\n\" + \"#{Time.now} \" + \"category.path = #{category.path}\" + \"\\r\\n\"\n        Rails.logger.info \"\\r\\n\" + \"#{Time.now} \" + \"category.tags_array #{category.tags_array}\" + \"\\r\\n\"\n        @competitors[\"#{category.path}\"] = Category.find_competitors_by_tags(category.tags_array)\n        Rails.logger.info \"\\r\\n\" + \"#{Time.now} \" + \"@competitors = #{@competitors}\" + \"\\r\\n\"\n        Rails.logger.info \"\\r\\n\" + \"#{Time.now} \" + \"@competitors.class = #{@competitors.class}\" + \"\\r\\n\"\n        Rails.logger.info \"\\r\\n\" + \"#{Time.now} \" + \"@competitors[#{category.path}] = #{@competitors[\"#{category.path}\"]}\" + \"\\r\\n\"\n        Rails.logger.info \"\\r\\n\" + \"#{Time.now} \" + \"End @categories.results.each (if params[:query])\" + \"\\r\\n\"\n      end\n\nPlease don't forget that locally on my linux machine, this code works fine. May be is something related to mongodb version of MongoLab/Heroku add-on provider ? \nWhat do you say ?\n\n> db.version()\n2.0.4\n> \n\n\nlsoave@ubuntu:~/rails/github/gitwatcher$ egrep \"bson|mongo\" Gemfile.lock\n  remote: https://github.com/mongoid/mongoid.git\n    mongoid (3.0.0)\n      mongo (~> 1.3)\n    bson (1.6.1)\n    bson_ext (1.6.1)\n      bson (~> 1.6.1)\n    mongo (1.6.1)\n      bson (~> 1.6.1)\n    mongoid_taggable_with_context (0.8.1)\n      mongoid (>= 2.0.0)\n      mongoid (>= 2.4.3)\n  bson\n  bson_ext\n  mongoid!\n  mongoid_taggable_with_context (= 0.8.1)\nlsoave@ubuntu:~/rails/github/gitwatcher$ \n\n. Thanks  karmi, \nI already had to_indexed_json in my Category MongoID method like following:\n\n    ...\n      def to_indexed_json\n        self.as_json\n      end\n   ...\n\nabout logger, ... at the end of the day I currently have logger setted like following:\nconfig/initializers/bonsai.rb\n\nTire.configure { logger $stdout, :level => 'debug' }\n\nif ENV['BONSAI_INDEX_URL']\n  Tire.configure do\n    url \"http://index.bonsai.io\"\n  end\n  BONSAI_INDEX_NAME = ENV['BONSAI_INDEX_URL'][/[^\\/]+$/]\nelse\n  app_name = Rails.application.class.parent_name.underscore.dasherize\n  app_env = Rails.env\n  BONSAI_INDEX_NAME = \"#{app_name}-#{app_env}\"\nend\n\n. OK, now is better :\n    \n    DEFINETLY WORKS BY:\n      1)  dropping out `to_indexed_json` from Category MongoID model ( at the opposit of README indication )\n      2)  removing and re-adding Bonsai.io Heroku add-on\n      3)  reindexing ElasticSearch by running `heroku run:detached rake environment tire:import CLASS='Category'`\n\n... infact, if you just run rake environment tire:import CLASS='Category' FORCE=true it doesn't work on Heroku/Bonsai.io, becuse of Bonsai authorized operation :\n\n  2012-06-11T20:39:30+00:00 app[run.1]: curl -X DELETE http://index.bonsai.io/my-fake-index-11223344\n  2012-06-11T20:39:30+00:00 app[run.1]: # 2012-06-11 20:39:30:749 [401]\n  2012-06-11T20:39:30+00:00 app[run.1]: #\n  2012-06-11T20:39:30+00:00 app[run.1]: # \"{\\\"error\\\": \\\"Not authorized: Some endpoints are admin-only, ask support@onemorecloud.com.\\\"}\\n\"\n\nthan if you want to reset/reindex your eventually dirty data, the only way is dropping the Bonsai add-on and recreate:\n\n  heroku addons:remove bonsai:test\n  heroku addons:add bonsai:test\n\nThat saied, I really had a dirty data indicated by :\n\n  BSON::InvalidObjectId (illegal ObjectId format: LAm-hNglS5mbrMzDlVVHCQ):\n  app/controllers/categories_controller.rb:25:in `index'\n\nIf you'll need the full gem stack to compare with other cases, here it is: https://gist.github.com/2912881\n. Could you please egrep \"bson|mongo\" Gemfile.lock on your local machine and > db.version() too with mongo shell ?\n. @benevolentmadman \nyou're right sorry, I didn't look at your class UserProfile < ActiveRecord::Base model ... \n. What do you get running heroku console ?\nheroku run rake environment tire:import CLASS='YOUR-MODEL-NAME' \n. Please, notice you should open a new issue, because this one is closed since a wile and does't grant sufficent visibility to your problem. Anyway, you can have a check before : \n-   you should have config/initializers/bonsai.rb like the following :\n\nTire.configure { logger $stdout, :level => 'debug' }\nif ENV['BONSAI_INDEX_URL']\n  Tire.configure do\n    url \"http://index.bonsai.io\"\n  end\n  BONSAI_INDEX_NAME = ENV['BONSAI_INDEX_URL'][/[^\\/]+$/]\nelse\n  app_name = Rails.application.class.parent_name.underscore.dasherize\n  app_env = Rails.env\n  BONSAI_INDEX_NAME = \"#{app_name}-#{app_env}\"\nend\n\n\nin your University model you should have this line :\n\n \n    index_name BONSAI_INDEX_NAME\n\nIf you already set/config like the previous, you can try the following 3 steps :\n\n      1)  drop out `to_indexed_json` from your University model ( at the opposit of README indication )\n      2)  remove & add back Bonsai.io/Heroku add-on ( this will remove your index on Heroku, be careful ! )\n      3)  reindex ElasticSearch by running `heroku run:detached rake environment tire:import CLASS='University'`\n           ( Heroku will bill this extra \"run\" process so know what you're doing )\n\nIf that still doesn't work, open a new issue with more details on your stack and configs.\n. ",
    "matsko": "So if you were to replicate the default url, then syntax ends up being:\nruby\nTire.configure do\n  url \"http://0.0.0.0:9200\"\nend\n. ",
    "romanbsd": "It's solved in #334\n. @timuckun there's nothing wrong with this approach, but it's unrelated to the multi search functionality. Multi search makes it possible to send different queries in one HTTP request. They can be to the same index or to different indeces.\n. Maybe you can merge this meanwhile, as otherwise the 0.4.x tire is unusable with graylog2-web-interface. Unless, of course, you are going over the codebase in the near future ;)\n. Added a test.\n. bump\n. I added a configuration example\n. No, I didn't experience something like this. On the other hand, our tire has some more modifications (though they are not related).\nCan you sniff the trafic with tcpdump or similar?\nP.S. We're using faraday 0.8.0\n. @brandonmeeteor I'm calling the run_request itself, https://github.com/technoweenie/faraday/blob/master/lib/faraday/connection.rb#L386 , so the body should be present.\n. I verified here that the following works as expected:\nruby\nrequire 'faraday'\nc = Faraday.new\nc.run_request(:get, 'http://localhost:3333/', '{foo:\"foo\"}', {})\nI also tried:\nruby\nTire.search {|s| s.sort { by :date }; s.query {|q| q.term 'bar', 'baz'} }.perform\nand I'm seeing:\n\nGET /_search HTTP/1.1\nAccept: */*\nUser-Agent: Ruby\nConnection: close\nHost: localhost:9200\nContent-Length: 57\nContent-Type: application/x-www-form-urlencoded\n\n{\"query\":{\"term\":{\"bar\":{\"term\":\"baz\"}}},\"sort\":[\"date\"]}\n\nThat makes me think, that the problem is with the faraday adapter. What Faraday adapter are you using? We're using net_http_persistent. Other adapters, such as typhoeus or patron might not send the body.\n. Any problem with this pull request?\n. You can accomplish it with a following monkey patch:\n``` ruby\nMonkey patch RestClient so it will always provide authentication\nrequire 'restclient'\nmodule RestClient\n  class Request\n    def execute_with_auth(&block)\n      @user ||= 'someuser'\n      @password ||= 'password'\n      execute_without_auth(&block)\n    end\n    alias_method_chain :execute, :auth\n  end\nend\n```\n. Better yet, use Faraday (https://github.com/karmi/tire/pull/337) and configure the auth.\n. What about Fixnum?\n. This is just an example. The FaradayMiddleware is a separate beast. I just copied our production setup verbatim as an example of what's possible to do with this adapter.\nhttps://github.com/pengwynn/faraday_middleware\n. ",
    "vpereira": "Thanks for your answer. To be honest. I did something like\nhttps://gist.github.com/2342609\nand i'm still not able to search by author. I'm doing a OR between two fields and filtering (almost like a scope) using filter. Is that the best way to do it? I saw in your examples. And I think you would add this \"scope\" in a must block. Right? I don't know if I read right, but at elastic search website it says that filter is faster then query.. is that right? \n. thank you for your help and the amazing gem. I owe you a beer :-)\n. done!\n. ",
    "kyledecot": "Yes. This has been fixed in the most recent version of Carrierwave. Sorry for the delayed response.\n. ",
    "veesahni": "@thoughtpunch simple use cases can be handled like this:\nTire::Configuration.client.delete \"#{Tire::Configuration.url}/twitter/tweets/_query?q=a:23\"\n. ",
    "Papipo": "In order to setup my tests, I want to delete all documents across al indexes, I tried using this:\n```\nindex = Tire::Index.new('_all')\nquery = Tire::Search::Search.new do\n  query { all }\nend\nTire::Configuration.client.delete [index.url, query.to_hash.to_param].join('?')\n```\nBut I can't search anymore. Does this delete indexes somehow?\n. Well, now I am using something simpler:\nTire::Configuration.client.delete \"#{Tire::Configuration.url}/_all\"\nMy problem was the async nature of the setup, so now I am calling refresh() after save and destroy.\n. ",
    "Will-Sommers": "@karmi any news on this now? \n. Hello guys. Its been four months and I'm curious where this issue stands, specifically re: multi_search\n. Karmi, you beat me to it. I've edited the question. Looking into it now.\n. @karmi is there a reason why there's no pagination in the multi_search integration test? I've taken a look at the integration test but it just confirms that the second example works but doesn't cast any light on the first. I've read through almost all of the gem's source code and integration tests but haven't found much help.\nhttps://github.com/karmi/tire/blob/master/test/integration/multi_search_test.rb\n. Sorry for the multiple posts at once... just delving into this... \nwill_paginate is accepting the options in the first case and building the pagination scheme accordingly however the results are still being shown without :page or :per_page being considered. \n. Could you give me a starting place to think on if I wanted to add pagination or retrieve records past 10?\nThanks so much for replying so fast. \n. Sorry man. Thanks for the help. We ended up going with your solution. \n. Karmi, you're right. I hadn't restarted the console but had changed ES URL. \n. ",
    "jurgens": "@karmi could you please explain why index gets deleted on \"manually invoking the \"Delete by Query\" API\"? (see comment above - https://github.com/karmi/tire/issues/309#issuecomment-9699113)\n. @ches thank you\n. Thank you @karmi that was helpful!\n. ",
    "command3r": "There are a couple of fixes to make this work, mainly the curl logging on failure and the json for query on ?source=. Please review https://github.com/karmi/retire/pull/964\n. ",
    "kennym": "Thanks for pointing that out, and sorry for the late response.\nI would like an index of owner, type article and id 1. Is that possible?\n. Thanks, this works!\n. ",
    "palodelincak": "Hi, I'm sorry - problem was on my side. I tried to reinstall Mongo (my system/installation was probably broken) and everything is working now.\n. And yes, my production database contained same data as development database, but I could not make any query through application in production environment.\nThanks for your help!\n. ",
    "johnpaulashenfelter": "I went ahead and made the changes -- took only a few minutes\n. I tend to agree as far as the changes -- squashing the deprecations in the tests is my only real goal.\nDo you think it's worthwhile to require MultiJson < 1.3? Not sure if there's anything in 1.3+ that makes the deprecations worth the upgrade -- you're probably in a better position for knowing that. That's what I did locally to get rid of the hassle Obviously that can be done on project by project basis in a gemfile was thinking maybe it makes sense to match this version of tire to the multijson it expects.\nThoughts?\n. I agree -- don't change req'd lib version until next release.\n. Agreed\n. ",
    "rubiii": "the changelog in the version file is great. maybe a link to the latest blob of that file in the readme would help others to find it.\n. ",
    "hale": "@karmi Do you have a rough timeline for when the defaults will use types instead of indices for the different models?  I understand this might not be a priority for you right now, I was just wondering if it was 'within the next month' or 'within the next 6 months'.\n. ",
    "mculp": "I'm also here through Bonsai research on multi-index vs. multi-type. Any word?\n. ",
    "dennisreimann": "Thanks for your feedback. While working on this patch I noticed that a proper HTTP abstraction would be the right way to go, you are absolutely right. In the meantime, this already helps us to debug connection issues we are facing sometimes.\n. We are experiencing similar errors, that's why I made a pull request to have a better debugging output for connection errors.\n. ",
    "davidhorsak": "It gets connected, but during the boot it throws the mentioned error.\nThanks @dbloete, I tried to use your branch and now I get\n```\nSkipping index creation, cannot connect to ElasticSearch\n(The original exception was: #)\nLoading development environment (Rails 3.2.3)\n...\n```\nI went throught the code and in console tried both ways of checking if the index exists. I used the curl way and the rest client way found in tire / lib / tire / index.rb and both were 200 success.\nIt really stopped having any sense, then I tried putting binding.pry before Tire configuration and the curious thing was that it threw the connection error before Tire even got configured!\n```\n$ bundle exec rails c\nSkipping index creation, cannot connect to ElasticSearch\n(The original exception was: #)\nFrom: /Users/davetsunami/Projects/Artvasion/config/initializers/elasticsearch.rb @ line 1:\n=> 1: binding.pry\n    2: \n    3: Tire.configure do \n    4:   url \"http://192.168.1.138:9200\" #APP_CONFIG[:elasticsearch][\"host\"]\n    5: end\n    6: \n[1] pry(main)> Tire::Configuration.url\n=> \"http://localhost:9200\"\n[2] pry(main)> exit\nLoading development environment (Rails 3.2.3)\n...\n```\n. The example app works without any connection problems.I tried connecting it with the provided ElasticSearch in vendor/ and my remote ElasticSearch server and both worked with Tire perfectly. I'll do further investigations and report back.\n. Got it. There seems to be an interference between Tire and Devise. Tire initializer is called elasticsearch.rb and Devise initializer is called devise.rb. When I placed Tire configuration before Devise configuration, it started OK with no exception messages. When I placed it back after Devise configuration, I got the thousand times mentioned connection error. \nThis is how my Devise config initializer looks:\n``` ruby\nDevise.setup do |config|\n# ==> ORM configuration\n  # Load and configure the ORM. Supports :active_record (default) and\n  # :mongoid (bson_ext recommended) by default. Other ORMs may be\n  # available as additional gems.\n  require 'devise/orm/active_record'\nconfig.mailer_sender = \"no-reply@artvasion.com\"\n  config.case_insensitive_keys = [ :email ]\n  config.strip_whitespace_keys = [ :email ]\n  config.http_authenticatable_on_xhr = false\n  config.paranoid = true\n  config.stretches = APP_CONFIG[:stretches]\n  config.pepper = APP_CONFIG[:pepper]\n  config.remember_for = 2.weeks\n  config.extend_remember_period = true\n  config.use_salt_as_remember_token = true\n  config.rememberable_options = {:secure => true}\n  config.password_length = User::PASSWORD_MIN_LENGTH..User::PASSWORD_MAX_LENGTH\n  config.lock_strategy = :failed_attempts\n  config.unlock_strategy = :none\n  config.maximum_attempts = 1/0.0\n  config.reset_password_within = 1.day\n  config.sign_out_all_scopes = true\n  config.sign_out_via = :delete\nend\n```\nThere's definitely something in that require 'devise/orm/active_record'\n. ",
    "xinming": "We are having the exact same problem, and despite how little sense it makes, the fix mentioned by @DaveTsunami works for us.. Although we have mongoid instead of active_record in that line. \n. ",
    "verdi327": "I am having the same issue, except I am not using Devise.  I hava no auth system in my app and I can imagine any other initializer I have that would be interfering.  My app is launched on Heroku at http://dynamiteurbanite.herokuapp.com/ and I am unable to get ES working consistently.  I am using the Bonsai add on provided by Heroku.  I run thru the practice examples provided on the add on page for Bonsai and everything responds how it should.  For example I can run this command \ncurl -XPOST http://index.bonsai.io/q3s8a05750388n7sa9wo/article -d '{\"title\": \"Hello, Bonsai.\"}'\nand I receive a JSON response telling me everything is working ok\n{\n  \"ok\":       true,\n  \"_index\":   \"q3s8a05750388n7sa9wo\",\n  \"_type\":    \"article\",\n  \"_id\":      \"7yYC-inLT7-eFkAdCAUv2A\",\n  \"_version\": 1\n}\nBut, when I go to enter a search param the app breaks.  Checking my heroku logs I see that I get this error everytime...\nStarted GET \"/cities?utf8=%E2%9C%93&query=georgia\" for 69.250.146.115 at 2012-07-04 01:12:53 +0000\n2012-07-04T01:12:53+00:00 app[web.1]: Processing by CitiesController#index as HTML\n2012-07-04T01:12:53+00:00 app[web.1]:   Parameters: {\"utf8\"=>\"\u2713\", \"query\"=>\"georgia\"}\n2012-07-04T01:12:56+00:00 app[web.1]: Completed 500 Internal Server Error in 3001ms\n2012-07-04T01:12:56+00:00 app[web.1]: Errno::ECONNREFUSED (Connection refused - connect(2)):\nThe weirdest part is that it was working at one point - I don't know how it fixed itself - but then when I went to use the app later that night (no code changes) I am back receiving this same error.\nSame guidance would be awesome!!!!\n. ",
    "wflanagan": "Same here, no devise.  But, I have this error.  Anyone have any thoughts on how to fix? \n. Yes, I tried it with that.  The port helps (9200). \nI saw the bugs, so I swung it off of Bonsai and onto my own EC2 instance. I'm not having that problem anymore.. but I am having another.  I have posted a question on it (with configs) to http://stackoverflow.com/questions/12041831/elasticsearch-tire-keywords-right-way-to-match-or-for-a-keyword-list \nThanks for the feedback on this issue.\n. ",
    "kristopher": "This is pretty old but I just ran into it so I thought I would leave an explanation for future reference. I'm not using devise but I believe this will happen if any initializer causes a model to load that includes a tire index. The model loads which causes tire to load the mapping and in doing so it tries to create the index but at the time the ENV['ELASTICSEARCH_URL'] hasn't been set yet by the tire/elasticsearch initializer so tire defaults to localhost and bam connection refused.\n. Documentation sounds like the right approach to me. The solution is simple once you know what the issue is.\n. Printing the URL sounds like a fine idea more info the better.\nIn addition you could get away from defaulting to localhost and throw an exception if the url is not set in the env or through the configuration interface. Seems like that could give the user a better idea of what the issue is and point them in the right direction when they go to debug. I'm not sure it's worth going for configuration over convention in this\nplace but it's an idea. \n. I think \"breaking the experience\" and \"shoot them in the foot\" is a bit over dramatic but I certainly understand where you are coming from. Just throwing an idea out there.\n. Confusing for sure, even if the wiki article didn't exist I still would have done the same thing, because who orders coordinates that way :)\nI'll edit it as soon as I get some time.\nPS - Great library, with it it was simple and quick to get search up and running.\n. Ya that page.\n. Updated the wiki page.  I just changed lat_lon to return a string and added a note about the ordering differences and a link to the documentation.\n. ",
    "iwarshak": "In the interest of completeness, moving the bonsai config into environment.rb did the trick\n```\nENV['ELASTICSEARCH_URL'] = ENV['BONSAI_URL'] \nInitialize the rails application\nApp::Application.initialize!\n```\n. @karmi The heroku/bonsai docs say to set the ELASTICSEARCH_URL variable in an initializer. Perhaps because the BONSAI_URL could be changed by them during a migration or something\n. Ok, so I guess you can't just chain the commands?\n. ",
    "danthompson": "I too have this issue, and request some direction.\n. ",
    "Caged": "I'm running 0.4.2 and haven't been able to get this to work yet, albeit it's complaining about a missing index this time.\n``` ruby\nTire.configure do\n    url BONSAI_URL\n    logger STDERR\n  end\narticles = [\n    { :id => '1', :type => 'article', :title => 'one',   :tags => ['ruby'],           :published_on => '2011-01-01' },\n    { :id => '2', :type => 'article', :title => 'two',   :tags => ['ruby', 'python'], :published_on => '2011-01-02' },\n    { :id => '3', :type => 'article', :title => 'three', :tags => ['java'],           :published_on => '2011-01-02' },\n    { :id => '4', :type => 'article', :title => 'four',  :tags => ['ruby', 'php'],    :published_on => '2011-01-03' }\n  ]\nTire.index 'articles' do\n    delete\n    create\nimport articles\n\nend\n```\n```\n\ncurl -X POST BONSAI_URL/articles -d '{}'\n2012-05-06 09:40:55:975 [201]\n[ERROR] 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}, retrying (1)...\n[ERROR] 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}, retrying (2)...\n[ERROR] 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}, retrying (3)...\n[ERROR] 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}, retrying (4)...\n[ERROR] 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}, retrying (5)...\n[ERROR] Too many exceptions occured, giving up. The HTTP response was: 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}\n2012-05-06 09:40:57:727 [BULK] (\"articles\")\n\ncurl -X POST BONSAI_URL/articles/_bulk -d '{... data omitted ...}'\n2012-05-06 09:40:57:727 [201]\n```\n. Yeah, it has heroku's generated index.\nSent from my iPhone\nOn May 6, 2012, at 12:02 PM, Nick Zadroznyreply@reply.github.com wrote:\n\nI'll try to get some more thorough manual testing in this week.  \nJustin, can you confirm that the full BONSAI_URL including the index name is in that curl command?\nMaybe the bulk import method is setting _index incorrectly? Should be easy to verify. \n\nNick Zadrozny\nOn Sunday, May 6, 2012 at 9:47, Justin Palmer wrote:\n\nI'm running 0.4.2 and haven't been able to get this to work yet, albeit it's complaining about a missing index this time.\n``` ruby\nTire.configure do\nurl BONSAI_URL\nlogger STDERR\nend\narticles = [\n{ :id => '1', :type => 'article', :title => 'one', :tags => ['ruby'], :published_on => '2011-01-01' },\n{ :id => '2', :type => 'article', :title => 'two', :tags => ['ruby', 'python'], :published_on => '2011-01-02' },\n{ :id => '3', :type => 'article', :title => 'three', :tags => ['java'], :published_on => '2011-01-02' },\n{ :id => '4', :type => 'article', :title => 'four', :tags => ['ruby', 'php'], :published_on => '2011-01-03' }\n]\nTire.index 'articles' do\ndelete\ncreate\nimport articles\nend\n```\n```\n\ncurl -X POST BONSAI_URL/articles -d '{}'\n2012-05-06 09:40:55:975 [201]\n[ERROR] 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}, retrying (1)...\n[ERROR] 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}, retrying (2)...\n[ERROR] 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}, retrying (3)...\n[ERROR] 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}, retrying (4)...\n[ERROR] 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}, retrying (5)...\n[ERROR] Too many exceptions occured, giving up. The HTTP response was: 404 > {\"error\":\"IndexMissingException[[articles] missing]\",\"status\":404}\n2012-05-06 09:40:57:727 [BULK] (\"articles\")\n\ncurl -X POST BONSAI_URL/articles/_bulk -d '{... data omitted ...}'\n2012-05-06 09:40:57:727 [201]\n```\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/327#issuecomment-5537116\n\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/327#issuecomment-5539715\n. @nz - awesome, that worked.   I had assumed the URL was structured /BONSAI_KEY/INDEX_NAME, bu the key is the index name.   Thanks for the thorough explanation.  Feel free to close this. \n. \n",
    "toddwschneider": "@karmi I just gave it a try with 0.4.2 and it works!  Thanks for the help!\n. ",
    "jch": "In case anyone is still running into this issue, if you upgrade to a newer version of Bonsai (> v530), they scope accounts by subdomain now instead of path. This works with tire's assumption about url paths. https://devcenter.heroku.com/articles/bonsai\n. ",
    "mando": "Yup - I apparently can't read.\nPlease don't tell anyone ;).\n. ",
    "odino": "+1, graylog web interface totally not working\n. ",
    "lumpidu": "If you say Title, which one do you mean: :title or :ngram_title ?\nYour :title is set to the snowball analyzer which removes stopwords per default, whereas your :ngram_title uses your own defined analyzer.\n. Ah,\ncame across this post here http://stackoverflow.com/questions/7969739/is-there-a-way-to-count-all-elements-of-an-index-in-elasticsearch-or-tire and could not find anything elsewhere.\nSeems, you can earn yourself another bonus credit on SO in completing the answer ... ;)\nAm 08.05.2012 um 15:36 schrieb Karel Minarik:\n\nIn fact, the search_type=count is supported: https://github.com/karmi/tire/blob/master/test/integration/count_test.rb\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/karmi/tire/issues/338#issuecomment-5574718\n. \n",
    "dpaluy": "@romanbsd +1 Good example. \n. http://stackoverflow.com/questions/10879991/attribute-dependent-dynamic-index-tire-rails-elasticsearch/10929376#10929376\n. ",
    "thedaniel": "After looking at my data that caused the problem, i think it might not be nested hashes, but Items inside arrays, like\n{\n  foo: 'bar',\n  baz: [Item(), Item()],\n}\n. whoops, just saw this - i had notifications turned off for some reason. i will make some time to try to write a failing test this week sometime\n. ",
    "azitabh": "Any update on this?\n. Sorry about that.\n. @karmi sorry for not closing the issue on time... got my answer.\n. ",
    "chendo": "+1 on 2.3 support. I was going to use Tire without using the ActiveModel stuff, but the Gemfile requiring activesupport means I have to use @loe's fork. It should be in another gem or something.\n. ",
    "TwP": "+1 on this. \nTwo gems would be highly recommended. The core DSL for interacting with ElasticSearch should be in the tire gem. A second gem needs to be created - tire-model perhaps - that contains all the ActiveModel functionality.\n@loe thanks for doing all the hard work on this!\n. @loe you should turn this issue into a pull request.\n. How to make Item usable within Rails without ActiveModel? As soon as you do some require 'tire/model', you need it.\nWe are not using Item at all with Tire at the moment; so this question is a bit of a moot point. There are actually better solutions to this problem in the form of intridea/hashie and ahoward/map.\nAll of the tire/model functionality should be in it's own separate gem. This involves splitting out the basic connectivity and request/response framework into a gem, and then build upon that basic framework the more advanced features such as the query DSL and ActiveModel support.\n\nHow to get rid of dependency on ActiveSupport? I hate this dependency as much as any other guy, but the problem is that eg. Hash#to_param is very convenient.\nThe to_param method can be implemented in roughly 20 to 30 lines of code. The same can be said of all the ActiveSupport niceties that Tire might need. So steal what is needed, and leave all the other unneeded things behind.\n\nWhy support integration tests on Ruby 1.8? Need a really convincing argument, there's a slim chance I'd vote for that.\nTire is definitely your project, Karel. The production environment we are working with is Ruby 1.8.7 at the moment. The solution we need today has to support 1.8.7. That is the convincing argument for me.\nLet's flip this argument on its head. Is there any compelling reason to abandon support for 1.8.7? Is there critical functionality that can only be implemented in Ruby 1.9?\nIf Tire is going to split into two gems (a base functionality layer and then advanced DSL and ActiveModel features as a separate gem), then keeping the base functionality compatible with 1.8.7 is not a hardship. The advanced features can be 1.9 only.\n. ",
    "kidpollo": "@loe @karmi has there been any updates in this issue? I would really like to help out. I am currently testing out @loe changes. I have not gotten to deep into the code so far bui it does seem dunctionality can be broken down. When I built https://github.com/kidpollo/tanker I really wanted to make it orm agnostic. Maybe you want to check out some approaches there.\n. Thanks for the response!\nI did look at your strategy. I actually went ahead and implemented a very similar version of it for now. We are going to test out different strategies in the following weeks I will let you know our discoveries. Our ops team really likes these challenges.\nIn the mean time I will try to dig in the code to see how possible is this with the current tire api. My main concern is the ability to provide different mappings for different tenants. I does not seem to be possible without separate indexes.\nThanks for the slides I will check them out for sure.!\n. Say we change index_name 'blah' to tindexed_at :some_method_name where some_method_name is a method on the 'leaser' routes to an index or sets the routing or the filtering required for the tenant. \nOr something similar where you could actually define the aproach you want to take.\n. ",
    "loe": "I have not worked on it, it is up to @karmi to decide how to restructure his project. I think the core/model/etc makes sense.\n. ",
    "vinhboy": "+1 for rails 2.3 support\n. ",
    "vamsi-krishna-E0029": "Waiting for this......\n. That was really a quick reply...Thanks a lot...\nBut, the index doesn't seem to be created. Still getting the same error as stated above. The Analyzer in the settings is causing the problem. Can you please throw some light on this.\n. Sorry for the previous comment, I actually forgot the :analysis => {} in the settings. When I added that everything worked fine. Thanks a lot for the help...\n. account id based search in a multi-tenant application. Sorry if I asked it in the wrong way...\n. The account_id is not included in all field in my actual index. Would that cause any problem? I'm actually getting zero hits.\nindexes :account_id, :include_in_all => false\nMy mapping:\n{\n    \"users\":\n        {\"user\":\n            {\"_routing\":\n                {\"required\":true,\n                 \"path\":\"account_id\"},\n                 \"properties\":\n                     {\n                      \"account_id\":\n                         {\"type\":\"string\",\n                          \"include_in_all\":false\n                         },\n                      \"name\":\n                         {\"type\":\"string\",\n                          \"boost\":10.0\n                         },\n                      \"company_name\":\n                         {\"type\":\"string\"}\n                     }\n            }\n        }\n}\n. Thanks for the suggestion. Yes. I do know that it would be analyzed as ovamsikrishna, gmail.com. I have set the default analyzer with uax_url_email tokenizer and applied the standard analyzer filters to it.\nI'm getting the following error:    \"error\":\"RoutingMissingException[routing is required for [users]/[user]/[1]]\",\"status\":500\nwhen I try to add routing. This is the reason I was getting zero hits I guess. As suggested I have added the routing option in the search query too. Not sure why I'm getting this error.\nThis error occurs when I try to import my users using\nrake environment tire:import CLASS='User'\n. How do I create an alias for automatically created index?\nMy index for user model is created using:\n```\n  include Tire::Model::Search\n  mapping do\n    indexes :name, :boost => 10\n    indexes :account_id\n    indexes :company_name\n  end\ndef to_indexed_json\n    to_json( :only => [:name, :account_id, :company_name], \n       )\n  end\n```\nHow do I access add_alias method in this case?\n. Creating a filtered alias for each account. How different is it from index per each account?\n. I have actually created an alias and filtered using account id (filtered alias). If I apply routing as specified below will that hit the specific shard or all the shards?\nUser model:\n```\n      include Tire::Model::Search\n  Tire.index('users') do\n    create(\n      :mappings => {\n        :user => {\n          :properties => {\n            :name => { :type => :string, :boost => 10 },\n            :account_id => { :type => :integer, :include_in_all => false }\n          }\n        }\n      }\n    )\n\n    add_alias \"index_users_2\", :filter => { :term => { :account_id => 2 } }, :routing => 2\n  end\n\n```\nOr do I have to specify the routing option in the search query also for hitting the specific shard?\nSearch query:\nTire.search ['users'], :load => true do\n          query do\n            filtered do\n                query { string search_string },\n                filter :term, :account_id => 2\n            end\n          end\n        end\nOne more question, this being a multi-tenant app, the account_id is varies. So, how do I create alias which varies as per current account id? I have excluded the callbacks inorder to support rails 2.3 ( as per https://github.com/loe/tire/commit/53d851098c86ba210a6b9d4168e65693941ce51b ). Will index_name work for me in that case?\n. How do I update an index based on it's name? I mean if I want to update an index like users, how can I do something like tire.update_index 'users'. The index name may vary in my case. Is that possible? The index name isn't  dynamically generated one in my case.\n. Thanks for the reply.\nI don't have a separate index for each model. I have a separate index for each account, in which the mappings for each model (user and comments) are specified. In this case if any change has been done for user model or comment model, the index that has been created for the related account has to be updated. Is this possible? Please let me know if yes.\nI guess this is the way I specify the mappings in my case. Correct me if I'm wrong.\nAccount Model:\n```\n  include Tire::Model::Search\nTire.index('account_1') do\n    create(\n      :mappings => {\n        :user => {\n          :properties => {\n            :name => { :type => :string, :boost => 10 },\n            :company_name => { :type => :string, :boost => 5 }\n          }\n        },\n        :comments => {\n          :properties => {\n            :description => { :type => :string, :boost => 5 }\n          }\n        }\n      }\n    )\n  end\n```\nThe index is getting created correctly with both the mappings for account index. But, I don't see a way where I can update the index when any model specified in the mappings are changed.\nWhenever a new user is added or if an user is updated the index created for the corresponding account has to be updated.\n. Oops sorry...Will look into it. Thanks for the reply.\n. Thanks for the reply...\nI agree, the index gets created only when it doesn't exist. I just want to know if there is anyway to stop the auto index creation itself. I want to create it, as and when I need it.\n. Thanks a lot.\n. My user model is not directly available in models directory of rails app folder. It is located in Admin folder inside.\napp->models->admin->user.rb\nso the class name is defined using namespace as Admin::User. In this case how do I provide the mapping definition.\nI'm sorry if the question I asked previously was total wrong.\n. Thanks a lot.\n. Thanks for the reply. Will look into it. Closing the ticket as of now.\n. Added the each_with_hit method in the collection.rb (As the version I'm using is 0.4.3). But, the highlight is always being returned as nil value.\n@search_results.each_with_hit do |result,hit|\n      puts \"$$$$$$ #{hit['_highlight']}\"\nend\nOUTPUT: $$$$$$ \n. Even that didn't work. I checked the hit output.\n    puts \"$$$$$$$ #{hit.inspect}\"\nOUTPUT: The output doesn't contain the highlight itself\n. Has gone through that. Anything can be done if the highlight is available in the results. But, I see only score, type, index, id and source.\nThe version I'm using is 0.4.3. Does that have anything to do with? I integrated the changes from link provided. I don't think any vital change is required apart from adding the each_with_hit in the collection.rb. Am I correct here?\nDo I have to specify anything while mapping for the relevant field which needs to be highlighted?\n. Tried with the latest master. Issue seems to be the same. Unable to get highlight in results.\n. My Mapping example:\n```\ninclude Tire::Model::Search\nTire.index('account_1') do\n    create(\n      :mappings => {\n        :user => {\n          :properties => {\n            :name => { :type => :string, :boost => 10 },\n            :company_name => { :type => :string, :boost => 5 }\n          }\n        },\n        :comments => {\n          :properties => {\n            :description => { :type => :string, :boost => 5 }\n          }\n        }\n      }\n    )\n  end\n```\nIn the above mapping I would like to filter search results with a particular company_name. The company_name can be Null also (This is just an example). As you can see the comments type doesn't have company_name field. So, I have to use exists filter also.\nsearch_key = 'test'\nTire.search [account_1] do\n          query do\n            filtered do\n                query { string search_key }\n                filter :or, { :not => { :exists => { :field => :company_name } } },\n                              { :term => { :company_name => 'test' } }\n            end\n          end\n        end\nThe above filter checks just the existence of company_name field or filters exact company_name if exists. I want to add null_value check along with this if company_name field exists.\nI want to avoid the results where company_name is nil for the user type along with the above filter.\n. I have to get results from comments type when field is not available. And then I have to avoid the results from user type if the company_name is null or if it is not a particular term. The results from comments type should always be available.\nSorry if that was confusing.\nThe main problem was, missing filter and exists with a combi of not, are getting combined results of non-existant and null_value.\n. This is exactly what I get as of now. But, actually what I need is I need to avoid the result which has assigned_to as nil. I mean I need the result set to be [\"1\", \"3\"]\nI have tried different combinations of almost all the filters. But couldn't achieve. I feel this is pretty small but equally confusing. This is driving me crazy since yesterday...\n. Is that possible with any combo of filters...?Please help.\n. Thanks for the reply Karmi...Will look into it.\n. Tire doesn't know about current_user instance. But, that will be available in your application code. So, you have to pass that in the options as a hash to the search query.\nOnce that is passed, you can use the filter.\noptions = {:current_user => current_user}\nTire.search ['index_name'], options do\n<do whatever u want using options[:current_user]>\nend\nAnswered as per my understanding of the question. Hope, this helps.\n. As per this \n```\nYou may have just stopped wondering: what if I have my own settings class method defined? Or what if some other gem defines settings, or some other Tire method, such as update_index? Things will break, right? No, they won't.\nIn fact, all this time you've been using only proxies to the real Tire methods, which live in the tire class and instance methods of your model. Only when not trampling on someone's foot \u2014 which is the majority of cases \u2014, will Tire bring its methods to the namespace of your class.\nSo, instead of writing Article.search, you could write Article.tire.search, and instead of @article.update_index you could write @article.tire.update_index, to be on the safe side. Let's have a look on an example with the mapping method:\n```\nI'm not calling that directly. Am I doing something wrong here? I'm using @item.tire.update_index\n. Thanks a lot... Will look into it. Closing this issue for now. Will add a comment for sure if this works.\n. When ThinkingSphinx is trying to call update_index the tire's update_index is getting called. I'm not aware why actually this is happening. \nFor now, in order to resolve the conflict, I have changed the update_index method name to update_tire_index in the tire/model/search.rb and callbacks.rb. \nWould that impact any other part of code? I mean change of the method name from update_index to update_tire_index.\n. My bad...Can import the account data specifying the index name as \"tire_index\".\n. I have a field with account number indexed in each document.\nUsing delete by query delete all the documents with account number (term filter). Then import documents from the account to the same index (\"tire_index\").\nWould that work?\nWill the Tire#index#reindex method work for the same index? (i.e., reindexing into same index)\n. Then the data will be available only in the new index. The old index doesn't contain the data which is transferred to new one.\nCorrect me if I'm wrong.\n. Using delete by query delete all the documents with account number (term filter). Then import documents from the account to the same index (\"tire_index\").\nWould the above solution work for my issue?\n. 'admin/user.id_user' this actually worked. That was a reindex problem for me. I have changed the mapping but forgot to reindex.\n. I have used Benchmark.bmbm\nand these are the results I get with and without the line. The only difference in code is, just the assignment of result set to a variable.\n(results = @items.results)\nuser       system      total          real\n0.000000   0.000000   0.000000   (  0.000191) - search without results\n0.360000   0.010000   0.370000   (  6.331976) - search with results\n. I got it, You mean that the response from elasticsearch itself is taking so long. Thanks for the info.\nAny idea why this could take so long at elasticsearch?\n. Closing the issue. Please let me know if you have any idea regarding the slow response time at elasticsearch.\n. With the same setup direct curl request (search query) was taking very less response time. But, when I use tire it was taking 3.5seconds. Does tire use anything else in between? I verified it with the took field in search result. Am I doing wrong here? As far as I know the took field represents the time taken by entire search API.\n. It is a multitenant app with an account per tenant. We created 6 indices one for each searchable model. Once an account is created, we create 6 aliases(one for each model) with routing and filter on account id. All the search, update, delete operations are done on these aliases.\nWe actually have a setup which consists of data nodes and non data nodes. The non data nodes are used for http requests. We use HaProxy before the non data nodes. Now we want to add another cluster and add all new accounts from now to the new cluster.\nIs there any ugly way to do the same? How does the tire connect to the elasticsearch cluster?\nTire version: 0.4.3 with some custom tweaks and additional changes to support rails 2.3.x. So, it is difficult to upgrade at this stage.\nElasticsearch version: 0.90.1\n. I'm not sure whether this can be done considering your \"Note\". \nYou should have indexed the document with a routing parameter or atleast you should have created aliases using routing while creating the index itself. \nUntil and unless you index the document with a routing parameter you cannot search it with routing. Its mutual, you index with routing, I give results with routing. You index without routing, I give results without routing.\n. @meejoe Did you try deleting and creating the index again after changing the mapping? If the mapping changes, the index has to be created again for the new changes to take effect.\nMoreover you are using the default dateOptionalTime format itself :).\nPlease check the below links for reference.\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-date-format.html\nhttp://joda-time.sourceforge.net/api-release/org/joda/time/format/ISODateTimeFormat.html#dateOptionalTimeParser\nhttp://joda-time.sourceforge.net/api-release/src-html/org/joda/time/format/ISODateTimeFormat.html\n. ",
    "tyler-smith": "I'm wondering what kind of support there is for doing this.  I'm on a Rails 2.3 app and about to implement Elastic.  Tire by far seems to the best option, and with a small amount of hackery I have gotten it at least somewhat operational in my stack.\nI'm more than happy to look into taking at stab at separating out the active model stuff + perhaps making a 2.3 version if @karmi and others are interested in going this route.\n. Certainly purposefully keeping 2.3 support is probably unnecessary at this point, however it would be nice to have Tire a bit more modular and not require rails at all for many of the more core features.  I understand that would require quite a bit of thought/work, and that it may not, and probably is not, quite the direction you're looking to go.\nLuckily at this point it seems fairly easy to get a lot of the good stuff by removing the ActiveModel related things, so at least for now maintaining a custom fork is probably a decent option.\n. Massive fail.  This wasn't supposed to be here. :(\n. ",
    "tejo": "+1\n. ",
    "neocanable": "+1 \n. ",
    "SLM-Linus": "This is causing great trouble for us. We are inserting some objects in to the database inside a transaction. Somewhere in the transaction an error occurs and we do a rollback on the database. This causes objects to exist only in elasticsearch and not in the main storage (MySQL in our case).\nIt would be great if some means of preventing this could be provided. I am currently hacking our fork of Tire to use where(id: ids) instead of find(ids) in collection.rb but I would love it if any official means of getting around the problem could be released.\n. Yes exactly, I probably should have made that clearer in my post. I want a solution that does not use the hack described but I still use it right now since it works for us...\n. ",
    "jlsherrill": "Changing tire to use where instead of find would just be masking the problem (Tire not handling transaction rollbacks).  Active record provides an after_rollback hook that we'd probably want to use to simulate rolling back the transaction in ES (which would probably just involve reindexing or deleting the object).\nChanging to use where(:id=>ids) would result in the search server still having the stale data, it would just be filtered when you load the AR models automatically.  For instances where you don't need to load the AR models (which is the majority of our queries), the data would still be wrong.\n. ",
    "atlantis": "To get the above monkey patch working I also had to override def __get_results_with_load and compact the results - otherwise a nil value would be returned in the results wherever there was a missing ActiveRecord model\n. @karmi I think the deal is that __get_results_with_load returns the results of the @response['hits']['hits'].map block, and since there's no .compact at the end (which is the only thing I added when I overloaded the function), there will always be one array element per hit returned, regardless of whether it's in records[type] or not.  So when I iterated over search results, if AR failed to find a record then I'd end up with a nil at that position because .detect couldn't find that result.  Thanks for an awesome gem btw!\n. ",
    "konsti": ":+1: This is a really annoying bug if you want to pass raw ElasticSearch queries via the payload.\nAny call to to_curl, to_json are stripping the payload from the options. Even without calling these functions in your code the log is empty afterwards.\n. I have the same problem on ElasticSearch 0.19.11 with:\n- Ruby 1.9.3p327\n- Rails 3.2.9\n- Tire 0.5.1\n- MongoId 3.0.13\n- Moped 1.2.9\n- Origin 1.0.10\n. @digitalplaywright We use this versions in a small side project, so I just don't use bulk import. Didn't find any time to investigate further. I'll keep you posted.\n. ",
    "chanil1218": "Rails server start is not a problem, because it rescues ES server status.\nBut Tire save phase via callback, I think it passes checking ES server status.\nand thus, whenever I create article, it returns error above and creation fails.\nI think when ES sever down, It should skip indexing and logging failed article.\nIs Tire callback only do update-index when Article creation or Article modification?\nThat is, include Tire callback equivalent to \nruby\nafter_save do\n  self.update_index\nend\n. Thanks, I will\n. ",
    "jkdeveyra": "Here's my code: https://gist.github.com/2766825\n. ",
    "fabiob": "Just found https://github.com/karmi/tire/pull/194, I think his code has better quality.\n. ",
    "alan": "@karmi has this been implemented or is it still not done?\n. Yes, saw it, just wondered if it had been implemented in another branch / pull request that I might have missed.\n. I also found this bug. I have no will_paginate in my Gemfile so I don't think it has any effect.\n. I'll take a look at reproducing it with a new Rails app, it's on my list of things to look at so hopefully I'll get around to do it in the next few days.\n. I don't have a setup with mongoDB and Tire, but I suspect that as long as the mongo client implements ActiveModel it should work.\nI've tested Model.count with a Mongoid Model and it works so I see no reason why it wouldn't work.\n. Any updates on this PR @karmi ? I believe Mongo based ORM's should keep working as expected. Let me know if you want me to check anything else to get this merged in.\n. ",
    "tim-vfiles": "Sorry, I copy and pasted from the initial example, not my modified code. So, I do have :type=>'article\" specified in the store statements (original post edited).\nAm I not sending the same structure? That is, analyzer with sub elements type and filter?\n```\n% curl localhost:9200/articles/_settings\n{\"articles\":{\"settings\":{\"index.number_of_shards\":\"5\",\"index.number_of_replicas\":\"1\"}}}\n% curl localhost:9200/articles/_mapping\n{\"articles\":\n  {\"article\":\n    {\"properties\":\n      {\"tags\":{\"type\":\"string\"},\n       \"title\":{\"type\":\"string\"},\n       \"published_on\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\n       \"type\":{\"type\":\"string\"}\n}}}}\n```\n. ",
    "ahfeel": "There you go :)\n. ",
    "jerryluk": "Would Love to have parent support. +1\n. ",
    "martinciu": "Hi @karmi I'll take a look at Sunspot's each_with_hit\nthanks!\n. ",
    "benevolentmadman": "egrep \"bson|mongo\" Gemfile.lock reports nothing, and no mongo shell, using MySQL locally and postgres on heroku\n. Since this project has to launch on heroku, and bonsai is still in beta, I've moved to sunspot/Websolr for now.\nI'll look into using this again when bonsai's out of beta.\n. ",
    "iterion": "Attempting to do something similar, here is my mapping that disappears on heroku:\nruby\nsettings :analysis => {\n            :filter => {\n              :name_ngram  => {\n                \"type\"     => \"nGram\",\n                \"max_gram\" => 8,\n                \"min_gram\" => 2 }\n              },\n              :analyzer => {\n                :name_analyzer => {\n                  \"tokenizer\"    => \"standard\",\n                  \"filter\"       => [\"lowercase\", \"stop\", \"name_ngram\"],\n                  \"type\"         => \"custom\"\n                }\n              }\n            } do\n      mapping do\n        indexes :id, type: 'integer'\n        indexes :name, analyzer: 'name_analyzer'\n        indexes :other_names, analyzer: 'name_analyzer'\n        indexes :city\n        indexes :state\n        indexes :domain\n      end\nend\n. Everything appears normal when importing:\n```\n[IMPORT] Starting import for the 'University' class\n\n2415/2415 | 100% ###############################################################\nImport finished in 4.06294 seconds\n```\nYet, the mapping from curl -X GET \"http://index.bonsai.io/[hidden]/university/_mapping\"\n{\n  \"university\" : {\n    \"properties\" : {\n      \"id\" : {\n        \"type\" : \"long\"\n      },\n      \"name\" : {\n        \"type\" : \"string\"\n      },\n      \"state\" : {\n        \"type\" : \"string\"\n      },\n      \"domain\" : {\n        \"type\" : \"string\"\n      },\n      \"other_names\" : {\n        \"type\" : \"string\"\n      },\n      \"city\" : {\n        \"type\" : \"string\"\n      }\n    }\n  }\n}\nOn my local install of elastic search it appears correctly.\n. Issue #386 added with more detail.\n. Exploring the console on heroku - I found this:\nirb(main):009:0> index.settings\nNoMethodError: undefined method `[]' for nil:NilClass\n    from /app/vendor/bundle/ruby/1.9.1/gems/tire-0.4.2/lib/tire/index.rb:62:in `settings'\n    from (irb):9\n    from /app/vendor/bundle/ruby/1.9.1/gems/railties-3.2.5/lib/rails/commands/console.rb:47:in `start'\n    from /app/vendor/bundle/ruby/1.9.1/gems/railties-3.2.5/lib/rails/commands/console.rb:8:in `start'\n    from /app/vendor/bundle/ruby/1.9.1/gems/railties-3.2.5/lib/rails/commands.rb:41:in `<top (required)>'\n    from script/rails:6:in `require'\n    from script/rails:6:in `<main>'\nirb(main):010:0> index.response\n=> 200 : {\"[bonsai_index_name]-[large_integer]\":{\"settings\":{\"index.number_of_replicas\":\"1\",\"index.number_of_shards\":\"1\"}}}\nThe error obviously comes from setting the index name based on the ENV variable as the index name in the response does not match.  Might this be preventing us from saving the mapping too?\nAlso, this:\nirb(main):021:0> index.create\n=> false\nirb(main):022:0> index.response\n=> 401 : {\"error\": \"Not authorized: Some endpoints are admin-only, ask support@onemorecloud.com.\"}\n--edit-- More Info:\nIf I modify the name of the index like so:\nruby\nindex.instance_variable_set :@name, \"#{index.name}-[large_int_that_was_returned]/university\"\nI can then run:\nruby\nindex.delete\n => true\n. @zacksiri take a look at https://saas.found.no/\nTheir support actually gets back to me, unlike bonsai.\n. @karmi no worries, things are working quite well for me with Found\nMost of the issues I saw with Bonsai came from restrictions on their end.\n. @karmi Yeah, migrating to a different provider fixed the issue, so it appears tire is fine.\n. ",
    "neutralino1": "This problem is still alive.\nIn my case, document_type is a belongs_to association.\nRedefining the document_type function like @edwinv suggested doesn't work as self[:document_type] does not exist, only self[:document_type_id] does.\nShouldn't document_type be fully qualified in Tire::Model::Search?\n. ",
    "OpakAlex": "```\nClassMethodsProxy::INTERFACE.each do |method|\n          base.class_eval <<-\"end;\", FILE, LINE unless base.public_methods.map(&:to_sym).include?(method.to_sym)\n            def self.#{method}(args, &block)                     # def search(args, &block)\n              tire.send(#{method.inspect}, args, &block)     #   tire.send(:search, args, &block)\n            end                                                   # end\n          end;\n        end\n    # Alias _Tire's_ instance methods in the top-level namespace of the model,\n    # unless there's a conflict with existing method\n    InstanceMethodsProxy::INTERFACE.each do |method|\n      base.class_eval <<-\"end;\", __FILE__, __LINE__ unless base.instance_methods.map(&:to_sym).include?(method.to_sym)\n        def #{method}(*args, &block)                          # def to_indexed_json(*args, &block)\n          tire.__send__(#{method.inspect}, *args, &block)     #   tire.__send__(:to_indexed_json, *args, &block)\n        end                                                   # end\n      end;\n    end\n\n```\nCrazy. Maybe we need add method :xyi? why not @karmi ???\n. @karmi, why you don't use some prefix? maybe tire_document_type ?  It's normal...\n. @edwinv Thanks! I redefine all this crazy methods. \n. ",
    "dongennl": "Actually, and this is pure luck, by looking at some of the code I seem to have got this to get the same result:\nfacet('country') { terms fields: [:geo_country1, :geo_country2]}\nnot sure what I'm doing, but it seems to work, need to test it more. Does this make sense?\n. hi, I noticed that my 'workaround' messes up any other facets that are defined after it. Not sure how/why.\nBut anyway, I found a better way to achieve the same result\nin the mapping:\n  indexes :geo_country, :type => 'string', :index => 'not_analyzed', as: 'country_display_for_search'\nand then:\n  def country_display_for_search\n    [self.geo_country1, self.geo_country2]\n  end\nthis then gives you a single field in the index (which contains an array)\nYou can now use the facet as normal:\n  facet 'country' do\n      terms :geo_country\n  end\nThis achieves the desired result. Just in case it helps someone else.\n. ",
    "andhapp": "I can't remember the issue we had related to this as it's been a while. \nThanks, anyways.\n. ",
    "kofno": "This is a big pull request. It mentioned multiget in the title, but I didn't see it in the impl. \n. ",
    "deanpcmad": "Looks like I can run Tire.index('invoices').reindex('invoices') and it reindexes. I couldn't see this in the documentation, maybe you could add it?\nThanks!\n. Well I need to import it directly into the database from our old invoicing system.\nAh ok, I didn't notice the rake task, thanks! :)\n. ",
    "mrazicz": "Thx for the quick reply. So i think this pull request is useless and i close it.\n. ",
    "cseeger": "whoops, didn't realize the pull request would open a new issue. closing this out\n. Example scope:\nruby\nclass Product...\n  default_scope lambda {\n    joins(:variants).where('available > 0').group('products.id')\n  }\nStack trace:\nruby\nActiveSupport::OrderedHash can't be coerced into Float\n.../gems/tire-0.4.2/lib/tire/tasks.rb:88:in `/'\n.../gems/ruby-1.9.2-p180@jambalaya/gems/tire-0.4.2/lib/tire/tasks.rb:88:in `block (4 levels) in <top (required)>'\n.../gems/ruby-1.9.2-p180@jambalaya/gems/tire-0.4.2/lib/tire/index.rb:132:in `import'\n.../gems/ruby-1.9.2-p180@jambalaya/gems/tire-0.4.2/lib/tire/tasks.rb:83:in `block (3 levels) in <top (required)>'\nEssentially, the import works fine, the failure is in rendering the progress bar.\n. ",
    "ericTsiliacos": "Thanks, the following is what I currently have. I wasn't really using the mapping way of doing it.\n```\nclass Log\n    attr_reader :date\ndef initialize(attributes={})\n  @attributes =  attributes\n  @attributes.each_pair { |name,value| instance_variable_set :\"@#{name}\", value }\nend\n\ndef type\n  'logs'\nend\n\ndef to_indexed_json\n   @attributes.to_json\nend\n\nend\n@conn = Mongo::Connection.new\n  @db   = @conn['mydb']\n  @coll = @db['test_collection']\n@coll.find.each do |doc| \n    row = JSON.parse(doc.to_json)\n    log = Log.new :date => row.fetch(\"date\")\nstore log\n\nend\nrefresh\nend\n```\nAnd in another file, I query and sort on the date attribute:\n@s = Tire.search( 'logs' ) do |search|\n      search.query { |query| query.string q }\n      search.sort do \n        by :date, 'desc' \n      end\nend\n. Thank you for your response. \nI am a little confused about why the space makes a difference between \n%Y/%m/%d %H:%M:%S,%L         and\n%Y/%m/%d %H:%M:%S. \nI would think the second format would also break because of the space but it doesn't, unless the space is part of the default. \n\nThis is what I have so far but it still doesn't seem to be working. Thank you again for your help.\n```\nindex = Tire.index 'logs' do\ndelete \ncreate :mappings => {\n:logs => {\n  :properties => {\n    :date             => { :type => 'date', :date_formats => 'basic_date_time'}\n  }\n}\n\n}\nend\n@conn = Mongo::Connection.new\n@db   = @conn['mydb']\n@coll = @db['test_collection']\n@coll.find.each do |doc| \n  row = JSON.parse(doc.to_json)\nlog = {}\n  log[:date] = Time.parse(row.fetch(\"date\")).strftime('%Y-%m-%dT%H:%M:%S.%L')\nindex.store log\nend\n```\n. works like a charm! Thank you for your help.\n. Thanks again for helping earlier. Tire sorts the dates great.\nI have another question. I am having trouble querying using date range with the above. I am thrown an error when I try the following via an input field. \ndate:[2012-07-10 08:09:43,180 TO 2012-07-10 08:10:21,085]\nHere is part of the error message that seems relevant:\nnested: ParseException[Cannot parse 'date:[2012-07-10 08:09:43,180 TO 2012-07-10 08:10:21,085] ': Encountered \\\" \\\"TO\\\" \\\"TO \\\"\\\" at line 1, column 30.\\nWas expecting:\\n    \\\"]\\\" ...\\n    ]; nested: ParseException[Encountered \\\" \\\"TO\\\" \\\"TO \\\"\\\" at line 1, column 30.\\nWas expecting:\\n    \\\"]\\\" ...\\n    ]; }{[3-yAuRASS5-eufkYlMUcpQ][logs][3]: SearchParseException[[logs][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\\"query\\\":{\\\"query_string\\\":{\\\"query\\\":\\\"date:[2012-07-10 08:09:43,180 TO 2012-07-10 08:10:21,085] \\\",\\\"default_operator\\\":\\\"AND\\\"}},\\\"sort\\\":[{\\\"date\\\":\\\"desc\\\"}],\\\"highlight\\\":{\\\"fields\\\":{\\\"file\\\":{\\\"number_of_fragments\\\":0},\\\"date\\\":{\\\"number_of_fragments\\\":0},\\\"location\\\":{\\\"number_of_fragments\\\":0},\\\"level\\\":{\\\"number_of_fragments\\\":0},\\\"path\\\":{\\\"number_of_fragments\\\":0},\\\"lcid\\\":{\\\"number_of_fragments\\\":0},\\\"sid\\\":{\\\"number_of_fragments\\\":0},\\\"uid\\\":{\\\"number_of_fragments\\\":0},\\\"message\\\":{\\\"number_of_fragments\\\":0}},\\\"pre_tags\\\":[\\\"<em class=\\\\\\\"highlight\\\\\\\">\\\"],\\\"post_tags\\\":[\\\"</em>\\\"]},\\\"size\\\":500,\\\"from\\\":0}]]]; nested: QueryParsingException[[logs] Failed to parse query [date:[2012-07-10 08:09:43,180 TO 2012-07-10 08:10:21,085] ]]; nested: ParseException[Cannot parse 'date:[2012-07-10 08:09:43,180 TO 2012-07-10 08:10:21,085] ': Encountered \\\" \\\"TO\\\" \\\"TO \\\"\\\" at line 1, column 30.\\nWas expecting:\\n    \\\"]\\\" ...\\n    ]; nested: ParseException[Encountered \\\" \\\"TO\\\" \\\"TO \\\"\\\" at line 1, column 30.\\nWas expecting:\\n    \\\"]\\\" ...\\n    ]; }{[3-yAuRASS5-eufkYlMUcpQ][logs][1]: SearchParseException[[logs][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\\"query\\\":{\\\"query_string\\\":{\\\"query\\\":\\\"date:[2012-07-10 08:09:43,180 TO 2012-07-10 08:10:21,085] \\\",\\\"default_operator\\\":\\\"AND\\\"}},\\\"sort\\\":[{\\\"date\\\":\\\"desc\\\"}],\\\"highlight\\\":{\\\"fields\\\":{\\\"file\\\":{\\\"number_of_fragments\\\":0},\\\"date\\\":{\\\"number_of_fragments\\\":0},\\\"location\\\":{\\\"number_of_fragments\\\":0},\\\"level\\\":{\\\"number_of_fragments\\\":0},\\\"path\\\":{\\\"number_of_fragments\\\":0},\\\"lcid\\\":{\\\"number_of_fragments\\\":0},\\\"sid\\\":{\\\"number_of_fragments\\\":0},\\\"uid\\\":{\\\"number_of_fragments\\\":0},\\\"message\\\":{\\\"number_of_fragments\\\":0}},\\\"pre_tags\\\":[\\\"<em class=\\\\\\\"highlight\\\\\\\">\\\"],\\\"post_tags\\\":[\\\"</em>\\\"]},\\\"size\\\":500,\\\"from\\\":0}]]]; nested: QueryParsingException[[logs] Failed to parse query [date:[2012-07-10 08:09:43,180 TO 2012-07-10 08:10:21,085] ]]; nested: ParseException[Cannot parse 'date:[2012-07-10 08:09:43,180 TO 2012-07-10 08:10:21,085] ': Encountered \\\" \\\"TO\\\" \\\"TO \\\"\\\" at line 1, column 30.\\nWas expecting:\\n    \\\"]\\\" ...\\n    ]; nested: ParseException[Encountered \\\" \\\"TO\\\" \\\"TO \\\"\\\" at line 1, column 30.\\nWas expecting:\\n    \\\"]\\\" ...\\n    ]; }]\",\"status\":500}\nIt seems to work when I put the individual date-times in quotes as seen below but I was under the impression that it should work without the quotes.\ndate:[\"2012-07-10 08:09:43,180\" TO \"2012-07-10 08:10:21,085\"]\nAlso, I noticed that the \" * \" wildcard does not work when I try doing the following:\ndate:[2012-07-10* TO 2012-07-10*]\nI was wondering if there was anyway I could get help with understanding why the above is the case and how I could fix it. Thank you!\n. Thank you for your help. I also tried: \ndate:[2012-07-03* TO 2012-07-04*]\nand was given the following error message as well:\nElasticSearchParseException[failed to parse date field [2012-07-03*], \ntried both date format [Y-M-d H:m:s,SSS], and timestamp number]; \nnested: IllegalArgumentException[Invalid format: \\\"2012-07-03*\\\" is malformed at \\\"*\\\"]; }]\",\n\"status\":500}\nJust as a confirmation and so that someone else may be able to recreate the problem, the following is the index mapping I used:\n```\n!/usr/bin/env ruby\nrequire 'tire'\nclass Index_mapping \n  def initialize\n    Tire.index 'logs' do\n  create :mappings => {\n\n    :log => {\n      :properties  =>  {\n        :id       => { :type => 'string', :index => 'not_analyzed', :include_in_all => false },\n        :date     => { :type => 'date', format: \"Y-M-d H:m:s,SSS\"                            }\n      }\n    }\n  }\nend\n\nend \nend\n```\nI'll let you know if something comes up. Thanks again for your help.\n. ",
    "cwalsh": "Romanbsd: Fixnum is an interesting one, see http://www.rubyfleebie.com/understanding-fixnums/ for reasons.  However, if you use Integer, it gets converted to Fixnum anyway.\nErickt, my patch doesn't handle Time, I might look in to it later. Does yours work with DateTime etc.?\n. Ok, https://github.com/karmi/tire/pull/352/files#diff-5 got merged so this is now irrelevant.\n. Oh whoops, I misread that and thought #352 was merged already, but it was just closed.  Yes, I've updated the pull request because I initially missed the option for :class => [Float] not just :class => Float.  Thanks for reopening.\n. ",
    "joshreed": "@karmi Yep, that's exactly correct. Ultimately it creates less gem dependencies that could be annoying for those who have code still using ruby 1.8.7. Like I said, it's completely up to you if you want to include it, it just solved my problem and I thought it could be helpful for others as well. \nEven though it could be considered slightly on the non-DRY side, to me the decrease in dependencies is worth it. Plus, since it is a back-port of ruby 1.9 functionality anyway, it doesn't seem bad to me for Tire to include its own copy of the necessary back-ported functionality.\nI only tested it with Rack 1.2+, and I didn't completely go through the Tire code to see if you use Rack for anything else (besides this back-ported functionality), but it may be possible to completely drop the dependency on Rack by making this change, which could be even better.\nAnyway, let me know if you have any other questions, I'd be glad to help.\n. ",
    "chrisyuska": "Thanks a lot, vhyza.\nMy issue was that I somehow left out the include: :comments in:\nruby\n  def to_indexed_json\n    to_json(include: :comments)\n  end\n. I've actually gotten this to work now using a slightly modified setup.  I'm just posting it here in case anyone else comes across the same issue.  If this is an incorrect/bad way of doing it, I'd appreciate the feedback.\n``` ruby\nclass Position < ActiveRecord::Base\n  belongs_to :person\nend\nclass Person < ActiveRecord::Base\n  has_many :positions\ninclude Tire::Model::Search\n  include Tire::Model::Callbacks\nsettings :analysis => {\n    :filter => {\n      :partial_filter => {\n        \"type\"     => \"nGram\",\n        \"min_gram\" => 2,\n        \"max_gram\" => 5 }\n    },\n    :analyzer => {\n      :exact_analyzer => {\n        \"filter\"    => [ \"lowercase\", \"standard\" ],\n        \"tokenizer\" => \"standard\" },\n      :partial_analyzer => {\n        \"filter\" => [ \"lowercase\", \"standard\", \"partial_filter\" ],\n        \"type\" => \"custom\",\n        \"tokenizer\"     => \"standard\" }\n    }\n  }\n  mapping do\n    indexes :name, type: 'string'\n    indexes :positions, :type => 'object', :properties => {\n                  :organization => {\n                    :type => \"multi_field\",\n                    :fields => {\n                      :exact => { :type => 'string', :analyzer => 'standard_analyzer', :boost => 4.0 },\n                      :partial => { :type => 'string', :analyzer => 'generic_analyzer' }\n                    }\n                  },\n                  :title => { :type => \"string\", :analyzer => 'generic_analyzer' }\n                }\n  end\ndef self.search(params={})\n    tire.search() do\n      query do\n        boolean do\n          should do\n            text \"positions.organization.exact\", params[:query], :operator => 'AND'\n          end\n          should do\n            text \"positions.organization.partial\", params[:query]\n          end\n        end\n      end\n    end\n    highlight \"positions.organization.exact\", \"positions.organization.partial\"\n  end\nself.include_root_in_json = false\n  def to_indexed_json\n    to_json(include: :positions)\n  end\nend\n```\n. I had this error in an application I'm working on now just a little bit ago.  I had an indexer running in the background that bulk indexes stuff as needed.  I tracked it down to instances where Tire.index.import was being called on an empty array.  e.g.\nruby\nTire.index( Settings.elasticsearch_product_index ).import []\nI'd check to see if you're ever calling the import method when Product.all returns empty for some reason.  It's hard to tell much else without seeing more code first.\n. This should work for you:\nruby\nfilter :bool, should: { term: { something: true } }\nfilter :bool, should: { term: { something_else: false } }\nLet me know if it doesn't.\n. ",
    "zacksiri": "This could be related to my issue although mine is having issues with callbacks not get request, I can run tire import but it seems if I update create or delete it doesn't work.\n. Btw interesting that I am also on 3.2.5 and tire 0.4.2 perhaps there is something not right with this combo?\n. hey, yea i am thinking of switching too been having too many issues with bonsai\n. ",
    "felipegs": "I have the same problem here\n. Sorry I was calling model.delete not model.destroy. model.delete doesn't call the callbacks\n. ",
    "travisbot": "This pull request passes (merged 1fcbdd5c into 5129f8a5).\n. This pull request fails (merged a7c3e970 into 5129f8a5).\n. This pull request passes (merged a9f1f3f0 into 5129f8a5).\n. This pull request passes (merged f1f297c2 into 5129f8a5).\n. This pull request passes (merged 583201ae into 5129f8a5).\n. This pull request fails (merged 9008adab into 5818e50e).\n. This pull request fails (merged 19d40ef8 into 5818e50e).\n. This pull request fails (merged 0646f7c8 into 5818e50e).\n. ",
    "borisruf": "I ran into the same problem and noticed that homebrew tricked me: It was not auto-updating itself (version 0.8) and kept installing and outdated version of elasticsearch. Enforcing updates of both fixed the issue.\n. ",
    "brunzino": "I had this problem too and also fixed it by updating homebrew. However, when I updated to Mountain Lion, my indexes were gone and when trying to re-index, got this same problem again. \n. ",
    "angusd": "I hit this earlier today with the latest (0.4.2) gem. Saw the freshness of the above comment and tried rolling back to 0.3.12 and it indexed fine.\n. ",
    "beunwa": "Hi, same problem here, I use Tire 0.4.3 without rails and I bulk import plain array on a remote server. \n[ERROR] 400 > {\"error\":\"InvalidTypeNameException[mapping type name [bulk] can't start with '']\",\"status\":400}, retrying (1)...\n. It's ok now with a newer version of elastic search, thank you karmi\n. ",
    "mubix": "Having this same issue with https://gist.github.com/953072\nTurns out I was using an old version of elasticsearch\n. ",
    "andrew": "I've been caught out by this a number of times, tire says its indexed 100% of the documents, but that doesnt mean it has put all (or any) of them into the index.\nThinking sphinx has quite a detailed output from its rake task: http://freelancing-god.github.com/ts/en/rake_tasks.html\n. ",
    "1st8": "Opened a Pull Request for this, sorry..\n. Hi, thanks for your detailed response :)\nWhat are the advantages of your solution against a default Logger interface implementation?\nIf there are none I would prefer the \"standard\" solution.\nI saw a simple example in mongoids implementation https://github.com/mongoid/mongoid/blob/master/lib/mongoid/loggable.rb\nFor tires log output it could be better to omit the Rails.logger usage ;)\nI can implement it later today or tomorrow if you want.\n. ",
    "dirkbolte": ":+1: I just hit a similar issue with using logger in tire. In my setup, I have to use a Lumberjack logger. Getting this working with tire resulted in a similar implementation as shown above. Having a way to assign a custom logger which sticks to the Logger interface would be great. \nAn alternative to the mongoid implementation would be: https://github.com/vigetlabs/loggable - it provides a stub per default which ignores logs, so instead of checking for null, you can actually provide a block which build the message - which would not be executed if no Log object is assigned.\nI personally prefer being able to assign a logger. If there are concerns with breaking the current interface, I can try to provide an implementation which supports both approaches. \n. ",
    "davidkpham": "So it turns out I can simply do \nself.phone.user.text_messages.each {|text_message| text_message.tire.update_index }\nin the after_save callback of the Contact model, although this does slow down my web application quite a bit. If anyone has a better idea, I would appreciate any thoughts!\n. ",
    "zunger": "ID's are definitely unique. I'm importing from a PostgreSQL table where the ID is a unique primary key. I even ran a \"group by id having count(*) > 1\" query on the table in PGSQL to make sure I wasn't going batty, which obviously confirmed they were unique.\nI believe the issue occurs when paging is used to break the recordset into chunks.\nFrom the console, after manually deleting the index:\n\"Region.import\" will result in the missing record scenario. \n\"Region.index.import Region.all\" imports as expected, all records are present. \nThe rake task:\n\"rake environment tire:import CLASS='Region' FORCE=true\" will result in missing records.\nThe only common thread between the two failing imports are that they both use paging. I'm using will_paginate 3.0.3.\nI read an old issue which involved paging not automatically sorting by any field, resulting in skipped records on import. This sounds like the case I'm experiencing, however the issue was marked as closed with a commit to auto sort by ID and I'm using the latest version of Tire, so it sounds unlikely.\nNeither there are no errors from either Tire or elasticsearch during or post import. Records in elasticsearch contain the correct ID's for those records that were imported, so the field is present - some records are just skipped and others imported more than once by the looks of it - which again points to the closed pagination issue.\nObviously I can use the import method that works, and past the initial import it is no longer an issue assuming I don't need to bulk import again so I could live with it - but if a bug exists I'd love to help track it down and fix it, I'm just not sure what else to look for.\n. @karmi I've been busy with other development for a while, but recently had to reimport this table and encountered the issue again.\nAs an update to the above, running the following command from the console works as expected, WITH paging:\nRegion.order('id').import\nWhereas \"Region.import\" doesn't.\nIs it possible there was a regression reversing the \"default sort order by id\" issue fix?\n. ",
    "scotthelm": "@zunger I have experienced this as well.  I'm using your workaound for a large index import from postgres.\n. ",
    "rbeene": "I ended up doing the following.\n```\nsearch          = Tire::Search::Search.new(self.name.downcase.pluralize)\nsearch.query {|search| search.boolean{|boolean| boolean.should{string search_criteria[:query], fuzziness: 0.5, default_operator: \"OR\"}}} unless search_criteria[:query].blank?\nsearch.query {|search| search.boolean{|boolean| boolean.must{string search_criteria[:boolean]}}} unless search_criteria[:boolean].blank?\nsearch.results\n\n```\nI did however need (I believe) to make a small change in the tire gem in search.rb.\ndef query(&block)\n  @query ||= Query.new\n   block.arity < 1 ? @query.instance_eval(&block) : block.call(@query)\n   self\nend\nI believe when I didn't have this in place, the second piece stomped on the first.\nThoughts?\n. Karmi - sorry for the extreme delay in reply. You were right. The problem was resolved after following your example. Thanks for all your hard work. \n. ",
    "driki": "Hi,\nI'm also running into an issue getting the index setting working in Persistance mode and have tried Document.create_elasticsearch_index. I have an index named 'municipalities' and two classes 'Municipality' and 'Document'. The Municipality documents exist in the 'municipalities' index as a 'municipality' type. Document types are also in the same 'municipalities' index stored as a 'document' type. Any thoughts about how to configure the settings or write code for my Document and Municipality classes so that I can use the explicit mappings defined in both regardless of which type was created first?\nIn my case the 'municipalities' index already exists prior to a new Document being created and added to the index. When I call: \n1.9.3p125 :004 > Document.create_elasticsearch_index\n => nil\nThe result is nil which I'm guessing is because the 'municipalities' index already exists? When I destroy the 'municipalities' index and then save a Document\ndoc = Document.new(:classification => \"minutes\", :municipality_state => 'MA', :municipality_slug => 'ma-somerville', :content_url => 'http://www.google.com')\ndoc.save\nI can see that the explicit Document mapping settings are used, but if I create a Municipality and persist it first I do not get the explicit Document mapping. Below is the result of saving a Document prior to a Municipality.\n{\n  \"municipalities\": {\n    \"document\": {\n      \"properties\": {\n        \"classification\": {\n          \"index\": \"not_analyzed\",\n          \"type\": \"string\"\n        },\n        \"content_type\": {\n          \"index\": \"not_analyzed\",\n          \"type\": \"string\"\n        },\n        \"content_url\": {\n          \"index\": \"not_analyzed\",\n          \"type\": \"string\"\n        },\n        \"id\": {\n          \"index\": \"not_analyzed\",\n          \"type\": \"string\"\n        },\n        \"municipality_name\": {\n          \"index\": \"not_analyzed\",\n          \"type\": \"string\"\n        },\n        \"municipality_slug\": {\n          \"index\": \"not_analyzed\",\n          \"type\": \"string\"\n        },\n        \"municipality_state\": {\n          \"index\": \"not_analyzed\",\n          \"type\": \"string\"\n        }\n      }\n    }\n  }\n}\nThis is the mapping for the Municipality class if an instance is persisted after a Document. Notice none of the explicit mapping settings are being used.\n{\n  properties: {\n    full_name: {\n      type: \"string\"\n    },\n    id: {\n      type: \"string\"\n    },\n    name: {\n      type: \"string\"\n    },\n    slug: {\n      type: \"string\"\n    },\n    state: {\n      type: \"string\"\n    }\n  }\n}\nThe Municipality class\n```\nrequire 'active_model'\nclass Municipality\ninclude Tire::Model::Persistence\nindex_name 'municipalities'\nmapping do\n    indexes :id, :index => 'not_analyzed'\n    indexes :state, :index => 'not_analyzed'\n    indexes :slug, :index => 'not_analyzed'\n    indexes :population, :index => 'not_analyzed', :type => 'integer'\n    indexes :population_density, :index => 'not_analyzed', :type => 'float'\n    indexes :latitude, :index => 'not_analyzed', :type => 'float'\n    indexes :longitude, :index => 'not_analyzed', :type => 'float'\n  end\nproperty :id\n  property :name\n  property :state\n  property :slug\n  property :full_name\n  property :website\n  property :website_accept_cookies\n  property :website_linkable_domains\n  property :website_skip_links\n  property :website_strip_params\n  property :code_fips\n  property :code_gnis\n  property :population\n  property :population_density\n  property :race_american_indian\n  property :race_asian\n  property :race_black\n  property :race_hispanic\n  property :race_multiple\n  property :race_non_hispanic\n  property :race_non_hispanic_white\n  property :race_other\n  property :race_pacific_islander\n  property :race_white\n  property :diversity\n  property :area_land\n  property :area_water\n  property :latitude\n  property :longitude\n  property :housing_units\n  property :housing_vacancies\n  property :created_at\n  property :updated_at\nend\n```\nAnd the Document class\n```\nrequire 'active_model'\nclass Document\ninclude Tire::Model::Persistence\nindex_name 'municipalities'\nmapping do\n    indexes :content_url, :index => 'not_analyzed'\n    indexes :id, :index => 'not_analyzed'\n    indexes :classification, :index => 'not_analyzed'\n    indexes :content_type, :index => 'not_analyzed'\n    indexes :municipality_name, :index => 'not_analyzed'\n    indexes :municipality_slug, :index => 'not_analyzed'\n    indexes :municipality_state, :index => 'not_analyzed'  \n  end\nproperty :id\n  property :classification\n  property :content_type\n  property :content_url\n  property :created_at\n  property :extracted_text\n  property :last_modified_at\n  property :legislative_body\n  property :likely_date\n  property :municipality_name\n  property :municipality_slug\n  property :municipality_state\n  property :phone_numbers\n  property :title\n  property :updated_at\nend\n```\nThanks,\nMatt\n. Hi,\nThanks for the pointers. I made a minor change and am able to query for parent/child as I was hoping to do (finding documents for a given municipality). Any tips you have on how to build more complex queries (OR,filters,sorting,facets) this way would be welcome.\n``` ruby\nrequire 'tire'\ns = Tire.search 'municipalities/document',\n                query: {\n                  has_parent: {\n                    type: 'municipality',\n                    query: { match: { name: { query: 'New', type: 'phrase_prefix' } } }\n                  }\n                }\nputs 'Query:',   s.to_curl\nputs 'Results:', s.results.to_a.inspect\n```\n. Wonderful! Thanks for the help.\n. ",
    "mhaseebkhan": "Can any one please tell me syntax for geo_polygon filter.How to write it in Tire.search block and pass it my polygon coordinates.\n. ",
    "peterwillcn": "ok\n. ",
    "john-bai": "I see what you mean about ActiveRecord.find being flexible about using a String id param for lookup. In all honesty I can also add such flexibility into my comparison logic and things will work just fine. Here's an example of a comparison that wouldn't go well:\n``` ruby\nproducts = Product.search(\"xbox games\");\n@unowned_products = []\nproducts.each do |product|\n  @unowned_products << product unless current_user.owned_product_ids.include?(product.id)\nend\n```\nI can use to_i to get it working, so no blockers there. I was just curious to ask about the preference of _id (which you explained).\nWhat do you think about using _source.id if defined, otherwise use _id?\nruby\ndocument = h['_source'] ? document.update( h['_source'] || {} ) : document.update( __parse_fields__(h['fields']) )\ndocument.update( {'id' => h['_id']} ) if document['id'].nil?\n. ",
    "mriley": "@karmi \u2014\u00a0Sorry for the delay. I'll try to knock out a test for it today and submit. \n. I just made a few small changes to the mock responses in the Alias test suite that should cause errors with the existing version of Tire before the patch in my first commit is applied. The issue is that you can create an alias with a single \"routing\" value or with two separate routing values \"index_routing\" and \"search_routing\". Regardless of which you choose, the routing values are stored separately in ES as \"index_routing\" and \"search_routing\" (if you simply pass \"routing\" when you create it the same value is used for both routings). I added \"index_routing\" and \"search_routing\" values to the mock responses, and the \"index_routing\" value breaks the code I modified in my patch.\nFwiw, I think it makes sense for Tire to expose two different helper methods for setting the index_routing and search_routing values separately, but I thought that was more appropriate as a second patch. I will try to put that together along with some new tests. \n. Did you have questions about the changes I made to the mock responses in tests? Those changes expose the original bug, and my patch fixes it.\n. Oh ok :) Looking forward to seeing it in there soon. \nLet me know if you want to chat about other changes to the Alias interface. Like I said in my earlier comment, we might want to expose two different helper methods for setting the index_routing and search_routing values separately. I was having a hard time finding a way to make that change fully backward-compatible, though. \n. ",
    "pex": "same issue here :scream:\n. ",
    "dementrock": "Thank you! It seems that I set up indexing the wrong way before. It works now.\n. ",
    "davekrupinski": "It is definitely just an added convenience to the library, but does bring the interface more in line with other common search gems (sunspot, thinking_sphinx, etc).\n. ",
    "crazed": "I removed the subclassing and received the same result\n. ",
    "andrelip": "I deleted everything and created a new item without carrierwave integration. It doesn't work.\nI have discovered something: Document.tire.search('').total_count results => 0 \nbut doing...\ns = Tire.search 'documents' do\n      query do\n        string ''\n      end\nthen s.results gives me:\n => #4, \"timed_out\"=>false, \"_shards\"=>{\"total\"=>5, \"successful\"=>5, \"failed\"=>0}, \"hits\"=>{\"total\"=>1, \"max_score\"=>1.0, \"hits\"=>[{\"_index\"=>\"documents\", \"_type\"=>\"Declara\u00e7\u00e3o de IR\", \"_id\"=>\"1\", \"_score\"=>1.0, \"_source\"=>{\"id\"=>1}}]}}, @options={}, @time=4, @total=1, @facets=nil, @wrapper=Tire::Results::Item> \nWhat could be happening?\n. ",
    "Datesta": "It's not working and it doesn't seems like it's a include_root_in_json problem. In rails console: \nProject.last.to_json\n => \"{\\\"_id\\\":\\\"503627b6d672600a0b000006\\\",\\\"c\\\":null,\\\"ci\\\":null,\\\"created_at\\\":\\\"2012-08-23T12:53:10Z\\\",\\\"deleted_at\\\":null,\\\"f\\\":null,\\\"fa\\\":null,\\\"l\\\":null,\\\"t\\\":\\\"herp22\\\",\\\"ta\\\":null,\\\"updated_at\\\":\\\"2012-08-23T12:53:10Z\\\",\\\"user_id\\\":null,\\\"votes\\\":{\\\"up\\\":[],\\\"down\\\":[],\\\"up_count\\\":0,\\\"down_count\\\":0,\\\"count\\\":0,\\\"point\\\":0}}\" \nAnd:\nProject.last.as_json\n => {\"_id\"=>\"503627b6d672600a0b000006\", \"c\"=>nil, \"ci\"=>nil, \"created_at\"=>Thu, 23 Aug 2012 12:53:10 UTC +00:00, \"deleted_at\"=>nil, \"f\"=>nil, \"fa\"=>nil, \"l\"=>nil, \"t\"=>\"herp22\", \"ta\"=>nil, \"updated_at\"=>Thu, 23 Aug 2012 12:53:10 UTC +00:00, \"user_id\"=>nil, \"votes\"=>{\"up\"=>[], \"down\"=>[], \"up_count\"=>0, \"down_count\"=>0, \"count\"=>0, \"point\"=>0}} \nSeems like there is something going on with the formatting that is satisfying Elasticsearch with to_json and not as_json.\n. So, are there any downsides to use to_json? Will this break something later down the chain?\n. Thanks @canadaduane, it seems to work so i'm just gonna roll with this\n. ",
    "canadaduane": "I think it's safe to use to_json. See http://stackoverflow.com/questions/11275714/haml-rendering-as-json-with-hash-rocket-instead-of-colon\n. Well, it just tripped me up when trying to debug. It would be a time saver\nin rare cases, if it didn't slightly mislead.\nOn Oct 23, 2012 10:38 AM, \"Karel Minarik\" notifications@github.com wrote:\n\nWell, I think the docs say that you should use the option when you're\nproviding the payload as @file. Nevertheless, the curl log is just\ninformational as it is -- otherwise it would dump loads of payload data\ninto your log... Leaning towards closing this?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/pull/452#issuecomment-9708846.\n. \n",
    "Justinwceo": "I'm working on spatial searching right now too. Did you get this working? Do you have the to_indexed_json  method up and running like:\ndef to_indexed_json\n   to_json(:methods => [:lat_long])\nend\n. ",
    "mylesmegyesi": "The index already existed with a different type for the location field. So, when I tried to post a geo_point to the index, it saw a type error. The solution is to delete the index and re-create it.\n. ",
    "finalarena": "In case anybody finds it useful, I found the way to achieve 1)\n``` ruby\nrequired_sorts = [{field: \"title.exact\", order: :desc}, {field: :popularity, order: :desc}]\nquery.sort do |s|\n  required_sorts.each do |one_sort|\n    options = {\"order\"=>one_sort[:order]}\n    options.merge!({\"missing\" => \"_last\"}) if field_is_numeric?(one_sort[:field])\n    s.by one_sort[:field], options\n  end\nend\n```\n- you can only use \"missing\" => \"_last\" on numeric fields (and only sort on :not_analyzed fields / sub_fields).\n- in my use case I'm pushing all missing to the end\n. Any chance this can be merged now?\n. ",
    "mrrooijen": "After a good night rest I wake up to this:\nruby\nafter_save do\n  update_index if published?\nend\nSeeing as in my tests I initially have published true, but then set it to false and expect it not to be there, it is. This is because it never updates the index when it isn't published. This is what I ended up doing, for anyone that might have this questions in the future:\n``` ruby\nclass Article\n  include Mongoid::Document\n  include Tire::Model::Search\nfield :published, type: Boolean, default: false\nmapping do\n    indexes :published, type: \"boolean\"\n  end\nend\n```\nruby\nArticle.search do\n  query do\n    boolean do\n      must { string params[:query] }\n      must { term :published, true }\n    end\n  end\nend\nPassing specs, good times.\nClosing issue.\n. ",
    "amencarini": "Just to chime in, I've had a similar issue, although I'm using ActiveRecord: in my app there a boolean flag that's false by default in the database; searching with a term would be alright on development and production, but somehow failing in test environment filtering by that boolean.\nAdding an after_create callback with an update_attributes on the model made the problem go away.\n. ",
    "mauricio": "Anyone? @karmi \n. I have updated the README and also added an errors.rb file.\n. No other comments on this @karmi ?\n. if I search for 'girl' it will only find records with girls in the title/description.\n\"girls\" or \"girl\" ?\nWhat analyzers are configured for the _all field?\nIt would be nice if you posted the full mapping for this type.\n. To avoid allowing you to define a config that's not supported, just failing early. But I can move this to the to_indexed_hash method too.\n. You could possibly use it for something like that, but you could also use a proc/lambda for it, my main concert here is to only accept something that we know how to handle. \nWhen I first used :as I was using it with a lambda and it was working, somewhere along the path it stopped working and I was clueless as to what I was missing (which was my own mistake, as I didn't notice the docs did not talk about lambdas, only procs), that's when I decided to check the code :)\nSo, this is mostly me scratching my own itch and making sure it is not possible to send in a value that's not \"known\".\nNow that's for you to decide if we should validate or just keep it as is ;D (i'm personally much more inclined to validate)\n. ",
    "kenshin54": "+1\n. ",
    "m4tm4t": "nobody deploy with tools in production ?!\n. I'm trying to deploy in production as most DRY way.\nSo above, this is just a simple rake task that is called from Mina.\nThat work, yeah. My question is: This is a best practice ?\nBecause there is no docs/article on Tire for production environment,I ask the question.\n. ",
    "jalkoby": "Maybe you are right. So I close this pull request. See you in tire-contrib)\n. ",
    "renuyarday": "My bad.\nStarted another node. with that replica is working.\n. ",
    "thbar": "Thanks, I was looking for that exact information!\n. ",
    "frankel": "Thanks, I will have a try\n. ",
    "amir20": "Great! Will try. \n. Unfortuantely this seems to be a problem with elasticsearch. I ran the following query\njson\n{\n\"script_fields\" : {\n      \"distance\" : {\n         \"script\" : \"doc['lat_lon'].distanceInKm(0,50.3)\"\n      }\n   },\n\"query\": {\n\"match_all\" : {}\n}\n}\nand the returned values were only \njson\n{\n    took: 1,\n    timed_out: false,\n    _shards: {\n        total: 5,\n        successful: 5,\n        failed: 0\n    },\n    hits: {\n        total: 1,\n        max_score: 1,\n        hits: [\n            {\n                _index: tilofy_test_channels,\n                _type: channel,\n                _id: 1,\n                _score: 1,\n                fields: {\n                    distance: 19629.025705718916\n                }\n            }\n        ]\n    }\n}\nSo I am going to ask this question on elasticsearch also. \n. Closing this since it is part of elasticsearch problem. \n. Thanks for the issue links. I was looking for them. \n. @karmi Sorry I had previously closed this issue. I thought you closed it then I accidentally opened it. But I see that you wanted it open. \n. @karmi I am using revision: 04cce99c2b341eb37f60ff6a11fa459d4c073e12 which is the latest on github. My mapping is as follows:\n``` ruby\ndef search_with_term(term, options = {})\n      lat_lon = to_lat_lon(options)\n  tire.search per_page: options[:limit] || Kaminari.config.default_per_page, page: options[:page] || 1 do\n    query { string term } if term.present?\n    filter :bool, must: { term: { visible: options[:visible] } } unless options[:visible].nil?\n    filter :bool, must: { term: { published: true } }\n    filter :term, privacy: options[:privacy] if options[:privacy]\n    filter :terms, tags: options[:tags] if options[:tags].present?\n    filter :bool, must: { terms: { id: options[:ids] } } if options[:ids].present?\n    filter :bool, must_not: { terms: { id: options[:not_ids] } } if options[:not_ids].present?\n    if lat_lon\n      filter :geo_distance, location: lat_lon, distance: (options[:distance] || options[:radius] || '10mi')\n      sort { by :_geo_distance, location: lat_lon, unit: :mi }\n      script_field :distance, script: \"doc['location'].arcDistance(#{lat_lon[1]}, #{ lat_lon[0]})\"\n      # fields Channel.mapping.keys\n    end\n    sort { by options[:sort], options[:order] || :asc } if Set.new([:created_at]).include?(options[:sort])\n  end\nend\n\n```\nNot that that fields Channel.mapping.keys is commented out. However my test cases are not passing. I am asserting that the name field is returned. \nruby\nit 'should have returned the right channel' do\n      results = Channel.search(lat_lon: [channel.lat + 0.001, channel.lon - 0.01])\n      expect(results.first.name).to eq channel.name\n    end\nI am looking at your commits and I see that your commit should clearly fix this. What am I doing wrong? \n. ",
    "parmarg": "I am also using MongoMapper and was finally able to get the pagination working by doing the following\nquery = Tire.search('assets',{:page => page , :per_page=>25, :size=>25, :from=>from})  do\nWithout the :from it did not work, where from is the offset.\n. ",
    "jredburn": "I recently ran into this and it turned out the problem was that I'd assumed that -- like will_paginate -- Tire had a default per_page setting. It appears that if you don't specify a per_page value then page has no effect. Easy to solve, but threw me off for a bit.\n. @karmi I'll write up a failing test later today, but the short version is:\n```\nresults = Relation.search 'label_id:1', :page => 2\nresults.first.id == \"2417\"\nresults = Relation.search 'label_id:1', :page => 2, :per_page => 10\nresults.first.id == \"19713\"\n```\n. @karmi Here's a failing test. I'm new to github, so let me know if you prefer this in a different form (e.g. pull request):\n```\n--- a/test/integration/active_record_searchable_test.rb\n+++ b/test/integration/active_record_searchable_test.rb\n@@ -212,6 +212,24 @@ module Tire\n             assert_nil results.first\n           end\n\ncontext \"without an explicit per_page\" do\n+\nshould \"find not find missing (second) page\" do\nresults = ActiveRecordArticle.search 'test*', :sort => 'title', :page => 2\nassert_equal 0, results.size\n+\n\nWillPaginate\n\n\n\n\nassert_equal 1, results.total_pages\nassert_equal 2, results.current_page\nassert_equal 1, results.previous_page\nassert_equal nil, results.next_page\n+\nassert_nil results.first\nend\n+\nend          \n+\n         end context \"and block searches\" do\n\n``\n. Taking a look at the code, it appearslib/tire/model/search.rbonly changesfromif bothpageandper_page` are set:\n\n\n```\npage     = options.delete(:page)\nper_page = options.delete(:per_page)\ns.size( per_page.to_i ) if per_page\ns.from( page.to_i <= 1 ? 0 : (per_page.to_i * (page.to_i-1)) ) if page && per_page\n```\nIf we change:\nper_page = options.delete(:per_page)\nto:\nper_page = options.delete(:per_page) || 10\nthat solves the problem though I suspect there's a better way to leverage the logic you already have in lib/tire/model/pagination.rb.\n. ",
    "zackfern": "After comparing both mappings I noticed they were different. \nUsing the tire:import FORCE=1 rake task solved this for me (although oddly enough I had been trying Article.index.import over and over). \nThanks for your help and creating such an awesome Gem. :smiley:\n. That sounds good. Do you have any tips for keeping the mapping up to date as I begin to roll this app out to production? \n. ",
    "NOX73": "OMG!\n. Oh, Yeah!\n. Done?\n. And now? :)\n. ",
    "mdpatrick": "Would you be OK with me re-submitting the PR with the change I made less the comment?\n. ",
    "simoncozens": "What I did was to turn on logging and try creating the index manually in rails console\nTire.configure { logger 'elasticsearch.log' }\nYourClass.create_elasticsearch_index\nIf you don't get an error from that, copy the index creation command from the bottom of the log file ( curl -X ...) and run it, and look at the errors from elasticsearch.\n. ",
    "carols10cents": "Does it not fail for you locally? What if you delete your Gemfile.lock, bundle, then run the tests? Since the Gemfile.lock isn't checked in and since a version isn't specified, Travis is getting the latest version of all the gems-- but you might have a local Gemfile.lock that has earlier versions.\nThe specific error I see on travis build 311.1 is:\n/home/travis/.rvm/gems/ruby-1.9.3-p194/gems/mocha-0.12.6/lib/mocha/integration/mini_test.rb:56:in \n`<class:TestCase>': No Mocha monkey-patch for MiniTest version (RuntimeError)\nThere's been some work/discussion around this:\nhttps://github.com/freerange/mocha/issues/87\nhttps://gist.github.com/3137094\nand the latest code in mocha's github repo works fine if I specify that in the Gemfile.\nBut it still looks like mocha 0.12.6 and minitest 4.1.0 are incompatible, so I don't think there's a way around this other than waiting for a new version of mocha to be released, using mocha's latest from github, or locking down the minitest version at something earlier than 4.\n. Looks like mocha released 0.12.7, so that's why it's working for you now (and passing on travis). Hopefully that'll be the end of this issue....\n. ",
    "unsay": "Sure enough!\n. If anyone can bite off the request by @karmi, I'm obliged.\nI won't be available to complete for 2 more weeks.\n. Back in the saddle. Will finish this up!\n. Derp. I'll do it now. :)\n. I'll have a complete test suite probably tomorrow. The core functionality hasn't changed.\n. @karmi let me know!\n. @loren, sorry about the noise. HTH. https://github.com/unsay/tire/commit/82dc19e9c433c85805a2b3a9fb2546053e895453\n. @loren Great! Once this is merged in, I'll apply similar tests and examples for nested filters.\n. ",
    "bgadoury": "I just realized we really need this, so I'm very excited to see it so close to hitting master. Thanks @unsay and @karmi !\n. @unsay Any news? :)\n. ",
    "loren": "@unsay This feature would be very useful for me. Any update on when it might ship?\n. @unsay, it works wonderfully. I had a slightly more complex nested mapping with different analyzers on the sub-fields, and I'm going to put it here just in case someone else stumbles upon the nested syntax.\nThank you for exposing this very useful feature in ES.\n:mappings => {\n            :position => {\n              :_ttl => {:enabled => true},\n              :properties => {\n                :title => {:type => 'string', :analyzer => 'snowball'},\n                :type => {:type => 'string'},\n                :locations => {\n                  :type => 'nested',\n                  :properties => {\n                    :city => {:type => 'string', :analyzer => 'whitespace'},\n                    :state => {:type => 'string', :analyzer => 'keyword'}\n                  }\n                },\n                :id => {:type => 'integer', :index => :not_analyzed, :include_in_all => false}\n              }\n            }\n          }\nOn Dec 28, 2012, at 2:43 PM, unsay notifications@github.com wrote:\n\n@loren, sorry about the noise. HTH. unsay@82dc19e\n\u2014\nReply to this email directly or view it on GitHub.\n. The problem I am having is really more of an ElasticSearch problem than a Tire problem, so I'm looking for help on it there and am closing the issue here:\nhttps://groups.google.com/forum/?fromgroups#!topic/elasticsearch/hKts0hO8NCQ\n. \n",
    "erlingwl": "I've just tested this, seems to work great, even with multiple levels of nesting.\n. ",
    "Xeago": "You should implement this in tire-contrib. It is more likely to be accepted there.\n. ",
    "jockee": "@brupm https://github.com/karmi/tire/pull/648 has been merged.\n. Added.\n. No worries. Thanks.\n. ",
    "bogn": "The issue is that Tire overwrites the _type column. ActiveRecord, MongoMapper and supposedly other mappers use that column for Single-Table-Inheritance. It should always return the class name as a string, e.g. \"SubThing\" for SubThing models, but Tire overwrites it with sub_thing which is done by the line that I referenced above. That is the reason MongoMapper's #count doesn't find it, ActiveRecord wouldn't be able to find it through the SubThing model either. _type is set to the response of elasticsearch which is not a good thing. In short _type has special meaning and it might be more safe if tire used more unique column names for its information. Something like:\nruby\ninstance._elasticsearch_index = response['_index'] if instance.respond_to?(:_elasticsearch_index=)\ninstance._elasticsearch_type = response['_type'] if instance.respond_to?(:_elasticsearch_type=)\ninstance._elasticsearch_version = response['_version'] if instance.respond_to?(:_elasticsearch_version=)\n. Wow, you are fast. Thanks for a great gem!\n. ",
    "wandenberg": "Hi,\nI moved the code from tasks.rb to utils.rb to be easier test it.\nIf you want I can apply the fix on tasks.rb only, just will stay without tests.\nCan I do that? The fix is simple but a good presentation be part of a good software.\nRegards,\nWandenberg\n. ",
    "rubydev": "Hi,\nI have this setting:\n\n ES_SETTINGS = {\n      analysis: {\n          analyzer: {\n              new_czech: {\n                  type: 'custom',\n                  tokenizer: 'standard',\n                  filter: %w(standard lowercase czech_stop czech_stemmer asciifolding)\n              }\n          },\n          filter: {\n              czech_stemmer: {\n                  type: 'stemmer',\n                  name: 'czech'\n              },\n              czech_stop: {\n                  type: 'stop',\n                  stopwords: ['_czech_']\n              }\n          }\n      }\n  }\n\nand I have problem with searching word \"medv\u011bda\" and \"medveda\". For example:\n\n  it \"searches with and without diacritics should by same\" do\n    Post.tire.index.delete\n    Post.create_elasticsearch_index\n    Post.make!(name: \"velk\u00e9ho ply\u0161ov\u00e9ho medv\u011bda\")\n    Post.make!(name: \"medveda nenazraneho australskeho mazliveho\")\n    Post.make!(name: \"Nic tu neni!\")\n    Post.tire.index.refresh\n    get :index, query: \"medved\", type: \"post\"\n    json_response.should_not be_empty\n    json_response.count.should eq(2)\n    get :index, query: \"medv\u011bd\", type: \"post\"\n    json_response.should_not be_empty\n    json_response.count.should eq(2)\n  end\n\n\n    indexes :name, type: 'multi_field', fields: {\n        english: {type: 'string', analyzer: 'snowball', boost: 10},\n        czech: {type: 'string', analyzer: 'new_czech', boost: 10}\n    }\n\nJson response count is always eq 1, why? If I removed \"czech_stemmer\" from settings and change words (in name fields) \"medv\u011bda\" and \"medveda\" to \"medv\u011bd\" and \"medved\" everything works fine. Why \"czech_stemmer\" probably has collision with \"asciifolding\"?\n. You are right, this works. Analyzer return token \"medved\".\nBut for this:\n\nTire.index(ES_INDEX_NAME).analyze('medveda', analyzer: 'new_czech')\n\nAnalyzer return token \"medvd\".\nBut that does not change the fact, that query \"medv\u011bd\" not return post with \"medveda nenazraneho australskeho mazliveho\".\n\n  it \"searches with and without diacritics should by same\" do\n    Post.tire.index.delete\n    Post.create_elasticsearch_index\n    Post.make!(name: \"velk\u00e9ho ply\u0161ov\u00e9ho medv\u011bda\")\n    Post.make!(name: \"medveda nenazraneho australskeho mazliveho\")\n    Post.make!(name: \"Nic tu neni!\")\n    Post.tire.index.refresh\n    get :index, query: \"medv\u011bd\", type: \"post\"\n    json_response.should_not be_empty\n    json_response.count.should eq(2) # return only one post\n  end\n\nmapping for name:\n\n indexes :name, type: 'multi_field', fields: {\n        english: {type: 'string', analyzer: 'snowball', boost: 10},\n        czech: {type: 'string', analyzer: 'new_czech', boost: 10}\n    }\n\nQuery:\n\n    search_fields = %w(name title keywords description terms about)\n    search_fields.map!{|field| \"#{field}.czech\"}\n    Tire.search(ES_INDEX_NAME, load: true) do\n      query do\n        boolean({:minimum_number_should_match => 1}) do\n          ...\n          must { string params[:query], fields: search_fields }\n          ...\n        end\n      end\n    end\n\nThanks for any advice!\nI use ruby 2.1.1, Rails 4.0.3, Tire 0.6.2, ES 0.90.10\n. Is any option for configuring hunspell for ES on the Heroku (bonsai)? I did not find any help. Thanks!\n. Now I've found, that depends on the filter order. This setting:\n\nES_SETTINGS = {\n      analysis: {\n          analyzer: {\n              new_czech: {\n                  type: 'custom',\n                  tokenizer: 'standard',\n                  filter: %w(czech_stop asciifolding standard lowercase czech_stemmer)\n              }\n          },\n          filter: {\n              czech_stemmer: {\n                  type: 'stemmer',\n                  name: 'czech'\n              },\n              czech_stop: {\n                  type: 'stop',\n                  stopwords: ['_czech_']\n              }\n          }\n      }\n  }\n\nreturns expected results.\n. ",
    "devilankur18": "Worked !!\n. ",
    "maxim": "Yeah this makes sense. I feel like there needs to be another abstraction on top of tire, which focuses on rails integration and popular use cases. Something worth considering. Appreciate your effort on this project.\n. I'm just talking off the top of my head, but what the hell.\nTire gives you the kind of sugar that follows ElasticSearch very closely. In ElasticSearch query is sort of an AST \u2014 so that's what tire gives you. A DSL to build the query as an AST.\nSo you have the following.\n- mapping which defines rules by which your models are loaded into index\n- to_indexed_json which defines what your model looks like when serialized and cached in json format\n- Search methods that build the query AST based on user input, and send it to ElasticSearch\n- Callbacks that are required to keep index in sync\nHowever, if you step away from ElasticSearch, and look at the popular web app use cases, you get something like following.\n1. I want to have a smart, flexible search across multiple text fields and associated models.\n2. If I have a field which is translated to multiple languages, I want my search query to find results across all possible translations of the field.\n3. I want to define facets on certain fields.\n4. I want search to stay in sync.\n5. I want to chain scopes.\n6. I want virtual fields to work just as well as persisted ones.\nPerhaps it could be done with syntax like this.\n``` ruby\nThe index here has a name :store_search which also defines a method\nProduct.store_search, and does one more thing (see include_index)\ndefine_index :store_search do\n  # Primary key field (e.g. id) should be indexed implicitly by default\n# All fields are automatically mapped to appropriate ElasticSearch types\n  # based on ActiveModel type, but overridable with options.\n  field :title\n  field :description\n# This would allow you to tell tire which code to run to get translations\n  # of the field\n  multilang_field :product_type, locales: [:en, :ru] do |locale|\n    I18n.translate(\"product_types.#{product_type}\", locale: locale)\n  end\n# Perhaps you could include indexes of other models with specific names,\n  # this would automatically do the right thing, make them objects for\n  # this index, etc\n  include_index :user, :store_search\n  include_index :comments, :store_search\nfacet :product_type\nend\n```\nThis declaration would give us a method like store_search(options = {}) which returns a scope. You see - all args in the method are optional. You could call it with store_search(query: 'foo', product_type: 'bar') or you could omit all options and get all results. This builds the query AST on the back end. \nFor people who want to avoid loading models from the db we could autogenerate another method: store_search_results.\nHow does store_search act as a scope? Simple, it gets all the results with store_search_results (accepting the same options), and does something like where(id: store_search_results(options).map(&:id)). This isn't the best, but it's sufficient, and could be improved.\nCallbacks would also be automatically generated for all involved models.\nSo all of the above considered, \n1. Combine mapping, to_indexed_json, Search, and Callbacks in one place with sane conventions that work for 80-90% of cases.\n2. Address specific use cases with helpers like multilang_field.\n3. Provide an ActiveModel scope.\n4. Still allow for results to be accessed without hitting the database.\n5. Still allow to bypass this entirely and go for pure tire.\n. I actually appreciate tire's approach a lot, and prefer its philosophy over the likes of ThinkingSphinx. It gives you enough flexibility to use ruby to say pretty much anything in ElasticSearch's \"language\". This is why I feel that tire shouldn't try to be anything more than that. It should act more as ElasticSearch's ruby bindings, and could even be called elasticsearch-ruby or something. An official ES-ruby driver if you will.\nI'd go further to suggest that it should be framework-agnostic and have no rails bindings. ActiveModel stuff should be extracted into a separate project. However, that separate project should be much more than mapping and a couple of modules. It should be something that builds on top of ruby bindings to resolve frequent web-app use cases. It could even provide view helpers for facets, sorting, and such. It could be opinionated on what type of analyzer you would need for a typical search field, it could take advantage of model relationships, etc. Similar libs could then be built on top of elasticsearch-ruby for sinatra, and other popular frameworks.\n. ",
    "reyjexter": "We initially thought of returning raw data and processing it on client but because time will be different per timezone, i think its best to do that on server for the following reasons:\n- It would save a little bit of battery on mobile phone to simply display human readable time than processing it. We thought of giving the server workload that could save battery time as much as we can.\n- We can change format anytime we want.\nThanks\n. ",
    "fonzo14": "OK, I will try.\n. ",
    "cameronkendall": "It was because my to_indexed_json was referencing all fields.  But explicitly stating which fields to index this fixed the issue.\n. Sorry, I didn't know where to post it so I posted on both stackoverflow and here.  But I've followed the examples and it still doesn't work.\nHere is where the \nstore.rb\nclass Store\n  include Mongoid::Document\n  include Mongoid::Timestamps::Created\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nfield :name, type: String\n  field :coordinates, type: Array\n#...\nindex coordinates: \"2d\"  # mongoid index\nindex_name \"stores-development\" # elasticsearch index\nmapping do\n    indexes :name, boost: 100\n    indexes :coordinates, type: 'geo_point'\n  end\nend\n. Whats the command to run that test?  I've never run a test from a ruby gem before.  I've only run tests that I created locally for my project.\n. I passed all of the mongoid_searchable_test.rb tests but when I ran all of the tests I received:\n734 tests, 722 passed, 8 failures, 4 errors, 0 skips, 1574 assertions\nHere are the failures and errors I did a cut and paste of:\n[cut]\n. Thanks! \n. i had an update_index command being called someplace else. \n. Found the answer - Just run a  User.find('50e75dda5d39f386f9000004').update_index. \n. This is in fact working.  I needed to use search_result.results.\n. ",
    "timon": "Thanks!\n. ",
    "richardrails": "Thx! It works. And this works also:\nruby\ntire.search load: true, page: p, per_page: pp do\n      query do\n        string q, default_operator: \"AND\"\n      end\n      filter :term, :is_public => true\n    end\nWhat do you think is the best way? Maybe that you have written.\n. ",
    "rcalderon": "The path returned is the correct path. You're right, the file isn't there (yet). If I comment out the call to encode the file there is no error and Paperclip will save the file. I'm not telling Paperclip to do anything else with the attachment except encoding it so that it's indexed.\n. @karmi sorry I missed your last update. I replaced paperclip with carrierwave and its working just fine. I might go back and troubleshoot paperclip asper your update, but for now, Tire is all good. Thanks for the help and the speedy replies!!\n. ",
    "abitdodgy": "I tried a curl command and got:\n$ curl http://192.168.1.138:9200\ncurl: (7) couldn't connect to host\nDoes Tire not start it up manually?\n. Yeah, that was it. I guess because Tire automatically installed and started Elastic Search for me when I added it, I expected it would start ES when I started my Rails app.\n. Hey there, thanks for getting back to me.\nI actually just solved this today. To be honest, I'm not 100% sure what solved it, but I essentially deleted everything and started my setup again. After manually creating an index in the terminal, everything worked fine. I had deleted an index manually during that day, which started a cascade of issues. In the end I think it was a combination of missteps and  lack of know-how on my part.\n. OK. I solved one problem. I can now search Item by product.properties.platform. I used delegate. But I can't filter Item by product.properties.platform. To avoid bashing my head on the wall, is that possible?\nThis does not work:\nruby\nfilter :term, product_platform: params[:platform] if params[:platform]\nHere's how I solved the other issue using delegate:\nruby\nclass Item < ActiveRecord::Base\n  belongs_to :product\n  delegate :platform, to: :product, prefix: true\nend\nHere's my item.to_indexed_json; notice product_platform is in the methods list:\nruby\n  def to_indexed_json\n    to_json(include: :product, methods: :product_platform)\n  end\n. I'm not sure if I understand you correctly:\ncurl -X GET 'http://localhost:9200/items/item/_search?size=12&pretty' -d '{\"sort\":[{\"created_at\":\"ASC\"}],\"filter\":{\"and\":[{\"term\":{\"availability\":\"in_stock\"}},{\"tem\":{\"product_platform\":\"Nintendo Wii\"}}]},\"size\":12}'\nThis is what that returns:\n{\n  \"took\" : 4,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 0,\n    \"max_score\" : null,\n    \"hits\" : [ ]\n  }\n}\n. Here's a search got a hit:\ncurl -X GET 'http://localhost:9200/items/item/_search?size=12&pretty' -d '{\"query\":{\"query_string\":{\"query\":\"Way\",\"default_operator\":\"AND\"}},\"sort\":[{\"created_at\":\"ASC\"}],\"filter\":{\"term\":{\"availability\":\"in_stock\"}},\"size\":12}'\n{\n  \"took\" : 1,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 1,\n    \"max_score\" : null,\n    \"hits\" : [ {\n      \"_index\" : \"items\",\n      \"_type\" : \"item\",\n      \"_id\" : \"2\",\n      \"_score\" : null, \"_source\" : {\"id\":2,\"product_id\":394,\"user_id\":1,\"condition\":\"New\",\"availability\":\"in_stock\",\"price\":\"68\",\"created_at\":\"2013-07-02T02:17:51.826Z\",\"updated_at\":\"2013-07-02T02:17:51.826Z\",\"product_name\":\"Way of the Samurai 3\",\"product_platform\":\"Xbox 360\",\"product\":{\"id\":394,\"name\":\"Way of the Samurai 3\",\"slug\":\"way-of-the-samurai-3\",\"category_id\":1,\"image_url\":\"2330\",\"properties\":{\"genre\":\"Action \\u0026 Adventure\",\"rating\":\"4.1\",\"platform\":\"Xbox 360\",\"subgenre\":\"Historic Action Adventure\",\"multiplayer\":\"\",\"online_play\":\"Y\",\"release_date\":\"2009-10-20\",\"classification\":\"16 Anos\",\"online_players\":\"\"}}},\n      \"sort\" : [ 1372731471826 ]\n    } ]\n  }\n}\n. OK, here's a sample. The data is being indexed (twice, once from the delegate methods, and once from the include call), but it's not working in the filter:\n'{\"id\":1,\"product_id\":234,\"user_id\":1,\"condition\":\"Good\",\"availability\":\"in_stock\",\"price\":\"119\",\"created_at\":\"2013-07-02T02:17:51.818Z\",\"updated_at\":\"2013-07-02T02:17:51.818Z\",\"product_name\":\"The\n  Godfather\",\"product_platform\":\"Playstation 3\",\"product\":{\"id\":234,\"name\":\"The\n  Godfather\",\"category_id\":1,\"image_url\":\"1280\",\"properties\":{\"genre\":\"Action\n  Adventure\",\"rating\":\"3.9\",\"platform\":\"Playstation 3\",\"subgenre\":\"Historic\n  Action Adventure\",\"developer\":\"Electornic Arts\",\"multiplay\":\"N\",\"publisher\":\"Electronic\n  Arts\",\"multiplayer\":\"\",\"online_play\":\"Y\",\"release_date\":\"2007-03-20\",\"online_players\":\"\"}}}'\n. I haven't defined any mappings. This is what the search looks like:\n``` ruby\n  def self.search(params)\n    tire.search(page: params[:page], per_page: 12) do\n      query do\n        string params[:query], default_operator: \"AND\"\n      end if params[:query].present?\n  filter :term, product_platform: params[:platform] if params[:platform]\nend\n\nend\ndef to_indexed_json\n    to_json(include: :product, methods: [:product_platform, :product_name])\n  end\n```\nIs it necessary to use mappings?\n. I'm sorry, could you just explain something to me: Why does not filter correctly by product_platform ?\n. I tried using an analyzer, and setup this mapping. It still produces 0 hits, with and without an analyzer. Could you point me some where to see examples of how to get this to run?\nI posted this on SO: http://stackoverflow.com/questions/17430899/tire-failing-to-filter-results\nruby\n  mapping date_detection: false do\n    indexes :id, type: 'integer'\n    indexes :user_id, type: 'integer'\n    indexes :product_id, type: 'integer'\n    indexes :product_platform, analyzer: 'keyword'\n  end\n. OK. I figured out the the filter will not work if we pass to it two words: Playstation 3, for example. It will work when I use a single word, and in lowercase, like playstation.\nHow do I modify this behaviour?\n. Thank you. Yes, the difference was the keyword analyzer. Before I close this, I have a quick question:\nMy Item model changes constantly. When an item is bought the in_stock column is changed, and the item must be removed/filtered from the index. Bought items are unique and must be removed. This can happen hundreds if not thousands of times per day. Is my use case something that ElasticSearch is meant to handle? Having to update the indecies so often? \n. Great, thank you.\n. ",
    "darrenboyd": "I'm seeing a similar error. I'm in a Rails application, with ActiveRecord Persistence.\nA JSON text must at least contain two octets!\n/Users/dboyd/.rvm/gems/ruby-1.9.3-p286/gems/json-1.7.5/lib/json/common.rb:155:in `initialize'\n/Users/dboyd/.rvm/gems/ruby-1.9.3-p286/gems/json-1.7.5/lib/json/common.rb:155:in `new'\n/Users/dboyd/.rvm/gems/ruby-1.9.3-p286/gems/json-1.7.5/lib/json/common.rb:155:in `parse'\n/Users/dboyd/.rvm/gems/ruby-1.9.3-p286/gems/multi_json-1.3.7/lib/multi_json/adapters/json_common.rb:7:in `load'\n/Users/dboyd/.rvm/gems/ruby-1.9.3-p286/gems/multi_json-1.3.7/lib/multi_json.rb:96:in `load'\n/Users/dboyd/.rvm/gems/ruby-1.9.3-p286/gems/tire-0.5.0/lib/tire/index.rb:324:in `logged'\n/Users/dboyd/.rvm/gems/ruby-1.9.3-p286/gems/tire-0.5.0/lib/tire/index.rb:21:in `exists?'\n/Users/dboyd/.rvm/gems/ruby-1.9.3-p286/gems/tire-0.5.0/lib/tire/model/indexing.rb:107:in `create_elasticsearch_index'\n/Users/dboyd/.rvm/gems/ruby-1.9.3-p286/gems/tire-0.5.0/lib/tire/model/indexing.rb:61:in `mapping'\n/Users/dboyd/Code/client_portal/app/models/content.rb:16:in `<class:Content>'\n/Users/dboyd/Code/client_portal/app/models/content.rb:5:in `<top (required)>'\nLine 5 of content.rb is...\nclass Content < ActiveRecord::Base\nLine 16 is...\ntire.mapping do\nThis is happening from Tire::Index#exists?, which gets logged.\nin Tire::Index#logged we see (starting at line 323)...\nruby\nbody = if @response\n  MultiJson.encode( MultiJson.load(@response.body), :pretty => Configuration.pretty)\nelse\n  MultiJson.encode( MultiJson.load(error.message), :pretty => Configuration.pretty) rescue ''\nend\nOf course, the request was curl -I \"http://localhost:9200/contents\", and it was successful.  Which means the body is going to be empty, which is invalid as JSON.\n. Yes.  When I change it to level: 'info', the problem goes away (at least, it doesn't present itself when running $ rake environment).\n. I've been trying to debug this by adding a test to index_test that fails.  I am running into some very odd behavior in the tests.  For example, the following test passes...\nruby\n  should \"fail on blank json\" do\n    MultiJson.load('')\n  end\nAnd yet, if I load the gem into an IRB environment, I see different behavior...\nruby\nnewton ~/src/tire master$ bundle exec irb -Ilib\nirb> require 'tire'\n  => true\nirb> MultiJson.load('')\nMultiJson::DecodeError: no object read at line 1, column 1 [load.c:1038]\n    from /Users/dboyd/.rvm/gems/ruby-1.9.3-p286/gems/multi_json-1.3.7/lib/multi_json/adapters/oj.rb:17:in `load'\n    from /Users/dboyd/.rvm/gems/ruby-1.9.3-p286/gems/multi_json-1.3.7/lib/multi_json/adapters/oj.rb:17:in `load'\n    from /Users/dboyd/.rvm/gems/ruby-1.9.3-p286/gems/multi_json-1.3.7/lib/multi_json.rb:96:in `load'\n    from (irb):2\n    from /Users/dboyd/.rvm/rubies/ruby-1.9.3-p286/bin/irb:16:in `<main>'\nThis latter behavior makes sense.  An empty String is not a valid JSON object (AFAIK).\nThoughts?\n. @Odaeus I believe the JSON libraries are behaving properly.  From what I understand, the empty string is not valid JSON.  At least, it doesn't appear that it is from the grammar found here: http://www.json.org/\nI believe this issue is with the caller  (the 'logged' method in Index).\n. I didn't realize that issue 'mentions' get propagated across forks... \nThe small change referenced above (here: https://github.com/darrenboyd/tire/commit/4ee579b79), should address this problem.  However, I'm reluctant to request a pull request for a couple reasons.\nI am unable to get the tests passing in my local environment.  This includes the index_test that is directly related (I have 1 failure).\nThere seems to be some dark magic going on in the test environment.  MultiJson.load behaves differently in the tire test environment than it does in irb.  For example...\n~/src/tire $ bundle exec irb\nirb> require 'multi_json'\n  => true\nirb> MultiJson.load('')\nMultiJson::DecodeError: no object read at line 1, column 1 [load.c:1038]\n    from /Users/dboyd/.rvm/gems/ruby-1.9.3-p327/gems/multi_json-1.3.7/lib/multi_json/adapters/oj.rb:17:in `load'\n    from /Users/dboyd/.rvm/gems/ruby-1.9.3-p327/gems/multi_json-1.3.7/lib/multi_json/adapters/oj.rb:17:in `load'\n    from /Users/dboyd/.rvm/gems/ruby-1.9.3-p327/gems/multi_json-1.3.7/lib/multi_json.rb:96:in `load'\n    from (irb):2\n    from /Users/dboyd/.rvm/rubies/ruby-1.9.3-p327/bin/irb:16:in `<main>'\nHowever, this test passes...\nruby\n  should \"fail on blank json\" do\n    MultiJson.load('')\n  end\nWhich effectively means testing my change to #logged is not possible, since the failure scenario will not fail in the test environment.\n. I am no unable to reproduce this.\nI'll close this now, and keep investigating.  It may be been an edge issue (maybe Rails?).\n. ",
    "Odaeus": "I can confirm this issue. Well done for working out it's the log level setting!\n. @darrenboyd I think this depends on the backend you are using. The one you are using there is \"OJ\", which is a development dependency of Tire. I think that because you are executing it within the Tire source directory with bundler it is picking up OJ and using that as the backend.\nMultiJson should probably be normalising this behaviour, rather than some backends producing exceptions and others simply nil.\n. ",
    "johnnyrodgers": "I can confirm this issue. I've set the following in an initializer:\nTire.configure do\n    logger 'elasticsearch.log', :level => 'debug'\nend\nBut when I start the Rails server (thin, in my case), I get this error:\n/Users/johnnyrodgers/.rvm/gems/ruby-1.9.2-p290/gems/json-1.7.5/lib/json/common.rb:155:in `initialize': A JSON text must at least contain two octets! (MultiJson::DecodeError)\n    from /Users/johnnyrodgers/.rvm/gems/ruby-1.9.2-p290/gems/json-1.7.5/lib/json/common.rb:155:in `new'\n    from /Users/johnnyrodgers/.rvm/gems/ruby-1.9.2-p290/gems/json-1.7.5/lib/json/common.rb:155:in `parse'\n    from /Users/johnnyrodgers/.rvm/gems/ruby-1.9.2-p290/gems/multi_json-1.3.6/lib/multi_json/adapters/json_common.rb:7:in `load'\n    from /Users/johnnyrodgers/.rvm/gems/ruby-1.9.2-p290/gems/multi_json-1.3.6/lib/multi_json.rb:93:in `load'\n    from /Users/johnnyrodgers/.rvm/gems/ruby-1.9.2-p290/gems/tire-0.5.1/lib/tire/index.rb:377:in `logged'\n    from /Users/johnnyrodgers/.rvm/gems/ruby-1.9.2-p290/gems/tire-0.5.1/lib/tire/index.rb:21:in `exists?'\n    from /Users/johnnyrodgers/.rvm/gems/ruby-1.9.2-p290/gems/tire-0.5.1/lib/tire/model/indexing.rb:107:in `create_elasticsearch_index'\n    from /Users/johnnyrodgers/.rvm/gems/ruby-1.9.2-p290/gems/tire-0.5.1/lib/tire/model/indexing.rb:61:in `mapping'\n    from /Users/johnnyrodgers/.rvm/gems/ruby-1.9.2-p290/gems/tire-0.5.1/lib/tire/model/search.rb:296:in `mapping'\nChanging the log level to 'info' fixes the error.\n. ",
    "daveworth": "@karmi, my pleasure!  The less thinking I have to do as a developer, as a side effect of documentation, the easier my life is!\nI am a huge fan of over-communication in these cases.  I might say \n\"Tire search results (and indexing) are fully compatible with common pagination libraries including will_paginate and Kaminari.  To use will_paginate [current example here].  To use Kaminari [new example here].\"\nThen the section below on indexing requires no changes at all.\nLet me know how I can help.\nThanks!\nDave\n. Excellent.  Will do!  I didn't realize that the rake task did that so I will make sure I understand this more thoroughly when writing the docs.  Thanks!\n. Sure, or you can hold me accountable to write this.  I should be able to do it this week.\n. That is an excellent point, that I hadn't considered.  I suppose some variety of mixin could add such a feature and then AR devs could opt-in.  That feels pretty heavy-weight but maybe lighter weight than adding Kaminari if you are never planning on displaying your objects in a paginated fashion in your app (which is admittedly rare).\nI agree with you.  Let's close this and encourage pagination tools.  :+1: \n. ",
    "weynsee": "It seems MultiJson loaded Oj when I ran the query, I just tried it with Yajl and got a different format for the value. For now I've worked around it by specifying a format recognized by ElasticSearch (i.e. strftime to ISO 8601). Do you think it'll be useful to do the formatting in Tire itself?\n. Yeah pre-processing values doesn't seem like an elegant solution for this. Another option I'm thinking of is if we can somehow specify the time format for the JSON library when serializing. I know oj supports it through the time_format option:\nMultiJson.encode(Time.now, time_format: :xmlschema)\nbut I doubt that this will work for all JSON libraries supported by MultiJson.\n. Yep, the extra options are ignored by the other json libraries. I just checked the other json libraries and their default format for Time is supported by ElasticSearch, so only oj has this problem.\n1.9.3p327 :001 > require 'multi_json'\n => true\n1.9.3p327 :002 > MultiJson.use :oj\n => MultiJson::Adapters::Oj\n1.9.3p327 :003 > MultiJson.encode Time.now\n => \"1355280070.200394000\"\n1.9.3p327 :004 > MultiJson.encode Time.now, time_format: :xmlschema\n => \"\\\"2012-12-12T10:41:14.717244000+08:00\\\"\"\n1.9.3p327 :005 > MultiJson.use :yajl\n => MultiJson::Adapters::Yajl\n1.9.3p327 :006 > MultiJson.encode Time.now, time_format: :xmlschema\n => \"\\\"2012-12-12 10:41:28 +0800\\\"\"\n1.9.3p327 :007 > MultiJson.use :json_gem\n => MultiJson::Adapters::JsonGem\n1.9.3p327 :008 > MultiJson.encode Time.now, time_format: :xmlschema\n => \"\\\"2012-12-12 10:41:48 +0800\\\"\"\n1.9.3p327 :009 > MultiJson.use :json_pure\n => MultiJson::Adapters::JsonPure\n1.9.3p327 :010 > MultiJson.encode Time.now, time_format: :xmlschema\n => \"\\\"2012-12-12 10:41:59 +0800\\\"\"\n1.9.3p327 :011 > MultiJson.use :ok_json\n => MultiJson::Adapters::OkJson\n1.9.3p327 :012 > MultiJson.encode Time.now, time_format: :xmlschema\n => \"\\\"2012-12-12 10:42:15 +0800\\\"\"\n. It's still happening with oj (2.0.3) with multi_json (1.5.0)\n. For 1.8 users, they would have to use the json gem.\n. Sounds good. Do you want to close this issue or keep it open for now?\n. ",
    "jeremygpeterson": "I'll dive into this one.\n. I reviewed MultiJson and they've resolved this issue December 11th.  It's not released yet, but we'll have very little to change in Tire.\n. I ran into this yesterday, when a create and update_attributes got rolled back.  So the entry in the database was deleted and the ElasticSearch document remained.\nI was thinking the 'after_callback' would have to check the database entry and update/delete ElasticSearch accordingly.\n. @karmi I see the caveat in the readme now, I didn't realize that including Tire::Model:Callbacks did not account for all possibilities on the active record model.  \nHow can I help on this issue, keeping the database synced with ElasticSearch really matters to me. I'm not sure if <after_commit> would handle my specific error.  As the rollback occurs during the creation process, but I'm open to implementing something for you to look at.\n. This is how I handled the transaction when a create is rolled back.\n``` ruby\nif base.respond_to?(:after_rollback)\n  index_delete = lambda {\n    @destroyed = true\n     tire.update_index\n  }\nbase.send :after_rollback, index_delete, on: :create\nend\n```\n. ",
    "cpuguy83": "Full Trace:\nrube\nLoadError: Expected /Users/bgoff/dev/emp2/app/models/tracker/db/service.rb to define Tracker::Db::Service\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194/gems/activesupport-3.2.8/lib/active_support/dependencies.rb:503:in `load_missing_constant'\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194/gems/activesupport-3.2.8/lib/active_support/dependencies.rb:192:in `block in const_missing'\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194/gems/activesupport-3.2.8/lib/active_support/dependencies.rb:190:in `each'\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194/gems/activesupport-3.2.8/lib/active_support/dependencies.rb:190:in `const_missing'\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194@global/gems/rake-0.9.2.2/lib/rake/ext/module.rb:36:in `const_missing'\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194/gems/activesupport-3.2.8/lib/active_support/inflector/methods.rb:230:in `block in constantize'\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194/gems/activesupport-3.2.8/lib/active_support/inflector/methods.rb:229:in `each'\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194/gems/activesupport-3.2.8/lib/active_support/inflector/methods.rb:229:in `constantize'\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194/gems/activesupport-3.2.8/lib/active_support/core_ext/string/inflections.rb:54:in `constantize'\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194/gems/tire-0.5.1/lib/tire/results/collection.rb:118:in `block in __get_results_with_load'\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194/gems/tire-0.5.1/lib/tire/results/collection.rb:112:in `each'\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194/gems/tire-0.5.1/lib/tire/results/collection.rb:112:in `__get_results_with_load'\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194/gems/tire-0.5.1/lib/tire/results/collection.rb:27:in `results'\n    from (irb):19\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194/gems/railties-3.2.8/lib/rails/commands/console.rb:47:in `start'\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194/gems/railties-3.2.8/lib/rails/commands/console.rb:8:in `start'\n    from /Users/bgoff/.rvm/gems/ruby-1.9.3-p194/gems/railties-3.2.8/lib/rails/commands.rb:41:in `<top (required)>'\n    from script/rails:6:in `require'\n    from script/rails:6:in `<main>'1.9.3p194 :020 >\n. Yeah, it looks like that's the issue.\nThe results have \"tracker/db/service\"  and when you do a camelize it changes it to 'Tracker::Db::Service'\n. Setting cache_classes to true in my development.rb seems to work.\nSo it looks like when tire goes to reload my class (since it's not cached by default in dev) it's giving the error.  Not a issue in production.\n. I lied, results are still not working.\n. I'm probably just going to rename my classes to Tracker::Db::, instead.\nDid a simple replace in my project and it seems to work just fine.  \nSeems a better solution than monkey patching code.  \n\nBrian Goff\nOn Monday, December 17, 2012 at 4:06, Karel Minarik wrote:\n\n@cpuguy83 (https://github.com/cpuguy83) Hello, any news on the situation?\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/karmi/tire/issues/513#issuecomment-11434523).\n. Yep, sorry.\n. \n",
    "yeehaa123": "Thanks for the quick reply! \nI tried both already, but neither one of them seem to work.\nMy entire model looks like this:\n```\nclass Reference\n  include Mongoid::Document\n  include Mongoid::Timestamps\n  include Mongoid::Slug\nfield   :title, type: String\n  field   :publication_date, type: DateTime\n  field   :tags, type: Array\n  field   :author_list, type: String\n  slug    :to_s\nattr_accessible :sessions, :title, :authors, :translators, :editors, \n              :publisher, :publication_date, :tags, :set_tags, :tags\nhas_and_belongs_to_many   :authors\n  has_and_belongs_to_many   :translators, class_name: \"Author\"\n  has_and_belongs_to_many   :editors, class_name: \"Author\"\n  has_and_belongs_to_many   :sessions\nvalidates_presence_of :title\ninclude Tire::Model::Search\n  include Tire::Model::Callbacks\n  index_name INDEX_NAME\nbefore_save :generate_author_list\n  before_save :set_tags \n  after_save do\n    Tire.index(INDEX_NAME).import(Reference.all)\n  end \ndef to_indexed_json\n    self.to_json\n  end\ndef self.search(params={})\n    tire.search(load: true, per_page: 50, type: nil) do\n      query do \n        boolean do\n          must { string params[:query], default_operator: \"AND\" } if params[:query].present?\n          must { term :tags, params[:tags] } if params[:tags].present?\n          must { terms :_type, [\"reference\", \"chapter\", \"magazine_article\", \"journal_article\", \"monograph\"] }\n       end\n     end\n     facet('tag_list') { terms :tags, order: 'term' }\n   end\nend\ndef set_tags\n  if tags\n    tags.each do |tag|\n      begin\n        Tag.find_by(name: tag)\n      rescue\n        Tag.create(name: tag)\n      end\n    end\n  end\nend\ndef to_s\n  title.titleize\nend\ndef generate_author_list\n  self.author_list = if authors.size > 0\n    authors.map(&:to_s).join(\". \")\n    else\n      \"\"\n    end\n  end\nend\n```\nWhat am I missing here?\n. ",
    "pi3r": "Hello, i understand that stub is better but, for example, when i have the following factory:\n``` ruby\n\nfactory :product do\n  ...\n  after(:create) do |product|\n    product.class.tire.index.refresh\n  end\nend\n```\nHow can i know if i need to refresh the index or not ? With a basic ugly piece of code i could do:\nruby\nif Product._save_callbacks.select{ |k| k.raw_filter == :_tire_update_index_callback }.present?\n  product.class.tire.index.refresh\nend\nThe main point for me is that using a method instead of a lambda gives visiblity.\nExample: \nWhen i call Model._save_callbacks (Callbacks as lambda), i get something like :\n@raw_filter=\n   #<Proc:0x00000005b67718@/home/pierre/.rbenv/versions/1.9.3-p125/lib/ruby/gems/1.9.1/gems/tire-0.5.1/lib/tire/model/callbacks.rb:21 (lambda)>>\nBut with the callbacks as method, i get something like:\n@raw_filter=:_tire_update_index_callback>\nWe can see instantly the purpose of the callback. \nCheers\n. ",
    "psykidellic": "I just started using ES/Tire and during development/staging to have this feature as we go on changing the mappings, as we go.\n. ",
    "nitsujw": "I use datamapper and ran into this with 0.5.1, if you \"include ActiveModel::Serialization\" that solves serializable_hash issue. \n. ",
    "pangkalizer": "im getting same issue .. trying to work geo_distance search with Tire::Model::Persistence .. \none thing i noticed is when you wrap the properties inside mapping -- your geo_point fields will be mapped with proper type BUT you can't query/filter or even sort them\nany idea?\n. @karmi its all good... dynamic mapping causes the problem... i thought i was recreating the mapping by calling MyModel.index.delete alone before populating the data... so i have to call MyModel.create_elasticsearch_index after index.delete to make sure everything is reset. however once you populate the index and pass hash value to one field causes dynamic mapping, causing weird behaviour on ES searching (tested outside tire). \nwhat is the proper way to disable dynamic mapping when using Persistence? i tried   :mapper => {  \"dynamic\" => \"false\"  }  but doesn't seems to work... it still generates dynamic mapping. i got rid of it by saving it as YAML or string. that good for now. Persistence model with geo_distance filter and sorting is now working.. \nReally cool gem you got there btw..\n. Solved...\nOn 27/12/2012 6:56 PM, \"Karel Minarik\" notifications@github.com wrote:\n\n@lister https://github.com/lister @pangkalizerhttps://github.com/pangkalizerSo, problems solved? Closing?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/issues/523#issuecomment-11703303.\n. \n",
    "lasssim": "sorry for my late reply. \ni was in kind of a hurry because i had to release the feature i was working on so i came up with this solution: i manually created my index by a method that i manually run a shell before the first start of the server of an environment. i also use this in my tests.\nhere is the snippet that creates the mapping for MyDocument.\nruby\nTire::Configuration.client.put(\n  \"#{Tire::Configuration.url}/my_index/#{MyDocument.document_type}/_mapping\",\n  MyDocument.mapping_to_hash.to_json\n)\ni had no time to investigate further, sry. but i might look at the issue again in the next weeks.\n. i know. sry, i didn't want to over complicate things in this ticket, which might have caused more confusion than i intended.\nthe index of MyDocument is actually a child index of another model. i didn't find any other solution for creating such a index without creating the index with my last snippet. the problem is, that if i create the index of my parent model first and then use create_elasticsearch_index to create the child type withing the parent index i get an IndexAlreadyExistsException from elasticsearch. modfying the index using the put method worked for me.\n. hi,\nsorry for the missing test. i just added them.\nthe tire_am_serializers gem only helps with autodetecting a serializer as far as i can tell from skimming the code.\nactive model serializers uses the read_attribute_for_serialization method to retrieve attributes from the objects it serializes. this is the part from active model serializers (activemodel-3.2.11/lib/active_model/serialization.rb:100)\n``` ruby\n      # Hook method defining how an attribute value should be retrieved for\n      # serialization. By default this is assumed to be an instance named after\n      # the attribute. Override this method in subclasses should you need to\n      # retrieve the value for a given attribute differently:\n      #\n      #   class MyClass\n      #     include ActiveModel::Validations\n      #\n      #     def initialize(data = {})\n      #       @data = data\n      #     end\n      #\n      #     def read_attribute_for_serialization(key)\n      #       @data[key]\n      #     end\n      #   end\n      #\n      alias :read_attribute_for_serialization :send\n```\nbr, simon\n. hi,\nthat is actually my point. i don't want to delete the records from the index. i want that they stay in the index. \nthe update_index method from tire removes destroyed records though.\nruby\n       def update_index\n          instance.run_callbacks :update_elasticsearch_index do\n            if instance.destroyed?\n              index.remove instance\n            else\n              response = index.store( instance, {:percolate => percolator} )\n              instance.tire.matches = response['matches'] if instance.tire.respond_to?(:matches=)\n              self\n            end\n          end\n        end\nbr, simon\n. hi,\nthanks for your replies.\nmy current solution is to implement the update_index method in my model like this:\nruby\n      def update_index\n        instance = self\n        instance.run_callbacks :update_elasticsearch_index do\n          response = index.store( instance, {:percolate => percolator} )\n          instance.tire.matches = response['matches'] if instance.tire.respond_to?(:matches=)\n          self\n        end\n      end\nwhich is working fine, but this is actually nothing want to do. i would miss changes on an update of the gem if the update_index method was changed.\nthe paranoid gem implements destroyed? like this:\nruby\n  def destroyed?\n    !self.deleted_at.nil?\n  end\n  alias :deleted? :destroyed?\nit also implements a method called paranoid? which could be used to find out if the model is paranoid. but i'm not sure if other paranoid-like gems implement this mehtod...\nbr, simon\n. ",
    "robinator": "Hey,\nJust loaded up the fresh code and it works great!  Thanks for the help and the quick response.\n. ",
    "tbk303": "I want to customize the JSON returned by my contoller, like this:\nrender json: search.results.as_json(except: [:foo, :bar])\nI know this is possible by passing those options to to_json as well, but the whole JSON-rendering-thing in Rails has changed (see the link I mentioned) and customizing the structure of JSON is now done through as_json while the actual rendering (i.e. producing a string) is done by to_json\n. Sorry, I kind of lost this. I directly feed the elasticsearch results through our REST API to the outside world. Hence I want to exclude some of the data (i.e. mostly the _* stuff like _score or _version). Through that overloaded as_json method, I am able to take control of the structure of the JSON returned by my API but leave the actual rendering to Rails (as explained in the link above, starting with Rails 3), like so:\nsearch = Foo.tire.search ...\nrender json: search.results.as_json(except: [:_score, :_version, :_explanation])\n. Great! Thank you!\n. ",
    "andywenk": "I just found out the following. When trying to set a type for id, like\nindexes :id, :type => 'integer', :index => :not_analyzed\nor\nindexes :id, :type => 'string', :index => :not_analyzed\nthe error is the same. But, when completely removing the line, it works. Did anything change in the API? I could not find it ... \n. Hi @ike-bloomfire and @karmi,\nthanks a lot for the hint. That worked for me.\nCheers\nAndy \n. sorry ... sure ... I close it now.\nThanks !\n. ",
    "sideshow": "See this pull request karmi/tire#535, which should fix this issue.\n. @karmi see if you can reproduce with gem \"mongoid\", \"~> 3.0.0\"\n. Good point. The BSON Object does include a JSON friendly object, but its in a format that is good for mongo and not for elasticsearch.  { \"$oid\": \"ABC...\"}. Perhaps it would be better to use document.id.is_a?(Moped::BSON::ObjectId) ?\n. ",
    "visasquare": "Got the answer by reading through the documents. we can set the pagination and acheive bulk import\n:per_page => 1000\nThanks Karmi!\n. Thank you for your response Karmi, we will try the multi_search query and let you know how it goes. \nWhat we are trying to achieve though is some way to get the total number of hits per index. If we run the query we mentioned above, we get following results (and facet counts)\nruby\n @response = {\n             \"took\" => 3898,\n        \"timed_out\" => false,\n          \"_shards\" => {\n                 \"total\" => 10,\n            \"successful\" => 10,\n                \"failed\" => 0\n        },\n             \"hits\" => {\n                \"total\" => 244,\n            \"max_score\" => 0.0,\n                 \"hits\" => []\n        },\nWhat we are looking for is total counts for each index, so in this case can be 4 hits on index 1 and 6 hits on index2.\nOur apologies, if we are asking very basic questions but we are just getting started with elastic search and are finding tire gem very very helpful. \nWe will also change from string query to match and let you know if we run into any issues.\nThanks for all your help.\n. Thank you so much! Works great! Thats exactly what we were looking for. The\nfacet on _index helped us with issuing multiple queries to elastic search.\nThanks again for all your help.\n. Thanks Karmi, but in the elastic search document it mentions that following, maybe we can check with Shay what it means? \nNote, the terms stats can work with multi valued key fields, or multi valued value fields, but not when both are multi valued (as ordering is not maintained).\nWe will look into implementing a script facet. Thank you so much for all your help!\n. Our apologies for the delayed response. Sure, we are trying some other workaround including one suggested by you. We will keep the group posted.\n. ",
    "concept47": "So I took a look at the source code and apparently this isn't possible in the current state. I'm wondering if anyone else would be interested in seeing this particular feature in Tire.\n. ",
    "matthewford": "@karmi is this always the case? when I tried this I was seeing that second mapping loaded did not include the analysers \n. Thanks, yes this caught me out too. I've now moved the index creation into a rake task.\n. Changing the text to use match throws the following error: \n500 : {\"error\":\"SearchPhaseExecutionException[Failed to execute phase [query_fetch], total failure; shardFailures {[5dElDQ3dRuil76eZpwNhSw][government][0]: SearchParseException[[government][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\\"query\\\":{\\\"bool\\\":{\\\"should\\\":[{\\\"match\\\":{\\\"title\\\":{\\\"query\\\":\\\"schools\\\",\\\"type\\\":\\\"phrase_prefix\\\",\\\"operator\\\":\\\"and\\\",\\\"analyzer\\\":\\\"query_default\\\",\\\"boost\\\":10,\\\"fuzziness\\\":0.5}}},{\\\"query_string\\\":{\\\"query\\\":\\\"schools\\\",\\\"default_operator\\\":\\\"and\\\",\\\"analyzer\\\":\\\"query_default\\\"}}]}},\\\"filter\\\":{\\\"and\\\":[{\\\"or\\\":[{\\\"term\\\":{\\\"format\\\":\\\"publication\\\"}},{\\\"term\\\":{\\\"format\\\":\\\"statistical_data_set\\\"}},{\\\"term\\\":{\\\"format\\\":\\\"consultation\\\"}}]},{\\\"range\\\":{\\\"public_timestamp\\\":{\\\"to\\\":\\\"2013-01-31\\\"}}}]},\\\"size\\\":20,\\\"from\\\":0}]]]; nested: QueryParsingException[[government] No query registered for [match]]; }]\",\"status\":500}\nThe logged query\n{\n      \"query\": {\n        \"bool\": {\n          \"should\": [\n            {\n              \"match\": {\n                \"title\": {\n                  \"query\": \"schools\",\n                  \"type\": \"phrase_prefix\",\n                  \"operator\": \"and\",\n                  \"analyzer\": \"query_default\",\n                  \"boost\": 10,\n                  \"fuzziness\": 0.5\n                }\n              }\n            },\n            {\n              \"query_string\": {\n                \"query\": \"schools\",\n                \"default_operator\": \"and\",\n                \"analyzer\": \"query_default\"\n              }\n            }\n          ]\n        }\n      },\n      \"filter\": {\n        \"and\": [\n          {\n            \"or\": [\n              {\n                \"term\": {\n                  \"format\": \"publication\"\n                }\n              },\n              {\n                \"term\": {\n                  \"format\": \"statistical_data_set\"\n                }\n              },\n              {\n                \"term\": {\n                  \"format\": \"consultation\"\n                }\n              }\n            ]\n          },\n          {\n            \"range\": {\n              \"public_timestamp\": {\n                \"to\": \"2013-01-31\"\n              }\n            }\n          }\n        ]\n      },\n      \"size\": 20,\n      \"from\": 0\n    }\n. ",
    "filiptepper": "Right, forgot about this. eh\nTried a couple of options, including inspecting the objects and HTTP requests. Always highlight was nil. Is that an issue with tire request or elasticsearch response?\n@results.each do |result|\n  p result.highlight\nend\n. Thanks @karmi and @ike-bloomfire, fixing the query actually solved my issue!\nHere's the working query:\nruby\n@results = Post.search :page => current_page, :per_page => 20 do\n  query do\n    match [:body, :title], q\n  end\n  highlight :body, :options => { :fragment_size => 50 }\nend\nThank you again for all your hard work!\n. ",
    "msonnabaum": "Added test. Passes with the PR, throws an error without.\n. I think that's a totally reasonable solution for now, and then it can be solved properly when ActiveModel is decoupled later on.\n. Hmm, I see this is actually covered in the README. Still, this is pretty unexpected behavior.\n. For this issue, I think it might be reasonable to say that if the index exists and there is a custom mapping defined, we check if a custom mapping exists, and if not, add it.\nAs a workaround, I just added all my mappings to a single \"mapping\" method on another object and run this when the application starts:\nruby\n  # Make sure all the custom mappings exist, and if not, add them.\n  def check_mappings\n    index_mappings = Tire.index(index_name).mapping\n    mappings.each do |type, mapping|\n      if index_mappings[type.to_s].nil?\n        Tire::Configuration.client.put Tire.index(index_name).url + \"/#{type}/_mapping\", {\n          type => mappings[type]\n        }.to_json\n      end\n    end\n  end\nIt doesn't seem like much work though to do this on a per-model basis.\n. Well, my thinking was, updating mapping is simply not supported, since that's not supported in elasticsearch, whereas adding new mappings to an index is.\nSeems sensible for tire to enforce the same limitations.\n. Right, you can add a new field but you can't change an existing mapped field. So maybe it'd be ideal to support that case as well.\nIf you agree that that's a worthwhile approach I can try to put together a PR.\n. No worries, we can revisit this with whatever get's built on the new lib.\n. ",
    "shreyas-satish": "All the tests in unit/results_collection_test.rb pass. \n. Fixed now. I had to change the options hash and set the 'size' and the 'from' values to\nruby\noptions = {page: params[:page], from: offset, size: 10, load: true}\nand had to define an offset method for the 'from' in the options hash.\n``` ruby\n  def offset\n    POSTS_PER_PAGE = 10\n    page = (params[:page] || 1).to_i - 1\n    (POSTS_PER_PAGE  * page)\n  end\n```\nShould I mention this in the docs for Tire?\nThanks.\n. ",
    "lorgio": "I think the issue is that you may be passing in params[:page] as a string.\n. ",
    "mhui": "I have the same problem. Passing params[:page] as string or integer makes no difference\n. ",
    "vsespb": "I've read wiki. We are using index#refresh but seems that is not enough.\nOur problem is:\nWe use tire in integration tests (we don't mock it)\nAt least last half of year we have random test failures on different servers, on CI server and development machines. On some machines it's 100% failures for some tests, for other it's random. And we have more failures when running tests in parallel (parallel_test gem). (index names of course different for parallel processes)\nThere are two kind of failures:\na) HTTP 500 with \"no active shards\" message\nb) Wrong items in search results. for example we expect one item, but there are two with same ID (and other fields).\nSeems that Adding sleep(1) after tire.index.refresh helps, at least I did not ever see that it did not help.\nAlso seems the following code helps (instead of sleep):\nTire::Configuration.client.get \"#{Tire::Configuration.url}/_cluster/health?wait_for_status=yellow&timeout=1s&wait_for_active_shards=1\"\nI found some evidences that that health requests should be indeed used:\n1) https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/wPKUJXiG2mw\n(it's the author of elasticsearch, who answered this ticket)\n2) http://grokbase.com/t/gg/elasticsearch/12ag6njpyz/no-active-shards-in-tests-using-the-java-api\n. Problem happens also when running single-process tests, but rarely.\nWe are using refresh. Failures appear even if refresh used right before search.\nOk, I'll check Tire integration tests, and check if we have signle shard.\n. Yes, we definitely are using different index names per each concurrent process.\nWell, ok, thanks for the info. I will check all again. You can close the ticket probably, I will re-open and come with proof-of-concept minimal code in case nothing help.\n. Hm, seems latest Tire's integration test is failed on my box (one of test random failing)\n1) \n[REQUEST FAILED] curl -X GET 'http://localhost:9200/active_record_articles/active_record_article/_search?size=10&pretty' -d '{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"[x\"\n    }\n  },\n  \"size\": 10\n}'\n2)\n```\n[REQUEST FAILED] curl -X GET 'http://localhost:9200/index-aliased/_search?pretty' -d '{\n  \"query\": {\n    \"match_all\": {\n}\n\n}\n}'\n```\n3)\n``\n    ERROR (8:00:30.631) test: Percolator when percolating a document should return an array of matching query names. \n          undefined methodsort' for nil:NilClass\n        @ test/integration/percolator_test.rb:66:in block (3 levels) in <class:PercolatorIntegrationTest>'\n          /home/vse/.rvm/gems/ruby-1.9.3-p0@set1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:incall'\n          /home/vse/.rvm/gems/ruby-1.9.3-p0@set1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:in block in create_test_from_should_hash'\n          /home/vse/.rvm/gems/ruby-1.9.3-p0@set1/gems/mocha-0.10.5/lib/mocha/integration/mini_test/version_230_to_262.rb:28:inrun'\n FAIL (8:00:35.473) test: Percolator when percolating a document should return an empty array when no query matches. \n      <[]> expected but was\n      <nil>.\n    @ test/integration/percolator_test.rb:61:in `block (3 levels) in <class:PercolatorIntegrationTest>'\n      /home/vse/.rvm/gems/ruby-1.9.3-p0@set1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:in `call'\n      /home/vse/.rvm/gems/ruby-1.9.3-p0@set1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:in `block in create_test_from_should_hash'\n      /home/vse/.rvm/gems/ruby-1.9.3-p0@set1/gems/mocha-0.10.5/lib/mocha/integration/mini_test/version_230_to_262.rb:28:in `run'\n\n```\n4) \nthis test is random failing:\nPASS (8:00:26.640) test: Percolator when percolating a document should return an array of matching query names for specific percolated queries.\nsometimes\n``\nFAIL (10:00:55.617) test: Percolator when percolating a document should return an array of matching query names for specific percolated queries. \n          <[\"weather\"]> expected but was\n          <nil>.\n        @ test/integration/percolator_test.rb:71:inblock (3 levels) in '\n          /home/vse/.rvm/gems/ruby-1.9.3-p0@set1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:in call'\n          /home/vse/.rvm/gems/ruby-1.9.3-p0@set1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:inblock in create_test_from_should_hash'\n          /home/vse/.rvm/gems/ruby-1.9.3-p0@set1/gems/mocha-0.10.5/lib/mocha/integration/mini_test/version_230_to_262.rb:28:in `run'\n```\nelasticsearch version:   \"version\" : {\n    \"number\" : \"0.19.9\",\n    \"snapshot_build\" : false\n  },\nunit tests seems ok.\n. However, no, unit tests have random failures too:\nTire::IndexTest\n     PASS (0:00:00.013) test: Index analyze support should properly encode format parameter. \n     PASS (0:00:00.014) test: Index analyze support should properly encode parameters for analyzer. \n     PASS (0:00:00.014) test: Index analyze support should properly encode parameters for field. \n     PASS (0:00:00.014) test: Index analyze support should send text to the Analyze API. \n     PASS (0:00:00.014) test: Index mapping should create index with mapping. \n     PASS (0:00:00.015) test: Index mapping should return the mapping. \n[ERROR] 400 > {\"error\":\"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0\"}, retrying (1)...\n[ERROR] 400 > {\"error\":\"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0\"}, retrying (2)...\n[ERROR] 400 > {\"error\":\"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0\"}, retrying (3)...\n[ERROR] 400 > {\"error\":\"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0\"}, retrying (4)...\n[ERROR] 400 > {\"error\":\"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0\"}, retrying (5)...\n[ERROR] Too many exceptions occured, giving up. The HTTP response was: 400 > {\"error\":\"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0\"}\n400 : {\"error\":\"MapperParsingException[mapping [model_with_incorrect_mapping]]; nested: MapperParsingException[No handler for type [boo] declared on field [title]]; \",\"status\":400}\n  * DEFERRED: Finders should raise error when document is not found. \n  * DEFERRED: Persistent model attribute methods should allow to set deeply nested attributes on initialization. \n  * DEFERRED: Persistent model attribute methods should allow to set deeply nested attributes on update. \n  * DEFERRED: Model::Search should not define destroyed? if class already implements it.\n. I added \"sleep(1)\" after\ncontext \"when percolating a document\" do                                                                                                                                                                                                                                 \n        setup do                                                                                                                                                                                                                                                               \n          @index.register_percolator_query('alert') { string 'warning' }                                                                                                                                                                                                       \n          @index.register_percolator_query('gantz') { string '\"y u no match\"' }                                                                                                                                                                                                \n          @index.register_percolator_query('weather', :tags => ['weather']) { string 'severe' }                                                                                                                                                                                \n          Tire.index('_percolator').refresh     \nsleep(1)                                                                                                                                                                                                                                                                       \n        end\nthat fixed my tests!\nPASS (0:00:04.309) test: Percolator when percolating a document should return an array of matching query names for specific percolated queries. \n     PASS (0:00:09.585) test: Percolator when percolating a document should return an array of matching query names. \n     PASS (0:00:14.885) test: Percolator when percolating a document should return an empty array when no query matches\nThat is absolutely same situation like I have with my Rails app (note - on several different servers).\nI tried with ruby 1.9.3p0 and 1.9.3.p194, elasticsearch versions 0.19.3 and 0.19.9\n(always using RVM).\nElasticsearch setup - not sure, probably default configuration.\nMaybe you have idea what this can be related to?\n. So, I installed it into empty virtual machine, same failures. Same - fixed by sleep(1)\n(Empty Ubuntu 12.04 LTS 64bit, Desktop edition. (in VirtualBox, host os is Ubuntu 10.04 64bit))\nbelow is my install script + new failure log.\n```\nunder root\napt-get install -y build-essential git libyaml-dev  libssl-dev libreadline5-dev zlib1g-dev libcurl4-openssl-dev libsqlite3-dev redis-server\ncd /tmp\nwget http://ftp.ruby-lang.org/pub/ruby/1.9/ruby-1.9.3-p327.tar.gz && tar xvzf ruby-1.9.3-p327.tar.gz && cd ruby-1.9.3-p327\n./configure && make -j 4 && make install\nmake test is ok too\ngem update --system\ngem update\ngem install bundle\naptitude install openjdk-6-jre\naptitude install elasticsearch\nunder user\ncd ~\nmkdir tiretest && cd tiretest\ngit clone https://github.com/karmi/tire.git\nrevision e76fcf214f1fed648155593920794a79daa3dce5)\ncd tire\nbundle install\nbundle exec rake test:integration\n```\n- one new failure (and I think some older failures disappear)\n``\nTire::IndexStoreIntegrationTest\n     PASS (1:00:56.344) test: Removing documents from the index should remove document from the index. \n     PASS (1:00:57.390) test: Retrieving documents from the index should retrieve document from the index. \n     FAIL (1:00:58.490) test: Retrieving documents from the index should return nil when retrieving missing document. \n          Expected <Item id: nil, _type: nil, _index: nil, _version: nil> to be nil.\n        @ test/integration/index_store_test.rb:105:inblock (2 levels) in '\n          /usr/local/lib/ruby/gems/1.9.1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:in call'\n          /usr/local/lib/ruby/gems/1.9.1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:inblock in create_test_from_should_hash'\n          /usr/local/lib/ruby/gems/1.9.1/gems/mocha-0.10.5/lib/mocha/integration/mini_test/version_230_to_262.rb:28:in `run'\n```\nUnit tests are without failures.\nNote: I've used default elasticsearch and redis configs. Elasticsearch version is \n{\n  \"ok\" : true,\n  \"status\" : 200,\n  \"name\" : \"Beaubier, Jeanne-Marie\",\n  \"version\" : {\n    \"number\" : \"0.19.12\",\n    \"snapshot_build\" : false\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\nJava version:\nOpenJDK Runtime Environment (IcedTea6 1.11.5) (6b24-1.11.5-0ubuntu1~12.04.1)\nOpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)\nAny ideas where to move next? Please advice.\n. I found similar ElasticSearch issue (however I can't reproduce it)\nhttps://github.com/elasticsearch/elasticsearch/issues/698 ( https://gist.github.com/b5775ddf2160cf9c8dd7 )\n(and maybe few others - not 100% sure that it's related)\nhttps://github.com/elasticsearch/elasticsearch/issues/233\nhttps://github.com/elasticsearch/elasticsearch/issues/2316\nhttps://github.com/elasticsearch/elasticsearch/issues/2267\nhttps://github.com/elasticsearch/elasticsearch/issues/699\n. Not sure how to get number of shards for that index? Do you mean need to add to Tire integration test code some request which get that number?\nI have default config:\n```\n grep number_of_shards elasticsearch.yml \nindex.number_of_shards: 5\nindex.number_of_shards: 1\nThe \"number_of_shards\" is a one-time setting for an index.\ngrep number_of_replicas elasticsearch.yml \nindex.number_of_replicas: 1\nindex.number_of_replicas: 0\nThe \"number_of_replicas\" can be increased or decreased anytime,\n```\n(i.e. commented out)\nI think that means 5 shards, 1 replicas\n```\nSet the number of shards (splits) of an index (5 by default):\nSet the number of replicas (additional copies) of an index (1 by default):\n```\nHealth requests returns:\ncurl http://localhost:9200/_cluster/health\n{\"cluster_name\":\"elasticsearch\",\"status\":\"yellow\",\"timed_out\":false,\"number_of_nodes\":1,\"number_of_data_nodes\":1,\"active_primary_shards\":6,\"active_shards\":6,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":5}\n. I added to Tire test code:\nTire.index('_percolator').refresh\n puts `curl http://localhost:9200/_cluster/health`\n(instead of sleep)\nso, I have:\n{\"cluster_name\":\"elasticsearch\",\"status\":\"red\",\"timed_out\":false,\"number_of_nodes\":1,\"number_of_data_nodes\":1,\"active_primary_shards\":36,\"active_shards\":36,\"relocating_shards\":0,\"initializing_shards\":4,\"unassigned_shards\":41}\n    ERROR (0:00:02.671) test: Percolator when percolating a document should return an array of matching query names. \n          undefined method `sort' for nil:NilClass\n        @ test/integration/percolator_test.rb:69:in `block (3 levels) in <class:PercolatorIntegrationTest>'\n          /usr/local/lib/ruby/gems/1.9.1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:in `call'\n          /usr/local/lib/ruby/gems/1.9.1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:in `block in create_test_from_should_hash'\n          /usr/local/lib/ruby/gems/1.9.1/gems/mocha-0.10.5/lib/mocha/integration/mini_test/version_230_to_262.rb:28:in `run'\n{\"cluster_name\":\"elasticsearch\",\"status\":\"red\",\"timed_out\":false,\"number_of_nodes\":1,\"number_of_data_nodes\":1,\"active_primary_shards\":36,\"active_shards\":36,\"relocating_shards\":0,\"initializing_shards\":4,\"unassigned_shards\":41}\n     FAIL (0:00:01.544) test: Percolator when percolating a document should return an array of matching query names for specific percolated queries. \n          <[\"weather\"]> expected but was\n          <nil>.\n        @ test/integration/percolator_test.rb:74:in `block (3 levels) in <class:PercolatorIntegrationTest>'\n          /usr/local/lib/ruby/gems/1.9.1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:in `call'\n          /usr/local/lib/ruby/gems/1.9.1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:in `block in create_test_from_should_hash'\n          /usr/local/lib/ruby/gems/1.9.1/gems/mocha-0.10.5/lib/mocha/integration/mini_test/version_230_to_262.rb:28:in `run'\ni.e. status is \"red\"\n. Hm, I uncommented\nindex.number_of_shards: 1\nindex.number_of_replicas: 0\nstopped and started service.\nps aux:\n115       3904  3.3  2.2 1948036 90824 ?       Sl   13:09   0:03 /usr/lib/jvm/java-6-openjdk-amd64//bin/java -Xms256m -Xmx1g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Delasticsearch -Des.pidfile=/var/run/elasticsearch.pid -Des.path.home=/usr/share/elasticsearch -cp :/usr/share/elasticsearch/lib/elasticsearch-0.19.12.jar:/usr/share/elasticsearch/lib/*:/usr/share/elasticsearch/lib/sigar/* -Des.default.config=/etc/elasticsearch/elasticsearch.yml -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.work=/tmp/elasticsearch -Des.default.path.conf=/etc/elasticsearch org.elasticsearch.bootstrap.ElasticSearch\n(i.e. ran with /etc/elasticsearch/elasticsearch.yml)\n```\ngrep number_of /etc/elasticsearch/elasticsearch.yml\nindex.number_of_shards: 5\nindex.number_of_replicas: 1\nindex.number_of_shards: 1\nindex.number_of_replicas: 0\nThe \"number_of_shards\" is a one-time setting for an index.\nThe \"number_of_replicas\" can be increased or decreased anytime,\n```\nhowever I see still more than one shard:\ncurl http://localhost:9200/_cluster/health\n{\"cluster_name\":\"elasticsearch\",\"status\":\"yellow\",\"timed_out\":false,\"number_of_nodes\":1,\"number_of_data_nodes\":1,\"active_primary_shards\":6,\"active_shards\":6,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":5}\nfirst test started working, other two still failed:\n``\n{\"cluster_name\":\"elasticsearch\",\"status\":\"red\",\"timed_out\":false,\"number_of_nodes\":1,\"number_of_data_nodes\":1,\"active_primary_shards\":12,\"active_shards\":12,\"relocating_shards\":0,\"initializing_shards\":1,\"unassigned_shards\":5}\n    ERROR (0:00:02.824) test: Percolator when percolating a document should return an array of matching query names. \n          undefined methodsort' for nil:NilClass\n        @ test/integration/percolator_test.rb:70:in block (3 levels) in <class:PercolatorIntegrationTest>'\n          /usr/local/lib/ruby/gems/1.9.1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:incall'\n          /usr/local/lib/ruby/gems/1.9.1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:in block in create_test_from_should_hash'\n          /usr/local/lib/ruby/gems/1.9.1/gems/mocha-0.10.5/lib/mocha/integration/mini_test/version_230_to_262.rb:28:inrun'\n```\n. Dropped indexes, restarted.\n$ curl -X DELETE localhost:9200\n{\"ok\":true,\"acknowledged\":true}\nHealth ok:\n$ curl http://localhost:9200/_cluster/health\n{\"cluster_name\":\"elasticsearch\",\"status\":\"green\",\"timed_out\":false,\"number_of_nodes\":1,\"number_of_data_nodes\":1,\"active_primary_shards\":0,\"active_shards\":0,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":0}\nbut tests not ok (and activeshards still big)\n``\n{\"cluster_name\":\"elasticsearch\",\"status\":\"green\",\"timed_out\":false,\"number_of_nodes\":1,\"number_of_data_nodes\":1,\"active_primary_shards\":8,\"active_shards\":8,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":0}\n     PASS (0:00:01.829) test: Percolator when percolating a document should return an array of matching query names for specific percolated queries. \n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   222  100   222    0     0  16081      0 --:--:-- --:--:-- --:--:-- 27750\n{\"cluster_name\":\"elasticsearch\",\"status\":\"red\",\"timed_out\":false,\"number_of_nodes\":1,\"number_of_data_nodes\":1,\"active_primary_shards\":7,\"active_shards\":7,\"relocating_shards\":0,\"initializing_shards\":1,\"unassigned_shards\":0}\n    ERROR (0:00:02.904) test: Percolator when percolating a document should return an array of matching query names. \n          undefined methodsort' for nil:NilClass\n        @ test/integration/percolator_test.rb:70:in block (3 levels) in <class:PercolatorIntegrationTest>'\n          /usr/local/lib/ruby/gems/1.9.1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:incall'\n          /usr/local/lib/ruby/gems/1.9.1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:in block in create_test_from_should_hash'\n          /usr/local/lib/ruby/gems/1.9.1/gems/mocha-0.10.5/lib/mocha/integration/mini_test/version_230_to_262.rb:28:inrun'\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   222  100   222    0     0  13607      0 --:--:-- --:--:-- --:--:-- 20181\n{\"cluster_name\":\"elasticsearch\",\"status\":\"red\",\"timed_out\":false,\"number_of_nodes\":1,\"number_of_data_nodes\":1,\"active_primary_shards\":7,\"active_shards\":7,\"relocating_shards\":0,\"initializing_shards\":1,\"unassigned_shards\":0}\n     FAIL (0:00:04.018) test: Percolator when percolating a document should return an empty array when no query matches. \n          <[]> expected but was\n          .\n        @ test/integration/percolator_test.rb:65:in block (3 levels) in <class:PercolatorIntegrationTest>'\n          /usr/local/lib/ruby/gems/1.9.1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:incall'\n          /usr/local/lib/ruby/gems/1.9.1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:in block in create_test_from_should_hash'\n          /usr/local/lib/ruby/gems/1.9.1/gems/mocha-0.10.5/lib/mocha/integration/mini_test/version_230_to_262.rb:28:inrun'\n```\n. It helps much, but after already I started that ticket, we found that it's actually not helping in 100% cases.\nNow I reproduced it:\nWhen I run test suite I am getting far less errors:\nbut here is one\n``\n{\"cluster_name\":\"elasticsearch\",\"status\":\"green\",\"timed_out\":false,\"number_of_nodes\":1,\"number_of_data_nodes\":1,\"active_primary_shards\":8,\"active_shards\":8,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":0}\n     FAIL (0:00:37.403) test: Bulk should extract the routing value from documents. \n          <1> expected but was\n          <2>.\n        @ test/integration/bulk_test.rb:34:inblock (2 levels) in '\n          /usr/local/lib/ruby/gems/1.9.1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:in call'\n          /usr/local/lib/ruby/gems/1.9.1/gems/shoulda-context-1.0.1/lib/shoulda/context/context.rb:398:inblock in create_test_from_should_hash'\n          /usr/local/lib/ruby/gems/1.9.1/gems/mocha-0.10.5/lib/mocha/integration/mini_test/version_230_to_262.rb:28:in `run'\n PASS (0:00:37.983) test: Bulk should store a collection of documents and refresh the index. \n PASS (0:00:41.498) test: Bulk should timeout when consistency factor is not met.\n\n```\n(status is green)\nbulk_test is modified\n``\n      should \"extract the routing value from documents\" do\n        @index.bulk_store [ { id: '1', title: 'A', _routing: 'a'}, { id: '2', title: 'B', _routing: 'b'} ]\n        @index.refresh\n putscurl http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=1s`\n```\n. OK, I understand that, several developers tried to fixed, but no results.\nI'll try investigate more, but few questions:\n1) do you see evidences that there is there something wrong with my elasticsearch setup ? For example number of shards in health request does\nnot match config etc ?\n2) Are you the maintainer of Elasticsearch? It might be related to elasticsearch, not Tire. Should I create ticket on elasticsearch issues page?\n3) Could you please give some information about you environment (which linux distro?, is that UTC timezone?,\nfull elasticsearch config for test, amd64 vs x86?, java version, SSD vs HDD) ?\nI control now at least 3 machines where this problem appears (AFAIK there are even more).\nOn two of them I tested Tire integration tests and it fail. One of them is empty, new virtual machine.\nLet me describe what is different and what is common in those machines' setup:\nDifferent:\nOS: Ubuntu 12.04, Ubuntu 10.04\nJAVA: Java HotSpot(TM) 64-Bit Server VM (build 20.12-b01, mixed mode), OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)\nELASTICSEARCH: 0.19.3, 0.19.9, 0.19.12\nREDIS  2.4.14, 2.2.12\nRUBY 1.9.3p0, 1.9.3p194, 1.9.3p327\nLinux kernel 3.2.0-24-generic 3.2.0-32-generic 2.6.38-16-generic\nCPU  i7-3930K i7-2600 \nMemory 4, 16 or 64 GB\nLocale LANG EN, RU\nTimezone: MSK, CET\nRVM: both: RVM and not RVM\nCommon:\nELASTISEARCH CONFIG: default\nCPU: Sandy Bridge\nArchitecture: x64\nRedis config: default\nLinux distro: Ubuntu\nInstall method: Debian package\nTimezone: Not UTC\nHDD: HDD, NOT SDD\nRuby 1.9.3.x\nActually now Elasticsearch config is empty (all lines are comments), execept those two:\nindex.number_of_shards: 1\nindex.number_of_replicas: 0\nI am going to install elasticsearch without .deb package, maybe it help (maybe there are some default values compiled in for debian package)\nAlso let me describe the nature of failure frequencies:\n1) if some test start failing, it will be failing in 99% or 100% cases until we \"fix\" it\n2) that same test does not necessary fail on another machine\n3) test start failing when we change our app/test source code\n4) most interesting: to fix test we change Rails code in testsuite, which is not related to the failing part (for example we add more\nlogging code or we change code in different test or we move test to another place of file, or we swap some lines)\n. Seems issue\nFAIL (0:00:03.555) test: Bulk should extract the routing value from documents. \n          <1> expected but was\n          <2>.\nis not related to the bug I am talking about.\nI think it's different bug.\nit only happens when config consists of two those lines:\nindex.number_of_shards: 1\nindex.number_of_replicas: 0\nSteps to reproduce:\n1)  empty the config file, put only two lines (above) into it\n2) run curl -X DELETE localhost:9200\n3) restart ElasticSearch\n4) run\nfor i in {1..15}; do ruby -I lib:test test/integration/bulk_test.rb; done\nfor revision e76fcf214f1fed648155593920794a79daa3dce5 of Tire\nElasticSearch 0.19.12 (tested on two different machines)\n(note: sleep(1), wait for status are not helping here)\nCan you confirm this?\n. Thank you for your hard work!\nSo, my recent testing show that there are 3 ways to fix random failing tests:\n1) Add wait for green/yellow status call after index refresh call.\n2) Set\nindex.number_of_shards: 1\nindex.number_of_replicas: 0\nit helps. now when bulk_test fixed I don't see failing test anymore\n3) Optimize machine performance.\nAll our machines are powerfull. Sandy Bridge i7 (4 or 6 cores) with 16Gb or 64 Gb RAM and 4 GB VM(host is 16Gb)\nBut moving elasticsearch DATADIR to ramdrive (/run/shm etc) actually fixed the tests on two machines.\nHowever, for example if I decrease number of CPU for VM from 8 to 1 tests start failing again.\nSeems (1) works almost always. But I saw once when it didn't help.\n(3) is making things much better, but not always helping. It's very sensitive to overall load.\nAnd seems (2) works always.\nThat my testing only related to Tire test suite. I am not sure about our application. Only waiting for failures during a work week can\nshow if we fixed it (strange but new test failures only appear when we change something in source code,\ntesting 100500 times same code unlikely produce new fails).\nI already use method (1) for our tests. Going to apply both (2) and (3).\nI will report here if we see that it helped.\nThanks for Vagrant manual, I will use it next time.\np.s.\n\"should extract the routing value from documents\" test was failing only with config\nindex.number_of_shards: 1\nindex.number_of_replicas: 0\nit was fine on default (empty) config. that can explain why it works on your Mac.\n. FYI\nI had one test which actually generates failures in ElasticSearch (mentioned in ticket #543). I found that this test outdated and is not needed anymore, so I removed it.\nAfter that seems that we don't have problems with elasticsearch race conditions.\nI currently only run wait  for yellow/green status after each reindex. (I did not \nchange shared in config, and did not optimize performance/move elastic search to RAM drive on CI server).\nI didn't try removing wait for yellow/green however.\nSo I have a feeling that shared failures can cause race conditions in other requests to same elasticsearch server.\n. yep, i watching 541.\n. Hello.\nThanks for fixing PercolatorIntegrationTest\nBut sad that other test race conditions not reproduced/not investigated.\n. Yes, I had the issue when this test failing with the following elasticsearch config:\nindex.number_of_shards: 1\nindex.number_of_replicas: 0\nit was working fine with default (empty) config file.\nI confirm that this fix fixes the issue. Now the test passes always. Thanks!\n. 1 - yes, but it depends on config or environment. I answered in https://github.com/karmi/tire/issues/537\n. UPD:\nno, seems it returns empty result, not old result.\n. Ok, clear. Thanks!\n. yes, solved.\n. ",
    "incorvia": "We've also had these same issues.. used sleep.. wasn't aware of #refresh (will look for documentation on this), and will try setting the shards and other suggested options.. Thanks for this thread.\n. ",
    "matteomelani": "This really help! Thank you so much!\n-Matteo\n. ",
    "mrcasals": "Hi @karmi, thanks for the quick reply! I'll try it and tell you if something goes wrong :)\n. I close this, working as expected :)\n. Ping\n. Ping. What do you think about this, @karmi?\n. @karmi sorry for the delay on the answer. The problem I found is that when my search returns results and all of them belong to the same class, Tire correctly wraps the results with the model class (so, if my search returns only products, Tire will wrap them with my Product class), so I will be able to use my own defined methods.\nBut if my search returns elements from two different classes (let's say, Product and Business), Tire will not wrap the results and will use its own Collection method. I added the :infer wrapper option that will automatically wrap the results with their own class. Hope this helps to understand the issue.\nAlso yeah, I can totally do what you say, but as there is a case when Tire will automatically wrap the results with my method, having to wrap them myself is unexpected to me :)\n. It's a Rails app, but I'm using Tire's Tire::Model::Persistence to get an ActiveModel/Record API. Also, I'm using the imperative way to work with Tire instead of its DSL.\n. Yes, your code works as expected.\nMy app has two models, and they include Tire::Model::Persistence to save and find objects. I need to search results from both of my models in the same search, so I use the Tire::Search::Search class and URI Parameters (more precisely, I'm using the q, sort, size, type parameters).\nI'm having problems understanding Tire's codebase (too much magic involved, I guess) and I don't find the DSL comfortable to work with, that's why I started using the URI Parameters. \nAnyway, my point was: using Tire::Search::Search, if the search returns results from 2 different models they are not properly wrapped. That's why I opened this PR :)\n. ",
    "mikebaldry": "In my case, I'm using an alias called \"products_current\" as my index_name. This works fine except when the alias doesn't exist, say I go to create an index with rake tire:import CLASS=\"Product\" INDEX=\"products-201305220900\" -- this will create an index \"products_current\" which will be empty and the expected \"products-201305220900\".. I'm currently using a script to delete the index if it exists and create/update alias.. \n. How bout a tire-rails gem?\nSent from my iPhone\nOn 18 Apr 2013, at 19:50, Karel Minarik notifications@github.com wrote:\n\n@jwaldrip I really think we're doing \"too much\" here. It's entirely in users hands how she wants to load the Rails environment? rake environment tire:import:model CLASS='MyModel' is perfectly fine, and will prevent any errors like this.\n\u2014\nReply to this email directly or view it on GitHub.\n. I wanted to test out Heroku addon SearchBox ElasticSearch \u03b2eta, which only supports single index currently (as its beta, I'm assuming)..\n\nBesides the point though :)\n. That sounds reasonable, my only concern is when more than one index is in\nplay and you want different functionality for each.. This is why I\nsuggested it in the model level.\nOn Wed, Jul 17, 2013 at 2:33 PM, Peter Schr\u00f6der notifications@github.comwrote:\n\nwhat do you think of putting this in a configuration like\nTire.configure do\n  auto_create falseend\nwould have the benefit of having it in place with other environment\nspecific configuration like urls etc.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/issues/798#issuecomment-21112772\n.\n\n\nMichael\nwww.brightbits.co.uk\nCompany number: 08133555\nRegistered in England\nRegistered office: 22 Finwell Road, Rainham, Kent, ME8 7PY\n. Maybe, I guess it depends on who you talk to.. I'd expect a prefix to be\napplied to everything, especially seen as its primarily used for separating\nenvironments.. regardless of wether you define the index_name manually or\nlet it be automatically generated, you'd still want it to be test_\nfor instance.\nFeedback from other users would be good!\nOn Fri, Sep 13, 2013 at 10:09 AM, Karel Minarik notifications@github.comwrote:\n\nHmm, that is correct, but I think the current behaviour might actually be\nless surprising? When you define an index name, you clearly want the index\nnamed like this.\nI mean, we can take index_prefix into account in\nhttps://github.com/karmi/tire/blob/master/lib/tire/model/naming.rb#L34\u2026Would like to hear some opinions on that.\n/cc @vhyza https://github.com/vhyza\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/issues/866#issuecomment-24382159\n.\n\n\nMichael\nwww.brightbits.co.uk\nCompany number: 08133555\nRegistered in England\nRegistered office: 22 Finwell Road, Rainham, Kent, ME8 7PY\n. Sorry, I'm actually using Mongoid, but this should be using a cursor anyway, and its not pulling all the documents in in 1 go, as its hitting the limit somewhere around 60% complete each time, as if the documents once loaded aren't getting collected..\n. I've actually finished my contract on the project we had this problem on now, but we did have identity map enabled and it would make perfect sense if that caches all the loaded documents... Maybe @carvil will see this as he is on that project.\n. ",
    "regedarek": "Thanks, now I can filter resources with app_id. One thing I have problem with, is how to merge results?\nruby\nmulti_search = Tire.multi_search do\n  search :all, indices: [:questions, :answers, :links, :service_providers,:events, :past_events, :reviews] do\n    query { match :_all, \"#{term}\" }\n  end\n  search :wihout_app, indices: [:events, :past_events, :reviews] do\n    query { all }\n  end\n  search :with_app, indices: [:questions, :answers, :links, :service_providers] do\n    query do\n      filtered do\n        query { all }\n        filter :terms, app_id: [75]\n      end\n    end\n  end\nend\nI would like to return multi_search.results[:without_app] + multi_search.results[:with_app]\nI need content, type and highlight attribute.\n. Hmm, Maybe I oversight something:\nputs \"ALL:  \" + multi_search.results[:all].map(&:title).inspect\nputs '---'\nputs \"REST: \" + multi_search.results[:rest].map(&:title).inspect\nGives me:\n```\nALL:  [\"Event test 1\", \"Event test 2\", \"Link test 1\", \"Link test 3\", \"Question test 1\", \"Question test 2\", \"Question test 3\"]\n\nREST: [\"Link test 1\", \"Link 2\", \"Question test 1\", \"Question test 2\"]\n```\nBut as final result I need\nALL_FILTERED_BY_APP_ID:  [\"Event test 1\", \"Event test 2\", \"Link test 1\", \"Question test 1\", \"Question test 2\"]\nSo I guess will be better to filter that in multi search instead of merging arrays. Could you give me some hints?\n. Basically because two of my models has not app_id attribute i want to filter rest of them(with app_id) in search and keep the highest performance if it is possible.\nUsing union does not work for me, I receive:\n``` ruby\nALL:  [\"Link test 3\", \"Question test 1\", \"Question test 3\", \"Event test 1\", \"Event test 2\", \"Link test 1\", \"Question test 2\"]\n\nREST: [\"Link 2\", \"Link test 1\", \"Question test 2\", \"Question test 1\"]\nCOMBINED: [\"Link test 3\", \"Question test 1\", \"Question test 3\", \"Event test 1\", \"Event test 2\", \"Link test 1\", \"Question test 2\", \"Link 2\", \"Link test 1\", \"Question test 2\", \"Question test 1\"]\n```\nSo I get unecessry Link 2 and duplication of Question test 1, Question test 2 and Link test 1, Link test 2\nSorry for bothering you because I guess there should be a simple solution.\nEDIT:\nFor now something like below works for me:\nruby\nmulti_search = Tire.multi_search do\n  search :without_app, indices: [:events, :past_events, :reviews] do\n    query { match :_all, \"#{term}\" }\n  end\n  search :with_app, indices: [:questions, :answers, :links, :service_providers] do\n    query do\n      filtered do\n        query { match :_all, \"#{term}\" }\n        filter :terms, app_id: app_ids\n      end\n    end\n  end\nend\nmulti_search.results[:with_app].to_a + multi_search.results[:without_app].to_a\n. ",
    "dwbutler": "Hi, I figured since you have Travis running the unit tests in 1.8.7, that it would be good to run them in JRuby 1.8 mode as well for completeness sake. Up to you if you want to leave that in or not.\n. Unfortunately, I've found it impossible to define platform-dependent dependencies in the .gemspec. There are several reasons for this.\n- The most important reason is that the .gemspec format does not have a way to define platform dependencies!\n- Currently, an officially released gem on rubygems.org must defined a fixed set of dependencies. If you take a look at https://rubygems.org/gems/tire, you'll see the fixed list of runtime and development dependencies. Running gem install tire will install all the listed runtime dependencies. Checking out the repo and running bundle install will install everything in the .gemspec, as well as in the Gemfile. This is what currently fails in JRuby.\n- You technically can put platform checks in the .gemspec, but this does not have the desired result. What happens is when you gem build, the gem will be built in whatever platform you happen to be running on your machine. The published gem spec on rubygems.org will then reflect whatever was built on your local machine.\nDoing the platform checks in the Gemfile is the only fool-proof way I know to actually get bundle install working, and I've seen this method used in many other gems.\nI also think it makes sense to only publish the platform-agnostic gems as development dependencies in the official gem spec on rubygems.org, if JRuby is to be supported.\nHowever, if you wish, you could do the platform checks in the .gemspec, and then make sure to always use MRI to actually release the gem. This would allow all the gems to be listed as development dependencies on rubygems.org.\n. @karmi, I made some improvements and implemented some of the suggestions you had (such as putting the development dependencies back in the gemspec). Travis is happy too (https://travis-ci.org/dwbutler/tire/builds/3940134). Let me know what you think!\n. ",
    "riniculous": "Sorry it was a miss match on multi_json gem. Mine was older. You might want to set a requirement for the newer version of tire to require the newer multi-json\n. ",
    "palanglung": "palanglung/tire ref: palanglung/tire@c2ce0f97a2c5d7f2ccd9c595b702f32e079205f9\n. ",
    "barakcoh": "Hi, treating nil collection as empty arrays sounds good to me and more consistent with ActiveModel associations.\nWill you be merging your commit into master?\nP.S.\nSorry for not adding regression tests. This is my first pull request and I figured I'd add tests if you were ok with my solution. \n. Seems to me this should be the default for all collections.\n. looks great, thx and keep up the great work..\n. ",
    "demirhanaydin": "I have spent few hours on your example but I can't run it successfully, so I created a new one. That's my working example of parent/child gist https://gist.github.com/demirhanaydin/6548883\n. ",
    "lemig": "The escaped ( \\' ) format closes the quotes around the request body.\n. Thanks for your prompt review.\nAccording to http://bit.ly/UsESzY, single quotes do not need to be escaped in Lucene.\nSo this is OK:\ns = Tire.search 'articles' do\n  query do\n    string \"l'amour\"\n  end\nend\nBut:\nputs s.to_curl\nProduces invalid:\ncurl -X GET 'http://localhost:9200/articles/_search?pretty' -d '{\n  \"query\":{\n    \"query_string\":{\n      \"query\":\"l'amour\"\n    }\n  }}'\nAfter format:\ncurl -X GET 'http://localhost:9200/articles/_search?pretty' -d '{\n  \"query\":{\n    \"query_string\":{\n      \"query\":\"l\\u0027amour\"\n    }\n  }}'\nAlso invalid ( \\' ):\ncurl -X GET 'http://localhost:9200/articles/_search?pretty' -d '{\n  \"query\":{\n    \"query_string\":{\n      \"query\":\"l\\'amour\"\n    }\n  }}'\nOther example: http://bit.ly/12NUNfZ\n. ",
    "jwoertink": "I just ran into this today myself. Haven't solved it, but there's this\nhttps://github.com/elasticsearch/elasticsearch/issues/1558\nFrom what I understand, if there's no results, then elasticsearch fails to sort on an empty result set, so they added that option. Just need to figure out how to do it via tire.\nEDIT\nThis works for me...\nruby\nsearch.sort do\n  by :occupation, {order: :asc, ignore_unmapped: true}\nend\n. ok, apparently it does this automatically ~_~ just move along, nothing to see here, folks.\n``` ruby\nsearch = Tire.multi_search do |s|\n  s.search :books, index: Book.index_name do\n    from 0\n    size 3\n    query { all }\n  end\n  s.search :videos, index: Video.index_name do\n    query { all }\n    from 0\n    size 3\n  end\nend\nsearch.results[:books].total #=> 30000\nsearch.results[:books].count #=> 3\n```\n. ",
    "danielgatis": "thanks\n. ",
    "ethier": "@jwoertink I'll join the chorus, thanks!\n. ",
    "krigar": "Yeah, I eventually found the code where you assign the class methods and saw that you left the existing ones as they were, so then it had to be rails that was acting differently.\nTurns out that in versions previous to 3.2.6 one could call index directly on ActiveRecord::Relations, even though it wasn't mentioned in the API. I've tried googling for it and checked various mail lists, but I haven't found anyone mention that it doesn't work any more post 3.2.6, so I suspect I was the only one on the planet that was using the index method directly on Relation without converting to an array first.\n. ",
    "timoschilling": "I don't use the model stuff, namespace is what i need.\nIs a namespace configuration of interest for tire? If yes, I will make a pull request\n. ok, i have done it by karmi/tire-contrib#12\n. The defined?(Mongoid) check is not the 'reale' condition, it is only to make klass < Mongoid::Document save.\nI think the require way is not good!\n1. Tire with Mongoid works out of the Box, why i need to require something for eager loading of Mongoid?\n2. Overwriting of __find_records_by_ids is not good while whats happen if you want to load Mongoid and ActiveRecord at one time?\n. @karmi Strategies is a good way! Give me 1 week, than i implement it. \n. ",
    "gonzofy": "Any thoughts on this?\n. Thanks for the Gist, @Gauravshah . However, we had to work around this issue a long time ago, actually abandoning export entirely in favor of simply rebuilding the ES index from scratch if required. Not ideal, but I'll definitely check this out if we revisit export/import again in the future.\n. ",
    "Gauravshah": "@kryptonik try this \nhttps://gist.github.com/Gauravshah/5368431\n. ",
    "moudy": "Ah yeah this works. The problem was that i had terms :tags, [tags], minimum_match: 1 after the filtered block which that canceled out the id filter. \nI moved it into the filtered block and made it a filter. The output from the log looks right but the results I'm getting include documents that are tagged 'javascript' but are not within the ids.\nThis and below makes it seem like the documents need to fill all the requirements (tagged with 'javascript' and have and id of 1, 2, or 3).\n\"filter\": {\n        \"and\": [\nTire DSL code\nTire.search 'users' do\n  query do\n    filtered do\n      query { string \"name:ni*\" }\n      filter :term, deleted: false\n      filter :term, approval_status: 'approved'\n      filter :terms, id: [1,2,3]\n      filter :terms, tags: ['javascript'], minimum_match: 1\n    end\n  end\nend\nFull output from the logs form about search\ncurl -X GET 'http://localhost:9200/users/_search?from=0&size=10&pretty' -d '{\n  \"query\": {\n    \"filtered\": {\n      \"query\": {\n        \"query_string\": {\n          \"query\": \"name:*\"\n        }\n      },\n      \"filter\": {\n        \"and\": [\n          {\n            \"term\": {\n              \"approval_status\": \"approved\"\n            }\n          },\n          {\n            \"term\": {\n              \"deleted\": false\n            }\n          },\n          {\n            \"terms\": {\n              \"id\": [\n               \"1\",\n               \"2\",\n               \"3\" \n              ]\n            }\n          },\n          {\n            \"terms\": {\n              \"tags\": [\n                \"javascript\"\n              ],\n              \"minimum_match\": 1\n            }\n          }\n        ]\n      }\n    }\n  }\n}'\nI tried re-indexing to make sure the mappings where right. Let me know if this is more of an elastic search question.\n. That works. Thanks for walking me through this. The issue was that I had the filter definition inside the query block. Moving it one level up into the main search block fixed the issue.\nTo clarify the the part about tags canceling out the id filter:\n```\nTire.search 'users' do\n  query do\n    filtered do\n      query { string query_string }\n  filter :term, approval_status: 'approved'\n  filter :term, deleted: false\n\n  filter :terms, id: user_ids\nend\n\n# this cancels out above id filter\nterms :tags, tags, minimum_match: 1\n\nend\nend\n```\nNot sure if this is intentional.\nThanks again!\n. Yeah removed minimum_match thanks\nThis should recreated the issue. \n```\nTire.index('test-filtered-and-filters') do\n  delete\n  create\nstore id: 1, title: 'Title 1', user_id: 1, tags: ['python', 'ruby', 'javascript']\n  store id: 2, title: 'Title 2', user_id: 2, tags: ['javascript']\nrefresh\nend\nTire.configure { logger STDERR }\ns = Tire.search('test-filtered-and-filters') do\n  query do\n    filtered do\n      query { string 'title' }\n      filter :terms, :user_id => [1]\n    end\nterms :tags, ['javascript']\n\nend\nend\ns.results.each do |d|\n  puts \"* TITLE: #{d.title}, USERID: #{d.user_id}, TAGS: #{d.tags}\"\nend\n```\nlog output:\n```\ncurl -X GET 'http://localhost:9200/test-filtered-and-filters/_search?pretty' -d '{\n  \"query\": {\n    \"terms\": {\n      \"tags\": [\n        \"javascript\"\n      ]\n    }\n  }\n}'\n\nTITLE: Title 2, USERID: 2, TAGS: [\"javascript\"]\nTITLE: Title 1, USERID: 1, TAGS: [\"python\", \"ruby\", \"javascript\"]\n```\n\nwhen I remove terms :tags, ['javascript']\n```\ncurl -X GET 'http://localhost:9200/test-filtered-and-filters/_search?pretty' -d '{\n  \"query\": {\n    \"filtered\": {\n      \"query\": {\n        \"query_string\": {\n          \"query\": \"title\"\n        }\n      },\n      \"filter\": {\n        \"and\": [\n          {\n            \"terms\": {\n              \"user_id\": [\n                1\n              ]\n            }\n          }\n        ]\n      }\n    }\n  }\n}'\n\nTITLE: Title 1, USERID: 1, TAGS: [\"python\", \"ruby\", \"javascript\"]\n```\n. as you can see from the data below the document's original mongo id is replaced by the elasticsearch id. i can see the use case for this but am confused about how to preserve the mongo id.\n\nquery\ns = Tire.search('users1') do\n  query do\n    match ['name.autocomplete', 'username.autocomplete'], 'moudy'\n  end\nend\nresult of s.results.first (note the missing mongo id)\n<Item id: \"762phS8zTuug89oqXaqhSg\", email: \"moudy@gmail.com\", name: \"Moudy\", username: \"moudy\", _score: 7.311601, _type: \"user\", _index: \"users_2_19_13\", _version: nil, sort: nil, highlight: nil, _explanation: nil>\ndata when looking at document through elasticsearch-head\n{\n  _index: users_2_19_13\n  _type: user\n  _id: 762phS8zTuug89oqXaqhSg\n  _score: 7.870416\n  _source: {\n    id: 5002c42eb1f4ed000201d6e6\n    email: moudy@gmail.com\n    name: Moudy\n    username: moudy\n  }\n}\n. ok sorry this is related to doing a bulk import. found the answer to my issue here https://github.com/elasticsearch/elasticsearch/issues/2638\n. ",
    "davekinkead": "Looking under the hood a bit more, it seems the problem can be addressed with the following:\n```\nclass Foo\ninclude Tire::Model::Persistence\n# => Override constructor to implement Tire Persistence\n  def initialize(attrs={})\n    attrs.each do |attr, value|\n      # => call Tire's property method\n      self.class.property attr unless self.class.property_types.keys.include? attr\n      # => set instance variable\n      instance_variable_set(\"@#{attr}\", value)\n    end\n  end\nend\n```\nAny thoughts to making this default behaviour?\n. I have to admit that I haven't ventured into the tire-contrib module as the docs are very sparse. \nIf you think that naming clashes could be a problem when searching, then the contrib module is fine.  I'm currently extending a base model class so this would do that in one include line.\nI'll have a sniff around  tire-contrib and submit a pull request shortly.\n. @karmi happy to do this as a contrib - just need to find some time this week :(\n. ",
    "hellvinz": "_run_update_elasticsearch_index_callbacks is not defined anymore via define_model_callback (https://github.com/karmi/tire/blob/master/lib/tire/model/search.rb#L279) because of this commit rails/rails@19357a7b023fd56a5b381cd8894bb520c60cdb59\nif you want to make it work, juste replace\nruby\ninstance.send :_run_update_elasticsearch_index_callbacks do\nby\nruby\ninstance.run_callbacks :update_elasticsearch_index do\n. from the stack trace it seems that you are running tire 0.5.4, the fix has been merged in 0.5.5\nregards\n. the change callback changes should be backward compatible: the run_callbacks method is in rails since 2.1. \nAnd there's already a test covering the update_index method: https://github.com/karmi/tire/blob/master/test/integration/active_model_indexing_test.rb#L42 \n. ",
    "romansklenar": "I was just dealing with this today, thx for solution @hellvinz !\n. As @hellvinz mentioned, it should be backwards compatible and current tests are passing.\n. ",
    "xpepermint": "It exists in 4.0.0.rc1.\n. Hey @karmi \nThe error says:\nruby\nNoMethodError (undefined method `_run_update_elasticsearch_index_callbacks' for #<User:0x007f9320a82428>):\n  app/controllers/users_controller.rb:30:in `block in create'\n  app/controllers/users_controller.rb:29:in `create'\nFull Trace:\n``` ruby\nactivemodel (4.0.0.rc1) lib/active_model/attribute_methods.rb:436:in method_missing'\nactiverecord (4.0.0.rc1) lib/active_record/attribute_methods.rb:131:inmethod_missing'\ntire (0.5.4) lib/tire/model/search.rb:144:in update_index'\ntire (0.5.4) lib/tire/model/callbacks.rb:21:inblock in included'\nactivesupport (4.0.0.rc1) lib/active_support/callbacks.rb:394:in _run__3500917153409669350__save__callbacks'\nactivesupport (4.0.0.rc1) lib/active_support/callbacks.rb:80:inrun_callbacks'\nactiverecord (4.0.0.rc1) lib/active_record/callbacks.rb:299:in create_or_update'\nactiverecord (4.0.0.rc1) lib/active_record/persistence.rb:103:insave'\nactiverecord (4.0.0.rc1) lib/active_record/validations.rb:51:in save'\nactiverecord (4.0.0.rc1) lib/active_record/attribute_methods/dirty.rb:32:insave'\nactiverecord (4.0.0.rc1) lib/active_record/transactions.rb:270:in block (2 levels) in save'\nactiverecord (4.0.0.rc1) lib/active_record/transactions.rb:326:inblock in with_transaction_returning_status'\nactiverecord (4.0.0.rc1) lib/active_record/connection_adapters/abstract/database_statements.rb:202:in block in transaction'\nactiverecord (4.0.0.rc1) lib/active_record/connection_adapters/abstract/database_statements.rb:210:inwithin_new_transaction'\nactiverecord (4.0.0.rc1) lib/active_record/connection_adapters/abstract/database_statements.rb:202:in transaction'\nactiverecord (4.0.0.rc1) lib/active_record/transactions.rb:209:intransaction'\nactiverecord (4.0.0.rc1) lib/active_record/transactions.rb:323:in with_transaction_returning_status'\nactiverecord (4.0.0.rc1) lib/active_record/transactions.rb:270:inblock in save'\nactiverecord (4.0.0.rc1) lib/active_record/transactions.rb:281:in rollback_active_record_state!'\nactiverecord (4.0.0.rc1) lib/active_record/transactions.rb:269:insave'\napp/controllers/users_controller.rb:30:in block in create'\nactionpack (4.0.0.rc1) lib/action_controller/metal/mime_responds.rb:363:incall'\nactionpack (4.0.0.rc1) lib/action_controller/metal/mime_responds.rb:363:in retrieve_collector_from_mimes'\nactionpack (4.0.0.rc1) lib/action_controller/metal/mime_responds.rb:189:inrespond_to'\napp/controllers/users_controller.rb:29:in create'\nactionpack (4.0.0.rc1) lib/action_controller/metal/implicit_render.rb:4:insend_action'\nactionpack (4.0.0.rc1) lib/abstract_controller/base.rb:189:in process_action'\nactionpack (4.0.0.rc1) lib/action_controller/metal/rendering.rb:10:inprocess_action'\nactionpack (4.0.0.rc1) lib/abstract_controller/callbacks.rb:18:in block in process_action'\nactivesupport (4.0.0.rc1) lib/active_support/callbacks.rb:432:in_run__1795920866754849653__process_action__callbacks'\nactivesupport (4.0.0.rc1) lib/active_support/callbacks.rb:80:in run_callbacks'\nactionpack (4.0.0.rc1) lib/abstract_controller/callbacks.rb:17:inprocess_action'\nactionpack (4.0.0.rc1) lib/action_controller/metal/rescue.rb:29:in process_action'\nactionpack (4.0.0.rc1) lib/action_controller/metal/instrumentation.rb:31:inblock in process_action'\nactivesupport (4.0.0.rc1) lib/active_support/notifications.rb:159:in block in instrument'\nactivesupport (4.0.0.rc1) lib/active_support/notifications/instrumenter.rb:20:ininstrument'\nactivesupport (4.0.0.rc1) lib/active_support/notifications.rb:159:in instrument'\nactionpack (4.0.0.rc1) lib/action_controller/metal/instrumentation.rb:30:inprocess_action'\nactionpack (4.0.0.rc1) lib/action_controller/metal/params_wrapper.rb:245:in process_action'\nactiverecord (4.0.0.rc1) lib/active_record/railties/controller_runtime.rb:18:inprocess_action'\nactionpack (4.0.0.rc1) lib/abstract_controller/base.rb:136:in process'\nactionpack (4.0.0.rc1) lib/abstract_controller/rendering.rb:44:inprocess'\nactionpack (4.0.0.rc1) lib/action_controller/metal.rb:195:in dispatch'\nactionpack (4.0.0.rc1) lib/action_controller/metal/rack_delegation.rb:13:indispatch'\nactionpack (4.0.0.rc1) lib/action_controller/metal.rb:231:in block in action'\nactionpack (4.0.0.rc1) lib/action_dispatch/routing/route_set.rb:80:incall'\nactionpack (4.0.0.rc1) lib/action_dispatch/routing/route_set.rb:80:in dispatch'\nactionpack (4.0.0.rc1) lib/action_dispatch/routing/route_set.rb:48:incall'\nactionpack (4.0.0.rc1) lib/action_dispatch/journey/router.rb:71:in block in call'\nactionpack (4.0.0.rc1) lib/action_dispatch/journey/router.rb:59:ineach'\nactionpack (4.0.0.rc1) lib/action_dispatch/journey/router.rb:59:in call'\nactionpack (4.0.0.rc1) lib/action_dispatch/routing/route_set.rb:654:incall'\nrack (1.5.2) lib/rack/etag.rb:23:in call'\nrack (1.5.2) lib/rack/conditionalget.rb:35:incall'\nrack (1.5.2) lib/rack/head.rb:11:in call'\nactionpack (4.0.0.rc1) lib/action_dispatch/middleware/params_parser.rb:27:incall'\nactionpack (4.0.0.rc1) lib/action_dispatch/middleware/flash.rb:241:in call'\nrack (1.5.2) lib/rack/session/abstract/id.rb:225:incontext'\nrack (1.5.2) lib/rack/session/abstract/id.rb:220:in call'\nactionpack (4.0.0.rc1) lib/action_dispatch/middleware/cookies.rb:486:incall'\nactiverecord (4.0.0.rc1) lib/active_record/query_cache.rb:36:in call'\nactiverecord (4.0.0.rc1) lib/active_record/connection_adapters/abstract/connection_pool.rb:626:incall'\nactiverecord (4.0.0.rc1) lib/active_record/migration.rb:366:in call'\nactionpack (4.0.0.rc1) lib/action_dispatch/middleware/callbacks.rb:29:inblock in call'\nactivesupport (4.0.0.rc1) lib/active_support/callbacks.rb:392:in _run__1125300739377227675__call__callbacks'\nactivesupport (4.0.0.rc1) lib/active_support/callbacks.rb:80:inrun_callbacks'\nactionpack (4.0.0.rc1) lib/action_dispatch/middleware/callbacks.rb:27:in call'\nactionpack (4.0.0.rc1) lib/action_dispatch/middleware/reloader.rb:64:incall'\nactionpack (4.0.0.rc1) lib/action_dispatch/middleware/remote_ip.rb:76:in call'\nactionpack (4.0.0.rc1) lib/action_dispatch/middleware/debug_exceptions.rb:17:incall'\nactionpack (4.0.0.rc1) lib/action_dispatch/middleware/show_exceptions.rb:30:in call'\nrailties (4.0.0.rc1) lib/rails/rack/logger.rb:38:incall_app'\nrailties (4.0.0.rc1) lib/rails/rack/logger.rb:21:in block in call'\nactivesupport (4.0.0.rc1) lib/active_support/tagged_logging.rb:67:inblock in tagged'\nactivesupport (4.0.0.rc1) lib/active_support/tagged_logging.rb:25:in tagged'\nactivesupport (4.0.0.rc1) lib/active_support/tagged_logging.rb:67:intagged'\nrailties (4.0.0.rc1) lib/rails/rack/logger.rb:21:in call'\nactionpack (4.0.0.rc1) lib/action_dispatch/middleware/request_id.rb:21:incall'\nrack (1.5.2) lib/rack/methodoverride.rb:21:in call'\nrack (1.5.2) lib/rack/runtime.rb:17:incall'\nactivesupport (4.0.0.rc1) lib/active_support/cache/strategy/local_cache.rb:83:in call'\nrack (1.5.2) lib/rack/lock.rb:17:incall'\nactionpack (4.0.0.rc1) lib/action_dispatch/middleware/static.rb:64:in call'\nrailties (4.0.0.rc1) lib/rails/engine.rb:511:incall'\nrailties (4.0.0.rc1) lib/rails/application.rb:96:in call'\nrack (1.5.2) lib/rack/lock.rb:17:incall'\nrack (1.5.2) lib/rack/content_length.rb:14:in call'\nrack (1.5.2) lib/rack/handler/webrick.rb:60:inservice'\n/Users/xpeper/.rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/webrick/httpserver.rb:138:in service'\n/Users/xpeper/.rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/webrick/httpserver.rb:94:inrun'\n/Users/xpeper/.rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/webrick/server.rb:295:in `block in start_thread'\n```\nModel example (i created a test app :) ):\n``` ruby\nclass User < ActiveRecord::Base\ninclude Tire::Model::Search\ninclude Tire::Model::Callbacks\n\nindex_name(\"#{Rails.env}-#{Rails.application.class.to_s.downcase}-users\")\n\nclass << self\n\ndef create_search_index\n  Tire.index(Widget.index_name) do\n    create(\n      :settings => {\n        \"analysis\" => {\n          \"analyzer\" => {\n            \"widget_name_analyzer\" => {\n              \"type\"      => \"custom\",\n              \"tokenizer\" => \"lowercase\",\n              \"filter\"    => [\"name_ngram\"]\n            }\n          },\n          \"filter\" => {\n            \"name_ngram\" => {\n              \"type\"     => 'edgeNGram',\n              \"min_gram\" => 2,\n              \"max_gram\" => 7,\n              \"side\"     => \"front\"\n            }\n          }\n        }\n      },\n      :mappings => {\n        :show => {\n          :properties => {\n            :id         => { :type => :integer                                     },\n            :full_name  => { :type => :string,  :analyzer => :widget_name_analyzer },\n            :email          => { :type => :string,  :analyzer => :widget_name_analyzer }\n          }\n        }\n      }        \n    )\n  end\nend\n\ndef delete_search_index\n  search_index.delete\nend\n\ndef search_index\n  Tire.index(Widget.index_name)\nend\n\nend \nend\n```\nI hope it helps.\n. I must have been editing the wrong Gemfile :). Thx! 0.5.8 works...\n. ",
    "anoldguy": "Putting together a greenfield, and was hoping to use this with rails 4, so I'm interested in when this might merge.  Also, the TravisCI link shows that the build failed.  I'm still learning about TravisCI, but @romansklenar says the tests are passing.  Are there other tests, and can I help at all?\n. ",
    "andrewslotin": "@karmi, the main goal is to check if the request was successful (i.e. server responded with 200 etc.) Sometimes ElasticSearch returns an empty response (i.e. when the search server is gone).\n. I'm putting my ElastcSearch server down and trying to call Tire::Index#retrieve which causes multi_json to raise MultiJson::DecodeError with an error message saying that I'm trying to parse an empty string as JSON, which is not valid.\n. Ok, got it. Looks like this issue can be reproduced when using Faraday as an HTTP client. Here is a code that replicates the issue:\n```\nrequire 'typhoeus'\nrequire 'tire/http/clients/faraday'\nTire.configure do |config|\n  Tire::HTTP::Client::Faraday.faraday_middleware = Proc.new do |builder|\n    builder.adapter :typhoeus\n  end\n  config.client Tire::HTTP::Client::Faraday\nend\nTire.index('articles').retrieve(:article, 1)\n```\nAnyway, checking if request was successful seems to be good idea :smile: \n(Cannot reopen this issue, should I make another pull request?)\n. @karmi, sounds fair. But this makes the gem to be dependent on implementation of HTTP client, right?\n. ",
    "kroepke": "well 'ree' doesn't work\u2026\n. ",
    "letronje": "Yup. I'm not relying on tire to index associations, doing it via overriding to_indexed_json.\n. ",
    "joshweinstein": "Thanks Karel! Was having the same issue, but upgrading ElasticSearch to 0.20.5 did the trick.\n. ",
    "cis-pjain": "Main problem is with the lat_lon method, I have remove join from [self.lat, self.lng].join(',') and now it works fine.... :) :)\n. Thanks Karmi, Yes its resolve.\n. ",
    "liangjh": "Basically - what I'm asking is whether tire supports facet filters - thanks\n. Looks like it was addressed here?  https://github.com/karmi/tire/pull/467\n. do facet filters allow for bool operators (i.e. \"and\") if there are multiple things that I want to filter by?   thanks.\n. thanks - \n. ",
    "amejiarosario": "So, I changed the workflow from:\nirb> Log.index.delete; Log.index.create; Log.import\nto\nirb> Log.index.delete; Log.create_elasticsearch_index; Log.import\nand now ES is picking up the mappings. Thanks!\n. Ok. Will do.\n. I have millions of records and it's eating up the memory... Any ideas to make it scale?\n. Great! This phrase_prefix has the default behavior I need for a search box... I see that I can also combine it with filters. However, I also need to use negative filter or something like this NOT serial_number:18402909040007. But, It seems to not be supported. e.g:\nruby\nTire.search('access_points') do\n    query do\n        match :_all, \"00:19:92:00:7\", type: 'phrase_prefix'\n        string \"NOT serial_number:2909040007\"\n    end\n    filter :term, domain: 'domain_2'\nend\nTire produce the following curl (omitting the match clause altogether)\n``` bash\nwith `string \"NOT serial_number:2909040007\"'\ncurl -X GET 'http://localhost:9200/access_points/_search?pretty' -d '{\"query\":{\"query_string\":{\"query\":\"NOT serial_number:2909040007\"}},\"filter\":{\"term\":{\"domain\":\"domain_2\"}}}'\nwithOUT `string \"NOT serial_number:2909040007\"'\ncurl -X GET 'http://localhost:9200/access_point/_search?pretty' -d '{\"query\":{\"match\":{\"_all\":{\"query\":\"00:19:92:00:7\",\"type\":\"phrase_prefix\"}}},\"filter\":{\"term\":{\"domain\":\"domain_2\"}}}'\n```\nDo you know if there is a way to make it work? Or if I can use negative filters? Or If can make a query{string(...)} behaves like a match_phrase_prefix?\n. boolean's must_not are the negatives filters :)\nSolution:\nruby\nTire.search('access_points') do\n  query do\n      boolean do\n          must { match :_all, \"00:19:92:00:7\", type: 'phrase_prefix' }\n          must_not { term :serial_number, '2909040007' }\n      end\n  end\n  filter :term, domain: 'domain_2'\nend\nthus,\nbash\ncurl -X GET 'http://localhost:9200/access_points/_search' -d '{\"query\":{\"bool\":{\"must\":[{\"match\":{\"_all\":{\"query\":\"00:19:92:00:7\",\"type\":\"phrase_prefix\"}}}],\"must_not\":[{\"term\":{\"serial_number\":{\"term\":\"2909040007\"}}}]}},\"filter\":{\"term\":{\"domain\":\"domain_2\"}}}'\n. For performance sake... I tried in the code above to use filter, but it doesn't work inside must_not block, so I ended up using term as you saw. Could you please provide an example how will must_not { term :serial_number, '2909040007' } be with filters? \n. Ok. I used filter :not with filtered and it worked too. For future reference, how do I know which ones has better performance?\n``` ruby\nUsing Filtered\nTire.search('access_points') do\n  query do\n    filtered do\n      query { match :_all, \"00:19:92:3\", type: 'phrase_prefix' }\n      filter :not, { term: { serial_number: \"4212050043\" } }\n    end\n  end\n  filter :term, domain: 'domain_2'\nend.results\nUsing Boolean\nTire.search('access_points') do\n  query do\n    boolean do\n      must { match :_all, \"00:19:92:3\", type: 'phrase_prefix' }\n      must_not { term :serial_number, '4212050043' }\n    end\n  end\n  filter :term, domain: 'domain_2'\nend.results\n```\n. ",
    "davidguthu": "Wow this wasted quite a bit of my time.  Could you put a deprecation warning on the other method or fix it?\n. ",
    "AaronRustad": "Fair enough. Thanks @darrenboyd and @karmi for your constructive criticism.\n. ",
    "dfuentes77": "Thanks Karel. I actually do that now.  I front it with Varnish and Nginx for load balancing and filtering/auth of requests and then front that with a CDN.  I just don't like the endpoint out on its own.  It adds a level of complexity that at this point, I have found unnecessary.   If I can just use a cloud service like Bonsai or Found through Tire without having to worry about blocking I/O, then it just becomes another Heroku service to us... which is where all this PAAS stuff is going anyway.  Thanks again.\n. Sounds good. Thanks.\n. I'm seeing the same thing when trying to populate a new Heroku Bonsai cluster.  We have a rake task that populates a cluster using aliases, so that there is no downtime during reindexing.  Our code may or may not assume there is an existing alias but we are noticing that on a local 0.20.6 elasticsearch AND on our self built multi-node ES cluster, this works fine.  On the Bonsai cluster, it bails with:\nNoMethodError: undefined method `empty?' for nil:NilClass\nvendor/bundle/ruby/1.9.1/gems/tire-0.4.2/lib/tire/alias.rb:97:in `block in all'\nvendor/bundle/ruby/1.9.1/gems/tire-0.4.2/lib/tire/alias.rb:95:in `each'\nvendor/bundle/ruby/1.9.1/gems/tire-0.4.2/lib/tire/alias.rb:95:in `inject'\nvendor/bundle/ruby/1.9.1/gems/tire-0.4.2/lib/tire/alias.rb:95:in `all'\nvendor/bundle/ruby/1.9.1/gems/tire-0.4.2/lib/tire/alias.rb:117:in `find'\nI'll email bonsai with this link to see if they have any input but I wanted to let you know this PR could be a general solution for people running into this kind of situation.\n. Great. Let me know when I can test again on our enterprise addon. Thanks. \n. If this will take long, what can I do in the mean time?  I can't populate our cluster with data because of this.\n. Just so that everyone knows, I just tried using Found.no for the first time as well, and our job ran fine and populated the cluster with our data without error, so far.  I'll be doing some more functional testing soon.\n. Thanks.  I'll test this soon.\n. ",
    "gondalez": "Hi, I just realised my mistake.\nI thought TBLENCWWWCOMP_ and TBLDECWWWCOMP_ were identical; missed the ENC/DEC. They are both needed.\nLate night eyes! :sweat_smile:\nI'll push a fix. \n. Oh sorry, that makes much more sense. Good call. I'll give it a shot.\n. How's that? :)\n. Thanks guys, I can confirm the logging is now gone :smile:\n. ",
    "mobilutz": "Would love to the the pull-request being merged. The warning are not bad, but would love to see them leave ;)\nThanks\n. Thanks for merging. Unfortunately you didn't update the version number, so bundler does not detect the changes.\nAny change of upgrading the version?\n. ",
    "luislavena": "I'm suffering similar issues with custom id field. Our ID field is a combination of several fields of the document.\nWe even expose to_indexed_json and use MultiJson.dump in it.\nI'm using Tire in a plain Ruby application so ActiveSupport is not added (even that is dependency of Tire)\nAfter I require active_support/json at the top of Tire::Index, things start working.\n. > Yeah, but that commit was really needed so Mongoid integration works at all...\nCan you simply require \"active_support/json\" at the top as I commented above?\nIn my case I'm using Tire outside Rails, so ActiveSupport::JSON is not automatically loaded.\nTire::Index should deal with its requires/dependencies, or when requiring tire it should load the needed part of ActiveSupport.\n. > Where is the exception thrown? In JSON library, in Tire, in Elasticsearch?\nOj.dump(nil) works on my end, but since Tire::Index is doing id.to_json and to_json isn't defined, is failing:\nirb(main):003:0> nil.to_json\nNoMethodError: undefined method `to_json' for nil:NilClass\n    from (irb):3\n    from /Users/luis/.rbenv/versions/1.9.3-p385/bin/irb:12:in `<main>'\nirb(main):004:0> require \"active_support/json\"\n=> true\nirb(main):005:0> nil.to_json\n=> \"null\"\n. ",
    "bpang": "I know this is an old thread, but I ran across the same issue.  It's due to the elasticsearch user not being able to traverse the directories that contain the term_synonyms.txt file.  chmod o+x /parentfolders should suffice.\n. ",
    "kranthitech": "@bpang Thanks for sharing the solution. I was facing the same problem.\nTo get it running, I had to run chmod o+x on all parent folders i.e. if the file was in \n/root/a/b/synonyms.txt\nchmod o+x /root\nchmod o+x /root/a\nchmod o+x /root/a/b\nPlease let me know if this was the right approach, or is there a better way? Thanks.\n. ",
    "jogaco": "In case anyone ends here with the same problem: This is the correct way of doing it: http://stackoverflow.com/a/35402714/345996\nBy adding the corresponding entry in the Java security policy file!\n. ",
    "vsizov": "hm. I tried that. But i have another context i think. When i try to pass array to match in my code i got error.\n. sorry. Works fine. Thanks for your time.\n. ",
    "relevante": "After incrementing a field called :view_count like this:\nruby\n  # in a model\n  def count_view!\n    self.inc(:view_count, 1)\n  end\ncalls to search including\nruby\n      s.sort { by :view_count, 'desc'}\nignore the updated value.  The model has the updated value coming back from Mongoid, but the search is ordered according to the value prior to the update.\nIf I call save on the model after incrementing the field, like this:\nruby\n  # in a model\n  def count_view!\n    self.inc(:view_count, 1)\n    self.save\n  end\nthe Tire index updates, and everything is fine.\n. It's not a timing-related issue, as the incorrect sort persists though many refreshes of the sorted result page.\nto_indexed_json is correct after the inc and before the save.  The issue appears to be that whatever hook causes Tire to update its indexes isn't called when inc is called.\n. OK, upon digging a little deeper, Mongoid doesn't fire any callbacks after atomic updates (https://github.com/mongoid/mongoid/issues/2751), so doesn't look like there's much that can be done on this end.  Thanks!\n. ",
    "miry": "@nz It is very good news about multi index search. I will try to use it again.\nAlso I have solved the problem to use one settings, I have combined all mappings and settings and do creation of index own self without Model's helper method create_elasticsearch_index https://github.com/karmi/tire/blob/master/lib/tire/model/indexing.rb#L106. Because it does not work good for my situation. It use incorrect mappings and settings for some type(models).\nIt is looks like https://gist.github.com/miry/4993652\nAlso I duplicate this mapping in models too, to be ensure that do store in elasticsearch only required fields.\n. you may close\n. ok\n. @karmi can you provide one of this parsers. \nI used these to test: \n- http://daringfireball.net/projects/markdown/dingus\n- http://markable.in/editor/\n- Rubymine plugin\n. hmm, it works like all other tools https://gist.github.com/miry/5497817. Used rdiscount-2.0.7.2.gem.\nThe reason is when I copy some of this examples to irb there are some extra spaces.\nAnd it is \"offended the eye\" (\"cut the eye\").\nnot critical\n. ",
    "marclennox": "I'm trying to use external version numbers and I see from this issue (and from the code) that the _version parameter is supported for bulk updates, but I'm not seeing support for it when calling the individual record \"store\" method on index.\nPlease confirm that _version is not supported on individual index store calls.\n. +1 to add it.  For now I've changed my method to use bulk store with an\narray of 1 document.\nThat being said, I can't figure out to how to properly set the version_type\nstring in the index settings.  I'm getting a VersionMismatch exception\nevery time despite the fact that I'm sending a larger index number in the\nbuik store.\nWhere (and how) do I set the version_type to \"external\" through the Tire\ngem?\nOn 9 April 2013 03:00, Karel Minarik notifications@github.com wrote:\n\nTrue, neither version nor version_type are supported in Index#store,\nthough trivial to add.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/issues/633#issuecomment-16097325\n.\n. I sent pull request https://github.com/karmi/tire/pull/698 for this.  Version is properly sent over the wire with this change, however I still can't figure out how to configure the index to accept my external version.  Getting this every time.\n\nruby\nVersionConflictEngineException[[openera_development:cloud_file:main:v1][0] [cloud_file][18549]: version conflict, current [-1], provided [1365518739]]\n. So in that case the index store method will need to also send version_type=external if a version is provided?  I don't see a way in the current method call to add the version_type parameter.\n. Will do\nOn 9 April 2013 11:36, Karel Minarik notifications@github.com wrote:\n\nYes, exactly. It's not supported yet, you can add it as part of your pull\nrequest?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/issues/633#issuecomment-16120365\n.\n. Thanks.\n\nI'm a bit confused though.  It seems that version was implemented in bulk\nstore, however the version_type=external is not in there, so it actually\ndoesn't work through the bulk API.  Also in elasticsearch documentation it\ndoesn't mention the version_type=external URL parameter, it mentions that\nit's a \"setting\".\nhttp://www.elasticsearch.org/guide/reference/api/index_/\nOn 9 April 2013 11:42, Karel Minarik notifications@github.com wrote:\n\nExample of the requests with version_type:\ncurl -X DELETE localhost:9200/test-version-type\ncurl -X POST localhost:9200/test-version-type\necho\ncurl -X PUT 'localhost:9200/test-version-type/d/1?version_type=external&version=100' -d '{\"title\":\"A\"}'echo\ncurl -X PUT 'localhost:9200/test-version-type/d/1?version_type=external&version=101' -d '{\"title\":\"B\"}'echo\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/issues/633#issuecomment-16120821\n.\n. OK, I'll add that to my PR.  I've tested it locally and it works.  Thanks.\n\nOn 9 April 2013 11:49, Karel Minarik notifications@github.com wrote:\n\nHmm, true -- version_type is not supported in Index#bulk_store...\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/issues/633#issuecomment-16121302\n.\n. OK, if you look at the comment history I had asked for direction on this before making the assumption...  but perhaps I wasn't clear.\n\nSo with that being said, can I get some direction on where I should take this pull request.  Should I rollback my bulk import changes and just get the single update with version working?\n. Thanks for this.  I'll update the PR hopefully tonight.\nOn 16 April 2013 17:14, Bavaro notifications@github.com wrote:\n\nHey marclennox, sorry for not responding to your request for comments. We\ndon't check this account as often as we should.\nThe rules kinda go:\n'_version' can be specified alone\n'_version' and '_version_type' can be specified together\n'_version_type' cannot be specified without '_version'\nAnd I would just depend on the user of Tire to set the '_version_type' to\nwhatever they want it to be, and let them handle the ES error Tire will\nreturn if they put something invalid. This way, in the future, if ES\nsupports other '_version_type' values, the code doesn't have to change. Our\ncode looks like:\n@meta_data.merge(_version: version.to_s, _version_type: \"external\")\nDoes this help?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/issues/633#issuecomment-16472220\n.\n. Thank you.\n\nOn 6 June 2013 12:28, Karel Minarik notifications@github.com wrote:\n\nHi, sorry for the ridiculous reaction time! I've merged and refactored the\ncommit a bit, it's now on master.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/issues/633#issuecomment-19057206\n.\n. Thanks Karel.  My assumption was that the prefix would apply in both cases.\n I suppose there's an argument to be made for both cases.  For my own\nusecase I would want the delimiter to be configurable also (I use : instead\nof _).  For now I'm just prepending my own prefix to the index name.\n\nOn 2 March 2013 06:56, Karel Minarik notifications@github.com wrote:\n\nYes, correct -- if you set index name via MyModel.index_name, the prefix\nis ignored. Do you think it makes sense to support it for custom index\nnames?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/issues/644#issuecomment-14327118\n.\n. Sounds good.\n. Will do, just need to figure out the version_type issue first.\n\nOn 9 April 2013 11:24, Karel Minarik notifications@github.com wrote:\n\nCan you add unit tests in the\nhttps://github.com/karmi/tire/blob/master/test/unit/index_test.rb#L243context?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/pull/698#issuecomment-16119492\n.\n. FYI, I've got versioning working in the PR both for bulk and individual updates but haven't resolved the testing failures or created new tests yet.\n. Rebased and squashed my commits.  Any chance to get this merged?\n. No there's more to it than that.  Let me resubmit and rebase without the formatting changes.\n. \n",
    "bavaro": "Ok, reopening because were kinda of confused as to what happened here...\nWe opened Issue #633 because we saw that the '_version' parameter is supported by Tire during bulk indexing, but the '_version_type' parameter is not (we only use bulk indexing so we did not bother to look at single document indexing).\nWe opened a pull request, Issue #634, adding support for the '_version_type' parameter, along with tests for it.  We closed this issue (Issue #633) because we figured having the pull request open would suffice (we originally thought pull requests were supposed to reference issues, but they are actually issues themselves, so sorry about that).\nThen 2 months later, marclennox commented that '_version_type' is not supported, and opened pull request Issue #698, with his own changes.  marclennox changes (from what we understood) seems to assume that if the '_version' parameter is specified, then Tire should assume that '_version_type' should be set to external.  That is not correct!\nSee: http://www.elasticsearch.org/guide/reference/api/index_/\nBasically,  you can specify '_version' without '_version_type'.  It is used for optimistic locking.  You can pull a document out of the ES index, keep note of the version, and post the same document back with modifications AND the version of the document you pulled.  If the version you're posting does not match the version the ES index has, the write is not accepted, and you must try again; someone updated that document between the time you pulled and reposted it.\nWhen using '_version' in conjunction with '_version_type' (set to 'external'), ES trusts that you will handle the versioning, and checks that the version you are posting is greater than the version in the ES index.\nThe two use cases differ, so however this is added, please don't assume '_version_type' should be external when '_version' is supplied.\nSo it seems we need support for '_version_type' in both Tire bulk and store operations.  Issue #634 adds it for bulk, but not for store.\n. Hey marclennox,  sorry for not responding to your request for comments.  We don't check this account as often as we should.\nThe rules kinda go:\n'_version' can be specified alone\n'_version' and '_version_type' can be specified together\n'_version_type' cannot be specified without '_version'\nAnd I would just depend on the user of Tire to set the '_version_type' to whatever they want it to be, and let them handle the ES error Tire will return if they put something invalid.  This way, in the future, if ES supports other '_version_type' values, the code doesn't have to change.  Our code looks like:\n@meta_data.merge(_version: version.to_s, _version_type: \"external\")\nDoes this help?\n. No problem.  Also see #634 if you need tests for the bulk import supporting '_version_type'\n. Sorry Issue 633 is basically a dupe. \n. ",
    "presdo": "Karmi,\nI m not sure what do you want me to look to? Which file does not have the right documentation?\nThanks.\n- Jerry\nOn Feb 16, 2013, at 3:25 PM, Karel Minarik notifications@github.com wrote:\n\nThe documentation is incorrect in the contributed module, see test for proper syntax: https://github.com/karmi/tire-contrib/blob/master/test/queries/custom_filters_score/custom_filters_score_test.rb\n@presdo: Can you look at the docs, please?\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "RoyLee": "I looked at the test example and tried the following:\nruby\nsearch_query = 'maz'\nsponsored_brands = ['ford', 'jeep', 'mazda']\nsearch = Tire.search 'cars' do\n  query = Query.new.custom_filters_score do\n      query { term :brand, 'bmw' }\n      filter do\n        filter :terms, :tags => sponsored_brands\n        boost 2.0\n      end\n      filter do\n        filter :terms, :tags => sponsored_brands\n        script '_score * 2.0'\n      end\n    end\n  end\nend\nI am getting the following error:\nruby\nNameError (uninitialized constant SearchesController::Query):\n. Thanks for looking into this.  I don't know exactly what is going on but I ran the exact code and am now getting the following error:\nruby\nNoMethodError (undefined method `custom_filters_score' for #<Tire::Search::Query:0x007f92b1591e38 @value={}>):\nDo I need to add a require somewhere to access that functionality? \nI am running Tire v0.5.4\n. I added the require line to the first line of the controller file where I'm running the search method and I get the following error:\nruby\nLoadError (cannot load such file -- tire/queries/custom_filters_score):\napp/controllers/searches_controller.rb:1:in `<top (required)>'\nAfter researching this online, it looks like the solution for when developers encounter this issue is requires shouldn't be needed and simply bundle installing the gem should allow for access.  Is there something I'm missing here?  I also tried putting the require in my model with a method that has the same custom_filters_score code and it gave me the same error.\n. ",
    "erez-rabih": "I'm on version 0.5.4\n. ",
    "tmaier": "What about if we leave after_save and add a after_rollback callback, which destroys the ES entry?\nThis would work with new records, but would create new inconsistencies when updating an existing record. Unless we cache the old entry...\n. @karmi you asked me to remind you. :)\nAt the moment, my database also has inconsistencies, as there are records in the ES database which got rolled back (for some reason) by the Postgre Database.\n. ",
    "aufi": "Thanks:)\nIn my view, not running Elasticsearch server should not prevent app models from save.\nSo, I changed Tire's model Callbacks from after_save to after_commit (and set up error notifications to reindex data manually).\n. Just patch https://gist.github.com/aufi/5038549, the rest will be later..\n. ",
    "metaskills": "Thanks! BTW, I had two test failures on my first run. But they went away when I ran them the second time.\n. ",
    "moktin": "Hi,\nThe test are green locally :  \n```\nFinished in 173.233226 seconds.\n774 tests, 774 passed, 0 failures, 0 errors, 0 skips, 1745 assertions\n```\nI also noticed that most of the past builds on Travis are failing (see https://travis-ci.org/karmi/tire/builds) and it seems to be exactly the same errors.\n@karmi are the builds having general problem on travis ? Or are they \"real\" failure ?\n. Thank you for your work and time !\n. ",
    "fbernier": "I finally fell on this conversation which looks to be what I want. Sorry for the noise.\nhttps://github.com/karmi/tire/pull/92#issuecomment-6929988\n. ",
    "mikecx": "After seeing some ideas around to_indexed_json tried that out too. Setting that to {}.to_json continues to show the problem. Also tried with just indexes :id and seeing the same thing. A quick estimation shows that even with the bulk importer we are looking at 10+ weeks because of this. This is a huge issue for anyone with any sizable data.\n. More information, seems to only be happening on the batch importing. Using Class.update_index only seems to make external database calls where needed.\n. karmi, certainly appreciate the updates to the docs. We've switched to using find_each and to_indexed_json which has it running much faster and will work. The importing problem using Class.import is that it appears to make n+1 queries even when the mappings only contains the model id and the to_indexed_json is returning only { :id => id }.to_json.\n. Sorry, been super busy over the last few days. Will try and get a test setup today and report back.\n. ",
    "s1monw": "hey,\nI don't think this is a tire issue not an ES issue. The problem here is that you are indexing documents that have all the same length (token wise) and you are searching on multi match wich builds a dismax top-level query and all the document will end up with the same score so the actual result list depends on the replicas the request is executed on and the order they answer the request. can you print the score and check if they are the same?\n. even further if you set search_type = dfs_query_then_fetch it should be consistent score wise across the board. It also depends on the document frequencies here\n. ",
    "mdi": "Turned on logging and tried with and without load: true. Same results either way. Here is a log file. Results are stored in mongo v2.2.3 and I'm using mongoid v3.1.2.\n. Ah, I think I know what it is. I need to load the records by passing their slug attribute to .find instead of id. How can I do that with tire?\n. The real problem is that mongoid-slug is broken with 2.0.0p0. It will only find records by slug instead of id and/or slug. Bug report here. So I need to work around this until that bug is fixed.\n. Thanks!\n. ",
    "gerred": ":space_invader: :+1: \n. ",
    "jwaldrip": "\"Why the call to GC.start for each batch?\"\nThe reason for this is that while importing a large set of data some odd 5million records of data with the find_in_batches call I found my memory climbing to over 3.5 gigs, GC.start fixes this in all instances.\n. In regards to the indexer scope, what would you think about doing the following:\nruby\nclass MyModel\n  include Tire::Model::Search\n  tire do\n    scope do\n      includes(:comments)\n    end\n    mapping do\n       indexes :name\n       indexes :comments_count, as: \"comments.count\"\n    end\n  end\nend\n. @nz I am about to do another commit that should explain the idea a litte better. With sunspot I had to use the default AR scope, the idea here is that there should be a scope that the indexer should use that may not be used elsewhere in the app.\n. @karmi take a look per our conversation, let me know if this addresses the issues you were concerned about.\n. - Improving the rake task moved to #655 \n- Adapter Specific importing moved to #656 \n- DSL addition: index_scope moved to #657\n. @karmi I am sure your week has been busy. What are your thoughts on this request?\n. @karmi any additional thoughts on this yet?\n. Looking at this further, I wonder if this should just be added to index#import. Or have a more sophisticated way of managing this.\n. @karmi I am sure your week has been busy. What are your thoughts on this request?\n. @karmi I think this needs more thought on my end. I will think deeper on it and provide a more elegant solution.\n. @karmi I am sure your week has been busy. What are your thoughts on this request?\n. @karmi Well, I have been digging even deeper into this. It is great that the import rake task supports this. But we have a very complex case. Here is what the code looks like. Also , I added some automagical includes for the active record api. As you may be able to determine from below is as the rake task supports some simple includes. A much more complex solution is very tedious.\n``` ruby\nmodule Tire::IndexingAdditions\n  extend ActiveSupport::Concern\ndef to_indexed_json\n    to_json mapping_to_json_options\n  end\ndef mapping_to_json_options(mapping = self.class.tire.mapping_to_hash)\n    options = mapping.reduce({}) do |options, (k, v)|\n      if v.has_key?(:properties)\n        options[:only]    = []\n        options[:methods] = []\n        options[:include] = {}\n        v[:properties].each do |k, v|\n          if v.has_key?(:properties) && self.class.reflect_on_association(k)\n            options[:include][k] = mapping_to_json_options({ k => v })\n          elsif attribute_names.include? k.to_s\n            options[:only] << k\n          else\n            options[:methods] << k\n          end\n        end\n      end\n      options\n    end\n    options.update({ root: false })\n  end\nmodule ClassMethods\ndef mapping_to_arel(arel=self.includes, mapping=self.tire.mapping_to_hash)\n  mapping.reduce(arel) do |arel, (k, v)|\n    arel = if v.has_key? :properties\n             v[:properties].reduce(arel) do |arel, (k, v)|\n               if reflect_on_association k\n                 arel = case arel\n                        when ActiveRecord::Relation\n                          arel.includes(k => mapping_to_arel([], { k => v }))\n                        when Array\n                          inner_mapping = mapping_to_arel([], { k => v })\n                          append = inner_mapping.present? ? { k => inner_mapping } : k\n                          arel << append\n                        end\n               end\n               arel\n             end\n           end\n    arel\n  end\nend\n\nend\nend\nmodule MedicalFacilityIndexer\n  extend ActiveSupport::Concern\n  include Tire::IndexingAdditions\nincluded do\n    include Tire::Model::Search\n    include Tire::Model::Callbacks\ntire do\n\n  index_scope do\n    mapping_to_arel.\n      includes(appointment_books: { appointment_reasons: {}, appointment_setting: {} }).\n      includes(:premium_profile).\n      includes(:administrative_area).\n      includes(physician_appointment_books: { appointment_reasons: {}, appointment_setting: {} }).\n      includes(:medical_facility_pre_registration).\n      includes(physician: [:account, :ratings, :premium_profile, :languages, { medical_facility_group: :premium_profile }]).\n      includes(medical_facility_group: { premium_profile: {} })\n  end\n\n  mapping do\n\n    # Basic Properties\n    indexes :name, type: 'string'\n    indexes :updated_at, type: 'date'\n\n    # Address\n    indexes :address\n    indexes :physical_state\n    indexes :physical_city\n    indexes :physical_zip\n    indexes :location, type: \"geo_point\", as: \"[lat, lng]\", lat_lon: true, geohash: true\n\n    # Properties\n    indexes :is_premium?, type: 'boolean'\n    indexes :includes_photo?, type: 'boolean'\n    indexes :includes_website_link?, type: 'boolean'\n\n    # Home Health Care\n    indexes :service_area, type: \"nested\" do\n      indexes :options, type: \"string\"\n    end\n\n    # Widget Ids\n    indexes :widgets, type: 'nested' do\n      indexes :id, type: \"integer\"\n    end\n\n    # Appointment Setting\n    indexes :pre_registration_enabled?, type: 'boolean'\n    indexes :is_bookable?, type: 'boolean'\n    indexes :appointment_book_enabled?, type: 'boolean'\n    indexes :appointment_reasons, type: 'string'\n\n    # Physician\n    indexes :has_physician?, type: 'boolean'\n    indexes :physician, type: \"nested\" do\n      indexes :id, type: \"integer\"\n      indexes :gender, type: \"string\"\n      indexes :experience, type: \"string\"\n      indexes :average_rating, type: \"integer\"\n      indexes :is_premium?, type: \"boolean\"\n      indexes :medical_specialties, type: \"nested\" do\n        indexes :id, type: \"integer\"\n        indexes :name, type: \"string\"\n      end\n      indexes :languages, type: \"nested\" do\n        indexes :id, type: \"integer\"\n        indexes :name, type: \"string\"\n      end\n    end\n\n    # Categories\n    indexes :medical_facility_categories, type: \"nested\" do\n      indexes :id, type: \"integer\"\n      indexes :category, type: \"string\"\n      indexes :slug, type: \"string\"\n    end\n\n    # Sub Categories\n    indexes :medical_facility_sub_categories, type: \"nested\" do\n      indexes :id, type: \"integer\"\n      indexes :name, type: \"string\"\n    end\n\n    # Account\n    indexes :account, type: \"nested\" do\n      indexes :id, type: \"integer\"\n      indexes :name, type: \"string\"\n    end\n\n    # Group\n    indexes :medical_facility_group, type: \"nested\" do\n      indexes :id, type: \"integer\"\n      indexes :name, type: \"string\"\n    end\n\n  end\nend\n\nend\nend\n```\n. @karmi I am sure your week has been busy. What are your thoughts on this request?\n. @karmi I am sure your week has been busy. What are your thoughts on this request?\n. When you load the instance with the query this works fine. Where it doesnt work is where you rely on the source from ES only. For example:\n``` ruby\nresults = MedicalFacility.tire.search page: (params[:page] || 1), per_page: (params[:per_page] || 10) do |search|\n  search.query { all }\n  search.fields \"_source\"\n  search.script_field :distance, script: \"doc['location'].distance(#{lat_lng.join(',')})\"\n  # ...\nend\nresults.first.distance\n-> nil\n```\nAs you can see without the fix, the script field distance returns nil, with the fix, I get the proper value.\n. @karmi  I may have a better way\n. I agree with that. Save auto environment for tire rails. Have you started on that @karmi? If not i can start laying the ground work.\n. Although I didn't do the original message here, I think the reason is to achieve a newline, I will update with some better wrapping.\n. Also, && is better to use than \"and\" in conditional arguments.\n. ",
    "tpitale": "Oh wow, that would be sad. I wonder if that's a Kaminari problem, more than a Mongoid one. I'll investigate.\n. Yeah, I saw that ticket too. I think, as long kaminari is still in that code, removing to_a would still be nice. Right now I use my own paginate, which is exactly the same as the one added by the rake task, except for the to_a :-)\n. Kaminari + Mongoid had lots of problems for a time, but there are currently no open issues that appear to mention anything related to this. I don't have an application that uses mongodb or mongoid handy, so I think that's all I can offer to the conversation for the moment. Let me know if you can't get to testing on a mongoid app of yours and I'll try to throw one together. Though, I doubt it would properly test mongoid pagination edge cases \u2026\n. We had another issue, my apologies. This was a red herring.\n. Thank you!\n. I'm also trying to add _type to the to_indexed_json method. We'll see if that helps.\n. Seems to be that when nesting like this, it's still just one real document, and only a document stores the _type. Here's hoping that tricking it with to_indexed_json might do the trick!\n. Win! It totally worked.\n. So, it would be awesome if _type passed to indexes in a nested mapping would work to save the _type as part of the nesting attributes. But, if not, I'm totally okay with this workaround.\n. DataMapper does something similar, but does not use mattr_accessor (Rails-only, maybe?). They call it descendants which matches the ancestors terminology quite well I think.\nhttps://github.com/datamapper/dm-core/blob/master/lib/dm-core/model.rb#L46\nhttps://github.com/datamapper/dm-core/blob/master/lib/dm-core/model.rb#L210\n. ",
    "hemantsingal": "Cool. So fast!\n. I can do that. That is correct. \nI want to know how to pass a routing parameter in this scenario.\n. Great!\n. ",
    "zywx": "You're a GOD :-) Finally working and I understand much better syntax for Tire !\nGreat thanks!\n. I think it can be a good thing to add this in nested test !\n. The second one doesn't work because it seems there is no query between path and filtered so there is an error: \n[nested] filter does not support [filtered]\n. Okay, this must be something like that:\nfilter :nested, { path: 'apps_events'}.merge({ query: nested_filter.to_hash })\nthanks !\n. Karmi my best tire/ES friend :) I have another problem, I think I understand why but I can't write the good ES or Tire code.\nI do that:\ncurl -X GET 'http://78.109.90.3:9200/users/user/_search?from=0&load=&size=20&pretty' -d '{\n\"query\": {\n    \"filtered\": {\n        \"query\": {\n            \"match_all\": {}\n        },\n        \"filter\": {\n            \"and\": [{\n                \"nested\": {\n                    \"path\": \"apps_events\",\n                    \"query\": {\n                        \"filtered\": {\n                            \"query\": {\n                                \"match_all\": {}\n                            },\n                            \"filter\": {\n                                \"and\": [{\n                                    \"term\": {\n                                        \"apps_events.status\": \"active\"\n                                    }\n                                }, {\n                                    \"or\": [{\n                                        \"and\": [{\n                                            \"range\": {\n                                                \"apps_events.locations.inf\": {\n                                                    \"gte\": 59850\n                                                }\n                                            }\n                                        }, {\n                                            \"range\": {\n                                                \"apps_events.locations.sup\": {\n                                                    \"lte\": 59851\n                                                }\n                                            }\n                                        }]\n                                    }]\n                                }]\n                            }\n                        }\n                    }\n                }\n            }]\n        }\n    }\n},\n\"size\": 20,\n\"from\": 0\n}'\nRequest is ok BUT:\nit returns for example user like that :\n{\n\"_index\": \"users\",\n\"_type\": \"user\",\n\"_id\": \"21578\",\n\"_score\": 1.0,\n\"_source\": {\n    \"_id\": 21578,\n    \"email\": \"xxx@gmail.com\",\n    \"affiliate_program_id\": 1,\n    \"affiliate_state_id\": 1,\n    \"status\": \"no_verify\",\n    \"apps_events\": [{\n        \"id\": 159026,\n        \"source\": \"bien_similaire\",\n        \"frequency\": \"real_time\",\n        \"status\": \"active\",\n        \"type\": \"rent\",\n        \"user_id\": 21578,\n        \"affiliate_program_id\": 1,\n        \"locations\": [{\n            \"location_id\": 7521,\n            \"inf\": 17198,\n            \"sup\": 17213\n        }, {\n            \"location_id\": 49364,\n            \"inf\": 98942,\n            \"sup\": 98943\n        }],\n        \"property_classes\": [\"appartment\", \"house\", \"appartment\", \"various\"],\n        \"price_min\": \"0\",\n        \"price_max\": \"1000\"\n    }, {\n        \"id\": 99662,\n        \"source\": \"bien_similaire\",\n        \"frequency\": \"real_time\",\n        \"status\": \"inactive\",\n        \"type\": \"rent\",\n        \"user_id\": 21578,\n        \"affiliate_program_id\": 1,\n        \"locations\": [{\n            \"location_id\": 7540,\n            \"inf\": 17138,\n            \"sup\": 17145\n        }, {\n            \"location_id\": 7647,\n            \"inf\": 17136,\n            \"sup\": 17137\n        }],\n        \"property_classes\": [\"appartment\", \"house\", \"appartment\", \"various\"]\n    }]\n  }\n}\nSo it search with inf AND sup but not on one location object. It seems ES do : \"inf greater than \"any inf in apps.events.locations.\u00a8\" AND \"sup less than any sup\"....\nIn my example, 98942 (inf) > 59850 AND 17213 (sup from same apps_events but not same locations)  < 59851 => wrong result.\nI think it's because elasticsearch doesn't understand my nested object, so now when I index, I do that:\nUser.index.create mappings: {\n  user: {\n    properties: {\n      name: { type: 'object' },\n      apps_events: {\n        properties: {\n          name: { type: 'object' },\n          locations: {\n            type: 'nested'\n          }\n        }\n      }\n    }\n  }\n}\nTire index is working BUT now my query doesn't work because I need to write it with double nested :)\nAnyone already do something like that ?\nIf I find a solution, I promise I write some integration test in Tire ;)\nThanks!\n. Here the answer :\nhttps://gist.github.com/imotov/5203687\nThanks a lot to @imotov\n. Seems good thanks !\n. Do you finally succeed? Because I'm stuck in the same situation with multiple terms stats :(\n. @karmi so if I need to do the same with TermStats to aggregate multi values on the same key, the best way to do that is:\n```\nrequire 'tire'\nrequire 'json'\nTire.index 'test-facet' do\n  delete\n  create\n  store title: 'One', channel_id: 'CHANNEL1', views_count: 12314, likes_count: 234\n  store title: 'Two', channel_id: 'CHANNEL2', views_count: 92834, likes_count: 678\n  store title: 'Three', channel_id: 'CHANNEL3', views_count: 213429, likes_count: 90\n  refresh\nend\nsearch = Tire.search 'test-facet' do\n  facet 'channels_views_count' do\n    terms_stats :channel_id, :views_count\n  end\nfacet 'channels_likes_count' do\n    terms_stats :channel_id, :likes_count\n  end\nend\nputs search.to_curl,\n \"---\",\n\n JSON.pretty_generate(search.results.facets)\n\n```\nAnd I merge the 2 arrays after? No solution to do that with 1 facet?\n. Yes so the only solution is to do multiple facet, 1 per fied. Seems good for me.\nI have an other question, how I can get on a terms stats the distinct count for example:\n```\nrequire 'tire'\nrequire 'json'\nTire.index 'test-facet' do\n  delete\n  create\n  store title: 'One', brand_id: 1, channel_id: 'CHANNEL1', views_count: 12314, likes_count: 234\n  store title: 'Two', brand_id: 2, channel_id: 'CHANNEL2', views_count: 92834, likes_count: 678\n  store title: 'Three', brand_id: 2, channel_id: 'CHANNEL3', views_count: 213429, likes_count: 90\n  refresh\nend\nsearch = Tire.search 'test-facet' do\n  facet 'brands_views_count' do\n    terms_stats :brand_id, :views_count\n  end\nfacet 'brands_channels' do\n    terms_stats :brand_id, :channel_id\n  end\nend\nputs search.to_curl,\n \"---\",\n\n JSON.pretty_generate(search.results.facets)\n\n```\nDoesn't work because it seems terms stats doesn't support string for value_field... Any idea that could help me move forward ?\n. Yes you answer in https://github.com/karmi/tire/issues/836\nThanks!\n. @karmi I don't think I can do that with just a 'terms' because I use terms stats to hash by brand_id and have the count for each one...\n. Sorry, but I don't understand what you're saying. If I do something like that:\n```\nrequire 'tire'\nrequire 'json'\nTire.index 'test-facet' do\n  delete\n  create\n  store title: 'One', brand_id: 1, channel_id: 'CHANNEL1', views_count: 12314, likes_count: 234\n  store title: 'Two', brand_id: 2, channel_id: 'CHANNEL2', views_count: 92834, likes_count: 678\n  store title: 'Three', brand_id: 2, channel_id: 'CHANNEL3', views_count: 213429, likes_count: 90\n  store title: 'Four', brand_id: 3, channel_id: 'CHANNEL2', views_count: 213429, likes_count: 90\n  refresh\nend\nsearch = Tire.search 'test-facet' do\n  facet 'brands_views_count' do\n    terms_stats :brand_id, :views_count\n  end\nfacet 'brands_channels' do\n    terms :channel_id\n  end\nend\nputs search.to_curl,\n \"---\",\n\n JSON.pretty_generate(search.results.facets)\n\n```\nThat's doesn't work because it return only channel_id.\nI would like:\nbrand_id : 1 channels_count : 1\nbrand_id : 2 channels_count : 2 (CHANNEL2 and CHANNEL3)\nbrand_id : 3 channels_count : 1\n. Yes\n. Okay for the record, you need to specified nil for value_field and the add the value script :+1: \nfacet 'advanced_stats', facet_filter: { terms: { :brand_ids => params[:brand_ids] } } do\n  terms_stats :brand_ids, nil, THE_VALUE_SCRIPT, size: 4, order: 'total'\nend\n@karmi no better solution right?\n. Okay thanks\n. ",
    "christoph-buente": "Well, this could be the reason. I do not need the mass import gem for the rails app to work. I have a minutely rake task running, which is doing the hard work. Parts of my gemfile look like this:\n```\nGemfile\ngem 'activerecord-import', '<0.3.0', :require => false\ngem 'tire'\nimport.rake\nrequire \"activerecord-import/base\"\nActiveRecord::Import.require_adapter('mysql2')\n```\nRequiring the activerecord-import gem in the Gemfile did the trick. Thanks a lot.\n. This is the backtrace when parsing the elasticsearch response json:\n[GEM_ROOT]/gems/multi_json-1.6.1/lib/multi_json/adapters/yajl.rb:12:in `parse'\n[GEM_ROOT]/gems/multi_json-1.6.1/lib/multi_json/adapters/yajl.rb:12:in `load'\n[GEM_ROOT]/gems/multi_json-1.6.1/lib/multi_json.rb:102:in `decode'\n[GEM_ROOT]/gems/tire-0.5.4/lib/tire/search.rb:141:in `perform'\n[GEM_ROOT]/gems/tire-0.5.4/lib/tire/search.rb:35:in `results'\n[GEM_ROOT]/gems/tire-0.5.4/lib/tire/model/search.rb:105:in `search'\napp/models/poi.rb:198:in `search_with_es'\n[GEM_ROOT]/gems/activerecord-3.0.20/lib/active_record/relation.rb:370:in `send'\n[GEM_ROOT]/gems/activerecord-3.0.20/lib/active_record/relation.rb:370:in `method_missing'\n[GEM_ROOT]/gems/activerecord-3.0.20/lib/active_record/relation.rb:125:in `scoping'\n[GEM_ROOT]/gems/activerecord-3.0.20/lib/active_record/relation.rb:370:in `method_missing'\napp/controllers/api/nodes_controller.rb:115:in `collection'\n[GEM_ROOT]/gems/inherited_resources-1.2.2/lib/inherited_resources/actions.rb:7:in `index!'\napp/controllers/api/nodes_controller.rb:19:in `index'\nWhat is the filtered filter? This is the resulting request when using alle options:\ncurl -X GET 'http://localhost:9200/development-wheelmap-pois/poi/_search?from=0&load=true&size=1000&\npretty' -d '{\n  \"from\": 0,\n  \"size\": 1000,\n  \"filter\": {\n    \"and\": [\n      {\n        \"geo_bounding_box\": {\n          \"location\": {\n            \"top_left\": [\n              13.264,\n              52.511\n            ],\n            \"bottom_right\": [\n              13.366,\n              52.489\n            ]\n          }\n        }\n      },\n      {\n        \"term\": {\n          \"category_id\": 1\n        }\n      },\n      {\n        \"term\": {\n          \"wheelchair\": \"yes\"\n        }\n      }\n    ]\n  },\n  \"sort\": [\n    {\n      \"_geo_distance\": {\n        \"order\": \"asc\",\n        \"location\": {\n          \"lat\": \"52.49328\",\n          \"lon\": \"13.34708\"\n        }\n      }\n    }\n  ]\n}'\nSeems to work fine. Except the weird errors :)\n. Ok, i'll give it a go with the filtered query\nThe problem might persist. I ditched elasticsearch in production for now, cause i couldn't figure out how to fix this issue.\n. I did, but there was nothing to see. Looked all normal.\nI had 1.5M record indexed, so the file is 1.4GB. I think it's not worth downloading. But here is the link: http://staging.wheelmap.org/tire_staging.log\n. ",
    "Hengjie": "Characters '[' or ']' will cause Elastic Search to crash. It's in section 2 of this article: http://blog.tutorwith.me/2012/02/implementing-your-own-site-search-in-a-rails-app/\n. I've tried using match and even then that fails when I use [ or any other special characters.\n. ",
    "jordan-thoms": "@karmi  - Create any elastic search model, run  Model.search \"[a}\" - and it'll fail, since the argument is not escaped.\nirb(main):001:0> Document.search \"[a}\"\n[REQUEST FAILED] curl -X GET \"http://elastic:password@elasticsearch:8080/documents/document/_search?pretty=true\" -d '{\"query\":{\"query_string\":{\"query\":\"[a}\"}}}'\nTire::Search::SearchRequestFailed: 500 : {\"error\":\"SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[XlBSTzQISwCs1zmdcm42-A][documents][0]: SearchParseException[[documents][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\\"query\\\":{\\\"query_string\\\":{\\\"query\\\":\\\"[a}\\\"}}}]]]; nested: QueryParsingException[[documents] Failed to parse query [[a}]]; nested: ParseException[Cannot parse '[a}': Encountered \\\"<EOF>\\\" at line 1, column 3.\\nWas expecting one of:\\n    \\\"TO\\\" ...\\n    <RANGEIN_QUOTED> ...\\n    <RANGEIN_GOOP> ...\\n    ]; nested: ParseException[Encountered \\\"<EOF>\\\" at line 1, column 3.\\nWas expecting one of:\\n    \\\"TO\\\" ...\\n    <RANGEIN_QUOTED> ...\\n    <RANGEIN_GOOP> ...\\n    ]; }{[XlBSTzQISwCs1zmdcm42-A][documents][1]: SearchParseException[[documents][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\\"query\\\":{\\\"query_string\\\":{\\\"query\\\":\\\"[a}\\\"}}}]]]; nested: QueryParsingException[[documents] Failed to parse query [[a}]]; nested: ParseException[Cannot parse '[a}': Encountered \\\"<EOF>\\\" at line 1, column 3.\\nWas expecting one of:\\n    \\\"TO\\\" ...\\n    <RANGEIN_QUOTED> ...\\n    <RANGEIN_GOOP> ...\\n    ]; nested: ParseException[Encountered \\\"<EOF>\\\" at line 1, column 3.\\nWas expecting one of:\\n    \\\"TO\\\" ...\\n    <RANGEIN_QUOTED> ...\\n    <RANGEIN_GOOP> ...\\n    ]; }{[XlBSTzQISwCs1zmdcm42-A][documents][2]: SearchParseException[[documents][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\\"query\\\":{\\\"query_string\\\":{\\\"query\\\":\\\"[a}\\\"}}}]]]; nested: QueryParsingException[[documents] Failed to parse query [[a}]]; nested: ParseException[Cannot parse '[a}': Encountered \\\"<EOF>\\\" at line 1, column 3.\\nWas expecting one of:\\n    \\\"TO\\\" ...\\n    <RANGEIN_QUOTED> ...\\n    <RANGEIN_GOOP> ...\\n    ]; nested: ParseException[Encountered \\\"<EOF>\\\" at line 1, column 3.\\nWas expecting one of:\\n    \\\"TO\\\" ...\\n    <RANGEIN_QUOTED> ...\\n    <RANGEIN_GOOP> ...\\n    ]; }{[XlBSTzQISwCs1zmdcm42-A][documents][3]: SearchParseException[[documents][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\\"query\\\":{\\\"query_string\\\":{\\\"query\\\":\\\"[a}\\\"}}}]]]; nested: QueryParsingException[[documents] Failed to parse query [[a}]]; nested: ParseException[Cannot parse '[a}': Encountered \\\"<EOF>\\\" at line 1, column 3.\\nWas expecting one of:\\n    \\\"TO\\\" ...\\n    <RANGEIN_QUOTED> ...\\n    <RANGEIN_GOOP> ...\\n    ]; nested: ParseException[Encountered \\\"<EOF>\\\" at line 1, column 3.\\nWas expecting one of:\\n    \\\"TO\\\" ...\\n    <RANGEIN_QUOTED> ...\\n    <RANGEIN_GOOP> ...\\n    ]; }{[XlBSTzQISwCs1zmdcm42-A][documents][4]: SearchParseException[[documents][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\\"query\\\":{\\\"query_string\\\":{\\\"query\\\":\\\"[a}\\\"}}}]]]; nested: QueryParsingException[[documents] Failed to parse query [[a}]]; nested: ParseException[Cannot parse '[a}': Encountered \\\"<EOF>\\\" at line 1, column 3.\\nWas expecting one of:\\n    \\\"TO\\\" ...\\n    <RANGEIN_QUOTED> ...\\n    <RANGEIN_GOOP> ...\\n    ]; nested: ParseException[Encountered \\\"<EOF>\\\" at line 1, column 3.\\nWas expecting one of:\\n    \\\"TO\\\" ...\\n    <RANGEIN_QUOTED> ...\\n    <RANGEIN_GOOP> ...\\n    ]; }]\",\"status\":500}\nClearly these methods are not designed for passing user input in since they don't do escaping, but in that case the example should be updated to follow the best practice when passing user input ( see https://github.com/karmi/tire/blob/master/examples/rails-application-template.rb#L204 ) - I don't know what that best practice is though, hence why I created this issue.\nThis is a gotcha with the search methods, and I do think it merits being included in the documentation.\nThanks for your work on tire BTW - we've found it very useful.\n. ",
    "nickcoyne": "Another example\nCurl::Err::ConnectionFailedError: Curl::Err::ConnectionFailedError\n\nvendor/bundle/ruby/1.9.1/gems/curb-0.8.3/lib/curl/easy.rb:60 \u2022 perform\nvendor/bundle/ruby/1.9.1/gems/tire-0.5.4/lib/tire/http/clients/curb.rb:33 \u2022 http_post\nvendor/bundle/ruby/1.9.1/gems/tire-0.5.4/lib/tire/http/clients/curb.rb:33 \u2022 post\nvendor/bundle/ruby/1.9.1/gems/tire-0.5.4/lib/tire/index.rb:87 \u2022 store\n. Yep, this is happening in the background when a new record is created. I'm tempted to try something like this approach:\n\nruby\ndef self.post(url, data)\n  tries = 2\n  begin\n    @client.url = url\n    @client.post_body = data\n    @client.http_post\n    Response.new @client.body_str, @client.response_code\n  rescue Curl::Err::GotNothingError => e\n    tries -= 1\n    sleep 0.1\n    retry if tries > 0\n  end\nend\nI'm not sure if the connectivity is lost for just a microsecond (which this might solve), or if its a few seconds, in which case things will still fail. Perhaps I should just try this and see if alleviates the issue.\nThere is no way I can rescue in my model code if I'm using the included Callbacks, is there?\n. Ignore my last question, I see it is quite straightforward to not include Callbacks.\n. Thanks, I think I will take that approach.\n. Sorry, yes.\n. Thanks, that helps. \nThen a question that I guess is more related to ElasticSearch than Tire... why does it only return a result when I specify the field in the query? Using the above example, Lesson.search('haystacks') does return a result (the lesson name includes the word \"haystacks\"), but Lesson.search('haystack') returns no result.\n. Thanks for that. Problem solved.\n. ",
    "9mm": "I'm not certain yet but I may be having the same issue.\n```\nA RestClient::RequestTimeout occurred in resources#create:\nRequest Timeout\n rest-client (1.6.7) lib/restclient/request.rb:184:in `rescue in transmit'\n```\nI forced a re-index to see maybe if that helps.\nrest-client (1.6.7) lib/restclient/request.rb:184:in `rescue in transmit'\n rest-client (1.6.7) lib/restclient/request.rb:140:in `transmit'\n rest-client (1.6.7) lib/restclient/request.rb:64:in `execute'\n rest-client (1.6.7) lib/restclient/request.rb:33:in `execute'\n rest-client (1.6.7) lib/restclient.rb:72:in `post'\n tire (0.6.1) lib/tire/http/client.rb:19:in `post'\n tire (0.6.1) lib/tire/index.rb:146:in `store'\n tire (0.6.1) lib/tire/model/search.rb:148:in `block in update_index'\n activesupport (3.2.15) lib/active_support/callbacks.rb:403:in `_run__1215428825891427617__update_elasticsearch_index__1803585120871585077__callbacks'\n activesupport (3.2.15) lib/active_support/callbacks.rb:405:in `__run_callback'\n activesupport (3.2.15) lib/active_support/callbacks.rb:385:in `_run_update_elasticsearch_index_callbacks'\n activesupport (3.2.15) lib/active_support/callbacks.rb:81:in `run_callbacks'\n tire (0.6.1) lib/tire/model/search.rb:144:in `update_index'\n tire (0.6.1) lib/tire/model/callbacks.rb:21:in `block in included'\nIf that doesn't work I'll reboot server maybe. It's been running for like 2 years now flawlessly with elasticsearch, so who knows.\n. ",
    "weblee": "Ok sorry for being thick where would i stub this?\n. Thanks for your help Karmi but im just not getting it. I have tried adding the stub to my spec_helper in various ways and its just not picking up the stub. Do I need to add the stub directly into the spec_helper.rb ?? Im new to testing if you have not noticed!! I hope you can advise a little bit more!\n. I found the issue.\nMy filter was close but should have been as string not an array ie,\n``` ruby\nfilter :geo_bounding_box, :location => {\n          :top_left => \"#{params[:geo_bounding_box][:top_left_lat]}, #{params[:geo_bounding_box][:top_left_lon]}\",\n          :bottom_right => \"#{params[:geo_bounding_box][:bottom_right_lat]}, #{params[:geo_bounding_box][:bottom_right_lon]}\"\n        }\n```\n. Did you set the host up in Tire config ?\nruby\nTire.configure do\n  url \"http://hostname.com\"\nend\n. ",
    "mowings": "Hi karmi. So, for us the concern is largely academic since we're not using the AR integration (mainly because we need a pretty decent separation between model and search logic for design reasons). \nHowever, just from looking at the code, it looks as if the after_save/destroy  callback installed by models/callbacks.rb invokes tire.update_index.\nThis method in turn calls index.store/remove when the model instance is saved or removed (model/persistence/storage.rb).\n'index' when called on an instance looks to be fetching the class instance of the index (model/search.rb ):\ndef index\n      instance.class.tire.index\n    end\nAll this would be fine, except that the store() method in index (along with other methods -- see index.rb) appears to save its results in an instance variable of the index itself and then decode it:\n@response = Configuration.client.post url, document\n  MultiJson.decode(@response.body)\nMy concern is that this bit isn't threadsafe; if two threads happen to call after_save against model instances of the same class, a single (class-level) index instance will be used to  perform the save. This could potentially allow @response to get stepped on by either thread, creating a race condition. Seems like this issue would be more of an issue with GIL-less jruby, but I imagine it could happen with MRI as well.\nOf course, I could have something wrong here or missed something. Cheers -- m\nUPDATED: typo\n. Hi karmi --sorry to get back post-close. I simply meant that we really don't need the ActiveRecord integration -- we index the same model in vastly different ways depending on a number of factors. We also don;'t necessarily index immediately at save time. \nTire without the AR integration is just fine for our purposes. Thanks again for the great work.\n. ",
    "stefanofontanelli": "I am really sorry @karmi for my delay responding you.\nI use ES on found.no, I use aliases in the same way reported by @dfuentes77 and I have same issues.\nI am not able to reproduce because the errors is reported to me by airbrake many times but not always during uptime of the system. It is hard to me send you information about how to reproduce it because the architecture is pretty complex: I have a lot of concurrent processes that take messages from queues and query ES to perform their tasks.\nI don't know the reason but I think that add a check for nil value should be done anyway :)\nI will send you more information if I will able to find it.\nThank you for the great work and support.\n. @karmi Yes it make sense :)\nMy Found.no cluster returns a JSON like this:\n```\n$ curl http://localhost:9200/_aliases?pretty\n{\n  \"data-28-03-2013\" : {\n    \"aliases\" : {\n      \"data\" : { }\n    }\n  },\n  \"data-30-03-2013\" : {\n    \"aliases\" : { }\n  }\n}\n$ curl http://localhost:9200/data/_aliases?pretty\n{\n  \"data-28-03-2013\" : {\n    \"aliases\" : {\n      \"data\" : { }\n    }\n  }\n}\n$ curl http://localhost:9200/data-28-03-2013/_aliases?pretty\n{\n  \"data-28-03-2013\" : {\n    \"aliases\" : {\n      \"data\" : { }\n    }\n  }\n}\n$ curl http://localhost:9200/data-30-03-2013/_aliases?pretty\n{\n  \"profiles-30-03-2013\" : {\n    \"aliases\" : { }\n  }\n}\n$ curl http://localhost:9200/not-existent/_aliases?pretty\n{}\n```\n. @karmi Yes, make sense.\n. ",
    "ggrillone": "No, I did not. Thanks! I'll try that out and update you on if it worked or not.\n. Yes, elasticsearch would be installed on my app server, but communicating with a remote database.\n. Yep. It is connected to the database using the host and credentials that are setup in the database.yml file\ndevelopment:\n  adapter: postgresql\n  encoding: unicode\n  database: database_name\n  pool: 5\n  timeout: 5000\n  port: 5432\n  username: db_username\n  password: db_password\n  host: ip_to_remote_server\n. I don't receive any error. It's just that when I update, add, or create a new record it is not auto-indexed so it doesn't appear in my table until I manually run the tire:import command. Is there any code or logger output I could post that would help?\n. What triggers a record to be auto indexed? Is it an actual database insert, or a rails action?\n. I'm using a sql query with ActiveRecord::Base.connection.execute(query) inside my create action, so maybe that's why it's not being indexed\n. For now to try and get it working, I added @order.save after I execute the sql query, in order to trigger the after_save callback. Tire then logs the text below to my console. \nBut then when I go and use the search function or try and sort to get the latest timestamp value it does not appear. I am using datatables to display the data by the way. And one interested thing noted here. So at the bottom of the table I display how many records are filtered out of all the records in the table. Keep in mind that I added 2 new records:\nShowing 1 to 30 of 4,164 entries (filtered from 4,162 total entries)\nOutput from console after creating record\n```\n2013-03-20 20:14:27:119 [order/9696] (\"orders\")\n\ncurl -X POST \"http://localhost:9200/orders/order/9696\" -d\n'{\n    \"ccy_base\": \"SEK\",\n    \"ccy_contra\": \"AUD\",\n    \"ccy_dealt\": \"SEK\",\n    \"ccy_pair\": \"SEKAUD\",\n    \"ccy_term\": \"AUD\",\n    \"counterparty_id\": null,\n    \"currency_pair_id\": 111,\n    \"execution_timestamp\": null,\n    \"id\": 9696,\n    \"input_date\": \"2013-03-21T00:14:26Z\",\n    \"locked\": false,\n    \"oms_number\": \"BFX-183\",\n    \"order_source\": \"GUI\",\n    \"order_status\": \"NO\",\n    \"order_sub_status\": null,\n    \"order_sub_type\": null,\n    \"order_type\": \"SPOT\",\n    \"scheduled_time\": null,\n    \"trade_date\": \"2013-03-21\",\n    \"ts\": \"2013-03-21T00:14:26Z\",\n    \"ts_timezone\": \"PST\",\n    \"user_id\": 1,\n    \"order_legs\": [\n        {\n            \"all_in_price\": null,\n            \"buy_sell_base\": \"BUY\",\n            \"contra_quantity\": null,\n            \"dealt_quantity\": 10000000,\n            \"exec_bid_ask\": \"ASK\",\n            \"fwd_pts\": null,\n            \"id\": 9697,\n            \"mult_or_divide\": \"M\",\n            \"near_far_indicator\": \"N\",\n            \"order_id\": 9696,\n            \"side\": \"BUY\",\n            \"spot_price\": null,\n            \"tenor_id\": 19,\n            \"value_date\": \"2013-01-03\",\n            \"order_accounts\": [\n                {\n                    \"account_id\": 49,\n                    \"all_in_price\": null,\n                    \"buy_sell_base\": \"BUY\",\n                    \"contra_quantity\": null,\n                    \"dealt_quantity\": 10000000,\n                    \"fwd_pts\": null,\n                    \"id\": 10215,\n                    \"order_leg_id\": 9697,\n                    \"side\": \"BUY\",\n                    \"spot_price\": null,\n                    \"account\": {\n                        \"account_no\": \"MCER\"\n                    }\n                }\n            ],\n            \"order_executions\": []\n        }\n    ],\n    \"order_charts\": []\n}'\n2013-03-20 20:14:27:119 [201]\n``\n. Scratch that. It does work! Thanks. Reason it wasn't showing up because in those 2 new records I set an attribute to nil, and when rendering that particular attribute on the view, I was usingstrftime`, which errored out because of the null value. All good now.\n. ",
    "rahulag": "I am using find_all_by_ids.. this doesn;t throw error when record is not there and returns an array even when there is a single record to be fetched. \nAlso using Array.new in terms query. This is for backward compatibility if a non array value is used for terms query.\n. ",
    "thalesfp": "To solve this I added \"require 'ANSI'\" after \"require 'rake'\" in tasks.rb (/opt/ruby/lib/ruby/gems/1.9.1/gems/tire-0.5.6/lib/tire/tasks.rb)\n. ",
    "sr3d": "@karmi has the gem been updated?\n. ",
    "ichilton": "I get this same error, and when i've checked tasks.rb, the change suggested above has already been made:\n$ bundle exec rake environment tire:import --trace\n* Invoke environment (first_time)\n* Execute environment\n* Invoke tire:import (first_time)\n* Invoke tire:import:model (first_time)\n* Execute tire:import:model\n[IMPORT] Rails detected, loading environment...\n* Invoke environment \nrake aborted!\nuninitialized constant HRULE\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/tire-0.5.7/lib/tire/tasks.rb:98:in block (3 levels) in <top (required)>'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/task.rb:228:incall'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/task.rb:228:in block in execute'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/task.rb:223:ineach'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/task.rb:223:in execute'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/task.rb:166:inblock in invoke_with_call_chain'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/monitor.rb:211:in mon_synchronize'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/task.rb:159:ininvoke_with_call_chain'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/task.rb:187:in block in invoke_prerequisites'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/task.rb:185:ineach'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/task.rb:185:in invoke_prerequisites'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/task.rb:165:inblock in invoke_with_call_chain'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/monitor.rb:211:in mon_synchronize'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/task.rb:159:ininvoke_with_call_chain'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/task.rb:152:in invoke'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/application.rb:143:ininvoke_task'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/application.rb:101:in block (2 levels) in top_level'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/application.rb:101:ineach'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/application.rb:101:in block in top_level'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/application.rb:110:inrun_with_threads'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/application.rb:95:in top_level'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/application.rb:73:inblock in run'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/application.rb:160:in standard_exception_handling'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/lib/rake/application.rb:70:inrun'\n/home/vagrant/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/rake-10.0.3/bin/rake:33:in <top (required)>'\n/home/vagrant/.rbenv/versions/1.9.3-p194/bin/rake:23:inload'\n/home/vagrant/.rbenv/versions/1.9.3-p194/bin/rake:23:in `'\nTasks: TOP => tire:import => tire:import:model\n. @karmi Still have the issue.\nAlthough it seems to only happen if I just run:\nbundle exec rake environment tire:import\nI just tried specifying a class, like:\nbundle exec rake environment tire:import CLASS='Article'\n...and that seemed to work fine.\nThanks,\nIan\n. ",
    "radixhound": "So helpful. Thank you!\n. Some more detail would probably improve your chances of a response. Why not include the actual errors and stack traces?\n. Thanks for the response. As I was working on a reply I found the culprit. Brilliant question.\nWe have a button in the admin that triggers a manual rebuild of the index. It sets the name of the index to the new name and triggers the rebuild, but doesn't change the name back to default, so whichever ruby process is used to trigger a manual rebuild will be stuck on the old index. whew.\n. ",
    "nazarhussain": "For first one, I tried the curl option and it is showing the same count as head plugin, which is almost 20K less than the records count in database. \nI will try the find_in_batches and will let you know. \n. Really thanks for your input. New feature find_in_batches has some performance improvment when every things is active record relation which is going to be used in to_indexed_json, \nIn my case, I am building that json of some complex business logic. I had enabled the logging and watched the logs deeply, major time is taking by ActiveRecord queries, and that's the reason it is slow. \nIs there any plugin or tool available that I made that index on some super fast computer and then export/import it to my live server? I googled and found knapsack, but for me it is not working, can you take a look into it https://github.com/jprante/elasticsearch-knapsack/issues/3 That will be great help for me. \n. @karmi Yes you are right. That's what I finally think to focus on. I am closing this issue, thanks for your great input. And thanks for that great tip of scp the indexed data. That will help us a lot. \n. ",
    "hyperrjas": "I have removed the tire.rb and I have added this code on top of my Object model:\nrequire 'tire'\nTire.configure { url \"http://myserver.com:9200\" }\nand now is working fine!\n. The error was in my /etc/mongodb.conf\nThank you!\n. I have added the source:\nTire.configure do\n  url \"http://remoteserver.com:9200\"\nend\nto my production.rb file and the problem was fixed.\nAlso, I have removed the config/initializers/tire.rb file\nThank you!\n. I have fixed the error with each_with_hit:\n    <% @cars.each_with_hit do |result, hit| %>\n<%= raw hit['highlight']['title'].map(&:inspect).join(',').gsub('\"', '') if hit['highlight']['title'] %>\n<%= raw hit['highlight']['description'].map(&:inspect).join(',').gsub('\"', '') if hit['highlight']['description'] %>\n<% end %>\nI don't know if this is the best approach...\nThanks!\n. Thank you very much for this explanation!\n. Thanks. I'm using this gem for autocomplete https://github.com/leschenko/elasticsearch_autocomplete. All facets with car fields does works fine except this.\nThe unique requirement of this gem is add fields like this:\nclass Car\n  include Mongoid::Document\n  ac_field :name, :user_city, :skip_settings => true\n  def self.ac_search(param)\n   .\n   .\n   .\n  end\n  def user_city\n    user.city\n  end\nend\nI have reindex the Car model with:\nbundle exec rake environment tire:import CLASS=Car FORCE=true\nWhat am I missing or what I'm doing wrong?\nThanks!\n. Thanks, I fixed the problem with:\ndef to_indexed_json\n  to_json(methods: [:user_city])\nend\nThanks!\n. ",
    "taimoor": "@hyperrjas can you elaborate what was the error in /etc/mongodb.conf\n. ",
    "mulderp": "Hm.. I can't confirm this commit to make my ActiveModelSerializer work. I am using:\nmovies = Movie.search(params[:q], :load => true)\n    render :json => movies, :each_serializer => MovieSerializer\nBut get a:\nNoMethodError (undefined method `<=' for false:FalseClass)\nLet me know, if I can test some variation.\n. ",
    "robinbrandt": "@karmi any thoughts?\n. We don't use it yet but probably in a couple of hours ;) I have to play around with it a bit more but my first expression was that geo distance is just another filter and the filtering DSL is flexible enough to support it so there's no need for another special geo syntax. \nBut I'll give it a try. When I find out I was wrong, I'll grab the ticket, ok?\n. Hi @karmi !\nAny idea if this could make it into the next tire release?\n. No problem, thanks for your work.\n. You're using Ruby 1.9 Hash syntax here. I don't know the general policy here (I just added my first pull request myself) but the other tests (at least the unit tests) use the 1.8 Hash syntax. But from some older commits and the documentation you can see that 1.8 compatibility is desired.\n. ",
    "jeyb": "@karmi @robinbrandt I can definitely update to Ruby 1.8 syntax.\n@karmi If you can point out where I can find a list of supported ORMs I can work on a Strategy implementation like done here https://github.com/karmi/tire/commit/efb9c14. Is this already being worked on? Don't want to dup effort. But this fix is critical for my project so I do wan't to work on it soon.\n. Didn't get a chance to work on this till a few minutes ago. How does this look? https://github.com/karmi/tire/pull/810\n. @karmi whatcha think?\n. ",
    "rubytastic": "Thanks that totally works, just merging in the params - smart - i was looking for tire to handle a default set of options but this works perfect!\n. This fails also, who knows how to do this? \nProfile.rb:\n```\naccepts_nested_attributes_for :match\ndelegate        :looking_for_skillset,         :to => :match\nmapping do\n  indexes :id,             type: \"integer\"\n  indexes :looking_for_skillset,    type: \"string\"\n end\n```\n. edit:\nThank you Karmi for your fast reply. I tried those suggestions but seem to have still some problems. \nWith Elasticsearch-head Browsing the contents I can see the fields are now listed but not with skillset value.\nMy added tire code for profile and match models is below, do you see my mistake? Followed instructions but stil fail :(\nProfile.rb\n```\n  has_one :match\n  after_touch() { tire.update_index }\n  self.include_root_in_json = false\ninclude Tire::Model::Search\n  include Tire::Model::Callbacks\ndef to_indexed_json\n    to_json(include: {match: {only: [:text]}})\n  end\nmapping do\n     indexes :id,                           type: \"integer\"\n     indexes :created_at,               type: 'date'\n     indexes :match do\n       indexes :looking_for_skillset, type: 'string'\n    end\n   end\n```\nMatch.rb\nbelongs_to    :profile, touch: true\n  validates     :profile_id, :uniqueness => true\nWith elasticsearch-head index browser html Im able to see the fields are added like \"profile.match.looking_for_skillset\" however there is not created an index and the search results return nothing ( I set 1 database column for skillset to value \"student\") this is not indexed however.\nUsing rails 3 latest as of now. Any thoughts on why this fails or perhaps what I did wrong? Kind regards \n. The output of p.to_indexed_json from rails console:\n2.0.0p0 :009 > p.to_indexed_json\n  Match Load (0.3ms)  SELECT `matches`.* FROM `matches` WHERE `matches`.`profile_id` = 1 LIMIT 1\n => \"{\\\"state\\\":\\\"3\\\",\\\"status\\\":\\\"student\\\",\\\"step\\\":0,\\\"updated_at\\\":\\\"2013-06-08T14:46:21+02:00\\\",\\\"match\\\":{}}\" \n2.0.0p0 :010 >\nThe output of localhost:9200/profiles/_search?pretty is:\n{\n  \"took\" : 3,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 2,\n    \"max_score\" : 1.0,\n    \"hits\" : [ {\n      \"_index\" : \"profiles\",\n      \"_type\" : \"profile\",\n      \"_id\" : \"1\",\n      \"_score\" : 1.0, \"_source\" : {\"state\":\"3\",\"status\":\"student\",\"step\":0,\"updated_at\":\"2013-06-08T14:46:21+02:00\",\"match\":{}}\n    }, {\n      \"_index\" : \"profiles\",\n      \"_type\" : \"profile\",\n      \"_id\" : \"2\",\n      \"_score\" : 1.0, \"_source\" : {\"state\":\"5\",\"status\":\"teacher\",\"step\":0,\"updated_at\":\"2013-06-04T00:10:37+02:00\",\"match\":{}}\n    } ]\n  }\n}\nThe skillset column seems not to be indexed. \nThank you for your help.\n. Have not been able to find the related issue on this, anyone any idea what might cause this?\n. ",
    "barkerja": "http://www.elasticsearch.org/guide/reference/query-dsl/missing-filter/\nPerhaps you may want to use a missing filter on your deleted_at column, instead of actually removing that record from the index. Using this would only return records with an empty deleted_at value when needed.\n. Thanks @karmi. It looks like ignore_malformed is exactly what I was looking for. Cheers.\n. ",
    "xxx": "eh, this isn't quite the issue.\nI guess if you change the settings for an index, you have to delete the current index, quit the console/irb session you're in, and restart it, or it will always use the default settings. I still think there's a bug here somewhere, but the issue described above isn't it.\n. ",
    "leschenko": "There is a lot of models with custom paginate method for old tire bulk import scheme. So it would be nice to have the ability to use them after tire update via WillPaginate strategy.\n1/ I didn't find a good place for test because import strategies didn't covered with unit tests yet\n2/ definitely!\n3/ may be\nMay be this approach isn't elegant, but I didn't find better way yet.\n. Hello, sorry for delay.\nI added unit test for this option and compressed the code into one line.\nI have question about integration test: is it a test for rake tasks or Tire::Tasks::Import module?\n. ",
    "dariusgm": "I think the example App should run instantly. As I downloaded it, it didn't work. After adding the run time it worked. I don't know if this should be handled by the user or be inside the example.\n. Using Ruby 2.0-p0 as Run time Environment create this error while launching the rails server. The Rake Task Abort with the message:\nCould not find a JavaScript runtime. See https://github.com/sstephenson/execjs for a list of available runtimes. (...) - same while indexing the database.\n. ",
    "brandones": "I see the fact that \"to_ary\" does not return an array as a specific problem with the implementation (or maybe the design).\nThe change doesn't appear to have broken my app nor tire's tests. But many tests errored out on my machine, so I'm not totally sure.\n. The change seems not to have affected either the website I've been building or the Tire template site (\"tired\").\nAs an additional note, I found while working with the template site that inserting\nartarr = @articles.to_ary\nputs artarr\nat app/controllers/articles_controller.rb:17 would throw \nTypeError in ArticlesController#search\ncan't convert Tire::Results::Collection to Array (Tire::Results::Collection#to_ary gives Tire::Results::Collection)\nwhich, with the change, it no longer does.\n. ",
    "ivobenedito": "Started using @brandones PR in a project with tire + rabl and everything seems to work fine.\nPreviously, wasn't able to render tire collections with rabl due to this bug.\n. ",
    "pzac": "The stack trace unfortunately isn't very helpful, it looks like ES is not returning the expected results. I wonder if it could be due to timing issues. Adding a refresh request after the record insertion could help maybe?\nTire::ConstantScoreQueriesIntegrationTest\n     FAIL (2:00:15.906) test: Constant score queries in the featured results scenario should return featured results first.\n          <\"Kitchen tool tool tool\"> expected but was\n          <\"Garage tool\">.\n        @ test/integration/constant_score_queries_test.rb:60:in `block (3 levels) in <class:ConstantScoreQueriesIntegrationTest>'\n          /Users/paozac/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/shoulda-context-1.1.1/lib/shoulda/context/context.rb:400:in `call'\n          /Users/paozac/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/shoulda-context-1.1.1/lib/shoulda/context/context.rb:400:in `block in create_test_from_should_hash'\n          /Users/paozac/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/mocha-0.13.3/lib/mocha/integration/mini_test/version_2112_to_320.rb:40:in `run'\nA new failure here:\nFAIL (5:00:00.146) test: Percolator when percolating a document should return an empty array when no query matches.\n          <[]> expected but was\n          <nil>.\n        @ test/integration/percolator_test.rb:61:in `block (3 levels) in <class:PercolatorIntegrationTest>'\n          /Users/paozac/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/shoulda-context-1.1.1/lib/shoulda/context/context.rb:400:in `call'\n          /Users/paozac/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/shoulda-context-1.1.1/lib/shoulda/context/context.rb:400:in `block in create_test_from_should_hash'\n          /Users/paozac/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/mocha-0.13.3/lib/mocha/integration/mini_test/version_2112_to_320.rb:40:in `run'\nI've ran the tests twice in a row and got the errors only the first time.\n. Great! Cheers\n. ",
    "tobycox": "I'm no mongoid expert either, but from what I understand mongoid queries (Criteria) are lazily evaluated and they wrap up the mongo cursor. The cursor by default returns 100 documents, or 1mb of data.\nSo calling klass.all wont load the entire collection into memory, it'll just load each object as it is iterated over - in this case only loading each batch into memory.\nWe didn't see any memory issues using klass.all.each on our 5GB collection.\n. OK cool. My commit still uses the same batch limit as before (1000 by default), that 100 document limit is just what happens under the hood in mongoid.\n. Ah, true. Good spotting!\n. ",
    "SixiS": "Passes locally.\n807 tests, 807 passed, 0 failures, 0 errors, 0 skips, 1827 assertions\n. Oh awesome, didn't know the Array constructor worked like that.\nAdded some tests to show the problem. Added integration tests for active model and persisted model.\nIt only breaks using the persisted model. \ntest/integration/persisted_model_test.rb:47 is the test that should break if you undo the fix.\n. Great, thanks! The active record test was not failing, I was having the issue using the persistence model.\nIf you do not use load: true with the persistence model, methods on the returned objects return nil. If you do use load :true before the array fix you got the detect problem.\nTho maybe the fact that the methods return nil and not what you expect is another problem on its own.\nIt only occurs when using Tire.search, items returned by Model.search work fine if you call methods on them.\nI think it is because the objects returned are not real instances of their models but a sort of casted Item model.\nThanks for taking the time to close this issue!\n. Exactly :smile: \nWe are using Tire.search for more complicated searches on the persisted indexes (multiple index and multiple search). Which is where we were running into the 'no method detect' problem because we wanted to use load: true.\n. ",
    "justinko": "Well let's merge this muthafucka! (Samuel L. Jackson voice)\n. ",
    "threez": "sure i when will be the next release this things are in?\n. I can tell you in the next 24h if it is working. Thx for taking care of the issue...\n. Nice, Faraday::Error::TimeoutError and Faraday::Error::ConnectionFailed are handled correctly.\n. The problem is, that the server is not reachable. So if you start your app and your elastic search cluster is not up and running yet you will face the problem. I think it max be the default to create the indexes on startup but should be  possible to disable it.\n. With the current HEAD i got:\nSkipping index creation, cannot connect to Elasticsearch\n(The original exception was: #<Faraday::Error::ConnectionFailed>)\n. No not really by faraday. But faraday supports many drivers. And currently Faraday only supports [:MissingDependency, :ClientError, :ConnectionFailed, :ResourceNotFound, :ParsingError, :TimeoutError] but what is, if one of the drivers throws this exception or something similar like Errno::EADDRNOTAVAIL? In our setup we use our own relover based on resolv (this is why i point this out). I guess the point is, that if you load the application the application should not fail because of a connection / name resolution can't be made. In that sense it is similar to #730.\n. I guess i my specific case #730 would help. In other words turning off the automatic index creation. Otherwise maybe an public interface, where i can tell tire which errors should be handled additionally when using automatic index creation.\n. A Tire.configure { model_auto_create_index false } is implementation of #730. This would help a lot already and would be great (i maybe over simplified the summary a bit). The question is just then if is helpful for users of tire if their app breaks to start if the index auto creation failed. I think this is a good default for development, but in some cases you might want to prevent this. So it might be, that people, that don't want do deactivate index creation are better of with Tire.configure { raise_if_model_auto_create_index_fails false }.\n. I think this are two different issues. Sure they are related, but the error handling of problems during the creation is something different for me. Another example: Suppose the ElasticSearch instance is behind a reverse proxy (for https and auth) then the reverse proxy search might return a 504 if elastic search is unavailable. This would raise a Faraday::Error::ClientError that currently is not handled also. No connection and resolve issues are important but not the only problems. \n. @karmi I prefer the no_timeout method since it is not silently failing like a with(options) block would do. Some of the options might be called different in the future. The specs might not detect the problem because they don't exceed the timeout. So i would stick to the no_timeout method. The no_timeout is only activated for that particular query, so  im not sure what you mean with closing the cursor after the block. Do you mean to kill the cursor if an error occurs?\n. Actually i'm not sure, i raised a question to the moped project. Because i think this should be handled in the driver. The driver should not allow leaking of cursors, even if they don't have a timeout. https://github.com/mongoid/moped/issues/206\n. Hi, can you tell a little bit more about your system env?\nI used this to find out about the tire mongoid interactions:\nTire.configure { logger 'elasticsearch.log', :level => 'debug' }\nMoped.logger.level = Logger::DEBUG\n. ",
    "meiraleal": "The model is already address, Karel. This is exactly the point. Tire tries\nto singularize address using classify, and it returns addres.\nTo fix it, I added on inflections that the \"singular\" of address is address\ntoo.\ninflect.singular 'address', 'address'\nOn Sun, May 19, 2013 at 3:35 AM, Karel Minarik notifications@github.comwrote:\n\nHi, Addres is not a singular form of adress, it's incorrect English\nsyntax. Rename your model to Adress (plural: adresses) and it should work.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/issues/733#issuecomment-18113157\n.\n\n\nAtenciosamente,\nAlan Carlos Meira Leal\nhttp://www.linkedin.com/in/alanmeira | (31) 9788-2008 | Skype: alanmeira\n. ",
    "tmlee": "@rizwanreza  Have you tried doing Announcement.index.delete to purge the index and Announcement.import to reindex ?\n. according to stack overflow, just to clarify, the issue was that you were trying to do a search with filter without query all, but getting nothing out of it? is that correct?\n. ",
    "chrishale": "Ok, really needed to do a query rather than a filter. I've forked it and managed to get it working. I've sent you a pull request! :)\nCheers,\nChris\n. Cool, sounds good. It would be nice if you could nest it in a query or a filter. I don't personally think it needs abstracting, but if the documentation is clear enough - some kind of DSL would be good. I'm using the geo filters, queries and facets pretty extensively in a current project, I might have a crack at another pull request in the next few weeks... or months ;)\n. ",
    "felixbuenemann": "@karmi unfortunately that didn't fix it for me, I had to modify the rake task to depend on the environment and remove require path to get it to work, otherwise it failed to load the models, because one model depended on code from app/uploaders.\n. @karmi rake environment tire:import:all works if I comment require pathinside task.rb, because that line causes the models to be loaded twice causing issues with overwriting scopes, redefining constants and in this case raising an exception on an embedded enum class.\nThe same happens if I enter the rails console and do a require \"./app/models/somemodel\".\nMaybe this could be fixed by trying to avoid double require like this?\nruby\nbegin\n  klass = model_filename.camelize.constantize\nrescue NameError\n  require(path) ? retry : raise\nend\nIf you think this is a viable solution I could make a PR.\n. @karmi ping\n. According to the newrelic_rpm changelog, this was fixed in v3.6.9 of the gem.\n. ",
    "Hungor": "SOLVED by\nadding the following index mapping \nmapping do\n    indexes :price, type: \"float\"\n end\n. ",
    "miguelm": "You are awesome!\nAfter i did your links i did another mapping structure like this:\nmapping do\n  indexes :weekdays, :type => \"nested\" do \n    indexes :type, :type => \"string\"\n    indexes :hours, :type => \"string\", :analyzer => 'keyword'\n  end\nend\nand a query construction like in your nested query test:\ns = Tire.search @index.name do\n      query do\n        boolean do\n          must do\n            match 'name', 'Duck'\n          end\n          must do\n            nested path: 'weekdays' do\n              query do\n                boolean do\n                  must { match 'weekdays.type',  '0' }\n                  must { range 'weekdays.hours', { gt: \"12:00\" }}\n                end\n              end\n            end\n          end\n        end\n      end\n    end\nIt all worked out.\nAnother last question if i may, since i will be returning an array a set of Venue results something similar to this:\n<Item (Venue) _id: \"xxxid\", name: \"xxx\", weekdays: [<Item _id: \"anotherid\", type: 0, hours: [\"14:30\", \"15:00\"]>,<Item _id: \"anotherid\", type: 1, hours: [\"14:30\", \"15:00\"]>]>\nWith this result i only want to retrieve the items of weekdays that match my boolean query (type = 0) is that possible? \nSince it's a nested document ideally i could do something like this:\ns.results.first.weekdays\nWhere the only weekdays items are there are the ones who match type = 0\nIs it possible without having to write the code to iterate over all the weekdays?\nLet me know if i didn't explain myself properly.\nOnce again awesome tips and love your work!\n. Nice catch, will follow up that issue elasticsearch/elasticsearch#3022\nFor now i will do a venue.weekdays.select{...} over it and extract the values.\nThe most difficult part is solved, do you want to keep this issue as it links to the elastic search issue or should i close it so it doesn't take up more space on your backlog?\nOnce again thank you very much and hope i can repay it sometime in the future :)\n. ",
    "atombender": "That works.\nInterestingly, if I stop ES, delete the entire ES folder, start ES and then do Comment.create_elasticsearch_index, nothing at all happens.\nIf I stop ES, delete the entire ES folder, start ES and then do Comment.update_index, I get the index without any of the mappings I specified.\n. Please don't just close issues like that before you have determine the cause or even solved the problem.\nI obviously meant Comment.first.update_index. That also results in a partial index. Touching the model does nothing, by the way. I have now played around with the models a bit, and it is evident that some models work, but others don't. The problem occurs even if I gut everything except a single mapping.\nI believe I have found the cause. At the top of comment.rb, I have:\nrequire 'event'\nThis is because the model stores entire objects (Event in this case) using Rails.cache, and this is required when Rails is in development mode, otherwise model classes will not be reloaded.\nI have not determined why this screws up Tire. The strange thing is that Tire does not complain; it just stores an incomplete mapping. Comment.mapping returns valid stuff, by the way, so that's pretty weird.\nI will investigate further. Any ideas?\n. All right, what's happening is that ES creates the mapping automatically when indexing the object. That's why it's missing stuff. Tire never creates the full mapping.\nThe reason is that Tire only creates a mapping when there is no existing index. But all my models use a single index (I use index_name in each model class). So what is happening is that require \"event\" requires event.rb, which declares Event, which has a mapping do ... block. mapping calls create_elasticsearch_index, which creates the index, but only for the mappings in Event. When it comes to the mappings do ... block in Comment, the index already exists, and no mappings are stored.\nSo this is a real bug. You seems to conflate the notions of \"index\" and \"mappings\", even though they are separate. An index stores data; mappings tell ES how to handle that data. There is no reason for Tire to assume that mappings exist even though the index itself exists.\nSomething like this would work:\ndef create_elasticsearch_index\n  unless index.exists?\n    new_index = index\n    unless result = new_index.create(:mappings => mapping_to_hash, :settings => settings)\n      STDERR.puts \"[ERROR] There has been an error when creating the index -- elasticsearch returned:\",\n                  new_index.response\n      result\n    end\n  else\n    index.put_mapping(document_type, properties: mapping_to_hash)\n  end\n  ...\n. It's still a bug. Tire makes an invalid assumption.\n. It is only necessary to update the mapping if it does not already exist. This will not introduce any conflicts, compatibility issues, etc. After all, you do the same thing with indexes.\nThe current approach (creating only the first mapping that is encountered) is inconsistent and the violates the principle of least surprise. After all, if I declare a mapping, I should expect it to be created, and not silently ignored.\n. ",
    "johnmcdowall": "Apologies. Here's more info:\nGiven:\nindexes :name,                :index    => :not_analyzed\nI'd like to be able to have the query results on :name be sorted alphabetically, but when I say\ns.sort { by :name, 'asc'  }\nThe results are sorted with capital case first, then lower case, like:\nAlbatross \n   Bamboo\n   Zebra\n   albany\n   church\n. I've just discovered this link: http://elasticsearch-users.115913.n3.nabble.com/Case-Insensitive-Sort-td4021310.html\nBut I'm not sure how that would translate into the Tire API, if at all? \n. Seem to have solved this with a custom analyzer:\n```\nsettings :analysis => {\n           :analyzer => {\n             :name_analyzer => {\n                \"tokenizer\"    => \"keyword\",\n                \"filter\"       => \"lowercase\",\n                \"type\"         => \"custom\" }\n           }\n          }\nindexes :name,                :analyzer=>'name_analyzer'\n```\n. ",
    "djcp": "Awesome. Yeah, we're actually seeing MORE tests pass under ruby 2 than 1.9 on the same system. We'll give it a go!\n. ",
    "sporto": "great, the proxy method is what I was looking for.\n. ",
    "orbanbotond": "Oh... I see now :) Thanks. May I close the issue?\n. ",
    "kelaban": "Yes thats correct\n. ",
    "gudata": "I have putting a field friendly_id but it doesn't work.\nIt works only if I specify the id like this\nphoto_path(publication.slug)\nand it should be \nphoto_path(publication)\nphoto_path is calling internally .to_param\nwhich returns the id as a number, and I expect friendly id to override this method and construct the slug dynamically (or just return the slug stored field)\nMaybe I am doing something wrong with the includes and friendly id doesn't have the chance to override the to_param method I don't know. \nDo you have an app which is working with result items and friendly id ?\n. The solution with the wrapper object will force creating a new proxy object for each result item because I have multiple types. Because of this the good way for me will be to overload to_key but I have to find a way to do it.\nThanks!\n. When use the wrappers everything is in place.\nThanks!\n. It was long time ago. check if this will help https://gist.github.com/gudata/7691749\n. ",
    "jeremylynch": "Hi Gudata, did you solve this issue in the end? How did you construct your wrapper? \n. Is there any update on this issue? It is still occurring for me with tire 0.6.2 and Mongoid 4.0.0\n@canercak's solution provides a temporary fix\n. I have a document with the attribute 'name' when the name attribute is updated, I would like to update the index for this document. Is it possible to update just the 'name' attribute or does the entire index have to be updated? \n. ",
    "danielhum": "no, I've verified that the docs are in the db. I'll continue debugging for now.\n. Thanks for the suggestions. Interestingly I can't seem to replicate this again, closing for now.\n. ",
    "alotela": "very interesting. Thank you.\nI think I need to study lucene query syntax deeper.\nI thought range filter was only for integer values. I will try all your solutions.\nand do you know how to select all non alphanumeric chars ?\n. Ok,\nI will try and see what is the best for me.\nThank you.\n. how can I search for non alpha first letter ?\nin my document, I have this field for example : \"creator_first_letter\":\"-\"\nindexes :creator_first_letter, type: 'string'\nbut my facet doesn't list it : facet('first_letter') { terms [\"creator_first_letter\"], size: 10000, order: \"term\" }\n(by the way, if you have a better solution than \"size: 10000\" to return all terms > 0, I'm happy to learn it.)\n. ok i remived the analyzer for now. works perfect.\nthank you for quick answers!\n. ok,\nI tried it before, it was one of my options:\nMyModel.tire.search do\n  query do\n    boolean do\n      should do string \"label:#{search_term}\" end\n      should do string \"shop.label:#{search_term}\" end\n      .....\n    end\n    filtered do\n      filter :and, selected_filters\n    end\n  end\n  facets...(I need data to be filtered for all facets)\n  sort\nend\nbut I think I have an error into my query ? ;) Nothing returned\n. I finished with this :\nruby\nquery do\n filtered do\n    query do \n      boolean do\n                  should { string \"label:*#{search_term}*\", default_operator: 'AND' }\n                  should { string \"creator:*#{search_term}*\", default_operator: 'AND' }\n                  should { string \"shop.label:*#{search_term}*\", default_operator: 'AND' }\n                end\n              end\n              filter :and, selected_filters\n            end\n          end\nwhich seems to be good. I needed to scope boolean block into query block.\nIs this the good solution ?\n. ok, I tried match, but don't know how to use wildcard. ES doc says it's not possible...?\nI think I found the way I wanted to do it:\nquery do\n            filtered do\n              query do \n                string \"#{search_term}\", default_operator: 'AND', fields: [:label, :creator, :'shop.uri_label']\n              end\n              filter :and, selected_filters\n            end\n          end\nwhat do you think ?\nNext, I need to work on removing widlcard to have faster searches with analyzed fields.\n. ",
    "jcf": "The chainable API is the preferred way of doing conditions etc. now and has been since Rails 3.0.\nWhat's your stance on Rails version support in Tire? I think this change will cover Rails 3 & 4, and can look into the tests sometime early next week if you want something like this mergeable into upstream.\n. I remember the good old\u2026\nruby\nUser.find(:all, :conditions => {:name => 'Karel'}, :includes => [:elastic_search])\n\u2026API. If only I could count the number of hashes I must have initialised during my time with Rails 2. Fond memories. :smile:\nThe first thing that comes to mind is taking a loader block that Tire could use, perhaps yielding the list of IDs to the block so the user can implement find in its entirety.\nruby\nloader = ->(ids) { Article.includes(:comments).where(published: true).find(ids) }\nArticle.search(loader: loader) do |s|\n  s.query # etc.\nend\nOr perhaps add a method to the class to do the finding with a list of IDs?\nruby\nclass Article\n  def self.find_for_tire(ids)\n    includes(:comments).where(id: ids)\n  end\nend\nI'm sure there's a better way but that's what popped into my mind.\nP.S. I've force pushed up to my includes branch as I was using @options[:load] instead of @options[:load][:include].\nThanks for sharing Tire with the Ruby community. Have a great weekend!\n. ",
    "gfrivolt": "I executed the tests and found 4 failing for the current state of the repository. The output is here.\nI'm using this declaration for the model:\n```\nclass Article\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\n  include Tire::Model::Persistence\nalias_attribute :content, :body\nproperty :author,       type: 'string'\n  property :title,        type: 'string'\n  property :summary,      type: 'string'\n  property :body,         type: 'string'\n  property :url,          type: 'string'\n  property :published_at, type: 'date'\n  property :full_page,    type: 'string'\nend\n```\nThe articles itself are created by a factory, which adds id to them:\n```\nrequire 'securerandom'\nclass ArticleFactory\ndef self.build &block\n    article = Article.new\n    article.id = SecureRandom.urlsafe_base64\n    article.instance_eval(&block) if block_given?\n    article\n  end\ndef self.create &block\n    article = build(&block)\n    article.save\n    article\n  end\nend\n```\nSaving model constructed this way passed well earlier.\n. hmm.. the issue shows up for me only in the specs, saving in running rails works..\nhere is the pastie for the traceback from my failing specs: http://pastie.org/8078721\n. no stubs/no mocks in the failing specs\n. I looked at the code again, actually the issue appears anytime when I try to save the tire persisted model. I created a simple rails 4 app, on saving it throws the error. The test rails is here.\n\u2192 rails c\n  Loading development environment (Rails 4.0.0)\n  irb(main):003:0> a = Article.new\n  => #<Article:0x007fcb3a53d290>\n  irb(main):004:0> a.title = 'title'\n  irb(main):009:0> a.valid?\n  => true\n  irb(main):010:0> a.save\n  NoMethodError: undefined method `[]' for #<Tire::Model::Search::InstanceMethodsProxy:0x007fcb3a6bf488>\n          from /usr/local/opt/rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/tire-0.6.0/lib/tire/model/persistence/storage.rb:60:in `block in save'\n          from /usr/local/opt/rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/activesupport-4.0.0/lib/active_support/callbacks.rb:373:in `_run__669920208294084740__save__callbacks'\n          from /usr/local/opt/rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/activesupport-4.0.0/lib/active_support/callbacks.rb:80:in `run_callbacks'\n          from /usr/local/opt/rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/tire-0.6.0/lib/tire/model/persistence/storage.rb:58:in `save'\n          from (irb):10\n          from /usr/local/opt/rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/railties-4.0.0/lib/rails/commands/console.rb:90:in `start'\n          from /usr/local/opt/rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/railties-4.0.0/lib/rails/commands/console.rb:9:in `start'\n          from /usr/local/opt/rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/railties-4.0.0/lib/rails/commands.rb:64:in `<top (required)>'\n          from bin/rails:4:in `require'\n          from bin/rails:4:in `<main>'\n. I included more modules, than necessary. Now I don't have any issues, as the correct callbacks are called. As this conflict of modules appeared only versions 0.5.8 and 0.6.0, I did not suspect my codes too much.\nI included Tire::Model::Search and Tire::Model::Callbacks before Tire::Model::Persistence. Including Tire::Model::Persistence includes Tire::Model::Search as well. Sorry for the confusion, now at least I had to take a bit deeper look into the tire codes.\nA possible enhancement could be raising an error if Tire::Model::Search is included before Tire::Model::Persistence, otherwise I think this issue was a false alarm.\n. ",
    "brknstrngz": "I am not sure, I have not tried that yet. I have changed the JSON representation of my models to include everything I needed so that I don't have to call into Mongo to fetch that information.\n. @karmi I had tried that as well, but get_id_from_document still returned the {\"$oid\" => \"value\"} hash as an ID.\nLooking at Mongoid's #2497 entry in the 4.0 ChangeLog, I think they used to do what I did, so Tire must have been working with the string representation of the Mongo ID before. I cannot test whether :load => true breaks anything, but I can do that later.\nWhat I am sure of, though, is that in my Rails controller there are at least a couple of methods that receive a model ID (as tweaked in my original post) as an argument that do call Mongoid and work.\n. @karmi Seems to work, thanks!\n. ",
    "paulgoetze": "For me it is the same as for @jonarrien  - for a non nested document. I'm running Rails 4.0.0, Mongoid 4.0 and Tire 0.6.0. \nAdditionally there is a strange switch of the ealsticsearch index entry ids:\nFor automatic generation while creating a document from the app an index entry looks like this with a document id as above: \n{\n            \"_index\": \"products\",\n            \"_type\": \"product\",\n            \"_id\": \"{\\\"$oid\\\"=>\\\"51d6e9a993b6a2a751000005\\\"}\",\n            \"_score\": 14.054651,\n            \"_source\": {\n               \"id\": {\n                  \"$oid\": \"51d6e9a993b6a2a751000005\"\n               },\n               \"name\": \"Product\",\n               \"price\": \"10.0\",\n               \"published_at\": \"2013-07-04T12:00:00.000Z\"\n            }\n         }\nand if I import the models to the index by\nrake environment tire:import CLASS=Product FORCE=true\nthen I got something like: 9M-8v3lqT_q7k4F4XdzfgA as the entry's _id\nWhat could be the matter with this?\n. ",
    "bhbryant": "Reverting karmi/tire@98e6dc7 will not fix the problem with Mongoid 4, you must explicitly cast the id to a string.\nFor those looking for a temporary fix, add the following to an initializer:\nruby\nrequire \"tire\"\nmodule Tire\n  class Index\n    def get_id_from_document(document)\n     case\n        when document.is_a?(Hash)\n          document[:_id] || document['_id'] || document[:id] || document['id']\n        when document.respond_to?(:id) && document.id != document.object_id\n          document.id.to_s   # was document.id.as_json\n      end\n    end\n  end\nend\n. ",
    "canercak": "I'm using searckick(based on tire) with mongoid. For the time being this worked for me: \nI created mongoid.rb in config/initialisers and placed this code\nmodule BSON\n  class ObjectId\n    def to_json(*args)\n      to_s.to_json\n    end\ndef as_json(*args)\n  to_s.as_json\nend\nend\nend\n. ",
    "tharry": "Is suggest api going to be added to tire anytime soon?\n. ",
    "lfv89": "@phoet thank's for your attention.\nYep I saw that method, but I don't think that works with MongoID, I just tried Article.find(id, only: :email) and nothing... So I forked tire to make that work with MongoID... Would that be an interesting PR to make ?\n. ",
    "TheHodge": "@karmi your curb suggestions seem to have fixed it so far.. will report back if it appears again\n. ",
    "fillman": "A BIG note here guys, when there's no match then you'll get no HIGHLIGHT field at all. This is sometimes tricky when you highlight one field but do search on several fields. So document is returned but no highlights. Be careful :)\n. ",
    "bradgriffin": "I have also been having this problem. It seems to be isolated to when I run within tmux. Are you using tmux by any chance?\n. ",
    "sajmoon": "I'm in the same project as @luuse. We want to return the entire list of results, not just a fix number. In this case it is intended to be a service returning the result as json. Then, returning paginated result is not very convenient, at least not for us. \nWe worked around the problem by defining per_page  to a big number. That works, but is quite ugly and I guess more then us might need to disable pagination. \n. ",
    "vonbirdie": "Hi! Sorry for not getting back to you. Yes, it can be closed and thanks for the help!\n. I think you need to use the scan API that Karmi linked to in #788 if I understand you correctly?\n. Ok, thanks!\n. ",
    "hbrock25": "Hmm... what about instance properties? I also have an object with an index field defined (\"foo.index\" gives a specific position of foo in a list of foos) and it appears to be overriding that as well. Do I simply need to explicitly define the property in the model?\n(Thanks for the quick response BTW)\n. Sure...\nSo I have a table like this:\nclass AddColumnsToArticles < ActiveRecord::Migration\n  def up\n    add_column(:articles, :subhead, :string)\n    add_column(:articles, :index, :integer, :null => false)\n  end\nIt backs a model like this:\n```\nclass Article < ActiveRecord::Base\ninclude Tire::Model::Search\n  include Tire::Model::Callbacks\n...\n  def up\n    before = Article.where(\"issue_id = ?\", self.issue.id)\n      .where(\"index < ?\", self.index)\n      .order(\"index DESC\").first\n    if before.present?\n      self.index, before.index = before.index, self.index\n      before.save\n      save\n    end\n  end\n```\nCalling \"up\" from my tests now yields this error:\n1) Article should create a valid article\n     Failure/Error: @article1 = FactoryGirl.create :article, :issue_id => @issue1.id\n     NoMethodError:\n       undefined method `>' for #<Tire::Index:0x0000000c47d248 @name=\"articles\">\n     # ./app/models/article.rb:71:in `set_index_if_null'\n     # ./spec/models/article_spec.rb:6:in `block (2 levels) in <top (required)>'\nYou can see that article.index now returns a Tire::Index, rather than the integer defined in the database.\nTo be clear, I don't have an \"index\" method defined anywhere in the model -- it's of course implied by the database table.\n--Hugh\n. ",
    "vdh": "I had this problem occur for AR collections, and my workaround was to replace index with find_index, since the docs for Array says that index is an alias of find_index.\n. ",
    "edrush": "Hi Karel,\nI did some searching here, without success (e.g. search for \"ActiveRecord::RecordNotFound\" or \"refresh on destroy\") and I tried index.refresh after destroy, but nothing changed, I'm still lost. What do you mean with overloading the finder method? I'm sure that many run into this, that's why I'm surprised that it's so hard to find answers. Maybe I'm searching for the wrong key words? It would be nice if you sent me some links - thanks a lot!\nWolfram\n. Ok, I figured it out, in the model class one needs to:\nafter_save do\n    tire.index.refresh\nend\nafter_destroy do\n    tire.index.refresh\nend\nThanks! I close this issue.\n. ",
    "swarzech": "I figured out that if I delete all of the indices from the alias it will remove the alias.\n. ",
    "abrahamD": "Hi @phoet,\nMy setup was working fine before I customized the search so I guess I can rule out configuration issues. The logs show a normal query. I do get a PhaseExecutionException of some sort that leads me to think the indexing fails and not the actual search. \nAgain it works for one model if I do the setup manually for that model; but as soon as I try indexing more it just fails to search anything.\nHere's the rake task I use for resetting indexes if that's any help\n``` ruby\n  namespace :single_index do\n    desc 'Resets all ActiveRecord model indices'\n    task :reset => :environment do\n  Rails.application.eager_load!\n\n  Product.tire.index.delete\n  Product.create_elasticsearch_index\n\n  ActiveRecord::Base.descendants.each do |model_class|\n    next unless model_class.respond_to? :tire\n\n    puts \"Resetting #{model_class}\"\n    model_class.import\n  end\n  puts 'All indices reset'\nend\n\nend\n```\n. So, can I just create the index in the rake task and set my custom analyzer inside it and use it in each individual mapping block in my models? (Like so:)\n``` ruby\nnamespace :single_index do\n  desc 'Resets all ActiveRecord model indices'\n  task :reset => :environment do\nRails.application.eager_load!\n\nTire.index INDEX_NAME do\n  delete\n  create :settings => {\n    :analysis => {\n      :filter => {\n        \"custom_ngram\"  => {\n          \"side\"     => \"front\",\n          \"max_gram\" => 50,\n          \"min_gram\" => 2,\n          \"type\"     => \"edgeNGram\"\n        }\n      },\n      :analyzer => {\n        :name_analyzer => {\n          \"tokenizer\" => \"standard\",\n          \"filter\" => %w{lowercase asciifolding custom_ngram},\n          \"type\" => 'custom'\n        }\n      }\n    }\n  }\nend\n\nActiveRecord::Base.descendants.each do |model_class|\n  next unless model_class.respond_to? :tire\n\n  puts \"Resetting #{model_class}\"\n  model_class.import\nend\n\nputs 'All indices reset'\n\nend\nend\n```\n. Hi @karmi ,\nThanks for the code snippet! I'll give it a try as soon as possible. Right now, the index is on hold but I'll update when we give it a try.\nThanks again,\nAbe\n. Hi @karmi,\nI gave it a try and it worked fine. Here is my final rake task:\n``` ruby\nnamespace :single_index do\n    desc 'Resets all ActiveRecord model indices'\n    task :reset => :environment do\n  Rails.application.eager_load!\n\n  Tire.index INDEX_NAME do\n    delete\n    create( :settings => {\n        :analysis => {\n            :filter => {\n                \"custom_ngram\"  => {\n                    \"side\"     => \"front\",\n                    \"max_gram\" => 50,\n                    \"min_gram\" => 2,\n                    \"type\"     => \"edgeNGram\"\n                }\n            },\n            :analyzer => {\n                :name_analyzer => {\n                    \"tokenizer\" => \"standard\",\n                    \"filter\" => %w{lowercase asciifolding custom_ngram},\n                    \"type\" => 'custom'\n                }\n            }\n        }\n    }, :mappings => ActiveRecord::Base.descendants.select do |m|\n      m.respond_to? :tire\n    end.reduce({}) do |s,m|\n      s[m.tire.document_type] = { properties: m.tire.mapping.to_hash}; s\n    end)\n  end\n\n  ActiveRecord::Base.descendants.each do |model_class|\n    next unless model_class.respond_to? :tire\n\n    puts \"Resetting #{model_class}\"\n    model_class.import\n  end\n\n  puts 'All indices reset'\nend\n\nend\n```\nThanks for the help.\n. ",
    "zengjing": "i got the same problem, i used the way in last comment, but i get the error:\nInvalid arguments!\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/activerecord-import-0.3.1/lib/activerecord-import/import.rb:199:in import'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/bundler/gems/tire-1341e9ee3616/lib/tire/tasks.rb:201:inblock (3 levels) in '\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/bundler/gems/tire-1341e9ee3616/lib/tire/tasks.rb:197:in each'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/bundler/gems/tire-1341e9ee3616/lib/tire/tasks.rb:197:inblock (2 levels) in '\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/task.rb:236:in call'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/task.rb:236:inblock in execute'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/task.rb:231:in each'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/task.rb:231:inexecute'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/task.rb:175:in block in invoke_with_call_chain'\n/home/zengjing/.rvm/rubies/ruby-1.9.3-p392/lib/ruby/1.9.1/monitor.rb:211:inmon_synchronize'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/task.rb:168:in invoke_with_call_chain'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/task.rb:161:ininvoke'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/application.rb:149:in invoke_task'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/application.rb:106:inblock (2 levels) in top_level'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/application.rb:106:in each'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/application.rb:106:inblock in top_level'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/application.rb:115:in run_with_threads'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/application.rb:100:intop_level'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/application.rb:78:in block in run'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/application.rb:165:instandard_exception_handling'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/lib/rake/application.rb:75:in run'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/gems/rake-10.1.0/bin/rake:33:in'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/bin/rake:23:in load'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/bin/rake:23:in'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/bin/ruby_noexec_wrapper:14:in eval'\n/home/zengjing/.rvm/gems/ruby-1.9.3-p392@tangor/bin/ruby_noexec_wrapper:14:in'\nplz help!\n. issue solved.\nclosed.\n. ",
    "rahul": "@phoet I came across that issue before and should have paid more attention to it then. I sat down with it today and got it to work. Thank you for your suggestion.\n. ",
    "anthias": "This worked perfectly for me after I moved the class from a namespace. I'm not sure if the fix was to remove the namespace (I see your integration tests consider this a special case) or to rename the class. If I run Issue.index.delete does that accomplish the same task as renaming the class?\nThank you for the fast response and great gem!\n. One more update... After working with this a bit more, I see what I think my problem was. If I create a new class and just start creating records, the above configuration works. If I then delete that index using Issue.index.delete and start creating records, for some reason the terms are not combined properly. If I delete the index using Issue.index.delete and then create the index using Issue.create_elasticsearch_index as suggested by @karmi , then start loading data everything is fine.\nThanks again.\n. ",
    "apneadiving": "Oh, sorry, when passing a string lat_lon is expected: http://www.elasticsearch.org/guide/reference/api/search/sort/\n. Mmm, their was an issue on qbox side + it seems POST isn't legit in this case, tire should do a PUT.\nI will dig this.\n. So ES doc states:\nparents & children\nA child document can be indexed by specifying it\u2019s parent when indexing. For example:\n$ curl -XPUT localhost:9200/blogs/blog_tag/1122?parent=1111 -d '{\n    \"tag\" : \"something\"\n}'\nso its a put not a post.\nShould I send a pull request?\n. Ok so I did this and it now works like a charm:\nhttps://github.com/apneadiving/retire/commit/a8a06e90014649f81a7cd846df0b7b68be28f6e8\nShould I do a PR?\n. Anyone?\nSeems like something useful for parent handling, too bad it's not shared\n. ",
    "GoFarGoLocal": "Sorry for indentation issue - I am trying to filter after query all block. Filter in under search block.\n. Hello @karmi ; i have fixed all the issues the only issue i left with is that i want to know how i can do touch on HABTM relation here is sample of my code;\n``` ruby\nclass Keyword < ActiveRecord::Base\n  has_and_belongs_to_many :experiences, touch: true\nafter_touch() { tire.update_index } \nend\nclass Experience < ActiveRecord::Base\n   has_and_belongs_to_many :keywords\nend\n```\n. @karmi  so does it meant that tire don't have ability to handle has_and_belongs_to_may association ?\n. ",
    "sgringwe": "@karmi i too would like to turn off automatic index creation, as I am using aliases and if the index is automatically created I then have to delete it in order to create the alias. Any progress?\n. ",
    "duyleekun": "Oh ok. \nI thought there is a 'facet' method to be called\nThanks\n. ",
    "mhussain": "@karmi Thanks for your reply (@luuse as well) \nThe result set that my query is returning is basically split in two tiers. One with score of say x and one with score of say y. This is because I am doing custom filter scores on them. I basically need to be able to get a count of both sets. \nI can do a total count on the result set that I get. But in order to find out the count of elements in that set which have a score x , I need to be able to iterate over the entire set, which I cannot do because the query is paginated. \nW.r.t the scan results suggestion, would that not mean that then I am basically querying the index twice ? Once to get the entire set and once to get paginated results ? \n. What is the scenario in which you need to update the index? Is this while the app is running or is this stand alone index sanitization ? \nAnd also, out of curiosity, why can't you use the callbacks ?\n. ruby\nbundle exec rake environment tire:import CLASS=\"Model\" INDEX=\"INDEX_NAME\" FORCE=true\nShould do it\n. ",
    "gautamrege": "Hah! \nClosing this because of incorrect usage.  If we are using Tire.search we should size and from instead of per_page and page respectively, as these are WillPaginate compliant and hence used only in ActiveModel. \nruby\ns = Tire.search(['development_receipts'], fields: nil, size: 14, load: true) do\n  query { string 'tom' }\nend\n. ",
    "vnorguet": "Sorry, I've posted a bit fast. If another one asked the question, cf:\nhttps://github.com/karmi/tire/issues/708\nrake environment tire:import:model CLASS='MyModel'\n. Hello,\nSorry for late I was out of internet for few days.\nHere's how I called the task in 0.5.8\nruby\nrake tire:import:all\nHere how I've fixed the problem for version 0.6.0 :\nruby\nrake environment tire:import:all\nFor me, this issue can be closed, since it was a bad practice not to use \"environment\" in the rake task.\n. ",
    "kapso": "We want to keep DB data in sync with the index hence the update. We have some specific requirements so cannot use callbacks.\n. I figured it out here - https://github.com/karmi/tire/blob/master/examples/tire-dsl.rb#L748\nruby\nTire.index 'users' do\n  update 'user', \"1\", doc: { country: 'US' }\nend\n. Also is there a DSL to do \"script\" updates to documents? So something like this (see counter & tags example) -\nhttp://www.elasticsearch.org/guide/reference/api/update/\n. Ah ok I see it - https://github.com/karmi/tire/pull/366\n. I guess I can just send a upsert hash instead of doc when updating.\n. Found it, pass doc_as_upsert to the update method\n. Yea this is how I got it working...\nruby\nfilters << { query: { multi_match: { query: \"my phrase\", operator: :and, fields: %w(field1 field2) } } }\n. When is this going to be released?\n. great thanks\n. ",
    "General9": "@question.to_indexed_json is returning the following:\nruby\n{\n  \"asker\": {\n    \"user_id\": \"1\",\n    \"display_name\": \"info\"\n  },\n  \"created_at\": \"2013-07-27T11:52:48.451+02:00\",\n  \"description\": \"This is adescription\",\n  \"down_count\": 0,\n  \"page_views\": 0,\n  \"tags\": [\n    \"tag1\",\n    \"tag2\"\n  ],\n  \"title\": \"This is a title 1?\",\n  \"up_count\": 0,\n  \"updated_at\": \"2013-07-27T11:52:48.451+02:00\",\n  \"vote_score\": 0,\n  \"votes\": []\n}\nThere is nothing about answers in there. Not too sure what I am missing?\n\nAnd on the Side Note: I was trying to specify that I don't want those fields on the model indexed at all. Will it work if I just leave the fields out from the mapping? \n. Using the StackOverflow example I implemented the to_indexed_json method as follows:\ndef to_indexed_json\n    to_json( include: { answers: { only: [:description] } } )\nend\nI did not use any of the touch stuff as that only works with mongoid relations on (belongs_to) in my case the Answer document is embedded_in Question. Running @question.answers.metadata.touchable? returns false.\nMy @question.to_indexed_json now includes answers but running a search for the text \"answer\" as an example still returns nothing. \n@question.to_indexed_json output below:\n\"{\\\"_id\\\":\\\"51f3a10d1363185d86000002\\\",\\\"answers\\\":[{\\\"description\\\":\\\"This i\ns an answer.\\\"}],\\\"asker\\\":{\\\"user_id\\\":\\\"1\\\",\\\"display_name\\\":\\\"info\\\"},\\\"creat\ned_at\\\":\\\"2013-07-27T12:29:33.204+02:00\\\",\\\"description\\\":\\\"This is a descriptio\nn\\\",\\\"down_count\\\":0,\\\"page_views\\\":2,\\\"tags\\\":[\\\"tag1\\\",\\\"tag2\\\"],\\\"title\\\":\\\"T\nhis is a title 1?\\\",\\\"up_count\\\":0,\\\"updated_at\\\":\\\"2013-07-27T12:29:33.204+02:0\n0\\\",\\\"vote_score\\\":0,\\\"votes\\\":[]}\"\n. Yes I have re-created the index. I have a rake task that purges my database, deletes the index and re-creates the index on seeding the database.\nIf I go back to exactly what I am trying to do maybe you can help me. It seems like it should be a simpler task than it is actually turning out to be. \nSo if I have a Question model of this form:\nclass QuestionDetail::Question\n  include Mongoid::Document\n  include Mongoid::Timestamps\n  include Tire::Model::Search\n  include Tire::Model::Callbacks\nindex_name(\"#{Tire::Model::Search.index_prefix}questions\")\nfield :title\n  field :description\n  field :page_views, type: Integer, default: 0\n  field :asker, type: QuestionDetail::User\ntaggable :tags\nembeds_many :answers, class_name: 'QuestionDetail::Answer'\nend\nand Answer model of this form:\nclass QuestionDetail::Answer\n  include Mongoid::Document\n  include Mongoid::Timestamps\nfield :description\n  field :answerer, type: QuestionDetail::User\nembedded_in :question, class_name: 'QuestionDetail::Question'\nend\nI am trying to use elastic search to be able to search for questions only on the following fields - (title, description, tags) from the Question model and (description) from the Answer model. The rest of the fields should not be searchable or should not return any results, for example if a user types in 2 in the search box and there is a question with 2 votes that should not return any results because votes should not be searchable.\nSo the question is, how do I define the mapping for that using the tire gem? My initial (1st) question shows how I was attempting to achieve this.\n. Ok so I have been battling with this the whole day and this is how far I have got.\nMy mappings:\nmapping do\n    indexes :created_at, :type => 'date', :index => :not_analyzed\n    indexes :vote_score, :type => 'integer', :index => :not_analyzed\n    indexes :title\n    indexes :description\n    indexes :tags\n    indexes :answers do\n      indexes :description\n    end\n  end\nMy to_indexed_json method:\ndef to_indexed_json\n    {\n        vote_score: vote_score,\n        created_at: created_at,\n        title: title,\n        description: description,\n        tags: tags,\n        answers: answers.map{|answer| answer.description}\n    }.to_json\n  end\nMy Search query:\ndef self.search(term='', order_by, page: 1)\n    tire.search(page: page, per_page: PAGE_SIZE, load: true) do\n      query { term.present? ? string(term) : all }\n      sort {\n        by case order_by\n             when LAST_POSTED then {created_at: 'desc'}\n             else {vote_score: 'desc', created_at: 'desc'}\n           end\n      }\n    end\n  end\nThe only issue I am battling with now is how do I make vote_score and created_at field not searchable but still manage to use them for sorting when I'm searching.\nI tried indexes :created_at, :type => 'date', :index => :no but that did not work.\n. I ended up going with the approach of only matching on the fields I want and that worked. This matches on multiple fields.\ntire.search(page: page, per_page: PAGE_SIZE, load: true) do\n      query { term.present? ? (match [:title, :description, :tags, :answers], term) : all }\n      sort {\n        by case order_by\n             when LAST_POSTED then {created_at: 'desc'}\n             else {vote_score: 'desc', created_at: 'desc'}\n           end\n      }\n    end\n. ",
    "marceldegraaf": "I would be very interested to see this merged in\n. ",
    "jqr": "Closing due to repo being retired.\n. ",
    "stijlfigurant": "ok it was our bad! nvm\n. ",
    "lreeves": "Geah good point, I'll update the PR tomorrow. Thanks!\nOn Sat, Aug 10, 2013 at 5:15 AM, Karel Minarik notifications@github.comwrote:\n\nHi, that's a fair point, but I think we need to mention Tire.configure {\nurl \"xxx\" } as well (see examples/tire-dsl.rb)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/tire/pull/831#issuecomment-22436719\n.\n. @karmi Updated the patch, let me know if that's good.\n. \n",
    "Nitrodist": "@karmi :shipit: \n. ",
    "emilebosch": "Closing due to inactivity. ",
    "isabanin": "I stumbled upon the same issue. Based on facet_filter's source code it will not work with multiple filters:\ndef facet_filter(type, *options)\n  @value[:facet_filter] = Filter.new(type, *options).to_hash\n  self\nend\n. Here's a workaround that I found:\n``` ruby\nstatus = Tire.search(\"comments\") do\n  facet \"tags\" do\n    terms :tags\n    filters = [{:term => {'account-id' => account.id}}]\nif project.present?\n  filters << {:term => {'project-id' => project.id}}\nend\n\nfacet_filter :and, filters\n  end\nend\n```\n. ",
    "embs": "Hi, @dqian. Did you find the answer for this issue?\n. ",
    "dqian": "@embs didn't get an answer.\n. ",
    "egze": "Don't include _Tire::Model::Callbacks_\nAnd then somewhere in your code, after your full_address has been created, you can do\nruby\nself.index.store self\n. ",
    "yaotti": "Yes, that's true!\nI should have done that :+1:\n. ",
    "sandeepbeit": "Using the logger in i can get the error message.\n{\"index\":{\"_index\":\"bw-listings\",\"_type\":\"listing\",\"_id\":\"ap-310186\",\"error\":\"MapperParsingException[Failed to parse [standout]]; nested: JsonParseException[Current token (VALUE_FALSE) not numeric, can not use numeric value accessors\\n at [Source: [B@2c9278b0; line: 1, column: 3690]]; \"}}\nThis is corresponding to one of the new boolean attribute we added to our index.\nthis help me to debug this issue.\n. I have the resque block why  it is not getting captured into it?\n. Also on more thing. when I'm using Tire.configure { logger STDERR, level: \"debug\" }, get requests are getting logged into my logs but response coming back from ES is not getting logged in log and it raises below mentioned exception any idea?\nLoadError: no such file to load -- {\"took\":7,\"items\":[{\"index\":{\"_index\":\"_\",\"_type\":\"*__\",\"id\":\"*\",\"error\":\"MapperParsingException[Failed to parse [standout]]; nested: JsonParseException[Current token (VALUE_FALSE) not numeric, can not use numeric value accessors\\n at [Source: [B@6e64b4f8; line: 1, column: 405]]; \"}}]}\nRegarding the \"MapperParsingException\" i think it is problem in ES core code.Instead of return success message and within body the error message.It should throws exception.\nOne question if ES returned error code/Exception we should raise an Exception in Tire gem so that we know something went wrong rather than silently failing.\n. ",
    "mrkaspa": "I got the same error here you can test http://107.22.43.154:9200/ the import doesn't work but it doesn't throw errors @karmi \n. ",
    "inspire22": "In case anyone else has this problem, this seems to work as an obvious workaround:\nids = request.results.to_a.collect {|a| a['id']}\n myclass.where(:id => ids)\n. That unfortunately requires the more-complicated step of finding which row was missing.\nI'm still lobbying for this \"Couldn't find al\" \"feature\" to be removed.\n. ",
    "Awea": "To force deletion on ES with Tire you can use something like this :\nruby\nMyModel.index.remove('my_model', object_id)\nMyModel.index.refresh #without I can't get it work\nThis help you ?\n. ",
    "vic": "@ryansch any idea why travis ci fails ?\n. ",
    "ryansch": "I'll have a look.\n. I suppose the problem is setting the log level to verbose means I lose all of the other debug logging.\n. Works for me.  We're already using the new elasticsearch gem and slowly letting it take over our project.  :smile: \n. ",
    "alexyakubenko": "Thank you!\n. ",
    "dzhlobo": "Empty array isn't really nil. @monfresh I think you can simply don't store blank fields. You just need to override #to_indexed_json in model. Look at current realization (https://github.com/karmi/retire/blob/master/lib/tire/model/search.rb#L167-190) and add filtering blank fields.\n. @threez I guess you have enabled mongoid's identity map. If so, iterating by all documents cause storing each of them in memory. Please see my explanation in #884.\nWould be nice if you try this change and give us feedback if it help you. You can do this, by using gem from my fork:\nruby\ngem 'tire', github: 'Proghat/tire', branch: 'mongoid-identity-map'\n. Hi @nickcoyne.\nLook at your requests. In the first case you search by 'haystack' but in the second case your search string is 'name:haystack'. Lesson.search('name:haystack') will do the same as your curl request. Also you can give a list of fields you want to search by:\nruby\nTire.search 'lessons' do\n  query do\n    match ['name', 'whatever'], 'haystack'\n  end\nend\n. > why does it only return a result when I specify the field in the query?\nWhen you don't specify field (in that particular case), Elasticsearch (actually Lucene) don't know which analyzer to apply for your request. And when you do, it uses the analyzer which you specified for the field in mapping.\nYou can specify analyzer by hand:\nruby\nTire.search 'lessons' do\n  query do\n    string 'haystack', analyzer: 'kstem_no_stopwords'\n  end\nend\n#search method in model generates \"Query String\" query without specifying any parameter except of query. Read more about this type of queries in Elasticsearch's guides: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html.\nHope it is clear :).\n. I guess you have identity map enabled. Tire iterates through all users and mongoid put them all in memory if identity map is enabled. I made pull request in which I disable identity map during process. See #884. It would be nice if you try and tell us if it is help you. (It helps in my projects.) \nAdd in your Gemfile: \nruby\ngem 'tire', github: 'Proghat/tire', branch: 'mongoid-identity-map'\n\nAlso, if I add more records to the User model will it recreate the full index and will take again hours to re-index?\n\nIt depends on you. You can simple use tire's callbacks which add new documents into index after creation or program another logic yourself. See https://github.com/karmi/retire#activemodel-integration.\n. See it in docs: http://mongoid.org/en/mongoid/docs/identity_map.html#unit_of_work\n. ",
    "fbatista": "Thanks @karmi for the feedback, i'll get on to it as soon as i can.\n. Great Job Fabio!\n. I believe this branch got pretty much everything we need. I'd love for @karmi to take a look and hopefully merge this into the master tree. I'm going to drop the other one https://github.com/karmi/retire/pull/857 .\n. ",
    "fabn": "@fbatista Any news on this?\n. @karmi Thanks for merging this. One question is there any public version of the upcoming elasticsearch-rails library?\n. ",
    "FestivalBobcats": "And apologies for the barrage of successive pull requests...\n. ",
    "emptyflask": "I'm implementing this on a project now, and IMHO it should go into Tire.\nMeanwhile, this kind of thing works:\nruby\ndef randomized(seed)\n  Tire.search(\"#{Tire::Model::Search.index_prefix}_ideas\", query: {\n    function_score: {\n      query: { match_all: {} },\n      random_score: { seed: seed }\n    }\n  }).results\nend\n. ",
    "mozcomp": "Problem solved.\nThe reason projects ceased working was that although I have had FortiClient software installed for a long time (mainly as vpn access to a customer), I usually shut it down after use, but following a software update, my system had rebooted as part of the update and the client was still active.\nBy the look, the web filtering function doesn't like the additional payload in an Http Get causing the EOFERROR in RestClient. As a POST it works fine.\nMy knowledge of RFC's and what is defined as acceptable/non-acceptable is limited, but given the fact that some software obviously believes the payload in a GET is \"bad\", would it be prudent to add a configuration option governing it's use, or is it already there in front of my nose?\nThanks\n. ",
    "bitgangsta": ":+1: it's a great article :) Thanks for merging this in!\n. ",
    "janoskaz": "Update: after some reading I have found out, that: \n\"This has been fixed in the 0.90 branch. Note: all_terms doesn't mean that it will return all terms in the index. It means that it will return all terms even if their count is zero, up to the specified size.\"\n(https://github.com/elasticsearch/elasticsearch/issues/1530)\nTherefore current functionality is correct and it is not a bug\n. ",
    "nicolasgarnil": "Does the \"sufficient high number\" workaround has any drawbacks? Isn't really any way to ensure all the terms are returned?\n. ",
    "phungleson": "Ok keep only gh-pages changes.\n. ",
    "sl4m": "Is there an announcement as to why this project is retiring?\nUpdate: Never mind, I see the new project elasticsearch-ruby\n. ",
    "tadyjp": "+1\n. ",
    "benpickles": "It's being retired in favour of an official gem (which doesn't match Tire's functionality):\nhttp://www.elasticsearch.org/blog/unleash-the-clients-ruby-python-php-perl/#ruby\n. ",
    "gottfrois": "+1\n. ",
    "adamcarlile": "It currently appears to be broken, however the work around is to explicitly declare the window and page within the search block\nruby\noptions = { load: true, page: (params[:page] || 1), size: 15 }\n@search = Tire.search(['index_1', 'index_2'], options) do\n  query { string 'search term' }\n  from options[:size].to_i * (options[:page].to_i-1)\nend\n. ",
    "colewinans": "Thanks guys\n. ",
    "bramu": "Hi Karmi,\nThanks for pointing me to the correct issue, but when I try to add from that fork, it is throwing me the below error. I think I am missing something here. \nirb(main):007:0> Artist.create_elasticsearch_index\n[ERROR] There has been an error when creating the index -- elasticsearch returned:\n400 : {\"error\":\"MapperParsingException[mapping [artist]]; nested: MapperParsingException[No handler for type [completion] declared on field [suggest]]; \",\"status\":400}\nCan you please give me the correct Gemfile entry for this commit?\nThanks!\n. ",
    "arthurccube": "I added\nconfig/environments/development.rb\nTire.configure { logger STDERR, level: \"debug\" }\napp/models/article.rb\nafter_save    { \n     Rails.logger.debug(\"Start index here...........\")\n      self.index.store self\n     Rails.logger.debug(\"Finished index here...........\")\n  }\nRestart, then\nAfter created a new record:\nlog/development.log - \nStart index here...........\n       SELECT users.* FROM users WHERE users.id = 21 LIMIT 1\n       SELECT profiles.* FROM profiles WHERE profiles.user_id = 21 LIMIT 1ESC[0m\n  Finished index here...........\n/elasticsearch/logs/elasticsearch.log  -  Nothing appened\nIs there any config I missed?\n. Besides,  after I run the rake tasts, the items can be searched.\nRAILS_ENV=development rake environment  tire:import CLASS='Article' FORCE=true\nthe after_save callbacks still fails.\n. ",
    "FabianOudhaarlem": "Try this:\nruby\nhas_many :services, through:  :professionals, after_add: [lambda { |a, c| a.__elasticsearch__.index_document }],\n           after_remove: [lambda { |a, c| a.__elasticsearch__.index_document }]\n. ",
    "melhotiby": "Looks like adding the size parameter since elasticsearch default size is 10 which must be only limiting to object1_index search space in your case\nsearch = Tire.search ['object1_index', 'object2_index'], size: 1000 do |search|\n...\n. ",
    "rafael": "Yeah, I thought that about that too. Thanks for the answer! Is it the new gem for Rails live already? \n. ",
    "benubois": "Ok thanks. It wouldn't be a problem to use elasticsearch-ruby and tire would it?\n. Oh cool, that makes sense. Thank you.\n. Hi @karmi,\nYes that makes sense, however I wasn't clear if elasticsearch-model and elasticsearch-rails should be used either since they say they are pre-release.\nI already use tire in production and if it is difficult to migrate to the replacement gems then it might be easier to continue using tire. Is there a migration guide available? Also is support for the percolate API implemented? I couldn't find it in the docs.\nThanks!\n. Thanks for all the info. I'll look into switching to elasticsearch*.\n. ",
    "lazing": "@karmi i have found a solution\nadd following to model\nruby\ntire do\n  index_name {\"products\"}   # WORKS!\nend\nthe code from retire/lib/tire/model/ search.rb:118\nruby\n        # Returns a Tire::Index instance for this model.\n        #\n        # Example usage: `Article.index.refresh`.\n        #\n        def index\n          name = index_name.respond_to?(:to_proc) ? klass.instance_eval(&index_name) : index_name\n          @index = Index.new(name)\n        end\nindex_name.respond_to?(:to_proc) always return true\nI am not sure the issue should be fixed, may add some document?\nUPDATE:\nirb(main):001:0> Mongoid::VERSION\n=> \"3.0.23\"\n. thanks all the same. since there is a solution, will close it.\n. ",
    "naveenagarwal": "Using your tire version with mongoid throws the error when I ran the rake tire:imort:all\nrake aborted!\nundefined method `unit_of_work' for Mongoid:Module\nI tried to search this method in the mongoid gem but it seems has been used in benchmarks but method is not defined anywhere.\n. ",
    "stabenfeldt": "I have the exact same problem. What are we doing wrong? \nTried to do Ad.import to make sure that ElasticSearch had indexed all my Ads first, but that did't work either.\n. @karmi \n``` json\n{\n  ads: {\n    ad: {\n      properties: {\n        $oid: {\n          type: \"string\"\n          }\n        }\n      }\n    }\n}\n```\n. I thought the index was added automatically when I  included Tire::Model::Search and Tire::Model::Callbacks in my model.\nRef what Ryan Bates said http://railscasts.com/episodes/306-elasticsearch-part-1?view=asciicast:\n\nThe first of these modules adds various searching and indexing methods while the second one adds callbacks \nso that when an article is created, updated or destroyed the index is automatically updated.\n. ``` ruby\n2.0.0p353 :162 > Ad.where(title: 'fat').size\n => 1\n2.0.0p353 :163 > Ad.search( 'title:fat').size\n\n2014-01-16 10:14:19:799 [_search] ([\"ads\"])\n\ncurl -X GET 'http://localhost:9200/ads/ad/_search?size=10&pretty' \n-d '{\"query\":{\"query_string\":{\"query\":\"title:fat\"}},\"size\":10}'\n2014-01-16 10:14:19:800 [200] (1 msec)\n\n{\"took\":1,\"timed_out\":false,\"_shards\":{\"total\":5,\"successful\":5,\"failed\":0},\"hits\":{\"total\":0,\"max_score\":null,\"hits\":[]}}\n=> 0\nSome more test queries\n2.0.0p353 :165 > Ad.search( 'title:*').size\n2014-01-16 10:29:15:646 [_search] ([\"ads\"])\n\ncurl -X GET 'http://localhost:9200/ads/ad/_search?size=10&pretty' \n-d '{\"query\":{\"query_string\":{\"query\":\"title:*\"}},\"size\":10}'\n{\"took\":1,\"timed_out\":false,\"_shards\":{\"total\":5,\"successful\":5,\"failed\":0},\"hits\":{\"total\":0,\"max_score\":null,\"hits\":[]}}\n=> 0\n2.0.0p353 :166 > Ad.search( ':').size\n2014-01-16 10:29:19:825 [_search] ([\"ads\"])\n\ncurl -X GET 'http://localhost:9200/ads/ad/_search?size=10&pretty' \n-d '{\"query\":{\"query_string\":{\"query\":\":\"}},\"size\":10}'\n{\"took\":2,\"timed_out\":false,\"_shards\":{\"total\":5,\"successful\":5,\"failed\":0},\"hits\":{\"total\":20,\"max_score\":1.0,\"hits\":[{\"_index\":\"ads\",\"_type\":\"ad\",\"\n[ cut...]\n=> 10\n```\nI can't see any errors here. :( \nAny tips?\n. bash\n\u2717 rake environment tire:import:model CLASS='Ad' FORCE=y\n[IMPORT] Deleting index 'ads'\n[IMPORT] Creating index 'ads' with mapping:\n{\"ad\":{\"properties\":{}}}\nAd:            100% |||||||||||||||||||||||||||||||||||||||||||| Time: 00:00:00\n[IMPORT] Done.\n``` ruby\n$ rails console\n\nAd.search( ':').size\n => 2\nAd.search( 'title:').size\n => 0\nAd.where(title: 'fat').size\n => 1\nAd.where(title: 'fat').first\n => #\n. I'm using **mongoid 4.0.0.alpha2** and  **rails 4.0.0**, if that could have any interference.\n. ruby\nAd.search('')\n[deprecated] I18n.enforce_available_locales will default to true in the future. If you really want to skip validation of your locale you can set I18n.enforce_available_locales = false to avoid this message.\nLoading development environment (Rails 4.0.0)\n2.0.0p353 :001 > Ad.search('*')\n => #2, \"timed_out\"=>false, \"_shards\"=>{\"total\"=>5, \"successful\"=>5, \"failed\"=>0}, \"hits\"=>{\"total\"=>2, \"max_score\"=>1.0, \"hits\"=>[{\"_index\"=>\"ads\", \"_type\"=>\"ad\", \"_id\"=>\"52d78d334d6172d6eb030000\", \"_score\"=>1.0, \"_source\"=>{\"_id\"=>{\"$oid\"=>\"52d78d334d6172d6eb030000\"}, \"_slugs\"=>[\"flott-fat\"], \"address\"=>nil, \"attachment\"=>{\"url\"=>\"https://trenger-images.s3-eu-west-1.amazonaws.com/uploads/ad/attachment/52d78d334d6172d6eb030000/7623480434_7a81d8b982_h.jpg?AWSAccessKeyId=AKIAI7CQW7NR73RDEFDA&Signature=z9Wo%2BEJJ7A5RLelqvmo1DtUeFKA%3D&Expires=1389870686\", \"standard_desktop\"=>{\"url\"=>\"https://trenger-images.s3-eu-west-1.amazonaws.com/uploads/ad/attachment/52d78d334d6172d6eb030000/standard_desktop_7623480434_7a81d8b982_h.jpg?AWSAccessKeyId=AKIAI7CQW7NR73RDEFDA&Signature=pOyCednDA%2BhEl2MjrjB9Hqg4Xwk%3D&Expires=1389870686\"}, \"prospect_desktop\"=>{\"url\"=>\"https://trenger-images.s3-eu-west-1.amazonaws.com/uploads/ad/attachment/52d78d334d6172d6eb030000/prospect_desktop_7623480434_7a81d8b982_h.jpg?AWSAccessKeyId=AKIAI7CQW7NR73RDEFDA&Signature=1YgeJ37BSiCnyS4L5zod2%2FsDlk8%3D&Expires=1389870686\"}, \"thumbnail_desktop\"=>{\"url\"=>\"https://trenger-images.s3-eu-west-1.amazonaws.com/uploads/ad/attachment/52d78d334d6172d6eb030000/thumbnail_desktop_7623480434_7a81d8b982_h.jpg?AWSAccessKeyId=AKIAI7CQW7NR73RDEFDA&Signature=sUxNZuVnT7IJ%2FUMPY8gSBdin2BE%3D&Expires=1389870686\"}, \"standard_tablet\"=>{\"url\"=>\"https://trenger-images.s3-eu-west-1.amazonaws.com/uploads/ad/attachment/52d78d334d6172d6eb030000/standard_tablet_7623480434_7a81d8b982_h.jpg?AWSAccessKeyId=AKIAI7CQW7NR73RDEFDA&Signature=tt5SZXgrrqJBcFwrKWnm1oBWo08%3D&Expires=1389870686\"}, \"prospect_tablet\"=>{\"url\"=>\"https://trenger-images.s3-eu-west-1.amazonaws.com/uploads/ad/attachment/52d78d334d6172d6eb030000/prospect_tablet_7623480434_7a81d8b982_h.jpg?AWSAccessKeyId=AKIAI7CQW7NR73RDEFDA&Signature=dBPdtxYicBHXvXUnrLtY2c9ipAk%3D&Expires=1389870686\"}, \"thumbnail_tablet\"=>{\"url\"=>\"https://trenger-images.s3-eu-west-1.amazonaws.com/uploads/ad/attachment/52d78d334d6172d6eb030000/thumbnail_tablet_7623480434_7a81d8b982_h.jpg?AWSAccessKeyId=AKIAI7CQW7NR73RDEFDA&Signature=sWrM4qNyIbtXybP4t75aJ1%2FEYKk%3D&Expires=1389870686\"}, \"standard_mobile\"=>{\"url\"=>\"https://trenger-images.s3-eu-west-1.amazonaws.com/uploads/ad/attachment/52d78d334d6172d6eb030000/standard_mobile_7623480434_7a81d8b982_h.jpg?AWSAccessKeyId=AKIAI7CQW7NR73RDEFDA&Signature=rpLKNM3RAsSEpPCXBBxOPIy4dKQ%3D&Expires=1389870686\"}, \"prospect_mobile\"=>{\"url\"=>\"https://trenger-images.s3-eu-west-1.amazonaws.com/uploads/ad/attachment/52d78d334d6172d6eb030000/prospect_mobile_7623480434_7a81d8b982_h.jpg?AWSAccessKeyId=AKIAI7CQW7NR73RDEFDA&Signature=ICtM79XQreLwN%2B%2FBcbWrHHvSGEM%3D&Expires=1389870686\"}, \"thumbnail_mobile\"=>{\"url\"=>\"https://trenger-images.s3-eu-west-1.amazonaws.com/uploads/ad/attachment/52d78d334d6172d6eb030000/thumbnail_mobile_7623480434_7a81d8b982_h.jpg?AWSAccessKeyId=AKIAI7CQW7NR73RDEFDA&Signature=%2B5WstJO1aB7hXL8qkAiZKufNIH8%3D&Expires=1389870686\"}}, \"coordinates\"=>nil, \"created_at\"=>\"2014-01-16T07:41:39.602Z\", \"deleted_at\"=>nil, \"description\"=>\"helt ny\", \"price\"=>1, \"state\"=>\"active\", \"title\"=>\"flott fat\", \"updated_at\"=>\"2014-01-16T07:41:39.602Z\", \"user_id\"=>{\"$oid\"=>\"52d78bf64d6172d6eb000000\"}}}, {\"_index\"=>\"ads\", \"_type\"=>\"ad\", \"_id\"=>\"52d78ed14d6172da71000000\", \"_score\"=>1.0, \"_source\"=>{\"_id\"=>{\"$oid\"=>\"52d78ed14d6172da71000000\"}, \"_slugs\"=>[\"fat\"], \"address\"=>nil, \"attachment\"=>{\"url\"=>nil, \"standard_desktop\"=>{\"url\"=>nil}, \"prospect_desktop\"=>{\"url\"=>nil}, \"thumbnail_desktop\"=>{\"url\"=>nil}, \"standard_tablet\"=>{\"url\"=>nil}, \"prospect_tablet\"=>{\"url\"=>nil}, \"thumbnail_tablet\"=>{\"url\"=>nil}, \"standard_mobile\"=>{\"url\"=>nil}, \"prospect_mobile\"=>{\"url\"=>nil}, \"thumbnail_mobile\"=>{\"url\"=>nil}}, \"coordinates\"=>nil, \"created_at\"=>\"2014-01-16T07:48:33.778Z\", \"deleted_at\"=>nil, \"description\"=>\"Neque necessitatibus quo ipsam quis nisi iure dolores aut.\", \"price\"=>2500, \"state\"=>\"active\", \"title\"=>\"fat\", \"updated_at\"=>\"2014-01-16T07:48:33.778Z\", \"user_id\"=>nil}}]}}, @options={:size=>10}, @time=2, @total=2, @facets=nil, @max_score=1.0, @wrapper=Tire::Results::Item>\n2.0.0p353 :002 >\n. ruby\n\n2.0.0p353 :002 >  Ad.create title: \"foobar\"; Ad.index.refresh; Ad.search(\"foobar\").size; Ad.search(\"title:foobar\").size\nArgumentError: wrong number of arguments (0 for 1..2)\n        from .rvm/gems/ruby-2.0.0-p353/bundler/gems/mongoid-0bcffc4da738/lib/mongoid/indexable.rb:93:in index'\n        from (irb):2\n        from .rvm/gems/ruby-2.0.0-p353/gems/railties-4.0.0/lib/rails/commands/console.rb:90:instart'\n        from .rvm/gems/ruby-2.0.0-p353/gems/railties-4.0.0/lib/rails/commands/console.rb:9:in start'\n        from .rvm/gems/ruby-2.0.0-p353/gems/railties-4.0.0/lib/rails/commands.rb:64:in'\n        from bin/rails:4:in require'\n        from bin/rails:4:in'\n. ruby\n\nAd.index.refresh\nArgumentError: wrong number of arguments (0 for 1..2)\n\n```\n. Tried this:\n``` ruby\nTire.index('ads').refresh\n => #\"application/json; charset=UTF-8\", :content_length=>\"60\"}>\n```\nStill cant find any:\nruby\nAd.create title: \"foobar\"; Tire.index('ads').refresh; Ad.search(\"foobar\").size; Ad.search(\"title:foobar\").size\n => 0\n. Yeah, same result with Ad.tire.index.refresh, but I guess that's the same as Tire.index('ads').refresh\n. @karmi, I created a new Rails 4 app that only contains an Ad model. Same problem there.  \nIt would be fantastic if you could have a look at it: https://github.com/stabenfeldt/TireMongoDB :+1: \n. I'm using elasticsearch: stable 0.90.9, HEAD. Installed using brew.\n. @karmi, any chance you had the time to check out the basic app scaffold I set up as proof of concept?\nPerhaps you know about any Rails 4, MongoDB, ElasticSearch example apps I could check out?\n. Thanks @karmi! I really appreciate that you took the time to look into this! :+1: :100: \n. ",
    "marc-villanueva": "Thank you for this! \n. ",
    "anthonator": "Any updates on this?\n. ",
    "kjeremy": "@karmi any news on this?\n. ",
    "robertomiranda": "I'm presenting the same issue, but when I try using curl without method option -X GET and is working\n. Example:\nThis Fails\ncurl -X GET 'http://0.0.0.0:9200/models/model/_search?size=51&pretty' -d '{\"query\":{\"query_string\":{\"query\":\"Roberto*\",\"default_operator\":\"AND\"}},\"sort\":[\"second_name\",\"first_name\"],\"size\":51}'\nThis Works\ncurl  http://0.0.0.0:9200/models/model/_search?size=51&pretty' -d '{\"query\":{\"query_string\":{\"query\":\"Roberto*\",\"default_operator\":\"AND\"}},\"sort\":[\"second_name\",\"first_name\"],\"size\":51}\n. In my case I fixed the issue upgrading a newest version of elasticsearch. I was using 0.19.9 and now I'm using 0.90.7. :smile: \n. ",
    "rrmartins": "Good notice! :blush:\n. Thanks! :blush:\n. I'm closed this issue! :blush:\n. ",
    "meejoe": "@ovamsikrishna I'm afraid I didn't express the problem clearly.\nThe format property in the mapping, 'format' => \"yyyy-MM-dd HH:mm:ss\", doesn't work.\n. ",
    "morphil": "that certainly did it. must have misread to docs.\nthanks\nMor Philosoph\nc: (011) 972 52 6172257\nH: (011) 972  3 6995102\nOn Thu, Dec 26, 2013 at 12:49 PM, Karel Minarik notifications@github.comwrote:\n\nThat's because you have an error in your string query syntax, correct is:\nshould { string \"gender:male\" }\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/karmi/retire/issues/924#issuecomment-31217743\n.\n. \n",
    "suresh4154": "Thanks, I go through the documentation and try to use the high frequency operator and low frequency operator, but It is raising some errors like unknown high_freq_operator.\nIf possible can you please build a search query using elasticsearch-rails by using the high or low frequency operator\n. ",
    "rockaBe": "Could this issue be closed then?\n. Could this issue be closed then?\n. ",
    "me": "Sorry, wrong repo :blush:\n. ",
    "jeroenr": "Thanks for your quick reply. I didn't define the to_indexed_json method. I rely on the default implementation which is based on to_hash. But, now I see indeed that the status attribute is not part of the to_hash output. So, I guess that explains the behaviour. Am I missing something basic?\nAnyway, It might be a good idea to define the to_indexed_json and make sure I include the non-persistent attributes. Or I can override the 'attributes' method and merge my attr_accessor attributes in, so to_hash should pick it up.\n. I found a related issue on stackoverflow http://stackoverflow.com/questions/11986792/non-persistent-activerecord-model-attributes \nSo when I do override the attributes method like is suggested, the 'status' field is present in the JSON generated by the default implementation of to_indexed_json. I believe this is acceptable for me. Anyway, it doesn't seem to be a Tire issue, but more like a general Rails issue.\nI definitely will checkout the new elasticsearch-model gem, thanks for the tip!\n. ",
    "dimanlin": "I am not a strong programmer, but I think it looks neater :)\n. ",
    "anishshah1210": "i already tried with \nrake environment tire:import CLASS='Article.all'\nrake environment tire:import all\nrake environment tire:import CLASS='Article'\nbut error remains same \n. when run \nrake environment tire:import CLASS='Article'\ngives\nundefined method `paginate' for Article:Class\n. I have already tried different version of tire gem like 0.6.0,0.6.2 and 0.5.1\n. can you please suggest me the probable cause of error because I am middle of my development and this doesn't seems  feasible idea to move tire gem to something else\n. i tried the example app. \nrails new searchapp -m https://raw.github.com/karmi/tire/master/examples/rails-application-template.rb\nand put my model code inside \nbut import gives me the same error .. feel very sad!\nmy article.rb\nMIN_TERM_CHARS = 2\n  MAX_NGRAM_CHARS = 20\ninclude SyncAttr\n  include Tire::Model::Persistence\n# TODO: maybe we want to swap synonym and stop word filters.\n  # TODO: maybe we want to swap asciifolding and lowercase filters.\n  # TODO: maybe we want to disable _all field and set default query field.\n  # TODO: we could disable the storing of _source JSON. or compress it.\nsettings \\\n    analysis: {\n      analyzer: {\n        \"default\" => {\n          \"type\" => \"custom\",\n          \"tokenizer\" => \"standard\",\n          \"filter\" => [\"standard\", \"asciifolding\", \"lowercase\", \"munk_length\", \"munk_decompounder\", \"munk_stop\", \"munk_synonym\"]\n        },\n        \"ngram_index_analyzer\" => {\n          \"type\" => \"custom\",\n          \"tokenizer\" => \"standard\",\n          \"filter\"    => [\"standard\", \"asciifolding\", \"lowercase\", \"munk_length\", \"munk_decompounder\", \"munk_stop\", \"munk_synonym\", \"ngram_filter\"]\n        },\n        \"ngram_search_analyzer\" => {\n            \"type\" => \"custom\",\n            \"tokenizer\" => \"standard\",\n            \"filter\"    => [\"standard\", \"asciifolding\", \"lowercase\", \"munk_length\", \"munk_decompounder\", \"munk_stop\", \"munk_synonym\"]\n        },\n        \"hierarchie_index_analyzer\" => {\n          \"type\" => \"custom\",\n          \"tokenizer\" => \"hierarchie_tokenizer\"\n        },\n        \"suggest_analyzer\" => {\n            \"type\" => \"custom\",\n            \"tokenizer\" => \"standard\",\n            # no asciifolding here because we want to autocomplete the actual query of the user. lowercase is necessary\n            # though because of inconsistent writing in article data as well as query strings.\n            # if we add stopwords then we probably want to set \"enable_position_increments\": \"false\".\n            # see http://getelastomer.com/blog/2013/01/searching-with-shingles/#.URAThFrjmoo\n            #\n            \"filter\" => [\"standard\", \"lowercase\", \"suggest_shingle\"]\n        }\n      },\n      tokenizer: {\n        \"hierarchie_tokenizer\" => {\n          \"type\" => \"path_hierarchy\",\n          \"delimiter\" => \"|\",\n        }\n      },\n      filter: {\n        \"munk_decompounder\" => {\n          \"type\" => \"dictionary_decompounder\",\n          \"word_list_path\" => Rails.root.join(\"config\", \"es\", \"munk_compound_words.txt\").to_s,\n        },\n        \"munk_stop\" => {\n          \"type\" => \"stop\",\n          \"stopwords_path\" => Rails.root.join(\"config\", \"es\", \"munk_stop_words.txt\").to_s,\n        },\n        \"munk_synonym\" => {\n          \"type\" => \"synonym\",\n          \"synonyms_path\" => Rails.root.join(\"config\", \"es\", \"munk_synonym_words.txt\").to_s,\n          \"expand\" => true   # we want to expand terms because of compound words\n        },\n        \"munk_length\" => {\n          \"type\" => \"length\",\n          \"min\" => MIN_TERM_CHARS,                     # require at least 2 chars. 2 is better than 3 here because of abbreviations.\n        },\n        \"ngram_filter\" => {\n          \"type\" => \"nGram\",\n          \"min_gram\" => MIN_TERM_CHARS,                # start ngrams with 3 chars. 2 chars are expensive and not neccessary\n          \"max_gram\" => MAX_NGRAM_CHARS    # TODO: longest search word?\n        },\n        \"suggest_shingle\" => {\n          \"type\" => \"shingle\",\n          \"min_shingle_size\" => 2,\n          \"max_shingle_size\" => 3\n        }\n      }\n    }\n# TODO: default index name\n  index_name { Thread.current[:index_name] || \"test\" }\n  document_type \"document\"\n# TODO: set index_options to doc for some fields for optimization\n# TODO: maybe use simple analyzer for 'id' fieldsart\n  property :artikelnummer, type: 'string', index: 'not_analyzed'\n  property :eannummer, type: 'string', index: 'not_analyzed'\nproperty :bezeichnung, type: 'multi_field', fields: {\n    bezeichnung: {type: 'string'},\n    ngram: {:type => 'string', :index_analyzer => 'ngram_index_analyzer', :search_analyzer => 'ngram_search_analyzer'},\n    suggest: {:type => 'string', :analyzer => 'suggest_analyzer'}\n  }\nproperty :bezeichnung_zusatz, type: 'multi_field', fields: {\n    bezeichnung_zusatz: {type: 'string'},\n    ngram: {:type => 'string', :index_analyzer => 'ngram_index_analyzer', :search_analyzer => 'ngram_search_analyzer'}\n  }\nproperty :matchcode, type: 'string', index: 'not_analyzed'\nproperty :mengeneinheit, type: 'string', include_in_all: false\nproperty :gewicht, type: 'float', include_in_all: false\nproperty :hersteller, type: 'multi_field', fields: {\n    hersteller: {type: 'string'},\n    unchanged: {type: 'string', :index => 'not_analyzed'},\n    ngram: {:type => 'string', :index_analyzer => 'ngram_index_analyzer', :search_analyzer => 'ngram_search_analyzer'}\n  }\nproperty :hersteller_nummer, type: 'string', index: 'not_analyzed', include_in_all: false\nproperty :hersteller_artikelnummer, type: 'string', index: 'not_analyzed'\nproperty :gruppe, type: 'multi_field', fields: {\n    gruppe: {type: 'string'},\n    ngram: {:type => 'string', :index_analyzer => 'ngram_index_analyzer', :search_analyzer => 'ngram_search_analyzer'}\n  }\nproperty :gruppe_zusatz, type: 'multi_field', fields: {\n    gruppe_zusatz: {type: 'string'},\n    ngram: {:type => 'string', :index_analyzer => 'ngram_index_analyzer', :search_analyzer => 'ngram_search_analyzer'}\n  }\nproperty :gruppe_nummer, type: 'string', index: 'not_analyzed', include_in_all: false\n#property :hauptgruppe, type: 'multi_field', fields: {\n  #  hauptgruppe: {type: 'string'},\n  #  ngram: {:type => 'string', :index_analyzer => 'ngram_index_analyzer', :search_analyzer => 'ngram_search_analyzer'}\n  #}\n# TODO: n-level facet\n  property :hierarchie, type: 'multi_field', fields: {\n    hierarchie: {type: 'string', index_analyzer: 'hierarchie_index_analyzer', search_analyzer: 'keyword', include_in_all: false},\n    ngram: {type: 'string', index_analyzer: 'ngram_index_analyzer', search_analyzer: 'ngram_search_analyzer'}\n  }\nproperty :langtext, type: 'multi_field', fields: {\n    langtext: {type: 'string'},\n    ngram: {:type => 'string', :index_analyzer => 'ngram_index_analyzer', :search_analyzer => 'ngram_search_analyzer'}\n  }\nproperty :infotext, type: 'multi_field', fields: {\n    infotext: {type: 'string'},\n    ngram: {:type => 'string', :index_analyzer => 'ngram_index_analyzer', :search_analyzer => 'ngram_search_analyzer'}\n  }\nproperty :bestelltext, type: 'multi_field', fields: {\n    bestelltext: {type: 'string'},\n    ngram: {:type => 'string', :index_analyzer => 'ngram_index_analyzer', :search_analyzer => 'ngram_search_analyzer'}\n  }\nproperty :dimension, type: 'string', include_in_all: false\nproperty :listenpreis_netto, type: 'float', index: 'not_analyzed', include_in_all: false\nproperty :listenpreis_brutto, type: 'float', index: 'not_analyzed', include_in_all: false\nvalidates_presence_of :artikelnummer\n  validates_presence_of :bezeichnung\n# TODO: enable later\n  #validates_presence_of :gruppe\n  #validates_presence_of :hauptgruppe\ndef self.set_current_index_name(name)\n    Thread.current[:index_name] = name\n  end\n. when I run this command\nrake environment tire:import CLASS=Article FORCE=true\ni got error\n[IMPORT] Importing 'Article'\nrake aborted!\nundefined method `paginate' for Article:Class\n. yes i have will_paginate in my gem file\nwhen i run it with the trace\nInvoke environment (first_time)\n* Execute environment\n* Invoke tire:import:model (first_time)\n** Execute tire:import:model\n[IMPORT] Importing 'Article'\nrake aborted!\nundefined method paginate' for Article:Class\n/home/helios/.rvm/gems/ruby-1.9.3-p484/gems/tire-0.6.2/lib/tire/index.rb:288:inimport'\n/home/helios/.rvm/gems/ruby-1.9.3-p484/gems/tire-0.6.2/lib/tire/model/import.rb:115:in import'\n/home/helios/.rvm/gems/ruby-1.9.3-p484/gems/tire-0.6.2/lib/tire/model/import.rb:22:inimport'\n/home/helios/.rvm/gems/ruby-1.9.3-p484/gems/tire-0.6.2/lib/tire/tasks.rb:49:in import_model'\n/home/helios/.rvm/gems/ruby-1.9.3-p484/gems/tire-0.6.2/lib/tire/tasks.rb:112:inblock (3 levels) in '\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/task.rb:236:in call'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/task.rb:236:inblock in execute'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/task.rb:231:in each'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/task.rb:231:inexecute'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/task.rb:175:in block in invoke_with_call_chain'\n/home/helios/.rvm/rubies/ruby-1.9.3-p484/lib/ruby/1.9.1/monitor.rb:211:inmon_synchronize'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/task.rb:168:in invoke_with_call_chain'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/task.rb:161:ininvoke'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/application.rb:149:in invoke_task'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/application.rb:106:inblock (2 levels) in top_level'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/application.rb:106:in each'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/application.rb:106:inblock in top_level'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/application.rb:115:in run_with_threads'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/application.rb:100:intop_level'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/application.rb:78:in block in run'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/application.rb:165:instandard_exception_handling'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/lib/rake/application.rb:75:in run'\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/rake-10.1.1/bin/rake:33:in'\n/home/helios/.rvm/gems/ruby-1.9.3-p484/bin/rake:23:in load'\n/home/helios/.rvm/gems/ruby-1.9.3-p484/bin/rake:23:in'\n/home/helios/.rvm/gems/ruby-1.9.3-p484/bin/ruby_executable_hooks:15:in eval'\n/home/helios/.rvm/gems/ruby-1.9.3-p484/bin/ruby_executable_hooks:15:in'\nTasks: TOP => tire:import:model\nabove error occure\n. that specify in my article.rb as in my first comment\nin lines\n93 to 165\n. My console says\nhelios@HS021:~/paragbhai_pro/dynalink$ rake environment tire:import:model CLASS='Article' FORCE=1\n/home/helios/.rvm/gems/ruby-1.9.3-p484@global/gems/bundler-1.5.2/lib/bundler.rb:295: warning: Insecure world writable dir /usr/local/bin in PATH, mode 040777\n[IMPORT] Deleting index 'test'\n[IMPORT] Creating index 'test' with mapping:\n{\"document\":{\"properties\":{\"artikelnummer\":{\"type\":\"string\",\"index\":\"not_analyzed\"},\"eannummer\":{\"type\":\"string\",\"index\":\"not_analyzed\"},\"bezeichnung\":{\"type\":\"multi_field\",\"fields\":{\"bezeichnung\":{\"type\":\"string\"},\"ngram\":{\"type\":\"string\",\"index_analyzer\":\"ngram_index_analyzer\",\"search_analyzer\":\"ngram_search_analyzer\"},\"suggest\":{\"type\":\"string\",\"analyzer\":\"suggest_analyzer\"}}},\"bezeichnung_zusatz\":{\"type\":\"multi_field\",\"fields\":{\"bezeichnung_zusatz\":{\"type\":\"string\"},\"ngram\":{\"type\":\"string\",\"index_analyzer\":\"ngram_index_analyzer\",\"search_analyzer\":\"ngram_search_analyzer\"}}},\"matchcode\":{\"type\":\"string\",\"index\":\"not_analyzed\"},\"mengeneinheit\":{\"type\":\"string\",\"include_in_all\":false},\"gewicht\":{\"type\":\"float\",\"include_in_all\":false},\"hersteller\":{\"type\":\"multi_field\",\"fields\":{\"hersteller\":{\"type\":\"string\"},\"unchanged\":{\"type\":\"string\",\"index\":\"not_analyzed\"},\"ngram\":{\"type\":\"string\",\"index_analyzer\":\"ngram_index_analyzer\",\"search_analyzer\":\"ngram_search_analyzer\"}}},\"hersteller_nummer\":{\"type\":\"string\",\"index\":\"not_analyzed\",\"include_in_all\":false},\"hersteller_artikelnummer\":{\"type\":\"string\",\"index\":\"not_analyzed\"},\"gruppe\":{\"type\":\"multi_field\",\"fields\":{\"gruppe\":{\"type\":\"string\"},\"ngram\":{\"type\":\"string\",\"index_analyzer\":\"ngram_index_analyzer\",\"search_analyzer\":\"ngram_search_analyzer\"}}},\"gruppe_zusatz\":{\"type\":\"multi_field\",\"fields\":{\"gruppe_zusatz\":{\"type\":\"string\"},\"ngram\":{\"type\":\"string\",\"index_analyzer\":\"ngram_index_analyzer\",\"search_analyzer\":\"ngram_search_analyzer\"}}},\"gruppe_nummer\":{\"type\":\"string\",\"index\":\"not_analyzed\",\"include_in_all\":false},\"hierarchie\":{\"type\":\"multi_field\",\"fields\":{\"hierarchie\":{\"type\":\"string\",\"index_analyzer\":\"hierarchie_index_analyzer\",\"search_analyzer\":\"keyword\",\"include_in_all\":false},\"ngram\":{\"type\":\"string\",\"index_analyzer\":\"ngram_index_analyzer\",\"search_analyzer\":\"ngram_search_analyzer\"}}},\"langtext\":{\"type\":\"multi_field\",\"fields\":{\"langtext\":{\"type\":\"string\"},\"ngram\":{\"type\":\"string\",\"index_analyzer\":\"ngram_index_analyzer\",\"search_analyzer\":\"ngram_search_analyzer\"}}},\"infotext\":{\"type\":\"multi_field\",\"fields\":{\"infotext\":{\"type\":\"string\"},\"ngram\":{\"type\":\"string\",\"index_analyzer\":\"ngram_index_analyzer\",\"search_analyzer\":\"ngram_search_analyzer\"}}},\"bestelltext\":{\"type\":\"multi_field\",\"fields\":{\"bestelltext\":{\"type\":\"string\"},\"ngram\":{\"type\":\"string\",\"index_analyzer\":\"ngram_index_analyzer\",\"search_analyzer\":\"ngram_search_analyzer\"}}},\"dimension\":{\"type\":\"string\",\"include_in_all\":false},\"listenpreis_netto\":{\"type\":\"float\",\"index\":\"not_analyzed\",\"include_in_all\":false},\"listenpreis_brutto\":{\"type\":\"float\",\"index\":\"not_analyzed\",\"include_in_all\":false}}}}\n[IMPORT] Importing 'Article'\nrake aborted!\nundefined method `paginate' for Article:Class\n. ",
    "Linuus": "I added this to my hosts file and the errors went away:\n127.0.0.1          localhost\nIt's a bit weird because it also solved the issues in my production environment where elasticsearch is running on another server, i.e. not localhost\n. I have the ENV['ELASTICSEARCH_URL'] = \"http://myserver.com:9200\" in my environment configs,\nstaging.rb, production.rb etc. since I want different urls for different servers.\nIf I use an initializer tire.rb I have to do something like:\nif Rails.env.staging?\n    Tire.config...\nelsif Rails.env.production?\n    Tire.config...\n....\nI mean, the environment config files is supposed to be used for specifying configs for different environments, right?\n. OK, and not use the ENV variable?\n. No, of course :) I meant that I should use\nTire.configure do\n    url....\nend\nand not the ENV style.\nMay I just ask why this would work but not the ENV variant? I mean, Tire uses the ENV if it is set, right? Otherwise it uses the config url or falls back to localhost.\n. Ok, thank you for the explanation!\n. Used after_commit callback instead.\n. Thank you. I used something like this instead:\n[...]\nquery.boolean({:minimum_number_should_match => 1}) do |bool|\n  bool.should { |m| m.match :_all, search_query.strip, fuzziness:  0.7 }\n  bool.should { |m| m.string \"title.autocomplete:#{search_query}\", default_operator: 'AND' }\nend\n[...]\nWhich seems to work fine for me.\n. No, the search_query is not empty\nI guess the escaping of reserved characters is the issue.\nError:\nQueryParsingException[[departments_development_20140515160944] Failed to parse query [title.autocomplete:artikel -]]; nested: ParseException[Cannot parse 'title.autocomplete:artikel -': Encountered \\\"\\\" at line 1, column 28.\\nWas expecting one of:\\n     ...\\n    \\\"(\\\" ...\\n    \\\"\\\" ...\\n     ...\\n     ...\\n     ...\\n     ...\\n     ...\\n    \\\"[\\\" ...\\n    \\\"{\\\" ...\\n     ...\\n     ...\\n    \\\"\\\" ...\\n    ]; nested: ParseException[Encountered \\\"\\\" at line 1, column 28.\\nWas expecting one of:\\n     ...\\n    \\\"(\\\" ...\\n    \\\"\\\" ...\\n     ...\\n     ...\\n     ...\\n     ...\\n     ...\\n    \\\"[\\\" ...\\n    \\\"{\\\" ...\\n     ...\\n     ...\\n    \\\"\\\" ...\\n    ]; }\nHow can I rewrite it to use match instead of string query?\nThis does not work:\nbool.should { |m| m.match \"title.autocomplete\", search_query, default_operator: 'AND', analyzer: 'default' }\n. Perhaps I cannot use default_operator' on a match query?\n. Yes, I have tried :) Thedefault_operatorseems to be calledoperator` in a match query.\nMy real query looks like this, rewritten with match and operator which seems to work:\nquery.boolean({minimum_number_should_match: 1}) do |bool|\n  bool.should { |m| m.match :_all, search_query.strip, fuzziness:  0.7 }\n  bool.should { |m| m.match \"title.autocomplete\", search_query, operator: 'AND', analyzer: 'default' }\n  bool.should { |m| m.match \"name.autocomplete\", search_query, operator: 'AND', analyzer: 'default' }\nend\nFrom what I can see in the integration tests I don't need to manually create a boolean query here, right? It creates one automatically when match is called several times, right?\nBut...\nDoes it use 'should'?\nHow do I specify minimum_number_should_match?\nEDIT:\nI'll just use the above query as it works OK as far as I can see.\n. Thank you for the answer. Yes, I guess a custom_query could do it.\nAs an alternative, would it be possible to create something like this with the Tire DSL:\n{\n  \"query\" : {\n    \"bool\" : {\n      \"must\" : {\n        query_string: {\n          query: \"<search term(s)>\"\n        }\n      },\n      \"should\" : [\n        {\n          \"range\" : {\n            \"publishedAt\" : {\n              \"boost\" : 5,\n              \"gte\" : \"<1 month ago>\"\n            }\n          }\n        },\n        {\n          \"range\" : {\n            \"publishedAt\" : {\n              \"boost\" : 4,\n              \"gte\" : \"<2 months ago>\"\n            }\n          }\n        },\n        {\n          \"range\" : {\n            \"publishedAt\" : {\n              \"boost\" : 3,\n              \"gte\" : \"<3 months ago>\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\nAnd yes, I want to migrate to the new gems but I have some issues with them. Mainly it is the lack of multi model searching, which is essential for me. But I already have an issue for that here: https://github.com/elasticsearch/elasticsearch-rails/issues/203\n. ",
    "sigmarstern": "@karmi please go on, close the ticket. We decided to keep our current project on ES 0.90.* until we managed to migrate to elasticsearch-rails. (I was just asking since I noticed other ES 1.0 related issues being solved here.)\n. ",
    "dfens": "+1\n. ",
    "craigsheen": "Ah sorry this is not actually an issue with tire.\nI was passing the facets in the url as I needed to show them again but filter down on them, When I had large sizes the uri request was becoming too large and causing it to crash.\n. ",
    "codng": "I've read that using UUID is less performant, so it would be nice if it was optional. \nFrom your link it seems you're right, so gonna close this now. The new gem looks great but migrating to it is no easy task, sorry.\n. ",
    "awaisilyastkxel": "I am searching on type and i have 100 of rows in database of type Job but i dont know its returning empty array :(\n. ",
    "Ashviniv": "@awaisilyastkxel check whether they are indexed properly by localhost:9200/index_name/_search?v in the browser. Because I was facing the same problem but with the different query. Solved by indexing all of them.\n. ",
    "sdbondi": "Hmm just noticed that an empty filtered is fine without a bool query: Could be an ES issue? I'll test with a previous version.\njson\n\"query\":{\n \"filtered\":{}\n },\n \"sort\":[\n {\n \"created_at\":\"desc\"\n }\n ],\n \"size\":10,\n \"from\":0\n}\n. Thanks for responding @karmi.\nI want to filter out all the results from the optional query. I have many filters.\nThe ES api is still weird to me.\nHere is my [abridged] query:\n``` ruby\n   self.tire.search(page: params[:page], per_page: params[:per_page]) do |s|\n      s.query do |qry|\n         qry.boolean do |b|\n                b.should { string \"company:#{q}\" }\n                b.should { string \"description:#{q}\" }         \n           end \n       unless cost_events.blank? && fee.blank?\n          qry.filtered do |f|\n              f.filter :terms, avg_cost_events_cd: cost_events unless cost_events.blank?\n              f.filter :terms, fee_cd: fee, execution: :and unless fee.blank?\n            end\n          end\n\n        end\n     end\n  end\n\n```\n. Even without filtered the the query fails with empty hashes:\nQueryParsingException[[(..)] [_na] query malformed, no field after start_object]\njson\n{\n  \"query\":{}, <- Fine without this\n  \"sort\":[\n    {\n      \"created_at\":\"desc\"\n    }\n  ],\n  \"size\":10,\n  \"from\":0\n}\nThe issue is that I have many optional filters and I don't want to have to check if I should call s.query only if I have any of my params not empty. Suppose that I can just check if params.blank? but what if I have other parameters that aren't filtered. Also that is stinky code.\nI think this code worked with ES 1.0.0 - I'll spin up a docker instance of 1.0.0 and see. (can't now poor internet)\n. Not using filtered - which should be inside a query like here and instead just filtering in the search does what I wanted.\n``` ruby\nself.tire.search(page: params[:page], per_page: params[:per_page]) do |s|\n      s.query do |qry|\n         qry.boolean do |b|\n                b.should { string \"company:#{q}\" }\n                b.should { string \"description:#{q}\" }         \n         end \n      end \n  s.filter :terms, avg_cost_events_cd: cost_events unless cost_events.blank?\n  s.filter :terms, fee_cd: fee unless fee.blank?\n\nend\n```\nI think I was using terms as a hash some of the time leading me to think that filter wasn't correct/working.\nWas:\ns.filter :terms => avg_cost_events_cd: cost_events unless cost_events.blank?\nshould be:\ns.filter :terms, avg_cost_events_cd: cost_events unless cost_events.blank?\ni.e as you probably suspected I was being a bit of a noob :P  So I suppose this is more of an elasticsearch/(re)tire docs/helpful error message issue. (Although, turns out it's pretty clear here) :smacks head:\n. ",
    "codomore": "I did in fact use Article.find(id), and it doesnt work .\n. ",
    "callmeed": "So, it should be like this? \nrake environment tire:import CLASS='Article' FORCE=true\nMy only question is that seems to differ from the outline when you rake -T: \nrake tire:import:all                             # Import all indices from `app/models` (or use DIR environment variable)\nrake tire:import:model                           # Import data from your model (pass name as CLASS environment variable)\nrake tire:index:drop                             # Delete indices passed in the INDEX/INDICES environment variable; separate multiple indices by comma\n. ",
    "diclophis": "Oops, this is based from the wrong thing\n. ",
    "taylor-a-barnette": "Apologies - wrong gem, closing and putting this in correct location for elasticsearch-rails.. ",
    "jonstokes": "It works for me.\nOn Apr 12, 2014 3:45 AM, \"Karel Minarik\" notifications@github.com wrote:\n\nIn lib/tire/index.rb:\n\n@@ -364,7 +364,7 @@ def retrieve(type, id, options={})\n       wrapper = options[:wrapper] || Configuration.wrapper\n       if wrapper == Hash then h\n       else\n-        return nil if h['exists'] == false\n-        return nil if (h['exists'] || h['found']) == false\n\n@jonstokes https://github.com/jonstokes This wasn't actually working I\nthink :)\n(false || nil) == false# => false\n\nReply to this email directly or view it on GitHubhttps://github.com/karmi/retire/pull/939/files#r11559105\n.\n. \n"
}